{"id": "1604.01729", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text", "abstract": "14.34 This paper investigates olbia how linguistic knowledge topkap\u0131 mined andjar from large text dyes corpora can udell aid retzer the fifty-seventh generation aviazione of natural euro439 language descriptions semi-sweet of videos. Specifically, martynova we integrate both bestseller a neural greaser language minambres model cazalet and kinescopes distributional semantics stergios trained on rounder large ingush text multi-lane corpora mpeg4 into a recent LSTM - tcherepnin based joujou architecture for sagely video sapta description. testu We evaluate our bluefield approach on a doubletwist collection takaya of sarakhs Youtube videos as well fregosi as pude two large achern movie affixes description aplacental datasets showing significant improvements in grammaticality ananiashvili while half-jewish maintaining zwinglian or modestly improving descriptive quality. mahwire Further, shamburger we manirakiza show 100-passenger that wc2006-eur such waguespack techniques can be astralwerks beneficial for describing him unseen zydrunas object khieu classes with no stoian paired remanded training oswaldo data (zeroshot blaszczykowski captioning ).", "histories": [["v1", "Wed, 6 Apr 2016 19:01:28 GMT  (708kb,D)", "https://arxiv.org/abs/1604.01729v1", null], ["v2", "Tue, 29 Nov 2016 20:37:42 GMT  (1584kb,D)", "http://arxiv.org/abs/1604.01729v2", "Accepted at EMNLP 2016. Project page:this http URL"]], "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["subhashini venugopalan", "lisa anne hendricks", "raymond j mooney", "kate saenko"], "accepted": true, "id": "1604.01729"}, "pdf": {"name": "1604.01729.pdf", "metadata": {"source": "CRF", "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text", "authors": ["Subhashini Venugopalan", "UT Austin", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko"], "emails": ["vsub@cs.utexas.edu", "anne@berkeley.edu", "mooney@cs.utexas.edu", "saenko@bu.edu"], "sections": [{"heading": "1 Introduction", "text": "The ability to automatically describe videos in natural language (NL) enables many important applications including content-based video retrieval and video description for the visually impaired. The most effective recent methods (Venugopalan et al., 2015a; Yao et al., 2015) use recurrent neural networks (RNN) and treat the problem as machine translation (MT) from video to natural language. Deep learning methods such as RNNs need large training corpora; however, there is a lack of highquality paired video-sentence data. In contrast, raw text corpora are widely available and exhibit rich linguistic structure that can aid video description. Most work in statistical MT utilizes both a language model trained on a large corpus of monolingual target language data as well as a translation model trained on more limited parallel bilingual data. This paper explores methods to incorporate knowledge from language corpora to capture general linguistic regularities to aid video description.\nThis paper integrates linguistic information into a video-captioning model based on Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) RNNs which have shown state-of-the-art performance on the task. Further, LSTMs are also effective as language models (LMs) (Sundermeyer et al., 2010). Our first approach (early fusion) is to pre-train the network on plain text before training on parallel video-text corpora. Our next two approaches, inspired by recent MT work (Gulcehre et al., 2015), integrate an LSTM LM with the existing video-to-text model. Furthermore, we also explore replacing the standard one-hot word encoding with distributional vectors trained on external corpora.\nWe present detailed comparisons between the approaches, evaluating them on a standard Youtube corpus and two recent large movie description datasets. The results demonstrate significant improvements in grammaticality of the descriptions (as determined by crowdsourced human evaluations) and more modest improvements in descriptive quality (as determined by both crowdsourced human judgements and standard automated comparison to human-generated descriptions). Our main contributions are 1) multiple ways to incorporate knowledge from external text into an existing captioning model, 2) extensive experiments comparing the methods on three large video-caption datasets, and 3) human judgements to show that external linguistic knowledge has a significant impact on grammar."}, {"heading": "2 LSTM-based Video Description", "text": "We use the successful S2VT video description framework from Venugopalan et al. (2015a) as our\nar X\niv :1\n60 4.\n01 72\n9v 2\n[ cs\n.C L\n] 2\n9 N\nov 2\nunderlying model and describe it briefly here. S2VT uses a sequence to sequence approach (Sutskever et al., 2014; Cho et al., 2014) that maps an input ~x = (x1, ... , xT ) video frame feature sequence to a fixed dimensional vector and then decodes this into a sequence of output words ~y = (y1, ... , yN ).\nAs shown in Fig. 1, it employs a stack of two LSTM layers. The input ~x to the first LSTM layer is a sequence of frame features obtained from the penultimate layer (fc7) of a Convolutional Neural Network (CNN) after the ReLu operation. This LSTM layer encodes the video sequence. At each time step, the hidden control state ht is provided as input to a second LSTM layer. After viewing all the frames, the second LSTM layer learns to decode this state into a sequence of words. This can be viewed as using one LSTM layer to model the visual features, and a second LSTM layer to model language conditioned on the visual representation. We modify this architecture to incorporate linguistic knowledge at different stages of the training and generation process. Although our methods use S2VT, they are sufficiently general and could be incorporated into other CNN-RNN based captioning models."}, {"heading": "3 Approach", "text": "Existing visual captioning models (Vinyals et al., 2015; Donahue et al., 2015) are trained solely on text from the caption datasets and tend to exhibit some linguistic irregularities associated with a restricted language model and a small vocabulary. Here, we investigate several techniques to integrate prior linguistic knowledge into a CNN/LSTM-based network for video to text (S2VT) and evaluate their effectiveness at improving the overall description.\nEarly Fusion. Our first approach (early fusion), is to pre-train portions of the network modeling language on large corpora of raw NL text and then continue \u201cfine-tuning\u201d the parameters on the paired video-text corpus. An LSTM model learns to estimate the probability of an output sequence given an input sequence. To learn a language model, we train the LSTM layer to predict the next word given the previous words. Following the S2VT architecture, we embed one-hot encoded words in lower dimensional vectors. The network is trained on web-scale text corpora and the parameters are learned through backpropagation using stochastic gradient descent.1 The weights from this network are then used to initialize the embedding and weights of the LSTM layers of S2VT, which is then trained on video-text data. This trained LM is also used as the LSTM LM in the late and deep fusion models.\nLate Fusion. Our late fusion approach is similar to how neural machine translation models incorporate a trained language model during decoding. At each step of sentence generation, the video caption model proposes a distribution over the vocabulary. We then use the language model to re-score the final output by considering the weighted average of the sum of scores proposed by the LM as well as the S2VT video-description model (VM). More specifically, if yt denotes the output at time step t, and if pVM and pLM denote the proposal distributions of the video captioning model, and the language models respectively, then for all words y\u2032 \u2208 V in the vocabulary we can recompute the score of each new word, p(yt = y\u2032) as:\n\u03b1 \u00b7 pVM (yt = y\u2032) + (1\u2212 \u03b1) \u00b7 pLM (yt = y\u2032) (1) Hyper-parameter \u03b1 is tuned on the validation set.\nDeep Fusion. In the deep fusion approach (Fig. 2), we integrate the LM a step deeper in the generation process by concatenating the hidden state of the language model LSTM (hLMt ) with the hidden state of the S2VT video description model (hVMt ) and use the combined latent vector to predict the output word. This is similar to the technique proposed by Gulcehre et al. (2015) for incorporating language models trained on monolingual corpora for machine translation. However, our approach differs in two\n1The LM was trained to achieve a perplexity of 120\nkey ways: (1) we only concatenate the hidden states of the S2VT LSTM and language LSTM and do not use any additional context information, (2) we fix the weights of the LSTM language model but train the full video captioning network. In this case, the probability of the predicted word at time step t is:\np(yt|~y<t, ~x) \u221d exp(Wf(hVMt , hLMt ) + b) (2)\nwhere ~x is the visual feature input, W is the weight matrix, and b the biases. We avoid tuning the LSTM LM to prevent overwriting already learned weights of a strong language model. But we train the full video caption model to incorporate the LM outputs while training on the caption domain.\nDistributional Word Representations. The S2VT network, like most image and video captioning models, represents words using a 1-of-N (one hot) encoding. During training, the model learns to embed \u201cone-hot\u201d words into a lower 500d space by applying a linear transformation. However, the embedding is learned only from the limited and possibly noisy text in the caption data. There are many approaches (Mikolov et al., 2013; Pennington et al., 2014) that use large text corpora to learn vector-space representations of words that capture fine-grained semantic and syntactic regularities. We propose to take advantage of these to aid video description. Specifically, we replace the embedding matrix from one-hot vectors and instead use 300-dimensional GloVe vectors (Pennington et al., 2014) pre-trained on 6B tokens from Gigaword and Wikipedia 2014. In addition to using the distributional vectors for the input, we\nalso explore variations where the model predicts both the one-hot word (trained on the softmax loss), as well as predicting the distributional vector from the LSTM hidden state using Euclidean loss as the objective. Here the output vector (yt) is computed as yt = (Wght + bg), and the loss is given by:\nL(yt, wglove) = \u2016(Wght + bg)\u2212 wglove\u20162 (3)\nwhere ht is the LSTM output, wglove is the word\u2019s GloVe embedding and W , b are weights and biases. The network then essentially becomes a multi-task model with two loss functions. However, we use this loss only to influence the weights learned by the network, the predicted word embedding is not used.\nEnsembling. The overall loss function of the video-caption network is non-convex, and difficult to optimize. In practice, using an ensemble of networks trained slightly differently can improve performance (Hansen and Salamon, 1990). In our work we also present results of an ensemble by averaging the predictions of the best performing models."}, {"heading": "4 Experiments", "text": "Datasets. Our language model was trained on sentences from Gigaword, BNC, UkWaC, and Wikipedia. The vocabulary consisted of 72,700 most frequent tokens also containing GloVe embeddings. Following the evaluation in Venugopalan et al. (2015a), we compare our models on the Youtube dataset (Chen and Dolan, 2011), as well as two large movie description corpora: MPII-MD (Rohrbach et al., 2015) and M-VAD (Torabi et al., 2015).\nEvaluation Metrics. We evaluate performance using machine translation (MT) metrics METEOR (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) to compare the machinegenerated descriptions to human ones. For the movie corpora which have just a single description we use only METEOR which is more robust.\nHuman Evaluation. We also obtain human judgements using Amazon Turk on a random subset of 200 video clips for each dataset. Each sentence was rated by 3 workers on a Likert scale of 1 to 5 (higher is better) for relevance and grammar. No video was provided during grammar evaluation. For movies, due to copyright, we only evaluate on grammar."}, {"heading": "4.1 Youtube Video Dataset Results", "text": "Comparison of the proposed techniques in Table 1 shows that Deep Fusion performs well on both METEOR and BLEU; incorporating Glove embeddings substantially increases METEOR, and combining them both does best. Our final model is an ensemble (weighted average) of the Glove, and the two Glove+Deep Fusion models trained on the external and in-domain COCO (Lin et al., 2014) sentences. We note here that the state-of-the-art on this dataset is achieved by HRNE (Pan et al., 2015) (METEOR 33.1) which proposes a superior visual processing pipeline using attention to encode the video.\nHuman ratings also correlate well with the METEOR scores, confirming that our methods give a modest improvement in descriptive quality. However, incorporating linguistic knowledge significantly2 improves the grammaticality of the results, making them more comprehensible to human users. Embedding Influence. We experimented multiple ways to incorporate word embeddings: (1) GloVe input: Replacing one-hot vectors with GloVe on the LSTM input performed best. (2) Fine-tuning: Initializing with GloVe and subsequently fine-tuning the embedding matrix reduced validation results by 0.4 METEOR. (3) Input and Predict. Training the LSTM to accept and predict GloVe vectors, as described in Section 3, performed similar to (1). All scores reported in Tables 1 and 2 correspond to the setting in (1) with GloVe embeddings only as input.\n2Using the Wilcoxon Signed-Rank test, results were significant with p < 0.02 on relevance and p < 0.001 on grammar."}, {"heading": "4.2 Movie Description Results", "text": "Results on the movie corpora are presented in Table 2. Both MPII-MD and M-VAD have only a single ground truth description for each video, which makes both learning and evaluation very challenging (E.g. Fig.3). METEOR scores are fairly low on both datasets since generated sentences are compared to a single reference translation. S2VT\u2020 is a re-implementation of the base S2VT model with the new vocabulary and architecture (embedding dimension). We observe that the ability of external linguistic knowledge to improve METEOR scores on these challenging datasets is small but consistent. Again, human evaluations show significant (with p < 0.0001) improvement in grammatical quality."}, {"heading": "5 Related Work", "text": "Following the success of LSTM-based models on Machine Translation (Sutskever et al., 2014; Bahdanau et al., 2015), and image captioning (Vinyals et al., 2015; Donahue et al., 2015), recent video description works (Venugopalan et al., 2015b; Venugopalan et al., 2015a; Yao et al., 2015) propose CNN-RNN based models that generate a vector representation for the video and \u201cdecode\u201d it using an LSTM sequence model to generate a description. Venugopalan et al. (2015b) also incorporate external data such as images with captions to improve video description, however in this work, our focus\nis on integrating external linguistic knowledge for video captioning. We specifically investigate the use of distributional semantic embeddings and LSTMbased language models trained on external text corpora to aid existing CNN-RNN based video description models.\nLSTMs have proven to be very effective language models (Sundermeyer et al., 2010). Gulcehre et al. (2015) developed an LSTM model for machine translation that incorporates a monolingual language model for the target language showing improved results. We utilize similar approaches (late fusion, deep fusion) to train an LSTM for translating video to text that exploits large monolingual-English corpora (Wikipedia, BNC, UkWac) to improve RNN based video description networks. However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of our approach is that the output of the monolingual language model is used (as an input) when training the full underlying video description network.\nContemporaneous to us, Yu et al. (2015), Pan et al. (2015) and Ballas et al. (2016) propose video description models focusing primarily on improving the video representation itself using a hierarchical visual pipeline, and attention. Without the attention mechanism their models achieve METEOR scores of 31.1, 32.1 and 31.6 respectively on the Youtube dataset. The interesting aspect, as demonstrated in our experiments (Table 1), is that the contribution of language alone is considerable and only slightly less than the visual contribution on this dataset. Hence, it is important to focus on both aspects to generate better descriptions."}, {"heading": "6 Conclusion", "text": "This paper investigates multiple techniques to incorporate linguistic knowledge from text corpora to aid video captioning. We empirically evaluate our approaches on Youtube clips as well as two movie description corpora. Our results show significant improvements on human evaluations of grammar while modestly improving the overall descriptive quality of sentences on all datasets. While the proposed techniques are evaluated on a specific video-caption network, they are generic and can be applied to other video and image captioning models (Hendricks et\nal., 2016; Venugopalan et al., 2016). The code and models are shared on http://vsubhashini. github.io/language_fusion.html."}, {"heading": "Acknowledgements", "text": "This work was supported by NSF awards IIS1427425 and IIS-1212798, and ONR ATL Grant N00014-11-1-010, and DARPA under AFRL grant FA8750-13-2-0026. Raymond Mooney and Kate Saenko also acknowledge support from a Google grant. Lisa Anne Hendricks is supported by the National Defense Science and Engineering Graduate (NDSEG) Fellowship."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate. ICLR", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Delving deeper into convolutional networks for learning video representations. ICLR", "author": ["Li Yao", "Chris Pal", "Aaron C. Courville"], "venue": null, "citeRegEx": "Ballas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballas et al\\.", "year": 2016}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["Chen", "Dolan2011] David Chen", "William Dolan"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "On the properties of neural machine translation: Encoder\u2013 decoder approaches. Syntax, Semantics and Structure in Statistical Translation, page 103", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Meteor universal: Language specific translation evaluation for any target", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": null, "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue et al.2015] Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Gulcehre et al.2015] C. Gulcehre", "O. Firat", "K. Xu", "K. Cho", "L. Barrault", "H.C. Lin", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1503.03535", "citeRegEx": "Gulcehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Neural network ensembles", "author": ["Hansen", "Salamon1990] L.K. Hansen", "P. Salamon"], "venue": "IEEE TPAMI,", "citeRegEx": "Hansen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1990}, {"title": "Deep compositional captioning: Describing novel object categories without paired training data", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Hendricks et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hendricks et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pan et al.2015] Pingbo Pan", "Zhongwen Xu", "Yi Yang", "Fei Wu", "Yueting Zhuang"], "venue": null, "citeRegEx": "Pan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A dataset for movie description", "author": ["Marcus Rohrbach", "Niket Tandon", "Bernt Schiele"], "venue": null, "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "Lstm neural networks for language modeling", "author": ["R. Schluter", "H. Ney"], "venue": "In INTERSPEECH", "citeRegEx": "Sundermeyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["Torabi et al.2015] Atousa Torabi", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": null, "citeRegEx": "Torabi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Torabi et al\\.", "year": 2015}, {"title": "Sequence to sequence - video", "author": ["M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Captioning images with diverse objects. arXiv preprint arXiv:1606.07770", "author": ["L.A. Hendricks", "M. Rohrbach", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Yu et al.2015] Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Further, LSTMs are also effective as language models (LMs) (Sundermeyer et al., 2010).", "startOffset": 59, "endOffset": 85}, {"referenceID": 6, "context": "Our next two approaches, inspired by recent MT work (Gulcehre et al., 2015), integrate an LSTM LM with the existing video-to-text model.", "startOffset": 52, "endOffset": 75}, {"referenceID": 18, "context": "We use the successful S2VT video description framework from Venugopalan et al. (2015a) as our ar X iv :1 60 4.", "startOffset": 60, "endOffset": 87}, {"referenceID": 16, "context": "S2VT uses a sequence to sequence approach (Sutskever et al., 2014; Cho et al., 2014) that maps an input ~x = (x1, .", "startOffset": 42, "endOffset": 84}, {"referenceID": 3, "context": "S2VT uses a sequence to sequence approach (Sutskever et al., 2014; Cho et al., 2014) that maps an input ~x = (x1, .", "startOffset": 42, "endOffset": 84}, {"referenceID": 21, "context": "Existing visual captioning models (Vinyals et al., 2015; Donahue et al., 2015) are trained solely on text from the caption datasets and tend to exhibit some linguistic irregularities associated with a restricted language model and a small vocabulary.", "startOffset": 34, "endOffset": 78}, {"referenceID": 5, "context": "Existing visual captioning models (Vinyals et al., 2015; Donahue et al., 2015) are trained solely on text from the caption datasets and tend to exhibit some linguistic irregularities associated with a restricted language model and a small vocabulary.", "startOffset": 34, "endOffset": 78}, {"referenceID": 6, "context": "This is similar to the technique proposed by Gulcehre et al. (2015) for incorporating language models trained on monolingual corpora for machine translation.", "startOffset": 45, "endOffset": 68}, {"referenceID": 10, "context": "There are many approaches (Mikolov et al., 2013; Pennington et al., 2014) that use large text corpora to learn vector-space representations of words that capture fine-grained semantic and syntactic regularities.", "startOffset": 26, "endOffset": 73}, {"referenceID": 13, "context": "There are many approaches (Mikolov et al., 2013; Pennington et al., 2014) that use large text corpora to learn vector-space representations of words that capture fine-grained semantic and syntactic regularities.", "startOffset": 26, "endOffset": 73}, {"referenceID": 13, "context": "Specifically, we replace the embedding matrix from one-hot vectors and instead use 300-dimensional GloVe vectors (Pennington et al., 2014) pre-trained on 6B tokens from Gigaword and Wikipedia 2014.", "startOffset": 113, "endOffset": 138}, {"referenceID": 14, "context": "(2015a), we compare our models on the Youtube dataset (Chen and Dolan, 2011), as well as two large movie description corpora: MPII-MD (Rohrbach et al., 2015) and M-VAD (Torabi et al.", "startOffset": 134, "endOffset": 157}, {"referenceID": 17, "context": ", 2015) and M-VAD (Torabi et al., 2015).", "startOffset": 18, "endOffset": 39}, {"referenceID": 16, "context": "Following the evaluation in Venugopalan et al. (2015a), we compare our models on the Youtube dataset (Chen and Dolan, 2011), as well as two large movie description corpora: MPII-MD (Rohrbach et al.", "startOffset": 28, "endOffset": 55}, {"referenceID": 12, "context": "We evaluate performance using machine translation (MT) metrics METEOR (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) to compare the machinegenerated descriptions to human ones.", "startOffset": 107, "endOffset": 130}, {"referenceID": 11, "context": "We note here that the state-of-the-art on this dataset is achieved by HRNE (Pan et al., 2015) (METEOR 33.", "startOffset": 75, "endOffset": 93}, {"referenceID": 16, "context": "Following the success of LSTM-based models on Machine Translation (Sutskever et al., 2014; Bahdanau et al., 2015), and image captioning (Vinyals et al.", "startOffset": 66, "endOffset": 113}, {"referenceID": 0, "context": "Following the success of LSTM-based models on Machine Translation (Sutskever et al., 2014; Bahdanau et al., 2015), and image captioning (Vinyals et al.", "startOffset": 66, "endOffset": 113}, {"referenceID": 21, "context": ", 2015), and image captioning (Vinyals et al., 2015; Donahue et al., 2015), recent video description works (Venugopalan et al.", "startOffset": 30, "endOffset": 74}, {"referenceID": 5, "context": ", 2015), and image captioning (Vinyals et al., 2015; Donahue et al., 2015), recent video description works (Venugopalan et al.", "startOffset": 30, "endOffset": 74}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015), and image captioning (Vinyals et al., 2015; Donahue et al., 2015), recent video description works (Venugopalan et al., 2015b; Venugopalan et al., 2015a; Yao et al., 2015) propose CNN-RNN based models that generate a vector representation for the video and \u201cdecode\u201d it using an LSTM sequence model to generate a description. Venugopalan et al. (2015b) also incorporate external data such as images with captions to improve video description, however in this work, our focus", "startOffset": 8, "endOffset": 383}, {"referenceID": 15, "context": "LSTMs have proven to be very effective language models (Sundermeyer et al., 2010).", "startOffset": 55, "endOffset": 81}, {"referenceID": 6, "context": "Gulcehre et al. (2015) developed an LSTM model for machine translation that incorporates a monolingual language model for the target language showing improved results.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "Gulcehre et al. (2015) developed an LSTM model for machine translation that incorporates a monolingual language model for the target language showing improved results. We utilize similar approaches (late fusion, deep fusion) to train an LSTM for translating video to text that exploits large monolingual-English corpora (Wikipedia, BNC, UkWac) to improve RNN based video description networks. However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of our approach is that the output of the monolingual language model is used (as an input) when training the full underlying video description network.", "startOffset": 0, "endOffset": 432}, {"referenceID": 20, "context": "Contemporaneous to us, Yu et al. (2015), Pan et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 10, "context": "(2015), Pan et al. (2015) and Ballas et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 1, "context": "(2015) and Ballas et al. (2016) propose video description models focusing primarily on improving the video representation itself using a hierarchical visual pipeline, and attention.", "startOffset": 11, "endOffset": 32}], "year": 2016, "abstractText": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.", "creator": "LaTeX with hyperref package"}}}