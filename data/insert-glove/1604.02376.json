{"id": "1604.02376", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2016", "title": "Finding Optimal Combination of Kernels using Genetic Programming", "abstract": "chinh In sheung Computer Vision, scorpene problem of identifying arrangers or keels classifying reposado the jarai objects present kuibyshev in an expressions image tournment is commercials called regathered Object deputed Categorization. nantahala It is a allmusic.com challenging sheave problem, especially 1-3-1 when the images 5,930 have clutter background, santam occlusions thin or different leonians lighting chah conditions. cearns Many vision features have heptagon been autotransformer proposed willemstad which 600-kilometer aid jewboy object categorization even 30.37 in rockface such dobyns adverse post-baccalaureate conditions. Past fallersleben research has shown that, przy\u0142\u0119k employing multiple lamsdorf features rather ludgrove than alwis any 75-megahertz single sovan features leads csupo to better recognition. 91-80 Multiple Kernel Learning (MKL) diebner framework has niki been \u00f6berg developed for yayr learning genotypic an congolese optimal combination of bicoid features for object categorization. adjacently Existing isaccea MKL exploits methods use linear gabbi combination of base sibilant kernels which abcl may not be optimal e46 for object categorization. propping Real - koryo world ijebu object wickedest categorization campestris may m.n. need to kavandi consider complex esposizione combination juru of kernels (ferriter non - aix-la-chapelle linear) oldster and m.p.h. not bolinder only linear combination. heron Evolving non - surveilled linear foster-mother functions 1979/1980 of hanumantha base kernels using Genetic Programming is proposed darleen in this broad-winged report. Experiment penrhos results rampage show liquid that amiya non - kernel carriages generated using epicycles genetic dehart programming gives good tuberosa accuracy emcee as compared cuajimalpa to guttata linear combination re-emerges of kernels.", "histories": [["v1", "Fri, 8 Apr 2016 15:33:30 GMT  (451kb,D)", "http://arxiv.org/abs/1604.02376v1", null], ["v2", "Fri, 22 Apr 2016 23:16:20 GMT  (668kb,D)", "http://arxiv.org/abs/1604.02376v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jyothi korra"], "accepted": false, "id": "1604.02376"}, "pdf": {"name": "1604.02376.pdf", "metadata": {"source": "CRF", "title": "Finding Optimal Combination of Kernels using Genetic Programming", "authors": ["Jyothi Korra"], "emails": ["jyothi.korra@christuniversity.in"], "sections": [{"heading": null, "text": "In Computer Vision, problem of identifying or classifying the objects present in an image is called Object Categorization. It is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. Many vision features have been proposed which aid object categorization even in such adverse conditions. Past research has shown that, employing multiple features rather than any single features leads to better recognition. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of features for object categorization. Existing MKL methods use linear combination of base kernels which may not be optimal for object categorization. Real-world object categorization may need to consider complex combination of kernels(non-linear) and not only linear combination. Evolving non-linear functions of base kernels using Genetic Programming is proposed in this report. Experiment results show that non-kernel generated using genetic programming gives good accuracy as compared to linear combination of kernels."}, {"heading": "1 Introduction", "text": "Object Categorization is the problem of categorizing the given image into predefined classes of objects like face, car, bike etc. This problem is well-studied in the field of Computer Vision. Face recognition is a less harder problem than Object Categorization. There are systems where computer can identify expressions and morph faces automatically [1]. Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. When a computer vision system or computer vision algorithm is designed the choice of feature representation can be a critical issue. In some cases, a higher level of detail in the description of a feature may be necessary for solving the problem, but this comes at the cost of having to deal with more data and more demanding processing. In this report, an instance of a feature representation is referred to as a (feature) descriptor.\nIn some applications like object categorization it is not sufficient to extract only one type of feature to obtain the relevant information from the image data. Instead many feature descriptors have been proposed which aid object categorization even in those adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative. Past research has shown that employing multiple feature descriptors rather than any single descriptor leads to better recognition. This report focuses on the problem of learning the optimal combination of the available descriptors for a particular classification task. Labels for classification task are mostly obtained by manual labelling process or through crowd-sourcing [2]\nar X\niv :1\n60 4.\n02 37\n6v 1\n[ cs\n.C V\n] 8\nA pr\nIn [3, 4, 5, 6], the authors employ the Multiple Kernel Learning (MKL) framework to find the optimal combination of feature descriptors (kernels). The goal of MKL is to simultaneously optimize the combination of kernels and the usual classification objective. Existing MKL methods for combining kernels are linear combinations of base kernels (see figure 1). But non-linear combination of base kernels may be helpful to boost performance of the classifier. This would be ideal for applications such as object categorization, in which a combination of the descriptors is known to perform better than any single descriptor.\nThe new hybrid model which uses Genetic Programming, for evolving kernel from base kernels and Support Vector Machine (SVM), for finding classifier function using evolved kernel from GP is proposed. This is the first attempt to have non-linear kernel combination for object categorization. Experiments will be conducted on datasets like Caltech-5, Caltech-101 etc to verify advantage of this hybrid model for improving object categorization."}, {"heading": "2 Organization of this manual", "text": "The outline of the report is as follows: section 3 discusses the past work in this area. section ?? briefly reviews genetic programming and its advantages. section 4 describes the methodology for carrying out object categorization using genetic programming using non-linear kernels. Section 7 gives the implementation details and results on two datasets. This is followed by the conclusion."}, {"heading": "3 Past Work", "text": "There have been some work in genetic programming for evolving kernels for Support Vector Machine [7, 8, 9]. [9] uses Genetic Programming for evolving the kernel for SVM classifier. The approach presented there combines the two techniques of SVMs and GP, using the GP to evolve a kernel for a SVM. The goal there is to eliminate the need for testing various kernels and their parameter settings. They claim the approach might also be possible to discover new kernels that are particularly useful for the type of data under analysis. They show that their method performs better than manual choosing of the kernel and adjusting parameters. [8] uses a set of standard kernels for evolving expression for new kernel which performs better for given problem using genetic programming. Terminal set contains feature vectors, first level from terminals in the GP trees contains only standard kernels defined before-hand. Variable set contains functions which take two kernels as arguments and provides a kernel as output. [7] tries to learn a regression function where kernels act as the regression variables. Each GP chromosome gives the complex combination of the set of kernels that is defined already. This is closely related to [7], where they try to evolve regression function using GP where kernels acts as regression variables. This report studies how these non-linear function of base kernels in the case descriptors affect the performance of the object categorization and tries to find a way to reduce the time taken for evolving functions using GP. Even though these try to evolve kernels, no work has been done on evolving non-linear kernels for object categorization. The state-of-the-art work in object categorization considers many descriptors and try to find the optimal combination of the descriptors. [3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning. Descriptors are extracted from the image and each descriptors will have many kernels formed using the feature vector of the descriptors. And these kernels are combined using the Multiple Kernel Learning(MKL) in Support Vector Machine(SVM) framework. The principle idea is to combine kernels linearly. But real-world object categorization may perform better when we have non-linear combination of these descriptors. In our work we find non-linear kernel combination for improving performance. The next section explains genetic programming and how to evolve non-linear kernel combination using GP."}, {"heading": "4 Optimal Combination of Kernels using Genetic Programming", "text": "For aiding object categorization in adverse conditions, many descriptors (and instance of feature representation) have been proposed in the computer vision literature. Suppose there are n descriptors d1, d2, ..., dn extracted from the m images I1, I2, ..., Im. Using these n descriptors, n kernels\nK1,K2, ...,Kn of size mXm are formed. Steps involved for producing the hybrid classifier using GP and SVM is described below."}, {"heading": "5 Steps Involved", "text": "\u2022 Create a random population of kernel functions, represented as trees \u2022 Evaluate the fitness of each individual by building an SVM from the kernel tree and test it\non the validation data\n\u2022 Select the fitter kernel trees as parents for recombination \u2022 Perform random crossover and mutation on the newly created offspring \u2022 Replace the old population with the offspring \u2022 Repeat Steps 2 to 5 until the population has converged \u2022 Build final SVM using the fittest kernel tree found from GP"}, {"heading": "6 Parameters for GP", "text": "The parameters for GP involves defining terminal set, function set and fitness function. Terminal set = {K1,K2,K3, ...,Kn}, where Ki is the kernel formed from any of the descriptors. Function set = {+, \u2217}. Fitness function is the classification error of the particular chromosome on the training set. That is fitness value for each chromosome in this GP will be based on the accuracy of SVM with that chromosome (the non-linear kernel combination given to SVM). One alternative is to base the fitness on a cross-validation test (e.g. leave-one-out cross-validation) in order to give a better estimation of a kernel trees ability to produce a model that generalizes well to unseen data."}, {"heading": "7 Results", "text": "The proposed idea was validated using real-world object datasets like Caltech-5 and Caltech-101. Caltech-5 contains five classes of objects cars, aeroplane, faces, leopards and bikes. Caltech-101\ncontains 101 categories of objects. Each category contains roughly from 30-100 images. Accuracy of the proposed method is compared to the best kernel (K1,K2, ...,Kn) and addition kernel which is K1 + K2 + ... + Kn. All the experiments follow 1-Vs-1 SVM classification method. Binary classification mentioned in the following section is carried out by taking two classes at a time and accuracy is found on these two classes."}, {"heading": "8 Results on Caltech-5 dataset", "text": "This section presents results on Caltech-5 using new MKL formulation and descriptors(csift, opponentsift, rgsift, sift, transformedcolorsift) provided from ColorDescriptor software. The experiments in this section are carried out using descriptors available from ColorDescriptor software Caltech-5 dataset contains images of airplanes, cars, faces, leopards and bikes. We have generated kernels on 5 descriptors provided using Gaussian kernel. This experimental procedure was repeated 10 times with different training-test data splits. It can be seen from Table 1 that the non-linear kernel method is giving better accuracy as compared to the best kernel and the addition of kernels. Figure 6 shows plot of mean accuracy as number of iterations. Note that in all the iterations, proposed non-linear kernel is better than other kernel combinations. Figure 7 shows plot of mean accuracy as number of binary classifier on Caltech5, totally 10 binary classification problem. Note that non-linear kernel combination gives higher accuracy than other kernel combinations in all binary classifications. Figure 8 shows non-linear kernel tree generated from GP which gives high accuracy than others on binary classification problem 9 in previous graph. In other binary classification problem, GP ends in selecting best kernel. Figure 9 10 11 shows some non-linear kernel tree generated from GP for Caltech5 dataset."}, {"heading": "9 Results on Caltech-101 dataset", "text": "This section presents results on Caltech-101 using new non-linear kernel combination and six kernels are taken from ucsd dataset We have taken 30 images for each class, of which 15 are randomly taken as the training in which 5 are taken for validation data and the remaining as test data. This experimental procedure was repeated 5 times with different training-test data splits. It can be seen from Table 1 that the non-linear kernel method is giving better accuracy as compared to the best kernel and the addition of kernels. Figure 13 shows plot of mean accuracy as number of iterations in Caltech 101 dataset. Note that in all the iterations proposed non-linear kernel is the best. Figure 14 shows non-linear kernel tree generated from GP for Caltech101 dataset. This kernel tree is nothing but the addition kernel, which is generated for iteration 4 in Caltech101(see figure 13) where GP non-kernel and addition kernel give almost same accuracy. Figure 15 shows some non-linear kernel tree generated from GP for Caltech101 dataset."}, {"heading": "10 Genetic Programming for Similar Images", "text": "Generated kernels from the previous sections are used for creating a demo for similar images. 1530 images from Caltech-101 have been taken by us and the kernels generated using the procedures discussed earlier. A matrix M is generated which has dimension 1530 X 1530. M(i, j) refers to the similarity between the ith and jth image. Five descriptors are generated for the images and therefore we have five kernels. If the addition of kernels is used then the kernels are combined linearly to find M. So\nM = K1 +K2 +K3 +K4 +K5\n. We have used a non-linear combination of kernels found by using the GP. So, for example, M maybe\nM = square(K1) +K1 \u2217K2 +K5\nIf a user clicks on the ith image, from the ith row in matrix M, the minimum element is selected and the image corresponding to that is selected as the most similar image. The next minimum value gives the next most similar image and so on. These results are displayed and shown to the user.\nFigure 12 shows how the similar image demo works. When an image is clicked [11], it shows the images most similar to that image."}, {"heading": "11 Conclusions", "text": "This paper proposed non-linear kernel combination using genetic programming. This eliminates the need for user to create non-linear kernel combination. Proposed framework is applied to Object Categorization. Experiments results shows that proposed framework for non-linear kernel combination using GP performance better than existing state-of-the-art kernel combinations."}], "references": [{"title": "Application of active appearance model to automatic face replacement", "author": ["D. Govindaraj"], "venue": "Journal of Applied Statistics, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Moneybee: Towards enabling a ubiquitous, efficient, and easy-to-use mobile crowdsourcing service in the emerging market", "author": ["D. Govindaraj", "N.K.V.M.", "A. Nandi", "G. Narlikar", "V. Poosala"], "venue": "Bell Labs Technical Journal, vol. 15, no. 4, pp. 79\u201392, 2011. [Online]. Available: http://dx.doi.org/10.1002/bltj.20473", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Support kernel machines for object recognition", "author": ["A. Kumar", "C. Sminchisescu"], "venue": "ICCV, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning the discriminative power-invariance trade-off", "author": ["M. Varma", "D. Ray"], "venue": "ICCV, 2007, pp. 1\u20138.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "On the algorithmics and applications of a mixed-norm based kernel learning formulation", "author": ["S.N. Jagarlapudi", "D. Govindaraj", "R. S", "C. Bhattacharyya", "A. Ben-tal", "R.K.r."], "venue": "pp. 844\u2013852, 2009. [Online]. Available: http://papers.nips.cc/paper/ 3880-on-the-algorithmics-and-applications-of-a-mixed-norm-based-kernel-learning-formulation.pdf", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Controlled sparsity kernel learning", "author": ["D. Govindaraj", "S. Raman", "S. Menon", "C. Bhattacharyya"], "venue": "CoRR, vol. abs/1401.0116, 2014. [Online]. Available: http://arxiv.org/abs/1401.0116", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Genetically designed multiple-kernels for improving the svm performance", "author": ["L. Diosan", "M. Oltean", "A. Rogozan", "J.P. Pecuchet"], "venue": "GECCO \u201907: Proceedings of the 9th annual conference on Genetic and evolutionary computation. New York, NY, USA: ACM, 2007, pp. 1873\u20131873.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Evolving kernels for support vector machine classification", "author": ["K.M. Sullivan", "S. Luke"], "venue": "GECCO \u201907: Proceedings of the 9th annual conference on Genetic and evolutionary computation. New York, NY, USA: ACM, 2007, pp. 1702\u20131707.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "The genetic evolution of kernels for support vector machine classifiers", "author": ["T. Howley", "M.G. Madden"], "venue": "In 15th Irish Conference on Artificial Intelligence, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Sparse classifier design based on the shapley value", "author": ["P. Ravipally", "D. Govindaraj"], "venue": "Proceedings of the World Congress on Engineering, vol. 1, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling attractiveness and multiple clicks in sponsored search results", "author": ["D. Govindaraj", "T. Wang", "S.V.N. Vishwanathan"], "venue": "CoRR, vol. abs/1401.0255, 2014. [Online]. Available: http://arxiv.org/abs/1401.0255 10", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "There are systems where computer can identify expressions and morph faces automatically [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "Labels for classification task are mostly obtained by manual labelling process or through crowd-sourcing [2]", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "In [3, 4, 5, 6], the authors employ the Multiple Kernel Learning (MKL) framework to find the optimal combination of feature descriptors (kernels).", "startOffset": 3, "endOffset": 15}, {"referenceID": 3, "context": "In [3, 4, 5, 6], the authors employ the Multiple Kernel Learning (MKL) framework to find the optimal combination of feature descriptors (kernels).", "startOffset": 3, "endOffset": 15}, {"referenceID": 4, "context": "In [3, 4, 5, 6], the authors employ the Multiple Kernel Learning (MKL) framework to find the optimal combination of feature descriptors (kernels).", "startOffset": 3, "endOffset": 15}, {"referenceID": 5, "context": "In [3, 4, 5, 6], the authors employ the Multiple Kernel Learning (MKL) framework to find the optimal combination of feature descriptors (kernels).", "startOffset": 3, "endOffset": 15}, {"referenceID": 6, "context": "3 Past Work There have been some work in genetic programming for evolving kernels for Support Vector Machine [7, 8, 9].", "startOffset": 109, "endOffset": 118}, {"referenceID": 7, "context": "3 Past Work There have been some work in genetic programming for evolving kernels for Support Vector Machine [7, 8, 9].", "startOffset": 109, "endOffset": 118}, {"referenceID": 8, "context": "3 Past Work There have been some work in genetic programming for evolving kernels for Support Vector Machine [7, 8, 9].", "startOffset": 109, "endOffset": 118}, {"referenceID": 8, "context": "[9] uses Genetic Programming for evolving the kernel for SVM classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] uses a set of standard kernels for evolving expression for new kernel which performs better for given problem using genetic programming.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] tries to learn a regression function where kernels act as the regression variables.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This is closely related to [7], where they try to evolve regression function using GP where kernels acts as regression variables.", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "[3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning.", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "[3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning.", "startOffset": 0, "endOffset": 16}, {"referenceID": 4, "context": "[3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning.", "startOffset": 0, "endOffset": 16}, {"referenceID": 5, "context": "[3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "[3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning.", "startOffset": 0, "endOffset": 16}], "year": 2017, "abstractText": "In Computer Vision, problem of identifying or classifying the objects present in an image is called Object Categorization. It is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. Many vision features have been proposed which aid object categorization even in such adverse conditions. Past research has shown that, employing multiple features rather than any single features leads to better recognition. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of features for object categorization. Existing MKL methods use linear combination of base kernels which may not be optimal for object categorization. Real-world object categorization may need to consider complex combination of kernels(non-linear) and not only linear combination. Evolving non-linear functions of base kernels using Genetic Programming is proposed in this report. Experiment results show that non-kernel generated using genetic programming gives good accuracy as compared to linear combination of kernels.", "creator": "LaTeX with hyperref package"}}}