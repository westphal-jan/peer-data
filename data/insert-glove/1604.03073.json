{"id": "1604.03073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Reservoir computing for spatiotemporal signal classification without trained output weights", "abstract": "Reservoir computing stampa is toras a formula recently introduced a5 machine learning mswati paradigm go-go that has lewit been shown irreversibly to kudirat be well - inadmissable suited ard\u00e8che for isl101 the enzhu processing bennani of spatiotemporal sharemarket data. Rather than ubinas training the 30,000-capacity network node easterday connections and 75-mile weights via backpropagation berler in traditional 144.2 recurrent neural moravcik networks, reservoirs 1,912 instead have balogun fixed 175-nation connections iser and weights fre among takaki the ` mahna hidden 1899 layer ' hatzfeld nodes, phrased and gruener traditionally only 49.0 the weights egelhoff to panahi the mohlglobe.com output layer ozar of symphonique neurons are actions trained using anzali linear regression. vonappen We claim rukungiri that for backpressure signal joachim classification tasks, bebeto one 123.10 may forgo gedeck the dezcallar weight training step longbenton entirely and instead use shengelia a jebsen simple souvlaki supervised f-5a clustering method. euro328 The proposed method is 5.0-5 analyzed 1932-1933 theoretically m16s and humboldt explored through numerical 9.612 experiments on real - narrated world data. dallastown The examples demonstrate vega@globe.com that competetive the proposed keiper clustering ravenscroft method outperforms the inefficient traditional trained output weight approach in terms of hawdon speed, accuracy, dony and middle-aged sensitivity idiakez to dukowski reservoir parameters.", "histories": [["v1", "Mon, 11 Apr 2016 19:14:05 GMT  (100kb,D)", "http://arxiv.org/abs/1604.03073v1", "10 pages, 5 figures"], ["v2", "Tue, 19 Jul 2016 13:28:17 GMT  (547kb,D)", "http://arxiv.org/abs/1604.03073v2", "12 pages, 5 figures"]], "COMMENTS": "10 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["ashley prater"], "accepted": false, "id": "1604.03073"}, "pdf": {"name": "1604.03073.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ashley Prater"], "emails": [], "sections": [{"heading": "Introduction", "text": "Reservoir computing is a recently developed bio-inspired machine learning paradigm for the processing of spatiotemporal data [1, 2]. In the language of neural networks, a reservoir is the collection of hidden layer nodes with nonlinear recurrent dynamics, where the nodes are sparsely connected with fixed weights that are not trained to fit specific data. Because the weights are fixed, using a reservoir requires only a simple initialization step, as opposed to more traditional recurrent neural networks whose weights and connections must be learned in a tedius backpropagation training step [3]. Strengths of the reservoir design include the ease of initialization, along with a reservoir\u2019s ability to promptly adapt to new data and applications.\nReservoirs, like all recurrent neural networks, are based on the premise that the state of the reservoir at a particular time should depend on the current value of the input signal, along with recent inputs and reservoir states. To be an effective method for computation, a reservoir should map input data into a sufficiently highdimensional space. It is desirable for a reservoir to operate \u2018at the edge of chaos\u2019 [4], so dissimilar inputs are sufficiently separated in the reservoir node states, yet inputs with only small perturbation-like differences do not stray too far apart. Reservoir dynamics demonstrate long short-term memory [5], so any individual point-wise errors in a signal will not corrupt the entire reservoir response.\nTwo reservoir variants that have emerged in literature are echo state networks (ESNs) and time-delay reservoirs (TDRs). An ESN uses randomly, yet sparsely, connected nodes with randomly assigned fixed weights [1, 2, 6]. A TDR uses a cyclic topology, where each node is connected to exactly two other nodes with fixed, non-random weights [7, 8, 9].\nThe output layer of both ESN and TDR-type reservoirs almost always use linear output weights, trained on a labeled dataset using least squares or ridge regression[1, 10, 11]. Although effective, this method is computationally expensive to apply to test data, since it requires many matrix-vector multiplications and comparisons. The cost is so high that it is difficult to use this method in real-time to classify incoming signals.\nIn this research, a simple supervised clustering method is proposed for use in classification tasks using ESNs and TDRs. The method used is based upon comparing the reservoir response of a signal against the principal components of the reservoir states for classes of labeled training data. The clustering method is shown to have lower computational complexity than using trained output weights to classify new input\n\u2217Cleared for public release by WPAFB Public Affairs on 11 April 2016. Case Number: 88ABW-2016-1812. \u2020Air Force Research Laboratory, Information Directorate, Rome, NY 13441. ashley.prater.3@us.af.mil\nar X\niv :1\n60 4.\n03 07\n3v 1\n[ cs\n.N E\n] 1\n1 A\npr 2\n01 6\nsignals. We present a rigorous analysis of the clustering method, including two theoreoms characterizing the upper bound of the difference in reservoir responses for two input signals, with the upper bound in terms of the input signals, the reservoir type, and the user-generated parameters. Moreover, we explore the difference in performance of the two methods through numerical simulations performed using a real-world dataset for both ESNs and TDRs for various reservoir parameters. In every simulation, the clustering approach outperforms the trained output weights in terms of both accuracy and CPU time required to classify test signals.\nThe following notation is used in this work. For a collection of signals {u}, the jth element in the collection is denoted by u(j). Training sets are partitioned into K classes. Let Ck be the collection of indices of signals in the kth class. That is, u(j) is in the kth class iff j \u2208 Ck. For a vector v, the `2 norm is given by \u2016v\u20162 = ( \u2211 j v 2 j ) 1/2, and the `1 seminorm is given by |v|1 = \u2211\nj |vj |. For a matrix A, \u03c1(A) is the spectral radius, i.e. the largest absolute value of an eigenvalue of A. We use O(\u00b7) with the standard \u2018big O\u2019 meaning, that f(x) = O(g(x)) if there exists M > 0 and x0 \u2208 R such that |f(x)| \u2264M |g(x)| for all x \u2265 x0."}, {"heading": "Reservoir Computing Models for Classification", "text": "Suppose u(t) is an input signal at time t, possibly after the application of a multiplexing mask. The values of the reservoir nodes at time t are called the reservoir states and are denoted by the vector x(t). The nth entry of this vector, xn(t) denotes the state of the n th reservoir node at time t. The dynamics of the ESN and TDR architectures are described by the following models:\nESN: x(t) = f(Winu(t) +Wresx(t\u2212 1)). (1)\nTDR: xn(t) = { f(\u03b1u(t) + \u03b2xN\u22121(t\u2212 1)), if n = 0, xN\u22121(t\u2212 1), if n \u2208 {1, 2, . . . , N \u2212 1}.\n(2)\nFor ease of notation, suppose each reservoir type has N nodes. In the ESN topology, the vector Win \u2208 RN weights the input signal feeding into the nodes, while the matrix Wres \u2208 RN\u00d7N determines the fixed connections and weights among the nodes. That is, node m feeds into node n weighted by the value Wn,m in the ESN model. The TDR has N \u2212 1 virtual nodes and one physical node, labeled as x0. The parameter \u03b1 is the input gain, and \u03b2 is the attenuation value. Notice in the TDR the node values are simply passed along the reservoir unchanged except at the physical node x0. Models of the ESN and TDR reservoirs are shown in Figure 1.\nThe function f in (1) and (2) is a nonlinear activation function. Typical choices for f include sinusoidal, logistic, sigmoidal, and piecewise linear functions."}, {"heading": "Output Weights Trained using Regularized Least Squares", "text": "Classically, signals are classified in a reservoir computer using a linear combination of the reservoir states, determined by a collection of trained output weights [1, 10, 11, 12]. To train the output weights, input signals\nof all classes of the training set are processed, storing all of the reservoir states in a matrix Xtrain \u2208 RTS\u00d7N , where T is the number of times the reservoir state is collected for each input signal, S is the total number of signals in the training set, and N is the number of nodes in the reservoir. The output weights are chosen to map the reservoir states onto given target functions. Typically a unique target vector yk \u2208 RT is chosen for each class k. All of the individual target vectors are combined into a block diagonal target matrix Ytarget, with diagonal blocks arranged to correspond to the class of the input signals. The most popular method to compute the output weights Wout is to use regularized least squares. That is, Wout is found that minimizes the equation\n\u2016Ytarget \u2212XtrainWout\u201622 + \u03bb \u2016Wout\u2016 2 2 .\nThe regularization constant helps to minimizes overfitting of the data. If \u03bb is very close to zero, then the weights will fix Xtrain to the targets Ytarget very well, but the coefficients in Wout may be large and cause sensitivity to noise. Instead, a moderately small value of \u03bb is chosen so the data fits well while dampening the weights. Since a single target vector may be insufficient to describe an entire class of input signals, it may be advantageous to define new targets in terms of the computed weights: Y\u0303target := XtrainWout.\nAfter computing Wout and Y\u0303target, the reservoir can now be used for classification tasks. Denote the j th\ncolumn of Wout by \u03c9 (j) and the jth column of Y\u0303target by y\u0303 (j), both coming from the input signal u(j) in the training set. Suppose u is a newly encountered input signal with reservoir dynamics X \u2208 RT\u00d7N . The signal u is determined to belong to the kth class if \u2225\u2225\u2225y\u0303(j) \u2212X\u03c9(j)\u2225\u2225\u22252\n2 (3)\nis minimized for j \u2208 Ck. For large training sets, it may be very time consuming to compute the output weights. However, the training step is computed offline, and once finished the weights can be stored in memory. Unfortunately, assigning a single unclassified input signal to a class using these stored weights is also expensive with a computational complexity of O(NTS). For moderately sized reservoirs and signals, this method may be prohibitively expensive for real-time classification. As shown in the following section, one can reduce the computational complexity by forgoing the trained weights entirely and performing classification using a supervised clustering method on the reservoir states."}, {"heading": "Classification via Clustering with Principal Components", "text": "The underlying idea for the training method (3) is that similar inputs to the reservoir produce similar outputs, even after the non-linear high-dimensional processing is applied. Under this assumption, it is feasible that one could classify data using a clustering method. Therefore, we propose the following method using the principal components of reservoir responses to perform classification.\nFor each input u(j) in the kth class of the training set, compute the vector v(j), whose entries are the norms of the reservoir node states: v(j)(t) = \u2225\u2225\u2225x(j)(t)\u2225\u2225\u2225\n2 , (4)\nwhere x(j)(t) \u2208 RN are the values of the reservoir nodes at time t for input u(j). Let u be an unclassified input vector with reservoir node states x and output vector v comptued as in (4). Then u is determined to be in class k if v is best described by the first few principal components of the collection Dk = { v(j) : j \u2208 Ck } . More specifically, for each class k compute the matrix Uk \u2208 RT\u00d7R whose columns are the first R principal components of Dk. Say u is in class k if\nk = argmin `\u2208{1,2,...,K}\n\u2016(I \u2212 U`U\u2217` ) v\u2016 2 2 . (5)\nThe method in Equation (5) requires O(KT 2) multiplications to assign a single input vector of length T to one of K classes. Despite the quadratic factor of T , this method tends to be significantly less expensive in practice than training the output weights as in Equation (3) since, in general, KT NS."}, {"heading": "Analysis of Reservoir Behavior", "text": "The clustering method (5) will be more accurate if small varations in the input signals lead to bounded differences in reservoir states, while large discrepancies in inputs are mapped farther apart. To confidently use (5) we must characterize reservoir behavior for similar inputs.\nSeveral studies of reservoir performance based on the type of reservoir architecture, chosen parameters, as well as the characteristics of the input data have been performed, with evidence that some combinations of the aforementioned factors can seriously degrade performance [2, 7, 10, 13]. However, the metrics used in the reservoir computing literature tend to be experimentally investigated. To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used. These all measure how well a reservoir can separate inputs from distinct classes, by having distances between disparate classes large while keeping similar inputs close. Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1]. These measures and properties concern the representation of inputs within the reservoir response and the reconstructability of an input signal from reservoir states. For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.\nAlthough the reservoir dynamics (1) and (2) have simple descriptions, rigorous treatment of their behavior have proven difficult, with few results so far. In Proposition 3 of [1], the distance between two reservoir states at a given time is bounded in terms of the reservoir states at the previous timestep and the spectral radius of the reservoir weights. Although mathematically proven, this Proposition covers only randomly connected ESNs incrementing one timestep with activation functions of the form f(x) = tanh(x). Theorem 3.5 of [23] bounds the distance between two output vectors of a TDR, determined using linear read-out weights, in terms of the reservoir parameters and the behavior of the input signals. In the Theorems below, we prove upper bounds for distances between two reservoir responses to different inputs in terms of reservoir parameters and the behavior of the inputs for both ESNs and TDRs, and in more generality than the results given in [23] and [1].\nFor readability, let us first introduce some notation. Let \u03b4i,j,t = |u(i)(t)\u2212u(j)(t)| be the difference between two input signals at time t, and let \u03b5i,j,t = \u2016x(i)(t)\u2212x(j)(t)\u2016 be the distance between the corresponding node states at time t. Suppose \u03b4i,j = sup{\u03b4i,j,t : t \u2208 R} is bounded for each pair (i, j), and that the nonlinear activation function f is Lipschitz continuous with optimal Lipschitz constant L. Finally, let [\u00b7]n denote a vector whose entries run over the range of the variable n.\nTheorem 1. Suppose the reservoir node states are determined using the ESN dynamics from Equation (1). If \u03c1(Wres) is the spectral radius of Wres, then the distance between the reservoir nodes at time t corresponding to two input signals u(i) and u(j) satisfies\n\u03b5i,j,t \u2264 L\u03b4i,j\u2016Win\u2016 1\u2212 (L\u03c1(Wres))t+1\n1\u2212 L\u03c1(Wres) .\nProof. By Equation (1) and the Lipschitz continuity of f , \u03b5i,j,t = \u2225\u2225\u2225x(i)(t)\u2212 x(j)(t)\u2225\u2225\u2225\n= \u2225\u2225\u2225f (Winu(i)(t) +Wresx(i)(t\u2212 1))\u2212 f (Winu(j)(t) +Wresx(j)(t\u2212 1))\u2225\u2225\u2225\n\u2264 L \u2225\u2225\u2225Win [u(i)(t)\u2212 u(j)(t)]+Wres [x(i)(t\u2212 1)\u2212 x(j)(t\u2212 1)]\u2225\u2225\u2225\n\u2264 L \u2016Win\u2016 \u03b4i,j,t + L\u03c1(Wres)\u03b5i,j,t\u22121.\nSince \u03b5i,j,\u22121 = 0, it follows by induction that\n\u03b5i,j,t \u2264 L \u2016Win\u2016 t\u2211\nr=0\n(L\u03c1(Wres) r\u03b4i,j,t\u2212r \u2264 L\u03b4i,j \u2016Win\u2016\n1\u2212 (L\u03c1(Wres))t+1\n1\u2212 L\u03c1(Wres) .\nTheorem 2. Suppose the reservoir node states are determined using the TDR dynamics from Equation (2). Then the distance between the reservoir nodes corresponding to two input signals u(i) and u(j) satisfies\n\u03b5i,j,t \u2264 \u03b1\u03b4i,jL \u221a N 1\u2212 (\u03b2L)bt/Nc+1\n1\u2212 \u03b2L .\nProof. By Equation (2) and the Lipschitz continuity of f , \u03b5i,j,t = \u2225\u2225\u2225[x(i)n (t)]n \u2212 [x(j)n (t)]n\u2225\u2225\u2225\n= \u2225\u2225\u2225[x(i)0 (t\u2212 n)]n \u2212 [x(j)0 (t\u2212 n)]n\u2225\u2225\u2225\n\u2264 \u03b1L \u2225\u2225\u2225[u(i)(t\u2212 n)\u2212 u(j)(t\u2212 n)]n\u2225\u2225\u2225+ \u03b2L\u2225\u2225\u2225[x(i)N\u22121(t\u2212 n\u2212 1)\u2212 x(j)N\u22121(t\u2212 n\u2212 1)]n\u2225\u2225\u2225 \u2264 \u03b1L\u03b4i,j \u221a N + \u03b2L \u2016[xn(t\u2212N)]n\u2016\n= \u03b1L\u03b4i,j + \u03b2L\u03b5i,j,t\u2212N .\nLet r \u2208 {0, 1, . . . , N \u2212 1} be the remainder when t is divided by N . By induction on the inequality above,\n\u03b5i,j,t \u2264 (\u03b2L)bt/Nc\u03b5i,j,r + \u03b1\u03b4i,jL bt/Nc\u22121\u2211\nk=0\n(\u03b2L)k.\nSince r < N , the nth reservoir node at time r can be characterized by\nxn(r) = { f(\u03b1u(r \u2212 n)), if n \u2264 r 0, if n > r ,\nyielding \u03b5i,j,r \u2264 \u03b1\u03b4i,jL \u221a r + 1 \u2264 \u03b1\u03b4i,jL \u221a N. Therefore\n\u03b5i,j,t \u2264 \u03b1\u03b4i,jL \u221a N bt/Nc\u2211 k=0 (\u03b2L)k = \u03b1\u03b4i,jL \u221a N 1\u2212 (\u03b2L)bt/Nc+1 1\u2212 \u03b2L .\nTheorems 1 and 2 show that for input signals with small pointwise discrepancies and well-chosen reservoir parameters, their associated reservoir state norms cluster well. However, the Theorems do not guarantee that very distinct inputs are mapped to dissimilar reservoir node state norms. For this, we turn to the separation ratio, introduced in [6] and further explored in [14]. For completeness, we include it here, modified for the classification scheme (5).\nLet\nMk(t) := 1 |Ck| \u2211 j\u2208Ck \u2225\u2225\u2225x(j)(t)\u2225\u2225\u2225 , the mean of the reservoir state norms at for all signals in the kth training class at time t The inter-class distance d and intra-class variance v at time t are\nd(t) = 1\nK2 K\u2211 k=1 K\u2211 `=1 |Mk(t)\u2212M`(t)| ,\nand\nw(t) = 1\nK K\u2211 k=1 1 |Ck| \u2211 j\u2208Ck \u2223\u2223\u2223Mk(t)\u2212 \u2016x(j)(t)\u2016\u2223\u2223\u2223 . Then the separation ratio at time t is defined as\nSep(t) = d(t)\n1 + w(t) . (6)\nThe larger Sep(t) is, the better the separation among the classes at time t."}, {"heading": "Numerical Example", "text": "Handwritten digits are classified using the clustering method proposed in Equation (5) and by linear readouts using weights trained as in Equation (3). The data used are from the United States Postal Service (USPS) database, obtained from [24]. A sample of these images is shown in Figure 2. Each image in the dataset is a 16 \u00d7 16 8-bit grayscale image, reshaped as a 256 length column vector. The data are split in ten classes of 1100 images each, representing the digits 0 through 9. For each simulation presented below, 400 images in each class are randomly selected to form the training set, while the remaining 700 images are used as the test set. Although the nearby pixel behavior is not preserved in the x-direction by transforming each image into a column vector, the correlations are still present in the reservoir response due to the long short term memory property.\nThe classification accuracy and CPU time required to classify all images in the test dataset, using both (5) and (3), for both the ESN and TDR reservoir topologies for several parameter choices are presented below. We also give the separation metric (6), and verify that the inequalities in the conclusions of Theorems 1 and 2 are satisfied. The clustering approach in (5) is applied to the reservoir states, but the raw input signals could also be classified according to similarity with principal components, without processing the data in the reservoir. The classification accuracy of this approach is also investigated.\nAll numerical experiments are implemented in MATLAB R2015b on a workstation with an Intel i7 CPU (2.40 GHz) and 16GB RAM."}, {"heading": "Experiment Setup", "text": "The reservoirs are set up with N nodes, selecting N from the set {25, 50, 100, 400}. Each image vector is multiplexed with a mask of length N \u2212 1, randomly taking values from the set {\u00b11}, creating input vectors u of length 256(N \u2212 1). The parameter \u03b1 varies over the set {0.1, 0.2, . . . , 0.9}. The nonlinear activation function is chosen to be f(x) = sin(x) for all simulations.\nWhen a reservoir with the ESN topology is used, the input weights are chosen as Win = [ \u03b1 0 \u00b7 \u00b7 \u00b7 0 ]> , and the internal weight matrix Wres is randomly filled to 20% density and scaled so that \u03c1(Wres) = \u03b2. For both ESN and TDR topologies, we use \u03b1 + \u03b2 = 1 in order to preserve the dynamical range of the data. The reservoir states are not sampled at every time step; instead we record the reservoir state values for t = r(N \u2212 1) + 1, with r \u2208 N.\nFor each simulation, the training dataset of size 400 is randomly chosen from the complete collection of images, and the remaining 700 images form the test dataset. However, the same selection is used for each pair (N,\u03b1), so the competing approaches can be fairly compared."}, {"heading": "Results", "text": "The results of these simulations are presented in Figures 3-5.\nFigure 3 displays the average value of the separation metric Sep(t) from Equation (6) over all t for each simulation. Notice that the separation quality of ESN-style reservoirs is modest, with a maximum value of 2.25. Moreover, the separation quality of ESNs depend on the parameter \u03b1, but does not seem to depend\non the reservoir size N . On the other hand, the separation quality for TDR-style reservoirs is much larger, and depends strongly on both \u03b1 and N , with larger reservoirs separating the classes better.\nFigure 4 displays the classification accuracy and the time required to classify all 7000 images in the test dataset for each reservoir type and combination of parameters. The clustering approach outperforms the linear output with trained weights in all simulations in terms of both accuracy and CPU time. The accuracy results are shown in the top two plots of Figure 4. For both reservoir types and classification strategies, the accuracy does not seem to depend on the reservoir size. The accuracy does depend on the parameter \u03b1, especially for TDR-type reservoirs. The time results are shown in the bottom two plots of Figure 4. The analysis of the two classification methods determined that the computational complexity of using trained output weights does depend on the N , but the clustering approach does not, which agrees with the results shown. Overall, the clustering approach used in conjunction with either reservoir type is at least an order of magnitude faster than using trained output weights for classification. Additionally, the clustering approach with the reservoir outperforms clustering the raw input data without the reservoir. Over 100 trials, the average accuracy of the clustering method applied to the raw input data is 95.27%. This suggests that the clustering method is well-suited to this problem, but processing the data in a reservoir improves accuracy for most parameter choices since the reservoir preserves the spatial correlations well.\nFigure 5 displays values of the inequalities (1) and (2) presented in Theorems 1 and 2, measuring the discrepancy of reservoir activations at time t for similar inputs. The two input signals were randomly selected from the class of \u20183s\u2019. The values shown in the Figure are\n\u03b5i,j,t/ ( L\u03b4i,j\u2016Win\u2016 1\u2212 (L\u03c1(Wres))t+1\n1\u2212 L\u03c1(Wres) ) in the left image, and\n\u03b5i,j,t/ ( \u03b1\u03b4i,jL \u221a N 1\u2212 (\u03b2L)bt/Nc+1\n1\u2212 \u03b2L ) in the right image, both plotted against t. The inequalities in the theorems are clearly satisfied, however the upper limits could be further refined in future research."}, {"heading": "Conclusion", "text": "The numerical experiments demonstrate that the proposed clustering method (5) outperforms the standard trained linear output weights approach (3) for classification tasks in terms of time and accuracy, for both ESNs and TDRs, for all reservoir parameters investigated. The accuracy achieved by the clustering approach does not strongly depend on the reservoir type, size, or parameter selection, unlike the results achieved by trained output weights. Moreover, the clustering approach has the potential to be several orders of magnitude faster, allowing its use in real-time systems. The clustering method described in this paper is shown to be robust, and is a promising approach for use in reservoir computers for classification tasks."}, {"heading": "Author contributions statement", "text": "A. P. conceived and performed all work."}, {"heading": "Additional information", "text": "The author declares no competing financial interests. Acknowledgement of Disclaimer: Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the view of the United States Air Force."}], "references": [{"title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks - with an Erratum note", "author": ["H. Jaeger"], "venue": "Fraunhofer Institute for Autonomous Intelligent Systems, Technical report: GMD Report", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Real-time computing without stable states: a new framework for neural computational based on perturbations", "author": ["W. Maass", "H. Natschl\u00e4ger", "H. Markram"], "venue": "Neural Comput", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P. Werbos"], "venue": "Proc. IEEE 89,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Real-time compuitation at the edge of chaos in recurrent neural networks", "author": ["N. Bertschinger", "H. Natschl\u00e4ger"], "venue": "Neural Comput", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Long short-term memory in echo state networks: Details of a simulation study", "author": ["H. Jaeger"], "venue": "Jacobs University Bremen Technical Report", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Spatiotemporal pattern recognition via liquid state machines", "author": ["E. Goodman", "D. Ventura"], "venue": "International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Optimal nonlinear information processing capacity in delay-based reservoir computers", "author": ["L. Grigoryeva", "J. Henriques", "L. Larger", "J. Ortega"], "venue": "Sci. Rep", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A unified framework for reservoir computing and extreme learning machines based on a single time-delayed neuron", "author": ["S Ort\u0301\u0131n"], "venue": "Sci. Rep. 5,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Optoelectronic reservoir computing", "author": ["Y Paquot"], "venue": "Sci. Rep. 2", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A comparitive study of reservoir computing for temporal signal processing", "author": ["A. Goudarzi", "P. Banda", "M. Lakin", "C. Teuscher", "D. Stefanovic"], "venue": "Technical Report 11pp,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger"], "venue": "Comp. Sci. Rev", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "A practical guide to applying echo state networks. in NN", "author": ["M. Luko\u0161evi\u010dius"], "venue": "Tricks of the Trade (ed. Montavon, G. et al.) 2nd edn. 650\u2013686 (Springer-Verlag Berlin Heidelberg", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Reservoir computing: a photonic neural network for information processing", "author": ["Y. Paquot", "J. Dambre", "B. Schrauwen", "M. Haelterman", "S. Massar"], "venue": "Proc. SPIE Nonlinear Optics and Applications IV", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Unifying quality metrics for reservoir", "author": ["T. Gibbons"], "venue": "networks. 2010 International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "An overview of reservoir computing: theory, applications and implementations", "author": ["B. Schrauwen", "D. Verstraeten", "J. Van Campenhout"], "venue": "ESANN 2007 proceedings - European Symposium on Artificial Neural Networks, Bruges Belgium", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "On the correlation between reservoir metrics and performance for time series classification under the influence of synaptic plasticity", "author": ["J. Chrol-Cannon", "Y. Jin"], "venue": "PLOS ONE Vol 9, Iss", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Edge of chaos and prediction of computational performance for neural circuit models", "author": ["R. Legenstein", "W. Maass"], "venue": "Neural Networks", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Minimal approach to neuroinspired information processing", "author": ["M. Soriano", "D. Brunner", "M. Escalona-Mor\u00e1n", "C. Mirasso", "I. Fischer"], "venue": "Front. Comput. Neurosci", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Information processing capacity of dynamical systems", "author": ["J. Dambre", "D. Verstraeten", "B. Schrauwen", "S. Massar"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "An experimental unification of reservoir computing methods", "author": ["D Verstraeten"], "venue": "Neural Networks 20,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "On computational power and the order-chaos phase transition in reservoir computing", "author": ["B Schrauwen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Real-time computation at the edge of chaos in recurrent neural networks", "author": ["B. Nils", "H. Natschl\u00e4ger"], "venue": "Neural Comput", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Reservoir computing dynamics for single nonlinear node with delay line structure", "author": ["C. DiMarco"], "venue": "Syracuse University Technical Report", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Data for MATLAB hackers", "author": ["S. Roweis"], "venue": "www.cs.nyu.edu/roweis/data.html. (Accessed:", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Reservoir computing is a recently developed bio-inspired machine learning paradigm for the processing of spatiotemporal data [1, 2].", "startOffset": 125, "endOffset": 131}, {"referenceID": 1, "context": "Reservoir computing is a recently developed bio-inspired machine learning paradigm for the processing of spatiotemporal data [1, 2].", "startOffset": 125, "endOffset": 131}, {"referenceID": 2, "context": "Because the weights are fixed, using a reservoir requires only a simple initialization step, as opposed to more traditional recurrent neural networks whose weights and connections must be learned in a tedius backpropagation training step [3].", "startOffset": 238, "endOffset": 241}, {"referenceID": 3, "context": "It is desirable for a reservoir to operate \u2018at the edge of chaos\u2019 [4], so dissimilar inputs are sufficiently separated in the reservoir node states, yet inputs with only small perturbation-like differences do not stray too far apart.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "Reservoir dynamics demonstrate long short-term memory [5], so any individual point-wise errors in a signal will not corrupt the entire reservoir response.", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "An ESN uses randomly, yet sparsely, connected nodes with randomly assigned fixed weights [1, 2, 6].", "startOffset": 89, "endOffset": 98}, {"referenceID": 1, "context": "An ESN uses randomly, yet sparsely, connected nodes with randomly assigned fixed weights [1, 2, 6].", "startOffset": 89, "endOffset": 98}, {"referenceID": 5, "context": "An ESN uses randomly, yet sparsely, connected nodes with randomly assigned fixed weights [1, 2, 6].", "startOffset": 89, "endOffset": 98}, {"referenceID": 6, "context": "A TDR uses a cyclic topology, where each node is connected to exactly two other nodes with fixed, non-random weights [7, 8, 9].", "startOffset": 117, "endOffset": 126}, {"referenceID": 7, "context": "A TDR uses a cyclic topology, where each node is connected to exactly two other nodes with fixed, non-random weights [7, 8, 9].", "startOffset": 117, "endOffset": 126}, {"referenceID": 8, "context": "A TDR uses a cyclic topology, where each node is connected to exactly two other nodes with fixed, non-random weights [7, 8, 9].", "startOffset": 117, "endOffset": 126}, {"referenceID": 0, "context": "The output layer of both ESN and TDR-type reservoirs almost always use linear output weights, trained on a labeled dataset using least squares or ridge regression[1, 10, 11].", "startOffset": 162, "endOffset": 173}, {"referenceID": 9, "context": "The output layer of both ESN and TDR-type reservoirs almost always use linear output weights, trained on a labeled dataset using least squares or ridge regression[1, 10, 11].", "startOffset": 162, "endOffset": 173}, {"referenceID": 10, "context": "The output layer of both ESN and TDR-type reservoirs almost always use linear output weights, trained on a labeled dataset using least squares or ridge regression[1, 10, 11].", "startOffset": 162, "endOffset": 173}, {"referenceID": 0, "context": "Output Weights Trained using Regularized Least Squares Classically, signals are classified in a reservoir computer using a linear combination of the reservoir states, determined by a collection of trained output weights [1, 10, 11, 12].", "startOffset": 220, "endOffset": 235}, {"referenceID": 9, "context": "Output Weights Trained using Regularized Least Squares Classically, signals are classified in a reservoir computer using a linear combination of the reservoir states, determined by a collection of trained output weights [1, 10, 11, 12].", "startOffset": 220, "endOffset": 235}, {"referenceID": 10, "context": "Output Weights Trained using Regularized Least Squares Classically, signals are classified in a reservoir computer using a linear combination of the reservoir states, determined by a collection of trained output weights [1, 10, 11, 12].", "startOffset": 220, "endOffset": 235}, {"referenceID": 11, "context": "Output Weights Trained using Regularized Least Squares Classically, signals are classified in a reservoir computer using a linear combination of the reservoir states, determined by a collection of trained output weights [1, 10, 11, 12].", "startOffset": 220, "endOffset": 235}, {"referenceID": 1, "context": "Several studies of reservoir performance based on the type of reservoir architecture, chosen parameters, as well as the characteristics of the input data have been performed, with evidence that some combinations of the aforementioned factors can seriously degrade performance [2, 7, 10, 13].", "startOffset": 276, "endOffset": 290}, {"referenceID": 6, "context": "Several studies of reservoir performance based on the type of reservoir architecture, chosen parameters, as well as the characteristics of the input data have been performed, with evidence that some combinations of the aforementioned factors can seriously degrade performance [2, 7, 10, 13].", "startOffset": 276, "endOffset": 290}, {"referenceID": 9, "context": "Several studies of reservoir performance based on the type of reservoir architecture, chosen parameters, as well as the characteristics of the input data have been performed, with evidence that some combinations of the aforementioned factors can seriously degrade performance [2, 7, 10, 13].", "startOffset": 276, "endOffset": 290}, {"referenceID": 12, "context": "Several studies of reservoir performance based on the type of reservoir architecture, chosen parameters, as well as the characteristics of the input data have been performed, with evidence that some combinations of the aforementioned factors can seriously degrade performance [2, 7, 10, 13].", "startOffset": 276, "endOffset": 290}, {"referenceID": 5, "context": "To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used.", "startOffset": 83, "endOffset": 90}, {"referenceID": 13, "context": "To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used.", "startOffset": 83, "endOffset": 90}, {"referenceID": 1, "context": "To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used.", "startOffset": 114, "endOffset": 121}, {"referenceID": 14, "context": "To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used.", "startOffset": 114, "endOffset": 121}, {"referenceID": 15, "context": "To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used.", "startOffset": 144, "endOffset": 148}, {"referenceID": 1, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 156, "endOffset": 168}, {"referenceID": 16, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 156, "endOffset": 168}, {"referenceID": 17, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 156, "endOffset": 168}, {"referenceID": 18, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 189, "endOffset": 193}, {"referenceID": 0, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 223, "endOffset": 226}, {"referenceID": 17, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 15, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 16, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 19, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 20, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 21, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 0, "context": "In Proposition 3 of [1], the distance between two reservoir states at a given time is bounded in terms of the reservoir states at the previous timestep and the spectral radius of the reservoir weights.", "startOffset": 20, "endOffset": 23}, {"referenceID": 22, "context": "5 of [23] bounds the distance between two output vectors of a TDR, determined using linear read-out weights, in terms of the reservoir parameters and the behavior of the input signals.", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "In the Theorems below, we prove upper bounds for distances between two reservoir responses to different inputs in terms of reservoir parameters and the behavior of the inputs for both ESNs and TDRs, and in more generality than the results given in [23] and [1].", "startOffset": 248, "endOffset": 252}, {"referenceID": 0, "context": "In the Theorems below, we prove upper bounds for distances between two reservoir responses to different inputs in terms of reservoir parameters and the behavior of the inputs for both ESNs and TDRs, and in more generality than the results given in [23] and [1].", "startOffset": 257, "endOffset": 260}, {"referenceID": 5, "context": "For this, we turn to the separation ratio, introduced in [6] and further explored in [14].", "startOffset": 57, "endOffset": 60}, {"referenceID": 13, "context": "For this, we turn to the separation ratio, introduced in [6] and further explored in [14].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "The data used are from the United States Postal Service (USPS) database, obtained from [24].", "startOffset": 87, "endOffset": 91}], "year": 2017, "abstractText": "Reservoir computing is a recently introduced machine learning paradigm that has been shown to be wellsuited for the processing of spatiotemporal data. Rather than training the network node connections and weights via backpropagation in traditional recurrent neural networks, reservoirs instead have fixed connections and weights among the \u2018hidden layer\u2019 nodes, and traditionally only the weights to the output layer of neurons are trained using linear regression. We claim that for signal classification tasks, one may forgo the weight training step entirely and instead use a simple supervised clustering method. The proposed method is analyzed theoretically and explored through numerical experiments on real-world data. The examples demonstrate that the proposed clustering method outperforms the traditional trained output weight approach in terms of speed, accuracy, and sensitivity to reservoir parameters.", "creator": "TeX"}}}