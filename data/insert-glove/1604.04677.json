{"id": "1604.04677", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2016", "title": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction", "abstract": "chibber We demonstrate scones that sigur\u00f0r an dc-2 attention - tsotsobe based encoder - decoder model can oooooooooooooooooooo be used para-alpine for bolkestein sentence - level grammatical ferrugia error sallenger identification 77,738 for divorces the Automated greeting Evaluation ibbs of Scientific exantus Writing (piacenza AESW) amarildo Shared tramadol Task 2016. giresse The lanceolate attention - 35,750 based encoder - subring decoder 2411 models farsta can clicking be jorien used poeme for gallantry the generation budrys of tozer corrections, in mcdull addition to naosuke error identification, which pacholczyk is of interest for 90.07 certain end - mekhennet user applications. nuzzling We yelovich show chupke that udalls a oude character - 2312 based encoder - top-to-bottom decoder model is particularly odobescu effective, stoneman outperforming other moldejazz results on preciousness the qingming AESW Shared jabarani Task on its transducers own, gorlice and yippies showing karnak gains over a non-operative word - based garhwali counterpart. saltford Our fingerpointing final model - - bargains a hej combination of three character - based encoder - hegemonic decoder models, 232-year one prof. word - wedgewood based encoder - brachystegia decoder zittau model, beppu and replaces a 15-point sentence - level CNN - - is gay-rights the diarmait highest 40million performing extremist system sekiwake on barleywine the promisor AESW 2016 cheroot binary prediction luque Shared bear-like Task.", "histories": [["v1", "Sat, 16 Apr 2016 01:49:09 GMT  (143kb,D)", "http://arxiv.org/abs/1604.04677v1", "To appear at BEA11, as part of the AESW 2016 Shared Task"]], "COMMENTS": "To appear at BEA11, as part of the AESW 2016 Shared Task", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["allen schmaltz", "yoon kim", "alexander m rush", "stuart m shieber"], "accepted": false, "id": "1604.04677"}, "pdf": {"name": "1604.04677.pdf", "metadata": {"source": "CRF", "title": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction", "authors": ["Allen Schmaltz", "Yoon Kim", "Alexander M. Rush", "Stuart M. Shieber"], "emails": [], "sections": [{"heading": null, "text": "We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications. We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart. Our final model\u2014a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN\u2014is the highest performing system on the AESW 2016 binary prediction Shared Task."}, {"heading": "1 Introduction", "text": "The recent confluence of data availability and strong sequence-to-sequence learning algorithms has the potential to lead to practical tools for writing support. Grammatical error identification is one such application of potential utility as a component of a writing support tool. Much of the recent work in grammatical error identification and correction has made use of hand-tuned rules and features that augment data-driven approaches, or individual classifiers for human-designated subsets of errors. Given a large, annotated dataset of scientific journal articles, we propose a fully data-driven approach for\nthis problem, inspired by recent work in neural machine translation and more generally, sequence-tosequence learning (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).\nThe Automated Evaluation of Scientific Writing (AESW) 2016 dataset is a collection of nearly 10,000 scientific journal articles (over 1 million sentences) published between 2006 and 2013 and annotated with corrections by professional, native English-speaking editors. The goal of the associated AESW Shared Task is to identify whether or not a given unedited source sentence was corrected by the editor (that is, whether a given source sentence has one or more grammatical errors, broadly construed).\nThis system report describes our approach and submission to the AESW 2016 Shared Task, which establishes the current highest-performing public baseline for the binary prediction task. Our primary contribution is to demonstrate the utility of an attention-based encoder-decoder model for the binary prediction task. We also provide evidence of tangible performance gains using a character-aware version of the model, building on the characteraware language modeling work of Kim et al. (2016). In addition to sentence-level classification, the models are capable of intra-sentence error identification and the generation of possible corrections. We also obtain additional gains by using an ensemble of a generative encoder-decoder and a discriminative CNN classifier."}, {"heading": "2 Background", "text": "Recent work in natural language processing has shown strong results in sequence-to-sequence trans-\nar X\niv :1\n60 4.\n04 67\n7v 1\n[ cs\n.C L\n] 1\n6 A\npr 2\nformations using recurrent neural network models (Cho et al., 2014; Sutskever et al., 2014). Grammar correction and error identification can be cast as a sequence-to-sequence translation problem, in which an unedited (source) sentence is \u201ctranslated\u201d into a corrected (target) sentence in the same language. Using this framework, sentence-level error identification then simply reduces to an equality check between the source and target sentences.\nThe goal of the AESW shared task is to identify whether a particular sentence needs to be edited (contains a \u201cgrammatical\u201d error, broadly construed1). The dataset consists of sentences taken from academic articles annotated with corrections by professional editors. Annotations are described via insertions and deletions, which are marked with start and end tags. Tokens to be deleted are surrounded with the deletion start tag <del> and the deletion end tag </del> and tokens to be inserted are surrounded with the insertion start tag <ins> and the insertion end tag </ins>. Replacements (as shown in Figure 1) are represented as deletioninsertion pairs. Unlike the related CoNLL-2014 Shared Task (Ng et al., 2014) data, errors are not labeled with fine-grained types (article or determiner error, verb tense error, etc.).\nMore formally, we assume a vocabulary V of natural language word types (some of which have orthographic errors) and a set Q = {<ins>,</ins>,<del>,</del>} of annotation tags. Given a sentence s = [s1, . . . , sI ], where si \u2208 V is the i-th token of the sentence of length I , we seek to predict whether or not the gold, annotated target sentence t = [t1, . . . , tJ ], where tj \u2208 Q \u222a V is the j-th token of the annotated sentence of length J , is identical to s. We are given both s and t for supervised training. At test time, we are only given access to sequence s. We learn to predict sequence t.\nEvaluation of this binary prediction task is via the F1-score, where the positive class is that indicating an error is present in the sentence (that is, where s 6= t)2.\n1Some insertions and deletions in the shared task data represent stylistic choices, not all of which are necessarily recoverable given the sentence or paragraph context. For the purposes here, we refer to all such edits as \u201cgrammatical\u201d errors.\n2The 2016 Shared Task also included a probabilistic esti-\nEvaluation is at the sentence level, but the paragraph-level context for each sentence is also provided. The paragraphs, themselves, are shuffled so that full article context is not available. A coarse academic field category is also provided for each paragraph. Our models described below do not make use of the paragraph context nor the field category, and they treat each sentence independently.\nFurther information about the task is available in the Shared Task report (Daudaravicius et al., 2016)."}, {"heading": "3 Related Work", "text": "While this is the first year for a shared task focusing on sentence-level binary error identification, previous work and shared tasks have focused on the related tasks of intra-sentence identification and correction of errors. Until recently, standard handannotated grammatical error datasets were not available, complicating comparisons and limiting the choice of methods used. Given the lack of a large hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers.\nMore recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al., 2012). The CoNLL-2013 Shared Task (Ng et al., 2013)3 focused on the correction of a limited set of five error types in essays by second-language learners of English at the National University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners.\nAs with machine translation (MT), evaluation of\nmation track. We leave for future work the adaptation of our approach to that task.\n3http://www.comp.nus.edu.sg/\u02dcnlp/ conll13st.html\n4http://www.comp.nus.edu.sg/\u02dcnlp/ conll14st.html\nthe full generation task is still an open research area, but a subsequent human evaluation ranked the output from the CoNLL-2014 Shared Task systems (Napoles et al., 2015). The system of Felice et al. (2014) ranked highest, utilizing a combination of a rule-based system and phrase-based MT, with re-ranking via a large web-scale language model. Of the non-MT based approaches, the IllinoisColumbia system was a strong performer, combining several classifiers trained for specific types of errors (Rozovskaya et al., 2014)."}, {"heading": "4 Models", "text": "We use an end-to-end approach that does not have separate components for candidate generation or reranking that make use of hand-tuned rules or explicit syntax, nor do we employ separate classifiers for human-differentiated subsets of errors, unlike some previous work for the related task of grammatical error correction.\nWe next introduce two approaches for the task of sentence-level grammatical error identification: A binary classifier and a sequence-to-sequence model that is trained for correction but can also be used for identification as a side-effect."}, {"heading": "4.1 Baseline Convolutional Neural Net", "text": "To establish a baseline, we follow past work that has shown strong performance with convolutional neural nets (CNNs) across various domains for sentence-level classification (Kim, 2014; Zhang and Wallace, 2015). We utilize the one-layer CNN architecture of Kim (2014) with the publicly available5 word vectors trained on the Google News dataset, which contains about 100 billion words (Mikolov et al., 2013). We experiment with keeping the word vectors static (CNN-STATIC) and fine-tuning the vectors (CNN-NONSTATIC). The CNN models only have access to sentence-level labels and are not given correction-level annotations."}, {"heading": "4.2 Encoder-Decoder", "text": "While it may seem more natural to utilize models trained for binary prediction, such as the aforementioned CNN, or for example, the recurrent network\n5https://code.google.com/archive/p/ word2vec/\napproach of Dai and Le (2015), we hypothesize that training at the lowest granularity of annotations may be useful for the task. We also suspect that the generation of corrections is of sufficient utility for endusers to further justify exploring models that produce corrections in addition to identification. We thus use the Shared Task as a means of assessing the utility of a full generation model for the binary prediction task.\nWe propose two encoder-decoder architectures for this task. Our word-based architecture (WORD) is similar to that of Luong et al. (2015). Our character-based models (CHAR) still make predictions at the word-level, but use a CNN and a highway network over characters instead of word embeddings as the input to the encoder and decoder, as depicted in Figure 1. We follow past work (Sutskever et al., 2014; Luong et al., 2015) in stacking multiple recurrent neural networks (RNNs), specifically Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, in both the encoder and decoder.\nHere, we model the probability of the target given the source, p(t | s), with an encoder neural network that summarizes the source sequence and a decoder neural network that generates a distribution over the target words and tags at each step given the source.\nWe start by describing the basic encoder and decoder architectures in terms of the WORD model, and then we describe the CHAR model departures from WORD.\nEncoder The encoder reads the source sentence and outputs a sequence of vectors, associated with each word in the sentence, which will be selectively accessed during decoding via a soft attentional mechanism. We use a LSTM network to obtain the hidden states hsi \u2208 Rn for each time step i,\nhsi = LSTM(h s i\u22121,x s i ).\nFor the WORD models, xsi \u2208 Rm is the word embedding for si, the i-th word in the source sentence. (The analogue for the CHAR models is discussed below.) The output of the encoder is the sequence of hidden state vectors [hs1, . . . ,h s I ]. The initial hidden state of the encoder is set to zero (i.e. hs0 \u2190 0).\nDecoder The decoder is another LSTM that produces a distribution over the next target word/tag\ngiven the source vectors [hs1, . . . ,h s I ] and the previously generated target words/tags t<j = [t1, . . . tj ]. Let\nhtj = LSTM(h t j\u22121,x t j)\nbe the summary of the target sentence up to the j-th word, where xtj is the word embedding for tj in the WORD models. The current target hidden state htj is combined with each of the memory vectors in the source to produce attention weights as follows,\nuj,i = h t j \u00b7W\u03b1hsi \u03b1j,i = expuj,i\u2211\nk\u2208[1,I] expuj,k\nThe source vectors are multiplied with the respective attention weights, summed, and interacted with the current decoder hidden state htj to produce a context vector cj ,\nvj = \u2211 i\u2208[1,I] \u03b1j,ih s i\ncj = tanh(W[vj ;h t j ])\nThe probability distribution over the next\nword/tag is given by applying an affine transformation to cj followed by a softmax,\np(tj+1 | s, t<j) = softmax(Ucj + b)\nFinally, as in Luong et al. (2015), we feed cj as additional input to the decoder for the next time step by concatenating it with xtj , so the decoder equation is modified to,\nhtj = LSTM(h t j\u22121, [x t j ; cj\u22121])\nThis allows the decoder to have knowledge of previous (soft) alignments at each time step. The decoder hidden state is initialized with the final hidden state of the encoder (i.e. ht0 \u2190 hsI ).\nCharacter Convolutional Neural Network For the CHAR models, instead of a word embedding, our input for each word in the source/target sentence is an output from a character-level convolutional neural network (CharCNN) (depicted in Figure 1). Our character model closely follows that of Kim et al. (2016).\nSuppose word si is composed of characters [p1, . . . , pl]. We concatenate the character embeddings to form the matrix Pi \u2208 Rc\u00d7l, where the k-th\ncolumn corresponds to the character embedding for pk (of dimension c).\nWe then apply a convolution between Pi and a filter H \u2208 Rc\u00d7w of width w, after which we add a bias and apply a nonlinearity to obtain a feature map fi \u2208 Rl\u2212w+1. The k-th element of fi is given by,\nfi[k] = tanh(\u3008Pi[\u2217, k : k + w \u2212 1],H\u3009+ b)\nwhere \u3008A,B\u3009 = Tr(ABT ) is the Frobenius inner product and Pi[\u2217, k : k + w \u2212 1] is the k-to-(k + w \u2212 1)-th column of Pi. Finally, we take the maxover-time\nzi = max k fi[k]\nas the feature corresponding to filter H. We use multiple filters H1, . . .Hh to obtain a vector zi \u2208 Rh as the representation for a given source/target word or tag. We have separate CharCNNs for the encoder and decoder.\nHighway Network Instead of replacing the word embedding xi with zi, we feed zi through a highway network (Srivastava et al., 2015). Whereas a multilayer perceptron produces a new set of features via the following transformation (given input z),\nz\u0302 = f(Wz+ b)\na highway network instead computes,\nz\u0302 = r f(Wz+ b) + (1\u2212 r) z\nwhere f is a non-linearity (in our models, ReLU), is the element-wise multiplication operator, and r = \u03c3(Wrz+br) and 1\u2212r are called the transform and carry gates. We feed zi into the highway network to obtain z\u0302i, which is used to replace the input word embeddings in both the encoder and the decoder.\nInference Exact inference is computationally infeasible for the encoder-decoder models given the combinatorial explosion of possible output sequences, but we follow past work in NMT using beam search. We do not constrain the generation process of words outside insertion tags to words in the source, and each low-frequency holder token generated in the target sentence is replaced with the source token associated with the maximum attention weight. We use a beam size of 10 for all models,\nwith the exception of one of the models in the final system combination, for which we use a beam of size 5, as noted in Section 6.\nNote that this model generates corrections, but we are only interested in determining the existence of any error at the sentence-level. As such, after beam decoding, we simply check for whether there were any corrections in the target. However, we found that decoding in this way under-predicts sentencelevel errors. It is therefore important to calibrate the weights associated with corrections, which we discuss in Section 5.3."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Data", "text": "The AESW task data differs from previous grammatical error datasets in terms of scale and genre. To the best of our knowledge, the AESW dataset is the first large-scale, publicly available professionally edited dataset of academic, scientific writing. The training set consists of 466,672 sentences with edits and 722,742 sentences without edits, and the development set contains 57,340 sentences with edits and 90,106 sentences without. The raw training and development datasets are provided as annotated sentences, t, from which the s sequences may be deterministically derived. There are 143,802 sentences in the Shared Task test set with hidden gold labels, which serve directly as s sequences.\nAs part of pre-processing, we treat each sentence independently, discarding paragraph context (which sentences, if any, were present in the same paragraph) and domain information, which is a coarse grouping by the field of the original journal (Engineering, Computer Science, Mathematics, Chemistry, Physics, etc.). We generate Penn Treebank style tokenizations of the input. Case is maintained and digits are not replaced with holder symbols. The vocabulary is restricted to the 50,000 most common tokens, with remaining low frequency tokens replaced with a special <unk> token. The CHAR model can encode but not decode over open vocabularies and hence we do not have any <unk> tokens on the source side of those models. For all of the encoder-decoder models, we replace the lowfrequency target symbols during inference as discussed above in Section 4.2.\nFor development against the provided data with labels, we set aside a 10,000 sentence sample from the original development set for tuning, and use the remaining 137,446 sentences for validation6. The encoder-decoder models are given all 466,672 pairs of s and t sequences with edits, augmented with varying numbers of pairs without edits. The CHAR+SAMPLE and WORD+SAMPLE models are given a random sample of 200,000 pairs without edits for a total of 666,672 pairs of s and t sequences. The CHAR+ALL and WORD+ALL models are given all 722,742 sentences without edits for a total of 1,189,414 pairs of s and t sequences. For some of the final testing models, we also train with the development sentences. In these latter cases, all sequence pairs are used. In training all of the encoder-decoder models, as indicated in Section 5.2, we drop sentences exceeding 50 tokens in length.\nWe also experimented with creating corrected versions of sentences for the CNN. The binary CNN classifiers are given 1,656,086 single-sentence training examples, of which 722,742 are error-free examples (in which s = t), and the remaining examples are constructed by removing the tags from the annotated sentences, t, to create tag-free examples that contain errors (466,672 instances) and additional error-free examples (466,672 instances)."}, {"heading": "5.2 Training", "text": "Training (along with testing) of all models was conducted on GPUs. Our models were implemented with the Torch7 framework.\nCNN Architecture and training approaches were informed by past work in sentence-level classification using CNNs (Kim, 2014; Zhang and Wallace, 2015). A limited grid search on the development set determined our use of filter windows of width 3, 4, and 5 and 1000 feature maps. We trained for 10 epochs. Training otherwise followed the approach of the correspondingly named CNN-STATIC and CNN-NONSTATIC models of Kim (2014).\n6Note that the number of sentences in the final development set without labels posted on CodaLab (http://codalab. org) differed from that originally posted on the AESW 2016 Shared Task website with labels.\n7http://torch.ch\nencoder-decoder Initial parameter settings (including architecture decisions such as the number of layers and embedding and hidden state sizes) were informed by concurrent work in neural machine translation and existing work such as that of Sutskever et al. (2014) and Luong et al. (2015). We used 4-layer LSTMs with 1000 hidden units in each layer. We trained for 14 epochs with a batch size of 64 and a maximum sequence length of 50. The parameters for the WORD model were uniformly initialized in [\u22120.1, 0.1], and those of the CHAR model were uniformly initialized in [\u22120.05, 0.05]. The L2normalized gradients were constrained to be \u2264 5. Our learning rate schedule started the learning rate at 1 and halved the learning rate after each epoch beyond epoch 10, or once the validation set perplexity no longer improved. The WORD model used 1000-dimensional word embeddings. For CHAR, the character embeddings were 25-dimensional, the filter width was 6, the number of feature maps was 1000, and 2 highway layers were used. The maximum word length was 35 characters for training CHAR. Note that we do not reverse the source (s) sequences, unlike some previous NMT work. Following the work of Zaremba et al. (2014), we employed dropout with a probability of 0.3 between the LSTM layers.\nTraining both WORD and CHAR on the training set took on the order of a few days using GPUs, with the former being more efficient than the latter. In practice, we used two GPUs for training CHAR due to memory requirements."}, {"heading": "5.3 Tuning", "text": "Post-hoc tuning was performed on the 10k held-out portion of the development set. In terms of maximizing the F1-score, this post-hoc tuning was important for these models, without which precision was high and recall was low. We leave to future work alternative approaches to this type of post-hoc tuning.\nFor the CNN models, after training, we tuned the decision boundary to maximize the F1-score on the held-out tuning set. Analogously, for the encoderdecoder models, after training the models, we tuned the bias weights (given as input to the final softmax layer generating the words/tags distribution) associated with the four annotation tags via a simple grid search by iteratively running beam search on the tun-\ning set. Due to the relatively high expense of decoding, we employed a coarse grid search in which the bias weights of the four annotation tags were uniformly varied."}, {"heading": "6 Results", "text": "Results on the development set, excluding the 10k tuning set, appear in Table 1. Here (and elsewhere) RANDOM is the result of randomly assigning a sentence to one of the binary classes. For the CNN classifiers, fine-tuning the word2vec embeddings improves performance. The encoder-decoder models improve over the CNN classifiers, even though\nthe latter are provided with additional data (via word2vec). The character-based models yield tangible improvements over the word-based models.\nFor consistency here, we kept the beam size at 10 across models, but subsequent analysis revealed that increasing the beam from 5 to 10 had a negligible effect on overall performance.\nTuning results appear in Figures 2 and 3, illustrating the importance of adjusting the bias weights associated with the annotation tags in balancing precision and recall to maximize the F1 score. The models trained on all sequence pairs without edits, CHAR+ALL and WORD+ALL, perform particularly poorly without tuning these bias weights, yielding F1 scores near that of RANDOM before tuning, which corresponds to a weight of 0.0 in Figure 2.\nThe official development set posted on CodaLab differed slightly from the original development set provided with labels, so we include those results in Table 2 for the encoder-decoder models. Here, evaluation is performed on the CodaLab server, as with the final test submission. The overall relative performance pattern is similar to that of the original development set.\nA comparison of our results with other shared task submissions appears in Table 3. (Teams were allowed to submit up to two results.) Our submission, COMBINATION was a simple majority vote at the system level (for each test sentence) of 5 models8: (1) a CNN-NONSTATIC model trained with the concatenation of the training and development sets (and using word2vec); (2) a WORD model trained on all sequence pairs in the training and development sets with a beam size of 10 for decoding; (3,4) a CHAR+SAMPLE model trained on the training set, decoding the test set twice, each time with different weight biases (the two highest performing via the grid search over the tuning set) with a beam size of 10; and (5) a CHAR model trained on all sequence pairs in the training and development sets, with training suspended at epoch 9 (out of 14) and a beam size of 5 to meet the Shared Task deadline. For reference, we also include the CodaLab evaluation for just the CHAR+SAMPLE model trained on the training set with a beam size of 10, with the bias\n8The choice of models was limited to those that were trained and tuned in time for the Shared Task deadline.\nweights being those that generated the highest F1score on the 10k tuning set."}, {"heading": "7 Discussion", "text": "Of particular interest, the CHAR+SAMPLE model performs well, both in terms of performance on the test set relative to other submissions, as well as on the development set relative to the WORD models and the CNN classifiers. It is possible this is due to the ability of the CHAR models to capture some types of orthographic errors.\nThe empirical results suggest that simply adding additional already correct source-target pairs when training the encoder-decoder models may not boost\nperformance, ceteris paribus, as seen in comparing the performance of CHAR+SAMPLE vs WORD+SAMPLE, and CHAR+ALL vs WORD+ALL. We leave to future work alternative approaches for introducing additional correct (target) sentences, as has been examined for neural machine translation models (Sennrich et al., 2015; Gu\u0308lc\u0327ehre et al., 2015).\nOur results provide initial evidence to support the hypothesis that training at the lowest granularity of annotation is a more efficient use of data than training against the binary label. In future work, we plan to compare against sentence classification using LSTMs (Dai and Le, 2015) and convolutional models that use correction-level annotations.\nAnother benefit of the encoder-decoder models is that they can be used to generate corrections (and identify locations of intra-sentence errors) for endusers. However, the added generation capabilities of the encoder-decoder models comes at the expense of considerably longer training and testing times compared to the CNN classifiers.\nWe found that post-hoc tuning provides a straightforward means of tuning the precision-recall tradeoff for these models, and we speculate (but leave to future work for investigation) that in practice, endusers might prefer greater emphasis placed on precision over recall."}, {"heading": "8 Conclusion", "text": "We have presented our submission to the AESW 2016 Shared Task, suggesting, in particular, the utility of a neural attention-based model for sentencelevel grammatical error identification. Our models do not make use of hand-tuned rules, are not trained with explicit syntactic annotations, and do not make use of individuals classifiers designed for humandesignated subsets of errors.\nFor the encoder-decoder models, modeling at the sub-word level was beneficial, even though predictions were still made at the word level. It would be of interest to push this further to eliminate the need for an initial tokenization step, in order to generalize the approach to other languages, such as Chinese and Japanese.\nWe plan to examine alternative approaches for training with additional correct (target) sentences. Inducing artificial errors to generate more incorrect (source) sentences is also a direction we intend to pursue.\nWe leave for future work an analysis of the generation quality of our encoder-decoder models on the AESW dataset and the CoNLL-2014 Shared Task data, as well as user studies to assess whether performance is sufficient in practice to be useful, including the utility of correction vs. identification.\nWe consider this to be just the beginning of the development of data-driven support tools for writers, and many areas remain to be explored."}, {"heading": "Acknowledgments", "text": "We would like to thank the organizers of the Shared Task for coordinating the task and making the unique AESW dataset available for research purposes. The Institute for Quantitative Social Science (IQSS) and the Harvard Initiative for Learning and Teaching (HILT) supported earlier, related research that led to our participation in the Shared Task. Jeffrey Ling graciously contributed a torch-based CNN implementation of Kim (2014)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le."], "venue": "CoRR, abs/1511.01432.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Helping our own: The hoo 2011 pilot shared task", "author": ["Robert Dale", "Adam Kilgarriff."], "venue": "Proceedings of the 13th European Workshop on Natural Language Generation, ENLG \u201911, pages 242\u2013249, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Dale and Kilgarriff.,? 2011", "shortCiteRegEx": "Dale and Kilgarriff.", "year": 2011}, {"title": "Hoo 2012: A report on the preposition and determiner error correction shared task", "author": ["Robert Dale", "Ilya Anisimoff", "George Narroway."], "venue": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54\u201362, Stroudsburg, PA,", "citeRegEx": "Dale et al\\.,? 2012", "shortCiteRegEx": "Dale et al\\.", "year": 2012}, {"title": "A report on the automatic evaluation of scientific writing shared task", "author": ["Vidas Daudaravicius", "Rafael E. Banchs", "Elena Volodina", "Courtney Napoles."], "venue": "Proceedings of the Eleventh Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Daudaravicius et al\\.,? 2016", "shortCiteRegEx": "Daudaravicius et al\\.", "year": 2016}, {"title": "Grammatical error correction using hybrid systems and type filtering", "author": ["Mariano Felice", "Zheng Yuan", "\u00d8istein E. Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Lan-", "citeRegEx": "Felice et al\\.,? 2014", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "On using monolingual corpora in neural machine translation. CoRR, abs/1503.03535", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput., 9(8):1735\u2013 1780, November.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "Proceedings of AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1412\u20131421, Lisbon, Portugal, September. Association", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neu-", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Ground truth for grammatical error correction metrics", "author": ["Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Napoles et al\\.,? 2015", "shortCiteRegEx": "Napoles et al\\.", "year": 2015}, {"title": "The CoNLL2013 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,", "citeRegEx": "Ng et al\\.,? 2013", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "The CoNLL-2014 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural", "citeRegEx": "Ng et al\\.,? 2014", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "Automated whole sentence grammar correction using a noisy channel model", "author": ["Y. Albert Park", "Roger Levy."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT \u201911,", "citeRegEx": "Park and Levy.,? 2011", "shortCiteRegEx": "Park and Levy.", "year": 2011}, {"title": "The Illinois-Columbia system in the CoNLL-2014 shared task", "author": ["Alla Rozovskaya", "Kai-Wei Chang", "Mark Sammons", "Dan Roth", "Nizar Habash."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 34\u2013", "citeRegEx": "Rozovskaya et al\\.,? 2014", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2014}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "CoRR, abs/1511.06709.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "CoRR, abs/1507.06228.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "CoRR, abs/1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Ye Zhang", "Byron Wallace."], "venue": "CoRR, abs/1510.03820.", "citeRegEx": "Zhang and Wallace.,? 2015", "shortCiteRegEx": "Zhang and Wallace.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Given a large, annotated dataset of scientific journal articles, we propose a fully data-driven approach for this problem, inspired by recent work in neural machine translation and more generally, sequence-tosequence learning (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 226, "endOffset": 291}, {"referenceID": 0, "context": "Given a large, annotated dataset of scientific journal articles, we propose a fully data-driven approach for this problem, inspired by recent work in neural machine translation and more generally, sequence-tosequence learning (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 226, "endOffset": 291}, {"referenceID": 1, "context": "Given a large, annotated dataset of scientific journal articles, we propose a fully data-driven approach for this problem, inspired by recent work in neural machine translation and more generally, sequence-tosequence learning (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 226, "endOffset": 291}, {"referenceID": 9, "context": "We also provide evidence of tangible performance gains using a character-aware version of the model, building on the characteraware language modeling work of Kim et al. (2016). In addition to sentence-level classification, the models are capable of intra-sentence error identification and the generation of possible corrections.", "startOffset": 158, "endOffset": 176}, {"referenceID": 1, "context": "formations using recurrent neural network models (Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 49, "endOffset": 91}, {"referenceID": 20, "context": "formations using recurrent neural network models (Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 49, "endOffset": 91}, {"referenceID": 15, "context": "Unlike the related CoNLL-2014 Shared Task (Ng et al., 2014) data, errors are not labeled with fine-grained types (article or determiner error, verb tense error, etc.", "startOffset": 42, "endOffset": 59}, {"referenceID": 5, "context": "Further information about the task is available in the Shared Task report (Daudaravicius et al., 2016).", "startOffset": 74, "endOffset": 102}, {"referenceID": 16, "context": "Given the lack of a large hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers.", "startOffset": 61, "endOffset": 82}, {"referenceID": 3, "context": "More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al.", "startOffset": 201, "endOffset": 228}, {"referenceID": 4, "context": "More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al., 2012).", "startOffset": 364, "endOffset": 383}, {"referenceID": 14, "context": "The CoNLL-2013 Shared Task (Ng et al., 2013)3 focused on the correction of a limited set of five error types in essays by second-language learners of English at the National University of Singapore.", "startOffset": 27, "endOffset": 44}, {"referenceID": 15, "context": "The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners.", "startOffset": 37, "endOffset": 54}, {"referenceID": 13, "context": "the full generation task is still an open research area, but a subsequent human evaluation ranked the output from the CoNLL-2014 Shared Task systems (Napoles et al., 2015).", "startOffset": 149, "endOffset": 171}, {"referenceID": 17, "context": "Of the non-MT based approaches, the IllinoisColumbia system was a strong performer, combining several classifiers trained for specific types of errors (Rozovskaya et al., 2014).", "startOffset": 151, "endOffset": 176}, {"referenceID": 6, "context": "The system of Felice et al. (2014) ranked highest, utilizing a combination of a rule-based system and phrase-based MT, with re-ranking via a large web-scale language model.", "startOffset": 14, "endOffset": 35}, {"referenceID": 10, "context": "To establish a baseline, we follow past work that has shown strong performance with convolutional neural nets (CNNs) across various domains for sentence-level classification (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 174, "endOffset": 210}, {"referenceID": 22, "context": "To establish a baseline, we follow past work that has shown strong performance with convolutional neural nets (CNNs) across various domains for sentence-level classification (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 174, "endOffset": 210}, {"referenceID": 12, "context": "We utilize the one-layer CNN architecture of Kim (2014) with the publicly available5 word vectors trained on the Google News dataset, which contains about 100 billion words (Mikolov et al., 2013).", "startOffset": 173, "endOffset": 195}, {"referenceID": 10, "context": "To establish a baseline, we follow past work that has shown strong performance with convolutional neural nets (CNNs) across various domains for sentence-level classification (Kim, 2014; Zhang and Wallace, 2015). We utilize the one-layer CNN architecture of Kim (2014) with the publicly available5 word vectors trained on the Google News dataset, which contains about 100 billion words (Mikolov et al.", "startOffset": 175, "endOffset": 268}, {"referenceID": 20, "context": "We follow past work (Sutskever et al., 2014; Luong et al., 2015) in stacking multiple recurrent neural networks (RNNs), specifically Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, in both the encoder and decoder.", "startOffset": 20, "endOffset": 64}, {"referenceID": 11, "context": "We follow past work (Sutskever et al., 2014; Luong et al., 2015) in stacking multiple recurrent neural networks (RNNs), specifically Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, in both the encoder and decoder.", "startOffset": 20, "endOffset": 64}, {"referenceID": 8, "context": ", 2015) in stacking multiple recurrent neural networks (RNNs), specifically Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, in both the encoder and decoder.", "startOffset": 106, "endOffset": 140}, {"referenceID": 2, "context": "word2vec/ approach of Dai and Le (2015), we hypothesize that training at the lowest granularity of annotations may be useful for the task.", "startOffset": 22, "endOffset": 40}, {"referenceID": 2, "context": "word2vec/ approach of Dai and Le (2015), we hypothesize that training at the lowest granularity of annotations may be useful for the task. We also suspect that the generation of corrections is of sufficient utility for endusers to further justify exploring models that produce corrections in addition to identification. We thus use the Shared Task as a means of assessing the utility of a full generation model for the binary prediction task. We propose two encoder-decoder architectures for this task. Our word-based architecture (WORD) is similar to that of Luong et al. (2015). Our character-based models (CHAR) still make predictions at the word-level, but use a CNN and a highway network over characters instead of word embeddings as the input to the encoder and decoder, as depicted in Figure 1.", "startOffset": 22, "endOffset": 580}, {"referenceID": 11, "context": "Finally, as in Luong et al. (2015), we feed cj as additional input to the decoder for the next time step by concatenating it with xj , so the decoder equation is modified to,", "startOffset": 15, "endOffset": 35}, {"referenceID": 9, "context": "Our character model closely follows that of Kim et al. (2016). Suppose word si is composed of characters [p1, .", "startOffset": 44, "endOffset": 62}, {"referenceID": 19, "context": "Highway Network Instead of replacing the word embedding xi with zi, we feed zi through a highway network (Srivastava et al., 2015).", "startOffset": 105, "endOffset": 130}, {"referenceID": 10, "context": "CNN Architecture and training approaches were informed by past work in sentence-level classification using CNNs (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 112, "endOffset": 148}, {"referenceID": 22, "context": "CNN Architecture and training approaches were informed by past work in sentence-level classification using CNNs (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 112, "endOffset": 148}, {"referenceID": 10, "context": "CNN Architecture and training approaches were informed by past work in sentence-level classification using CNNs (Kim, 2014; Zhang and Wallace, 2015). A limited grid search on the development set determined our use of filter windows of width 3, 4, and 5 and 1000 feature maps. We trained for 10 epochs. Training otherwise followed the approach of the correspondingly named CNN-STATIC and CNN-NONSTATIC models of Kim (2014).", "startOffset": 113, "endOffset": 422}, {"referenceID": 17, "context": "ch encoder-decoder Initial parameter settings (including architecture decisions such as the number of layers and embedding and hidden state sizes) were informed by concurrent work in neural machine translation and existing work such as that of Sutskever et al. (2014) and Luong et al.", "startOffset": 244, "endOffset": 268}, {"referenceID": 11, "context": "(2014) and Luong et al. (2015). We used 4-layer LSTMs with 1000 hidden units in each layer.", "startOffset": 11, "endOffset": 31}, {"referenceID": 11, "context": "(2014) and Luong et al. (2015). We used 4-layer LSTMs with 1000 hidden units in each layer. We trained for 14 epochs with a batch size of 64 and a maximum sequence length of 50. The parameters for the WORD model were uniformly initialized in [\u22120.1, 0.1], and those of the CHAR model were uniformly initialized in [\u22120.05, 0.05]. The L2normalized gradients were constrained to be \u2264 5. Our learning rate schedule started the learning rate at 1 and halved the learning rate after each epoch beyond epoch 10, or once the validation set perplexity no longer improved. The WORD model used 1000-dimensional word embeddings. For CHAR, the character embeddings were 25-dimensional, the filter width was 6, the number of feature maps was 1000, and 2 highway layers were used. The maximum word length was 35 characters for training CHAR. Note that we do not reverse the source (s) sequences, unlike some previous NMT work. Following the work of Zaremba et al. (2014), we employed dropout with a probability of 0.", "startOffset": 11, "endOffset": 955}, {"referenceID": 18, "context": "We leave to future work alternative approaches for introducing additional correct (target) sentences, as has been examined for neural machine translation models (Sennrich et al., 2015; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 161, "endOffset": 207}, {"referenceID": 7, "context": "We leave to future work alternative approaches for introducing additional correct (target) sentences, as has been examined for neural machine translation models (Sennrich et al., 2015; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 161, "endOffset": 207}, {"referenceID": 2, "context": "In future work, we plan to compare against sentence classification using LSTMs (Dai and Le, 2015) and convolutional models that use correction-level annotations.", "startOffset": 79, "endOffset": 97}, {"referenceID": 10, "context": "Jeffrey Ling graciously contributed a torch-based CNN implementation of Kim (2014).", "startOffset": 72, "endOffset": 83}], "year": 2016, "abstractText": "We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications. We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart. Our final model\u2014a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN\u2014is the highest performing system on the AESW 2016 binary prediction Shared Task.", "creator": "LaTeX with hyperref package"}}}