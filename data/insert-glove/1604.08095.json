{"id": "1604.08095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Accent Classification with Phonetic Vowel Representation", "abstract": "english-french Previous durbrow accent classification godlessness research isenberg focused mainly on dorpat detecting 88-86 accents with pure knafel acoustic information rumps without fetherston recognizing winneke accented speech. spens This work denigrates combines cassetti phonetic knowledge such 280-pound as vowels komisarjevsky with danica acoustic 22.93 information to appanages build flyball Guassian Mixture Model (gashi GMM) hagiographers classifier huld with Perceptual horrid Linear ahlmark Predictive (flashbacks PLP) hyaluronan features, praunheim optimized insurable by jpi Hetroscedastic Linear Discriminant Analysis (yenagoa HLDA ). With input sagredo about 20 - second l\u0119borska accented rotative speech, 13-footer this 30-story system schervish achieves classification nystedt rate drosophilidae of peterffy 51% on a 90e 7 - holdover way 52.91 classification system climbs focusing yukichi on the mini major types makis of accents in English, which halep is bownes competitive jenness to bnp the state - an\u00f3n of - the - sarkisov art darwitz results in this sacchi field.", "histories": [["v1", "Wed, 24 Feb 2016 02:50:44 GMT  (779kb,D)", "http://arxiv.org/abs/1604.08095v1", "Asian Conference on Pattern Recognition (ACPR) 2015"]], "COMMENTS": "Asian Conference on Pattern Recognition (ACPR) 2015", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["zhenhao ge", "yingyi tan", "aravind ganapathiraju"], "accepted": false, "id": "1604.08095"}, "pdf": {"name": "1604.08095.pdf", "metadata": {"source": "CRF", "title": "Accent Classification with Phonetic Vowel Representation", "authors": ["Zhenhao Ge", "Yingyi Tan", "Aravind Ganapathiraju"], "emails": ["aravind.ganapathiraju}@inin.com"], "sections": [{"heading": "1. Introduction", "text": "Improving speech recognition for accented speakers is becoming increasingly more important as businesses become more international. However, handling calls with accents is still a major challenge for companies specializing in speech recognition support services. It requires an accurate and efficient accent classification algorithm, which can identify the accent of the call during a short amount of time, after which, an accent-adapted speech recognition engine can be employed to better recognize accented speech.\nAccent classification recently gains more interests, probably due to the increasing demands for better speaker recognition with accented speech. Recently, Choueiter et al. achieved 32% classification rate on 23-way classification of accented English [2], using methods such as Maximum Mutual Information (MMI) training and Gaussian tokenization. Omar et al. used Support Vector Machine (SVM) classifier integrated with Universal Background Model (UBM) and claimed they outperformed the results in [2] by 75.3% relatively [14]. Another work in [12] reported classification rates of 73% and 58.9% for German vs. Spanish classification using Gaussian Mixture Models (GMMs) and naive Bayes classification respectively. In addition, classification rates of 36.2%, 17.7% and 13.2% were reported for 4-, 13- and 23-way classification using naive Bayes. To the best of our knowledge, these are the only three works, which used the same dataset as we used in this work.\nIn this paper, a baseline accent classifier is created using GMMs with purely acoustic features, such as Perceptual Linear Predictive (PLP), which were then discriminatively optimized by Heteroscedastic Linear Discriminant Analysis (HLDA). Based on the fact that most of accents are presented from the pronunciation of vowels rather than consonants, for each type of accents, various GMMs are computed using the same PLP-HLDA features for the vowels extracted from speech. Then, these GMMs are combined to form a single GMM. With the partial transcription of the database for 7 major types of accents, which is absent in [2] and [12], the vowels are extracted and with these phonetic information, the classification rate of 7-way classification is improved from 46% to 51%, compared with the baseline.\nThis work was initiated during the first author\u2019s internship at Interactive Intelligence [4]. The algorithm and experiment was later refined for better accuracy and efficiency. The following sections are organized as follows: Sec. 2 introduces the database and features used here; the main concept of creating accent-adapted features based on vowel representation is illustrated in Sec. 3; in Sec. 4 and 5, the implementation and results for the baseline GMM-HLDA classier and the improved accent classifier with vowel extraction and representation are described in details, followed by the summary and future work in Sec. 6."}, {"heading": "2. Data Preparation", "text": "The database used here for developing accent classifiers is Foreign Accented English (FAE) corpus. It was originally collected by the Center of Speech & Language Understanding (CSLU) at Oregon Health & Science University (OHSU). It contains 4925 sentences about 20 seconds long each, from speakers with 23 types of accents.\nWe group them into 7 regional accents and one type of accents in each group was selected for developing a 7- way accent classifier, including Arabic (AR), Brazilian Portuguese (BR), French (FR), German (GE), Hindi (HI), Mandarin (MA) and Russian (RU). Tab. 1 provides a summary of these accents with the number of utterances in each type and their proportion of the entire FAE corpus. In order to perform phoneme alignment which is necessary to extract features with phonetic information, we also transcribed the\nar X\niv :1\n60 4.\n08 09\n5v 1\n[ cs\n.S D\n] 2\n4 Fe\nb 20\n16\nTab. 1: Summary of selected accents in FAE corpus Accents No. of Proportion Total Total Comp. (Abbr.) utterances (%) Duration1 Duration2 rate (%)\nAR 112 2.27 0:34:32 0:29:11 84.5 BP 459 9.32 2:34:24 2:09:58 84.2 FR 284 5.77 1:31:05 1:18:44 86.4 GE 325 6.6 1:36:04 1:22:18 85.7 HI 348 7.07 1:56:10 1:36:31 83.1 MA 282 5.73 1:30:37 1:16:06 84.0 RU 236 4.79 1:11:13 0:59:54 84.1\nNote: duration1 and duration2 are the duration before and after silence removal\naudio data of these 7 major accents, which is originally absent in the LDC\u2019s release.\nData from these accents were then preprocessed with silence removal by thresholding on its short-time energy rate and spectral centroids, using method in [8]. Given the audio samples si(n), n \u2208 [1, N ] in the ith frame, its short-time energy rate, denoted as ei, can be formulated as\nei = 1\nN N\u2211 n=1 |si(n)|2, (1)\nwhere N is the number of samples in one frame. The spectral centroid can be defined as\nci = K\u2211 k=1 (k + 1)Si(k)/ K\u2211 k=1 Si(k), (2)\nwhere Si(k), k \u2208 [1,K] is the Discrete Fourier Transform (DFT) coefficients of si. The short-time energy rate is the most useful feature to discriminate silence with environmental noise from speech, and the spectral centroids can be used to remove non-speech noise, such as coughing, due to its lower energy concentration in the spectrum, ralative to that of regular human speech. Fig. 1 demonstrates the silence removal using both measurements on file FAR00042.wav in FAE corpus with Arabic accents. The portion of speech is considered to be silence when either the smoothed short-time energy rate and the smoothed spectral centroids are below certain thresholds. Tab. 1 shows the\nComparison\u00a0of\u00a05\u00a0vowels\u00a0locations\u00a0in\u00a0 standard\u00a0and\u00a0accented\u00a0language\ntotal durations before and after silence removal and their corresponding compression rate.\nThe silence-removed data of the selected accents were then converted to 39-dimensional PLP features [3]. Feature Mean and Variance Normalization (MVN) were applied afterwards. They were randomly divided into training, development and testing with ratio 70 : 15 : 15."}, {"heading": "3. Vowel Representation", "text": "Inspired by the work from Minematsu et al. [13] and Suzuki et al. [15], where they measured the overall structure of the speaker\u2019s phonetic space, one type of accent-adapted features can be obtained by extracting vowels from speech and use them to identify accents. For each type of accented version of a target language, such as English, as well as the standard one, it is assumed that the features of the five fundamental vowels are located relatively constantly in the feature space. In Fig. 2, the first two feature dimensions are taken to illustrate the position of five vowels in accented and non-accented (standard) languages [13]. The center in each pentagon is the weighted average of five vowels based on their positions in feature space and frequency of appearance in the corpus. By matching the center of the pentagon of the standard and the accented language into the overlapped pentagon in the bottom of Fig. 2, the Bhattacharyya distances [1] between each pair of corresponding vowels and their angles can be computed and stored in a vector. This vector Vi represents the difference from the accented language Li to the standard one L. To classify the test speech into one of the accent categories L1, L2, . . . , LN , where N is the number of accents, the difference from Vj to Vi, i \u2208 [1, N ] and V (category of the standard language) are computed and classified to the nearest category of accent."}, {"heading": "4. Baseline with Pure Acoustic Information", "text": "As mentioned in Sec. 1, the baseline accent classification system is implemented using GMM classifier with PLP feature discriminatively optimized by HLDA, which is a generalization of LDA allowing features to have different variances in different feature dimensions. Here we briefly describe the key components of GMM, LDA and HLDA, then discuss the implementation and results of the baseline."}, {"heading": "4.1. GMM Classifer", "text": "Motivated by the method of modeling attributes of speakers using Gaussian Mixture Models (GMMs) in [6], here we use GMMs to model the attributes of accents. Gaussian mixture density models the feature distribution of each accent as a weighted sum of multiple Gaussian distributions. Given row feature vector x in M \u00d7K feature matrix X , where M is feature dimension and K is the number of feature vectors, in the probability of x can be formulated as\np(x|\u03bb) = N\u2211 i=1 wibi(x), (3)\nwhere N is the number of mixture components, and\nbi(x) = 1 (2\u03c0)M/2|\u03a3i|1/2 exp{\u22121 2 (x\u2212\u00b5i)T\u03a3\u22121i (x\u2212\u00b5i)}. (4) bi(x), i = 1, . . . , N , are the component densities, wi are the mixture weight for ith mixture, and \u03bb = {wi, \u00b5i,\u03a3i} is the collective representation of the parameters.\nGiven feature matrix X of accent type s, Maximum Likelihood Estimation (MLE) is used to maximize the GMM likelihood, which can be written as\n\u03bb\u2217 = arg max \u03bb p(X|\u03bb) = arg max \u03bb K\u220f k=1 p(xk|\u03bb). (5)\nSince this expression is non-linear and direct maximization is difficult, the parameter set \u03bb = {w, \u00b5,\u03a3} is iteratively estimated using a special case of the ExpectationMaximization (EM) algorithm and is summarized below:\nw\u0304i = 1\nK K\u2211 k=1 p(i|xk, \u03bb);\n\u00b5\u0304i = \u2211K k=1 p(i|xk, \u03bb)xk\u2211K k=1 p(i|xk, \u03bb) ; (6)\n\u03c3\u03042i = \u2211K k=1 p(i|xk, \u03bb)x2k\u2211K k=1 p(i|xk, \u03bb) \u2212 \u00b5\u03042i ,\nwhere w\u0304i, \u00b5\u0304i, \u03c3\u03042i , i = 1, ..., N are the mixture weights, means, and variances for the ith component; p(i|xk, \u03bb) is the a posteriori probability for the i-th component given by\np(i|xk, \u03bb) = wibi(xk)\u2211N j=1 wjbj(xk) . (7)\nThese estimates are based on the assumption of independence among feature dimension, so for each accent type s, the non-zero values of the covariance matrix are only on the diagonals. This algorithm guarantees a monotonic increase of the model\u2019s likelihood on each EM iteration.\nAfter obtaining the GMM parameter set \u03bbs for accent class s \u2208 [1, S], the GMM-based classifier, which maximize a posteriori probability for feature matrix X is:\nS\u0302 = arg max s\u2208[1,S] p(\u03bbs|X) = arg max s\u2208[1,S] p(X|\u03bbs)p(\u03bbs) p(X)\n\u221d arg max s\u2208[1,S] p(X|\u03bbs)\n\u221d arg max s\u2208[1,S] K\u2211 k=1 logp(xk|\u03bbs). (8)\nThe first equation is due to Bayes\u2019 rule. The first proportion is assuming p(\u03bbs) = 1/S and p(X) is the same for all accent models. The second proportion uses logarithm and independence between input samples xk, k \u2208 [1,K]."}, {"heading": "4.2. LDA and HLDA", "text": "Compared with Principle Component Analysis (PCA), which transforms data into eigenspace and preserves the data dimensions with larger variation [5], Linear Discriminant Analysis (LDA) reduces dimensions by mapping data into a subspace while maximizing the discriminative information. Assume there are K = \u2211S s=1Ks number of M - dimensional data vectors xk in S classes, where Ks is the number of vectors in class s \u2208 [1, S]. Let the global mean \u03a6 over all classes be \u03a6 = 1K \u2211K k xk and the local mean\n\u03a6s for each class s be \u03a6s = 1Ks \u2211\nxk\u2208s xk respectively. Then, we define between-class scatter SB and within-class scatter SW by\nSB = 1\nK K\u2211 k=1 (xk \u2212\u03a6)(xk \u2212\u03a6)T , (9)\nSW = 1\nS S\u2211 s=1 \u2211 xk\u2208s (xk \u2212\u03a6s)(xk \u2212\u03a6s)T . (10)\nIf we choose w from the underlying space W , then wTSBw and wTSWw are the projections of SB and SW onto the direction w. Searching the directions w for the best class discrimination is equivalent to maximizing the ratio of (wTSBw)/(w\nTSWw) subject to wTSWw = 1. The latter is called the Fisher Discriminant Function and can be converted to by Lagrange multipliers and solved by eigendecomposition of S\u22121W SB . By selecting eigenvectors associated with the most significant m eigenvalues of S\u22121W SB , one can map the original M -dimensional data into a mdimensional subspace for discriminative feature reduction.\nLDA is derived with the assumption that features in various dimensions have the same variance, which may not be the case in the real problem. For example, consider two classes of data with the Gaussian distributions shown in\nwww.inin.com \u00a92012\u00a0Interactive\u00a0Intelligence\u00a0Group\u00a0Inc.\nFig. 3. They have the same variance and slightly different means in one direction, while same mean and significantly different variances in the other distribution. LDA will project the data to the first direction, since it maximizes the ratio of between-class scatter SB and within-class scatter SW . However, the other direction will lead to the best discriminant information in this case. This work uses Kumar\u2019s method [10] to generalize LDA to HLDA using Maximum Likelihood Estimation (MLE) on Gaussian distributions."}, {"heading": "4.3. Results for Baseline", "text": "The diagram of the 7-way accent classification based on pure acoustic information is demonstrated in Fig. 4. PLPHLDA features with context-size 1 and reduced dimension 20 is used. The context-size factor is used to duplicate features for potential performance improvement. For example, with context-size 1, the original feature frame is elongated with the concatenation from its 1 left frame and 1 right frame. Both the GMM classifier and the improved GMM-HLDA classifier were trained with features of various types of accents of 256 Gaussian mixtures. These parameters, including order of GMM, feature dimension in PLP and HLDA, and context-size were optimized with development set. The performance on the testing set achieve 40% and 46% accuracies using GMM classifier and GMMHLDA classifier."}, {"heading": "5. Improved with Vowel Representation", "text": "To construct the classifier with vowel representation, instead of directly measuring vowel shifting from standard speech to accented one, the same vowel of various types of accents are trained as separated GMMs; instead of using only the fundamental 5 vowels described in Sec. 3, the\nTab. 2: Vowels in Arpabet Vowel aa ae ah ao aw ay eh er Example father fast sun hot how my red bird Vowel ey ih iy ow oy uh uw Example say big meet show boy book food\nsame concept is generalized and all 15 vowels in Arpabet [16] listed in Tab. 2 are used.\nGiven S types of accents and T numbers of vowels,X(t) is the extracted feature set for tth vowel, the improved GMM classifer as the combination of GMM classifers of all vowels, can be formulated as:\nS\u0302 = arg max s\u2208[1,S] T\u2211 t=1 wtp(\u03bbs,t|X(t))\n\u221d arg max s\u2208[1,S] T\u2211 t=1 wt K\u2211 k=1 logp(x (t) k |\u03bbs,t). (11)\nwhere \u03bbs,t is the GMM for sth accent and tth type of vowels, and wt is the proportion of tth vowel in the whole vowel set.\nAdding this additional layer on the GMM classifier is critical for finding the vowel sets which preserve the accents and is shown to improve on classifying accents. However, it requires recognizing these vowels in the front end. During training and development, the phoneme alignment is performed to extract vowels, while during testing, a subset of the recognized vowels with certain level of confidence are selected after phoneme recognition. The HTK Speech Recognition Toolkit was used here for the phoneme alignment and recognition with triphone acoustic models."}, {"heading": "5.1. Phoneme Alignment and Recognition", "text": "In the system developement, with the partial in-house transcriptions of the speech from 7 major accents, we prepare dictionary needed for phoneme alignment using HVite in HTK. Fig. 5 demonstrates the process of dictionary preparation and phoneme alignment for FAE. The dictionary file is a list of word-pronunciations pairs in HTK format, which can be obtained through the process of word collection, word-to-pronunciation conversion with an inhouse lexicon tester and HTK dictionary file creation. In the phoneme alignment, the HTK configuration file, the HMM model definition and the tired list are all trained using Fisher corpus.\nIn the system test, since there is no transcription available , in order to find features corresponding to vowels, accented speech is recognized using HTK and only a subset of recognized vowels with certain level of confidence based on the n-gram log likelihood are used. This threshold is predefined with training and development data."}, {"heading": "5.2. Results for Improved Classifier", "text": "Here 39-dimensional PLP features with MVNs are used in the implementation of accent classification. After training GMMs on seperated vowels, GMMs of 7 vowels out\nwww.inin.com \u00a92012\u00a0Interactive\u00a0Intelligence\u00a0Group\u00a0Inc.\nDictionary\u00a0Preparation\u00a0and\u00a0Phoneme\u00a0Alignment\nof 15 of each accent are selected to form the mixed GMM classifer for that accent. The overall classification accuracy is 50.9%, which gains 4.5% improvement from the GMM classifer trained with HLDA features. Tab. 3 compares the performances of all three methods. The combination of classifier and features include a) GMMs with PLP, trained per accent; b) GMMs with HDLA (20 dimensions with context size 1, optimized from 39-dimensional PLP), trained per accent; and c) GMMs with HLDA, trained per accent and per vowel. The accuracy is obtained from accented speech about 20-second duration, and is competitive compared with the state-of-art results in [2], [14] and [12].\nTab. 3: Result comparison of 7-way accent classification Model GMM256base GMM 256 base GMM 256 vowel\nFeature PLP39MV N HLDA 20 C1 HLDA 20 C1 Accuracy 40.3% 46.4% 50.9%"}, {"heading": "6. Summary and Future Work", "text": "This work shows the classification accuracy improvement with HLDA feature optimization and extracted vowels from accented speech for developing GMM classifiers. There are at least several areas that can be addressed for further improvement. First, more sophisticated classifiers such as deep neural network classifier [9, 7] may also be used for accent classification. Second, since the data for each accent is very limited, a universal classifier based on Restricted Boltzmann Machine (RBM), instead of traditional GMMs for each accents can be explore [11]. RBM is trained using data of all accents, with capability to deviate with different accents. Third, accent clustering based on certain distance measurements, such as Bhattacharyya distance [1] can also be used to pre-classify accents into several clusters, which may potentially help narrow down the search scope and improve the classification accuracy. Forth, in the triphone phoneme alignment and recognition, currently all triphones with the same mid-phone are treated the same. However, the accent patterns may stay in the transition of phonemes, which can be investigated later."}], "references": [{"title": "On a measure of divergence between two multinomial populations", "author": ["A. Bhattacharyya"], "venue": "Sankhya\u0304: The Indian Journal of Statistics (1933-1960),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1946}, {"title": "An empirical study of automatic accent classification", "author": ["G. Choueiter", "G. Zweig", "P. Nguyen"], "venue": "In ICASSP", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "PLP and RASTA (and MFCC, and inversion) in MATLAB. http://labrosa.ee.columbia.edu/ matlab/rastamat", "author": ["D. Ellis"], "venue": "Ac essed 2015-07-01", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Mispronunciation detection for language learning and speech recognition adaptation", "author": ["Z. Ge"], "venue": "Ph.D. dissertation, Purdue University West Lafayette,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Pca method for automated detection of mispronounced words. In SPIE Defense, Security, and Sensing, pages 80581D\u201380581D", "author": ["Z. Ge", "S.R. Sharma", "M.J. Smith"], "venue": "International Society for Optics and Photonics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "PCA/LDA approach for text-independent speaker recognition", "author": ["Z. Ge", "S.R. Sharma", "M.J. Smith"], "venue": "In Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Sleep stages classification using neural networks with multi-channel neural data", "author": ["Z. Ge", "Y. Sun"], "venue": "In Brain Informatics and Health,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A method for silence removal and segmentation of speech signals, implemented in matlab", "author": ["T. Giannakopoulos"], "venue": "University of Athens, Athens,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "In ICASSP. IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Investigation of silicon auditory models and generalization of linear discriminant analysis for improved speech recognition", "author": ["N. Kumar", "A.G. Andreou"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Acoustic adaptation and accent identification in the ICSI MR and FAE corpora", "author": ["J. Mac\u00edas-Guarasa"], "venue": "In ICSI Meeting slides,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Yet another acoustic representation of speech sounds", "author": ["N. Minematsu"], "venue": "In ICASSP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "A novel approach to detecting non-native speakers and their native language", "author": ["M.K. Omar", "J. Pelecanos"], "venue": "In ICASSP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Improved structure-based automatic estimation of pronunciation proficiency", "author": ["M. Suzuki", "L. Dean", "N. Minematsu", "K. Hirose"], "venue": "Proc. SLaTE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "achieved 32% classification rate on 23-way classification of accented English [2], using methods such as Maximum Mutual Information (MMI) training and Gaussian tokenization.", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "used Support Vector Machine (SVM) classifier integrated with Universal Background Model (UBM) and claimed they outperformed the results in [2] by 75.", "startOffset": 139, "endOffset": 142}, {"referenceID": 12, "context": "3% relatively [14].", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "Another work in [12] reported classification rates of 73% and 58.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "With the partial transcription of the database for 7 major types of accents, which is absent in [2] and [12], the vowels are extracted and with these phonetic information, the classification rate of 7-way classification is improved from 46% to 51%, compared with the baseline.", "startOffset": 96, "endOffset": 99}, {"referenceID": 10, "context": "With the partial transcription of the database for 7 major types of accents, which is absent in [2] and [12], the vowels are extracted and with these phonetic information, the classification rate of 7-way classification is improved from 46% to 51%, compared with the baseline.", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "This work was initiated during the first author\u2019s internship at Interactive Intelligence [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "Data from these accents were then preprocessed with silence removal by thresholding on its short-time energy rate and spectral centroids, using method in [8].", "startOffset": 154, "endOffset": 157}, {"referenceID": 2, "context": "The silence-removed data of the selected accents were then converted to 39-dimensional PLP features [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 11, "context": "[13] and Suzuki et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15], where they measured the overall structure of the speaker\u2019s phonetic space, one type of accent-adapted features can be obtained by extracting vowels from speech and use them to identify accents.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "2, the first two feature dimensions are taken to illustrate the position of five vowels in accented and non-accented (standard) languages [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "2, the Bhattacharyya distances [1] between each pair of corresponding vowels and their angles can be computed and stored in a vector.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Motivated by the method of modeling attributes of speakers using Gaussian Mixture Models (GMMs) in [6], here we use GMMs to model the attributes of accents.", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "Compared with Principle Component Analysis (PCA), which transforms data into eigenspace and preserves the data dimensions with larger variation [5], Linear Discriminant Analysis (LDA) reduces dimensions by mapping data into a subspace while maximizing the discriminative information.", "startOffset": 144, "endOffset": 147}, {"referenceID": 9, "context": "This work uses Kumar\u2019s method [10] to generalize LDA to HLDA using Maximum Likelihood Estimation (MLE) on Gaussian distributions.", "startOffset": 30, "endOffset": 34}, {"referenceID": 1, "context": "The accuracy is obtained from accented speech about 20-second duration, and is competitive compared with the state-of-art results in [2], [14] and [12].", "startOffset": 133, "endOffset": 136}, {"referenceID": 12, "context": "The accuracy is obtained from accented speech about 20-second duration, and is competitive compared with the state-of-art results in [2], [14] and [12].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "The accuracy is obtained from accented speech about 20-second duration, and is competitive compared with the state-of-art results in [2], [14] and [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 8, "context": "First, more sophisticated classifiers such as deep neural network classifier [9, 7] may also be used for accent classification.", "startOffset": 77, "endOffset": 83}, {"referenceID": 6, "context": "First, more sophisticated classifiers such as deep neural network classifier [9, 7] may also be used for accent classification.", "startOffset": 77, "endOffset": 83}, {"referenceID": 0, "context": "Third, accent clustering based on certain distance measurements, such as Bhattacharyya distance [1] can also be used to pre-classify accents into several clusters, which may potentially help narrow down the search scope and improve the classification accuracy.", "startOffset": 96, "endOffset": 99}], "year": 2016, "abstractText": "Previous accent classification research focused mainly on detecting accents with pure acoustic information without recognizing accented speech. This work combines phonetic knowledge such as vowels with acoustic information to build Guassian Mixture Model (GMM) classifier with Perceptual Linear Predictive (PLP) features, optimized by Hetroscedastic Linear Discriminant Analysis (HLDA). With input about 20-second accented speech, this system achieves classification rate of 51% on a 7-way classification system focusing on the major types of accents in English, which is competitive to the state-of-the-art results in this field.", "creator": "LaTeX with hyperref package"}}}