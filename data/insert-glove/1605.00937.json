{"id": "1605.00937", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2016", "title": "Dictionary Learning for Massive Matrix Factorization", "abstract": "Sparse e-journals matrix factorization casters is mantin a popular salmen tool pymble to jer obtain interpretable data elvet decompositions, which avery are also yetter effective collodi to perform data ad-free completion richter or denoising. Its subsist applicability to large ofran datasets has been addressed victorinox with 9-month online fleener and ogof randomized csere methods, cpgs that reduce the toray complexity in 16,000-member one red-backed of syquest the matrix dimension, but not in rancocas both le\u015bna of fatwa them. comienza In this anik paper, we yelland tackle paveena very mrkos large matrices harlesden in guha both shabbethai dimensions. We stationers propose eccc a new tilbian factoriza - tion method that 62.62 scales gracefully cavelike to terabyte - mcentegart scale v\u00e4rlden datasets, that could not nazr be vanderlaan processed by previous daallo algorithms in wilrijk a reasonable 370.4 amount of glaza time. gucci We latshaw demonstrate the canjar efficiency nefteyugansk of our 5.14 approach tendering on massive enfolds functional wurts Magnetic Resonance antiandrogens Imaging (haberlin fMRI) madonna data, verba and on matrix 6-to-8 completion government-led problems hopefully for recommender systems, ovate where we obtain libertine significant aerialbots speed - .214 ups compared bornem to anastacio state - of - the nare art pay-per-click coordinate ettaba descent -2.1 methods.", "histories": [["v1", "Tue, 3 May 2016 15:05:32 GMT  (1739kb,D)", "https://arxiv.org/abs/1605.00937v1", null], ["v2", "Thu, 26 May 2016 06:33:22 GMT  (1367kb,D)", "http://arxiv.org/abs/1605.00937v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG q-bio.QM", "authors": ["arthur mensch", "julien mairal", "bertrand thirion", "ga\u00ebl varoquaux"], "accepted": true, "id": "1605.00937"}, "pdf": {"name": "1605.00937.pdf", "metadata": {"source": "META", "title": "Dictionary Learning for Massive Matrix Factorization", "authors": ["Arthur Mensch", "Julien Mairal", "Bertrand Thirion", "Ga\u00ebl Varoquaux"], "emails": ["ARTHUR.MENSCH@M4X.ORG", "JULIEN.MAIRAL@INRIA.FR", "BETRAND.THIRION@INRIA.FR", "GAEL.VAROQUAUX@INRIA.FR"], "sections": [{"heading": null, "text": "Matrix factorization is a flexible tool for uncovering latent factors in low-rank or sparse models. For instance, building on low-rank structure, it has proven very powerful for matrix completion, e.g. in recommender systems (Srebro et al., 2004; Cand\u00e8s & Recht, 2009). In signal processing and computer vision, matrix factorization with a sparse regularization is often called dictionary learning and has proven very effective for denoising and visual feature encoding (see Mairal, 2014, for a review). It is also flexible enough to accommodate a large set of constraints and regularizations, and has gained significant attention in scientific domains where interpretability is a key aspect, such as ge-\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nnetics and neuroscience (Varoquaux et al., 2011).\nAs a widely-used model, the literature of matrix factorization is very rich and two main classes of formulations have emerged. The first one addresses an optimization problem involving a convex penalty, such as the trace or max norms (Srebro et al., 2004). These penalties promote lowrank structures, have strong theoretical guarantees (Cand\u00e8s & Recht, 2009), but they do not encourage sparse factors and lack scalability for very-large datasets. For these reasons, our paper is focused on a second type of approach, that relies on nonconvex optimization. Specifically, the motivation of our work originally came from the need to analyze huge-scale fMRI datasets, and the difficulty of current algorithms to process them.\nTo gain scalability, stochastic (or online) optimization methods have been developed; unlike classical alternate minimization procedures, they learn matrix decompositions by observing a single matrix column (or row) at each iteration. In other words, they stream data along one matrix dimension. Their cost per iteration is significantly reduced, leading to faster convergence in various practical contexts. More precisely, two approaches have been particularly successful: stochastic gradient descent (see Bottou, 2010) has been widely used in recommender systems (see Bell & Koren, 2007; Rendle & Schmidt-Thieme, 2008; Rendle, 2010; Blondel et al., 2015, and references therein), and stochastic majorization-minimization methods for dictionary learning with sparse and/or structured regularization (Mairal et al., 2010; Mairal, 2013). Yet, stochastic algorithms for dictionary learning are currently unable to deal efficiently with matrices that are large in both dimensions.\nIn a somehow orthogonal way, the growth of dataset size has proven to be manageable by randomized methods, that exploit random projections (Johnson & Lindenstrauss, 1984; Bingham & Mannila, 2001) to reduce data dimension\nar X\niv :1\n60 5.\n00 93\n7v 2\n[ st\nat .M\nL ]\n2 6\nM ay\nwithout deteriorating signal content. Due to the way they are generated, large-scale datasets generally have an intrinsic dimension that is significantly smaller than their ambient dimension. Biological datasets (McKeown et al., 1998) and physical acquisitions with an underlying sparse structure enabling compressed sensing (Cand\u00e8s & Tao, 2006) are good examples. In this context, matrix factorization can be performed by using random summaries of coefficients. Recently, those have been used to compute PCA (Halko et al., 2009), a classical matrix decomposition technique. Yet, using random projections as a pre-processing step is not appealing in our applicative context since the factors learned on reduced data loses interpretability.\nMain contribution. In this paper, we propose a dictionary learning algorithm that (i) scales both in the signal dimension (number of rows) and number of signals (number of columns), (ii) deals with various structured sparse regularization penalties, (iii) handles missing values, and (iv) provides an explicit dictionary with easy interpretation. As such, it is non-trivial extension of the online dictionary learning method of Mairal et al. (2010), where, at every iteration, signals are partially observed with a random mask, and with low-complexity update rules that depend on the (small) mask size instead of the signal size.\nTo the best of our knowledge, our algorithm is the first that enjoys all aforementioned features; in particular, we are not aware of any other dictionary learning algorithm that is scalable in both matrix dimensions. For instance, Pourkamali-Anaraki et al. (2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al., 2006) that does not scale well in the number of training signals. Online matrix decomposition in the context of missing values was also proposed by Szab\u00f3 et al. (2011), but without scalability in the signal (row) size.\nOn a massive fMRI dataset (2TB, n = 2.4\u00b7106, p = 2\u00b7105), we were able to learn interpretable dictionaries in about 10 hours on a single workstation, an order of magnitude faster than the online approach of Mairal et al. (2010). On collaborative filtering experiments, where sparsity is not needed, our algorithm performs favorably well compared to state-of-the-art coordinate descent methods. In both experiments, benefits for the practitioner were significant."}, {"heading": "1. Background on Dictionary Learning", "text": "In this section, we introduce dictionary learning as a matrix factorization problem, and present stochastic algorithms that observe one column (or a minibatch) at every iteration."}, {"heading": "1.1. Problem Statement", "text": "The goal of matrix factorization is to decompose a matrix\nX \u2208 Rp\u00d7n \u2013 typically n signals of dimension p \u2013 as a product of two smaller matrices:\nX \u2248 DA with D \u2208 Rp\u00d7k, A \u2208 Rk\u00d7n, (1)\nwith potential sparsity or structure requirements on D and A. In statistical signal applications, this is often a dictionary learning problem, enforcing sparse coefficients A. In such a case, we call D the \u201cdictionary\u201d and A the sparse codes. We use this terminology throughout the paper.\nLearning the dictionary is typically performed by minimizing a quadratic data-fitting term, with constraints and/or penalties over the code and the dictionary:\nmin D\u2208C\nA=[\u03b11,...,\u03b1n]\u2208Rk\u00d7n\nn\u2211 i=1 1 2 \u2225\u2225xi \u2212D\u03b1i\u2225\u222522 + \u03bb\u2126(\u03b1i), (2) where C is a convex set of Rp\u00d7k, and a \u2126 : Rp \u2192 R is a penalty over the code, to enforce structure or sparsity. In large n and large p settings, typical in recommender systems, this problem is solved via block coordinate descent, which boils down to alternating least squares if regularizations on D and \u03b1 are quadratic (Hastie et al., 2014).\nConstraints and penalties. The constraint set C is traditionally a technical constraint ensuring that the coefficients \u03b1 do not vanish, making the effect of the penalty \u2126 disappear. However, other constraints can also be used to enforce sparsity or structure on the dictionary (see Varoquaux et al., 2013). In our paper,C is the Cartesian product of a `1 or `2 norm ball:\nC = {D \u2208 Rp\u00d7k s.t. \u03c8(dj) \u2264 1 \u2200j = 1, . . . , k}, (3)\nwhere D = [d1, . . . ,dk] and \u03c8 = \u2016 \u00b7 \u20161 or \u03c8 = \u2016 \u00b7 \u20162. The choice of \u03c8 and \u2126 typically offers some flexibility in the regularization effect that is desired for a specific problem; for instance, classical dictionary learning uses \u03c8 = \u2016 \u00b7 \u20162 and \u2126 = \u2016\u00b7\u20161, leading to sparse coefficients \u03b1, whereas our experiments on fMRI uses \u03c8 = \u2016\u00b7\u20161 and \u2126 = \u2016\u00b7\u201622, leading to sparse dictionary elements dj that can be interpreted as brain activation maps."}, {"heading": "1.2. Streaming Signals with Online Algorithms", "text": "In stochastic optimization, the number of signals n is assumed to be large (or potentially infinite), and the dictionary D can be written as a solution of\nmin D\u2208C\nf(D) where f(D) = Ex [ l(x,D) ] (4)\nl(x,D) = min \u03b1\u2208Rk\n1 2 \u2016x\u2212D\u03b1\u201622 + \u03bb\u2126(\u03b1),\nwhere the signals x are assumed to be i.i.d. samples from an unknown probability distribution. Based on this formu-\nlation, Mairal et al. (2010) have introduced an online dictionary learning approach that draws a single signal xt at iteration t (or a minibatch), and computes its sparse code \u03b1t using the current dictionary Dt\u22121 according to\n\u03b1t \u2190 argmin \u03b1\u2208Rk\n1 2 \u2016xt \u2212Dt\u22121\u03b1\u201622 + \u03bb\u2126(\u03b1). (5)\nThen, the dictionary is updated by approximately minimizing the following surrogate function\ngt(D) = 1\nt t\u2211 i=1 1 2 \u2225\u2225xi \u2212D\u03b1i\u2225\u222522 + \u03bb\u2126(\u03b1i), (6) which involves the sequence of past signals x1, . . . ,xt and the sparse codes \u03b11, . . . ,\u03b1t that were computed in the past iterations of the algorithm. The function gt is called a \u201csurrogate\u201d in the sense that it only approximates the objective f . In fact, it is possible to show that it converges to a locally tight upper-bound of the objective, and that minimizing gt at each iteration asymptotically provides a stationary point of the original optimization problem. The underlying principle is that of majorization-minimization, used in a stochastic fashion (Mairal, 2013).\nOne key to obtain efficient dictionary updates is the observation that the surrogate gt can be summarized by a few sufficient statistics that are updated at every iteration. In other words, it is possible to describe gt without explicitly storing the past signals xi and codes \u03b1i for i \u2264 t. Indeed, we may define two matrices Bt \u2208 Rp\u00d7k and Ct \u2208 Rk\u00d7k\nCt = 1\nt t\u2211 i=1 \u03b1i\u03b1 > i Bt = 1 t t\u2211 i=1 xi\u03b1 > i , (7)\nand the surrogate function is then written:\ngt(D) = 1\n2 Tr(D>DCt \u2212D>Bt) +\n\u03bb\nt t\u2211 i=1 \u2126(\u03b1i). (8)\nThe gradient of gt can be computed as\n\u2207Dgt(D) = DCt \u2212Bt. (9)\nMinimization of gt is performed using block coordinate descent on the columns of D. In practice, the following updates are successively performed by cycling over the dictionary elements dj for j = 1, . . . , k\ndj \u2190 Proj\u03c8(.)\u22641 [ dj \u2212\n1\nCt[j, j] \u2207djgt(D)\n] , (10)\nwhere Proj denotes the Euclidean projection over the constraint norm constraint \u03c8. It can be shown that this update corresponds to minimizing gt with respect to dj when fixing the other dictionary elements (see Mairal et al., 2010)."}, {"heading": "1.3. Handling Missing Values", "text": "Factorization of matrices with missing value have raised a significant interest in signal processing and machine learning, especially as a solution for recommender systems. In the context of dictionary learning, a similar effort has been made by Szab\u00f3 et al. (2011) to adapt the framework to missing values. Formally, a mask M, represented as a binary diagonal matrix in {0, 1}p\u00d7p, is associated with every signal x, such that the algorithm can only observe the product Mtxt at iteration t instead of a full signal xt. In this setting, we naturally derive the following objective\nmin D\u2208C\nf(D) where f(D) = Ex,M [ l(x,M,D) ] (11)\nl(x,M,D) = min \u03b1\u2208Rk\np\n2Tr M \u2016M(x\u2212D\u03b1)\u201622 + \u03bb\u2126(\u03b1),\nwhere the pairs (x,M) are drawn from the (unknown) data distribution. Adapting the online algorithm of Mairal et al. (2010) would consist of drawing a sequence of pairs (xt,Mt), and building the surrogate\ngt(D) = 1\nt t\u2211 i=1 p 2si \u2225\u2225Mi(xi \u2212D\u03b1i)\u2225\u222522 + \u03bb\u2126(\u03b1i), (12) where si = Tr Mi is the size of the mask and\n\u03b1i \u2208 argmin \u03b1\u2208Rk\np\n2si \u2016Mi(xi \u2212Di\u22121\u03b1)\u201622 + \u03bb\u2126(\u03b1). (13)\nUnfortunately, this surrogate cannot be summarized by a few sufficient statistics due to the masks Mi: some approximations are required. This is the approach chosen by Szab\u00f3 et al. (2011). Nevertheless, the complexity of their update rules is linear in the full signal size p, which makes them unadapted to the large-p regime that we consider."}, {"heading": "2. Dictionary Learning for Massive Data", "text": "Using the formalism exposed above, we now consider the problem of factorizing a large matrix X in Rp\u00d7n into two factors D in Rp\u00d7k and A in Rk\u00d7n with the following setting: both n and p are large (greater than 100 000 up to several millions), whereas k is reasonable (smaller than 1 000 and often near 100), which is not the standard dictionarylearning setting; some entries of X may be missing. Our objective is to recover a good dictionary D taking into account appropriate regularization.\nTo achieve our goal, we propose to use an objective akin to (11), where the masks are now random variables independant from the samples. In other words, we want to combine ideas of online dictionary learning with random subsampling, in a principled manner. This leads us to consider an infinite stream of samples (Mtxt)t\u22650, where the signals xt are i.i.d. samples from the data distribution \u2013 that\nis, a column of X selected at random \u2013 and Mt \u201cselects\u201d a random subset of observed entries in X. This setting can accommodate missing entries, never selected by the mask, and only requires loading a subset of xt at each iteration.\nThe main justification for choosing this objective function is that in the large sample regime p k that we consider, computing the code \u03b1i using only a random subset of the data xt according to (13) is a good approximation of the code that may be computed with the full vector xt in (5). This of course requires choosing a mask that is large enough; in the fMRI dataset, a subsampling factor of about r = 10 \u2013 that is only 10% of the entries of xt are observed \u2013 resulted in a similar 10\u00d7 speed-up (see experimental section) to achieve the same accuracy as the original approach without subsampling. This point of view also justifies the natural scaling factor pTr M introduced in (11).\nAn efficient algorithm must address two challenges: (i) performing dictionary updates that do not depend on p but only on the mask size; (ii) finding an approximate surrogate function that can be summarized by a few sufficient statistics. We provide a solution to these two issues in the next subsections and present the method in Algorithm 1."}, {"heading": "2.1. Approximate Surrogate Function", "text": "To approximate the surrogate (8) from \u03b1t computed in (13), we consider ht defined by\nht(D) = 1\n2 Tr(D>DCt\u2212D>Bt)+\n\u03bb\nt t\u2211 i=1 si p \u2126(\u03b1i) (14)\nwith the same matrix Ct as in (8), which is updated as Ct \u2190 (\n1\u2212 1 t\n) Ct\u22121 + 1\nt \u03b1t\u03b1\n> t , (15)\nand to replace Bt in (8) by the matrix\nBt = ( t\u2211 i=1 Mi )\u22121 t\u2211 i=1 Mixi\u03b1 > i , (16)\nwhich is the same as (7) when Mi = I. Since Mi is a diagonal matrix, \u2211t i=1 Mi is also diagonal and simply \u201ccounts\u201d how many times a row has been seen by the algorithm. Bt thus behaves like Ex[x\u03b1(x,Dt)>] for large t, as in the fully-observed algorithm. By design, only rows of Bt selected by the mask differ from Bt\u22121. The update can therefore be achieved in O(sik) operations:\nBt = Bt\u22121+ ( t\u2211 i=1 Mi )\u22121 ( Mtxt\u03b1 > t \u2212MtBt\u22121 ) (17) This only requires keeping in memory the diagonal matrix\u2211t i=1 Mi, and updating the rows of Bt\u22121 selected by the mask. All operations only depend on the mask size si instead of the signal size p."}, {"heading": "2.2. Efficient Dictionary Update Rules", "text": "With a surrogate function in hand, we now describe how to update the codes \u03b1 and the dictionary D when only partial access to data is possible. The complexity for computing the sparse codes \u03b1t is obviously independent from p since (13) consists in solving a reduced penalized linear regression of Mtxt in Rst on MtDt\u22121 in Rst\u00d7k. Thus, we focus here on dictionary update rules.\nThe naive dictionary update (18) has complexity O(kp) due to the matrix-vector multiplication for computing \u2207djgt(D). Reducing the single iteration complexity of a factor pst requires reducing the dimensionality of the dictionary update phase. We propose two strategies to achieve that, both using block coordinate descent, by considering\ndj \u2190 Proj\u03c8(.)\u22641 [ dj \u2212\n1\nCt[j, j] Mt\u2207djht(D)\n] , (18)\nwhere Mt\u2207djht(D) is the partial derivative of ht with respect to the j-th column and rows selected by the mask.\nGradient step. The update (18) represents a classical block coordinate descent step involving particular blocks. Following Mairal et al. (2010), we perform one cycle over the columns warm-started on Dt\u22121. Formally, the gradient step without projection for the j-th component consists of updating the vector dj\ndj \u2190 dj \u2212 1\nCt[j, j] Mt\u2207djht(D)\n= dj \u2212 1\nCt[j, j] (MtDc\nt j \u2212Mtbtj),\n(19)\nwhere ctj ,b t j are the j-th columns of Ct, Bt respectively. The update has complexityO(kst) since it only involves st rows of D and only st entries of dj have changed.\nProjection step. Block coordinate descent algorithms require orthogonal projections onto the constraint set C. In our case, this amounts to the projection step on the unit ball corresponding to the norm \u03c8 in (18). The complexity of such a projection is usually O(p) both for `2 and `1-norms (see Duchi et al., 2008). We consider here two strategies.\nExact lazy projection for `2. When \u03c8 = `2, it is possible to perform the projection implicitly with complexity O(st). The computational trick is to notice that the projection amounts to a simple rescaling operation\ndj \u2190 dj\nmax(1, \u2016dj\u20162) , (20)\nwhich may have low complexity if the dictionary elements dj are stored in memory as a product\nProcedure 1 Dictionary Learning for Massive Data Input: Initial dictionary: D0 \u2208 Rp\u00d7k, tolerance: C0 \u2190 0 \u2208 Rk\u00d7k; B0 \u2190 0 \u2208 Rp\u00d7k; E0 \u2190 0 \u2208 Rp\u00d7p (diagonal); t\u2190 1; repeat\nDraw a pair (xt,Mt); \u03b1t\u2190argmin\u03b1 12\u2016Mt(xt\u2212Dt\u22121\u03b1)\u201622+\u03bbTr Mtp \u2126(\u03b1); Et \u2190 Et + Mt; At \u2190 (1\u2212 1t )At\u22121 + 1t\u03b1t\u03b1t>; Bt \u2190 Bt\u22121 + E\u22121t (Mtxt\u03b1t> \u2212MtBt\u22121); Dt \u2190 dictionary_update(Bt,Ct,Dt\u22121,Mt);\nuntil |ht\u22121(Dt\u22121)ht(Dt) \u2212 1| < Output: D\ndj=fj/max(1, lj) where fj is in Rp and lj is a rescaling coefficient such that lj = \u2016fj\u20162. We code the gradient step (19) followed by `2-ball projection by the updates\nnj \u2190 \u2016Mjfj\u201622\nfj \u2190 fj \u2212 max(1, lj)\nCt[j, j] (MtDc\nt j \u2212Mtbtj)\nlj \u2190 \u221a l2j \u2212 nj + \u2016Mjfj\u201622\n(21)\nNote that the update of fj corresponds to the gradient step without projection (19) which costs O(kst), whereas the norm of fj is updated in O(st) operations. The computational complexity is thus independent of p and the only price to pay is to rescale the dictionary elements on the fly, each time we need access to them.\nExact lazy projection for `1. The case of `1 is slightly different but can be handled in a similar manner, by storing an additional scalar lj for each dictionary element dj . More precisely, we store a vector fj in Rp such that dj = Proj\u03c8(.)\u22641[fj ], and a classical result (see Duchi et al., 2008) states that there exists a scalar lj such that\ndj = Slj [fj ], S\u03bb(u) = sign(u).max(|u| \u2212 \u03bb, 0) (22)\nwhere S\u03bb is the soft-thresholding operator, applied elementwise to the entries of fj . Similar to the case `2, the \u201clazy\u201d projection consists of tracking the coefficient lj for each dictionary element and updating it after each gradient step, which only involves st coefficients. For such sparse updates followed by a projection onto the `1-ball, Duchi et al. (2008) proposed an algorithm to find the threshold lj in O(st log(p)) operations. The lazy algorithm involves using particular data structures such as red-black trees and is not easy to implement; this motivated us to investigate another simple heuristic that also performs well in practice.\nApproximate low-dimension projection. The heuristic consists in performing the projection by forcing the\nProcedure 2 Dictionary Update Input: B,C,D,M\nfor j \u2208 1, . . . , k do dj \u2190 dj \u2212 1C[j,j] (MDcj \u2212Mbj); if approximate projection then vj \u2190 ProjTj [Mdj ], (see main text for the definition of Tj); dj \u2190 dj + Mvj \u2212Mdj ;\nelse if exact (lazy) projection then or dj \u2190 Proj\u03c8(.)\u22641 [dj ];\nend if end for\ncoefficients outside the mask not to change. This results in the orthogonal projection of each dj on Tt,j = {d s.t. \u03c8(d) \u2264 1, (I\u2212Mt)d = (I\u2212Mt)dt\u22121j }, which is a subset of the original constraint set \u03c8(\u00b7) \u2264 1. All the computations require only 4 matrices kept in memory B, C, D, E with additional F, l matrices and vectors for the exact projection case, as summarized in Alg. 1."}, {"heading": "2.3. Discussion", "text": "Relation to classical matrix completion formulation. Our model is related to the classical `2-penalized matrix completion model (e.g. Bell & Koren, 2007) we rewrite n\u2211 i=1 \u2016Mi(xi \u2212D>\u03b1i)\u201622 + \u03bbsi\u2016\u03b1i\u201622 + \u03bb\u2016( n\u2211 i=1 Mi) 1 2D\u201622 (23) With quadratic regularization on D and A \u2013 that is, using \u2126 = \u2016.\u201622 and \u03c8 = \u2016.\u20162 \u2013 (11) only differs in that it uses a penalization on D instead of a constraint. Srebro et al. (2004) introduced the trace-norm regularization to solve a convex problem equivalent to (23). The major difference is that we adopt a non-convex optimization strategy, thus losing the benefits of convexity, but gaining on the other hand the possibility of using stochastic optimization.\nPractical considerations. Our algorithm can be slightly modified to use weights wt that differ from 1t for B and C, as advocated by Mairal (2013). It also proves beneficial to perform code computation on mini-batches of masked samples. Update of the dictionary is performed on the rows that are seen at least once in the masks (Mt)batch."}, {"heading": "3. Experiments", "text": "The proposed algorithm was designed to handle massive datasets: masking data enables streaming a sequence (Mtxt)t instead of (xt)t, reducing single-iteration computational complexity and IO stress of a factor r = pE(Tr M) , while accessing an accurate description of the data. Hence,\nwe analyze in detail how our algorithm improves performance for sparse decomposition of fMRI datasets. Moreover, as it relies on data masks, our algorithm is well suited for matrix completion, to reconstruct a data stream (xt)t from the masked stream (Mtxt)t. We demonstrate the accuracy of our algorithm on explicit recommender systems and show considerable computational speed-ups compared to an efficient coordinate-descent based algorithm.\nWe use scikit-learn (Pedregosa et al., 2011) in experiments, and have released a python package1 for reproducibility."}, {"heading": "3.1. Sparse Matrix Factorization for fMRI", "text": "Context. Matrix factorization has long been used on functional Magnetic Resonance Imaging (McKeown et al., 1998). Data are temporal series of 3D images of brain activity, to decompose in spatial modes capturing regions that activate together. The matrices to decompose are dense and heavily redundant, both spatially and temporally: close voxels and successive records are correlated. Data can be huge: we use the whole HCP dataset (Van Essen et al., 2013), with n = 2.4 \u00b7 106 (2000 records, 1 200 time points) and p = 2 \u00b7 105, totaling 2 TB of dense data. Interesting dictionaries for neuroimaging capture spatiallylocalized components, with a few brain regions. This can be obtained by enforcing sparsity on the dictionary: in our formalism, this is achieved with `1-ball projection for D. We set C = Bk1 , and \u2126 = \u2016 \u00b7 \u201622. Historically, such decomposition have been obtained with the classical dictionary learning objective on transposed data (Varoquaux et al., 2013): the code A holds sparse spatial maps and voxel time-series are streamed. However, given the size of n for our dataset, this method is not usable in practice.\nHandling such volume of data sets new constraints. First, efficient disk access becomes critical for speed. In our case, learning the dictionary is done by accessing the data in row batches, which is coherent with fMRI data storage: no time is lost seeking data on disk. Second, reducing IO load on\n1http://github.com/arthurmensch/modl\nthe storage is also crucial, as it lifts bottlenecks that appear when many processes access the same storage at the same time, e.g. during cross-validation on \u03bb within a supervised pipeline. Our approach reduces disk usage by a factor r. Finally, parallel methods based on message passing, such as asynchronous coordinate descent, are unlikely to be efficient given the network / disk bandwidth that each process requires to load data. This makes it crucial to design efficient sequential algorithms.\nExperiment We quantify the effect of random subsampling for sparse matrix factorization, in term of speed and accuracy. A natural performance evaluation is to measure an empirical estimate of the loss l defined in Eq. 4 from unseen data, to rule out any overfitting effect. For this, we evaluate l on a test set (xi)i<N . Pratically, we sample (xt)t in a pseudo-random manner: we randomly select a record, from where we select a random batch of rows xt \u2013 we use a batch size of 40, empirically found to be efficient. We load Mtxt in memory and perform an iteration of the algorithm. The mask sequence is sampled by breaking random permutation vectors into chunks of size p/r.\nResults Fig. 1(a) compares our algorithm with subsampling ratios r in {4, 8, 12} to vanilla online dictionary learning algorithm (r = 1), plotting trajectories of the test objective against real CPU time. There is no obvious choice of \u03bb due to the unsupervised nature of the problem:\nwe use 10\u22123 and 10\u22124, that bounds the range of \u03bb providing interpretable dictionaries.\nFirst, we observe the convergence of the objective function for all tested r, providing evidence that the approximations made in the derivation of update rules does not break convergence for such r. Fig. 1(b) shows the validity of the obtained dictionary relative to the reference output: both objective function and `1/`2 ratio \u2013 the relevant value to measure sparsity in our setting \u2013 are comparable to the baseline values, up to r = 8. For high regularization and r = 12, our algorithm tends to yield somewhat sparser solutions (5% lower `1/`2) than the original algorithm, due to the approximate `1-projection we perform. Obtained maps still proves as interpretable as with baseline algorithm.\nOur algorithm proves much faster than the original one in finding a good dictionary. Single iteration time is indeed reduced by a factor r, which enables our algorithm to go over a single epoch r times faster than the vanilla algorithm and capture the variability of the dataset earlier. To quantify speed-ups, we plot the empirical objective value of D against the number of observed records in Fig. 3. For r \u2264 12, increasing r little reduces convergence speed per epoch: random subsampling does not shrink much the quantity of information learned at each iteration.\nThis brings a near \u00d7r speed-up factor: for high and low regularization respectively, our algorithm converges in 3 and 10 hours with subsampling factor r = 12, whereas the vanilla online algorithm requires about 30 and 100 hours. Qualitatively, Fig. 2 shows that with the same time budget, the proposed reduction approach with r = 12 on half of the data gives better results than processing a small fraction of the data without reduction: segmented regions are less noisy and closer to processing the full data.\nThese results advocates the use of a subsampling rate of r \u2248 10 in this setting. When sparse matrix decomposition is part of a supervised pipeline with scoring capabilities, it is possible to find r efficiently: start by setting it derea-\nsonably high and decrease it geometrically until supervised performance (e.g. in classification) ceases to improve."}, {"heading": "3.2. Collaborative Filtering with Missing Data", "text": "We validate the performance of the proposed algorithm on recommender systems for explicit feedback, a well-studied matrix completion problem. We evaluate the scalability of our method on datasets of different dimension: MovieLens 1M, MovieLens 10M, and 140M ratings Netflix dataset.\nWe compare our algorithm to a coordinate-descent based method (Yu et al., 2012), that provides state-of-the art convergence time performance on our largest dataset. Although stochastic gradient descent methods for matrix factorization can provide slightly better single-run performance (Tak\u00e1cs et al., 2009), these are notoriously hard to tune and require a precise grid search to uncover a working schedule of learning rates. In contrast, coordinate descent methods do not require any hyper-parameter setting and are therefore more efficient in practice. We benchmarked various recommender-system codes (MyMediaLite, LibFM, SoftImpute, spira2), and chose coordinate descent algorithm from spira as it was by far the fastest.\nCompletion from dictionary Dt. We stream user ratings to our algorithm: p is the number of movies and n is the number of users. As n p on Netflix dataset, this increases the benefit of using an online method. We have observed comparable prediction performance streaming item ratings. Past the first epoch, at iteration t, every column i of X can be predicted by the last code \u03b1l(i,t) that was computed from this column at iteration l(i, t). At iteration t, for all i < [n], xpredi = D\u03b1l(i,t). Prediction thus only requires an additional matrix computation after the factorization.\nPreprocessing. Successful prediction should take into account user and item biases. We compute these biases\n2https://github.com/mblondel/spira\non train data following Hastie et al. (2014) (alternated debiasing). We use them to center the samples (xt)t that are streamed to the algorithm, and to perform final prediction.\nTools and experiments. Both baseline and proposed algorithm are implemented in a computationally optimal way, enabling fair comparison based on CPU time. Benchmarks were run using a single 2.7 GHz Xeon CPU, with a 30 components dictionary. For Movielens datasets, we use a random 25% of data for test and the rest for training. We average results on five train/test split for MovieLens in Table 1. On Netflix, the probe dataset is used for testing. Regularization parameter \u03bb is set by cross-validation on the training set: the training data is split 3 times, keeping 33% of Movielens datasets for evaluation and 1% for Netflix, and grid search is performed on 15 values of \u03bb between 10\u22122 and 10. We assess the quality of obtained decomposition by measuring the root mean square error (RMSE) between prediction on the test set and ground truth. We use mini-batches of size n100 .\nResults. We report the evolution of test RMSE along time in Fig. 4, along with its value at convergence and numerical convergence time in Table 1. Benchmarks are performed on the final run, after selection of parameter \u03bb.\nThe two variants of the proposed method converge toward a solution that is at least as good as that of coordinate descent, and slightly better on Movielens 10M and Netflix. Our algorithm brings a substantial performance improvement on medium and large scale datasets. On Netflix, con-\nvergence is almost reached in 4 minutes (score under 0.1% deviation from final RMSE), which makes our method 6.8 times faster than coordinate descent. Moreover, the relative performance of our algorithm increases with dataset size. Indeed, as datasets grow, less epochs are needed for our algorithm to reach convergence (Fig. 4). This is a significant advantage over coordinate descent, that requires a stable number of cycle on coordinates to reach convergence, regardless of dataset size. The algorithm with partial projection performs slightly better. This can be explained by the extra regularization on (Dt)t brought by this heuristic.\nLearning weights. Unlike SGD, and similar to the vanilla online dictionary learning algorithm, our method does not critically suffer from hyper-parameter tuning. We tried weights wt = 1t\u03b2 as described in Sec. 2.3, and observed that a range of \u03b2 yields fast convergence. Theoretically, Mairal (2013) shows that stochastic majorizationminimization converges when \u03b2 \u2208 (.75, 1]. We verify this empirically, and obtain optimal convergence speed for \u03b2 \u2208 [.85, 0.95]. (Fig. 5). We report results for \u03b2 = 0.9."}, {"heading": "4. Conclusion", "text": "Whether it is sensor data, as fMRI, or e-commerce databases, sample sizes and number of features are rapidly growing, rendering current matrix factorization approaches intractable. We have introduced a online algorithm that leverages random feature subsampling, giving up to 8-fold speed and memory gains on large data. Datasets are getting bigger, and they often come with more redundancies. Such approaches blending online and randomized methods will\nyield even larger speed-ups on next-generation data."}, {"heading": "Acknowledgements", "text": "The research leading to these results was supported by the ANR (MACARON project, ANR-14-CE23-0003-01 \u2013 NiConnect project, ANR-11-BINF-0004NiConnect) and has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 604102 (HBP)."}], "references": [{"title": "kSVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["Aharon", "Michal", "Elad", "Michael", "Bruckstein", "Alfred"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Aharon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2006}, {"title": "Lessons from the Netflix prize challenge", "author": ["Bell", "Robert M", "Koren", "Yehuda"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Bell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bell et al\\.", "year": 2007}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Bingham", "Ella", "Mannila", "Heikki"], "venue": "In Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Bingham et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bingham et al\\.", "year": 2001}, {"title": "Convex factorization machines", "author": ["Blondel", "Mathieu", "Fujino", "Akinori", "Ueda", "Naonori"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Blondel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2015}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["Bottou", "L\u00e9on"], "venue": "In Proceedings of COMPSTAT,", "citeRegEx": "Bottou and L\u00e9on.,? \\Q2010\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["Cand\u00e8s", "Emmanuel J", "Recht", "Benjamin"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2009}, {"title": "Near-optimal signal recovery from random projections: Universal encoding strategies", "author": ["Cand\u00e8s", "Emmanuel J", "Tao", "Terence"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "Efficient projections onto the l 1ball for learning in high dimensions", "author": ["Duchi", "John", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Halko", "Nathan", "Martinsson", "Per-Gunnar", "Tropp", "Joel A"], "venue": "[math],", "citeRegEx": "Halko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2009}, {"title": "Matrix completion and low-rank SVD via fast alternating least squares", "author": ["Hastie", "Trevor", "Mazumder", "Rahul", "Lee", "Jason", "Zadeh", "Reza"], "venue": "[stat],", "citeRegEx": "Hastie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2014}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["Johnson", "William B", "Lindenstrauss", "Joram"], "venue": "Contemporary mathematics,", "citeRegEx": "Johnson et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 1984}, {"title": "Stochastic majorization-minimization algorithms for large-scale optimization", "author": ["Mairal", "Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mairal and Julien.,? \\Q2013\\E", "shortCiteRegEx": "Mairal and Julien.", "year": 2013}, {"title": "Sparse Modeling for Image and Vision Processing", "author": ["Mairal", "Julien"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Mairal and Julien.,? \\Q2014\\E", "shortCiteRegEx": "Mairal and Julien.", "year": 2014}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["Mairal", "Julien", "Bach", "Francis", "Ponce", "Jean", "Sapiro", "Guillermo"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Analysis of fMRI Data by Blind Separation into Independent Spatial Components", "author": ["M.J. McKeown", "S. Makeig", "G.G. Brown", "T.P. Jung", "S.S. Kindermann", "A.J. Bell", "T.J. Sejnowski"], "venue": "Human Brain Mapping,", "citeRegEx": "McKeown et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McKeown et al\\.", "year": 1998}, {"title": "Scikit-learn: machine learning in Python", "author": ["Matthieu", "Duchesnay", "\u00c9douard"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Matthieu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Matthieu et al\\.", "year": 2011}, {"title": "Efficient dictionary learning via very sparse random projections", "author": ["Pourkamali-Anaraki", "Farhad", "Becker", "Stephen", "Hughes", "Shannon M"], "venue": "In Proceedings of the IEEE International Conference on Sampling Theory and Applications,", "citeRegEx": "Pourkamali.Anaraki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pourkamali.Anaraki et al\\.", "year": 2015}, {"title": "Factorization machines", "author": ["Rendle", "Steffen"], "venue": "In Proceedings of the IEEE International Conference on Data Mining,", "citeRegEx": "Rendle and Steffen.,? \\Q2010\\E", "shortCiteRegEx": "Rendle and Steffen.", "year": 2010}, {"title": "Onlineupdating regularized kernel matrix factorization models for large-scale recommender systems", "author": ["Rendle", "Steffen", "Schmidt-Thieme", "Lars"], "venue": "In Proceedings of the ACM Conference on Recommender systems,", "citeRegEx": "Rendle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2008}, {"title": "Maximum-margin matrix factorization", "author": ["Srebro", "Nathan", "Rennie", "Jason", "Jaakkola", "Tommi S"], "venue": "In Advances in Neural Information Processing Systems, pp", "citeRegEx": "Srebro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2004}, {"title": "Online group-structured dictionary learning", "author": ["Szab\u00f3", "Zolt\u00e1n", "P\u00f3czos", "Barnab\u00e1s", "Lorincz", "Andr\u00e1s"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szab\u00f3 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Szab\u00f3 et al\\.", "year": 2011}, {"title": "Scalable collaborative filtering approaches for large recommender systems", "author": ["Tak\u00e1cs", "G\u00e1bor", "Pil\u00e1szy", "Istv\u00e1n", "N\u00e9meth", "Botty\u00e1n", "Tikk", "Domonkos"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Tak\u00e1cs et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tak\u00e1cs et al\\.", "year": 2009}, {"title": "The WU-Minn Human Connectome Project: An overview", "author": ["Van Essen", "David C", "Smith", "Stephen M", "Barch", "Deanna M", "Behrens", "Timothy E. J", "Yacoub", "Essa", "Ugurbil", "Kamil"], "venue": null, "citeRegEx": "Essen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Essen et al\\.", "year": 2013}, {"title": "Cohort-level brain mapping: learning cognitive atoms to single out specialized regions", "author": ["Varoquaux", "Ga\u00ebl", "Schwartz", "Yannick", "Pinel", "Philippe", "Thirion", "Bertrand"], "venue": "In Proceedings of the Information Processing in Medical Imaging Conference,", "citeRegEx": "Varoquaux et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Varoquaux et al\\.", "year": 2013}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["Yu", "Hsiang-Fu", "Hsieh", "Cho-Jui", "Dhillon", "Inderjit"], "venue": "In Proceedings of the International Conference on Data Mining,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "in recommender systems (Srebro et al., 2004; Cand\u00e8s & Recht, 2009).", "startOffset": 23, "endOffset": 66}, {"referenceID": 19, "context": "The first one addresses an optimization problem involving a convex penalty, such as the trace or max norms (Srebro et al., 2004).", "startOffset": 107, "endOffset": 128}, {"referenceID": 13, "context": ", 2015, and references therein), and stochastic majorization-minimization methods for dictionary learning with sparse and/or structured regularization (Mairal et al., 2010; Mairal, 2013).", "startOffset": 151, "endOffset": 186}, {"referenceID": 14, "context": "Biological datasets (McKeown et al., 1998) and physical acquisitions with an underlying sparse structure enabling compressed sensing (Cand\u00e8s & Tao, 2006) are good examples.", "startOffset": 20, "endOffset": 42}, {"referenceID": 8, "context": "Recently, those have been used to compute PCA (Halko et al., 2009), a classical matrix decomposition technique.", "startOffset": 46, "endOffset": 66}, {"referenceID": 13, "context": "As such, it is non-trivial extension of the online dictionary learning method of Mairal et al. (2010), where, at every iteration, signals are partially observed with a random mask, and with low-complexity update rules that depend on the (small) mask size instead of the signal size.", "startOffset": 81, "endOffset": 102}, {"referenceID": 0, "context": "(2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al., 2006) that does not scale well in the number of training signals.", "startOffset": 79, "endOffset": 100}, {"referenceID": 14, "context": "For instance, Pourkamali-Anaraki et al. (2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al.", "startOffset": 14, "endOffset": 47}, {"referenceID": 0, "context": "(2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al., 2006) that does not scale well in the number of training signals. Online matrix decomposition in the context of missing values was also proposed by Szab\u00f3 et al. (2011), but without scalability in the signal (row) size.", "startOffset": 80, "endOffset": 263}, {"referenceID": 0, "context": "(2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al., 2006) that does not scale well in the number of training signals. Online matrix decomposition in the context of missing values was also proposed by Szab\u00f3 et al. (2011), but without scalability in the signal (row) size. On a massive fMRI dataset (2TB, n = 2.4\u00b7106, p = 2\u00b7105), we were able to learn interpretable dictionaries in about 10 hours on a single workstation, an order of magnitude faster than the online approach of Mairal et al. (2010). On collaborative filtering experiments, where sparsity is not needed, our algorithm performs favorably well compared to state-of-the-art coordinate descent methods.", "startOffset": 80, "endOffset": 541}, {"referenceID": 9, "context": "In large n and large p settings, typical in recommender systems, this problem is solved via block coordinate descent, which boils down to alternating least squares if regularizations on D and \u03b1 are quadratic (Hastie et al., 2014).", "startOffset": 208, "endOffset": 229}, {"referenceID": 13, "context": "lation, Mairal et al. (2010) have introduced an online dictionary learning approach that draws a single signal xt at iteration t (or a minibatch), and computes its sparse code \u03b1t using the current dictionary Dt\u22121 according to", "startOffset": 8, "endOffset": 29}, {"referenceID": 20, "context": "In the context of dictionary learning, a similar effort has been made by Szab\u00f3 et al. (2011) to adapt the framework to missing values.", "startOffset": 73, "endOffset": 93}, {"referenceID": 13, "context": "Adapting the online algorithm of Mairal et al. (2010) would consist of drawing a sequence of pairs (xt,Mt), and building the surrogate", "startOffset": 33, "endOffset": 54}, {"referenceID": 20, "context": "This is the approach chosen by Szab\u00f3 et al. (2011). Nevertheless, the complexity of their update rules is linear in the full signal size p, which makes them unadapted to the large-p regime that we consider.", "startOffset": 31, "endOffset": 51}, {"referenceID": 13, "context": "Following Mairal et al. (2010), we perform one cycle over the columns warm-started on Dt\u22121.", "startOffset": 10, "endOffset": 31}, {"referenceID": 7, "context": ")\u22641[fj ], and a classical result (see Duchi et al., 2008) states that there exists a scalar lj such that dj = Slj [fj ], S\u03bb(u) = sign(u).max(|u| \u2212 \u03bb, 0) (22) where S\u03bb is the soft-thresholding operator, applied elementwise to the entries of fj . Similar to the case `2, the \u201clazy\u201d projection consists of tracking the coefficient lj for each dictionary element and updating it after each gradient step, which only involves st coefficients. For such sparse updates followed by a projection onto the `1-ball, Duchi et al. (2008) proposed an algorithm to find the threshold lj in O(st log(p)) operations.", "startOffset": 38, "endOffset": 525}, {"referenceID": 19, "context": "Srebro et al. (2004) introduced the trace-norm regularization to solve a convex problem equivalent to (23).", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Matrix factorization has long been used on functional Magnetic Resonance Imaging (McKeown et al., 1998).", "startOffset": 81, "endOffset": 103}, {"referenceID": 23, "context": "Historically, such decomposition have been obtained with the classical dictionary learning objective on transposed data (Varoquaux et al., 2013): the code A holds sparse spatial maps and voxel time-series are streamed.", "startOffset": 120, "endOffset": 144}, {"referenceID": 24, "context": "We compare our algorithm to a coordinate-descent based method (Yu et al., 2012), that provides state-of-the art convergence time performance on our largest dataset.", "startOffset": 62, "endOffset": 79}, {"referenceID": 21, "context": "Although stochastic gradient descent methods for matrix factorization can provide slightly better single-run performance (Tak\u00e1cs et al., 2009), these are notoriously hard to tune and require a precise grid search to uncover a working schedule of learning rates.", "startOffset": 121, "endOffset": 142}, {"referenceID": 9, "context": "on train data following Hastie et al. (2014) (alternated debiasing).", "startOffset": 24, "endOffset": 45}], "year": 2016, "abstractText": "Sparse matrix factorization is a popular tool to obtain interpretable data decompositions, which are also effective to perform data completion or denoising. Its applicability to large datasets has been addressed with online and randomized methods, that reduce the complexity in one of the matrix dimension, but not in both of them. In this paper, we tackle very large matrices in both dimensions. We propose a new factorization method that scales gracefully to terabyte-scale datasets. Those could not be processed by previous algorithms in a reasonable amount of time. We demonstrate the efficiency of our approach on massive functional Magnetic Resonance Imaging (fMRI) data, and on matrix completion problems for recommender systems, where we obtain significant speed-ups compared to state-of-the art coordinate descent methods. Matrix factorization is a flexible tool for uncovering latent factors in low-rank or sparse models. For instance, building on low-rank structure, it has proven very powerful for matrix completion, e.g. in recommender systems (Srebro et al., 2004; Cand\u00e8s & Recht, 2009). In signal processing and computer vision, matrix factorization with a sparse regularization is often called dictionary learning and has proven very effective for denoising and visual feature encoding (see Mairal, 2014, for a review). It is also flexible enough to accommodate a large set of constraints and regularizations, and has gained significant attention in scientific domains where interpretability is a key aspect, such as geProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s). netics and neuroscience (Varoquaux et al., 2011). As a widely-used model, the literature of matrix factorization is very rich and two main classes of formulations have emerged. The first one addresses an optimization problem involving a convex penalty, such as the trace or max norms (Srebro et al., 2004). These penalties promote lowrank structures, have strong theoretical guarantees (Cand\u00e8s & Recht, 2009), but they do not encourage sparse factors and lack scalability for very-large datasets. For these reasons, our paper is focused on a second type of approach, that relies on nonconvex optimization. Specifically, the motivation of our work originally came from the need to analyze huge-scale fMRI datasets, and the difficulty of current algorithms to process them. To gain scalability, stochastic (or online) optimization methods have been developed; unlike classical alternate minimization procedures, they learn matrix decompositions by observing a single matrix column (or row) at each iteration. In other words, they stream data along one matrix dimension. Their cost per iteration is significantly reduced, leading to faster convergence in various practical contexts. More precisely, two approaches have been particularly successful: stochastic gradient descent (see Bottou, 2010) has been widely used in recommender systems (see Bell & Koren, 2007; Rendle & Schmidt-Thieme, 2008; Rendle, 2010; Blondel et al., 2015, and references therein), and stochastic majorization-minimization methods for dictionary learning with sparse and/or structured regularization (Mairal et al., 2010; Mairal, 2013). Yet, stochastic algorithms for dictionary learning are currently unable to deal efficiently with matrices that are large in both dimensions. In a somehow orthogonal way, the growth of dataset size has proven to be manageable by randomized methods, that exploit random projections (Johnson & Lindenstrauss, 1984; Bingham & Mannila, 2001) to reduce data dimension ar X iv :1 60 5. 00 93 7v 2 [ st at .M L ] 2 6 M ay 2 01 6 Dictionary Learning for Massive Matrix Factorization without deteriorating signal content. Due to the way they are generated, large-scale datasets generally have an intrinsic dimension that is significantly smaller than their ambient dimension. Biological datasets (McKeown et al., 1998) and physical acquisitions with an underlying sparse structure enabling compressed sensing (Cand\u00e8s & Tao, 2006) are good examples. In this context, matrix factorization can be performed by using random summaries of coefficients. Recently, those have been used to compute PCA (Halko et al., 2009), a classical matrix decomposition technique. Yet, using random projections as a pre-processing step is not appealing in our applicative context since the factors learned on reduced data loses interpretability. Main contribution. In this paper, we propose a dictionary learning algorithm that (i) scales both in the signal dimension (number of rows) and number of signals (number of columns), (ii) deals with various structured sparse regularization penalties, (iii) handles missing values, and (iv) provides an explicit dictionary with easy interpretation. As such, it is non-trivial extension of the online dictionary learning method of Mairal et al. (2010), where, at every iteration, signals are partially observed with a random mask, and with low-complexity update rules that depend on the (small) mask size instead of the signal size. To the best of our knowledge, our algorithm is the first that enjoys all aforementioned features; in particular, we are not aware of any other dictionary learning algorithm that is scalable in both matrix dimensions. For instance, Pourkamali-Anaraki et al. (2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al., 2006) that does not scale well in the number of training signals. Online matrix decomposition in the context of missing values was also proposed by Szab\u00f3 et al. (2011), but without scalability in the signal (row) size. On a massive fMRI dataset (2TB, n = 2.4\u00b7106, p = 2\u00b7105), we were able to learn interpretable dictionaries in about 10 hours on a single workstation, an order of magnitude faster than the online approach of Mairal et al. (2010). On collaborative filtering experiments, where sparsity is not needed, our algorithm performs favorably well compared to state-of-the-art coordinate descent methods. In both experiments, benefits for the practitioner were significant. 1. Background on Dictionary Learning In this section, we introduce dictionary learning as a matrix factorization problem, and present stochastic algorithms that observe one column (or a minibatch) at every iteration. 1.1. Problem Statement The goal of matrix factorization is to decompose a matrix X \u2208 Rp\u00d7n \u2013 typically n signals of dimension p \u2013 as a product of two smaller matrices: X \u2248 DA with D \u2208 Rp\u00d7k, A \u2208 Rk\u00d7n, (1) with potential sparsity or structure requirements on D and A. In statistical signal applications, this is often a dictionary learning problem, enforcing sparse coefficients A. In such a case, we call D the \u201cdictionary\u201d and A the sparse codes. We use this terminology throughout the paper. Learning the dictionary is typically performed by minimizing a quadratic data-fitting term, with constraints and/or penalties over the code and the dictionary: min D\u2208C A=[\u03b11,...,\u03b1n]\u2208R n \u2211 i=1 1 2 \u2225\u2225xi \u2212D\u03b1i\u2225\u222522 + \u03bb\u03a9(\u03b1i), (2) where C is a convex set of Rp\u00d7k, and a \u03a9 : R \u2192 R is a penalty over the code, to enforce structure or sparsity. In large n and large p settings, typical in recommender systems, this problem is solved via block coordinate descent, which boils down to alternating least squares if regularizations on D and \u03b1 are quadratic (Hastie et al., 2014). Constraints and penalties. The constraint set C is traditionally a technical constraint ensuring that the coefficients \u03b1 do not vanish, making the effect of the penalty \u03a9 disappear. However, other constraints can also be used to enforce sparsity or structure on the dictionary (see Varoquaux et al., 2013). In our paper,C is the Cartesian product of a `1 or `2 norm ball: C = {D \u2208 Rp\u00d7k s.t. \u03c8(dj) \u2264 1 \u2200j = 1, . . . , k}, (3) where D = [d1, . . . ,dk] and \u03c8 = \u2016 \u00b7 \u20161 or \u03c8 = \u2016 \u00b7 \u20162. The choice of \u03c8 and \u03a9 typically offers some flexibility in the regularization effect that is desired for a specific problem; for instance, classical dictionary learning uses \u03c8 = \u2016 \u00b7 \u20162 and \u03a9 = \u2016\u00b7\u20161, leading to sparse coefficients \u03b1, whereas our experiments on fMRI uses \u03c8 = \u2016\u00b7\u20161 and \u03a9 = \u2016\u00b7\u20162, leading to sparse dictionary elements dj that can be interpreted as brain activation maps. 1.2. Streaming Signals with Online Algorithms In stochastic optimization, the number of signals n is assumed to be large (or potentially infinite), and the dictionary D can be written as a solution of min D\u2208C f(D) where f(D) = Ex [ l(x,D) ] (4) l(x,D) = min \u03b1\u2208Rk 1 2 \u2016x\u2212D\u03b1\u20162 + \u03bb\u03a9(\u03b1), where the signals x are assumed to be i.i.d. samples from an unknown probability distribution. Based on this formuDictionary Learning for Massive Matrix Factorization lation, Mairal et al. (2010) have introduced an online dictionary learning approach that draws a single signal xt at iteration t (or a minibatch), and computes its sparse code \u03b1t using the current dictionary Dt\u22121 according to \u03b1t \u2190 argmin \u03b1\u2208Rk 1 2 \u2016xt \u2212Dt\u22121\u03b1\u20162 + \u03bb\u03a9(\u03b1). (5) Then, the dictionary is updated by approximately minimizing the following surrogate function", "creator": "LaTeX with hyperref package"}}}