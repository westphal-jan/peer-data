{"id": "1605.03148", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Coverage Embedding Models for Neural Machine Translation", "abstract": "gumushane In this wita paper, we enhance 13.71 the attention - based neural disincentive machine gabara translation by xwa adding quaver an petit-breton explicit coverage embedding pallmeyer model to alleviate issues of thoth-amon repeating 8-track and dropping mustache translations laminated in samlee NMT. hajee For toothfish each source word, our model drais starts honorat with fettis a darlinghurst full coverage embedding kilrush vector, conform and then keeps anglican updating azzara it alveolar with 37.76 a hennon gated recurrent replicator unit unseaworthy as rokitno the translation franko goes. 4id All billionaires the initialized 602,410 coverage 70-69 embeddings and updating metrical matrix worlwide are learned in the training declaims procedure. aeri Experiments on pci-x the uhe large - lipschutz scale Chinese - to - bondra English task show that our enhanced nongshim model improves greater the translation lechery quality qf6 significantly on various sundar test sets albyn over broadsided the ghiyath strong gogrial large torrente vocabulary mariological NMT griffeys system.", "histories": [["v1", "Tue, 10 May 2016 18:44:34 GMT  (913kb,D)", "http://arxiv.org/abs/1605.03148v1", "6 pages"], ["v2", "Mon, 29 Aug 2016 15:10:34 GMT  (1106kb,D)", "http://arxiv.org/abs/1605.03148v2", "6 pages; In Proceddings of EMNLP 2016"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haitao mi", "baskaran sankaran", "zhiguo wang", "abe ittycheriah"], "accepted": true, "id": "1605.03148"}, "pdf": {"name": "1605.03148.pdf", "metadata": {"source": "CRF", "title": "A Coverage Embedding Model for Neural Machine Translation", "authors": ["Haitao Mi Baskaran", "Sankaran Zhiguo Wang", "Abe Ittycheriah"], "emails": ["abei}@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word. However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage information typically for each source word. As a result, this kind of model suffers from issues of repeating or dropping translations.\nThe traditional statistical machine translation (SMT) systems (e.g. (Koehn, 2004; Chiang, 2005)) address the above issues by employing a source side \u201ccoverage vector\u201d to indicate explicitly which words have been translated, which parts have not yet. A coverage vector starts with all zero, which means no word has been translated. If a source word at position j got translated, the coverage vector remembers position j as 1, and they won\u2019t use this source word in future translation. This mechanism avoids the repeating or dropping translation problems.\nHowever, it is not easy to adapt the \u201ccoverage vector\u201d to NMT directly, as the attentions are soft probabilities, not 0 or 1. Furthermore, traditional SMT approaches handle one-to-many fertility problem by using phrases or hiero rules (can predict several words in a row). But NMT systems can only predict one word at each step. In order to alleviate all those problems, in this paper, we borrow the basic idea of \u201ccoverage vector\u201d, and introduce a coverage embedding vector for each source word. Starting from a full coverage embedding for a source word, if this word is translated, the embedding vector should come to empty. We model this procedure by using a gated recurrent unit (GRU) (Cho et al., 2014). When we start the translation, we initialize each source word at different position with its own coverage embedding, then, at each translation step, we feed the coverage embedding at each source position to the first layer of the attention model, and compute the attention probabilities. After we get attentions at each position, we update our coverage embedding model by feeding attentions back (Section 3). Large-scale experiments over Chinese-to-English on various test sets show that our method improves the translation quality significantly over the large vocabulary NMT system (Section 5)."}, {"heading": "2 Neural Machine Translation", "text": "As shown in Figure 1, attention-based neural machine translation (Bahdanau et al., 2014) is an encoder-decoder network. the encoder employs a bidirectional recurrent neural network to encode the source sentence x = (x1, ..., xl), where l is the sentence length, into a sequence of hidden states h = (h1, ..., hl), each hi is a concatenation of a leftto-right \u2212\u2192 hi and a right-to-left \u2190\u2212 hi ,\nhi = [\u2190\u2212 h i\u2212\u2192 h i ] = [\u2190\u2212 f (xi, \u2190\u2212 h i+1)\u2212\u2192 f (xi, \u2212\u2192 h i\u22121) ] ,\nar X\niv :1\n60 5.\n03 14\n8v 1\n[ cs\n.C L\n] 1\n0 M\nay 2\n01 6\nwhere \u2190\u2212 f and \u2212\u2192 f are two gated recurrent units (GRU). Given the encoded h, the decoder predicts the target translation by maximizing the conditional log-probability of the correct translation y\u2217 = (y\u22171, ...y \u2217 m), where m is the sentence length. At each time t, the probability of each word yt from a target vocabulary Vy is:\np(yt|h, y\u2217t\u22121..y\u22171) = g(st, y\u2217t\u22121), (1)\nwhere g is a two layer feed-forward neural network (ot is a intermediate state) over the embedding of the previous word y\u2217t\u22121, and the hidden state st. The st is computed as:\nst = q(st\u22121, y \u2217 t\u22121, Ht) (2)\nHt =\n[\u2211l i=1 (\u03b1ti \u00b7 \u2190\u2212 h i)\u2211l\ni=1 (\u03b1ti \u00b7 \u2212\u2192 h i)\n] , (3)\nwhere q is a GRU, Ht is a weighted sum of h, the weights, \u03b1, are computed with a two layer feedforward neural network r:\n\u03b1ti = exp{r(st\u22121, hi, y\u2217t\u22121)}\u2211l k=1 exp{r(st\u22121, hk, y\u2217t\u22121)}\n(4)"}, {"heading": "3 A Coverage Embedding Model", "text": "In traditional statistical machine translation (e.g. (Koehn, 2004; Chiang, 2005)), they employ a source side \u201ccoverage vector\u201d to indicate explicitly which word has been translated. A coverage vector starts with all zero, which means no word has been translated. If a source word at position j got translated, the coverage vector remembers position j as 1, and we won\u2019t use this source word in future translation. This mechanism avoids the repeating or dropping translation problems.\nHowever, in attention-based NMT systems, there is no explicit \u201ccoverage vector\u201d, as attentions in each step are probabilities. Furthermore, the intermediate state Atj only takes into account of encoding state hj , the previous target word yt\u22121, and the previous hidden state st\u22121, there is no information from the history for this particular position j. Thus, it is possible that an attention at position j will be repeated in different steps, or some positions never get any attention.\nOur idea is to introduce a coverage embedding for each source word, and keep updating this embedding with a GRU at each time step.\nFigure 2 shows the coverage embedding model. As different words have different fertilities (one-toone, one-to-many, or one-to-zero), similar to word embeddings, we also initialize a special coverage embedding for each source word. For simplicity, the source vocabulary size of coverage embeddings is the same as the word embedding vocabulary size. At the beginning of translation, we initialize c0,1, c0,2, ...c0,l by looking up the coverage embedding matrix. Then, at time step t, we feed ct\u22121,j to the attention model (shown in red dotted line in Figure 1). After we get the attention \u03b1t,j , we feed yt and \u03b1t,j to the coverage model (shown in Figure 2),\nzt,j = \u03c3(W zyyt +W z\u03b1\u03b1t,j + U zct\u22121,j) rt,j = \u03c3(W ryyt +W r\u03b1\u03b1t,j + U rct\u22121,j) c\u0303t,j = tanh(Wyt +W \u03b1\u03b1t,j + rt,j \u25e6 Uct\u22121,j) ct,j = zt,j \u25e6 ct\u22121,j + (1\u2212 zt,j) \u25e6 c\u0303t,j ,\nwhere, zt is the update gate, rt is the reset gate, c\u0303t is the new memory content, and ct is the final memory. The matrix W zy, W z\u03b1, U z , W ry, W r\u03b1, U r, W y, W\u03b1 and U are shared across different position j. \u25e6 is a pointwise operation.\nThe reason why we only pick yt and \u03b1t,j as input is that coverage embedding ct,j is highly related to the translation word yt and the attention probability \u03b1t,j at this step t and source position j. Other states, like st, contain too much information, it may not be clear how much influence will be imposed to ct,j . So if yt is close to the embedding ct\u22121,j , this means yt and ct,j are equivalent translations, thus, ct,j will be close to empty. Similarly, if \u03b1t,j is very high, it also results in almost empty of ct,j . Hopefully, if yt is partial translation of ct\u22121,j , it only remove partial information of ct\u22121,j . The same to the \u03b1t,j . In this way, we enable coverage embeddings c to encode fertility information for each source word x.\nIn order to check whether \u03b1 is important for coverage embedding c, we run two models, one is with \u03b1 in the input (\u03b1 \u2192 c), the other one is without \u03b1 (shown in the red dotted line in Figure 2)."}, {"heading": "4 Related Work", "text": "There are several parallel related work (Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). Tu et al. (2016) is the most relevant one, where they also employ a GRU to model the coverage vector. One main difference is that our model initializes each source word from a specific coverage embedding matrix, in contrast, their work initializes the word coverage\nvector with a uniform distribution. Another difference lays in the fertility part, Tu et al. (2016) add an accumulate operation and a fertility function to simulate the process of one-to-many alignments. In our approach, we add fertility information directly to coverage embeddings, as each source word has its own embedding. Furthermore, we only feed the previous word, attention to coverage embedding layer, which is much simpler than theirs. The last difference is that we run experiments on 5 million and 11 million sentence pairs, which are significant larger than theirs. Finally, our baseline system is similar to the large vocabulary NMT of Jean et al. (2015) with candidate list decoding and UNK replacement, a much stronger baseline system.\nFeng et al. (2016) propose some methods to add more connections between different hidden states in a baseline model. feed the previous attention context directly to attention model. By contrast, we have an explicit model for coverage embedding. And they\nrun experiments only on 0.5 million sentence pairs, which are not large enough to train a good NMT system.\nCohn et al. (2016) augment the attention model with well-known features in traditional SMT, including positional bias, Markov conditioning, fertility and agreement over translation directions. This work is orthogonal to our work. And they only run experiments on smaller training sets and on reranking task only."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Data Preparation", "text": "We run our experiments on Chinese to English task. We train our machine translation systems on two training sets. The first training corpus consists of approximately 5 million sentences available within the DARPA BOLT Chinese-English task. The corpus includes a mix of newswire, broadcast news, webblog and comes from various sources. which do not include HK Law, HK Hansard and UN data. The second training corpus includes HK Law, HK Hansard and UN data, the total number of training sentence pairs is 11 million. The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF).\nOur development set is the concatenation of several tuning sets (GALE Dev, P1R6 Dev, and Dev 12) initially released under the DARPA GALE program. The development set is 4491 sentences in total. Our test sets are NIST MT06 (1664 sentences), MT08 news (691 sentences), and MT08 web (666 sentences).\nFor all NMT systems, the full vocabulary sizes for thr two training sets are 300k and 500k respectively. The coverage embedding vector size is 100. In the training procedure, we use AdaDelta (Zeiler, 2012) to update model parameters with a mini-batch size 80. Following Mi et al. (2016), the output vocabulary for each mini-batch or sentence is a sub-set of the full vocabulary. For each source sentence, the sentence-level target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the\ntranslation reachable. Following Jean et al. (2015), We dump the alignments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word.\nOur traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tree-to-string rules on the training set. Our language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07) and Google News). We tune our system with PRO (Hopkins and May, 2011) to minimize (TER- BLEU)/2 on the development set."}, {"heading": "5.2 Results", "text": "Table 1 shows the final results of all systems. The traditional hybrid syntax-based system achieves 9.45, 12.90, and 17.72 on MT06, MT08 News, and MT08 Web sets respectively, 13.36 on average in terms of (TER- BLEU)/2. The associated BLEU scores are 34.93, 31.12, and 23.45 respectively.\nThe large-vocabulary NMT (LVNMT), our baseline, achieves an average (TER- BLEU)/2 score at 15.74, which is about 2 points worse than the hybrid system. But, interestingly, LVNMT performs slightly better on MT08 Web test set. This suggests that NMT has the potential ability of handling informal text better than traditional SMT systems, as NMT represents the source sentence as dense vectors instead of surface strings.\nWith the help of the coverage embedding model, without feeding the attentions back, we improve the scores by 1.1 points in average over LVNMT. If we feed attentions into our coverage embedding model, we further boost our performance by almost 0.4 points on average. Our best NMT system is only 1 point worse than the strong tree-to-string system, and we can get significantly better results on informal text set, like MT08 Web.\nTable 2 shows the results of all the systems trained on 11 million sentence pairs, LVNMT achieves an average (TER-BLEU)/2 at 13.27, which is about 2.5 points better than 5 million LVNMT. The result\nof our coverage model with \u03b1 \u2192 c gives almost 1 point gain over LVNMT. Those results suggest that the more training data we use, the stronger the baseline system becomes, and the harder to get improvements. In order to get a reasonable or strong NMT system, we have to conduct experiments over a large-scale training set.\nIn order to verify that our coverage embedding model improves the attention accuracy, we conduct alignment experiments over the LVNMT and our coverage model (with \u03b1 \u2192 c). Table 3 shows the F1 scores on the alignment test set (447 hand aligned sentences). The MaxEnt model is trained on 67k hand-aligned data, and achieves an F1 score at 75.96. For NMT systems, we dump the alignment matrixes and covert them into alignments with following steps. For each target word, we sort the alphas1, and add the max probability link if it is higher than 0.2. Results show that coverage embeddings improve the F1 score by 1.6 points over\n1The alignment probabilities to each source word and the symbol of end-of-sentence.\nthe LVNMT. But NMT scores are still far behind the MaxEnt model."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a simple, yet effective, coverage embedding model for attention-based NMT. Our model learns a special coverage embedding vector for each source word to start with. We keep updating those coverage embeddings as the translation goes. Experiments on the large-scale Chinese-to-English task show significant improvements over the strong LVNMT system."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "ArXiv e-prints, September.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of ACL.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "CoRR, abs/1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Flexible and efficient hypergraph interactions for joint hierarchical and forest-to-string decoding", "author": ["Martin Cmejrek", "Haitao Mi", "Bowen Zhou."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in", "citeRegEx": "Cmejrek et al\\.,? 2013", "shortCiteRegEx": "Cmejrek et al\\.", "year": 2013}, {"title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "author": ["T. Cohn", "C.D.V. Hoang", "E. Vymolova", "K. Yao", "C. Dyer", "G. Haffari."], "venue": "ArXiv e-prints, January.", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model", "author": ["S. Feng", "S. Liu", "M. Li", "M. Zhou."], "venue": "ArXiv e-prints, January.", "citeRegEx": "Feng et al\\.,? 2016", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Tuning as ranking", "author": ["Mark Hopkins", "Jonathan May."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Hopkins and May.,? 2011", "shortCiteRegEx": "Hopkins and May.", "year": 2011}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL, pages 1\u201310, Beijing, China, July.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Pharaoh: a beam search decoder for phrase-based statistical machine translation models", "author": ["Philipp Koehn."], "venue": "Proceedings of AMTA, pages 115\u2013124.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Joint decoding with multiple translation models", "author": ["Yang Liu", "Haitao Mi", "Yang Feng", "Qun Liu."], "venue": "In", "citeRegEx": "Liu et al\\.,? 2009", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421, Lisbon, Portu-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Vocabulary manipulation for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Proceedings of ACL, Berlin, Germany, August.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Coveragebased Neural Machine Translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li."], "venue": "ArXiv e-prints, January.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Generalizing local and non-local word-reordering patterns for syntaxbased machine translation", "author": ["Bing Zhao", "Yaser Al-onaizan."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201908, pages 572\u2013581, Strouds-", "citeRegEx": "Zhao and Al.onaizan.,? 2008", "shortCiteRegEx": "Zhao and Al.onaizan.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 8, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 11, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 9, "context": "(Koehn, 2004; Chiang, 2005)) address the above issues by employing a source side \u201ccoverage vector\u201d to indicate explicitly which words have been translated, which parts have not yet.", "startOffset": 0, "endOffset": 27}, {"referenceID": 1, "context": "(Koehn, 2004; Chiang, 2005)) address the above issues by employing a source side \u201ccoverage vector\u201d to indicate explicitly which words have been translated, which parts have not yet.", "startOffset": 0, "endOffset": 27}, {"referenceID": 2, "context": "We model this procedure by using a gated recurrent unit (GRU) (Cho et al., 2014).", "startOffset": 62, "endOffset": 80}, {"referenceID": 0, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word.", "startOffset": 76, "endOffset": 207}, {"referenceID": 0, "context": "As shown in Figure 1, attention-based neural machine translation (Bahdanau et al., 2014) is an encoder-decoder network.", "startOffset": 65, "endOffset": 88}, {"referenceID": 9, "context": "(Koehn, 2004; Chiang, 2005)), they employ a source side \u201ccoverage vector\u201d to indicate explicitly which word has been translated.", "startOffset": 0, "endOffset": 27}, {"referenceID": 1, "context": "(Koehn, 2004; Chiang, 2005)), they employ a source side \u201ccoverage vector\u201d to indicate explicitly which word has been translated.", "startOffset": 0, "endOffset": 27}, {"referenceID": 0, "context": "Figure 1: The architecture of attention-based neural machine translation (Bahdanau et al., 2014).", "startOffset": 73, "endOffset": 96}, {"referenceID": 13, "context": "There are several parallel related work (Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016).", "startOffset": 40, "endOffset": 95}, {"referenceID": 6, "context": "There are several parallel related work (Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016).", "startOffset": 40, "endOffset": 95}, {"referenceID": 4, "context": "There are several parallel related work (Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016).", "startOffset": 40, "endOffset": 95}, {"referenceID": 4, "context": ", 2016; Cohn et al., 2016). Tu et al. (2016) is the most relevant one, where they also employ a GRU to model the coverage vector.", "startOffset": 8, "endOffset": 45}, {"referenceID": 4, "context": ", 2016; Cohn et al., 2016). Tu et al. (2016) is the most relevant one, where they also employ a GRU to model the coverage vector. One main difference is that our model initializes each source word from a specific coverage embedding matrix, in contrast, their work initializes the word coverage vector with a uniform distribution. Another difference lays in the fertility part, Tu et al. (2016) add an accumulate operation and a fertility function to simulate the process of one-to-many alignments.", "startOffset": 8, "endOffset": 394}, {"referenceID": 4, "context": ", 2016; Cohn et al., 2016). Tu et al. (2016) is the most relevant one, where they also employ a GRU to model the coverage vector. One main difference is that our model initializes each source word from a specific coverage embedding matrix, in contrast, their work initializes the word coverage vector with a uniform distribution. Another difference lays in the fertility part, Tu et al. (2016) add an accumulate operation and a fertility function to simulate the process of one-to-many alignments. In our approach, we add fertility information directly to coverage embeddings, as each source word has its own embedding. Furthermore, we only feed the previous word, attention to coverage embedding layer, which is much simpler than theirs. The last difference is that we run experiments on 5 million and 11 million sentence pairs, which are significant larger than theirs. Finally, our baseline system is similar to the large vocabulary NMT of Jean et al. (2015) with candidate list decoding and UNK replacement, a much stronger baseline system.", "startOffset": 8, "endOffset": 962}, {"referenceID": 14, "context": "In the training procedure, we use AdaDelta (Zeiler, 2012) to update model parameters with a mini-batch size 80.", "startOffset": 43, "endOffset": 57}, {"referenceID": 5, "context": "For each source sentence, the sentence-level target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013).", "startOffset": 210, "endOffset": 229}, {"referenceID": 11, "context": "Following Mi et al. (2016), the output vocabulary for each mini-batch or sentence is a sub-set of the full vocabulary.", "startOffset": 10, "endOffset": 27}, {"referenceID": 8, "context": "Following Jean et al. (2015), We dump the alignments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word.", "startOffset": 10, "endOffset": 29}, {"referenceID": 15, "context": "Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al.", "startOffset": 72, "endOffset": 99}, {"referenceID": 10, "context": "Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013).", "startOffset": 144, "endOffset": 184}, {"referenceID": 3, "context": "Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013).", "startOffset": 144, "endOffset": 184}, {"referenceID": 7, "context": "We tune our system with PRO (Hopkins and May, 2011) to minimize (TER- BLEU)/2 on the development set.", "startOffset": 28, "endOffset": 51}], "year": 2017, "abstractText": "In this paper, we enhance the attention-based neural machine translation by adding an explicit coverage embedding model to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector, and then keeps updating it with a gated recurrent unit as the translation goes. All the initialized coverage embeddings and updating matrix are learned in the training procedure. Experiments on the large-scale Chineseto-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.", "creator": "TeX"}}}