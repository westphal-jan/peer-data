{"id": "1605.03852", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning", "abstract": "2.16 We use runic Bayesian manalang optimization oomycetes to learn curricula tait\u014d for cleaved word representation left-over learning, optimizing performance on downstream paleis tasks 20.6 that 250-strong depend pre-spanish on mesmer the nyako learned moleskin representations as features. The ginta curricula are wilkey modeled rajoelina by seku a babos linear killion ranking function which 444-8874 is intersected the scalar marietta product of a learned enormously weight 22,300 vector 3,541 and an complet engineered feature 3:2 vector that characterizes spin-stabilized the different aspects vidiprinter of used the yergin complexity of brujo each 1.5950 instance komedia in romalis the training euro22 corpus. We show that learning the curriculum improves performance on reinfried a variety of downstream tasks over brener random orders 107.71 and chaoshan in =( comparison armholes to the a19 natural sempione corpus stiff order.", "histories": [["v1", "Thu, 12 May 2016 15:15:58 GMT  (121kb,D)", "http://arxiv.org/abs/1605.03852v1", "ACL 2016 submission, 10 pages"], ["v2", "Tue, 21 Jun 2016 18:35:29 GMT  (124kb,D)", "http://arxiv.org/abs/1605.03852v2", "In proceedings of ACL 2016, 10 pages"]], "COMMENTS": "ACL 2016 submission, 10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yulia tsvetkov", "manaal faruqui", "wang ling", "brian macwhinney", "chris dyer"], "accepted": true, "id": "1605.03852"}, "pdf": {"name": "1605.03852.pdf", "metadata": {"source": "CRF", "title": "Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning", "authors": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Chris Dyer"], "emails": ["ytsvetko@cs.cmu.edu", "mfaruqui@cs.cmu.edu", "lingwang@cs.cmu.edu", "cdyer@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "It is well established that in language acquisition, there are robust patterns in the order by which phenomena are acquired. For example, prototypical concepts are acquired earlier; concrete words tend to be learned before abstract ones (Rosch, 1978). The acquisition of lexical knowledge in artificial systems proceeds differently. In general, models will improve during the course of parameter learning, but the time course of acquisition is not generally studied beyond generalization error as a function of training time or data size. We revisit this issue of choosing the order of learning\u2014curriculum learning\u2014framing it as an optimization problem so that a rich array of factors\u2014including nuanced measures of difficulty, as well as prototypicality and diversity\u2014can be exploited.\nPrior research focusing on curriculum strategies in NLP is scarce, and has conventionally been following a paradigm of \u201cstarting small\u201d (Elman, 1993), i.e., initializing the learner with \u201csimple\u201d examples first, and then gradually increasing data\ncomplexity (Bengio et al., 2009; Spitkovsky et al., 2010). In language modeling, this preference for increasing complexity has been realized by curricula that increase the entropy of training data by growing the size of the training vocabulary from frequent to less frequent words (Bengio et al., 2009). In unsupervised grammar induction, an effective curriculum comes from increasing length of training sentences as training progresses (Spitkovsky et al., 2010). These case studies have demonstrated that carefully designed curricula can lead to better results. However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990; Skinner, 1938). Had different heuristics been chosen, the results would have been different. In this paper, we use curriculum learning to create improved word representations. However, rather than testing a small number of curricula, we search for an optimal curriculum using Bayesian optimization. A curriculum is defined to be the ordering of the training instances, in our case it is the ordering of paragraphs in which the representation learning model reads the corpus. We use a linear ranking function to conduct a systematic exploration of interacting factors that affect curricula of representation learning models. We then analyze our findings, and compare them to human intuitions and learning principles.\nWe treat curriculum learning as an outer loop in the process of learning and evaluation of vectorspace representations of words; the iterative procedure is (1) predict a curriculum; (2) train word embeddings; (3) evaluate the embeddings on tasks that use word embeddings as the sole features. Through this model we analyze the impact of curriculum on word representation models and on extrinsic tasks. To quantify curriculum properties, we define three groups of features aimed at analyzing statistical and linguistic content and structure\nar X\niv :1\n60 5.\n03 85\n2v 1\n[ cs\n.C L\n] 1\n2 M\nay 2\n01 6\nof training data: (1) diversity, (2) simplicity, and (3) prototypicality. A function of these features is computed to score each paragraph in the training data, and the curriculum is determined by sorting corpus paragraphs by the paragraph scores. We detail the model in \u00a72. Word vectors are learned from the sorted corpus, and then evaluated on partof-speech tagging, parsing, named entity recognition, and sentiment analysis (\u00a73). Our experiments confirm that training data curriculum affects model performance, and that models with optimized curriculum consistently outperform baselines trained on shuffled corpora (\u00a74). We analyze our findings in \u00a75.\nThe contributions of this work are twofold. First, this is the first framework that formulates curriculum learning as an optimization problem, rather then shuffling data or relying on human intuitions. We experiment with optimizing the curriculum of word embeddings, but in principle the curriculum of other models can be optimized in a similar way. Second, to the best of our knowledge, this study is the first to analyze the impact of distributional and linguistic properties of training texts on the quality of task-specific word embeddings."}, {"heading": "2 Curriculum Learning Model", "text": "We are considering the problem of maximizing a performance of an NLP task through sequentially optimizing the curriculum of training data of word vector representations that are used as features in the task.\nLet X = {x1, x2, . . . , xn} be the training corpus with n lines (sentences or paragraphs). The curriculum of word representations is quantified by scoring each of the paragraphs according to the following linear function:\nw\u1d40 \u03c6(X )\u2212 \u00b5\n\u03c3 , (1)\nwhere \u03c6(X ) \u2208 R`\u00d7n is a real-valued matrix containing ` linguistic features extracted for each paragraph, and w \u2208 R`\u00d71 denote the weights learned for these features. Each paragraph is scored as the z-normalized dot-product between the features and weights. These scores are used to specify the order of the paragraphs in the corpus\u2014 the curriculum. We sort the paragraphs by their scores.\nAfter the paragraphs are curriculum-ordered, the reordered training corpus is used to generate\nword representations. These word representations are then used in a subsequent NLP task. We define the objective function eval : X \u2192 R, which is the quality estimation metric for this task performed on a held-out dataset (e.g., correlation, accuracy, F1 score, BLEU). Our goal is to define the features \u03c6(X ) and to find the optimal weights w that maximize eval.\nWe optimize the feature weights using Bayesian optimization; we detail the model in \u00a72.1. Distributional and linguistic features inspired by prior research in language acquisition and second language learning are described in \u00a72.2. Figure 1 shows the computation flow diagram."}, {"heading": "2.1 Bayesian Optimization for Curriculum Learning", "text": "As no assumptions are made regarding the form of eval(w), gradient-based methods cannot be applied, and performing a grid search over parameterizations of w would require a exponentially growing number of parameterizations to be traversed. Thus, we propose to use Bayesian Optimization (BayesOpt) as the means to maximize eval(w). BayesOpt is a methodology to globally optimize expensive, multimodal black-box functions (Shahriari et al., 2016; Bergstra et al., 2011; Snoek et al., 2012). It can be viewed as a sequential approach to performing a regression from high-level model parameters (e.g., learning rate, number of layers in a neural network, and in our model\u2013curriculum weights w) to the loss function or the performance measure (eval).\nAn arbitrary objective function, eval, is treated as a black-box, and BayesOpt uses Bayesian inference to characterize a posterior distribution over functions that approximate eval. This model of eval is called the surrogate model. Then, the\nBayesOpt exploits this model to make decisions about eval, e.g., where is the expected maximum of the function, and what is the expected improvement that can be obtained over the best iteration so far. The strategy function, estimating the next set of parameters to explore given the current beliefs about eval is called the acquisition function. The surrogate model and the acquisition function are the two key components in the BayesOpt framework; their interaction is shown in Algorithm 1.\nPopular choices for the surrogate model are Gaussian Processes (Rasmussen, 2006; Snoek et al., 2012, GP), providing convenient and powerful prior distribution on functions, and tree-structured Parzen estimators (Bergstra et al., 2011, TPE), tailored to handle conditional spaces. We use the latter since our features are not mutually independent (e.g., #types and #types#tokens are not independent).\nThe surrogate model allows us to cheaply approximate the quality of a set of parameters w without running eval(w), and the acquisition function uses this surrogate to choose a new value of w. However, a trade-off must be made: should the acquisition function move w into a region where the surrogate believes an optimal value will be found, or should it explore regions of the space that reveal more about how eval behaves, perhaps discovering even better values? That is, acquisition functions balance a tradeoff between exploration\u2014by selecting w in the regions where the uncertainty of the surrogate model is high, and exploitation\u2014by querying the regions where the model prediction is high. Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Moc\u030ckus et al., 1978; Jones, 2001), GP upper confidence bound (Srinivas et al., 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al. (2016) for an extensive comparison. Yogatama et al. (2015) found that the combination of EI as the acquisition function and TPE as the surrogate model performed favorably in Bayesian optimization of text representations; we follow this choice in our model."}, {"heading": "2.2 Distributional and Linguistic Features", "text": "To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects\nAlgorithm 1 Bayesian optimization 1: H \u2190 \u2205 . Initialize observation history 2: A \u2190 EI . Initialize acquisition function 3: S0 \u2190 TPE . Initialize surrogate model 4: for t\u2190 1 to T do 5: wt \u2190 argmaxwA(w;St\u22121,H) . Predict\nwt by optimizing acquisition function 6: eval(wt) . Evaluate wt on extrinsic task 7: H \u2190 H\u222a(wt, eval(wt)) . Update obser-\nvation history 8: Estimate St givenH 9: end for\n10: returnH\nof training data. We now detail the feature categories along with motivations for feature selection.\nDIVERSITY. Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity are used in many contrasting fields, from ecology and biology (Rosenzweig, 1995; Magurran, 2013), to economics and social studies (Stirling, 2007). Diversity has been shown effective in related research on curriculum learning in language modeling, vision, and multimedia analysis (Bengio et al., 2009; Jiang et al., 2014).\nLet pi and pj correspond to probabilities of word types ti and tj in the training data, and dij is their semantic (cosine) similarity in vector space. We annotate each paragraph with the following diversity features:\n\u2022 Number of word types: #types \u2022 Type-token ratio: #types#tokens \u2022 Entropy: \u2212 \u2211 i piln(pi)\n\u2022 Simpson\u2019s index (Simpson, 1949): \u2211\ni pi 2 \u2022 Quadratic entropy (Rao, 1982):1 \u2211\ni,j dijpipj\nSIMPLICITY. Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity\n1Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words.\nmeasures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Meurers, 2012). We use the the following features to measure phonological, lexical, and syntactic complexity of training paragraphs:\n\u2022 Language model score \u2022 Character language model score \u2022 Average sentence length \u2022 Verb-token ratio \u2022 Noun-token ratio \u2022 Parse tree depth \u2022 Number of noun phrases: #NPs \u2022 Number of verb phrases: #V Bs \u2022 Number of prepositional phrases: #PPs\nPROTOTYPICALITY. This is a group of semantic features that use insights from cognitive linguistics and child language acquisition. The goal is to characterize the curriculum of representation learning in terms of the curriculum of human language learning. We resort to the Prototype theory (Rosch, 1978), which posits that semantic categories include more central (or prototypical) as well as less prototypical words. For example, in the ANIMAL category, dog is more prototypical than sloth (because dog is more frequent); dog is more prototypical than canine (because dog is more concrete); and dog is more prototypical than bull terrier (because dog is less specific). According to the theory, more prototypical words are acquired earlier. We use lexical semantic databases to operationalize insights from the prototype theory in the following semantic features; the features are computed on token level and averaged over paragraphs:\n\u2022 Age of acquisition (AoA) of words was extracted from the crowd-sourced database, containing over 50 thousand English words (Kuperman et al., 2012). For example, the AoA of run is 4.47 (years), of flee is 8.33, and of abscond is 13.36. If a word was not found in the database it was assigned the maximal age of 25. \u2022 Concreteness ratings on the scale of 1\u20135 (1 is\nmost abstract) for 40 thousand English lemmas (Brysbaert et al., 2014). For example, cookie is rated as 5, and spirituality as 1.07.\n\u2022 Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2-regularized logistic regression classifier. \u2022 Conventionalization features count the num-\nber of \u201cconventional\u201d words and phrases in a paragraph. Assuming that a Wikipedia title is a proxy to a conventionalized concept, we counted the number of existing titles (from a database of over 4.5 million titles) in the paragraph. \u2022 Number of syllables scores are also extracted\nfrom the AoA database; out-of-database words were annotated as 5-syllable words. \u2022 Relative frequency in a supersense was com-\nputed by marginalizing the word frequencies in the training corpus over coarse semantic categories defined in the WordNet (Fellbaum, 1998; Ciaramita and Altun, 2006). There are 41 supersense types: 26 for nouns and 15 for verbs, e.g., NOUN.ANIMAL and VERB.MOTION. For example, in NOUN.ANIMAL the relative frequency of human is 0.06, of dog is 0.01, of bird is 0.01, of cattle is 0.009, and of bumblebee is 0.0002. \u2022 Relative frequency in a synset was calculated\nsimilarly to the previous feature category, but word frequencies were marginalized over WordNet synsets (more fine-grained synonym sets). For example, in the synset {vet, warhorse, veteran, oldtimer, seasoned stager}, veteran is the most prototypical word, scoring 0.87."}, {"heading": "3 Evaluation Benchmarks", "text": "We evaluate the utility of the pretrained word embeddings as features in downstream NLP tasks. We choose the following off-the-shelf models that utilize pretrained word embeddings as features:\nSentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use the average of the word vectors of a given sentence as a feature vector for classification (Faruqui et al., 2015; Sedoc\net al., 2016). The `2-regularized logistic regression classifier is tuned on the dev set and accuracy is reported on the test set.\nNamed Entity Recognition (NER). Named entity recognition is the task of identifying proper names in a sentence, such as names of persons, locations etc. We use the recently proposed LSTMCRF NER model (Lample et al., 2016) which trains a forward-backward LSTM on a given sequence of words (represented as word vectors), the hidden units of which are then used as (the only) features in a CRF model (Lafferty et al., 2001) to predict the output label sequence. We use the CoNLL 2003 English NER dataset (Tjong Kim Sang and De Meulder, 2003) to train our models and present results on the test set.\nPart of Speech Tagging (POS). For POS tagging, we again use the LSTM-CRF model (Lample et al., 2016), but instead of predicting the named entity tag for every word in a sentence, we train the tagger to predict the POS tag of the word. The tagger is trained and evaluated with the standard Penn TreeBank (PTB) (Marcus et al., 1993) training, development and test set splits as described in Collins (2002).\nDependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for English on the universal dependencies v1.1 treebank (Agic\u0301 et al., 2015) with the standard dev and test splits, reporting unlabeled attachment scores (UAS) on the test data. We remove all part-of-speech and morphology features from the data, and prevent the model from optimizing the word embeddings used to represent each word in the corpus, thereby forcing the parser to rely completely on the pretrained embeddings."}, {"heading": "4 Experiments", "text": "Data. All models were trained on Wikipedia articles, split to paragraph-per-line. Texts were cleaned, tokenized, numbers were normalized by replacing each digit with \u201cDG\u201d, all types that occur less than 10 times were replaces by the \u201cUNK\u201d token, the data was not lowercased. We list data sizes in table 1.\nSetup. Word embeddings were trained using the cbow model implemented in the word2vec toolkit (Mikolov et al., 2013).2 All training data was used, either shuffled or ordered by a curriculum. As described in \u00a73, we modified the extrinsic tasks to learn solely from word embeddings, without additional features. All models were learned under same conditions, across curricula: in Parse, NER, and POS we limited the number of training iterations to 3, 3, and 1, respectively. This setup allowed us to evaluate the effect of curriculum without additional interacting factors.\nExperiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (\u00a73). We tune the tasks on dev data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus\u2014the curriculum. We compare the following experimental setups:\n\u2022 Shuffled baselines: the curriculum is defined by random shuffling the training data. We shuffled the data 10 times, and trained 10 word embeddings models, each model was then evaluated on downstream tasks. Following Bengio et al. (2009), we report test results for the system that is closest to the median in dev scores. To evaluate variability and a range of scores that can be obtained from shuffling the data, we also report test results for systems that obtained the highest dev scores. \u2022 Sorted baselines: the curriculum is defined\nby sorting the training data by sentence length in increasing/decreasing order, similarly to (Spitkovsky et al., 2010). \u2022 Coherent baselines: the curriculum is defined\nby just concatenating Wikipedia articles. The goal of this experiment is to evaluate the importance of semantic coherence in training data. Our intuition is that a coherent curriculum can improve models, since words with simi2To evaluate the impact of curriculum learning, we enforced sequential processing of data organized in a predefined order of training examples. To control for sequential processing, word embedding were learned by running the cbow using a single thread for one iteration.\nlar meanings and similar contexts are grouped when presented to the learner.\n\u2022 Optimized curriculum models: the curriculum is optimized using the BayesOpt. We evaluate and compare models optimized using features from one of the three feature groups (\u00a72.2). As in the shuffled baselines, we fix the number of trials (here, BayesOpt iterations) to 10, and we report test results of systems that obtained best dev scores.\nResults. Experimental results are listed in table 2. Most systems trained with curriculum substantially outperform the strongest of all baselines. These results are encouraging, given that all word embedding models were trained on the same set of examples, only in different order, and display the indirect influence of the data curriculum on downstream tasks. These results support our assumption that curriculum matters. Albeit not as pronounced as with optimized curriculum, sorting paragraphs by length can also lead to substantial improvements over random baselines, but there is no clear recipe on whether the models prefer curricula sorted in an increasing or decreasing order. These results also support the advantage of a taskspecific optimization framework over a general, intuition-guided recipe. An interesting result, also, that shuffling is not essential: systems trained on coherent data are on par (or better) than the shuffled systems.3 In the next section, we analyze these results qualitatively."}, {"heading": "5 Analysis", "text": "What are task-specific curriculum preferences? We manually inspect learned features and curriculum-sorted corpora, and find that best systems are obtained when their embeddings are learned from curricula appropriate to the downstream tasks. We discuss below several examples.\n3Note that in the shuffled NER baselines, best dev results yield lower performance on the test data. This implies that the in standard dev/test splits the dev and test sets are not fully compatible or not large enough. We also observe this problem in the curriculum-optimized Parse-prototypicality and Sentidiversity systems. The dev scores for the Parse systems are 76.99, 76.47, 76.47 for diversity, prototypicality, and simplicity, respectively, but the prototypicality-sorted parser performs poorly on test data. Similarly in the sentiment analysis task, the dev scores are 69.15, 69.04, 69.49 for diversity, prototypicality, and simplicity feature groups. Senti-diversity scores, however, are lower on the test data, although the dev results are better than in Senti-simplicity. This limitation of the standard dev/test splits is beyond the scope of this paper.\nPOS and Parse systems converge to the same set of weights, when trained on features that provide various measures of syntactic simplicity. The features with highest coefficients (and thus the most important features in sorting) are #NPs, Parse tree depth, #V Ps, and #PPs (in this order). The sign in the #NPs feature weight, however, is the opposite from the other three feature weights (i.e., sorted in different order). #NPs is sorted in the increasing order of the number of noun phrases in a paragraph, and the other features are sorted in the decreasing order. Since Wikipedia corpus contains a lot of partial phrases (titles and headings), such curriculum promotes more complex, full sentences, and demotes partial sentences.\nBest Senti system is sorted by prototypicality features. Most important features (with the highest coefficients) are Concreteness, Relative frequency in a supersense, and the Number of syllables. First two are sorted in decreasing order (i.e. paragraphs are sorted from more to less concrete, and from more to less prototypical words), and the Number of syllables is sorted in increasing order (this also promotes simpler, shorter words which are more prototypical). We hypothesize that this soring reflects the type of data that Sentiment analysis task is trained on: it is trained on movie reviews, that are usually written in a simple, colloquial language.\nUnlike POS, Parse, and Senti systems, all NER systems prefer curricula in which texts are sorted from short to long paragraphs. The most important features in the best (simplicity-sorted) system are #PPs and Verb-token ratio, both sorted from less to more occurrences of prepositional and verb phrases. Interestingly, most of the top lines in the NER system curricula contain named entities, although none of our features marks named entities explicitly. We show top lines in the simplicityoptimized system in figure 2.\nFinally, in all systems sorted by prototypicality, the last line is indeed not a prototypical word Donaudampfschiffahrtselektrizit\u00e4tenhauptbetriebswerkbauunterbeamtengesellschaft, which is an actual word in German, frequently used as an example of morphological complexity of agglutinative languages, but rarely (or never?) used by German speakers.\nWeighting examples according to curriculum. Another way to integrate curriculum in word embedding training is to weight training examples\naccording to curriculum during word representation training. We modify the cbow objective\u2211T\nt=1 log p(wt|wt\u2212c..wt+c) as follows:4\nT\u2211 t=1 ( 1 1 + e\u2212weight(wt) + \u03bb) log p(wt|wt\u2212c..wt+c)\nHere, weight(wt) denotes the score attributed to the token wt, which is the z-normalized score of the paragraph they appear in (equation 1); \u03bb=0.5 is determined empirically. log p(wt)|wt\u2212c..wt+c) computes the probability of predicting word wt, using the context of c words to the left and right of wt. Notice that this quantity is no longer a proper probability, as we are not normalizing over\n4The modified word2vec tool is publicly released, but anonymized here.\nthe weights weight(wt) over all tokens. However, the optimization in word2vec is performed using stochastic gradient descent, optimizing for a single token at each iteration. This yields a normalizer of 1 for each iteration, yielding the same gradient as the original cbow model.\nWe retrain our best curriculum-sorted systems with the modified objective, also controlling for curriculum. The results are shown in table 3. We find that the benefit of integrating curriculum in training objective of word representations is not evident across tasks: Senti and NER systems trained on vectors with the modified objective substantially outperform best results in table 2; POS and Parse perform better than the baselines but worse than the systems with the original objective.\nAre we learning task-specific curricula? One way to assess whether we learn meaningful taskspecific curriculum preferences is to compare curricula learned by one downstream task across different feature groups. If learned curricula are similar in, say, NER system, despite being optimized once using diversity features and once using prototypicality features\u2014two disjoint feature sets\u2014we can infer that the NER task prefers word embeddings learned from examples presented in a certain order, regardless of specific optimization features. For each downstream task, we thus measure Spearman\u2019s rank correlation between the curricula optimized using diversity (D), or prototypicality (P), or simplicity (S) feature sets. Prior to measuring correlations, we remove duplicate lines from\nthe training corpora. Correlation results across tasks and across feature sets are shown in table 4.\nThe general pattern of results is that if two systems score higher than baselines, training sentences of their feature embeddings have similar curricula (i.e., the Spearman\u2019s \u03c1 is positive), and if two systems disagree (one is above and one is below the baseline), then their curricula also disagree (i.e., the Spearman\u2019s \u03c1 is negative or close to zero). NER systems all outperform the baselines and their curricula have high correlations. Moreover, NER sorted by diversity and simplicity have better scores than NER sorted by prototypicality, and in line with these results \u03c1(S,D)NER > \u03c1(P,S)NER and \u03c1(S,D)NER > \u03c1(D,P)NER. Similar pattern of results is in POS correlations. In Parse systems, also, diversity and simplicity features yielded best parsing results, and \u03c1(S,D)Parse has high positive correlation. The prototypicality-optimized parser performed poorly, and its correlations with better systems are negative. The best parser was trained using the diversity-optimized curriculum, and thus \u03c1(D,P)Parse is the lowest. Senti results follow similar pattern of curricula correlations.\nCurriculum learning vs. data selection. We compare the task of curriculum learning to the task of data selection (reducing the set of training instances to more important or cleaner examples). We reduce the training data to the subset of 10% of tokens, and train downstream tasks on the reduced training sets. We compare system performance trained using the top 10% of tokens in the best curriculum-sorted systems (Senti-prototypicality, NER-implicity, POS-simplicity, Parse-diversity) to the systems trained using the top 10% of tokens in a corpus with randomly shuffled paragraphs.5 The results are listed in table 5. The curriculum-based systems are better in POS\n5Top n% tokens are used rather than top n% paragraphs because in all tasks except NER curriculum-sorted corpora begin with longer paragraphs. Thus, with top n% paragraphs our systems would have an advantage over random systems due to larger vocabulary sizes and not necessarily due to a better subset of data.\nand in Parse systems, mainly because these tasks prefer vectors trained on curricula that promote well-formed sentences (as discussed above). Conversely, NER prefers vectors trained on corpora that begin with named entities, so most of the tokens in the reduced training data are constituents in short noun phrases. These results suggest that the tasks of data selection and curriculum learning are different. Curriculum is about strong initialization of the models and time-course learning, which is not necessarily sufficient for data reduction."}, {"heading": "6 Related Work", "text": "Two prior studies on curriculum learning in NLP are discussed in the paper (Bengio et al., 2009; Spitkovsky et al., 2010). Curriculum learning and related research on self-paced learning has been explored more deeply in computer vision (Bengio et al., 2009; Kumar et al., 2010; Lee and Grauman, 2011) and in multimedia analysis (Jiang et al., 2015). Bayesian optimization has also received little attention in NLP. GPs were used in the task of machine translation quality estimation (Cohn and Specia, 2013) and in temporal analysis of social media texts (Preotiuc-Pietro and Cohn, 2013); TPEs were used by Yogatama et al. (2015) for optimizing choices of feature representations\u2014ngram size, regularization choice, etc.\u2014in supervised classifiers."}, {"heading": "7 Conclusion", "text": "We used Bayesian optimization to optimize curricula for training dense distributed word representations, which, in turn, were used as the sole features in NLP tasks. Our experiments confirmed that better curricula yield stronger models. We also conducted an extensive analysis, which sheds better light on understanding of text properties that are beneficial for model initialization. The proposed novel technique for finding an optimal curriculum is general, and can be used with other datasets and models."}], "references": [{"title": "Universal dependencies 1.1. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague", "author": ["Vincze", "Daniel Zeman"], "venue": null, "citeRegEx": "Vincze and Zeman.,? \\Q2015\\E", "shortCiteRegEx": "Vincze and Zeman.", "year": 2015}, {"title": "Algorithms for hyper-parameter optimization", "author": ["R\u00e9mi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl"], "venue": "In Proc. NIPS,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Concreteness ratings for 40 thousand generally known english word lemmas", "author": ["Amy Beth Warriner", "Victor Kuperman"], "venue": "Behavior research methods,", "citeRegEx": "Brysbaert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brysbaert et al\\.", "year": 2014}, {"title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger", "author": ["Ciaramita", "Yasemin Altun"], "venue": "In Proc. EMNLP,", "citeRegEx": "Ciaramita et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2006}, {"title": "Modelling annotator bias with multitask Gaussian processes: An application to machine translation quality estimation", "author": ["Cohn", "Specia2013] Trevor Cohn", "Lucia Specia"], "venue": "In Proc. ACL,", "citeRegEx": "Cohn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2013}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proc. EMNLP,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["Jeffrey L Elman"], "venue": null, "citeRegEx": "Elman.,? \\Q1993\\E", "shortCiteRegEx": "Elman.", "year": 1993}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "In Proc. NAACL", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "A systematic exploration of diversity in machine translation", "author": ["Gimpel et al.2013] Kevin Gimpel", "Dhruv Batra", "Chris Dyer", "Gregory Shakhnarovich"], "venue": "In Proc. EMNLP,", "citeRegEx": "Gimpel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "Combining lexical and grammatical features to improve readability measures for first and second language texts", "author": ["Kevyn Collins-Thompson", "Jamie Callan", "Maxine Eskenazi"], "venue": "In Proc. NAACL,", "citeRegEx": "Heilman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2007}, {"title": "Entropy search for information-efficient global optimization", "author": ["Hennig", "Schuler2012] Philipp Hennig", "Christian J Schuler"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Hennig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hennig et al\\.", "year": 2012}, {"title": "Portfolio allocation for Bayesian optimization", "author": ["Eric Brochu", "Nando de Freitas"], "venue": "In Proc. UAI,", "citeRegEx": "Hoffman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2011}, {"title": "Self-paced learning with diversity", "author": ["Jiang et al.2014] Lu Jiang", "Deyu Meng", "Shoou-I Yu", "Zhenzhong Lan", "Shiguang Shan", "Alexander Hauptmann"], "venue": "In Proc. NIPS,", "citeRegEx": "Jiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2014}, {"title": "Self-paced curriculum learning", "author": ["Jiang et al.2015] Lu Jiang", "Deyu Meng", "Qian Zhao", "Shiguang Shan", "Alexander G Hauptmann"], "venue": "In Proc. AAAI,", "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "A taxonomy of global optimization methods based on response surfaces", "author": ["Donald R Jones"], "venue": "Journal of global optimization,", "citeRegEx": "Jones.,? \\Q2001\\E", "shortCiteRegEx": "Jones.", "year": 2001}, {"title": "The development of memory in children", "author": ["Robert Kail"], "venue": "W. H. Freeman and Company,", "citeRegEx": "Kail.,? \\Q1990\\E", "shortCiteRegEx": "Kail.", "year": 1990}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "Benjamin Packer", "Daphne Koller"], "venue": "In Proc. NIPS,", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Age-of-acquisition ratings for 30,000 english words", "author": ["Hans Stadthagen-Gonzalez", "Marc Brysbaert"], "venue": "Behavior Research Methods,", "citeRegEx": "Kuperman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuperman et al\\.", "year": 2012}, {"title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise", "author": ["Harold J Kushner"], "venue": "Journal of Basic Engineering,", "citeRegEx": "Kushner.,? \\Q1964\\E", "shortCiteRegEx": "Kushner.", "year": 1964}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "In Proc. NAACL", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Learning the easy things first: Self-paced visual category discovery", "author": ["Lee", "Grauman2011] Yong Jae Lee", "Kristen Grauman"], "venue": "In Proc. CVPR,", "citeRegEx": "Lee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Measuring biological diversity", "author": ["Anne E Magurran"], "venue": null, "citeRegEx": "Magurran.,? \\Q2013\\E", "shortCiteRegEx": "Magurran.", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proc. ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On Bayesian methods for seeking the extremum", "author": ["Mo\u010dkus et al.1978] Jonas Mo\u010dkus", "Vytautas Tiesis", "Antanas \u017dilinskas"], "venue": "Towards global optimization,", "citeRegEx": "Mo\u010dkus et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Mo\u010dkus et al\\.", "year": 1978}, {"title": "Revisiting readability: A unified framework for predicting text quality", "author": ["Pitler", "Nenkova2008] Emily Pitler", "Ani Nenkova"], "venue": "In Proc. EMNLP,", "citeRegEx": "Pitler et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pitler et al\\.", "year": 2008}, {"title": "A temporal model of text periodicities using gaussian processes", "author": ["Preotiuc-Pietro", "Cohn2013] Daniel Preotiuc-Pietro", "Trevor Cohn"], "venue": "In Proc. EMNLP,", "citeRegEx": "Preotiuc.Pietro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Preotiuc.Pietro et al\\.", "year": 2013}, {"title": "Diversity and dissimilarity coefficients: a unified approach. Theoretical population biology, 21(1):24\u201343", "author": ["C Radhakrishna Rao"], "venue": null, "citeRegEx": "Rao.,? \\Q1982\\E", "shortCiteRegEx": "Rao.", "year": 1982}, {"title": "Gaussian Processes for machine learning", "author": ["Carl Edward Rasmussen"], "venue": null, "citeRegEx": "Rasmussen.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen.", "year": 2006}, {"title": "Principles of categorization", "author": ["Eleanor Rosch"], "venue": "Cognition and categorization,", "citeRegEx": "Rosch.,? \\Q1978\\E", "shortCiteRegEx": "Rosch.", "year": 1978}, {"title": "Species diversity in space and time", "author": ["Michael L Rosenzweig"], "venue": null, "citeRegEx": "Rosenzweig.,? \\Q1995\\E", "shortCiteRegEx": "Rosenzweig.", "year": 1995}, {"title": "Reading level assessment using support vector machines and statistical language models", "author": ["Schwarm", "Ostendorf2005] Sarah E. Schwarm", "Mari Ostendorf"], "venue": "In Proc. ACL,", "citeRegEx": "Schwarm et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schwarm et al\\.", "year": 2005}, {"title": "Semantic word clusters using signed normalized graph cuts. arXiv preprint arXiv:1601.05403", "author": ["Sedoc et al.2016] Jo\u00e3o Sedoc", "Jean Gallier", "Lyle Ungar", "Dean Foster"], "venue": null, "citeRegEx": "Sedoc et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sedoc et al\\.", "year": 2016}, {"title": "Taking the human out of the loop: A review of Bayesian optimization", "author": ["Kevin Swersky", "Ziyu Wang", "Ryan P Adams", "Nando de Freitas"], "venue": "Proc. IEEE,", "citeRegEx": "Shahriari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shahriari et al\\.", "year": 2016}, {"title": "The behavior of organisms: an experimental analysis", "author": ["Burrhus Frederic Skinner"], "venue": "An Experimental Analysis", "citeRegEx": "Skinner.,? \\Q1938\\E", "shortCiteRegEx": "Skinner.", "year": 1938}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Snoek et al.2012] Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": "In Proc. NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proc. EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "From baby steps to leapfrog: How less is more in unsupervised dependency parsing", "author": ["Hiyan Alshawi", "Dan Jurafsky"], "venue": "In Proc. NAACL,", "citeRegEx": "Spitkovsky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["Andreas Krause", "Sham M Kakade", "Matthias Seeger"], "venue": "In Proc. ICML,", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "A general framework for analysing diversity in science, technology and society", "author": ["Andy Stirling"], "venue": "Journal of the Royal Society Interface,", "citeRegEx": "Stirling.,? \\Q2007\\E", "shortCiteRegEx": "Stirling.", "year": 2007}, {"title": "An analysis of diversity measures", "author": ["Tang et al.2006] E Ke Tang", "Ponnuthurai N Suganthan", "Xin Yao"], "venue": "Machine Learning,", "citeRegEx": "Tang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2006}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proc. CoNLL", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Metaphor detection with cross-lingual model transfer", "author": ["Leonid Boytsov", "Anatole Gershman", "Eric Nyberg", "Chris Dyer"], "venue": "In Proc. ACL", "citeRegEx": "Tsvetkov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2014}, {"title": "On improving the accuracy of readability classification using insights from second language acquisition", "author": ["Vajjala", "Meurers2012] Sowmya Vajjala", "Detmar Meurers"], "venue": "In Proc. BEA,", "citeRegEx": "Vajjala et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vajjala et al\\.", "year": 2012}, {"title": "MRC psycholinguistic database: Machine-usable", "author": ["Michael Wilson"], "venue": null, "citeRegEx": "Wilson.,? \\Q1988\\E", "shortCiteRegEx": "Wilson.", "year": 1988}, {"title": "Bayesian optimization of text representations", "author": ["Lingpeng Kong", "Noah A Smith"], "venue": "In Proc. EMNLP,", "citeRegEx": "Yogatama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 31, "context": "tend to be learned before abstract ones (Rosch, 1978).", "startOffset": 40, "endOffset": 53}, {"referenceID": 7, "context": "Prior research focusing on curriculum strategies in NLP is scarce, and has conventionally been following a paradigm of \u201cstarting small\u201d (Elman, 1993), i.", "startOffset": 136, "endOffset": 149}, {"referenceID": 39, "context": ", initializing the learner with \u201csimple\u201d examples first, and then gradually increasing data complexity (Bengio et al., 2009; Spitkovsky et al., 2010).", "startOffset": 103, "endOffset": 149}, {"referenceID": 39, "context": "In unsupervised grammar induction, an effective curriculum comes from increasing length of training sentences as training progresses (Spitkovsky et al., 2010).", "startOffset": 133, "endOffset": 158}, {"referenceID": 16, "context": "However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990; Skinner, 1938).", "startOffset": 124, "endOffset": 151}, {"referenceID": 36, "context": "However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990; Skinner, 1938).", "startOffset": 124, "endOffset": 151}, {"referenceID": 35, "context": "tions (Shahriari et al., 2016; Bergstra et al., 2011; Snoek et al., 2012).", "startOffset": 6, "endOffset": 73}, {"referenceID": 1, "context": "tions (Shahriari et al., 2016; Bergstra et al., 2011; Snoek et al., 2012).", "startOffset": 6, "endOffset": 73}, {"referenceID": 37, "context": "tions (Shahriari et al., 2016; Bergstra et al., 2011; Snoek et al., 2012).", "startOffset": 6, "endOffset": 73}, {"referenceID": 19, "context": "Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Mo\u010dkus et al.", "startOffset": 72, "endOffset": 87}, {"referenceID": 26, "context": "Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Mo\u010dkus et al., 1978; Jones, 2001), GP upper confidence bound (Srinivas et al.", "startOffset": 115, "endOffset": 149}, {"referenceID": 15, "context": "Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Mo\u010dkus et al., 1978; Jones, 2001), GP upper confidence bound (Srinivas et al.", "startOffset": 115, "endOffset": 149}, {"referenceID": 40, "context": ", 1978; Jones, 2001), GP upper confidence bound (Srinivas et al., 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 43, "context": ", 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al.", "startOffset": 27, "endOffset": 43}, {"referenceID": 12, "context": ", 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al.", "startOffset": 136, "endOffset": 158}, {"referenceID": 12, "context": ", 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al. (2016) for an extensive comparison.", "startOffset": 137, "endOffset": 188}, {"referenceID": 12, "context": ", 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al. (2016) for an extensive comparison. Yogatama et al. (2015) found that the combination of EI as the acquisition function and TPE as the surrogate model performed favorably in Bayesian optimization of text representations; we follow this choice in our model.", "startOffset": 137, "endOffset": 240}, {"referenceID": 42, "context": "known measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013).", "startOffset": 78, "endOffset": 118}, {"referenceID": 9, "context": "known measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013).", "startOffset": 78, "endOffset": 118}, {"referenceID": 41, "context": "2013), to economics and social studies (Stirling, 2007).", "startOffset": 39, "endOffset": 55}, {"referenceID": 13, "context": "Diversity has been shown effective in related research on curriculum learning in language modeling, vision, and multimedia analysis (Bengio et al., 2009; Jiang et al., 2014).", "startOffset": 132, "endOffset": 173}, {"referenceID": 29, "context": "\u2022 Entropy: \u2212 \u2211 i piln(pi) \u2022 Simpson\u2019s index (Simpson, 1949): \u2211 i pi 2 \u2022 Quadratic entropy (Rao, 1982):1 \u2211 i,j dijpipj", "startOffset": 90, "endOffset": 101}, {"referenceID": 39, "context": "Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings.", "startOffset": 0, "endOffset": 25}, {"referenceID": 10, "context": "spired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Meurers, 2012).", "startOffset": 105, "endOffset": 209}, {"referenceID": 31, "context": "We resort to the Prototype theory (Rosch, 1978), which posits that semantic cat-", "startOffset": 34, "endOffset": 47}, {"referenceID": 18, "context": "\u2022 Age of acquisition (AoA) of words was extracted from the crowd-sourced database, containing over 50 thousand English words (Kuperman et al., 2012).", "startOffset": 125, "endOffset": 148}, {"referenceID": 2, "context": "\u2022 Concreteness ratings on the scale of 1\u20135 (1 is most abstract) for 40 thousand English lemmas (Brysbaert et al., 2014).", "startOffset": 95, "endOffset": 119}, {"referenceID": 47, "context": "\u2022 Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988).", "startOffset": 72, "endOffset": 86}, {"referenceID": 45, "context": "lowing Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2-regularized logistic regression classifier.", "startOffset": 7, "endOffset": 30}, {"referenceID": 37, "context": "Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts.", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "We use the recently proposed LSTMCRF NER model (Lample et al., 2016) which trains a forward-backward LSTM on a given sequence of words (represented as word vectors), the hidden units of which are then used as (the only) features in a CRF model (Lafferty et al.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": ", 2016) which trains a forward-backward LSTM on a given sequence of words (represented as word vectors), the hidden units of which are then used as (the only) features in a CRF model (Lafferty et al., 2001) to predict the output label sequence.", "startOffset": 183, "endOffset": 206}, {"referenceID": 5, "context": "1993) training, development and test set splits as described in Collins (2002).", "startOffset": 64, "endOffset": 79}, {"referenceID": 6, "context": "For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for English on the universal dependencies v1.", "startOffset": 58, "endOffset": 77}, {"referenceID": 25, "context": "Word embeddings were trained using the cbow model implemented in the word2vec toolkit (Mikolov et al., 2013).", "startOffset": 86, "endOffset": 108}, {"referenceID": 39, "context": "\u2022 Sorted baselines: the curriculum is defined by sorting the training data by sentence length in increasing/decreasing order, similarly to (Spitkovsky et al., 2010).", "startOffset": 139, "endOffset": 164}, {"referenceID": 39, "context": "Two prior studies on curriculum learning in NLP are discussed in the paper (Bengio et al., 2009; Spitkovsky et al., 2010).", "startOffset": 75, "endOffset": 121}, {"referenceID": 17, "context": "related research on self-paced learning has been explored more deeply in computer vision (Bengio et al., 2009; Kumar et al., 2010; Lee and Grauman, 2011) and in multimedia analysis (Jiang et al.", "startOffset": 89, "endOffset": 153}, {"referenceID": 14, "context": ", 2010; Lee and Grauman, 2011) and in multimedia analysis (Jiang et al., 2015).", "startOffset": 58, "endOffset": 78}, {"referenceID": 48, "context": "GPs were used in the task of machine translation quality estimation (Cohn and Specia, 2013) and in temporal analysis of social media texts (Preotiuc-Pietro and Cohn, 2013); TPEs were used by Yogatama et al. (2015) for", "startOffset": 191, "endOffset": 214}], "year": 2017, "abstractText": "We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned representations as features. The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order.", "creator": "LaTeX with hyperref package"}}}