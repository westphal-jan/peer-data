{"id": "1605.04475", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2016", "title": "Capturing divergence in dependency trees to improve syntactic projection", "abstract": "euro232 Obtaining tunein syntactic maschwitz parses universul is rashied a crucial 6:1 part of many camillus NLP pipelines. However, most shahri of divulged the sack world ' s j.b. languages kelabit do sunless not dracaena have pistou large kaltenbrunner amounts of syntactically snowshoes annotated sulfa corpora bosbyshell available for 10:28 building parsers. Syntactic frago projection outrebounding techniques varone attempt 2.12 to address this fukushima issue piccuta by using parallel swanson corpora gescard consisting cornelis of timequake resource - freundschaft poor and heping resource - rich language superbad pairs, taking advantage of a parser kurtley for micra the resource - rich language rockfish and word verdine alignment \u0161\u00e1rka between breadbasket the 56.91 languages to project eraniel the parses gunge onto the wolfville data for the resource - poor greenwillow language. ivy These projection ballyhoo methods can suffer, cs5 however, alcester when d'ulisse the colluding two 1271 languages are 30.05 divergent. In temperton this melloni paper, htein we arpels investigate bajirao the wuchiu possibility caver of lowest-scoring using annet small, kuptsov parallel, annotated corpora to adelaider automatically detect friends divergent structural eastweek patterns kowroski between two aiba languages. powered These schmear patterns amex can ryer then be sueter used to passera improve hgp structural librettist projection algorithms, allowing for mainlander better performing NLP tools noach for overindulged resource - saybolt poor languages, in berra particular ltc those sni that may 2,921 not conroe have nephites large poignance amounts jamsetji of annotated d31 data necessary for traditional, fully - supervised urth methods. While this hemoglobinuria detection cardinal-bishop process is 194-page not exhaustive, we 51.04 demonstrate that helf common patterns of divergence can be robotti identified automatically corroborating without prior 1949-53 knowledge of hipple a tomasa given stefan\u00f3w language mictlan pair, and the ncw patterns can baritone be z$ used s\u00fcdwestrundfunk to kovas improve jameer performance of tigernach projection algorithms.", "histories": [["v1", "Sat, 14 May 2016 22:11:07 GMT  (864kb,D)", "http://arxiv.org/abs/1605.04475v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ryan georgi", "fei xia", "william d lewis"], "accepted": false, "id": "1605.04475"}, "pdf": {"name": "1605.04475.pdf", "metadata": {"source": "CRF", "title": "Measuring Divergence in Dependency Trees to Improve Projection Algorithms", "authors": ["Ryan Georgi", "Fei Xia", "William D. Lewis"], "emails": ["rgeorgi@uw.edu", "fxia@uw.edu", "wilewis@microsoft.com"], "sections": [{"heading": null, "text": "Keywords Multilingualism \u00b7 Translation Divergence \u00b7 Syntactic Projection\nR. Georgi F. Xia University of Washington Department of Linguistics Box 354340 Seattle, WA 98195-4340 Tel.: +1 (206) 543-2046 Fax: +1 (206) 685-7978 E-mail: rgeorgi@uw.edu E-mail: fxia@uw.edu\nW. D. Lewis Microsoft Research, Bldg 99 14820 NE 36th St, Redmond, WA 98052-6399 E-mail: wilewis@microsoft.com\nar X\niv :1\n60 5.\n04 47\n5v 1\n[ cs\n.C L\n] 1\n4 M\nay 2\n01 6"}, {"heading": "1 Introduction", "text": "When it comes to resources for natural language processing, a small handful of languages account for the vast majority of available resources. Out of the resources listed by the LRE Map (Calzolari et al., 2012), English accounts for 30% of all recorded resources, and the ten most resourced languages for 62% of all resources. A broad variety of tools are available for these resourcerich languages, as the time and effort spent to annotate resources for these languages allows for state-of-the-art systems to be built utilizing supervised and semi-supervised methods.\nThe availability of such resources is the result of a large investment over many years on a per-language-basis. Because creating high-quality annotation is expensive and labor intensive, the vast majority of the world\u2019s languages lack such resources and high-performance NLP tools. To address this issue, recent studies (Lewis and Xia, 2008; Benajiba and Zitouni, 2010; Georgi et al., 2012) have proposed to take advantage of bitext and resources for resource-rich languages; that is, use tools for resource-rich languages to process one side of bitext (the resource-rich language) and project the information to the other side of bitext (the resource-poor language) via word alignments.\nWhile projecting annotation from one language to another is a promising method for adding annotation to languages using automated methods, it relies on the assumption that simple word alignments between languages are sufficient to represent analogous meanings and structures between the languages. For reasons we will discuss in the following sections, this assumption is useful, but often erroneous.\nFinding out whether and when this assumption fails for a given language pair is not easy without knowledge about the two languages. It would be useful if, given a small set of seed data, a language pair could be analyzed to find where and in what ways the languages diverge, and use these detected patterns as corrective guidelines for performing projection upon other sentences of the language pair.\nIn this paper, we propose a method for analyzing a language pair and determining the degree and types of divergence between two languages. This systematic identification of divergence types could then lead to better informed syntactic projections, and subsequently can improve the tools built upon such data."}, {"heading": "2 Background", "text": "While there is a growing body of work on projection methods as a means to bootstrap resources from one language to another, there are not many studies on how to treat the issue of linguistic divergence between these languages. In this section, we provide a brief review of work on divergence and projection algorithms. We will also introduce interlinear glossed text (IGT), a common format for linguists to represent language examples.\n2.1 Projection Methods\nProjection algorithms have been the target of a fair amount of research in the last decade as attempts have been made to utilize statistical alignment methods to match words between languages with parallel data and \u201cproject\u201d annotation between the two. Figure 1 shows an example bitext in the form of an interlinear glossed text (IGT), while Figure 2 shows how this bitext may be used to project a dependency tree from the English to the Hindi.\nSome of the initial research on the subject of projecting word-level annotation from one language to another was published in Yarowsky and Ngai (2001). Here, the authors used IBM Model 3 (Brown et al., 1990) to align large parallel corpora between English\u2013Chinese and English\u2013French. A POS tagger was trained for French using projection from English, and NP bracketers were trained similarly for both French and Chinese. The authors identified noisy statistical alignment and 1-to-many alignments as two main issues in performing projection. The first of these issues is indeed a difficult problem for resource-poor languages, as high-quality statistical word alignment often requires much more bitext than the data available for the language. While it is not a full solution to the problem, many of the language pairs we use in this work are drawn from collection of interlinear glossed text (IGT), as shown in Figure 1, which provides unique shortcuts for obtaining word alignment with a small amount of data. IGT will be discussed further in \u00a72.2.\nThe second issue of 1-to-many alignments is one that may be the result of linguistic divergence between a language pair where the source is morphologically richer than the target. In cases such as this, finding common patterns of conflation can be useful for generalizing a projection to new data. For instance, Fig. 3 shows a very simple but common case of conflation in the SMULTRON corpus (Volk et al., 2010) where a single German word aligns to multiple English words. Using direct projection alone, the same POS tag would be projected to both English tokens. In this case, using a universal tagset such as those presented by Petrov et al. (2012) could help alleviate the problem, but for more complex cases, learning the pattern would be more critical.\nHwa et al. (2004) investigate the issues in using projection between languages to develop and train syntactic parsers, and outline the Direct Correspondence Assumption (DCA), the assumption made in projection algorithms that the target language tree should be homomorphic to the source language tree. While useful, this assumption often does not hold, as the authors point out. In order to fix some of the errors made by faulty projections, Hwa et al. use an approach that applies post-projection correction rules. For projection from English to Spanish, the accuracy of the projected structures increased from 36.8% to 70.3%. The accuracy of the English to Chinese projection increased from 38.1% and 67.3%.\nWhile these language-specific rewrite rules are promising, they still require language-specific knowledge. What we seek to accomplish in this paper is a general framework for automatically detecting this divergence, both in specific language pairs, and its frequency throughout a large number of languages. With the use of this automated divergence detection, it may be possible to learn these rewrite rules from a small annotated corpus and use them to improve projection algorithms.\n2.2 Interlinear Glossed Text\nAs mentioned in the preceding section, much of the data for our experiments was drawn from the unique IGT data type. IGT instances are a common way for linguists to give illustrative examples for a language being studied. Figure 1 shows an instance of IGT for Hindi and English. Of special interest in IGT instances is the middle gloss line which gives a word-by-word gloss of the source language. This annotated gloss typically contains words from the English translation, albeit reordered and with extra morphological and semantic information. The matching of gloss and translation can be utilized to obtain high-quality automatic word alignment between source and target, without the need for far larger amounts of data typically required by statistical alignment algorithms.\nIn Xia and Lewis (2007), enriched IGT data for 7 language pairs was created using this augmented alignment and structural projection, then handcorrected to create gold standards with minimal expert intervention. They showed the potential for using IGT as a resource for languages for which finding resources would otherwise be extremely difficult or impossible to obtain. We will use this data for the current work. A breakdown of the language pairs can be seen in \u00a74.1.\nLewis and Xia (2008) used projected phrase structures to determine the basic word order for 97 languages using a database of IGT instances. By using the alignment method described above and projecting phrase structures from English to the language line, the word order in the foreign language could be inferred. For languages with just 10\u201339 IGT instances, the accuracy of predicting basic word order was 79%; with more than 40 instances, the accuracy jumped to 99%.\n2.3 Linguistic Divergence\nThese studies illustrate the promise of projection for bootstrapping new tools for resource-poor languages, but one limitation is their reliance on the assumption that syntactic structures of the two sentences in a given sentence pair are similar. While Hwa et al.\u2019s Direct Correspondence Assumption (DCA) describes the assumption made for projection, Dorr (1994) makes a deeper analysis of divergence in languages. Dorr outlines lexical conceptual structures\n(LCS) that provide a general framework to describe these exceptions to the DCA. This framework is capable of representing divergence stemming from syntactic, lexical, or semantic differences, but for the purposes of this paper we will focus primarily on those that are lexical and syntactic.\nDorr identifies a number of ways in which languages may diverge, specifically syntactic and semantic differences in mappings between languages. Our goal in this work is to create a methodology by which some common types of divergences can be detected automatically from bitexts in order to improve the performance of existing structural projection methods."}, {"heading": "3 Methodology", "text": "In our approach to automatically detecting divergent structures between language pairs, we first propose a metric to measure the degree of matched edges between source and target trees (\u00a73.1). Second, we define three operations on trees in order to capture three common types of divergence (\u00a73.2). Third, we apply the operations on a tree pair and show how the operations could affect the degree of tree match (\u00a73.3). After explaining the relationship of our operations to Dorr\u2019s divergence types (\u00a73.4), we discuss how knowing these divergence types can be useful in improving structural projection algorithms (\u00a73.5).\n3.1 Comparing Dependency Trees\nOne of the key aspects of our method was devising a metric to compare dependency trees cross-linguistically, as most existing tree similarity measures are intended to compare tree representations with the same number of tokens. Comparing between languages, on the other hand, means that the number of tokens can vary. We instead look for a method to determine similarity by means of matched edges in the tree, as shown in Figure 4.\nGiven an IGT, let F be the parse tree for the language line (aka source tree), E be the parse tree for the translation line (aka target tree), and A be the word alignment between the language line and the translation line. F is\nmade up of the words W , in the language line, and a set of edges between the words, as follows:\nF = (WF , TF ) (1)\nWF = (f1 . . . fn) (2) TF = { (fi, fk) . . . (fn, fm) } (3)\nE is defined similarly, except words in the translation line are denoted as ei, not fi. The alignment A is a set of word pairs:\nA = { (fi, ek) . . . (fj , el) }\n(4)\nWe call an (F,E,A) tuple an aligned tree pair. A Corpus, C, in our experiments, is a set of (F,E,A) tuples.\nAn edge (fi, fk) in the foreign-language tree is said to match an edge (ei, ek) in the english-language tree if fi is aligned to ei and fk is aligned to ek. Because the alignment between a sentence pair can be many-to-many, we define the following functions, which map a word from one sentence to the set of words in the other sentence.\nRF\u2192E(fi, A) = { e|(fi, e) \u2208 A } (5)\nRE\u2192F (ei, A) = { f |(f, ei) \u2208 A } (6)\nWe then define the boolean function match, as follows:\nmatch(fi, fj , TE , A) =  1 if \u2203ea, eb (( ea \u2208 RF\u2192E(fi) ) \u2227 ( eb \u2208 RF\u2192E(fj) ) \u2227 ( (ea, eb) \u2208 TE ))\n0 otherwise\n(7)\nThat is, an edge (fi, fj) in F matches some edge in E according to A if there exists two target words, ea and eb in E such that ea aligns to fi, eb aligns to fj , and (ea, eb) is an edge in E.\nGiven an aligned tree pair (F,E,A), we define SentMatch(F,E,A) as the percentage of edges in F that match some edge in E. Given a corpus C, we define CorpusMatchSrc\u2192Tgt(C) as the percentage of edges in the source trees that match some edges in the corresponding target trees. Similarly, CorpusMatchTgt\u2192Src(C) is the percentage of edges in the target trees that match some edges in the corresponding source trees.\nCorpusMatchSrc\u2192Tgt(C) =\n\u2211 (F,E,A) \u2208 C  \u2211 (fi,fj) \u2208 TF match(fi, fj , TE , A)  \u2211\n(F,E,A)\u2208C \u2223\u2223TF \u2223\u2223 (8)\n3.2 Defining Tree Operations\nWhen an edge (fi, fk) in a tree does not match any edge in the other tree, it may be caused by one of the following cases:\nC1. fi or fk are spontaneous (they do not align with any words in the other tree). C2. fi and fk are both aligned with the same node ei in the other tree (Fig 4(b)). C3. fi and fk are both aligned with nodes in the other tree, ek and ei, but in a reversed parent-child relationship (Fig 4(c)). C4. There is some other structural differences not caused by C1\u2013C3.\nThe first three cases are common. To capture them, we define three operations on a tree \u2014 remove, merge, and swap."}, {"heading": "O1 \u2013 Remove", "text": "The remove operation is used to remove spontaneous words. As shown in Figure 5(a), removal of the node l is accomplished by removing the link between node l and its parent, j, and adding links between the parent and the removed node\u2019s children.\nThis result of this operation looks can be seen in Figure 5(a), using the relation Children, which maps a word to the set of all its children in the tree.\n1 Algorithm: Remove(w, T ) Input : T = { (wi, wj) . . . (wm, wn) } ; // Input tree\nInput : w ; // Word to remove.\nOutput: T \u2032 ; // Modified tree 2 begin 3 T \u2032 = T \u2212 { (w,wi)|wi = parent(w, T ) } // Remove edge between w and parent wi\n4 \u2212 { (wj , w)|w = parent(wj , T ) } // Remove edges for children of w\n5 + { (wj , wi)|wi = parent(w, T ), w = parent(wj , T ) } ;\n/* Finish by \u2018\u2018promoting\u2019\u2019 former children of w to now attach to w\u2019s parent, wi. */\n6 return T \u2032\nAlgorithm 1: Remove a token w from the tree T ."}, {"heading": "O2 \u2013 Merge", "text": "The merge operation is used when a node and some or all of its children in one tree align to the same node(s) in the other tree, as can be seen in Figure 4(b). The parent j and child l are collapsed into a merged node, as indicated by l+j in Figure 5(b), and the children of l are promoted to become children of the new node l+j. The result can be seen in Figure 5(b).\n1 Algorithm: Merge(wc, wp, T ) Input : T = { (wi, wj) . . . (wm, wn) } ; // Input tree\nInput : wc ; // Child word to merge. Input : wp ; // Parent word to merge.\nOutput: T \u2032 ; // Modified tree 2 begin 3 T \u2032 = T \u2212 { (wc, wp) } 4 \u2212 { (wi, wc)|wc = parent(wi, T )\n} 5 + { (wi, wp)|wc = parent(wi, T ) } ; 6 return T \u2032\nAlgorithm 2: Merge a child wc and parent wp in the tree T , and \u201cpromote\u201d the children of wc to be children of wp."}, {"heading": "O3 \u2013 Swap", "text": "The swap operation is used when two nodes in one tree are aligned to two nodes in the other tree, but in a reciprocal relationship, as shown in Figure 4(c). This operation can be used to handle certain divergence types such as demotional and promotional divergence, which will be discussed in more detail in \u00a73.4.\nFigure 5(c) illustrates how the swap operation takes place by swapping nodes l and j. Node j, the former parent, is demoted, keeping its attachment to its children. Node l, the former child, is promoted, and its children become siblings of node j, the result of which can be seen in Figure 5(c).\n1 Algorithm: Swap(wc, wp, T ) Input : TL = { (wi, wj) . . . (wm, wn) } ; // Input tree\nInput : wc ; // Child word to swap. Input : wp ; // Parent word to swap.\nOutput: T \u2032 ; // Modified tree 2 begin 3 T \u2032 = T \u2212 { (wc, wp) } + { (wp, wc) } // Swap the order in the (wc, wp) edge\n4 \u2212 { (wp, wi)|wi = parent(wp, T ) } // Remove edges for parent wp\n5 + { (wc, wi)|wi = parent(wp, T ) } ; // Add edges from wp\u2019s parent to wc 6 return T \u2032L\nAlgorithm 3: Swap a child wc and parent wp in the tree T .\n3.3 Calculating Tree Matches After Applying Operations\nThe operations O1\u2013O3 are proposed to handle common divergence cases in C1\u2013C3. To measure how common C1\u2013C3 is in a language pair, we design an algorithm that transforms a tree pair based on a word alignment.\nThe algorithm takes a tree pair (F,E) and a word alignment A as input and creates a modified tree pair (S\u2032, T \u2032) and an updated word alignment A\u2032 as output. It has several steps. First, spontaneous nodes (nodes that do not align to any node on the other tree) are removed from each tree. Next, if a node and its parent align to the same node on the other tree, they are merged and the word alignment is changed accordingly. Finally, the swap operation is applied to a node fi and its parent fp in one tree if they align to ei and ep respectively and ep is a child of ei in the other tree. The pseudocode of the algorithm is shown in Algorithm 4.\nNow given a corpus C and word alignment between each sentence pair, we can measure the impact of C1\u2013C3 by comparing CorpusMatchSrc\u2192Tgt(C)\nAlgorithm 4: Algorithm for altering an aligned tree pair. input : c = ( F,E,A ) ; // A parallel sentence with alignment\noutput: c\u2032 = ( F \u2032, E\u2032, A\u2032 ) ; // modified output sentence.\n1 Let F = (WF , TF ) ; 2 Let E = (WE , TE) ; 3 Let A = {(fi, ej), . . . , (fk, el)} ; 4 begin // Step 1(a): Remove spontaneous nodes from F 5 foreach fi \u2208WF do 6 if @ ej : ( fi, ej ) \u2208 A then 7 TF = Remove(fi, TF ) ; // See Algorithm 1\n// Step 1(b): Remove spontaneous nodes from E 8 foreach ej \u2208WE do 9 if @ fi : ( fi, ej ) \u2208 A then\n10 TE = Remove(ei, TE) ; // See Algorithm 1\n// Step 2(a): Find nodes to merge in F and merge them 11 foreach (fi, ej) \u2208 A do 12 Let fp = parent(fi, TF ) ; 13 if (fp, ej) \u2208 A then 14 TF = Merge(fi, fp, TF ) ; // See Algorithm 2 15 A = A\u2212 {(fi, ej)} ;\n// Step 2(b): Find nodes to merge in E and merge them 16 foreach (fi, ej) \u2208 A do 17 Let ep = parent(ej , TE) ; 18 if (fi, ep) \u2208 A then 19 TE = Merge(ej , ep, TE) ; // See Algorithm 2 20 A = A\u2212 {(fi, ej)} ;\n// Step 3: Find nodes to swap in F and swap them 21 foreach (fi, ej) \u2208 A do 22 Let fp = parent(fi, TF ) ; 23 if \u2203 ec : ej = parent(ec, TE) and (fp, ec) \u2208 A then 24 TF = Swap(fi, fp, TF ) ; // See Algorithm 3\n25 return (F \u2032, E\u2032, A\u2032) ;\nscores before and after applying operations O1\u2013O3. This process can also reveal some patterns of divergence (e.g., what types of nodes are often merged), and the patterns can later be used to enhance existing projection algorithms.\n3.4 Relationship to Dorr (1994)\nDorr (1994) lists seven types of divergence for language pairs. While our analysis method is more coarse-grained than the Lexical Conceptual Structure (LCS) that Dorr proposes, it is nonetheless able to capture some of the same cases.\nFor instance, Figure 6 illustrates an example of what Dorr identified as \u201cpromotional\u201d divergence, where usually, a dependent of the verb goes in En-\nglish, is \u201cpromoted\u201d to become the main verb, suele in Spanish. In this case, the direction of the dependency between usually and goes is reversed in Spanish, and thus the swap operation can be applied to the English tree and result in a tree that looks very much like the Spanish tree. A similar operation is performed for demotional divergence cases, such as aligning \u201cI like eating\u201d with the German translation \u201cIch esse gern\u201d (\u201cI eat likingly\u201d). Here, the main verb in English (\u201clike\u201d) is demoted to an adverbial modifier in German (\u201cgern\u201d). The swap operation is applicable to both types of divergence and treats them equivalently, and so it essentially can handle a superset of promotional and demotional divergence, namely,\u201chead-swapping.\u201d\nAnother type of divergence that can be captured by our approach is Dorr\u2019s \u201cstructural\u201d divergence type, as illustrated in Figure 7. The difference between the English and Spanish structures in this case is the form of the argument that the verb takes. In English, it is a noun phrase; in Spanish, it is a prepositional phrase. While the tree operations defined previously do not explicitly recognize this difference in syntactic labels, the divergence can be handled by the remove operation, where the spontaneous \u201cen\u201d in the Spanish side is removed.\nNext, Dorr\u2019s description of conflational divergence lines up well with the merge operation (see Fig 5(b)). Figure 8 illustrates an example for English and Hindi, where both sides have spontaneous words (e.g., to and a in English) and a causative verb in Hindi corresponds to multiple verbs in English. Figure 8(b) shows the original tree pair, 8(c) demonstrates the altered tree pair after removing spontaneous words from both sides. Figure 8(d) shows the tree pairs after the English verbs are merged into a single node. It is clear that the remove and merge operations make the Hindi and English trees much similar to each other.\nIn addition to the four divergence types mentioned above, additional operations could be added to handle other divergence types. For instance, if dependency types (e.g., patient, agent) are given in the dependency structure, we can define a new operation that changes the dependency type of an edge\nto account for thematic divergence, where thematic roles are switched as in \u201cI like Mary\u201d in English vs. \u201cMar\u0301\u0131a me gusta a m\u0131\u0301\u201d (Mary pleases me) in Spanish. Similarly, an operation that changes the POS tag of a word can be added to cover categorial divergence where words representing the same semantic content have different word categories in the two languages, such as in \u201cI am hungry\u201d in English versus \u201cIch habe Hunger\u201d (I have hunger) in German.\nCompared to Dorr\u2019s divergence types, whose identification requires knowledge about the language pairs, our operations on the dependency structure relies on word alignment and tree pairs and can be applied automatically.\n3.5 Extending Projection Algorithms\nUsing the techniques described above, and following Hwa et al. (2004), we can find post-processing rules automatically. Our method couples the divergent cases C1\u2013C3 with corresponding operations O1\u2013O3. As the operations are applied, statistics are kept on the nodes that are affected, and thus common divergence patterns can be detected by analyzing this data. By generalizing the data found in this analysis, rules that can handle common divergence types could be applied to particular language pairs that exhibit such patterns in the small training corpus. We will discuss one such attempt at learning a rewrite rule in \u00a74.4."}, {"heading": "4 Experiments", "text": "With the matching function and tree operations defined in the previous section, we looked at a total of eleven language pairs, using the corpora described in Table 1.\n4.1 Data\nOur work utilizes three corpora for a total of eleven language pairs. The corpora used are the SMULTRON treebank (Volk et al., 2010), the guideline sentences in IGT form from the Hindi treebank (Bhatt et al., 2009), and several sets of IGT data as used in (Xia and Lewis, 2007). The statistics of the corpora are shown in Table 1. Ten of the language pairs use English as one side of the language, while the eleventh uses the pair of German and Swedish from the SMULTRON corpus.\nIn the SMULTRON Treebank, the German and Swedish phrase trees are marked for head children, allowing for the automatic extraction of dependency trees. The English side of the phrase structures do not contain edge labels and are instead converted to dependency trees using a head percolation table (Collins, 1999).\nFrom the Hindi Treebank guidelines, we extracted example sentences in the form of IGT (i.e., Hindi sentences, English gloss, and English translation) and the Hindi dependency structures manually created by the guideline designers. We obtained dependency structures for the English translation by running the Stanford dependency parser de Marneffe et al. (2006) and then we hand corrected the structures. Word alignment is initially derived from the IGT instances using heuristic alignment following Xia and Lewis (2007), and later hand-corrected. The IGT data from Xia and Lewis (2007) was obtained in the manually corrected dependency forms described in \u00a72.2.\n4.2 Match Results\nBy running Algorithm 5, we can calculate the CorpusMatchSrc\u2192Tgt and CorpusMatchTgt\u2192Src before and after each operation and see how the operation affects the percentage of matched edges in the corpus. As the operations are applied, the percentage of matches between the trees should increase until all the divergence cases that can be handled by operations O1\u2013O3 have been resolved. At this point, the final match percentage can be seen as an estimate of the upper-bound on performance of a simple projection algorithm, if C1-C3 can be identified and handled by O1-O3. Table 2 shows the full results of this process for English and Hindi, while Table 3 shows a summary for the results in the remaining ten languages.\n4.3 Operation Breakdown By POS\nAfter performing the operations as seen in \u00a74.2, we can get further insight into what precisely is happening within each language by breaking down the operations by the POS tags on which the operations apply. Table 4 shows some of these POS tag breakdowns for a number of languages, and the frequency with which the given operation applies to the POS tag or POS tag pair out of\nall the times it is seen in that language. Table 4 shows phenomena from the language pairs that we would hope to see. For instance, rows 6 and 7 show the English\u2192German pair merging many nouns as multiple English words are expressed as compounds in German. In another case, Row 8 shows that all Hindi Nouns undergo swap with prepositions, as Hindi uses postpositions. Noticing this extremely high regularity leads us to the next experiment, where we examine how such regularly-occurring rules might be harnessed to improve projection.\n4.4 Analyzing Trees For Post-Processing Rules\nGiven that we can break down the operations as they apply to particular projected POS tag sequences, we also would like to see if the detected patterns can be used after projection, to correct the projected trees. In Table 5, 80% of the Hindi data was used to train a small corpus on swap patterns were detected. The projection algorithm was run on the remaining 20%. The first row of Table 6 shows the results of the baseline projection algorithm.\nNext, we used the dependencies as projected into Hindi from the English trees as shown in Table 5 and automatically selected those that occurred with 80% or higher frequency. The projection algorithm was run as usual, and then\nAlgorithm 5: Calculating the percentage of matched edges in a corpus C.\ninput : A corpus C output: CorpusMatchSrc\u2192Tgt(C)\nCorpusMatchTgt\u2192Src(C) 1 begin 2 Let F \u2192 E matches = 0 ; 3 Let E \u2192 F matches = 0 ; 4 foreach (F,E,A) \u2208 C do 5 Let F = (WF , TF ) ; 6 Let E = (WE , TE) ; 7 Let A = {(fi, ej), . . . , (fk, el)} ; // Get matches for F \u2192 E 8 foreach ( fc, fp ) \u2208 TF do\n/* If the child and parent in this edge align with the child\u2192parent edge of the other tree... */\n9 if \u2203 ec, ep : ep = parent(ec, TE) 10 and (fp, ep) \u2208 A 11 and (fc, ec) \u2208 A 12 then\n// Increase the match count. 13 ( F \u2192 E matches ) ++;\n// Get matches for E \u2192 F 14 foreach ( ec, ep ) \u2208 TE do\n/* If the child and parent in this edge align with the child\u2192parent edge of the other tree... */\n15 if \u2203 fc, fp : fp = parent(fc, TF ) 16 and (fp, ep) \u2208 A 17 and (fc, ec) \u2208 A 18 then\n// Increase the match count. 19 ( E \u2192 F matches ) ++; 20 return Match(F \u2192 E) = 100\u00d7 ( F\u2192Ematches ) |TF | ;\n21 return Match(E \u2192 F ) = 100\u00d7 ( E\u2192Fmatches ) |TE | ;\nevery dependency that matched the given pattern was swapped to correct the trees. The second row of the table shows that applying all rules above this threshold actually brought the F1 score down from 64.95 to 63.59. To ensure that only swaps which were both regular and frequent occurred, we tightened the restriction so that only rules that occurred three or more times were chosen. This resulted in an improvement to an F1 score of 73.35 as shown in the final row of Table 6.\n4.5 Remaining Cases\nAfter applying three operations, there may still be unmatched edges. An example is given in Figure 9. 1 The dependency edge (in, America) can be reversed by the swap operation to match the Hindi counterpart. The difficult part is the\n1 It is a topic of debate whether mentally in English should depend on in or am. If it depends on in, handling the divergence would be more difficult.\nadverb mentally in English corresponds to the noun mana (mind) in Hindi. If the word alignment includes the three word pairs as indicated by the dotted lines, one potential way to handle this kind of divergence is to extend the definition of merge to allow edges to be merged on both sides simultaneously \u2013 in this case, merging am and mentally in the English side, and hE (is) and mana (mind) on the Hindi side."}, {"heading": "5 Discussion of Results", "text": "The results of the experiments above show that the match scoring that we have introduced here has the potential to address many interesting issues arising between language pairs. In this section, we highlight some interesting observations based on the experimental results.\n5.1 Match Scores\nThe results of tables 2 and 3 are interesting in comparing similarity both across languages and corpora. For instance, in the scores for the baseline ODIN data, we see that the baseline for matches between English and German is the highest out of all the pairs at 76.7%. Scots Gaelic and Welsh are 72% and 75.4%, respectively. Hausa, Malagasy, Korean, and Yaqui all show baseline scores between 54\u201357%. This seems in line with what we would expect, with German and the Celtic languages being closely related to English, and the others being quite unrelated.\nAnother stark contrast can be seen between all the languages in the ODIN data and the languages in the SMULTRON corpus. While the ODIN sentences tend to be short sentences used primarily for illustrative purposes, the SMULTRON corpus consists of economic, literary, and weather domains. As Table 1 shows, the SMULTRON sentences are much longer on average. A closer look at the SMULTRON translations also shows them to be much freer translations\nthan those found in the ODIN data. While the size of the data sets used here are very small, and the ODIN IGT data may be biased towards illustrative purposes (described as the \u201cIGT Bias\u201d in Xia and Lewis (2007)), it would appear that these results illustrate that the match detection is capable of two interesting corpus analyses. First, by comparing baselines match results among comparable corpora, basic similarities between languages appear to pattern as expected. Second, the freer translations in the SMULTRON data appear with lower scores all across.\nOne final item of interest from the match results can be seen in the Hindi data in Table 2. Here, there appears to be a large jump after the swap operation has been performed. Seeing as swapping such as this would be problematic for projection algorithms, this is the inspiration for automatically inferring the swap rules in \u00a74.4.\n5.2 POS Breakdowns\nThe breakdown of the operations by language and POS in Table 4 provides a good opportunity to check that the defined operations conform with expectations for specific languages.\nFor instance, Row 1 in Table 4 shows Modals (MD) merging with a parent (VB). This is in line with instances such as Figure 8(c) where Hindi combines aspect with a verb that is typically expressed as a separate word in English. This does not appear to be a very frequent occurrence, however, as it only occurs for 42.9% of MD\u2192VB dependencies.\nRow 3, going from Hindi to English shows the case where auxiliary verbs VAUX merge with main verbs VM. These cases typically represent those where Hindi represents tense as an auxiliary verb, whereas English tense is expressed by inflection on the verb.\nWith regard to spontaneous words in English and Hindi, Row 14 shows that 69.8% of case markers (PSP) were removed from Hindi that were either absent in English or applied as inflections to the noun, while 86% of determiners in English were removed, as they are not seen in Hindi (Row 12).\nExamining the English and German data in Table 4, we first see in Row 5 that 66.7% of NN-NNS dependencies in English merge. This, along with the 65.4% of NN-NN dependencies merging, is something we would expect to see in German, as it compounds nouns with far more frequency than English. Interestingly, as row 7 shows, a plural noun child never merges with a parent noun.\nFinally, looking more closely at the swaps, we see a surprising 100% of NN\u2192IN dependencies are swapped in Hindi, giving further impetus for the rules as described in \u00a74.4.\n5.3 Automatically Created Rules\nAs Table 6 illustrates, the potential for automatically created post-processing rules for a projection algorithm is clearly shown by the 13% increase in F1 score from the uncorrected projection algorithm. While this example focuses only on the easily correctable swap operation, the success here suggests that a similar analysis of the merge operations could be used to correct the 1-to-many projections from English onto other languages that Yarowsky and Ngai (2001) cited as problematic.\n5.4 Discussion of Issues\nTwo large issues that our methodology faces are data sparsity and translation quality of the sentence pairs in the data sets. The former is somewhat inevitable given the task\u2014a reasonable amount of annotated data is not always likely to exist for languages with scarce electronic resources, and guaranteeing coverage is difficult. As with the Hindi data, however, using IGT as a resource has convenience in both covering wide varieties of phenomena in a language, and providing a gloss that assists in creating word-level alignments. Creating dependency annotation on a small set of data from a source like ODIN (Lewis, 2006) can get a lot of mileage with a small amount of investment.\nPerhaps the more challenging issue is determining whether divergence in a language pair is caused by fundamental differences between the languages, or simply stylistic choices in translation. The latter of these scenarios appeared to be common in portions of the SMULTRON data, where translations appeared to be geared toward naturalness in the target language; in contrast, the translations in the Hindi guideline sentences were intended to be as literal as possible. Again, IGT provides a good possible solution, as such examples are often intended specifically for illustrative purposes."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we have demonstrated a generalizable approach to detecting patterns of structural divergence across language pairs using simple tree operations based on word alignment. We have shown that this methodology can be used to detect similarities between languages on a course level, as well as serve as a general measure of similarity between dependency corpora. Finally, we showed that harnessing these detection methods has potential for improving projection algorithms with little to no expert involvement.\nThis work further shows that there is still plenty of room for improvement in existing projection methods. In future work, we plan to investigate further ways of learning post-processing rules in projected trees, such as how to properly correct for 1-to-many alignments, and methods for reattaching spontaneous words in the foreign language. In future work, we would also like to\nexamine how labeled dependency edges may play a role in describing divergences between languages, and how these might be more adequately corrected for.\nFinally, while Georgi et al. (2012) demonstrated that projected trees can be improved upon by using the projected edges as features in a statistical parser, we would like to follow up on this work by examining how the alignment types outlined in this paper might be also used as features in a parser to further improve dependency parsing for resource-poor languages.\nThe techniques described here are promising for maximizing the effectiveness of existing resources such as IGT for languages with little more available. While the amount of electronic resources continues to increase for those languages in which electronic communication is increasingly common, many of these resource-poor languages are still left behind. Though projection techniques may not ultimately be full replacements for large treebank projects, the ability of these techniques to be rapidly deployed is extremely useful for researchers seeking to experiment with new languages at minimal cost."}], "references": [{"title": "Enhancing mention detection using projection via aligned corpora", "author": ["Yassine Benajiba", "Imed Zitouni"], "venue": "In 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Benajiba and Zitouni.,? \\Q2010\\E", "shortCiteRegEx": "Benajiba and Zitouni.", "year": 2010}, {"title": "A multi-representational and multilayered treebank for Hindi/Urdu. In The Third Linguistic Annotation Workshop (The LAW III) in conjunction with ACL/IJCNLP", "author": ["Rajesh Bhatt", "Bhuvana Narasimhan", "Martha Palmer", "Owen Rambow", "Dipti Misra Sharma", "Fei Xia"], "venue": null, "citeRegEx": "Bhatt et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bhatt et al\\.", "year": 2009}, {"title": "A statistical approach to machine translation", "author": ["Peter F Brown", "John Cocke", "Stephen A Della Pietra", "Vincent J Della Pietra", "Fredrick Jelinek", "John D Lafferty", "Robert L Mercer", "Paul S Roossin"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1990}, {"title": "The LRE Map. Harmonising Community Descriptions of Resources", "author": ["Nicoletta Calzolari", "Riccardo Del Gratta", "Gil Francopoulo", "Joseph Mariani", "Francesco Rubino", "Irene Russo", "Claudia Soria"], "venue": "In LREC (International Conference on Language Resources and Evaluation),", "citeRegEx": "Calzolari et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Calzolari et al\\.", "year": 2012}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins"], "venue": "PhD thesis, University of Pennsylvania,", "citeRegEx": "Collins.,? \\Q1999\\E", "shortCiteRegEx": "Collins.", "year": 1999}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine de Marneffe", "Bill MacCartney", "Christopher D Manning"], "venue": "In Proceedings of LREC", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Machine translation divergences: a formal description and proposed solution", "author": ["Bonnie Jean Dorr"], "venue": "Computational Linguistics,", "citeRegEx": "Dorr.,? \\Q1994\\E", "shortCiteRegEx": "Dorr.", "year": 1994}, {"title": "Evaluating translational correspondence using annotation projection", "author": ["Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Okan Kolak"], "venue": "In Proceedings of ACL", "citeRegEx": "Hwa et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hwa et al\\.", "year": 2002}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak"], "venue": "Natural Language Engineering,", "citeRegEx": "Hwa et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hwa et al\\.", "year": 2004}, {"title": "ODIN: A model for adapting and enriching legacy infrastructure", "author": ["William D Lewis"], "venue": "In e-Science", "citeRegEx": "Lewis.,? \\Q2006\\E", "shortCiteRegEx": "Lewis.", "year": 2006}, {"title": "Automatically identifying computationally relevant typological features", "author": ["William D Lewis", "Fei Xia"], "venue": "In Proceedings of IJCNLP", "citeRegEx": "Lewis and Xia.,? \\Q2008\\E", "shortCiteRegEx": "Lewis and Xia.", "year": 2008}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": "In Proceedings of LREC,", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "SMULTRON (version 3.0) \u2014 The Stockholm MULtilingual parallel TReebank, 2010. URL http://www.cl.uzh.ch/research/paralleltreebanks/ smultron_en.html. An English-French-German-Spanish-Swedish parallel treebank with sub-sentential alignments", "author": ["Martin Volk", "Anne G\u00f6hring", "Torsten Marek", "Yvonne Samuelsson"], "venue": null, "citeRegEx": "Volk et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Volk et al\\.", "year": 2010}, {"title": "Multilingual structural projection across interlinear text", "author": ["Fei Xia", "William D Lewis"], "venue": "In Human Language Technologies:", "citeRegEx": "Xia and Lewis.,? \\Q2007\\E", "shortCiteRegEx": "Xia and Lewis.", "year": 2007}, {"title": "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora", "author": ["David Yarowsky", "Grace Ngai"], "venue": "In Proceedings of NAACL, Stroudsburg, PA,", "citeRegEx": "Yarowsky and Ngai.,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky and Ngai.", "year": 2001}], "referenceMentions": [{"referenceID": 3, "context": "Out of the resources listed by the LRE Map (Calzolari et al., 2012), English accounts for 30% of all recorded resources, and the ten most resourced languages for 62% of all resources.", "startOffset": 43, "endOffset": 67}, {"referenceID": 10, "context": "To address this issue, recent studies (Lewis and Xia, 2008; Benajiba and Zitouni, 2010; Georgi et al., 2012) have proposed to take advantage of bitext and resources for resource-rich languages; that is, use tools for resource-rich languages to process one side of bitext (the resource-rich language) and project the information to the other side of bitext (the resource-poor language) via word alignments.", "startOffset": 38, "endOffset": 108}, {"referenceID": 0, "context": "To address this issue, recent studies (Lewis and Xia, 2008; Benajiba and Zitouni, 2010; Georgi et al., 2012) have proposed to take advantage of bitext and resources for resource-rich languages; that is, use tools for resource-rich languages to process one side of bitext (the resource-rich language) and project the information to the other side of bitext (the resource-poor language) via word alignments.", "startOffset": 38, "endOffset": 108}, {"referenceID": 12, "context": "3 Simple but frequent example of 1-to-many German\u2013English alignment found in the Sophie\u2019s World portion of the SMULTRON corpus (Volk et al., 2010).", "startOffset": 127, "endOffset": 146}, {"referenceID": 2, "context": "Here, the authors used IBM Model 3 (Brown et al., 1990) to align large parallel corpora between English\u2013Chinese and English\u2013French.", "startOffset": 35, "endOffset": 55}, {"referenceID": 13, "context": "Some of the initial research on the subject of projecting word-level annotation from one language to another was published in Yarowsky and Ngai (2001). Here, the authors used IBM Model 3 (Brown et al.", "startOffset": 126, "endOffset": 151}, {"referenceID": 12, "context": "3 shows a very simple but common case of conflation in the SMULTRON corpus (Volk et al., 2010) where a single German word aligns to multiple English words.", "startOffset": 75, "endOffset": 94}, {"referenceID": 11, "context": "In this case, using a universal tagset such as those presented by Petrov et al. (2012) could help alleviate the problem, but for more complex cases, learning the pattern would be more critical.", "startOffset": 66, "endOffset": 87}, {"referenceID": 9, "context": "In Xia and Lewis (2007), enriched IGT data for 7 language pairs was created using this augmented alignment and structural projection, then handcorrected to create gold standards with minimal expert intervention.", "startOffset": 11, "endOffset": 24}, {"referenceID": 9, "context": "In Xia and Lewis (2007), enriched IGT data for 7 language pairs was created using this augmented alignment and structural projection, then handcorrected to create gold standards with minimal expert intervention. They showed the potential for using IGT as a resource for languages for which finding resources would otherwise be extremely difficult or impossible to obtain. We will use this data for the current work. A breakdown of the language pairs can be seen in \u00a74.1. Lewis and Xia (2008) used projected phrase structures to determine the basic word order for 97 languages using a database of IGT instances.", "startOffset": 11, "endOffset": 492}, {"referenceID": 6, "context": "\u2019s Direct Correspondence Assumption (DCA) describes the assumption made for projection, Dorr (1994) makes a deeper analysis of divergence in languages.", "startOffset": 88, "endOffset": 100}, {"referenceID": 6, "context": "4 Relationship to Dorr (1994)", "startOffset": 18, "endOffset": 30}, {"referenceID": 6, "context": "6 An example of promotional divergence from Dorr (1994). The reverse in parent-child relation is handled by the Swap operation.", "startOffset": 44, "endOffset": 56}, {"referenceID": 7, "context": "Using the techniques described above, and following Hwa et al. (2004), we can find post-processing rules automatically.", "startOffset": 52, "endOffset": 70}, {"referenceID": 12, "context": "The corpora used are the SMULTRON treebank (Volk et al., 2010), the guideline sentences in IGT form from the Hindi treebank (Bhatt et al.", "startOffset": 43, "endOffset": 62}, {"referenceID": 1, "context": ", 2010), the guideline sentences in IGT form from the Hindi treebank (Bhatt et al., 2009), and several sets of IGT data as used in (Xia and Lewis, 2007).", "startOffset": 69, "endOffset": 89}, {"referenceID": 13, "context": ", 2009), and several sets of IGT data as used in (Xia and Lewis, 2007).", "startOffset": 49, "endOffset": 70}, {"referenceID": 4, "context": "The English side of the phrase structures do not contain edge labels and are instead converted to dependency trees using a head percolation table (Collins, 1999).", "startOffset": 146, "endOffset": 161}, {"referenceID": 1, "context": ", 2010), the guideline sentences in IGT form from the Hindi treebank (Bhatt et al., 2009), and several sets of IGT data as used in (Xia and Lewis, 2007). The statistics of the corpora are shown in Table 1. Ten of the language pairs use English as one side of the language, while the eleventh uses the pair of German and Swedish from the SMULTRON corpus. In the SMULTRON Treebank, the German and Swedish phrase trees are marked for head children, allowing for the automatic extraction of dependency trees. The English side of the phrase structures do not contain edge labels and are instead converted to dependency trees using a head percolation table (Collins, 1999). From the Hindi Treebank guidelines, we extracted example sentences in the form of IGT (i.e., Hindi sentences, English gloss, and English translation) and the Hindi dependency structures manually created by the guideline designers. We obtained dependency structures for the English translation by running the Stanford dependency parser de Marneffe et al. (2006) and then we hand corrected the structures.", "startOffset": 70, "endOffset": 1029}, {"referenceID": 1, "context": ", 2010), the guideline sentences in IGT form from the Hindi treebank (Bhatt et al., 2009), and several sets of IGT data as used in (Xia and Lewis, 2007). The statistics of the corpora are shown in Table 1. Ten of the language pairs use English as one side of the language, while the eleventh uses the pair of German and Swedish from the SMULTRON corpus. In the SMULTRON Treebank, the German and Swedish phrase trees are marked for head children, allowing for the automatic extraction of dependency trees. The English side of the phrase structures do not contain edge labels and are instead converted to dependency trees using a head percolation table (Collins, 1999). From the Hindi Treebank guidelines, we extracted example sentences in the form of IGT (i.e., Hindi sentences, English gloss, and English translation) and the Hindi dependency structures manually created by the guideline designers. We obtained dependency structures for the English translation by running the Stanford dependency parser de Marneffe et al. (2006) and then we hand corrected the structures. Word alignment is initially derived from the IGT instances using heuristic alignment following Xia and Lewis (2007), and later hand-corrected.", "startOffset": 70, "endOffset": 1188}, {"referenceID": 1, "context": ", 2010), the guideline sentences in IGT form from the Hindi treebank (Bhatt et al., 2009), and several sets of IGT data as used in (Xia and Lewis, 2007). The statistics of the corpora are shown in Table 1. Ten of the language pairs use English as one side of the language, while the eleventh uses the pair of German and Swedish from the SMULTRON corpus. In the SMULTRON Treebank, the German and Swedish phrase trees are marked for head children, allowing for the automatic extraction of dependency trees. The English side of the phrase structures do not contain edge labels and are instead converted to dependency trees using a head percolation table (Collins, 1999). From the Hindi Treebank guidelines, we extracted example sentences in the form of IGT (i.e., Hindi sentences, English gloss, and English translation) and the Hindi dependency structures manually created by the guideline designers. We obtained dependency structures for the English translation by running the Stanford dependency parser de Marneffe et al. (2006) and then we hand corrected the structures. Word alignment is initially derived from the IGT instances using heuristic alignment following Xia and Lewis (2007), and later hand-corrected. The IGT data from Xia and Lewis (2007) was obtained in the manually corrected dependency forms described in \u00a72.", "startOffset": 70, "endOffset": 1254}, {"referenceID": 9, "context": "While the size of the data sets used here are very small, and the ODIN IGT data may be biased towards illustrative purposes (described as the \u201cIGT Bias\u201d in Xia and Lewis (2007)), it would appear that these results illustrate that the match detection is capable of two interesting corpus analyses.", "startOffset": 164, "endOffset": 177}, {"referenceID": 14, "context": "While this example focuses only on the easily correctable swap operation, the success here suggests that a similar analysis of the merge operations could be used to correct the 1-to-many projections from English onto other languages that Yarowsky and Ngai (2001) cited as problematic.", "startOffset": 238, "endOffset": 263}, {"referenceID": 9, "context": "Creating dependency annotation on a small set of data from a source like ODIN (Lewis, 2006) can get a lot of mileage with a small amount of investment.", "startOffset": 78, "endOffset": 91}], "year": 2016, "abstractText": "Obtaining syntactic parses is a crucial part of many NLP pipelines. However, most of the world\u2019s languages do not have large amounts of syntactically annotated corpora available for building parsers. Syntactic projection techniques attempt to address this issue by using parallel corpora consisting of resource-poor and resource-rich language pairs, taking advantage of a parser for the resource-rich language and word alignment between the languages to project the parses onto the data for the resource-poor language. These projection methods can suffer, however, when the two languages are divergent. In this paper, we investigate the possibility of using small, parallel, annotated corpora to automatically detect divergent structural patterns between two languages. These patterns can then be used to improve structural projection algorithms, allowing for better performing NLP tools for resource-poor languages, in particular those that may not have large amounts of annotated data necessary for traditional, fully-supervised methods. While this detection process is not exhaustive, we demonstrate that common patterns of divergence can be identified automatically without prior knowledge of a given language pair, and the patterns can be used to improve performance of projection algorithms.", "creator": "LaTeX with hyperref package"}}}