{"id": "1605.04859", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "Reducing the Model Order of Deep Neural Networks Using Information Theory", "abstract": ".258 Deep sch\u00fctzenberger neural lmu networks blizzard are like-named typically represented by joklik a brittingham much larger number of parameters sulami than shallow models, maquet making them prohibitive deamination for small khangai footprint devices. skyhooks Recent dockrell research shows potager that two-inch there intentionality is ko\u017amin considerable parlato redundancy selectv in 2,796 the dillion parameter space scamming of deep neural hifikepunye networks. In comique this misick paper, we propose a zinsser method to 62-acre compress deep neural 52-69 networks sargodha by using groesbeck the wanly Fisher Information metric, which 54.27 we short-form estimate through a teletoon stochastic optimization method that jangipur keeps intellectualized track gateposts of lachner second - matsumoku order arenavirus information tuch\u00f3w in myernick the network. We first frankenthal remove unimportant parameters eniola and sleaziest then use klippen non - jawad uniform fixed umrao point quantization to cnic assign more hepner bits to fag parameters with lanfranchi higher pozdnyakov Fisher clendenin Information 106.26 estimates. We evaluate 35.47 our method lepsis on chernyshov a classification task vice with a convolutional anglomania neural brailovsky network trained on 266.1 the multiheaded MNIST assaying data set. torgerson Experimental 52.01 results show that kokkola our unfp method south-westerly outperforms turker existing hulkower methods for both network pruning and hesychius quantization.", "histories": [["v1", "Mon, 16 May 2016 18:12:45 GMT  (1022kb,D)", "http://arxiv.org/abs/1605.04859v1", "To appear in ISVLSI 2016 special session"]], "COMMENTS": "To appear in ISVLSI 2016 special session", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["ming tu", "visar berisha", "yu cao", "jae-sun seo"], "accepted": false, "id": "1605.04859"}, "pdf": {"name": "1605.04859.pdf", "metadata": {"source": "CRF", "title": "Reducing the Model Order of Deep Neural Networks Using Information Theory", "authors": ["Ming Tu", "Visar Berisha", "Yu Cao", "Jae-sun Seo"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nDeep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7]. For example, while the original LeNet5 network [8] (a classification system based on convolutional neural network) has less than 100K parameters, the winner of the 2012 ImageNet competition [9] has over 60M parameters. The memory access costs alone can make these larger networks unsuitable for low-power settings.\nIt has been posited that the expressive power of DNNs comes from their large parameter spaces and hierarchical structure; however recent studies have shown that there is often a great deal of parameter redundancy in DNNs [10], [11], making them unnecessarily complex. As a result, reducing the complexity of DNNs has been an area of great interest to the research community in recent years. For example, the authors in [12], [13] used low rank decomposition of the weights to reduce the parameter set and applied this method to a DNNbased acoustic model and to convolutional neural networks (CNN) for image classification. Similarly, the authors in [11] showed that over 95% parameters of DNNs can be predicted without any training and without impacting accuracy.\nIn addition to low-rank parameter decomposition, network pruning and quantization methods have also been proposed [14]. Neural network pruning has been investigated in early studies, including pruning weights with small magnitudes, optimal brain damage [15] and optimal brain surgeon [16]. The last two methods require estimation of the Hessian matrix\nof network parameters to decide on their importance; however, the sizes of existing networks make the estimation of this large matrix prohibitive. As a result, for large-scale DNNs, magnitude-based weight pruning is still a popular method [17], [14], [18].\nFor fixed-point implementations of DNNs, parameter quantization is also required. The studies in [19], [20] discretized the weights of a neural network according to the range of the weights. The methods in [21] and [22] used uniform scalar parameter quantization to implement fixed-point versions of the networks. In [23], a new fixed-point representation for DNN training was proposed, using stochastic rounding for the parameters. Vector quantization based schemes have been applied to CNNs for both computer vision and automatic speech recognition tasks [24], [25], [26].\nIn this paper, we propose a new method that ranks the parameters of a DNN for both network pruning and parameter quantization. We investigate an information-theoretic approach to reduce the DNN parameter space by using the Fisher Information as a proxy for parameter importance. The Fisher criterion is a natural metric for quantifying the relative importance of DNN parameters since it provides an estimate of how much information a random variable carries about a parameter of the distribution. In [27], we introduced a new method to calculate the diagonal of the Fisher Information Matrix (FIM) and showed that it can be used to reduce the size of DNNs. In this paper, we extend this work by using a lower-complexity estimate of the FIM diagonal and evaluating the technique on a much larger network. We validate the method on the MNIST dataset using a CNN and show that our method results in smaller networks with fewer parameters to quantize at a lower bit rate.\nThe remainder of this paper is organized as follows: In the next section, we introduce our network pruning and quantization scheme. In section III, we validate the algorithm on the MNIST data. We end the paper with a discussion of the results in section IV and concluding remarks in section V."}, {"heading": "II. DNNS PRUNING AND QUANTIZATION", "text": ""}, {"heading": "A. Fisher Information and DNNs", "text": "We show a notional DNN architecture in Fig. 1. Let us consider the output y of the DNN as a conditional probability distribution p(y|x;\u03b8) parameterized by the DNN input, x, and its parameters, \u03b8. The FIM evaluated at a particular value of \u03b8 is defined as:\nar X\niv :1\n60 5.\n04 85\n9v 1\n[ cs\n.L G\n] 1\n6 M\nay 2\n01 6\nF(\u03b8) = Ey\n[( \u2202 log p(y|x;\u03b8)\n\u2202 \u03b8\n)( \u2202 log p(y|x;\u03b8)\n\u2202 \u03b8\n)T] .\n(1) We can see from eqn. (1) that the FIM is the covariance of the gradient of log likelihood with regards to its parameter \u03b8. Thus, F\u03b8 is a p\u00d7 p symmetric positive semidefinite matrix.\nIt is easy to see that the diagonal elements of the FIM can be calculated by the expectation of the element-wise multiplication of the gradient:\nFD(\u03b8) = Ey [g g] , (2)\nwhere g = \u2202 log p(y|x;\u03b8)\u2202 \u03b8 is the gradient of the log-likelihood and represents element-wise multiplication; FD(\u03b8) is a p\u00d71 vector, each element of which is the Fisher Information of a corresponding parameter.\nThe Fisher Information provides an estimate of the amount of information a random variable carries about a parameter of the distribution. In the context of a DNN, this provides a natural metric for quantifying the relative importance of any given parameter in the network. The less information an output variable carries about a parameter, the less important that parameter is to the output statistics of the network. As a result, we assume that removing parameters with low entries in FD(\u03b8) will not greatly affect the output of the network. That is precisely the approach we take in this paper - we will rank the parameters in a DNN based on their corresponding entries in the FIM diagonal. In the ensuing section, we describe the method we use for approximating FD(\u03b8)."}, {"heading": "B. Estimating the Fisher Information", "text": "A number of recent studies on natural gradient descent (NGD) [28] exploit the information geometry of the underlying parameter manifold and apply it to gradient-based optimization of DNNs. Natural gradient descent uses the inverse FIM to constrain the magnitude of the update steps such that the Kullback-Leibler (KL) divergence between the output distribution of the network at iteration t and iteration t + 1 is constant [29], [30], [31]. This approach avoids large update steps and results in faster convergence.\nAlgorithm 1 Adam algorithm, excerpted from [32] Require: step size \u03b1, exponential decay rates \u03b21, \u03b22, Given initial parameter vector \u03b80, initial first and second moment vectorsm0 \u2190 0 and v0 \u2190 0 and initial timestep t While \u03b8t not converged do:\n1. t\u2190 t+ 1 2. gt \u2190 \u2207\u03b8ft(\u03b8t\u22121) (Get gradients) 3. mt \u2190 \u03b21mt\u22121 + (1\u2212 \u03b21)gt 4. vt \u2190 \u03b22vt\u22121 + (1\u2212 \u03b22)gt gt 5. m\u0302t \u2190mt/(1\u2212 \u03b2t1) 6. F\u0302D(\u03b8t)\u2190 vt/(1\u2212 \u03b2t2) 7. \u03b8t \u2190 \u03b8t\u22121 \u2212 \u03b1m\u0302t/(F\u0302D(\u03b8t) + )\nend while return \u03b8t, F\u0302D(\u03b8t)\nFor classification problems, DNNs are trained by minimizing the cross-entropy loss function:\nf(\u03b8) = \u2212 N\u2211 i=1 C\u2211 k=1 1(y(i) = k)log(p(y(i)|x(i);\u03b8)), (3)\nwhere N is the number of training samples, C is the number of classes, y(i) is the true label of ith sample x(i) and 1 {\u00b7} is the indicator function. A recent paper proposed a new stochastic optimization method called \u201cAdam\u201d and showed we can efficiently estimate the FIM diagonal at each iteration while minimizing this loss function [32]. Similar to NGD, Adam uses the approximated FIM diagonal to adapt to the geometry of the data. As a result, in this study, we use Adam to train our DNN classification system. The details of the parameter update scheme for Adam are shown in algorithm 1. As the algorithm shows, after the training algorithm converges, it returns both the optimal \u03b8t and the approximated Fisher Information F\u0302D(\u03b8t). We should note that Adam is not the only choice as the optimizer because standard stochastic gradient descent can also be used; however this would require some other means of estimating F\u0302D(\u03b8t)."}, {"heading": "C. Network Pruning and Quantization", "text": "The simplest approach to network pruning is to rank the parameters by comparing their entries in the FIM diagonal and removing the ones with the lowest entries. However, as we will see in the results section, this method does not work well since estimating small values in the FIM diagonal is challenging and unreliable. When the model is over-parameterized, the actual parameter space is much smaller than the number of parameters used in the network. As a result, after training, a number of parameters become close to zero and estimating their influence based on the Fisher Information is challenging [27]. To address this problem we use a combination of magnitude-based and FIM-based pruning. For example, if we want to prune L parameters from the network, we first remove L(1 \u2212 r) parameters with the smallest magnitude. Then, we rank the remaining parameters based on their FIM diagonal\nentries and remove the additional Lr parameters with the smallest entries in the FIM diagonal. The parameter r is between 0 and 1 and can be optimized using cross-validation.\nAfter network pruning, we want to quantize the remaining parameters with the lowest bit representation possible. After removing L parameters, we rank the remaining p \u2212 L parameters by comparing their entries in the FIM diagonal and then apply k-means clustering to separate the parameters into several groups. We quantize groups with higher Fisher Information values using more bits and groups with lower Fisher Information using fewer bits."}, {"heading": "III. EXPERIMENTS AND RESULTS ANALYSIS", "text": "In this section, we present the experiments and results in two parts: network pruning and network quantization. All the experiments were done using the Python neural network library Keras [33] implemented using Theano on an NVIDIA GTX 760 GPU.\nWe evaluated our algorithms on the MNIST digits data set, which consists of 60K binary images for training and 10K for testing. There are 10 classes (digits from 0 to 9) in the data and the size of each image (and the input dimension of the neural network) is 28\u00d728. We trained a convolutional neural network (CNN) with 2 convolutional layers each with 32 filters. The size of the convolutional kernel was 3 \u00d7 3 and the rectified linear unit (ReLU) activation function was used. There was a 2 \u00d7 2 max-pooling layer following the two convolutional layers with 0.25 dropout probability. Before the output layer, there was a fully connected layer with 128 nodes with ReLU activations and 0.5 dropout probability. The output layer had 10 nodes with softmax activations.\nThe loss function used to train the network was the categorical cross-entropy shown in eqn. (3). We used Adam as the optimizer with the following settings 1: batch size = 256, number of epochs = 50, step size \u03b1 = 0.001, \u03b21 = 0.9, \u03b22 = 0.999, = 1E\u22128. The accuracy of this model on the MNIST classification task without any pruning and quantization was 99.29%.\nBelow we describe the performance of the proposed algorithm for both pruning and quantization tasks. While we focus on CNNs in this section, our method is in no way restricted to only CNNs or only classification networks. Indeed, our probabilistic interpretation of the DNN output makes the methodology applicable across all network types, provided that the Fisher Information can be accurately estimated. Since the fully connected layers of CNNs accounts for \u223c 90% of the total weights [34], we only focus on the weights (including bias terms) in the fully connected layers in this paper as others have done in [24] [25]."}, {"heading": "A. Network pruning", "text": "The trained network consisted of a total of 591,242 parameters in the fully connected layers. We removed different numbers of parameters using three different methods: (1) magnitude-based pruning where parameters with the smallest\nmagnitude were removed; (2) Fisher Information based pruning where parameters with small entries in the FIM diagonal were removed; and (3) a combination of magnitude and Fisher Information based pruning, where we traded off between the two methods using the parameter r (see Sec. II-C). The number of pruned parameters ranged from 1.0E4 (1.69% of the total parameters) to 5.9E5 (99.79% of the total parameters) with a step size 2.0E4. For the third method, we fixed the r value to 0.05. On this network we found that r values below 0.1 yield good results; however for other networks crossvalidation could be used to identify an appropriate value of r.\nAfter removing unimportant parameters in the network, we evaluated the results on the test data and noted both the accuracy of the model and test scores (loss function evaluated on test data) as different numbers of parameters were removed. Since there was no obvious accuracy drop until 4.1E5 parameters were removed (69.35% of the total parameters), we only show the accuracy and test score starting with 69.35% parameters removed. The results are shown in Fig. 2 and 3. In Fig. 2, we show the accuracy of the model on the test data as we remove an increased number of parameters; In Fig. 3, we show the same plot, but with the test score instead of the\naccuracy. In these figures, \u201cmag\u201d represents magnitude based pruning, \u201cfisher\u201d represents Fisher Information based pruning and \u201cmag fisher\u201d represents a combination of magnitude and Fisher Information based pruning.\nAs Fig. 2 shows, as additional parameters are removed, the accuracy of the model eventually decreases. We find that using only Fisher Information based pruning, the accuracy of the model decreases quickly. This is because estimating the FIM for small parameter values is difficult as explained in Sec. II-C. This is consistent with our finding in [27], where we used the FIM criterion to remove parameters in an autoencoder. Using magnitude-based pruning, there is a clear drop-off in performance after 5.5E5 parameters (93.02% of the total parameters) are removed; however, by using our combination of magnitude and Fisher Information pruning there is no obvious accuracy drop until 5.7E5 parameters (96.41% of the total parameters) are removed. The advantage of the combined method shows that about 2.0E4 more parameters (3.38% of the total parameters) can be removed compared to magnitude based pruning with minimal impact on model performance. The test score in Fig. 3 follows the same trend as the accuracy plot in Fig. 2.\nTo further highlight the differences in performance between \u201cmag\u201d and \u201cmag fisher\u201d, we zoom in at the point where the accuracy starts to decrease by running the experiment with a smaller step size. These findings are shown in Fig. 4, where we show the accuracy (99.29%) without any pruning. The starting point of \u201cmag fisher\u201d is 98.75% while for \u201cmag\u201d it is 98.02%. From this result, we can more clearly see the advantage of our combined method compared to magnitude based pruning. Consistent with findings from our previous work [27], the Fisher Information better captures the importance of larger parameters when compared to magnitude-based pruning."}, {"heading": "B. Network quantization", "text": "After pruning, the remaining parameters in the network must be quantized for fixed-point implementations. Our quantization method is a non-uniform quantization method based on kmeans clustering. We rank-order the weights by an importance metric (\u201cfisher\u201d or \u201cmag\u201d) and cluster them into k clusters. The\nclusters are then quantized using varying bit depths, from 1 bit/parameter (least important) to k bits (most important).\nTo remove the effects of different pruning methods, we first removed 5.4E5 parameters (91.33% of the total parameters) using magnitude-based pruning only. The resulting model had 51,242 (8.67%) parameters remaining and an overall accuracy of 98.67% (less than 1% accuracy loss). We quantized the remaining parameters using three different methods: (1) nonuniform quantization based on Fisher Information ranking; (2) non-uniform quantization based on magnitude-based ranking; and (3) uniform quantization. For methods (1) and (2), we varied the number of clusters (from 3 to 10) and estimated the accuracy of the model for each value of k.\nThe results are shown in Fig. 5 and Fig. 6: \u201cfisher quant\u201d represents non-uniform quantization based on Fisher Information, \u201cmag quant\u201d represents non-uniform quantization based on magnitude ranking, \u201cuniform\u201d represents uniform quantization and \u201cOriginal\u201d represents the original non-quantized model after removing 5.4E5 parameters (accuracy is 98.67%).\nFrom the result, we see that non-uniform quantization based on the Fisher Information can achieve almost the same accuracy as the original model with only 2.4 bits/parameter. This number is 3 for uniform quantization. Non-uniform quantization based on the magnitude never achieves the accuracy\nof the original model. The same pattern is seen for the test score. This shows that on this classification task, non-uniform quantization based on the Fisher Information achieves the highest compression ratio compared to non-uniform quantization based on magnitude ranking and uniform quantization."}, {"heading": "IV. DISCUSSION", "text": "To evaluate the compression ratio for the example shown here, we analyze the effects of both network reduction and quantization. As we previously saw in Fig. 3, if we limit our acceptable reduction in performance to 1%, then we can remove 92.18% parameters (accuracy is 98.37%) for magnitude based pruning. This results in a compression ratio of 12.8\u00d7. For our proposed combination of magnitude and Fisher Information based pruning, we can remove 94.72% parameters (accuracy is 98.38%) and the compression ratio is 18.9\u00d7.\nFor network quantization, we can choose between uniform or non-uniform quantization. If we assume the original parameters are saved in FLOAT32 format, as shown in Fig. 5 using uniform quantization, we can achieve a reduction of 32 3 = 10.7\u00d7; by using non-uniform quantization, we can achieve a reduction of 322.4 = 13.3\u00d7. This means that the total compression ratio can be as high as 18.9 \u00d7 13.3 \u2248 251.4 with less than 1% accuracy loss. This is likely an overestimate of the overall compression ratio because we need additional space to store the indices of the parameters that have been removed.\nThere is a relationship between our method and other previous methods based on estimation of the Hessian diagonal, namely optimal brain damage [15], optimal brain surgeon [16] and our previous work [27]. The first two methods use the entries in the Hessian diagonal of the resulting cost function to identify important and unimportant parameters. These approaches are closely related to our approach when the cost function is the log-likelihood since the second derivative of log-likelihood function (Hessian) evaluated at the maximum likelihood estimate is the observed Fisher Information [35]. Our approach in [27] made use of the relationship between Fisher Information and the family of f -divergences to estimate the FIM diagonal. The principal difference between those approaches and the one we use here is scalability - the stochastic optimization method we use to estimate the FIM diagonal can be scaled to much larger network sizes.\nFinally, it is important to note that further gains in performance can be obtained by retraining the network after pruning and before quantization [14]. In this study, in an attempt to isolate the effects of network reduction and quantization, we elected not to retrain the network after the fact."}, {"heading": "V. CONCLUSION", "text": "In this paper, we propose a new network reduction and quantization scheme that uses a combination of the parameter magnitude and the Fisher Information as a measure of parameter importance. For network reduction, the proposed algorithm first removes parameters with small magnitude and\nthen further reduces the network by removing additional parameters based on the Fisher Information. Following network reduction, we propose a non-uniform quantization scheme for remaining parameters based on the same Fisher criterion. The results show that the combination of network reduction and quantization results in large compression ratios. In future, our aim is to embed complexity reduction criteria in the training process instead of using it as a post-processing step."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported in part by the Office of Naval Research grant N000141410722 (Berisha), an ASU-Mayo seed grant, and a hardware grant from NVIDIA."}], "references": [{"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, vol. abs/1409.1556, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u2013 97, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764\u20131772.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg"], "venue": "arXiv preprint arXiv:1510.00726, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A 240 g-ops/s mobile coprocessor for deep neural networks", "author": ["Vinayak Gokhale", "Jonghoon Jin", "Aysegul Dundar", "Berin Martini", "Eugenio Culurciello"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2014, pp. 682\u2013687.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Cheng", "Felix X Yu", "Rogerio S Feris", "Sanjiv Kumar", "Alok Choudhary", "Shi-Fu Chang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2857\u20132865.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2148\u20132156.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "INTERSPEECH, 2013, pp. 2365\u20132369.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1269\u20131277.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1135\u20131143.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla"], "venue": "Advances in Neural Information Processing Systems, 1990, pp. 598\u2013 605.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G Stork"], "venue": "Advances in Neural Information Processing Systems, 1993, pp. 164\u2013171.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Dong Yu", "Frank Seide", "Gang Li", "Li Deng"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 4409\u20134412.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Weight quantization for multi-layer perceptrons using soft weight sharing", "author": ["Fatih K\u00f6ksal", "Ethem Alpaydyn", "G\u00fcnhan D\u00fcndar"], "venue": "Artificial Neural Networks (ICANN) 2001, pp. 211\u2013216. Springer, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Boundary contraction training for acoustic models based on discrete deep neural networks", "author": ["Ryu Takeda", "Naoyuki Kanda", "Nobuo Nukaga"], "venue": "INTERSPEECH, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Accurate and compact large vocabulary speech recognition on mobile devices", "author": ["Xin Lei", "Andrew Senior", "Alexander Gruenstein", "Jeffrey Sorensen"], "venue": "INTERSPEECH, 2013, pp. 662\u2013665.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2011, vol. 1.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "arXiv preprint arXiv:1502.02551, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Compression of deep neural networks on the fly", "author": ["Guillaume Souli\u00e9", "Vincent Gripon", "Ma\u00eblys Robert"], "venue": "arXiv preprint arXiv:1509.08745, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Small-footprint highperformance deep neural network-based speech recognition using splitvq", "author": ["Yongqiang Wang", "Jinyu Li", "Yifan Gong"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4984\u20134988.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Ranking the parameters of deep neural network using the fisher information", "author": ["Ming Tu", "Visar Berisha", "Martin Woolf", "Jae-sun Seo", "Yu Cao"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, under publication, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Natural gradient works efficiently in learning", "author": ["Shun-Ichi Amari"], "venue": "Neural computation, vol. 10, no. 2, pp. 251\u2013276, 1998.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Revisiting natural gradient for deep networks", "author": ["Razvan Pascanu", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1301.3584, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Parallel training of dnns with natural gradient and parameter averaging", "author": ["Daniel Povey", "Xiaohui Zhang", "Sanjeev Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural neural networks", "author": ["Guillaume Desjardins", "Karen Simonyan", "Razvan Pascanu"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2062\u20132070.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Keras", "author": ["Franois Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "Computer vision\u2013ECCV 2014, pp. 818\u2013833. Springer, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Assessing the accuracy of the maximum likelihood estimator: Observed versus expected fisher information", "author": ["Bradley Efron", "David V Hinkley"], "venue": "Biometrika, vol. 65, no. 3, pp. 457\u2013483, 1978.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1978}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 1, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 170, "endOffset": 173}, {"referenceID": 4, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 206, "endOffset": 209}, {"referenceID": 5, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 211, "endOffset": 214}, {"referenceID": 6, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 319, "endOffset": 322}, {"referenceID": 7, "context": "For example, while the original LeNet5 network [8] (a classification system based on convolutional neural network) has less than 100K parameters, the winner of the 2012 ImageNet competition [9] has over 60M parameters.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "For example, while the original LeNet5 network [8] (a classification system based on convolutional neural network) has less than 100K parameters, the winner of the 2012 ImageNet competition [9] has over 60M parameters.", "startOffset": 190, "endOffset": 193}, {"referenceID": 9, "context": "structure; however recent studies have shown that there is often a great deal of parameter redundancy in DNNs [10], [11], making them unnecessarily complex.", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "structure; however recent studies have shown that there is often a great deal of parameter redundancy in DNNs [10], [11], making them unnecessarily complex.", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "For example, the authors in [12], [13] used low rank decomposition of the weights to", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "For example, the authors in [12], [13] used low rank decomposition of the weights to", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "Similarly, the authors in [11] showed that over 95% parameters of DNNs can be predicted without any training and without impacting accuracy.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "In addition to low-rank parameter decomposition, network pruning and quantization methods have also been proposed [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "Neural network pruning has been investigated in early studies, including pruning weights with small magnitudes, optimal brain damage [15] and optimal brain surgeon [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "Neural network pruning has been investigated in early studies, including pruning weights with small magnitudes, optimal brain damage [15] and optimal brain surgeon [16].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "As a result, for large-scale DNNs, magnitude-based weight pruning is still a popular method [17], [14], [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "As a result, for large-scale DNNs, magnitude-based weight pruning is still a popular method [17], [14], [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 17, "context": "As a result, for large-scale DNNs, magnitude-based weight pruning is still a popular method [17], [14], [18].", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "The studies in [19], [20] discretized the weights of a neural network according to the range of the weights.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "The studies in [19], [20] discretized the weights of a neural network according to the range of the weights.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "The methods in [21] and [22] used uniform scalar parameter quantization to implement fixed-point versions of the networks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "The methods in [21] and [22] used uniform scalar parameter quantization to implement fixed-point versions of the networks.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "In [23], a new fixed-point representation for DNN training was proposed, using stochastic rounding for the parameters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "Vector quantization based schemes have been applied to CNNs for both computer vision and automatic speech recognition tasks [24], [25], [26].", "startOffset": 124, "endOffset": 128}, {"referenceID": 24, "context": "Vector quantization based schemes have been applied to CNNs for both computer vision and automatic speech recognition tasks [24], [25], [26].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "Vector quantization based schemes have been applied to CNNs for both computer vision and automatic speech recognition tasks [24], [25], [26].", "startOffset": 136, "endOffset": 140}, {"referenceID": 26, "context": "In [27], we introduced a new method to calculate the diagonal of the Fisher Information Matrix (FIM) and showed that it can be used to reduce the size of DNNs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "A number of recent studies on natural gradient descent (NGD) [28] exploit the information geometry of the underlying parameter manifold and apply it to gradient-based", "startOffset": 61, "endOffset": 65}, {"referenceID": 28, "context": "Natural gradient descent uses the inverse FIM to constrain the magnitude of the update steps such that the Kullback-Leibler (KL) divergence between the output distribution of the network at iteration t and iteration t + 1 is constant [29], [30], [31].", "startOffset": 234, "endOffset": 238}, {"referenceID": 29, "context": "Natural gradient descent uses the inverse FIM to constrain the magnitude of the update steps such that the Kullback-Leibler (KL) divergence between the output distribution of the network at iteration t and iteration t + 1 is constant [29], [30], [31].", "startOffset": 240, "endOffset": 244}, {"referenceID": 30, "context": "Natural gradient descent uses the inverse FIM to constrain the magnitude of the update steps such that the Kullback-Leibler (KL) divergence between the output distribution of the network at iteration t and iteration t + 1 is constant [29], [30], [31].", "startOffset": 246, "endOffset": 250}, {"referenceID": 31, "context": "Algorithm 1 Adam algorithm, excerpted from [32]", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "A recent paper proposed a new stochastic optimization method called \u201cAdam\u201d and showed we can efficiently estimate the FIM diagonal at each iteration while minimizing this loss function [32].", "startOffset": 185, "endOffset": 189}, {"referenceID": 26, "context": "number of parameters become close to zero and estimating their influence based on the Fisher Information is challenging [27].", "startOffset": 120, "endOffset": 124}, {"referenceID": 32, "context": "All the experiments were done using the Python neural network library Keras [33] implemented using Theano on an NVIDIA GTX 760 GPU.", "startOffset": 76, "endOffset": 80}, {"referenceID": 33, "context": "Since the fully connected layers of CNNs accounts for \u223c 90% of the total weights [34], we only focus on the weights (including", "startOffset": 81, "endOffset": 85}, {"referenceID": 23, "context": "bias terms) in the fully connected layers in this paper as others have done in [24] [25].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "bias terms) in the fully connected layers in this paper as others have done in [24] [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 26, "context": "This is consistent with our finding in [27], where we used the FIM criterion to remove parameters in an autoencoder.", "startOffset": 39, "endOffset": 43}, {"referenceID": 26, "context": "Consistent with findings from our previous work [27], the Fisher Information better captures the importance of larger parameters when compared to magnitude-based pruning.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "There is a relationship between our method and other previous methods based on estimation of the Hessian diagonal, namely optimal brain damage [15], optimal brain surgeon [16] and our previous work [27].", "startOffset": 143, "endOffset": 147}, {"referenceID": 15, "context": "There is a relationship between our method and other previous methods based on estimation of the Hessian diagonal, namely optimal brain damage [15], optimal brain surgeon [16] and our previous work [27].", "startOffset": 171, "endOffset": 175}, {"referenceID": 26, "context": "There is a relationship between our method and other previous methods based on estimation of the Hessian diagonal, namely optimal brain damage [15], optimal brain surgeon [16] and our previous work [27].", "startOffset": 198, "endOffset": 202}, {"referenceID": 34, "context": "These approaches are closely related to our approach when the cost function is the log-likelihood since the second derivative of log-likelihood function (Hessian) evaluated at the maximum likelihood estimate is the observed Fisher Information [35].", "startOffset": 243, "endOffset": 247}, {"referenceID": 26, "context": "Our approach in [27] made use of the relationship between Fisher Information and the family of f -divergences to estimate the FIM diagonal.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Finally, it is important to note that further gains in performance can be obtained by retraining the network after pruning and before quantization [14].", "startOffset": 147, "endOffset": 151}], "year": 2016, "abstractText": "Deep neural networks are typically represented by a much larger number of parameters than shallow models, making them prohibitive for small footprint devices. Recent research shows that there is considerable redundancy in the parameter space of deep neural networks. In this paper, we propose a method to compress deep neural networks by using the Fisher Information metric, which we estimate through a stochastic optimization method that keeps track of secondorder information in the network. We first remove unimportant parameters and then use non-uniform fixed point quantization to assign more bits to parameters with higher Fisher Information estimates. We evaluate our method on a classification task with a convolutional neural network trained on the MNIST data set. Experimental results show that our method outperforms existing methods for both network pruning and quantization.", "creator": "LaTeX with hyperref package"}}}