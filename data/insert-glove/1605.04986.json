{"id": "1605.04986", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "A Constant-Factor Bi-Criteria Approximation Guarantee for $k$-means++", "abstract": "messier This meghan paper elco studies gardena the $ hro k $ - means + + m\u0101y\u0101 algorithm self-administration for clustering as 561 well as elberon the .434 class brucie of $ recharge D ^ \\ comrat ell $ ithier sampling algorithms unsolved to jawdeh which $ tiptop k $ - furrier means + + belongs. andersonii It gedolot is aberle shown gaelic-speaking that heungseon for grow any constant factor $ \\ beta & 92.33 gt; chickenfoot 1 $, marinara selecting $ \\ cleanly beta k $ sigalet cluster absenting centers methyltransferases by $ passages D ^ \\ ell $ sampling battiato yields a constant - hovde factor ludwik\u00f3w approximation rapid-fire to wasteland the nasa optimal clustering 36,100 with $ k $ alterman centers, closser in expectation 194.3 and actors/actresses without vid\u00e9o conditions hetzel on deplorable the gomhuriya dataset. This emoscosocoxnews.com result orgeron extends barzagli the previously innately known $ salvatores O (\\ log gulacsi k) $ 0350 guarantee totoro for randers the case $ \\ yome beta = 1 $ nuriootpa to omerovic the mitsouko constant - factor bi - criteria co-starred regime. mohmmed It tseung also quintile improves swabians upon pfalz an dejazmach existing constant - amolops factor wi\u017cajny bi - jangha criteria result that 1283 holds dribbling only tankfire with 4.690 constant mildert probability.", "histories": [["v1", "Mon, 16 May 2016 23:41:55 GMT  (50kb,D)", "http://arxiv.org/abs/1605.04986v1", "17 pages, 1 figure"]], "COMMENTS": "17 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG cs.CG", "authors": ["dennis wei"], "accepted": true, "id": "1605.04986"}, "pdf": {"name": "1605.04986.pdf", "metadata": {"source": "CRF", "title": "A Constant-Factor Bi-Criteria Approximation Guarantee for k-means++", "authors": ["Dennis Wei"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The k-means problem and its variants constitute one of the most popular paradigms for clustering [Jain, 2010]. Given a set of n data points, the task is to group them into k clusters, each defined by a cluster center, such that the sum of distances from points to cluster centers (raised to a power `) is minimized. Optimal clustering in this sense is known to be NP-hard, in particular for k-means (` = 2) [Dasgupta, 2008, Aloise et al., 2009, Mahajan et al., 2009, Awasthi et al., 2015] and k-medians (` = 1) [Jain et al., 2002]. In practice, the most widely used algorithm remains Lloyd\u2019s [1957, 1982] (often referred to as the k-means algorithm), which alternates between updating centers given cluster assignments and re-assigning points to clusters.\nIn this paper, we study an enhancement to Lloyd\u2019s algorithm known as k-means++ [Arthur and Vassilvitskii, 2007] and the more general class of D` sampling algorithms to which k-means++ belongs. These algorithms select cluster centers randomly from the given data points with probabilities proportional to their current costs. The clustering can then be refined using Lloyd\u2019s algorithm. D` sampling is attractive for two reasons: First, it is guaranteed to yield an expected O(log k) approximation to the optimal clustering with k centers [Arthur and Vassilvitskii, 2007]. Second, it is as simple as Lloyd\u2019s algorithm, both conceptually as well as computationally with O(nkd) running time in d dimensions.\nThe particular focus of this paper is on the setting where an optimal k-clustering remains the benchmark but more than k cluster centers can be sampled to improve the approximation. Specifically, it is shown (see Theorem 1 and Corollary 1) that for any constant factor \u03b2 > 1, if \u03b2k centers are chosen byD` sampling, then a constant-factor approximation to the optimal k-clustering is obtained. This guarantee holds in expectation and for all datasets, like the one in Arthur and Vassilvitskii [2007], and improves upon the O(log k) factor therein. Such a result is known as a constant-factor bi-criteria approximation since both the optimal cost and the relevant degrees of freedom (k in this case) are exceeded but only by constant factors.\nIn the context of clustering, bi-criteria approximations can be valuable because an appropriate number of clusters k is almost never known or pre-specified in practice. Approaches to determining k from the data are all ideally based on knowing how the optimal cost decreases as k increases, but obtaining this optimal trade-off between cost and k is NP-hard as mentioned earlier. Alternatively, a simpler algorithm that has a constant-factor bi-criteria guarantee would ensure that the trade-off curve generated by this algorithm\nar X\niv :1\n60 5.\n04 98\n6v 1\n[ cs\n.L G\n] 1\n6 M\nay 2\ndeviates by no more than constant factors along both axes from the optimal curve. This may be more appealing than a deviation along the cost axis that grows with k. Furthermore, if a solution with a specified number of clusters k is truly required, then linear programming techniques can be used to select a k-subset from the \u03b2k cluster centers while still maintaining a constant-factor approximation [Aggarwal et al., 2009, Charikar et al., 2002].\nThe main result in this paper differs from the constant-factor bi-criteria approximation established in Aggarwal et al. [2009] in that the latter holds only with constant probability as opposed to in expectation. Using Markov\u2019s inequality, a constant-probability corollary can be derived from Theorem 1 herein, and doing so improves upon the approximation factor of Aggarwal et al. [2009] by more than a factor of 2. The present paper also differs from recent work on more general bi-criteria approximation of k-means by Makarychev et al. [2015], which analyzes substantially more complex algorithms.\nIn the next section, existing work on D` sampling and clustering approximations in general is reviewed in more detail. Section 2 gives a formal statement of the problem, the D` sampling algorithm, and existing lemmas regarding the algorithm. Section 3 states the main results of the paper and compares them to previous results. Proofs are presented in Section 4 and the paper concludes in Section 5."}, {"heading": "1.1 Related Work", "text": "There is a considerable literature on approximation algorithms for k-means, k-medians, and related problems, spanning a wide range in the trade-off between tighter approximation factors and lower algorithm complexity. At one end, exact algorithms [Inaba et al., 1994] and several polynomial-time approximation schemes (PTAS) [Matous\u030cek, 2000, Badoiu et al., 2002, de la Vega et al., 2003, Har-Peled and Mazumdar, 2004, Kumar et al., 2010, Chen, 2009, Feldman et al., 2007, Jaiswal et al., 2014] have been proposed for k-means and k-medians. While these have polynomial running times in n, the dependence on k and sometimes on the dimension d is exponential or worse. A simpler local search algorithm was shown to yield a ((3+2/p)`+ ) approximation for k-means (` = 2) in Kanungo et al. [2004] and k-medians (` = 1) in Arya et al. [2004], the latter under the additional constraint that centers are chosen from a finite set. This local search however requires a polynomial number of iterations of complexity nO(p), and Kanungo et al. [2004] also rely on a discretization to an -approximate centroid set [Matous\u030cek, 2000] of size O(n \u2212d log(1/ )). Linear programming algorithms offer similar constant-factor guarantees with similar running times for kmedians (again the finite set variant) and the related problem of facility location [Charikar et al., 2002, Jain and Vazirani, 2001].\nIn contrast to the above, this paper focuses on simpler algorithms in the D` sampling class, including k-means++. In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime. Arthur and Vassilvitskii [2007] also provided a matching lower bound, exhibiting a dataset on which k-means++ achieves an expected \u2126(log k) approximation.\nSampling algorithms have been shown to yield improved O(1) approximation factors provided that the dataset satisfies certain conditions. Such a result was established in Ostrovsky et al. [2012] for k-means++ and other variants of Lloyd\u2019s algorithm under the condition that the dataset is well-suited in a sense to partitioning into k clusters. In Mettu and Plaxton [2004], anO(1) approximation was shown for a somewhat more complicated algorithm called successive sampling with O(n(k + log n) + k2 log2 n) running time, subject to a bound on the dispersion of the points. A constant-factor approximation with slightly superlinear running time has also been obtained in the streaming setting [Guha et al., 2003].\nFor k-means++, the \u2126(log k) lower bound in Arthur and Vassilvitskii [2007], which holds in expectation, has spurred follow-on works on the question of whether k-means++ might guarantee a constant-factor approximation with reasonably large probability. Negative answers were provided by Brunsch and Ro\u0308glin\n[2013], who showed that an approximation factor better than (2/3) log k cannot be achieved with probability higher than a decaying exponential in k, and Bhattacharya et al. [2014], who showed that a similar statement holds even in 2 dimensions.\nIn a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009].\nRecently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means. Their work differs from the present paper in studying more complex algorithms. First, similar to Kanungo et al. [2004], Makarychev et al. [2015] reduce the k-means problem to an -approximate, finite-set instance of k-medians of size nO(log(1/ )/\n2). Subsequently, linear programming and local search algorithms are considered, the latter the same as in Kanungo et al. [2004], Arya et al. [2004], and both with polynomial complexity in the size of the k-medians instance."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Problem Definition", "text": "We are given n points x1, . . . , xn in a real metric space X with metric D(x, y). The objective is to choose t cluster centers c1, . . . , ct in X and assign points to the nearest cluster center to minimize the potential function\n\u03c6 = n\u2211 i=1 min j=1,...,t D(xi, cj) `. (1)\nA cluster is thus defined by the points xi assigned to a center cj , where ties (multiple closest centers) are broken arbitrarily. For a subset of points S, define \u03c6(S) = \u2211 xi\u2208S minj=1,...,tD(xi, cj)\n` to be the contribution to the potential from S; \u03c6(xi) is the contribution from a single point xi.\nThe exponent ` \u2265 1 in (1) is regarded as a problem parameter. Letting ` = 2 and D be Euclidean distance, we have what is usually known as the k-means problem, so-called because the optimal cluster centers are means of the points assigned to them. The choice ` = 1 is also popular and corresponds to the k-medians problem.\nThroughout this paper, an optimal clustering will always refer to one that minimizes (1) over solutions with t = k clusters, where k \u2265 2 is given. Likewise, the term optimal cluster and symbol A will refer to one of the k clusters from this optimal solution. The goal is to approximate the potential \u03c6\u2217 of this optimal k-clustering using t = \u03b2k cluster centers for \u03b2 \u2265 1."}, {"heading": "2.2 D` Sampling Algorithm", "text": "The D` sampling algorithm chooses cluster centers randomly from x1, . . . , xn with probabilities proportional to their current contributions to the potential, as detailed in Algorithm 1. Following Arthur and Vassilvitskii [2007], the case ` = 2 is referred to as the k-means++ algorithm and the probabilities used after the first iteration are referred to as D2 weighting (hence D` in general). For t cluster centers, the running time of D` sampling is O(ntd) in d dimensions.\nAlgorithm 1 D` Sampling Input: Data points x1, . . . , xn, number of clusters t Select first cluster center c1 uniformly at random from x1, . . . , xn. Compute \u03c6(xi) for i = 1, . . . , n. for j = 2 to t do\nSelect jth center cj = xi with probability \u03c6(xi)/\u03c6. Update \u03c6(xi) for i = 1, . . . , n.\nIn practice, Algorithm 1 is used as an initialization to Lloyd\u2019s algorithm, which usually produces further decreases in the potential. The analysis herein pertains only to Algorithm 1 and not to the subsequent improvement due to Lloyd\u2019s algorithm."}, {"heading": "2.3 Existing Lemmas Regarding D` Sampling", "text": "The following lemmas synthesize results from Arthur and Vassilvitskii [2007] that bound the expected potential within a single optimal cluster due to selecting a center from that cluster with uniform or D` weighting, as in Algorithm 1. These lemmas define the constant r(`)D appearing in the main results below and are also used in their proof.\nLemma 1. [Arthur and Vassilvitskii, 2007, Lemmas 3.1 and 5.1] Given an optimal cluster A, let \u03c6 be the potential resulting from selecting a first cluster center randomly from A with uniform weighting. Then E[\u03c6(A)] \u2264 r(`)u \u03c6\u2217(A) for any A, where\nr(`)u = { 2, ` = 2 and D is Euclidean, 2`, otherwise.\nLemma 2. [Arthur and Vassilvitskii, 2007, Lemma 3.2] Given an optimal cluster A and an initial potential \u03c6, let \u03c6\u2032 be the potential resulting from adding a cluster center selected randomly fromA withD` weighting. Then E[\u03c6\u2032(A)] \u2264 r(`)D \u03c6\u2217(A) for any A, where r (`) D = 2 `r (`) u .\nThe factor of 2` between r(`)u and r (`) D for general ` is explained just before Theorem 5.1 in Arthur and\nVassilvitskii [2007]."}, {"heading": "3 Main Results", "text": "The main results of this paper are stated below in terms of the single-cluster approximation ratio r(`)D defined by Lemma 2. Subsequently in Section 3.1, the results are discussed in the context of previous work.\nTheorem 1. Let \u03c6 be the potential resulting from selecting \u03b2k cluster centers according to Algorithm 1, where \u03b2 \u2265 1. The expected approximation ratio is then bounded as\nE[\u03c6] \u03c6\u2217 \u2264 r(`)D\n( 1 + min { \u03d5(k \u2212 2)\n(\u03b2 \u2212 1)k + \u03d5 ,Hk\u22121\n}) \u2212\u0398 ( 1\nn\n) ,\nwhere \u03d5 = (1 + \u221a\n5)/2 . = 1.618 is the golden ratio and Hk = 1 + 12 + \u00b7 \u00b7 \u00b7+ 1 k \u223c log k is the kth harmonic\nnumber.\nIn the proof of Theorem 1 in Section 4.2, it is shown that the 1/n term is indeed non-positive and can therefore be omitted, with negligible loss for large n.\nThe approximation ratio bound in Theorem 1 is stated as a function of k. The following corollary confirms that the theorem also implies a constant-factor bi-criteria approximation.\nCorollary 1. With the same definitions as in Theorem 1, the expected approximation ratio is bounded as\nE[\u03c6] \u03c6\u2217 \u2264 r(`)D\n( 1 + \u03d5\n\u03b2 \u2212 1\n) .\nProof. The minimum appearing in Theorem 1 is bounded from above by its first term. This term is in turn increasing in k with asymptote \u03d5/(\u03b2 \u2212 1), which can therefore be taken as a k-independent bound.\nIt follows from Corollary 1 that a constant \u201coversampling\u201d ratio \u03b2 > 1 leads to a constant-factor approximation. Theorem 1 offers a further refinement for finite k.\nThe bounds in Theorem 1 and Corollary 1 consist of two factors. As \u03b2 increases, the second, parenthesized factor decreases to 1 either exactly or approximately as 1/(\u03b2 \u2212 1). The first factor of r(`)D however is no smaller than 4, and is a direct consequence of Lemma 2. Any improvement of Lemma 2 would therefore strengthen the approximation factors above. This subject is briefly discussed in Section 5."}, {"heading": "3.1 Comparisons to Existing Results", "text": "A comparison of Theorem 1 to results in Arthur and Vassilvitskii [2007] is implicit in its statement since the Hk\u22121 term in the minimum comes directly from Arthur and Vassilvitskii [2007, Theorems 3.1 and 5.1]. For k = 2, 3, the first term in the minimum is smaller than Hk\u22121 for any \u03b2 \u2265 1, and hence Theorem 1 is always an improvement. For k > 3, Theorem 1 improves upon Arthur and Vassilvitskii [2007] for \u03b2 greater than the critical value\n\u03b2c = 1 + \u03c6(k \u2212 2\u2212Hk\u22121)\nkHk\u22121 .\nNumerical evaluation of \u03b2c shows that it reaches a maximum value of 1.204 at k = 22 and then decreases back toward 1 roughly as 1/Hk\u22121. It can be concluded that for any k, at most 20% oversampling is required for Theorem 1 to guarantee a better approximation than Arthur and Vassilvitskii [2007].\nThe most closely related result to Theorem 1 and Corollary 1 is found in Aggarwal et al. [2009, Theorem 1]. The latter establishes a constant-factor bi-criteria approximation that holds with constant probability, as opposed to in expectation. Since a bound on the expectation implies a bound with constant probability via Markov\u2019s inequality, a direct comparison with Aggarwal et al. [2009] is possible. Specifically, for ` = 2 and the t = d16(k + \u221a k)e cluster centers assumed in Aggarwal et al. [2009], Theorem 1 in the present work implies that\nE[\u03c6] \u03c6\u2217 \u2264 8\n( 1 + min { \u03d5(k \u2212 2)\nd15k + 16 \u221a ke+ \u03d5 ,Hk\u22121 }) \u2264 8 ( 1 + \u03d5\n15\n) ,\nafter taking k \u2192\u221e. Then by Markov\u2019s inequality,\n\u03c6 \u03c6\u2217 \u2264 8 0.97\n( 1 + \u03d5\n15\n) . = 9.137\nwith probability at least 1 \u2212 0.97 = 0.03 as in Aggarwal et al. [2009]. This 9.137 approximation factor is less than half the factor of 20 in Aggarwal et al. [2009].\nCorollary 1 may also be compared to the results in Makarychev et al. [2015], although it should be re-emphasized that the latter analyzes different, substantially more complex algorithms, with running time at least nO(log(1/ )/\n2) for reasonably small . The main difference between Corollary 1 and the bounds in Makarychev et al. [2015] is the extra factor of r(`)D since the factor of 1 + \u03c6/(\u03b2 \u2212 1) is comparable, at least for moderate values of \u03b2 that are of practical interest. As discussed above and in Section 5, the factor of r(`)D is due to Lemma 2 and is unlikely to be intrinsic to the D` sampling algorithm."}, {"heading": "4 Proofs", "text": "The overall strategy used to prove Theorem 1 is similar to that in Arthur and Vassilvitskii [2007]. The key intermediate result is Lemma 3 below, which relates the potential at a later iteration in Algorithm 1 to the potential at an earlier iteration. Section 4.1 is devoted to proving Lemma 3. Subsequently in Section 4.2, Theorem 1 is proven by an application of Lemma 3.\nIn the sequel, we say that an optimal cluster A is covered by a set of cluster centers if at least one of the centers lies in A. Otherwise A is uncovered. Also define \u03c1 = r(`)D \u03c6\u2217 as an abbreviation.\nLemma 3. For an initial set of centers leaving u optimal clusters uncovered, let \u03c6 denote the potential, U the union of uncovered clusters, and V the union of covered clusters. Let \u03c6\u2032 denote the potential resulting from adding t \u2265 u centers, each selected randomly with D` weighting as in Algorithm 1. Then the new potential is bounded in expectation as\nE[\u03c6\u2032 | \u03c6] \u2264 cV(t, u)\u03c6(V) + cU (t, u)\u03c1(U)\nfor coefficients cV(t, u) and cU (t, u) that depend only on t, u. This holds in particular for\ncV(t, u) = 1 + \u03d5u\nt\u2212 u+ \u03d5 , (2a)\ncU (t, u) = 1 + \u03d5(u\u2212 1) t\u2212 u+ \u03d5 , u > 0,\n0, u = 0. (2b)"}, {"heading": "4.1 Proof of Lemma 3", "text": "Lemma 3 is proven using induction, showing that if it holds for (t, u) and (t, u + 1), then it also holds for (t + 1, u + 1), similar to the proof of Arthur and Vassilvitskii [2007, Lemma 3.3]. The proof is organized into three parts. Section 4.1.1 provides base cases. In Section 4.1.2, sufficient conditions on the coefficients cV(t, u), cU (t, u) are derived that allow the inductive step to be completed. In Section 4.1.3, it is shown that the closed-form expressions in (2) are consistent with the base cases in Section 4.1.1 and satisfy the sufficient conditions from Section 4.1.2, thus completing the proof."}, {"heading": "4.1.1 Base cases", "text": "This subsection exhibits two base cases of Lemma 3. While the second of these base cases does not conform to the functional forms in (2), it is shown later in Section 4.1.3 that the same base cases also hold with coefficients given by (2).\nThe first case corresponds to u = 0, for which we have \u03c6(V) = \u03c6. Since adding centers cannot increase the potential, i.e. \u03c6\u2032 \u2264 \u03c6 deterministically, Lemma 3 holds with\ncV(t, 0) = 1, cU (t, 0) = 0, t \u2265 0. (3)\nThe second base case occurs for t = u, u \u2265 1. For this purpose, a slightly strengthened version of Arthur and Vassilvitskii [2007, Lemma 3.3] is used, as given next.\nLemma 4. With the same definitions as in Lemma 3 except with t \u2264 u, we have\nE[\u03c6\u2032 | \u03c6] \u2264 (1 +Ht)\u03c6(V) + (1 +Ht\u22121)\u03c1(U) + u\u2212 t u \u03c6(U),\nwhere we define H0 = 0 and H\u22121 = \u22121 for convenience.\nThe improvement is in the coefficient in front of \u03c1(U), from (1 +Ht) to (1 +Ht\u22121). The proof follows that of Arthur and Vassilvitskii [2007, Lemma 3.3] with some differences and is deferred to Appendix A.\nSpecializing to the case t = u, Lemma 4 coincides with Lemma 3 with coefficients\ncV(u, u) = 1 +Hu, cU (u, u) = 1 +Hu\u22121. (4)"}, {"heading": "4.1.2 Sufficient conditions on coefficients", "text": "In this subsection, it is assumed inductively that Lemma 3 holds for (t, u) and (t, u + 1). The induction to the case (t+ 1, u+ 1) is then completed under the following sufficient conditions on the coefficients:\ncV(t, u+ 1) \u2265 1, (5a) (cV(t, u+ 1)\u2212 cU (t, u+ 1))cV(t, u)2 \u2265 (cU (t, u+ 1)\u2212 cV(t, u))2, (5b)\nand\ncV(t+ 1, u+ 1) \u2265 1\n2\n[ cV(t, u) + ( cV(t, u) 2 + 4 max{cV(t, u+ 1)\u2212 cV(t, u), 0} )1/2] , (6a)\ncU (t+ 1, u+ 1) \u2265 cV(t, u). (6b)\nThe first pair of conditions (5) applies to the coefficients involved in the inductive hypothesis for (t, u) and (t, u+1). The second pair (6) can be seen as a recursive specification of the new coefficients for (t+1, u+1). This inductive step together with base cases (3) and (4) are sufficient to extend Lemma 3 to all t > u, starting with (t+ 1, u+ 1) = (2, 1) from (t, u) = (1, 0) and (t, u+ 1) = (1, 1).\nThe inductive step is broken down into a series of three lemmas, each building upon the last. The first lemma applies the inductive hypothesis to derive a bound on the potential that depends not only on \u03c6(V) and \u03c1(U) but also on \u03c6(U).\nLemma 5. Assume that Lemma 3 holds for (t, u) and (t, u + 1). Then for the case (t + 1, u + 1), i.e. \u03c6 corresponding to u+ 1 uncovered clusters and \u03c6\u2032 resulting after adding t+ 1 centers,\nE[\u03c6\u2032 | \u03c6] \u2264 min { cV(t, u)\u03c6(U) + cV(t, u+ 1)\u03c6(V)\n\u03c6(U) + \u03c6(V) \u03c6(V)\n+ cV(t, u)\u03c6(U) + cU (t, u+ 1)\u03c6(V)\n\u03c6(U) + \u03c6(V) \u03c1(U), \u03c6(U) + \u03c6(V)\n} .\nProof. We consider the two cases in which the first of the t+1 new centers is chosen from either the covered set V or the uncovered set U , similar to the proof of Lemma 4. Denote by \u03c61 the potential after adding the first new center.\nCovered case: This case occurs with probability \u03c6(V)/\u03c6 and leaves the covered and uncovered sets unchanged. We then invoke Lemma 3 with (t, u+ 1) (one fewer center to add) and \u03c61 playing the role of \u03c6. The contribution to E[\u03c6\u2032 | \u03c6] from this case is then bounded by\n\u03c6(V) \u03c6\n( cV(t, u+ 1)\u03c6 1(V) + cU (t, u+ 1)\u03c1(U) )\n\u2264 \u03c6(V) \u03c6 (cV(t, u+ 1)\u03c6(V) + cU (t, u+ 1)\u03c1(U)) , (7)\nnoting that \u03c61(S) \u2264 \u03c6(S) for any set S. Uncovered case: We consider each uncovered cluster A \u2286 U separately. With probability \u03c6(A)/\u03c6, the first new center is selected from A, moving A from the uncovered to the covered set and reducing the number of uncovered clusters by one. Applying Lemma 3 for (t, u), the contribution to E[\u03c6\u2032 | \u03c6] is bounded by\n\u03c6(A) \u03c6\n[ cV(t, u) ( \u03c61(V) + \u03c61(A) ) + cU (t, u)(\u03c1(U)\u2212 \u03c1(A)) ] .\nTaking the expectation with respect to possible centers in A and using Lemma 2 and \u03c61(V) \u2264 \u03c6(V), we obtain the further bound\n\u03c6(A) \u03c6 [cV(t, u)(\u03c6(V) + \u03c1(A)) + cU (t, u)(\u03c1(U)\u2212 \u03c1(A))] .\nSumming over A \u2286 U yields\n\u03c6(U) \u03c6 (cV(t, u)\u03c6(V) + cU (t, u)\u03c1(U)) + cV(t, u)\u2212 cU (t, u) \u03c6 \u2211 A\u2286U \u03c6(A)\u03c1(A)\n\u2264 \u03c6(U) \u03c6 cV(t, u)(\u03c6(V) + \u03c1(U)), (8)\nusing the inner product bound (18). The result follows from summing (7) and (8) and combining with the trivial bound E[\u03c6\u2032 | \u03c6] \u2264 \u03c6 = \u03c6(U) + \u03c6(V).\nAs noted above, the bound in Lemma 5 depends on \u03c6(U), the potential over uncovered clusters. This quantity can be arbitrarily large or small. In the next lemma, \u03c6(U) is eliminated by maximizing with respect to it.\nLemma 6. Assume that Lemma 3 holds for (t, u) and (t, u + 1) with cV(t, u + 1) \u2265 1. Then for the case (t+ 1, u+ 1) in the sense of Lemma 5,\nE[\u03c6\u2032 | \u03c6] \u2264 1 2 cV(t, u)(\u03c6(V) + \u03c1(U)) + 1 2 max\n{ cV(t, u)(\u03c6(V) + \u03c1(U)), \u221a Q } ,\nwhere\nQ = ( cV(t, u) 2 \u2212 4cV(t, u) + 4cV(t, u+ 1) ) \u03c6(V)2\n+ 2 ( cV(t, u) 2 \u2212 2cV(t, u) + 2cU (t, u+ 1) ) \u03c6(V)\u03c1(U) + cV(t, u)2\u03c1(U)2.\nProof. The result is obtained by maximizing the bound in Lemma 5 with respect to \u03c6(U). Let B1(\u03c6(U)) and B2(\u03c6(U)) denote the two terms in the minimum. The derivative of B1(\u03c6(U)) is given by\nB\u20321(\u03c6(U)) = \u03c6(V) (\u03c6(U) + \u03c6(V))2 [ (cV(t, u)\u2212 cV(t, u+ 1))\u03c6(V) + (cV(t, u)\u2212 cU (t, u+ 1))\u03c1(U) ] ,\nwhich does not change sign as a function of \u03c6(U). The two cases B\u20321(\u03c6(U)) \u2265 0 and B\u20321(\u03c6(U)) < 0 are considered separately below. Taking the maximum of the resulting bounds (9), (10) establishes the lemma.\nCase B\u20321(\u03c6(U)) \u2265 0: Both B1(\u03c6(U)) and B2(\u03c6(U)) are non-decreasing functions of \u03c6(U). The former has the finite supremum\ncV(t, u)(\u03c6(V) + \u03c1(U)), (9)\nwhereas the latter increases without bound. Therefore B1(\u03c6(U)) eventually becomes the smaller of the two and (9) can be taken as an upper bound on min{B1(\u03c6(U)), B2(\u03c6(U))}.\nCase B\u20321(\u03c6(U)) < 0: At \u03c6(U) = 0, we have B1(0) = cV(t, u + 1)\u03c6(V) + cU (t, u + 1)\u03c1(U) and B2(0) = \u03c6(V). The assumption cV(t, u + 1) \u2265 1 implies that B1(0) \u2265 B2(0). Since B1(\u03c6(U)) is now a decreasing function, the two functions must intersect and the point of intersection then provides an upper bound on min{B1(\u03c6(U)), B2(\u03c6(U))}.\nSolving for the intersection leads after some algebra to a quadratic equation in \u03c6(U):\n0 = \u03c6(U)2 + [2\u03c6(V)\u2212 cV(t, u)(\u03c6(V) + \u03c1(U))]\u03c6(U) + \u03c6(V) (\u03c6(V)\u2212 cV(t, u+ 1)\u03c6(V)\u2212 cU (t, u+ 1)\u03c1(U)) .\nAgain by the assumption cV(t, u + 1) \u2265 1, the constant term in this quadratic equation is non-positive, implying that one of the roots is also non-positive and can be discarded. The remaining positive root is given by\n\u03c6(U) = 1 2 cV(t, u)(\u03c6(V) + \u03c1(U))\u2212 \u03c6(V) + 1 2\n\u221a Q\nafter simplifying the discriminant to match the stated expression for Q. Evaluating either B1(\u03c6(U)) or B2(\u03c6(U)) at this root gives\n1 2 cV(t, u)(\u03c6(V) + \u03c1(U)) + 1 2\n\u221a Q. (10)\nThe bound in Lemma 6 is a function of \u03c6(V) and \u03c1(U) only but is nonlinear, in contrast to the desired form in Lemma 3. The next step is to linearize the bound by imposing additional conditions (5) on the coefficients.\nLemma 7. Assume that Lemma 3 holds for (t, u) and (t, u+ 1) with coefficients satisfying (5). Then for the case (t+ 1, u+ 1) in the sense of Lemma 5,\nE[\u03c6\u2032 | \u03c6] \u2264 1 2\n[ cV(t, u) + ( cV(t, u) 2 + 4 max{cV(t, u+ 1)\u2212 cV(t, u), 0} )1/2] \u03c6(V) + cV(t, u)\u03c1(U).\nProof. It suffices to linearize the \u221a Q term in Lemma 6. In particular, we aim to bound the quadratic function Q from above by the square (a\u03c6(V) + b\u03c1(U))2 for all \u03c6(V), \u03c1(U) and some choice of a, b \u2265 0. The cases \u03c6(V) = 0 and \u03c1(U) = 0 require that\na2 \u2265 cV(t, u)2 + 4(cV(t, u+ 1)\u2212 cV(t, u)), b2 \u2265 cV(t, u)2.\nSetting these inequalities to equalities, the remaining condition for the cross-term is\nab \u2265 cV(t, u)2 + 2(cU (t, u+ 1)\u2212 cV(t, u)).\nEquivalently for a, b \u2265 0, a2b2 = ( cV(t, u) 2 + 4(cV(t, u+ 1)\u2212 cV(t, u)) ) cV(t, u) 2\n\u2265 ( cV(t, u) 2 + 2(cU (t, u+ 1)\u2212 cV(t, u)) )2 .\nWe rearrange to obtain\n4(cV(t, u+ 1)\u2212 cV(t, u))cV(t, u)2\n\u2265 4cV(t, u)2(cU (t, u+ 1)\u2212 cV(t, u)) + 4(cU (t, u+ 1)\u2212 cV(t, u))2,\n(cV(t, u+ 1)\u2212 cU (t, u+ 1))cV(t, u)2 \u2265 (cU (t, u+ 1)\u2212 cV(t, u))2, the last of which is true by assumption (5). Thus we conclude that\u221a\nQ \u2264 \u221a cV(t, u)2 + 4(cV(t, u+ 1)\u2212 cV(t, u))\u03c6(V) + cV(t, u)\u03c1(U).\nCombining this last inequality with Lemma 6 proves the result.\nGiven conditions (5) and Lemma 7, the inductive step for Lemma 3 can be completed by defining cV(t+ 1, u+ 1) and cU (t+ 1, u+ 1) recursively as in (6).\nEquations (5) and (6) provide sufficient conditions on the coefficients cV(t, u) and cU (t, u) to establish Lemma 3 by induction. Section 4.1.3 shows that these conditions are satisfied by (2). To motivate the functional form chosen in (2), we first explore the behavior of solutions that satisfy (6) in particular. This is done by treating (6) as a recursion, taking the inequalities to be equalities, and numerically evaluating cV(t+ 1, u+ 1) and cU (t+ 1, u+ 1) starting from the base cases (3) and (4) as boundary conditions. More specifically, the computation is carried out as an outer loop over increasing u starting from u + 1 = 1, and an inner loop over t starting from t = u+ 1. Figure 1 plots the resulting values for cV(t, u) over the region t \u2265 u (cU (t, u) is simply a shifted copy). The most striking feature of Figure 1 is that the level contours appear to be lines t \u221d u emanating from the origin. Sampling values at multiple points (t, u) suggests that cV(t, u) \u2248 t/(t \u2212 u). The plot also has the properties that cV(t, u) is decreasing in t for fixed u and increasing in u for fixed t. These observations lead to the functional form for cV(t, u) proposed in Section 4.1.3.\nAs for conditions (5), it can be verified directly using the base cases (3) and (4) that they are satisfied for (t, u) = (1, 0). The subsequent numerical values in Figure 1 were found to satisfy (5) for all t > u as well. This suggests that recursion (6) is self-perpetuating in the sense that if (5) are satisfied for (t, u), then the values for cV(t + 1, u + 1), cU (t + 1, u + 1) resulting from (6) will also satisfy (5) for (t + 1, u) and (t+ 1, u+ 1), i.e. points to the right and upper-right. This self-perpetuating property is not proven however in the present paper. Instead, it is shown that the proposed functional form (11) satisfies (5) directly."}, {"heading": "4.1.3 Proof with specific form for coefficients", "text": "We now prove that Lemma 3 holds for coefficients cV(t, u), cU (t, u) given by (11) below. These expressions are more general than (2) and are based on the observations drawn from Figure 1.\ncV(t, u) = t+ au+ b\nt\u2212 u+ b = 1 +\n(a+ 1)u t\u2212 u+ b , t \u2265 u, (11a)\ncU (t, u) = { cV(t\u2212 1, u\u2212 1), t \u2265 u > 0, 0, t \u2265 u = 0.\n(11b)\nHere a and b are parameters introduced to add flexibility to the basic form t/(t\u2212 u) suggested by Figure 1, subject to the constraints a > \u22121, b > 0,\na+ 1 \u2265 b, (12a) ab \u2265 1. (12b)\nEquation (2) is obtained at the end from (11) by optimizing the parameters a and b. Note that with a+1 > 0, (11a) is decreasing in t for fixed u > 0 and increasing in u for fixed t.\nGiven the inductive approach and the results established in Sections 4.1.1 and 4.1.2, the proof requires the remaining steps below. First, it is shown that the base cases (3), (4) from Section 4.1.1 imply that Lemma 3 is true for the same base cases but with cV(t, u), cU (t, u) given by (11) instead. Second, (11) is shown to satisfy conditions (5) for all t > u, thus permitting Lemma 7 to be used. Third, (11) is also shown to satisfy (6), which combined with Lemma 7 completes the induction.\nConsidering the base cases, for u = 0, (3) and (11) coincide so there is nothing to prove. For the case t = u, u \u2265 1, Lemma 3 with coefficients given by (4) implies the same with coefficients given by (11) provided that\n(1 +Hu)\u03c6(V) + (1 +Hu\u22121)\u03c1(U) \u2264 ( 1 + (a+ 1)u\nb\n) \u03c6(V) + ( 1 +\n(a+ 1)(u\u2212 1) b\n) \u03c1(U)\n\u2200 \u03c6(V), \u03c1(U).\nThis in turn is ensured if the coefficients satisfy Hu \u2264 (a+ 1)u/b for all u \u2265 1. The most stringent case is u = 1 and is met by assumption (12a).\nFor the second step of establishing (5), it is clear that (5a) is satisfied by (11a). A direct calculation presented in Appendix B shows that (5b) is also true.\nLemma 8. Condition (5b) is satisfied for all t > u if cV(t, u) and cU (t, u) are given by (11) and (12b) holds.\nSimilarly for the third step, it suffices to show that (11a) satisfies recursion (6a) since (11b) automatically satisfies (6b). A proof is provided in Appendix C.\nLemma 9. Recursion (6a) is satisfied for all t > u if cV(t, u) is given by (11a) and (12b) holds.\nHaving shown that Lemma 3 is true for coefficients given by (11) and (12), the specific expressions in (2) are obtained by minimizing cV(t, u) in (11a) with respect to a, b, subject to (12). For fixed a, minimizing with respect to b yields b = a+ 1 in light of (12a), and\ncV(t, u) = 1 + (a+ 1)u\nt\u2212 u+ (a+ 1) .\nMinimizing with respect to a then results in a(a + 1) = 1 from (12b). The solution satisfying a > \u22121 is a = \u03d5\u2212 1 and b = \u03d5."}, {"heading": "4.2 Proof of Theorem 1", "text": "Denote by nA the number of points in optimal clusterA. In the first iteration of Algorithm 1, the first cluster center is selected from some A with probability nA/n. Conditioned on this event, Lemma 3 is applied with covered set V = A, u = k \u2212 1 uncovered clusters, and t = \u03b2k \u2212 1 remaining cluster centers. This bounds the final potential \u03c6\u2032 as\nE[\u03c6\u2032 | \u03c6] \u2264 cV(\u03b2k \u2212 1, k \u2212 1)\u03c6(A) + cU (\u03b2k \u2212 1, k \u2212 1)(\u03c1\u2212 \u03c1(A))\nwhere cV(t, u), cU (t, u) are given by (2). Taking the expectation over possible centers in A and using Lemma 1,\nE[\u03c6\u2032 | A] \u2264 r(`)u cV(\u03b2k \u2212 1, k \u2212 1)\u03c6\u2217(A) + cU (\u03b2k \u2212 1, k \u2212 1)(\u03c1\u2212 \u03c1(A)).\nTaking the expectation over clusters A and recalling that \u03c1 = r(`)D \u03c6\u2217,\nE[\u03c6\u2032] \u2264 r(`)D cU (\u03b2k \u2212 1, k \u2212 1)\u03c6 \u2217 \u2212 C \u2211 A nA n \u03c6\u2217(A), (13)\nwhere C = r\n(`) D cU (\u03b2k \u2212 1, k \u2212 1)\u2212 r (`) u cV(\u03b2k \u2212 1, k \u2212 1).\nNext we aim to further bound the last term in (13). Using (2) and r(`)D = 2 `r (`) u from Lemma 2,\nC = r(`)u ( 2`cU (\u03b2k \u2212 1, k \u2212 1)\u2212 cV(\u03b2k \u2212 1, k \u2212 1) ) = r(`)u\n2` ((\u03b2 \u2212 1)k + \u03d5(k \u2212 1))\u2212 (\u03b2 \u2212 1 + \u03d5)k (\u03b2 \u2212 1)k + \u03d5\n= r(`)u (2` \u2212 1)(\u03b2 \u2212 1)k + \u03d5((2` \u2212 1)(k \u2212 1)\u2212 1)\n(\u03b2 \u2212 1)k + \u03d5 .\nThe last expression for C is seen to be non-negative for \u03b2 \u2265 1, k \u2265 2, and ` \u2265 1. Furthermore, since nA = 1 (a singleton cluster) implies that \u03c6\u2217(A) = 0, we have\u2211\nA nA\u03c6 \u2217(A) = \u2211 A:nA\u22652 nA\u03c6 \u2217(A) \u2265 2\u03c6\u2217, (14)\nwith equality if \u03c6\u2217 is completely concentrated in clusters of size 2. Substituting (2b) and (14) into (13), we obtain\nE[\u03c6\u2032] \u03c6\u2217 \u2264 r(`)D\n( 1 +\n\u03d5(k \u2212 2) (\u03b2 \u2212 1)k + \u03d5\n) \u2212 2C\nn . (15)\nThe last step is to recall Arthur and Vassilvitskii [2007, Theorems 3.1 and 5.1], which together state that\nE[\u03c6\u2032] \u03c6\u2217 \u2264 r(`)D (1 +Hk\u22121) (16)\nfor \u03c6\u2032 resulting from selecting exactly k cluster centers. In fact, (16) also holds for \u03b2k centers, \u03b2 \u2265 1, since adding centers cannot increase the potential. The proof is completed by taking the minimum of (15) and (16)."}, {"heading": "5 Conclusion and Future Work", "text": "This paper has shown that simple D` sampling algorithms, including k-means++, are guaranteed in expectation to attain a constant-factor bi-criteria approximation to an optimal clustering. The contributions herein extend and improve upon previous results concerning D` sampling [Arthur and Vassilvitskii, 2007, Aggarwal et al., 2009].\nAs noted in Section 3, the constant r(`)D in Theorem 1 and Corollary 1 represents an opportunity to further improve the approximation bounds. One possibility is to tighten Lemmas 3.2 and 5.1 in Arthur and Vassilvitskii [2007], which are the lemmas responsible for the r(`)D factor. A more significant improvement may result from considering not only the covering of optimal clusters by at least one cluster center, but also the effect of selecting more than one center from a single optimal cluster. As the number of selected centers increases, an approximation factor analogous to r(`)D would be expected to decrease. Analysis of algorithms with similar simplicity to D` sampling is also of interest."}, {"heading": "A Proof of Lemma 4", "text": "The proof follows the inductive proof of Arthur and Vassilvitskii [2007, Lemma 3.3] with the notational changes Xu \u2192 U , Xc \u2192 V , and 8\u03c6OPT \u2192 \u03c1. For brevity, only the differences are presented.\nFor the first base case t = 0, u > 0, Arthur and Vassilvitskii [2007] already show that the lemma holds with coefficients 1 = 1+H0, 0 = 1+H\u22121, and 1 = (u\u22120)/u. Similarly for the second base case t = u = 1, Arthur and Vassilvitskii [2007] show that E[\u03c6\u2032 | \u03c6] \u2264 2\u03c6(V) + \u03c1(U) = (1 +H1)\u03c6(V) + (1 +H0)\u03c1(U), as required for the stronger version here.\nFor the first \u201ccovered\u201d case considered in the inductive step, the argument is the same and the upper bound on the contribution to E[\u03c6\u2032 | \u03c6] is changed to\n\u03c6(V) \u03c6\n[ (1 +Ht\u22121)\u03c6(V) + (1 +Ht\u22122)\u03c1(U) +\nu\u2212 t+ 1 u\n\u03c6(U) ] . (17)\nFor the second \u201cuncovered\u201d case, the first displayed expression in the right-hand column of Arthur and Vassilvitskii [2007, page 1030] becomes (after applying the bound \u2211 a\u2208A pa\u03c6a \u2264 \u03c1(A) from Lemma 2)\n\u03c6(A) \u03c6\n[ (1 +Ht\u22121)(\u03c6(V) + \u03c1(A)) + (1 +Ht\u22122)(\u03c1(U)\u2212 \u03c1(A)) +\nu\u2212 t u\u2212 1\n(\u03c6(U)\u2212 \u03c6(A)) ] .\nSumming over all uncovered clusters A \u2286 U , the contribution to E[\u03c6\u2032 | \u03c6] is bounded from above by\n\u03c6(U) \u03c6\n[ (1 +Ht\u22121)\u03c6(V) + (1 +Ht\u22122)\u03c1(U) +\nu\u2212 t u\u2212 1\n\u03c6(U) ]\n+ 1\n\u03c6 (Ht\u22121 \u2212Ht\u22122) \u2211 A\u2286U \u03c6(A)\u03c1(A)\u2212 u\u2212 t u\u2212 1 \u2211 A\u2286U \u03c6(A)2  .\nThe inner product above can be bounded as\u2211 A\u2286U \u03c6(A)\u03c1(A) \u2264 \u03c6(U)\u03c1(U), (18)\nwith equality if both \u03c6(U), \u03c1(U) are completely concentrated in the same cluster A. The sum of squares term can be bounded using the power-mean inequality as in Arthur and Vassilvitskii [2007]. Hence the contribution to E[\u03c6\u2032 | \u03c6] is further bounded by\n\u03c6(U) \u03c6\n[ (1 +Ht\u22121)\u03c6(V) + (1 +Ht\u22121)\u03c1(U) +\nu\u2212 t u\n\u03c6(U) ] . (19)\nSumming the bounds in (17), (19), we have E[\u03c6\u2032 | \u03c6] \u2264 (1 +Ht\u22121)\u03c6(V) + ( 1 + \u03c6(V)Ht\u22122 + \u03c6(U)Ht\u22121\n\u03c6\n) \u03c1(U) + u\u2212 t\nu \u03c6(U) + \u03c6(V) \u03c6 \u03c6(U) u .\nRecalling that \u03c6 = \u03c6(V) +\u03c6(U), the right-hand side is seen to be increasing in \u03c6(U). Taking the worst case as \u03c6(U)\u2192 \u03c6 gives\nE[\u03c6\u2032 | \u03c6] \u2264 ( 1 +Ht\u22121 + 1\nu\n) \u03c6(V) + (1 +Ht\u22121)\u03c1(U) +\nu\u2212 t u \u03c6(U)\n\u2264 (1 +Ht)\u03c6(V) + (1 +Ht\u22121)\u03c1(U) + u\u2212 t u \u03c6(U)\nsince 1/u \u2264 1/t. This completes the induction."}, {"heading": "B Proof of Lemma 8", "text": "Substituting (11) into the left-most factor in (5b),\ncV(t, u+ 1)\u2212 cU (t, u+ 1) = cV(t, u+ 1)\u2212 cV(t\u2212 1, u)\n= (a+ 1)(u+ 1) t\u2212 u\u2212 1 + b \u2212 (a+ 1)u t\u2212 1\u2212 u+ b = a+ 1\nt\u2212 u\u2212 1 + b .\nSimilarly on the right-hand side of (5b),\ncU (t, u+ 1)\u2212 cV(t, u) = cV(t\u2212 1, u)\u2212 cV(t, u)\n= (a+ 1)u t\u2212 1\u2212 u+ b \u2212 (a+ 1)u t\u2212 u+ b\n= (a+ 1)u\n(t\u2212 u+ b)(t\u2212 u\u2212 1 + b) .\nHence\n(cV(t, u+ 1)\u2212 cU (t, u+ 1))cV(t, u)2 \u2212 (cU (t, u+ 1)\u2212 cV(t, u))2\n= a+ 1\nt\u2212 u\u2212 1 + b\n( 1 + 2 (a+ 1)u\nt\u2212 u+ b +\n(a+ 1)2u2\n(t\u2212 u+ b)2\n) \u2212 (a+ 1) 2u2\n(t\u2212 u+ b)2(t\u2212 u\u2212 1 + b)2\n= a+ 1\nt\u2212 u\u2212 1 + b\n( 1 + 2 (a+ 1)u\nt\u2212 u+ b\n) +\n(a+ 1)2u2 [(a+ 1)(t\u2212 u\u2212 1 + b)\u2212 1] (t\u2212 u+ b)2(t\u2212 u\u2212 1 + b)2 . (20)\nThe first of the two summands in (20) is positive for t > u \u2265 0. The second summand is also non-negative as long as (a+ 1)(t\u2212u\u2212 1 + b) \u2265 1. The most stringent case occurs for t = u+ 1 and is implied by (12b). We conclude that (20) is positive, i.e. (5b) holds."}, {"heading": "C Proof of Lemma 9", "text": "As noted earlier, (11a) has the property that cV(t, u+ 1) \u2265 cV(t, u) for all t, u. Therefore (6a) is equivalent to\n2cV(t+ 1, u+ 1)\u2212 cV(t, u) \u2265 \u221a cV(t, u)2 + 4(cV(t, u+ 1)\u2212 cV(t, u)). (21)\nSubstituting (11a) into the left-hand side,\n2cV(t+ 1, u+ 1)\u2212 cV(t, u) = 1 + 2 (a+ 1)(u+ 1) t\u2212 u+ b \u2212 (a+ 1)u t\u2212 u+ b\n= 1 + (a+ 1)(u+ 2)\nt\u2212 u+ b ,\nwhich is seen to be positive for t > u \u2265 0. Hence (21) is in turn equivalent to\n(2cV(t+ 1, u+ 1)\u2212 cV(t, u))2 \u2265 cV(t, u)2 + 4(cV(t, u+ 1)\u2212 cV(t, u)).\nOn the left-hand side,\n(2cV(t+ 1, u+ 1)\u2212 cV(t, u))2 = 1 + 2 (a+ 1)(u+ 2)\nt\u2212 u+ b +\n(a+ 1)2(u+ 2)2\n(t\u2212 u+ b)2 . (22)\nOn the right-hand side,\ncV(t, u+ 1)\u2212 cV(t, u) = (a+ 1)(u+ 1) t\u2212 u\u2212 1 + b \u2212 (a+ 1)u t\u2212 u+ b\n= (a+ 1)(t+ b)\n(t\u2212 u+ b)(t\u2212 u\u2212 1 + b)\n= a+ 1\nt\u2212 u+ b\n( 1 +\nu+ 1\nt\u2212 u\u2212 1 + b\n) ,\ncV(t, u) 2 = 1 + 2\n(a+ 1)u t\u2212 u+ b +\n(a+ 1)2u2\n(t\u2212 u+ b)2 ,\ncV(t, u) 2 + 4(cV(t, u+ 1)\u2212 cV(t, u))\n= 1 + 2 (a+ 1)(u+ 2)\nt\u2212 u+ b +\n(a+ 1)2u2\n(t\u2212 u+ b)2 + 4\n(a+ 1)(u+ 1)\n(t\u2212 u+ b)(t\u2212 u\u2212 1 + b) . (23)\nSubtracting (23) from (22) yields\n4(a+ 1)2(u+ 1)\n(t\u2212 u+ b)2 \u2212 4 (a+ 1)(u+ 1) (t\u2212 u+ b)(t\u2212 u\u2212 1 + b)\n= 4 (a+ 1)(u+ 1) [a(t\u2212 u\u2212 1 + b)\u2212 1]\n(t\u2212 u+ b)2(t\u2212 u\u2212 1 + b) ,\nwhich is non-negative provided that a(t\u2212 u\u2212 1 + b) \u2265 1. As in the proof of Lemma 8, the most stringent case occurs for t = u+ 1 and is covered by (12b). We conclude that (22) is at least as large as (23), i.e. (6a) holds."}], "references": [{"title": "Adaptive sampling for k-means clustering", "author": ["A. Aggarwal", "A. Deshpande", "R. Kannan"], "venue": "In Proceedings of the 12th International Workshop and 13th International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques,", "citeRegEx": "Aggarwal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Aggarwal et al\\.", "year": 2009}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ailon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2009}, {"title": "NP-hardness of Euclidean sum-of-squares clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Machine Learning,", "citeRegEx": "Aloise et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Aloise et al\\.", "year": 2009}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In Proceedings of the 18th ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Arthur and Vassilvitskii.,? \\Q2007\\E", "shortCiteRegEx": "Arthur and Vassilvitskii.", "year": 2007}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["V. Arya", "N. Garg", "R. Khandekar", "A. Meyerson", "K. Munagala", "V. Pandit"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Arya et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Arya et al\\.", "year": 2004}, {"title": "The hardness of approximation of Euclidean kmeans", "author": ["P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A.K. Sinop"], "venue": "In Proceedings of the 31st International Symposium on Computational Geometry,", "citeRegEx": "Awasthi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2015}, {"title": "Approximate clustering via core-sets", "author": ["M. Badoiu", "S. Har-Peled", "P. Indyk"], "venue": "In Proceedings of the 34th ACM Symposium on Theory of Computing,", "citeRegEx": "Badoiu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Badoiu et al\\.", "year": 2002}, {"title": "A tight lower bound instance for k-means++ in constant dimension, volume 8402 of Lecture Notes in Computer Science, pages 7\u201322", "author": ["A. Bhattacharya", "R. Jaiswal", "N. Ailon"], "venue": null, "citeRegEx": "Bhattacharya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bhattacharya et al\\.", "year": 2014}, {"title": "A bad instance for k-means++", "author": ["T. Brunsch", "H. R\u00f6glin"], "venue": "Theoretical Computer Science,", "citeRegEx": "Brunsch and R\u00f6glin.,? \\Q2013\\E", "shortCiteRegEx": "Brunsch and R\u00f6glin.", "year": 2013}, {"title": "A constant-factor approximation algorithm for the k-median problem", "author": ["M. Charikar", "S. Guha", "E. Tardos", "D.B. Shmoys"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Charikar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2002}, {"title": "On coresets for k-median and k-means clustering in metric and Euclidean spaces and their applications", "author": ["K. Chen"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Chen.,? \\Q2009\\E", "shortCiteRegEx": "Chen.", "year": 2009}, {"title": "The hardness of k-means clustering", "author": ["S. Dasgupta"], "venue": "Technical Report CS2008-0916, Department of Computer Science and Engineering,", "citeRegEx": "Dasgupta.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta.", "year": 2008}, {"title": "Approximation schemes for clustering problems", "author": ["W.F. de la Vega", "M. Karpinski", "C. Kenyon", "Y. Rabani"], "venue": "In Proceedings of the 35th ACM Symposium on Theory of Computing,", "citeRegEx": "Vega et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Vega et al\\.", "year": 2003}, {"title": "A PTAS for k-means clustering based on weak coresets", "author": ["D. Feldman", "M. Monemizadeh", "C. Sohler"], "venue": "In Proceedings of the 23rd International Symposium on Computational Geometry,", "citeRegEx": "Feldman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2007}, {"title": "Clustering data streams: Theory and practice", "author": ["S. Guha", "A. Meyerson", "N. Mishra", "R. Motwani", "L. O\u2019Callaghan"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Guha et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guha et al\\.", "year": 2003}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "In Proceedings of the 36th ACM Symposium on Theory of Computing,", "citeRegEx": "Har.Peled and Mazumdar.,? \\Q2004\\E", "shortCiteRegEx": "Har.Peled and Mazumdar.", "year": 2004}, {"title": "Applications of weighted Voronoi diagrams and randomization to variancebased k-clustering", "author": ["M. Inaba", "N. Katoh", "H. Imai"], "venue": "In Proceedings of the 10th International Symposium on Computational Geometry,", "citeRegEx": "Inaba et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Inaba et al\\.", "year": 1994}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Jain.,? \\Q2010\\E", "shortCiteRegEx": "Jain.", "year": 2010}, {"title": "Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and Lagrangian relaxation", "author": ["K. Jain", "V.V. Vazirani"], "venue": "Journal of the ACM,", "citeRegEx": "Jain and Vazirani.,? \\Q2001\\E", "shortCiteRegEx": "Jain and Vazirani.", "year": 2001}, {"title": "A new greedy approach for facility location problems", "author": ["K. Jain", "M. Mahdian", "A. Saberi"], "venue": "In Proceedings of the 34th ACM Symposium on Theory of Computing,", "citeRegEx": "Jain et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2002}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Computational Geometry,", "citeRegEx": "Kanungo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kanungo et al\\.", "year": 2004}, {"title": "Linear-time approximation schemes for clustering problems in any dimensions", "author": ["A. Kumar", "Y. Sabharwal", "S. Sen"], "venue": "Journal of the ACM,", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Least squares quantization in PCM", "author": ["S. Lloyd"], "venue": "Technical report, Bell Laboratories,", "citeRegEx": "Lloyd.,? \\Q1957\\E", "shortCiteRegEx": "Lloyd.", "year": 1957}, {"title": "Least squares quantization in PCM", "author": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "The planar k-means problem is NP-hard", "author": ["M. Mahajan", "P. Nimbhorkar", "K. Varadarajan"], "venue": "In Proceedings of the 3rd International Workshop on Algorithms and Computation,", "citeRegEx": "Mahajan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mahajan et al\\.", "year": 2009}, {"title": "A bi-criteria approximation algorithm for k means", "author": ["K. Makarychev", "Y. Makarychev", "M. Sviridenko", "J. Ward"], "venue": "Technical Report arXiv:1507.04227,", "citeRegEx": "Makarychev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Makarychev et al\\.", "year": 2015}, {"title": "On approximate geometric k-clustering", "author": ["J. Matou\u0161ek"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "Matou\u0161ek.,? \\Q2000\\E", "shortCiteRegEx": "Matou\u0161ek.", "year": 2000}, {"title": "Optimal time bounds for approximate clustering", "author": ["R.R. Mettu", "C.G. Plaxton"], "venue": "Machine Learning,", "citeRegEx": "Mettu and Plaxton.,? \\Q2004\\E", "shortCiteRegEx": "Mettu and Plaxton.", "year": 2004}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "author": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "Journal of the ACM,", "citeRegEx": "Ostrovsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ostrovsky et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "The k-means problem and its variants constitute one of the most popular paradigms for clustering [Jain, 2010].", "startOffset": 97, "endOffset": 109}, {"referenceID": 19, "context": ", 2015] and k-medians (` = 1) [Jain et al., 2002].", "startOffset": 30, "endOffset": 49}, {"referenceID": 3, "context": "In this paper, we study an enhancement to Lloyd\u2019s algorithm known as k-means++ [Arthur and Vassilvitskii, 2007] and the more general class of D` sampling algorithms to which k-means++ belongs.", "startOffset": 79, "endOffset": 111}, {"referenceID": 3, "context": "D` sampling is attractive for two reasons: First, it is guaranteed to yield an expected O(log k) approximation to the optimal clustering with k centers [Arthur and Vassilvitskii, 2007].", "startOffset": 152, "endOffset": 184}, {"referenceID": 2, "context": "Optimal clustering in this sense is known to be NP-hard, in particular for k-means (` = 2) [Dasgupta, 2008, Aloise et al., 2009, Mahajan et al., 2009, Awasthi et al., 2015] and k-medians (` = 1) [Jain et al., 2002]. In practice, the most widely used algorithm remains Lloyd\u2019s [1957, 1982] (often referred to as the k-means algorithm), which alternates between updating centers given cluster assignments and re-assigning points to clusters. In this paper, we study an enhancement to Lloyd\u2019s algorithm known as k-means++ [Arthur and Vassilvitskii, 2007] and the more general class of D` sampling algorithms to which k-means++ belongs. These algorithms select cluster centers randomly from the given data points with probabilities proportional to their current costs. The clustering can then be refined using Lloyd\u2019s algorithm. D` sampling is attractive for two reasons: First, it is guaranteed to yield an expected O(log k) approximation to the optimal clustering with k centers [Arthur and Vassilvitskii, 2007]. Second, it is as simple as Lloyd\u2019s algorithm, both conceptually as well as computationally with O(nkd) running time in d dimensions. The particular focus of this paper is on the setting where an optimal k-clustering remains the benchmark but more than k cluster centers can be sampled to improve the approximation. Specifically, it is shown (see Theorem 1 and Corollary 1) that for any constant factor \u03b2 > 1, if \u03b2k centers are chosen byD` sampling, then a constant-factor approximation to the optimal k-clustering is obtained. This guarantee holds in expectation and for all datasets, like the one in Arthur and Vassilvitskii [2007], and improves upon the O(log k) factor therein.", "startOffset": 108, "endOffset": 1644}, {"referenceID": 0, "context": "Furthermore, if a solution with a specified number of clusters k is truly required, then linear programming techniques can be used to select a k-subset from the \u03b2k cluster centers while still maintaining a constant-factor approximation [Aggarwal et al., 2009, Charikar et al., 2002]. The main result in this paper differs from the constant-factor bi-criteria approximation established in Aggarwal et al. [2009] in that the latter holds only with constant probability as opposed to in expectation.", "startOffset": 237, "endOffset": 411}, {"referenceID": 0, "context": "Furthermore, if a solution with a specified number of clusters k is truly required, then linear programming techniques can be used to select a k-subset from the \u03b2k cluster centers while still maintaining a constant-factor approximation [Aggarwal et al., 2009, Charikar et al., 2002]. The main result in this paper differs from the constant-factor bi-criteria approximation established in Aggarwal et al. [2009] in that the latter holds only with constant probability as opposed to in expectation. Using Markov\u2019s inequality, a constant-probability corollary can be derived from Theorem 1 herein, and doing so improves upon the approximation factor of Aggarwal et al. [2009] by more than a factor of 2.", "startOffset": 237, "endOffset": 673}, {"referenceID": 0, "context": "Furthermore, if a solution with a specified number of clusters k is truly required, then linear programming techniques can be used to select a k-subset from the \u03b2k cluster centers while still maintaining a constant-factor approximation [Aggarwal et al., 2009, Charikar et al., 2002]. The main result in this paper differs from the constant-factor bi-criteria approximation established in Aggarwal et al. [2009] in that the latter holds only with constant probability as opposed to in expectation. Using Markov\u2019s inequality, a constant-probability corollary can be derived from Theorem 1 herein, and doing so improves upon the approximation factor of Aggarwal et al. [2009] by more than a factor of 2. The present paper also differs from recent work on more general bi-criteria approximation of k-means by Makarychev et al. [2015], which analyzes substantially more complex algorithms.", "startOffset": 237, "endOffset": 830}, {"referenceID": 16, "context": "At one end, exact algorithms [Inaba et al., 1994] and several polynomial-time approximation schemes (PTAS) [Matou\u0161ek, 2000, Badoiu et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 26, "context": "[2004] also rely on a discretization to an -approximate centroid set [Matou\u0161ek, 2000] of size O(n \u2212d log(1/ )).", "startOffset": 69, "endOffset": 85}, {"referenceID": 14, "context": "A constant-factor approximation with slightly superlinear running time has also been obtained in the streaming setting [Guha et al., 2003].", "startOffset": 119, "endOffset": 138}, {"referenceID": 4, "context": ", 1994] and several polynomial-time approximation schemes (PTAS) [Matou\u0161ek, 2000, Badoiu et al., 2002, de la Vega et al., 2003, Har-Peled and Mazumdar, 2004, Kumar et al., 2010, Chen, 2009, Feldman et al., 2007, Jaiswal et al., 2014] have been proposed for k-means and k-medians. While these have polynomial running times in n, the dependence on k and sometimes on the dimension d is exponential or worse. A simpler local search algorithm was shown to yield a ((3+2/p)`+ ) approximation for k-means (` = 2) in Kanungo et al. [2004] and k-medians (` = 1) in Arya et al.", "startOffset": 82, "endOffset": 532}, {"referenceID": 3, "context": "[2004] and k-medians (` = 1) in Arya et al. [2004], the latter under the additional constraint that centers are chosen from a finite set.", "startOffset": 32, "endOffset": 51}, {"referenceID": 3, "context": "[2004] and k-medians (` = 1) in Arya et al. [2004], the latter under the additional constraint that centers are chosen from a finite set. This local search however requires a polynomial number of iterations of complexity nO(p), and Kanungo et al. [2004] also rely on a discretization to an -approximate centroid set [Matou\u0161ek, 2000] of size O(n \u2212d log(1/ )).", "startOffset": 32, "endOffset": 254}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets.", "startOffset": 3, "endOffset": 35}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime.", "startOffset": 3, "endOffset": 203}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime. Arthur and Vassilvitskii [2007] also provided a matching lower bound, exhibiting a dataset on which k-means++ achieves an expected \u03a9(log k) approximation.", "startOffset": 3, "endOffset": 302}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime. Arthur and Vassilvitskii [2007] also provided a matching lower bound, exhibiting a dataset on which k-means++ achieves an expected \u03a9(log k) approximation. Sampling algorithms have been shown to yield improved O(1) approximation factors provided that the dataset satisfies certain conditions. Such a result was established in Ostrovsky et al. [2012] for k-means++ and other variants of Lloyd\u2019s algorithm under the condition that the dataset is well-suited in a sense to partitioning into k clusters.", "startOffset": 3, "endOffset": 619}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime. Arthur and Vassilvitskii [2007] also provided a matching lower bound, exhibiting a dataset on which k-means++ achieves an expected \u03a9(log k) approximation. Sampling algorithms have been shown to yield improved O(1) approximation factors provided that the dataset satisfies certain conditions. Such a result was established in Ostrovsky et al. [2012] for k-means++ and other variants of Lloyd\u2019s algorithm under the condition that the dataset is well-suited in a sense to partitioning into k clusters. In Mettu and Plaxton [2004], anO(1) approximation was shown for a somewhat more complicated algorithm called successive sampling with O(n(k + log n) + k2 log n) running time, subject to a bound on the dispersion of the points.", "startOffset": 3, "endOffset": 797}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime. Arthur and Vassilvitskii [2007] also provided a matching lower bound, exhibiting a dataset on which k-means++ achieves an expected \u03a9(log k) approximation. Sampling algorithms have been shown to yield improved O(1) approximation factors provided that the dataset satisfies certain conditions. Such a result was established in Ostrovsky et al. [2012] for k-means++ and other variants of Lloyd\u2019s algorithm under the condition that the dataset is well-suited in a sense to partitioning into k clusters. In Mettu and Plaxton [2004], anO(1) approximation was shown for a somewhat more complicated algorithm called successive sampling with O(n(k + log n) + k2 log n) running time, subject to a bound on the dispersion of the points. A constant-factor approximation with slightly superlinear running time has also been obtained in the streaming setting [Guha et al., 2003]. For k-means++, the \u03a9(log k) lower bound in Arthur and Vassilvitskii [2007], which holds in expectation, has spurred follow-on works on the question of whether k-means++ might guarantee a constant-factor approximation with reasonably large probability.", "startOffset": 3, "endOffset": 1211}, {"referenceID": 4, "context": "[2013], who showed that an approximation factor better than (2/3) log k cannot be achieved with probability higher than a decaying exponential in k, and Bhattacharya et al. [2014], who showed that a similar statement holds even in 2 dimensions.", "startOffset": 153, "endOffset": 180}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability.", "startOffset": 63, "endOffset": 86}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k).", "startOffset": 63, "endOffset": 522}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation.", "startOffset": 63, "endOffset": 614}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al.", "startOffset": 63, "endOffset": 847}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means.", "startOffset": 63, "endOffset": 883}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means. Their work differs from the present paper in studying more complex algorithms. First, similar to Kanungo et al. [2004], Makarychev et al.", "startOffset": 63, "endOffset": 1072}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means. Their work differs from the present paper in studying more complex algorithms. First, similar to Kanungo et al. [2004], Makarychev et al. [2015] reduce the k-means problem to an -approximate, finite-set instance of k-medians of size nO(log(1/ )/ 2).", "startOffset": 63, "endOffset": 1098}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means. Their work differs from the present paper in studying more complex algorithms. First, similar to Kanungo et al. [2004], Makarychev et al. [2015] reduce the k-means problem to an -approximate, finite-set instance of k-medians of size nO(log(1/ )/ 2). Subsequently, linear programming and local search algorithms are considered, the latter the same as in Kanungo et al. [2004], Arya et al.", "startOffset": 63, "endOffset": 1328}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means. Their work differs from the present paper in studying more complex algorithms. First, similar to Kanungo et al. [2004], Makarychev et al. [2015] reduce the k-means problem to an -approximate, finite-set instance of k-medians of size nO(log(1/ )/ 2). Subsequently, linear programming and local search algorithms are considered, the latter the same as in Kanungo et al. [2004], Arya et al. [2004], and both with polynomial complexity in the size of the k-medians instance.", "startOffset": 63, "endOffset": 1348}, {"referenceID": 3, "context": "Following Arthur and Vassilvitskii [2007], the case ` = 2 is referred to as the k-means++ algorithm and the probabilities used after the first iteration are referred to as D2 weighting (hence D` in general).", "startOffset": 10, "endOffset": 42}, {"referenceID": 3, "context": "3 Existing Lemmas Regarding D Sampling The following lemmas synthesize results from Arthur and Vassilvitskii [2007] that bound the expected potential within a single optimal cluster due to selecting a center from that cluster with uniform or D` weighting, as in Algorithm 1.", "startOffset": 84, "endOffset": 116}, {"referenceID": 3, "context": "[Arthur and Vassilvitskii, 2007, Lemma 3.2] Given an optimal cluster A and an initial potential \u03c6, let \u03c6\u2032 be the potential resulting from adding a cluster center selected randomly fromA withD` weighting. Then E[\u03c6\u2032(A)] \u2264 r D \u03c6\u2217(A) for any A, where r (`) D = 2 `r (`) u . The factor of 2` between r u and r (`) D for general ` is explained just before Theorem 5.1 in Arthur and Vassilvitskii [2007].", "startOffset": 1, "endOffset": 397}, {"referenceID": 3, "context": "1 Comparisons to Existing Results A comparison of Theorem 1 to results in Arthur and Vassilvitskii [2007] is implicit in its statement since the Hk\u22121 term in the minimum comes directly from Arthur and Vassilvitskii [2007, Theorems 3.", "startOffset": 74, "endOffset": 106}, {"referenceID": 3, "context": "1 Comparisons to Existing Results A comparison of Theorem 1 to results in Arthur and Vassilvitskii [2007] is implicit in its statement since the Hk\u22121 term in the minimum comes directly from Arthur and Vassilvitskii [2007, Theorems 3.1 and 5.1]. For k = 2, 3, the first term in the minimum is smaller than Hk\u22121 for any \u03b2 \u2265 1, and hence Theorem 1 is always an improvement. For k > 3, Theorem 1 improves upon Arthur and Vassilvitskii [2007] for \u03b2 greater than the critical value \u03b2c = 1 + \u03c6(k \u2212 2\u2212Hk\u22121) kHk\u22121 .", "startOffset": 74, "endOffset": 438}, {"referenceID": 2, "context": "It can be concluded that for any k, at most 20% oversampling is required for Theorem 1 to guarantee a better approximation than Arthur and Vassilvitskii [2007]. The most closely related result to Theorem 1 and Corollary 1 is found in Aggarwal et al.", "startOffset": 128, "endOffset": 160}, {"referenceID": 0, "context": "The most closely related result to Theorem 1 and Corollary 1 is found in Aggarwal et al. [2009, Theorem 1]. The latter establishes a constant-factor bi-criteria approximation that holds with constant probability, as opposed to in expectation. Since a bound on the expectation implies a bound with constant probability via Markov\u2019s inequality, a direct comparison with Aggarwal et al. [2009] is possible.", "startOffset": 73, "endOffset": 391}, {"referenceID": 0, "context": "The most closely related result to Theorem 1 and Corollary 1 is found in Aggarwal et al. [2009, Theorem 1]. The latter establishes a constant-factor bi-criteria approximation that holds with constant probability, as opposed to in expectation. Since a bound on the expectation implies a bound with constant probability via Markov\u2019s inequality, a direct comparison with Aggarwal et al. [2009] is possible. Specifically, for ` = 2 and the t = d16(k + \u221a k)e cluster centers assumed in Aggarwal et al. [2009], Theorem 1 in the present work implies that E[\u03c6] \u03c6\u2217 \u2264 8 ( 1 + min { \u03c6(k \u2212 2) d15k + 16 \u221a ke+ \u03c6 ,Hk\u22121 })", "startOffset": 73, "endOffset": 504}, {"referenceID": 0, "context": "03 as in Aggarwal et al. [2009]. This 9.", "startOffset": 9, "endOffset": 32}, {"referenceID": 0, "context": "03 as in Aggarwal et al. [2009]. This 9.137 approximation factor is less than half the factor of 20 in Aggarwal et al. [2009].", "startOffset": 9, "endOffset": 126}, {"referenceID": 25, "context": "Corollary 1 may also be compared to the results in Makarychev et al. [2015], although it should be re-emphasized that the latter analyzes different, substantially more complex algorithms, with running time at least nO(log(1/ )/ 2) for reasonably small .", "startOffset": 51, "endOffset": 76}, {"referenceID": 25, "context": "Corollary 1 may also be compared to the results in Makarychev et al. [2015], although it should be re-emphasized that the latter analyzes different, substantially more complex algorithms, with running time at least nO(log(1/ )/ 2) for reasonably small . The main difference between Corollary 1 and the bounds in Makarychev et al. [2015] is the extra factor of r D since the factor of 1 + \u03c6/(\u03b2 \u2212 1) is comparable, at least for moderate values of \u03b2 that are of practical interest.", "startOffset": 51, "endOffset": 337}, {"referenceID": 3, "context": "The overall strategy used to prove Theorem 1 is similar to that in Arthur and Vassilvitskii [2007]. The key intermediate result is Lemma 3 below, which relates the potential at a later iteration in Algorithm 1 to the potential at an earlier iteration.", "startOffset": 67, "endOffset": 99}, {"referenceID": 0, "context": "The contributions herein extend and improve upon previous results concerning D` sampling [Arthur and Vassilvitskii, 2007, Aggarwal et al., 2009]. As noted in Section 3, the constant r D in Theorem 1 and Corollary 1 represents an opportunity to further improve the approximation bounds. One possibility is to tighten Lemmas 3.2 and 5.1 in Arthur and Vassilvitskii [2007], which are the lemmas responsible for the r D factor.", "startOffset": 122, "endOffset": 370}], "year": 2016, "abstractText": "This paper studies the k-means++ algorithm for clustering as well as the class of D sampling algorithms to which k-means++ belongs. It is shown that for any constant factor \u03b2 > 1, selecting \u03b2k cluster centers by D sampling yields a constant-factor approximation to the optimal clustering with k centers, in expectation and without conditions on the dataset. This result extends the previously known O(log k) guarantee for the case \u03b2 = 1 to the constant-factor bi-criteria regime. It also improves upon an existing constant-factor bi-criteria result that holds only with constant probability.", "creator": "LaTeX with hyperref package"}}}