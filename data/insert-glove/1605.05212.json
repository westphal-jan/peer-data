{"id": "1605.05212", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Multimodal Sparse Coding for Event Detection", "abstract": "countout Unsupervised owarai feature itinerary learning 100.93 methods joksimovic have flagpoles proven swamped effective lamberg for classification hery tasks based wkbw on a trisakti single modality. We present multimodal sparse naosuke coding chenrezig for learning feature husserl representations circumferences shared across grouper multiple modalities. The 17.19 shared representations wivina are 49.35 applied to huffingtons multimedia ikos event detection (MED) cresent and evaluated in comparison babbs to unimodal counterparts, amaryllidaceae as zadneprovskis well as vedam other countrywomen feature out-and-out learning methods such as h\u0113 GMM supervectors lunkhead and sparse RBM. darion We report grav the cross - 2,996 validated classification ampicillin accuracy and mean refinish average activisits precision of robocalls the MED system historiographer trained on biggin features learned 2011 from our imola unimodal amparano and guianan multimodal uwfi settings agur for ludde a subset of the katana TRECVID 33.07 MED 905 2014 hentgen dataset.", "histories": [["v1", "Tue, 17 May 2016 15:37:19 GMT  (450kb,D)", "http://arxiv.org/abs/1605.05212v1", "Multimodal Machine Learning Workshop at NIPS 2015"]], "COMMENTS": "Multimodal Machine Learning Workshop at NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["youngjune gwon", "william campbell", "kevin brady", "douglas sturim", "miriam cha", "h t kung"], "accepted": false, "id": "1605.05212"}, "pdf": {"name": "1605.05212.pdf", "metadata": {"source": "CRF", "title": "Multimodal Sparse Coding for Event Detection", "authors": ["Youngjune Gwon", "William M. Campbell", "Kevin Brady", "Douglas Sturim", "Miriam Cha H. T. Kung"], "emails": [], "sections": [{"heading": null, "text": "Unsupervised feature learning methods have proven effective for classification tasks based on a single modality. We present multimodal sparse coding for learning feature representations shared across multiple modalities. The shared representations are applied to multimedia event detection (MED) and evaluated in comparison to unimodal counterparts, as well as other feature learning methods such as GMM supervectors and sparse RBM. We report the cross-validated classification accuracy and mean average precision of the MED system trained on features learned from our unimodal and multimodal settings for a subset of the TRECVID MED 2014 dataset."}, {"heading": "1 Introduction", "text": "Multimedia Event Detection (MED) aims to identify complex activities occurring at a specific place and time involving various interactions of human actions and objects. MED is considered more difficult than concept analysis such as action recognition and has received significant attention in computer vision and machine learning research. In this paper, we propose the use of sparse coding for multimodal feature learning in the context of MED. Originally proposed to explain neurons encoding sensory information [7], sparse coding provides an unsupervised method to learn basis vectors for efficient data representation. More recently, sparse coding has been used to model the relationship between correlated data sources. By jointly training dictionaries with audio and video tracks from the same multimedia clip, we can force the two modalities to share a similar sparse representation whose benefit includes robust detection and cross-modality retrieval.\nIn the next section, we will describe audio-video feature learning in various unimodal and multimodal settings for sparse coding. We then present our experiments with TRECVID MED dataset. We will discuss the empirical results, compare them to other methods, and conclude."}, {"heading": "2 Audio-video Feature Learning", "text": "In summary, our approach is to build feature vectors by sparse coding on the low-level audio and video features. Multiple feature vectors (i.e., sparse codes) are aggregated via max pooling. The resulting pooled feature vectors can scale to file level, and we use them to train an array of classifiers for MED.\n*This work was sponsored by the Department of Defense under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the United States Government.\nar X\niv :1\n60 5.\n05 21\n2v 1\n[ cs\n.L G\n] 1\n7 M\nay 2"}, {"heading": "2.1 Low-level feature extraction and preprocessing", "text": "We begin by locating the keyframes of a given multimedia clip. We apply a simple two-pass algorithm that computes color histogram difference of any two successive frames and determines a keyframe candidate based on the threshold calculated on the mean and standard deviation of the histogram differences. We examine the number of different colors present in the keyframe candidates and discard the ones with less than 26 colors. This ensures that our keyframes are not all-black or all-white blank images.\nAround each keyframe, we extract 5-sec audio data and additional 10 uniformly sampled video frames within the duration as illustrated in Figure 1a. If extracted audio is stereo, we take only the left channel. The audio waveform is resampled to 22.05 kHz and regularized by the time-frequency automatic gain control (TF-AGC) to balance the energy in sub-bands. We form audio frames using a 46-msec Hann window with 50% overlap between successive frames for smoothing. For each frame, we compute 16 the Mel-frequency cepstral coefficients (MFCCs) as the low-level audio feature. In addition, we append 16 delta cepstral and 16 delta-delta cepstral coefficients, which make our lowlevel audio feature vectors 48 dimensional. Finally, we apply PCA whitening before unsupervised learning. The complete audio preprocessing steps are described in Figure 1b.\nFor video preprocessing, we take a deep learning approach. We have tried out pretrained convolutional neural network (CNN) models and ended up choosing VGG ILSVRC 19 layers, the 19- layer model by University of Oxford\u2019s Visual Geometry Group (VGG) [9] for the ImageNet Largescale Visual Recognition Challenge (ILSVRC). As depicted in Figure 1c, we run the CNN feedforward passes with the extracted video frames. For each video frame, we take 4,096-dimensional hidden activation from fc7, the highest hidden layer before the final ReLU (i.e., the rectification non-linearity). By PCA whitening, we reduce the dimensionality to 128."}, {"heading": "2.2 High-level feature modeling via sparse coding", "text": "We use sparse coding to model high-level features that can train classifiers for event detection.\nUnimodal feature learning. A straightforward approach for sparse coding with two heterogeneous data modalities is to learn a separate dictionary of basis vectors for each modality. Figure 2 depicts unimodal sparse coding schemes. Recall the preprocessed audio and video input vectors xA and xV. Audio-only sparse coding is done by\nmin DA,y (i) A nA\u2211 i=1 \u2016x(i)A \u2212DAy (i) A \u2016 2 2 + \u03bb\u2016y (i) A \u20161 (1)\nwhere we feed nA unlabeled audio examples to simultaneously learn the unimodal dictionary DA and sparse codes y(i)A under the sparsity regularization parameter \u03bb. (We denote x (i) A the ith training example for audio.) Similarly, using nV unlabeled video examples, we learn\nmin DV,y (i) V nV\u2211 i=1 \u2016x(i)V \u2212DVy (i) V \u2016 2 2 + \u03bb\u2016y (i) V \u20161. (2)\nWe can form yA+V = [yA yV]>, a union of the audio and video feature vectors from unimodal sparse coding illustrated in Figure 2c.\nMultimodal feature learning. The feature union yA+V encapsulates both audio and video sparse codes. However, the training is done in a parallel, unimodal fashion such that sparse coding dictionary for each modality is learned independently of the other. To remedy the lack of joint learning, we propose a multimodal sparse coding scheme described in Figure 3a. We use the joint sparse coding technique used in image super-resolution [11]\nmin DAV,y (i) AV n\u2211 i=1 \u2016x(i)AV \u2212DAVy (i) AV\u2016 2 2 + \u03bb \u2032\u2016y(i)AV\u20161. (3)\nHere, we feed the concatenated audio-video input vector x(i)AV = [ 1\u221a NA x (i) A 1\u221a NV x (i) V ] >, where NA and NV are dimensionalities of xA and xV, respectively. As an interesting property, we can decompose the jointly learned dictionary DAV = [ 1\u221aNADAV\u2212A 1\u221a NV DAV\u2212V] > to perform the following audio-only and video-only sparse coding\nmin DAV\u2212A,y (i) AV\u2212A nA\u2211 i=1 \u2016x(i)AV\u2212A \u2212DAV\u2212Ay (i) AV\u2212A\u2016 2 2 + \u03bb \u2032\u2032\u2016y(i)AV\u2212A\u20161, (4)\nmin DAV\u2212V,y (i) AV\u2212V nV\u2211 i=1 \u2016x(i)AV\u2212V \u2212DAV\u2212Vy (i) AV\u2212V\u2016 2 2 + \u03bb \u2032\u2032\u2016y(i)AV\u2212V\u20161. (5)\nIn principle, joint sparse coding via Eq. (3) combines the objectives of Eqs. (4) and (5), forcing the sparse codes y(i)AV\u2212A and y (i) AV\u2212V to share the same representation. Note the relationship between the\nregularization parameters \u03bb\u2032 = ( 1NA + 1 NV )\u03bb\u2032\u2032. Ideally, we could have y(i)AV = y (i) AV\u2212A = y (i) AV\u2212V, although empirical values determined by the three different optimizations differ in reality. Feature formation possibilities on multimodal sparse coding are explained in Figure 3."}, {"heading": "3 Evaluation", "text": ""}, {"heading": "3.1 Dataset, task, and experiments", "text": "We use the TRECVID MED 2014 dataset [1] to evaluate our schemes. We consider the event detection and retrieval tasks using the 10Ex and 100Ex data scenarios, where 10Ex includes 10 multimedia examples per event, and 100 examples for 100Ex. There are 20 event classes (E021 to E040) with event names such as \u201cBike trick,\u201d \u201cDog show,\u201d and \u201cMarriage proposal.\u201d For evaluation, we compute classification accuracy and mean average precision (mAP) metrics according to the NIST standard on the following experiments:\n1. Cross-validation on 10Ex; 2. 10Ex/100Ex (train with 10Ex and test on 100Ex).\nWe use the number of basis vectors K = 512 same for all dictionaries DA, DV, and DAV. We aggregate sparse codes around each keyframe of a training example by max pooling to form feature vectors for classification. We train linear, 1-vs-all SVM classifiers for each event whose hyperparameters are determined by 5-fold cross-validation on 10Ex. We use the INRIA SPAMS (SPArse Modeling Software) [2], VOICEBOX Speech Processing Toolkit [3], MatConvNet [10] to drive the pretrained deep CNN models, and LIBSVM [5]."}, {"heading": "3.2 Other feature learning methods for comparison", "text": "We consider other unsupervised methods to learn audio-video features for comparison. We evaluate the performance of Gaussian mixture model (GMM) and restricted Boltzmann machine (RBM) [8] under similar unimodal and multimodal settings. For GMM, we use the expectation-maximization (EM) to fit the preprocessed input vectors xA, xV, xAV in 512 mixtures and form GMM supervectors [4] as feature that contain posterior probabilities with respect to each Gaussian. The max-pooled GMM supervectors are applied to train linear SVMs. We adopt the shallow bimodal pretraining model by Ngiam et al. [6] for RBM. Activations from the hidden layer of a size 512 are also max pooled before SVM. We have applied a target sparsity of 0.1 to both GMM and RBM."}, {"heading": "3.3 Results", "text": "Table 1 presents the classification accuracy and mAP performance of unimodal and multimodal sparse coding schemes. For the 10Ex/100Ex experiment, we have used the best parameter setting from the 10Ex cross-validation to test 100Ex examples. In general, we observe that the union of audio and video feature vectors perform better than using only unimodal or cross-modal features. The union schemes perform better than the joint schemes. The union schemes, however, double feature dimensionality (i.e., from 512 to 1,024) since our union operation concatenates the two feature vectors. Joint feature vector is an economical way of combining both the audio and video features while keeping the same dimensionality as audio-only or video-only.\nIn Table 2, we report the mean accuracy and mAP for GMM and RBM under the union and joint feature learning schemes on the 10Ex/100Ex experiment. Our results show that sparse coding is better than GMM by 5\u20136% in accuracy and 7\u20138% in mAP. However, we find that the performance of RBM is on par with sparse coding. This leaves us a good next step to develop joint feature learning scheme for RBM."}, {"heading": "4 Conclusion", "text": "We have presented multimodal sparse coding for MED. Our approach can build joint sparse feature vectors learned from different modalities and scale to file-level descriptors suitable for training classifiers in a MED system. Using the TRECVID MED 2014 dataset, we have empirically validated our\napproach and achieved promising performance measured in accuracy and precision metrics recommended by the NIST standard. Our future work includes an integration with more features, training for larger event coverage, and finetuning."}], "references": [{"title": "Support Vector Machines Using GMM Supervectors for Speaker Verification", "author": ["W.M. Campbell", "D.E. Sturim", "D.A. Reynolds"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "LIBSVM: A Library for Support Vector Machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Multimodal Deep Learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Restricted Boltzmann Machines for Collaborative Filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Image Super-Resolution via Sparse Representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "Originally proposed to explain neurons encoding sensory information [7], sparse coding provides an unsupervised method to learn basis vectors for efficient data representation.", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "We have tried out pretrained convolutional neural network (CNN) models and ended up choosing VGG ILSVRC 19 layers, the 19layer model by University of Oxford\u2019s Visual Geometry Group (VGG) [9] for the ImageNet Largescale Visual Recognition Challenge (ILSVRC).", "startOffset": 187, "endOffset": 190}, {"referenceID": 7, "context": "We use the joint sparse coding technique used in image super-resolution [11]", "startOffset": 72, "endOffset": 76}, {"referenceID": 6, "context": "We use the INRIA SPAMS (SPArse Modeling Software) [2], VOICEBOX Speech Processing Toolkit [3], MatConvNet [10] to drive the pretrained deep CNN models, and LIBSVM [5].", "startOffset": 106, "endOffset": 110}, {"referenceID": 1, "context": "We use the INRIA SPAMS (SPArse Modeling Software) [2], VOICEBOX Speech Processing Toolkit [3], MatConvNet [10] to drive the pretrained deep CNN models, and LIBSVM [5].", "startOffset": 163, "endOffset": 166}, {"referenceID": 4, "context": "We evaluate the performance of Gaussian mixture model (GMM) and restricted Boltzmann machine (RBM) [8] under similar unimodal and multimodal settings.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "For GMM, we use the expectation-maximization (EM) to fit the preprocessed input vectors xA, xV, xAV in 512 mixtures and form GMM supervectors [4] as feature that contain posterior probabilities with respect to each Gaussian.", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "[6] for RBM.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Unsupervised feature learning methods have proven effective for classification tasks based on a single modality. We present multimodal sparse coding for learning feature representations shared across multiple modalities. The shared representations are applied to multimedia event detection (MED) and evaluated in comparison to unimodal counterparts, as well as other feature learning methods such as GMM supervectors and sparse RBM. We report the cross-validated classification accuracy and mean average precision of the MED system trained on features learned from our unimodal and multimodal settings for a subset of the TRECVID MED 2014 dataset.", "creator": "LaTeX with hyperref package"}}}