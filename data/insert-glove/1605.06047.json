{"id": "1605.06047", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "AMSOM: Adaptive Moving Self-organizing Map for Clustering and Visualization", "abstract": "Self - Organizing izquierda Map (SOM) shockheaded is veces a neural jgp network wiess model epidemiological which evliya is habits used adelboden to morii obtain vsepr a psinet topology - 5.9375 preserving lyakhov mapping minchinhampton from the (usually high frere-jones dimensional) input / feature space lithocarpus to 2,172 an deignan output / jeffy map space of dake fewer 3,429 dimensions (precisely usually littrell two or three in order genk to souster facilitate visualization ). cognizance Neurons in 52-16 the output jongno space are youthfulness connected halkett with each other but adeniken this astatine structure etar remains fixed throughout training and learning is achieved sportwagen through muschamp the dismount updating of incapacitating neuron knowns reference ekran vectors leakes in feature cpp space. repenting Despite the cadwell fact that growing jussieu variants of aver SOM gitelman overcome aspirin the surur fixed platting structure sambucus limitation ,210 they grabby increase computational cost scuticaria and also daoed do not est\u00e9vez allow progres the removal of armeno a neuron after its introduction. interstices In nns1 this paper, ninety-nine a premachandran variant brandywell of SOM gnassingbe is proposed 15,900 called monash AMSOM (Adaptive haemus Moving l-glutamine Self - Organizing Map) 106.53 that suruma on notra the 15-19 one colonnese hand barabanki creates a more stirchley flexible palp structure disposables where 301.6 neuron extinguished positions epilepsy are dynamically tsukiji altered us-1 during vernes training santiago and pincode on carino the other hand tackles imint the euro475 drawback of having a vaginally predefined grid by cruisecritic.com allowing plaxton neuron penetration addition yanxi and / linky or removal (816) during korvettenkapit\u00e4n training. Experiments using multiple literature 70.53 datasets show that rotan the trikuta proposed girder method improves training performance tzorvas of purveyed SOM, leads to a eritrean better visualization pre-transition of shimba the input phoenicopteriformes dataset and tabled provides a framework for marrese determining the optimal number and structure of mayors neurons.", "histories": [["v1", "Thu, 19 May 2016 16:41:00 GMT  (1370kb,D)", "http://arxiv.org/abs/1605.06047v1", "ICAART 2016 accepted full paper"]], "COMMENTS": "ICAART 2016 accepted full paper", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["gerasimos spanakis", "gerhard weiss"], "accepted": false, "id": "1605.06047"}, "pdf": {"name": "1605.06047.pdf", "metadata": {"source": "CRF", "title": "AMSOM: Adaptive Moving Self-organizing Map for Clustering and Visualization", "authors": ["Gerasimos Spanakis", "Gerhard Weiss"], "emails": ["gerhard.weiss}@maastrichtuniversity.nl"], "sections": [{"heading": "1 INTRODUCTION", "text": "The Self-Organizing Map (SOM) (Kohonen, 2001) is an unsupervised neural network model which effectively maps high-dimensional data to a lowdimensional space (usually two-dimensional). The low-dimensional space (also called output space) consists of a grid of neurons connected with each other, according to a specific structure (can be hexagonal, rectangular, etc.). This structure allows the topology preservation of input data (i.e., similar input patterns are expected to be mapped to neighboring neurons in the output grid) (Kohonen, 1981). By this way, SOM manages to achieve dimensionality reduction, abstraction, clustering and visualization of the input data and this is the reason that it has been applied successfully to many different domains and datasets like financial data (Deboeck and Kohonen, 2013), speech recognition (Kohonen, 1988), image classification (Lu, 1990), document clustering (Lagus et al., 1999), (Spanakis et al., 2012) .\nThe SOM algorithm raises some issues and problems: (1) SOM\u2019s architecture is fixed and predefined in terms of number and arrangement of neurons. In case of largely unknown input data, it is difficult to determine apriori the correct structure that provides\nsatisfactory results. There is some work in this area in order to how to add/remove neurons but none of current approaches adjusts neuron positions on the grid according to training progress. (2) Training a SOM comes with a large computation cost, especially in cases of large datasets and/or large maps. Many epochs might be needed in order for the SOM to converge and the map to reach a final state.\nIn this paper we propose an extension of the traditional SOM, which handles both issues described above: First, it allows neurons to change positions during training which provides better visualization and faster training time. Second, number of neurons can be adjusted (neurons can be either added or removed) according to dataset requirements and training progress. Due to this enhanced training scheme, the number of epochs required for training is significantly reduced. The rest of the paper is organized as follows. Section 2 presents background work on SOM, extensions on the traditional algorithm and their limitations. The proposed method is presented in Section 3 while experimental setup is described in Section 4. Finally, Section 5 concludes the paper. ar X\niv :1\n60 5.\n06 04\n7v 1\n[ cs\n.A I]\n1 9\nM ay\n2 01\n6"}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 SOM and Competitive Learning", "text": "The Self-Organizing Map (SOM) is a fully connected single-layer linear neural network. The SOM uses a set of neurons, often arranged in a 2-D rectangular or hexagonal grid, to form a discrete topological mapping of an input space, X \u2208 RD. Input space consists of a set of vectors x j \u2208 RD:\nx j = [x j1,x j2, ...,x jD]T (1)\nwi is the weight vector associated to neuron i and is a vector of the same dimension (D) of the input space, M is the total number of neurons. Obviously, these weights represent the synaptic connections of each neuron i and can be denoted:\nwi = [wi1,wi2, ...,wiD]T (2)\nThe fundamental principle of SOM is the soft competition between the nodes in the output layer; not only the node (winner) but also its neighbors are updated (Kohonen, 2012).\nA SOM architecture can be found in Figure 1.\nAll the weights w1,w2, ...,wM are initialized to random numbers, in the range of the corresponding input characteristics. We also introduce a discrete time index t such that x(t), t = 0,1, ... is presented to network at time t and wi(t) is the weight vector of neuron i computed at time t. The available input vectors are recycled during the training (or learning) process: a single pass over the input data is called an epoch."}, {"heading": "2.1.1 On-line Training of SOM", "text": "In the conventional \u201con-line\u201d or \u201cflow-through\u201d method, the weight vectors are updated recursively after the presentation of each input vector. As each\ninput vector is presented, the Euclidean distance between the input vector and each weight vector is computed:\ndi(t) = ||x(t)\u2212wi(t)||2 (3)\nNext, the winning or best-matching node (denoted by subscript c) is determined by:\nc = {i,minidi(t)} (4)\nNote that we suppress the implicit dependence of c on discrete time t. The weight vectors are updated using the following rule:\nwi(t +1) = wi(t)+\u03b1(t) \u00b7hci(t) \u00b7 [x(t)\u2212wi(t)] (5)\nwhere \u03b1(t) is the learning-rate factor and hci(t) is the neighborhood function. The learning rate factor controls the overall magnitude of the correction to the weight vectors, and is reduced monotonically during the training phase. The neighborhood function controls the extent to which wi(t) is allowed to adjust in response to an input most closely resembling wc(t) and is typically a decreasing function of the distance on the 2-D lattice between nodes c and i. We use the standard Gaussian neighborhood function:\nhci(t) = exp ( \u2212||ri\u2212 rc|| 2\n\u03c3(t)2\n) (6)\nwhere ri and rc denote the coordinates of nodes i and c, respectively, on the output space (usually twodimensional grid). The width \u03c3(t) of the neighborhood function decreases during training, from an initial value comparable to the dimension of the lattice to a final value effectively equal to the width of a single cell. It is this procedure which produces the selforganization and topology preserving capabilities of the SOM: presentation of each input vector adjusts the weight vector of the winning node along with those of its topological neighbors to more closely resemble the input vector. The converged weight vectors approximate the input probability distribution function, and can be viewed as prototypes representing the input data."}, {"heading": "2.1.2 Batch Training of SOM", "text": "The SOM update given by Equation (5) is \u201con-line\u201d in the sense that the weight vectors are updated after the presentation of each input record. In the batch SOM algorithm (proposed in (Kohonen, 1993)), the weights are updated only at the end of each epoch according to:\nwi(t f ) = \u2211\nt \u2032=t f t \u2032=t0 h\u0303ci(t \u2032) \u00b7x(t \u2032)\n\u2211 t \u2032=t f t \u2032=t0 h\u0303ci(t \u2032) (7)\nwhere t0 and t f denote the start and finish of the present epoch, respectively, and wi(t f ) are the weight vectors computed at the end of the present epoch. Hence, the summations are accumulated during one complete pass over the input data. The winning node at each presentation of new input vector is computed using:\nd\u0303i(t) = ||x(t)\u2212wi(t0)||2 (8) c = {i,minid\u0303i(t)} (9)\nwhere wi(t0) are the weight vectors computed at the end of the previous epoch. The neighborhood functions h\u0303ci(t) are computed using Equation (6), but with the winning nodes determined from Equation (9). This procedure for computing the neighborhood function is identical to the Voronoi partinioning. As is in the on-line method, the width of the neighborhood function decreases monotonically over the training phase.\nA more concrete explanation of the batch algorithm is given by the following Equation:\nwi = \u2211 j n j \u00b7h ji \u00b7 x\u0303 j\n\u2211 j n j \u00b7h ji (10)\nwhere n j is the number of input items mapped into node j and the index j runs over the nodes in the neighborhood of node i. The basic idea is that for every node j in the grid, the average x\u0303 j of all those input items x(t) is formed that have node j (i.e., vector w j) as the closest node. The above Equation is used for updating the node weight vectors and this is repeated for a few times, always using the same batch of input data items to determine the updated x\u0303 j.\nThe batch SOM offers several advantages over the conventional on-line SOM method. Since the weight updates are not recursive, there is no dependence upon the order in which the input records are presented. In addition to facilitating the development of data-partitioned parallel methods, this also eliminates concerns (Mulier and Cherkassky, 1994) that input records encountered later in the training sequence may overly influence the final results. The learning rate parameter \u03b1(t) does not appear in the batch SOM algorithm, thus eliminating a potential source of poor convergence (Ceccarelli et al., 1993) if this parameter is not properly specified.\nThe mathematical theory of the SOM is very complicated and only the one-dimensional case has been analyzed completely (Fort, 2006), since the SOM\nbelongs to the \u2018ill posed\u2019 problems in mathematics. The SOM can also be looked at as a \u2018nonlinear projection\u2019 of the probability density function of highdimensional input data onto the two-dimensional display.\nUsually, the input is mapped onto a 1- or 2- dimensional map. Mapping onto higher dimensions is possible as well, but complicates the visualization. The neurons connected to adjacent neurons by a neighborhood relationship define the structure of the map. The two most common 2-dimensional grids are the hexagonal grid and the rectangular grid and are shown in Figure 2.\nThe neighborhood function defines the correlation between neurons. The simplest neighborhood function is called bubble; it is constant over the neighborhood of the winner neuron and zero otherwise. The neighborhood of different sizes in rectangular and hexagonal maps can be seen in Figure 2. A more flexible definition is the gaussian neighborhood function defined by Equation (6).\nThe number of neurons, the dimensions of the map grid, the map lattice and shape must be specified before training. The more neurons the grid has, the more flexible the mapping becomes but the computation complexity of the training phase increases as well. The choice of the map structure and size is both related to the type of problem and the subjective choice of the user."}, {"heading": "2.2 Flexible Structure in Neural Networks and SOM", "text": "The norm in artificial neural nets is that classic techniques involve simple and often fixed network topologies trained via stimulus-based methods such as backpropagation. However, there are cases in which the structural design of the network is strongly influenced by the environment and by utilizing constructive and pruning algorithms. Both these algorithmic categories deliver a network which is gradually adjusted in re-\nsponse to training data. There are many approaches which apply these algorithms in classic neural networks (Islam et al., 2009), (Bortman and Aladjem, 2009), (Han and Qiao, 2013), (Yang and Chen, 2012).\nAlso, there are many variations of SOM that allow a more flexible structure of the output map which can be divided into two categories: In the first type, we include growing grid (GG) (Fritzke, 1995), incremental GG (Blackmore and Miikkulainen, 1993), growing SOM (GSOM) (Alahakoon et al., 2000) all coming with different variants. GG is the only variant which allows growing a new node from the interior of the grid (but this is a whole row or column of nodes). In the rest cases, new nodes are generated by a boundary node, despite the fact that the highest error could have been generated by an internal node. The idea is that the error will be propagated to the exterior to guarantee that growing can only be from the boundaries but this process can lead to a map structure with not perfect topology preservation. Therefore, map size becomes very wide after a limited number of insertions, with some additional nodes, which have no effect. MIGSOM (Ayadi et al., 2012) allows a more flexible structure by adding neurons internally and from the boundary but still does not offer the ability to remove neurons if necessary.\nIn the second type of growing variants, the rectangular grid is replaced with some connected nodes. We denote growing cell structures (GCSs) (Fritzke, 1994), growing neural gas (GNG) (Fritzke et al., 1995) and growing where required (Marsland et al., 2002). These works add just the necessary nodes at the same time, to fine-tune the optimal map size. Nevertheless, GCS and GNG are facing many difficulties for visualizing high-dimensional data. Visualization in these cases is guaranteed only with lowdimensional data.\nLimitations in growing and visualization led to hierarchical variants of the previous model like the Growing Hierarchical SOM (GHSOM) (Rauber et al., 2002). With GHSOM you can get an idea of the hierarchical structure of the map, but the growing parameter of the map has to be defined beforehand. Other approaches (like TreeGNG (Doherty et al., 2005) or TreeGCS (Hodge and Austin, 2001)) use dendrograms for representation but due to this tree structure they lose the topological properties.\nDisadvantages of these approaches are: (a) the high computational cost due to the fact that structure starts from a very basic architecture and has to grow in order to reach an acceptable structure for the data and (b) the fact that after adding neurons there is not the possibility of removing a neuron if performance is not improving."}, {"heading": "3 EXPANDING THE IDEA OF", "text": "SELF-ORGANIZATION IN NEURON LOCATIONS\nDuring the classic SOM algorithm neuron positions remain unchanged and the grid is fixed from the beginning till the end of the training. This facilitates the process of learning (since neighborhood structure is known beforehand) but is restricting regarding the final result and ways of visualizing it. We propose a different and more flexible scheme in regard to position vectors ri of neurons, which allows a more adaptive form of the neuron grid and acts as an extension to the batch learning algorithm.\nStarting from an already grown map size, AMSOM can adapt both its size and structure in order to better represent the data at a specific level of detail. After a specific number of steps, neurons are analyzed to see whether the level of representation is sufficient or adjustments are needed: removal and/or addition of neurons. Initially, connections between neurons are determined based on the grid structure but as training advances, these can change and adjust according to the way that neuron positions are also changed during the process. The algorithm flow is described in Figure 3 and more details about the steps are presented in the following subsections."}, {"heading": "3.1 Phase I: AMSOM Initialization", "text": ""}, {"heading": "3.1.1 Grid Structure and Size.", "text": "The first step of AMSOM algorithm is to define the initial grid structure (as the classic SOM). This process facilitates training time in contrast to starting from a small-size structure and building on that as other approaches do (Vesanto et al., 2000). It is also in agreement with the neural development which suggests that nearly all neural cells used through human lifetime have been produced in the first months of life (Dowling, 2007). This overproduction of neuron cells is thought to have evolved as a competitive strategy for the establishment of efficient connectivity (Changeux and Danchin, 1976).\nHaving this in mind, the initial structure of SOM is determined. Several empirical rules (Park et al., 2006) suggest that the number of neurons should be 5 \u00b7 \u221a N where N is the number of patterns in the dataset. In this case, the two largest eigenvalues of the training data are first calculated, then the ratio between side lengths of the map grid is set to the ratio between the two maximum eigenvalues. The actual side lengths are finally set so that their product is close to the number of map units determined according to (Vesanto\net al., 2000) rule. The eigenvalues ratio shows how well the data is flattened and elongated (Este\u0301vez et al., 2012). At this point a more precise determination of the number of neurons is not essential, since this number will be fine tuned during the training process. Initially, neurons are connected with their neighbors following the idea of Figure 2 using a rectangular or hexagonal grid. For example, if the algorithm suggests that the initial grid of the AMSOM should be 5x4 (let\u2019s suppose rectangular), every neuron has 4 neighbors (except the marginal ones). Figure 4 demonstrates two different topologies, a rectangular and a hexagonal one with the corresponding connections between neurons."}, {"heading": "3.1.2 Vector, Matrix and Parameters Initialization.", "text": "For each neuron the following are defined and initialized accordingly:\n\u2022 Neuron vector (weight vector, wi): It is the same as the classic SOM (see Equation (2)) and shows the representation of the neuron in the feature (in-\nput) space. Initialization of neuron vectors is random according to literature standards.\n\u2022 Neuron position (position vector, ri): Depending on the output space (mostly it is twodimensional), it\u2019s a vector that shows the position of the neuron. Initial position vectors are equal\nto the positions of the neurons in the grid, i.e., in Figure 4 one can see the coordinates of neurons according to the structure (hexagonal or rectangular).\nSince the structure of the grid is subject to changes during training, we need to keep track of the neighbors of each neuron. There is the possibility that some neurons which where connected in the initial grid become disconnected after some time (or vice versa). In order to keep track of these changes we introduce the orthogonal and symmetrical matrices E and A (both size M\u00d7M) where E(p,q) shows if neurons p and q are connected (0 translates to no connection, 1 translates to connected neurons) and A(p,q) shows the age of edge (as implied by E(p,q)) between neurons p and q: This will be used in order to determine which neurons had incidental connections to other neurons or strong connections as training moves forward. When A(p,q) is 0 that means that neurons p and q were closest neighbors at current epoch but any other value (i.e., 2) implies that neurons p and q were closest neighbors some epochs before (i.e., 2). An example of matrices E and A is seen in Figure 5.\nIn this example, neurons number (M) is 4 and connectivity matrix E shows how neurons are connected to each other (as implied by the graph). Age matrix A shows how many epochs an edge has \u2018survived\u2019: Connection between neuron #1 and #2 has age 2 whereas connection between neuron #2 and #4 has age 0. Notice that age 0 can either mean that neurons are not connected, like neurons #1 and #4 or that neurons are connected at this current epoch (so their connection is \u2018recent\u2019), like neurons #2 and #4.\nAlso, at this stage the growing threshold GT of the map is defined as a function of data dimension (D) and\na spread factor (SF) defined by the user. Formula used is GT = \u2212ln(D)\u00d7 ln(SF) (from (Alahakoon et al., 2000)). Generally, a SF value of 0.5 always yields good results but its fine tuning is up to the user requirements and the dataset structure."}, {"heading": "3.2 Phase II: Training", "text": ""}, {"heading": "3.2.1 Weight and Position Updating", "text": "For the weight learning of neurons, the SOM batch algorithm is utilized, as it was given in Equations 7- 10, which are repeated here for clarity.\nwi(t +1) = \u2211 j n j(t) \u00b7h ji(t) \u00b7 x\u0303 j(t)\n\u2211 j n j(t) \u00b7h ji(t) (11)\nh ji(t) = exp ( \u2212 ||r j\u2212 ri||2\n\u03c3(t)2\n) (12)\nwhere:\n\u2022 wi(t + 1) marks neurons i updated weight (at epoch t +1),\n\u2022 t marks current epoch and t + 1 marks the next epoch,\n\u2022 n j(t) marks the number of patterns that are assigned to neuron j,\n\u2022 h ji(t) marks the neighborhood function and is a measure of how close are neuron j and neuron i,\n\u2022 x\u0303 j(t) is the mean feature vector of all x that are assigned to neuron j at epoch t,\n\u2022 r j,ri are the position vectors (in the output space) for neurons j and i,\n\u2022 \u03c3(t) is the adaptation factor, decreasing through training\nBuilding on top of this, at the end of each epoch, the neuron position vectors are adjusted in a similar manner to the SOM training algorithm. In more detail, at the end of each epoch and after the neuron weight vectors update is over, the distances between the neuron vectors (wi) are computed. These distances show how close neurons are (in the input space) and can be used as a measure in order to update neuron positions (in the output space). This is achieved through the following Equations:\nri(t +1) = ri(t)+\u03b1(t) \u00b7 \u2211 j n j(t) \u00b7\u03b4 ji(t)(r j(t)\u2212 ri(t))\n\u2211 j n j(t) \u00b7\u03b4 ji(t) (13)\n\u03b4 ji(t) = exp ( \u2212 ||w j\u2212wi||2\n\u03b3\u00d7\u03c3(t)2\n) (14)\nwhere:\n\u2022 t, n j(t) were defined in Equations 11 and 12, \u2022 \u03b1(t) denotes the learning rate at epoch t and con-\ntrols the rate that positions of neurons are moving, \u2022 \u03b4 ji(t) is a neighborhood function denoting how\nclose neurons j and i are (during time t and is based on their distance in the input space (i.e., distance computed based on their vectors wi),\n\u2022 \u03b3 is a parameter that controls the neighborhood shrinking as a fraction of \u03c3 which was used in Equation (12) Notice the similarity of \u03b4 ji with h ji: both are neighborhood functions and are used to determine how close two neurons are but the first one does so using their distances in the feature (input) space while the latter does so using their distances in the output space (map).\nEquation (13) will adjust neuron\u2019s i position vector according to the neurons which proved winners for more patterns in its neighborhood and less (or even none) according to neurons which were winners for few patterns (or none). This process enhances the concept of neighborhood around the neurons that attract more patterns and also allows to cover any empty spaces in the data representation. It is expected to improve the training speed, since position updating will lead to more accurate position vectors that will be used for the next training epoch and leads to more insightful representations of the neurons in the output space.\nLearning rate \u03b1(t) can also be set to a small value 0.01 since the neighborhood function controls well the percentage of change in the position vectors. It was selected to update the position vectors with this hybrid on-line-batch SOM rule, due to the fact that output space is much smaller (in most SOM applications) than the input space, so in many cases minor adjustments (than major repositioning of the neurons) are more necessary in order to guarantee satisfactory training but also representation. Also note that the parameter \u03b3 which controls neighborhood shrinking for position can also control how fast the map will be updated and how neurons are going to affect each other."}, {"heading": "3.2.2 Adding and Removing Neurons", "text": "During the weight updating process, for each input (pattern) the best matching neuron is determined (Na) and also the second best matching (Nb). At this step the age of all edges between Na and its neighbors is increased. Afterwards, Na is connected to Nb. If both of the neurons were already connected then their age is reset to zero. This is another step that implements the competitive learning rule, since for each new pattern, a new edge connecting the two closest neurons\nis drawn. This process is repeated for all patterns as they are presented to the AMSOM. Finally, at the end of the epoch for each incident edge between neurons (i, j), if A(i, j) \u2265 agemax, then this edge is removed. agemax can be set to a value not small enough (so as to avoid many disconnections) but also not big enough (so as to avoid having a fully connected grid). In our experiments this value was 30. If either of the implicated neurons becomes isolated from the remainder of the structure, then it is removed from the grid. The aim here is to remove edges that are no longer useful because they are replaced by younger edges that are created during the AMSOM training. That is the reason that each time two neurons are connected by an edge, then its age is reset to zero. By this process, neurons that were connected incidentally -especially at the beginning of the training when the map is still under forming- are disconnected after some epochs. This process has two distinct advantages: (a) selforganization and competitive learning will allow after some epochs the removal of redundant number of neurons and (b) adjustment of connections between neurons so as to enhance topological properties of the dataset. An example of a removal of a neuron is shown in Figure 6 along with the necessary adjustments to matrices A and E.\nAlso, there is the possibility that after some epochs (tadd), new neurons are added. The criterion is based on the training progress and when an addition happens, then new neurons can be added only after a number of epochs (tadd) in order to allow weight adaptation of the map, before evaluating current structure. First step is to spot the neuron Nu with the largest quantization error. A new neuron will be added, if its\nquantization error is higher than GT , where GT is the growing threshold of the map: A high value for GT will result in less spread out map and a low GT will produce a more spread map. If the quantization error satisfies the above condition then its Voronoi region is considered to be under-represented and therefore a new neuron has to be added to share the load of the high-error-valued neuron.\nRegarding the new neuron that will be added, we follow the the biological process of \u2018cell division\u2019 (Odri et al., 1993). By this way the neuron with the highest quantization error is \u2018splitted\u2019 to two new neurons (instead of just adding one new neuron somewhere at random with no connections at all). Both new neurons preserve the same connectivity (and also they are connected to each other) with the original neuron, thus we achieve a preservation of behavioral link between the parent and the offspring. Regarding the exact position of the two neurons the following process is followed: Neuron with the largest error among Nu\u2019s neighbors is spotted (let it be Nv). One neuron will preserve Nu\u2019s position and the other one will be placed in the middle between Nu and Nv. In detail, weights and positions of the two new neurons (u1 and u2) are calculated using the following Equations:\nwu1 = (1+\u03b2)\u00d7wu (15)\nwu2 =\u2212\u03b2\u00d7wu (16)\nru1 = ru (17)\nru2 = ru + rv\n2 (18)\nwhere wu refers to the weight vector of neuron u (neuron that is splitted) and \u03b2 is a mutation parameter which can take either a fixed or random value according to a certain distribution rule (following (Odri et al., 1993)). In any case, value of \u03b2 has to be chosen small in order to avoid a large change both in network topology but also in the weight vectors. In this paper, \u03b2 takes a random value according to a Gaussian distribution with a mean of zero and variance of one. New neurons retain the same connectivity to other neurons as the parent neuron but age weights are zeroed. The process of adding a new neuron (along with any changes in matrices E and A) is described in Figure 7.\nIt has to be pointed out that there is the possibility that a neuron would be removed from a region of the map and to be added in another region (removing and adding neurons are consecutive processes). This comes to agreement with several theories in neural organization, suggesting that cortical regions can adapt to their input sources and are somewhat interchangeable or \u2018reusable\u2019 by other modalities, especially in\nvision- or hearing-impaired subjects (Wedeen et al., 2012)."}, {"heading": "3.2.3 Architecture Adaptation and Termination Criterion", "text": "As it is described before, initial structure of AMSOM is adapted through learning and training in order to find what is optimal for the number of neurons, their weights and their connections. The adaptation process starts by training the initial structure of AMSOM. When the criteria of adding or removing neurons are satisfied, then the network is adapted. In order to maintain (as possible) the initial structure (i.e., rectangular or hexagonal or any other lattice selected), after this adaptation process we re-evaluate all connections of all neurons and make sure that each neuron has at most Q neighbors (where Q is decided in the beginning, i.e., in the case of rectangular lattice, Q = 4): This can be ensured by checking edge matrix E after each epoch and if a neuron is found to have more than Q connections then only the Q-\u2018recent\u2019 are kept (utilizing age of edges in matrix A). This process is presented in Figure 8.\nBy this training scheme, AMSOM adapts simultaneously the structure of the map (number of neurons and connections) and the weight vectors. Removing and adding neurons occur when different criteria are met, so they can be applied in any sequence, depending on when the criteria are satisfied. By applying these operations repeatedly, AMSOM is expected to find a near-optimal structure and representation of a given dataset.\nFinally, like every SOM algorithm, AMSOM has\nan upper limit of epochs that training takes place. This number is set to 1000 but there is also a premature termination criterion depending on the mean quantization error change between two consecutive epochs. Thus, if mqe(t)\u2212mqe(t\u22121)< \u03b51 where \u03b51 is a small value (like 1E\u221206) then the map has reached the desired size (according to the GT provided) and training is terminated."}, {"heading": "3.3 Phase III: AMSOM Finalization", "text": "Final phase of AMSOM happens when learning is complete and structure of the network is not any more changing. No neurons are added or removed at this phase and no connections between neurons are added or removed but weight and position vector adaptation is continued with a lower rate. Purpose of this process is to smooth out any quantization error and fine tune weights and positions of the neurons, especially for neurons added at the latter epochs. For this purpose, neighborhood function (both for Equations 11 and 13 is constrained only to the immediate neighborhood and learning rate \u03b1(t) in Equation (13) is set to 0.001 (even smaller than in phase II). Phase III is concluded when there is no significant change in change in mean quantization error (i.e., when mqe(t)\u2212mqe(t \u2212 1) < \u03b52), where \u03b52 is set to a smaller value than \u03b51 (like 1E\u221210)."}, {"heading": "4 EXPERIMENTS", "text": "AMSOM performance has been tested with several literature datasets in order to evaluate both map\nquality (in terms of topology preservation) and the number of epochs needed to converge. Quantization Error (QE) and Topographic Error (TE) were used as intrinsic measures of evaluation (for more details readers are encouraged to read (Bauer et al., 1999)). All datasets were provided by the UCI repository 1, except the CLUSTER dataset which is a simple and random but large two-dimensional dataset with four groups. All datasets used with their characteristics are presented in Table 1.\nEach dataset in shuffled and split to training, testing and validation set (60%, 20% and 20% respectively). Each experiment was performed 20 times and the results presented here are average over these runs (deviations were small and are not presented here). Results for AMSOM QE and TE (compared to classic SOM) along with the number of neurons used by each model are presented in Table 2. From this Table it is obvious that AMSOM\u2019s performance is much better than classic SOM. AMSOM starts from the same number of neurons as classic SOM but by removing and adding neurons when necessary reaches a number which is suitable to represent the dataset. Both QE and TE are improved using AMSOM algorithm and this improvement is more significant in TE because of the neuron position changing which allows the map to better adjust to the dataset.\nVisualization results for three of the datasets are presented in Figures 9 through 11. In these figures final positions of the neurons and their final positions are represented. For each neuron a majority vote criterion was used in order to determine the class that this neuron represents. For the simple CLUSTER dataset it is obvious that the four classes are identified and the grid structure can effectively represent their relations. For the IRIS dataset one class is completely identified whereas the other two (which are more similar) are also highlighted. Also notice that neurons that belong to the same class are mostly connected with each other on the grid and only some spontaneous connections between classes exist. Finally, for the more demanding IONOSPHERE dataset (see the relatively higher QE), AMSOM manages to differen-\n1http://archive.ics.uci.edu/ml/\ntiate in a great degree the two classes. Neuron grids in all figures also reveal that the percentage of dead units (neurons that do not represent any pattern) is significantly small, which is an improvement to the classic SOM algorithm (fewer inactive neurons).\nRegarding the spread factor (SF) which controls the growing threshold (GT ), a value 0.5 was chosen for this series of experiments because for all datasets it yielded satisfactory results. In the general case that there is no prior knowledge on the data examined, a low value of SF (0-0.3) will allow highlighting of the most significant clusters. Regarding \u03b3 parameter of Equation (13) it was found that it can effectively control the spreading or shrinking of neighborhood during position updating and by this way creating more isolated or more connected clusters. Several experiments were conducted (not presented here due to space limitations) and showed that small values of gamma (1 till 10) produce the best results for all datasets. The higher the \u03b3, the better topographic preservation (reduced TE) but the quantization error (QE) rises. Also, high values of \u03b3 tend to increase the number of neurons that remain unused (dead units) whereas values close to 100 tend to approach the classic SOM algorithm (position updating is minimal). Two more parameters that need to be adjusted are agemax and tadd . For both parameters, 30 epochs were\nfound to be optimal, which is sound given the fact that 30 epochs are enough time to see if current structure performs well (reduced QE) or if adjustments needed (adding/removing neurons).\nComplexity of the developed algorithm is slightly increased due to the need for updating matrices A and E and also due to the more flexible structure. This overhead is partly counterbalanced by the faster training process (in all experiments there was a decrease in epochs number around 20%) since updating neuron positions clearly improves training time (requires less epochs) but for memory intensive tasks (like big datasets) this might be become a drawback for the algorithm."}, {"heading": "5 CONCLUSION", "text": "In this paper we presented AMSOM, an extension to original SOM algorithm which allows neurons to change positions according to a similar competitive technique used in classic SOM training. Moreover, neurons can be added or removed during this \u201cdouble\u201d training process allowing for a more flexible structure grid which is able to represent the dataset more efficiently. Experimental results on different\ndatasets improve performance of AMSOM compared to classic SOM algorithm. AMSOM produces better reference vectors by reducing the quantization error, topology is preserved through the neuron moving by significantly reducing the Topographic Error and the visualization result matches as much as possible the original dataset partitions. Also, AMSOM produces fewer nodes with no significant effect while at the same time it reduces required number of epochs.\nObtained results give new insights on how to utilize the concept of competitive learning and selforganization in neural networks and will be examined in more detail so as to further improve performance and investigate behavior in bigger and real-life datasets (images, text, etc.). For this purpose, drawbacks of the algorithm (the need to tune four parameters and the memory cost) will be addressed in order to further facilitate visualization and clustering (especially in large datasets)."}], "references": [{"title": "Dynamic self-organizing maps with controlled growth for knowledge discovery", "author": ["D. Alahakoon", "S.K. Halgamuge", "B. Srinivasan"], "venue": "Neural Networks, IEEE Transactions on, 11(3):601\u2013 614.", "citeRegEx": "Alahakoon et al\\.,? 2000", "shortCiteRegEx": "Alahakoon et al\\.", "year": 2000}, {"title": "MIGSOM: multilevel interior growing self-organizing maps for high dimensional data clustering", "author": ["T. Ayadi", "T.M. Hamdani", "A.M. Alimi"], "venue": "Neural processing letters, 36(3):235\u2013 256.", "citeRegEx": "Ayadi et al\\.,? 2012", "shortCiteRegEx": "Ayadi et al\\.", "year": 2012}, {"title": "Neural maps and topographic vector quantization", "author": ["Bauer", "H.-U.", "M. Herrmann", "T. Villmann"], "venue": "Neural networks, 12(4):659\u2013676.", "citeRegEx": "Bauer et al\\.,? 1999", "shortCiteRegEx": "Bauer et al\\.", "year": 1999}, {"title": "Incremental grid growing: encoding highdimensional structure into a two-dimensional", "author": ["J. Blackmore", "R. Miikkulainen"], "venue": null, "citeRegEx": "Blackmore and Miikkulainen,? \\Q1993\\E", "shortCiteRegEx": "Blackmore and Miikkulainen", "year": 1993}, {"title": "A growing and pruning method for radial basis function networks", "author": ["M. Bortman", "M. Aladjem"], "venue": "Neural Networks, IEEE Transactions on, 20(6):1039\u20131045.", "citeRegEx": "Bortman and Aladjem,? 2009", "shortCiteRegEx": "Bortman and Aladjem", "year": 2009}, {"title": "Competitive neural networks on message-passing parallel computers", "author": ["M. Ceccarelli", "A. Petrosino", "R. Vaccaro"], "venue": "Concurrency: Practice and Experience, 5(6):449\u2013470.", "citeRegEx": "Ceccarelli et al\\.,? 1993", "shortCiteRegEx": "Ceccarelli et al\\.", "year": 1993}, {"title": "Selective stabilisation of developing synapses as a mechanism for the specification of neuronal networks", "author": ["Changeux", "J.-P.", "A. Danchin"], "venue": "Nature, 264(5588):705\u2013712.", "citeRegEx": "Changeux et al\\.,? 1976", "shortCiteRegEx": "Changeux et al\\.", "year": 1976}, {"title": "Visual explorations in finance: with self-organizing maps", "author": ["G. Deboeck", "T. Kohonen"], "venue": "Springer Science & Business Media.", "citeRegEx": "Deboeck and Kohonen,? 2013", "shortCiteRegEx": "Deboeck and Kohonen", "year": 2013}, {"title": "TreeGNG-hierarchical topological clustering", "author": ["K. Doherty", "R. Adams", "N. Davey"], "venue": "ESANN, pages 19\u201324.", "citeRegEx": "Doherty et al\\.,? 2005", "shortCiteRegEx": "Doherty et al\\.", "year": 2005}, {"title": "The Great Brain Debate: Nature Or Nurture", "author": ["J.E. Dowling"], "venue": null, "citeRegEx": "Dowling,? \\Q2007\\E", "shortCiteRegEx": "Dowling", "year": 2007}, {"title": "Advances in Self-Organizing Maps: 9th International Workshop, WSOM 2012 Santiago, Chile, December 12-14, 2012 Proceedings", "author": ["P.A. Est\u00e9vez", "J.C. Pr\u0131\u0301ncipe", "P. Zegers"], "venue": null, "citeRegEx": "Est\u00e9vez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Est\u00e9vez et al\\.", "year": 2012}, {"title": "Soms mathematics", "author": ["Fort", "J.-C."], "venue": "Neural Networks, 19(6):812\u2013816.", "citeRegEx": "Fort and J..C.,? 2006", "shortCiteRegEx": "Fort and J..C.", "year": 2006}, {"title": "Growing cell structuresa selforganizing network for unsupervised and supervised learning", "author": ["B. Fritzke"], "venue": "Neural networks, 7(9):1441\u2013 1460.", "citeRegEx": "Fritzke,? 1994", "shortCiteRegEx": "Fritzke", "year": 1994}, {"title": "Growing grida self-organizing network with constant neighborhood range and adaptation strength", "author": ["B. Fritzke"], "venue": "Neural Processing Letters, 2(5):9\u201313.", "citeRegEx": "Fritzke,? 1995", "shortCiteRegEx": "Fritzke", "year": 1995}, {"title": "A growing neural gas network learns topologies", "author": ["B Fritzke"], "venue": "Advances in neural information processing systems, 7:625\u2013632.", "citeRegEx": "Fritzke,? 1995", "shortCiteRegEx": "Fritzke", "year": 1995}, {"title": "A structure optimisation algorithm for feedforward neural network construction", "author": ["Han", "H.-G.", "Qiao", "J.-F."], "venue": "Neurocomputing, 99:347\u2013357.", "citeRegEx": "Han et al\\.,? 2013", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Hierarchical growing cell structures: TreeGCS", "author": ["V.J. Hodge", "J. Austin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 13(2):207\u2013218.", "citeRegEx": "Hodge and Austin,? 2001", "shortCiteRegEx": "Hodge and Austin", "year": 2001}, {"title": "A new adaptive merging and growing algorithm for designing artificial neural networks", "author": ["M. Islam", "A. Sattar", "F. Amin", "X. Yao", "K. Murase"], "venue": "Systems, Man, and Cybernetics, Part B:", "citeRegEx": "Islam et al\\.,? 2009", "shortCiteRegEx": "Islam et al\\.", "year": 2009}, {"title": "Automatic formation of topological maps of patterns in a self-organizing system", "author": ["T. Kohonen"], "venue": null, "citeRegEx": "Kohonen,? \\Q1981\\E", "shortCiteRegEx": "Kohonen", "year": 1981}, {"title": "The\u2019neural\u2019phonetic typewriter", "author": ["T. Kohonen"], "venue": "Computer, 21(3):11\u201322.", "citeRegEx": "Kohonen,? 1988", "shortCiteRegEx": "Kohonen", "year": 1988}, {"title": "Things you haven\u2019t heard about the Self-Organizing Map", "author": ["T. Kohonen"], "venue": "Neural Networks, 1993., IEEE International Conference on, pages 1147\u20131156. IEEE.", "citeRegEx": "Kohonen,? 1993", "shortCiteRegEx": "Kohonen", "year": 1993}, {"title": "Self-organizing Maps, vol", "author": ["T. Kohonen"], "venue": "30 of Springer Series in Information Sciences. Springer Berlin.", "citeRegEx": "Kohonen,? 2001", "shortCiteRegEx": "Kohonen", "year": 2001}, {"title": "Self-organization and associative memory, volume 8", "author": ["T. Kohonen"], "venue": "Springer.", "citeRegEx": "Kohonen,? 2012", "shortCiteRegEx": "Kohonen", "year": 2012}, {"title": "WEBSOM for textual data mining", "author": ["K. Lagus", "T. Honkela", "S. Kaski", "T. Kohonen"], "venue": "Artificial Intelligence Review, 13(5-6):345\u2013364.", "citeRegEx": "Lagus et al\\.,? 1999", "shortCiteRegEx": "Lagus et al\\.", "year": 1999}, {"title": "Pattern classification using selforganizing feature maps", "author": ["Lu", "S.-y."], "venue": "1990 IJCNN International Joint Conference on, pages 471\u2013480.", "citeRegEx": "Lu and S..y.,? 1990", "shortCiteRegEx": "Lu and S..y.", "year": 1990}, {"title": "A self-organising network that grows when required", "author": ["S. Marsland", "J. Shapiro", "U. Nehmzow"], "venue": "Neural Networks, 15(8):1041\u20131058.", "citeRegEx": "Marsland et al\\.,? 2002", "shortCiteRegEx": "Marsland et al\\.", "year": 2002}, {"title": "Learning rate schedules for self-organizing maps", "author": ["F. Mulier", "V. Cherkassky"], "venue": "Pattern Recognition, 1994. Vol. 2-Conference B: Computer Vision &amp; Image Processing., Proceedings of the 12th IAPR International. Con-", "citeRegEx": "Mulier and Cherkassky,? 1994", "shortCiteRegEx": "Mulier and Cherkassky", "year": 1994}, {"title": "Evolutional development of a multilevel neural network", "author": ["S.V. Odri", "D.P. Petrovacki", "G.A. Krstonosic"], "venue": "Neural Networks, 6(4):583\u2013595.", "citeRegEx": "Odri et al\\.,? 1993", "shortCiteRegEx": "Odri et al\\.", "year": 1993}, {"title": "Application of a selforganizing map to select representative species in multivariate analysis: A case study determining diatom distribution patterns across France", "author": ["Park", "Y.-S", "J. Tison", "S. Lek", "Giraudel", "J.-L", "M. Coste", "F. Delmas"], "venue": null, "citeRegEx": "Park et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Park et al\\.", "year": 2006}, {"title": "The growing hierarchical self-organizing map: exploratory analysis of high-dimensional data", "author": ["A. Rauber", "D. Merkl", "M. Dittenbach"], "venue": "Neural Networks, IEEE Transactions on, 13(6):1331\u20131341.", "citeRegEx": "Rauber et al\\.,? 2002", "shortCiteRegEx": "Rauber et al\\.", "year": 2002}, {"title": "DoSO: a document self-organizer", "author": ["G. Spanakis", "G. Siolas", "A. Stafylopatis"], "venue": "Journal of Intelligent Information Systems, 39(3):577\u2013610.", "citeRegEx": "Spanakis et al\\.,? 2012", "shortCiteRegEx": "Spanakis et al\\.", "year": 2012}, {"title": "SOM toolbox for Matlab 5", "author": ["J. Vesanto", "J. Himberg", "E. Alhoniemi", "J. Parhankangas"], "venue": "Citeseer.", "citeRegEx": "Vesanto et al\\.,? 2000", "shortCiteRegEx": "Vesanto et al\\.", "year": 2000}, {"title": "The geometric structure of the brain fiber pathways", "author": ["V.J. Wedeen", "D.L. Rosene", "R. Wang", "G. Dai", "F. Mortazavi", "P. Hagmann", "J.H. Kaas", "Tseng", "W.-Y.I."], "venue": "Science, 335(6076):1628\u2013 1634.", "citeRegEx": "Wedeen et al\\.,? 2012", "shortCiteRegEx": "Wedeen et al\\.", "year": 2012}, {"title": "An evolutionary constructive and pruning algorithm for artificial neural networks and its prediction applications", "author": ["Yang", "S.-H.", "Chen", "Y.-P."], "venue": "Neurocomputing, 86:140\u2013149.", "citeRegEx": "Yang et al\\.,? 2012", "shortCiteRegEx": "Yang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "The Self-Organizing Map (SOM) (Kohonen, 2001) is an unsupervised neural network model which effectively maps high-dimensional data to a lowdimensional space (usually two-dimensional).", "startOffset": 30, "endOffset": 45}, {"referenceID": 18, "context": ", similar input patterns are expected to be mapped to neighboring neurons in the output grid) (Kohonen, 1981).", "startOffset": 94, "endOffset": 109}, {"referenceID": 7, "context": "SOM manages to achieve dimensionality reduction, abstraction, clustering and visualization of the input data and this is the reason that it has been applied successfully to many different domains and datasets like financial data (Deboeck and Kohonen, 2013), speech recognition (Kohonen, 1988), image classification (Lu, 1990), document clustering (Lagus et al.", "startOffset": 229, "endOffset": 256}, {"referenceID": 19, "context": "SOM manages to achieve dimensionality reduction, abstraction, clustering and visualization of the input data and this is the reason that it has been applied successfully to many different domains and datasets like financial data (Deboeck and Kohonen, 2013), speech recognition (Kohonen, 1988), image classification (Lu, 1990), document clustering (Lagus et al.", "startOffset": 277, "endOffset": 292}, {"referenceID": 23, "context": "SOM manages to achieve dimensionality reduction, abstraction, clustering and visualization of the input data and this is the reason that it has been applied successfully to many different domains and datasets like financial data (Deboeck and Kohonen, 2013), speech recognition (Kohonen, 1988), image classification (Lu, 1990), document clustering (Lagus et al., 1999), (Spanakis et al.", "startOffset": 347, "endOffset": 367}, {"referenceID": 30, "context": ", 1999), (Spanakis et al., 2012) .", "startOffset": 9, "endOffset": 32}, {"referenceID": 22, "context": "The fundamental principle of SOM is the soft competition between the nodes in the output layer; not only the node (winner) but also its neighbors are updated (Kohonen, 2012).", "startOffset": 158, "endOffset": 173}, {"referenceID": 20, "context": "In the batch SOM algorithm (proposed in (Kohonen, 1993)), the weights", "startOffset": 40, "endOffset": 55}, {"referenceID": 26, "context": "In addition to facilitating the development of data-partitioned parallel methods, this also eliminates concerns (Mulier and Cherkassky, 1994) that input records encountered later in the training sequence may overly influence the final results.", "startOffset": 112, "endOffset": 141}, {"referenceID": 5, "context": "The learning rate parameter \u03b1(t) does not appear in the batch SOM algorithm, thus eliminating a potential source of poor convergence (Ceccarelli et al., 1993) if this parameter is not properly specified.", "startOffset": 133, "endOffset": 158}, {"referenceID": 17, "context": "There are many approaches which apply these algorithms in classic neural networks (Islam et al., 2009), (Bortman and Aladjem, 2009), (Han and Qiao, 2013), (Yang and Chen, 2012).", "startOffset": 82, "endOffset": 102}, {"referenceID": 4, "context": ", 2009), (Bortman and Aladjem, 2009), (Han and Qiao, 2013), (Yang and Chen, 2012).", "startOffset": 9, "endOffset": 36}, {"referenceID": 13, "context": "Also, there are many variations of SOM that allow a more flexible structure of the output map which can be divided into two categories: In the first type, we include growing grid (GG) (Fritzke, 1995), incremental GG (Blackmore and Miikkulainen, 1993), growing SOM (GSOM) (Alahakoon et al.", "startOffset": 184, "endOffset": 199}, {"referenceID": 3, "context": "Also, there are many variations of SOM that allow a more flexible structure of the output map which can be divided into two categories: In the first type, we include growing grid (GG) (Fritzke, 1995), incremental GG (Blackmore and Miikkulainen, 1993), growing SOM (GSOM) (Alahakoon et al.", "startOffset": 216, "endOffset": 250}, {"referenceID": 0, "context": "Also, there are many variations of SOM that allow a more flexible structure of the output map which can be divided into two categories: In the first type, we include growing grid (GG) (Fritzke, 1995), incremental GG (Blackmore and Miikkulainen, 1993), growing SOM (GSOM) (Alahakoon et al., 2000) all coming with different variants.", "startOffset": 271, "endOffset": 295}, {"referenceID": 1, "context": "MIGSOM (Ayadi et al., 2012) allows a more flexible structure by adding neurons internally and from the", "startOffset": 7, "endOffset": 27}, {"referenceID": 12, "context": "We denote growing cell structures (GCSs) (Fritzke, 1994), growing neural gas (GNG) (Fritzke et al.", "startOffset": 41, "endOffset": 56}, {"referenceID": 25, "context": ", 1995) and growing where required (Marsland et al., 2002).", "startOffset": 35, "endOffset": 58}, {"referenceID": 29, "context": "Limitations in growing and visualization led to hierarchical variants of the previous model like the Growing Hierarchical SOM (GHSOM) (Rauber et al., 2002).", "startOffset": 134, "endOffset": 155}, {"referenceID": 8, "context": "Other approaches (like TreeGNG (Doherty et al., 2005) or TreeGCS (Hodge and Austin, 2001)) use dendrograms for representation but due to this tree structure they lose the topological properties.", "startOffset": 31, "endOffset": 53}, {"referenceID": 16, "context": ", 2005) or TreeGCS (Hodge and Austin, 2001)) use dendrograms for representation but due to this tree structure they lose the topological properties.", "startOffset": 19, "endOffset": 43}, {"referenceID": 31, "context": "This process facilitates training time in contrast to starting from a small-size structure and building on that as other approaches do (Vesanto et al., 2000).", "startOffset": 135, "endOffset": 157}, {"referenceID": 9, "context": "It is also in agreement with the neural development which suggests that nearly all neural cells used through human lifetime have been produced in the first months of life (Dowling, 2007).", "startOffset": 171, "endOffset": 186}, {"referenceID": 28, "context": "Several empirical rules (Park et al., 2006) suggest that the number of neurons should be 5 \u00b7 \u221a N", "startOffset": 24, "endOffset": 43}, {"referenceID": 10, "context": "The eigenvalues ratio shows how well the data is flattened and elongated (Est\u00e9vez et al., 2012).", "startOffset": 73, "endOffset": 95}, {"referenceID": 0, "context": "Formula used is GT = \u2212ln(D)\u00d7 ln(SF) (from (Alahakoon et al., 2000)).", "startOffset": 42, "endOffset": 66}, {"referenceID": 27, "context": "Regarding the new neuron that will be added, we follow the the biological process of \u2018cell division\u2019 (Odri et al., 1993).", "startOffset": 101, "endOffset": 120}, {"referenceID": 27, "context": "where wu refers to the weight vector of neuron u (neuron that is splitted) and \u03b2 is a mutation parameter which can take either a fixed or random value according to a certain distribution rule (following (Odri et al., 1993)).", "startOffset": 203, "endOffset": 222}, {"referenceID": 32, "context": "vision- or hearing-impaired subjects (Wedeen et al., 2012).", "startOffset": 37, "endOffset": 58}, {"referenceID": 2, "context": "Quantization Error (QE) and Topographic Error (TE) were used as intrinsic measures of evaluation (for more details readers are encouraged to read (Bauer et al., 1999)).", "startOffset": 146, "endOffset": 166}], "year": 2016, "abstractText": "Self-Organizing Map (SOM) is a neural network model which is used to obtain a topology-preserving mapping from the (usually high dimensional) input/feature space to an output/map space of fewer dimensions (usually two or three in order to facilitate visualization). Neurons in the output space are connected with each other but this structure remains fixed throughout training and learning is achieved through the updating of neuron reference vectors in feature space. Despite the fact that growing variants of SOM overcome the fixed structure limitation they increase computational cost and also do not allow the removal of a neuron after its introduction. In this paper, a variant of SOM is proposed called AMSOM (Adaptive Moving Self-Organizing Map) that on the one hand creates a more flexible structure where neuron positions are dynamically altered during training and on the other hand tackles the drawback of having a predefined grid by allowing neuron addition and/or removal during training. Experiments using multiple literature datasets show that the proposed method improves training performance of SOM, leads to a better visualization of the input dataset and provides a framework for determining the optimal number and structure of neurons.", "creator": "TeX"}}}