{"id": "1605.06792", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2016", "title": "Active Nearest-Neighbor Learning in Metric Spaces", "abstract": "We politican propose subscription-based a winrod pool - neuenschwander based lc1 non - armscor parametric nationalrat active learning algorithm springdale for abbitt general metric crossbill spaces, called called MArgin Regularized comic Metric gncc Active Nearest 54.88 Neighbor (MARMANN ), slamboree which estrildidae outputs a pfund nearest - t\u00e9miscouata neighbor competes classifier. spoetzl We mixer give skittering prediction error fellas guarantees cugat that cacos depend kargo on rudra the noisy - suther margin properties expro of ausbil the input bocaccio sample, and laurie are competitive with those obtained kdpi by previously huling proposed passive learners. We prove stoneleigh that resorting the antiplatelet label complexity of MARMANN is hetz significantly iko lower atok than reverend that macneal of any passive learner with waipa similar metn error hoarfrost guarantees. Our 25.17 algorithm is crecy based geekiness on a grabin generalized warez sample 2-0-13-0 compression zh00r scheme and madow a new 49.85 label - efficient active 33-billion model - selection knoedler procedure.", "histories": [["v1", "Sun, 22 May 2016 14:00:27 GMT  (23kb)", "https://arxiv.org/abs/1605.06792v1", null], ["v2", "Sun, 16 Oct 2016 08:23:18 GMT  (37kb)", "http://arxiv.org/abs/1605.06792v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.ST stat.TH", "authors": ["aryeh kontorovich", "sivan sabato", "ruth urner"], "accepted": true, "id": "1605.06792"}, "pdf": {"name": "1605.06792.pdf", "metadata": {"source": "CRF", "title": "Active Nearest-Neighbor Learning in Metric Spaces", "authors": ["Aryeh Kontorovich", "Sivan Sabato", "Ruth Urner"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n06 79\n2v 2\n[ cs\n.L G\n] 1"}, {"heading": "1 Introduction", "text": "Active learning is a framework for reducing the amount of label supervision for prediction tasks. While labeling large amounts of data can be expensive and time-consuming, unlabeled data is often much easier to come by. In this paper we propose a non-parametric pool-based active learning algorithm for general metric spaces, which outputs a nearest-neighbor classifier.\nIn pool-based active learning [McCallum and Nigam, 1998], a collection of random examples is provided, and the algorithm can interactively query an oracle to label some of the examples. The goal is good prediction accuracy, while keeping the label complexity \u2014 that is, the number of labels queried \u2014 low. Our algorithm, MArgin Regularized Metric Active Nearest Neighbor (MARMANN), receives a pool of unlabeled examples in a general metric space, and outputs a variant of the 1-nearest-neighbor classifier, implemented by a 1-nearest-neighbor rule. The algorithm obtains a prediction error guarantee that depends on a noisy-margin property of the input sample, and has a provably smaller label complexity than any passive learner with a similar guarantee.\nActive learning has been mostly studied in a parametric setting, in which learning takes place with respect to a fixed hypothesis class with a bounded capacity. In contrast, the question of whether active querying strategies can yield label savings for non-parametric methods in a general setting, without distributional assumptions, had not been analyzed prior to this work. Here, we provide a first demonstration that this is indeed possible. We discuss related work in detail in Section 1.1 below.\nOur contributions. MARMANN is a new non-parametric pool-based active learning algorithm, which obtains an error guarantee competitive with that of a noisy-margin-based passive learner. Additionally, it provably uses significantly fewer labels in nontrivial regimes. This is the first non-parametric active learner for general metric spaces, which achieves competitive prediction error guarantees to the passive learner, while provably improving label complexity. The guarantees of MARMANN are given in Theorem 3.1 in Section 3. We further provide a passive learning lower bound (Theorem 3.2), which together with Theorem 3.1 shows that MARMANN can have a significantly reduced label complexity compared to any passive learner. The passive lower bound is more general than previous lower bounds, relies on a novel technique, and may be of independent interest. Additionally, we give an active label complexity lower bound (Theorem 3.3), which holds for any active learner with similar error guarantees as MARMANN. The proof of this active lower bound relies on a new No-Free-Lunch type result, which holds for active learning algorithms.\nOur approach. Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale.\nA central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information.\nWe derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al. [2016b], proposed in Gottlieb et al. [2014a], has been shown to be Bayes consistent [Kontorovich and Weiss, 2015]. The Bayes-consistency of the passive version of our approach is the subject of ongoing work.\nPaper structure. Related work is discussed in Section 1.1. We lay down the preliminaries in Section 2. In Section 3 we provide our main result: Theorem 3.1, which gives error and label complexity guarantees for MARMANN. Additionally we state the passive and active lower bounds, Theorem 3.2 and Theorem 3.3. The rest of the paper is devoted to the description and analysis of MARMANN, and proof of the main results. Section 4 shows how MARMANN defines the nearest neighbor rule for a given scale, and Section 5 describes the model selection procedure of MARMANN. Theorem 3.1 is proved in Section 6, based on a framework for compression with side information. The passive lower bound in Theorem 3.2 is proved in Section 7. The active lower bound Theorem 3.3 is proved in Section 8. We conclude with a discussion in Section 9."}, {"heading": "1.1 Related work", "text": "The theory of active learning has received considerable attention in the past decade [e.g., Dasgupta, 2004, Balcan et al., 2007, 2009, Hanneke, 2011, Hanneke and Yang, 2015]. Active learning theory has been mostly studied in a parametric setting (that is, learning with respect to a fixed hypothesis class with a bounded capacity). Benefits and limitations of various active querying strategies have been proven in the realizable setting [Dasgupta, 2004, Balcan et al., 2007, Gonen et al., 2013b,a] as well as in the agnostic case [Balcan et al., 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010].\nThe potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees.\nCastro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals. The paradigm of cluster-based active learning [Dasgupta and Hsu, 2008] has been shown to provide label savings under some distributional clusterability assumptions [Urner et al., 2013, Kpotufe et al., 2015]. Dasgupta and Hsu [2008] showed that a suitable cluster-tree can yield label savings in this framework, and papers following up quantified the label savings under distributional clusterability assumptions. However, no active non-parametric strategy has been proposed so far that has label complexity guarantees for general distributions and for general metric spaces. Here, we provide the first such algorithm and guarantees.\nThe passive nearest-neighbor classifier, introduced by Fix and Hodges [1951, 1989], is popular among theorists and practitioners alike [Fix and Hodges, 1989, Cover and Hart, 1967, Stone, 1977, Kulkarni and Posner, 1995, Boiman et al., 2008]. This paradigm is applicable in general metric spaces, and its simplicity is an attractive feature for both implementation and analysis. When appropriately regularized \u2014 either by taking a majority vote among the k nearest neighbors [Stone, 1977, Devroye and Gy\u00f6rfi, 1985, Zhao, 1987], or by enforcing a margin separating the classes [von Luxburg and Bousquet, 2004, Gottlieb et al., 2014a, Kontorovich and Weiss, 2015] \u2014 this type of learner can be made Bayes-consistent. Another desirable property of nearest-neighbor-based methods is their ability to generalize at a rate that scales with the intrinsic data dimension, which can be much lower than that of the ambient space [Kpotufe, 2011, Gottlieb et al., 2014a, 2016a, Chaudhuri and Dasgupta, 2014]. Furthermore, margin-based regularization makes nearest neighbor classifiers ideally suited for sample compression, which yields a compact representation, faster classification runtime, and improved generalization performance [Gottlieb et al., 2014b]. The resulting error guarantees can be stated in terms of the sample\u2019s noisy-margin, which depends on the distances between differently-labeled examples in the input sample.\nActive learning strategies specific to nearest neighbor classification have recently received attention. It has been shown that certain active querying rules maintain Bayes consistency for nearest neighbor classification, while other, seemingly natural, rules do not lead\nto a consistent algorithm [Dasgupta, 2012]. A selective querying strategy has been shown to be beneficial for nearest neighbors under covariate shift [Berlind and Urner, 2015], where one needs to adapt to a change in the data generating process. However, the querying rule in that work is based solely on information in the unlabeled data, to account for a shift in the distribution over the covariates. It does not imply any label savings in the standard learning setting, where training and test distribution are identical. In contrast, our current work demonstrates how an active learner can take label information into account, to reduce the label complexity of a general nearest neighbor method in the standard setting."}, {"heading": "2 Preliminaries", "text": "In this section we lay down the necessary preliminaries. We formally define the setting and necessary notation in Section 2.1. We discuss nets in metric spaces in Section 2.2, and present the guarantees of the compression-based passive learner of Gottlieb et al. [2016b] in Section 2.3."}, {"heading": "2.1 Setting and notation", "text": "We consider learning in a general metric space (X , \u03c1), where X is a set and \u03c1 is the metric on X . Our problem setting is that of classification of the instance space X into some finite label set Y . Assume that there is some distribution D over X \u00d7 Y , and let S \u223c Dm be a labeled sample of size m, where m is an integer. Denote the sequence of unlabeled points in S by U(S). We sometimes treat S and U(S) as multisets, since the order is unimportant. For a labeled multiset S \u2286 X \u00d7 Y and y \u2208 Y , denote Sy := {x | (x, y) \u2208 S}; in particular, U(S) = \u222ay\u2208YSy .\nThe error of a classifier h : X \u2192 Y on D is denoted\nerr(h,D) := P[h(X) 6= Y ],\nwhere (X,Y ) \u223c D. The empirical error on a labeled sample S instantiates to\nerr(h, S) = 1 |S| \u2211 I[h(X) 6= Y ].\nA passive learner receives a labeled sample Sin as input. An active learner receives the unlabeled part of the sample Uin := U(Sin) as input, and is allowed to adaptively select examples from Uin and request their label from Sin. When either learner terminates, it outputs a classifier h\u0302 : X \u2192 Y , with the goal of achieving a low err(h\u0302,D). An additional goal of the active learner is to achieve a performance competitive with that of the passive learner, while querying considerably fewer labels.\nThe diameter of a set A \u2286 X is defined by\ndiam(A) := sup a,a\u2032\u2208A\n\u03c1(a, a\u2032).\nDenote the index of the closest point in U to x \u2208 X by\n\u03ba(x, U) := argmin i:xi\u2208U \u03c1(x, xi).\nWe assume here and throughout this work that when there is more than one minimizer for \u03c1(x, xi), ties are broken arbitrarily (but in a consistent fashion). Any labeled sample S = ((xi, yi))i\u2208[k] naturally induces the nearest-neighbor classifier hnnS : X \u2192 Y , via hnnS (x) := y\u03ba(x,U(S)). For a set Z \u2286 X , denote\n\u03ba(Z,U) := {\u03ba(z, U) | z \u2208 Z}.\nFor x \u2208 X , and t > 0, denote by ball(x, t) the (closed) ball of radius t around x:\nball(x, t) := {x\u2032 \u2208 X | \u03c1(x, x\u2032) \u2264 t} ."}, {"heading": "2.2 Nets", "text": "A set A \u2286 X is t-separated if infa,a\u2032\u2208A:a 6=a\u2032 \u03c1(a, a\u2032) \u2265 t. For A \u2286 B \u2286 X , the set A is a t-net of B if A is t-separated and B \u2286 \u22c3a\u2208A ball(a, t).\nThe size of a t-net of a metric space is strongly related to its doubling dimension. The doubling dimension is the effective dimension of the metric space, which controls generalization and runtime performance of nearest-neighbors [Kpotufe, 2011, Gottlieb et al., 2014a]. It is defined as follows. Let \u03bb = \u03bb(X ) be the smallest number such that every ball in X can be covered by \u03bb balls of half its radius, where all balls are centered at points of X . Formally,\n\u03bb(X ) := min{\u03bb \u2208 N : \u2200x \u2208 X , r > 0, \u2203x1, . . . , x\u03bb \u2208 X : ball(x, r) \u2286 \u222a\u03bbi=1ball(xi, r/2)}.\nThen the doubling dimension of X is defined by ddim(X ) := log2 \u03bb. In line with modern literature, we work in the low-dimension, large-sample regime, where the doubling dimension is assumed to be constant, and hence sample complexity and algorithmic runtime may depend on it exponentially. This exponential dependence is unavoidable, even under margin assumptions, as previous analyses [Kpotufe, 2011, Gottlieb et al., 2014a] indicate. Generalization bounds in terms of the doubling dimension of the hypothesis space were established in Bshouty et al. [2009], while runtime and generalization errors in terms of ddim(X ) were given in Gottlieb et al. [2014a].\nConstructing a minimum size t-net for a general set B is NP-hard [Gottlieb and Krauthgamer, 2010]. However, a simple greedy algorithm constructs a (not necessarily minimal) t-net in time O(m2) [Gottlieb et al., 2014b, Algorithm 1]. There is also an algorithm for constructing a t-net in time 2O(ddim(X ))m log(1/t) [Krauthgamer and Lee, 2004, Gottlieb et al., 2014b]. The size of any t-net of a metric space A \u2286 X is at most \u2308diam(A)/t\u2309ddim(X )+1 [Krauthgamer and Lee, 2004]. In addition, the size of any t-net is at most 2ddim(A) times the size of the minimal t-net, as the following easy lemma shows.\nLemma 2.1 (comparison of two nets). Let t > 0 and suppose that M1,M2 are t-nets of A \u2286 X . Then |M1| \u2264 2ddim(A)|M2|. Proof. Suppose that |M1| \u2265 k|M2| for some positive integer k. SinceM1 \u2286 \u22c3\nx\u2208M2 ball(x, t), it follows from the pigeonhole principle that at least one of the points in M2 must cover at least k points in M1. Thus, suppose that x \u2208 M2 covers the set Z = {z1, . . . , zl} \u2286 M1, meaning that Z \u2286 ball(x, t), where l = |Z| \u2265 k. By virtue of belonging to the t-net M1, the set Z is t-separated. Therefore, from the definition of the doubling dimension, we have |Z| \u2264 2ddim(A), hence k \u2264 |Z| \u2264 2ddim(A).\nThroughout the paper, we fix a deterministic procedure for constructing a t-net, and denote its output for a multiset U \u2286 X by Net(U, t). Let Par(U, t) be a partition of X into regions induced by Net(U, t), that is: for Net(U, t) = {x1, . . . , xN}, define Par(U, t) := {P1, . . . , PN}, where Pi = {x \u2208 X | \u03ba(x,Net(U, t)) = i}. For t > 0, let N (t) := |Net(Uin, t)| be the size of the t-net for the input sample.\nAs shown in Gottlieb and Krauthgamer [2013], the doubling dimension is \u201calmost hereditary\u201d in the sense that forA \u2282 X , we have ddim(A) \u2264 cddim(X ) for some universal constant c \u2264 2 [Feldmann et al., 2015, Lemma 6.6]. For simplicity, the bounds above are presented in terms of ddim(X ), the doubling dimension of the ambient space. It should be noted that one can obtain tighter bounds in terms of ddim(U(S)) when the latter is substantially lower than that of the ambient space, and it is also possible to perform metric dimensionality reduction, as in Gottlieb et al. [2013]."}, {"heading": "2.3 Passive compression-based nearest-neighbors", "text": "Non-parameteric binary classification admits performance guarantees that scale with the sample\u2019s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al. [2010] analyzed the generalization performance via the technique of Lipschitz extension. Later, it was noticed in Gottlieb et al. [2014b] that the presence of a margin allows for compression \u2014 in fact, nearly optimally so.\nWe say that a labeled multiset S is (\u03bd, t)-separated, for \u03bd \u2208 [0, 1] and t > 0 (representing a margin t with noise \u03bd), if one can remove a \u03bd-fraction of the points in S, and in the resulting multiset, points with different labels are at least t-far from each other. Formally, we have the following definition.\nDefinition 2.2. S is (\u03bd, t)-separated if there exists a subsample S\u0303 \u2286 S such that\n1. |S \\ S\u0303| \u2264 \u03bd|S| and\n2. \u2200y1 6= y2 \u2208 Y, a \u2208 S\u0303y1 , b \u2208 S\u0303y2 , we have \u03c1(a, b) \u2265 t.\nFor a given labeled sample S, denote by \u03bd(t) the smallest value \u03bd such that S is (\u03bd, t)separated. Gottlieb et al. [2016b] propose a passive learner with the following guarantees1 as a function of the separation of S. Setting \u03b1 := m/(m\u2212N), define the following form of a generalization bound:\nGB(\u01eb,N, \u03b4,m, k) := \u03b1\u01eb+ 2\n3\n(N + 1) log(mk) + log(1\u03b4 )\nm\u2212N + 3\u221a 2\n\u221a\n\u03b1\u01eb((N + 1) log(mk) + log(1\u03b4 ))\nm\u2212N .\nFurther, for an integer m and \u03b4 \u2208 (0, 1), denote\nGmin(m, \u03b4) := min t>0\nGB(\u03bd(t),N (t), \u03b4,m, 1).\n1The guarantees hold for the more general case of semimetrics.\nTheorem 2.3 (Gottlieb et al. [2016b]). Let m be an integer, \u03b4 \u2208 (0, 1). There exists a passive learning algorithm that returns a nearest-neighbor classifier hnnSpas , where Spas \u2286 Sin, such that, with probability 1\u2212 \u03b4,\nerr(hnnSpas ,D) \u2264 Gmin(m, \u03b4).\nThe passive algorithm of Gottlieb et al. [2016b] generates Spas of size approximately N (t) for the optimal scale t > 0 (found by searching over all scales), by removing the |Sin|\u03bd(t) points that obstruct the t-separation between different labels in Sin, and then selecting a subset of the remaining labeled examples to form Spas, so that the examples are a t-net for Sin (not including the obstructing points). For the binary classification case (|Y| = 2) an efficient algorithm is shown in Gottlieb et al. [2016b]. However, in the general multiclass case, it is not known how to find a minimal t-separation efficiently \u2014 a naive approach requires solving the NP-hard problem of vertex cover. Our approach, which we detail below, circumvents this issue, and provides an efficient algorithm also for the multiclass case."}, {"heading": "3 Main results", "text": "We propose a novel approach for generating a subset for a nearest-neighbor rule. This approach, detailed in the following sections, does not require finding and removing all the obstructing points in Sin, and can be implemented in an active setting using a small number of labels. The resulting active learning algorithm, MARMANN, has an error guarantee competitive with that of the passive learner, and a label complexity that can be significantly lower. We term the subset used by the nearest-neighbor rule a compression set.\nAlgorithm 1 MARMANN: MArgin Regularized Metric Active Nearest Neighbor input Unlabeled sample Uin of size m, \u03b4 \u2208 (0, 1). t\u0302 \u2190 SelectScale(\u03b4). # SelectScale is given in Section 5, Alg. 4. S\u0302 \u2190 GenerateNNSet(t\u0302, [N (t\u0302)], \u03b4). # GenerateNNSet is given in Section 4, Alg. 2. Output hnn\nS\u0302 .\nMARMANN, listed in Alg. 1, operates as follows. First, a scale t\u0302 > 0 is selected, by calling t\u0302 \u2190 SelectScale(\u03b4), where SelectScale is our model selection procedure. SelectScale has access to Uin, and queries labels from Sin as necessary. It estimates the generalization error bound GB for several different scales, and executes a procedure similar to binary search to identify a good scale. The binary search keeps the number of estimations (and thus requested labels) small. Crucially, our estimation procedure is designed to prevent the search from spending a number of labels that depends on the net size of the smallest possible scale t, so that the total label complexity of MARMANN depends only on the error of the selected t\u0302. Second, the selected scale t\u0302 is used to generate the compression set by calling S\u0302 \u2190 GenerateNNSet(t\u0302, [N (t\u0302)], \u03b4), where GenerateNNSet is our procedure for generating the compression set. Our main result is the following guarantee for MARMANN.\nTheorem 3.1 (Main result; Guarantee for MARMANN). Let Sin \u223c Dm, where m \u2265 max(6, |Y|), \u03b4 \u2208 (0, 14 ). Let hnnS\u0302 be the output of MARMANN(Uin, \u03b4), where S\u0302 \u2286 X \u00d7 Y ,\nand let N\u0302 := |S\u0302|. Let h\u0302 := hnn S\u0302 and \u01eb\u0302 := err(h\u0302, Sin), and denote G\u0302 := GB(\u01eb\u0302, N\u0302 , \u03b4,m, 1). With a probability of 1\u2212 \u03b4 over Sin and randomness of MARMANN,\nerr(h\u0302,D) \u2264 2G\u0302 \u2264 O (Gmin(m, \u03b4)) ,\nand the number of labels from Sin requested by MARMANN is at most\nO\n(\nlog3( m\n\u03b4 )\n(\n1 G\u0302 log( 1 G\u0302 ) +mG\u0302\n))\n.\nHere the O(\u00b7) notation hides only universal multiplicative constants.\nOur error guarantee is thus a constant factor over the error guarantee of the passive learner of [Gottlieb et al., 2016b], given in Theorem 2.3.\nTo observe the advantages of MARMANN over a passive learner, consider a scenario in which the upper bound Gmin of Theorem 2.3, as well as the Bayes error of D, are of order \u0398\u0303(1/ \u221a m). Then G\u0302 = \u0398(1/ \u221a m) as well. Therefore, MARMANN obtains a prediction error guarantee of \u0398\u0303(1/ \u221a m), similarly to the passive learner, but it uses only \u0398\u0303( \u221a m) labels instead of m. In contrast, the following result shows that no learner that selects labels uniformly at random from Sin can compete with MARMANN: Theorem 3.2 below shows that for any passive learner that uses \u0398\u0303( \u221a m) random labels from Sin, there exists a distribution D with the above properties, for which the prediction error of the passive learner in this case is \u2126\u0303(m\u22121/4), a decay rate which is almost quadratically slower than the O\u0303(1/ \u221a m) rate achieved by MARMANN. Thus, the guarantees of MARMANN cannot be matched by any passive learner.\nTheorem 3.2 (Passive lower bound). Let m > 0 be an integer, and suppose that (X , \u03c1) is a metric space such that for some t\u0304 > 0, there is a t\u0304-net T of X of size \u0398(\u221am). Consider any passive learning algorithm that maps i.i.d. samples S\u2113 \u223c D\u2113 from some distribution D over X \u00d7 {\u22121, 1}, to functions h\u0302\u2113 : X \u2192 {\u22121, 1}. For any such algorithm and any \u2113 = \u0398\u0303( \u221a m), there exists a distribution D such that:\ni. The Bayes error of D is \u0398(1/\u221am);\nii. With at least a constant probability, both of the following events occur:\n(a) The passive learner achieves error err(h\u0302\u2113,D) = \u2126\u0303(m\u22121/4), (b) Gmin(m, \u03b4) = \u0398\u0303(1/ \u221a m).\nWe note that while this lower bound assumes that the passive learner observes only the random labeled sample of size \u2113, in fact the proof of this theorem holds also if the algorithm has access to the full unlabeled sample of size m, from which S\u2113 is sampled. This proves that MARMANN even improves over a semi-supervised learner that has access to the same amount of unlabeled data. That is, the label savings of MARMANN stem from actively selecting labels, and are not achievable by merely exploiting information from unlabeled data or by randomly selecting examples to label.\nWe deduce Theorem 3.2 from a more general result, which might be of independent interest. Theorem 7.1, given in Section 7, improves existing passive learning sample complexity\nlower bounds. In particular, our result removes the restrictions of previous lower bounds on the relationship between the sample size, the VC-dimension, and the noise level, which render existing bounds inapplicable to our parameter regime. The proof of Theorem 3.2 is given thereafter in Section 7, as a consequence of Theorem 7.1.\nWe further provide a label complexity lower bound, in Theorem 3.3 below, which holds for any active learner that obtains similar guarantees to those of MARMANN. The lower bound shows that any such active learning algorithm has a label complexity which is \u2126\u0303(mGmin(m, \u03b4)), for a wide range of values of Gmin(m, \u03b4). This implies that the term mG\u0302 in the upper bound of the label complexity of MARMANN in Theorem 3.1 cannot be significantly improved.\nTheorem 3.3 (Active lower bound). Let X = R, \u03b4 \u2208 (0, 1/14). Let C \u2265 1, and let A be an active learning algorithm that outputs h\u0302. Suppose that for any distribution D over X \u00d7 Y , if the input unlabeled sample is of size m, then err(h\u0302,D) \u2264 CGmin(m, \u03b4) with probability at least 1\u2212 \u03b4. Then for any \u03b1 \u2208 ( log(m)+log(28)\n8 \u221a 2m , 1240C ) there exists an a distribution D such with probability at least 128 over S \u223c Dm and the randomness of A, both of the following events hold:\n1. \u03b1 \u2264 Gmin(m, \u03b4) \u2264 30\u03b1\n2. A queries at least 12 \u230a mGmin(m,\u03b4)\u2212log(m\u03b4 ) 30 log(m) \u230b labels.\nIn the rest of the paper, the components of MARMANN are described in detail, and the main results are proved."}, {"heading": "4 Active nearest-neighbor at a given scale", "text": "A main challenge for active learning in our non-parametric setting is performing model selection, that is, selecting a good scale t similarly to the passive learner of Gottlieb et al. [2016b]. In the passive supervised setting, the approach developed in several previous works [Gottlieb et al., 2010, 2014b, Kontorovich and Weiss, 2014, Gottlieb et al., 2014a, Kontorovich and Weiss, 2015] performs model selection by solving a minimum vertex cover problem for each considered scale t, so as to eliminate all of the t-blocking pairs \u2014 i.e., pairs of differently labeled points within a distance t. The passive algorithm generates a compression set by first finding and removing from Sin all points that obstruct (\u03bd, t)-separation at a given scale t > 0. This incurs a computational cost but no significant sample complexity increase, aside from the standard logarithmic factor that comes from stratifying over data-dependent hierarchies [Shawe-Taylor et al., 1998].\nWhile this approach works for passive learning, in the active setting we face a crucial challenge: estimating the error of a nearest-neighbor rule at scale t using a small number of samples. A key insight that we exploit in this work is that instead of eliminating the blocking pairs, one may simply relabel some of the points in the compression set, and this would also generate a low-error nearest neighbor rule. This new approach enables estimation of the sample accuracy of a (possibly relabeled) t-net by label-efficient active sampling. In addition, this approach is significantly simpler than estimating the size of the minimum vertex cover of the t-blocking graph. Moreover, we gain improved algorithmic efficiency, by avoiding the relatively expensive vertex cover procedure.\nA small technical difference, which will be evident below, is that in this new approach, examples in the compression set might have a different label than their original label in Sin. Standard sample compression analysis [e.g. Graepel et al., 2005] assumes that the classifier is determined by a small number of labeled examples from Sin. This does not allow the examples in the compression set to have a different label than their original label in Sin. Therefore, we require a slight generalization of previous compression analysis (following previous works on compression, see details in Section 6.1), which allows adding side information to the compression set. This side information will be used to set the label of each of the examples in the compression set. The generalization incurs a small statistical penalty, which we quantify in Section 6, as a preliminary to proving Theorem 3.1.\nWe now describe our approach to generating a compression set for a given scale t > 0. Recall that \u03bd(t) is the smallest value for which Sin is (\u03bd, t)-separated. We define two compression sets. The first one, denoted Sa(t), represents an ideal compression set, which induces an empirical error of at most \u03bd(t), but calculating it might require many labels. The second compression set, denoted S\u0302a(t), represents an approximation to Sa(t), which can be constructed using a small number of labels, and induces a sample error of at most 4\u03bd(t) with high probability. MARMANN constructs only S\u0302a(t), while Sa(t) is defined solely for the sake of analysis.\nWe first define the ideal set Sa(t) := {(x1, y1), . . . , (xN , yN)}. The examples in Sa(t) are the points in Net(Uin, t/2), and the label of each example is the majority label, out of the labels of the examples in Sin to which xi is closest. Formally, {x1, . . . , xN} := Net(Uin, t/2), and for i \u2208 [N ], yi := argmaxy\u2208Y |Sy \u2229 Pi|, where Pi = {x \u2208 X | \u03ba(x,Net(U, t/2)) = i} \u2208 Par(Uin, t/2). Lemma 4.1. Let S be a labeled sample of size m, and let {P1, . . . , PN} be a partition of U(S), with maxi diam(Pi) \u2264 t for some t \u2265 0. For i \u2208 [N ], let \u039bi := Syi \u2229 Pi. Then\n\u03bd(t) \u2265 1\u2212 1 m \u2211\ni\u2208[N ] |\u039bi|.\nProof. Let S\u0303 \u2286 S be a subsample that witnesses the (\u03bd(t), t)-separation of S, so that |S\u0303| \u2265 m(1\u2212 \u03bd(t)), and for any two points (x, y), (x\u2032, y\u2032) \u2208 S\u0303, if \u03c1(x, x\u2032) \u2264 t then y = y\u2032. Denote U\u0303 := U(S\u0303). Since maxi diam(Pi) \u2264 t, for any i \u2208 [N ] all the points in U\u0303 \u2229Pi must have the same label in S\u0303. Therefore,\n\u2203y \u2208 Y s.t. U\u0303 \u2229 Pi \u2286 S\u0303y \u2229 Pi.\nHence |U\u0303 \u2229 Pi| \u2264 |\u039bi|. It follows\n|S| \u2212 \u2211\ni\u2208[N ] |\u039bi| \u2264 |S| \u2212\n\u2211\ni\u2208[N ] |U\u0303 \u2229 Pi| = |S| \u2212 |S\u0303| = m \u00b7 \u03bd(t).\nDividing by m we get the statement of the lemma.\nFrom Lemma 4.1, we get the following corollary, which upper bounds the empirical error of hnnSa(t) by \u03bd(t).\nCorollary 4.2. For every t > 0, err(hnnSa(t), Sin) \u2264 \u03bd(t).\nThis corollary is immediate from Lemma 4.1, since for anyPi \u2208 Par(Uin, t/2), diam(Pi) \u2264 t, and\nerr(hnnSa(t), Sin) = 1\u2212 1\nm\n\u2211\ni\u2208[N ] |\u039bi|.\nNow, calculating Sa(t) requires knowing most of the labels in Sin. MARMANN constructs instead an approximation S\u0302a(t), in which the examples are the points in Net(Uin, t/2) (so that U(S\u0302a(t)) = U(Sa(t)) ), but the labels are determined using a bounded number of labels requested from Sin. The labels in S\u0302a(t) are calculated by the simple procedure GenerateNNSet given in Alg. 2. The empirical error of the output of GenerateNNSet is bounded in Theorem 4.3 below.2\nA technicality in Alg. 2 requires explanation: In MARMANN, the generation of S\u0302a(t) will be split into several calls to GenerateNNSet, so that different calls determine the labels of different points in S\u0302a(t). Therefore GenerateNNSet has an additional argument I , which specifies the indices of the points in Net(Uin, t/2) for which the labels should be returned this time. Crucially, if during the run of MARMANN, GenerateNNSet is called again for the same scale t and the same point in Net(Uin, t/2), then GenerateNNSet returns the same label that it returned before, rather than recalculating it using fresh labels from Sin. This guarantees that despite the randomness in GenerateNNSet, the full S\u0302a(t) is well-defined within any single run of MARMANN, and is distributed like the output of GenerateNNSet(t, [N (t/2)], \u03b4), which is convenient for the analysis. Define\nQ := \u2308 18 log(4m3/\u03b4) \u2309 . (1)\nAlgorithm 2 GenerateNNSet(t, I, \u03b4) input Scale t > 0, a target set I \u2286 [N (t/2)], confidence \u03b4 \u2208 (0, 1). output A labeled set S \u2286 X \u00d7 Y of size |I| {x1, . . . , xN} \u2190 Net(Uin, t/2), {P1, . . . , PN} \u2190 Par(Uin, t/2), S \u2190 () for i \u2208 I do\nif y\u0302i has not already been calculated for Uin with this value of t then Draw Q points uniformly at random from Pi and query their labels. Let y\u0302i be the majority label observed in these Q queries. end if S \u2190 S \u222a {(xi, y\u0302i)}.\nend for Output S\nTheorem 4.3. Let S\u0302a(t) be the output of GenerateNNSet(t, [N (t/2)], \u03b4). With a probability at least 1\u2212 \u03b42m2 , the following event, which we denote by E(t), holds:\nerr(hnn S\u0302a(t) , Sin) \u2264 4\u03bd(t). 2In the case of binary labels (|Y| = 2), the problem of estimating Sa(t) can be formulated as a special case of the benign noise setting for parametric active learning, for which tight lower and upper bounds are provided in Hanneke and Yang [2015]. However, our case is both more general (as we allow multiclass labels) and more specific (as we are dealing with a specific \u201chypothesis class\u201d). Thus we provide our own procedure and analysis.\nProof. By Cor. 4.2, err(hnnSa(t), Sin) \u2264 \u03bd(t). In Sa(t), the labels assigned to each point in Net(Uin, t/2) are the majority labels (based onSin) of the points in the regions inPar(Uin, t/2). As above, we denote the majority label for region Pi by yi := argmaxy\u2208Y |Sy \u2229 Pi|. We now compare these labels to the labels y\u0302i assigned by Alg. 2. Let p(i) = |\u039bi|/|Pi| be the fraction of points in Pi which are labeled by the majority label yi, where \u039bi is as defined in Lemma 4.1. Let p\u0302(i) be the fraction of labels equal to yi out of those queried by Alg. 2 in round i. Let \u03b2 := 1/6. By Hoeffding\u2019s inequality and union bounds, we have that with a probability of at least\n1\u2212 2N (t/2) exp(\u2212Q 18 ) \u2265 1\u2212 \u03b4 2m2 ,\nwe have maxi\u2208[N (t/2)] |p\u0302(i)\u2212 p(i)| \u2264 \u03b2. Denote this \u201cgood\u201d event by E\u2032. We now prove that E\u2032 \u21d2 E(t). Let J \u2286 [N (t/2)] = {i | p\u0302(i) > 12}. It can be easily seen that y\u0302i = yi for all i \u2208 J . Therefore, for all x such that \u03ba(x,U(Sa(t))) \u2208 J , hnnS\u0302a(t)(x) = h nn Sa(t) (x), and hence err(hnnS , Sin) \u2264 PX\u223cSin [\u03ba(X,U(Sa(t))) /\u2208 J ] + err(hnnSa(t), Uin).\nThe second term is at most \u03bd(t) by Cor. 4.2, and it remains to bound the first term, on the condition that E\u2032 holds. We have PX\u223cU [\u03ba(X,U(Sa(t))) /\u2208 J ] = 1m \u2211\ni/\u2208J |Pi|. If E\u2032 holds, then for any i /\u2208 J , p(i) \u2264 12 + \u03b2, therefore\n|Pi| \u2212 |\u039bi| = (1\u2212 p(i))|Pi| \u2265 ( 1\n2 \u2212 \u03b2)|Pi|.\nRecall that, by Lemma 4.1, \u03bd(t) \u2265 1\u2212 1m \u2211 i\u2208[N (t/2)] |\u039bi|. Therefore,\n\u03bd(t) \u2265 1\u2212 1 m \u2211\ni\u2208[N (t/2)] |\u039bi|\n= 1\nm\n\u2211\ni\u2208[N (t/2)] (|Pi| \u2212 |\u039bi|)\n\u2265 1 m \u2211\ni/\u2208J (|Pi| \u2212 |\u039bi|)\n\u2265 1 m \u2211 i/\u2208J ( 1 2 \u2212 \u03b2)|Pi|.\nThus, under E\u2032,\nPX\u223cU [\u03ba(X,U(Sa(t))) /\u2208 J ] \u2264 \u03bd(t) 1 2 \u2212 \u03b2 = 3\u03bd(t).\nIt follows that under E\u2032, err(hnnS , Uin) \u2264 4\u03bd(t)."}, {"heading": "5 Model Selection", "text": "We now show how to select the scale t\u0302 that will be used to generate the output nearestneighbor rule. The main challenge is to do this with a low label complexity: Generating the\nfull classification rule for scale t requires a number of labels that depends on N (t), which might be very large. We would like the label complexity of MARMANN to depend only on N (t\u0302) (where t\u0302 is the selected scale), which is of the order mG\u0302. Therefore, during model selection we can only invest a bounded number of labels in each tested scale. In addition, to keep the label complexity low, we would like to avoid testing all scales. In Section 5.1 we describe how we estimate the error on a given scale. In Section 5.2 we provide a search procedure, resembling binary search, which uses the estimation procedure to select a single scale t\u0302."}, {"heading": "5.1 Estimating the error at a given scale", "text": "For t > 0, let S\u0302a(t) be the compressed sample that MARMANN would generate if the selected scale were set to t. Our model selection procedure performs a search, similar to binary search, over the possible scales. For each tested scale t, the procedure estimates the empirical error \u01eb(t) := err(hnn\nS\u0302a(t) , S) within a certain accuracy, using an estimation\nprocedure given below, called EstimateErr. EstimateErr outputs an estimate \u01eb\u0302(t) of \u01eb(t), up to a given threshold \u03b8 > 0, using labels requested from Sin.\nTo estimate the error, we sample random labeled examples from Sin, and check the prediction error of hnn\nS\u0302a(t) on these examples. The prediction error of any fixed hypothesis h\non a random labeled example from Sin is an independent Bernoulli variable with expectation err(h, Sin). EstimateErr is implemented using the following procedure, EstBer, which adaptively estimates the expectation of a Bernoulli random variable to an accuracy specified by the parameter \u03b8, using a small number of random independent Bernoulli experiments. Let B1, B2, . . . \u2208 {0, 1} be i.i.d. Bernoulli random variables. For an integer n, denote p\u0302n = 1 n \u2211n i=1 Bi. The estimation procedure EstBer is given in Alg. 3. We prove a guarantee for this procedure in Theorem 5.1.\nAlgorithm 3 EstBer(\u03b8, \u03b2, \u03b4) input A threshold parameter \u03b8 > 0, a budget parameter \u03b2 \u2265 7, confidence \u03b4 \u2208 (0, 1) S \u2190 {B1, . . . , B4} K \u2190 4\u03b2\u03b8 log( 8\u03b2 \u03b4\u03b8 )\nfor i = 3 : \u2308log2(\u03b2 log(2K/\u03b4)/\u03b8)\u2309 do n \u2190 2i S \u2190 S \u222a {Bn/2+1, . . . , Bn}. if p\u0302n > \u03b2 log(2n/\u03b4)/n then\nbreak end if end for Output p\u0302n.\nTheorem 5.1. Let \u03b4 \u2208 (0, 1), \u03b8 > 0, \u03b2 \u2265 7. Let B1, B2, . . . \u2208 {0, 1} be i.i.d Bernoulli random variables with expectation p. Let po be the output of EstBer(\u03b8, \u03b2, \u03b4). The following holds with a probability of 1\u2212 \u03b4, where f(\u03b2) := 1 + 83\u03b2 + \u221a 2 \u03b2 .\n1. If po \u2264 \u03b8, then p \u2264 f(\u03b2)\u03b8. Otherwise, pf(\u03b2) \u2264 po \u2264 p 2\u2212f(\u03b2) .\n2. Let\u03c8 := max(\u03b8, p/f(\u03b2)). The number of random draws in EstBer is at most 8\u03b2 log( 8\u03b2 \u03b4\u03c8 )\n\u03c8 .\nProof. First, consider any single round i with n = 2i. By the empirical Bernstein bound [Maurer and Pontil, 2009, Theorem 4],with a probability of 1\u2212 \u03b4/n, for n \u2265 8, we have3\n|p\u0302n \u2212 p| \u2264 8 log(2n/\u03b4)\n3n +\n\u221a\n2p\u0302n log(2n/\u03b4)\nn . (2)\nDefine g := (\u03b2 + 8/3 + \u221a 2\u03b2), so that f(\u03b2) = g/\u03b2. Conditioned on Eq. (2), there are two cases:\n(a) p\u0302n \u2264 \u03b2 log(2n/\u03b4)/n. In this case, p \u2264 g log(2n/\u03b4)/n.\n(b) p\u0302n > \u03b2 log(2n/\u03b4)/n. In this case, n \u2265 \u03b2 log(2n/\u03b4)/p\u0302n. Thus, by Eq. (2),\n|p\u0302n \u2212 p| \u2264 p\u0302n( 8 3\u03b2 + \u221a 2/\u03b2) = p\u0302n(g/\u03b2 \u2212 1).\nTherefore \u03b2p\ng \u2264 p\u0302n \u2264\np\n2\u2212 g/\u03b2 .\nTaking a union bound on all the rounds, we have that the guarantee holds for all rounds with a probability of at least 1\u2212 \u03b4.\nCondition now on the event that these guarantees all hold. First, we prove the label complexity bound. Note that since \u03b2 \u2265 7, K \u2265 28, thus we have 2 log(2K) > 8, therefore there is always at least one round. Let no be the value of n in the last round the algorithm runs, and let po = p\u0302no . Suppose that the algorithm reaches round i. To reach round i + 1, it must have p\u0302n \u2264 \u03b2 log(2n/\u03b4)/n for n = 2i, therefore \u03b2pg \u2264 p\u0302n \u2264 p 2\u2212g/\u03b2 , which means\np \u2264 gp\u0302n/\u03b2 \u2264 g log(2n/\u03b4)/n.\nTherefore, if the algorithm reaches round i+ 1, n \u2264 g log(2n/\u03b4)/p. It follows that\nno \u2264 2g log(4g/(\u03b4p))/p.\nIn addition, the number of random draws in the algorithm is at most\n2no \u2264 2\u2308log2(\u03b2 log(2K/\u03b4)/\u03b8)\u2309 \u2264 2 \u00b7 2log2(\u03b2 log(2K/\u03b4)/\u03b8) \u2264 4\u03b2 log(2K/\u03b4)/\u03b8.\nTherefore we have the following bound on the number of random draws:\n2no \u2264 min( 4\u03b2 log(2K/\u03b4) \u03b8 , 4g log(4g/(\u03b4p)) p ).\n3This follows from Theorem 4 of Maurer and Pontil [2009] since 7 3(n\u22121) \u2264 8 3n for n \u2265 8.\nPlugging in the definition of K yields\n\u03b2min( 4 log(8\u03b2\u03b4\u03b8 log( 8\u03b2 \u03b4\u03b8 )) \u03b8 , 4f(\u03b2) log(4\u03b2f(\u03b2)\u03b4p ) p ) \u2264\n\u03b2min( 8 log(8\u03b2\u03b4\u03b8 )) \u03b8 , 4f(\u03b2) log(4\u03b2f(\u03b2)\u03b4p ) p ) \u2264\n8\u03b2min( 1\n\u03b8 (log(\n8\u03b2\n\u03b4 ) + log(\n1 \u03b8 )), f(\u03b2) p (log( 4\u03b2 \u03b4 ) + log( f(\u03b2) p ))).\nUsing the definition of \u03c8, we get that the number of draws is at most 8\u03b2 log( 8\u03b2 \u03b4\u03c8 )\n\u03c8 . Next, we prove the accuracy of po (item 1 in the theorem statement) by considering two\ncases.\n(I) If po > \u03b2 log(2no/\u03b4)/no, then case (b) above holds for no, thus\n\u03b2p\ng \u2264 po \u2264\np\n2\u2212 g/\u03b2 .\nIn addition, if po \u2264 \u03b8, the LHS implies p \u2264 f(\u03b2)\u03b8. Thus item 1 in the theorem statement holds in this case.\n(II) If po \u2264 \u03b2 log(2no/\u03b4)/no, then EstBer could not have ended by breaking out of the loop, thus it ran until the last round. Therefore no \u2265 \u03b2 log(2K/\u03b4)/\u03b8. In addition, case (a) holds, therefore\np \u2264 g log(2no/\u03b4) no \u2264 g\u03b8 log(2n0/\u03b4) \u03b2 log(2K/\u03b4) . (3)\nNow, for any possible value of no,\nno \u2264 2\u03b2 log(2K/\u03b4)/\u03b8 \u2264 K.\nThe first inequality follows from the bound on i in EstBer, and the second inequality holds since, as defined in EstBer, K \u2265 4\u03b2\u03b8 log( 8\u03b2 \u03b8\u03b4 ). Since n0 \u2264 K , Eq. (3) implies that\np \u2264 g\u03b8 \u03b2 = f(\u03b2)\u03b8.\nIn addition, we have\npo \u2264 \u03b2 log(2no/\u03b4)/no \u2264 \u03b8 log(2n0/\u03b4)\nlog(2K/\u03b4) \u2264 \u03b8.\nTherefore in this case, necessarily po \u2264 \u03b8 and p \u2264 f(\u03b2)\u03b8, which satisfies item 1 in the theorem statement.\nIn both cases item 1 holds, thus the theorem is proved.\nThe procedure EstimateErr(t, \u03b8, \u03b4) is then implemented as follows:\n\u2022 Call EstBer(\u03b8, 52, \u03b4/(2m2)), where the random variables Bi are independent copies of the Bernoulli variable\nB := I[hnn S\u0302a(t) (X) 6= Y ]\nand (X,Y ) \u223c Sin.\n\u2022 To draw a single Bi, sample a random pair (x\u2032, y\u2032) from Sin, set\ni := \u03ba(x\u2032,Net(Uin, t/2)),\nand get S \u2190 GenerateNNSet(t, {i}, \u03b4). This returns S = ((xi, y\u0302i)) where y\u0302i is the label of xi in S\u0302a(t). Then Bi := I[y\u0302i 6= y\u2032]. Note that Bi is indeed distributed like B, and E[B] = \u01eb(t). Note further that this call to GenerateNNSet(t, {i}, \u03b4) uses Q(m) label queries. Therefore the overall label complexity of a single draw of a Bi is Q(m) + 1.\nCor. 5.2 gives a guarantee for the accuracy and label complexity of EstimateErr. The proof is immediate from Theorem 5.1, by setting \u03b2 = 52, which implies f(\u03b2) \u2264 5/4.\nCorollary 5.2. Let t, \u03b8 > 0 and \u03b4 \u2208 (0, 1), and let \u01eb\u0302(t) \u2190 EstimateErr(t, \u03b8, \u03b4). Let Q as defined in Eq. (1) The following properties hold with a probability of 1 \u2212 \u03b42m2 over the randomness of EstimateErr (and conditioned on S\u0302a(t)).\n1. If \u01eb\u0302(t) \u2264 \u03b8, then \u01eb(t) \u2264 5\u03b8/4. Otherwise,\n4\u01eb(t) 5 \u2264 \u01eb\u0302(t) \u2264 4\u01eb(t) 3 .\n2. Let \u03c8\u2032 := max(\u03b8, \u01eb(t)). The number of labels that EstimateErr requests is at most\n520(Q+ 1) log(1040m 2\n\u03b4\u03c8\u2032 )\n\u03c8\u2032 .\nTo derive item 2. above from Theorem 5.1, note that for \u03b2 = 52,\n\u03c8\u2032 = max(\u03b8, \u01eb(t)) \u2264 f(\u03b2)max(\u03b8, \u01eb(t)/f(\u03b2)) = f(\u03b2)\u03c8 \u2264 5 4 \u03c8,\nwhere \u03c8 is as defined in Theorem 5.1. Below we denote the event that the two properties in Cor. 5.2 hold for t by V (t)."}, {"heading": "5.2 Selecting a scale", "text": "The model selection procedure SelectScale, given in Alg. 4, implements its search based on the guarantees in Cor. 5.2. First, we introduce some notation. We would like MARMANN to obtain a generalization guarantee that is competitive with Gmin(m, \u03b4). Denote\n\u03c6(t) := (N (t) + 1) log(m) + log(1\u03b4 )\nm , (4)\nand let\nG(\u01eb, t) := \u01eb+ 2\n3 \u03c6(t) + 3\u221a 2 \u221a \u01eb\u03c6(t).\nNote that for all \u01eb, t,\nGB(\u01eb,N (t), \u03b4,m, 1) = m m\u2212N (t)G(\u01eb, t).\nWhen referring to G(\u03bd(t), t), G(\u01eb(t), t), or G(\u01eb\u0302(t), t) we omit the second t for brevity. Instead of directly optimizing G(\u03bd(t)), we will select a scale based on our estimate G(\u01eb\u0302(t)) of G(\u01eb(t)). Let Dist denote the set of pairwise distances in the unlabeled dataset Uin (note that |Dist| < ( m 2 )\n). We remove from Dist some distances, so that the remaining distances have a net size N (t) that is monotone non-increasing in t. We also remove values with a very large net size. Concretely, define\nDistmon := Dist \\ {t | N (t) + 1 > m/2} \\ {t | \u2203t\u2032 \u2208 Dist, t\u2032 < t and N (t\u2032) < N (t)}.\nThen for all t, t\u2032 \u2208 Distmon such that t\u2032 < t, we have N (t\u2032) \u2265 N (t). The output of SelectScale is always a value in Distmon. The following lemma shows that it suffices to consider these scales.\nLemma 5.3. Assume m \u2265 6 and let t\u2217m \u2208 argmint\u2208Dist G(\u03bd(t)). If Gmin(m, \u03b4) \u2264 1/3 then t\u2217m \u2208 Distmon. Proof. Assume by way of contradiction that t\u2217m \u2208 Dist \\Distmon. First, since G(\u03bd(t\u2217m)) \u2264 Gmin(m, \u03b4) \u2264 1/3 we have\nN (t\u2217m) + 1 m\u2212N (t\u2217m) log(m) \u2264 1 2 .\nTherefore, since m \u2265 6, it is easy to verify N (t\u2217m) + 1 \u2264 m/2. Therefore, by definition of Distmon there exists a t \u2264 t\u2217m with \u03c6(t) < \u03c6(t\u2217m). Since \u03bd(t) is monotone over all of t \u2208 Dist, we also have \u03bd(t) \u2264 \u03bd(t\u2217m). Now, \u03c6(t) < \u03c6(t\u2217m) and \u03bd(t) \u2264 \u03bd(t\u2217m) together imply that G(\u03bd(t)) < G(\u03bd(t\u2217m)), a contradiction. Hence, t \u2217 m \u2208 Distmon.\nSelectScale follows a search procedure similar to binary search, however the conditions for going right and for going left are not exhaustive, thus it is possible that neither condition holds. The search ends either when neither conditions hold, or when no additional scale should be tested. The final output of the algorithm is based on minimizing G(\u01eb\u0302(t)) over some of the values tested during search.\nFor c > 0, define\n\u03b3(c) := 1 + 2\n3c + 3\u221a 2c and \u03b3\u0303(c) := 1 c + 2 3 + 3\u221a 2c .\nFor all t, \u01eb > 0 we have the implications\n\u01eb \u2265 c\u03c6(t) \u21d2 \u03b3(c)\u01eb \u2265 G(\u01eb, t) and \u03c6(t) \u2265 c\u01eb \u21d2 \u03b3\u0303(c)\u03c6(t) \u2265 G(\u01eb, t). (5)\nThe following lemma uses Eq. (5) to show that the estimate G(\u01eb\u0302(t)) is close to the true G(\u01eb(t)).\nAlgorithm 4 SelectScale(\u03b4) input \u03b4 \u2208 (0, 1) output Scale t\u0302\n1: T \u2190 Distmon, # T maintains the current set of possible scales 2: while T 6= \u2205 do 3: t \u2190 the median value in T # break ties arbitrarily 4: \u01eb\u0302(t) \u2190 EstimateErr(t, \u03c6(t), \u03b4). 5: if \u01eb\u0302(t) < \u03c6(t) then 6: T \u2190 T \\ [0, t] # go right in the binary search 7: else if \u01eb\u0302(t) > 1110\u03c6(t) then 8: T \u2190 T \\ [t,\u221e) # go left in the binary search 9: else\n10: t0 \u2190 t, T0 \u2190 {t0}. 11: break from loop 12: end if 13: end while 14: if T0 was not set yet then 15: If the algorithm ever went to the right, let t0 be the last value for which this happened, and let T0 := {t0}. Otherwise, T0 := \u2205. 16: end if 17: Let TL be the set of all t that were tested and made the search go left 18: Output t\u0302 := argmint\u2208TL\u222aT0 G(\u01eb\u0302(t))\nLemma 5.4. Let t > 0, \u03b4 \u2208 (0, 1), and suppose that SelectScale calls \u01eb\u0302(t) \u2190 EstimateErr(t, \u03c6(t), \u03b4). Suppose that V (t) as defined in Cor. 5.2 holds. Then\n1 6 G(\u01eb\u0302(t)) \u2264 G(\u01eb(t)) \u2264 6.5G(\u01eb\u0302(t)).\nProof. Under V (t), we have that if \u01eb\u0302(t) < \u03c6(t) then \u01eb(t) \u2264 54\u03c6(t). In this case,\nG(\u01eb(t)) \u2264 \u03b3\u0303(4/5)\u03c6(t) \u2264 4.3\u03c6(t),\nby Eq. (5). Therefore\nG(\u01eb(t)) \u2264 3 \u00b7 4.3 2 G(\u01eb\u0302(t)).\nIn addition, G(\u01eb(t)) \u2265 23\u03c6(t) (from the definition of G), and by Eq. (5) and \u03b3\u0303(1) \u2264 4,\n\u03c6(t) \u2265 1 4 G(\u01eb\u0302(t)).\nTherefore G(\u01eb(t)) \u2265 16G(\u01eb\u0302(t)). On the other hand, if \u01eb\u0302(t) \u2265 \u03c6(t), then by Cor. 5.2 4\n5 \u01eb(t) \u2264 \u01eb\u0302(t) \u2264 4 3 \u01eb(t).\nTherefore G(\u01eb\u0302(t)) \u2264 43G(\u01eb(t)) and G(\u01eb(t)) \u2264 54G(\u01eb\u0302(t)). Taking the worst-case of both possibilities, we get the bounds in the lemma.\nThe next theorem bounds the label complexity of SelectScale. Let Ttest \u2286 Distmon be the set of scales that are tested during SelectScale (that is, their \u01eb\u0302(t) was estimated).\nTheorem 5.5. Suppose that the event V (t) defined in Cor. 5.2 holds for all t \u2208 Ttest for the calls \u01eb\u0302(t) \u2190 EstimateErr(t, \u03c6(t), \u03b4). If the output of SelectScale is t\u0302, then the number of labels requested by SelectScale is at most\n19240|Ttest|(Q+ 1) 1\nG(\u01eb(t\u0302)) log(\n38480m2 \u03b4G(\u01eb(t\u0302)) ).\nProof. The only labels used by the procedure are those used by calls to EstimateErr. Let \u03c8t := max(\u03c6(t), \u01eb(t)), and \u03c8min := mint\u2208Ttest \u03c8t. Denote also \u03c8\u0302t := max(\u03c6(t), \u01eb\u0302(t)). From Cor. 5.2 we have that the total number of labels in all the calls to EstimateErr in SelectScale is at most\n\u2211\nt\u2208Ttest\n520(Q+ 1) log(1040m 2\n\u03b4\u03c8t )\n\u03c8t \u2264 |Ttest|\n520(Q+ 1) log(1040m 2\n\u03b4\u03c8min )\n\u03c8min . (6)\nWe now lower bound \u03c8min using G(\u01eb(t\u0302)). By Lemma 5.4 and the choice of t\u0302,\nG(\u01eb(t\u0302)) \u2264 6.5G(\u01eb\u0302(t\u0302)) = 6.5 min t\u2208TL\u222aT0 G(\u01eb\u0302(t)).\nFrom the definition of G, for any t > 0,\nG(\u01eb\u0302(t)) \u2264 \u03b3(1)max(\u03c6(t), \u01eb\u0302(t)) \u2264 25\u03c8\u0302t.\nTherefore G(\u01eb(t\u0302)) \u2264 25 min\nt\u2208TL\u222aT0 \u03c8\u0302t. (7)\nWe will show a similar upper bound when minimizing over all of Ttest and not just over TL \u222a T0. This is trivial if Ttest = TL \u222a T0, therefore consider now the case TL \u222a T0 ( Ttest. For any t \u2208 Ttest, one of the following options hold:\n\u2022 The search went left on t (step 8), hence t \u2208 TL.\n\u2022 The search went nowhere on t and the loop broke (step 11), hence t = t0 \u2208 T0.\n\u2022 The search went right on t (step 6) and this was the last value for which this happened, hence t = t0 \u2208 T0.\n\u2022 The search went right on t (step 6) and this was not the last value for which this happened. Hence t \u2208 Ttest \\ (TL \u222a T0).\nSet some t1 \u2208 Ttest \\ (TL \u222a T0). Since the search went right on t1, then t0 also exists, since the algorithm did go to the right for some t (see step 15). Since the binary search went right on t1, we have \u01eb\u0302(t1) \u2264 \u03c6(t1). Since the binary search did not go left on t0 (it either broke from the loop or went right), \u01eb\u0302(t0) \u2264 1110\u03c6(t0).\nIn addition, t0 \u2265 t1 (since the search went right at t1, and t0 was tested later than t1), thus \u03c6(t0) \u2264 \u03c6(t1) (since t0, t1 \u2208 Distmon). Therefore,\n\u03c8\u0302t0 = max(\u03c6(t0), \u01eb\u0302(t0)) \u2264 11\n10 \u03c6(t0) \u2264\n11 10 \u03c6(t1) = 11 10 max(\u03c6(t1), \u01eb\u0302(t1)) = \u03c8\u0302t1 .\nIt follows that for any such t1,\nmin t\u2208TL\u222aT0\n\u03c8\u0302t \u2264 11\n10 \u03c8\u0302t1 .\nTherefore\nmin t\u2208TL\u222aT0\n\u03c8\u0302t \u2264 11\n10 min t\u2208Ttest \u03c8\u0302t.\nTherefore, by Eq. (7) G(\u01eb(t\u0302)) \u2264 27.5 min\nt\u2208Ttest \u03c8\u0302t."}, {"heading": "By Cor. 5.2, \u01eb\u0302(t) \u2264 max(\u03c6(t), 4\u01eb(t)/3), therefore \u03c8\u0302t \u2264 43\u03c8t. Therefore G(\u01eb(t\u0302)) \u2264 37\u03c8min.", "text": "Therefore, from Eq. (6), the total number of labels is at most\n19240|Ttest|(Q+ 1) 1\nG(\u01eb(t\u0302)) log(\n38480m2 \u03b4G(\u01eb(t\u0302)) ).\nThe following theorem provides a competitive error guarantee for the selected scale t\u0302.\nTheorem 5.6. Suppose that V (t) and E(t), defined in Cor. 5.2 and Theorem 4.3, hold for all values t \u2208 Ttest, and that Gmin(m, \u03b4) \u2264 1/3. Then SelectScale outputs t\u0302 \u2208 Distmon such that GB(\u01eb(t\u0302),N (t\u0302), \u03b4,m, 1) \u2264 O(Gmin(m, \u03b4)), Where the O(\u00b7) notation hides only universal multiplicative constants.\nThe full proof of this theorem is given below. The idea of the proof is as follows: First, we show (using Lemma 5.4) that it suffices to prove that G(\u03bd(t\u2217m)) \u2265 O(G(\u01eb\u0302(t\u0302))) to derive the bound in the theorem. Now, SelectScale ends in one of two cases: either T0 is set within the loop, or T = \u2205 and T0 is set outside the loop. In the first case, neither of the conditions for turning left and turning right holds for t0, so we have \u01eb\u0302(t0) = \u0398(\u03c6(t0)) (where \u0398 hides numerical constants). We show that in this case, whether t\u2217m \u2265 t0 or t\u2217m \u2264 t0, G(\u03bd(t\u2217m)) \u2265 O(G(\u01eb\u0302(t0))). In the second case, there exist (except for edge cases, which are also handled) two values t0 \u2208 T0 and t1 \u2208 TL such that t0 caused the binary search to go right, and t1 caused it to go left, and also t0 \u2264 t1, and (t0, t1) \u2229Distmon = \u2205. We use these facts to show that for t\u2217m \u2265 t1, G(\u03bd(t\u2217m)) \u2265 O(G(\u01eb\u0302(t1))), and for t\u2217m \u2264 t0, G(\u03bd(t\u2217m)) \u2265 O(G(\u01eb\u0302(t0))). Since t\u0302 minimizes over a set that includes t0 and t1, this gives G(\u03bd(t\u2217m)) \u2265 O(G(\u01eb\u0302(t\u0302))) in all cases.\nof Theorem 5.6. First, note that it suffices to show that there is a constant C, such that for the output t\u0302 of SelectScale, we have G(\u01eb(t\u0302)) \u2264 CG(\u03bd(t\u2217m)). This is because of the following argument: From Lemma 5.3 we have that if Gmin(m, \u03b4) \u2264 1/3, then t\u2217m \u2208 Distmon. Now\nGmin(m, \u03b4) = m m\u2212N (t\u2217m) G(\u03bd(t\u2217m)) \u2265 G(\u03bd(t\u2217m)).\nAnd, if we have the guarantee on G(\u01eb(t\u0302)) and Gmin(m, \u03b4) \u2264 1/3 we will have\nGB(\u01eb(t\u0302),N (t\u0302), \u03b4,m, 1) = m m\u2212N (t\u0302)G(\u01eb(t\u0302)) \u2264 2G(\u01eb(t\u0302)) \u2264 2CG(\u03bd(t \u2217 m)) \u2264 2CGmin(m, \u03b4).\n(8) We now prove the existence of such a guarantee and set C. Denote the two conditions checked in SelectScale during the binary search by Condition 1: \u01eb\u0302(t) < \u03c6(t) and Condition 2: \u01eb\u0302(t) > 1110\u03c6(t). The procedure ends in one of two ways: either T0 is set within the loop (Case 1), or T = \u2205 and T0 is set outside the loop (Case 2). We analyze each case separately.\nIn Case 1, none of the conditions 1 and 2 hold for t0. Therefore\n\u03c6(t0) \u2264 \u01eb\u0302(t0) \u2264 11\n10 \u03c6(t0).\nTherefore, by Eq. (5),\n\u03c6(t0) \u2265 G(\u01eb\u0302(t0))/\u03b3\u0303( 10\n11 ).\nBy Cor. 5.2, since \u01eb\u0302(t0) > \u03c6(t0),\n3 4 \u03c6(t0) \u2264 3 4 \u01eb\u0302(t0) \u2264 \u01eb(t0) \u2264 5 4 \u01eb\u0302(t0) \u2264 55 40 \u03c6(t0).\nSuppose t\u2217m \u2265 t0, then\nG(\u03bd(t\u2217m)) \u2265 \u03bd(t\u2217m) \u2265 \u03bd(t0) \u2265 1\n4 \u01eb(t0) \u2265\n3\n16 \u03c6(t0).\nhere we used \u01eb(t0) \u2264 4\u03bd(t0) by Theorem 4.3. Therefore, from Eq. (5) and Lemma 5.4,\nG(\u03bd(t\u2217m)) \u2265 3\n16 \u03c6(t0) \u2265\n3 16\u03b3\u0303 (\n40 55\n)G(\u01eb(t0)) \u2265 1 2\n16\u03b3\u0303(4055 ) G(\u01eb\u0302(t0)).\nNow, suppose t\u2217m < t0, then\nG(\u03bd(t\u2217m)) \u2265 2 3 \u03c6(t\u2217m) \u2265 2 3 \u03c6(t0) \u2265\n2\n3\u03b3\u0303(1011 ) G(\u01eb\u0302(t0)).\nIn this inequality we used the fact that t\u2217m, t0 \u2208 Distmon, hence \u03c6(t\u2217m) \u2265 \u03c6(t0). Combining the two possibilities for t\u2217m, we have in Case 1,\nG(\u01eb\u0302(t0)) \u2264 max(32\u03b3\u0303( 40 55 ), 3\u03b3\u0303(1011 ) 2 )G(\u03bd(t\u2217m)).\nSince t\u0302 minimizes G(\u01eb\u0302(t)) on a set that includes t0, we have, using Lemma 5.4\nG(\u01eb(t\u0302)) \u2264 6.5G(\u01eb\u0302(t\u0302)) \u2264 6.5G(\u01eb\u0302(t0)).\nTherefore, in Case 1,\nG(\u01eb(t\u0302)) \u2264 6.5max(32\u03b3\u0303(40 55\n), 3\u03b3\u0303(1011 )\n2 )G(\u03bd(t\u2217m)). (9)\nIn Case 2, the binary search halted without satisfying Condition 1 nor Condition 2 and with T = \u2205. Let t0 be as defined in this case in SelectScale (if it exists), and let t1 be the smallest value in TL (if it exists). At least one of these values must exist. If both values exist, we have t0 \u2264 t1 and (t0, t1) \u2229Distmon = \u2205.\nIf t0 exists, it is the last value for which the search went right. We thus have \u01eb\u0302(t0) < \u03c6(t0). If t\u2217m \u2264 t0, from condition 1 on t0 and Eq. (5) with \u03b3\u0303(1) \u2264 4,\nG(\u03bd(t\u2217m)) \u2265 2 3 \u03c6(t\u2217m) \u2265 2 3 \u03c6(t0) \u2265 1 6 G(\u01eb\u0302(t0)).\nHere we used the monotonicity of \u03c6 on t\u2217m, t0 \u2208 Distmon, and Eq. (5) applied to condition 1 for t0.\nIf t1 exists, the search went left on t1, thus \u01eb\u0302(t1) > 1110\u03c6(t1). By Cor. 5.2, it follows that \u01eb\u0302(t1) \u2264 43\u01eb(t1). Therefore, if t\u2217m \u2265 t1,\nG(\u03bd(t\u2217m)) \u2265 \u03bd(t\u2217m) \u2265 \u03bd(t1) \u2265 1\n4 \u01eb(t1) \u2265\n3\n16 \u01eb\u0302(t1) \u2265\n3\n16\u03b3(11/10) G(\u01eb\u0302(t1)).\nHere we used \u01eb(t1) \u2264 4\u03bd(t1) by Theorem 4.3 and Eq. (5). Combining the two cases for t\u2217m, we get that if t0 exists and t\u2217m \u2264 t0, or t1 exists and t\u2217m \u2265 t1,\nG(\u03bd(t\u2217m)) \u2265 min( 1\n6 ,\n3\n16\u03b3(11/10) ) min t\u2208TE G(\u01eb\u0302(t)).\nwhere we define TE = {t \u2208 {t0, t1} | t exists}. We now show that this covers all possible values for t\u2217m: If both t0, t1 exist, then since (t0, t1) \u2229 Distmon = \u2205, it is impossible to have t\u2217m \u2208 (t0, t1). If only t0 exists, then the search never went left, which means t0 = max(Distmon), thus t\u2217m \u2264 t0. If only t1 exists, then the search never went right, which means t1 = min(Distmon), thus t\u2217m \u2265 t1.\nSince t\u0302 minimizes G(\u01eb\u0302(t)) on a set that has TE as a subset, we have, using Lemma 5.4 G(\u01eb(t\u0302)) \u2264 6.5G(\u01eb\u0302(t\u0302)) \u2264 6.5mint\u2208TE G(\u01eb\u0302(t)). Therefore in Case 2,\nG(\u03bd(t\u2217m)) \u2265 1\n6.5 min(\n1 6 ,\n3\n16\u03b3(11/10) )G(\u01eb(t\u0302)). (10)\nFrom Eq. (9) and Eq. (10) we get that in both cases\nG(\u03bd(t\u2217m)) \u2265 1\n6.5 min(\n1 6 ,\n3\n16\u03b3(11/10) ,\n2\n3\u03b3\u0303(10/11) ,\n1\n32\u03b3\u0303(4055 ) )G(\u01eb(t\u0302)) \u2265 G(\u01eb(t\u0302))/865.\nCombining this with Eq. (8) we get the statement of the theorem.\n6 Bounding the label complexity of MARMANN\nWe are now almost ready to prove Theorem 3.1. Our last missing piece is quantifying the effect of side information on the generalization of sample compression schemes in Section 6.1. We then prove Theorem 3.1 in Section 6.2."}, {"heading": "6.1 Sample compression with side information", "text": "It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995]. As noted in Section 4, our algorithm relies on a generalized sample compression scheme, which requires side information. This side information is used to represent the labels of the sample points in the compression set. A similar idea appears in Floyd and Warmuth [1995] for hypotheses with short description length. Here we provide a generalization that is useful for the analysis of MARMANN.\nLet \u03a3 be a finite alphabet, and define a mapping RecN : (X \u00d7Y)N \u00d7\u03a3N \u2192 YX .4 This is a reconstruction function mapping a labeled sequence of size N with side information T \u2208 \u03a3N to a classifier. For I \u2286 [|S|], denote by S[I] the subsequence of S indexed by I . For a labeled sample S, define the set of possible hypotheses reconstructed from a compression of S of size N with side information in\u03a3: HN (S) := { h : X \u2192 Y | h = RecN (S[I], T ), I \u2208 [m]N , T \u2208 \u03a3N }\n. The following result closely follows the sample compression arguments in Graepel et al. [2005, Theorem 2], and Gottlieb et al. [2016b, Theorem 6], but incorporates side information.\nTheorem 6.1. Let m be an integer and \u03b4 \u2208 (0, 1). Let S \u223c Dm. With probability at least 1 \u2212 \u03b4, if there exist N < m and h \u2208 HN (S) with \u01eb := err(h, S) \u2264 12 , then err(h,D) \u2264 GB(\u01eb,N, \u03b4,m, |\u03a3|). Proof. We recall a result of Dasgupta and Hsu [2008, Lemma 1]: if p\u0302 \u223c Bin(n, p)/n and \u03b4 > 0, then the following holds with probability at least 1\u2212 \u03b4:\np \u2264 p\u0302+ 2 3n log 1 \u03b4 +\n\u221a\n9p\u0302(1\u2212 p\u0302) 2n log 1 \u03b4 . (11)\nNow fix N < m, and suppose that h \u2208 HN (S) has \u01eb\u0302 \u2264 12 . Let I \u2208 [m]N , T \u2208 \u03a3N such that h = RecN (S[I], T ). We have err(h, S[[m] \\ I]) \u2264 \u01eb\u0302mm\u2212N = \u03b8\u01eb\u0302. Substituting into (11) p := err(h,D), n := m \u2212N and p\u0302 := err(h, S[[m] \\ I]) \u2264 \u03b8\u01eb\u0302, yields that for a fixed S[I] and a random S[[m] \\ I] \u223c Dm\u2212N , with probability at least 1\u2212 \u03b4,\nerr(h,D) \u2264 \u03b8\u01eb\u0302+ 2 3(m\u2212N) log 1 \u03b4 +\n\u221a\n9\u03b8\u01eb\u0302 2(m\u2212N) log 1 \u03b4 . (12)\nTo make (12) hold simultaneously for all (I, T ) \u2208 [m]N \u00d7 \u03a3N , divide \u03b4 by (m|\u03a3|)N . To make the claim hold for all N \u2208 [m], stratify (as in Graepel et al. [2005, Lemma 1]) over the (fewer than) m possible choices of N , which amounts to dividing \u03b4 by an additional factor of m.\n4If X is infinite, replace YX with the set of measurable functions from X to Y .\nFor MARMANN, we use the following sample compression scheme with \u03a3 = Y . Given a subsequence S\u2032 := S[I] := (x\u20321, . . . , x \u2032 N ) and T = (t1, . . . , tN ) \u2208 YN , the reconstruction function RecN (S[I], T ) generates the nearest-neighbor rule induced by the labeled sample \u03c8(S\u2032, T ) := ((x\u2032i, ti))i\u2208[N ]. Formally, RecN (S\n\u2032, T ) = hnn\u03c8(S\u2032,T ). Note the slight abuse of notation: formally, the yi in Sa(t) should be encoded as side information T , but for clarity, we have opted to \u201crelabel\u201d the examples {x1, . . . , xN} as dictated by the majority in each region. The following corollary is immediate from Theorem 6.1 and the construction above.\nTheorem 6.2. Let m \u2265 |Y| be an integer, \u03b4 \u2208 (0, 14 ). Let Sin \u223c Dm. With probability at least 1 \u2212 \u03b4, if there exist N < m and S \u2286 (X \u00d7 Y)N such that U(S) \u2286 Uin and \u01eb := err(hnnS , Sin) \u2264 12 , then err(hnnS ,D) \u2264 GB(\u01eb,N, \u03b4,m, |Y|) \u2264 2GB(\u01eb,N, 2\u03b4,m, 1).\nIf the compression set includes only the original labels, the compression analysis of Gottlieb et al. [2016b] gives the bound GB(\u01eb,N, \u03b4,m, 1). Thus the effect of allowing the labels to change is only logarithmic in |Y|, and does not appreciably degrade the prediction error."}, {"heading": "6.2 Proof of Theorem 3.1", "text": "The proof of the main theorem, Theorem 3.1, which gives the guarantee for MARMANN, is almost immediate from Theorem 6.2, Theorem 4.3, Theorem 5.6 and Theorem 5.5.\nof Theorem 3.1. We have |Distmon| \u2264 ( m 2 )\n. By a union bound, the events E(t) and V (t) of Theorem 4.3 and Cor. 5.2 hold for all t \u2208 Ttest \u2286 Distmon with a probability of at least 1\u2212 \u03b4/2. Under these events, we have by Theorem 5.6 that if Gmin(m, \u03b4) \u2264 1/3,\nGB(\u01eb(t\u0302),N (t\u0302), \u03b4,m, 1) \u2264 O (\nmin t\nGB(\u03bd(t),N (t), \u03b4,m, 1) ) .\nBy Theorem 6.2, with a probability at least 1\u2212 \u03b4/2, if \u01eb(t\u0302) \u2264 12 then\nerr(h\u0302,D) \u2264 2GB(\u01eb(t\u0302),N (t\u0302), \u03b4,m, 1). The statement of the theorem follows. Note that the statement trivially holds forGmin(m, \u03b4) \u2265\n1/3 and for \u01eb(t\u0302) \u2265 12 , thus these conditions can be removed. To bound the label complexity, note that the total number of labels used by MARMANN is at most the number of labels used by SelectScale plus the number of labels used byGenerateNNSet when the final compression set is generated.\nBy Theorem 5.5, since Q = O(log(m/\u03b4)), the number of labels used by SelectScale is at most\nO\n( |Ttest| log2(m/\u03b4)\nG(\u01eb(t\u0302)) log\n(\n1\nG(\u01eb(t\u0302)\n))\n.\nIn addition, G(\u01eb(t\u0302)) \u2265 GB(\u01eb(t\u0302),N (t\u0302), \u03b4,m, 1) = G\u0302.\nThe number of tested scales in SelectScale is bounded by\n|Ttest| \u2264 \u230alog2(|Distmon|) + 1\u230b \u2264 2 log2(m)\nTherefore the number of labels used by SelectScale is\nO\n( log3(m/\u03b4)\nG\u0302 log\n(\n1\nG\u0302\n))\n.\nThe number of labels used by GenerateNNSet is at most QN (t\u0302), where Q \u2264 O(log(m/\u03b4), and from the definition of G\u0302, N (t\u0302) \u2264 O(mG\u0302/ log(m)). Summing up the number of labels used by SelectScale and the number used by GenerateNNSet, this gives the bound in the statement of the theorem."}, {"heading": "7 Passive learning lower bounds", "text": "Theorem 3.2 lower bounds the performance of a passive learner who observes a limited number \u2113 of random labels from Sin. The number \u2113 is chosen so that it is of the same order as the number of labels MARMANN observes for the case analyzed in Section 3. We deduce Theorem 3.2 from a more general result pertaining to the sample complexity of passive learning. The general result is given as Theorem 7.1 in Section 7.1. The proof of Theorem 3.2 is provided in Section 7.2.\nWe note that while the lower bounds below assume that the passive learner observes only the random labeled sample of size \u2113, in fact their proofs hold also if the algorithm has access to the full unlabeled sample of size m of which S\u2113 is sampled. This is because the lower bound is based on requiring the learner to distinguish between distributions that all have the same marginal. Under this scenario, access to unlabeled examples does not provide any additional information to the algorithm."}, {"heading": "7.1 A general lower bound", "text": "In this section we show a general sample complexity lower bound for passive learning, which may be of independent interest. We are aware of two existing lower bounds for agnostic PAC with bounded noise: Devroye et al. [1996, Theorem 14.5] and Audibert [2009, Theorem 8.8]. Both place restrictions on the relationship between the sample size, VC-dimension, and noise level, which render them inapplicable as stated to some parameter regimes, including the one needed for proving Theorem 3.2.\nLet H be a hypothesis class with VC-dimension d and suppose that L is a passive learner5 mapping labeled samples S\u2113 = (Xi, Yi)i\u2208[\u2113] to hypotheses h\u0302\u2113 \u2208 H. For any distribution D over X \u00d7 {\u22121, 1}, define the excess risk of h\u0302\u2113 by\n\u2206(h\u0302\u2113,D) := err(h\u0302\u2113,D)\u2212 inf h\u2208H err(h,D).\nLet D(\u03b7) be the collection of all \u03b7-noise bounded distributions D over X \u00d7 {\u22121, 1}, which satisfy infh\u2208H err(h,D) \u2264 \u03b7. We say that Z \u2208 {\u22121, 1} has Rademacher distribution with parameter b \u2208 [\u22121, 1], denoted Z \u223c Rb, if\nP[Z = 1] = 1\u2212 P[Z = \u22121] = 1 2 + b 2 .\n5 We allow L access to an independent internal source of randomness.\nAll distributions on {\u22121, 1} are of this form. For k \u2208 N and b \u2208 [0, 1], define the function\nbayes(k, b) = 12\n( 1\u2212 12 \u2225 \u2225 \u2225 Rkb \u2212Rk\u2212b \u2225 \u2225 \u2225\n1\n)\n,\nwhere Rk\u00b1b is the corresponding product distribution on {\u22121, 1}k and 12 \u2016\u00b7\u20161 is the total variation norm. This expression previously appeared in Berend and Kontorovich [2015, Equation (25)] in the context of information-theoretic lower bounds; the current terminology was motivated in Kontorovich and Pinelis [2016], where various precise estimates on bayes(\u00b7) were provided. In particular, the function bay\u030ces(\u03ba, b) was defined as follows: for each fixed b \u2208 [0, 1], bay\u030ces(\u00b7, b) is the largest convex minorant on [0,\u221e) of the function bayes(\u00b7, b) on {0, 1, . . .}. It was shown in Kontorovich and Pinelis [2016, Proposition 2.8] that bay\u030ces(\u00b7, b) is the linear interpolation of bayes(\u00b7, b) at the points 0, 1, 3, 5, . . . .\nTheorem 7.1. Let 0 < \u03b7 < 12 , \u2113 \u2265 1, and H be a hypothesis class with VC-dimension d > 1. Then, for all 0 < b, p < 1 satisfying\np\n(\n1 2 \u2212 b 2\n)\n\u2264 \u03b7, (13)\nwe have\ninf h\u0302\u2113 sup D\u2208D(\u03b7) E D\u2113\n[ \u2206(h\u0302\u2113,D) ] \u2265 bp bay\u030ces(\u2113p/(d\u2212 1), b). (14)\nFurthermore, for 0 \u2264 u < 1,\ninf h\u0302\u2113 sup D\u2208D(\u03b7) P\n[ \u2206(h\u0302\u2113,D\u03c3,b,p) > bpu ] > bay\u030ces(\u2113p/(d\u2212 1), b)\u2212 u. (15)\nProof. This proof uses ideas from Devroye et al. [1996, Theorem 14.5], Anthony and Bartlett [1999, Theorem 5.2] and Kontorovich and Pinelis [2016, Theorem 2.1].\nWe will construct adversarial distributions supported on a shattered subset of size d, and hence there is no loss of generality in taking X = [d] and H = {\u22121, 1}X . A random distribution D\u03c3,b,p over X \u00d7{\u22121, 1}, parametrized by a random \u03c3 \u223c Unif({\u22121, 1}d\u22121) and scalars b, p \u2208 (0, 1) to be specified later, is defined as follows. The point x = d \u2208 X gets a marginal weight of 1 \u2212 p where p is a parameter to be set; the remaining d \u2212 1 points each get a marginal weight of p/(d\u2212 1):\nP X\u223cD\u03c3,b,p [X = d] = 1\u2212 p, P X\u223cD\u03c3,b,p\n[X < d] = p\nd\u2212 1 . (16)\nThe distribution of Y conditional on X is given by P(X,Y )\u223cD\u03c3,b,p [Y = 1 |X = d] = 1 and\nP (X,Y )\u223cD\u03c3,b,p [Y = \u00b11 |X = j < d] = 1 2 \u00b1 b\u03c3j 2 . (17)\nSuppose that (Xi, Yi)i\u2208[\u2113] is a sample drawn fromD\u2113\u03c3,b,p. The assumption that D\u03c3,b,p \u2208 D(\u03b7) implies that b and p must satisfy the constraint (13).\nA standard argument (e.g., Anthony and Bartlett [1999] p. 63 display (5.5)) shows that, for any hypothesis h\u0302\u2113,\n\u2206(h\u0302\u2113,D\u03c3,b,p) = err(h\u0302\u2113,D\u03c3,b,p)\u2212 inf h\u2208H err(h,D\u03c3,b,p)\n= P X\u223cD\u03c3,b,p [X = d, h\u0302\u2113(X) 6= 1] + b P X\u223cD\u03c3,b,p [X < d, h\u0302\u2113(X) 6= \u03c3(X)]\n\u2265 b P X\u223cD\u03c3,b,p [X < d, h\u0302\u2113(X) 6= \u03c3(X)]\n= bp P X\u223cD\u03c3,b,p\n[h\u0302\u2113(X) 6= \u03c3(X)|X < d]. (18)\nIt follows from Kontorovich and Pinelis [2016, Theorems 2.1, 2.5] that\nE \u03c3 P X\u223cD\u03c3,b,p [h\u0302\u2113(X) 6= \u03c3(X)|X < d] \u2265 E N\u223cBin(\u2113,p/(d\u22121)) [bayes(N, b)]\n\u2265 E N\u223cBin(\u2113,p/(d\u22121)) [bay\u030ces(N, b)] \u2265 bay\u030ces(E[N ], b) = bay\u030ces(\u2113p/(d\u2212 1), b),\nwhere the second inequality holds because bay\u030ces is, by definition, a convex minorant of bayes, and the third follows from Jensen\u2019s inequality. Combined with (18), this proves (14).\nTo show (15), define the random variable\nZ = Z(\u03c3,L) = P X\u223cD\u03c3,b,p [h\u0302\u2113(X) 6= \u03c3(X)|X < d].\nSince Z \u2208 [0, 1], Markov\u2019s inequality implies\nP[Z > u] \u2265 E[Z]\u2212 u 1\u2212 u > E[Z]\u2212 u, 0 \u2264 u < 1.\nNow (18) implies that \u2206(h\u0302\u2113,D\u03c3,b,p) \u2265 bpZ and hence, for 0 \u2264 u < 1,\ninf h\u0302\u2113 sup D\u2208D(\u03b7) P\n[ \u2206(h\u0302\u2113,D\u03c3,b,p) > bpu ]\n= inf h\u0302\u2113 sup D\u2208D(\u03b7) P[Z > u]\n> inf h\u0302\u2113 sup D\u2208D(\u03b7)\nE[Z]\u2212 u\n\u2265 1 bp inf h\u0302\u2113 sup D\u2208D(\u03b7) E[\u2206(h\u0302\u2113,D\u03c3,b,p)]\u2212 u \u2265 bay\u030ces(\u2113p/(d\u2212 1), b)\u2212 u."}, {"heading": "7.2 Proof of Theorem 3.2", "text": "We break down the proof into several steps.\n(i) Defining a family of adversarial distributions. Let T be a t\u0304-net of X of size \u0398(\u221am) and \u03b7 = \u0398(1/ \u221a m). For any passive learning algorithm mapping i.i.d. samples of size \u2113 = \u0398\u0303( \u221a m) to hypotheses h\u0302\u2113 : X \u2192 {\u22121, 1}, we construct a random adversarial distribution D\u03c3,b,p with Bayes error \u03b7. We accomplish this via the construction described in the proof of Theorem 7.1, with |T | = d = \u0398(\u221am). The marginal distribution over T = {x1, . . . , xd} puts a mass of 1 \u2212 p on xd \u2208 T and spreads the remaining mass uniformly over the other points, as in (16). The \u201cheavy\u201d point has a deterministic label and the remaining \u201clight\u201d points have noisy labels drawn from a random distribution with symmetric noise level b, as in (17). We proceed to choose b and p; namely,\np = d\u2212 1 2\u2113 \u221a \u03b7 = \u0398\u0303(m\u22121/4), b = 1\u2212 2\u03b7 p = 1\u2212 \u0398\u0303(m\u22121/4),\nwhich makes the constraint in (13) hold with equality; this means that the Bayes error is exactly \u03b7 and in particular, establishes (i).\n(ii.a) Lower-bounding the passive learner\u2019s error. Our choice of p implies that \u2113p/(d\u2212 1) = \u221a \u03b7/2 =: \u03ba < 1, for this range of \u03ba, Kontorovich and Pinelis [2016, Proposition 2.8] implies that bay\u030ces(\u03ba, b) = 12 (1 \u2212 \u03bab) = \u0398(1). Choosing u = 14 (1 \u2212 \u03bab) = \u0398(1) in (15), Theorem 7.1 implies that\ninf h\u0302\u2113 sup D\u2208D(\u03b7)\nP[\u2206(h\u0302\u2113,D) > \u2126\u0303(m\u22121/4)] > \u2126(1).\nIn more formal terms, there exist constants c0, c1 > 0 such that\ninf h\u0302\u2113 sup D\u2208D(\u03b7)\nP[\u2206(h\u0302\u2113,D) > c0p] > c1. (19)\n(ii.b) Upper-bounding \u03bd(t\u0304). To establish (ii.b), it suffices to show that for (Xi, Yi)i\u2208[\u2113] \u223c D\u2113\u03c3,b,p, we will have \u03bd(t\u0304) = O(m\u22121/2) with sufficiently high probability. Indeed, the latter condition implies the requisite upper bound on mint>0:N (t)<m GB(\u03bd(t),N (t), \u03b4,m, 1), while (i) implies the lower bound, since the latter quantity cannot be asymptotically smaller than the Bayes error.\nRecall that the t\u0304-net points {x1, . . . , xd\u22121} are the \u201clight\u201d ones (i.e., each has weight p/(d\u2212 1)) and define the random sets Jj \u2282 [\u2113] by\nJj = {i \u2208 [\u2113] : Xi = xj} , j \u2208 [d\u2212 1].\nIn words, Jj consists of the indices i of the sample points for which Xi falls on the net point xj . For y \u2208 {\u22121, 1}, put \u03c4yj = \u2211\ni\u2208Jj I[Yi = y] and define the minority count \u03bej at the net point xj by\n\u03bej = min y\u2208{\u22121,1}\n\u03c4yj = 1 2 (|\u03c4+j + \u03c4\u2212j | \u2212 |\u03c4+j \u2212 \u03c4\u2212j |).\nObserve that by virture of being a t\u0304-net, T is t\u0304-separated and hence the only contribution to \u03bd(t\u0304) is from the minority counts (to which the \u201cheavy\u201d point xd does not contribute due to\nits deterministic label):\n\u03bd(t\u0304) = 1\n\u2113\nd\u22121 \u2211\nj=1\n\u03bej .\nNow\nE |\u03c4+j + \u03c4\u2212j | = E |Jj | = \u2113p d\u2212 1 = \u0398(m \u22121/4)\nand\nE |\u03c4+j \u2212 \u03c4\u2212j | = E \u03c3j E[|\u03c4+j \u2212 \u03c4\u2212j | \u2223 \u2223\u03c3j ]\n\u2265 E \u03c3j\n\u2223 \u2223E[\u03c4 + j \u2212 \u03c4\u2212j \u2223 \u2223\u03c3j ] \u2223 \u2223 .\nComputing\nE[\u03c4 + j |\u03c3j = +1] = (\n1 2 + b 2 ) \u2113p d\u2212 1 , E[\u03c4 \u2212 j |\u03c3j = +1] = ( 1 2 \u2212 b 2 ) \u2113p d\u2212 1 ,\nwith an analogous calculation when conditioning on \u03c3j = \u22121, we get\nE |\u03c4+j \u2212 \u03c4\u2212j | \u2265 b\u2113p\nd\u2212 1 and hence\nE[\u03bej ] \u2264 1\n2\n(\n\u2113p d\u2212 1 \u2212 b \u2113p d\u2212 1\n)\n= (1\u2212 b) \u2113p 2(d\u2212 1) = 2\u03b7 p \u00b7 \u2113p 2(d\u2212 1) = \u03b7\u2113 d\u2212 1 .\nIt follows that\nE[\u03bd(t\u0304)] = 1\n\u2113\nd\u22121 \u2211\nj=1\nE[\u03bej ]\n\u2264 d\u2212 1 \u2113 \u00b7 \u03b7\u2113 d\u2212 1 = \u03b7 = \u0398(m \u22121/2).\nTo give tail bounds on \u03bd(t\u0304), we use Markov\u2019s inequality: for all c2 > 0,\nP[\u03bd(t\u0304) > c2 E[\u03bd(t\u0304)] \u2264 1\nc2 .\nChoosing c2 sufficiently large that 1\u2212 1/c2 > c1 (the latter from (19)) implies the existence of constants c0, c2, c3 > 0 such that\ninf h\u0302\u2113 sup D\u2208D(\u03b7) P\n[( \u2206(h\u0302\u2113,D) > c0p ) \u2227 (\u03bd(t\u0304) \u2264 c2\u03b7) ] > c3.\nSince p = \u0398\u0303(m\u22121/4) and \u03b7 = \u0398(m\u22121/2), this establishes (ii) and concludes the proof of Theorem 3.2."}, {"heading": "8 Active learning lower bound", "text": "We now prove the active learning lower bound stated in Theorem 3.3. To prove the theorem, we first prove a result which is similar to the classical No-Free-Lunch theorem, except it holds for active learning algorithms. The proof follows closely the proof of the classical No-Free-Lunch theorem given in Shalev-Shwartz and Ben-David [2014, Theorem 5.1], with suitable modifications.\nTheorem 8.1. Let \u03b2 \u2208 [0, 12 ), and m be an integer. Let A any active learning algorithm over a finite domain X which gets as input a random labeled sample S \u223c Dm (with hidden labels) and outputs h\u0302. If A queries fewer than X/2 labels from S, then there exists a distribution D over X \u00d7 {0, 1} such that\n\u2022 Its marginal on X is uniform, and for each x \u2208 X , P[Y = 1 | X = x] \u2208 {\u03b2, 1\u2212 \u03b2}.\n\u2022 E[err(h\u0302,D)] \u2265 14 .\nProof. Let F = {f1, . . . , fT } be the set of possible functions fi : X \u2192 {0, 1}. Let Di to be a distribution with a uniform marginal over X , and P(X,Y )\u223cDi [Y = 1 | X = x] = fi(x)(1 \u2212 \u03b2) + (1 \u2212 fi(x))\u03b2. Consider the following random process: First, draw an unlabeled sample X = (x1, . . . , xm) i.i.d. from DmX . Then, draw B = (b1, . . . , bm) independently from a Bernoulli distribution with P[bi = 1] = \u03b2. For i \u2208 [T ], let Si(X,B) = ((x1, y1), . . . , (xm, ym)) such that xi are set by X , and yi = fi(x) if bi = 0 and 1 \u2212 fi(x) otherwise. Clearly, Si(X,B) is distributed according to Dmi . Let hi(S) be the output of A when the labeled sample is S. Denote by h\u0302i the (random) output of A when the sample is drawn from Di. Clearly\nE[err(h\u0302i,Di)] = E X,B [err(hi(S(X,B)),Di)].\nTherefore (as in (5.4) in Shalev-Shwartz and Ben-David [2014]), for some j,X,B it holds that\nE[err(h\u0302j ,Dj)] \u2265 1\nT\nT \u2211\ni=1\nE[err(h\u0302i,Di)] \u2265 1\nT\nT \u2211\ni=1\nerr(hi(S(X,B)),Di). (20)\nFix X,B, j as above, and denote for brevity hi := hi(S(X,B)). Let Vi be the set of examples x \u2208 X for which that A does not observe their label if the labeled sample is Si(X,B) (this includes both examples that are not in the sample at all as well as examples that are in the sample but their label is not requested by A). We have |Vi| > |X |/2 by assumption. Then (as in Eq. (5.6) therein)\n1\nT\nT \u2211\ni=1\nerr(hi,Di) \u2265 1\nT\nT \u2211\ni=1\n1\n2|Vi| \u2211 x\u2208Vi I[hi(x) 6= fi(x)]. (21)\nSince A is active, it selects which examples to request, which can depend on the labels observed by A so far. Therefore, Vi can be different for different i. However, an argument similar to that of the No-Free-Lunch theorem for the passive case still goes through, as follows.\nLet i, i\u2032 such that fi(x) = fi\u2032(x) for all x /\u2208 Vi, and fi(x) = 1 \u2212 fi\u2032(x) for all x \u2208 Vi. Since X,B are fixed, A observes the same labels for all x /\u2208 Vi for both Si \u2032\n(X,B) and Si(X,B), thus all its decisions and requests are identical for both samples, and so Vi = Vi\u2032 , and hi = hi\u2032 . Therefore, it is possible to partition T into T/2 pairs of indices i, i\u2032 such that for each such pair,\n1\n2|Vi| \u2211 x\u2208Vi I[hi(x) 6= fi(x)] +\n1\n2|Vi\u2032 | \u2211 x\u2208Vi\u2032 I[hi\u2032(x) 6= fi\u2032(x)]\n= 1 2|Vi| \u2211 x\u2208Vi I[hi(x) 6= fi(x)] + I[hi(x) 6= 1\u2212 fi(x)] = 1\n2 .\nTherefore, 1T \u2211T i=1 err(hi,Di) \u2265 14 . Therefore, from Eq. (21), 1T \u2211T i=1 err(hi,Di) \u2265 14 . Combining this with Eq. (20), it follows that E[err(h\u0302j ,Dj)] \u2265 14 .\nWe will also make use of the following simple lemma.\nLemma 8.2. Let \u03b2 \u2208 [0, 1]. Let D be a distribution over X\u00d7{0, 1} such that for (X,Y ) \u223c D, for any x in the support of D, P[Y = 1 | X = x] \u2208 {\u03b2, 1\u2212 \u03b2}. Let N be the size of the support of D. Let S \u223c Dm. Denote by nx the number of sample pairs (x\u2032, y\u2032) in S where x\u2032 = x, and let n+x be the number of sample pairs (x\n\u2032, y\u2032) where x\u2032 = x and y\u2032 = 1. Let p\u0302+x = n + x /nx (or zero if nx = 0). Then\n2\u03b2(1\u2212 \u03b2)(m\u2212N) \u2264 \u2211\nx\u2208X E[2nxp\u0302\n+ x (1\u2212 p\u0302+x )] \u2264 2\u03b2(1\u2212 \u03b2)m.\nProof. We have\nE[2nxp\u0302 + x (1 \u2212 p\u0302+x )] =\n\u221e \u2211\ni=1\nP[nx = i] \u00b7 i \u00b7 E[2p\u0302+x (1 \u2212 p\u0302+x ) | nx = i].\nNote that E[2p\u0302+x (1 \u2212 p\u0302+x ) | nx = 1] = 0. For i > 1, let y1, . . . , yi be the labels of the examples that are equal to x in S, then\n\u2211\nj,k\u2208[i] I[yk 6= yj] = 2n+x (i \u2212 n+x ) = i2 \u00b7 2p\u0302+x (1\u2212 p\u0302+x ).\nTherefore, letting (X1, Y1), (X2, Y2) \u223c D2,\nE S\u223cDm\n[2p\u0302+x (1\u2212 p\u0302+x ) | nx = i] = 1\ni2 E S\u223cDm [\n\u2211\nj,k\u2208[nx] I[yk 6= yj ] | nx = i]\n= i2 \u2212 i i2 P[Y1 6= Y2 | X1 = X2 = x] = 2(1\u2212 1 i )\u03b2(1 \u2212 \u03b2),\nThus\nE[2nxp\u0302 + x (1 \u2212 p\u0302+x )] = 2\u03b2(1\u2212 \u03b2)\n\u221e \u2211\ni=2\n(i\u2212 1)P[nx = i]\n= 2\u03b2(1\u2212 \u03b2)(E[nx] + P[nx = 0]\u2212 1).\nTo complete the proof, sum over all x in the support of D, and note that \u2211x E[nx] = m, and \u2211\nx(P[nx = 0]\u2212 1) \u2208 [\u2212N, 0].\nWe now prove our lower bound, stated in Theorem 3.3, on the number of queries required by any active learning with competitive guarantees similar to ours.\nof Theorem 3.3. Let N = \u230a m\u03b1\u2212log(m \u03b4 )\nlog(m)\n\u230b\n. Let \u03b2 = 8\u03b1 \u2264 12 . Consider a marginal distribution DX over X which is uniform over N points 1, . . . , N \u2208 R. Consider the following family of distributions: D such that its marginal over X is DX , and for each x \u2208 X , P[Y = 1 | X = x] \u2208 {\u03b2, 1 \u2212 \u03b2}. Thus the Bayes optimal error for each of these distributions is \u03b2.\nLet S \u223c Dm. If one example in S is changed, \u03bd(12 ) changes by at most 1/m. Hence, by McDiarmid\u2019s inequality [McDiarmid, 1989], with probability at least 1 \u2212 128 , |\u03bd(12 ) \u2212 E[\u03bd(12 )]| \u2264 \u221a log(28) 2m . Denote the event that this holds EM . Since \u03b2 = 8\u03b1 \u2265 log(m)+log(28)\u221a 2m\n, it follows that under EM ,\n|\u03bd(1 2 )\u2212 E[\u03bd( 1 2 )]| \u2264 \u03b2/8. (22)\nWe now bound E[\u03bd(12 )]. Using the notation p\u0302 + x , nx, n + x as in Lemma 8.2, we have\n\u03bd( 1\n2 ) =\n1\nm\n\u2211 x\u2208X min(n+x , nx \u2212 n+x ) =\n1\nm\n\u2211 x\u2208X nx min(p + x , 1\u2212 p+x )\nAlso, for all p \u2208 [0, 1], min(p, 1\u2212 p) \u2264 2p(1\u2212 p) \u2264 2min(p, 1\u2212 p). Therefore\n1\n2m\n\u2211 x\u2208X 2nxp + x (1\u2212 p+x ) \u2264 \u03bd(\n1 2 ) \u2264 1 m \u2211\nx\u2208X 2nxp\n+ x (1\u2212 p+x ).\nBy Lemma 8.2, it follows that\nm\u2212N m \u03b2(1 \u2212 \u03b2) \u2264 E[\u03bd(1 2 )] \u2264 2\u03b2(1\u2212 \u03b2).\nSince N \u2264 m/2 and \u03b2 \u2208 [0, 12 ], E[\u03bd(12 )] \u2208 (\u03b2/4, 2\u03b2). Combining this with Eq. (22), we get that under EM , \u03b1 = \u03b2/8 \u2264 \u03bd(12 ) \u2264 178 \u03b2 = 17\u03b1.\nNow, we bound Gmin from above and below assuming EM holds. Denote\nG(t) := GB(\u03bd(t),N (t), \u03b4,m, 1).\nTo establish a lower bound onGmin(m, \u03b4), note thatGmin(m, \u03b4) = mint>0 G(t) \u2265 mint>0 \u03bd(t). For t \u2208 (0, 12 ), \u03bd(t) = \u03bd(12 ) (since the distances between any two distinct points in S is at\nleast 1). In addition, since \u03bd is monotonically increasing, we have \u03bd(t) \u2265 \u03bd(12 ) for t \u2265 12 . Hence mint>0 \u03bd(t) \u2265 \u03bd(12 ) \u2265 \u03b2/8 = \u03b1.\nTo show an upper bound on Gmin(m, \u03b4), we upper bound G(12 ). Note that N (12 ) \u2264 N . Recall the definition of \u03c6(t) in Eq. (4). We have\n\u03c6( 1\n2 ) =\n(N + 1) log(m) + log(1\u03b4 )\nm \u2264 \u03b1.\nThen, since \u03bd(12 ) \u2264 17\u03b1,\nG( 1 2 ) \u2264 m m\u2212N (\u03bd( 1 2 ) + 2 3 \u03b1+ 3\u221a 2\n\u221a\n\u03bd( 1\n2 )\u03b1) \u2264 30\u03b1.\nIn the last inequality we used the fact that mm\u2212N \u2264 10/9. So if EM holds, Gmin(m, \u03b4) \u2264 G(12 ) \u2264 30\u03b1.\nFrom the assumption onA, with probability at least 1\u2212\u03b4, we have err(h\u0302,D) \u2264 CGmin(m, \u03b4) \u2264 30C\u03b1 \u2264 1/8 (since \u03b1 \u2264 1240C ). Let EL(D) denote the event that A queries fewer than N/2 labels, where the probability is over the randomness of S and A. Let h\u2032 be the output of an algorithm that behaves like A in cases where EL(D) holds, and queries at most N/2 otherwise. By Theorem 8.1, there exists some D in the family of distributions such that E[err(h\u2032,D)] \u2265 14 . By Markov\u2019s inequality, P[err(h\u2032,D) \u2265 18 ] \u2265 1/7. Also, P[h\u2032 = h\u0302] \u2265 P[EL(D)]. Therefore\nP[err(h\u0302,D) \u2265 1 8 ] \u2265 P[err(h\u2032,D) \u2265 1 8 ] + P[EL(D)]\u2212 1 = P[EL(D)] \u2212 6/7.\nTherefore P[EL(D)] \u2212 6/7 \u2264 P[err(h\u0302,D) \u2265 18 ] \u2264 \u03b4. Since by assumption \u03b4 \u2264 1/14, it follows that P[EL(D)] \u2264 6/7 + \u03b4 \u2264 13/14. It follows that with a probability of at least 1/14, the negation of EL(D) holds. Since also EM holds with probability at least 1\u2212 128 , it follows that with a probability of at least 128 , both EM and the negation of EL(D) hold. Now, as shown above, EM implies the bounds on Gmin(m, \u03b4) (item 1 in the theorem statement). In addition, the negation of EL(D) implies that A queries at least N/2 = 12 \u230a m\u03b1\u2212log(m \u03b4 ) log(m) \u230b labels (item 2 in the theorem statement). This completes the proof."}, {"heading": "9 Discussion", "text": "We have presented an efficient fully empirical proximity-based non parametric active learner. Our approach provides competitive error guarantees for general distributions, in a general metric space, while keeping label complexity significantly lower than any passive learner with the same guarantees. MARMANN yields fully empirical error estimates, easily computable from finite samples. This is in contrast with classic techniques, that present bounds and rates that depend on unknown distribution-dependent quantities.\nAn interesting question is whether the guarantees can be related to the Bayes error of the distribution. Our error guarantees give a constant factor over the error guarantees of Gottlieb et al. [2016b]. A variant of this approach [Gottlieb et al., 2010] was shown to be Bayes-consistent [Kontorovich and Weiss, 2015], and we conjecture that this holds also for\nthe algorithm of Gottlieb et al. [2016b]. Since in our analysis MARMANN obtains a constant factor over the error of the passive learner, Bayes-consistency cannot be inferred from our present techniques; we leave this problem open for future research."}, {"heading": "Acknowledgements", "text": "Sivan Sabato was partially supported by the Israel Science Foundation (grant No. 555/15). Aryeh Kontorovich was partially supported by the Israel Science Foundation (grants No. 1141/12 and 755/15) and a Yahoo Faculty award. We thank Lee-Ad Gottlieb and Dana Ron for the helpful discussions."}], "references": [{"title": "Neural Network Learning: Theoretical Foundations", "author": ["M. Anthony", "P.L. Bartlett"], "venue": null, "citeRegEx": "Anthony and Bartlett.,? \\Q1999\\E", "shortCiteRegEx": "Anthony and Bartlett.", "year": 1999}, {"title": "Fast learning rates in statistical inference through aggregation", "author": ["J.-Y. Audibert"], "venue": "Ann. Statist., 37(4):1591\u20131646,", "citeRegEx": "Audibert.,? \\Q2009\\E", "shortCiteRegEx": "Audibert.", "year": 2009}, {"title": "The power of localization for efficiently learning linear separators with noise", "author": ["P. Awasthi", "M. Balcan", "P.M. Long"], "venue": "In Symposium on Theory of Computing,", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Margin based active learning", "author": ["M. Balcan", "A.Z. Broder", "T. Zhang"], "venue": "In Proceedings of the 20th Annual Conference on Learning Theory, COLT", "citeRegEx": "Balcan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2007}, {"title": "The true sample complexity of active learning", "author": ["M. Balcan", "S. Hanneke", "J.W. Vaughan"], "venue": "Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2010}, {"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Balcan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2009}, {"title": "A finite sample analysis of the naive bayes classifier", "author": ["D. Berend", "A. Kontorovich"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Berend and Kontorovich.,? \\Q2015\\E", "shortCiteRegEx": "Berend and Kontorovich.", "year": 2015}, {"title": "Active nearest neighbors in changing environments", "author": ["C. Berlind", "R. Urner"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Berlind and Urner.,? \\Q2015\\E", "shortCiteRegEx": "Berlind and Urner.", "year": 2015}, {"title": "In defense of nearest-neighbor based image classification", "author": ["O. Boiman", "E. Shechtman", "M. Irani"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Boiman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Boiman et al\\.", "year": 2008}, {"title": "Using the doubling dimension to analyze the generalization of learning algorithms", "author": ["N.H. Bshouty", "Y. Li", "P.M. Long"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Bshouty et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bshouty et al\\.", "year": 2009}, {"title": "Minimax bounds for active learning", "author": ["R.M. Castro", "R.D. Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Castro and Nowak.,? \\Q2008\\E", "shortCiteRegEx": "Castro and Nowak.", "year": 2008}, {"title": "Faster rates in regression via active learning", "author": ["R.M. Castro", "R. Willett", "R.D. Nowak"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Castro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2005}, {"title": "Active learning on trees and graphs", "author": ["N. Cesa-Bianchi", "C. Gentile", "F. Vitale", "G. Zappella"], "venue": "In Proceedings of rhe 23rd Conference on Learning Theory, COLT", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2010}, {"title": "Rates of convergence for nearest neighbor classification", "author": ["K. Chaudhuri", "S. Dasgupta"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chaudhuri and Dasgupta.,? \\Q2014\\E", "shortCiteRegEx": "Chaudhuri and Dasgupta.", "year": 2014}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover and Hart.,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart.", "year": 1967}, {"title": "S2: an efficient graph based active learning algorithm with application to nonparametric classification", "author": ["G. Dasarathy", "R.D. Nowak", "X. Zhu"], "venue": "In Proceedings of the 28th Annual Conference on Learning Theory,", "citeRegEx": "Dasarathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dasarathy et al\\.", "year": 2015}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dasgupta.,? \\Q2004\\E", "shortCiteRegEx": "Dasgupta.", "year": 2004}, {"title": "Consistency of nearest neighbor classification under selective sampling", "author": ["S. Dasgupta"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory, COLT", "citeRegEx": "Dasgupta.,? \\Q2012\\E", "shortCiteRegEx": "Dasgupta.", "year": 2012}, {"title": "Hierarchical sampling for active learning", "author": ["S. Dasgupta", "D. Hsu"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Dasgupta and Hsu.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Hsu.", "year": 2008}, {"title": "Nonparametric density estimation: the L1 view. Wiley Series in Probability and Mathematical Statistics: Tracts on Probability and Statistics", "author": ["L. Devroye", "L. Gy\u00f6rfi"], "venue": null, "citeRegEx": "Devroye and Gy\u00f6rfi.,? \\Q1985\\E", "shortCiteRegEx": "Devroye and Gy\u00f6rfi.", "year": 1985}, {"title": "A probabilistic theory of pattern recognition, volume 31 of Applications of Mathematics (New York)", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": null, "citeRegEx": "Devroye et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 1996}, {"title": "A (1 + \u01eb)-embedding of low highway dimension graphs into bounded treewidth graphs", "author": ["A.E. Feldmann", "W.S. Fung", "J. K\u00f6nemann", "I. Post"], "venue": "CoRR, abs/1502.04588,", "citeRegEx": "Feldmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feldmann et al\\.", "year": 2015}, {"title": "Report Number 4, Project Number 21-49-004", "author": ["E. Fix", "J.L.J. Hodges"], "venue": "USAF School of Aviation", "citeRegEx": "Fix and Hodges,? \\Q1951\\E", "shortCiteRegEx": "Fix and Hodges", "year": 1951}, {"title": "Discriminatory analysis. nonparametric discrimination: Consistency properties", "author": ["E. Fix", "J.L.J. Hodges"], "venue": "International Statistical Review / Revue Internationale de Statistique,", "citeRegEx": "Fix and Hodges,? \\Q1989\\E", "shortCiteRegEx": "Fix and Hodges", "year": 1989}, {"title": "Sample compression, learnability, and the vapnik-chervonenkis dimension", "author": ["S. Floyd", "M.K. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Floyd and Warmuth.,? \\Q1995\\E", "shortCiteRegEx": "Floyd and Warmuth.", "year": 1995}, {"title": "Efficient active learning of halfspaces: an aggressive approach", "author": ["A. Gonen", "S. Sabato", "S. Shalev-Shwartz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gonen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gonen et al\\.", "year": 2013}, {"title": "Efficient active learning of halfspaces: an aggressive approach", "author": ["A. Gonen", "S. Sabato", "S. Shalev-Shwartz"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Gonen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gonen et al\\.", "year": 2013}, {"title": "Efficient classification for metric data", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Gottlieb et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2014}, {"title": "Near-optimal sample compression for nearest neighbors", "author": ["L. Gottlieb", "A. Kontorovich", "P. Nisnevitch"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gottlieb et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2014}, {"title": "Proximity algorithms for nearly-doubling spaces", "author": ["L.-A. Gottlieb", "R. Krauthgamer"], "venue": "In APPROX-RANDOM, pages 192\u2013204,", "citeRegEx": "Gottlieb and Krauthgamer.,? \\Q2010\\E", "shortCiteRegEx": "Gottlieb and Krauthgamer.", "year": 2010}, {"title": "Proximity algorithms for nearly doubling spaces", "author": ["L.-A. Gottlieb", "R. Krauthgamer"], "venue": "SIAM J. Discrete Math.,", "citeRegEx": "Gottlieb and Krauthgamer.,? \\Q2013\\E", "shortCiteRegEx": "Gottlieb and Krauthgamer.", "year": 2013}, {"title": "Efficient classification for metric data", "author": ["L.-A. Gottlieb", "L. Kontorovich", "R. Krauthgamer"], "venue": "In Proceedings of the 23rd Annual Conference on Learning Theory, COLT", "citeRegEx": "Gottlieb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2010}, {"title": "Adaptive metric dimensionality reduction", "author": ["L.-A. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "In Proceedings of the 24th International Conference on Algorithmic Learning Theory, ALT", "citeRegEx": "Gottlieb et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2013}, {"title": "Adaptive metric dimensionality reduction", "author": ["L.-A. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "Theoretical Computer Science,", "citeRegEx": "Gottlieb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2016}, {"title": "Nearly optimal classification for semimetrics", "author": ["L.-A. Gottlieb", "A. Kontorovich", "P. Nisnevitch"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gottlieb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2016}, {"title": "Pac-bayesian compression bounds on the prediction error of learning algorithms for classification", "author": ["T. Graepel", "R. Herbrich", "J. Shawe-Taylor"], "venue": "Machine Learning,", "citeRegEx": "Graepel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graepel et al\\.", "year": 2005}, {"title": "Rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "The Annals of Statistics,", "citeRegEx": "Hanneke.,? \\Q2011\\E", "shortCiteRegEx": "Hanneke.", "year": 2011}, {"title": "Minimax analysis of active learning", "author": ["S. Hanneke", "L. Yang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hanneke and Yang.,? \\Q2015\\E", "shortCiteRegEx": "Hanneke and Yang.", "year": 2015}, {"title": "Exact lower bounds for the agnostic probably-approximatelycorrect (PAC) machine learning model", "author": ["A. Kontorovich", "I. Pinelis"], "venue": null, "citeRegEx": "Kontorovich and Pinelis.,? \\Q2016\\E", "shortCiteRegEx": "Kontorovich and Pinelis.", "year": 2016}, {"title": "Maximum margin multiclass nearest neighbors", "author": ["A. Kontorovich", "R. Weiss"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Kontorovich and Weiss.,? \\Q2014\\E", "shortCiteRegEx": "Kontorovich and Weiss.", "year": 2014}, {"title": "A bayes consistent 1-nn classifier", "author": ["A. Kontorovich", "R. Weiss"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kontorovich and Weiss.,? \\Q2015\\E", "shortCiteRegEx": "Kontorovich and Weiss.", "year": 2015}, {"title": "k-nn regression adapts to local intrinsic dimension", "author": ["S. Kpotufe"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kpotufe.,? \\Q2011\\E", "shortCiteRegEx": "Kpotufe.", "year": 2011}, {"title": "Hierarchical label queries with data-dependent partitions", "author": ["S. Kpotufe", "R. Urner", "S. Ben-David"], "venue": "In Proceedings of the 28th Annual Conference on Learning Theory, COLT", "citeRegEx": "Kpotufe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kpotufe et al\\.", "year": 2015}, {"title": "Navigating nets: Simple algorithms for proximity search", "author": ["R. Krauthgamer", "J.R. Lee"], "venue": "In 15th Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Krauthgamer and Lee.,? \\Q2004\\E", "shortCiteRegEx": "Krauthgamer and Lee.", "year": 2004}, {"title": "Rates of convergence of nearest neighbor estimation under arbitrary sampling", "author": ["S.R. Kulkarni", "S.E. Posner"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Kulkarni and Posner.,? \\Q1995\\E", "shortCiteRegEx": "Kulkarni and Posner.", "year": 1995}, {"title": "Relating data compression and learnability, unpublished", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": null, "citeRegEx": "Littlestone and Warmuth.,? \\Q1986\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1986}, {"title": "Empirical Bernstein bounds and sample-variance penalization", "author": ["A. Maurer", "M. Pontil"], "venue": "In Proceedings of the 22nd Annual Conference on Learning", "citeRegEx": "Maurer and Pontil.,? \\Q2009\\E", "shortCiteRegEx": "Maurer and Pontil.", "year": 2009}, {"title": "Employing EM and pool-based active learning for text classification", "author": ["A. McCallum", "K. Nigam"], "venue": "In Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "McCallum and Nigam.,? \\Q1998\\E", "shortCiteRegEx": "McCallum and Nigam.", "year": 1998}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": null, "citeRegEx": "McDiarmid.,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid.", "year": 1989}, {"title": "Active regression by stratification", "author": ["S. Sabato", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sabato and Munos.,? \\Q2014\\E", "shortCiteRegEx": "Sabato and Munos.", "year": 2014}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": null, "citeRegEx": "Shalev.Shwartz and Ben.David.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Ben.David.", "year": 2014}, {"title": "Structural risk minimization over data-dependent hierarchies", "author": ["J. Shawe-Taylor", "P.L. Bartlett", "R.C. Williamson", "M. Anthony"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Shawe.Taylor et al\\.,? \\Q1926\\E", "shortCiteRegEx": "Shawe.Taylor et al\\.", "year": 1926}, {"title": "Consistent nonparametric regression", "author": ["C.J. Stone"], "venue": "The Annals of Statistics,", "citeRegEx": "Stone.,? \\Q1977\\E", "shortCiteRegEx": "Stone.", "year": 1977}, {"title": "PLAL: cluster-based active learning", "author": ["R. Urner", "S. Wulff", "S. Ben-David"], "venue": "In Proceedings of the 26th Annual Conference on Learning Theory, COLT", "citeRegEx": "Urner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urner et al\\.", "year": 2013}, {"title": "Distance-based classification with Lipschitz functions", "author": ["U. von Luxburg", "O. Bousquet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Luxburg and Bousquet.,? \\Q2004\\E", "shortCiteRegEx": "Luxburg and Bousquet.", "year": 2004}, {"title": "Submodularity in data subset selection and active learning", "author": ["K. Wei", "R.K. Iyer", "J.A. Bilmes"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Wei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "Exponential bounds of mean error for the nearest neighbor estimates of regression functions", "author": ["L.C. Zhao"], "venue": "J. Multivariate Anal.,", "citeRegEx": "Zhao.,? \\Q1987\\E", "shortCiteRegEx": "Zhao.", "year": 1987}, {"title": "Combining active learning and semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "J. Lafferty", "Z. Ghahramani"], "venue": "In ICML 2003 workshop,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 47, "context": "In pool-based active learning [McCallum and Nigam, 1998], a collection of random examples is provided, and the algorithm can interactively query an oracle to label some of the examples.", "startOffset": 30, "endOffset": 56}, {"referenceID": 35, "context": ", 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005].", "startOffset": 79, "endOffset": 101}, {"referenceID": 40, "context": "[2014a], has been shown to be Bayes consistent [Kontorovich and Weiss, 2015].", "startOffset": 47, "endOffset": 76}, {"referenceID": 27, "context": "Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale. A central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information. We derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al.", "startOffset": 115, "endOffset": 1430}, {"referenceID": 27, "context": "Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale. A central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information. We derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al. [2016b], proposed in Gottlieb et al.", "startOffset": 115, "endOffset": 1478}, {"referenceID": 27, "context": "Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale. A central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information. We derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al. [2016b], proposed in Gottlieb et al. [2014a], has been shown to be Bayes consistent [Kontorovich and Weiss, 2015].", "startOffset": 115, "endOffset": 1515}, {"referenceID": 49, "context": "Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014].", "startOffset": 92, "endOffset": 116}, {"referenceID": 4, "context": "An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010].", "startOffset": 87, "endOffset": 108}, {"referenceID": 18, "context": "The paradigm of cluster-based active learning [Dasgupta and Hsu, 2008] has been shown to provide label savings under some distributional clusterability assumptions [Urner et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 2, "context": ", 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010]. The potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees. Castro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals.", "startOffset": 23, "endOffset": 855}, {"referenceID": 2, "context": ", 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010]. The potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees. Castro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals.", "startOffset": 23, "endOffset": 880}, {"referenceID": 2, "context": ", 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010]. The potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees. Castro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals. The paradigm of cluster-based active learning [Dasgupta and Hsu, 2008] has been shown to provide label savings under some distributional clusterability assumptions [Urner et al., 2013, Kpotufe et al., 2015]. Dasgupta and Hsu [2008] showed that a suitable cluster-tree can yield label savings in this framework, and papers following up quantified the label savings under distributional clusterability assumptions.", "startOffset": 23, "endOffset": 1337}, {"referenceID": 17, "context": "to a consistent algorithm [Dasgupta, 2012].", "startOffset": 26, "endOffset": 42}, {"referenceID": 7, "context": "A selective querying strategy has been shown to be beneficial for nearest neighbors under covariate shift [Berlind and Urner, 2015], where one needs to adapt to a change in the data generating process.", "startOffset": 106, "endOffset": 131}, {"referenceID": 27, "context": "2, and present the guarantees of the compression-based passive learner of Gottlieb et al. [2016b] in Section 2.", "startOffset": 74, "endOffset": 98}, {"referenceID": 29, "context": "Constructing a minimum size t-net for a general set B is NP-hard [Gottlieb and Krauthgamer, 2010].", "startOffset": 65, "endOffset": 97}, {"referenceID": 43, "context": "The size of any t-net of a metric space A \u2286 X is at most \u2308diam(A)/t\u2309ddim(X )+1 [Krauthgamer and Lee, 2004].", "startOffset": 79, "endOffset": 106}, {"referenceID": 9, "context": "Generalization bounds in terms of the doubling dimension of the hypothesis space were established in Bshouty et al. [2009], while runtime and generalization errors in terms of ddim(X ) were given in Gottlieb et al.", "startOffset": 101, "endOffset": 123}, {"referenceID": 9, "context": "Generalization bounds in terms of the doubling dimension of the hypothesis space were established in Bshouty et al. [2009], while runtime and generalization errors in terms of ddim(X ) were given in Gottlieb et al. [2014a]. Constructing a minimum size t-net for a general set B is NP-hard [Gottlieb and Krauthgamer, 2010].", "startOffset": 101, "endOffset": 223}, {"referenceID": 26, "context": "As shown in Gottlieb and Krauthgamer [2013], the doubling dimension is \u201calmost hereditary\u201d in the sense that forA \u2282 X , we have ddim(A) \u2264 cddim(X ) for some universal constant c \u2264 2 [Feldmann et al.", "startOffset": 12, "endOffset": 44}, {"referenceID": 21, "context": "As shown in Gottlieb and Krauthgamer [2013], the doubling dimension is \u201calmost hereditary\u201d in the sense that forA \u2282 X , we have ddim(A) \u2264 cddim(X ) for some universal constant c \u2264 2 [Feldmann et al., 2015, Lemma 6.6]. For simplicity, the bounds above are presented in terms of ddim(X ), the doubling dimension of the ambient space. It should be noted that one can obtain tighter bounds in terms of ddim(U(S)) when the latter is substantially lower than that of the ambient space, and it is also possible to perform metric dimensionality reduction, as in Gottlieb et al. [2013].", "startOffset": 183, "endOffset": 577}, {"referenceID": 27, "context": "3 Passive compression-based nearest-neighbors Non-parameteric binary classification admits performance guarantees that scale with the sample\u2019s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al.", "startOffset": 189, "endOffset": 289}, {"referenceID": 27, "context": "3 Passive compression-based nearest-neighbors Non-parameteric binary classification admits performance guarantees that scale with the sample\u2019s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al. [2010] analyzed the generalization performance via the technique of Lipschitz extension.", "startOffset": 189, "endOffset": 316}, {"referenceID": 27, "context": "3 Passive compression-based nearest-neighbors Non-parameteric binary classification admits performance guarantees that scale with the sample\u2019s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al. [2010] analyzed the generalization performance via the technique of Lipschitz extension. Later, it was noticed in Gottlieb et al. [2014b] that the presence of a margin allows for compression \u2014 in fact, nearly optimally so.", "startOffset": 189, "endOffset": 447}, {"referenceID": 27, "context": "Gottlieb et al. [2016b] propose a passive learner with the following guarantees1 as a function of the separation of S.", "startOffset": 0, "endOffset": 24}, {"referenceID": 27, "context": "3 (Gottlieb et al. [2016b]).", "startOffset": 3, "endOffset": 27}, {"referenceID": 27, "context": "3 (Gottlieb et al. [2016b]). Let m be an integer, \u03b4 \u2208 (0, 1). There exists a passive learning algorithm that returns a nearest-neighbor classifier h Spas , where Spas \u2286 Sin, such that, with probability 1\u2212 \u03b4, err(h Spas ,D) \u2264 Gmin(m, \u03b4). The passive algorithm of Gottlieb et al. [2016b] generates Spas of size approximately N (t) for the optimal scale t > 0 (found by searching over all scales), by removing the |Sin|\u03bd(t) points that obstruct the t-separation between different labels in Sin, and then selecting a subset of the remaining labeled examples to form Spas, so that the examples are a t-net for Sin (not including the obstructing points).", "startOffset": 3, "endOffset": 286}, {"referenceID": 27, "context": "3 (Gottlieb et al. [2016b]). Let m be an integer, \u03b4 \u2208 (0, 1). There exists a passive learning algorithm that returns a nearest-neighbor classifier h Spas , where Spas \u2286 Sin, such that, with probability 1\u2212 \u03b4, err(h Spas ,D) \u2264 Gmin(m, \u03b4). The passive algorithm of Gottlieb et al. [2016b] generates Spas of size approximately N (t) for the optimal scale t > 0 (found by searching over all scales), by removing the |Sin|\u03bd(t) points that obstruct the t-separation between different labels in Sin, and then selecting a subset of the remaining labeled examples to form Spas, so that the examples are a t-net for Sin (not including the obstructing points). For the binary classification case (|Y| = 2) an efficient algorithm is shown in Gottlieb et al. [2016b]. However, in the general multiclass case, it is not known how to find a minimal t-separation efficiently \u2014 a naive approach requires solving the NP-hard problem of vertex cover.", "startOffset": 3, "endOffset": 753}, {"referenceID": 27, "context": "A main challenge for active learning in our non-parametric setting is performing model selection, that is, selecting a good scale t similarly to the passive learner of Gottlieb et al. [2016b]. In the passive supervised setting, the approach developed in several previous works [Gottlieb et al.", "startOffset": 168, "endOffset": 192}, {"referenceID": 36, "context": "2In the case of binary labels (|Y| = 2), the problem of estimating Sa(t) can be formulated as a special case of the benign noise setting for parametric active learning, for which tight lower and upper bounds are provided in Hanneke and Yang [2015]. However, our case is both more general (as we allow multiclass labels) and more specific (as we are dealing with a specific \u201chypothesis class\u201d).", "startOffset": 224, "endOffset": 248}, {"referenceID": 46, "context": "3This follows from Theorem 4 of Maurer and Pontil [2009] since 7 3(n\u22121) \u2264 8 3n for n \u2265 8.", "startOffset": 32, "endOffset": 57}, {"referenceID": 43, "context": "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al.", "startOffset": 132, "endOffset": 163}, {"referenceID": 20, "context": "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995].", "startOffset": 167, "endOffset": 189}, {"referenceID": 20, "context": "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995]. As noted in Section 4, our algorithm relies on a generalized sample compression scheme, which requires side information.", "startOffset": 167, "endOffset": 243}, {"referenceID": 20, "context": "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995]. As noted in Section 4, our algorithm relies on a generalized sample compression scheme, which requires side information. This side information is used to represent the labels of the sample points in the compression set. A similar idea appears in Floyd and Warmuth [1995] for hypotheses with short description length.", "startOffset": 167, "endOffset": 515}, {"referenceID": 27, "context": "If the compression set includes only the original labels, the compression analysis of Gottlieb et al. [2016b] gives the bound GB(\u01eb,N, \u03b4,m, 1).", "startOffset": 86, "endOffset": 110}, {"referenceID": 6, "context": "This expression previously appeared in Berend and Kontorovich [2015, Equation (25)] in the context of information-theoretic lower bounds; the current terminology was motivated in Kontorovich and Pinelis [2016], where various precise estimates on bayes(\u00b7) were provided.", "startOffset": 39, "endOffset": 210}, {"referenceID": 0, "context": ", Anthony and Bartlett [1999] p.", "startOffset": 2, "endOffset": 30}, {"referenceID": 50, "context": "4) in Shalev-Shwartz and Ben-David [2014]), for some j,X,B it holds that E[err(\u0125j ,Dj)] \u2265 1 T T", "startOffset": 6, "endOffset": 42}, {"referenceID": 48, "context": "Hence, by McDiarmid\u2019s inequality [McDiarmid, 1989], with probability at least 1 \u2212 1 28 , |\u03bd(2 ) \u2212 E[\u03bd(12 )]| \u2264 \u221a", "startOffset": 33, "endOffset": 50}, {"referenceID": 31, "context": "A variant of this approach [Gottlieb et al., 2010] was shown to be Bayes-consistent [Kontorovich and Weiss, 2015], and we conjecture that this holds also for", "startOffset": 27, "endOffset": 50}, {"referenceID": 40, "context": ", 2010] was shown to be Bayes-consistent [Kontorovich and Weiss, 2015], and we conjecture that this holds also for", "startOffset": 41, "endOffset": 70}, {"referenceID": 27, "context": "Our error guarantees give a constant factor over the error guarantees of Gottlieb et al. [2016b]. A variant of this approach [Gottlieb et al.", "startOffset": 73, "endOffset": 97}, {"referenceID": 27, "context": "the algorithm of Gottlieb et al. [2016b]. Since in our analysis MARMANN obtains a constant factor over the error of the passive learner, Bayes-consistency cannot be inferred from our present techniques; we leave this problem open for future research.", "startOffset": 17, "endOffset": 41}], "year": 2016, "abstractText": "We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure.", "creator": "LaTeX with hyperref package"}}}