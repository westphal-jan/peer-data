{"id": "1605.07139", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Fairness in Learning: Classic and Contextual Bandits", "abstract": "We introduce wyomissing the macgibbon study 239.7 of transports fairness in modric multi - vek armed bandit problems. help Our fairness definition cubillo can be interpreted as 325 demanding shina that given disrespects a leglock pool of applicants (terrority say, 15p for sl\u0103nic college gargrave admission or stricta mortgages ), 52.68 a springsteen worse fideicommissum applicant f.g. is ronni never bahagia favored high-tech over a stanislao better mourvedre one, fmoore despite a undersigned learning cleanly algorithm ' awani s pyrah uncertainty over the true shard payoffs. We prove results of two starforce types.", "histories": [["v1", "Mon, 23 May 2016 18:58:24 GMT  (511kb,D)", "http://arxiv.org/abs/1605.07139v1", null], ["v2", "Mon, 7 Nov 2016 15:49:05 GMT  (150kb,D)", "http://arxiv.org/abs/1605.07139v2", "A condensed version of this work appears in the 30th Annual Conference on Neural Information Processing Systems (NIPS), 2016"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["matthew joseph", "michael kearns", "jamie h morgenstern", "aaron roth"], "accepted": true, "id": "1605.07139"}, "pdf": {"name": "1605.07139.pdf", "metadata": {"source": "CRF", "title": "Fairness in Learning: Classic and Contextual Bandits\u2217", "authors": ["Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth"], "emails": ["majos@cis.upenn.edu.", "mkearns@cis.upenn.edu.", "jamiemor@cis.upenn.edu.", "aaroth@cis.upenn.edu."], "sections": [{"heading": null, "text": "First, in the important special case of the classic stochastic bandits problem (i.e. in which there are no contexts), we provide a provably fair algorithm based on chained confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence. When combined with regret bounds for standard non-fair algorithms such as UCB, this proves a strong separation between fair and unfair learning, which extends to the general contextual case.\nIn the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm, and conversely any fair contextual bandit algorithm can be transformed into a KWIK learning algorithm. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms.\n\u2217Department of Computer and Information Sciences, University of Pennsylvania. {majos,mkearns,jamiemor,aaroth}@cis.upenn.edu. AR is supported in part by an NSF CAREER award, a Sloan Foundation Fellowship, and a Google Faculty Research Award.\nar X\niv :1\n60 5.\n07 13\n9v 1\n[ cs\n.L G\n] 2\n3 M\nay 2\n01 6\nContents"}, {"heading": "1 Introduction 3", "text": "1.1 Fairness and Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Other Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"}, {"heading": "2 Preliminaries 6", "text": "2.1 Specializing to Classic Stochastic Bandits . . . . . . . . . . . . . . . . . . . . . . . . 7"}, {"heading": "3 Fair Classic Stochastic Bandits: An Algorithm 8", "text": ""}, {"heading": "4 Fair Classic Stochastic Bandits: A Lower Bound 11", "text": ""}, {"heading": "5 KWIK Learnability Implies Fair Bandit Learnability 15", "text": ""}, {"heading": "6 Fair Bandit Learnability Implies KWIK Learnability 18", "text": "6.1 An Exponential Separation Between Fair and Unfair Learning . . . . . . . . . . . . . 20"}, {"heading": "A Missing Proofs for the Classic Stochastic Bandits Upper Bound 23", "text": "A.1 Missing Derivation of R(T ) for Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . 24"}, {"heading": "B Missing Proofs for the Classic Stochastic Bandits Lower Bound 24", "text": "C Missing Proofs for the Contextual Bandit Setting 26"}, {"heading": "1 Introduction", "text": "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people\u2019s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al., 2015]. These high stakes uses of machine learning have led to increasing concern in law and policy circles about the potential for (often opaque) machine learning techniques to be discriminatory or unfair [Coglianese and Lehr, 2016, Barocas and Selbst, 2016]. Moreover, these concerns are not merely hypothetical: Sweeney [2013] observed that contextual ads for public record services shown in response to Google searches for stereotypically African American names were more likely to contain text referring to arrest records, compared to comparable ads shown in response to searches for stereotypically Caucasian names, which showed more neutral text. She confirmed that this was not because of stated preferences of the advertisers, but rather the automated outcome of Google\u2019s targeting algorithms. Despite the recognized importance of this problem, very little is known about technical solutions to the problem of \u201cunfairness\u201d, or the extent to which \u201cfairness\u201d is in conflict with the goals of learning.1\nIn this paper, we consider the extent to which a natural fairness notion is compatible with learning in a general setting (the contextual bandit setting), which can be used to model many of the applications mentioned above in which machine learning is currently employed. In this model, the learner is a sequential decision maker, which must choose at each time step t which decision to make, out of a finite set of k choices (for example, which of k loan applicants \u2013 potentially from different populations or racial groups \u2013 to give a loan to). Before the learner makes its decision at round t, it observes some context xtj for each choice of arm j (x t j could, for example, represent the contents of the loan application of an individual from population j at round t). When the learner chooses arm j at time t, it obtains a stochastic reward rtj whose expectation is determined\nby some unknown function of the context: E [ rtj ] = fj(x t j). The goal of the learning algorithm is to maximize its expected reward \u2013 i.e. to approximate the optimal policy, which at each round,\nchooses arm j to maximize E [ rtj ] . The difficulty in this task stems from the unknown functions fj which map contexts to rewards; these functions must be learned. Despite this, there are many known algorithms for learning the optimal policy (in the absence of any fairness constraint)."}, {"heading": "1.1 Fairness and Learning", "text": "Our notion of individual fairness is very simple: it states that it is unfair to preferentially choose one individual (e.g. for a loan, a job, admission to college, etc.) over another if he or she is not as qualified as the other individual. This definition of fairness is apt for our setting, since in contextual learning, the quality of an arm is clear: its expected reward. We view different arms\n1 For example, a 2014 White House report [Podesta et al., 2014] notes that \u201c[t]he increasing use of algorithms to make eligibility decisions must be carefully monitored for potential discriminatory outcomes for disadvantaged groups, even absent discriminatory intent. . . additional research in measuring adverse outcomes due to the use of scores or algorithms is needed to understand the impacts these tools are having and will have in both the private and public sector as their use grows.\u201d Along the same lines, a 2016 White House report [Munoz et al., 2016] observes that \u201c[a]s improvements in the uses of big data and machine learning continue, it will remain important not to place too much reliance on these new systems without questioning and continuously testing the inputs and mechanics behind them and the results they produce.\u201d Similarly, in a recent speech FTC Commissioner Julie Brill [Julie Brill, 2015] observed, \u201c. . . a lot remains unknown about how big data-driven decisions may or may not use factors that are proxies for race, sex, or other traits that U.S. laws generally prohibit from being used in a wide range of commercial decisions . . . What can be done to make sure these products and services\u2013and the companies that use them treat consumers fairly and ethically?\u201d\nj as representing different populations (e.g. different ethnic groups, cultures, or other divisions within society), and view the context xtj at round t as representing information about a particular individual from that population. Each population has its own underlying function fj which maps contexts to expected payoff2. At each time step t, the algorithm is asked to choose between specific members of each population, represented by the contexts xtj . The quality of an individual is thus\nexactly E [ rtj ] = fj(x t j). Our fairness condition translates thus: for any pair of arms j, j \u2032 at time t, if fj(x t j) \u2265 fj\u2032(xtj\u2032), then an algorithm is said to be discriminatory if it preferentially chooses the lower quality arm j\u2032. Said another way, an algorithm is fair if it guarantees the following: with high probability, over all rounds t, and for all pairs of arms j, j\u2032, whenever fj(x t j) \u2265 fj\u2032(xtj\u2032), the algorithm chooses arm j with probability at least that with which it chooses arm j\u20323. It is worth noting that this definition of fairness (formalized in the preliminaries) is entirely consistent with the optimal policy, which can simply choose at each round to play uniformly at\nrandom from the arms arg maxj ( E [ rtj ]) which maximize the expected reward. This is because \u2013 it seems \u2013 the goal of fairness as enunciated above is entirely consistent with the goal of maximizing expected reward. Indeed, the fairness constraint exactly states that the algorithm cannot favor low reward arms!\nOur main conceptual result is that this intuition is incorrect in the face of unknown reward functions. Even though the constraint of fairness is consistent with implementing the optimal policy, it is not necessarily consistent with learning the optimal policy. We show that fairness always has a cost, in terms of the achievable learning rate of the algorithm. For some problems, the cost is mild, but for others, the cost is large."}, {"heading": "1.2 Our Results", "text": "We divide our results into two parts. First, we study the classic stochastic multi-armed bandit problem [Lai and Robbins, 1985, Katehakis and Robbins, 1995]. In this case, there are no contexts, and each arm i has a fixed but unknown average reward \u00b5i. Note that this is a special case of the contextual bandit problem in which the contexts are the same every day. In this setting, our fairness constraint specializes to require that with probability 1 \u2212 \u03b4, for any pair of arms i, j for which \u00b5i \u2265 \u00b5j , at no round t does the algorithm play arm j with probability higher than that with which it plays arm i. Note that even this special case models interesting scenarios from the point of view of fairness in learning. It models, for example, the case in which choices are made by a loan officer after applicants have been categorized into k internally indistinguishable equivalence classes based on their applications.\nWithout a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002]. In Section 3, we give an algorithm that satisfies our fairness constraint and is able to guarantee non-trivial regret after T = O(k3) rounds. We then show in Section 4 that it is not possible to do better \u2013 any\n2It is natural that different populations should have different underlying functions \u2013 for example, in a college admissions setting, the function mapping applications to college success probability might weight SAT scores less in a wealthy population that employs SAT tutors, and more in a working-class population that does not \u2013 see Dwork et al. [2012] for more discussion of this issue and Munoz et al. [2016] for examples.\n3Note that the definition does not require equality of outcomes on a population wide basis, also known as statistical parity. If some population j is less credit-worthy on average than another population j\u2032, we do not necessarily say that an algorithm is discriminatory if it ends up giving fewer loans to individuals from population j. Our notion of discrimination is on an individual basis \u2013 it requires that even if population j is less credit worthy on average than population j\u2032, if it happens that on some day, an individual appears from population j who is at least as credit worthy as the individual from population j\u2032, then the algorithm cannot favor the individual from population j\u2032.\nfair learning algorithm can be forced to endure constant per-round regret for T = \u2126(k3) rounds. Thus, we tightly characterize the optimal regret attainable by fair algorithms in this setting, and formally separate it from the regret attainable by algorithms absent a fairness constraint. Note that this already shows a separation between the best possible learning rates for contextual bandit learning with and without the fairness constraint \u2013 the stochastic multi-armed bandit problem is a special case of every contextual bandit problem, and for general contextual bandit problems, it is also known how to get non-trivial regret after only T = O(k) many rounds [Agarwal et al., 2014, Beygelzimer et al., 2011, Chu et al., 2011].\nWe then move on to the general contextual bandit setting and prove a broad characterization result, relating fair contextual bandit learning to KWIK learning [Li et al., 2011]. The KWIK model, which stands for Knows What it Knows and has a close relationship with reinforcement learning, is a model of sequential supervised classification in which the learning algorithm must be confident in its predictions. Informally, a KWIK learning algorithm receives a sequence of unlabeled examples, whose true labels are defined by some unknown function in a class C. For each example, the algorithm may either predict a label, or announce \u201cI Don\u2019t Know\u201d. The KWIK requirement is that with high probability, for each example, if the algorithm predicts a label, then its prediction must be very close to the true label. The quality of a KWIK learning algorithm is characterized by its \u201cKWIK bound\u201d, which provides an upper bound on the maximum number of times the algorithm can be forced to announce \u201cI Don\u2019t Know\u201d. For any contextual bandit problem (defined by the set of functions C from which the payoff functions fj may be selected), we show that the optimal learning rate of any fair algorithm is determined by the best KWIK bound for the class C. We prove this constructively \u2013 we give a reduction showing how to convert a KWIK learning algorithm into a fair contextual bandit algorithm in Section 5, and vice versa in Section 6. Both reductions show that the KWIK bound of the KWIK algorithm is polynomially related to the regret of the fair algorithm.\nThis general connection has immediate implications, because it allows us to import known results for KWIK learning [Li et al., 2011]. For example, it implies that some fair contextual bandit problems are easy, in that there are fair algorithms which can obtain non-trivial regret guarantees after polynomially many rounds. This is the case, for example, for the important linear special case in which the contexts xtj \u2208 Rd are real valued vectors and the unknown functions fj are linear: fj(x t j) = \u3008\u03b8j , xtj\u30094. In this case, the KWIK-learnability of noisy linear regression problems [Strehl and Littman, 2008, Li et al., 2011] implies that we can construct a fair contextual bandit algorithm whose per-round regret is polynomial in d. Conversely, it also implies that some contextual bandit problems which are easy without the fairness constraint become hard once we impose the fairness constraint, in that any fair algorithm must suffer constant per-round regret for exponentially many rounds. This is the case, for example, when the context consists of boolean vectors xtj \u2208 {0, 1}d, and the unknown functions fj : {0, 1}d \u2192 {0, 1} are conjunctions \u2013 the \u201cand\u201ds of some unknown set of features5. The impossibility of non-trivial KWIK-learning of conjunctions [Li, 2009, Li et al., 2011] implies that no fair learner in the contextual bandit setting can achieve non-trivial regret before exponentially many (in d) rounds.\n4This corresponds to the case in which the probability that an individual pays back his or her loan is determined by a standard linear regression model.\n5For example, a conjunction might predict that an individual is likely to pay back his loan if all of the following conditions are satisfied: he or she has graduated from college, has a clean driving history, and has not previously defaulted on any loans."}, {"heading": "1.3 Other Related Work", "text": "Several papers study the problem of fairness in machine learning. One line of work aims to give algorithms for batch classification which achieve group fairness otherwise known as equality of outcomes, statistical parity \u2013 or algorithms that avoid disparate impact (see e.g. Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al. [2015], Fish et al. [2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level.\nOur definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a \u201cfair\u201d metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes. Our fairness constraint, conversely, is entirely aligned with the goal of the algorithm designer in that it is satisfied by the optimal policy; nevertheless, it affects the space of feasible learning algorithms, because it interferes with learning an optimal policy, which depends on the unknown reward functions.\nAt a technical level, our work is related to Amin et al. [2012] and Amin et al. [2013], which also relate KWIK learning to bandit learning in a different context, unrelated to fairness (when the arm space is very large)."}, {"heading": "2 Preliminaries", "text": "We study the stochastic contextual bandit setting, which is defined by a domain X , a set of \u201carms\u201d [k] := {1, . . . , k} and a class C of functions of the form f : X \u2192 [0, 1]. For each arm j there is some function fj \u2208 C, unknown to the learner. In rounds t = 1, . . . , T , an adversary reveals to the algorithm a context xtj for each arm\n6. An algorithm A then chooses an arm it, and observes reward rtit for the arm it chose. We assume r t j \u223c Dtj , E [ rtj ] = fj(x t j), for some distribution Dtj over [0, 1].\nLet \u03a0 be the set of policies mapping contexts to distributions over arms Xk \u2192 \u2206k, and \u03c0\u2217 the optimal policy which selects a distribution over arms as a function of contexts to maximize the expected reward of those arms. The pseudo-regret of an algorithm A on contexts x1, . . . , xT is\n6Often, the contextual bandit problem is defined such that there is a single context xt every day. Our model is equivalent \u2013 we could take xtj := x t for each j.\ndefined as follows, where \u03c0t represents A\u2019s distribution on arms at round t:\n\u2211 t Eit\u2217\u223c\u03c0\u2217(xt) [ fit\u2217(x t it\u2217 ) ] \u2212 Eit\u223c\u03c0t [\u2211 t fit(x t it) ] = Regret(x1, . . . , xT ).\nWe hereafter refer to this as the regret of A. The optimal policy \u03c0\u2217 pulls arms with highest expectation at each round, so:\nRegret(x1, . . . , xT ) = \u2211 t max j ( fj(x t j) ) \u2212 Eit\u223c\u03c0t [\u2211 t fit(x t it) ] .\nWe say that A satisfies regret bound R(T ) if maxx1,...,xT Regret(x1, . . . , xt) \u2264 R(T ). Let the history ht \u2208 ( X k \u00d7 [k]\u00d7 [0, 1] )t\u22121 be a record of t \u2212 1 rounds experienced by A, t \u2212 1 3-tuples which encode for each t the realization of the contexts, arm chosen, and reward observed. We write \u03c0tj|ht to denote the probability that A chooses arm j after observing contexts x t, given ht. For notational simplicity, we will often drop the superscript t on the history when referring to the distribution over arms: \u03c0tj|h := \u03c0 t j|ht .\nWe now define what it means for a contextual bandit algorithm to be \u03b4-fair with respect to its arms. Informally, this will mean that A will play arm i with higher probability than arm j in round t only if i has higher mean than j in round t, for all i, j \u2208 [k], and in all rounds t.\nDefinition 1 (\u03b4-fair). A is \u03b4-fair if, for all sequences of contexts x1, . . . , xt and all payoff distributions Dt1, . . . ,Dtk, with probability at least 1\u2212 \u03b4 over the realization of the history h, for all rounds t \u2208 [T ] and all pairs of arms j, j\u2032 \u2208 [k],\n\u03c0tj|h > \u03c0 t j\u2032|h only if fj(x t j) > fj\u2032(x t j\u2032).\nRemark 1. Definition 1 prohibits favoring lower payoff arms over higher payoff arms. One relaxed definition only requires that \u03c0tj|h = \u03c0 t j\u2032|h when fj(x t j) = fj\u2032(x t j\u2032) \u2013 requiring only identical individuals (concerning expected payoff) be treated identically. This relaxation is a special case of Dwork et al. [2012]\u2019s proposed family of definitions, which require that \u201csimilar individuals be treated similarly\u201d. We use Definition 1 as it is better motivated in its implications for fair treatment of individuals, but all of our results \u2013 including our lower bounds \u2013 apply also to this relaxation.\nKWIK learning Let B be an algorithm which takes as input a sequence of examples x1, . . . , xT , and when given some xt \u2208 X , outputs either a prediction y\u0302t \u2208 [0, 1] or else outputs y\u0302t = \u22a5, representing \u201cI don\u2019t know\u201d. When y\u0302t = \u22a5, B receives feedback yt such that E [ yt ]\n= f(xt). B is an ( , \u03b4)-KWIK learning algorithm for C : X \u2192 [0, 1], with KWIK bound m( , \u03b4) if for any sequence of examples x1, x2, . . . and any target f \u2208 C, with probability at least 1\u2212 \u03b4, both:\n1. Its numerical predictions are accurate: for all t, y\u0302t \u2208 {\u22a5} \u222a [f(xt)\u2212 , f(xt) + ], and\n2. B rarely outputs \u201cI Don\u2019t Know\u201d: \u2211\u221e t=1 I [ y\u0302t = \u22a5 ] \u2264 m( , \u03b4)."}, {"heading": "2.1 Specializing to Classic Stochastic Bandits", "text": "In Sections 3 and 4, we study the classic stochastic bandit problem, an important special case of the contextual bandit setting described above. Here we specialize our notation to this setting, in which there are no contexts. For each arm j \u2208 [k], there is an unknown distribution Dj over [0, 1]\nwith unknown mean \u00b5j . A learning algorithm A chooses an arm it in round t, and observes the reward rtit \u223c Dit for the arm that it chose. Let i\n\u2217 \u2208 [k] be the arm with highest expected reward: i\u2217 \u2208 arg maxi\u2208[k] \u00b5i. The pseudo-regret of an algorithm A on D1, . . . ,Dk is now just:\nT \u00b7 \u00b5i\u2217 \u2212 Eit\u223c\u03c0t  \u2211 0\u2264t\u2264T \u00b5it  = Regret(T,D1, . . . ,Dk) Let ht \u2208 ([k]\u00d7 [0, 1])t\u22121 denote a record of the t \u2212 1 rounds experienced by the algorithm so far, represented by t \u2212 1 2-tuples encoding the previous arms chosen and rewards observed. We write \u03c0tj|ht to denote the probability that A chooses arm j given history h t. Again, we will often drop the superscript t on the history when referring to the distribution over arms: \u03c0tj|h := \u03c0 t j|ht .\n\u03b4-fairness in the classic bandit setting specializes as follows:\nDefinition 2 (\u03b4-fairness in the classic bandits setting). A is \u03b4-fair if, for all distributionsD1, . . . ,Dk, with probability at least 1\u2212 \u03b4 over the history h, for all t \u2208 [T ] and all j, j\u2032 \u2208 [k]:\n\u03c0tj|h > \u03c0 t j\u2032|h only if \u00b5j > \u00b5j\u2032 ."}, {"heading": "3 Fair Classic Stochastic Bandits: An Algorithm", "text": "In this section, we describe a simple and intuitive modification of the standard UCB algorithm [Auer et al., 2002], called FairBandits, prove that it is fair, and analyze its regret bound. The algorithm and its analysis highlight a key idea that is important to the design of fair algorithms in this setting: that of chaining confidence intervals. Intuitively, as a \u03b4-fair algorithm explores different arms it must play two arms j1 and j2 with equal probability until it has sufficient data to deduce, with confidence 1 \u2212 \u03b4, either that \u00b5j1 > \u00b5j2 or vice versa. FairBandits does this by maintaining empirical estimates of the means of both arms, together with confidence intervals around those means. To be safe, the algorithm must play the arms with equal probability while their confidence intervals overlap. The same reasoning applies simultaneously to every pair of arms. Thus, if the confidence intervals of each pair of arms ji and ji+1 overlap for each i \u2208 [k], the algorithm is forced to play all arms j with equal probability. This is the case even if the confidence intervals around arm jk and arm j1 are far from overlapping \u2013 i.e. when the algorithm can be confident that \u00b5j1 > \u00b5jk .\nThis approach initially seems naive: in an attempt to achieve fairness, it seems overly conservative when ruling out arms, and can be forced to play arms uniformly at random for long periods of time. This is reflected in its regret bound, which is only non-trivial after T k3, whereas the UCB algorithm [Auer et al., 2002] achieves non-trivial regret after T = O(k) rounds. However, our lower bound in Section 4 shows that any fair algorithm must suffer constant per-round regret for T k3 rounds on some instances.\nWe now give an overview of the behavior of FairBandits. At every round t, FairBandits identifies the arm it\u2217 = arg maxi u t i that has the largest upper confidence interval amongst the active arms. At each round t, we say i is linked to j if [`ti, u t i] \u2229 [`tj , utj ] 6= \u2205, and i is chained to j if i and j are in the same component of the transitive closure of the linked relation. FairBandits plays uniformly at random among all active arms chained to arm it\u2217.\nInitially, the active set contains all arms. The active set of arms at each subsequent round is defined to be the set of arms that are chained to the arm with highest upper confidence bound at the previous round. The algorithm can be confident that arms that have become unchained to the\narm with the highest upper confidence bound at any round have means that are lower than the means of any chained arms, and hence such arms can be safely removed from the active set, never to be played again. This has the useful property that the active set of arms can only shrink: at any round t, St \u2286 St\u22121; see Figure 1 for an example of active set evolution over time.\n1: procedure FairBandits(\u03b4) 2: S0 \u2190 {1, . . . , k} . Initialize the active set 3: for i = 1, . . . k do 4: \u00b5\u03020i \u2190 12 , u 0 i \u2190 1, `0i \u2190 0, n0i \u2190 0 . Initialize each arm 5: for t = 1 to T do 6: it\u2217 \u2190 arg maxi\u2208St uti . Find arm with highest ucb 7: St \u2190 {j | j chains to it\u2217, j \u2208 St\u22121} . Update active set 8: j\u2217 \u2190 (x \u2208R St) . Select active arm at random 9: nt+1j\u2217 \u2190 ntj\u2217 + 1\n10: \u00b5\u0302t+1j\u2217 \u2190 1\nnt+1 j\u2217\n(\u00b5\u0302tj\u2217 \u00b7 ntj\u2217 + rtj\u2217) . Pull arm j\u2217, update its mean estimate 11: B \u2190 \u221a ln((\u03c0[t+1])2/3\u03b4)\n2nt+1 j\u2217\n12:\n[ `t+1j\u2217 , u t+1 j\u2217 ] \u2190 [ \u00b5\u0302t+1j\u2217 \u2212B, \u00b5\u0302 t+1 j\u2217 +B ] . Update interval for pulled arm\n13: for j \u2208 St, j 6= j\u2217 do 14: \u00b5\u0302t+1j \u2190 \u00b5\u0302tj , n t+1 j \u2190 ntj , u t+1 j \u2190 utj , ` t+1 j \u2190 `tj\nWe first observe that with probability 1 \u2212 \u03b4, all of the confidence intervals maintained by FairBandits (\u03b4) contain the true means of their respective arms over all rounds. We prove this claim, along with all other claims in this section without proofs, in Appendix A.\nLemma 1. With probability at least 1\u2212 \u03b4, for every arm i and round t `ti \u2264 \u00b5i \u2264 uti.\nThe fairness of FairBandits follows almost immediately from this guarantee.\nTheorem 1. FairBandits (\u03b4) is \u03b4-fair.\nProof. By Lemma 1, with probability at least 1\u2212\u03b4 all confidence intervals contain their true means across all rounds. Thus, with probability 1\u2212\u03b4, at every round t, for every i \u2208 St, j /\u2208 St, it must be that \u00b5j < \u00b5i \u2013 the arms not in the active set have strictly smaller means than those in the active set; if not, utj \u2265 \u00b5j \u2265 \u00b5i \u2265 `ti implies j would be chained to it\u2217 if i is. Finally, all arms in St are played uniformly at random \u2013 but since all such arms are played with the same probability, this does not cause the fairness constraint to bind for any pair i, i\u2032 \u2208 St, for any realization of \u00b5i, \u00b5\u2032i which lie within their confidence intervals.\nNext, we upper bound the regret of FairBandits.\nTheorem 2. If \u03b4 < 1/ \u221a T , then FairBandits has regret\nR(T ) = O (\u221a k3T ln Tk\n\u03b4\n) .\nRemark 2. Before proving Theorem 2, we highlight two points. First, this bound becomes nontrivial (i.e. the average per-round regret is 1) for T = \u2126(k3). As we show in the next section, it is not possible to improve on this. Second, the bound may appear to have suboptimal dependence on T when compared to unconstrained regret bounds (where the dependence on T is often described\nas logarithmic). However, it is known that \u2126 (\u221a kT ) regret is necessary even in the unrestricted\nsetting (without fairness) if one does not make data-specific assumptions on an instance [Bubeck and Cesa-Bianchi, 2012] (e.g. that there is a lower bound on the gap between the best and second best arm). It would be possible to state a logarithmic dependence on T in our setting as well while making assumptions on the gaps between arms, but since our fairness constraint manifests itself as a cost that depends on k, we choose for clarity to avoid such assumptions. Without such assumptions, our dependence on T is also optimal.\nWe now prove Theorem 2. Lemma 2 upper bounds the probability any arm i active in round t has been pulled substantially fewer times than its expectation, i.e. nti tk . Lemma 3 upper bounds the width of any confidence interval used by FairBandits in round t by \u03b7(t), conditioned on i being pulled the number of times guaranteed by Lemma 2. Finally, we stitch this together to prove Theorem 2 by upper bounding the total regret incurred for T rounds by noticing that the regret of any arm active in round t is at most k\u03b7(t).\nWe begin by lower bounding the probability that any arm active in round t has been pulled substantially fewer times than its expectation.\nLemma 2. With probability at least 1\u2212 \u03b4 2t2 ,\nnti \u2265 t\nk \u2212\n\u221a t\n2 ln ( 2k \u00b7 t2 \u03b4 ) for all i \u2208 St (for all active arms in round t).\nWe now use this lower bound on the number of pulls of active arm i in round t to upper-bound \u03b7(t), an upper bound on the confidence interval width FairBandits uses for any active arm i in round t.\nLemma 3. Consider any round t and any arm i \u2208 St. Condition on nti \u2265 tk \u2212\n\u221a t ln( 2kt 2\n\u03b4 )\n2 . Then,\nuti \u2212 `ti \u2264 2 \u221a\u221a\u221a\u221a\u221a\u221a ln ( (\u03c0 \u00b7 t)2 /3\u03b4 )\n2 \u00b7 tk \u2212\n\u221a t ln( 2kt 2\n\u03b4 )\n2\n= \u03b7(t).\nFinally, we prove the bound on the total regret of the algorithm, using the bound on the width of any active arm\u2019s confidence interval in round t provided by Lemma 3.\nProof of Theorem 2. We condition on \u00b5i \u2208 [`ti, uti] for all i, t. This occurs with probability at least 1\u2212 \u03b4, by Lemma 1. We claim that this implies that arm i\u2217 with highest expected reward is always in the active set. This follows from the fact that \u00b5i\u2217 \u2208 [`ti\u2217 , u t i\u2217 ] and \u00b5j \u2208 [` t j , u t j ] for all j, t; thus, if \u00b5i\u2217 > \u00b5j , it must be that u t i\u2217 \u2265 ` t j . Thus, this holds for i t \u2217, the arm with highest upper confidence bound in round t, so i\u2217 must be chained to i t \u2217 in round t for all t.\nWe further condition on the event that for all j, t,\nntj \u2265 t\nk \u2212\n\u221a t\n2 ln\n( 2kt2\n\u03b4\n) ,\nwhich holds with probability at least 1\u2212 \u03c0\u03b42 by Lemma 2 and a union bound over all times t. This implies that, for all rounds t, for every active arm j \u2208 St, Lemma 3 applies, and therefore\nutj \u2212 `tj \u2264 \u03b7(t).\nFinally, we upper-bound the per-round regret of pulling any active arm i \u2208 St at round t. Since i\u2217 is active, any i \u2208 St is chained to arm i\u2217. Since all active arms have confidence interval width at most \u03b7(t) and i must be chained using at most k arms\u2019 confidence intervals, we have that\n`ti \u2265 uti\u2217 \u2212 k \u00b7 \u03b7(t).\nSince \u00b5i \u2265 `ti and uti\u2217 \u2265 \u00b5i\u2217 , it follows that |\u00b5i \u2212 \u00b5i\u2217 | \u2264 k \u00b7 \u03b7(t) for any i \u2208 S t. Finally, summing up over all rounds t \u2208 T , we know that\nR(T ) \u2264 T\u2211 t:0 min(1, k \u00b7 \u03b7(t)) + ( 1 + \u03c0 2 ) \u03b4T\n\u2264 k  T\u2211 t: t k >2 \u221a t ln 2tk \u03b4 \u221a ln t\u03b4 t 2k + T\u2211 t: t k \u22642 \u221a t ln 2tk \u03b4 1 + (1 + \u03c02) \u03b4T = O\u0303(k 32 \u221a T ln kT \u03b4 + k3)\nwhere this bound is derived in Appendix A.1."}, {"heading": "4 Fair Classic Stochastic Bandits: A Lower Bound", "text": "We now show that the regret bound for FairBandits has an optimal dependence on k: no fair algorithm has diminishing regret before T = \u2126(k3) rounds. All missing proofs are in Appendix B. The main result of this section is the following.\nTheorem 3. There is a distribution P over k-arm instances of the stochastic multi-armed bandit problem such that any fair algorithm run on P experiences constant per-round regret for at least\nT = \u2126 ( k3 ln 1\n\u03b4 ) rounds.\nDespite the fact that regret is defined in a prior-free way, the proof of Theorem 3 proceeds via Bayesian reasoning. We construct a family of lower bound instances such that arms have payoffs drawn from Bernoulli distributions, denoted B(\u00b5) for mean \u00b5. So, to specify a problem instance, it suffices to specify a mean for each of k arms: \u00b51, . . . , \u00b5k. The proof formalizes the following outline.\n1. We define an instance distribution P = P1\u00d7 . . .\u00d7Pk over means \u00b5i (Definition 3). P will have two important properties. First, we will draw means from P such that for any i \u2208 [k \u2212 1], \u00b5i = \u00b5i+1 with probability at least 1/4. Second, for any realization of means drawn from P , if an algorithm plays uniformly at random over [k], it will suffer constant per-round regret.\n2. We treat Pi as a prior distribution over mean \u00b5i, and analyze the posterior distribution Pi(r 1 i , , . . . , r t i) over means that results after applying Bayes\u2019 rule to the payoff observations\nr1i , . . . , r t i made by the algorithm. Bayes\u2019 rule implies (Lemma 4) the joint distribution over rewards and means drawn from P is identical to the distribution which first draws means according to P , then draws rewards conditioned on those means, and finally resamples the means from the posterior distribution on means. Thus, we can reason about fairness (a frequentist quantity) by analyzing the Bayesian posterior distribution on means conditioned on the observed rewards.\n3. A \u03b4-fair algorithm, for any set of means realized from the instance (prior) distribution, must not play arm i + 1 with lower probability than arm i if \u00b5i = \u00b5i+1, except with probability \u03b4. By the above change of perspective, therefore, any \u03b4-fair algorithm must play arms i and i+1 with equal probability until the posterior distribution on means given observed rewards, satisfies P [\u00b5i = \u00b5i+1|h] < \u03b4 (Lemmas 5 and 6).\n4. We finally lower bound the number of reward observations necessary before the posterior distribution on means given payoffs is such that P [\u00b5i = \u00b5i+1|h] < \u03b4 for any pair of adjacent arms i, i + 1. We show that this is \u2126(k2) (Lemma 7). Since fair algorithms must play from among the k arms uniformly at random until this point, with high probability, no arm accumulates sufficiently many reward observations until T = \u2126(k3) rounds of play.\nWe begin by describing our distribution over instances. Each arm i\u2019s payoff distribution will be Bernoulli with mean \u00b5i \u223c Pi independently of each other arm.\nDefinition 3 (Prior Distribution over \u00b5i). For each arm i, \u00b5i is distributed according to the distribution with the following probability mass function:\nPi(x) =\n{ 1 2 if x = 1 3 + i 3k\n1 2 if x = 1 3 + i+1 3k . Let P = \u220f i Pi denote the joint distribution on arms\u2019 expected payoffs.\nWe treat P as a prior distribution over instances, and analyze the posterior distribution on instances given the realized rewards. Lemma 4 justifies this reasoning.\nLemma 4. Consider the following two experiments: In the first, let \u00b5i \u223c Pi and r1i , . . . , rti \u223c B(\u00b5i), and W denote the joint distribution on (\u00b5i, r 1 i , . . . , r t i). In the second, let \u00b5i \u223c Pi, and r1i , . . . , r t i \u223c B(\u00b5i), and then re-draw the mean \u00b5\u2032i \u223c Pi(r1i , . . . , rti) from its posterior distribution given the rewards. Let (\u00b5\u2032i, r 1 i , . . . , r t i) \u223cW \u2032. Then, W and W \u2032 are identical distributions.\nNext, we lower-bound the number of reward observations necessary such that for some i \u2208 [k]: P [ \u00b5i = \u00b5i+1|ht ] < \u03b4 with respect to the posterior. It will be useful to refer to an algorithm\u2019s histories as distinguishing the mean of an arm given that history with high probability.\nDefinition 4 (\u03b4-distinguishing). We will say ht \u03b4-distinguishes arm i for A if, for some \u03b1 \u2208 [0, 1],\nP\u00b5\u2032i\u223cPi(ht) [ \u00b5\u2032i = \u03b1 ] \u2265 1\u2212 \u03b4.\nThe next lemma shows that if no arm is \u221a \u03b4-distinguished by a history, all pairs of arms i, i+ 1\nhave posterior probability strictly greater than \u03b4 of having equal means. Lemma 5. Suppose A has history ht, and that ht does not \u221a \u03b4-distinguish any arm i. Then, for all arms i, i+ 1, P [ \u00b5i = \u00b5i+1|ht ] > \u03b4.\nNow, we prove that for any fair algorithm, with probability \u2265 12 over the draw of histories h t, ht\nmust \u221a\n2\u03b4-distinguish some arm, or the algorithm must play uniformly across all k arms conditioned on ht.\nLemma 6. Suppose an algorithm A is \u03b4-fair. Then:\nPht\u223cA [ ht does not \u221a 2\u03b4-distinguish any i \u2227 \u2203t\u2032 \u2264 t, i \u2208 [k] : \u03c0t\u2032 i\u2032|ht\u2032 6= 1\nk\n] \u2264 1\n2 .\nWe now lower-bound the number of observations from arm i which are required to \u03b4-distinguish it. Lemma 7. Fix any \u03b4 < 18 . Let \u00b5i \u223c Pi as in Definition 3. Then, arm i is \u221a 2\u03b4-distinguishable by ht only if Ti = \u2126(k 2 ln 1\u03b4 ), where Ti = |{t \u2032 : ht \u2032 2 = i, t \u2032 \u2264 t}| is the number of times arm i is played.\nProof. Write p, p + 13k to represent the two possible realizations that \u00b5i might take, when drawn from the distribution over instances given in Definition 3. Let A represent the event that \u00b5i = p and B the event that \u00b5i = p+ 1 3k . Let \u03b4 \u2032 = \u221a\n2\u03b4 throughout. Fix a history ht, and let m = Ti represent the number of observations of arm i\u2019s reward. We will abuse notation and use hti to refer to the payoff sequence of arm i observed in history h t. hti is therefore a binary sequence of length m; let ||hti||0 = s denote the number of 1s in the sequence. We will calculate conditions under which hti,m, s will imply that either 1\u2212\u03b4\u2032 \u03b4\u2032 \u2264\nP[B|hti] P[A|hti] or P[B|hti] P[A|hti] \u2264 \u03b4\u20321\u2212\u03b4\u2032 holds, implying that one of A or B has posterior probability at least 1 \u2212 \u03b4\u2032, conditioned on the observed rewards. If neither of these is the case, i is not \u03b4\u2032-distinguished by ht.\nWe begin by rearranging our definition of this ratio X = P[B|hti] P[A|hti] = P[hti|B] P[hti|A] ,which follows from Bayes\u2019 rule and the fact that P [A] = P [B]. We wish to upper and lower bound X in terms of hti\u2019s value. By definition of the Bernoulli distribution, we have that\nX = P [ hti|B ] P [hti|A] = ( p+ 13k )s ( 1\u2212 p\u2212 13k )m\u2212s (p)s (1\u2212 p)m\u2212s = ( 1 + 1 3kp )s( 1\u2212 1 3k(1\u2212 p) )m\u2212s .\nWe now calculate under what conditions either (a) X \u2264 \u03b4\u20321\u2212\u03b4\u2032 , or (b) X \u2265 1\u2212\u03b4\u2032 \u03b4\u2032 . One of these must hold if i is \u03b4\u2032-distinguished. Before we do so, we mention that a Chernoff bound implies that with probability 1\u2212 \u03b4\u2032, for events A and B, Equations 1 and 2, respectively:\n|s\u2212mp| \u2264 \u221a 2m ln 2\n\u03b4\u2032 (1) |s\u2212mp\u2212 m 3k | \u2264\n\u221a 2m ln 2\n\u03b4\u2032 (2)\nsince the mean of m Bernoulli trials with mean p ( or p+ 13k ) is mp (or mp+ m 3k ).\nWe begin by analyzing case (a), where \u03b4\u2032 = \u221a 2\u03b4 < 1/2 implies\n2\u03b4\u2032 > \u03b4\u2032\n1\u2212 \u03b4\u2032 \u2265 X =\n( 1 + 1\n3kp\n)s( 1\u2212 1\n3k(1\u2212 p)\n)m\u2212s .\nTaking logarithms on both sides, we have that\nln(2\u03b4\u2032) > s ln ( 1 + 1\n3kp\n) + (m\u2212 s) ln ( 1\u2212 1\n3k(1\u2212 p)\n) \u2265 s 1\n3kp+ 1 \u2212 (m\u2212 s) 1 3k(1\u2212 p)\u2212 1\nwhere the inequality follows from ln(1 + x) \u2265 xx+1 for x \u2208 [\u22121,\u221e]. Then, this implies that\n(3kp+ 1)(3k(1\u2212 p)\u2212 1) ln(2\u03b4\u2032) > s(3k(1\u2212 p)\u2212 1)\u2212 (m\u2212 s)(3kp+ 1) = 3ks\u2212 3kpm\u2212m.\nMultiplying both sides by \u22121, this implies that\n(3kp+ 1)(3k(1\u2212 p)\u2212 1) ln 1 2\u03b4\u2032 < m+ 3kpm\u2212 3ks.\nEquation 1 implies |3ks\u2212 3kmp\u2212m| \u2264 m+ 3k \u221a\n2m ln 2\u03b4\u2032 , which with the previous line implies\n(3kp+ 1)(3k(1\u2212 p)\u2212 1) ln 1 2\u03b4\u2032 < m+ 3k\n\u221a 2m ln 2\n\u03b4\u2032 .\nSince p, 1\u2212 p \u2208 [1/3, 2/3] and \u03b4\u2032 = \u221a\n2\u03b4, solving for m implies that m = \u2126(k2 ln 1\u03b4\u2032 ). In case (b), we have\n1\n2\u03b4\u2032 <\n1\u2212 \u03b4\u2032\n\u03b4\u2032 \u2264 X =\n( 1 + 1\n3kp\n)s( 1\u2212 1\n3k(1\u2212 p)\n)m\u2212s \u2264 e s 3kp e \u2212(m\u2212s) 3k(1\u2212p)\nwhere we used the fact that 1 + x \u2264 ex for all x. Taking logarithms, this will imply that\ns 3kp \u2212 m\u2212 s 3k(1\u2212 p) > ln 1 2\u03b4\u2032 \u21d2 s\u2212mp = s(1\u2212 p)\u2212 (m\u2212 s)p > 3kp(1\u2212 p) ln 1 2\u03b4\u2032 \u2265 6k 9 ln 1 2\u03b4\u2032 (3)\nwhose last inequality comes the range of p. Combining this inequality with Equation 2, this implies\u221a 2m ln 1\n2\u03b4\u2032 + m k > 6k 9 ln 1 2\u03b4\u2032\nand solving for m and substituting for \u03b4\u2032 gives that m = \u2126(k2 ln 1\u03b4 ).\nThus, if either X \u2265 1\u2212\u03b4\u2032\u03b4\u2032 or X \u2264 \u03b4\u2032 1\u2212\u03b4\u2032 , it must be that m = \u2126(k 2 ln 1\u03b4 ).\nWe now have the tools in hand to prove Theorem 3.\nProof of Theorem 3. Assume A is some \u03b4-fair algorithm where \u03b4 < 1/8. Fix T ; we claim that with probability at least 12 , for any t = o(k 3 ln 1\u03b4 ), t \u2264 T , that \u03c0 t j|ht = 1 k for all j. Since the payoff for uniformly random play is \u2264 12 + 1 k , while the best arm has payoff \u2265 2 3 , in any round t where \u03c0ti|ht = \u03c0 t i\u2032|ht for all i, i\n\u2032 \u2208 [k], the algorithm suffers \u2126(1) regret in that round. Lemma 6 implies that, with probability at least 12 over the distribution over histories h t, either\n(a) \u03c0t \u2032 i|ht\u2032 = \u03c0 t\u2032 i\u2032|ht\u2032 for all i, i \u2032 \u2208 [k], t\u2032 \u2264 t or (b) ht must\n\u221a 2\u03b4-distinguish some arm i. Case (a) implies\nour claim. In case (b), Lemma 7 states than an arm i is \u221a\n2\u03b4-distinguishable only if Ti = \u2126(k 2 ln 1\u03b4 ).\nWe now argue that unless t = \u2126(k3 ln 1\u03b4 ), Ti = o(k 2 ln 1\u03b4 ), which will imply our claim for case (b).\nFix some i, t. We lower-bound t for which, with probability at least 1\u2212 \u03b4\u2032k over histories h t, it\nwill be the case that nti \u2265 c \u00b7 k2 ln 1\u03b4 when \u03c0 t\u2032 i|ht\u2032 = \u03c0 t\u2032 i\u2032|ht\u2032 for all i, i \u2032 \u2208 [k], t\u2032 \u2264 t. Let X1, . . . , Xt be indicator variables of arm i being played in round t\u2032 \u2264 t. Note that for all t\u2032 \u2264 t, E[Xt\u2032 ] = 1k , since in all rounds prior to t, we have all arms are played with equal probability. For any \u2208 [0, 1], as nt \u2032 i are nondecreasing in t \u2032, an additive Chernoff bound implies\nP [ \u2203t\u2032 \u2264 t : nt\u2032i \u2265 t\nk + t\n] \u2264 P \u2211 t\u2032\u2264t Xt\u2032 > t k + t  \u2264 e\u22122t 2 which, for t = \u221a t ln 2k\n\u03b4\u2032 2 , becomes P [\u2211 t\u2032\u2264tXt\u2032 > t k + t ] \u2264 \u03b4\u2032k . So, using a union bound over all k\narms, with probability 1 \u2212 \u03b4\u2032, for some fixed t and all i, nti \u2264 tk + \u221a t ln 2k \u03b4\u2032\n2 . We condition on the event that nti satisfies this inequality for a fixed t and all i. If n t i \u2265 c \u00b7 k2 ln 1\u03b4 , this implies\nt k +\n\u221a t ln 2k\u03b4\u2032\n2 \u2265 c \u00b7 k2 ln 1 \u03b4 \u21d2 t \u2265 \u2212k\n\u221a t ln 2k\u03b4\u2032\n2 + c \u00b7 k3 ln 1 \u03b4 .\nIf k\n\u221a t ln 2k\n\u03b4\u2032 2 \u2264 c 2 \u00b7 k 3 ln 1\u03b4 , then t \u2265 c 2 \u00b7 k 3 ln 1\u03b4 ; if not, then t \u2265 c2 2 k4 ln2 1 \u03b4\nln 2k \u03b4\u2032\n. Thus, nti < c \u00b7 k2 ln 1\u03b4 with probability 1\u2212 \u03b4\u2032 for all i unless t \u2265 min ( c 2 \u00b7 k 3 ln 1\u03b4 , c2 2 k4 ln2 1 \u03b4\nln 2k \u03b4\u2032\n) = \u2126(k3 ln 1\u03b4 ) for \u03b4 \u2032 \u2208 [12 , 1]."}, {"heading": "5 KWIK Learnability Implies Fair Bandit Learnability", "text": "In this section, we show if a class of functions is KWIK learnable, then there is a fair algorithm for learning the same class of functions in the contextual bandit setting, with a regret bound polynomially related to the function class\u2019 KWIK bound. Intuitively, KWIK-learnability of a class of functions guarantees we can learn the function\u2019s behavior to a high degree of accuracy with a high degree of confidence. As fairness constrains an algorithm most before the algorithm has determined the payoff functions\u2019 behavior accurately, this guarantee enables us to learn fairly without incurring much additional regret. Formally, we prove the following polynomial relationship.\nTheorem 4. For an instance of the contextual multi-armed bandit problem where fj \u2208 C for all j \u2208 [k], if C is ( , \u03b4)-KWIK learnable with bound m( , \u03b4), KWIKToFair (\u03b4, T ) is \u03b4-fair and achieves regret bound:\nR(T ) = O ( max ( k2 \u00b7m ( \u2217, min (\u03b4, 1/T )\nT 2k\n) , k3 ln k\n\u03b4 )) for \u03b4 \u2264 1\u221a\nT where \u2217 = arg min (max( \u00b7 T, k \u00b7m( , min(\u03b4,1/T )kT 2 ))).\nFirst, we construct an algorithm KWIKToFair(\u03b4, T ) that uses the KWIK learning algorithm as a subroutine, and prove that it is \u03b4-fair. A call to KWIKToFair(\u03b4, T ) will initialize a KWIK learner for each arm, and in each of the T rounds will implicitly construct a confidence interval around the prediction of each learner. If a learner makes a numeric valued prediction, we will interpret this as a confidence interval centered at the prediction with width \u2217. If a learner outputs \u22a5, we interpret this as a trivial confidence interval (covering all of [0, 1]). We use the same chaining technique that we use in the classic stochastic setting. In every round t, KWIKToFair (\u03b4, T ) identifies the arm it\u2217 = arg maxi u t i that has the largest upper confidence bound. At each round t, we will say i is linked to j if [`ti, u t i] \u2229 [`tj , utj ] 6= \u2205, and i is chained to j if they are in the same component of the transitive closure of the linked relation. Then, it plays uniformly at random amongst all arms chained to arm it\u2217. Whenever all learners output predictions, they need no feedback. When a learner for j outputs \u22a5, if j is selected then we have feedback rtj to give it; on the other hand, if j isn\u2019t selected, we \u201croll back\u201d the learning algorithm for j to before this round by not updating the algorithm\u2019s state.\n1: procedure KWIKToFair(\u03b4, T ) 2: \u03b4\u2217 \u2190 min(\u03b4, 1 T )\nkT 2 , \u2217 \u2190 arg min (max( \u00b7 T, k \u00b7m( , \u03b4\u2217)))\n3: Initialize KWIK( \u2217, \u03b4\u2217)-learner Li, hi \u2190 [ ] \u2200i \u2208 [k] 4: for 1 \u2264 t \u2264 T do 5: S \u2190 \u2205 . Initialize set of predictions S 6: for i = 1, . . . , k do 7: sti \u2190 Li(xti, hi) 8: S \u2190 S \u222a sti . Store prediction sti 9: if \u22a5\u2208 S then 10: Pull j\u2217 \u2190 (x \u2208R [k]), receive reward rtj\u2217 . Pick arm at random from all arms 11: else 12: it\u2217 \u2190 arg maxi sti 13: St \u2190 {j | (stj \u2212 \u2217, stj + \u2217) chains to (stit\u2217 \u2212 \u2217, stit\u2217 + \u2217)} 14: Pull j\u2217 \u2190 (x \u2208R St), receive reward rtj\u2217 . Pick arm at random from active set sti\u2217 15: hj\u2217 \u2190 hj\u2217 :: (xtj\u2217 , rtj\u2217) . Update the history for Li\nWe begin by bounding the probability of certain failures of KWIKToFair in Lemma 8, proven in Appendix C. This in turn lets us prove the fairness of KWIKToFair in Theorem 5.\nLemma 8. With probability at least 1 \u2212min(\u03b4, 1T ), for all rounds t and all arms j, (a) if s t i \u2208 R then |sti \u2212 fi(xti)| \u2264 \u2217 and (b) \u2211 t I [ sti = \u22a5 ] \u2264 m( \u2217, \u03b4\u2217).\nTheorem 5. KWIKToFair(\u03b4, T ) is \u03b4-fair.\nProof of Theorem 5. We condition on both (a) and (b) holding for all arms i and rounds t from Lemma 8, which occur with probability 1 \u2212 \u03b4 for all arms and all times t. Therefore, we proceed by conditioning on the event that for all arms i and all rounds t, if Li = s t i for s t i 6= \u22a5 then |sti \u2212 fi(xti)| \u2264 \u2217. Having done so, there are two possibilities for each round t. In case 1, for each i we have that Li(x t i) = s t i 6= \u22a5. By the condition above, for any arms i and j, fi(x t i) \u2265 fj(xtj) implies that sti + \u2217 \u2265 stj \u2212 \u2217. Since in this case no learner outputs \u22a5, arm j chains to the top arm only if arm i does. Therefore \u03c0ti|h \u2265 \u03c0 t j|h. In case 2, there exists some i such that Li(x t i) = \u22a5. Then we choose uniformly at random across all arms, so \u03c0ti|h = \u03c0 t j|h for all i and j. Thus, with probability at least 1\u2212\u03b4, for each round t, fi(xti) \u2265 fj(xtj) implies that \u03c0ti|h \u2265 \u03c0 t j|h.\nWe now use the KWIK bounds of the KWIK learners to upper-bound the regret of KWIKToFair(\u03b4, T ).\nLemma 9. KWIKToFair(\u03b4, T ) achieves regret O(max(k2 \u00b7m( \u2217, \u03b4\u2217), k3 ln Tk\u03b4 )).\nProof. We first condition on the event that both (a) and (b) from Lemma 8 hold for all t, i, which holds with probability 1 \u2212 min(\u03b4, 1T ), and bound the regret when they both hold. Choose an arbitrary round t in the execution of KWIKToFair(\u03b4, T ). As above, there are two cases. In the first case, Li(x t i) = s t i 6= \u22a5 for all i and we choose uniformly at random from the arms chained by\n\u2217-intervals to the arm with the highest prediction. Since we have conditioned on the event that all KWIK learners are correct, i\u2217 \u2208 St. Furthermore, for any i, j \u2208 St, we have that |sti \u2212 stj | \u2264 2k \u2217, and in particular that |sti \u2212 sti\u2217 | \u2264 2k \u2217. Thus, the regret is at most 2k \u2217 in such a round. In the second case some arm outputs \u22a5, so we choose randomly from all k arms, and the worst-case regret is 1. Thus, the total regret will be at most 2k \u2217+n+ \u03b4T where n is the number of rounds in which some Li outputs \u22a5.\nWe now upper bound ni, the number of rounds in which some arm outputs \u22a5. Fix some arm i which outputs \u22a5 in ni rounds. Arm i is played and therefore receives feedback every time it outputs \u22a5 with probability at least 1/k. Thus, using a Chernoff bound, with probability 1 \u2212 \u03b4\u2032, arm i receives feedback for ni outputs of \u22a5 in at least nik \u2212 \u221a 2ni ln 2 \u03b4\u2032 rounds. Li has the guarantee that there can be at most m( \u2217, \u03b4\u2217) many such rounds (in which it outputs \u22a5 and receives feedback). Thus,\nm( \u2217, \u03b4\u2217) \u2265 ni k \u2212 \u221a 2ni ln 2 \u03b4\u2032 .\nIf ni \u2265 ck \u00b7m( \u2217, \u03b4\u2217), this implies\nm( \u2217, \u03b4\u2217) \u2265 c \u00b7m( \u2217, \u03b4\u2217)\u2212 \u221a\n2ck \u00b7m( \u2217, \u03b4\u2217) ln 2 \u03b4\u2032 .\nWe now analyze cases in which (1) k ln 2\u03b4\u2032 \u2264 m( \u2217, \u03b4\u2217) and (2) k ln 2\u03b4\u2032 > m( \u2217, \u03b4\u2217). Case (1) this implies\nm( \u2217, \u03b4\u2217) \u2265 c \u00b7m( \u2217, \u03b4\u2217)\u2212 \u221a 2c \u00b7m( \u2217, \u03b4\u2217).\nFor c > 4, this leads to contradiction. Thus, in this case, if we set \u03b4\u2032 = \u03b4k , we know that with probability 1 \u2212 \u03b4, ni \u2264 4k \u00b7m( \u2217, \u03b4\u2217) which summing up over all i implies \u2211 i ni \u2264 4k2 \u00b7m( \u2217, \u03b4\u2217), as desired. In case (2), we have that\nln 2 \u03b4\u2032 \u2265 ni k \u2212 \u221a 2ni ln 2 \u03b4\u2032\nwhich solving for ni implies that ni = O(k 2 ln 1\u03b4\u2032 ), so \u2211 i ni = O(k 3 ln k\u03b4 ) by setting \u03b4 \u2032 = \u03b4k and taking a union bound. Thus, there are at most n = O(max(k2 \u00b7m( , \u03b4\u2217), k3 ln k\u03b4 )) rounds in expectation during the execution of KWIKToFair(\u03b4, T ) in which some arm outputs \u22a5.\nCombining both cases, the total regret incurred by KWIKToFair(\u03b4, T ) across all T rounds is\nR(T ) = 4k2 \u00b7m( \u2217, \u03b4\u2217) + k3 ln k \u03b4 ) + T \u00b7 2k \u2217 + T \u00b7min(\u03b4, 1 T ) = O(max(k2 \u00b7m( \u2217, \u03b4\u2217), k3 ln k \u03b4 )).\nOur presentation of KWIKToFair(\u03b4, T ) has a known time horizon T . Its guarantees extend to the case in which T is unknown via the standard \u201cdoubling trick\u201d to prove Theorem 4 in Appendix C.\nAn important instance of the contextual bandit problem is the linear case, where C consists of the set of all linear functions of bounded norm in d dimensions. This captures the natural setting in which the rewards of each arm are governed by an underlying linear regression model on a d-dimensional real valued feature space. The linear case is well studied, and there are known KWIK algorithms [Strehl and Littman, 2008] for the set of linear functions C, which allows us via our reduction to give a fair contextual bandit algorithm for this setting with a polynomial regret bound.\nLemma 10 ([Strehl and Littman, 2008]). Let C = {f\u03b8|f\u03b8(x) = \u3008\u03b8, x\u3009, \u03b8 \u2208 Rd, ||\u03b8|| \u2264 1} and X = {x \u2208 Rd : ||x|| \u2264 1}. C is KWIK learnable with KWIK bound m( , \u03b4) = O\u0303(d3/ 4).\nThen, an application of Theorem 4 implies that KWIKToFair has a polynomial regret guarantee for the class of linear functions. This proof can be found in Appendix C.\nCorollary 1. Let C and X be as in Lemma 10, and fj \u2208 C for each j \u2208 [k]. Then, KWIKToFair(T, \u03b4) using the learner from [Strehl and Littman, 2008] has regret:\nR(T ) = O\u0303 ( max ( T 4/5k6/5d3/5, k3 ln k\n\u03b4\n)) ."}, {"heading": "6 Fair Bandit Learnability Implies KWIK Learnability", "text": "In this section, we show how to use a fair, no-regret contextual bandit algorithm to construct a KWIK learning algorithm whose KWIK bound has logarithmic dependence on the number of rounds T . Intuitively, any fair algorithm which achieves low regret must both be able to find and exploit an optimal arm (since the algorithm is no-regret) and can only exploit that arm once it has a tight understanding of the qualities of all arms (since the algorithm is fair). Thus, any fair no-regret algorithm will ultimately have tight (1\u2212 \u03b4)-confidence about each arm\u2019s reward function.\nTheorem 6. Suppose A is a \u03b4-fair algorithm for the contextual bandit problem over the class of functions C, with regret bound R(T, \u03b4). Suppose also there exists f \u2208 C, x(`) \u2208 X such that for every ` \u2208 [d1 e], f(x(`)) = ` \u00b7 . Then, FairToKWIK is an ( , \u03b4)-KWIK algorithm for C with KWIK bound m( , \u03b4), with m( , \u03b4) the solution to m( ,\u03b4) 4 = R(m( , \u03b4), \u03b4 2T ).\nRemark 3. The condition that C should contain a function that can take on values that are multiples of is for technical convenience; C can always be augmented by adding a single such function.\nOur aim is to construct a KWIK algorithm B to predict labels for a sequence of examples labeled with some unknown function f\u2217 \u2208 C. To do this, we will run our fair contextual bandit algorithm A on an instance that we construct online as examples xt arrive for B. The idea is to simulate a two arm instance, in which one arm\u2019s rewards are governed by f\u2217 (the function to be KWIK learned), and the other arm\u2019s rewards are governed by a function f that we can set to take any value in {0, , 2 , . . . , 1}. For each input xt, we perform a thought experiment and consider A\u2019s probability distribution over arms when facing a context which forces arm 2\u2019s payoff to take each of the values 0, \u2217, 2 \u2217, . . . , 1. Since A is fair, A will play arm 1 with weakly higher probability than arm 2 for those ` : ` \u2217 \u2264 f(xt); analogously, A will play arm 1 with weakly lower probability than arm 2 for those ` : ` \u2217 \u2265 f(xt). If there are at least 2 values of ` for which arm 1 and arm 2 are\nplayed with equal probability, one of those contexts will force A to suffer \u2217 regret, so we continue the simulation of A on one of those instances selected at random, forcing at least \u2217/2 regret in expectation, and at the same time have B return \u22a5. B receives f\u2217(xt) on such a round, which is used to construct feedback for A. Otherwise, A must transition from playing arm 1 with strictly higher probability to playing 2 with strictly higher probability as ` increases: the point at which that occurs will \u201csandwich\u201d the value of f(xt), since A\u2019s fairness implies this transition must occur when the expected payoff of arm 2 exceeds that of arm 1. B uses this value to output a numeric prediction.\nAn important fact we exploit is that we can query A\u2019s behavior on (xt, x(`)), for any xt and ` \u2208 [ d 1 \u2217 e ] without providing it feedback (and instead \u201croll back\u201d its history to ht not including the query (xt, x(`))). We update A\u2019s history by providing it feedback only in rounds where B outputs \u22a5.\n1: procedure FairToKWIK( , \u03b4, T ) 2: \u2217 \u2190 2 , \u03b4 \u2217 \u2190 \u03b4\u00b7 \u2217T , h\u2190 [ ], initialize fair A(\u03b4 \u2217, T ) for class C 3: for 1 \u2264 t \u2264 T do 4: ht \u2190 h 5: for ` \u2208 [ d 1 \u2217 e ] do 6: Let (pt,`1 , p t,` 2 ) = (\u03c01|ht , \u03c02|ht)) be A(h t, (xt, x(`)))\u2019s dist. over arms given h\n7: if \u2203` 6= `\u2032 : `, `\u2032 \u2208 [ d 1 \u2217 e ] : pt,`1 = p t,` 2 = p t,`\u2032 1 = p t,`\u2032\n2 then 8:\n9: Choose x(\u02c6\u0300) \u2208R {x(`), x(`\u2032)} . One of x(`), x(`\u2032) must cause \u2217 regret 10: Select at \u223c A(ht, xt, x) . Run A to get a predicted arm 11: Predict y\u0302t \u2190 \u22a5 12: if at = 1 then 13: rt1 \u2190 yt and h\u2190 ht :: ((xt, x(\u02c6\u0300)), 1, rt1) . Use KWIK feedback 14: else 15: rt2 \u2190 \u02c6\u0300 \u2217 16: h\u2190 ht :: ((xt, x(\u02c6\u0300)), 2, rt2) . Construct feedback for arm 2 17: else . A\u2019s history is not updated 18: if pt,`1 \u2264 p t,` 2 for all ` \u2208 [ d 1 \u2217 e ] then 19: \u02c6\u0300\u2190 0 20: else Let \u02c6\u0300 be the largest index for which pt, \u02c6\u0300\n1 > p t,\u02c6\u0300 2\n21: Predict y\u0302t \u2190 \u02c6\u0300\u00b7 \u2217 Proof. For a fixed run of A, we calculate the probability that for all times t and ` \u2208 [ d 1 \u2217 e ] , it is the case that pt,`1 > p t,` 2 only if f \u2217(xt) > ` \u00b7 \u2217 and also pt,`1 < p t,` 2 only if f\n\u2217(xt) < ` \u00b7 \u2217. In this run, A is queried on T \u2217 histories and contexts: prefixes of h along with (x\nt, x(`)) for each t \u2208 [T ], ` \u2208 [ d 1 \u2217 e ] . The fairness of A implies for any fixed ht and fixed (xt, x(`)), with probability 1\u2212 \u03b4\u2217, pt,`1 > p t,` 2 only if f \u2217(xt) > ` \u2217 and pt,`1 < p t,` 2 only if f\n\u2217(xt) < ` \u2217. Then, by a union bound over t \u2208 [T ], ` \u2208 [ d 1 \u2217 e ] , with probability at least 1 \u2212 \u03b4\u2217 T \u2217 = 1 \u2212 \u03b4, A(h\nt, xt, x(`)) will satisfy this property for all t \u2208 [T ], ` \u2208 [ d 1 \u2217 e ] . We condition on this holding in the remainder of the proof.\nWe now argue that the numeric predictions of B are correct within an additive . Let:\nEt = {` : pt,`1 = p t,` 2 }.\nWhen B(xt) = yt \u2208 [0, 1], note that |Et| \u2264 1, else B would have output \u22a5.\nIf pt,`1 \u2264 p t,` 2 for all `, since |Et| \u2264 1, either p t,0 1 < p t,0 2 or p t,1 1 < p t,1 2 , which we have conditioned on implying that either f\u2217(xt) < f(x(0)) = 0 or f\u2217(xt) < f(x(1)) = \u2217. Since f\u2217(xt) \u2265 0, this implies f\u2217(xt) \u2208 [0, \u2217) = [\u02c6\u0300 \u2217, \u2217) = [y\u0302t, y\u0302t + \u2217).\nOtherwise, we have that pt, \u02c6\u0300 1 > p t,\u02c6\u0300 2 , and p t,` 1 \u2264 p t,` 2 for all ` > \u02c6\u0300. If (a) \u02c6\u0300= d 1 \u2217 e, then f \u2217(xt) > 1,\na contradiction, so \u02c6\u0300 < d 1 \u2217 e. If (b) \u02c6\u0300 = d 1 \u2217 e \u2212 1, then f \u2217(xt) > f(x(\u02c6\u0300)) = (d 1 \u2217 e \u2212 1) \u2217 and so f\u2217(xt) \u2208 ((d 1 \u2217 e \u2212 1) \u2217, 1] = (y\u0302t, y\u0302t + \u2217], so y\u0302t is \u2217-accurate. If neither (a) nor (b), then (c) it must be \u02c6\u0300< d 1 \u2217 e \u2212 1. Since |E t| \u2264 1, for some ` \u2208 {\u02c6\u0300+ 1, \u02c6\u0300+ 2}, we know that pt,`1 < p t,` 2 ; thus, f\u2217(xt) < f(x(`)) \u2264 (\u02c6\u0300+ 2) \u2217 and therefore f\u2217(xt) \u2208 (\u02c6\u0300 \u2217, (\u02c6\u0300+ 2) \u2217) = (y\u0302t, y\u0302t + 2 \u2217). Finally, we upper-bound m( , \u03b4), the number of rounds t : B(xt) = \u22a5. For each such t, A runs on a random draw of one of two contexts, one of whose arms\u2019 payoffs differ by at least \u2217. Thus, for one of those contexts, either f\u2217(xt) \u2265 f(x)\u2212 \u2217 or f\u2217(xt) \u2264 f(x)\u2212 \u2217. In either case, since pt,`1 = p t,` 2 = 1 2 for x(`) = x, A suffers expected regret at least \u22172 for that context, and at least \u2217 4 when faced with one chosen at random. Thus, m( , \u03b4) \u00b7 \u22174 = m( , \u03b4) \u00b7 8 < R(m( , \u03b4), \u03b4\n\u2217) = R(m( , \u03b4), \u03b42T ), since A\u2019s regret is upper bounded by this quantity over m( , \u03b4) rounds (which is an upper bound on the number of rounds for which A is actually run and updated)."}, {"heading": "6.1 An Exponential Separation Between Fair and Unfair Learning", "text": "In this section, we exploit the other direction of the equivalence we have proven between fair contextual bandit algorithms and KWIK learning algorithms to give a simple contextual bandit problem for which fairness imposes an exponential cost in its regret bound. This is in contrast to the case in which the underlying class of functions is linear, for which we gave fair contextual bandit algorithms with regret bounds within a polynomial factor of their unconstrained counterparts. In this problem, the context domain is the d-dimensional boolean hypercube: X = {0, 1}d \u2013 i.e. the context each round for each individual consists of d boolean attributes. Our class of functions C is the class of boolean conjunctions:\nC = {f | f(x) = xi1 \u2227 xi2 \u2227 . . . \u2227 xik where 0 \u2264 k \u2264 d and i1, . . . , ik \u2208 [d]}.\nWe first give a simple but unfair algorithm, ConjunctionBandit, for this problem which obtains a regret bound which is linear in d. It maintains a set of candidate variables C\u2217j for each conjunction fj ; this set shrinks across rounds, while always containing the true set of variables over which fj is defined. We denote the boolean value of variable m in the context for arm j in round t by xtj,m.\nThe formal claim and proof that ConjunctionBandit achieves regret R(T ) = O(k2d), as well as ConjunctionBandit\u2019s formal description, can be found in Appendix C. ConjunctionBandit violates the fairness in every round t in which it predicts 0 for arm i but 1 for arm j even though fi(x t) = fj(x t) = 1, as \u03c0ti = 0 < 1 k < \u03c0 t j .\nWe now show that fair algorithms cannot guarantee subexponential regret in d. This relies upon a known lower bound for KWIK learning conjunctions [Li, 2009]:\nLemma 11. There exists a sequence of examples (x1, . . . , x2 d\u22121) such that for , \u03b4 \u2264 1/2, every ( , \u03b4)-KWIK learning algorithm B for the class C of conjunctions on d variables must output \u22a5 for xt for each t \u2208 [2d \u2212 1]. Thus, B has a KWIK bound of at least m( , \u03b4) = \u2126(2d).\nWe then use the equivalence between fair algorithms and KWIK learning to translate this lower bound on m( , \u03b4) into a minimum worst case regret bound for fair algorithms on conjunctions. We modify Theorem 6 to yield the following lemma, proven in Appendix C.\nLemma 12. Suppose A is a \u03b4-fair algorithm for the contextual bandit problem over the class C of conjunctions on d variables. If A has regret bound R(T, \u03b4) then for \u03b4\u2032 = 2T\u03b4, FairToKWIK is an (0, \u03b4\u2032)-KWIK algorithm for C with KWIK bound m(0, \u03b4\u2032) = 4R(m(0, \u03b4\u2032), \u03b4).\nLemma 11 then lets us lower-bound the worst case regret of fair learning algorithms on conjunctions.\nCorollary 2. For \u03b4 < 12T , any \u03b4-fair algorithm for the contextual bandit problem over the class C of conjunctions on d boolean variables has a worst case regret bound of R(T ) = \u2126(2d).\nProof. Let T \u2264 2d\u22121. We know then that if \u03b4\u2032 < 1, Lemma 11 guarantees the existence of a sequence of contexts x1, . . . , xT for which any (0, \u03b4\u2032)-KWIK algorithm has KWIK bound m(T, 0, \u03b4\u2032) = T .\nLemma 12 implies 4R(m(T, 0, \u03b4\u2032), \u03b4) gives a KWIK bound of m(T, 0, \u03b4\u2032) when \u03b4\u2032 = 2T\u03b4. Thus,\nif \u03b4 < 12T , then \u03b4 \u2032 < 1 and so R(m(T, 0, \u03b4\u2032), \u03b4) = m(T,0,\u03b4 \u2032) 4 = T 4 .\nTogether with the analysis of ConjunctionBandit, this demonstrates a strong separation between fair and unfair contextual bandit algorithms: when the underlying functions mapping contexts to payoffs are conjunctions on d variables, there exist a sequence of contexts on which fair algorithms must incur regret exponential in d while unfair algorithms can achieve regret linear in d."}, {"heading": "A Missing Proofs for the Classic Stochastic Bandits Upper Bound", "text": "We begin by proving Lemma 1, used in Section 3 to prove the fairness of the FairBandits algorithm.\nProof of Lemma 1. Choose an arbitrary arm i and round t and define indicator variablesX1, . . . , Xni(t) where Xn takes on the reward of pull n of arm i. By a Chernoff bound, for any a \u2265 0,\nP [ |\u00b5\u0302ti \u2212 \u00b5i| \u2264 a\nnti\n] \u2264 2 exp(\u22122a2/nti).\nIn particular for a = \u221a nti ln((\u03c0t) 2/3\u03b4)/2, it is the case that\nP [ \u00b5i 6\u2208 ( \u00b5\u0302ti \u2212 \u221a ln((\u03c0t)2/3\u03b4)\n2nti , \u00b5\u0302ti +\n\u221a ln((\u03c0t)2/3\u03b4)\n2nti )] \u22642 exp(\u22122nti ln((\u03c0t)2/3\u03b4)/2nti) = 2 exp(ln(3\u03b4/(\u03c0t)2)) = 6\u03b4/(\u03c0t)2.\nBy a union bound over all rounds t, the probability of any true mean ever falling outside of its confidence interval is at most \u03b4( 6 \u03c02 \u2211\u221e t=1 1 t2 ) = \u03b4.\nNext, we prove Lemma 2, which we used in Section 3 to bound the regret of FairBandits in Theorem 2.\nProof of Lemma 2. Let X1, ..., Xt be indicator variables of whether i was pulled at each time t \u2032 \u2208 [t]. Let Mt = \u2211\nt\u2032\u2264tXt\u2032 , with E [Mt] = pt. For any \u2208 [0, 1], a standard additive Chernoff bound states that\nP [Mt \u2264 pt \u2212 t] \u2264 e\u22122t 2 .\nSince i \u2208 St, it must be that i \u2208 St\u2032 for all t\u2032 \u2264 t and all i \u2208 St, by the definition of FairBandits. Thus, P [Xi = 1] \u2265 1k for any i \u2208 S t, and therefore pt \u2265 tk . so this also implies that\nP [ Mt \u2264 t\nk \u2212 t\n] \u2264 e\u22122t 2 .\nSetting t =\n\u221a t ln( 2t\n2k \u03b4 )\n2 , this bound becomes\nP Mt \u2264 t k \u2212 \u221a t ln(2t 2k \u03b4 ) 2  \u2264 \u03b4 2kt2\nas desired. Then, taking a union bound over all active arms of which there are at most k, the claim follows.\nProof of Lemma 3. This follows from the definition of `ti, u t i and the lower bound on n t i provided by the assumption of the lemma.\nA.1 Missing Derivation of R(T ) for Theorem 2\nR(T ) \u2264 T\u2211 t:0 min(1, k \u00b7 \u03b7(t)) + ( 1 + \u03c0 2 ) \u03b4T\n\u2264 T\u2211 t:0 k \u00b7min(1, \u03b7(t)) + ( 1 + \u03c0 2 ) \u03b4T\n\u2264 k  T\u2211 t: t k >2 \u221a t ln tk \u03b4 \u221a\u221a\u221a\u221a ln t\u03b4 t k \u2212 \u221a t ln tk\u03b4 + T\u2211 t: t k \u22642 \u221a t ln tk \u03b4 1 +O(\u03b4)T\n\u2264 k  T\u2211 t: t k >2 \u221a t ln tk \u03b4 \u221a ln t\u03b4 t 2k + T\u2211 t: t k \u22642 \u221a t ln tk \u03b4 1 +O(\u03b4T ) \u2264 k\n(\u222b T t=0 \u221a ln t\u03b4 t 2k + \u222b O\u0303(k2 ln k \u03b4 ) t=1 1 ) +O(\u03b4T )\n\u2264 k 3 2 \u222b T t=1 \u221a ln t\u03b4 t + O\u0303(k3 ln k \u03b4 ) +O(\u03b4T )\n= k 3 2 \u221a 2T \u221a ln kT\n\u03b4 + O\u0303(k3 ln\nk \u03b4 ) +O(\u03b4T )\n= O\u0303(k 3 2 \u221a T ln kT\n\u03b4 + k3) +O(\u03b4T )\n= O\u0303(k 3 2 \u221a T ln kT\n\u03b4 + k3)\nwhere the final step follows from \u03b4 \u2264 1\u221a T ."}, {"heading": "B Missing Proofs for the Classic Stochastic Bandits Lower Bound", "text": "All lemmas in this section are used in Section 4 to prove the fair lower bound in Theorem 3. The first, Lemma 4, lets us analyze distributions over payoffs.\nProof of Lemma 4. Let Ri represent the joint distribution on rewards for either experiment: in both cases, the joint distribution on rewards is identical, since the process which generates them is the same.\nWe will use the notation m, d1, . . . , dt to represent some fixed realization of the random variables \u00b5i, r 1 i , . . . , r t i and \u00b5 \u2032 i, r 1 i , . . . , r t i . In particular, it suffices to show that\nP(\u00b5i,r1i ,...,rti)\u223cW [ (\u00b5i, r 1 i , . . . , r t i) = (m, d 1, . . . , dt) ] = P(\u00b5\u2032i,r1i ,...,rti)\u223cW \u2032 [ (\u00b5\u2032i, r 1 i , . . . , r t i) = (m, d 1, . . . , dt) ] .\nThe first experiment which generates (\u00b5i, r 1 i , . . . , r t i) according to W has probability mass on this particular value of its random variables:\nP(\u00b5i,r1i ,...,rti)\u223cW [ (\u00b5i, r 1 i , . . . , r t i) = (m, d 1, . . . , dt) ] = P\u00b5i\u223cPi [\u00b5i = m] \u00b7 Pr1i ,...,rti\u223cB(\u00b5i) [ (r1i , . . . , r t i) = (d 1, . . . , dt) ]\nThe second experiment has joint probability: P(\u00b5\u2032i,r1i ,...,rti)\u223cW \u2032 [ (\u00b5\u2032i, r 1 i , . . . , r t i) = (m, d 1, . . . , dt) ]\n= P\u00b5\u2032i\u223cPi(r1i ,...,rti) [ \u00b5\u2032i = m ] \u00b7 P(r1i ,...,r1t )\u223cRi [ (r1i , . . . , r 1 t ) = (d 1, . . . , dt) ]\n= P(\u00b5\u2032i,r1i ,...,rti)\u223cW [ (\u00b5\u2032i, r 1 i , . . . , r t i) = (m, d 1, . . . , dt) ]\nwhere equality follows from Bayes\u2019 Rule.\nNext, we prove Lemma 5, used to reason about distinguishing between arms.\nProof of Lemma 5. Since neither i nor i + 1 is \u221a \u03b4-distinguished by ht, for any \u03b1i \u2208 {13 + i 3k , 1 3 + i+1 3k }, \u03b1i+1 \u2208 { 1 3 + i+1 3k , 1 3 + i+2 3k }, the posterior probability of \u00b5i = \u03b1i is less than 1 \u2212 \u221a \u03b4, and in particular for \u03b1 = 13 + i+1 3k , it must be the case that\nP\u00b5i\u223cPi(ht) [\u00b5i = \u03b1] > \u221a \u03b4 and also P\u00b5i+1\u223cPi(ht) [\u00b5i+1 = \u03b1] > \u221a \u03b4.\nSince P = \u220f i Pi, we know that\nP\u00b5i,\u00b5i+1\u223cP (ht) [\u00b5i = \u03b1 = \u00b5i+1] = P\u00b5i\u223cPi(ht) [\u00b5i = \u03b1] \u00b7 P\u00b5i+1\u223cPi+1(ht) [\u00b5i+1 = \u03b1] > \u03b4\nwhich completes the proof.\nFinally, we prove Lemma 6, which lets us reason about how fair algorithm choices depend on histories.\nProof of Lemma 6. We will define a set of histories which cause A to play some pair of arms i and i + 1 with different probabilities when \u00b5i = \u00b5i+1. Define the set unfair(A, \u00b5) such that ht \u2208 unfair(A, \u00b5) if there exist i \u2208 [k \u2212 1], t\u2032 \u2208 [t] such that \u00b5i = \u00b5i+1 but \u03c0t \u2032\ni|ht\u2032 6= \u03c0 t\u2032 i+1|ht\u2032 .\nConsider some ht which has not \u221a\n2\u03b4-distinguished any arm, such that there exists some i, t\u2032 for which \u03c0t \u2032\ni|ht\u2032 6= 1 k . Then, in particular, there exists some i \u2208 [k \u2212 1] such that \u03c0\nt\u2032 i|ht\u2032 6= \u03c0 t\u2032 i+1|ht\u2032 . By Lemma 5, for all i and in particular this i, it is the case that 2\u03b4 < P\u00b5\u2032\u223cP |ht [ \u00b5\u2032i = \u00b5 \u2032 i+1 ] = X and so\n2\u03b4 < X = P\u00b5\u2032\u223cP |ht [ \u00b5\u2032i = \u00b5 \u2032 i+1 \u2229 \u03c0t \u2032 i|ht 6= \u03c0 t\u2032 i+1|ht ] \u2264 P\u00b5\u2032\u223cP |ht [ ht \u2208 unfair(A, \u00b5\u2032) ] (4)\nwhere the first equality comes from the fact that ht is a history for which \u03c0t \u2032 i|ht 6= \u03c0 t\u2032\ni+1|ht and the second equality from the definition of the set unfair.\nWe will show that Equation 4 cannot hold with probability more than 12 over the draw of \u00b5, h t from the underlying distribution, or else A would not satisfy \u03b4-fairness. Since A is \u03b4-fair, for any fixed \u00b5\n\u03b4 \u2265 Pht\u223cA|\u00b5 [ ht \u2208 unfair(A, \u00b5) ] and therefore for any distribution P over \u00b5 that\n\u03b4 \u2265 P\u00b5\u223cP,ht\u223cA|\u00b5 [ ht \u2208 unfair(A, \u00b5) ] .\nLemma 4 implies also \u03b4 \u2265 P\u00b5\u223cP,ht\u223cA|\u00b5,\u00b5\u2032\u223cP |ht [ ht \u2208 unfair(A, \u00b5\u2032) ] , so by Markov\u2019s inequality\n1 2 \u2265 P\u00b5\u223cP,ht\u223cA|\u00b5\n[ P\u00b5\u2032\u223cP |ht [ ht \u2208 unfair(A, \u00b5\u2032) ] \u2265 2\u03b4 ] .\nThus, with probability at least 12 over the distribution over histories and means,\nP\u00b5\u2032\u223cP |ht [ ht \u2208 unfair(A, \u00b5\u2032) ] \u2264 2\u03b4.\nHowever, Equation 4 shows this does not hold for any ht which does not \u221a\n2\u03b4-distinguish any arm but for which \u03c0t \u2032\ni|ht\u2032 6= 1 k for some i \u2208 [k], t \u2032 \u2264 t. Thus, for at least 12 of all probability mass over histories, either \u03c0t \u2032\ni|ht\u2032 = 1 k for all i, t\n\u2032 \u2264 t, or ht must \u221a 2\u03b4-distinguish some arm."}, {"heading": "C Missing Proofs for the Contextual Bandit Setting", "text": "We begin by proving two results related to KWIKToFair. The first, Lemma 8, was used in Section 5 to prove that KWIKToFair is \u03b4-fair in Theorem 5.\nProof of Lemma 8. We will refer to a violation of either (a) or (b) as a failure of learner Li. For each Li, the set of queries asked of it are pairs (hi, x t i), histories along with new contexts. There are at most T contexts queried, and at most T histories on which Li is queried for a fixed run of our algorithm (namely, prefixes of Li\u2019s final history). Thus, there are at most T 2 queries for Li. Thus, by a union bound over these T 2 queries for learner Li, by the KWIK guarantee, P [Li fails in some round] \u2264 T 2\u03b4\u2217 = min(\u03b4, 1T )/k, and by a union bound over k arms, P [A learner fails in a round] \u2264 min(\u03b4, 1T ).\nWe proceed to Theorem 4, used in Section 5 to construct a \u03b4-fair algorithm with quantified regret from KWIK learners.\nProof of Theorem 4. We use repeated calls to KWIKToFair (\u03b4, T ) to run for an indefinite number of rounds. Specifically, we will make calls E = 1, 2, . . . to KWIKToFair (6\u03b4/\u03c0(log(T )2, 2E). We will refer to each such call to KWIKToFair by its epoch E. By Lemma 8, each epoch E is 6\u03b4/\u03c0E2k-fair, i.e. has a 6\u03b4/\u03c0E2k probability of violating fairness. Therefore by a union bound across epochs, the probability of ever violating fairness through repeated calls to KWIKToFair is bounded above by \u2211\u221e E=1 6\u03b4 (\u03c0E)2 = 6\u03b4 \u03c02 \u2211\u221e E=1 1 E2 = \u03b4, so the overall algorithm is \u03b4-fair.\nNext, by Lemma 9 each epoch E contributes at most regret 3 \u00b7 2Ek \u2217E where \u2217E denotes the value of \u2217 used in epoch E, i.e. \u2217E satisfying \u2217 E = k \u00b7m( \u2217E , 6\u03b4/\u03c0E2, 2E). Then since each epoch E covers 2E rounds, through round T the algorithm has used fewer than log(T ) epochs, and by the\ndoubling trick achieves regret R(T ) < \u2211log(T )\nE=1 3 \u00b7 2Ek \u2217E = O(Tk \u2217) = O(k2 \u00b7m( \u2217, \u03b4\u2217)).\nNext, we address the special subcase of KWIKToFair for linear functions outlined in Corollary 1.\nProof of Corollary 1. By Lemma 10, for each arm j the associated learner Lj has mistake bound m( , \u03b4) = O\u0303(d3/ 4). Since \u2217 satisfies \u2217 = k \u00b7m( \u2217, \u03b4)/T we get \u2217 = ( kd3\nT\n)1/5 Substituting this\ninto Theorem 4, the overall regret guarantee satisfies regret R(T ) = O(k2 \u00b7m( \u2217, \u03b4\u2217)) = O(Tk \u2217) = O(T 4/5k6/5d3/5).\nThis brings us to the formal algorithm description of ConjunctionBandit and its corresponding regret bound, used in Section 6.1 as an example of an unfair learning algorithm for conjunctions.\n1: procedure ConjunctionBandit 2: Let C\u2217j \u2190 {1, 2, . . . , d} for all j \u2208 [k] . Initialize set of candidate variables for fj\n3: for t = 1, 2, . . . do 4: St \u2190 \u2205 . Initialize active set of arms 5: for j = 1, 2, . . . , k do 6: if \u2227m\u2208C\u2217j x t j,m = 1 then 7: St \u2190 St \u222a {j} . Add arm j to active set 8: if St = \u2205 then 9: Pull arm j\u2217 \u2190 (x \u2208R [k]) . Pull arm at random 10: if rtj\u2217 = 1 then 11: C\u2217j\u2217 \u2190 C\u2217j\u2217 \\ {m | xtj,m = 0} 12: else 13: Pull arm j\u2217 \u2190 (x \u2208R St) . Pull arm from active set at random\nWe can now upper bound the regret achieved by ConjunctionBandit.\nLemma 13. ConjunctionBandit achieves regret R(T ) = O(k2d).\nProof of Lemma 13. First, we claim that for every j, for the duration of the algorithm, that Cj \u2286 C\u2217j , where Cj is the true set of variables corresponding to fj . This holds at initialization: Cj \u2286 [d] = C\u2217j . Suppose the claim holds prior to round t: if C \u2217 j is updated in this round, then fj(x t j) = 1 \u21d2 \u2200m \u2208 [d] : xtj,m = 0,m /\u2208 Cj . Thus, C\u2217j = C\u2217j \\{m : xtj,m = 0} = C\u2217j \\{m : xtj,m = 0\u2229m /\u2208 Cj} \u2283 Cj . Therefore, the algorithm never makes false positive mistakes: in any round t, j \u2208 St \u21d2 fj(xtj) = 1. Therefore ConjunctionBandit only accumulates regret in rounds where it makes false negative mistakes by predicting that all arms have reward 0 when some arm has reward 1.\nThen, we have Regret(x1, . . . , xT ) = \u2211\nt maxj ( fj(x t j) ) \u2212 E [\u2211 t fit(x t it) ] . We then rewrite the\nfirst term as \u2211\nt maxj ( fj(x t j) ) = \u2211 t I{fj(xtj) = 1 for some j \u2208 [k]} and the second term as\nE [\u2211 t fit(x t it) ] = T \u2212 E [\u2211 t I{St = \u2205 \u2227 fj\u2217(xtj\u2217) = 0 \u2227 fj(xtj) = 1 for some j \u2208 [k]} ]\n\u2265 T \u2212 E [\u2211 t I{fj\u2217(xtj\u2217) = 0 | St = \u2205 \u2227 fj(xtj) = 1 for some j \u2208 [k]} ]\n\u2265 T \u2212 \u2211 j\u2208[k] E [\u2211 t I{fj\u2217(xtj\u2217) = 0 | St = \u2205 \u2227 fj(xtj) = 1} ]\n\u2265 T \u2212 \u2211 j\u2208[k] kd = T \u2212 k2d\nwhere the last inequality follows from P [ j\u2217 = j | St = \u2205 \u2227 fj(xtj) = 1 ] = 1k and the fact that if St = \u2205 and fj\u2217(xtj\u2217) = 1 then Cj\u2217 loses at least one of d variables, and this loss can therefore occur at most d times for each arm j. Substituting this into the original regret expression then yields\nRegret(x1, . . . , xT ) \u2264 \u2211 t I{fj(xtj) = 1 for some j \u2208 [k]} \u2212 (T \u2212 k2d)\n\u2264 T \u2212 (T \u2212 k2d) \u2264 k2d.\nFinally, we prove Lemma 12, which we used in Section 6.1 to translate between fair and KWIK learning on conjunctions.\nProof of Lemma 12. We mimic the structure of the proof of Theorem 6, once again using FairToKWIK to construct a KWIK learner B by running the given fair algorithm A on a constructed bandit instance for each context xt.\nThere are two primary modifications for the specific case of conjunctions: as conjunctions output either 0 or 1 we set = 0, \u2217 = 1, and \u03b4\u2217 = \u03b42T . A therefore runs on 2T histories and contexts, either of form (xt, x(0) = 0) or (xt, x(1) = 1). Since we initialize A to be \u03b4\u2217-fair, if we fix history ht along with context and arm assignment (xt, x(`)) then, with probability at least 1\u2212 \u03b4\u2217, pt,`1 > p t,` 2 implies f\u2217(xt) > `/2 and similarly pt,`2 > p t,` 1 implies f\n\u2217(xt) < `/2. Union bounding over all such t and ` yields that A satisfies this fairness over all t and ` with probability at least 1 \u2212 \u03b4, and we condition on this event for the rest of the proof.\nWe proceed to prove that the resulting KWIK learner B is -accurate. Here, as = 0, this requires showing that all of B\u2019s numerical predictions are correct. Assume instead that B outputs an incorrect prediction on (xt, x(`)). By the construction of FairToKWIK, a prediction from B implies that at least one of pt,01 , p t,1 1 , p t,0 2 and p t,1 2 is distinct from the others. We condition on this distinctness to get two cases. In the first case, pt,`1 \u2264 p t,` 2 for both ` = 0 and 1. By distinctness, this means that either p t,0 1 < p t,0 2 or pt,11 < p t,1 2 . By the fairness assumption, this respectively implies that f \u2217(xt) < f(x(0)) = 0 or f\u2217(xt) < f(x(1)) = 1. In either event, f\u2217x(t) = 0 = y\u0302t. In the second case, pt,`1 > p t,` 2 for at least one of ` = 0 or 1. pt,11 > p t,1 2 violates the fairness assumption on A as f(x(1)) = 1, so it must be that pt,01 > p t,0 2 . Fairness then implies that f\n\u2217(xt) = 1 = y\u0302t. Therefore B is -accurate. It remains to upper bound m( , \u03b4). Any round where B outputs \u22a5 means a choice between two contexts, one of which has a difference of 1 between arms. It follows that choosing randomly between both arms and contexts incurs expected regret 1/4. Therefore m( ,\u03b4)4 < R(m( , \u03b4), \u03b4 \u2217, d) = R(m( , \u03b4), \u03b42T , d)."}], "references": [{"title": "Auditing black-box models by obscuring features", "author": ["Philip Adler", "Casey Falk", "Sorelle A. Friedler", "Gabriel Rybeck", "Carlos Scheidegger", "Brandon Smith", "Suresh Venkatasubramanian"], "venue": "CoRR, abs/1602.07043,", "citeRegEx": "Adler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Adler et al\\.", "year": 2016}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel J. Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Graphical models for bandit problems", "author": ["Kareem Amin", "Michael Kearns", "Umar Syed"], "venue": "arXiv preprint arXiv:1202.3782,", "citeRegEx": "Amin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2012}, {"title": "Large-scale bandit problems and kwik learning", "author": ["Kareem Amin", "Michael Kearns", "Moez Draief", "Jacob D Abernethy"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Amin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2013}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Big data\u2019s disparate impact", "author": ["Solon Barocas", "Andrew D. Selbst"], "venue": "California Law Review,", "citeRegEx": "Barocas and Selbst.,? \\Q2016\\E", "shortCiteRegEx": "Barocas and Selbst.", "year": 2016}, {"title": "The new science of sentencing", "author": ["Anna Maria Barry-Jester", "Ben Casselman", "Dana Goldstein"], "venue": "The Marshall Project, August", "citeRegEx": "Barry.Jester et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barry.Jester et al\\.", "year": 2015}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert E. Schapire"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2011}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Artificial intolerance. MIT Technology Review, March 28 2016", "author": ["Nanette Byrnes"], "venue": "URL https://www. technologyreview.com/s/600996/artificial-intolerance/. Retrieved 4/28/2016", "citeRegEx": "Byrnes.,? \\Q2016\\E", "shortCiteRegEx": "Byrnes.", "year": 2016}, {"title": "Three naive bayes approaches for discrimination-free classification", "author": ["Toon Calders", "Sicco Verwer"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Calders and Verwer.,? \\Q2010\\E", "shortCiteRegEx": "Calders and Verwer.", "year": 2010}, {"title": "Contextual bandits with linear payoff functions", "author": ["Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E. Schapire"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Chu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2011}, {"title": "Regulating by robot: Administrative decision-making in the machine-learning era", "author": ["Cary Coglianese", "David Lehr"], "venue": "Georgetown Law Journal,", "citeRegEx": "Coglianese and Lehr.,? \\Q2016\\E", "shortCiteRegEx": "Coglianese and Lehr.", "year": 2016}, {"title": "Fairness through awareness", "author": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,", "citeRegEx": "Dwork et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2012}, {"title": "Certifying and removing disparate impact", "author": ["Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "Feldman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2015}, {"title": "A confidence-based approach for balancing fairness and accuracy", "author": ["Benjamin Fish", "Jeremy Kun", "\u00c1d\u00e1m D Lelkes"], "venue": "SIAM International Symposium on Data Mining,", "citeRegEx": "Fish et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fish et al\\.", "year": 2016}, {"title": "Navigating the \u201ctrackless ocean\u201d: Fairness in big data research and decision making", "author": ["FTC Commisioner Julie Brill"], "venue": "Keynote Address at the Columbia", "citeRegEx": "Brill.,? \\Q2015\\E", "shortCiteRegEx": "Brill.", "year": 2015}, {"title": "Fairness-aware learning through regularization approach", "author": ["Toshihiro Kamishima", "Shotaro Akaho", "Jun Sakuma"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "Kamishima et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kamishima et al\\.", "year": 2011}, {"title": "Sequential choice from several populations", "author": ["Michael N Katehakis", "Herbert Robbins"], "venue": "PROCEEDINGS-NATIONAL ACADEMY OF SCIENCES USA,", "citeRegEx": "Katehakis and Robbins.,? \\Q1995\\E", "shortCiteRegEx": "Katehakis and Robbins.", "year": 1995}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "A unifying framework for computational reinforcement learning theory", "author": ["Lihong Li"], "venue": "PhD thesis, Rutgers, The State University of New Jersey,", "citeRegEx": "Li.,? \\Q2009\\E", "shortCiteRegEx": "Li.", "year": 2009}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Lihong Li", "Michael L Littman", "Thomas J Walsh", "Alexander L Strehl"], "venue": "Machine learning,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "k-nn as an implementation of situation testing for discrimination discovery and prevention", "author": ["Binh Thanh Luong", "Salvatore Ruggieri", "Franco Turini"], "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Luong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2011}, {"title": "Can an algorithm hire better than a human", "author": ["Clair C Miller"], "venue": "The New York Times, June", "citeRegEx": "Miller.,? \\Q2015\\E", "shortCiteRegEx": "Miller.", "year": 2015}, {"title": "Big data: A report on algorithmic systems, opportunity, and civil rights", "author": ["Cecilia Munoz", "Megan Smith", "DJ Patil"], "venue": "Technical report, Executive Office of the President,", "citeRegEx": "Munoz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munoz et al\\.", "year": 2016}, {"title": "Big data: Seizing opportunities, protecting values. Technical report, Executive Office of the President", "author": ["John Podesta", "Penny Pritzker", "Ernest J. Moniz", "John Holdern", "Jeffrey Zients"], "venue": null, "citeRegEx": "Podesta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Podesta et al\\.", "year": 2014}, {"title": "Online linear regression and its application to model-based reinforcement", "author": ["Michael L Littman"], "venue": null, "citeRegEx": "Strehl and Littman.,? \\Q2016\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2016}, {"title": "Discrimination in online ad delivery", "author": ["Rich Zemel", "Yu Wu", "Kevin Swersky", "Toni Pitassi", "Cynthia Dwork"], "venue": "Neural Information Processing Systems, pages 1417\u20131424,", "citeRegEx": "Zemel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zemel et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 23, "context": "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people\u2019s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al.", "startOffset": 173, "endOffset": 187}, {"referenceID": 9, "context": "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people\u2019s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al.", "startOffset": 197, "endOffset": 211}, {"referenceID": 6, "context": "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people\u2019s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al., 2015].", "startOffset": 266, "endOffset": 293}, {"referenceID": 5, "context": "These high stakes uses of machine learning have led to increasing concern in law and policy circles about the potential for (often opaque) machine learning techniques to be discriminatory or unfair [Coglianese and Lehr, 2016, Barocas and Selbst, 2016]. Moreover, these concerns are not merely hypothetical: Sweeney [2013] observed that contextual ads for public record services shown in response to Google searches for stereotypically African American names were more likely to contain text referring to arrest records, compared to comparable ads shown in response to searches for stereotypically Caucasian names, which showed more neutral text.", "startOffset": 226, "endOffset": 322}, {"referenceID": 25, "context": "1 For example, a 2014 White House report [Podesta et al., 2014] notes that \u201c[t]he increasing use of algorithms to make eligibility decisions must be carefully monitored for potential discriminatory outcomes for disadvantaged groups, even absent discriminatory intent.", "startOffset": 41, "endOffset": 63}, {"referenceID": 24, "context": "\u201d Along the same lines, a 2016 White House report [Munoz et al., 2016] observes that \u201c[a]s improvements in the uses of big data and machine learning continue, it will remain important not to place too much reliance on these new systems without questioning and continuously testing the inputs and mechanics behind them and the results they produce.", "startOffset": 50, "endOffset": 70}, {"referenceID": 4, "context": "Without a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002].", "startOffset": 149, "endOffset": 168}, {"referenceID": 4, "context": "Without a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002]. In Section 3, we give an algorithm that satisfies our fairness constraint and is able to guarantee non-trivial regret after T = O(k3) rounds. We then show in Section 4 that it is not possible to do better \u2013 any It is natural that different populations should have different underlying functions \u2013 for example, in a college admissions setting, the function mapping applications to college success probability might weight SAT scores less in a wealthy population that employs SAT tutors, and more in a working-class population that does not \u2013 see Dwork et al. [2012] for more discussion of this issue and Munoz et al.", "startOffset": 150, "endOffset": 735}, {"referenceID": 4, "context": "Without a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002]. In Section 3, we give an algorithm that satisfies our fairness constraint and is able to guarantee non-trivial regret after T = O(k3) rounds. We then show in Section 4 that it is not possible to do better \u2013 any It is natural that different populations should have different underlying functions \u2013 for example, in a college admissions setting, the function mapping applications to college success probability might weight SAT scores less in a wealthy population that employs SAT tutors, and more in a working-class population that does not \u2013 see Dwork et al. [2012] for more discussion of this issue and Munoz et al. [2016] for examples.", "startOffset": 150, "endOffset": 793}, {"referenceID": 21, "context": "We then move on to the general contextual bandit setting and prove a broad characterization result, relating fair contextual bandit learning to KWIK learning [Li et al., 2011].", "startOffset": 158, "endOffset": 175}, {"referenceID": 21, "context": "This general connection has immediate implications, because it allows us to import known results for KWIK learning [Li et al., 2011].", "startOffset": 115, "endOffset": 132}, {"referenceID": 7, "context": "Calders and Verwer [2010], Luong et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 7, "context": "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 7, "context": "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al.", "startOffset": 0, "endOffset": 72}, {"referenceID": 7, "context": "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al. [2015], Fish et al.", "startOffset": 0, "endOffset": 95}, {"referenceID": 7, "context": "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al. [2015], Fish et al. [2016] and Adler et al.", "startOffset": 0, "endOffset": 115}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact).", "startOffset": 11, "endOffset": 31}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems.", "startOffset": 11, "endOffset": 232}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness.", "startOffset": 11, "endOffset": 577}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d.", "startOffset": 11, "endOffset": 850}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer.", "startOffset": 11, "endOffset": 1423}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a \u201cfair\u201d metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes.", "startOffset": 11, "endOffset": 1955}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a \u201cfair\u201d metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes. Our fairness constraint, conversely, is entirely aligned with the goal of the algorithm designer in that it is satisfied by the optimal policy; nevertheless, it affects the space of feasible learning algorithms, because it interferes with learning an optimal policy, which depends on the unknown reward functions. At a technical level, our work is related to Amin et al. [2012] and Amin et al.", "startOffset": 11, "endOffset": 2437}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a \u201cfair\u201d metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes. Our fairness constraint, conversely, is entirely aligned with the goal of the algorithm designer in that it is satisfied by the optimal policy; nevertheless, it affects the space of feasible learning algorithms, because it interferes with learning an optimal policy, which depends on the unknown reward functions. At a technical level, our work is related to Amin et al. [2012] and Amin et al. [2013], which also relate KWIK learning to bandit learning in a different context, unrelated to fairness (when the arm space is very large).", "startOffset": 11, "endOffset": 2460}, {"referenceID": 13, "context": "This relaxation is a special case of Dwork et al. [2012]\u2019s proposed family of definitions, which require that \u201csimilar individuals be treated similarly\u201d.", "startOffset": 37, "endOffset": 57}, {"referenceID": 4, "context": "In this section, we describe a simple and intuitive modification of the standard UCB algorithm [Auer et al., 2002], called FairBandits, prove that it is fair, and analyze its regret bound.", "startOffset": 95, "endOffset": 114}, {"referenceID": 4, "context": "This is reflected in its regret bound, which is only non-trivial after T k3, whereas the UCB algorithm [Auer et al., 2002] achieves non-trivial regret after T = O(k) rounds.", "startOffset": 103, "endOffset": 122}, {"referenceID": 8, "context": "However, it is known that \u03a9 (\u221a kT ) regret is necessary even in the unrestricted setting (without fairness) if one does not make data-specific assumptions on an instance [Bubeck and Cesa-Bianchi, 2012] (e.", "startOffset": 170, "endOffset": 201}, {"referenceID": 20, "context": "This relies upon a known lower bound for KWIK learning conjunctions [Li, 2009]:", "startOffset": 68, "endOffset": 78}], "year": 2017, "abstractText": "We introduce the study of fairness in multi-armed bandit problems. Our fairness definition can be interpreted as demanding that given a pool of applicants (say, for college admission or mortgages), a worse applicant is never favored over a better one, despite a learning algorithm\u2019s uncertainty over the true payoffs. We prove results of two types: First, in the important special case of the classic stochastic bandits problem (i.e. in which there are no contexts), we provide a provably fair algorithm based on chained confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence. When combined with regret bounds for standard non-fair algorithms such as UCB, this proves a strong separation between fair and unfair learning, which extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm, and conversely any fair contextual bandit algorithm can be transformed into a KWIK learning algorithm. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms. \u2217Department of Computer and Information Sciences, University of Pennsylvania. {majos,mkearns,jamiemor,aaroth}@cis.upenn.edu. AR is supported in part by an NSF CAREER award, a Sloan Foundation Fellowship, and a Google Faculty Research Award. 1 ar X iv :1 60 5. 07 13 9v 1 [ cs .L G ] 2 3 M ay 2 01 6", "creator": "LaTeX with hyperref package"}}}