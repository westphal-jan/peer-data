{"id": "1605.07571", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Sequential Neural Models with Stochastic Layers", "abstract": "How can we renovo efficiently 60.63 propagate 1,8 uncertainty in a derochette latent state vandermeer representation with nicolini recurrent ledovskikh neural meseznikov networks? This paper bullough introduces stochastic recurrent blankety neural shapely networks which 100-0 glue 81.96 a deterministic tatsunami recurrent alt-country neural regenerator network duri and a verelis state space ioana model murdoch together to college-bound form 1922/23 a stochastic and kerlin sequential neural generative 335-pound model. civ The arnaut clear separation nobbs of anne-marie deterministic bakes and sugata stochastic layers barendrecht allows a laxa structured variational inference network blago to track 6:44 the littlefield factorization of preformed the testimonial model ' igwe s posterior nextlevel distribution. By retaining both intruding the worgu nonlinear recursive manufactory structure student/teacher of mauritz a indianhead recurrent newmarket-on-fergus neural nritya network and averaging over the 0.28 uncertainty in 48 a latent schwarzenau path, realties like a kenyang state rodrik space model, we improve the tawan state sandy of the subodh art al-azm results on the telescreen Blizzard remaking and TIMIT speech zenit-2 modeling data sets by rfc a large dermoid margin, while krippner achieving comparable performances to 7.83 competing methods 64-qam on l'\u00e9ducation polyphonic music talismans modeling.", "histories": [["v1", "Tue, 24 May 2016 18:23:58 GMT  (341kb,D)", "http://arxiv.org/abs/1605.07571v1", null], ["v2", "Sun, 13 Nov 2016 18:04:41 GMT  (181kb,D)", "http://arxiv.org/abs/1605.07571v2", "NIPS 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["marco fraccaro", "s\u00f8ren kaae s\u00f8nderby", "ulrich paquet", "ole winther"], "accepted": true, "id": "1605.07571"}, "pdf": {"name": "1605.07571.pdf", "metadata": {"source": "CRF", "title": "Sequential Neural Models with Stochastic Layers", "authors": ["Marco Fraccaro", "S\u00f8ren Kaae S\u00f8nderby", "Ulrich Paquet", "Ole Winther"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Recurrent neural networks (RNNs) are able to represent long-term dependencies in sequential data, by adapting and propagating a deterministic hidden (or latent) state [6, 17]. There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16]. In this paper we add a new direction to the explorer\u2019s map of treating the hidden RNN states as uncertain paths, by including the world of state space models (SSMs) as an RNN layer. By cleanly delineating a SSM layer, certain independence properties of variables arise, which are beneficial for making efficient posterior inferences. The result is a generative model for sequential data, with a matching inference network that has its roots in variational auto-encoders (VAEs).\nSSMs can be viewed as a probabilistic extension of RNNs, where the hidden states are assumed to be random variables. Although SSMs have an illustrious history [27], their stochasticity has limited their widespread use in the deep learning community, as inference can only be exact for two relatively simple classes of SSMs, namely hidden Markov models and linear Gaussian models, neither of which are well-suited to modeling long-term dependencies and complex probability distributions over high-dimensional sequences. On the other hand modern RNNs rely on gated nonlinearities such as long short-term memory (LSTM) [17] cells or gated recurrent units (GRUs) [7], that let the deterministic hidden state of the RNN act as an internal memory for the model. This internal memory seems fundamental to capturing complex relationships in the data through a statistical model.\nThis paper introduces the stochastic recurrent neural network (SRNN) in Section 3. SRNNs combine the gated activation mechanism of RNNs with the stochastic states of SSMs, and are formed by stacking a RNN and a nonlinear SSM. The state transitions of the SSM are nonlinear and are parametrized by a neural network that also depend on the corresponding RNN hidden state. The SSM can therefore utilize long-term information captured by the RNN.\nWe use recent advances in variational inference to efficiently approximate the intractable posterior distribution over the latent states with an inference network [21, 26]. The form of our variational\n\u2217Now at Google DeepMind.\nar X\niv :1\n60 5.\n07 57\n1v 1\n[ st\nat .M\nL ]\n2 4\nM ay\n2 01\napproximation is inspired by the independence properties of the true posterior distribution over the latent states of the model, and allows us to improve inference by conveniently using the information coming from the whole sequence at each time step. The posterior distribution over the latent states of the SRNN is highly non-stationary while we are learning the parameters of the model. To further improve the variational approximation, we show that we can construct the inference network so that it only needs to learn how to compute the mean of the variational approximation at each time step given the mean of the predictive prior distribution.\nIn Section 4 we test the performances of SRNN on speech and polyphonic music modelling tasks. SRNN improves the state of the art results on the Blizzard and TIMIT speech data sets by a large margin, and performs comparably to competing models on polyphonic music modeling. Finally, other models that extend RNNs by adding stochastic units will be reviewed and compared to SRNN in Section 5."}, {"heading": "2 Recurrent Neural Networks and State Space Models", "text": "Recurrent neural networks and state space models are widely used to model temporal sequences of vectors x1:T = (x1,x2, . . . ,xT ) that possibly depend on inputs u1:T = (u1,u2, . . . ,uT ). Both models rest on the assumption that the sequence x1:t of observations up to time t can be summarized by a latent state dt or zt, which is deterministically determined (dt in a RNN) or treated as a random variable which is averaged away (zt in a SSM). The difference in treatment of the latent state has traditionally led to vastly different models: RNNs recursively compute dt = f(dt\u22121,ut) using a parameterized nonlinear function f , like a LSTM cell or a GRU. The RNN observation probabilities p(xt|dt) are equally modeled with nonlinear functions. SSMs, like linear Gaussian or hidden Markov models, explicitly model uncertainty in the latent process through z1:T . Parameter inference in a SSM require z1:T to be averaged out, and hence p(zt|zt\u22121,ut) and p(xt|zt) are often restricted to the exponential family of distributions to make many existing approximate inference algorithms applicable. On the other hand, averaging a function over the deterministic path d1:T in a RNN is a trivial operation. The striking similarity in factorization between these models is illustrated in Figures 1a and 1b.\nCan we combine the best of both worlds, and make the stochastic state transitions of SSMs nonlinear whilst keeping the gated activation mechanism of RNNs? Below, we show that a more expressive model can be created by stacking a SSM on top of a RNN, and that by keeping them layered, the functional form of the true posterior distribution over z1:T guides the design of a backwards-recursive structured variational approximation."}, {"heading": "3 Stochastic Recurrent Neural Networks", "text": "We define a SRNN as a generative model p\u03b8 by temporally interlocking a SSM with a RNN, as illustrated in Figure 2a. The joint probability of a single sequence and its latent states, assuming knowledge of the starting states z0 = 0 and d0 = 0, and inputs u1:T , factorizes as\np\u03b8(x1:T , z1:T ,d1:T |u1:T , z0,d0) = p\u03b8x(x1:T |z1:T ,d1:T ) p\u03b8z(z1:T |d1:T , z0) p\u03b8d(d1:T |u1:T ,d0)\n= T\u220f t=1 p\u03b8x(xt|zt,dt) p\u03b8z(zt|zt\u22121,dt) p\u03b8d(dt|dt\u22121,ut) . (1)\nThe SSM and RNN are further tied with skip-connections from dt to xt. The joint density in (1) is parameterized by \u03b8 = {\u03b8x, \u03b8z, \u03b8d}, which will be adapted together with parameters \u03c6 of a so-called \u201cinference network\u201d q\u03c6 to best model N independently observed data sequences {xi1:Ti} N i=1 that are described by the log marginal likelihood or evidence\nL(\u03b8) = log p\u03b8 ( {xi1:Ti} | {u i 1:Ti , z i 0,d i 0}Ni=1 ) = \u2211 i log p\u03b8(x i 1:Ti |u i 1:Ti , z i 0,d i 0) = \u2211 i Li(\u03b8) . (2)\nThroughout the paper, we omit superscript i when only one sequence is referred to, or when it is clear from the context. In each log likelihood term Li(\u03b8) in (2), the latent states z1:T and d1:T were averaged out of (1). Integrating out d1:T is done by simply substituting its deterministically obtained value, but z1:T requires more care, and we return to it in Section 3.2. Following Figure 2a, the states d1:T are determined from d0 and u1:T through the recursion dt = f\u03b8d(dt\u22121,ut). In our implementation f\u03b8d is a GRU network with parameters \u03b8d. For later convenience we denote the value of d1:T , as computed by application of f\u03b8d , by d\u03031:T . Therefore p\u03b8d(dt|dt\u22121,ut) = \u03b4(dt \u2212 d\u0303t), i.e. d1:T follows a delta distribution centered at d\u03031:T .\nUnlike the VRNN [8], zt directly depends on zt\u22121, as it does in a SSM, via p\u03b8z(zt|zt\u22121,dt). This split makes a clear separation between the deterministic and stochastic parts of p\u03b8; the RNN remains entirely deterministic and its recurrent units do not depend on noisy samples of zt, while the prior over zt follows the Markov structure of SSMs. The split allows us to later mimic the structure of the posterior distribution over z1:T and d1:T in its approximation q\u03c6. We let the prior transition distribution p\u03b8z(zt|zt\u22121,dt) = N (zt;\u00b5 (p) t ,v (p) t ) be a Gaussian with a diagonal covariance matrix, whose mean and log-variance are parameterized by neural networks that depend on zt\u22121 and dt,\n\u00b5 (p) t = NN (p) 1 (zt\u22121,dt) , logv (p) t = NN (p) 2 (zt\u22121,dt) , (3)\nwhere NN denotes a neural network. Parameters \u03b8z denote all weights of NN (p) 1 and NN (p) 2 , which are two-layer feed-forward networks in our implementation. Similarly, the parameters of the emission distribution p\u03b8x(xt|zt,dt) depend on zt and dt through a similar neural network that is parameterized by \u03b8x."}, {"heading": "3.1 Variational inference for the SRNN", "text": "The stochastic variables z1:T of the nonlinear SSM cannot be analytically integrated out to obtain L(\u03b8) in (2). Instead of maximizing L with respect to \u03b8, we maximize a variational evidence lower\nbound (ELBO) F(\u03b8, \u03c6) = \u2211 i Fi(\u03b8, \u03c6) \u2264 L(\u03b8) with respect to both \u03b8 and the variational parameters \u03c6 [18]. The ELBO is a sum of lower bounds Fi(\u03b8, \u03c6) \u2264 Li(\u03b8), one for each sequence i,\nFi(\u03b8, \u03c6) = \u222b\u222b\nq\u03c6(z1:T ,d1:T |x1:T , A) log p\u03b8(x1:T , z1:T ,d1:T |A) q\u03c6(z1:T ,d1:T |x1:T , A) dz1:T dd1:T , (4)\nwhere A = {u1:T , z0,d0} is a notational shorthand. Each sequence\u2019s approximation q\u03c6 shares parameters \u03c6 with all others, to form the auto-encoding variational Bayes inference network or variational auto encoder (VAE) [21, 26] shown in Figure 2b. Maximizing F(\u03b8, \u03c6) \u2013 which we call \u201ctraining\u201d the neural network architecture with parameters \u03b8 and \u03c6 \u2013 is done by stochastic gradient ascent, and in doing so, both the posterior and its approximation q\u03c6 change simultaneously. All the intractable expectations in (4) would typically be approximated by sampling, using the reparameterization trick [21, 26] or control variates [24] to obtain low-variance estimators of its gradients. We use the reparameterization trick in our implementation. Iteratively maximizing F over \u03b8 and \u03c6 separately would yield an expectation maximization-type algorithm, which has formed a backbone of statistical modeling for many decades [9]. The tightness of the bound depends on how well we can approximate the i = 1, . . . , N factors p\u03b8(zi1:Ti ,d i 1:Ti |xi1:Ti , A\ni) that constitute the true posterior over all latent variables with their corresponding factors q\u03c6(zi1:Ti ,d i 1:Ti |xi1:Ti , A\ni). In what follows, we show how q\u03c6 could be judiciously structured to match the posterior factors.\nWe add initial structure to q\u03c6 by noticing that the prior p\u03b8d(d1:T |u1:T ,d0) in the generative model is a delta function over d\u03031:T , and so is the posterior p\u03b8(d1:T |x1:T ,u1:T ,d0). Consequently, we let the inference network use exactly the same deterministic state setting d\u03031:T as that of the generative model, and we decompose it as\nq\u03c6(z1:T ,d1:T |x1:T ,u1:T , z0,d0) = q\u03c6(z1:T |d1:T ,x1:T , z0) q(d1:T |x1:T ,u1:T ,d0)\ufe38 \ufe37\ufe37 \ufe38 = p\u03b8d (d1:T |u1:T ,d0) . (5)\nThis choice exactly approximates one delta-function by itself, and simplifies the ELBO by letting them cancel out. By further taking the outer average in (4), one obtains\nFi(\u03b8, \u03c6) = Eq\u03c6 [ log p\u03b8(x1:T |z1:T , d\u03031:T ) ] \u2212KL ( q\u03c6(z1:T |d\u03031:T ,x1:T , z0) \u2225\u2225 p\u03b8(z1:T |d\u03031:T , z0)) , (6)\nwhich still depends on \u03b8d, u1:T and d0 via d\u03031:T . The first term is an expected log likelihood under q\u03c6(z1:T |d\u03031:T ,x1:T , z0), while KL denotes the Kullback-Leibler divergence between two distributions. Having stated the second factor in (5), we now turn our attention to parameterizing the first factor in (5) to resemble its posterior equivalent, by exploiting the temporal structure of p\u03b8."}, {"heading": "3.2 Exploiting the temporal structure", "text": "The true posterior distribution of the stochastic states z1:T , given both the data and the deterministic states d1:T , factorizes as p\u03b8(z1:T |d1:T ,x1:T ,u1:T , z0) = \u220f t p\u03b8(zt|zt\u22121,dt:T ,xt:T ). This can be verified by considering the conditional independence properties of the graphical model in Figure 2a using d-separation [14]. This shows that, knowing zt\u22121, the posterior distribution of zt does not depend on the past outputs and deterministic states, but only on the present and future ones; this was also noted in [22]. Instead of factorizing q\u03c6 as a mean-field approximation across time steps, we keep the structured form of the posterior factors, including zt\u2019s dependence on zt\u22121, in the variational approximation\nq\u03c6(z1:T |d1:T ,x1:T , z0) = \u220f t q\u03c6(zt|zt\u22121,dt:T ,xt:T ) = \u220f t q\u03c6z(zt|zt\u22121,at = g\u03c6a(at+1, [dt,xt])) , (7) where [dt,xt] is the concatenation of the vectors dt and xt. The graphical model for the inference network is shown in Figure 2b. Apart from the direct dependence of the posterior approximation at time t on both dt:T and xt:T , the distribution also depends on d1:t\u22121 and x1:t\u22121 through zt\u22121. We mimic each posterior factor\u2019s nonlinear long-term dependence on dt:T and xt:T through a backwardsrecurrent function g\u03c6a , shown in (7), which we will return to in greater detail in Section 3.3. The inference network in Figure 2b is therefore parameterized by \u03c6 = {\u03c6z, \u03c6a} and \u03b8d. In (7) all time steps are taken into account when constructing the variational approximation at time t; this can therefore be seen as a smoothing problem. In our experiments we also consider filtering,\nwhere only the information up to time t is used to define q\u03c6(zt|zt\u22121,dt,xt). As the parameters \u03c6 are shared across time steps, we can easily handle sequences with variable length in both cases.\nAs both the generative model and inference network factorize over time steps in (1) and (7), the ELBO in (6) separates as a sum over the time steps\nFi(\u03b8, \u03c6) = \u2211 t Eq\u2217\u03c6(zt\u22121) [ Eq\u03c6(zt|zt\u22121,d\u0303t:T ,xt:T ) [ log p\u03b8(xt|zt, d\u0303t) ] +\n\u2212KL ( q\u03c6(zt|zt\u22121, d\u0303t:T ,xt:T ) \u2225\u2225 p\u03b8(zt|zt\u22121, d\u0303t))] , (8) where q\u2217\u03c6(zt\u22121) denotes the marginal distribution of zt\u22121 in the variational approximation to the posterior q\u03c6(z1:t\u22121|d\u03031:T ,x1:T , z0), given by\nq\u2217\u03c6(zt\u22121) = \u222b q\u03c6(z1:t\u22121|d\u03031:T ,x1:T , z0) dz1:t\u22122 = Eq\u2217\u03c6(zt\u22122) [ q\u03c6(zt\u22121|zt\u22122, d\u0303t\u22121:T ,xt\u22121:T ) ] .\n(9) We can interpret (9) as having a VAE at each time step t, with the VAE being conditioned on the past through the stochastic variable zt\u22121. To compute (8), the dependence on zt\u22121 needs to be integrated out, using our posterior knowledge at time t\u2212 1 which is given by q\u2217\u03c6(zt\u22121). We approximate the outer expectation in (8) using a Monte Carlo estimate, as samples from q\u2217\u03c6(zt\u22121) can be efficiently obtained by ancestral sampling. The sequential formulation of the inference model in (7) allows such samples to be drawn and reused, as given a sample z(s)t\u22122 from q \u2217 \u03c6(zt\u22122), a sample z (s) t\u22121 from q\u03c6(zt\u22121|z(s)t\u22122, d\u0303t\u22121:T ,xt\u22121:T ) will be distributed according to q\u2217\u03c6(zt\u22121)."}, {"heading": "3.3 Parameterization of the inference network", "text": "The variational distribution q\u03c6(zt|zt\u22121,dt:T ,xt:T ) needs to approximate the dependence of the true posterior p\u03b8(zt|zt\u22121,dt:T ,xt:T ) on dt:T and xt:T , and as alluded to in (7), this is done by running a RNN with inputs d\u0303t:T and xt:T backwards in time. Specifically, we initialize the hidden state of the backwards-recursive RNN in Figure 2b as aT+1 = 0, and recursively compute at = g\u03c6a(at+1, [d\u0303t,xt]). The function g\u03c6a represents a recurrent neural network with, for example, LSTM or GRU units. Each sequence\u2019s variational approximation factorizes over time with q\u03c6(z1:T |d1:T ,x1:T , z0) = \u220f t q\u03c6z(zt|zt\u22121,at), as shown in (7). We let q\u03c6z(zt|zt\u22121,at) be a Gaussian with diagonal covariance, whose mean and the log-variance are parameterized with \u03c6z as\n\u00b5 (q) t = NN (q) 1 (zt\u22121,at) , logv (q) t = NN (q) 2 (zt\u22121,at) . (10)\nInstead of smoothing, we can also do filtering by using a neural network to approximate the dependence of the true posterior p\u03b8(zt|zt\u22121,dt,xt) on dt and xt, through for instance at = NN(a)(dt,xt).\nImproving the posterior approximation. In our experiments we found that during training, the parameterization introduced in (10) can lead to small values of the KL term KL(q\u03c6(zt|zt\u22121,at) \u2016 p\u03b8(zt|zt\u22121, d\u0303t)) in the ELBO in (8). This happens when g\u03c6 in the inference network does not rely on the information propagated back from future outputs in at, but it is mostly using the hidden state d\u0303t to imitate the behavior of the prior. The inference network could therefore get stuck by trying to optimize the ELBO through sampling from the prior of the model, making the variational approximation to the posterior useless. To overcome this issue, we directly include some knowledge of the predictive prior dynamics in the parameterization of the inference network, using our approximation of the posterior distribution q\u2217\u03c6(zt\u22121) over the previous latent states. In the spirit of sequential Monte Carlo methods [11], we improve the parameterization of q\u03c6(zt|zt\u22121,at) by using q\u2217\u03c6(zt\u22121) from (9). As we are constructing the variational distribution sequentially, we approximate the predictive prior mean, i.e. our \u201cbest guess\u201d on the prior dynamics of zt, as\n\u00b5\u0302 (p) t =\n\u222b NN\n(p) 1 (zt\u22121,dt) p(zt\u22121|x1:T ) dzt\u22121 \u2248\n\u222b NN\n(p) 1 (zt\u22121,dt) q \u2217 \u03c6(zt\u22121) dzt\u22121 , (11)\nwhere we used the parameterization of the prior distribution in (3). We estimate the integral required to compute \u00b5\u0302(p)t by reusing the samples that were needed for the Monte Carlo estimate of the ELBO\nin (8). This predictive prior mean can then be used in the parameterization of the mean of the variational approximation q\u03c6(zt|zt\u22121,at),\n\u00b5 (q) t = \u00b5\u0302 (p) t +NN (q) 1 (zt\u22121,at) , (12)\nAlgorithm 1 Inference of SRNN with Resq parameterization from (12).\n1: inputs: d\u03031:T and a1:T 2: Initialize z0 3: for t = 1 to T do 4: \u00b5\u0302(p)t = NN (p) 1 (zt\u22121, d\u0303t) 5: \u00b5(q)t = \u00b5\u0302 (p) t +NN (q) 1 (zt\u22121,at) 6: logv(q)t = NN (q) 2 (zt\u22121,at) 7: zt \u223c N (zt;\u00b5(q)t ,v (q) t ) 8: end for\nand we refer to this parameterization as Resq in the results in Section 4. Rather than directly learning \u00b5(q)t , we learn the residual between \u00b5\u0302(p)t and \u00b5 (q) t . It is straightforward to show that with this parameterization the KL-term in (8) will not depend on \u00b5\u0302(p)t , but only NN (q) 1 (zt\u22121,at). Learning the residual improves inference, making it seemingly easier for the inference network to track changes in the generative model while the model is trained, as it will only have to learn how to \u201ccorrect\u201d the predictive prior dynamics by using the information coming from d\u0303t:T and xt:T . We did not see any improvement in results by parameterizing logv(q)t in a similar way. The inference procedure of SRNN with Resq parameterization for one sequence is summarized in Algorithm 1."}, {"heading": "4 Results", "text": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16]. We test SRNN on the Blizzard [19] and TIMIT raw audio data sets (Table 1) used in [8]. The preprocessing of the data sets and the testing performance measures are identical to those reported in [8]. Blizzard is a dataset of 300 hours of English, spoken by a single female speaker. TIMIT is a dataset of 6300 English sentences read by 630 speakers. As done in [8], for Blizzard we report the average log-likelihood for half-second sequences and for TIMIT we report the average log likelihood per sequence for the test set sequences. Note that the sequences in the TIMIT test set are on average 3.1s long, and therefore 6 times longer than those in Blizzard. For the raw audio datasets we use a fully factorized Gaussian output distribution. Additionally, we test SRNN for modeling sequences of polyphonic music (Table 2), using the four data sets of MIDI songs introduced in [4]. Each data set contains more than 7 hours of polyphonic music of varying complexity: folk tunes (Nottingham data set), the four-part chorales by J. S. Bach (JSB chorales), orchestral music (MuseData) and classical piano music (Piano-midi.de). For polyphonic music we use a Bernoulli output distribution to model the binary sequences of piano notes.\nAll models where implemented using Theano [2], Lasagne [10] and Parmesan2. Training using a NVIDIA Titan X GPU took around 1.5 hours for TIMIT, 18 hours for Blizzard, less than 15 minutes for the JSB chorales and Piano-midi.de data sets, and around 30 minutes for the Nottingham and MuseData data sets. To reduce the computational requirements we use only 1 sample to approximate all the intractable expectations in the ELBO (notice that the KL term can be computed analytically). Further implementation and experimental details can be found in the Supplementary Material.\nBlizzard and TIMIT. Table 1 compares the average log-likelihood per test sequence of SRNN to the results from [8]. For RNNs and VRNNs the authors of [8] test two different output distributions, namely a Gaussian distribution (Gauss) and a Gaussian Mixture Model (GMM). VRNN-I differs from the VRNN in that the prior over the latent variables is independent across time steps, and it is therefore similar to STORN [3]. For SRNN we compare the smoothing and filtering performance (denoted as smooth and filt in Table 1), both with the residual term in (12) and without it in (10) (denoted as Resq if present). We prefer to only report the more conservative evidence lower bound for SRNN, as the approximation of the log-likelihood using standard importance sampling is known to be difficult to compute accurately in the sequential setting [11]. We see from Table 1 that SRNN outperforms all the competing methods for speech modeling. As the test sequences in TIMIT are on average more than 6 times longer than the ones for Blizzard, the results obtained with SRNN for TIMIT are in line with those obtained for Blizzard. The VRNN, which performs well when the voice\n2https://github.com/casperkaae/parmesan. The code for SRNN will be made available online.\nModels Blizzard TIMIT SRNN\n(smooth+Resq) \u226511991 \u2265 60550 SRNN (smooth) \u2265 10991 \u2265 59269 SRNN (filt+Resq) \u2265 10572 \u2265 52126 SRNN (filt) \u2265 10846 \u2265 50524\nVRNN-GMM \u2265 9107 \u2265 28982 \u2248 9392 \u2248 29604 VRNN-Gauss \u2265 9223 \u2265 28805 \u2248 9516 \u2248 30235 VRNN-I-Gauss \u2265 8933 \u2265 28340 \u2248 9188 \u2248 29639\nRNN-GMM 7413 26643 RNN-Gauss 3539 -1900\nTable 1: Average log-likelihood per sequence on the test sets. For TIMIT the average test set length is 3.1s, while the Blizzard sequences are all 0.5s long. The non-SRNN results are reported as in [8]. Smooth: g\u03c6a is a GRU running backwards; filt: g\u03c6a is a feed-forward network; Resq: parameterization with residual in (12).\nA vg\n. K L\n(q ||p\n) R\naw s\nig na\nl\nExample 1 Example 2\nR ec\non . \u00b5\n0.0 0.5 1.0 Seconds\nR ec\non . l\nog \u03c3\n2\n0.0 0.5 1.0 Seconds\nFigure 3: Visualization of the average KL term and reconstructions of the output mean and log-variance for two examples from the Blizzard test set.\nof the single speaker from Blizzard is modeled, seems to encounter difficulties when modeling the 630 speakers in the TIMIT data set. As expected, for SRNN the variational approximation that is obtained when future information is also used (smoothing) is better than the one obtained by filtering. Learning the residual between the prior mean and the mean of the variational approximation, given in (12), further improves the performance in 3 out of 4 cases.\nIn the first two lines of Figure 3 we plot two raw signals from the Blizzard test set and the average KL term between the variational approximation and the prior distribution. We see that the KL term increases whenever there is a transition in the raw audio signal, meaning that the inference network is using the information coming from the output symbols to improve inference. Finally, the reconstructions of the output mean and log-variance in the last two lines of Figure 3 look consistent with the original signal.\nPolyphonic music. Table 2 compares the average log-likelihood on the test sets obtained with SRNN and the models introduced in [3, 4, 13, 16]. As done for the speech data, we prefer to report the more conservative estimate of the ELBO in Table 2, rather than approximating the log-likelihood with importance sampling as some of the other methods do. We see that SRNN performs comparably to other state of the art methods in all four data sets. We report the results using smoothing and learning the residual between the mean of the predictive prior and the one of the variational approximation, but the performances using filtering and learning directly the mean of the variational approximation are now similar. We believe that this is due to the small amount of data and the fact that modeling MIDI music is much simpler than modeling raw speech signals."}, {"heading": "5 Related work", "text": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16]. The performances of these models are highly dependent on how the dependence among stochastic units is modeled over time, on the type of interaction between stochastic units and deterministic ones, and on the procedure that is used to evaluate the typically intractable log likelihood. Figure 4 highlights how SRNN differs from some of these works.\nIn STORN [3] (Figure 4a) and DRAW [15] the stochastic units at each time step have an isotropic Gaussian prior and are independent between time steps. The stochastic units are used as an input to the deterministic units in a RNN. As in our work, the reparametrization trick [21, 26] is used to optimize an ELBO.\ndtdt\u22121\nxt\nut\nzt\n(a) STORN\nzt\u22121\ndtdt\u22121\nzt\nxt\nut\n(b) VRNN\nzt\u22121 zt\nxt\nut\n(c) Deep Kalman Filter\nunits allows us to improve the posterior approximation by doing smoothing, as the stochastic units still depend on each other when we condition on d1:T . In the VRNN, on the other hand, the stochastic units are conditionally independent given the states d1:T . Because the inference and generative networks in the VRNN share the deterministic units, the variational approximation would not improve by making it dependent on the future through at, when calculated with a backward GRU, as we do in our model. Unlike STORN, DRAW and VRNN, the SRNN separates the \u201cnoisy\u201d stochastic units from the deterministic ones, forming an entire layer of interconnected stochastic units. We found in practice that this gave better performance and was easier to train. The works by [1, 22] (Figure 4c) show that it is possible to improve inference in SSMs by using ideas from VAEs, similar to what is done in the stochastic part (the top layer) of SRNN. Towards the periphery of related works, [16] approximates the log likelihood of a SSM with sequential Monte Carlo, by learning flexible proposal distributions parameterized by deep networks, while [13] uses a recurrent model with discrete stochastic units that is optimized using the NVIL algorithm [23]."}, {"heading": "6 Conclusion", "text": "This work has shown how to extend the modeling capabilities of recurrent neural networks by combining them with nonlinear state space models. Inspired by the independence properties of the intractable true posterior distribution over the latent states, we designed an inference network in a principled way. The variational approximation for the stochastic layer was improved by using the information coming from the whole sequence and by using the Resq parameterization to help the inference network to track the non-stationary posterior. SRNN achieves state of the art performances in the Blizzard and TIMIT speech data set, and performs comparably to competing methods for polyphonic music modeling."}, {"heading": "Acknowledgements", "text": "We thank Casper Kaae S\u00f8nderby and Lars Maal\u00f8e for many fruitful discussions, and NVIDIA Corporation for the donation of TITAN X and Tesla K40 GPUs. Marco Fraccaro is supported by Microsoft Research through its PhD Scholarship Programme."}, {"heading": "A Experimental setup", "text": "A.1 Blizzard and TIMIT\nThe sampling rate is 16KHz and the raw audio signal is normalized using the global mean and standard deviation of the traning set. We split the raw audio signals in chunks of 2 seconds. The waveforms are then divided into non-overlapping vectors of size 200. The RNN thus runs for 160 steps3. The model is trained to predict the next vector (xt) given the current one (ut). During training we use backpropagation through time (BPTT) for 0.5 seconds, i.e we have 4 updates for each 2 seconds of audio. For the first 0.5 second we initialize hidden units with zeros and for the subsequent 3 chunks we use the previous hidden states as initialization.\nFor Blizzard we split the data using 90% for training, 5% for validation and 5% for testing. For testing we report the average log-likelihood per 0.5s sequences. For TIMIT we use the predefined test set for testing and split the rest of the data into 95% for training and 5% for validation. The training and testing setup are identical to the ones for Blizzard. For TIMIT the test sequences have variable length and are on average 3.1s, i.e. more than 6 times longer than Blizzard.\nWe model the output using a fully factorized Gaussian distribution for p\u03b8x(xt|zt,dt). The deterministic RNNs use GRUs [7], with 2048 units for Blizzard and 1024 units for TIMIT. In both cases, zt is a 256-dimensional vector. All the neural networks have 2 layers, with 1024 units for Blizzard and 512 for TIMIT, and use leaky rectified nonlinearities with leakiness 13 and clipped at \u00b13. In both generative and inference models we share a neural network to extract features from the raw audio signal. The sizes of the models were chosen to roughly match the number of parameters used in [8]. In all experiments it was fundamental to gradually introduce the KL term in the ELBO, as shown in [5, 28, 25]. We therefore multiply a temperature \u03b2 to the KL term, i.e. \u03b2KL, and linearly increase \u03b2 from 0.2 to 1 in the beginning of training (for Blizzard we increase it by 0.0001 after each update, while for TIMIT by 0.0003). In both data sets we used the ADAM optimizer [20]. For Blizzard we use a learning rate of 0.0003 and batch size of 128, for TIMIT they are 0.001 and 64 respectively.\nA.2 Polyphonic music\nWe use the same model architecture as in Section 4, except for the output Bernoulli variables used to model the active notes. We reduced the number of parameters in the model to 300 deterministic hidden units for the GRU networks, and 100 stochastic units whose distributions are parameterized with neural networks with 1 layer of 500 units.\n32s\u00b716Khz / 200 = 160"}], "references": [{"title": "Black box variational inference for state space models", "author": ["E. Archer", "I.M. Park", "L. Buesing", "J. Cunningham", "L. Paninski"], "venue": "arXiv:1511.07367", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "arXiv:1211.5590", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning stochastic recurrent networks", "author": ["J. Bayer", "C. Osendorfer"], "venue": "arXiv:1411.7610", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "arXiv:1206.6392", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating sentences from a continuous space", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": "arXiv:1511.06349", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, pages 1724\u20131734", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv:1412.3555", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A recurrent latent variable model for sequential data", "author": ["J. Chung", "K. Kastner", "L. Dinh", "K. Goel", "A.C. Courville", "Y. Bengio"], "venue": "NIPS, pages 2962\u20132970", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B, 39(1)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1977}, {"title": "and A", "author": ["S. Dieleman", "J. Schl\u00fcter", "C. Raffel", "E. Olson", "S.K. S\u00f8nderby", "D. Nouri", "E. Battenberg"], "venue": "van den Oord. Lasagne: First release", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "An introduction to sequential Monte Carlo methods. In Sequential Monte Carlo Methods in Practice, Statistics for Engineering and Information", "author": ["A. Doucet", "N. de Freitas", "N. Gordon"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Variational recurrent auto-encoders", "author": ["O. Fabius", "J.R. van Amersfoort"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Deep temporal sigmoid belief networks for sequence modeling", "author": ["Z. Gan", "C. Li", "R. Henao", "D.E. Carlson", "L. Carin"], "venue": "NIPS, pages 2458\u20132466", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Identifying independence in Bayesian networks", "author": ["D. Geiger", "T. Verma", "J. Pearl"], "venue": "Networks, 20:507\u2013534", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "ICML", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural adaptive sequential Monte Carlo", "author": ["S. Gu", "Z. Ghahramani", "R.E. Turner"], "venue": "NIPS, pages 2611\u20132619", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine Learning, 37(2):183\u2013233", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "The Blizzard challenge 2013", "author": ["S. King", "V. Karaiskos"], "venue": "The Ninth Annual Blizzard Challenge", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-encoding variational Bayes", "author": ["D. Kingma", "M. Welling"], "venue": "ICLR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Kalman filters", "author": ["R.G. Krishnan", "U. Shalit", "D. Sontag"], "venue": "arXiv:1511.05121", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "arXiv:1402.0030", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Variational Bayesian inference with stochastic search", "author": ["J.W. Paisley", "D.M. Blei", "M.I. Jordan"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Building blocks for variational Bayesian learning of latent variable models", "author": ["T. Raiko", "H. Valpola", "M. Harva", "J. Karhunen"], "venue": "Journal of Machine Learning Research, 8:155\u2013201", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ICML, pages 1278\u20131286", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "A unifying review of linear Gaussian models", "author": ["S. Roweis", "Z. Ghahramani"], "venue": "Neural Computation, 11(2):305\u201345", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 5, "context": "Recurrent neural networks (RNNs) are able to represent long-term dependencies in sequential data, by adapting and propagating a deterministic hidden (or latent) state [6, 17].", "startOffset": 167, "endOffset": 174}, {"referenceID": 16, "context": "Recurrent neural networks (RNNs) are able to represent long-term dependencies in sequential data, by adapting and propagating a deterministic hidden (or latent) state [6, 17].", "startOffset": 167, "endOffset": 174}, {"referenceID": 2, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 3, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 7, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 11, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 12, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 15, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 26, "context": "Although SSMs have an illustrious history [27], their stochasticity has limited their widespread use in the deep learning community, as inference can only be exact for two relatively simple classes of SSMs, namely hidden Markov models and linear Gaussian models, neither of which are well-suited to modeling long-term dependencies and complex probability distributions over high-dimensional sequences.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "On the other hand modern RNNs rely on gated nonlinearities such as long short-term memory (LSTM) [17] cells or gated recurrent units (GRUs) [7], that let the deterministic hidden state of the RNN act as an internal memory for the model.", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "On the other hand modern RNNs rely on gated nonlinearities such as long short-term memory (LSTM) [17] cells or gated recurrent units (GRUs) [7], that let the deterministic hidden state of the RNN act as an internal memory for the model.", "startOffset": 140, "endOffset": 143}, {"referenceID": 20, "context": "We use recent advances in variational inference to efficiently approximate the intractable posterior distribution over the latent states with an inference network [21, 26].", "startOffset": 163, "endOffset": 171}, {"referenceID": 25, "context": "We use recent advances in variational inference to efficiently approximate the intractable posterior distribution over the latent states with an inference network [21, 26].", "startOffset": 163, "endOffset": 171}, {"referenceID": 7, "context": "Unlike the VRNN [8], zt directly depends on zt\u22121, as it does in a SSM, via p\u03b8z(zt|zt\u22121,dt).", "startOffset": 16, "endOffset": 19}, {"referenceID": 17, "context": "bound (ELBO) F(\u03b8, \u03c6) = \u2211 i Fi(\u03b8, \u03c6) \u2264 L(\u03b8) with respect to both \u03b8 and the variational parameters \u03c6 [18].", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "Each sequence\u2019s approximation q\u03c6 shares parameters \u03c6 with all others, to form the auto-encoding variational Bayes inference network or variational auto encoder (VAE) [21, 26] shown in Figure 2b.", "startOffset": 166, "endOffset": 174}, {"referenceID": 25, "context": "Each sequence\u2019s approximation q\u03c6 shares parameters \u03c6 with all others, to form the auto-encoding variational Bayes inference network or variational auto encoder (VAE) [21, 26] shown in Figure 2b.", "startOffset": 166, "endOffset": 174}, {"referenceID": 20, "context": "All the intractable expectations in (4) would typically be approximated by sampling, using the reparameterization trick [21, 26] or control variates [24] to obtain low-variance estimators of its gradients.", "startOffset": 120, "endOffset": 128}, {"referenceID": 25, "context": "All the intractable expectations in (4) would typically be approximated by sampling, using the reparameterization trick [21, 26] or control variates [24] to obtain low-variance estimators of its gradients.", "startOffset": 120, "endOffset": 128}, {"referenceID": 23, "context": "All the intractable expectations in (4) would typically be approximated by sampling, using the reparameterization trick [21, 26] or control variates [24] to obtain low-variance estimators of its gradients.", "startOffset": 149, "endOffset": 153}, {"referenceID": 8, "context": "Iteratively maximizing F over \u03b8 and \u03c6 separately would yield an expectation maximization-type algorithm, which has formed a backbone of statistical modeling for many decades [9].", "startOffset": 174, "endOffset": 177}, {"referenceID": 13, "context": "This can be verified by considering the conditional independence properties of the graphical model in Figure 2a using d-separation [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 21, "context": "This shows that, knowing zt\u22121, the posterior distribution of zt does not depend on the past outputs and deterministic states, but only on the present and future ones; this was also noted in [22].", "startOffset": 190, "endOffset": 194}, {"referenceID": 10, "context": "In the spirit of sequential Monte Carlo methods [11], we improve the parameterization of q\u03c6(zt|zt\u22121,at) by using q\u2217 \u03c6(zt\u22121) from (9).", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16].", "startOffset": 205, "endOffset": 223}, {"referenceID": 7, "context": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16].", "startOffset": 205, "endOffset": 223}, {"referenceID": 11, "context": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16].", "startOffset": 205, "endOffset": 223}, {"referenceID": 12, "context": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16].", "startOffset": 205, "endOffset": 223}, {"referenceID": 15, "context": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16].", "startOffset": 205, "endOffset": 223}, {"referenceID": 18, "context": "We test SRNN on the Blizzard [19] and TIMIT raw audio data sets (Table 1) used in [8].", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "We test SRNN on the Blizzard [19] and TIMIT raw audio data sets (Table 1) used in [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "The preprocessing of the data sets and the testing performance measures are identical to those reported in [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "As done in [8], for Blizzard we report the average log-likelihood for half-second sequences and for TIMIT we report the average log likelihood per sequence for the test set sequences.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "Additionally, we test SRNN for modeling sequences of polyphonic music (Table 2), using the four data sets of MIDI songs introduced in [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "All models where implemented using Theano [2], Lasagne [10] and Parmesan2.", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "All models where implemented using Theano [2], Lasagne [10] and Parmesan2.", "startOffset": 55, "endOffset": 59}, {"referenceID": 7, "context": "Table 1 compares the average log-likelihood per test sequence of SRNN to the results from [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "For RNNs and VRNNs the authors of [8] test two different output distributions, namely a Gaussian distribution (Gauss) and a Gaussian Mixture Model (GMM).", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "VRNN-I differs from the VRNN in that the prior over the latent variables is independent across time steps, and it is therefore similar to STORN [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 10, "context": "We prefer to only report the more conservative evidence lower bound for SRNN, as the approximation of the log-likelihood using standard importance sampling is known to be difficult to compute accurately in the sequential setting [11].", "startOffset": 229, "endOffset": 233}, {"referenceID": 7, "context": "The non-SRNN results are reported as in [8].", "startOffset": 40, "endOffset": 43}, {"referenceID": 12, "context": "The TSBN results are from [13], NASMC from [16], STORN from [3], RNN-NADE and RNN from [4].", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "The TSBN results are from [13], NASMC from [16], STORN from [3], RNN-NADE and RNN from [4].", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "The TSBN results are from [13], NASMC from [16], STORN from [3], RNN-NADE and RNN from [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "The TSBN results are from [13], NASMC from [16], STORN from [3], RNN-NADE and RNN from [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Table 2 compares the average log-likelihood on the test sets obtained with SRNN and the models introduced in [3, 4, 13, 16].", "startOffset": 109, "endOffset": 123}, {"referenceID": 3, "context": "Table 2 compares the average log-likelihood on the test sets obtained with SRNN and the models introduced in [3, 4, 13, 16].", "startOffset": 109, "endOffset": 123}, {"referenceID": 12, "context": "Table 2 compares the average log-likelihood on the test sets obtained with SRNN and the models introduced in [3, 4, 13, 16].", "startOffset": 109, "endOffset": 123}, {"referenceID": 15, "context": "Table 2 compares the average log-likelihood on the test sets obtained with SRNN and the models introduced in [3, 4, 13, 16].", "startOffset": 109, "endOffset": 123}, {"referenceID": 2, "context": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16].", "startOffset": 106, "endOffset": 124}, {"referenceID": 7, "context": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16].", "startOffset": 106, "endOffset": 124}, {"referenceID": 11, "context": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16].", "startOffset": 106, "endOffset": 124}, {"referenceID": 12, "context": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16].", "startOffset": 106, "endOffset": 124}, {"referenceID": 15, "context": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16].", "startOffset": 106, "endOffset": 124}, {"referenceID": 2, "context": "In STORN [3] (Figure 4a) and DRAW [15] the stochastic units at each time step have an isotropic Gaussian prior and are independent between time steps.", "startOffset": 9, "endOffset": 12}, {"referenceID": 14, "context": "In STORN [3] (Figure 4a) and DRAW [15] the stochastic units at each time step have an isotropic Gaussian prior and are independent between time steps.", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "As in our work, the reparametrization trick [21, 26] is used to optimize an ELBO.", "startOffset": 44, "endOffset": 52}, {"referenceID": 25, "context": "As in our work, the reparametrization trick [21, 26] is used to optimize an ELBO.", "startOffset": 44, "endOffset": 52}, {"referenceID": 7, "context": "The authors of the VRNN [8] (Figure 4b) note that it is beneficial to add information coming from the past states to the prior over latent variables zt.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "The works by [1, 22] (Figure 4c) show that it is possible to improve inference in SSMs by using ideas from VAEs, similar to what is done in the stochastic part (the top layer) of SRNN.", "startOffset": 13, "endOffset": 20}, {"referenceID": 21, "context": "The works by [1, 22] (Figure 4c) show that it is possible to improve inference in SSMs by using ideas from VAEs, similar to what is done in the stochastic part (the top layer) of SRNN.", "startOffset": 13, "endOffset": 20}, {"referenceID": 15, "context": "Towards the periphery of related works, [16] approximates the log likelihood of a SSM with sequential Monte Carlo, by learning flexible proposal distributions parameterized by deep networks, while [13] uses a recurrent model with discrete stochastic units that is optimized using the NVIL algorithm [23].", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "Towards the periphery of related works, [16] approximates the log likelihood of a SSM with sequential Monte Carlo, by learning flexible proposal distributions parameterized by deep networks, while [13] uses a recurrent model with discrete stochastic units that is optimized using the NVIL algorithm [23].", "startOffset": 197, "endOffset": 201}, {"referenceID": 22, "context": "Towards the periphery of related works, [16] approximates the log likelihood of a SSM with sequential Monte Carlo, by learning flexible proposal distributions parameterized by deep networks, while [13] uses a recurrent model with discrete stochastic units that is optimized using the NVIL algorithm [23].", "startOffset": 299, "endOffset": 303}], "year": 2016, "abstractText": "How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model\u2019s posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.", "creator": "LaTeX with hyperref package"}}}