{"id": "1605.07779", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Neural Universal Discrete Denoiser", "abstract": "rsis We lustige present 249.95 a new d'exploitation framework m27 of buschschulte applying cistercian deep brussel neural :p networks (1995-97 DNN) to devise b&c a judaic universal discrete 14f denoiser. Unlike 21-12 other k\u00fcnstler approaches that utilize ofahengaue supervised learning kollias for mcelrath denoising, an\u010di\u0107 we 4,341 do microdrives not require shayne any additional weeded training haglund data. estaci\u00f3n In salote such setting, euro207 while 1403 the ground - nuami truth 3:26.00 label, beckett i. dma.org e. , jarek the clean data, gatemouth is not 1,825 available, we cochin devise \" pseudo - backlit labels \" and a novel objective petersen function husaini such that 85-billion DNN haly can tindall be trained stadium in tenaglia a pimentel same 19-26 way as kjartan supervised learning fascinate to become a discrete denoiser. We encase experimentally show that tudors our resulting \u4e2d\u66f8\u7701 algorithm, dubbed kye as germaniae Neural benefitted DUDE, significantly munyenyembe outperforms the previous easterly state - uttrakhand of - unprofessional the - d\u2019art art in several microprocessor applications with kyaukphyu a systematic rule of kulturbund choosing 35-4 the bonnet hyperparameter, dunkeld which is fira-aer an attractive winkle feature wi\u0144sko in mini-cons practice.", "histories": [["v1", "Wed, 25 May 2016 08:50:21 GMT  (590kb,D)", "https://arxiv.org/abs/1605.07779v1", "Submitted to NIPS 2016"], ["v2", "Wed, 24 Aug 2016 01:50:04 GMT  (590kb,D)", "http://arxiv.org/abs/1605.07779v2", "Accepted to NIPS 2016"]], "COMMENTS": "Submitted to NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["taesup moon", "seonwoo min", "byunghan lee", "sungroh yoon"], "accepted": true, "id": "1605.07779"}, "pdf": {"name": "1605.07779.pdf", "metadata": {"source": "CRF", "title": "Neural Universal Discrete Denoiser", "authors": ["Taesup Moon", "Seonwoo Min", "Byunghan Lee", "Sungroh Yoon"], "emails": ["tsmoon@dgist.ac.kr", "mswzeus@gmail.com", "bhannara@gmail.com", "sryoon@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Cleaning noise-corrupted data, i.e., denoising, is a ubiquotous problem in signal processing and machine learning. Discrete denoising, in particular, focuses on the cases in which both the underlying clean and noisy data take their values in some finite set. Such setting covers several applications in different domains, such as image denoising [17, 14], DNA sequence denoising[12], and channel decoding[16].\nA conventional approach for addressing the denoising problem is the Bayesian approach, which can often yield a computatioanlly efficient algorithm with reasonable performance. However, limitations can arise when the assumed stochastic models do not accurately reflect the real data distribution. Particularly, while the models for the noise can often be obtained relatively reliably, obtaining the accurate model for the original clean data is more tricky; the model for the clean data may be wrong, changing, or may not exist at all.\nIn order to alleviate the above mentioned limitations, [22] proposed a universal approach for discrete denoising. Namely, they first considered a general setting that the clean finite-valued source symbols are corrupted by a discrete memoryless channel (DMC), a noise mechanism that corrupts each source symbol independently and statistically identically. Then, they devised an algorithm called DUDE (Discrete Universal DEnoiser) and showed rigorous performance guarantees for the semi-stochastic setting; namely, that where no stochastic modeling assumptions are made on the underlying source data, while the corruption mechanism is assumed to be governed by a known DMC. DUDE is shown to universally attain the optimum denoising performance for any source data as the data size grows.\nIn addition to the strong theoretical performance guarantee, DUDE can be implemented as a computationally efficient sliding window denoiser; hence, it has been successfully applied and extended to some practical applications, e.g., [17, 12, 16, 14]. However, it also had limitations; namely, the performance is sensitive on the choice of sliding window size k, which has to be hand-tuned without any systematic rule. Moreover, when k becomes large and the alphabet size of the signal increases, DUDE suffers from the data sparsity problem, which significantly deteriorates the performance.\n\u2217Corresponding author.\nar X\niv :1\n60 5.\n07 77\n9v 2\n[ cs\n.L G\n] 2\n4 A\nug 2\n01 6\nIn this paper, we present a novel framework of addressing above limitations of DUDE by adopting the machineries of deep neural networks (DNN) [7], which recently have seen great empirical success in many practical applications. While there have been some previous attempts of applying neural networks to grayscale image denoising [4, 24], they all remained in supervised learning setting, i.e., large-scale training data that consists of clean and noisy image pairs was necessary. Such approach requires significant computation resources and training time and is not always transferable to other denoising applications, in which collecting massive training data is often expensive, e.g., DNA sequence denoising [11].\nHenceforth, we stick to the setting of DUDE, which requires no additional data other than the given noisy data. In this case, however, it is not straightforward to adopt DNN since there is no ground-truth label for supervised training of the networks. Namely, the target label that a denoising algorithm is trying to estimate from the observation is the underlying clean signal, hence, it can never be observed to the algorithm. Therefore, we carefully exploit the known DMC assumption and the finiteness of the data values, and devise \u201cpseudo-labels\u201d for training DNN. They are based on the unbiased estimate of the true loss a denoising algorithm is incurring, and we show that it is possible to train a DNN as a universal discrete denoiser using the devised pseudo-labels and generalized cross-entropy objective function. As a by-product, we also obtain an accurate estimator of the true denoising performance, with which we can systematically choose the appropriate window size k. In results, we experimentally verify that our DNN based denoiser, dubbed as Neural DUDE, can achieve significantly better performance than DUDE maintaining robustness with respect to k. Furthermore, we note that although the work in this paper is focused on discrete denoising, we believe the proposed framework can be extended to the denoising of continuous-valued signal as well, and we defer it to the future work.\nThe rest of the paper is organized as follows. In Section 2, we set up some necessary notations as well as give a more concrete explanation on DUDE and related work. In Secion 3, we give an alternative interpretation of the DUDE algorithm, and Section 4 presents the main idea of Neural DUDE. Section 5 gives the experimental results and concretely examines the benefits of our method. Finally, Section 6 concludes with potential future research directions."}, {"heading": "2 Notations and related work", "text": ""}, {"heading": "2.1 Problem setting of discrete denoising", "text": "Throughout this paper, we will generally denote a sequence (n-tuple) as, e.g., an = (a1, . . . , an), and aji refers to the subsequence (ai, . . . , aj). In discrete denoising problem, we denote the clean, underlying source data as xn and assume each component xi takes a value in some finite set X. The source sequence is corrupted by a DMC and results in a noisy version of the source zn, of which each component zi takes a value in , again, some finite set Z. The DMC is completely characterized by the channel transition matrix \u03a0 \u2208 R|X|\u00d7|Z|, of which the (x, z)-th element, \u03a0(x, z), stands for Pr(Zi = z|Xi = x), i.e., the conditional probability of the noisy symbol taking value z given the original source symbol was x. An essential but natural assumption we make is that \u03a0 is of the full row rank.\nAs shown in Fig. 1, upon observing the entire noisy data zn, a discrete denoiser reconstructs the original data with X\u0302n = (X\u03021(zn), . . . , X\u0302n(zn)), where each reconstructed symbol X\u0302i(zn) also takes its value in a finite set X\u0302. The goodness of the reconstruction by a discrete denoiser X\u0302n is measured by the average loss, LX\u0302n(X n, Zn) = 1n \u2211n i=1 \u039b(xi, X\u0302i(z n)), where \u039b(xi, x\u0302i) is a single-letter loss function that measures the loss incurred by estimating xi with x\u0302i at location i. The loss function can\nbe also represented with a loss matrix \u039b \u2208 R|X|\u00d7|X\u0302|. Throughout the paper, for simplicity, we will assume X = Z = X\u0302, thus, assume that \u03a0 is invertible."}, {"heading": "2.2 Discrete Universal DEnoiser (DUDE)", "text": "DUDE in [22] is a two-pass algorithm that has a linear complexity in the data size n. During the first pass, the algorithm collects the statistics vector\nm[zn, lk, rk](a) = \u2223\u2223{i : k + 1 \u2264 i \u2264 n\u2212 k, zi+ki\u2212k = lkark}\u2223\u2223, (1)\nfor all a \u2208 Z, which is the count of the occurrence of the symbol a \u2208 Z along the noisy sequence zn that has the double-sided context (lk, rk) \u2208 Z2k. Once the m vector is collected, for the second pass, DUDE applies the rule\nX\u0302i,DUDE(z n) = arg min x\u0302\u2208X m[zn, ci] >\u03a0\u22121[\u03bbx\u0302 \u03c0zi ] for each k + 1 \u2264 i \u2264 n\u2212 k, (2)\nwhere ci , (zi\u22121i\u2212k, z i+k i+1 ) is the context of zi, \u03c0zi is the zi-th column of the channel matrix \u03a0, \u03bbx\u0302 is the x\u0302-th column of the loss matrix \u039b, and stands for the element-wise product. The form of (2) shows that DUDE is a sliding window denoiser with window size 2k + 1; namely, DUDE returns the same denoised symbol at all locations i\u2019s with the same value of zi+ki\u2212k . We will call such denoisers as the k-th order sliding window denoiser from now on.\nDUDE is shown to be universal, i.e., for any underlying clean sequence xn, it can always attain the performance of the best k-th order sliding window denoiser as long as k|Z|2k = o(n/ log n) holds [22, Theorem 2]. For more rigorous analyses, we refer to the original paper [22]."}, {"heading": "2.3 Deep neural networks (DNN) and related work", "text": "Deep neural networks (DNN), often dubbed as deep learning algorithms, have recently made significant impacts in several practical applications, such as speech recognition, image recognition, and machine translation, etc. For a thorough review on recent progresses of DNN, we refer the readers to [7] and refereces therein.\nRegarding denoising, [4, 24, 9] have successfully applied the DNN to grayscale image denoising by utilizing supervised learning at the small image patch level. Namely, they generated clean and noisy image patches and trained neural networks to learn a mapping from noisy to clean patches. While such approach attained the state-of-the-art performance, as mentioned in Introduction, it has several limitations. That is, it typically requires massive amount of training data, and multiple copies of the data need to be generated for different noise types and levels to achieve robust performance. Such requirement of large training data cannot be always met in other applications, e.g., in DNA sequence denoising, collecting large scale clean DNA sequences is much more expensive than obtaining training images on the web. Moreover, for image denoising, working in the small patch level makes sense since the image patches may share some textual regularities, but in other applications, the characterstics of the given data for denoising could differ from those in the pre-collected training set. For instance, the characteristics of substrings of DNA sequences vary much across different species and genes, hence, the universal setting makes more sense in DNA sequence denoising."}, {"heading": "3 An alternative interpretation of DUDE", "text": ""}, {"heading": "3.1 Unbiased estimated loss", "text": "In order to make an alternative interpretation of DUDE, which can be also found in [13], we need the tool developed in [23]. To be self-contained, we recap the idea here. Consider a single letter case, namely, a clean symbol x is corrupted by \u03a0 and resulted in the noisy observation Z\u2020. Then, suppose a single-symbol denoiser s : Z\u2192 X\u0302 is applied and obtained the denoised symbol X\u0302 = s(Z). In this case, the true loss incurred by s for the clean symbol x and the noisy observation Z is \u039b(x, s(Z)). It is clear that s cannot evaluate its loss since it does not know what x is, but the following shows an unbiased estimate of the expected true loss, which is only based on Z and s, can be derived.\n\u2020We use uppercase letter to stress it is a random variable\nFirst, denote S as the set of all possible single-symbol denoisers. Note |S| = |X\u0302||Z|. Then, we define a matrix \u03c1 \u2208 R|X|\u00d7|S| with\n\u03c1(x, s) = \u2211 z\u2208Z \u03a0(x, z)\u039b(x, s(z)) = Ex\u039b(x, s(Z)), x \u2208 X, s \u2208 S. (3)\nThen, we can define an estimated loss matrix\u2021 L , \u03a0\u22121\u03c1 \u2208 R|Z|\u00d7|S|. With this definition, we can show that L(Z, s) is an unbiased estimate of Ex\u039b(x, s(Z)) as follows (as shown in [23]): ExL(Z, s) =\n\u2211 z \u03a0(x, z) \u2211 x\u2032 \u03a0\u22121(z, x\u2032)\u03c1(x\u2032, s) = \u03b4(x, x\u2032)\u03c1(x\u2032, s) = \u03c1(x, s) = Ex\u039b(x, s(Z))."}, {"heading": "3.2 DUDE: Minimizing the sum of estimated losses", "text": "As mentioned in Section 2.2, DUDE with context size k is the k-th order sliding window denoiser. Generally, we can denote such k-th order sliding window denoiser as sk : Z2k+1 \u2192 X\u0302, which obtains the reconstruction at the i-th location as\nX\u0302i(z n) = sk(z i+k i\u2212k) = sk(ci, zi). (4)\nTo recall, ci = (zi\u22121i\u2212k, z i+k i+1 ). Now, from the formulation (4), we can interpret that sk defines a single-symbol denoiser at location i, i.e., sk(ci, \u00b7), depending on ci. With this view on sk, as derived in [13], we can show that the DUDE defined in (2) is equivalent to finding a single-symbol denoiser\nsk,DUDE(c, \u00b7) = arg min s\u2208S \u2211 {i:ci=c} L(zi, s), (5)\nfor each context c \u2208 Ck , {(lk, rk) : (lk, rk) \u2208 Z2k} and obtaining the reconstruction at location i as X\u0302i,DUDE(zn) = sk,DUDE(ci, zi). The interpretation (5) gives some intuition on why DUDE enjoys strong theoretical guarantees in [22]; since L(Zi, s) is an unbiased estimate of Exi\u039b(xi, s(Zi)),\u2211\ni\u2208{i:ci=c} L(Zi, s) will concentrate on \u2211\ni\u2208{i:ci=c} \u039b(xi, s(Zi)) as long as |{i : ci = c}| is sufficiently large. Hence, the single symbol denoiser that minimizes the sum of the estimated losses for each c (i.e., (5)) will also make the sum of the true losses small, which is the goal of a denoiser.\nWe can also express (5) using vector notations, which will become useful for deriving the Neural DUDE in the next section. That is, we let \u2206|S| be a probability simplex in R|S|. (Suppose we have uniquely assigned each coordinate of R|S| to each single-symbol denoiser in S from now on.) Then, we can define a probability vector for each c,\np\u0302(c) , arg min p\u2208\u2206|S| ( \u2211 {i:ci=c} 1>ziL ) p, (6)\nwhich will be on the vertex of \u2206|S| that corresponds to sk,DUDE(c, \u00b7) in (5). The reason is because the objective function in (7) is a linear function in p. Hence, we can simply obtain sk,DUDE(c, \u00b7) = arg maxs p\u0302(c)s, where p\u0302(c)s stands for the s-th coordinate of p\u0302(c)."}, {"heading": "4 Neural DUDE: A DNN based discrete denoiser", "text": "As seen in the previous section, DUDE utilizes the estimated loss matrix L, which does not depend on the clean sequence xn. However, the main drawback of DUDE is that, as can be seen in (5), it treats each context c independently from others. Namely, when the context size k grows, then the number of different contexts |Ck| = |Z|2k will grow exponentially with k, hence, the sample size for each context |{i : ci = c}| will decrease exponentially for a given sequence length n. Such phenomenon will hinder the concentration of \u2211 i\u2208{i:ci=c} L(Zi, s) mentioned in the previous section, which causes the performance of DUDE deteriorate when k grows too large.\nIn order to resolve above problem, we develop Neural DUDE, which adopts a single neural network such that the information from similar contexts can be shared via network parameters. We note that our usage of DNN resembles that of the neural language model (NLM) [2], which improved upon the conventional N -gram models. The difference is that NLM is essentially a prediction problem, hence the ground truth label for supervised training is easily availble, but in denoising, this is not the case. Before describing the algorithm more in detail, we need one following lemma. \u2021For general case in which \u03a0 is not a square matrix, \u03a0\u22121 can be replaced with the right inverse of \u03a0."}, {"heading": "4.1 A lemma", "text": "Let R|S|+ be the space of all |S|-dimensional vectors of which elements are nonnegative. Then, for any g \u2208 R|S|+ and any p \u2208 \u2206|S|, define a cost function C(g,p) , \u2212 \u2211|S| i=1 gi log pi, i.e., a generalized cross-entropy function with the first argument not normalized to a probability vector. Note C(g,p) is linear in g and convex in p. Now, following lemma shows another way of obtaining DUDE.\nLemma 1 Define Lnew , \u2212L + Lmax11> in which Lmax , maxz,s L(z, s), the maximum element of L. Using the cost function C(\u00b7, \u00b7) defined above, for each c \u2208 Ck, let us define\np\u2217(c) , arg min p\u2208\u2206|S| \u2211 {i:ci=c} C ( L>new1zi ,p ) .\nThen, we have sk,DUDE(c, \u00b7) = arg maxs p\u2217(c)s.\nProof: Recalling\np\u0302(c) , arg min p\u2208\u2206|S| ( \u2211 {i:ci=c} 1>ziL ) p, (7)\nwe derive\np\u0302(c) = arg max p\u2208\u2206|S| ( \u2211 {i:ci=c} 1>zi(\u2212L + Lmax11 >\ufe38 \ufe37\ufe37 \ufe38\n=Lnew\n) ) p = arg max\np\u2208\u2206|S| ( \u2211 {i:ci=c} L>new1zi )> p (8)\nin which the first equality follows from flipping the sign of L and the fact that arg max does not change by adding a constant to the objective. Furthermore, since C(\u00b7, \u00b7) is linear in the first argument,\np\u2217(c) = arg min p\u2208\u2206|S| \u2211 i\u2208{ci=c} C ( L>new1zi ,p ) = arg min p\u2208\u2206|S| C ( \u2211 i\u2208{ci=c} L>new1zi ,p ) . (9)\nNow, from comparing (8) and (9), and from the fact that \u2211\ni\u2208{ci=c} L > new1zi \u2208 R |S| + , we can show\nthat arg max\ns p\u0302(c)s = arg max s p\u2217(c)s = arg max s ( \u2211\ni\u2208{ci=c}\nL>new1zi)s\nby considering Lagrangian of (9) and applying KKT condition. That is, p\u2217(c) no longer is on one of the vertex of \u2206|S|, but still puts the maximum probability mass on the vertex p\u0302(c). Since sk,DUDE(c, \u00b7) = arg maxs p\u0302(c)s as shown in the previous section, the proof is done."}, {"heading": "4.2 Neural DUDE", "text": "The main idea for Neural DUDE is to use a single neural network to learn the k-th order slinding window denoising rule for all c\u2019s. Namely, we define p(w, \u00b7) : Z2k \u2192 \u2206|S| as a feed-forward neural network that takes the context vector c \u2208 Ck as input and outputs a probability vector on \u2206|S|. We let w stand for all the parameters in the network. The network architecture of p(w, \u00b7) has the softmax output layer, and it is analogous to that used for the multi-class classification. Thus, when the parameters are properly learned, we expect that p(w, ci) will give predictions on which single-symbol denoiser to apply at location i with the context ci."}, {"heading": "4.2.1 Learning", "text": "When not resorting to the supervised learning framework, learning the network parameters w is not straightforward as mentioned in the Introduction. However, inspired by Lemma 1, we define the objective function to minimize for learning w as\nL(w, zn) , 1\nn n\u2211 i=1 C ( L>newIzi ,p(w, ci) ) , (10)\nwhich resembles the widely used cross-entropy objective function in supervised multi-class classification. Namely, in (10), {(ci,L>newIzi)}ni=1, which solely depends on the noisy sequence zn, can\nbe analogously thought of as the input-label pairs in supervised learning\u00a7. But, unlike classification, in which the ground-truth label is given as a one-hot vector, we treat L>newIzi \u2208 R |S| + as a target \u201cpseudo-label\u201d on S.\nOnce the objective function is set as in (10), we can then use the widely used optimization techniques, namely, the back-propagaion and Stochastic Gradient Descent (SGD)-based methods, for learning the parameters w. In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w. Note that there is no notion of generalization in our setting, since the goal of denoising is to simply achieve as small average loss as possible for the given noisy sequence zn, rather than performing well on the separate unseen test data. Hence, we do not use any regularization techniques such as dropout in our learning, but simply try to minimize the objective function."}, {"heading": "4.2.2 Denoising", "text": "After sufficient iterations of weight updates, the objective function (10) will converge, and we will denote the converged parameters as w\u2217. The Neural DUDE algorithm then applies the resulting network p(w\u2217, \u00b7) to the exact same noisy sequence zn used for learning to denoise. Namely, for each c \u2208 Ck, we obtain a single-symbol denoiser\nsk,Neural DUDE(c, \u00b7) = arg max s p(w\u2217, c)s (11)\nand the reconstruction at location i by X\u0302i,DUDE(zn) = sk,Neural DUDE(ci, zi).\nFrom the objective function (10) and the definition (11), it is apparent that Neural DUDE does share information across different contexts since w\u2217 is learnt from all data and shared across all contexts. Such property enables Neural DUDE to robustly run with much larger k\u2019s than DUDE without running into the data sparsity problem. As shown in the experimental section, Neural DUDE with large k can significantly improve the denoising performance compared to DUDE. Furthermore, in the experimental section, we show that the concentration\n1\nn n\u2211 i=1 L(Zi, sk,Neural DUDE(ci, \u00b7)) \u2248 1 n n\u2211 i=1 \u039b(xi, sk,Neural DUDE(ci, Zi)) (12)\nholds with high probability even for very large k\u2019s, whereas such concentration quickly breaks for DUDE as k grows. While deferring the analyses on why such concentration always holds to the future work, we can use the property to provide a systematic mechanism for choosing the best context size k for Neural DUDE - simply choose k\u2217 = arg mink 1n \u2211n i=1 L(Zi, sk,Neural DUDE(ci, \u00b7)). As shown in the experiments, such choice of k for Neural DUDE gives an excellent denoising performace.\nAlgorithm 1 summarizes the Neural DUDE algorithm.\nAlgorithm 1 Neural DUDE algorithm Input: Noisy sequence zn, \u03a0, \u039b, Maximum context size kmax Output: Denoised sequence X\u0302nNeural DUDE = {X\u0302i,Neural DUDE(zn)}ni=1\nCompute L = \u03a0\u22121\u03c1 as in Section 3.1 and Lnew as in Lemma 1 for k = 1, . . . , kmax do\nInitialize p(w, \u00b7) with input dimension 2k|Z| (using one-hot encoding of each noisy symbol) Obtain w\u2217k minimizing L(w, z\nn) in (10) using SGD-like optimization method Obtain sk,Neural DUDE(c, \u00b7) for all c \u2208 Ck as in (11) using w\u2217k Compute Lk , 1n \u2211n i=1 L(zi, sk,Neural DUDE(ci, \u00b7))\nend for Compute the best context size k\u2217 = arg mink=1,...,kmax Lk Obtain X\u0302i,Neural DUDE(zn) = sk\u2217,Neural DUDE(ci, zi) for i = 1, . . . , n\nRemark: One may wonder why we need the cost function in (10) and cannot directly work with a simpler linear objective like (5). The reason is that if we use 1n \u2211n i=1(L >1zi) >p(w, ci) as an objective function for learning w, it becomes highly non-convex in w, and the solution w\u2217 becomes \u00a7For i \u2264 k and i \u2265 n\u2212 k, dummy variables are padded for obtaining ci.\nvery unstable. In constrast, minimizing (10), which has the same functional form as the cross-entropy loss function, yields a stable solution. Note also that using Lnew instead of L in the cost function is important, since L may have negative valued components, hence, the cost function C(\u00b7, \u00b7) will not become convex in the second argument when simply L is used."}, {"heading": "5 Experimental results", "text": "In this section, we show the denoising results of Neural DUDE for the synthetic binary data, real binary images, and real nanopore DNA sequence data. All of our experiments were done with Python 2.7 and Keras package (http://keras.io) with Theano [1] backend."}, {"heading": "5.1 Synthetic binary data", "text": "We first experimented with a simple synthetic binary data to highlight the core strength of Neural DUDE. That is, we assume X = Z = X\u0302 = {0, 1} and \u03a0 is a binary symmetric channel (BSC) with crossover probability \u03b4 = 0.1. We set \u039b as the Hamming loss. We generated the clean binary\nsequence xn of length n = 106 from a binary symmentric Markov chain (BSMC) with transition probability \u03b1 = 0.1. The noise-corrupted sequence zn is generated by passing xn through \u03a0. Since we use the Hamming loss, the average loss of a denoiser X\u0302n, 1n \u2211n i=1 \u039b(xi, X\u0302i(z\nn)), is equal to the bit error rate (BER). Note that in this setting, the noisy sequence zn is a hidden Markov process. Therefore, when the stochastic model of the clean sequence is exactly known to the denoiser, the Viterbi-like Forward-Backward (FB) recursion algorithm can attain the optimum BER.\nFigure 2 shows the denoising results of DUDE and Neural DUDE, which do not know anything about the characteristics of the clean sequence xn. For DUDE, the window size k is the single hyperparameter to choose. For Neural DUDE, we used the feed-forward fully connected neural networks for p(w, \u00b7) and varied the depth of the network between 1 \u223c 4 while also varying k. Neural DUDE(1L) corresponds to the simple linear softmax regression model. For deeper models, we used 40 hidden nodes in each layer with Restricted Linear Unit (ReLU) activations. We used Adam [10] with default setting in Keras as an optimizer to minimize (10). We used the mini-batch size of 100 and ran 10 epochs for learning. The performance of Neural DUDE was robust to the initializtion of the parameters w.\nFigure 2(a) shows the BERs of DUDE and Neural DUDE with respect to varying k. Firstly, we see that minimum BERs of both DUDE and Neural DUDE(4L), i.e., 0.563\u03b4 with k = 5, get very close to the optimum BER (0.558\u03b4) obtained by FB recursion. Secondly, we observe that Neural DUDE quickly approaches the optimum BER as we increase the depth of the network. This shows that as the descriminative power of the model increases with the depth of the network, p(w, \u00b7) can successfully learn the denoising rule for each context c with a shared parameter w. Thirdly, we clearly see that in contrast to the performance of DUDE being sensitive to k, that of Neural DUDE(4L) is robust to k by sharing information across contexts. Such robustness with respect to k is obviously a very desirable property in practice.\nFigure 2(b) and Figure 2(c) plot the average estimated BER, 1n \u2211n\ni=1 L(Zi, sk(ci, \u00b7)), against the true BER for DUDE and Neural DUDE (4L), respectively, to show the concentration phenomenon described in (12). From the figures, we can see that while the estmated BER drastically diverges from\ntrue BER for DUDE as k increases, it strongly concentrates on true BER for Neural DUDE (4L) for all k. This result suggests the concrete rule for selecting the best k described in Algorithm 1. Such rule is used for the experiments using real data in the following subsections."}, {"heading": "5.2 Real binary image denoising", "text": "In this section, we experiment with real, binary image data. The settings of \u03a0 and \u039b are identical to Section 5.1, while the clean sequence was generated by converting image to a 1-D sequence via raster scanning. We tested with 5 representative binary images with various textual characteristics: Einstein, Lena, Barbara, Cameraman, and scanned Shannon paper. Einstein and Shannon images had the resolution of 256\u00d7 256 and the rest had 512\u00d7 512. For Neural DUDE, we tested with 4 layer model with 40 hidden nodes with ReLU activations in each layer.\nFigure 3(b) shows the result of denoising Einstein image in Figure 3(a) for \u03b4 = 0.1. We see that the BER of Neural DUDE(4L) continues to drop as we increase k, whereas DUDE quickly fails to denoise for larger k\u2019s. Furthermore, we observe that the estimated BER of Neural DUDE(4L) again strongly correlates with the true BER. Note that when k = 36, we have 272 possible different contexts, which are much more than the number of pixels, 216(256 \u00d7 256). However, we see that Neural DUDE can still learn a good denoising rule from such many different contexts by aggregating information from similar contexts.\nTable 1 summarizes the denoising results on six binary images for \u03b4 = 0.1, 0.15. We see that Neural DUDE always significantly outperforms DUDE using much larger context size k. We believe this is a significant result since DUDE is shown to outperform many state-of-the-art sliding window denoisers in practice such as median filters [22, 17]. Furthermore, folloiwng DUDE\u2019s extension to grayscale image denoising [14], the result gives strong motivation for extending Neural DUDE to grayscale image denoising."}, {"heading": "5.3 Nanopore DNA sequence denoising", "text": "We now go beyond binary data and apply Neural DUDE to DNA sequence denoising. As surveyed in [11], denoising DNA sequences are becoming increasingly important as the sequencing devices are getting cheaper, but injecting more noise than before.\nFor our experiment, we used simulated MinION nanopore reads, which were generated as follows; we obtained 16S rDNA reference sequences for 20 species [3] and randomly generated noiseless\ntemplate reads from them. The number of reads and read length for each species were set as identical to those of real MinION nanopore reads [3]. Then, based on \u03a0 of nanopore sequencer (Figure 4(a)) obtained in [8] (with 20.375% average error rate), we induced substitution errors to the reads and obtained the corresponding noisy reads. Note that we are only considering substitution errors, while there also exist insertion/deletion errors in real nanopore sequenced data. The reason is that substitution errors can be directly handled by DUDE and Neural DUDE, so we focus on quantitatively evaluating the performance on those errors. We sequentially merged 2,372 reads from 20 species and formed 1-D sequence of 2,469,111 base pairs long. We used two Neural DUDE (4L) models with 40 and 80 hidden nodes in each layer, and denoted as (40-40-40) and (80-80-80), respectively.\nFigure 4(b) shows the denoising results. We observe that Neural DUDE with large k\u2019s (around k = 100) can achieve less than half of the error rate of DUDE. Furthermore, as the complexity of model increases, the performance of Neural DUDE gets significantly better. We could not find right baseline scheme, since most of nanopore error correction tool, e.g., nanocorr [6], did not produce read-by-read correction sequence, but returns downstream analyses results after denoising. Coral [19], which gives read-by-read denoising result for Illumina data, completely failed for nanopore data. Given that DUDE ourperforms state-of-the-art schemes, including Coral, for Illumina sequenced data as shown in [12], we expect the improvement of Neural DUDE over DUDE could translate into fruitful downstream analyses gain for nanopore data."}, {"heading": "6 Concluding remark and future work", "text": "We showed Neural DUDE significantly improves upon DUDE and has a systematic mechanism for choosing the best k. There are several future research directions. First, we plan to do thorough experiments on DNA sequence denoising and quantify the impact of Neural DUDE in the downstream analysis. Second, we plan to give theoretical analyses on the concentration (12) and justify the derived k selection rule. Third, extending the framework to deal with continuous-valued signal and finding connection with SURE principle [20] would be fruitful. Finally, applying recurrent neural networks (RNN) in place of DNNs could be another promising direction."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "JMLR, 3:1137\u20131155", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Species level resolution of 16S rRNA gene amplicons sequenced through MinIONTM portable nanopore sequencer", "author": ["A. Benitez-Paez", "K. Portune", "Y. Sanz"], "venue": "bioRxiv:021758", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "and S", "author": ["H. Burger", "C. Schuler"], "venue": "Harmeling. Image denoising: Can plain neural networks compete with BM3D? In CVPR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, 12:2121\u20132159", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Oxford nanopore sequencing", "author": ["S. Goodwin", "J. Gurtowski", "S Ethe-Sayers", "P. Deshpande", "M. Schatz", "W.R. McCombie"], "venue": "hybrid error correction, and de novo assembly of a eukaryotic genome. Genome Res.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning", "author": ["G. Hinton", "Y. LeCun", "Y. Bengio"], "venue": "Nature, 521:436\u2013444", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved data analysis for the minion nanopore sequencer", "author": ["M. Jain", "I. Fiddes", "K. Miga", "H. Olsen", "B. Paten", "M. Akeson"], "venue": "Nature Methods, 12:351\u2013356", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural image denoising with convolutional networks", "author": ["V. Jain", "H.S. Seung"], "venue": "NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Denoising DNA deep sequencing data\u2013 high-throughput sequencing errors and their corrections", "author": ["D. Laehnemann", "A. Borkhardt", "A.C. McHardy"], "venue": "Brief Bioinform, 17(1):154\u2013179", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "DUDE-Seq: Fast", "author": ["B. Lee", "T. Moon", "S. Yoon", "T. Weissman"], "venue": "flexible, and robust denoising of nucleotide sequences. arXiv:1511.04836", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Discrete denoising with shifts", "author": ["T. Moon", "T. Weissman"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "The iDUDE framework for grayscale image denoising", "author": ["Giovanni Motta", "Erik Ordentlich", "Ignacio Ramirez", "Gadiel Seroussi", "Marcelo J. Weinberger"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A method of solving a convex programming problem with convergence rate o(1/sqr(k))", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady, 27:372\u2013376", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1983}, {"title": "Universal algorithms for channel decoding of uncompressed sources", "author": ["E. Ordentlich", "G. Seroussi", "S. Verd\u00fa", "K. Viswanathan"], "venue": "IEEE Trans. Information Theory, 54(5):2243\u20132262", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "A universal discrete image denoiser and its application to binary images", "author": ["E. Ordentlich", "G. Seroussi", "S. Verd\u00fa", "M.J. Weinberger", "T. Weissman"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics, 4(5):1\u201317", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1964}, {"title": "Correcting errors in short reads by multiple alignments", "author": ["L. Salmela", "J. Schroder"], "venue": "BioInformatics, 27(11):1455\u20131461", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimation of the mean of a multivariate normal distribution", "author": ["C. Stein"], "venue": "The Annals of Statistics, 9(6):1135\u20131151", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1981}, {"title": "RMSProp: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "G. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Universal discrete denoising: Known channel", "author": ["T. Weissman", "E. Ordentlich", "G. Seroussi", "S. Verd\u00fa", "M.J. Weinberger"], "venue": "IEEE Transactions on Information Theory, 51(1):5\u201328", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Universal filtering via prediction", "author": ["T. Weissman", "E. Ordentlich", "M. Weinberger", "A. Somekh-Baruch", "N. Merhav"], "venue": "IEEE Trans. Inform. Theory, 53(4):1253\u20131264", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["J. Xie", "L. Xu", "E. Chen"], "venue": "Image Denoising and Inpainting with Deep Neural Networks, NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M. Zeiler"], "venue": "arXiv:1212.5701", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "Such setting covers several applications in different domains, such as image denoising [17, 14], DNA sequence denoising[12], and channel decoding[16].", "startOffset": 87, "endOffset": 95}, {"referenceID": 13, "context": "Such setting covers several applications in different domains, such as image denoising [17, 14], DNA sequence denoising[12], and channel decoding[16].", "startOffset": 87, "endOffset": 95}, {"referenceID": 11, "context": "Such setting covers several applications in different domains, such as image denoising [17, 14], DNA sequence denoising[12], and channel decoding[16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "Such setting covers several applications in different domains, such as image denoising [17, 14], DNA sequence denoising[12], and channel decoding[16].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "In order to alleviate the above mentioned limitations, [22] proposed a universal approach for discrete denoising.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": ", [17, 12, 16, 14].", "startOffset": 2, "endOffset": 18}, {"referenceID": 11, "context": ", [17, 12, 16, 14].", "startOffset": 2, "endOffset": 18}, {"referenceID": 15, "context": ", [17, 12, 16, 14].", "startOffset": 2, "endOffset": 18}, {"referenceID": 13, "context": ", [17, 12, 16, 14].", "startOffset": 2, "endOffset": 18}, {"referenceID": 6, "context": "In this paper, we present a novel framework of addressing above limitations of DUDE by adopting the machineries of deep neural networks (DNN) [7], which recently have seen great empirical success in many practical applications.", "startOffset": 142, "endOffset": 145}, {"referenceID": 3, "context": "While there have been some previous attempts of applying neural networks to grayscale image denoising [4, 24], they all remained in supervised learning setting, i.", "startOffset": 102, "endOffset": 109}, {"referenceID": 23, "context": "While there have been some previous attempts of applying neural networks to grayscale image denoising [4, 24], they all remained in supervised learning setting, i.", "startOffset": 102, "endOffset": 109}, {"referenceID": 10, "context": ", DNA sequence denoising [11].", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "DUDE in [22] is a two-pass algorithm that has a linear complexity in the data size n.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "For more rigorous analyses, we refer to the original paper [22].", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "For a thorough review on recent progresses of DNN, we refer the readers to [7] and refereces therein.", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "Regarding denoising, [4, 24, 9] have successfully applied the DNN to grayscale image denoising by utilizing supervised learning at the small image patch level.", "startOffset": 21, "endOffset": 31}, {"referenceID": 23, "context": "Regarding denoising, [4, 24, 9] have successfully applied the DNN to grayscale image denoising by utilizing supervised learning at the small image patch level.", "startOffset": 21, "endOffset": 31}, {"referenceID": 8, "context": "Regarding denoising, [4, 24, 9] have successfully applied the DNN to grayscale image denoising by utilizing supervised learning at the small image patch level.", "startOffset": 21, "endOffset": 31}, {"referenceID": 12, "context": "In order to make an alternative interpretation of DUDE, which can be also found in [13], we need the tool developed in [23].", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "In order to make an alternative interpretation of DUDE, which can be also found in [13], we need the tool developed in [23].", "startOffset": 119, "endOffset": 123}, {"referenceID": 22, "context": "With this definition, we can show that L(Z, s) is an unbiased estimate of Ex\u039b(x, s(Z)) as follows (as shown in [23]): ExL(Z, s) = \u2211", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "With this view on sk, as derived in [13], we can show that the DUDE defined in (2) is equivalent to finding a single-symbol denoiser sk,DUDE(c, \u00b7) = arg min s\u2208S \u2211", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "The interpretation (5) gives some intuition on why DUDE enjoys strong theoretical guarantees in [22]; since L(Zi, s) is an unbiased estimate of Exi\u039b(xi, s(Zi)), \u2211 i\u2208{i:ci=c} L(Zi, s) will concentrate on \u2211 i\u2208{i:ci=c} \u039b(xi, s(Zi)) as long as |{i : ci = c}| is sufficiently large.", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "We note that our usage of DNN resembles that of the neural language model (NLM) [2], which improved upon the conventional N -gram models.", "startOffset": 80, "endOffset": 83}, {"referenceID": 14, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 85, "endOffset": 93}, {"referenceID": 17, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 85, "endOffset": 93}, {"referenceID": 20, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 130, "endOffset": 145}, {"referenceID": 9, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 130, "endOffset": 145}, {"referenceID": 4, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 130, "endOffset": 145}, {"referenceID": 24, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 130, "endOffset": 145}, {"referenceID": 0, "context": "io) with Theano [1] backend.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "We used Adam [10] with default setting in Keras as an optimizer to minimize (10).", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "We believe this is a significant result since DUDE is shown to outperform many state-of-the-art sliding window denoisers in practice such as median filters [22, 17].", "startOffset": 156, "endOffset": 164}, {"referenceID": 16, "context": "We believe this is a significant result since DUDE is shown to outperform many state-of-the-art sliding window denoisers in practice such as median filters [22, 17].", "startOffset": 156, "endOffset": 164}, {"referenceID": 13, "context": "Furthermore, folloiwng DUDE\u2019s extension to grayscale image denoising [14], the result gives strong motivation for extending Neural DUDE to grayscale image denoising.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "As surveyed in [11], denoising DNA sequences are becoming increasingly important as the sequencing devices are getting cheaper, but injecting more noise than before.", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "For our experiment, we used simulated MinION nanopore reads, which were generated as follows; we obtained 16S rDNA reference sequences for 20 species [3] and randomly generated noiseless", "startOffset": 150, "endOffset": 153}, {"referenceID": 2, "context": "The number of reads and read length for each species were set as identical to those of real MinION nanopore reads [3].", "startOffset": 114, "endOffset": 117}, {"referenceID": 7, "context": "Then, based on \u03a0 of nanopore sequencer (Figure 4(a)) obtained in [8] (with 20.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": ", nanocorr [6], did not produce read-by-read correction sequence, but returns downstream analyses results after denoising.", "startOffset": 11, "endOffset": 14}, {"referenceID": 18, "context": "Coral [19], which gives read-by-read denoising result for Illumina data, completely failed for nanopore data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "Given that DUDE ourperforms state-of-the-art schemes, including Coral, for Illumina sequenced data as shown in [12], we expect the improvement of Neural DUDE over DUDE could translate into fruitful downstream analyses gain for nanopore data.", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "Third, extending the framework to deal with continuous-valued signal and finding connection with SURE principle [20] would be fruitful.", "startOffset": 112, "endOffset": 116}], "year": 2016, "abstractText": "We present a new framework of applying deep neural networks (DNN) to devise a universal discrete denoiser. Unlike other approaches that utilize supervised learning for denoising, we do not require any additional training data. In such setting, while the ground-truth label, i.e., the clean data, is not available, we devise \u201cpseudolabels\u201d and a novel objective function such that DNN can be trained in a same way as supervised learning to become a discrete denoiser. We experimentally show that our resulting algorithm, dubbed as Neural DUDE, significantly outperforms the previous state-of-the-art in several applications with a systematic rule of choosing the hyperparameter, which is an attractive feature in practice.", "creator": "LaTeX with hyperref package"}}}