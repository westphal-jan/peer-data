{"id": "1605.07918", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks with Feedback Negative Sampling", "abstract": "ruysdael Previous pentathlon studies natpe in Open jalayir Information 5\u03b1-reductase Extraction (catechins Open IE) bardaweel are organizaci\u00f3n mainly origines based maarouf on petrushka extraction spiez patterns. They nitpickers manually vyborny define patterns or 98.1 automatically 27c learn 3,658 them carians from formalisms a akinobu large corpus. sunjong However, these approaches are chitti limited nakata when c-terminal grasping the counterdemonstrations context plaything of a hesychasts sentence, and agaton they 41.83 fail contributive to capture keras implicit relations. houser In this mockingjay paper, we stans address this problem gahbauer with the fyth following shikata methods. First, maraba we exploit long nudge short - term memory (vehicules LSTM) networks bawag to marceca extract klos higher - level vadodra features colossus along dur\u00e1n the shortest soysa dependency redemptive paths, connecting headwords of relations and arguments. nduhirubusa The vicentino path - \u03c02 level engineer-in-chief features from LSTM relaxants networks usubov provide wilton useful clues bhupinder regarding bipole contextual elvi information and long-planned the brohm validity of balangay arguments. Second, mudflow we constructed :04 samples yubileyny to train 892 LSTM networks without the mahima need kilrush for manual labeling. cukor In 2.37 particular, 88.28 feedback 1993-1995 negative sampling picks algernon highly dumpty negative winkle samples lamamra among non - gleno positive dpj samples 08:45 through a greasy model perrins trained with positive diplome samples. The experimental results frisbie show dysfunctional that our jarkko approach delicatessens produces 1980s-era more raconteur precise dehydroepiandrosterone and abundant udmr extractions than rothmann state - of - arenys the - art open rupnik IE systems. stede To the mouths best of our knowledge, this blacklight is the tuners first work to pre-1986 apply deep graveside learning slum to rereleasing Open IE.", "histories": [["v1", "Wed, 25 May 2016 14:59:46 GMT  (1464kb,D)", "http://arxiv.org/abs/1605.07918v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["byungsoo kim", "hwanjo yu", "gary geunbae lee"], "accepted": false, "id": "1605.07918"}, "pdf": {"name": "1605.07918.pdf", "metadata": {"source": "CRF", "title": "Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks with Feedback Negative Sampling", "authors": ["Byungsoo Kim", "Hwanjo Yu", "Gary Geunbae Lee"], "emails": ["gblee}@postech.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "Open Information Extraction (Open IE) is a task that involves taking sentences and extracting the arguments and the relations between them. Open IE systems extract this information in the form of a triple or n-tuple. Consider the following input sentence: \u2018Boeing announced the 747 ASB in 1986\u2019. An Open IE system will extract <Boe-\ning; announced; the 747 ASB> and <Boeing; announced the 747 ASB; in 1986>, or <Boeing; announced; the 747 ASB; in 1986>. Open IE has been successfully applied in many NLP tasks, such as question answering (Fader et al., 2014), knowledge base (KB) population (Soderland et al., 2013), and ontology extension (Moro and Navigli, 2013). The major difference between traditional IE and Open IE is domain dependency. Traditional IE requires a pre-defined set of relations, whereas Open IE (Banko et al., 2007) does not. Open IE represents relations with the words in a sentence. This new paradigm removes domain dependency, extending the relation set to whole word-sets. Thus, it is possible to run Open IE at the scale of the Web.\nPrevious Open IE systems adopt two main approaches. The first approach involves manually defining the extraction patterns to find the relationships between arguments. Reverb (Fader et al., 2011) showed that simple parts-of-speech (POS) patterns can cover the majority of relationships. Gamallo et al. (2012) and KRAKEN (Akbik and Lo\u0308ser, 2012) manually define extraction rules in dependency parse trees. The second approach involves automatically learning a set of dependency-based extraction patterns from a large corpus. Methods adopting this second approach include WOE (Wu and Weld, 2010), OLLIE (Mausam et al., 2012), and ReNoun (Yahya et al., 2014).\nAlthough previous Open IE systems have been used in many other studies, these systems only extract relations that are represented explicitly in a sentence. For example, previous systems find the (explicit) relation of \u2018capital\u2019 between \u2018Vilnius\u2019 and \u2018Lithuania\u2019 in the following sentences: \u2018The two countries were officially at war over Vilnius, the capital of Lithuania\u2019; \u2018The geographical midpoint of Europe is just north of Lithuania's\nar X\niv :1\n60 5.\n07 91\n8v 1\n[ cs\n.C L\n] 2\n5 M\nay 2\n01 6\ncapital, Vilnius\u2019; and \u2018Vilnius was the capital of Lithuania, the residence of the Grand Duke\u2019. The explicit relations accompany text snippets, which are strong clues regarding the relation (\u2018Vilnius, the capital of Lithuania\u2019, \u2018Lithuania's capital, Vilnius\u2019, and \u2018Vilnius was the capital of Lithuania\u2019). However, previous Open IE systems fail to find the relation when it is implicitly represented in a sentence, such as \u2018He returned to Lithuania and then lived in the capital, Vilnius, until his death\u2019. Unlike explicit relations, an implicit relation is not captured merely with textual patterns. Extracting these implicit relations involves a deeper understanding of the context of a sentence.\nIn this paper, we propose a novel Open IE system that automatically extracts features using long short-term memory (LSTM) networks. The bidirectional recurrent architecture with LSTM units automatically extracts higher-level features along the shortest dependency paths connecting headwords of relations and arguments. Because these paths contain only informative words that are relevant to finding the proper arguments of the relation, the extracted features can grasp contextual information without superfluous information. Because there are no prevalent datasets for training Open IE systems, we propose methods for constructing training samples. In particular, feedback negative sampling selects highly negative samples among non-positive samples, and decreases disagreements between positive and negative samples. The procedure for constructing the training set is fully automatic. It does not require any manual labeling. The experimental results show that our proposed system produces 1.62 to 4.32 times more correct extractions, including implicit relations, with higher precision than state-of-the-art Open IE systems.\nThe remainder of this paper is organized as follows. Section 2 describes the two types of relations that our system aims to extract. Section 3 defines Open IE as two tasks: argument detection and preposition classification. Section 4 describes the procedure for automatically constructing the training set. Sections 5 and 6 provide detailed explanations of the neural network architectures for argument detection and preposition classification, respectively. Section 7 describes how triples are extracted from the outputs in argument detection and preposition classification. Section 8 describes experimental settings and shows evalua-\ntion results. Finally, Section 9 concludes our work."}, {"heading": "2 Types of Relation", "text": "The first type of relation is a verb-mediated relation. A relation of this type is a verb phrase. It often forms an n-ary relation. Consider as an example: \u2018Boeing announced the 747 ASB in 1986\u2019. The relation \u2018announced\u2019 has 3 arguments: \u2018Boeing\u2019, \u2018the 747 ASB\u2019, and \u20181986\u2019. This n-ary relation is represented as an n-tuple: <Boeing; announced; the 747 ASB; in 1986>. However, because a binary relation is a core concept of the semantic web and ontological KB, the n-ary relation must be converted to binary relations. This conversion involves handling the problem of incomplete relations. In the above example, by merely spanning all pairs of arguments, the triples are <Boeing; announced; the 747 ASB>, <Boeing; announced in; 1986>, and <the 747 ASB; announced in; 1986>. However, the relation <Boeing; announced in; 1986> omits critical information\u2014namely, \u2018the 747 ASB\u2019\u2014and fails to find the complete relation between \u2018Boeing\u2019 and \u20181986\u2019. The appropriate triple, without loss of information, is <Boeing; announced the 747 ASB in; 1986>. Because \u2018the 747 ASB\u2019 is a patient of \u2018announce\u2019 in the case of <the 747 ASB; announced in; 1986>, the appropriate triple is <the 747 ASB; be announced in; 1986>. Note that we do not restore the complete passive form (\u2018was announced in\u2019). Rather, \u2018be announced in\u2019 is sufficient for indicating the passive form and for downward application.\nAnother type of relation is a noun-mediated relation. A relation of this type is a noun phrase. As described in Yahya et al. (2014), a noun-mediated relation is an attribute of an argument. Consider as an example: \u2018He sat on the board of Meadows Bank, an independent bank in Nevada\u2019. A triple with a noun-mediated relation is<Meadows Bank; an independent bank in; Nevada>. In this triple, \u2018Nevada\u2019 is the target of an attribute, \u2018an independent bank\u2019, and \u2018Meadows Bank\u2019 is the value of the attribute. We add \u2018be\u2019 to the relation phrase in order to specify its meaning as an attribute, resulting in <Meadows Bank; be an independent bank in; Nevada>. Unlike verb-mediated relations, the conversion from a noun-mediated n-ary relation to binary relations merely involves spanning all pairs of arguments."}, {"heading": "3 Task Definition", "text": "We define Open IE as two tasks: argument detection, and preposition classification. Given a sentence, detecting the argument involves regarding a certain word (rel) as a headword of a relation and then classifying other words (arg) as to whether they are the proper headwords of the arguments for that relation. As an input, the classifier takes the shortest dependency path connecting rel to arg. We denote this path as path(rel, arg). By considering the shortest dependency path connecting two words, we can concentrate on informative words that are useful for understanding the relation between the two words (Bunescu and Mooney, 2005). For example, in Figure 1(a), \u2018Boeing\u2019, \u2018ASB\u2019, \u2018the\u2019, and \u2018747\u2019 are irrelevant for determining whether \u20181986\u2019 is a proper argument for \u2018announced\u2019. We define four classes for argument detection: arg1, arg2, argN, and null. For a verb-mediated relation, arg1 and arg2 are the agent and patient of a relation, respectively, and argN denotes other arguments. For a nounmediated relation, arg1 and arg2 are the value and target of a relation, respectively. We do not classify argN in the case of a noun-mediated relation. Finally, null denotes a term that is not an argument. In Figure 1(a), argument detection classifies path(announced, Boeing), path(announced, ASB), and path(announced, 1986) as arg1, arg2, and argN, respectively. Other paths are classified as\nnull. If the argument detection classifies path(rel, arg) with the verb rel as argN or the noun rel as arg2, the preposition classification finds the appropriate preposition between rel and arg. In Figure 1(a), preposition classification selects \u2018in\u2019 as the appropriate preposition between \u2018announced\u2019 and \u20181986\u2019."}, {"heading": "4 Automatically Constructing the Training Set", "text": ""}, {"heading": "4.1 Highly Precise Tuple Extraction", "text": "As Christensen et al. (2011) leveraged semantic role labeling (SRL) to find n-ary relations, we used SRL1 to extract highly precise tuples with verbmediated relations2. We assign rel to predicate, and arg1, arg2, and argN to the labeled word with the roles A0, A1, and AM, respectively. If the word is a preposition, we apply the assignment to its child, while retaining the lemma of the preposition for preposition classification. Consider the following example: \u2018In addition to the French Open, Nadal won 10 other singles titles in 2005\u2019. The SRL output is \u2018predicate: win, A0: Nadal, A1: titles, AM-DIS: In, AM-TMP: in\u2019, and our assignment extracts the tuple as \u2018rel: win, arg1: Nadal, arg2: titles, argN (in): addition, argN (in): 2005\u2019. To minimize tuple-extraction errors, we only extract tuples from the top 1M sentences with the highest SRL confidence scores.\nWe define ten dependency-based extraction patterns to extract highly precise tuples with nounmediated relations (see Figure 2). The patterns are applied to subgraphs of a dependency parse tree. The circles and arrows in the patterns represent words and dependency relations in the subgraph, respectively. For the preposition classification, we retain a lemma of a word at a circle with IN. If there is no circle with IN in a pattern, we retain \u2018of\u2019. If a pattern is matched, we assign arg1, rel, and arg2 to the words in the circles with arg1, rel, and arg2, respectively. For example, from the dependency parse tree of the sentence, \u2018The agency is located in Gaborone, capital of Botswana\u2019, the seventh pattern in Figure 2 is matched and our assignment extracts the tuple as \u2018rel: capital, arg1: Gaborone, arg2 (of): Botswana\u2019. Like verb-mediated tuple extraction,\n1We used ClearNLP (www.clearnlp.com) for the natural language processing pipeline.\n2We used the English Wikipedia corpus to construct the training set.\nwe only extract tuples from the top 1M sentences with the highest dependency-parsing confidence scores."}, {"heading": "4.2 Training Set Augmentation", "text": "The goal of training set augmentation is to find sentences representing relations in highly precise tuples that SRL and the patterns failed to capture. Similar to OLLIE (Mausam et al., 2012) and ReNoun (Yahya et al., 2014), this augmentation process is based on seed-based distant supervision: if arguments in a seed triple appear in a sentence, their relation is likely to appear in the sentence. The augmentation begins by converting the tuple to triples. For tuples with a verb-mediated relation, we convert each tuple to <arg1; rel; arg2>, <arg1; rel; argN>, and <arg2; rel; argN>. Tuples with a noun-mediated relation are converted to <arg1; rel; arg2>. Among the converted triples, we acquire 55K seeds satisfying the following constraints: (1) the arguments are proper nouns or cardinal numbers; (2) arguments with a proper noun are properly linked to entities in DBpedia (Auer et al., 2007); and (3) the lemma of a relation is not \u2018be\u2019 or \u2018do\u2019. We use DBpedia Spotlight (Daiber et al., 2013) for entity linking. For each seed triple, we find sentences containing the same linked entities of arguments with a proper noun or the same surface forms of arguments with a cardinal number. Because the distant supervision hypothesis is often erroneous, we include the fol-\nlowing constraints: (1) the sentence contains the lemma of a relation; (2) the headwords of relations and arguments are connected via a linear dependency path; and (3) triples with verb-mediated relations have a path length of less than seven. For example, we acquired the seed triple from the tuple, \u2018rel: capital, arg1: Gaborone, arg2 (of): Botswana\u2019 with its arguments linked to DBpedia entities, \u2018Gaborone\u2019 and \u2018Botswana\u2019. We retrieved the corpus and found \u2018Now Prime Minister of Bechuanaland, Khama continued to push for Botswana's independence, from the newly established capital of Gaborone\u2019. The augmentation produces 110K (sentence, seed triple) pairs that cannot be covered by highly precise tuples. We label path(rel, arg1), path(rel, arg2), and path(rel, argN) from these pairs and highly precise tuples as arg1, arg2, and argN, respectively. These labeled paths comprise positive samples for argument detection. We also label path(rel, argN) and path(rel, arg2) with the noun rel as their prepositions to comprise samples for preposition classification."}, {"heading": "4.3 Feedback Negative Sampling", "text": "Samples from the previous stages merely indicate which paths are arg1, arg2, and argN. They do not describe which paths are null (negative). One possible option for negative sampling is to regard non-positive paths as negative ones. However, this risks treating uncaptured positive paths as negative ones. For example, we found that path(spoke, Moses) is an uncaptured positive path with the label argN in the sentence: \u2018Their presumption was rebuffed by God who affirmed Moses' uniqueness as the one with whom the LORD spoke face to face\u2019 (see Figure 3). Our strategy for addressing this problem begins from the observation that there are two features to a highly negative path: (1) it contains a positive path, or a positive path contains it; and (2) the more similar it is to the positive path, the more negative the path is. For example, in Figure 3, path(rebuffed, their), path(was, presumption), and path(by, presumption) are highly negative paths. They contain path(rebuffed, presumption), which is positive, and they have only one more node than the positive path. From this observation, we describe feedback negative sampling in Algorithm 1. The rationale behind this algorithm is as follows: because a model3 trained with positive samples assigns high confidence to\n3We describe the details for this model in the next section.\nAlgorithm 1: Feedback negative sampling Input: NP = A set of non-positive paths N = An empty set of negative samples F = A model trained on positive samples p = A prediction score threshold foreach non-positive path np \u2208 NP do F (np)i = Prediction score of np on the i-th class m = argmax\ni F (np)i\nif F (np)m > p then N = N \u222a {np}\nreturn N\npositive paths, it also assigns high confidence to non-positive paths with these features. For each non-positive path, we obtain clues (feedback) regarding these features. If the path has these features, the model assigns a high prediction score to a certain class. We regard the path as a negative sample when the score exceeds a certain threshold."}, {"heading": "5 Argument Detection", "text": "Figure 4 describes the architecture for the neural network used for argument detection. At each time-step, the network acquires an input vector from a node in path(rel, arg). The input vector is a concatenation of vectors from the following features: word, POS, dependency relation, and named entity.\nMost of the deep learning applied NLP tasks\nleverage word embeddings trained with a large corpus in an unsupervised manner. The word embeddings capture the syntactic and semantic information of the words based on the context in the corpus. We pre-trained word embeddings from the English Wikipedia corpus with the skip-gram model in word2vec (Mikolov et al., 2013). In doing so, we acquired the word embedding matrix, Mword \u2208 Rdimword\u00d7|W |, where W is a set of words.\nPOS and dependency relations provide essential information regarding the syntactic structure of a sentence. However, there is no prevailing method for pre-training POS and dependency relation embeddings. In this work, we randomly initialized the POS and dependency relation embedding matrix, Mpos \u2208 Rdimpos\u00d7|P | and Mdep \u2208 Rdimdep\u00d7|D|, where P and D are sets of POS tags and dependency labels, respectively. We then finetuned them in a supervised manner with backpropagation training.\nNamed entity recognition classifies each word into a pre-defined semantic category. We thus acquire the semantic types of words from the categories they belong to. Once again, the named entity embedding matrix, Mne \u2208 Rdimne\u00d7|N | (where N is a set of named entity tags), is randomly initialized and updated through back-propagation training.\nThe word, POS tag, dependency label, and named entity tag of the t-th node in path(rel, arg) are associated with a vector, wordt \u2208 Rdimword , post \u2208 Rdimpos , dept \u2208 Rdimdep , and net \u2208 Rdimne in the embedding matri-\nces, respectively. We concatenate these vectors to produce a single input vector of the t-th node, xt = [wordt, post, dept, net] \u2208 Rdimword+dimpos+dimdep+dimne .\nA recurrent neural network (RNN) obtains the previous hidden state at each time-step, and creates and maintains the internal memory. By doing so, it can process arbitrary sequences of inputs. However, traditional RNNs have two wellknown problems: vanishing and exploding gradients. If the input sequence is too long, the gradient can either decay or grow exponentially. An RNN with long short-term memory (LSTM) units was first introduced by Hochreiter and Schmidhuber (1997) in order to tackle this problem with an adaptive gating mechanism. Among the many LSTM variants, we selected LSTM with peephole connections in the spirit of Gers and Schmidhuber (2000). Furthermore, we use both the forward and backward directional recurrent LSTM layer (see Appendix A). This bi-directional architecture makes predictions based on information from both the past and the future. We obtain a bi-directional output vector ht \u2208 RdimL at each time-step from a vector sum of the forward (hfwt ) and the backward (hbwt ) LSTM layer output vectors.\nht = h fw t + h bw t (1)\nWe then convert an arbitrary number of bidirectional output vectors to a path-level feature vector hpath through a max-over-time operation (Collobert et al., 2011). This operation picks the salient features along the sequence of vectors to produce a single vector that is no longer related to the length of the sequence.\nhpath = max t {(ht)i} (0 \u2264 i \u2264 dimL) (2)\nSubsequently, a fully connected layer non-linearly transforms the path-level feature vector to learn more complex features. We select the hyperbolic tangent activation function to obtain a higher-level feature vector hhigher \u2208 RdimH .\nhhigher = tanh(Mhigher \u00b7 hpath) (3)\nFinally, a softmax output layer projects hhigher into a vector with dimensions equivalent to the number of classes. The softmax operation is then applied to obtain a vector hout \u2208 R4 with its elements representing the conditional probability for each class.\nhout = softmax(Mout \u00b7 hhigher) (4)"}, {"heading": "6 Preposition Classification", "text": "The neural network used for preposition classification is almost the same as the model used in the previous section. There are only two modifications to the preposition classification model. First, there is no penultimate fully connected layer in the model. We directly connect the max pooling layer to the softmax output layer. The second modification is to the number of output classes. The number of classes for preposition classification depends on the number of prepositions that appear in the positive samples. With 88 prepositions in the positive samples and one additional class for nonprepositions, the neural network model for preposition classification has hout \u2208 R89."}, {"heading": "7 Triple Extraction", "text": "Triple extraction begins by aligning the prediction results as defined in the extraction template (see Table 1). This alignment produces incomplete triples of arguments and relations that are incomplete phrases. We span the dependents of aligned words, arg1, rel, arg2, and argN, to ensure that the triples contain sufficient information from the sentences.\nPrevious Open IE systems assign a score for each extracted triple. The score is used to indicate the degree of correctness, since extracted triples are not always correct. We define a scoring function as below.\nscore(t) = dep(s)\u00d7\n\u2211 arg\u2208args prob(arg)\n| args | (5)\nwhere t is an extracted triple from a sentence s, dep(s) is the dependency parsing confidence score of s, args is a set of arguments in t, and prob(arg) is the conditional probability of arg from the softmax output. Since errors in path(rel, arg) are propagated to the final extraction, our scoring function is a mean of the conditional probabilities for arguments weighted by the dependency parsing confidence score of a sentence."}, {"heading": "8 Experiments", "text": ""}, {"heading": "8.1 Evaluation Settings", "text": "We crawled news articles on the Web and randomly sampled 100 sentences for evaluation. Because Open IE extracts totally new relations from the sentences, there is no ground-truth set of extractions. For this reason, our natural choice for a performance metric was to calculate the precision over the number of extractions. This is a common metric in previous Open IE studies. The extractions were manually annotated for correctness and sorted according to their score, in descending order. We set our system to output extractions with scores over 0.75, in order to clarify our evaluation results."}, {"heading": "8.2 Comparison with State-of-the-Art Open", "text": "IE Systems\nWe compared our system with three widely used Open IE systems: Open IE 4.24, OLLIE, and Reverb. Unlike Open IE 4.2 and OLLIE, our system does not determine whether the extractions are factual. Thus, we considered all extractions from Open IE 4.2 and OLLIE in the comparison without distinguishing the factuality of the extractions. Because there is no way to convert unary relations to binary relations, we discarded unary relations from Open IE 4.2. Our proposed system produced more extractions than the other Open IE systems, and it achieved the highest precision in all areas regarding the number of extractions (see Figure 5). Specifically, the proposed system produced 1.62, 1.94, and 4.32 times more correct extractions than Open IE 4.2, OLLIE, and Reverb, respectively.\nIn addition to outperforming previous Open IE systems in terms of both precision and the to-\n4https://github.com/knowitall/openie\ntal number of extractions, our system extracted implicit relations (see Appendix C). Extracting the implicit relations requires analyzing the context of the sentences, rather than merely setting boundaries to split the relations and arguments in sentences. Despite the relatively small proportion of implicit relations among correct extractions (3.8%), they were indeed worth extracting, because they contributed to more abundant extractions. We compared our system to a model trained with samples without the augmented training set and found that these extractions were made from properly learning the relations from the augmented training set. All Open IE systems, apart from the proposed system, failed to extract implicit relations. Open IE 4.2 heuristically converted SRL outputs to produce most of its extractions. Because of its high reliance on SRL, it missed the implicit relations that SRL failed to capture. In a manner similar to the augmented training set, OLLIE automatically constructed training samples with seed-based distant supervision. However, OLLIE converted dependency paths connecting headwords of relations and arguments into pattern templates. Consequently, OLLIE failed to extract complex features from sentences. Reverb assumed that arguments and their relations appear consecutively in a sentence. Although this assumption is often correct, it is unsuitable when extracting implicit relations."}, {"heading": "8.3 Comparison with Different System Settings", "text": "Next, we analyzed how our system benefits from bi-directional LSTM networks (Figure 6(a)). We compared two sets of extractions: extractions from a model trained with samples from highly precise tuples (Without Augmentation), and extractions from a method using highly precise tuple extraction (Highly Precise Tuples). The former set contained 1.25 times more correct extractions than the latter set. Moreover, when quality extractions were considered, the first set contained extractions that were more precise.\nWe analyzed the effect of augmenting the training set by comparing two models: a model trained with the augmented samples (With Augmentation), and samples from the highly precise tuples (Without Augmentation). Figure 6(a) shows that augmented training set contributed to the production of 1.12 times more correct extractions with a slight\nboost in precision. Furthermore, the model without augmentation produced no extractions with implicit relations.\nWe analyzed the quality of samples from the augmented training set. Because the samples were from (sentence, seed triple) pairs, we manually checked whether in each pair the seed triple represented a valid relation in the sentence. Among the 200 randomly sampled pairs, 83.5% were valid relations. Among pairs with invalid relations, 68% were due to a failure in the distant supervision assumption, 29% were due to errors in the seed triples, and 3% were due to entity linking errors.\nWe compared two negative sampling strategies:\nfeedback negative sampling (Feedback Negative Sampling), and random sampling of non-positive paths (Random) (see Figure 6(b)). Feedback negative sampling achieved higher precision overall. The loss of precision from random sampling of non-positive paths was due to disagreements between the positive and negative samples.\nWe also analyzed how each input feature contributed to the extraction performance (see Figure 6(c)). We set a baseline model with only the word feature (Word) as the input. We then added the POS (Word+POS), dependency relation (Word+Dep), and named entity (Word+NE) features one-by-one. Higher precision was achieved when the features were combined, compared to when only the word feature was used. Notably, the dependency relation feature boosted the precision considerably. By combining all four features (Word+POS+Dep+NE), the precision further increased, with the added advantage of expanding the total number of correct extractions."}, {"heading": "8.4 Extraction Error Analysis", "text": "We analyzed incorrect extractions and investigated the source of the errors. According to our analysis, 20% of the errors were due to incorrect dependency parsing. Because the proposed system acquires a dependency path as an input, errors in dependency parsing were propagated throughout our system. Among the incorrect extractions without dependency parsing errors, 98% of the errors were from argument detection, and 4% were from preposition classification."}, {"heading": "9 Conclusion", "text": "Our novel Open IE system with LSTM networks produced more precise and abundant extractions than state-of-the-art Open IE systems. In particular, the proposed system extracted implicit relations, unlike other Open IE systems. The advantages to the proposal stem from two contributions: a bi-directional recurrent architecture with LSTM units, enabling the extraction of higher-level features containing the contextual information in a sentence; and feedback negative sampling, which reduces the disagreements between positive and negative samples. To the best of our knowledge, this is the first work to apply deep learning to Open IE."}, {"heading": "A Bi-Directional Recurrent Layer with LSTM Units", "text": "We begin from the forward-directional recurrent layer with LSTM units that receive input sequences from beginning to end (Equations 6-11).\nffwt = \u03c3(W fw f \u00b7 xt + U fw f \u00b7 ht\u22121\n+ V fwf \u00b7 c fw t\u22121 + b fw f )\n(6)\nifwt = \u03c3(W fw i \u00b7 xt + U fw i \u00b7 ht\u22121\n+ V fwi \u00b7 c fw t\u22121 + b fw i )\n(7)\ngfwt = tanh(W fw g \u00b7 xt + Ufwg \u00b7 ht\u22121\n+ bfwg ) (8)\ncfwt = i fw t \u2297 g fw t + f fw t \u2297 c fw t\u22121 (9) ofwt = \u03c3(W fw o \u00b7 xt + Ufwo \u00b7 ht\u22121\n+ V fwo \u00b7 c fw t + b fw o )\n(10)\nhfwt = o fw t \u2297 tanh(c fw t ) (11)\nThere are four components in the LSTM unit: a forget gate ffwt , an input gate i fw t , a candidate memory content gfwt , and an output gate o fw t . The forget and input gate receive the current input xt, the previous output ht\u22121, and the previous memory content cfwt\u22121. These are then multiplied with the matrices W fw, Ufw, and V fw, respectively. Then, the multiplied values are summed with a bias bfw, and the result is non-linearly transformed through the sigmoid function \u03c3 (Equations 6-7). The candidate memory content receives the current input and the previous output, which are multiplied with the matrices W fw and Ufw, respectively. Then, the multiplied values are summed with a bias bfw, and the result is non-linearly transformed through the hyperbolic tangent function tanh (Equation 8). The output gate also receives the current input and the previous output, but it considers the current memory content, rather than the previous memory content (Equation 10). The current memory content is a combination of candidate memory content and previous memory content, weighted by the values of the input gate and the forget gate, respectively (Equation 9). Finally, the current output is a normalized current\nmemory content through the hyperbolic tangent function, weighted by the value of the output gate (Equation 11).\nA potential problem with the forward LSTM layer is that it only considers information from the past. It thus fails to capture information from the future. We address this problem using an additional backward LSTM layer that receives input sequences from the end to the beginning (Equations 12-17).\nf bwt = \u03c3(W bw f \u00b7 xt + U bwf \u00b7 ht+1\n+ V bwf \u00b7 cbwt+1 + bbwf ) (12)\nibwt = \u03c3(W bw i \u00b7 xt + U bwi \u00b7 ht+1\n+ V bwi \u00b7 cbwt+1 + bbwi ) (13)\ngbwt = tanh(W bw g \u00b7 xt + U bwg \u00b7 ht+1\n+ bbwg ) (14)\ncbwt = i bw t \u2297 gbwt + f bwt \u2297 cbwt+1 (15) obwt = \u03c3(W bw o \u00b7 xt + U bwo \u00b7 ht+1\n+ V bwo \u00b7 cbwt + bbwo ) (16)\nhbwt = o bw t \u2297 tanh(cbwt ) (17)"}, {"heading": "B Training Details", "text": "We set the prediction score threshold p to 0.9 during feedback negative sampling. Furthermore, we set dimword to 300, and dimpos, dimdep, and dimne to 50. Moreover, dimL was set to 450, which was equivalent to the dimensions of the input vector, and dimH was set to 50. Much like the regularization method used in Xu et al. (2015), we assigned the input vector a dropout rate of 0.5. Because we apply a softmax operation for the final output, the natural choice for a training objective is cross-entropy.\nJ(\u03b8) = \u2211 t\u2208T log p(y(t) | x(t), \u03b8) (18)\nIn the above equation, T is a set of training samples, and \u03b8 = (Mpos, Mdep, Mne, MLSTM , Mhigher, Mout) represents the network parameters, where MLSTM denotes the parameters in the LSTM units. We used the ADAM (Kingma and Ba, 2014) update rule to maximize the training objective through stochastic gradient descent over shuffled mini-batches. We set \u03b21 to 0.9, \u03b22 to 0.999, and to 1e-8 for the ADAM parameters.\nC Extraction Examples"}], "references": [{"title": "Kraken: Nary facts in open information extraction", "author": ["Alan Akbik", "Alexander L\u00f6ser."], "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX), pages 52\u201356, Montr\u00e9al,", "citeRegEx": "Akbik and L\u00f6ser.,? 2012", "shortCiteRegEx": "Akbik and L\u00f6ser.", "year": 2012}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives."], "venue": "Proceedings of the 6th International The Semantic Web and 2Nd Asian Conference on Asian Seman-", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Open information extraction from the web", "author": ["Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matt Broadhead", "Oren Etzioni."], "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI\u201907.", "citeRegEx": "Banko et al\\.,? 2007", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Razvan Bunescu", "Raymond Mooney."], "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 724\u2013", "citeRegEx": "Bunescu and Mooney.,? 2005", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2005}, {"title": "An analysis of open information extraction based on semantic role labeling", "author": ["Janara Christensen", "Mausam", "Stephen Soderland", "Oren Etzioni."], "venue": "Proceedings of the Sixth International Conference on Knowledge Capture, K-CAP \u201911, pages 113\u2013120,", "citeRegEx": "Christensen et al\\.,? 2011", "shortCiteRegEx": "Christensen et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537, November.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Improving efficiency and accuracy in multilingual entity extraction", "author": ["Joachim Daiber", "Max Jakob", "Chris Hokamp", "Pablo N. Mendes."], "venue": "Proceedings of the 9th International Conference on Semantic Systems, I-SEMANTICS \u201913, pages 121\u2013", "citeRegEx": "Daiber et al\\.,? 2013", "shortCiteRegEx": "Daiber et al\\.", "year": 2013}, {"title": "Identifying relations for open information extraction", "author": ["Anthony Fader", "Stephen Soderland", "Oren Etzioni."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535\u20131545, Edinburgh, Scotland, UK.,", "citeRegEx": "Fader et al\\.,? 2011", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914,", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Dependency-based open information extraction", "author": ["Pablo Gamallo", "Marcos Garcia", "Santiago Fern\u00e1ndez-Lanza."], "venue": "Proceedings of the Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP, pages 10\u201318, Avignon, France,", "citeRegEx": "Gamallo et al\\.,? 2012", "shortCiteRegEx": "Gamallo et al\\.", "year": 2012}, {"title": "Recurrent nets that time and count", "author": ["F.A. Gers", "J. Schmidhuber."], "venue": "Proceedings of the IEEEINNS-ENNS International Joint Conference on Neural Networks, 2000. IJCNN 2000, volume 3, pages 189\u2013194 vol.3.", "citeRegEx": "Gers and Schmidhuber.,? 2000", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "The Journal of Neural Computation, 9(8):1735\u20131780, November.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Open language learning for information extraction", "author": ["Mausam", "Michael Schmitz", "Stephen Soderland", "Robert Bart", "Oren Etzioni."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Mausam et al\\.,? 2012", "shortCiteRegEx": "Mausam et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Integrating syntactic and semantic analysis into the open information extraction paradigm", "author": ["Andrea Moro", "Roberto Navigli."], "venue": "Proceedings of the 23rd International Joint Conference on Artifical Intelligence, IJCAI\u201913.", "citeRegEx": "Moro and Navigli.,? 2013", "shortCiteRegEx": "Moro and Navigli.", "year": 2013}, {"title": "Open information extraction to KBP relations in 3 hours", "author": ["Stephen Soderland", "John Gilmer", "Robert Bart", "Oren Etzioni", "Daniel S. Weld."], "venue": "Text Analysis Conference, TAC\u201913.", "citeRegEx": "Soderland et al\\.,? 2013", "shortCiteRegEx": "Soderland et al\\.", "year": 2013}, {"title": "Open information extraction using wikipedia", "author": ["Fei Wu", "Daniel S. Weld."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118\u2013127, Uppsala, Sweden, July. Association for Computational Linguis-", "citeRegEx": "Wu and Weld.,? 2010", "shortCiteRegEx": "Wu and Weld.", "year": 2010}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Renoun: Fact extraction for nominal attributes", "author": ["Mohamed Yahya", "Steven Whang", "Rahul Gupta", "Alon Halevy."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 325\u2013335, Doha, Qatar,", "citeRegEx": "Yahya et al\\.,? 2014", "shortCiteRegEx": "Yahya et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Open IE has been successfully applied in many NLP tasks, such as question answering (Fader et al., 2014), knowledge base (KB) population (Soderland et al.", "startOffset": 84, "endOffset": 104}, {"referenceID": 16, "context": ", 2014), knowledge base (KB) population (Soderland et al., 2013), and ontology extension (Moro and Navigli, 2013).", "startOffset": 40, "endOffset": 64}, {"referenceID": 15, "context": ", 2013), and ontology extension (Moro and Navigli, 2013).", "startOffset": 32, "endOffset": 56}, {"referenceID": 2, "context": "Traditional IE requires a pre-defined set of relations, whereas Open IE (Banko et al., 2007) does not.", "startOffset": 72, "endOffset": 92}, {"referenceID": 7, "context": "Reverb (Fader et al., 2011) showed that simple parts-of-speech (POS) patterns can cover the majority of relationships.", "startOffset": 7, "endOffset": 27}, {"referenceID": 0, "context": "(2012) and KRAKEN (Akbik and L\u00f6ser, 2012) manually define extraction rules in dependency parse trees.", "startOffset": 18, "endOffset": 41}, {"referenceID": 17, "context": "Methods adopting this second approach include WOE (Wu and Weld, 2010), OLLIE (Mausam et al.", "startOffset": 50, "endOffset": 69}, {"referenceID": 13, "context": "Methods adopting this second approach include WOE (Wu and Weld, 2010), OLLIE (Mausam et al., 2012), and ReNoun (Yahya et al.", "startOffset": 77, "endOffset": 98}, {"referenceID": 19, "context": ", 2012), and ReNoun (Yahya et al., 2014).", "startOffset": 20, "endOffset": 40}, {"referenceID": 6, "context": "Reverb (Fader et al., 2011) showed that simple parts-of-speech (POS) patterns can cover the majority of relationships. Gamallo et al. (2012) and KRAKEN (Akbik and L\u00f6ser, 2012) manually define extraction rules in dependency parse trees.", "startOffset": 8, "endOffset": 141}, {"referenceID": 19, "context": "As described in Yahya et al. (2014), a noun-mediated relation is an attribute of an argument.", "startOffset": 16, "endOffset": 36}, {"referenceID": 3, "context": "By considering the shortest dependency path connecting two words, we can concentrate on informative words that are useful for understanding the relation between the two words (Bunescu and Mooney, 2005).", "startOffset": 175, "endOffset": 201}, {"referenceID": 4, "context": "As Christensen et al. (2011) leveraged semantic role labeling (SRL) to find n-ary relations, we used SRL1 to extract highly precise tuples with verbmediated relations2.", "startOffset": 3, "endOffset": 29}, {"referenceID": 13, "context": "Similar to OLLIE (Mausam et al., 2012) and ReNoun (Yahya et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 19, "context": ", 2012) and ReNoun (Yahya et al., 2014), this augmentation process is based on seed-based distant supervision: if arguments in a seed triple appear in a sentence, their relation is likely to appear in the sentence.", "startOffset": 19, "endOffset": 39}, {"referenceID": 1, "context": "Among the converted triples, we acquire 55K seeds satisfying the following constraints: (1) the arguments are proper nouns or cardinal numbers; (2) arguments with a proper noun are properly linked to entities in DBpedia (Auer et al., 2007); and (3) the lemma of a relation is not \u2018be\u2019 or \u2018do\u2019.", "startOffset": 220, "endOffset": 239}, {"referenceID": 6, "context": "We use DBpedia Spotlight (Daiber et al., 2013) for entity linking.", "startOffset": 25, "endOffset": 46}, {"referenceID": 14, "context": "We pre-trained word embeddings from the English Wikipedia corpus with the skip-gram model in word2vec (Mikolov et al., 2013).", "startOffset": 102, "endOffset": 124}, {"referenceID": 10, "context": "An RNN with long short-term memory (LSTM) units was first introduced by Hochreiter and Schmidhuber (1997) in order to tackle this problem with an adaptive gating mechanism.", "startOffset": 72, "endOffset": 106}, {"referenceID": 10, "context": "Among the many LSTM variants, we selected LSTM with peephole connections in the spirit of Gers and Schmidhuber (2000). Furthermore, we use both the forward and backward directional recurrent LSTM layer (see Appendix A).", "startOffset": 90, "endOffset": 118}, {"referenceID": 5, "context": "We then convert an arbitrary number of bidirectional output vectors to a path-level feature vector hpath through a max-over-time operation (Collobert et al., 2011).", "startOffset": 139, "endOffset": 163}], "year": 2016, "abstractText": "Previous studies in Open Information Extraction (Open IE) are mainly based on extraction patterns. They manually define patterns or automatically learn them from a large corpus. However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations. In this paper, we address this problem with the following methods. First, we exploit long shortterm memory (LSTM) networks to extract higher-level features along the shortest dependency paths, connecting headwords of relations and arguments. The path-level features from LSTM networks provide useful clues regarding contextual information and the validity of arguments. Second, we constructed samples to train LSTM networks without the need for manual labeling. In particular, feedback negative sampling picks highly negative samples among non-positive samples through a model trained with positive samples. The experimental results show that our approach produces more precise and abundant extractions than state-of-the-art open IE systems. To the best of our knowledge, this is the first work to apply deep learning to Open IE.", "creator": "TeX"}}}