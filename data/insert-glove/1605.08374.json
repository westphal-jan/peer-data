{"id": "1605.08374", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Kronecker Determinantal Point Processes", "abstract": "microbrews Determinantal Point Processes (DPPs) gantries are jean-guy probabilistic winterlude models over gouache all subsets lacalle a tailwinds ground set of $ krutch N $ reinvestment items. They have fone recently gained prominence snickered in several applications that rely faucette on \" trzy diverse \" subsets. 9m However, seea their applicability to hadramawt large problems yoob is 444 still limited due sametime to the $ \\ mathcal 34,688 O (cyro N ^ kloden 3) $ shahdadkot complexity meftah of stingless core centercourt tasks such as reports sampling demayo and vtu learning. malahide We th\u00e9odore enable efficient aidar sampling l\u00e9ry and learning eloquence for lechlade DPPs by introducing spitters KronDPP, a paules DPP meldrum model asadoorian whose kernel re-build matrix airbus-320 decomposes as a tensor longbottom product richness of multiple smaller kernel egar matrices. dule This occitania decomposition immediately enables fast exact sampling. 11-year But contrary mid-eastern to galumphing what one may y\u0131lmaz expect, krazy leveraging ujazd the go-away Kronecker alisa product tigo structure iddi for speeding up bosut DPP learning turns 1,154 out to kemboi be low-tech more kolonaki difficult. mtf We 2,659 overcome this challenge, obstructs and cradling derive euro527 batch vebjoern and bordley stochastic friday.the optimization \u0111in\u0111i\u0107 algorithms rivermaya for zentiva efficiently judaization learning 2,392 the parameters duvenhage of a chengqing KronDPP.", "histories": [["v1", "Thu, 26 May 2016 17:33:31 GMT  (220kb,D)", "http://arxiv.org/abs/1605.08374v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["zelda e mariet", "suvrit sra"], "accepted": true, "id": "1605.08374"}, "pdf": {"name": "1605.08374.pdf", "metadata": {"source": "CRF", "title": "Kronecker Determinantal Point Processes", "authors": ["Zelda Mariet"], "emails": ["zelda@csail.mit.edu", "suvrit@mit.edu"], "sections": [{"heading": null, "text": "of N items. They have recently gained prominence in several applications that rely on \u201cdiverse\u201d subsets. However, their applicability to large problems is still limited due to theO(N3) complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KRONDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KRONDPP."}, {"heading": "1 Introduction", "text": "Determinantal Point Processes (DPPs) are discrete probability models over the subsets of a ground set of N items. They provide an elegant model to assign probabilities to an exponentially large sample, while permitting tractable (polynomial time) sampling and marginalization. They are often used to provide models that balance \u201cdiversity\u201d and quality, characteristics valuable to numerous problems in machine learning and related areas [17].\nThe antecedents of DPPs lie in statistical mechanics [24], but since the seminal work of [15] they have made inroads into machine learning. By now they have been applied to a variety of problems such as document and video summarization [6, 21], sensor placement [14], recommender systems [31], and object retrieval [2]. More recently, they have been used to compress fully-connected layers in neural networks [26] and to provide optimal sampling procedures for the Nystr\u00f6m method [20]. The more general study of DPP properties has also garnered a significant amount of interest, see e.g., [1, 5, 7, 12, 16\u201318, 23].\nHowever, despite their elegance and tractability, widespread adoption of DPPs is impeded by the O(N3) cost of basic tasks such as (exact) sampling [12, 17] and learning [10, 12, 17, 25]. This cost has motivated a string of recent works on approximate sampling methods such as MCMC samplers [13, 20] or core-set based samplers [19]. The task of learning a DPP from data has received less attention; the methods of [10, 25] cost O(N3) per iteration, which is clearly unacceptable for realistic settings. This burden is partially ameliorated in [9], who restrict to learning low-rank DPPs, though at the expense of being unable to sample subsets larger than the chosen rank.\nThese considerations motivate us to introduce KRONDPP, a DPP model that uses Kronecker (tensor) product kernels. As a result, KRONDPP enables us to learn large sized DPP kernels, while also permitting efficient (exact and approximate) sampling. The use of Kronecker products to scale matrix models is a popular and effective idea in several machine-learning settings [8, 27, 28, 30]. But as we will see, its efficient execution for DPPs turns out to be surprisingly challenging.\nTo make our discussion more concrete, we recall some basic facts now. Suppose we have a ground set of N items Y = {1, . . . , N}. A discrete DPP over Y is a probability measure P on 2Y parametrized by a positive definite matrix K (the marginal kernel) such that 0 K I , so that for any Y \u2208 Y drawn from P , the measure satisfies\n\u2200A \u2286 Y, P(A \u2286 Y ) = det(KA), (1)\nar X\niv :1\n60 5.\n08 37\n4v 1\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 6\nwhere KA is the submatrix of K indexed by elements in A (i.e., KA = [Kij ]i,j\u2208A). If a DPP with marginal kernel K assigns nonzero probability to the empty set, the DPP can alternatively be parametrized by a positive definite matrix L (the DPP kernel) so that\nP(Y ) \u221d det(LY ) =\u21d2 P(Y ) = det(LY )\ndet(L+ I) . (2)\nA brief manipulation (see e.g., [17, Eq. 15]) shows that when the inverse exists, L = K(I \u2212K)\u22121. The determinants, such as in the normalization constant in (2), make operations over DPPs typically cost O(N3), which is a key impediment to their scalability.\nTherefore, if we consider a class of DPP kernels whose structure makes it easy to compute determinants, we should be able to scale up DPPs. An alternative approach towards scalability is to restrict the size of the subsets, as done in k-DPP [16] or when using rank-k DPP kernels [9] (where k N ). Both these approaches still requireO(N3) preprocessing for exact sampling; another caveat is that they limit the DPP model by assigning zero probabilities to sets of cardinality greater than k.\nIn contrast, KRONDPP uses a kernel matrix of the form L = L1 \u2297 . . . \u2297 Lm, where each subkernel Li is a smaller positive definite matrix. This decomposition has two key advantages: (i) it significantly lowers the number of parameters required to specify the DPP from N2 to O(N2/m) (assuming the sub-kernels are roughly the same size); and (ii) it enables fast sampling and learning.\nFor ease of exposition, we describe specific details of KRONDPP for m = 2; as will become clear from the analysis, typically the special cases m = 2 and m = 3 should suffice to obtain low-complexity sampling and learning algorithms.\nContributions. Our main contribution is the KRONDPP model along with efficient algorithms for sampling from it and learning a Kronecker factored kernel. Specifically, inspired by the algorithm of [25], we develop KRK-PICARD (Kronecker-Kernel Picard), a block-coordinate ascent procedure that generates a sequence of Kronecker factored estimates of the DPP kernel while ensuring monotonic progress on its (difficult, nonconvex) objective function. More importantly, we show how to implement KRK-PICARD to run in O(N2) time when implemented as a batch method, and in O(N3/2) time and O(N) space, when implemented as a stochastic method. As alluded to above, unlike many other uses of Kronecker models, KRONDPP does not admit trivial scaling up, largely due to extensive dependence of DPPs on arbitrary submatrices of the DPP kernel. An interesting theoretical nugget that arises from our analysis is the combinatorial problem that we call subset clustering, a problem whose (even approximate) solution can lead to further speedups of our algorithms."}, {"heading": "2 Preliminaries", "text": "We begin by recalling basic properties of Kronecker products needed in our analysis; we omit proofs of these well-known results for brevity. The Kronecker (tensor) product of A \u2208 Rp\u00d7q with B \u2208 Rr\u00d7s two matrices is defined as the pr \u00d7 qs block matrix A\u2297B = [aijB]p,qi,j=1.\nWe denote the block aijB in A \u2297 B by (A \u2297 B)(ij) for any valid pair (i, j), and extend the notation to non-Kronecker product matrices to indicate the submatrix of size r \u00d7 s at position (i, j).\nProposition 2.1. Let A,B,C,D be matrices of sizes so that AC and BD are well-defined. Then,\n(i) If A,B 0, then, A\u2297B 0; (ii) If A and B are invertible then so is A\u2297B, with (A\u2297B)\u22121 = A\u22121 \u2297B\u22121;\n(iii) (A\u2297B)(C \u2297D) = (AC)\u2297 (BD).\nAn important consequence of Prop. 2.1(iii) is the following corollary.\nCorollary 2.2. Let A = PADAP>A and B = PBDBP>B be the eigenvector decompositions of A and B. Then, A\u2297B diagonalizes as (PA \u2297 PB)(DA \u2297DB)(PA \u2297 PB)>.\nWe will also need the notion of partial trace operators, which are perhaps less well-known:\nDefinition 2.3. LetA \u2208 RN1N2\u00d7N1N2 . The partial traces Tr1(A) and Tr2(A) are defined as follows:\nTr1(A) := [ Tr(A(ij) ] 1\u2264i,j\u2264N1 \u2208 R N1\u00d7N1 , Tr2(A) := \u2211N1 i=1 A(ii) \u2208 RN2\u00d7N2 .\nThe action of partial traces is easy to visualize: indeed, Tr1(A\u2297B) = Tr(B)A and Tr2(A\u2297B) = Tr(A)B. For us, the most important property of partial trace operators is their positivity.\nProposition 2.4. Tr1 and Tr2 are positive operators, i.e., for A 0, Tr1(A) 0 and Tr2(A) 0.\nProof. Please refer to [4, Chap. 4]."}, {"heading": "3 Learning the kernel matrix for KRONDPP", "text": "In this section, we consider the key difficult task for KRONDPPs: learning a Kronecker product kernel matrix from n observed subsets Y1, . . . , Yn. Using the definition (2) of P(Yi), maximum-likelihood learning of a DPP with kernel L results in the optimization problem:\narg max L 0\n\u03c6(L), \u03c6(L) = 1\nn n\u2211 i=1 (log det(LYi)\u2212 log det(L+ I)) . (3)\nThis problem is nonconvex and conjectured to be NP-hard [15, Conjecture 4.1]. Moreover the constraint L 0 is nontrivial to handle. Writing Ui as the indicator matrix for Yi of size N \u00d7 |Yi| so that LYi = U > i LUi, the gradient of \u03c6 is easily seen to be\n\u2206 := \u2207\u03c6(L) = 1 n \u2211n i=1 UiL \u22121 Yi U>i \u2212 (L+ I)\u22121. (4)\nIn [25], the authors derived an iterative method (\u201cthe Picard iteration\u201d) for computing an L that solves \u2206 = 0 by running the simple iteration\nL\u2190 L+ L\u2206L. (5)\nMoreover, iteration (5) is guaranteed to monotonically increase the log-likelihood \u03c6 [25]. But these benefits accrue at a cost of O(N3) per iteration, and furthermore a direct application of (5) cannot guarantee the Kronecker structure required by KRONDPP."}, {"heading": "3.1 Optimization algorithm", "text": "Our aim is to obtain an efficient algorithm to (locally) optimize (3). Beyond its nonconvexity, the Kronecker structure L = L1 \u2297 L2 imposes another constraint. As in [25] we first rewrite \u03c6 as a function of S = L\u22121, and re-arrange terms to write it as\n\u03c6(S) = log det(S)\ufe38 \ufe37\ufe37 \ufe38 f(S)\n+ 1\nn \u2211n i=1 log det ( U>i S \u22121Ui ) \u2212 log det(I + S)\ufe38 \ufe37\ufe37 \ufe38\ng(S)\n. (6)\nIt is easy to see that f is concave, while a short argument shows that g is convex [25]. An appeal to the convex-concave procedure [29] then shows that updating S by solving \u2207f(S(k+1)) +\u2207g(S(k)) = 0, which is what (5) does [25, Thm. 2.2], is guaranteed to monotonically increase \u03c6.\nBut for KRONDPP this idea does not apply so easily: due the constraint L = L1\u2297L2 the function g\u2297 : (S1, S2)\u2192 1n \u2211n\ni=1 log det\n( U>i (S1 \u2297 S2)\u22121Ui ) \u2212 log det(I + S1 \u2297 S2),\nfails to be convex, precluding an easy generalization. Nevertheless, for fixed S1 or S2 the functions{ f1 : S1 7\u2192 f(S1 \u2297 S2) g1 : S1 7\u2192 g(S1 \u2297 S2) , { f2 : S2 \u2192 f(S1 \u2297 S2) g2 : S2 \u2192 g(S1 \u2297 S2)\nare once again concave or convex. Indeed, the map \u2297 : S1 \u2192 S1 \u2297 S2 is linear and f is concave, and f1 = f \u25e6 \u2297 is also concave; similarly, f2 is seen to be concave and g1 and g2 are convex. Hence, by generalizing the arguments of [29, Thm. 2] to our \u201cblock-coordinate\u201d setting, updating via\n\u2207fi ( Si (k+1) ) = \u2212\u2207gi ( Si (k) ) , for i = 1, 2, (7)\nshould increase the log-likelihood \u03c6 at each iteration. We prove below that this is indeed the case, and that updating as per (7) ensure positive definiteness of the iterates as well as monotonic ascent."}, {"heading": "3.1.1 Positive definite iterates and ascent", "text": "In order to show the positive definiteness of the solutions to (7), we first derive their closed form.\nProposition 3.1 (Positive definite iterates). For S1 0, S2 0, the solutions to (7) are given by the following expressions:\n\u2207f1(X) = \u2212\u2207g1(S1) \u21d0\u21d2 X\u22121 = Tr1((I \u2297 S2)(L+ L\u2206L)) /N2 \u2207f2(X) = \u2212\u2207g2(S2) \u21d0\u21d2 X\u22121 = Tr2 ((S1 \u2297 I)(L+ L\u2206L)) /N1.\nMoreover, these solutions are positive definite.\nProof. The details are somewhat technical, and are hence given in Appendix A. We know that L 0 =\u21d2 L+ L\u2206L \u2265 0, because L\u2212 L(I + L)\u22121L 0. Since the partial trace operators are positive (Prop. 2.4), it follows that the solutions to (7) are also positive definite.\nWe are now ready to establish that these updates ensure monotonic ascent in the log-likelihood.\nTheorem 3.2 (Ascent). Starting with L(0)1 0, L (0) 2 0, updating according to (7) generates positive definite iterates L(k)1 and L (k) 2 , and the sequence { \u03c6 ( L (k) 1 \u2297 L (k) 2 )} k\u22650 is non-decreasing.\nProof. Updating according to (7) generates positive definite matrices Si, and hence positive definite subkernels Li = Si. Moreover, due to the convexity of g1 and concavity of f1, for matrices A,B 0\nf1(B) \u2264 f1(A) +\u2207f1(A)>(B \u2212A), g1(A) \u2265 g1(B) +\u2207g1(B)>(A\u2212B).\nHence, f1(A) + g1(A) \u2265 f1(B) + g1(B) + (\u2207f1(A) +\u2207g1(B))>(A\u2212B). Thus, if S(k)1 , S (k+1) 1 verify (7), by setting A = S (k+1) 1 and B = S (k) 1 we have\n\u03c6 ( L (k+1) 1 \u2297 L (k) 2 ) = f1 ( S (k+1) 1 ) + g1 ( S (k+1) 1 ) \u2265 f1 ( S (k) 1 ) + g1 ( S (k) 1 ) = \u03c6 ( L (k) 1 \u2297 L (k) 2 ) .\nThe same reasoning holds for L2, which proves the theorem.\nAs Tr1((I \u2297 S2)L) = N2L1 (and similarly for L2), updating as in (7) is equivalent to updating L1 \u2190 L1 + Tr1 ( (I \u2297 L\u221212 )(L\u2206L) ) /N2, L2 \u2190 L2 + Tr2 ( (L\u221211 \u2297 I)(L\u2206L) ) /N1.\nGenearlization. We can generalize the updates to take an additional step-size parameter a: L1 \u2190 L1 + aTr1 ( (I \u2297 L\u221212 )(L\u2206L) ) /N2, L2 \u2190 L2 + aTr2 ( (L\u221211 \u2297 I)(L\u2206L) ) /N1.\nExperimentally, a > 1 (as long as the updates remain positive definite) can provide faster convergence, although the monotonicity of the log-likelihood is no longer guaranteed. We found experimentally that the range of admissible a is larger than for Picard, but decreases as N grows larger.\nThe arguments above easily generalize to the multiblock case. Thus, when learning L = L1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 Lm, by writing Eij the matrix with a 1 in position (i, j) and zeros elsewhere, we update Lk as\n(Lk)ij \u2190 (Lk)ij +Nk/(N1 . . . Nm) Tr [(L1 \u2297 . . .\u2297 Lk\u22121 \u2297 Eij \u2297 Lk+1 \u2297 . . .\u2297 Lm)(L\u2206L)] .\nFrom the above updates it is not transparent whether the Kronecker product saves us any computation. In particular, it is not clear whether the updates can be implemented to run faster than O(N3). We show below in the next section how to implement these updates efficiently."}, {"heading": "3.1.2 Algorithm and complexity analysis", "text": "From Theorem 3.2, we obtain Algorithm 1 (which is different from the Picard iteration in [25], because it operates alternatingly on each subkernel). It is important to note that a further speedup to Algorithm 1 can be obtained by performing stochastic updates, i.e., instead of computing the full gradient of the log-likelihood, we perform our updates using only one (or a small minibatch) subset Yi at each step instead of iterating over the entire training set; this uses the stochastic gradient \u2206 = UiL \u22121 Yi U>i \u2212 (I + L)\u22121. The crucial strength of Algorithm 1 lies in the following result:\nAlgorithm 1 KRK-PICARD iteration Input: Matrices L1, L2, training set T , parameter a. for i = 1 to maxIter do L1 \u2190 L1 + aTr1 ( (I \u2297 L\u221212 )(L\u2206L) ) /N2 // or update stochastically\nL2 \u2190 L2 + aTr2 ( (L\u221211 \u2297 I)(L\u2206L) ) /N1 // or update stochastically\nend for return (L1, L2)\nTheorem 3.3 (Complexity). For N1 \u2248 N2 \u2248 \u221a N , the updates in Algorithm 1 can be computed in O(n\u03ba3 +N2) time andO(N2) space, where \u03ba is the size of the largest training subset. Furthermore, stochastic updates can be computed in O(N\u03ba2 +N3/2) time and O(N + \u03ba2) space.\nIndeed, by leveraging the properties of the Kronecker product, the updates can be obtained without computing L\u2206L. This result is non-trivial: the components of \u2206, 1n \u2211 i UiL \u22121 Yi U>i and (I + L)\u22121, must be considered separately for computational efficiency. The proof is provided in App. B. However, it seems that considering more than 2 subkernels does not lead to further speed-ups.\nIf N1 \u2248 N2 \u2248 \u221a N , these complexities become:\n\u2013 for non-stochastic updates: O(n\u03ba3 +N2) time, O(N2) space, \u2013 for stochastic updates: O(N\u03ba3 +N3/2) time, O(\u03ba2 +N) space.\nThis is a marked improvement over [25], which runs in O(N2) space and O(n\u03ba3 + N3) time (non-stochastic) or O(N3) time (stochastic); Algorithm 1 also provides faster stochastic updates than [9]. However, one may wonder if by learning the sub-kernels by alternating updates the loglikelihood converges to a sub-optimal limit. The next section discusses how to jointly update L1 and L2."}, {"heading": "3.2 Joint updates", "text": "We also analyzed the possibility of updating L1 and L2 jointly: we update L\u2190 L+ L\u2206L and then recover the Kronecker structure of the kernel by defining the updates L\u20321 and L\n\u2032 2 such that:{\n(L\u20321, L \u2032 2) minimizes \u2016L+ L\u2206L\u2212 L\u20321 \u2297 L\u20322\u20162F L\u20321 0, L\u20322 0, \u2016L\u20321\u2016 = \u2016L\u20322\u2016 (8)\nWe show in appendix C that such solutions exist and can be computed by from the first singular value and vectors of the matrix R = [ vec((L\u22121 + \u2206)(ij))> ]N1 i,j=1\n. Note however that in this case, there is no guaranteed increase in log-likelihood. The pseudocode for the related algorithm (JOINT-PICARD) is given in appendix C.1. An analysis similar to the proof of Thm. 3.3 shows that the updates can be obtained O(n\u03ba3 + max(N1, N2)4)."}, {"heading": "3.3 Memory-time trade-off", "text": "Although KRONDPPS have tractable learning algorithms, the memory requirements remain high for non-stochastic updates, as the matrix \u0398 = 1n \u2211 i UiL \u22121 Yi U>i needs to be stored, requiring O(N2)\nmemory. However, if the training set can be subdivised such that\n{Y1, . . . , Yn} = \u222amk=1Sk s.t. \u2200k, |\u222aY \u2208SkY | < z, (9)\n\u0398 can be decomposed as 1n \u2211m k=1 \u0398k with \u0398k = \u2211 Yi\u2208Sk UiL \u22121 Yi U>i . Due to the bound in Eq. 9, each \u0398k will be sparse, with only z2 non-zero coefficients. We can then store each \u0398k with minimal storage and update L1 and L2 in O(n\u03ba3 +mz2 +N3/2) time and O(mz2 +N) space.\nDetermining the existence of such a partition of size m is a variant of the NP-Hard Subset-Union Knapsack Problem (SUKP) [11] with m knapsacks and where the value of each item (i.e. each Yi) is equal to 1: a solution to SUKP of value n with m knapsacks is equivalent to a solution to 9. However, an approximate partition can also be simply constructed via a greedy algorithm."}, {"heading": "4 Sampling", "text": "Sampling exactly (see Alg. 2 and [17]) from a full DPP kernel costs O(N3 +Nk3) where k is the size of the sampled subset. The bulk of the computation lies in the initial eigendecomposition of L; the k orthonormalizations cost O(Nk3). Although the eigendecomposition need only happen once for many iterations of sampling, exact sampling is nonetheless intractable in practice for large N .\nAlgorithm 2 Sampling from a DPP kernel L Input: Matrix L. Eigendecompose L as {(\u03bbi, vi)}1\u2264i\u2264N . J \u2190 \u2205 for i = 1 to N do J \u2192 J \u222a {i} with probability \u03bbi/(\u03bbi + 1).\nend for V \u2190 {vi}i\u2208J , Y \u2190 \u2205 while |V | > 0 do\nSample i from {1 . . . N} with probability 1|V | \u2211 v\u2208V v 2 i\nY \u2190 Y \u222a {i}, V \u2190 V\u22a5, where V\u22a5 is an orthonormal basis of the subspace of V orthonormal to ei end while return Y\nIt follows from Prop. 2.2 that for KRONDPPS, the eigenvalues \u03bbi can be obtained inO(N31 +N32 ), and the k eigenvectors in O(kN) operations. For N1 \u2248 N2 \u2248 \u221a N , exact sampling thus only costs O(N3/2 + Nk3). If L = L1 \u2297 L2 \u2297 L3, the same reasoning shows that exact sampling becomes linear in N , only requiring O(Nk3) operations.\nOne can also resort to MCMC sampling; for instance such a sampler was considered in [13] (though with an incorrect mixing time analysis). The results of [20] hold only for k-DPPs, but suggest their MCMC sampler may possibly take O(N2 log(N/ )) time for full DPPs, which is impractical. Nevertheless if one develops faster MCMC samplers, they should also be able to profit from the Kronecker product structure offered by KRONDPP."}, {"heading": "5 Experimental results", "text": "In order to validate our learning algorithm, we compared KRK-PICARD to JOINT-PICARD and to the Picard iteration (PICARD) on multiple real and synthetic datasets.1"}, {"heading": "5.1 Synthetic tests", "text": "All three algorithms were used to learn from synthetic data drawn from a \u201ctrue\u201d kernel. The subkernels were initialized by Li = X>X , with X\u2019s coefficients drawn uniformly from [0, \u221a 2]; for PICARD, L was initialized with L1 \u2297 L2. For Figures 1a and 1b, training data was generated by 1All experiments were repeated 5 times and averaged, using MATLAB on a Linux Mint system with 16GB of RAM and an i7-4710HQ CPU @ 2.50GHz.\nsampling 100 subsets from the true kernel with sizes uniformly distributed between 10 and 190. To evaluate KRK-PICARD on matrices too large to fit in memory and with large \u03ba, we drew samples from a 50 \u00b7 103\u00d750 \u00b7 103 kernel of rank 1, 000 (on average |Yi| \u2248 1, 000), and learned the kernel stochastically (only KRK-PICARD could be run due to the memory requirements of other methods); the likelihood drastically improves in only two steps (Fig.1c).\nAs shown in Figures 1a and 1b, KRK-PICARD converges significantly faster than PICARD, especially for large values of N . However, although JOINT-PICARD also increases the log-likelihood at each iteration, it converges much slower and has a high standard deviation, whereas the standard deviations for PICARD and KRK-PICARD are barely noticeable. For these reasons, we drop the comparison to JOINT-PICARD in the subsequent experiments."}, {"heading": "5.2 Small-scale real data: baby registries", "text": "We compared KRK-PICARD to PICARD and EM [10] on the baby registry dataset (described in-depth in [10]), which has also been used to evaluate other DPP learning algorithms [9, 10, 25]. The dataset contains 17 categories of baby-related products obtained from Amazon. We learned kernels for the 6 largest categories (N = 100); in this case, PICARD is sufficiently efficient to be prefered to KRK-PICARD; this comparison serves only to evaluate the quality of the final kernel estimates.\nThe initial marginal kernel K for EM was sampled from a Wishart distribution with N degrees of freedom and an identity covariance matrix, then scaled by 1/N ; for PICARD, L was set to K(I \u2212 K)\u22121; for KRK-PICARD, L1 and L2 were chosen (as in JOINT-PICARD) by minimizing \u2016L\u2212 L1 \u2297 L2\u2016. Convergence was determined when the objective change dipped below a threshold \u03b4. As one EM iteration takes longer than one Picard iteration but increases the likelihood more, we set \u03b4PIC = \u03b4KRK = 10\u22124 and \u03b4EM = 10\u22125.\nThe final log-likelihoods are shown in Table 1; we set the step-sizes to their largest possible values, i.e. aPIC = 1.3 and aKRK = 1.8. Table 1 shows that KRK-PICARD obtains comparable, albeit slightly worse log-likelihoods than PICARD and EM, which confirms that for tractable N , the better modeling capability of full kernels make them preferable to KRONDPPS."}, {"heading": "5.3 Large-scale real dataset: GENES", "text": "Finally, to evaluate KRK-PICARD on large matrices of real-world data, we train it on data from the GENES [3] dataset (which has also been used to evaluate DPPs in [3, 19]). This dataset consists in 10,000 genes, each represented by 331 features corresponding to the distance of a gene to hubs in the BioGRID gene interaction network.\nWe construct a ground truth Gaussian DPP kernel on the GENES dataset and use it to obtain 100 training samples with sizes uniformly distributed between 50 and 200 items. Similarly to the\nsynthetic experiments, we initialized KRK-PICARD\u2019s kernel by setting Li = X>i Xi where Xi is a random matrix of size N1 \u00d7N1; for PICARD, we set the initial kernel L = L1 \u2297 L2.\nFigure 2 shows the performance of both algorithms. As with the synthetic experiments, KRKPICARD converges much faster; stochastic updates increase its performance even more, as shown in Fig. 2b. Average runtimes and speed-up are given in Table 2: KRK-PICARD runs almost an order of magnitude faster than PICARD, and stochastic updates are more than two orders of magnitude faster, while providing slightly larger initial increases to the log-likelihood."}, {"heading": "6 Conclusion and future work", "text": "We introduced KRONDPPS, a variant of DPPs with kernels structured as the Kronecker product of m smaller matrices, and showed that typical operations over DPPs such as sampling and learning the kernel from data can be made efficient for KRONDPPS on previously untractable ground set sizes.\nBy carefully leveraging the properties of the Kronecker product, we derived for m = 2 a lowcomplexity algorithm to learn the kernel from data which guarantees positive iterates and a monotonic increase of the log-likelihood, and runs in O(n\u03ba3 +N2) time. This algorithm provides even more significant speed-ups and memory gains in the stochastic case, requiring only O(N3/2 +N\u03ba2) time\nand O(N + \u03ba2) space. Experiments on synthetic and real data showed that KRONDPPS can be learned efficiently on sets large enough that L does not fit in memory.\nWhile discussing learning the kernel, we showed that L1 and L2 cannot be updated simultaneously in a CCCP-style iteration since g is not convex over (S1, S2). However, it can be shown that g is geodesically convex over the Riemannian manifold of positive definite matrices, which suggests that deriving an iteration which would take advantage of the intrinsic geometry of the problem may be a viable line of future work.\nKRONDPPS also enable fast sampling, in O(N3/2 + Nk3) operations when using two subkernels and in O(Nk3) when using three sub-kernels; this allows for exact sampling at comparable or even better costs than previous algorithms for approximate sampling. However, as we improve computational efficiency L, the subset size k becomes limiting, due to the O(Nk3) cost of sampling and learning. A necessary line of future work to allow for truly scalable DPPs is thus to overcome this computational bottleneck."}, {"heading": "Appendix: Kronecker Determinantal Point Processes", "text": ""}, {"heading": "A Proof of Prop. 3.1", "text": "We use \u2018vec\u2019 to denote the operator that stacks columns of a matrix to form a vector; conversely, \u2018mat\u2019 takes a vector with k2 coefficients and returns a k \u00d7 k matrix.\nLet L = L1 \u2297 L2, S1 = L\u221211 , S2 = L \u22121 2 and S = S1 \u2297 S2 = L\u22121. We note Eij the matrix with\nall zeros except for a 1 at position (i, j), its size being clear from context. We wish to solve\n\u2207f2(X) = \u2212\u2207g2(S1) and \u2207f1(X) = \u2212\u2207g1(S2) (10)\nIt follows from the fact that\nlog det(S1 \u2297 S2) = N2 log detS1 +N1 log detS2\nthat\u2207fS2(X) = N2X\u22121 and \u2207fS1(X) = N1X\u22121. Moreover, we know that\n\u2207g(S) = \u2212(I + S)\u22121 \u2212 S\u22121 1 n \u2211 i Ui(U > i S \u22121Ui) \u22121UiS \u22121\n= \u2212S\u22121 \u2212 S\u22121 ( 1\nn \u2211 i Ui(U > i S \u22121Ui) \u22121Ui \u2212 (I + S\u22121)\u22121 ) S\u22121\n= \u2212(L+ L\u2206L).\nThe Jacobian of S1 \u2192 S1\u2297S2 is given by J = ( vec(E11 \u2297 S2), . . . , vec(EN1N1 \u2297 S2) ) . Hence,\n\u2207f1(X)ij = \u2212(\u2207g1(S1))ij \u21d0\u21d2 N2X\u22121ij = (J > vec(\u2212\u2207g(S)))ij\n\u21d0\u21d2 N2X\u22121ij = vec(Eij \u2297 S2) > vec(L+ L\u2206L) \u21d0\u21d2 N2X\u22121ij = Tr((Eij \u2297 S2)(L+ L\u2206L)) \u21d0\u21d2 N2X\u22121ij = Tr(S2(L+ L\u2206L)(ij)) \u21d0\u21d2 N2X\u22121ij = Tr ( ((I \u2297 S2)(L+ L\u2206L))(ij)\n) The last equivalence is simply the result of indices manipulation. Thus, we have\n\u2207f2(X) = \u2212\u2207g2(S1) \u21d0\u21d2 X\u22121 = 1\nN2 Tr1((I \u2297 S2)(L+ L\u2206L))\nSimilarly, by setting J \u2032 = (vec(S1 \u2297 E11), . . . , vec(S1 \u2297 EN1N1)), we have that\n\u2207f2(X)ij = \u2212(\u2207g2(S2))ij \u21d0\u21d2 N1X\u22121ij = (J \u2032> vec(\u2212\u2207g(S)))ij\n\u21d0\u21d2 N1X\u22121ij = vec(S1 \u2297 Eij) > vec(L+ L\u2206L) \u21d0\u21d2 N1X\u22121ij = Tr((S1 \u2297 Eij)(L+ L\u2206L))\n\u21d0\u21d2 N1X\u22121ij = (\u2211N1\nk,`=1 S1k`(L+ L\u2206L)(`k) ) ij\n\u21d0\u21d2 N1X\u22121ij = (\u2211N1\n`=1 ((S1 \u2297 I)(L+ L\u2206L))(``) ) ij\nHence,\n\u2207fS1(X) = \u2212\u2207gS1(S2) \u21d0\u21d2 X\u22121 = 1\nN1 Tr2 ((S1 \u2297 I)(L+ L\u2206L)) ,\nwhich proves Prop. 3.1."}, {"heading": "B Efficient updates for KRK-PICARD", "text": "The updates to L1 and L2 are obtained efficiently through different methods; hence, the proof to Thm. 3.3 is split into two sections. We write\n\u0398 = 1\nn n\u2211 i=1 UiL \u22121 Yi U>i (or \u0398 = UiL \u22121 Yi U>i for stochastic updates)\nso that \u2206 = \u0398\u2212 (I + L)\u22121. Recall that (A\u2297B)(ij) = aijB.\nB.1 Updating L1 We wish to compute X = Tr1 ( (I \u2297 L\u221212 )(L\u2206L) ) efficiently. We have\nXij = Tr [ ((I \u2297 L\u221212 )(L\u2206L))(ij) ] = Tr [ L\u221212 (L\u2206L)(ij)\n] = Tr [ L\u221212 \u2211N1 k,`=1 L(ik)\u2206(k`)L(`j)\n] = \u2211N1\nk,`=1 L1ikL1`j Tr(L\n\u22121 2 L2\u2206(k`)L2)\n= \u2211N1\nk,`=1 L1ikL1`j Tr(\u0398(k`)L2)\ufe38 \ufe37\ufe37 \ufe38\nAk`\n\u2212Tr((I + L)\u22121(k`)L2)\ufe38 \ufe37\ufe37 \ufe38 Bk`\n= (L1AL1 \u2212 L1BL1)ij .\nThe N1 \u00d7 N1 matrix A can be computed in O(n\u03ba3 + N21N22 ) simply by pre-computing \u0398 in O(n\u03ba3) and then computing all N21 traces in O(N22 ) time. When doing stochastic updates for which \u0398 is sparse with only \u03ba2 non-zero coefficients, computing A can be done in O(N21\u03ba2 + \u03ba3).\nBy diagonalizing L1 = P1D1P>1 and L2 = P2D2P > 2 , we have (I + L) \u22121 = PDP> with P = P1\u2297P2 andD = (I+D1\u2297D2)\u22121. P1, P2, D1, D2 andD can all be obtained inO(N31 +N32 +N1N2) as a consequence of Prop. 2.1. Then\nBij = Tr((I + L) \u22121 (ij)L2) = \u2211 k Tr(P(ik)D(kk)P > (kj)L2)\n= \u2211 k P1ikP1jk Tr(P2D(kk)P > 2 P2D2P > 2 )\n= \u2211 k\nP1ikP1jk Tr(D(kk)D2)\ufe38 \ufe37\ufe37 \ufe38 \u03b1k .\nLet D\u0302 = diag(\u03b11, . . . , \u03b1N1), which can be computed in O(N1N2). Then L1BL1 = P1D1D\u0302D1P1 is computable in O(N31 +N32 ).\nOverall, the update to L1 can be computed inO(n\u03ba3 +N21N22 +N31 +N32 ), or inO(N21\u03ba2 +\u03ba3 + N31 +N 3 2 ) if the updates are stochastic. Moreover, if \u0398 is sparse with only z non-zero coefficients (for stochastic updates z = \u03ba),A can be computed inO(\u03ba2) space, leading to an overallO(z2+N21 +N22 ) memory cost.\nB.2 Updating L2 We wish to compute X = Tr2 [ (L\u221211 \u2297 I)(L\u2206L) ] efficiently.\nX = \u2211N1\ni=1\n( (L\u221211 \u2297 I)(L\u2206L) ) (ii)\n= \u2211N1\ni=1\n( (I \u2297 L2)(\u0398\u2212 (I + L)\u22121)(L1 \u2297 L2) ) (ii)\n= \u2211N1\ni,j=1 L1ijL2\u0398(ij)L2 \u2212 N1\u2211 i=1 ((I \u2297 L2)(I + L)\u22121(L1 \u2297 L2))(ii)\n= L2 \u2211N1\ni,j=1 L1ij\u0398(ij)L2\ufe38 \ufe37\ufe37 \ufe38 A\n\u2212 \u2211N1\ni=1 ((I \u2297 L2)(I + L)\u22121(L1 \u2297 L2))(ii)\ufe38 \ufe37\ufe37 \ufe38\nB\nA can be computed in O(n\u03ba3 +N21N22 +N32 ). As before, when doing stochastic updates A can be computed in O(N21\u03ba2 + \u03ba3 +N32 ) time and O(N22 +N21 + \u03ba2) space due to the sparsity of \u0398.\nRegarding B, as all matrices commute, we can write\n(I \u2297 L2)(I + L)\u22121(L1 \u2297 L2) = (P1 \u2297 P2)\u039b(P1 \u2297 P2)\nwhere \u039b = (I\u2297D2)(I+D1\u2297D2)\u22121(D1\u2297D2) is diagonal and is obtained inO(N31 +N32 +N1N2). Moreover,\nB = \u2211N1\ni=1 (P\u039bP>)(ii) = P2 (\u2211N1 i,k=1 P1ik\u039b(kk)P1ik ) P>2 ,\nwhich allows us to compute B in O(N21N2 +N32 +N31 ) total. Overall, we can obtain X in O(n\u03ba3 +N21N22 +N31 +N32 ) or in O(N21\u03ba2 +N21N2 +N31 +N32 ) for stochastic updates, in which case only O(N21 +N22 + \u03ba2) space is necessary."}, {"heading": "C Proof of validity for joint updates", "text": "In order to minimize the number of matrix multiplications, we equivalently (due to the properties of the Frobenius norm) minimize the equation\n\u2016L\u22121 + \u2206\u2212X \u2297 Y \u20162F (11)\nand set { L\u20321 \u2190 L1XL1 L\u20322 \u2190 L2Y L2. .\nTheorem C.1. Let L 0. Define R := [vec(L(11))>; . . . ; vec(L(N1N1))>] N1 i,j=1 \u2208 RN1N1\u00d7N2N2 .\nSuppose that R has an eigengap between its largest singular value and the next, and let u, v, \u03c3 be the first singular vectors and value of R. Let U = mat(u) and V = mat(v). Then U and V are either both positive definite or negative definite.\nMoreover, for any value \u03b1 6= 0, the pair (\u03b1U, \u03c3/\u03b1V ) minimizes \u2016L\u2212X \u2297 Y \u20162F .\nThe proof is a consequence of [22, Thm. 11]. This shows that if L is initially positive definite, setting the sign of \u03b1 based on whether U and V are positive or negative definite2, and updating{\nL1 \u2190 \u03b1L1UL1 L2 \u2190 \u03c3/\u03b1L2V L2\n2This can easily be done simply by checking the sign of the first diagonal coefficient of U , which will be positive if and only if U 0.\nmaintains positive definite iterates. Given that if L1 0 and L2 0, L1 \u2297 L2 0, a simple induction then shows that by choosing an initial kernel estimate L 0, subsequent values of L will remain positive definite.\nBy choosing \u03b1 such that the new estimates L1 and L2 verify \u2016L1\u2016 = \u2016L2\u2016, we verify all the conditions of Eq. 8.\nC.1 Algorithm for joint updates Theorem C.1 leads to a straightforward iteration for learning matrices L1 and L2 based on the decomposition of the Picard estimate as a Kronecker product.\nAlgorithm 3 JOINT-PICARD iteration Input: Matrices L1, L2, training set T , step-size a \u2265 1. for i = 1 to maxIter do U, \u03c3, V \u2190 power_method(L\u22121 + \u2206) to obtain the first singular value and vectors of matrix R. \u03b1\u2190 sgn(U11) \u221a \u03c3\u2016L2V L2\u2016/\u2016L1UL1\u2016\nL1 \u2190 L1 + a(\u03b1L1UL1 \u2212 L1) L2 \u2190 L2 + a(\u03c3/\u03b1L2V L2)\nend forreturn (L1, L2)"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set<lb>of N items. They have recently gained prominence in several applications that rely on \u201cdiverse\u201d<lb>subsets. However, their applicability to large problems is still limited due to theO(N) complexity<lb>of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs<lb>by introducing KRONDPP, a DPP model whose kernel matrix decomposes as a tensor product of<lb>multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling.<lb>But contrary to what one may expect, leveraging the Kronecker product structure for speeding up<lb>DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and<lb>stochastic optimization algorithms for efficiently learning the parameters of a KRONDPP.", "creator": "LaTeX with hyperref package"}}}