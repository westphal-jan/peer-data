{"id": "1605.08988", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2016", "title": "On Explore-Then-Commit strategies", "abstract": "full-page We spotts study gesar the seeger problem of minimising yau regret chd in jetliner two - armed bandit mkz problems 1990-98 with \u0101rya Gaussian ganor rewards. cemento Our prebendal objective armm is to kochta use this damascene simple ehrman setting danz to t. illustrate that malalai strategies collaborators based canteloupe on an exploration broom phase (up to a dyble stopping 15-percent time) followed 4-6-2 by rockleigh exploitation are degrees necessarily iskar suboptimal. The 71-56 results flensing hold sehs regardless sanghas of whether or kokernag not the ripka difference jovito in binga means between nrotc the two arms six-storey is anac known. tdt Besides backlands the main message, 3,610 we also mcgirr refine southington existing deviation otava inequalities, which skopljak allow ndur us to design italcementi fully harpeth sequential custis strategies with 6-cent finite - delay time aristos regret te'o guarantees that are (a) port-la-joye asymptotically optimal 36sec as the horizon blocking grows and (b) order - libu\u0161e optimal asana in mostafavi the dpuc minimax zhang sense. Furthermore ense\u00f1anza we provide empirical evidence that rukh the theory also 303.5 holds pleiotropic in practice and discuss extensions to friburguense non - new-built gaussian and telluride multiple - armed case.", "histories": [["v1", "Sun, 29 May 2016 10:35:33 GMT  (641kb,D)", "http://arxiv.org/abs/1605.08988v1", null], ["v2", "Mon, 14 Nov 2016 12:40:20 GMT  (322kb,D)", "http://arxiv.org/abs/1605.08988v2", null]], "reviews": [], "SUBJECTS": "math.ST cs.LG stat.TH", "authors": ["aur\u00e9lien garivier", "tor lattimore", "emilie kaufmann"], "accepted": true, "id": "1605.08988"}, "pdf": {"name": "1605.08988.pdf", "metadata": {"source": "CRF", "title": "On Explore-Then-Commit Strategies", "authors": ["Aur\u00e9lien Garivier", "Emilie Kaufmann"], "emails": ["aurelien.garivier@math.univ-toulouse.fr", "emilie.kaufmann@inria.fr", "tor.lattimore@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Imagine a company wanting to optimise its daily profit by choosing between one of two possible website layouts. A natural approach is to start with a period of A/B Testing (exploration) during which the two versions are uniformly presented to users. Once the testing is complete, the company displays the version believed to generate the most profit for the rest of the day (exploitation). The time spent exploring may be chosen adaptively based on past observations, but could also be fixed in advance. Our contribution is to show that strategies of this form are much worse than if the company is allowed to dynamically select which website to display without restrictions for the whole day.\nOur analysis focusses on a simple sequential decision problem played over T time-steps. In time-step t \u2208 1, 2, . . . , T the agent chooses an action At \u2208 {1, 2} and receives a normally distributed reward Zt \u223c N (\u00b5At , 1) where \u00b51, \u00b52 \u2208 R are the unknown mean rewards for actions 1 and 2 respectively.\n\u2217This work was partially supported by the CIMI (Centre International de Math\u00e9matiques et d\u2019Informatique) Excellence program while Emilie Kaufmann visited Toulouse in November 2015. The authors acknowledge the support of the French Agence Nationale de la Recherche (ANR), under grants ANR-13-BS01-0005 (project SPADRO) and ANR-13-CORD-0020 (project ALICIA).\nar X\niv :1\n60 5.\n08 98\n8v 1\n[ m\nThe goal is to find a strategy \u03c0 (a way of choosing each action At based on past observation) that maximises the cumulative reward over T steps in expectation, or equivalently minimises the regret\nR\u03c0\u00b5(T ) = T max {\u00b51, \u00b52} \u2212 E\u00b5 [ T\u2211 t=1 \u00b5At ] . (1)\nThis framework is known as the multi-armed bandit problem, which has many applications and has been studied for almost a century [Thompson, 1933]. Although this setting is now quite well understood, the purpose of this article is to show that strategies based on distinct phases of exploration and exploitation are necessarily suboptimal. This is an important message because exploration followed by exploitation is the most natural approach and is often implemented in applications (including the website optimisation problem described above). Moreover, strategies of this kind have been proposed in the literature for more complicated settings [Auer and Ortner, 2010, Perchet and Rigollet, 2013, Perchet et al., 2015]. Recent progress on optimal exploration policies (e.g., by Garivier and Kaufmann [2016]) could have suggested that well-tuned variants of two-phase strategies might be near-optimal. We show, on the contrary, that optimal strategies for multi-armed bandit problems must be fully-sequential, and in particular should mix exploration and exploitation.\nWe study two settings, one when the gap \u2206 = |\u00b51 \u2212 \u00b52| is known and the other when it is not. The most straight-forward strategy in the former case is to explore each action a fixed number of times n and subsequently exploit by choosing the action that appeared best while exploring. It is easy to calculate the optimal n and consequently show that this strategy suffers a regret of R\u03c0\u00b5(T ) \u223c 4 log(T )/\u2206. A more general approach is to use a so-called Explore-Then-Commit (ETC) strategy, following a nomenclature introduced by Perchet et al. [2015]. An ETC strategy explores each action alternately until some data-dependent stopping time and subsequently commits to a single action for the remaining time-steps. We show in Theorem 2 that by using a sequential probability ratio test (SPRT) it is possible to design an ETC strategy for which R\u03c0\u00b5(T ) \u223c log(T )/\u2206, which improves on the above result by a factor of 4. We also prove a lower bound showing that no ETC strategy can improve on this result. Surprisingly it is possible to do even better by using a fully sequential strategy inspired by the UCB algorithm for multi-armed bandits [Katehakis and Robbins, 1995]. We design a new strategy for which R\u03c0\u00b5(T ) \u223c log(T )/(2\u2206), which improves on the fixed-design strategy by a factor of 8 and on SPRT by a factor of 2. Again we prove a lower bound showing that no strategy can improve on this result.\nFor the case where \u2206 is unknown, fixed-design strategies are hopeless because there is no reasonable tuning for the exploration budget n. However, it is possible to design an ETC strategy for unknown gaps. Our approach uses a modified fixed-budget best arm identification (BAI) algorithm in its exploration phase (see e.g., Even-Dar et al. [2006], Garivier and Kaufmann [2016]) and chooses the recommended arm for the remaining time-steps. In Theorem 5 we show that a strategy based on this idea satisfies R\u03c0\u00b5(T ) \u223c 4 log(T )/\u2206, which again we show is optimal within the class of ETC strategies. As before, strategies based on ETC are suboptimal by a factor of 2 relative to the optimal rates achieved by fully sequential strategies such as UCB, which satisfies R\u03c0\u00b5(T ) \u223c 2 log(T )/\u2206 [Katehakis and Robbins, 1995].\nIn a nutshell, strategies based on fixed-design or ETC are necessarily suboptimal. That this failure occurs even in the simple setting considered here is a strong indicator that they are suboptimal in more complicated settings. Our main contribution, presented in more details in Section 2, is to fully characterise the achievable asymptotic regret when \u2206 is either known or unknown and the strategies are either fixed-design, ETC or fully sequential. All upper bounds have explicit finite-time forms, which allow us to derive optimal minimax guarantees. For the lower bounds we give a novel and generic proof of all results. All proofs contain new, original ideas that we believe are fundamental to the understanding of sequential analysis."}, {"heading": "2 Notation and Summary of Results", "text": "We assume that the horizon T is known to the agent. The optimal action is a\u2217 = arg max(\u00b51, \u00b52), its mean reward is \u00b5\u2217 = \u00b5a\u2217 , and the gap between the means is \u2206 = |\u00b51 \u2212 \u00b52|. LetH = R2 be the set of all possible pairs of means, andH\u2206 = { \u00b5 \u2208 R2 : |\u00b51 \u2212 \u00b52| = \u2206 } . For i \u2208 {1, 2} and n \u2208 N let \u00b5\u0302i,n be the empirical mean of the ith action based on the first n samples. Let At be the action chosen in time-step t and Ni(t) = \u2211t s=1 1 {As = i} be the number of times the ith action has been chosen after time-step t. We denote by \u00b5\u0302i(t) = \u00b5\u0302i,Ni(t) the empirical mean of the ith arm after time-step t.\nA strategy is denoted by \u03c0, which is a function from past actions/rewards to a distribution over the next actions. An ETC strategy is governed by a sampling rule (which determines which arm to sample at each step), a stopping rule (which specifies when to stop the exploration phase) and a decision rule indicating which arm is chosen in the exploitation phase. As we consider two-armed, Gaussian bandits with equal variances, we focus here on uniform sampling rules, which have been shown in Kaufmann et al. [2014] to be optimal in that setting. For this reason, we define an ETC strategy as a pair (\u03c4, a\u0302), where \u03c4 is an even stopping time with respect to the filtration (Ft = \u03c3(Z1, . . . , Zt))t and a\u0302 \u2208 {1, 2} is F\u03c4 -measurable. In all the ETC strategies presented in this paper, the stopping time \u03c4 depends on the horizon T (although this is not reflected in the notation). At time t, the action\npicked by the ETC strategy is At =  1 if t \u2264 \u03c4 and t is odd , 2 if t \u2264 \u03c4 and t is even , a\u0302 otherwise .\nThe regret for strategy \u03c0, given in Eq. (1), depends on T and \u00b5. Assuming, for example that \u00b51 = \u00b52 + \u2206, then an ETC strategy \u03c0 chooses the suboptimal arm N2(T ) = \u03c4\u2227T2 + (T \u2212 \u03c4)+1 {a\u0302 = 2} times, and the regret R\u03c0\u00b5(T ) = \u2206E\u00b5[N2(T )] thus satisfies\n\u2206E\u00b5[(\u03c4 \u2227 T )/2] \u2264 R\u03c0\u00b5(T ) \u2264 (\u2206/2)E\u00b5[\u03c4 ] + \u2206T P\u00b5(\u03c4 \u2264 T, a\u0302 6= a\u2217) . (2)\nWe denote the set of all ETC strategies by \u03a0ETC. A fixed-design strategy is and ETC strategy for which there exists an integer n such that \u03c4 = 2n almost surely, and the set of all such strategies is denoted by \u03a0DETC. The set of all strategies is denoted by \u03a0ALL. For S \u2208 {H,H\u2206}, we are interested in strategies \u03c0 that are uniformly efficient on S, in the sense that\n\u2200\u00b5 \u2208 S,\u2200\u03b1 > 0, R\u03c0\u00b5(T ) = o(T\u03b1). (3)\n\u03a0ALL \u03a0ETC \u03a0DETC\nH 2 4 NA H\u2206 1/2 1 4 We show in this paper that any uniformly efficient strategy in \u03a0 has a regret at least equal to C\u03a0S log(T )/|\u00b51 \u2212 \u00b52|(1\u2212 oT (1)) for every parameter \u00b5 \u2208 S , where C\u03a0S is given in the adjacent table. Furthermore, we prove that these results are tight. In each case, we propose a uniformly efficient strategy matching this bound. In addition, we prove a tight and non-asymptotic regret bound which also implies, in particular, minimax rate-optimality.\nThe paper is organised as follows. First we consider ETC and fixed-design strategies when \u2206 known and unknown (Section 3). We then analyse fully sequential strategies that interleave exploration and exploitation in an optimal way (Section 4). For known \u2206 we present a novel algorithm that exploits the additional information to improve the regret. For unknown \u2206 we briefly recall the well-known results, but also propose a new regret analysis of the UCB* algorithm, a variant of UCB that can be traced back to Lai [1987], for which we also obtain order-optimal minimax regret. A novel, unified proof for all the lower bounds is given in Section 5. All other proofs are deferred to the appendix, which also contains some numerical experiments corroborating the theoretical results.We conclude with a short discussion on non-uniform exploration, and on models with more than 2 arms, possibly non Gaussian.\n3 Explore-Then-Commit Strategies input: T and \u2206 n := \u2308 2W ( T 2\u22064/(32\u03c0) ) /\u22062\n\u2309 for k \u2208 {1, . . . , n} do\nchoose A2k\u22121 = 1 and A2k = 2 end for a\u0302 := arg maxi \u00b5\u0302i,n for t \u2208 {2n+ 1, . . . , T} do\nchoose At = a\u0302 end for Algorithm 1: FB-ETC algorithm Fixed Design Strategies for Known Gaps. As a warm-up we start with the fixed-design ETC setting where \u2206 is known and where the agent chooses each action n times before committing for the remainder. The optimal decision rule is obviously a\u0302 = arg maxi \u00b5\u0302i,n with ties broken arbitrarily. The formal description of the strategy is given in Algorithm 1, where W denotes the Lambert function implicitly defined for y > 0 by W (y) exp(W (y)) = y. We denote the regret associated to the choice of n by Rn\u00b5(T ). The following theorem is not especially remarkable except that the bound is sufficiently refined to show certain negative lower-order terms that would otherwise not be apparent.\nTheorem 1. Let \u00b5 \u2208 H\u2206, and let\nn =\n\u2308 2\n\u22062 W\n( T 2\u22064\n32\u03c0\n)\u2309 . Then Rn\u00b5(T ) \u2264 4\n\u2206 log\n( T\u22062\n4.46\n) \u2212 2\n\u2206 log log\n( T\u22062\n4 \u221a 2\u03c0\n) + \u2206\nwhenever T\u22062 > 4 \u221a 2\u03c0e, andRn\u00b5(T ) \u2264 T\u2206/2+\u2206 otherwise. In all cases, Rn\u00b5(T ) \u2264 2.04 \u221a T +\u2206. Furthermore, for all \u03b5 > 0, T \u2265 1 and n \u2264 4(1\u2212 \u03b5) log(T )/\u22062,\nRn\u00b5(T ) \u2265 (\n1\u2212 2 n\u22062\n)( 1\u2212 8 log(T )\n\u22062T\n) \u2206T \u03b5\n2 \u221a \u03c0 log(T ) .\nAs Rn\u00b5(T ) \u2265 n\u2206, this entails that inf 1\u2264n\u2264T Rn\u00b5(T ) \u223c 4 log(T )/\u2206.\nThe proof of Theorem 1 is in Appendix A. Note that the \"asymptotic lower bound\" 4 log(T )/\u2206 is actually not a lower bound, even up to an additive constant: Rn\u00b5(T ) \u2212 4 log(T )/\u2206 \u2192 \u2212\u221e when T \u2192\u221e. Actually, the same phenomenon applies many other cases, and it should be no surprise that, in numerical experiments, some algorithm reach a regret smaller than Lai and Robbins asymptotic lower bound, as was already observed in several articles (see e.g. Garivier et al. [2016]). Also note that the term \u2206 at the end of the upper bound is necessary: if \u2206 is large, the problem is statistically so simple that one single observation is sufficient to identify the best arm; but that observation cannot be avoided.\nExplore-Then-Commit Strategies for Known Gaps. We now show the existence of ETC strategies that improve on the optimal fixed-design strategy. Surprisingly, the gain is significant. We describe an algorithm inspired by ideas from hypothesis testing and prove an upper bound on its regret that is minimax optimal and that asymptotically matches our lower bound.\nLet P be the law of X \u2212 Y , where X (resp. Y ) is a reward from arm 1 (resp. arm 2). As \u2206 is known, the exploration phase of an ETC algorithm can be viewed as a statistical test of the hypothesis H1 : (P = N (\u2206, 2)) against H2 : (P = N (\u2212\u2206, 2)). The work of Wald [1945] shows that a significant gain in terms of expected number of samples can be obtained by using a sequential rather than a batch test. Indeed, for a batch test, a sample size of n \u223c (4/\u22062) log(1/\u03b4) is necessary to guarantee that both type I and type II errors are upper bounded by \u03b4. In contrast, when a random number of samples is permitted, there exists a sequential probability ratio test (SPRT) with the same guarantees that stops after a random number N of samples with expectation E[N ] \u223c log(1/\u03b4)/\u22062 under both H1 and H2. The SPRT stops when the absolute value of the log-likelihood ratio between H1 and H2 exceeds some threshold. Asymptotic upper bound on the expected number of samples used by a SPRT, as well as the (asymptotic) optimality of such procedures among the class of all sequential tests can be found in [Wald, 1945, Siegmund, 1985].\ninput: T and \u2206 A1 = 1, A2 = 2, s := 2 while (s/2)\u2206 |\u00b5\u03021(s)\u2212 \u00b5\u03022(s)| < log ( T\u22062 ) do\nchoose As+1 = 1 and As+2 = 2 s := s+ 2\nend while a\u0302 := arg maxi \u00b5\u0302i(s) for t \u2208 {s+ 1, . . . , T} do\nchoose At = a\u0302 end for\nAlgorithm 2: SPRT ETC algorithm\nAlgorithm 2 is an ETC strategy that explores each action alternately, halting when sufficient confidence is reached according to a SPRT. The threshold depends on the gap \u2206 and the horizon T corresponding to a risk of \u03b4 = 1/(T\u22062). The exploration phase ends at the stopping time\n\u03c4 = inf { t = 2n : \u2223\u2223\u00b5\u03021,n\u2212\u00b5\u03022,n\u2223\u2223 \u2265 log(T\u22062) n\u2206 } .\nIf \u03c4 < T then the empirical best arm a\u0302 at time \u03c4 is played until time T . If T\u22062 \u2264 1, then \u03c4 = 1 (one could even define \u03c4 = 0 and pick a random arm). The following theorem gives a non-asymptotic upper bound on the regret of the algorithm. The results rely on non-asymptotic upper bounds on the expectation of \u03c4 , which are interesting in their own right. Theorem 2. If T\u22062 \u2265 1, then the regret of the SPRT-ETC algorithm is upper-bounded as\nRSPRT-ETC\u00b5 (T ) \u2264 log(eT\u22062)\n\u2206 +\n4 \u221a log(T\u22062) + 4\n\u2206 + \u2206 . Otherwise it is upper bounded by T\u2206/2+\u2206, and for all T and \u2206 the regret is less than 10 \u221a T/e+\u2206.\nThe proof of Theorem 2 is given in Appendix B. The following lower bound shows that no uniformly efficient ETC strategy can improve on the asymptotic regret of Algorithm 2. The proof is given in Section 5 together with the other lower bounds. Theorem 3. Let \u03c0 be an ETC strategy that is uniformly efficient onH\u2206. Then for all \u00b5 \u2208 H\u2206,\nlim inf T\u2192\u221e\nR\u03c0\u00b5(T )\nlog(T ) \u2265 1 \u2206 .\nExplore-Then-Commit Strategies for Unknown Gaps. When the gap is unknown it is not possible to tune a fixed-design strategy that achieves logarithmic regret. ETC strategies can enjoy logarithmic regret and these are now analysed. We start with the asymptotic lower bound. Theorem 4. Let \u03c0 be a uniformly efficient ETC strategy onH. For all \u00b5 \u2208 H, if \u2206 = |\u00b51 \u2212 \u00b52| then\nlim inf T\u2192\u221e\nR\u03c0\u00b5(T )\nlog(T ) \u2265 4 \u2206 .\nA simple idea for constructing an algorithm that matches the lower bound is to use a (fixed-confidence) best arm identification algorithm for the exploration phase. Given a risk parameter \u03b4, a \u03b4-PAC BAI algorithm consists of a sampling rule (At), a stopping rule \u03c4 and a recommendation rule a\u0302 which is F\u03c4 measurable and satisfies, for all \u00b5 \u2208 H such that \u00b51 6= \u00b52, P\u00b5(a\u0302 = a\u2217) \u2265 1\u2212 \u03b4. In a bandit model with two Gaussian arms, Kaufmann et al. [2014] propose a \u03b4-PAC algorithm using a uniform sampling rule and a stopping rule \u03c4\u03b4 that asymptotically attains the minimal sample complexity E\u00b5[\u03c4\u03b4] \u223c (8/\u22062) log(1/\u03b4). Using the regret decomposition (2), it is easy to show that the ETC algorithm using the stopping rule \u03c4\u03b4 for \u03b4 = 1/T matches the lower bound of Theorem 4.\ninput: T (\u2265 3) A1 = 1, A2 = 2, s := 2 while |\u00b5\u03021(s)\u2212 \u00b5\u03022(s)| < \u221a 8 log(T/s) s do choose As+1 = 1 and As+2 = 2 s := s+ 2 end while a\u0302 := arg maxi \u00b5\u0302i(s) for t \u2208 {s+ 1, . . . , T} do\nchoose At = a\u0302 end for\nAlgorithm 3: BAI-ETC algorithm\nAlgorithm 3 is a slight variant of this optimal BAI algorithm, based on the stopping time\n\u03c4 = inf t = 2n : |\u00b5\u03021,n \u2212 \u00b5\u03022,n|> \u221a 4 log ( T/(2n) ) n . The motivation for the difference (which comes from a more carefully tuned threshold featuring log(T/2n) in place of log(T )) is that the confidence level should depend on the unknown gap \u2206, which determines the regret when a mis-identification occurs. The improvement only appears in the non-asymptotic regime where we are able to prove both asymptotic optimality and order-optimal minimax regret. The latter would not be possible using a fixed-confidence BAI strategy. The proof of this result can be found in Appendix C. The main difficulty is developing a sufficiently strong deviation bound, which we do in Appendix F, and that may be of independent interest. Note that a similar strategy was proposed and analysed by Lai et al. [1983], but in the continuous time framework and with asymptotic analysis only. Theorem 5. If T\u22062 > 4e2, the regret of the BAI-ETC algorithm is upper bounded as\nRBAI-ETC\u00b5 (T ) \u2264 4 log\n( T\u22062\n4 ) \u2206 + 334 \u221a log ( T\u22062 4 ) \u2206 + 178 \u2206 + \u2206.\nIt is upper bounded by T\u2206 otherwise, and by 32 \u221a T + \u2206 in any case."}, {"heading": "4 Fully Sequential Strategies for Known and Unknown Gaps", "text": "In the previous section we saw that allowing a random stopping time leads to a factor of 4 improvement in terms of the asymptotic regret relative to the naive fixed-design strategy. We now turn our attention to fully sequential strategies when \u2206 is known and unknown. The latter case is the classic 2-armed bandit problem and is now quite well understood. Our modest contribution in that case is the first algorithm that is simultaneously asymptotically optimal and order optimal in the minimax sense. For\nthe former case, we are not aware of any previous research where the gap is known except the line of work by Bubeck et al. [2013], Bubeck and Liu [2013], where different questions are treated. In both cases we see that fully sequential strategies improve on the best ETC strategies by a factor of 2.\nKnown Gaps. We start by stating the lower bound (proved in Section 5), which is a straightforward generalisation of Lai and Robbins\u2019 lower bound.\nTheorem 6. Let \u03c0 be a strategy that is uniformly efficient onH\u2206. Then for all \u00b5 \u2208 H\u2206,\nlim inf T\u2192\u221e\nR\u03c0\u00b5(T )\nlog T \u2265 1 2\u2206\nWe are not aware of any existing algorithm matching this lower bound, which motivates us to introduce a new strategy called \u2206-UCB that exploits the knowledge of \u2206 to improve the performance of UCB. In each round the algorithm chooses the arm that has been played most often so far unless the other arm has an upper confidence bound that is close to \u2206 larger than the empirical estimate of the most played arm. Like ETC strategies, \u2206-UCB is not anytime in the sense that it requires the knowledge of both the horizon T and the gap \u2206.\n1: input: T and \u2206 2: \u03b5T = \u2206 log\n\u2212 18 (e+ T\u22062)/4 3: for t \u2208 {1, . . . , T} do 4: let At,min := arg min\ni\u22081,2 Ni(t\u2212 1) and At,max = 3\u2212At,min\n5: if \u00b5\u0302At,min(t\u2212 1) + \u221a\u221a\u221a\u221a2 log ( TNAt,min (t\u22121)) NAt,min(t\u2212 1) \u2265 \u00b5\u0302At,max(t\u2212 1) + \u2206\u2212 2\u03b5T then 6: choose At = At,min 7: else 8: choose At = At,max 9: end if\n10: end for Algorithm 4: \u2206-UCB\nTheorem 7. If T (2\u2206 \u2212 3\u03b5T )2 \u2265 2 and T\u03b52T \u2265 e2, the regret of the \u2206-UCB algorithm is upper bounded as\nR\u2206-UCB\u00b5 (T ) \u2264 log ( 2T\u22062 ) 2\u2206(1\u2212 3\u03b5T /(2\u2206))2 + \u221a \u03c0 log (2T\u22062) 2\u2206(1\u2212 3\u03b5T /\u2206)2\n+ \u2206\n[ 30e \u221a\nlog(\u03b52TT ) \u03b52T + 80 \u03b52T +\n2\n(2\u2206\u2212 3\u03b5T )2\n] + 5\u2206.\nMoreover lim supT\u2192\u221eR \u2206-UCB \u00b5 (T )/ log(T ) \u2264 (2\u2206)\u22121 and \u2200\u00b5 \u2208 H\u2206, R\u2206-UCB\u00b5 (T ) \u2264 328\n\u221a T + 5\u2206.\nThe proof may be found in Appendix D.\nUnknown Gaps. In the classical bandit setting where \u2206 is unknown, UCB by Katehakis and Robbins [1995] is known to be asymptotically optimal: RUCB\u00b5 (T ) \u223c 2 log(T )/\u2206, which matches the lower bound of Lai and Robbins [1985]. Non-asymptotic regret bounds are given for example by Auer et al. [2002], Capp\u00e9 et al. [2013]. Unfortunately, UCB is not optimal in the minimax sense, which is so far only achieved by algorithms that are not asymptotically optimal [Audibert and Bubeck, 2009, Lattimore, 2015]. Here, with only two arms, we are able to show that Algorithm 5 below is simultaneously minimax order-optimal and asymptotically optimal. The strategy is essentially the same as suggested by Lai [1987], but with a fractionally smaller confidence bound. The proof of Theorem 8 is given in Appendix E. Empirically the smaller confidence bonus used by UCB\u2217 leads to a significant improvement relative to UCB.\n1: input: T 2: for t \u2208 {1, . . . , T} do\n3: At = arg max i\u2208{1,2} \u00b5\u0302i(t\u2212 1) +\n\u221a 2\nNi(t\u2212 1) log\n( T\nNi(t\u2212 1) ) 4: end for\nAlgorithm 5: UCB\u2217\nTheorem 8. For all \u03b5 \u2208 (0,\u2206), if T (\u2206\u2212 \u03b5)2 \u2265 2 and T\u03b52 \u2265 e2, the regret of the UCB\u2217 strategy is upper bounded as\nRUCB \u2217 \u00b5 (T ) \u2264 2 log\n( T\u22062\n2 ) \u2206 ( 1\u2212 \u03b5\u2206 )2 + 2 \u221a \u03c0 log ( T\u22062 2 ) \u2206 ( 1\u2212 \u03b5\u2206 )2 + \u2206 ( 30e \u221a log(\u03b52T ) + 16e \u03b52 ) +\n2 \u2206 ( 1\u2212 \u03b5\u2206 )2 + \u2206. Moreover, lim supT\u2192\u221eR \u03c0 \u00b5(T )/ log(T ) = 2/\u2206 and for all \u00b5 \u2208 H, R\u03c0\u00b5(T ) \u2264 33 \u221a T + \u2206.\nNote that if there are K > 2 arms, then the strategy above is still asymptotically optimal, but suffers a minimax regret of \u2126( \u221a TK log(K)), which is a factor of \u221a log(K) suboptimal."}, {"heading": "5 Proof of the Lower Bounds (Theorems 3, 4, 6 and Lai&Robbins)", "text": "Let \u03c0 be a uniformly efficient strategy on some class S, as defined in (3), and let \u03bb \u2208 H. If m(\u03bb) = arg min{\u03bb1, \u03bb2}, as E\u03bb[R\u03c0\u00b5(T )] = |\u03bb1 \u2212 \u03bb2|E\u03bb[Nm(\u03bb)(T )] this implies in particular that\n\u2200\u03b1 \u2208]0, 1], E\u03bb[Nm(\u03bb)(T )] = o(T\u03b1).\nWithout loss of generality we assume that \u00b51 = \u00b52 + \u2206 with \u2206 > 0. All the lower bounds are based on a change of measure argument, which involves considering an alternative reward vector (\u00b5\u20321, \u00b5 \u2032 2) that is \u201cnot too far\u201d from (\u00b51, \u00b52), but for which the expected behaviour of the algorithm is very different. This is the same approach used by Lai and Robbins [1985], but rewritten and generalised in a more powerful way (in particular regarding the ETC strategies). The improvements come thanks to Inequality 4 in [Garivier et al., 2016], which states that for every (\u00b5\u20321, \u00b5 \u2032 2) \u2208 H and for every stopping time \u03c3 such that N2(T ) is F\u03c3-measurable,\nE\u00b5 [ N1(\u03c3) ] (\u00b5\u20321 \u2212 \u00b51)2 2 + E\u00b5 [ N2(\u03c3) ] (\u00b5\u20322 \u2212 \u00b52)2 2 \u2265 kl ( E\u00b5 [ N2(T ) T ] , E\u00b5\u2032 [ N2(T ) T ]) ,\nwhere kl(p, q) is the relative entropy between Bernoulli distributions with parameters p, q \u2208 [0, 1] respectively. Since kl(p, q) \u2265 (1\u2212 p) log(1/(1\u2212 q))\u2212 log(2) for all p, q \u2208 (0, 1), one obtains\nE\u00b5 [ N1(\u03c3) ] (\u00b5\u20321 \u2212 \u00b51)2 2 + E\u00b5 [ N2(\u03c3) ] (\u00b5\u20322 \u2212 \u00b52)2 2\n\u2265 (\n1\u2212 E\u00b5[N2(T )] T\n) log ( T\nE\u00b5\u2032 [N1(T )]\n) \u2212 log(2) . (4)\nFor \u00b5\u2032 \u2208 S such that \u00b5\u20321 < \u00b5\u20322, E\u00b5[N2(T )] = o(T\u03b1) and E\u00b5\u2032 [N1(T )] = o(T\u03b1) for all \u03b1 \u2208]0, 1], thus\nlim inf T\u2192\u221e\nE\u00b5 [ N1(\u03c3) ] (\u00b5\u20321 \u2212 \u00b51)2/2 + E\u00b5 [ N2(\u03c3) ] (\u00b5\u20322 \u2212 \u00b52)2/2\nlog T \u2265 1 .\nLet us now draw the conclusions in each setting.\nKnown gap, General strategy: S = H\u2206. By choosing \u03c3 = T , \u00b5\u20321 = \u00b51 and \u00b5 \u2032 2 = \u00b51 + \u2206 = \u00b52 + 2\u2206, we obtain\nlim inf T\u2192\u221e\nE\u00b5 [ N2(T ) ] log T \u2265 1 (2\u2206)2/2 .\nUnknown gap, General strategy: S = H. We use the same choices, except \u00b5\u20322 = \u00b51 + \u03b5 for some \u03b5 > 0:\nlim inf T\u2192\u221e\nE\u00b5 [ N2(T ) ] log T \u2265 1 (\u2206 + \u03b5) 2 /2 .\nKnown gap, ETC strategy: S = H\u2206. For an ETC strategy \u03c0 with a stopping rule \u03c4 , one hasN1(\u03c4\u2227T ) = N2(\u03c4 \u2227 T ) = (\u03c4 \u2227 T )/2. Besides, N2(T ) is indeed F\u03c4\u2227T -measurable: after \u03c4 \u2227T draws, the agent knows whether she will draw arm 2 for the last T \u2212 \u03c4 \u2227 T steps or not. With \u00b5\u20321 = \u00b52, \u00b5 \u2032 2 = \u00b51, Inequality (4) thus yields:\nlim inf T\u2192\u221e\nE\u00b5 [ \u03c4\u2227T\n2 ] log T \u2265 1 2\u22062/2 .\nUnknown gap, ETC strategy: S = H. Choosing this time \u00b5\u20321 = (\u00b51 + \u00b52 \u2212 \u03b5)/2 and \u00b5\u20322 = (\u00b51 + \u00b52 + \u03b5)/2, for some \u03b5 > 0 yields\nlim inf T\u2192\u221e\nE\u00b5 [ \u03c4\u2227T\n2 ] log T \u2265 1( \u2206+\u03b5\n2\n)2 .\nR\u03c0\u00b5(T ) = \u2206E\u00b5[N2(T )] for general strategies, while Equation (2) shows that R\u03c0\u00b5(T ) \u2265 \u2206E\u00b5[(\u03c4 \u2227 T )/2] for ETC strategies. Therefore letting \u03b5 go to zero when needed shows that\nlim inf T\u2192\u221e\nR\u03c0\u00b5(T )\nlog(T ) \u2265 C\n\u03a0 S\n\u2206 , for the value C\u03a0S given at the end of Section 2.\nNote that the asymptotic lower bound of Theorem 1 can also be proved by similar arguments, if one really wants to bring in an elephant to kill a mouse. Moreover, note that this proof may also lead to (not-so-simple) non-asymptotic lower-bounds, as shown in Garivier et al. [2016] for example."}, {"heading": "6 Beyond Uniform Exploration, Two Arms and Gaussian distributions", "text": "It is worth emphasising the impossibility of non-trivial lower bounds on the regret of ETC strategies using any possible (non-uniform) sampling rule. Indeed, using UCB as a sampling rule together with an a.s. infinite stopping rule defines an artificial but formally valid ETC strategy that achieves the best possible rate for general strategies. This strategy is not a faithful counter-example to our claim that ETC strategies are sub-optimal, because UCB is not a satisfying exploration rule. If exploration is the objective, then uniform sampling is known to be optimal in the two-armed Gaussian case [Kaufmann et al., 2014], which justifies the uniform sampling assumption.\nThe use of ETC strategies for regret minimisation (e.g., as presented by Perchet and Rigollet [2013]) is certainly not limited to bandit models with 2 arms. The extension to multiple arms is based on the successive elimination idea in which a set of active arms is maintained with arms chosen according to a round robin within the active set. Arms are eliminated from the active set once their optimality becomes implausible and the exploration phase terminates when the active set contains only a single arm (an example is by Auer and Ortner [2010]). The Successive Elimination algorithm has been introduced by Even-Dar et al. [2006] for best-arm identification in the fixed-confidence setting. But as shown in Garivier and Kaufmann [2016], Successive Elimination is suboptimal for the best arm identification task in almost all settings except two-armed Gaussian bandits. It is therefore interesting to investigate the performance in terms of regret of an ETC algorithm using an optimal BAI algorithm. This is actually possible not only for Gaussian distributions, but more generally for one-parameter exponential families, for which Garivier and Kaufmann [2016] propose the asymptotically optimal Track-and-Stop strategy. Denoting d(\u00b5, \u00b5\u2032) = KL(\u03bd\u00b5, \u03bd\u00b5\u2032) the Kullback-Leibler divergence between two distributions parameterised by \u00b5 and \u00b5\u2032, they provide results which can be adapted to obtain the following bound.\nProposition 1. For \u00b5 such that \u00b51 > maxa6=1 \u00b5a, the regret of the ETC strategy using Track-and-Stop exploration with risk 1/T satisfies\nlim sup T\u2192\u221e\nRTaS\u00b5 (T )\nlog T \u2264 T \u2217(\u00b5) ( K\u2211 a=2 w\u2217a(\u00b5)(\u00b51 \u2212 \u00b5a) ) ,\nwhere T \u2217(\u00b5) (resp. w\u2217(\u00b5)) is the the maximum (resp. maximiser) of the optimisation problem\nmax w\u2208\u03a3K inf a6=1\n[ w1d ( \u00b51,\nw1\u00b51 + wa\u00b5a w1 + wa\n) + wad ( \u00b5a,\nwa\u00b51 + wa\u00b5a w1 + wa\n)] ,\nwhere \u03a3K is the set of probability distributions on {1, . . . ,K}.\nIn general, it is not easy to quantify the difference to the lower bound of Lai and Robbins\nlim inf T\u2192\u221e\nR\u03c0\u00b5(T )\nlog T \u2265 K\u2211 a=2 \u00b51 \u2212 \u00b5a d(\u00b5a, \u00b51) .\nEven for Gaussian distributions, there is no general closed-form formula for T \u2217(\u00b5) and w\u2217(\u00b5) except when K = 2. However, we conjecture that the worst case is when \u00b51 and \u00b52 are much larger than the other means: then, the regret is almost the same as in the 2-arm case, and ETC strategies are suboptimal by a factor 2. On the other hand, the most favourable case (in terms of relative efficiency) seems to be when \u00b52 = \u00b7 \u00b7 \u00b7 = \u00b5K : then\nw\u22171(\u00b5) =\n\u221a K \u2212 1\nK \u2212 1 + \u221a K \u2212 1\n, w\u22172(\u00b5) = \u00b7 \u00b7 \u00b7 = w\u2217K(\u00b5) = 1\nK \u2212 1 + \u221a K \u2212 1\nand T \u2217 = 2( \u221a K \u2212 1 + 1)2/\u22062, leading to\nlim sup T\u2192\u221e\nRTaS\u00b5 (T ) log(T ) \u2264 ( 1 + 1\u221a K \u2212 1 ) 2(K \u2212 1) \u2206 ,\nwhile Lai and Robbins\u2019 lower bound yields 2(K \u2212 1)/\u2206. Thus, the difference grows with K as 2 \u221a K \u2212 1 log(T )/\u2206 , but the relative difference decreases."}, {"heading": "A Proof of Theorem 1", "text": "Let n \u2264 T/2. The number of draws of the suboptimal arm 2 is N2 = n + (T \u2212 2n)1{Sn \u2264 0}, where Sn = (X1 \u2212 Y1) + \u00b7 \u00b7 \u00b7+ (Xn \u2212 Yn) \u223c N (n\u2206, 2n). The expected regret of the strategy using 2n exploration steps is\nRn\u00b5(T ) = \u2206E\u00b5[N2] = \u2206 ( n+ (T \u2212 2n)P\u00b5(Sn \u2264 0) ) . (5)\nBut\nP\u00b5(Sn \u2264 0) = P\u00b5 ( Sn \u2212 n\u2206\u221a\n2n \u2264 \u2212n\u2206\u221a 2n = \u2212\u2206\n\u221a n\n2\n) .\nDenote by \u03a6 (resp. \u03c6) the pdf (resp. cdf) of the standard Gaussian distribution, and recall thatW is the Lambert function defined for all y > 0 by W (y) exp(W (y)) = y. The regret is thus upper-bounded as Rn\u00b5(T ) \u2264 \u2206g(n) where, for all x > 0, g(x) = x+ T\u03a6(\u2212\u2206 \u221a x/2). By differentiating g, one can see that its maximum is reached at x\u2217 such that\n\u03c6 ( \u2206 \u221a x\u2217\n2\n) = \u2206 \u221a x\u2217/2\nT\u22062/4 , and thus x\u2217 =\n2\n\u22062 W\n( T 2\u22064\n32\u03c0\n) .\nBy choosing n = dx\u2217e, we obtain\nRn\u00b5(T ) \u2264 2\n\u2206 W\n( T 2\u22064\n32\u03c0\n) + \u2206 + \u2206T\u03a6 ( \u2212\u2206 \u221a x\u2217\n2\n) .\nAs g(n) \u2264 g(x\u2217) + 1 \u2264 g(0) + 1 = T/2 + 1, Rn\u00b5(T ) \u2264 (T/2 + 1)\u2206 \u2264 2.04 \u221a T + \u2206 for T\u22062 \u2264 4 \u221a 2\u03c0e. If T\u22062 \u2265 4 \u221a 2\u03c0e, the inequality W (y) \u2264 log ( (1 + e\u22121)y/ log(y) ) valid for all y \u2265 e (see Hoorfar and Hassani [2008]) entails\nW\n( T 2\u22064\n32\u03c0\n) \u2264 log ( (1 + e\u22121)T 2\u22064 32\u03c0\nlog ( T 2\u22064\n32\u03c0\n) ) = 2 log 18 \u221a 1 + e\u22121 \u03c0\nT\u22062\u221a log ( T\u22062\n4 \u221a 2\u03c0\n) \nIn addition, the classic bound on the Gaussian tail \u03a6(\u2212y) \u2264 \u03c6(y)/y yields by definition of x\u2217:\n\u03a6 ( \u2212\u2206 \u221a x\u2217\n2\n) \u2264 \u03c6 ( \u2212\u2206 \u221a x\u2217 2 ) \u2206 \u221a x\u2217\n2\n= 4\nT\u22062 .\nHence, for T\u22062 \u2265 4 \u221a 2\u03c0e,\nRn\u00b5(T ) \u2264 4\n\u2206 log e8 \u221a 1 + e\u22121 \u03c0\nT\u22062\u221a log ( T\u22062\n4 \u221a 2\u03c0\n) + \u2206 < 4\u2206 log  T\u22062 4.46 \u221a log ( T\u22062\n4 \u221a 2\u03c0\n) + \u2206 .\nTo complete the proof of the uniform upper-bound, we start from\nRn\u00b5(T ) \u2264 2\n\u2206 W\n( T 2\u22064\n32\u03c0\n) + 4\n\u2206 + \u2206 .\nDenoting by r \u2248 1.09 the root of (4r \u2212 1)W (r) = 2, the maximum of 2W ( T 2\u22064\n32\u03c0\n) /\u2206 + 4/\u2206 is\nreached at \u2206 = ( 32\u03c0r/T 2 )1/4 , and is equal to\n2 \u2206 W\n( T 2\u22064\n32\u03c0\n) + 4\n\u2206 =\n8r3/4 (4r \u2212 1)(2\u03c0)1/4 \u221a T < 2 \u221a T .\nLet us now prove that choosing n too small leads to catastrophic regret. If n \u2264 4(1\u2212 \u03b5) log(T )/\u22062, then Equation (5) yields\nP\u00b5(Sn \u2264 0) \u2265 1\n\u2206 \u221a \u03c0n\n( 1\u2212 2\nn\u22062\n) exp ( \u2212\u2206 2n\n4 ) \u2265 (\n1\u2212 2 n\u22062\n) 1\n2 \u221a \u03c0(1\u2212 \u03b5) log(T )\nexp ( \u2212 (1\u2212 \u03b5) log(T ) ) \u2265 (\n1\u2212 2 n\u22062\n) T \u03b5\u22121\n2 \u221a \u03c0 log(T ) .\nThus, the expected regret is lower-bounded as Rn\u00b5(T ) \u2265 \u2206(T \u2212 2n)P\u00b5(Sn \u2264 0) \u2265 \u2206 (\n1\u2212 2 n\u22062\n)( 1\u2212 8 log(T )\n\u22062T\n) T \u03b5\n2 \u221a \u03c0 log(T ) .\nLet us now turn to the last statement of the theorem. The previous inequality shows that for all \u03b5 > 0,\nlim inf T inf 3\n\u22062 <n\u2264 4(1\u2212\u03b5) log(T ) \u22062\nRn\u00b5(T ) log(T ) = +\u221e .\nFor n \u2264 3/\u22062, we have that\nP\u00b5(Sn \u2264 0) \u2265 P\u00b5\n( Sn \u2212 n\u2206\u221a\n2n \u2264 \u2212\n\u221a 3\n2\n) > 0 ,\nand hence that lim infT infn\u22643/\u22062 Rn\u00b5(T )/T > 0. As R n \u00b5(T ) \u2265 n\u2206, the result follows."}, {"heading": "B Proof of Theorem 2", "text": "Recall that \u00b51 > \u00b52. Using (2), one has\nR\u03c0\u00b5(T ) \u2264 \u2206E\u00b5 [\u03c4\n2\n] + T\u2206P\u00b5(\u03c4 < T, a\u0302 = 2) .\nIf T\u22062 \u2264 1, then \u03c4 = 2 and a\u0302 is based on a single sample from each action. Therefore a\u0302 = 2 with probability less than 1/2 and the regret is upper-bounded by \u2206 + T\u2206/2. Otherwise, let S0 = 0, Sn = (X1\u2212Y1)+ \u00b7 \u00b7 \u00b7+(Xn\u2212Yn) for every n \u2265 1. For every u > 0, let nu = (log(T\u22062)+u)/\u22062. Observe that {\u03c4\n2 \u2265 \u2308 log(T\u22062) + u \u22062 \u2309} \u2282 { Sdnue \u2264 log(T\u22062) \u2206 } .\nMoreover, if nu \u2208 N then Snu \u223c N (nu\u2206, 2nu) and\nP\u00b5 ( Snu \u2264 log(T\u22062)\n\u2206\n) = P\u00b5 ( Snu \u2212 nu\u2206\u221a\n2nu \u2264 log(T\u2206 2)/\u2206\u2212\u2206(log(T\u22062) + u)/\u22062\u221a 2(log(T\u22062) + u)/\u22062\n)\n= P\u00b5\n( Snu \u2212 nu\u2206\u221a\n2nu \u2264 \u2212u\u221a 2(log(T\u22062) + u)\n) \u2264 exp ( \u2212 u 2\n4 ( log(T\u22062) + u )) .\nHence, for a = 2 \u221a\nlog(T\u22062),\u222b \u221e na P\u00b5 (\u03c4 2 \u2212 1 \u2265 v ) dv = \u222b \u221e a P\u00b5 ( \u03c4 2 \u2212 1 \u2265 log(T\u2206 2) + u \u22062 ) du \u22062\n\u2264 \u222b \u221e a P\u00b5 ( \u03c4 2 \u2265 \u2308 log(T\u22062) + u \u22062 )\u2309 du \u22062\n\u2264 1 \u22062 \u222b \u221e a exp ( \u2212 u 2 4 ( log(T\u22062) + u )) du\n\u2264 1 \u22062 \u222b \u221e a exp ( \u2212 u 2 \u221a log(T\u22062) + 4 ) du as log(T\u22062) \u2264 u \u221a log(\u22062T )/2\n\u2264 1 \u22062 \u222b \u221e 0 exp ( \u2212 u 2 \u221a log(T\u22062) + 4 ) du\n= 2 \u221a log(T\u22062) + 4\n\u22062 ,\nand E\u00b5 [\u03c4\n2\n] \u2264 1 + na + \u222b \u221e na P\u00b5 (\u03c4 2 \u2212 1 \u2265 v ) dv \u2264 1 + log(T\u22062) + 4 \u221a log(T\u22062) + 4 \u22062 .\nTo conclude the proof of the first statement, it remains to show that P(\u03c4 < T, a\u0302 = 2) \u2264 1/(T\u22062). Since X1 \u2212 Y1 \u223c N (\u2206, 2), E\u00b5[exp(\u2212\u2206(X1 \u2212 Y1))] = exp(\u2212\u22062 + 2\u22062/2) = 1 and Mn = exp(\u2212\u2206Sn) is a martingale. Let \u03c42 = T \u2227 inf{n \u2265 1 : Sn \u2264 \u2212 log(T\u22062)/\u2206}, and observe that{\n\u03c4 < T, a\u0302 = 2 } \u2282 { \u2203n < T : Sn \u2264 \u2212 log(T\u22062)\n\u2206\n} = { \u03c42 < T } .\nDoob\u2019s optional stopping theorem yields E\u00b5[M\u03c42 ] = E\u00b5[M0] = 1. But as M\u03c42 = exp(\u2212\u2206S\u03c42) \u2265 exp(\u2206 log(T\u22062)/\u2206) = T\u22062 on the event {\u03c42 < T},\nP\u00b5(\u03c4 < T, a\u0302 = 2) \u2264 P\u00b5(\u03c42 < T ) \u2265 E\u00b5 [ 1{\u03c42 < T}\nM\u03c42 T\u22062\n] \u2264 E\u00b5 [ M\u03c42 ] T\u22062 = 1 T\u22062 .\nThe last statement Theorem 2 is obtained by maximising the bound (except for the \u2206 summand). Let u = 1/(\u2206 \u221a T ) and f(u) = \u2212u log(u/ \u221a e) + 2u \u221a \u2212 log(u). Denoting ` = \u2212 log(u), f \u2032(u) = `\u2212 1/2+2 \u221a `\u22121/ \u221a ` = ( \u221a `+2)(`\u22121/2)/ \u221a `, thus the maximum is reached at ` = log ( \u2206 \u221a T ) = 1/2\nand \u2206 = \u221a e/T . Re-injecting this value into the bound, we obtain that for every \u2206 > 0,\nRSPRT-ETC\u00b5 (T ) \u2264 2 + 4 \u221a 1 + 4\u221a\ne/T = 10\n\u221a T\ne ."}, {"heading": "C Proof of Theorem 5", "text": "Recall that \u00b51 = \u00b52 + \u2206 with \u2206 > 0. Let Ws = (Xs\u2212Ys\u2212\u2206)/ \u221a\n2, which means that W1,W2, . . . are i.i.d. standard Gaussian random variables. Introducing F = (a\u0302 6= 1, \u03c4 < T ), one has by (2) that\nRBAI-ETC\u00b5 (T ) \u2264 T\u2206P\u00b5(F) + \u2206E\u00b5[\u03c4 ]. From the definition of \u03c4 and Lemma 1.(c) in Appendix F, assuming that T\u22062 \u2265 4e2, one obtains\nP\u00b5 (F) \u2264 P\u00b5 ( \u2203s : 2s \u2264 T, \u00b5\u03021,s \u2212 \u00b5\u03022,s \u2264 \u2212 \u221a 4\ns log\n( T\n2s\n))\n= P\u00b5 ( \u2203s \u2264 T/2 : \u2211s i=1Wi s \u2264 \u2212 \u221a 2 s log ( T/2 s ) \u2212 \u2206\u221a\n2\n)\n\u2264 120e\n\u221a log(\u22062T/4)\n\u22062T +\n64e\n\u22062T .\nThe last step is bounding E\u00b5[\u03c4 ] for which as T\u22062 \u2265 4, Lemma 1.(b) yields\nE\u00b5[\u03c4 ] \u2264 T\u2211 s=1 P\u00b5 (\u03c4 \u2265 s) \u2264 T\u2211 s=1 P\u00b5 (\u2211s i=1Wi s \u2264 \u221a 2 s log ( T/2 s ) \u2212 \u2206\u221a 2 )\n\u2264 4 log\n( T\u22062\n4 ) \u22062 + 4 \u221a \u03c0 log ( T\u22062 4 ) \u22062 + 4 \u22062 + 1 ,\nTherefore, if T\u22062 \u2265 4e2,\nRBAI-ETC\u00b5 (T ) \u2264 4 log\n( T\u22062\n4 ) \u2206 + 4 \u221a \u03c0 log ( T\u22062 4 ) \u2206 + 4 \u2206 + \u2206 + 120e \u221a log ( \u22062T 4 ) \u2206 + 64e \u2206\n\u2264 4 log\n( T\u22062\n4 ) \u2206 + 334 \u221a log ( T\u22062 4 ) \u2206 + 178 \u2206 + \u2206.\nTaking the limit as T \u2192 \u221e shows that lim supT\u2192\u221eRBAI-ETC\u00b5 (T )/ log(T ) \u2264 4. Noting that RBAI-ETC\u00b5 (T ) \u2264 \u2206T and taking the minimum of this bound and the finite-time bound given above leads arduously to RBAI-ETC\u00b5 (T ) \u2264 \u2206 + 32 \u221a T for all \u00b5."}, {"heading": "D Proof of Theorem 7", "text": "Define random time \u03c4 = max {\u03c41, \u03c42} where \u03c4i is given by\n\u03c4i = min\n{ t \u2264 T : sup\ns\u2265t |\u00b5\u0302i,s \u2212 \u00b5i| < \u03b5T\n} .\nBy the concentration Lemma 1.(a) in Appendix F we have E\u00b5[\u03c4i] \u2264 1 + 9/\u03b52T and so E\u00b5[\u03c4 ] \u2264 E\u00b5[\u03c41 + \u03c42] \u2264 2 + 18/\u03b52T . For t > 2\u03c4 we have |\u00b5\u0302At,max(t \u2212 1) \u2212 \u00b5At,max | < \u03b5T . Therefore the expected number of draws of the suboptimal arm may be bounded by\nE\u00b5[N2(T )] = E\u00b5 [ T\u2211 t=1 1 {It = 2} ] \u2264 E\u00b5[2\u03c4 ] + E\u00b5 [ T\u2211 t=2\u03c4+1 1 {It = 2} ]\n\u2264 E\u00b5[2\u03c4 ] + E\u00b5 [ T\u2211 t=1 1 { \u00b5\u03022(t\u2212 1) + \u221a 2 log(T/N2(t\u2212 1)) N2(t\u2212 1) \u2265 \u00b51 + \u2206\u2212 3\u03b5T and It = 2 }]\n+ E\u00b5 [ T\u2211 t=1 1 { \u00b5\u03021(t\u2212 1) + \u221a 2 log(T/N1(t\u2212 1)) N1(t\u2212 1) \u2264 \u00b52 + \u2206\u2212 \u03b5T = \u00b51 \u2212 \u03b5T }] (6)\nBy Lemma 1.(b), whenever T (2\u2206\u2212 3\u03b5T )2 \u2265 2,\nE\u00b5 [ T\u2211 t=1 1 { \u00b5\u03022(t\u2212 1) + \u221a 2 log(T/N2(t\u2212 1)) N2(t\u2212 1) \u2265 \u00b51 + \u2206\u2212 3\u03b5T = \u00b52 + 2\u2206\u2212 3\u03b5T and It = 2 }]\n\u2264 T\u2211 s=1 P\u00b5\n( \u00b5\u03022,s \u2212 \u00b52 + \u221a 2 log(T/s)\ns \u2265 2\u2206\u2212 3\u03b5T\n)\n\u2264 2 log\n( T (2\u2206\u22123\u03b5T )2\n2 ) (2\u2206\u2212 3\u03b5T )2 + 2 \u221a \u03c0 log ( T (2\u2206\u22123\u03b5T )2 2 ) (2\u2206\u2212 3\u03b5T )2 + 2 (2\u2206\u2212 3\u03b5T )2 + 1\nFor the second term in (6) we apply Lemma 1.(c) to obtain, whenever T\u03b52T \u2265 e2,\nE\u00b5 [ T\u2211 t=1 1 { \u00b5\u03021(t\u2212 1) + \u221a 2 log(T/N1(t\u2212 1)) N1(t\u2212 1) \u2264 \u00b51 \u2212 \u03b5T }]\n\u2264 TP\u00b5 ( \u2203s \u2208 N : \u00b5\u03021,s + \u221a 2\ns log(T/s) \u2264 \u00b51 \u2212 \u03b5T\n) \u2264 30e \u221a log(\u03b52TT )\n\u03b52T +\n16e \u03b52T .\nTherefore, if T (2\u2206\u2212 3\u03b5T )2 \u2265 2 and T\u03b52T \u2265 e2,\nE\u00b5[N2(T )] \u2264 2 log\n( T (2\u2206\u22123\u03b5T )2\n2 ) (2\u2206\u2212 3\u03b5T )2 + 2 \u221a \u03c0 log ( T (2\u2206\u22123\u03b5T )2 2 ) (2\u2206\u2212 3\u03b5T )2 + 30e \u221a log(\u03b52TT ) \u03b52T\n+ 80\n\u03b52T +\n2\n(2\u2206\u2212 3\u03b5T )2 + 5.\nThe first result follows since the regret is R\u2206-UCB\u00b5 (T ) = \u2206E\u00b5[N2(T )]. For the second it easily noted that for the choice of \u03b5T given in the definition of the algorithm that lim supT\u2192\u221e E\u00b5[N2(T )]/ log(T ) \u2264 1/(2\u22062). Therefore lim supT\u2192\u221eR\u2206-UCB\u00b5 (T )/ log(T ) \u2264 1/(2\u2206). The third result follows from a laborious optimisation step to upper-bound the minimum of T\u2206 and the finite-time regret bound above."}, {"heading": "E Proof of Theorem 8", "text": "For any \u03b5 \u2208 (0,\u2206) we have\nE\u00b5[N2(T )] = E\u00b5 [ T\u2211 t=1 1 {At = 2} ]\n\u2264 E\u00b5 [ T\u2211 t=1 1 { At = 2 and \u00b5\u03022(t\u2212 1) + \u221a 2 N2(t\u2212 1) log ( T N2(t\u2212 1) ) \u2265 \u00b51 \u2212 \u03b5 }]\n+ TP\u00b5 ( \u2203s : \u00b5\u03021,s + \u221a 2\ns log\n( T\ns\n) \u2264 \u00b51 \u2212 \u03b5 ) .\nBy the concentration Lemma 1.(b) in Appendix F we have, whenever T (\u2206\u2212 \u03b5)2 \u2265 2,\nE\u00b5 [ T\u2211 t=1 1 { At = 2 and \u00b5\u03022(t\u2212 1) + \u221a 2 N2(t\u2212 1) log ( T N2(t\u2212 1) ) \u2265 \u00b51 \u2212 \u03b5 }]\n\u2264 T\u2211 s=1 P\u00b5\n( \u00b5\u03022,s \u2212 \u00b52 + \u221a 2\ns log\n( T\ns\n) \u2265 \u2206\u2212 \u03b5 )\n\u2264 2 log\n( T (\u2206\u2212\u03b5)2\n2 ) (\u2206\u2212 \u03b5)2 + 2 \u221a \u03c0 log ( T (\u2206\u2212\u03b5)2 2 ) (\u2206\u2212 \u03b5)2 + 2 (\u2206\u2212 \u03b5)2 + 1\nFor the second term we apply Lemma 1.(c) to obtain, whenever T\u03b52 \u2265 e2,\nTP\u00b5 ( \u2203s : \u00b5\u03021,s + \u221a 2\ns log\n( T\ns\n) \u2264 \u00b51 \u2212 \u03b5 ) \u2264 30e \u221a log(\u03b52T )\n\u03b52 +\n16e\n\u03b52 .\nFinally, if T (\u2206\u2212 \u03b5)2 \u2265 2 and T\u03b52 \u2265 e2,\nRUCB\u00b5 (T ) \u2264 \u2206E\u00b5[N2(T )]\n\u2264 2 log\n( T (\u2206\u2212\u03b5)2\n2 ) \u2206 (1\u2212 \u03b5/\u2206)2 + 2 \u221a \u03c0 log ( T (\u2206\u2212\u03b5)2 2 ) \u2206 (1\u2212 \u03b5/\u2206)2 + 2 \u2206 (1\u2212 \u03b5/\u2206)2 + \u2206 + \u2206 ( 30e \u221a log(\u03b52T ) \u03b52 + 16e \u03b52 )\nThe asymptotic result follows by taking the limit as T tends to infinity and choosing \u03b5 = log\u2212 1 8 (T ) while the minimax result follows by finding the minimum of the finite-time bound given above and the naive RUCB\u00b5 (T ) \u2264 T\u2206."}, {"heading": "F Deviation Inequalities", "text": "As was already the case in the proof of Theorem 1, we heavily rely on the following well-known inequality on the tail of a Gaussian distribution: if X \u223c N (0, 1), then for all x > 0\nP (X \u2265 x) \u2264 min { 1, 1\nx \u221a 2\u03c0\n} exp ( \u2212x2/2 ) .\nLemma 1 gathers some more specific results that are useful in our regret analyses, and that we believe to be of a certain interest on their own. Lemma 1. Let \u03b5 > 0 and \u2206 > 0 and W1,W2, . . . be standard i.i.d. Gaussian random variables and \u00b5\u0302t = \u2211t s=1Ws/t. Then the following hold:\n(a). E [ min { t : sup\ns\u2265t |\u00b5\u0302s| \u2265 \u03b5\n}] \u2264 1 + 9/\u03b52\n(b). if T\u22062 \u2265 2 then T\u2211 n=1 P\n( \u00b5\u0302n + \u221a 2\nn log\n( T\nn\n) \u2265 \u2206 ) \u2264 2 log ( T\u22062 2 ) \u22062 + 2 \u221a \u03c0 log ( T\u22062 2 ) \u22062 + 2 \u22062 + 1\n(c). if T\u03b52 \u2265 e2 then P ( \u2203s \u2208 N : \u00b5\u0302s + \u221a 2\ns log\n( T\ns\n) + \u03b5 \u2264 0 ) \u2264 30e \u221a log(\u03b52T )\n\u03b52T +\n16e\n\u03b52T\nThe proof of Lemma 1 follows from standard peeling techniques and inequalities for Gaussian sums of random variables. So far we do not know if this statement holds for subgaussian random variables where slightly weaker results may be shown by adding log log terms to the confidence interval, but unfortunately by doing this one sacrifices minimax optimality.\nProof of Lemma 1.(a). We use a standard peeling argument and the maximal inequality.\nP (\u2203s \u2265 t : |\u00b5\u0302s| \u2265 \u03b5) \u2264 \u221e\u2211 k=1 P (\u2203s \u2208 [kt, (k + 1)t] : |\u00b5\u0302s| \u2265 \u03b5)\n\u2264 \u221e\u2211 k=1 P (\u2203s \u2264 (k + 1)t : |s\u00b5\u0302s| \u2265 kt\u03b5)\n\u2264 \u221e\u2211 k=1 2 exp ( \u2212 (kt\u03b5) 2 2(k + 1)t ) = \u221e\u2211 k=1 2 exp ( \u2212 kt\u03b5 2 2 (1 + 1/k) )\n\u2264 \u221e\u2211 k=1 2 exp ( \u2212kt\u03b5 2 4 )\nTherefore E[\u03c4 ] \u2264 1 + \u221e\u2211 t=1 P (\u03c4 \u2265 t) \u2264 1 + \u221e\u2211 t=1 min { 1, 2 \u221e\u2211 k=1 exp ( \u2212kt\u03b5 2 4 )}\n= 1 + \u221e\u2211 t=1 min { 1,\n2\nexp (t\u03b52/4)\u2212 1\n} \u2264 1 + \u222b \u221e 0 min { 1,\n2\nexp (t\u03b52/4)\u2212 1\n} dt\n\u2264 1 + 4 log(4) \u03b52 + \u222b \u221e 4/\u03b52 log(4)\n8\n3 exp (t\u03b52/4) dt = 1 +\n4 log(4)\n\u03b52 +\n8 3\u03b52 \u2264 1 + 9 \u03b52 .\nProof of Lemma 1.(b). Let \u03bd be the solution of \u221a 2 log(T/n)/n = \u2206, that is \u03bd = 2W ( \u22062T/2 ) /\u22062. Then T\u2211 n=1 P ( \u00b5\u0302n + \u221a 2 n log ( T n ) \u2265 \u2206 ) \u2264 \u03bd + T\u2211 n=d\u03bde P ( \u00b5\u0302n \u2265 \u2206\u2212 \u221a 2 n log ( T n )) .\nAs for all n \u2265 \u03bd 2\nn log\nT n \u2264 2 \u03bd log\n( T\n\u03bd\n) \u03bd\nn = \u22062\n\u03bd n ,\nT\u2211 n=d\u03bde P\n( \u00b5\u0302n \u2265 \u2206\u2212 \u221a 2\nn log\n( T\nn\n)) \u2264\n\u221e\u2211 n=d\u03bde P ( \u00b5\u0302n \u2265 \u2206 ( 1\u2212 \u221a \u03bd n ))\n\u2264 \u221e\u2211\nn=d\u03bde\nexp ( \u2212n\u2206 2\n2\n( 1\u2212 \u221a \u03bd\nn\n)2)\n= \u221e\u2211 n=d\u03bde exp ( \u2212\u2206 2 2 (\u221a n\u2212 \u221a \u03bd )2)\n\u2264 1 + \u222b \u221e \u03bd exp ( \u2212\u2206 2 2 (\u221a x\u2212 \u221a \u03bd )2) dx\n= 1 + 2\n\u2206 \u222b \u221e 0 ( y \u2206 + \u221a \u03bd ) exp ( \u2212y 2 2 ) dy\n= 1 + 2\n\u22062 +\n\u221a 2\u03c0\u03bd\n\u2206 .\nHence, as \u03bd \u2264 2 log(\u22062T/2)/\u22062 whenever T\u22062 \u2265 2, T\u2211 n=1 P ( \u00b5\u0302n + \u221a 2 n log ( T n ) \u2265 \u2206 ) \u2264 2 \u22062 ( log ( T\u22062 2 ) + 1 + \u221a \u03c0 log ( T\u22062 2 )) + 1 .\nProof of Lemma 1.(c). Let x > 0 and n \u2208 N, then by the reflection principle (eg., M\u00f6rters and Peres [2010]) it holds that\nP (\u2203s \u2264 n : s\u00b5\u0302s + x \u2264 0) = 2P (n\u00b5\u0302n + x \u2264 0) \u2264 2 min { 1\nx\n\u221a n\n2\u03c0 , 1\n} exp ( \u2212x 2\n2n\n) (7)\nWe prepare to use the peeling technique with a carefully chosen grid. Let\n\u03b7 = log(\u03b52T )\nlog(\u03b52T )\u2212 1 and Gk = [\u03b7k, \u03b7k+1[ .\nAs T\u03b52 > e2, one has \u03b7 \u2208]1, 2[. Moreover, our choice of \u03b7 leads to the following inequality, that will be useful in the sequel\n\u2200x \u2265 \u03b5\u22122, (x/T ) 1 \u03b7 \u2264 e (x/T ) (8)\nUsing a union bound and then Eq. (7), one can write\nP ( \u2203s \u2208 N : \u00b5\u0302s + \u221a 2\ns log\n( T\ns\n) + \u03b5 \u2264 0 )\n\u2264 \u221e\u2211 k=0 P\n( \u2203s \u2208 Gk : s\u00b5\u0302s + \u221a 2\u03b7k log ( T\n\u03b7k+1\n) + \u03b7k\u03b5 \u2264 0 )\n\u2264 \u221e\u2211 k=0 2 min 1, \u221a\n\u03b7 4\u03c0 log (\nT \u03b7k+1\n)  ( \u03b7k+1 T ) 1 \u03b7 exp ( \u2212\u03b7 k\u22121\u03b52 2 )\n= \u221e\u2211 k=0 f(k) \u2264 2 max k f(k) + \u222b \u221e 0 f(u)du ,\nwhere the function f is defined on [0,+\u221e[ by\nf(u) = 2 min 1, \u221a\n\u03b7 4\u03c0 log (\nT \u03b7u+1\n)  ( \u03b7u+1 T ) 1 \u03b7 exp ( \u2212\u03b7 u\u22121\u03b52 2 ) .\nThe last inequality relies on the fact that f can be checked to be unimodal, which permits to upper bound the sum with an integral. The maximum of f is easily upper bounded as follows, using notably (8):\nmax k f(k) \u2264 2 sup k\u22650\n( \u03b7k+1\nT\n) 1 \u03b7\nexp ( \u2212\u03b7 k\u22121\u03b52\n2\n) = 2 exp(\u22121/\u03b7) ( 2\u03b7\n\u03b52T\n) 1 \u03b7\n\u2264 8e \u03b52T .\nThe remainder of the proof is spent bounding the integral, which will be split into three disjoint intervals with boundaries at constants k1 < k2 given by\nk1 = log(1/\u03b5 2)/ log(\u03b7) k2 = 1 +\nlog ( \u2212 log log(\u03b7)\n\u03b52 ) log(\u03b7) .\nThese are chosen such that\n\u03b7k1 = \u03b5\u22122 exp ( \u2212\u03b7 k2\u22121\u03b52\n2\n) = \u221a log(\u03b7) .\nFirst, one has\nI1 := \u222b k1 0 f(u)du \u2264 \u222b k1 0 \u221a 2\u221a \u03c0 log (\nT \u03b7u+1\n) (\u03b7u+1T ) 1 \u03b7 exp ( \u2212\u03b7 u\u22121\u03b52 2 ) du\n\u2264 \u221a 2\u221a \u03c0 log ( T\n\u03b7k1+1\n) \u222b k1 0 ( \u03b7u+1 T ) 1 \u03b7 du \u2264 \u221a 2\u221a \u03c0 log ( \u03b52T \u03b7 ) \u03b7log(\u03b7) ( \u03b7\u03b52T ) 1\u03b7\n\u2264 e \u221a 2\u03b72\u221a \u03c0 log ( \u03b52T \u03b7 ) log(\u03b7)\u03b52T \u2264 e \u221a 2\u03b72\u221a \u03c0 log ( \u03b52T \u03b7 ) log(\u03b52T )\u03b52T , where the last inequality follows from the fact that (log(\u03b7))\u22121 \u2264 \u03b7/(\u03b7 \u2212 1) = log ( \u03b52T ) .\nSecondly,\nI2 := \u222b k2 k1 f(u)du \u2264 \u222b k2 k1 \u221a 2\u221a \u03c0 log (\nT \u03b7u+1\n) (\u03b7u+1T ) 1 \u03b7 exp ( \u2212\u03b7 u\u22121\u03b52 2 ) du\n\u2264 e \u221a 2\u221a \u03c0 log ( T\n\u03b7k2+1\n) \u222b \u221e k1 \u03b7u+1 T exp ( \u2212\u03b7 u\u22121\u03b52 2 ) du =\ne \u221a 2\u221a \u03c0 log ( T\n\u03b7k2+1\n) 2\u03b72 exp ( \u2212\u03b7 k1\u22121\u03b52 2 ) \u03b52T log(\u03b7)\n\u2264 2e \u221a 2\u221a \u03c0 log ( T\n\u03b7k2+1\n) \u03b72\u03b52T log(\u03b7) \u2264 2e \u221a 2\u03b72\u221a \u03c0 log ( \u03b52T\n\u2212\u03b72 log log(\u03b7) ) log(\u03b52T )\u03b52T \u2264 2e \u221a 2\u03b72\u221a\n\u03c0 log (\n\u03b52T \u03b72 log log(\u03b52T )\n) log(\u03b52T )\u03b52T\nThe second inequality follows from (8), since by definition of k1, one has \u03b7u+1 \u2265 \u03b5\u22122 for u \u2265 k1, whereas the last inequalities use again that (log(\u03b7))\u22121 \u2264 log(\u03b52T ). Using similar arguments for the third term, one has\nI3 := \u222b \u221e k2 f(u)du \u2264 2 \u222b \u221e k2 ( \u03b7u+1 T ) 1 \u03b7 exp ( \u2212\u03b7 u\u22121\u03b52 2 ) du\n\u2264 2e \u222b \u221e k2 \u03b7u+1 T exp ( \u2212\u03b7 u\u22121\u03b52 2 ) du =\n4e\u03b72 exp ( \u2212\u03b7 k2\u22121\u03b52\n2 ) \u03b52T log(\u03b7)\n= 4e\u03b72 \u03b52T \u221a log(\u03b7) \u2264 4e\u03b7 2 \u03b52T\n\u221a log(\u03b52T )\nCombining the three upper bounds yield\u222b \u221e 0 f(u)du \u2264 I1 + I2 + I3\n\u2264 e\u03b7 2 log(\u03b52T )\n\u03b52T\n \u221a\n2\u221a \u03c0 log ( \u03b52T \u03b7 ) + 2\u221a2\u221a \u03c0 log ( \u03b52T \u03b72 log log(\u03b52T ) ) + 4\u221a log(\u03b52T )\n\n\u2264 4e log(\u03b5 2T )\n\u03b52T\n \u221a\n2\u221a \u03c0 log ( \u03b52T\n2\n) + 2\u221a2\u221a \u03c0 log ( \u03b52T\n4 log log(\u03b52T )\n) + 4\u221a log(\u03b52T ) \nIt can be shown, using notably the inequality log u \u2264 u/e, that for all \u03b52T \u2265 e2,\nlog(\u03b52T/2) \u2265 (1/2) log(\u03b52T )\nlog\n( \u03b52T\n4 log log(\u03b52T )\n) \u2265 ( 1\u2212 4\ne2\n) log(\u03b52T )\nand one obtains\u222b \u221e 0 f(u)du \u2264 4e log(\u03b5 2T ) \u03b52T \u00d7 2 + 2e \u221a 2/( \u221a e2 \u2212 4) + 4 \u221a \u03c0\u221a \u03c0 log(\u03b52T ) \u2264 30e \u03b52T \u221a log(\u03b52T ).\nFinally,\nP ( \u2203s \u2208 N : \u00b5\u0302s + \u221a 2\ns log\n( T\ns\n) + \u03b5 \u2264 0 ) \u2264 \u222b \u221e\n0\nf(u)du+ 2 max k f(k) \u2264 30e \u03b52T\n\u221a log(\u03b52T ) + 16e\n\u03b52T ."}, {"heading": "G Numerical Experiments", "text": "We represent here the regret of the five strategies presented in this article on a bandit problem with \u2206 = 1/5, for different values of the horizon. The regret is estimated by 4.105 Monte-Carlo replications. In the legend, the estimated slopes of \u2206R\u03c0(T ) (in logarithmic scale) are indicated after the policy names.\n50 100 200 500 1000 2000 5000 10000 20000 50000\n0 20\n40 60\n80 10\n0\nFB\u2212ETC : 3.65 BAI\u2212ETC : 2.98 UCB : 1.59 SPRT\u2212ETC : 1.03 D\u2212UCB : 0.77\nThe experimental behavior of the algorithms reflects the theoretical results presented above: the regret asymptotically grows as the logarithm of the horizon, the experimental coefficients correspond approximately to theory, and the relative ordering of the policies is respected. However, it should be noted that for short horizons the hierarchy is not quite the same, and the growth rate is not logarithmic; this question is raised in Garivier et al. [2016]."}], "references": [{"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem", "author": ["Peter Auer", "Ronald Ortner"], "venue": "Periodica Mathematica Hungarica,", "citeRegEx": "Auer and Ortner.,? \\Q2010\\E", "shortCiteRegEx": "Auer and Ortner.", "year": 2010}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f3 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Prior-free and prior-dependent regret bounds for thompson sampling", "author": ["S\u00e9bastien Bubeck", "Che-Yu Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bubeck and Liu.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck and Liu.", "year": 2013}, {"title": "Bounded regret in stochastic multi-armed bandits", "author": ["S\u00e9bastien Bubeck", "Vianney Perchet", "Philippe Rigollet"], "venue": "In Proceedings of the 26th Conference On Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "Kullback\u2013Leibler upper confidence bounds for optimal sequential allocation", "author": ["Olivier Capp\u00e9", "Aur\u00e9lien Garivier", "Odalric-Ambrym Maillard", "R\u00e9mi Munos", "Gilles Stoltz"], "venue": "The Annals of Statistics,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2013}, {"title": "Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems", "author": ["Eyal Even-Dar", "Shie Mannor", "Yishay Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2006}, {"title": "Optimal best arm identification with fixed confidence", "author": ["Aur\u00e9lien Garivier", "Emilie Kaufmann"], "venue": "In Proceedings of the 29th Conference On Learning Theory (to appear),", "citeRegEx": "Garivier and Kaufmann.,? \\Q2016\\E", "shortCiteRegEx": "Garivier and Kaufmann.", "year": 2016}, {"title": "Explore first, exploit next: The true shape of regret in bandit problems", "author": ["Aur\u00e9lien Garivier", "Pierre M\u00e9nard", "Gilles Stoltz"], "venue": "arXiv preprint arXiv:1602.07182,", "citeRegEx": "Garivier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garivier et al\\.", "year": 2016}, {"title": "Inequalities on the lambert w function and hyperpower function", "author": ["Abdolhossein Hoorfar", "Mehdi Hassani"], "venue": "J. Inequal. Pure and Appl. Math,", "citeRegEx": "Hoorfar and Hassani.,? \\Q2008\\E", "shortCiteRegEx": "Hoorfar and Hassani.", "year": 2008}, {"title": "Sequential choice from several populations", "author": ["Michael N Katehakis", "Herbert Robbins"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Katehakis and Robbins.,? \\Q1995\\E", "shortCiteRegEx": "Katehakis and Robbins.", "year": 1995}, {"title": "On the Complexity of A/B Testing", "author": ["Emilie Kaufmann", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier"], "venue": "In Proceedings of the 27th Conference On Learning Theory,", "citeRegEx": "Kaufmann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2014}, {"title": "Adaptive treatment allocation and the multi-armed bandit problem", "author": ["Tze Leung Lai"], "venue": "The Annals of Statistics,", "citeRegEx": "Lai.,? \\Q1987\\E", "shortCiteRegEx": "Lai.", "year": 1987}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Sequential design of comparative clinical trials", "author": ["Tze Leung Lai", "Herbert Robbins", "David Siegmund"], "venue": "Recent advances in statistics: papers in honor of Herman Chernoff on his sixtieth birthday,", "citeRegEx": "Lai et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1983}, {"title": "Optimally confident UCB: Improved regret for finite-armed bandits", "author": ["Tor Lattimore"], "venue": "Technical report,", "citeRegEx": "Lattimore.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore.", "year": 2015}, {"title": "Brownian motion, volume 30", "author": ["Peter M\u00f6rters", "Yuval Peres"], "venue": null, "citeRegEx": "M\u00f6rters and Peres.,? \\Q2010\\E", "shortCiteRegEx": "M\u00f6rters and Peres.", "year": 2010}, {"title": "The multi-armed bandit with covariates", "author": ["Vianney Perchet", "Philippe Rigollet"], "venue": "The Annals of Statistics,", "citeRegEx": "Perchet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Perchet and Rigollet.", "year": 2013}, {"title": "Batched bandit problems", "author": ["Vianney Perchet", "Philippe Rigollet", "Sylvain Chassang", "Eric Snowberg"], "venue": "In Proceedings of the 28th Conference On Learning Theory,", "citeRegEx": "Perchet et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Perchet et al\\.", "year": 2015}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "Sequential Tests of Statistical Hypotheses", "author": ["Abraham Wald"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Wald.,? \\Q1945\\E", "shortCiteRegEx": "Wald.", "year": 1945}], "referenceMentions": [{"referenceID": 19, "context": "This framework is known as the multi-armed bandit problem, which has many applications and has been studied for almost a century [Thompson, 1933].", "startOffset": 129, "endOffset": 145}, {"referenceID": 10, "context": "Surprisingly it is possible to do even better by using a fully sequential strategy inspired by the UCB algorithm for multi-armed bandits [Katehakis and Robbins, 1995].", "startOffset": 137, "endOffset": 166}, {"referenceID": 10, "context": "As before, strategies based on ETC are suboptimal by a factor of 2 relative to the optimal rates achieved by fully sequential strategies such as UCB, which satisfies R \u03bc(T ) \u223c 2 log(T )/\u2206 [Katehakis and Robbins, 1995].", "startOffset": 188, "endOffset": 217}, {"referenceID": 1, "context": "Moreover, strategies of this kind have been proposed in the literature for more complicated settings [Auer and Ortner, 2010, Perchet and Rigollet, 2013, Perchet et al., 2015]. Recent progress on optimal exploration policies (e.g., by Garivier and Kaufmann [2016]) could have suggested that well-tuned variants of two-phase strategies might be near-optimal.", "startOffset": 102, "endOffset": 263}, {"referenceID": 1, "context": "Moreover, strategies of this kind have been proposed in the literature for more complicated settings [Auer and Ortner, 2010, Perchet and Rigollet, 2013, Perchet et al., 2015]. Recent progress on optimal exploration policies (e.g., by Garivier and Kaufmann [2016]) could have suggested that well-tuned variants of two-phase strategies might be near-optimal. We show, on the contrary, that optimal strategies for multi-armed bandit problems must be fully-sequential, and in particular should mix exploration and exploitation. We study two settings, one when the gap \u2206 = |\u03bc1 \u2212 \u03bc2| is known and the other when it is not. The most straight-forward strategy in the former case is to explore each action a fixed number of times n and subsequently exploit by choosing the action that appeared best while exploring. It is easy to calculate the optimal n and consequently show that this strategy suffers a regret of R \u03bc(T ) \u223c 4 log(T )/\u2206. A more general approach is to use a so-called Explore-Then-Commit (ETC) strategy, following a nomenclature introduced by Perchet et al. [2015]. An ETC strategy explores each action alternately until some data-dependent stopping time and subsequently commits to a single action for the remaining time-steps.", "startOffset": 102, "endOffset": 1072}, {"referenceID": 1, "context": "Moreover, strategies of this kind have been proposed in the literature for more complicated settings [Auer and Ortner, 2010, Perchet and Rigollet, 2013, Perchet et al., 2015]. Recent progress on optimal exploration policies (e.g., by Garivier and Kaufmann [2016]) could have suggested that well-tuned variants of two-phase strategies might be near-optimal. We show, on the contrary, that optimal strategies for multi-armed bandit problems must be fully-sequential, and in particular should mix exploration and exploitation. We study two settings, one when the gap \u2206 = |\u03bc1 \u2212 \u03bc2| is known and the other when it is not. The most straight-forward strategy in the former case is to explore each action a fixed number of times n and subsequently exploit by choosing the action that appeared best while exploring. It is easy to calculate the optimal n and consequently show that this strategy suffers a regret of R \u03bc(T ) \u223c 4 log(T )/\u2206. A more general approach is to use a so-called Explore-Then-Commit (ETC) strategy, following a nomenclature introduced by Perchet et al. [2015]. An ETC strategy explores each action alternately until some data-dependent stopping time and subsequently commits to a single action for the remaining time-steps. We show in Theorem 2 that by using a sequential probability ratio test (SPRT) it is possible to design an ETC strategy for which R \u03bc(T ) \u223c log(T )/\u2206, which improves on the above result by a factor of 4. We also prove a lower bound showing that no ETC strategy can improve on this result. Surprisingly it is possible to do even better by using a fully sequential strategy inspired by the UCB algorithm for multi-armed bandits [Katehakis and Robbins, 1995]. We design a new strategy for which R \u03bc(T ) \u223c log(T )/(2\u2206), which improves on the fixed-design strategy by a factor of 8 and on SPRT by a factor of 2. Again we prove a lower bound showing that no strategy can improve on this result. For the case where \u2206 is unknown, fixed-design strategies are hopeless because there is no reasonable tuning for the exploration budget n. However, it is possible to design an ETC strategy for unknown gaps. Our approach uses a modified fixed-budget best arm identification (BAI) algorithm in its exploration phase (see e.g., Even-Dar et al. [2006], Garivier and Kaufmann [2016]) and chooses the recommended arm for the remaining time-steps.", "startOffset": 102, "endOffset": 2271}, {"referenceID": 1, "context": "Moreover, strategies of this kind have been proposed in the literature for more complicated settings [Auer and Ortner, 2010, Perchet and Rigollet, 2013, Perchet et al., 2015]. Recent progress on optimal exploration policies (e.g., by Garivier and Kaufmann [2016]) could have suggested that well-tuned variants of two-phase strategies might be near-optimal. We show, on the contrary, that optimal strategies for multi-armed bandit problems must be fully-sequential, and in particular should mix exploration and exploitation. We study two settings, one when the gap \u2206 = |\u03bc1 \u2212 \u03bc2| is known and the other when it is not. The most straight-forward strategy in the former case is to explore each action a fixed number of times n and subsequently exploit by choosing the action that appeared best while exploring. It is easy to calculate the optimal n and consequently show that this strategy suffers a regret of R \u03bc(T ) \u223c 4 log(T )/\u2206. A more general approach is to use a so-called Explore-Then-Commit (ETC) strategy, following a nomenclature introduced by Perchet et al. [2015]. An ETC strategy explores each action alternately until some data-dependent stopping time and subsequently commits to a single action for the remaining time-steps. We show in Theorem 2 that by using a sequential probability ratio test (SPRT) it is possible to design an ETC strategy for which R \u03bc(T ) \u223c log(T )/\u2206, which improves on the above result by a factor of 4. We also prove a lower bound showing that no ETC strategy can improve on this result. Surprisingly it is possible to do even better by using a fully sequential strategy inspired by the UCB algorithm for multi-armed bandits [Katehakis and Robbins, 1995]. We design a new strategy for which R \u03bc(T ) \u223c log(T )/(2\u2206), which improves on the fixed-design strategy by a factor of 8 and on SPRT by a factor of 2. Again we prove a lower bound showing that no strategy can improve on this result. For the case where \u2206 is unknown, fixed-design strategies are hopeless because there is no reasonable tuning for the exploration budget n. However, it is possible to design an ETC strategy for unknown gaps. Our approach uses a modified fixed-budget best arm identification (BAI) algorithm in its exploration phase (see e.g., Even-Dar et al. [2006], Garivier and Kaufmann [2016]) and chooses the recommended arm for the remaining time-steps.", "startOffset": 102, "endOffset": 2301}, {"referenceID": 11, "context": "As we consider two-armed, Gaussian bandits with equal variances, we focus here on uniform sampling rules, which have been shown in Kaufmann et al. [2014] to be optimal in that setting.", "startOffset": 131, "endOffset": 154}, {"referenceID": 12, "context": "For unknown \u2206 we briefly recall the well-known results, but also propose a new regret analysis of the UCB* algorithm, a variant of UCB that can be traced back to Lai [1987], for which we also obtain order-optimal minimax regret.", "startOffset": 162, "endOffset": 173}, {"referenceID": 8, "context": "Garivier et al. [2016]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "The work of Wald [1945] shows that a significant gain in terms of expected number of samples can be obtained by using a sequential rather than a batch test.", "startOffset": 12, "endOffset": 24}, {"referenceID": 11, "context": "In a bandit model with two Gaussian arms, Kaufmann et al. [2014] propose a \u03b4-PAC algorithm using a uniform sampling rule and a stopping rule \u03c4\u03b4 that asymptotically attains the minimal sample complexity E\u03bc[\u03c4\u03b4] \u223c (8/\u2206) log(1/\u03b4).", "startOffset": 42, "endOffset": 65}, {"referenceID": 12, "context": "Note that a similar strategy was proposed and analysed by Lai et al. [1983], but in the continuous time framework and with asymptotic analysis only.", "startOffset": 58, "endOffset": 76}, {"referenceID": 3, "context": "the former case, we are not aware of any previous research where the gap is known except the line of work by Bubeck et al. [2013], Bubeck and Liu [2013], where different questions are treated.", "startOffset": 109, "endOffset": 130}, {"referenceID": 3, "context": "[2013], Bubeck and Liu [2013], where different questions are treated.", "startOffset": 8, "endOffset": 30}, {"referenceID": 7, "context": "In the classical bandit setting where \u2206 is unknown, UCB by Katehakis and Robbins [1995] is known to be asymptotically optimal: R \u03bc (T ) \u223c 2 log(T )/\u2206, which matches the lower bound of Lai and Robbins [1985].", "startOffset": 59, "endOffset": 88}, {"referenceID": 7, "context": "In the classical bandit setting where \u2206 is unknown, UCB by Katehakis and Robbins [1995] is known to be asymptotically optimal: R \u03bc (T ) \u223c 2 log(T )/\u2206, which matches the lower bound of Lai and Robbins [1985]. Non-asymptotic regret bounds are given for example by Auer et al.", "startOffset": 59, "endOffset": 207}, {"referenceID": 1, "context": "Non-asymptotic regret bounds are given for example by Auer et al. [2002], Capp\u00e9 et al.", "startOffset": 54, "endOffset": 73}, {"referenceID": 1, "context": "Non-asymptotic regret bounds are given for example by Auer et al. [2002], Capp\u00e9 et al. [2013]. Unfortunately, UCB is not optimal in the minimax sense, which is so far only achieved by algorithms that are not asymptotically optimal [Audibert and Bubeck, 2009, Lattimore, 2015].", "startOffset": 54, "endOffset": 94}, {"referenceID": 0, "context": "Unfortunately, UCB is not optimal in the minimax sense, which is so far only achieved by algorithms that are not asymptotically optimal [Audibert and Bubeck, 2009, Lattimore, 2015]. Here, with only two arms, we are able to show that Algorithm 5 below is simultaneously minimax order-optimal and asymptotically optimal. The strategy is essentially the same as suggested by Lai [1987], but with a fractionally smaller confidence bound.", "startOffset": 137, "endOffset": 383}, {"referenceID": 8, "context": "The improvements come thanks to Inequality 4 in [Garivier et al., 2016], which states that for every (\u03bc1, \u03bc \u2032 2) \u2208 H and for every stopping time \u03c3 such that N2(T ) is F\u03c3-measurable, E\u03bc [ N1(\u03c3) ] (\u03bc1 \u2212 \u03bc1) 2 + E\u03bc [ N2(\u03c3) ] (\u03bc2 \u2212 \u03bc2) 2 \u2265 kl ( E\u03bc [ N2(T ) T ] , E\u03bc\u2032 [ N2(T ) T ]) ,", "startOffset": 48, "endOffset": 71}, {"referenceID": 11, "context": "This is the same approach used by Lai and Robbins [1985], but rewritten and generalised in a more powerful way (in particular regarding the ETC strategies).", "startOffset": 34, "endOffset": 57}, {"referenceID": 8, "context": "Moreover, note that this proof may also lead to (not-so-simple) non-asymptotic lower-bounds, as shown in Garivier et al. [2016] for example.", "startOffset": 105, "endOffset": 128}, {"referenceID": 11, "context": "If exploration is the objective, then uniform sampling is known to be optimal in the two-armed Gaussian case [Kaufmann et al., 2014], which justifies the uniform sampling assumption.", "startOffset": 109, "endOffset": 132}, {"referenceID": 8, "context": "If exploration is the objective, then uniform sampling is known to be optimal in the two-armed Gaussian case [Kaufmann et al., 2014], which justifies the uniform sampling assumption. The use of ETC strategies for regret minimisation (e.g., as presented by Perchet and Rigollet [2013]) is certainly not limited to bandit models with 2 arms.", "startOffset": 110, "endOffset": 284}, {"referenceID": 1, "context": "Arms are eliminated from the active set once their optimality becomes implausible and the exploration phase terminates when the active set contains only a single arm (an example is by Auer and Ortner [2010]).", "startOffset": 184, "endOffset": 207}, {"referenceID": 1, "context": "Arms are eliminated from the active set once their optimality becomes implausible and the exploration phase terminates when the active set contains only a single arm (an example is by Auer and Ortner [2010]). The Successive Elimination algorithm has been introduced by Even-Dar et al. [2006] for best-arm identification in the fixed-confidence setting.", "startOffset": 184, "endOffset": 292}, {"referenceID": 1, "context": "Arms are eliminated from the active set once their optimality becomes implausible and the exploration phase terminates when the active set contains only a single arm (an example is by Auer and Ortner [2010]). The Successive Elimination algorithm has been introduced by Even-Dar et al. [2006] for best-arm identification in the fixed-confidence setting. But as shown in Garivier and Kaufmann [2016], Successive Elimination is suboptimal for the best arm identification task in almost all settings except two-armed Gaussian bandits.", "startOffset": 184, "endOffset": 398}, {"referenceID": 1, "context": "Arms are eliminated from the active set once their optimality becomes implausible and the exploration phase terminates when the active set contains only a single arm (an example is by Auer and Ortner [2010]). The Successive Elimination algorithm has been introduced by Even-Dar et al. [2006] for best-arm identification in the fixed-confidence setting. But as shown in Garivier and Kaufmann [2016], Successive Elimination is suboptimal for the best arm identification task in almost all settings except two-armed Gaussian bandits. It is therefore interesting to investigate the performance in terms of regret of an ETC algorithm using an optimal BAI algorithm. This is actually possible not only for Gaussian distributions, but more generally for one-parameter exponential families, for which Garivier and Kaufmann [2016] propose the asymptotically optimal Track-and-Stop strategy.", "startOffset": 184, "endOffset": 822}, {"referenceID": 9, "context": "If T\u2206 \u2265 4 \u221a 2\u03c0e, the inequality W (y) \u2264 log ( (1 + e\u22121)y/ log(y) ) valid for all y \u2265 e (see Hoorfar and Hassani [2008]) entails", "startOffset": 92, "endOffset": 119}, {"referenceID": 16, "context": ", M\u00f6rters and Peres [2010]) it holds that", "startOffset": 2, "endOffset": 27}, {"referenceID": 8, "context": "However, it should be noted that for short horizons the hierarchy is not quite the same, and the growth rate is not logarithmic; this question is raised in Garivier et al. [2016].", "startOffset": 156, "endOffset": 179}], "year": 2017, "abstractText": "We study the problem of minimising regret in two-armed bandit problems with Gaussian rewards. Our objective is to use this simple setting to illustrate that strategies based on an exploration phase (up to a stopping time) followed by exploitation are necessarily suboptimal. The results hold regardless of whether or not the difference in means between the two arms is known. Besides the main message, we also refine existing deviation inequalities, which allow us to design fully sequential strategies with finite-time regret guarantees that are (a) asymptotically optimal as the horizon grows and (b) order-optimal in the minimax sense. Furthermore we provide empirical evidence that the theory also holds in practice and discuss extensions to non-gaussian and multiple-armed case.", "creator": "LaTeX with hyperref package"}}}