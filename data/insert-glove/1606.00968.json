{"id": "1606.00968", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Smooth Imitation Learning for Online Sequence Prediction", "abstract": "We undiagnosed study the lance-shaped problem of then-future smooth annu imitation distinguishable learning for online resorting sequence sakay prediction, ceri where 2303 the paracompact goal afvs is brightmail to train overdramatized a tadjourah policy briere that can roosmalen smoothly imitate demonstrated behavior in a dynamic and suvorin continuous handbells environment batterberry in flivver response 174.4 to online, masurians sequential context metzen input. Since the carlisi mapping from context to behavior salutatorian is often complex, we take andrej a learning anti-vaccination reduction l'etat approach to reduce miloon smooth 13.32 imitation learning 60.0 to a valia regression problem urquhart using complex function phenomenological classes that clubber are zermatt regularized joint-stock to slac ensure smoothness. chcs We evictees present shishkina a arness learning combichrist meta - anatoly algorithm uprights that achieves moyale fast and offenheiser stable convergence to a good live-action policy. Our dico approach enjoys 6:57 several chaplygin attractive ciganlija properties, blotz including ufr being nsimba fully deterministic, universit\u00e0 employing u.s.-educated an adaptive learning fidele rate that can pre-recording provably yield larger policy improvements olubunmi compared to easler previous z\u0101r approaches, and yellow-eyed the vyner ability 49.80 to ensure chizuru stable cheekily convergence. Our empirical highroad results demonstrate sagna significant badji performance gains over laths previous bolocco approaches.", "histories": [["v1", "Fri, 3 Jun 2016 05:25:27 GMT  (912kb,D)", "http://arxiv.org/abs/1606.00968v1", "ICML 2016"]], "COMMENTS": "ICML 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hoang minh le 0002", "andrew kang", "yisong yue", "peter carr 0001"], "accepted": true, "id": "1606.00968"}, "pdf": {"name": "1606.00968.pdf", "metadata": {"source": "META", "title": "Smooth Imitation Learning for Online Sequence Prediction", "authors": ["Hoang M. Le", "Andrew Kang", "Yisong Yue", "Peter Carr"], "emails": ["HMLE@CALTECH.EDU", "AKANG@CALTECH.EDU", "YYUE@CALTECH.EDU", "PETER.CARR@DISNEYRESEARCH.COM"], "sections": [{"heading": "1. Introduction", "text": "In many complex planning and control tasks, it can be very challenging to explicitly specify a good policy. For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).\nIn this paper, we study the problem of imitation learning for smooth online sequence prediction in a continuous regime.\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nOnline sequence prediction is the problem of making online decisions in response to exogenous input from the environment, and is a special case of reinforcement learning (see Section 2). We are further interested in policies that make smooth predictions in a continuous action space.\nOur motivating example is the problem of learning smooth policies for automated camera planning (Chen et al., 2016): determining where a camera should look given environment information (e.g., noisy person detections) and corresponding demonstrations from a human expert.1 It is widely accepted that a smoothly moving camera is essential for generating aesthetic video (Gaddam et al., 2015). From a problem formulation standpoint, one key difference between smooth imitation learning and conventional imitation learning is the use of a \u201csmooth\u201d policy class (which we formalize in Section 2), and the goal now is to mimic expert demonstrations by choosing the best smooth policy.\nThe conventional supervised learning approach to imitation learning is to train a classifier or regressor to predict the expert\u2019s behavior given training data comprising input/output pairs of contexts and actions taken by the expert. However, the learned policy\u2019s prediction affects (the distribution of) future states during the policy\u2019s actual execution, and so violates the crucial i.i.d. assumption made by most statistical learning approaches. To address this issue, numerous learning reduction approaches have been proposed (Daume\u0301 III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011), which iteratively modify the training distribution in various ways such that any supervised learning guarantees provably lift to the sequential imitation setting (potentially at the cost of statistical or computational efficiency).\nWe present a learning reduction approach to smooth imitation learning for online sequence prediction, which we call SIMILE (Smooth IMItation LEarning). Building\n1Access data at http://www.disneyresearch.com/ publication/smooth-imitation-learning/ and code at http://github.com/hoangminhle/SIMILE.\nar X\niv :1\n60 6.\n00 96\n8v 1\n[ cs\n.L G\n] 3\nJ un\n2 01\nupon learning reductions that employ policy aggregation (Daume\u0301 III et al., 2009), we provably lift supervised learning guarantees to the smooth imitation setting and show much faster convergence behavior compared to previous work. Our contributions can be summarized as: \u2022 We formalize the problem of smooth imitation learn-\ning for online sequence prediction, and introduce a family of smooth policy classes that is amenable to supervised learning reductions. \u2022 We present a principled learning reduction approach, which we call SIMILE. Our approach enjoys several attractive practical properties, including learning a fully deterministic stationary policy (as opposed to SEARN (Daume\u0301 III et al., 2009)), and not requiring data aggregation (as opposed to DAgger (Ross et al., 2011)) which can lead to super-linear training time. \u2022 We provide performance guarantees that lift the the underlying supervised learning guarantees to the smooth imitation setting. Our guarantees hold in the agnostic setting, i.e., when the supervised learner might not achieve perfect prediction. \u2022 We show how to exploit a stability property of our smooth policy class to enable adaptive learning rates that yield provably much faster convergence compared to SEARN (Daume\u0301 III et al., 2009). \u2022 We empirically evaluate using the setting of smooth camera planning (Chen et al., 2016), and demonstrate the performance gains of our approach."}, {"heading": "2. Problem Formulation", "text": "Let X \u201c tx1, . . . , xT u \u0102 X T denote a context sequence from the environment X , and A \u201c ta1, . . . , aT u \u0102 AT denote an action sequence from some action space A. Context sequence is exogenous, meaning at does not influence future context xt`k for k \u011b 1. Let \u03a0 denote a policy class, where each \u03c0 P \u03a0 generates an action sequence A in response to a context sequence X. Assume X \u0102 Rm,A \u0102 Rk are continuous and infinite, with A non-negative and bounded such that ~0 \u013a a \u013a R~1 @a P A. Predicting actions at may depend on recent contexts xt, . . . , xt\u00b4p and actions at\u00b41, . . . , at\u00b4q . Without loss of generality, we define a state space S as tst \u201c rxt, at\u00b41su.2 Policies \u03c0 can thus be viewed as mapping states S \u201c X\u02c6A to actions A. A roll-out of \u03c0 given context sequence X \u201c tx1, . . . , xT u is the action sequence A \u201c ta1, . . . , aT u:\nat \u201c \u03c0pstq \u201c \u03c0prxt, at\u00b41sq, st`1 \u201c rxt`1, ats @t P r1, . . . , T s .\nNote that unlike the general reinforcement learning problem, we consider the setting where the state space splits into external and internal components (by definition, at influences subsequent states st`k, but not xt`k). The use\n2We can always concatenate consecutive contexts and actions.\nof exogenous contexts txtu models settings where a policy needs to take online, sequential actions based on external environmental inputs, e.g. smooth self-driving vehicles for obstacle avoidance, helicopter aerobatics in the presence of turbulence, or smart grid management for external energy demand. The technical motivation of this dichotomy is that we will enforce smoothness only on the internal state.\nConsider the example of autonomous camera planning for broadcasting a sport event (Chen et al., 2016). X can correspond to game information such as the locations of the players, the ball, etc., and A can correspond to the pantilt-zoom configuration of the broadcast camera. Manually specifying a good camera policy can be very challenging due to sheer complexity involved with mapping X to A. It is much more natural to train \u03c0 P \u03a0 to mimic observed expert demonstrations. For instance, \u03a0 can be the space of neural networks or tree-based ensembles (or both).\nFollowing the basic setup from (Ross et al., 2011), for any policy \u03c0 P \u03a0, let d\u03c0t denote the distribution of states at time t if \u03c0 is executed for the first t\u00b41 time steps. Furthermore, let d\u03c0 \u201c 1T \u0159T t\u201c1 d \u03c0 t be the average distribution of states if we follow \u03c0 for all T steps. The goal of imitation learning is to find a policy \u03c0\u0302 P \u03a0 which minimizes the imitation loss under its own induced distribution of states: \u03c0\u0302 \u201c argmin\n\u03c0P\u03a0 `\u03c0p\u03c0q \u201c argmin \u03c0P\u03a0 Es\u201ed\u03c0 r`p\u03c0psqqs , (1)\nwhere the (convex) imitation loss `p\u03c0psqq captures how well \u03c0 imitates expert demonstrations for state s. One common ` is squared loss between the policy\u2019s decision and the expert demonstration: `p\u03c0psqq \u201c }\u03c0psq\u00b4\u03c0\u02dapsq}2 for some norm }.}. Note that computing ` typically requires having access to a training set of expert demonstrations \u03c0\u02da on some set of context sequences. We also assume an agnostic setting, where the minimizer of (1) does not necessarily achieve 0 loss (i.e. it cannot perfectly imitate the expert)."}, {"heading": "2.1. Smooth Imitation Learning & Smooth Policy Class", "text": "In addition to accuracy, a key requirement of many continuous control and planning problems is smoothness (e.g., smooth camera trajectories). Generally, \u201csmoothness\u201d may reflect domain knowledge about stability properties or approximate equilibria of a dynamical system. We thus formalize the problem of smooth imitation learning as minimizing (1) over a smooth policy class \u03a0.\nMost previous work on learning smooth policies focused on simple policy classes such as linear models (Abbeel & Ng, 2004), which can be overly restrictive. We instead define a much more general smooth policy class \u03a0 as a regularized space of complex models.\nDefinition 2.1 (Smooth policy class \u03a0). Given a complex model class F and a class of smooth regularizers H, we define smooth policy class \u03a0 \u0102 F\u02c6H as satisfying:\n\u03a0 fi t\u03c0 \u201c pf, hq,f P F , h P H | \u03c0psq is close to both fpx, aq and hpaq @ induced state s \u201c rx, as P Su\nwhere closeness is controlled by regularization. For instance, F can be the space of neural networks or decision trees and H be the space of smooth analytic functions. \u03a0 can thus be viewed as policies that predict close to some f P F but are regularized to be close to some h P H. For sufficiently expressive F , we often have that \u03a0 \u0102 F . Thus optimizing over \u03a0 can be viewed as constrained optimization over F (by H), which can be challenging. Our SIMILE approach integrates alternating optimization (between F and H) into the learning reduction. We provide two concrete examples of \u03a0 below.\nExample 2.1 (\u03a0\u03bb). Let F be any complex supervised model class, and define the simplest possibleH fi thpaq \u201c au. Given f P F , the prediction of a policy \u03c0 can be viewed as regularized optimization over the action space to ensure closeness of \u03c0 to both f and h:\n\u03c0px, aq \u201c argmin a1PA\n\u203a \u203afpx, aq \u00b4 a1 \u203a \u203a 2 ` \u03bb \u203a \u203ahpaq \u00b4 a1 \u203a \u203a 2\n\u201c fpx, aq ` \u03bbhpaq 1` \u03bb \u201c fpx, aq ` \u03bba 1` \u03bb , (2)\nwhere regularization parameter \u03bb trades-off closeness to f and to previous action. For large \u03bb, \u03c0px, aq is encouraged make predictions that stays close to previous action a.\nExample 2.2 (Linear auto-regressor smooth regularizers). Let F be any complex supervised model class, and define H using linear auto-regressors,H fi thpaq \u201c \u03b8Jau, which model actions as a linear dynamical system (Wold, 1939). We can define \u03c0 analogously to (2).\nIn general, SIMILE requires that \u03a0 satisfies a smooth property stated below. This property, which is exploited in our theoretical analysis (see Section 5), is motivated by the observation that given a (near) constant stream of context sequence, a stable behavior policy should exhibit a corresponding action sequence with low curvature. The two examples above satisfy this property for sufficiently large \u03bb.\nDefinition 2.2 (H-state-smooth imitation policy). For small constant 0 \u0103 H ! 1, a policy \u03c0prx, asq is Hstate-smooth if it is H-smooth w.r.t. a, i.e. for fixed x P X , @a, a1 P A, @i: \u203a \u203a\u2207\u03c0iprx, asq \u00b4\u2207\u03c0iprx, a1sq \u203a \u203a\n\u02da \u010f H }a\u00b4 a1} where \u03c0i indicates the ith component of vector-valued function3 \u03c0psq \u201c \u201c \u03c01psq, . . . , \u03c0kpsq \u2030\nP Rk, and }.} and }.}\u02da are some norm and dual norm respectively. For twice differentiable policy \u03c0, this is equivalent to having the bound on the Hessian\u22072\u03c0iprx, asq \u013a HIk @i.\n3This emphasizes the possibility that \u03c0 is a vector-valued function of a. The gradient and Hessian are viewed as arrays of k gradient vectors and Hessian matrices of 1-d case, since we simply treat action in Rk as an array of k standard functions."}, {"heading": "3. Related Work", "text": "The most popular traditional approaches for learning from expert demonstration focused on using approximate policy iteration techniques in the MDP setting (Kakade & Langford, 2002; Bagnell et al., 2003). Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009). Some focus on continuous state space (Abbeel & Ng, 2005), but requires a linear model for the system dynamics. In contrast, we focus on learning complex smooth functions within continuous action and state spaces. One natural approach to tackle the more general setting is to reduce imitation learning to a standard supervised learning problem (Syed & Schapire, 2010; Langford & Zadrozny, 2005; Lagoudakis & Parr, 2003). However, standard supervised methods assume i.i.d. training and test examples, thus ignoring the distribution mismatch between training and rolled-out trajectories directly applied to sequential learning problems (Kakade & Langford, 2002). Thus a naive supervised learning approach normally leads to unsatisfactory results (Ross & Bagnell, 2010). Iterative Learning Reductions. State-of-the-art learning reductions for imitation learning typically take an iterative approach, where each training round uses standard supervised learning to learn a policy (Daume\u0301 III et al., 2009; Ross et al., 2011). In each round n, the following happens: \u2022 Given initial state s0 drawn from the starting distribu-\ntion of states, the learner executes current policy \u03c0n, resulting in a sequence of states sn1 , . . . , s n T . \u2022 For each snt , a label pant (e.g., expert feedback) is collected indicating what the expert would do given snt , resulting in a new dataset Dn \u201c tpst,pant qu. \u2022 The learner integrates Dn to learn a policy \u03c0\u0302n. The learner updates the current policy to \u03c0n`1 based on \u03c0\u0302n and \u03c0n.\nThe main challenge is controlling for the cascading errors caused by the changing dynamics of the system, i.e., the distribution of states in each Dn \u201e d\u03c0n . A policy trained using d\u03c0n induces a different distribution of states than d\u03c0n , and so is no longer being evaluated on the same distribution as during training. A principled reduction should (approximately) preserve the i.i.d. relationship between training and test examples. Furthermore the state distribution d\u03c0 should converge to a stationary distribution.\nThe arguably most notable learning reduction approaches for imitation learning are SEARN (Daume\u0301 III et al., 2009) and DAgger (Ross et al., 2011). At each round, SEARN learns a new policy \u03c0\u0302n and returns a distribution (or mixture) over previously learned policies: \u03c0n`1 \u201c \u03b2\u03c0\u0302n`p1\u00b4 \u03b2q\u03c0n for \u03b2 P p0, 1q. For appropriately small choices of \u03b2, this stochastic mixing limits the \u201cdistribution drift\u201d between \u03c0n and \u03c0n`1 and can provably guarantee that the\nperformance of \u03c0n`1 does not degrage significantly relative to the expert demonstrations.4\nDAgger, on the other hand, achieves stability by aggregating a new dataset at each round to learn a new policy from the combined data set D \u00d0 D Y Dn. This aggregation, however, significantly increases the computational complexity and thus is not practical for large problems that require many iterations of learning (since the training time grows super-linearly w.r.t. the number of iterations).\nBoth SEARN and DAgger showed that only a polynomial number of training rounds is required for convergence to a good policy, but with a dependence on the length of horizon T . In particular, to non-trivially bound the total variation distance }d\u03c0new \u00b4 d\u03c0old}1 of the state distributions between old and new policies, a learning rate \u03b2 \u0103 1T is required to hold (Lemma 1 of Daume\u0301 III, Langford, and Marcu (2009) and Theorem 4.1 of Ross, Gordon, and Bagnell (2011)). As such, systems with very large time horizons might suffer from very slow convergence. Our Contributions. Within the context of previous work, our SIMILE approach can be viewed as extending SEARN to smooth policy classes with the following improvements:\n\u2022 We provide a policy improvement bound that does not depend on the time horizon T , and can thus converge much faster. In addition, SIMILE has adaptive learning rate, which can further improve convergence. \u2022 For the smooth policy class described in Section 2, we show how to generate simulated or \u201cvirtual\u201d expert feedback in order to guarantee stable learning. This alleviates the need to have continuous access to a dynamic oracle / expert that shows the learner what to do when it is off-track. In this regard, the way SIMILE integrates expert feedback subsumes the set-up from SEARN and DAgger. \u2022 Unlike SEARN, SIMILE returns fully deterministic policies. Under the continuous setting, deterministic policies are strictly better than stochastic policies as (i) smoothness is critical and (ii) policy sampling requires holding more data during training, which may not be practical for infinite state and action spaces. \u2022 Our theoretical analysis reveals a new sequential prediction setting that yields provably fast convergence, in particular for smooth policy classes on finitehorizon problems. Existing settings that enjoy such results are limited to Markovian dynamics with discounted future rewards or linear model classes."}, {"heading": "4. Smooth Imitation Learning Algorithm", "text": "Our learning algorithm, called SIMILE (Smooth IMItation LEarning), is described in Algorithm 1. At a high level, the process can be described as:\n4A similar approach was adopted in Conservative Policy Iteration for the MDP setting (Kakade & Langford, 2002).\nAlgorithm 1 SIMILE (Smooth IMItation LEarning) Input: features X \u201c txtu, human trajectory A\u02da \u201c ta\u02dat u,\nbase routine Train, smooth regularizers h P H 1: Initialize A0 \u00d0 A\u02da,S0 \u00d0 t \u201c xt, a \u02da t\u00b41 \u2030 u,\nh0 \u201c argmin hPH\nT \u0159\nt\u201c1\n\u203a \u203aa\u02dat \u00b4 hpa\u02dat\u00b41q \u203a \u203a\n2: Initial policy \u03c00 \u201c \u03c0\u03020 \u00d0 TrainpS0,A0| h0q 3: for n \u201c 1, . . . , N do 4: An \u201c tant u \u00d0 \u03c0n\u00b41pSn\u00b41q //sequential roll-out 5: Sn \u00d0 tsnt \u201c \u201c xt, a n t\u00b41 \u2030\nu //snt \u201c rxt:t\u00b4p, at\u00b41:t\u00b4qs 6: pAn \u201c tpant u @snt P Sn // collect smooth feedback\n7: hn \u201c argmin hPH\nT \u0159\nt\u201c1\n\u203a \u203a pant \u00b4 hppant\u00b41q \u203a \u203a //new regularizer\n8: \u03c0\u0302n \u00d0 TrainpSn, pAn| hnq // train policy 9: \u03b2 \u00d0 \u03b2p`p\u03c0\u0302nq, `p\u03c0n\u00b41qq //adaptively set \u03b2 10: \u03c0n \u201c \u03b2\u03c0\u0302n ` p1\u00b4 \u03b2q\u03c0n\u00b41 // update policy 11: end for output Last policy \u03c0N\n1. Start with some initial policy \u03c0\u03020 (Line 2). 2. At iteration n, use \u03c0n\u00b41 to build a new state distribu-\ntion Sn and dataset Dn \u201c tpsnt ,pant qu (Lines 4-6). 3. Train \u03c0\u0302n \u201c argmin\u03c0P\u03a0 Es\u201eSn r`np\u03c0psqqs, where `n\nis the imitation loss (Lines 7-8). Note that `n needs not be the original `, but simply needs to converge to it. 4. Interpolate \u03c0\u0302n and \u03c0n\u00b41 to generate a new deterministic policy \u03c0n (Lines 9-10). Repeat from Step 2 with n\u00d0 n` 1 until some termination condition is met.\nSupervised Learning Reduction. The actual reduction is in Lines 7-8, where we follow a two-step procedure of first updating the smooth regularize hn, and then training \u03c0\u0302n via supervised learning. In other words, Train finds the best f P F possible for a fixed hn. We discuss how to set the training targets pant below. Policy Update. The new policy \u03c0n is a deterministic interpolation between the previous \u03c0n\u00b41 and the newly learned \u03c0\u0302n (Line 10). In contrast, for SEARN, \u03c0n is a stochastic interploation (Daume\u0301 III et al., 2009). Lemma 5.2 and Corollary 5.3 show that deterministic interpolation converges at least as fast as stochastic for smooth policy classes.\nThis interpolation step plays two key roles. First, it is a form of myopic or greedy online learning. Intuitively, rolling out \u03c0n leads to incidental exploration on the mistakes of \u03c0n, and so each round of training is focused on refining \u03c0n. Second, the interpolation in Line 10 ensures a slow drift in the distribution of states from round to round, which preserves an approximate i.i.d. property for the supervised regression subroutine and guarantees convergence.\nHowever this model interpolation creates an inherent tension between maintaining approximate i.i.d. for valid su-\npervised learning and more aggressive exploration (and thus faster convergence). For example, SEARN\u2019s guarantees only apply for small \u03b2 \u0103 1{T . SIMILE circumvents much of this tension via a policy improvement bound that allows \u03b2 to adaptively increase depending on the quality of \u03c0\u0302n (see Theorem 5.6), which thus guarantees a valid learning reduction while substantially speeding up convergence. Feedback Generation. We can generate training targets pant using \u201cvirtual\u201d feedback from simulating expert demonstrations, which has two benefits. First, we need not query the expert \u03c0\u02da at every iteration (as done in DAgger (Ross et al., 2011)). Continuously acquiring expert demonstrations at every round can be seen as a special case and a more expensive strategy. Second, virtual feedback ensures stable learning, i.e., every \u03c0\u0302n is a feasible smooth policy.\n000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053\n054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107\nCVPR #307\nCVPR #307\nCVPR 2015 Submission #307. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\nLearning Online Smooth Predictors for Realtime Camera Planning\nAnonym u CVPR submission\nPaper ID 307\nAbstract\nData-driven prediction methods are extremely useful in many computer vision applications. However, the estimators are normally learned within a time independent context. When used for online prediction, the results are jittery. Although smoothing can be added after the fact (such as a Kalman filter), the approach is not ideal. Instead, temporal smoothness should be incorporated into the learning process. In this paper, we show how the \u2018search and learn\u2019 algorithm (which has been used previously for tagging parts of speech) can be adapted to efficiently learn regressors for temporal signals. We apply our data-driven learning technique to a camera planning problem: given noisy basketball player detection data, we learn where the camera should look based on examples from a human operator. Our experimental results show how a learning algorithm which takes into account temporal consistency of sequential predictions has significantly better performance than time independent estimators.\n1. Introduction In this work, we investigate the problem of determining where a camera should look when broadcasting a basketball game (see Fig. 1). Realtime camera planning shares many similarities with online object tracking: in both cases, the algorithms must constantly revise an estimated target position as new evidence is acquired. Noise and other ambiguities cause non-ideal jittery trajectories: they are are not good representations of how objects actually move, and in camera planning, lead to unaesthetic results. In practice, temporal regularization is employed to minimize jitter. The amount of regularization is a design parameter, and controls a trade-off between precision and smoothness. In contrast to object tracking, smoothness is of paramount importance in camera control: fluid movements which maintain adequate framing are preferable to erratic motions which pursue perfect composition.\nModel-free estimation methods, such as random forests, are very popular because they can be learned directly from\nFigure 1: Camera Planning. The objective is to predict\nan appropriate pan angle for a broadcast camera based on noisy player detection data. Consider two planning algorithms (shown as blue and red curves in the schematic) which both make the same mistake at time A but recover to a good framing by C (the ideal camera trajectory is shown in black). The blue solution quickly corrects by time B using a jerky motion, whereas the red curve conducts a gradual correction. Although the red curve has a larger discrepancy with the ideal motion curve, its velocity characteristics are most similar to the ideal motion path.\ndata. Often, the estimator is learned within a time independent paradigm, and temporal regularization is integrated as a post-processing stage (such as a Kalman filter). However, this two stage approach is not ideal because the data-driven estimator is prevented from learning any temporal patterns. In this paper, we condition the data-driven estimator on previous predictions, which allows it to learn temporal patterns within the data (in addition to any direct feature-based relationships). However, this recursive formulation (similar to reinforcement learning) makes the problem much more difficult to solve. We employ a variant of the \u2018search and learn\u2019 (SEARN) algorithm to keep training efficient. Its strategy is to decouple the recursive relationships using an auxiliary reference signal. This allows the predictor to be learned efficiently using supervised techniques, and our experiments demonstrate significant improvements when using this holistic approach.\nProblem Definition In the case of camera planning, we assume there is an underlying function f : X 7! Y which describes the ideal camera work that should occur at the\n1\nConsider Figure 1, where our policy \u03c0n (blue/red) made a mistake at location A, and where we have only a single expert demonstration from \u03c0\u02da (black). Depending on the smoothness requirements of the policy class, we can simulate virtual expert feedback as via ei-\nther the red line (more smooth) or blue (less smooth) as a tr deoff between squared imitation loss and smoothness.\nWhen the roll-out of \u03c0n\u00b41 (i.e. An) differs substantially from A\u02da, especially during early iterati ns, using smoother feedback (red instead of blue) can result in more stable learning. We formalize this notion for \u03a0\u03bb in Proposition 5.8. Intuitively, whenever \u03c0n\u00b41 makes a mistake, resulti g in a \u201cbad\u201d state snt , the feedback should recommend a smooth correction pant w.r.t. An to make training \u201ceasier\u201d for the learner.5 The virtual feedback pant should converge to the expert\u2019s action a\u02dat . In practice, we use pant \u201c \u03c3ant ` p1\u00b4 \u03c3qa\u02dat with \u03c3 \u00d1 0 s n increases (which satisfies Proposition 5.8)."}, {"heading": "5. Theoretical Results", "text": "All proofs re deferred to the supplementary m erial."}, {"heading": "5.1. Stability Conditions", "text": "One natural smoothness condition is that \u03c0prx, asq should be stable w.r.t. if x is fixed. Consider the camera planning setting: the expert policy \u03c0\u02da should have very small curvatur , since constant inputs should correspond to constant actions. This motivates Definition 2.2, which requires that \u03a0 has low curvature given fixed context. We also show that smooth policies per Definition 2.2 lead to stable actions, in the sense that \u201cn arby\u201d states are m pped to \u201cne rby\u201d actions. The following helper lemma is useful:\n5A similar idea was proposed (He et al., 2012) for DAggertype algorithm, albeit only for linear model classes.\nLemma 5.1. For a fixed x, define \u03c0prx, asq fi \u03d5paq. If \u03d5 is non-negative and H-smooth w.r.t. a., then:\n@a, a1 : ` \u03d5paq \u00b4 \u03d5pa1q \u02d82 \u010f 6H ` \u03d5paq ` \u03d5pa1q \u02d8 \u203a \u203aa\u00b4 a1 \u203a \u203a 2 .\nWriting \u03c0 as \u03c0prx, asq fi \u201c \u03c01prx, asq, . . . , \u03c0kprx, asq \u2030 with each \u03c0iprx, asq H-smooth, Lemma 5.1 implies }p\u03c0prx, asq \u00b4 \u03c0prx, a1sqq} \u010f ? 12HR }a\u00b4 a1} for R upper bounding A. Bounded action space means that a sufficiently small H leads to the following stability conditions:\nCondition 1 (Stability Condition 1). \u03a0 satisfies the Stability Condition 1 if for a fixed input feature x, the actions of \u03c0 in states s \u201c rx, as and s1 \u201c rx, a1s satisfy }\u03c0psq \u00b4 \u03c0ps1q} \u010f }a\u00b4 a1} for all a, a1 P A. Condition 2 (Stability Condition 2). \u03a0 satisfies Stability Condition 2 if each \u03c0 is \u03b3-Lipschitz continuous in the action component a with \u03b3 \u0103 1. That is, for a fixed x the actions of \u03c0 in states s \u201c rx, as and s1 \u201c rx, a1s satisfy }\u03c0psq \u00b4 \u03c0ps1q} \u010f \u03b3 }a\u00b4 a1} for all a, a1 P A. These two conditions directly follow from Lemma 5.1 and assuming sufficiently small H . Condition 2 is mildly stronger than Condition 1, and enables proving much stronger policy improvement compared to previous work."}, {"heading": "5.2. Deterministic versus Stochastic", "text": "Given two policies \u03c0 and \u03c0\u0302, and interpolation parameter \u03b2 P p0, 1q, consider two ways to combine policies:\n1. stochastic: \u03c0stopsq \u201c \u03c0\u0302psq with probability \u03b2, and \u03c0stopsq \u201c \u03c0psq with probability 1\u00b4 \u03b2 2. deterministic: \u03c0detpsq \u201c \u03b2\u03c0\u0302psq ` p1\u00b4 \u03b2q\u03c0psq\nPrevious learning reduction approaches only use stochastic interpolation (Daume\u0301 III et al., 2009; Ross et al., 2011), whereas SIMILE uses deterministic. The following result shows that deterministic and stochastic interpolation yield the same expected behavior for smooth policy classes.\nLemma 5.2. Given any starting state s0, sequentially execute \u03c0det and \u03c0sto to obtain two separate trajectories A \u201c tatuTt\u201c1 and A\u0303 \u201c ta\u0303tuTt\u201c1 such that at \u201c \u03c0detpstq and a\u0303t \u201c \u03c0stops\u0303tq, where st \u201c rxt, at\u00b41s and s\u0303t \u201c rxt, a\u0303t\u00b41s. Assuming the policies are stable as per Condition 1, we have EA\u0303ra\u0303ts \u201c at @t \u201c 1, . . . , T , where the expectation is taken over all random roll-outs of \u03c0sto.\nLemma 5.2 shows that deterministic policy combination (SIMILE) yields unbiased trajectory roll-outs of stochastic policy combination (as done in SEARN & CPI). This represents a major advantage of SIMILE, since the number of stochastic roll-outs of \u03c0sto to average to the deterministic trajectory of \u03c0det is polynomial in the time horizon T , leading to much higher computational complexity. Furthermore, for convex imitation loss `\u03c0p\u03c0q, Lemma 5.2 and Jensen\u2019s inequality yield the following corollary, which states that under convex loss, deterministic policy performs at least no worse than stochastic policy in expectation:\nCorollary 5.3 (Deterministic Policies Perform Better). For deterministic \u03c0det and stochastic \u03c0sto interpolations of two policies \u03c0 and \u03c0\u0302, and convex loss `, we have:\n`\u03c0detp\u03c0detq \u201c `\u03c0stopEr\u03c0stosq \u010f E r`\u03c0stop\u03c0stoqs\nwhere the expectation is over all roll-outs of \u03c0sto.\nRemark. We construct a simple example to show that Condition 1 may be necessary for iterative learning reductions to converge. Consider the case where contexts X \u0102 R are either constant or vary neglibly. Expert demonstrations should be constant \u03c0\u02daprxn, a\u02dasq \u201c a\u02da for all n. Consider an unstable policy \u03c0 such that \u03c0psq \u201c \u03c0prx, asq \u201c ka for fixed k \u0105 1. The rolled-out trajectory of \u03c0 diverges \u03c0\u02da at an exponential rate. Assume optimistically that \u03c0\u0302 learns the correct expert behavior, which is simply \u03c0\u0302psq \u201c \u03c0\u0302prx, asq \u201c a. For any \u03b2 P p0, 1q, the updated policy \u03c01 \u201c \u03b2\u03c0\u0302`p1\u00b4\u03b2q\u03c0 becomes \u03c01prx, asq \u201c \u03b2a`p1\u00b4\u03b2qka. Thus the sequential roll-out of the new policy \u03c01 will also yield an exponential gap from the correct policy. By induction, the same will be true in all future iterations."}, {"heading": "5.3. Policy Improvement", "text": "Our policy improvement guarantee builds upon the analysis from SEARN (Daume\u0301 III et al., 2009), which we extend to using adaptive learning rates \u03b2. We first restate the main policy improvement result from Daume\u0301 III et al. (2009).\nLemma 5.4 (SEARN\u2019s policy nondegradation - Lemma 1 from Daume\u0301 III et al. (2009)). Let `max \u201c sup\u03c0,s `p\u03c0psqq, \u03c01 is defined as \u03c0sto in lemma 5.2. Then for \u03b2 P p0, 1{T q:\n`\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f \u03b2TEs\u201ed\u03c0 r`p\u03c0\u0302psqqs ` 1\n2 \u03b22T 2`max.\nSEARN guarantees that the new policy \u03c01 does not degrade from the expert \u03c0\u02da by much only if \u03b2 \u0103 1{T . Analyses of SEARN and other previous iterative reduction methods (Ross et al., 2011; Kakade & Langford, 2002; Bagnell et al., 2003; Syed & Schapire, 2010) rely on bounding the variation distance between d\u03c0 and d\u03c01 . Three drawbacks of this approach are: (i) non-trivial variation distance bound typically requires \u03b2 to be inversely proportional to time horizon T , causing slow convergence; (ii) not easily applicable to the continuous regime; and (iii) except under MDP framework with discounted infinite horizon, previous variation distance bounds do not guarantee monotonic policy improvements (i.e. `\u03c01p\u03c01q \u0103 `\u03c0p\u03c0q).\nWe provide two levels of guarantees taking advantage of Stability Conditions 1 and 2 to circumvent these drawbacks. Assuming the Condition 1 and convexity of `, our first result yields a guarantee comparable with SEARN.\nTheorem 5.5 (T-dependent Improvement). Assume ` is convex and L-Lipschitz, and Condition 1 holds. Let \u201c max s\u201ed\u03c0 }\u03c0\u0302psq \u00b4 \u03c0psq}. Then:\n`\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f \u03b2 LT ` \u03b2 p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq . (3)\nIn particular, choosing \u03b2 P p0, 1{T q yields: `\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f L` \u03b2 p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq . (4)\nSimilar to SEARN, Theorem 5.5 also requires \u03b2 P p0, 1{T q to ensure the RHS of (4) stays small. However, note that the reduction term \u03b2p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq allows the bound to be strictly negative if the policy \u03c0\u0302 trained on d\u03c0 significantly improves on `\u03c0p\u03c0q (i.e., guaranteed policy improvement). We observe empirically that this often happens, especially in early iterations of training.\nUnder the mildly stronger Condition 2, we remove the dependency on the time horizon T , which represents a much stronger guarantee compared to previous work. Theorem 5.6 (Policy Improvement). Assume ` is convex and L-Lipschitz-continuous, and Condition 2 holds. Let \u201c max\ns\u201ed\u03c0 }\u03c0\u0302psq \u00b4 \u03c0psq}. Then for \u03b2 P p0, 1q:\n`\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f \u03b2\u03b3 L\np1\u00b4 \u03b2qp1\u00b4 \u03b3q ` \u03b2p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq. (5)\nCorollary 5.7 (Monotonic Improvement). Following the notation from Theorem 5.6, let \u2206 \u201c `\u03c0p\u03c0q \u00b4 `\u03c0p\u03c0\u0302q and \u03b4 \u201c \u03b3 L1\u00b4\u03b3 . Then choosing step size \u03b2 \u201c \u2206\u00b4\u03b4 2\u2206 , we have:\n`\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f \u00b4 p\u2206\u00b4 \u03b4q2\n2p\u2206` \u03b4q . (6)\nThe terms and `\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0q on the RHS of (4) and (5) come from the learning reduction, as they measure the \u201cdistance\u201d between \u03c0\u0302 and \u03c0 on the state distribution induced by \u03c0 (which forms the dataset to train \u03c0\u0302). In practice, both terms can be empirically estimated from the training round, thus allowing an estimate of \u03b2 to minimize the bound.\nTheorem 5.6 justifies using an adaptive and more aggressive interpolation parameter \u03b2 to update policies. In the worst case, setting \u03b2 close to 0 will ensure the bound from (5) to be close to 0, which is consistent with SEARN\u2019s result. More generally, monotonic policy improvement can be guaranteed for appropriate choice of \u03b2, as seen from Corollary 5.7. This strict policy improvement was not possible under previous iterative learning reduction approaches such as SEARN and DAgger, and is enabled in our setting due to exploiting the smoothness conditions."}, {"heading": "5.4. Smooth Feedback Analysis", "text": "Smooth Feedback Does Not Hurt: Recall from Section 4 that one way to simulate \u201cvirtual\u201d feedback for training a new \u03c0\u0302 is to set the target a\u0302t \u201c \u03c3at ` p1 \u00b4 \u03c3qa\u02dat for \u03c3 P p0, 1q, where smooth feedback corresponds to \u03c3 \u00d1 1. To see that simulating smooth \u201cvirtual\u201d feedback target does not hurt the training progress, we alternatively view SIMILE as performing gradient descent in a smooth function space (Mason et al., 1999). Define the cost functional C : \u03a0 \u00d1 R over policy space to be the average imitation loss over S as Cp\u03c0q \u201c \u015f\nS }\u03c0psq \u00b4 \u03c0\u02dapsq}2 dP psq. The gra-\ndient (Ga\u0302teaux derivative) of Cp\u03c0q w.r.t. \u03c0 is:\n\u2207Cp\u03c0qpsq \u201c BCp\u03c0 ` \u03b1\u03b4sqB\u03b1 \u02c7 \u02c7 \u02c7 \u03b1\u201c0 \u201c 2p\u03c0psq \u00b4 \u03c0\u02dapsqq,\nwhere \u03b4s is Dirac delta function centered at s. By first order approximationCp\u03c01q \u201c Cp\u03b2\u03c0\u0302`p1\u00b4\u03b2q\u03c0q \u201c Cp\u03c0`\u03b2p\u03c0\u0302\u00b4 \u03c0qq \u00ab Cp\u03c0q` \u03b2x\u2207Cp\u03c0q, \u03c0\u0302\u00b4 \u03c0y. Like traditional gradient descent, we want to choose \u03c0\u0302 such that the update moves the functional along the direction of negative gradient. In other words, we want to learn \u03c0\u0302 P \u03a0 such that x\u2207Cp\u03c0q, \u03c0\u0302\u00b4 \u03c0y ! 0. We can evaluate this inner product along the states induced by \u03c0. We thus have the estimate:\nx\u2207Cp\u03c0q, \u03c0\u0302 \u00b4 \u03c0y \u00ab 2 T\nT \u00ff t\u201c1 p\u03c0pstq \u00b4 \u03c0\u02dapstqqp\u03c0\u0302pstq \u00b4 \u03c0pstqq\n\u201c 2 T\nT \u00ff t\u201c1 pat \u00b4 a\u02dat qp\u03c0\u0302prxt, at\u00b41sq \u00b4 atq.\nSince we want x\u2207Cp\u03c0q, \u03c0\u0302 \u00b4 \u03c0y \u0103 0, this motivates the construction of new data set D with states trxt, at\u00b41suTt\u201c1 and labels tpatuTt\u201c1 to train a new policy \u03c0\u0302, where we want pat\u00b4a\u02dat qppat\u00b4atq \u0103 0. A sufficient solution is to set target pat \u201c \u03c3at ` p1 \u00b4 \u03c3qa\u02dat (Section 4), as this will point the gradient in negative direction, allowing the learner to make progress.\nSmooth Feedback is Sometimes Necessary: When the current policy performs poorly, smooth virtual feedback may be required to ensure stable learning, i.e. producing a feasible smooth policy at each training round. We formalize this notion of feasibility by considering the smooth policy class \u03a0\u03bb in Example 2.1. Recall that smooth regularization of \u03a0\u03bb via H encourages the next action to be close to the previous action. Thus a natural way to measure smoothness of \u03c0 P \u03a0\u03bb is via the average first order difference of consecutive actions 1T \u0159T t\u201c1 }at \u00b4 at\u00b41}. In particular, we want to explicitly constrain this difference relative to the expert trajectory 1T \u0159T t\u201c1 }at \u00b4 at\u00b41} \u010f \u03b7 at each iteration, where \u03b79 1T \u0159T t\u201c1 \u203a \u203aa\u02dat \u00b4 a\u02dat\u00b41 \u203a \u203a.\nWhen \u03c0 performs poorly, i.e. the \u201daverage gap\u201d between current trajectory tatu and ta\u02dat u is large, the training target for \u03c0\u0302 should be lowered to ensure learning a smooth policy is feasible, as inferred from the following proposition. In practice, we typically employ smooth virtual feedback in early iterations when policies tend to perform worse.\nProposition 5.8. Let \u03c9 be the average supervised training error from F , i.e. \u03c9 \u201c min\nfPF Ex\u201eX r}fprx, 0sq \u00b4 a\u02da}s.\nLet the rolled-out trajectory of current policy \u03c0 be tatu. If the average gap between \u03c0 and \u03c0\u02da is such that Et\u201eUniformr1:T s r}a\u02dat \u00b4 at\u00b41}s \u011b 3\u03c9 ` \u03b7p1 ` \u03bbq, then using ta\u02dat u as feedback will cause the trained policy \u03c0\u0302 to be non-smooth, i.e.:\nEt\u201eUniformr1:T s r}a\u0302t \u00b4 a\u0302t\u00b41}s \u011b \u03b7, (7) for ta\u0302tu the rolled-out trajectory of \u03c0\u0302."}, {"heading": "6. Experiments", "text": "Automated Camera Planning. We evaluate SIMILE in a case study of automated camera planning for sport broadcasting (Chen & Carr, 2015; Chen et al., 2016). Given noisy tracking of players as raw input data txtuTt\u201c1, and demonstrated pan camera angles from professional human operator as ta\u02dat uTt\u201c1, the goal is to learn a policy \u03c0 that produces trajectory tatuTt\u201c1 that is both smooth and accurate relative to ta\u02dat uTt\u201c1. Smoothness is critical in camera control: fluid movements which maintain adequate framing are preferable to jittery motions which constantly pursue perfect tracking (Gaddam et al., 2015). In this setting, time horizon T is the duration of the event multiplied by rate of sampling. Thus T tends to be very large. Smooth Policy Class. We use a smooth policy class following Example 2.2: regression tree ensembles F regularized by a class of linear autoregressor functions H (Chen et al., 2016). See Appendix B for more details. Summary of Results. \u2022 Using our smooth policy class leads to dramatically\nsmoother trajectories than not regularizing usingH. \u2022 Using our adaptive learning rate leads to much faster\nconvergence compared to conservative learning rates from SEARN (Daume\u0301 III et al., 2009). \u2022 Using smooth feedback ensures stable learning of smooth policies at each iteration. \u2022 Deterministic policy interpolation performs better than stochastic interpolation used in SEARN.\nSmooth versus Non-Smooth Policy Classes. Figure 2 shows a comparison of using a smooth policy class versus a non-smooth one (e.g., not using H). We see that our\napproach can reliably learn to predict trajectories that are both smooth and accurate. Adaptive vs. Fixed \u03b2: One can, in principle, train using SEARN, which requires a very conservative \u03b2 in order to guarantee convergence. In contrast, SIMILE adaptively selects \u03b2 based on relative empirical loss of \u03c0 and \u03c0\u0302 (Line 9 of Algorithm 1). Let errorp\u03c0\u0302q and errorp\u03c0q denote the mean-squared errors of rolled-out trajectories ta\u0302tu, tatu, respectively, w.r.t. ground truth ta\u02dat u. We can set \u03b2 as:\n\u03b2\u0302 \u201c errorp\u03c0q errorp\u03c0\u0302q ` errorp\u03c0q , (8)\nwhich encourages the learner to disregard bad policies when interpolating, thus allowing fast convergence to a good policy (see Theorem 5.6). Figure 3 compares the convergence rate of SIMILE using adaptive \u03b2 versus conservative fixed values of \u03b2 commonly used in SEARN (Daume\u0301 III et al., 2009). We see that adaptively choosing \u03b2 enjoys substantially faster convergence. Note that very large fixed \u03b2 may overshoot and worsen the combined policy after a few initial improvements. Smooth Feedback Generation: We set the target labels to a\u0302nt \u201c \u03c3ant ` p1 \u00b4 \u03c3qa\u02dat for 0 \u0103 \u03c3 \u0103 1 (Line 6 of Algorithm 1). Larger \u03c3 corresponds to smoother (a\u0302nt is closer to ant\u00b41) but less accurate target (further from a \u02da t ), as seen in Figure 4. Figure 5 shows the trade-off between\nsmoothness loss (blue line, measured by first order difference in Proposition 5.8) and imitation loss (red line, measured by mean squared distance) for varying \u03c3. We navigate this trade-off by setting\n\u03c3 closer to 1 in early iterations, and have \u03c3 \u00d1 0 as n increases. This \u201cgradual increase\u201d produces more stable policies, especially during early iterations where the learning policy tends to perform poorly (as formalized in Proposition 5.8). In Figure 4, when the initial policy (green trajectory) has poor performance, setting smooth\ntargets (Figure 4b) allows learning a smooth policy in the subsequent round, in contrast to more accurate but less stable performance of \u201cdifficult\u201d targets with low \u03c3 (Figure 4c-d). Figure 6 visualizes the behavior of the the intermediate policies learned by SIMILE, where we can see that each intermediate policy is a smooth policy. Deterministic vs. Stochastic Interpolation: Finally, we evaluate the benefits of using deterministic policy averaging intead of stochastically combine different policies, as done in SEARN. To control for other factors, we set \u03b2 to a fixed value of 0.5, and keep the new training dataset Dn the same for each iteration n. The average imitation loss of stochastic policy sampling are evaluated after 50 stochastic roll-outs at each iterations. This average stochastic policy error tends to be higher compared to the empirical error of the deterministic trajectory, as seen from Figure 7, and confirms our finding from Corollary 5.3."}, {"heading": "7. Conclusion", "text": "We formalized the problem of smooth imitation learning for online sequence prediction, which is a variant of imitation learning that uses a notion of a smooth policy class. We proposed SIMILE (Smooth IMItation LEarning), which is an iterative learning reduction approach to learning smooth policies from expert demonstrations in a continuous and dynamic environment. SIMILE utilizes an adaptive learning rate that provably allows much faster convergence compared to previous learning reduction approaches, and also enjoys better sample complexity than previous work by being fully deterministic and allowing for virtual simulation of training labels. We validated the efficiency and practicality of our approach on a setting of online camera planning."}, {"heading": "A. Detailed Theoretical Analysis and Proofs", "text": "A.1. Proof of lemma 5.1\nLemma Statement. (Lemma 5.1) For a fixed x, define \u03c0prx, asq fi \u03d5paq. If \u03d5 is non-negative and H-smooth w.r.t. a., then:\n@a, a1 : ` \u03d5paq \u00b4 \u03d5pa1q \u02d82 \u010f 6H ` \u03d5paq ` \u03d5pa1q \u02d8 \u203a \u203aa\u00b4 a1 \u203a \u203a 2 .\nThe proof of Lemma 5.1 rests on 2 properties of H-smooth functions (differentiable) in R1, as stated below\nLemma A.1 (Self-bounding property of Lipschitz-smooth functions). Let \u03c6 : R \u00d1 R be an H-smooth non-negative function. Then for all a P R: |\u2207\u03c6paq| \u010f a 4H\u03c6paq\nProof. By mean value theorem, for any a, a1 we have D \u03b7 P pa, a1q (or pa1, aq) such that \u03c6pa1q \u201c \u03c6paq`\u2207\u03c6p\u03b7qpa1\u00b4aq. Since \u03c6 is non-negative,\n0 \u010f \u03c6pa1q \u201c \u03c6paq `\u2207\u03c6paqpa1 \u00b4 aq ` p\u2207\u03c6p\u03b7q \u00b4\u2207\u03c6paqqpa1 \u00b4 aq\n\u010f \u03c6paq `\u2207\u03c6paqpa1 \u00b4 aq `H|\u03b7 \u00b4 a||a1 \u00b4 a| \u010f \u03c6paq `\u2207\u03c6paqpa1 \u00b4 aq `H|a1 \u00b4 a|2\nChoosing a1 \u201c a\u00b4 \u2207\u03c6paq2H proves the lemma.\nLemma A.2 (1-d Case (Srebro et al., 2010)). Let \u03c6 : R\u00d1 R be an H-smooth non-negative function. Then for all a, a1 P R:\n` \u03c6paq \u00b4 \u03c6pa1q \u02d82 \u010f 6H ` \u03c6paq ` \u03c6pa1q \u02d8 ` a\u00b4 a1 \u02d82\nProof. As before, D\u03b7 P pa, a1q such that \u03c6pa1q \u00b4 \u03c6paq \u201c \u2207\u03c6p\u03b7qpa1 \u00b4 aq. By assumption of \u03c6, we have |\u2207\u03c6p\u03b7q \u00b4 \u2207\u03c6paq| \u010f H|\u03b7 \u00b4 a| \u010f H|a1 \u00b4 a|. Thus we have:\n|\u2207\u03c6p\u03b7q| \u010f |\u2207\u03c6paq|`H|a\u00b4 a1| (9) Consider two cases:\nCase 1: If |a \u00b4 a1| \u010f |\u2207\u03c6paq|5H , then by equation 9 we have |\u2207\u03c6p\u03b7q| \u010f 6{5|\u2207\u03c6paq|. Thus\n` \u03c6paq \u00b4 \u03c6pa1q \u02d82 \u201c p\u2207\u03c6p\u03b7qq2 ` a\u00b4 a1 \u02d82\n\u010f 36 25 p\u2207\u03c6paqq2 ` a\u00b4 a1 \u02d82 \u010f 144 25 H\u03c6paq ` a\u00b4 a1 \u02d82\nby lemma A.1. Therefore, p\u03c6paq \u00b4 \u03c6pa1qq2 \u010f 6H\u03c6paq pa\u00b4 a1q2 \u010f 6H p\u03c6paq ` \u03c6pa1qq pa\u00b4 a1q2\nCase 2: If |a \u00b4 a1| \u0105 |\u2207\u03c6paq|5H , then equation 9 gives |\u2207\u03c6p\u03b7q| \u010f 6H|a\u00b4 a1|. Once again `\n\u03c6paq \u00b4 \u03c6pa1q \u02d82 \u201c ` \u03c6paq \u00b4 \u03c6pa1q \u02d8 \u2207\u03c6p\u03b7q ` a\u00b4 a1 \u02d8\n\u010f | ` \u03c6paq \u00b4 \u03c6pa1q \u02d8 ||\u2207\u03c6p\u03b7q|| ` a\u00b4 a1 \u02d8 | \u010f | ` \u03c6paq \u00b4 \u03c6pa1q \u02d8 | \u00b4 6H ` a\u00b4 a1 \u02d82 \u00af\n\u010f 6H ` \u03c6paq ` \u03c6pa1q \u02d8 ` a\u00b4 a1 \u02d82\nProof of Lemma 5.1. The extension to the multidimensional case is straightforward. For any a, a1 P Rk, consider the function \u03c6 : R \u00d1 R such that \u03c6ptq \u201c \u03d5pp1 \u00b4 tqa ` ta1q, then \u03c6 is a differentiable, non-negative function and \u2207tp\u03c6ptqq \u201c x\u2207\u03d5pa ` tpa1 \u00b4 aqq, a1 \u00b4 ay. Thus:\n|\u03c61pt1q \u00b4 \u03c61pt2q| \u201c |x\u2207\u03d5pa` t1pa1 \u00b4 aqq\u00b4 \u2207\u03d5pa` t2pa1 \u00b4 aqq, a1 \u00b4 ay|\n\u010f \u203a \u203a\u2207\u03d5pa` t1pa1 \u00b4 aqq \u00b4\u2207\u03d5pa` t2pa1 \u00b4 aqq \u203a \u203a\n\u02da\n\u203a \u203aa1 \u00b4 a \u203a \u203a\n\u010f H|t1 \u00b4 t2| \u203a \u203aa\u00b4 a1 \u203a \u203a 2\nTherefore \u03c6 is an H }a\u00b4 a1}2-smooth function in R. Apply lemma A.2 to \u03c6, we have:\np\u03c6p1q \u00b4 \u03c6p0qq2 \u010f 6H \u203a \u203aa\u00b4 a1 \u203a \u203a 2 p\u03c6p1q ` \u03c6p0qq p1\u00b4 0q2\nwhich is the same as p\u03d5paq \u00b4 \u03d5pa1qq2 \u010f 6Hp\u03d5paq ` \u03d5pa1qq }a\u00b4 a1}2\nA.2. Proof of lemma 5.2\nLemma Statement. (Lemma 5.2) Given any starting state s0, sequentially execute \u03c0det and \u03c0sto to obtain two separate trajectories A \u201c tatuTt\u201c1 and A\u0303 \u201c ta\u0303tuTt\u201c1 such that at \u201c \u03c0detpstq and a\u0303t \u201c \u03c0stops\u0303tq, where st \u201c rxt, at\u00b41s and s\u0303t \u201c rxt, a\u0303t\u00b41s. Assuming the policies are stable as per Condition 1, we have EA\u0303ra\u0303ts \u201c at @t \u201c 1, . . . , T , where the expectation is taken over all random roll-outs of \u03c0sto.\nProof. Given a starting state s0, we prove by induction that EA\u0303ra\u0303ts \u201c at.\nIt is easily seen that the claim is true for t \u201c 1.\nNow assuming that EA\u0303ra\u0303t\u00b41s \u201c at\u00b41. We have EA\u0303ra\u0303ts \u201c EA\u0303rEra\u0303t|s\u0303tss\n\u201c EA\u0303r\u03b2\u03c0\u0302ps\u0303tq ` p1\u00b4 \u03b2q\u03c0ps\u0303tqs \u201c \u03b2EA\u0303r\u03c0\u0302ps\u0303tqs ` p1\u00b4 \u03b2qEA\u0303r\u03c0ps\u0303tqs\nThus: \u203a\n\u203aEA\u0303ra\u0303ts \u00b4 at \u203a \u203a \u201c \u203a \u203aEA\u0303ra\u0303ts \u00b4 \u03b2\u03c0\u0302pstq \u00b4 p1\u00b4 \u03b2q\u03c0pstq \u203a \u203a\n\u201c }\u03b2EA\u0303r\u03c0\u0302ps\u0303tqs ` p1\u00b4 \u03b2qEA\u0303r\u03c0ps\u0303tqs \u00b4 \u03b2\u03c0\u0302pstq \u00b4 p1\u00b4 \u03b2q\u03c0pstq} \u010f \u03b2 \u203a\n\u203aEA\u0303r\u03c0\u0302ps\u0303tqs \u00b4 \u03c0\u0302pstq \u203a \u203a ` p1\u00b4 \u03b2q \u203a\n\u203aEA\u0303r\u03c0ps\u0303tqs \u00b4 \u03c0pstq \u203a \u203a\n\u010f \u03b2 \u203a \u203aEA\u0303ra\u0303t\u00b41s \u00b4 at\u00b41 \u203a \u203a\n` p1\u00b4 \u03b2q \u203a \u203aEA\u0303ra\u0303t\u00b41s \u00b4 at\u00b41 \u203a \u203a\n\u201c 0 per inductive hypothesis. Therefore we conclude that EA\u0303ra\u0303ts \u201c at @t \u201c 1, . . . , T\nA.3. Proof of theorem 5.6 and corollary 5.7 - Main policy improvement results\nIn this section, we provide the proof to theorem 5.6 and corollary 5.7.\nTheorem Statement. (theorem 5.6) Assume ` is convex and L-Lipschitz-continuous, and Condition 2 holds. Let \u201c max s\u201ed\u03c0 }\u03c0\u0302psq \u00b4 \u03c0psq}. Then for \u03b2 P p0, 1q:\n`\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f \u03b2\u03b3 L\np1\u00b4 \u03b2qp1\u00b4 \u03b3q ` \u03b2p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq.\nProof. First let\u2019s review the notations: let T be the trajectory horizon. For a policy \u03c0 in the deterministic policy class \u03a0, given a starting state s0, we roll out the full trajectory s0 \u03c0\u00dd\u00d1 s1 \u03c0\u00dd\u00d1 . . . \u03c0\u00dd\u00d1 sT , where st \u201c rxt, \u03c0pst\u00b41qs, with xt encodes the featurized input at current time t, and \u03c0pst\u00b41q encodes the dependency on previous predictions. Let `p\u03c0psqq be the loss of taking action \u03c0psq at state s, we can define the trajectory loss of policy \u03c0 from starting state s0 as\n`p\u03c0|s0q \u201c 1\nT\nT \u00ff t\u201c1 `p\u03c0pstqq\nFor a starting state distribution \u00b5, we define policy loss of \u03c0 as the expected loss along trajectories induced by \u03c0: `\u03c0p\u03c0q \u201c Es0\u201e\u00b5r`p\u03c0|s0qs. Policy loss `\u03c0p\u03c0q can be understood as\n`\u03c0p\u03c0q \u201c \u017c\ns0\u201e\u00b5\nE xt\u201eX\n1\nT\n\u00ab\nT \u00ff t\u201c1 `p\u03c0pstqq\nff\nd\u00b5ps0q\nTo prove policy improvement, we skip the subscript of algorithm 1 to consider general policy update rule within\neach iteration:\n\u03c01 \u201c \u03c0new \u201c \u03b2\u03c0\u0302 ` p1\u00b4 \u03b2q\u03c0 (10) where \u03c0 \u201c \u03c0old is the current policy (combined up until the previous iteration), \u03c0\u0302 is the trained model from calling the base regression routine TrainpS, pA|hq. Learning rate (step-size) \u03b2 may be adaptively chosen in each iteration. Recall that this update rule reflects deterministic interpolation of two policies.\nWe are interested in quantifying the policy improvement when updating \u03c0 to \u03c01. Specifically, we want to bound\n\u0393 \u201c `\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q where `\u03c0p\u03c0q (respectively `\u03c01p\u03c01q) denotes the trajectory loss of \u03c0 (respectively \u03c01) on the state distribution induced by \u03c0 (resp. \u03c01)\nWe will bound the loss difference of old and new policies conditioned on a common starting state s0. Based on update rule (10), consider rolling out \u03c01 and \u03c0 from the same starting state s0 to obtain two separate sequences \u03c01 \u00de\u00dd\u00d1 ts0 \u00d1 s11 . . . \u00d1 s1T u and \u03c0 \u00de\u00dd\u00d1 ts0 \u00d1 s1 . . . \u00d1 sT u corresponding to the same stream of inputs x1, . . . , xT .\n\u0393ps0q \u201c 1\nT\nT \u00ff t\u201c1 `p\u03c01ps1tqq \u00b4 `p\u03c0pstqq\n\u201c 1 T\nT \u00ff t\u201c1 `p\u03c01ps1tqq \u00b4 `p\u03c01pstqq ` `p\u03c01pstqq \u00b4 `p\u03c0pstqq\n(11)\nAssume convexity of ` (e.g. sum of square losses):\n`p\u03c01pstqq \u201c `p\u03b2\u03c0\u0302pstq ` p1\u00b4 \u03b2q\u03c0pstqq \u010f \u03b2`p\u03c0\u0302pstqq ` p1\u00b4 \u03b2q`p\u03c0pstqq\nThus we can begin to bound individual components of \u0393ps0q as\n`p\u03c01ps1tqq \u00b4 `p\u03c0pstqq \u010f `p\u03c01ps1tqqq \u00b4 `p\u03c01pstqq ` \u03b2 r`p\u03c0\u0302pstqq \u00b4 `p\u03c0pstqqs\nSince ` is L-Lipschitz continuous, we have\n`p\u03c01ps1tqq \u00b4 `p\u03c01pstqq \u010f L \u203a \u203a\u03c01ps1tq \u00b4 \u03c01pstq \u203a \u203a\n\u010f L\u03b3 \u203a \u203as1t \u00b4 st \u203a \u203a (12)\nwhere (12) is due to the smoothness condition [2] of policy class \u03a0. Given a policy class \u03a0 with \u03b3 \u0103 1, the following claim can be proved by induction: Claim: }s1t \u00b4 st} \u010f \u03b2 p1\u00b4\u03b2qp1\u00b4\u03b3q\nProof. For the base case, given the same starting state s0, we have s11 \u201c rx1, \u03c01ps0qs and s1 \u201c rx1, \u03c0ps0qs. Thus }s11 \u00b4 s1} \u201c }\u03c01ps0q \u00b4 \u03c0ps0q} \u201c }\u03b2\u03c0\u0302ps0q ` p1\u00b4 \u03b2q\u03c0ps0q \u00b4 \u03c0ps0q} \u201c \u03b2 }\u03c0\u0302ps0q \u00b4 \u03c0ps0q} \u010f \u03b2 \u010f \u03b2 p1\u00b4\u03b2qp1\u00b4\u03b3q .\nIn the inductive case, assume we have \u203a \u203as1t\u00b41 \u00b4 st\u00b41 \u203a \u203a \u010f\n\u03b2 p1\u00b4\u03b2qp1\u00b4\u03b3q . Then similar to before, the definition of s 1 t and st leads to \u203a \u203as1t \u00b4 st \u203a \u203a \u201c \u203a \u203a \u201c xt, \u03c0 1ps1t\u00b41q \u2030 \u00b4 rxt, \u03c0pst\u00b41qs \u203a \u203a\n\u201c \u203a \u203a\u03c01ps1t\u00b41q \u00b4 \u03c0pst\u00b41q \u203a \u203a \u010f \u203a\n\u203a\u03c01ps1t\u00b41q \u00b4 \u03c01pst\u00b41q \u203a \u203a` \u203a \u203a\u03c01pst\u00b41q \u00b4 \u03c0pst\u00b41q \u203a \u203a\n\u010f \u03b3 \u203a \u203as1t\u00b41 \u00b4 st\u00b41 \u203a \u203a` \u03b2 }\u03c0\u0302pst\u00b41q \u00b4 \u03c0pst\u00b41q}\n\u010f \u03b3 \u03b2 p1\u00b4 \u03b2qp1\u00b4 \u03b3q ` \u03b2\n\u010f \u03b2 p1\u00b4 \u03b2qp1\u00b4 \u03b3q\nApplying the claim to equation (12), we have\n`p\u03c01ps1tqq \u00b4 `p\u03c01pstqq \u010f \u03b2\u03b3 L\np1\u00b4 \u03b2qp1\u00b4 \u03b3q which leads to\n`p\u03c01ps1tq \u00b4 `p\u03c0pstqqq \u010f \u03b2\u03b3 L\np1\u00b4 \u03b2qp1\u00b4 \u03b3q ` \u03b2p`p\u03c0\u0302pstqq \u00b4 `p\u03c0pstqqq (13)\nIntegrating (13) over the starting state s0 \u201e \u00b5 and input trajectories txtuTt\u201c1, we arrive at the policy improvement bound:\n`\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f \u03b2\u03b3 L\np1\u00b4 \u03b2qp1\u00b4 \u03b3q ` \u03b2p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq\nwhere `\u03c0p\u03c0\u0302q is the expected loss of the trained policy \u03c0\u0302 on the state distribution induced by policy \u03c0 (reduction term, analogous to policy advantage in the traditional MDP terminologies (Kakade & Langford, 2002))\nThis means in the worst case, as we choose \u03b2 \u00d1 0, we have r`\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0qs \u00d1 0, meaning the new policy does not degrade much for a small choice of \u03b2. However if `\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0q ! 0, we can choose \u03b2 to enforce monotonic improvement of the policy by adaptively choosing \u03b2 that minimizes the right-hand side. In particular, let the reduction term be \u2206 \u201c `\u03c0p\u03c0q \u00b4 `\u03c0p\u03c0\u0302q \u0105 0 and let \u03b4 \u201c \u03b3 L1\u00b4\u03b3 , then for \u03b2 \u201c \u2206\u00b4\u03b42\u2206 we have the following monotonic policy improvement:\n`\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f \u00b4 p\u2206\u00b4 \u03b4q2\n2p\u2206` \u03b4q\nA.4. Proof of theorem 5.5 - T -dependent improvement\nTheorem Statement. (theorem 5.5) Assume ` is convex and L-Lipschitz, and Condition 1 holds. Let \u201c max s\u201ed\u03c0 }\u03c0\u0302psq \u00b4 \u03c0psq}. Then:\n`\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f \u03b2 LT ` \u03b2 p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq .\nIn particular, choosing \u03b2 P p0, 1{T q yields: `\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f L` \u03b2 p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq .\nProof. The proof of theorem 5.5 largely follows the structute of theorem 5.6, except that we are using the slighty weaker Condition 1 which leads to weaker bound on the policy improvement that depends on the trajectory horizon T . For any state s0 taken from the starting state distribution \u00b5, sequentially roll-out policies \u03c01 and \u03c0 to receive two separate trajectories \u03c01 : s0 \u00d1 s11 \u00d1 . . . \u00d1 s1T and \u03c01 : s0 \u00d1 s1 \u00d1 . . . \u00d1 sT . Consider a pair of states s1t \u201c rxt, \u03c01ps1t\u00b41qs and st \u201c rxt, \u03c0pst\u00b41qs corresponding to the same input feature xt, as before we can decompose `p\u03c01ps1tqq\u00b4 `p\u03c0pstqq \u201c `p\u03c01ps1tqq\u00b4 `p\u03c01pstqq` `p\u03c01pstqq\u00b4 `p\u03c0pstqq \u010f L }\u03c01ps1tq \u00b4 \u03c01pstq} ` \u03b2p`p\u03c0\u0302pstqq \u00b4 `p\u03c0pstqqq due to convexity and L-Lipschitz continuity of `.\nCondition 1 further yields: `p\u03c01ps1tqq \u00b4 `p\u03c0pstqq \u010f L }s1t \u00b4 st} ` \u03b2p`p\u03c0\u0302pstqq \u00b4 `p\u03c0pstqqq. By the construction of the states, note that \u203a \u203as1t \u00b4 st \u203a \u203a \u201c \u203a \u203a\u03c01ps1t\u00b41q \u00b4 \u03c0pst\u00b41q \u203a \u203a\n\u010f \u203a \u203a\u03c01ps1t\u00b41q \u00b4 \u03c01pst\u00b41q \u203a \u203a` \u203a \u203a\u03c01pst\u00b41q \u00b4 \u03c0pst\u00b41q \u203a \u203a \u010f \u203a\n\u203as1t\u00b41 \u00b4 st\u00b41 \u203a \u203a` \u03b2p}\u03c0\u0302pst\u00b41q \u00b4 \u03c0pst\u00b41q}q \u010f \u203a\n\u203as1t\u00b41 \u00b4 st\u00b41 \u203a \u203a` \u03b2 (by condition 1 and definition of ).\nFrom here, one can use this recursive relation to easily show that }s1t \u00b4 st} \u010f \u03b2 t for all t P r1, T s.\nAveraging over the T time steps and integrating over the starting state distribution, we have:\n`\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f \u03b2 LpT ` 1q{2` \u03b2p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq \u010f \u03b2 LT ` \u03b2p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq\nIn particular, \u03b2 P p0, 1{T q yields `\u03c01p\u03c01q \u00b4 `\u03c0p\u03c0q \u010f L ` \u03b2p`\u03c0p\u03c0\u0302q \u00b4 `\u03c0p\u03c0qq.\nA.5. Proof of proposition 5.8 - smooth expert proposition\nProposition Statement. (Proposition 5.8) Let \u03c9 be the average supervised training error from F , i.e. \u03c9 \u201c min fPF Ex\u201eX r}fprx, 0sq \u00b4 a\u02da}s. Let the rolled-out trajectory of current policy \u03c0 be tatu. If the average gap between \u03c0 and \u03c0\u02da is such that Et\u201eUniformr1:T s r}a\u02dat \u00b4 at\u00b41}s \u011b 3\u03c9 ` \u03b7p1`\u03bbq, then using ta\u02dat u as feedback will cause the trained policy \u03c0\u0302 to be non-smooth, i.e.:\nEt\u201eUniformr1:T s r}a\u0302t \u00b4 a\u0302t\u00b41}s \u011b \u03b7, for ta\u0302tu the rolled-out trajectory of \u03c0\u0302.\nProof. Recall that \u03a0\u03bb is formed by regularizing a class of supervised learners F with the singleton class of smooth function H fi thpaq \u201c au, via a hyper-parameter \u03bb\nthat controls the trade-off between being close to the two classes.\nMinimizing over \u03a0\u03bb can be seen as a regularized optimization problem:\n\u03c0\u0302px, aq \u201c argmin \u03c0P\u03a0 `p\u03c0prx, asqq\n\u201c argmin fPF,hPH\npfpx, aq \u00b4 a\u02daq2 ` \u03bbpfpx, aq \u00b4 hpaqq2\n\u201c argmin fPF\npfpx, aq \u00b4 a\u02daq2 ` \u03bbpfpx, aq \u00b4 aq2\n(14)\nwhere hyper-parameter \u03bb trades-off the distance of fpx, aq relative to a (smoothness) and a\u02da (imitation accuracy), and a P R1.\nSuch a policy \u03c0, at execution time, corresponds to the regularized minimizer of:\nat \u201c \u03c0prx, at\u00b41sq \u201c argmin\na }a\u00b4 fprxt, at\u00b41sq}2 ` \u03bb }a\u00b4 at\u00b41}2\n\u201c fprxt, at\u00b41sq ` \u03bbat\u00b41 1` \u03bb (15)\nwhere f P F is the minimizer of equation 14 Thus we enforce smoothness of learning policy from \u03a0\u03bb by encouraging low first order difference of consecutive actions of the executed trajectory tatu. In practice, we may contrain this first order difference relative to the human trajectory 1T \u0159T t\u201c1 }at \u00b4 at\u00b41} \u010f \u03b7, where \u03b79 1T \u0159T t\u201c1 \u203a \u203aa\u02dat \u00b4 a\u02dat\u00b41 \u203a \u203a.\nConsider any given iteration with the following set-up: we execute old policy \u03c0 \u201c \u03c0old to get rolled-out trajectory tatuTt\u201c1. Form the new data set as D \u201c tpst, a\u02dat quTt\u201c1 with predictors st \u201c rxt, at\u00b41s and feedback labels simply the human actions a\u02dat . Use this data set to train a policy \u03c0\u0302 by learning a supervised f\u0302 P F from D. Similar to \u03c0, the execution of \u03c0\u0302 corresponds to a\u0302t where:\na\u0302t \u201c \u03c0\u0302prxt, a\u0302t\u00b41sq\n\u201c argmin a\n\u203a \u203a \u203a a\u00b4 f\u0302prxt, a\u0302t\u00b41sq \u203a \u203a \u203a\n2\n` \u03bb }a\u00b4 a\u0302t\u00b41}2\n\u201c f\u0302prxt, a\u0302t\u00b41sq ` \u03bba\u0302t\u00b41 1` \u03bb (16)\nDenote by f0 the \u201dnaive\u201d supervised learner from F . In other words, f0 \u201c argmin\nfPF\nT \u0159\nt\u201c1 }fprxt, 0sq \u00b4 a\u02dat } 2. Let \u03c9 be\nthe average gap between human trajectory and the rolledout trajectory of f0, i.e.\n\u03c9 \u201c 1 T\nT \u00ff t\u201c1 }f0prxt, 0sq \u00b4 a\u02dat }\nNote that it is reasonable to assume that the average errors\nof f and f\u0302 are no worse than f0, since in the worst case we can simply discard the extra features at\u00b41 (resp. a\u0302t\u00b41) of f (resp. f\u0302 ) to recover the performance of the naive learner f0:\n1\nT\nT \u00ff t\u201c1 }fprxt, at\u00b41sq \u00b4 a\u02dat } \u010f \u03c9\n1\nT\nT \u00ff\nt\u201c1\n\u203a \u203a \u203a f\u0302prxt, a\u0302t\u00b41sq \u00b4 a\u02dat \u203a \u203a \u203a \u010f \u03c9\nAssume that the old policy \u03c0 \u201c \u03c0old is \u201dbad\u201d in the sense that the rolled-out trajectory tatuTt\u201c1 differs substantially from human trajectory ta\u02dat uTt\u201c1. Specifically, denote the gap:\n1\nT\nT \u00ff t\u201c1 }a\u02dat \u00b4 at\u00b41} \u201c \u2126 \" \u03c9\nThis means the feedback correction a\u02dat to st \u201c rxt, at\u00b41s is not smooth. We will show that the trained policy \u03c0\u0302 from D will not be smooth. From the definition of at and a\u0302t from equations 15 and 16, we have for each t:\nat\u00b4a\u0302t \u201c \u03bb 1` \u03bb pat\u00b41\u00b4a\u0302t\u00b41q` fprxt, at\u00b41sq \u00b4 f\u0302prxt, a\u0302t\u00b41sq\n1` \u03bb Applying triangle inequality and summing up over t, we have:\n1\nT\nT \u00ff t\u201c1 }at \u00b4 a\u0302t} \u010f 2\u03c9\nFrom here we can provide a lower bound on the smoothness of the new trajectory a\u0302t, as defined by the first order difference 1T \u0159T t\u201c1 }a\u0302t \u00b4 a\u0302t\u00b41}. By definition of a\u0302t:\n}a\u0302t \u00b4 a\u0302t\u00b41} \u201c\n\u203a \u203a \u203a \u203a \u203a f\u0302prxt, a\u0302t\u00b41sq \u00b4 a\u0302t\u00b41 1` \u03bb \u203a \u203a \u203a \u203a \u203a\n\u201c\n\u203a \u203a \u203a \u203a \u203a f\u0302prxt, a\u0302t\u00b41sq \u00b4 a\u02dat ` a\u02dat \u00b4 at\u00b41 ` at\u00b41 \u00b4 a\u0302t\u00b41 1` \u03bb \u203a \u203a \u203a \u203a \u203a\n\u011b }a\u02dat \u00b4 at\u00b41} \u00b4\n\u203a \u203a \u203a f\u0302prxt, a\u0302t\u00b41sq \u00b4 a\u02dat \u203a \u203a \u203a \u00b4 }at\u00b41 \u00b4 a\u0302t\u00b41}\n1` \u03bb Again summing up over t and taking the average, we obtain:\n1\nT\nT \u00ff t\u201c1 }a\u0302t \u00b4 a\u0302t\u00b41} \u011b \u2126\u00b4 3\u03c9 1` \u03bb\nHence for \u2126 \" \u03c9, meaning the old trajectory is sufficiently far away from the ideal human trajectory, setting the learning target to be the ideal human actions will cause the learned trajectory to be non-smooth.\nB. Imitation Learning for Online Sequence Prediction With Smooth Regression Forests\nB.1. Variant of SIMILE Using Smooth Regression Forest Policy Class\nWe provide a specific instantiation of algorithm 1 that we used for our experiment, based on a policy class \u03a0 as a smooth regularized version of the space of treebased ensembles. In particular, F is the space of random forests and H is the space of linear auto-regressors H fi thpat\u00b41:t\u00b4\u03c4 q \u201c \u0159\u03c4 i\u201c1 ciat\u00b4iu. In combination, F andH form a complex tree-based predictor that can predict smooth sequential actions.\nEmpirically, decision tree-based ensembles are among the best performing supervised machine learning method (Caruana & Niculescu-Mizil, 2006; Criminisi et al., 2012). Due to the piece-wise constant nature of decision treebased prediction, the results are inevitably non-smooth. We propose a recurrent extension based on H, where the prediction at the leaf node is not necessarily a constant, but rather is a smooth function of both static leaf node prediction and its previous predictions. By merging the powerful tree-based policy class with a linear auto-regressor, we provide a novel approach to train complex models that can accommodate smooth sequential prediction using modelbased smooth regularizer, at the same time leveraging the expressiveness of complex model-free function class (one can similarly apply the framework to the space of neural networks). Algorithm 2, which is based on SIMILE, describes in more details our training procedure used for the automated camera planning experiment. We first describe the role of the linear autoregressor class H, before discussing how to incorporate H into decision tree training to make smooth prediction (see the next section).\nThe autoregresor h\u03c0pa\u00b41, . . . , a\u00b4\u03c4 q is typically selected from a class of autoregressors H. In our experiments, we use regularized linear autoregressors asH. Consider a generic learning policy \u03c0 with a rolled-out trajectory A \u201c tatuTt\u201c1 corresponding to the input sequence X \u201c txtuTt\u201c1. We form the state sequence S \u201c tstuTt\u201c1 \u201c trxt, . . . , xt\u00b4\u03c4 , at\u00b41, . . . , at\u00b4\u03c4 suTt\u201c1, where \u03c4 indicates the past horizon that is adequate to approximately capture the full state information. We approximate the smoothness of the trajectory A by a linear autoregressor\nh\u03c0 \u201d h\u03c0pstq \u201d \u03c4 \u00ff\ni\u201c1 ciat\u00b4i\nfor a (learned) set of coefficients tciu\u03c4i\u201c1 such that at \u00ab h\u03c0 pstq. Given feedback target pA \u201c ta\u0302tu, the joint loss\nAlgorithm 2 Imitation Learning for Online Sequence Prediction with Smooth Regression Forest Input: Input features X \u201c txtuTt\u201c1, expert demonstration\nA\u02da \u201c ta\u02dat uTt\u201c1, base routine Forest, past horizon \u03c4 , sequence of \u03c3 P p0, 1q\n1: Initialize A0 \u00d0 A\u02da,S0 \u00d0 t \u201c xt:t\u00b4\u03c4 , a \u02da t\u00b41:t\u00b4\u03c4 \u2030 u,\nh0 \u201c argmin c1,...,c\u03c4\nT \u0159\nt\u201c1\n` a\u02dat \u00b4 \u0159\u03c4 i\u201c1 cia \u02da t\u00b4i\n\u02d82\n2: Initial policy \u03c00 \u201c \u03c0\u03020 \u00d0ForestpS0,A0| h0q 3: for n \u201c 1, . . . , N do 4: An \u201c tant u \u00d0 t\u03c0n\u00b41p \u201c xt:t\u00b4\u03c4 , a n\u00b41 t\u00b41:t\u00b4\u03c4 \u2030\nqu //sequential roll-out old policy\n5: Sn \u00d0 tsnt \u201c \u201c xt:t\u00b4\u03c4 , a n t\u00b41:t\u00b4\u03c4 \u2030 u //Form states in 1d case 6: pAn \u201c tpant \u201c \u03c3ant ` p1\u00b4 \u03c3qa\u02dat u @snt P Sn // collect smooth 1-step feedback\n7: hn \u201c argmin c1,...,c\u03c4\nT \u0159\nt\u201c1\n` a\u0302nt \u00b4 \u0159\u03c4 i\u201c1 cia\u0302 n t\u00b4i \u02d82 //update ci\nvia regularized least square 8: \u03c0\u0302n \u00d0ForestpSn, pAn| hnq // train with smooth decision forests. See section B.2 9: \u03b2 \u00d0 errorp\u03c0qerrorp\u03c0\u0302q`errorp\u03c0q //set \u03b2 to weighted\nempirical errors 10: \u03c0n \u201c \u03b2\u03c0\u0302n ` p1\u00b4 \u03b2q\u03c0n\u00b41 // update policy 11: end for output Last policy \u03c0N\nfunction thus becomes\n`pa, a\u0302tq \u201c `dpa, a\u0302tq ` \u03bb`Rpa, stq\n\u201c pa\u00b4 a\u0302tq2 ` \u03bbpa\u00b4 \u03c4 \u00ff\ni\u201c1 ciat\u00b4iq2\nHere \u03bb trades off between smoothness versus absolute imitation accuracy. The autoregressor h\u03c0 acts as a smooth linear regularizer, the parameters of which can be updated at each iteration based on feedback target pA according to\nh\u03c0 \u201c argmin hPH\n\u203a \u203a \u203a pA\u00b4 hppAq \u203a \u203a \u203a\n2\n\u201c argmin c1,...,c\u03c4\np T \u00ff\nt\u201c1 pa\u0302t \u00b4\n\u03c4 \u00ff i\u201c1 cia\u0302t\u00b4iq2q, (17)\nIn practice we use a regularized version of equation (17) to learn a new set of coefficients tciu\u03c4i\u201c1. The Forest procedure (Line 8 of algorithm 2) would use this updated h\u03c0 to train a new policy that optimizes the trade-off between at \u00ab a\u0302t (feedback) versus smoothness as dictated by at \u00ab \u0159\u03c4 i\u201c1 ciat\u00b4i.\nB.1.1. SMOOTH REGULARIZATION WITH LINEAR AUTOREGRESSORS\nOur application of Algorithm 1 to realtime camera planning proceeds as follows: At each iteration, we form a state se-\nquence S based on the rolled-out trajectory A and tracking input data X such that st \u201c rxt, . . . , xt\u00b4\u03c4 , at\u00b41, . . . , at\u00b4\u03c4 s for appropriate \u03c4 that captures the history of the sequential decisions. We generate feedback targets pA based on each st P S following a\u0302t \u201c \u03c3at ` p1\u00b4 \u03c3qa\u02dat using a parameter \u03c3 P p0, 1q depending on the Euclidean distance between A and A\u02da. Typically, \u03c3 gradually decreases to 0 as the rolledout trajectory improves on the training set. After generating the targets, a new linear autoregressor h\u03c0 (new set of coefficients tciu\u03c4i\u201c1) is learned based on pA using regularized least squares (as described in the previous section). We then train a new model \u03c0\u0302 based on S, pA, and the updated coefficients tciu, using Forest - our recurrent decision tree framework that is capable of generating smooth predictions using autoregressor h\u03c0 as a smooth regularizer (see the following section for how to train smooth decision trees). Note that typically this creates a \u201dchicken-and-egg\u201d problem. As the newly learned policy \u03c0\u0302 is greedily trained with respect to pA, the rolled-out trajectory of \u03c0\u0302 may have a state distribution that is different from what the previously learned h\u03c0 would predict. Our approach offers two remedies to this circular problem. First, by allowing feedback signals to vary smoothly relative to the current rolled-out trajectory A, the new policy \u03c0\u0302 should induce a new autoregresor that is similar to previously learned h\u03c0 . Second, by interpolating distributions (Line 10 of Algorithm 2) and having pA eventually converge to the original human trajectory A\u02da, we will have a stable and converging state distribution, leading to a stable and converging h\u03c0 .\nThroughout iterations, the linear autoregressor h\u03c0 and regularization parameter \u03bb enforces smoothness of the rolledout trajectory, while the recurrent decision tree framework Forest learns increasingly accurate imitation policy. We generally achieve a satisfactory policy after 5-10 iterations in our sport broadcasting data sets. In the following section, we describe the mechanics of our recurrent decision tree training.\nB.2. Smooth Regression Tree Training\nGiven states s as input, a decision tree specifies a partitioning of the input state space. Let D \u201c tpsm, a\u0302mquMm\u201c1 denote a training set of state/target pairs. Conventional regression tree learning aims to learn a partitioning such that each leaf node, node, makes a constant prediction via minimizing the squared loss function:\na\u0304node \u201c argmin a\n\u00ff\nps,a\u0302qPDnode\n`dpa, a\u0302q\n\u201c argmin a\n\u00ff\nps,a\u0302qPDnode\npa\u0302\u00b4 aq2, (18)\nwhereDnode denotes the training data fromD that has partitioned into the leaf node. For squared loss, we have:\na\u0304node \u201c mean ta\u0302 |ps, a\u0302q P Dnode u . (19)\nIn the recurrent extension to Forest, we allow the decision tree to branch on the input state s, which includes the previous predictions a\u00b41, . . . , a\u00b4\u03c4 . To enforce more explicit smoothness requirements, let h\u03c0pa\u00b41, . . . , a\u00b4\u03c4 q denote an autoregressor that captures the temporal dynamics of \u03c0 over the distribution of input sequences dx, while ignoring the inputs x. At time step t, h\u03c0 predicts the behavior at \u201c \u03c0pstq given only at\u00b41, . . . , at\u00b4\u03c4 .\nOur policy class \u03a0 of recurrent decision trees \u03c0 makes smoothed predictions by regularizing the predictions to be close to its autoregressor h\u03c0 . The new loss function incorporates both the squared distance loss `d, as well as a smooth regularization loss such that:\nLDpaq \u201c \u00ff\nps,a\u0302qPD `dpa, a\u0302q ` \u03bb`Rpa, sq\n\u201c \u00ff\nps,a\u0302qPD pa\u00b4 a\u0302q2 ` \u03bbpy \u00b4 h\u03c0psqq2\nwhere \u03bb is a hyper-parameter that controls how much we care about smoothness versus absolute distance loss.\nMaking prediction: For any any tree/policy \u03c0, each leaf node is associated with the terminal leaf node value a\u0304node such that prediction a\u0303 given input state s is:\na\u0303psq \u201d \u03c0psq \u201c argmin a pa\u00b4 a\u0304nodepsqq2 ` \u03bbpa\u00b4 h\u03c0psqq2,\n(20)\n\u201c a\u0304nodepsq ` \u03bbh\u03c0psq\n1` \u03bb . (21)\nwhere nodepsq denotes the leaf node of the decision tree that s branches to.\nSetting terminal node value: Given a fixed h\u03c0 and decision tree structure, navigating through consecutive binary queries eventually yields a terminal leaf node with associated training data Dnode \u0102 D.\nOne option is to set the terminal node value a\u0304node to satisfy:\na\u0304node \u201c argmin a\n\u00ff\nps,a\u0302qPDnode\n`dpa\u0303ps|aq, a\u0302q\n\u201c argmin a\n\u00ff\nps,a\u0302qPDnode\npa\u0303ps|aq \u00b4 a\u0302q2 (22)\n\u201c argmin a\n\u00ff\nps,a\u0302qPDnode\n\u02c6\na` \u03bbh\u03c0psq 1` \u03bb \u00b4 a\u0302\n\u02d92\nfor a\u0303ps|aq defined as in (21) with a \u201d a\u0304nodepsq. Similar to (19), we can write the closed-form solution of (22) as:\na\u0304node \u201c mean tp1` \u03bbqa\u0302\u00b4 \u03bbh\u03c0psq |ps, a\u0302q P Dnode u . (23) When \u03bb \u201c 0, (23) reduces to (19).\nNote that (22) only looks at imitation loss `d, but not smoothness loss `R. Alternatively in the case of joint imitation and smoothness loss, the terminal leaf node is set to minimize the joint loss function:\na\u0304node \u201c argmin a LDnodepa\u0303ps|aqq\n\u201c argmin a\n\u00ff\nps,a\u0302qPDnode\n`dpa\u0303ps|aq, a\u0302q ` \u03bb`Rpa\u0303ps|aq, sq\n\u201c argmin a\n\u00ff\nps,a\u0302qPDnode\npa\u0303ps|aq \u00b4 a\u0302q2 ` \u03bbpa\u0303ps|aq \u00b4 h\u03c0psqq2\n(24)\n\u201c argmin a\n\u00ff\nps,a\u0302qPDnode\n\u02c6\na` \u03bbh\u03c0psq 1` \u03bb \u00b4 a\u0302\n\u02d92\n` \u03bb \u02c6 a` \u03bbh\u03c0psq 1` \u03bb \u00b4 h\u03c0psq\n\u02d92\n\u201c mean ta\u0302 |ps, a\u0302q P Dnode u , (25)\nNode splitting mechanism: For a node representing a subset Dnode of the training data, the node impurity is defined as:\nInode \u201c LDnodepa\u0304nodeq \u201c \u00ff\nps,a\u0302qPDnode\n`dpa\u0304node, a\u0302q ` \u03bb`Rpa\u0304node, sq\n\u201c \u00ff\nps,a\u0302qPDnode\npa\u0304node \u00b4 a\u0302q2 ` \u03bbpa\u0304node \u00b4 h\u03c0psqq2\nwhere a\u0304node is set according to equation (23) or (25) over ps, a\u0302q\u2019s in Dnode. At each possible splitting point where Dnode is partitioned into Dleft and Dright, the impurity of the left and right child of the node is defined similarly. As with normal decision trees, the best splitting point is chosen as one that maximizes the impurity reduction: Inode \u00b4 |Dleft||Dnode|Ileft \u00b4 |Dright| |Dnode| Iright"}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Abbeel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2005}, {"title": "A survey of robot learning from demonstration", "author": ["Argall", "Brenna D", "Chernova", "Sonia", "Veloso", "Manuela", "Browning", "Brett"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Policy search by dynamic programming", "author": ["Bagnell", "J Andrew", "Kakade", "Sham M", "Schneider", "Jeff G", "Ng", "Andrew Y"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Bagnell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bagnell et al\\.", "year": 2003}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["Caruana", "Rich", "Niculescu-Mizil", "Alexandru"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Caruana et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Caruana et al\\.", "year": 2006}, {"title": "Mimicking human camera operators", "author": ["Chen", "Jianhui", "Carr", "Peter"], "venue": "In IEEE Winter Conference Applications of Computer Vision (WACV),", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning online smooth predictors for real-time camera planning using recurrent decision trees", "author": ["Chen", "Jianhui", "Le", "Hoang M", "Carr", "Peter", "Yue", "Yisong", "Little", "James J"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning", "author": ["Criminisi", "Antonio", "Shotton", "Jamie", "Konukoglu", "Ender"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Criminisi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Criminisi et al\\.", "year": 2012}, {"title": "Search-based structured prediction", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Imitation learning by coaching", "author": ["He", "Eisner", "Jason", "Daume", "Hal"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Learning trajectory preferences for manipulators via iterative improvement", "author": ["Jain", "Ashesh", "Wojcik", "Brian", "Joachims", "Thorsten", "Saxena", "Ashutosh"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Reinforcement learning as classification: Leveraging modern classifiers", "author": ["Lagoudakis", "Michail", "Parr", "Ronald"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Lagoudakis et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis et al\\.", "year": 2003}, {"title": "Relating reinforcement learning performance to classification performance", "author": ["Langford", "John", "Zadrozny", "Bianca"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Langford et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2005}, {"title": "Functional gradient techniques for combining hypotheses", "author": ["Mason", "Llew", "Baxter", "Jonathan", "Bartlett", "Peter L", "Frean", "Marcus"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Mason et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mason et al\\.", "year": 1999}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["Ratliff", "Nathan", "Silver", "David", "Bagnell", "J. Andrew"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "Drew"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "Stephane", "Gordon", "Geoff", "Bagnell", "J. Andrew"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Smoothness, low noise and fast rates", "author": ["Srebro", "Nathan", "Sridharan", "Karthik", "Tewari", "Ambuj"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "A reduction from apprenticeship learning to classification", "author": ["Syed", "Umar", "Schapire", "Robert E"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Syed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2010}, {"title": "A study in the analysis of stationary time series, 1939", "author": ["Wold", "Herman"], "venue": null, "citeRegEx": "Wold and Herman.,? \\Q1939\\E", "shortCiteRegEx": "Wold and Herman.", "year": 1939}], "referenceMentions": [{"referenceID": 15, "context": "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).", "startOffset": 209, "endOffset": 331}, {"referenceID": 2, "context": "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).", "startOffset": 209, "endOffset": 331}, {"referenceID": 17, "context": "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).", "startOffset": 209, "endOffset": 331}, {"referenceID": 10, "context": "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).", "startOffset": 209, "endOffset": 331}, {"referenceID": 6, "context": "Our motivating example is the problem of learning smooth policies for automated camera planning (Chen et al., 2016): determining where a camera should look given environment information (e.", "startOffset": 96, "endOffset": 115}, {"referenceID": 17, "context": "To address this issue, numerous learning reduction approaches have been proposed (Daum\u00e9 III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011), which iteratively modify the training distribution in various ways such that any supervised learning guarantees provably lift to the sequential imitation setting (potentially at the cost of statistical or computational efficiency).", "startOffset": 81, "endOffset": 146}, {"referenceID": 17, "context": ", 2009)), and not requiring data aggregation (as opposed to DAgger (Ross et al., 2011)) which can lead to super-linear training time.", "startOffset": 67, "endOffset": 86}, {"referenceID": 6, "context": "\u2022 We empirically evaluate using the setting of smooth camera planning (Chen et al., 2016), and demonstrate the performance gains of our approach.", "startOffset": 70, "endOffset": 89}, {"referenceID": 6, "context": "Consider the example of autonomous camera planning for broadcasting a sport event (Chen et al., 2016).", "startOffset": 82, "endOffset": 101}, {"referenceID": 17, "context": "Following the basic setup from (Ross et al., 2011), for any policy \u03c0 P \u03a0, let dt denote the distribution of states at time t if \u03c0 is executed for the first t \u03011 time steps.", "startOffset": 31, "endOffset": 50}, {"referenceID": 3, "context": "The most popular traditional approaches for learning from expert demonstration focused on using approximate policy iteration techniques in the MDP setting (Kakade & Langford, 2002; Bagnell et al., 2003).", "startOffset": 155, "endOffset": 202}, {"referenceID": 9, "context": "Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009).", "startOffset": 66, "endOffset": 145}, {"referenceID": 15, "context": "Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009).", "startOffset": 66, "endOffset": 145}, {"referenceID": 2, "context": "Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009).", "startOffset": 66, "endOffset": 145}, {"referenceID": 17, "context": "State-of-the-art learning reductions for imitation learning typically take an iterative approach, where each training round uses standard supervised learning to learn a policy (Daum\u00e9 III et al., 2009; Ross et al., 2011).", "startOffset": 176, "endOffset": 219}, {"referenceID": 17, "context": ", 2009) and DAgger (Ross et al., 2011).", "startOffset": 19, "endOffset": 38}, {"referenceID": 17, "context": "First, we need not query the expert \u03c0 \u030a at every iteration (as done in DAgger (Ross et al., 2011)).", "startOffset": 78, "endOffset": 97}, {"referenceID": 9, "context": "A similar idea was proposed (He et al., 2012) for DAggertype algorithm, albeit only for linear model classes.", "startOffset": 28, "endOffset": 45}, {"referenceID": 17, "context": "Previous learning reduction approaches only use stochastic interpolation (Daum\u00e9 III et al., 2009; Ross et al., 2011), whereas SIMILE uses deterministic.", "startOffset": 73, "endOffset": 116}, {"referenceID": 17, "context": "Analyses of SEARN and other previous iterative reduction methods (Ross et al., 2011; Kakade & Langford, 2002; Bagnell et al., 2003; Syed & Schapire, 2010) rely on bounding the variation distance between d\u03c0 and d\u03c01 .", "startOffset": 65, "endOffset": 154}, {"referenceID": 3, "context": "Analyses of SEARN and other previous iterative reduction methods (Ross et al., 2011; Kakade & Langford, 2002; Bagnell et al., 2003; Syed & Schapire, 2010) rely on bounding the variation distance between d\u03c0 and d\u03c01 .", "startOffset": 65, "endOffset": 154}, {"referenceID": 7, "context": "Policy Improvement Our policy improvement guarantee builds upon the analysis from SEARN (Daum\u00e9 III et al., 2009), which we extend to using adaptive learning rates \u03b2. We first restate the main policy improvement result from Daum\u00e9 III et al. (2009). Lemma 5.", "startOffset": 95, "endOffset": 247}, {"referenceID": 7, "context": "Policy Improvement Our policy improvement guarantee builds upon the analysis from SEARN (Daum\u00e9 III et al., 2009), which we extend to using adaptive learning rates \u03b2. We first restate the main policy improvement result from Daum\u00e9 III et al. (2009). Lemma 5.4 (SEARN\u2019s policy nondegradation - Lemma 1 from Daum\u00e9 III et al. (2009)).", "startOffset": 95, "endOffset": 328}, {"referenceID": 14, "context": "To see that simulating smooth \u201cvirtual\u201d feedback target does not hurt the training progress, we alternatively view SIMILE as performing gradient descent in a smooth function space (Mason et al., 1999).", "startOffset": 180, "endOffset": 200}, {"referenceID": 6, "context": "We evaluate SIMILE in a case study of automated camera planning for sport broadcasting (Chen & Carr, 2015; Chen et al., 2016).", "startOffset": 87, "endOffset": 125}, {"referenceID": 6, "context": "2: regression tree ensembles F regularized by a class of linear autoregressor functions H (Chen et al., 2016).", "startOffset": 90, "endOffset": 109}, {"referenceID": 18, "context": "2 (1-d Case (Srebro et al., 2010)).", "startOffset": 12, "endOffset": 33}, {"referenceID": 7, "context": "Empirically, decision tree-based ensembles are among the best performing supervised machine learning method (Caruana & Niculescu-Mizil, 2006; Criminisi et al., 2012).", "startOffset": 108, "endOffset": 165}], "year": 2016, "abstractText": "We study the problem of smooth imitation learning for online sequence prediction, where the goal is to train a policy that can smoothly imitate demonstrated behavior in a dynamic and continuous environment in response to online, sequential context input. Since the mapping from context to behavior is often complex, we take a learning reduction approach to reduce smooth imitation learning to a regression problem using complex function classes that are regularized to ensure smoothness. We present a learning meta-algorithm that achieves fast and stable convergence to a good policy. Our approach enjoys several attractive properties, including being fully deterministic, employing an adaptive learning rate that can provably yield larger policy improvements compared to previous approaches, and the ability to ensure stable convergence. Our empirical results demonstrate significant performance gains over previous approaches.", "creator": "LaTeX with hyperref package"}}}