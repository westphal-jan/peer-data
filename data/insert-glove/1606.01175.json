{"id": "1606.01175", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "Infant directed speech is consistent with teaching", "abstract": "k\u00fcnzelsau Infant - directed cooz speech (IDS) has barsalou distinctive properties floren that nazar\u00e9 differ llevo from 70s adult - skyrail directed 36,000-strong speech (hatchell ADS ). adia Why it zonk has grm these properties - - and yleisradio whether they stadthalle are ratchaburi intended to brindisi facilitate language grindler learning - - is ghawar matter of contention. spnea We jpa argue that aylor much airtricity of this disagreement protaras stems 4,810 from monde lack twizzle of a istra formal, guiding theory college-based of houlahan how hif phonetic categories should best be taught 9.91 to homeotic infant - like n\u00facleo learners. In the courthouse absence of sandwich such bonal a theory, khullar researchers have relied bibtex on phoned intuitions prest about run learning recommence to kutless guide 119.17 the argument. amsalem We hims use a tomescu formal theory of iso9000 teaching, validated dandelion through paydirt experiments votto in other domains, eighteenth as the eble basis for recently a detailed analysis of antinori whether cloak IDS is well - lift-off designed for teaching monday-friday phonetic categories. 15.08 Using placente the lifestyle theory, 1.32 we 92.1 generate 099 ideal conferment data for teaching phonetic categories in teilhard English. We 1920-1939 qualitatively taraborrelli compare the assemby simulated awabakal teaching 122.02 data with 1.4755 human sembawang IDS, roffey finding that the teaching kavarna data naqvi exhibit many features of compartmented IDS, heah including gordeeva some that have been valjala taken ends as evidence IDS flaunts is not for calleri teaching. The simulated data 10-episode reveal potential pitfalls non-hazardous for sydal experimentalists exploring cultural the hone role 4,039 of darweesh IDS defied in cyclotron language learning. maribor Focusing ten-foot on nygatan different formants andronikou and phoneme delf\u00edn sets subjugating leads to olivenza different pilchards conclusions, daglish and swineherd the servos benefit of the sadomasochism teaching entrain data to learners hga is not perot apparent c\u00edrculo until sumati a aflpa sufficient number of gades examples have been romir provided. immolations Finally, we ipc investigate 45.13 transfer pissarides of arranz IDS delfay to celulas learning ADS. booms The aaliyah teaching data improves pompeya classification parents-in-law of ADS sorption data, but only for countenanced the learner chamfered they briefing were oando generated demetra to teach; not 196.1 universally engelbart across endevor all globalgiving classes of learner. sharqiya This tabloidization research offers nili a theoretically - brookens grounded insures framework that empowers wordly experimentalists tenera to 1852 systematically resell evaluate whether IDS is for kinky teaching.", "histories": [["v1", "Wed, 1 Jun 2016 00:59:41 GMT  (1234kb,D)", "http://arxiv.org/abs/1606.01175v1", "21 pages, 5 figures"]], "COMMENTS": "21 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["baxter s eaves jr", "naomi h feldman", "thomas l griffiths", "patrick shafto"], "accepted": false, "id": "1606.01175"}, "pdf": {"name": "1606.01175.pdf", "metadata": {"source": "CRF", "title": "Infant-directed speech is consistent with teaching", "authors": ["Baxter S. Eaves Jr.", "Rutgers University\u2013Newark", "Naomi H. Feldman", "Thomas L. Griffiths", "Patrick Shafto"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Infant-directed speech, language acquisition, social learning, Bayesian model\nChildren learn language from input, but often the input children receive differs markedly from normal speech. Infant-directed speech (IDS, also known as \u201cmotherese\u201d) is characterized by reduced speed, elevated pitch and affect, and unusual prosody. Infants are able to distinguish IDS from normal, adult-directed speech (ADS) and prefer IDS over ADS [1]. Subsequently, researchers have sought to answer why it is that adults speak to children in this unusual way. Seminal work by\nThis work was funded in part by NSF CAREER award DRL-1149116.\nar X\niv :1\n60 6.\n01 17\n5v 1\n[ cs\n.L G\n] 1\nKuhl et al. [2] found that IDS has unusual formant-level properties. Formants are the representative frequencies of vowel phonemes and manifest as peaks in the spectral envelope. The first formant is the lowest frequency peak, the second formant is the second lowest, and so on. When plotted in formant space, the vowels that form the \u201ccorners\u201d of the space of possible vowels (/A/, as in pot; /i/, as in beet; /u/, as in boot) are hyper-articulated, making them more different from one another. This results in an expansion of the vowel space. Intuitively speaking, hyper-articulation should improve the learnability of vowel categories. All things being equal, example clusters that are more distant are easier to identify. This sparked the idea that IDS is for teaching; an idea that after nearly two decades remains a matter of controversy among researchers.\nResearch suggests that corner vowel hyper-articulation is not simply an unintended consequence of highly-affectual speech. Corner vowel hyper-articulation is present in speech to infants but not speech to pets [3]. Additionally, corner vowel hyper-articulation is found in speech to foreigners [4], which, outwardly, sounds more like normal, adult speech. In fact, the social learning literature refers to IDS as an ostensive cue: a social cue that engages stricter learning mechanisms in its target [5]. It would appear that IDS and its unique features are optimized to teach learners the vowel categories of their language.\nHowever, recent work has discovered statistical features of IDS that are potentially detrimental to learning. Other, non-corner vowels are hypo-articulated (closer together) in IDS [6, 7] and withinphoneme variability increases for some vowels [8, 9]. Hypo-articulation is argued to be detrimental to learning because clusters of examples become less distinct as they become nearer. Increased variability is argued to be detrimental because as clusters increase in size, their effective borders shrink or overlap, which makes them less discriminable. Additionally, Martin et al. [10] found that temporally sequential pairs of vowel phonemes are less discriminable in IDS than in ADS. It would appear that IDS and its unique features may make learning phonetic categories more difficult.1\nOver the course of the debate about the role of IDS in language learning, researchers have attempted to quantitatively evaluate the benefit of IDS to learners by comparing the outcome of different learning algorithms given IDS and ADS data [6, 8, 9]. These studies have achieved mixed results. de Boer & Kuhl [8] found that a mixture model trained using the expectation-maximization algorithm was better able to recover the means of IDS corner vowel categories from IDS data than it was to recover the means of ADS corner vowel categories from ADS data. Kirchhoff & Schimmel [6] explored the usefulness of IDS to training Bayesian automatic speech recognition systems (ASR), finding that the IDS-trained ASR classified certain types of data more effectively than ADS-trained ASR and other types more poorly. McMurray et al. [9] found that multinomial logistic regression trained on IDS data correctly classified fewer new IDS examples than its ADS-trained counterpart classified new ADS examples. Based on these results, the debate appears only to be farther from being resolved.\nWe argue that much of the disagreement in the literature with respect to whether IDS is optimized for teaching stems from a lack of a coherent theoretical framework for characterizing teaching. In the absence of such a framework, researchers have substituted intuitions about learning. This has three significant limitations. First, researchers have largely intuited which qualitative features are desirable and which are not. Second, existing computational approaches have attempted to assess teaching indirectly through improvements in learning using various, very different, computational models. Moreover, assessments of model performance have not focused on the key question: the implications of training on IDS for categorization of ADS. Third, the literature tends to focus attention on subsets of the data, both in terms of the vowels and the formants considered for any given analysis.\nEach limitation potentially undermines interpretation. First, computational models are preferable to intuitive arguments precisely because intuition is fallible, especially when considering the kinds of interactions involved in teaching many categories in a low-dimensional space. Second,\n1Related but orthogonal work suggests that infant- and child-directed speech is less intelligible to adults [11, 12].\nwhile we would expect teaching to lead to better learning, teaching is defined in terms of the intent of the speaker, thus improvements in learning are not a necessary implication\u2014especially if the learner used for performance benchmarking solves a different problem than the learner for whom the teacher generates data. Moreover, given that learners ultimately need to acquire ADS, any improvements in learning should be in transfer between IDS and ADS. Third, because teaching involves considering not just the target vowel but also potentially confusable alternatives, any results derived from subsets of the data may lead to unrepresentative predictions. It is thus important to investigate whether these limitations do affect conclusions in the literature. Our contribution to the debate is a formal theoretical analysis of how phonetic categories should optimally be taught to infant-like learners. This is the first work to directly address whether IDS is consistent with optimal teaching. We begin by defining the teaching and learning problems under a probabilistic framework. From this model, we generate data designed to teach. We address whether certain features of data are consistent with teaching by qualitatively comparing the features of the teaching data with those of IDS. We address whether IDS-like data are beneficial for learning normal (ADS) speech, and whether these effects generalize, by comparing learning transfer under the target learning model and under standard machine learning algorithms. We also identify some important caveats related to computational analyses based on subsets of data. We address the problems with looking at dimensional and categorical subsets of the data by comparing the features of, and learning outcomes given the original teaching data with those of the teaching data projected onto two-formant space, and we compare the effect of sample size (the number of IDS examples) on learning performance given ADS data and teaching data. We conclude by discussing limitations of the current work and future directions."}, {"heading": "Teaching and learning", "text": "To simulate teaching, we must define the components of teaching. In this section we define, in mathematical terms, the components of the problem: the teacher, the learner, and the concept to be learned and taught. Mathematically defining the concept (the phonetic category model) is matter of applying a formalism that is sufficiently representative of the concept. Similarly, defining a learner requires applying a learning framework that is capable of learning the concept and does so in a psychologically-valid way. And, as we shall see, defining a teacher requires defining a data selection method that is intended to induce the defined concept in the defined learner. Throughout the paper, the words teacher and learner will be used to refer to the definitions in this section; we will make the necessary distinction when referring to human learners.\nWhat is being taught and what is being learned\nIn their work on automatic speech recognition, Kirchhoff & Schimmel [6] posed the question of what is being learned from IDS. If IDS is for teaching then what does IDS teach? While it is typically implied that the intent would be to teach normal speech, existing computational studies compare the effectiveness of IDS at teaching IDS with the effectiveness of ADS at teaching ADS [8, 9]. That is, these studies evaluate whether IDS is better at teaching an abnormal (non-adult) speech model than ADS is at teaching the normal speech model. Here, we assume that it is the intent of a teacher to teach the set of phonetic categories used in normal speech. Building on previous research formalizing phonetic categories, we adopt a Gaussian mixture model (GMM) framework [8, 13\u201315]. Each phonetic category is represented as a multidimensional Gaussian in formant space. We focus on the first, second, and third formants, denoted F1, F2, and F3, which we capture with 3-dimensional Gaussians.\nA GMM is defined by the probability density function\nf(X|\u03c01, . . . \u03c0k, \u00b51, . . . , \u00b5k,\u03a31, . . .\u03a3k) = k\u2211 i=1 \u03c0iN (X|\u00b5i,\u03a3i), (1)\nwhere {\u03c01, . . . , \u03c0k} is a set of k components weights (real numbers between 0 and 1 inclusive and which sum to 1), {\u00b51, . . . , \u00b5k} is a set of component means, {\u03a31, . . . ,\u03a3k} is the set of component covariance matrices, and N (X|\u00b5,\u03a3) is the Normal (Gaussian) probability density function applied to the data X given \u00b5 and \u03a3. Importantly, we view the whole system of phonetic categories as being the object that is being taught. The best data for teaching a single phonetic category might be different from the best data for teaching that category in the context of a set of other categories. When learning a single category, data that are representative of that category are sufficient to communicate the relevant statistical information. When learning multiple categories, without a clear indication of what category each sound belongs to, the possible ambiguity of each sound interacts with the need to provide good information about the statistics of each category to create a much more complex problem."}, {"heading": "Learning", "text": "Teaching data are by definition generated with the learner in mind [16, 17]. A teacher chooses data to induce the correct belief in learners, hence we must define the learner. Previous computational accounts of learning under IDS have evaluated learning in computational learners that know the correct number of categories [8] or learn from labeled data [9]. These approaches miss an important difficulty of the learning problem infants face. Infants are not born knowing how many phonemes comprise their native language nor are they given veridical feedback as to which phonetic categories individual components of utterances belong to. In order to learn the locations (means, \u00b5) and shapes (covariance matrices, \u03a3) of phonetic categories, infants must learn how many there are; all while inferring to which phonetic categories each example belongs. Learning the nature and the number of categories simultaneously can be done using the Dirichlet process Gaussian Mixture Model (DPGMM) [18\u201321]. The basic idea is that when a learner cannot assume a fixed number of categories, she must allow for the possibility that there may be as many categories as there are data. This problem can be addressed by using a probabilistic process that determines which data are assigned to which categories [see 20]. Rather than learning the weights of infinitely many categories, the learner learns an assignment, Z = {z1, . . . , zn} where zi is an integer indicating to which component of the mixture the ith datum belongs. Imagine that we have observed n examples to which we have attributed k categories. Assuming no upper bound on the number of categories, a new example may be assigned to one of the k existing categories or\u2014if it is especially anomalous\u2014may warrant creation of a new, singleton category (a category of which datum n + 1 is the only member). The mixture weights are then implicit in Z. Components with more assigned data have higher weights. We outline this approach in more detail in Appendix A."}, {"heading": "Teaching", "text": "We employ an existing model of teaching that has been used successfully to capture human learning in a variety of scenarios [16, 17, 22, 23], under which optimal teaching data derive from the inverse of the learning process. Rather than sampling data randomly from the true distribution, optimal data for teaching are sampled from the distribution that leads learners to the correct inference. Thus teaching involves directing learners\u2019 inferences; not just toward the correct hypothesis, but away from alternatives. Mathematically, the goal of the teacher is to maximize the posterior probability that the learner ends up with the correct hypothesis\u2014in this case, the correct estimate of the category assignments Z and the mixture parameters \u00b5 (all the means \u00b5) and \u03a3 (all the covariance matrices \u03a3). To express this idea\u2014and allow for the fact that there will be some stochasticity in teaching\u2014we define the probability that the optimal teacher generates data X to be proportional to the posterior probability of the correct hypothesis given that value of X. Formally,\nPopt(X|Z,\u00b5,\u03a3) = P (Z,\u00b5,\u03a3|X)\u222b\nX P (Z,\u00b5,\u03a3|X)dX\n(2)\nwhere the denominator normalizes the distribution, ensuring that it sums to 1 over all X. Recall that arguments for or against IDS as pedagogical input in existing research rely on the assumption that the pedagogical intent of data can be measured by its benefit to learners. To the contrary, as we shall see, the benefit of data to learners is not a strict indication of the pedagogical intent of data even in our ideal teacher-learner scenario. For example, if the target concept is complex, large amounts of data may be required before any benefit over random data (data generated directly from the target concept) becomes apparent. Alternatively, the adherence of some data to patterns consistent with pedagogically-selected data does provide evidence of pedagogical intent. But without a rigorous definition of pedagogical data selection one can only guess at what these patterns are. The output of the teaching model is dependent on what is being taught and how it is being taught. Because our goal is to evaluate a claim in the literature, in keeping with the literature\u2014 which is framed in terms of learning phonemes from formants\u2014we generate data to teach a subset of language (a specific phonetic category model derived from Hillenbrand et al. [24]) by manipulating first, second, and third formant values. Formants are known to correlate with vowel identity [24, 25], though the dimensions that listeners use when storing and categorizing sounds may be more complex than absolute encoding of formant frequencies [26\u201329]. Listeners\u2019 reliance on perceptual dimensions may also change over the course of development [30, 31]. Thus, our characterization is a significant simplification of the real-world problem. It makes the teaching problem both easier and more difficult. It is easier because a less complicated model requires less computation to teach, and a teacher need not be concerned with which features are relevant to learners or whether learners must learn which features are relevant. On the other hand, the task is more difficult because we have reduced the information to the learner and reduced the number of manipulable dimensions for the teacher. Thus, the teaching output should be interpreted with care. Differences between our formalization of the problem and nature\u2019s will result in differences between the model output and empirical data. We expect the output to be qualitatively similar to human IDS, but do not expect all observed trends to match exactly."}, {"heading": "Comparison with Human Infant-Directed Speech", "text": "To evaluate the predictions that this formal model makes about the optimal data for teaching a system of phonetic categories, we focus on twelve American English vowel phonemes and their first, second, and third formants, F1, F2, and F3. Hillenbrand et al. [24] provide 48 examples of each phoneme from female speakers. Examples with unmeasurable formant values were discarded, leaving several phonemes with fewer examples (see Table 1). The target model \u2013 the one that teachers should be trying to convey to learners \u2013 was derived from the means and covariance matrices calculated from each phoneme\u2019s examples (the full list of phonemes and their means and variances can be found in Table 1). Using an algorithm outlined in Appendix A, we generated a total of 10,000 samples from the distribution defined in Equation 2, each consisting of one example of each of the 12 phonetic categories. We then analyzed these samples, comparing them to human ADS and IDS. Figure 1a shows the distributions of the ADS vowels and the model predictions for IDS along the first and second formants. The model predicts that the simulated teaching data do not simply parrot the target distribution but modify it in ways that match infant-directed speech. Specifically, consistent with previous research [2, 3, 7] the corner vowels are hyper-articulated. Additionally, features that researchers have used to argue against the potential pedagogical intent of IDS are present in the teaching data. Figure 2 shows the predicted change in Euclidean distance between all pairs of vowels. We chose Euclidean distance rather than a variance-based measure of intelligibility because hyperarticulation is defined in terms of movement, and because teaching is meant to communicate the entire category model, not just pairs of phonemes. Most vowel pairs are hyper-articulated, but consistent with IDS, and contrary to previous arguments that IDS is not for teaching [7], the simulated teaching\ndata include hypo-articulation of some vowel pairs. Figure 3 shows the predicted effects on withincategory variability. Consistent with IDS [7, 8], but contra previous arguments [9], the statistically optimal input includes increases in within-category variability for most categories. Of note is the the difference in behaviour between variances and covariances. Other than /A/ in F1 and /\u00c7/ in F3, each phoneme\u2019s variance increases. The covariance behavior is less uniform. Four of twelve phonemes decrease F1-F2 covariance, six of twelve decrease F3-F1 covariance, and four of twelve decrease F3-F2 covariance. This suggests that though the teaching data in general exhibit greater variance, orientation plays a role. It is important to note that trends in hyper- and hypo-articulation change when the threeformant data are flattened onto two dimensions (Figure 2a, b). Figure 2a shows the change in distance between each phoneme pair in three dimensions (F1, F2, F3) and Figure 2b shows the change in distance in the same data within the F1-F2 plane. All corner vowel pairs are hyper-articulated in both sets, but many of the pairs that are hyper-articulated in three-formant space show little change, or are hypo-articulated, in two-formant space. This demonstrates that measures (and thus, conclusions) derived from a dimensional subset of teaching data may provide an incomplete view of the data. For example, it is not appropriate to argue that the data are not for teaching because the /o/-/u/ and /O/-/\u00c7/ pairs are hypo-articulated in the two-formant projection because the data were not generated to teach using only F1 and F2.More broadly, the absolute formant values that are typically analyzed in IDS research differ from the relative formant encodings hypothesized in many perceptual theories. It has been suggested, for example, that listeners rely on ratios among formants [28, 29, 32] or on comparisons of formants among different vowels from the same speaker [33\u201335]. Inaccurate assumptions about perceptual dimensions can potentially lead to incorrect conclusions about whether IDS is for teaching. The simulated teaching data include some divergences from human IDS. IDS studies focus on different languages and dialects, and different interior vowels; because the model output is designed to teach an American English phonetic category model, we limit our discussion of systematic deviations to those between the model output and American English IDS. Though the corner vowels hyperarticulate in the teaching data, American English IDS corner vowels hyper-articulate more uniformly [see 2, 7] than the teaching data, which exhibit most hyper-articulation in /A/. In general, the phonemes in the teaching data move away from the interior of the vowel space in the F1-F2 plane, while McMurray et al. [9] observed that /\u00c7/ and /\u00e6/ moved toward the interior.2 Cristia & Seidl\n2We assume McMurray et al. [9] focused on native American English speakers though they only specify that\n[7] observed that the F1-F2 distance between the /i/-/I/ pair did not change (or hypo-articulated, depending on the measure) from ADS to IDS. Given these discrepancies, our analysis cannot be taken on its own to provide conclusive evidence that IDS is optimized for teaching. It does, however, motivate further investigation of previous findings in the literature that have been presented as evidence against IDS serving a teaching function."}, {"heading": "Effect on learning", "text": "Earlier we argued that the benefit of teaching data is not a strict indication of its pedagogical intent\u2014the implication being that finding that human IDS does or does not improve the performance of some learning algorithm is not, on its own, evidence that IDS is or is not meant to teach. This raises the question of why we should bother investigating learning at all. Certain patterns of learning behavior may be indicative of the presence or absence of pedagogical intent if they are consistent or inconsistent with the predictions of the theory. In this section we venture to identify\nparticipants were \u201cfrom the Ripon, WI area\u201d and \u201call were Caucasian and lived in homes where English was the primary language\u201d (p. 366).\nsuch patterns. We explore the benefit of the simulated teaching data to several classes of learner, focusing on classification of IDS and ADS data, as well as the effect training on IDS data has on future classification of ADS data. We also investigate how learning performance changes when learning from specific subsets of formants and as a function of sample size.\nWe first evaluated whether the simulated teaching data, with their unintuitive pedagogical properties, are detrimental to learners\u2019 ability to classify example phonemes. We will first evaluate learning performance under several learning models: logistic regression [9], support vector machines (SVM) with linear kernels, expectation-maximization on Gaussian mixture models (GMM) [8], and the Dirichlet process Gaussian Mixture model (DPGMM; the learner model outlined above, and used as the basis for generating the teaching data). We used the scikit-learn [36] implementation for each algorithm except DPGMM, which we implemented using the standard sequential Gibbs sampling algorithm [37, Algorithm 3] coupled with intermittent split-merge transitions [38], which improves mixing by allowing the Markov Chain to more easily move between modes in the probability distribution.\nEach algorithm classified, in batch, random subsets of the teaching data and sets of ADS data randomly generated from the empirical distribution.3 Each set of data consisted of 500 examples of each phoneme (6000 data points total). Each algorithm classified 500 sets of ADS data and 500 sets of teaching data. Logistic regression and SVM, which must first fit a model to labeled data, were provided an identically sized set of different training data and the GMM was provided with the correct number of categories. The DPGMM\u2019s prior distribution was identical to the teacher\u2019s. The choice of prior is important; the patterns of movement (hyper- and hypo-articulation and variance\n3As researchers, we acknowledge that human learning does not happen in batch, but over time from sequential examples. Sequential Monte Carlo [SMC; see 21] algorithms are designed to handle exactly these problems, but to evaluate sequential learning we must make assumptions about the sequence in which examples arrive. In the absence of a reasonable assumption about the order of examples we must marginalize (enumerate and average) over the N ! possible orders, which is computationally intractable.\nincrease) depend on the prior assumed by the teacher (the teacher chooses data to teach a learner with a certain prior), hence the benefit of patterns of movement to the learner depend on the level of agreement between the teacher\u2019 assumed prior and the learner\u2019s prior. We evaluated the DPGMM based on its inferred assignment at the 500th simulation step. We also evaluated the transfer of learning from teaching data to ADS by having each algorithm classify ADS data after having learned a model from teaching data. This transfer condition can be thought of as a simulation of the transfer of IDS to ADS. While this has not been evaluated in previous analyses of IDS, it is the critical condition for determining whether IDS helps learners acquire normal speech. Similarity between each algorithm\u2019s inferred category assignments and the correct category assignments was evaluated via the adjusted Rand Index [ARI, see 39]. The ARI offers a measure of similarity between categorizations in circumstances in which it does not make sense to count the number of correct categorizations (i.e. to count the number of times items with label z are assigned to category z). It makes sense to use counting with logistic regression and SVM because these algorithms fit models given labeled training data and are then used to explicitly label new examples. The GMM, however, is only provided with the number of categories and does not care about their labels; a GMM can label k categories k! different ways. And in addition to not caring about labels, the DPGMM is not guaranteed to have the same number of categories as the true distributions. We use ARI to evaluate all four models. ARI is provided two partitions of data into categories: the true partition, which is part of the target model; and the inferred partition, which is generated by the learning algorithm. As an example, the partition [1, 2, 3, 3], of four data into three categories implies that datum one belongs to category one, datum two belongs to category two, and data three and four belong to category three. ARI takes on values from -1 to 1 with expected value 0, and assumes the value 1 when the two partitions of stimuli into categories are identical (disregarding labels). For two partitions U and V of N data points into i and j categories, ARI is computed as follows:\nARI =\n\u2211 ij ( nij 2 ) \u2212 [ \u2211 i ( ai 2 )\u2211 j ( bj 2 ) ]/ ( N 2 ) 1 2 [ \u2211 i ( ai 2 ) + \u2211 j ( bj 2 ) ]\u2212 [ \u2211 i ( ai 2 )\u2211 j ( bj 2 ) ]/ ( N 2\n) . (3) where nij is the number of datapoints assigned to i in U and j in V, ai is the sum \u2211 j nij , and bj is\nthe sum \u2211 i nij . ARI is an adjusted-for-chance version of the Rand Index [40], which is a normalized sum of the number of pairs of data points that are assigned to the same category in U and the same category in V, and the number of data points that are assigned to different categories in U and different categories in V.\nFigure 4 (top row) shows that the teaching data (dark) lead to improved classification over\nADS (light) data in each of the algorithms we tested. Of the four algorithms, DPGMM performs the worst on the ADS data. This is unsurprising because of the four algorithms, DPGMM has the most to learn. However, DPGMM outperforms GMM on the teaching data. On the full, threeformant data, Logistic regression, SVM, and GMM all perform worst in the transfer condition (gray) compared with the ADS-only and teaching-data-only conditions, while the target learner (DPGMM) classifies ADS data better after having learned from the teaching data. These results show that the teaching data are themselves more classifiable than ADS and improve classification of ADS, in this case, only for the class of learner for which they were intended: the class of learner which must learn the number of phonetic categories. The transfer result is of particular importance and suggests that data that are statistically very different from data generated directly by the true concept can improve learning of the true concept. The real-world implication of this finding is that early learning from IDS may improve future ADS comprehension.\nWe see that many of the induced ARI distributions in Figure 4 are multimodal. Two-sample Kolmogorov-Smirnov (KS) tests indicates that the distribution of ARI given three-formant ADS and teaching data differ under each algorithm; the statistic for each is significant at the p < 10\u221240 level (see Table 2).4 The categorization outcome differs when the three-formant data are projected onto\n4We use the notation KSLOGIT (500, 500) = 0.668 to denote that the resulting statistic of a two-sample\nthe F1-F2 plane (see Figure 4 bottom row). Categorization performance generally decreases when F3 is removed. More features (dimensions) provide learners with more information by which they can form categories. For example, in Figure 1b and c we see that locating and categorizing /\u00c7/ (as in Bert) becomes trivial given F3.\nIn the previous paragraphs we demonstrated that the simulated teaching data are indeed beneficial to several classes of learners. It is important to note that these learners benefited from sets of data consisting of a fixed number (500) of examples per phoneme.\nLearning as a function of the number of examples\nHere we investigate how this benefit changes as the number of examples increases or decreases by investigating the effect of the number of examples per phoneme on the classification ability of the target learner (DPGMM). The DPGMM classified 128 random sets of data comprising 2, 4, 8, 16, . . . , 2048 examples of each phoneme. The results can be seen in Figure 5. The behavior induced in the DPGMM by the ADS (light) and Teaching (dark) data differ. Adding ADS data appears not to benefit the learner between about 32 and 256 examples per phoneme while adding teaching data continues to improve categorization at an approximately logarithmic rate. This suggest that the benefits of IDS to learners may not be apparent from a small number of data points and that researchers may benefit from comparing learning performance as a function of the number of data points. Learning under ADS begins to improve again after 512 examples, while the benefit of adding ADS examples decreases; and at 2048 examples per phoneme the transfer of IDS results in mean performance similar to ADS. Teaching data are intended to be efficient, thus they should improve learning over random data given a smaller number of examples. If the number of examples is too small, learning is difficult regardless of the data\u2019s origin; if the number of examples is sufficiently large, teaching data offer no benefit over random data.\nKolmogorov-Smirnov test on two samples, both containing 500 data points, equals 0.668\nHypoarticulation and increasing variance to teach\nIt may be obvious why a teacher would hyper-articulate examples, but the pedagogical usefulness of hypo-articulation and variance increase deserves discussion. Keep in mind that the teacher seeks to increase the likelihood of a globally correct inference. Hypo-articulation can improve categorization when it is the result of disambiguating movement\u2014that is, movement of one cluster away from another cluster it may be mistaken with. Increased variability can be used to mitigate any negative affects of hypo-articulation by making close or overlapping clusters more distinguishable from each other. Imagine two very closely overlapping, circular clusters: examples from these clusters may appear to come from one large cluster. If we wish to express that there are two clusters we could stretch each cluster perpendicularly so the resulting data manifest as an \u2018X\u2019 rather than a single Gaussian blob; indeed, the teaching model produces this behavior. The teaching data offer similar examples of how hypo-articulation and increased variability, when employed systematically, do not necessarily reduce learning. For purposes of clarity, we shall look only at the F1-F2 plane (Figure 1a). The phonemes (/\u00c7/; /u/; /U/, as in put; /o/, as in boat) are difficult to distinguish in AD speech. In the teaching data, /u/, /U/, and /o/ are pressed into each other (hypo-articulated) which makes /\u00c7/ more distinguishable. The corner vowel /u/ greatly increases its F2 variance and decreases its F1-F2 covariance and /o/ greatly increases its F1 variance. This causes /o/ and /u/ to overlap through each other. Their tails then emerge conspicuously from the main mass of examples which makes them more identifiable. The hypoarticulation and directional changes in variance reduce the muddling effect of general increases in within-phoneme variance. Looking at the categorization performance of this subset of the flattened data shows that different algorithms come to different conclusions as to which data are better for learning (we chose categorization results on 500 examples per phoneme). SVM performs better on the ADS data (MADS = 0.431,MTeach = 0.403;KS(500, 500) = 0.716, p < 0.001; d = 2.019)\nand logistic regression performs similarly on ADS and teaching data (MADS = 0.294,MTeach = 0.292;KS(500, 500) = 0.070, p = 0.166; d = 0.109). GMM performs better on the teaching data (MADS = 0.347,MTeach = 0.353;KS(500, 500) = 0.184, p < 0.001; d = \u22120.301), as does DPGMM (MADS = 0.275,MTeach = 0.283;KS(500, 500) = 0.14, p < 0.001; d = \u22120.231). These result show first, that hypo-articulation and increased variance do not necessarily damage local inferences in the target model (DPGMM); and second, that looking at categorical subsets of teaching data may lead to conflicting conclusions from different learning algorithms with respect to the benefit of data to learners."}, {"heading": "Discussion", "text": "In this paper we have explored the question of whether IDS is for teaching. We rigorously defined both the learning and teaching problems in a psychologically-valid, probabilistic theory. Using this theory, we generated data designed to teach a subset of the phonetic category model of adult speech to naive, infant-like learners using the F1, F2, and F3 formants. In the process, we have identified, concretely demonstrated, and provided possible solutions to a number of issues in the existing literature. We address each in turn. We then conclude by noting the positive results of our analysis, limitations of our results, and recommendations for future research. First, the existing literature has relied on intuitive arguments regarding which features of IDS may or may not be desirable. Hyper-articulation (expansion) of the corner vowels has been identified as a feature that would facilitate learning. However, hypo-articulation such as observed between /I/ and /i/ by Cristia & Seidl [7], and increases in variance of categories such as /\u00e6/ and /\u00c7/ observed by McMurray et al. [9], have been argued to impede learning. Our results show that, when considered in aggregate, hypo-articulation and increases in variance are indeed consistent with teaching. Our analysis leads to predictions about when and why one may see these surprising properties. Hypo-articulation appears when vowels move away from more confusable alternatives. To compensate for this, hypo-articulated categories appear in conjunction with hyper-articulation on other formant dimensions (F3) and/or increases in (co)variance that would facilitate the learner\u2019s inference that there is more than one category present. /o/ and /u/ are hypo-articulated in F1\u00d7F2, but are hyper-articulated in F1\u00d7F2\u00d7F3. Both of these phonemes increase their F1 and F2 variance, but /o/ increases its F1-F2 covariance while /u/ decreases its F1 \u2212 F2 covariance, which causes the two phonemes to become more conspicuous by overlapping through each other. Thus, our results show that researchers\u2019 intuitive theories of which features of IDS are beneficial for teaching are contradicted by a more precise, computational analysis of teaching phoneme categories. Second, existing computational approaches have attempted to assess teaching indirectly through improvements in learning using various, very different, computational models and have assessed the benefits of learning from IDS with transfer to IDS. We have argued that the existing models make unreasonable assumptions about the problem faced by the learner. Specifically, models assume that infants either know the number of phonemes in their language a priori [8] or that the data they receive is accompanied by correct labels [9]. Prima facie, these assumptions are too strong. The problem the learner faces includes learning the number of categories. Analyses based on this problem lead to consequential differences in results. Learners who face the problem of learning the number of categories show positive effects of transfer from the simulated teaching data to ADS, while algorithms that assume labeled data or a known number of categories do not (see Figure 4). Our results based on more realistic assumptions about the learning problem contradict previous conclusions in the literature. Third, the literature tends to focus attention on subsets of the data, both in terms of the vowels and the formants considered for any given analysis. Both empirical and computational analyses tend to focus on subsets of IDS. Rather than measuring F1, F2 and F3, many analyses rely only on F1 and F2. Similarly, rather than recording data for all vowel categories, results tend to focus on subsets that are relevant to intuitively derived qualitative predictions. Our results show that predictions for teaching depend on knowledge of both of these aspects of context, and thus interpretation of\nempirical results do as well. As illustrated in Figure 2, hypo-articulation cannot be determined from F1 and F2 alone; the vowels may be separated on F3. In fact, rhotic vowels such as /\u00c7/ and /Ar/ (as in start) are characterized by low F3 frequencies. Similarly, hypo-articulation may be accompanied by increases in variance, which optimize the learner\u2019s ability to infer the existence of more than one category. Thus, our results show that more comprehensive data are necessary to develop accurate computational models and interpret empirical results. Our results are based on the Hillenbrand et al. [24] data, which do not include many of the interior and rhotic vowels use in other studies [7, 9]. Because our results show that quantitative predictions are sensitive to the specifics of context, we do not expect a perfect match to the behavioral data. As we noted, the trends in the simulated teaching data did not exactly match trends others have reported in human IDS. The vowels /\u00c7/ and /\u00e6/ did not exhibit the interior movement reported by McMurray et al. [9], nor did /i/ and /I/ exhibit F1-F2 hypoarticulation as reported by Cristia & Seidl [7]. The qualitative implications of our analysis are more powerful as a consequence: these points illustrate the need for more comprehensive data sets to ensure progress in the debate. Building on previous computational models of teaching, we have introduced an approach that may allow direct assessment of whether IDS is intended to teach. The analyses presented here suggest that surprising features identified by researchers are indeed predicted by the model and that IDS is indeed effective for teaching ADS categories provided one assumes a realistic model of learning. Our results also highlight challenges for research investigating the purpose of IDS. Implicit in this problem is thus a dependence of teaching data on assumptions of what is being taught. Indeed, this dependence on the set of alternatives is likely what makes desirable features tricky to intuit. If IDS is only for teaching phonetic categories, a more complete set of phonemic data is necessary. Though we derived our target phonetic category model from a fairly extensive data set, we hardly encompass the full category model of American English, and may also differ in the perceptual dimensions we assume.5 We lack many of the interior vowels investigated by other researchers [see 7, 9]. However, it possible that IDS may be optimized for teaching a larger subset of language. Indeed, research has shown that IDS improves word segmentation [41], word recognition [42], and label learning [43]. Though daunting, our results highlight the need to systematically consider these alternatives. Our approach, in which we consider categories defined over F1 and F2 versus F1, F2 and F3, can be viewed as a modest start in that direction. With such computational models in hand, it becomes an empirical question, albeit one that requires more comprehensive data than we currently have available. Another concern that has not yet been addressed in the literature is differences in learning from individual caregivers and from aggregated data from multiple caregivers. Computational research has sought to answer the question of how people solve inference problems that are computationally intractable, positing that people use approximations [21]. If this is the case, it is reasonable to assume that different caregivers will arrive at different solutions through stochastic search (e.g. Markov chain Monte Carlo). The distribution of teaching data is highly multi-modal and Markov Chains often find themselves stuck in local maxima. Pilot research suggest data from single chains is far more beneficial to learners than the data aggregated over chains\u2014perhaps due to lower within-phoneme variability compared with aggregated data. We use the aggregated data because it represents the correct probabilistic solution, however because infants are exposed to only a few primary speakers, the literature\u2019s tendency to make comparisons over many individuals may misrepresent the problem [see 44, for a detailed discussion on how language learners may handle inter-speaker variability]. This work is also relevant to the articulation literature, where the theoretical underpinnings of speakers\u2019 speech manipulations are under debate [see 45]. The teaching model, coupled with a temporal model of articulation, could predict hyper- or hypo-articulation, and duration increases or decreases. Temporal effects that are explained in terms of a number of heuristics such as planning economy, phonetic neighborhood density, or binary-feature-based addressee-driven attenuation [46\u2013\n5Additionally, phonemes in Hillenbrand et al. [24] were measured only from words beginning with an \u2018h\u2019 and ending with a \u2018d\u2019 e.g., /A/, /i/, and /u/ were taken only from the words \u2018hod\u2019, \u2018heed\u2019, and \u2018who\u2019d\u2019 respectively.\n48], may in fact be consistent with pedagogical manipulation (as explicitly suggested by Lindblom [46], Jaeger [49]). Related research indicates that speakers adapt when their communications are unsuccessful [50\u201352]. However, until the scaling of the teaching model is improved, the problem of temporal articulation will be unapproachable."}, {"heading": "Conclusion", "text": "Increasingly, research has highlighted ways in which other people may affect learning [22, 23, 53, 54]. The problem of language, viewed as statistical learning, is in principle no different. Research has shown that people systematically vary their speech to different targets, with infant directed speech being a canonical example. It is natural to ask, why. Is it for teaching? We have argued that precise formalization of these hypotheses is a necessary step toward the answer. Building off work in social learning, our computational model of teaching phonemes illustrates limitations in the existing literature. Our approach also points a way forward, through collection of more comprehensive datasets, and development of computational accounts that more accurately reflect the problems faced by learners and hypotheses posited by researchers.\nReferences\n1. Pegg, J., Werker, J. & McLeod, P. Preference for Infant-directed over adult-directed speech: Evidence from 7-week-old infants. Infant Behavior and Development 15, 325\u2013345 (1992).\n2. Kuhl, P. K. et al. Cross-Language Analysis of Phonetic Units in Language Addressed to Infants. Science 277, 684\u2013686 (1997).\n3. Burnham, D., Kitamura, C. & Vollmer-Conna, U. What\u2019s new, pussycat? On talking to babies and animals. Science (New York, N.Y.) 296, 1435 (2002).\n4. Uther, M., Knoll, M. & Burnham, D. Do you speak E-NG-L-I-SH? A comparison of foreignerand infant-directed speech. Speech Communication 49, 2\u20137 (2007).\n5. Gergely, G., Egyed, K. & Kira\u0301ly, I. On pedagogy. Developmental science 10, 139\u201346 (2007).\n6. Kirchhoff, K. & Schimmel, S. Statistical properties of infant-directed versus adult-directed speech: Insights from speech recognition. The Journal of the Acoustical Society of America 117, 2238 (2005).\n7. Cristia, A. & Seidl, A. The hyperarticulation hypothesis of infant-directed speech. Journal of Child Language, 1\u201322 (2013).\n8. De Boer, B. & Kuhl, P. K. Investigating the role of infant-directed speech with a computer model. Acoustics Research Letters Online 4, 129 (2003).\n9. McMurray, B., Kovack-Lesh, K., Goodwin, D. & McEchron, W. Infant directed speech and the development of speech perception: enhancing development or an unintended consequence? Cognition 129, 362\u201378 (2013).\n10. Martin, A. et al. Mothers speak less clearly to infants: A comprehensive test of the hyperarticulation hypothesis. Psychological science, 1\u20137 (2015).\n11. Bard, E. G. & Anderson, A. H. The unintelligibility of speech to children. Journal of Child Language 10, 265\u2013292 (1983).\n12. Bard, E. G. & Anderson, A. H. The unintelligibility of speech to children: Effects of referent availability. Journal of Child Language 21, 623\u2013648 (1994).\n13. Vallabha, G. K., McClelland, J. L., Pons, F., Werker, J. F. & Amano, S. Unsupervised learning of vowel categories from infant-directed speech. Proceedings of the National Academy of Sciences of the United States of America 104, 13273\u20138 (2007).\n14. Feldman, N. H., Griffiths, T. L., Goldwater, S. & Morgan, J. L. A role for the developing lexicon in phonetic category acquisition. Psychological review 120, 751 (2013).\n15. McMurray, B., Aslin, R. N. & Toscano, J. C. Statistical learning of phonetic categories: insights from a computational approach. Developmental science 12, 369\u201378 (2009).\n16. Shafto, P. & Goodman, N. D. in Proceedings of the Thirtieth Annual Conference of the Cognitive Science Society (2008).\n17. Shafto, P., Goodman, N. D. & Griffiths, T. L. A rational account of pedagogical reasoning: Teaching by, and learning from, examples. Cognitive psychology 71C, 55\u201389 (2014).\n18. Anderson, J. The adaptive nature of human categorization. Psychological Review 98, 409 (1991).\n19. Escobar, M. D. & West, M. Bayesian density estimation and inference using mixtures. 90, 577\u2013588 (1995).\n20. Rasmussen, C. The infinite Gaussian mixture model. Advances in neural information processing, 554\u2013560 (2000).\n21. Sanborn, A. N., Griffiths, T. L. & Navarro, D. J. Rational approximations to rational models: alternative algorithms for category learning. Psychological review 117, 1144\u20131167 (2010).\n22. Bonawitz, E. et al. The double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery. Cognition 120, 322\u201330 (2011).\n23. Gweon, H., Shafto, P. & Schulz, L. E. in Proceedings of the 36th Annual Conference of the Cognitive Science Society (2014), 565\u2013570.\n24. Hillenbrand, J, Getty, L. a., Clark, M. J. & Wheeler, K. Acoustic characteristics of American English vowels. The Journal of the Acoustical Society of America 97, 3099\u2013111 (1995).\n25. Peterson, G. E. & Barney, H. L. Control Methods Used in a Study of the Vowels. Journal of the Acoustical Society of America 24, 175\u2013184 (1952).\n26. Apfelbaum, K. S. & McMurray, B. Relative cue encoding in the context of sophisticated models of categorization: Separating information from categorization. Psychonomic Bulletin and Review 22, 916\u2013943 (2015).\n27. McMurray, B. & Jongman, A. What information is necessary for speech categorization? Harnessing variability in the speech signal by integrating cues computed relative to expectations. Psychological Review 118, 219\u2013246 (2011).\n28. Monahan, P. J. & Idsardi, W. J. Auditory sensitivity to formant ratios: Toward an account of vowel normalisation. Language and Cognitive Processes 25, 808\u2013839 (2010).\n29. Peterson, G. E. Parameters of Vowel Quality. Journal of Speech and Hearing Research 4, 10\u201329 (1961).\n30. Jusczyk, P. W. in. Phonological Development: Models, Research, Implications (eds Ferguson, C. A., Menn, L. & Stoel-Gammon, C.) 17\u201364 (York, Timonium, MD, 1992).\n31. Nittrouer, S. The role of temporal and dynamic signal components in the perception of syllablefinal stop voicing by children and adults. Journal of the Acoustical Society of America 115, 1777\u20131790 (2004).\n32. Miller, J. D. Auditory-perceptual interpretation of the vowel. Journal of the Acoustical Society of America 85, 2114\u20132134 (1989).\n33. Cole, J., Linebaugh, G., Munson, C. M. & McMurray, B. Unmasking the acoustic effects of vowel-to-vowel coarticulation: A statistical modeling approach. Journal of Phonetics 38, 167\u2013 184 (2010).\n34. Gerstman, L. J. Classification of Self-Normalized Vowels. IEEE Transactions on Audio and Electroacoustics 16, 78\u201380 (1968).\n35. Lobanov, B. M. Classification of Russian Vowels Spoken by Different Speakers. Journal of the Acoustical Society of America 49, 606\u2013608 (1971).\n36. Pedregosa, F. et al. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12, 2825\u20132830 (2011).\n37. Neal, R. M. Markov chain sampling methods for Dirichlet process mixture models. Journal of computational and graphical statistics 9, 249\u2013265 (2000).\n38. Jain, S. & Neal, R. M. A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model. Journal of Computational and Graphical Statistics 13 (2004).\n39. Hubert, L. & Arabie, P. Comparing partitions. Journal of classification 2, 193\u2013218 (1985).\n40. Rand, W. M. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association 66, 846\u2013850 (1971).\n41. Thiessen, E. D., Hill, E. a. & Saffran, J. R. Infant-Directed Speech Facilitates Word Segmentation. Infancy 7, 53\u201371 (2005).\n42. Singh, L., Nestor, S., Parikh, C. & Yull, A. Influences of Infant-Directed Speech on Early Word Recognition. Infancy 14, 654\u2013666 (2009).\n43. Graf Estes, K. & Hurley, K. Infant-Directed Prosody Helps Infants Map Sounds to Meanings. Infancy, n/a\u2013n/a (2012).\n44. Kleinschmidt, D. F. & Jaeger, T. F. Robust speech perception: Recognize the familiar, generalize to the similar, and adapt to the novel. Psychological Review 122, 148\u2013203 (2015).\n45. Jaeger, T. F. & Buz, E. in Handbook of Psycholinguistics (eds Fernandez, E. M. & Cairns, H. S.) (Wiley-Blackwell, accepted for publication).\n46. Lindblom, B. Explaining phonetic variation: a sketch of the H&H theory. Speech Production and Speech Modelling, 403\u2013439 (1990).\n47. Munson, B. & Solomon, N. P. The effect of phonological neighborhood density on vowel articulation. Journal of speech, language, and hearing research 47, 1048\u20131058 (2004).\n48. Galati, A. & Brennan, S. E. Attenuating information in spoken communication: For the speaker, or for the addressee? Journal of Memory and Language 62, 35\u201351 (2010).\n49. Jaeger, T. F. Production preferences cannot be understood without reference to communication. Frontiers in psychology 4, 1\u20134 (2013).\n50. Stent, A. J., Huffman, M. K. & Brennan, S. E. Adapting speaking after evidence of misrecognition: Local and global hyperarticulation. Speech Communication 50, 163\u2013178 (2008).\n51. Schertz, J. Exaggeration of featural contrasts in clarifications of misheard speech in English. Journal of Phonetics 41, 249\u2013263 (2013).\n52. Buz, E., Tanenhaus, M. K. & Jaeger, T. F. Dynamically adapted context-specific hyperarticulation: Feedback from interlocutors affects speakers\u2019 subsequent pronunciations. Journal of Memory and Language (in press).\n53. Gergely, G., Bekkering, H. & Kira\u0301ly, I. Rational imitation in preverbal infants. Nature 415, 755 (2002).\n54. Koenig, M. & Harris, P. L. Preschoolers mistrust ignorant and inaccurate speakers. Child development 76, 1261\u201377 (2005).\n55. Teh, Y. W., Jordan, M. I., Beal, M. J. & Blei, D. M. Hierarchical dirichlet processes. Journal of the american statistical association 101 (2006).\n56. Murphy, K. P. Conjugate Bayesian analysis of the Gaussian distribution tech. rep. (University of British Columbia, 2007).\n57. Hastings, W. Monte Carlo sampling methods using Markov chains and their applications. Biometrika 57, 97\u2013109 (1970).\n58. Roberts, G. O., Gelman, a. & Gilks, W. R. Weak convergence and optimal scaling of random walk Metropolis algorithms. Annals of Applied Probability 7, 110\u2013120 (1997).\nAppendix A Details of model\nHere we describe the mathematical details of the model. We construct the teaching model from the learning model."}, {"heading": "Learner model", "text": "We formalize phonetic category acquisition as learning an infinite Gaussian mixture model [GMM; see 18, 20]. A Gaussian mixture model comprises a set of k multidimensional Gaussian components \u03b8 = {{\u00b51,\u03a31}, . . . , {\u00b5k,\u03a3k}}, where \u00b5j and \u03a3j are the mean and covariance matrix of the jth mixture component; and an k-length vector of mixture weights \u03c0 = {\u03c01, . . . , \u03c0k}, where each \u03c0j is a positive real number and the set \u03c0 sums to 1. The likelihood of some data, X = {xi, . . . , xn}, under a GMM is the product of weighted sums,\nP (X|\u03b8, \u03c0) = n\u220f i=1 k\u2211 j=1 \u03c0jN (xi|\u00b5i,\u03a3i), (4)\nwhere N (x|\u00b5,\u03a3) is the Gaussian probability density function applied to x given \u00b5 and \u03a3. We are concerned with the case where the learner infers the assignment of data to categories rather than the component weights. We introduce a length n assignment vector Z = [z1, . . . , zn] where zi is an integer in 1, . . . , k representing to which component datum i is assigned. Because the assignment is explicit, we no longer sum over each component. The likelihood is then,\nP (X|\u03b8, Z) = n\u220f i=1 k\u2211 j=1 N (xi|\u00b5i,\u03a3i)\u03b4zi,j , (5)\nwhere \u03b4zi,j is the Kronecker delta function, which takes the value 1 if zi = j (data point xi is assigned to the jth category) and the value 0 otherwise. Learning is then a problem of inferring \u03b8 and Z. Prior distributions on individual components, {\u00b5j ,\u03a3j}, correspond to a learner\u2019s prior beliefs about the general location (\u00b5), and the size and shape (\u03a3) of categories. We assume that \u00b5j and \u03a3j are distributed according to Normal Inverse-Wishart (NIW). Though we made this choice primarily for mathematical convenience, priors of this and similar form have been used successfully to model human behavior [e.g. 14, 44]:\n\u00b5j ,\u03a3j \u223c NIW(\u00b50,\u039b0, \u03ba0, \u03bd0) \u2200 j \u2208 {1, . . . , k}, (6)\nwhich implies\n\u03a3j \u223c Inverse-Wishart\u03bd0(\u039b\u221210 ), (7)\n\u00b5j |\u03a3j \u223c N (\u00b50,\u03a3k/\u03ba0) \u2200 j \u2208 {1, . . . , k}, (8)\nwhere \u039b0 is the prior scale matrix, \u00b50 is the prior mean, \u03bd0 is the prior degrees of freedom, and \u03ba0 is the number of prior observations. For simulations, we chose vague prior parameters derived from the data.\n\u03bd0 = 3, (9)\n\u03ba0 = 1, (10)\n\u00b50 = 1\nN N\u2211 i=1 Xi, (11)\n\u039b0 = 1\nK K\u2211 k=1 \u03a3 (Xk) , (12)\nwhere \u03a3 (Xk) is the empirical covariance matrix of the adult data belonging to category k. The prior mean, \u00b50, is the mean over the entire data set, and the prior covariance matrix, \u039b0, is the average of each category\u2019s covariance matrix (see Table 1). To formalize inference over the number of categories, we introduce a prior on the partitioning of data points into components via the Chinese Restaurant Process [55], denoted CRP(\u03b1), where the parameter \u03b1 affects the probability of new components. Higher \u03b1 creates a higher bias toward new components. Data points are assigned to components as follows:\nP (zi = j|Z\u2212i, \u03b1) =\n{ nj\nn\u22121+\u03b1 if j \u2208 1 . . . k \u03b1\nn\u22121+\u03b1 if j = k + 1 , (13)\nwhere Z\u2212i is Z less entry i, k is the current number of components and nj is the number of data points assigned to component j. One is a minimally informative value of \u03b1 corresponding to a uniform weight over components. The standard learning problem involves recovering the true model, defined by \u03b8 and Z, from the data, X, (give any prior beliefs) according to Bayes\u2019 theorem,\nP (\u03b8, Z|X,\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1) = P (Z|\u03b1)P (\u03b8|\u00b50,\u039b0, \u03ba0, \u03bd0)P (X|\u03b8, Z)\nP (X|\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1) . (14)\nThe Normal Inverse-Wishart prior allows us to calculate the marginal likelihood, P (X|\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1), analytically [56], thus, for a small number of data points (the specific number being limited by one\u2019s computing power and patience; in our case, the number being thirteen or fewer) we can exactly calculate the above quantity via enumeration. Expanding the terms, the numerator is,\nP (Z|\u03b1)  k\u220f j=1 NIW(\u00b5j ,\u03a3j |\u00b50,\u039b0, \u03ba0, \u03bd0)  k\u220f j=1 N ({xi \u2208 X : Zi = j}|\u00b5j ,\u03a3j), (15)\nwhere the first term, P (Z|\u03b1), is the probability of Z under CRP(\u03b1); the second term is the prior probability of the parameters in each component under Normal Inverse-Wishart; and the third term is the (normal) likelihood of the data in each component given the component parameters. The denominator of Equation 14 is calculable by summing over all possible assignment vectors, {Z \u2208 Z}, and integrating over all possible component parameters,\nP (X|\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1) = \u2211 Z\u2208Z P (Z|\u03b1) kZ\u220f j=1 \u222b\u222b \u03b8 N ({xi \u2208 X : Zi = j}|\u03b8)NIW(\u03b8|\u00b50,\u039b0, \u03ba0, \u03bd0)d\u03b8(16)\n= \u2211 Z\u2208Z P (Z|\u03b1) kZ\u220f j=1 P ({xi \u2208 X : Zi = j}|\u00b50,\u039b0, \u03ba0, \u03bd0), (17)\nwhere kZ is the number of components in the assignment Z and P ({xi \u2208 X : Zi = j}|\u00b50,\u039b0, \u03ba0, \u03bd0) is the marginal likelihood of the set of data points in X assigned to component j in Z under a Normal likelihood with Normal Inverse-Wishart prior (this quantity is calculable in closed-form)."}, {"heading": "Teacher model", "text": "Optimal data for teaching are sampled from the distribution that leads learners to the correct inference and away from incorrect inferences [16, 17]. The teacher must consider the learner\u2019s inferences given all possible choices of data. Thus, we normalize over all possible data X,\nPopt(X|\u03b8, Z, \u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1) \u221d P (\u03b8, Z|X,\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1)\u222b\nX P (\u03b8, Z|X,\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1)dX\n, (18)\n= P (Z|\u03b1)P (X|\u03b8,Z)P (\u03b8|\u00b50,\u039b0,\u03ba0,\u03bd0) P (X|\u00b50,\u039b0,\u03ba0,\u03bd0,\u03b1)\u222b\nX P (X|\u03b8,Z)P (\u03b8|\u00b50,\u039b0,\u03ba0,\u03bd0)P (Z|\u03b1) P (X|\u00b50,\u039b0,\u03ba0,\u03bd0,\u03b1) dX . (19)\nThe term,\nP (\u03b8, Z|X,\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1) = P (X|\u03b8, Z)P (\u03b8|\u00b50,\u039b0, \u03ba0, \u03bd0)P (Z|\u03b1)\nP (X|\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1) , (20)\nis the posterior probability of the true hypothesis given the data\u2014the learner\u2019s inference. The learner\u2019s inference over alternative hypotheses is captured by the marginal likelihood of the data, P (X|\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1). The teacher\u2019s optimization of the choice of data is captured by the normalizing constant, \u222b\nX\nP (\u03b8, Z|X,\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1)dX. (21)\nWe avoid the need to calculate this quantity directly by sampling from Popt using the Metropolis algorithm (Hastings [57], see Appendix B) according to the acceptance probability,\nA(X \u2032|X) = min [ 1, P (X \u2032|\u03b8, Z)P (X|\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1) P (X|\u03b8, Z)P (X \u2032|\u00b50,\u039b0, \u03ba0, \u03bd0, \u03b1) ] . (22)\nAppendix B Algorithm for generating samples\nThe normalizing constant in Equation 2 (also Equation 21 in Appendix A) is analytically intractable. We use the Metropolis-Hastings algorithm to sample from the distribution of teaching data without having to calculate the normalizing constant [57]. The Metropolis-Hastings algorithm can be applied to draw samples from a probability distribution with density p : x\u2192 R+ when p can be calculated up to a constant. That is, when there exists a function f(x), where p(x) = cf(x) and c is a constant. A proposal distribution, q(x\u2032|x), is defined that proposes new samples, x\u2032, given the current sample, x. Beginning with a sample, x, a proposed sample, x\u2032, is drawn from q. The acceptance ratio, A, is calculated from f and q,\nA = f(x\u2032)q(x|x\u2032) f(x)q(x\u2032|x) . (23)\nIt is easy to see that\nf(x\u2032)q(x|x\u2032) f(x)q(x\u2032|x) = cf(x\u2032)q(x|x\u2032) cf(x)q(x\u2032|x) = p(x\u2032)q(x|x\u2032) p(x)q(x\u2032|x) . (24)\nIf q is symmetric, that is q(x\u2032|x) = q(x|x\u2032) for all x, x\u2032, then q(x|x \u2032)\nq(x\u2032|x) (the Hastings ratio) cancels from\nthe equation, leaving,\nA = f(x\u2032)\nf(x) , (25)\nfrom which we calculate the probability with which x\u2032 is accepted,\nP (x\u2032|x) = min [1, A] . (26)\nTo sample from the distribution of teaching data using the Metropolis algorithm, we calculate the numerator of Equation 2 exactly via enumeration and propose symmetric Gaussian perturbations to resample data. The acceptance probability is thus,\nP (X \u2032|X) = min [ 1, P (X \u2032|Z,\u00b5,\u03a3)P (X) P (X|Z,\u00b5,\u03a3)P (X \u2032) ] . (27)\nFor the simulations, the sampler simulated one datapoint for each phoneme (twelve total). X comprised twelve data points, one for each phoneme. X was initialized by sampling data from the prior parameters, that is X0 \u223c N(\u00b50,\u039b0/\u03ba0) (see Appendix A). At each iteration, new data, X \u2032, were generated from X by adding Gaussian noise distributed N(0, 40). This proposal distribution was chosen so that the acceptance rate of X \u2032 was near the optimal value of 0.23 [58]. X \u2032 was then accepted according to Equation 27. The final data comprise samples from 10 independent runs to the sampler. The first 500 samples of each run were discarded, then each 20th sample was collected until 1000 samples had been collected. The full set of data thus contains 10,000 total samples of twelve data points each (one for each of the twelve phonemes) for a total of 120,000 examples. Aggregating data over speakers is common practice in the IDS literature; we conduct analyses on data aggregated over independent runs of the sampler."}], "references": [{"title": "Preference for Infant-directed over adult-directed speech: Evidence from 7-week-old infants", "author": ["J. Pegg", "J. Werker", "P. McLeod"], "venue": "Infant Behavior and Development 15,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1992}, {"title": "Cross-Language Analysis of Phonetic Units in Language Addressed to Infants", "author": ["Kuhl", "P. K"], "venue": "Science 277,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "What\u2019s new, pussycat? On talking to babies and animals", "author": ["D. Burnham", "C. Kitamura", "U. Vollmer-Conna"], "venue": "Science (New York, N.Y.) 296,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Do you speak E-NG-L-I-SH? A comparison of foreignerand infant-directed speech", "author": ["M. Uther", "M. Knoll", "D. Burnham"], "venue": "Speech Communication", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Statistical properties of infant-directed versus adult-directed speech: Insights from speech recognition", "author": ["K. Kirchhoff", "S. Schimmel"], "venue": "The Journal of the Acoustical Society of America", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "The hyperarticulation hypothesis of infant-directed speech", "author": ["A. Cristia", "A. Seidl"], "venue": "Journal of Child Language,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Investigating the role of infant-directed speech with a computer model", "author": ["B. De Boer", "P.K. Kuhl"], "venue": "Acoustics Research Letters Online 4,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Infant directed speech and the development of speech perception: enhancing development or an unintended consequence", "author": ["B. McMurray", "K. Kovack-Lesh", "D. Goodwin", "W. McEchron"], "venue": "Cognition 129,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Mothers speak less clearly to infants: A comprehensive test of the hyperarticulation hypothesis", "author": ["A Martin"], "venue": "Psychological science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "The unintelligibility of speech to children", "author": ["E.G. Bard", "A.H. Anderson"], "venue": "Journal of Child Language 10,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1983}, {"title": "The unintelligibility of speech to children: Effects of referent availability", "author": ["E.G. Bard", "A.H. Anderson"], "venue": "Journal of Child Language", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Unsupervised learning of vowel categories from infant-directed speech", "author": ["G.K. Vallabha", "J.L. McClelland", "F. Pons", "J.F. Werker", "S. Amano"], "venue": "Proceedings of the National Academy of Sciences of the United States of America 104,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "A role for the developing lexicon in phonetic category acquisition", "author": ["N.H. Feldman", "T.L. Griffiths", "S. Goldwater", "J.L. Morgan"], "venue": "Psychological review 120,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Statistical learning of phonetic categories: insights from a computational approach", "author": ["B. McMurray", "R.N. Aslin", "J.C. Toscano"], "venue": "Developmental science 12,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "A rational account of pedagogical reasoning: Teaching by, and learning from, examples", "author": ["P. Shafto", "N.D. Goodman", "T.L. Griffiths"], "venue": "Cognitive psychology 71C,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "The adaptive nature of human categorization", "author": ["J. Anderson"], "venue": "Psychological Review 98,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["M.D. Escobar", "M. West"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "The infinite Gaussian mixture model", "author": ["C. Rasmussen"], "venue": "Advances in neural information processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Rational approximations to rational models: alternative algorithms for category learning", "author": ["A.N. Sanborn", "T.L. Griffiths", "D.J. Navarro"], "venue": "Psychological review 117,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "The double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery", "author": ["E Bonawitz"], "venue": "Cognition 120,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Acoustic characteristics of American English vowels", "author": ["J Hillenbrand", "Getty", "L. a", "M.J. Clark", "K. Wheeler"], "venue": "The Journal of the Acoustical Society of America", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "Control Methods Used in a Study of the Vowels", "author": ["G.E. Peterson", "H.L. Barney"], "venue": "Journal of the Acoustical Society of America", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1952}, {"title": "Relative cue encoding in the context of sophisticated models of categorization: Separating information from categorization", "author": ["K.S. Apfelbaum", "B. McMurray"], "venue": "Psychonomic Bulletin and Review 22,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "What information is necessary for speech categorization? Harnessing variability in the speech signal by integrating cues computed relative to expectations", "author": ["B. McMurray", "A. Jongman"], "venue": "Psychological Review 118,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Auditory sensitivity to formant ratios: Toward an account of vowel normalisation", "author": ["P.J. Monahan", "W.J. Idsardi"], "venue": "Language and Cognitive Processes", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Parameters of Vowel Quality", "author": ["G.E. Peterson"], "venue": "Journal of Speech and Hearing Research", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1961}, {"title": "Phonological Development: Models, Research, Implications", "author": ["Jusczyk", "P.W. in"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1992}, {"title": "The role of temporal and dynamic signal components in the perception of syllablefinal stop voicing by children and adults", "author": ["S. Nittrouer"], "venue": "Journal of the Acoustical Society of America 115,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Auditory-perceptual interpretation of the vowel", "author": ["J.D. Miller"], "venue": "Journal of the Acoustical Society of America", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1989}, {"title": "Unmasking the acoustic effects of vowel-to-vowel coarticulation: A statistical modeling approach", "author": ["J. Cole", "G. Linebaugh", "C.M. Munson", "B. McMurray"], "venue": "Journal of Phonetics", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Classification of Self-Normalized Vowels", "author": ["L.J. Gerstman"], "venue": "IEEE Transactions on Audio and Electroacoustics 16,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1968}, {"title": "Classification of Russian Vowels Spoken by Different Speakers", "author": ["B.M. Lobanov"], "venue": "Journal of the Acoustical Society of America", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1971}, {"title": "Scikit-learn: Machine Learning in Python", "author": ["F Pedregosa"], "venue": "Journal of Machine Learning Research", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Markov chain sampling methods for Dirichlet process mixture models", "author": ["R.M. Neal"], "venue": "Journal of computational and graphical statistics", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model", "author": ["S. Jain", "R.M. Neal"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["W.M. Rand"], "venue": "Journal of the American Statistical association", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1971}, {"title": "Infant-Directed Speech Facilitates Word Segmentation", "author": ["E.D. Thiessen", "Hill", "E. a", "J.R. Saffran"], "venue": "Infancy 7,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "Influences of Infant-Directed", "author": ["L. Singh", "S. Nestor", "C. Parikh", "A. Yull"], "venue": "Speech on Early Word Recognition. Infancy", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Infant-Directed Prosody Helps Infants Map Sounds to Meanings", "author": ["K. Graf Estes", "K. Hurley"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Robust speech perception: Recognize the familiar, generalize to the similar, and adapt to the novel", "author": ["D.F. Kleinschmidt", "T.F. Jaeger"], "venue": "Psychological Review 122,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Explaining phonetic variation: a sketch of the H&H theory", "author": ["B. Lindblom"], "venue": "Speech Production and Speech Modelling,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1990}, {"title": "The effect of phonological neighborhood density on vowel articulation", "author": ["B. Munson", "N.P. Solomon"], "venue": "Journal of speech, language, and hearing research", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2004}, {"title": "Attenuating information in spoken communication: For the speaker, or for the addressee", "author": ["A. Galati", "S.E. Brennan"], "venue": "Journal of Memory and Language 62,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Production preferences cannot be understood without reference to communication", "author": ["T.F. Jaeger"], "venue": "Frontiers in psychology 4,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "Adapting speaking after evidence of misrecognition: Local and global hyperarticulation", "author": ["A.J. Stent", "M.K. Huffman", "S.E. Brennan"], "venue": "Speech Communication", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2008}, {"title": "Exaggeration of featural contrasts in clarifications of misheard speech in English", "author": ["J. Schertz"], "venue": "Journal of Phonetics", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Rational imitation in preverbal infants", "author": ["G. Gergely", "H. Bekkering", "I. Kir\u00e1ly"], "venue": "Nature 415,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2002}, {"title": "Preschoolers mistrust ignorant and inaccurate speakers", "author": ["M. Koenig", "P.L. Harris"], "venue": "Child development 76,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2005}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the american statistical association", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2006}, {"title": "Conjugate Bayesian analysis of the Gaussian distribution tech", "author": ["K.P. Murphy"], "venue": "rep. (University of British Columbia,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2007}, {"title": "Monte Carlo sampling methods using Markov chains and their applications", "author": ["W. Hastings"], "venue": "Biometrika 57,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1970}], "referenceMentions": [{"referenceID": 0, "context": "Infants are able to distinguish IDS from normal, adult-directed speech (ADS) and prefer IDS over ADS [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "[2] found that IDS has unusual formant-level properties.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Corner vowel hyper-articulation is present in speech to infants but not speech to pets [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "Additionally, corner vowel hyper-articulation is found in speech to foreigners [4], which, outwardly, sounds more like normal, adult speech.", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "Other, non-corner vowels are hypo-articulated (closer together) in IDS [6, 7] and withinphoneme variability increases for some vowels [8, 9].", "startOffset": 71, "endOffset": 77}, {"referenceID": 5, "context": "Other, non-corner vowels are hypo-articulated (closer together) in IDS [6, 7] and withinphoneme variability increases for some vowels [8, 9].", "startOffset": 71, "endOffset": 77}, {"referenceID": 6, "context": "Other, non-corner vowels are hypo-articulated (closer together) in IDS [6, 7] and withinphoneme variability increases for some vowels [8, 9].", "startOffset": 134, "endOffset": 140}, {"referenceID": 7, "context": "Other, non-corner vowels are hypo-articulated (closer together) in IDS [6, 7] and withinphoneme variability increases for some vowels [8, 9].", "startOffset": 134, "endOffset": 140}, {"referenceID": 8, "context": "[10] found that temporally sequential pairs of vowel phonemes are less discriminable in IDS than in ADS.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Over the course of the debate about the role of IDS in language learning, researchers have attempted to quantitatively evaluate the benefit of IDS to learners by comparing the outcome of different learning algorithms given IDS and ADS data [6, 8, 9].", "startOffset": 240, "endOffset": 249}, {"referenceID": 6, "context": "Over the course of the debate about the role of IDS in language learning, researchers have attempted to quantitatively evaluate the benefit of IDS to learners by comparing the outcome of different learning algorithms given IDS and ADS data [6, 8, 9].", "startOffset": 240, "endOffset": 249}, {"referenceID": 7, "context": "Over the course of the debate about the role of IDS in language learning, researchers have attempted to quantitatively evaluate the benefit of IDS to learners by comparing the outcome of different learning algorithms given IDS and ADS data [6, 8, 9].", "startOffset": 240, "endOffset": 249}, {"referenceID": 6, "context": "de Boer & Kuhl [8] found that a mixture model trained using the expectation-maximization algorithm was better able to recover the means of IDS corner vowel categories from IDS data than it was to recover the means of ADS corner vowel categories from ADS data.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "Kirchhoff & Schimmel [6] explored the usefulness of IDS to training Bayesian automatic speech recognition systems (ASR), finding that the IDS-trained ASR classified certain types of data more effectively than ADS-trained ASR and other types more poorly.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "[9] found that multinomial logistic regression trained on IDS data correctly classified fewer new IDS examples than its ADS-trained counterpart classified new ADS examples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "1Related but orthogonal work suggests that infant- and child-directed speech is less intelligible to adults [11, 12].", "startOffset": 108, "endOffset": 116}, {"referenceID": 10, "context": "1Related but orthogonal work suggests that infant- and child-directed speech is less intelligible to adults [11, 12].", "startOffset": 108, "endOffset": 116}, {"referenceID": 4, "context": "In their work on automatic speech recognition, Kirchhoff & Schimmel [6] posed the question of what is being learned from IDS.", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "If IDS is for teaching then what does IDS teach? While it is typically implied that the intent would be to teach normal speech, existing computational studies compare the effectiveness of IDS at teaching IDS with the effectiveness of ADS at teaching ADS [8, 9].", "startOffset": 254, "endOffset": 260}, {"referenceID": 7, "context": "If IDS is for teaching then what does IDS teach? While it is typically implied that the intent would be to teach normal speech, existing computational studies compare the effectiveness of IDS at teaching IDS with the effectiveness of ADS at teaching ADS [8, 9].", "startOffset": 254, "endOffset": 260}, {"referenceID": 6, "context": "Building on previous research formalizing phonetic categories, we adopt a Gaussian mixture model (GMM) framework [8, 13\u201315].", "startOffset": 113, "endOffset": 123}, {"referenceID": 11, "context": "Building on previous research formalizing phonetic categories, we adopt a Gaussian mixture model (GMM) framework [8, 13\u201315].", "startOffset": 113, "endOffset": 123}, {"referenceID": 12, "context": "Building on previous research formalizing phonetic categories, we adopt a Gaussian mixture model (GMM) framework [8, 13\u201315].", "startOffset": 113, "endOffset": 123}, {"referenceID": 13, "context": "Building on previous research formalizing phonetic categories, we adopt a Gaussian mixture model (GMM) framework [8, 13\u201315].", "startOffset": 113, "endOffset": 123}, {"referenceID": 14, "context": "Teaching data are by definition generated with the learner in mind [16, 17].", "startOffset": 67, "endOffset": 75}, {"referenceID": 6, "context": "Previous computational accounts of learning under IDS have evaluated learning in computational learners that know the correct number of categories [8] or learn from labeled data [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "Previous computational accounts of learning under IDS have evaluated learning in computational learners that know the correct number of categories [8] or learn from labeled data [9].", "startOffset": 178, "endOffset": 181}, {"referenceID": 15, "context": "Learning the nature and the number of categories simultaneously can be done using the Dirichlet process Gaussian Mixture Model (DPGMM) [18\u201321].", "startOffset": 135, "endOffset": 142}, {"referenceID": 16, "context": "Learning the nature and the number of categories simultaneously can be done using the Dirichlet process Gaussian Mixture Model (DPGMM) [18\u201321].", "startOffset": 135, "endOffset": 142}, {"referenceID": 17, "context": "Learning the nature and the number of categories simultaneously can be done using the Dirichlet process Gaussian Mixture Model (DPGMM) [18\u201321].", "startOffset": 135, "endOffset": 142}, {"referenceID": 18, "context": "Learning the nature and the number of categories simultaneously can be done using the Dirichlet process Gaussian Mixture Model (DPGMM) [18\u201321].", "startOffset": 135, "endOffset": 142}, {"referenceID": 14, "context": "We employ an existing model of teaching that has been used successfully to capture human learning in a variety of scenarios [16, 17, 22, 23], under which optimal teaching data derive from the inverse of the learning process.", "startOffset": 124, "endOffset": 140}, {"referenceID": 19, "context": "We employ an existing model of teaching that has been used successfully to capture human learning in a variety of scenarios [16, 17, 22, 23], under which optimal teaching data derive from the inverse of the learning process.", "startOffset": 124, "endOffset": 140}, {"referenceID": 20, "context": "[24]) by manipulating first, second, and third formant values.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Formants are known to correlate with vowel identity [24, 25], though the dimensions that listeners use when storing and categorizing sounds may be more complex than absolute encoding of formant frequencies [26\u201329].", "startOffset": 52, "endOffset": 60}, {"referenceID": 21, "context": "Formants are known to correlate with vowel identity [24, 25], though the dimensions that listeners use when storing and categorizing sounds may be more complex than absolute encoding of formant frequencies [26\u201329].", "startOffset": 52, "endOffset": 60}, {"referenceID": 22, "context": "Formants are known to correlate with vowel identity [24, 25], though the dimensions that listeners use when storing and categorizing sounds may be more complex than absolute encoding of formant frequencies [26\u201329].", "startOffset": 206, "endOffset": 213}, {"referenceID": 23, "context": "Formants are known to correlate with vowel identity [24, 25], though the dimensions that listeners use when storing and categorizing sounds may be more complex than absolute encoding of formant frequencies [26\u201329].", "startOffset": 206, "endOffset": 213}, {"referenceID": 24, "context": "Formants are known to correlate with vowel identity [24, 25], though the dimensions that listeners use when storing and categorizing sounds may be more complex than absolute encoding of formant frequencies [26\u201329].", "startOffset": 206, "endOffset": 213}, {"referenceID": 25, "context": "Formants are known to correlate with vowel identity [24, 25], though the dimensions that listeners use when storing and categorizing sounds may be more complex than absolute encoding of formant frequencies [26\u201329].", "startOffset": 206, "endOffset": 213}, {"referenceID": 26, "context": "Listeners\u2019 reliance on perceptual dimensions may also change over the course of development [30, 31].", "startOffset": 92, "endOffset": 100}, {"referenceID": 27, "context": "Listeners\u2019 reliance on perceptual dimensions may also change over the course of development [30, 31].", "startOffset": 92, "endOffset": 100}, {"referenceID": 20, "context": "[24] provide 48 examples of each phoneme from female speakers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Specifically, consistent with previous research [2, 3, 7] the corner vowels are hyper-articulated.", "startOffset": 48, "endOffset": 57}, {"referenceID": 2, "context": "Specifically, consistent with previous research [2, 3, 7] the corner vowels are hyper-articulated.", "startOffset": 48, "endOffset": 57}, {"referenceID": 5, "context": "Specifically, consistent with previous research [2, 3, 7] the corner vowels are hyper-articulated.", "startOffset": 48, "endOffset": 57}, {"referenceID": 5, "context": "Most vowel pairs are hyper-articulated, but consistent with IDS, and contrary to previous arguments that IDS is not for teaching [7], the simulated teaching", "startOffset": 129, "endOffset": 132}, {"referenceID": 20, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Consistent with IDS [7, 8], but contra previous arguments [9], the statistically optimal input includes increases in within-category variability for most categories.", "startOffset": 20, "endOffset": 26}, {"referenceID": 6, "context": "Consistent with IDS [7, 8], but contra previous arguments [9], the statistically optimal input includes increases in within-category variability for most categories.", "startOffset": 20, "endOffset": 26}, {"referenceID": 7, "context": "Consistent with IDS [7, 8], but contra previous arguments [9], the statistically optimal input includes increases in within-category variability for most categories.", "startOffset": 58, "endOffset": 61}, {"referenceID": 24, "context": "It has been suggested, for example, that listeners rely on ratios among formants [28, 29, 32] or on comparisons of formants among different vowels from the same speaker [33\u201335].", "startOffset": 81, "endOffset": 93}, {"referenceID": 25, "context": "It has been suggested, for example, that listeners rely on ratios among formants [28, 29, 32] or on comparisons of formants among different vowels from the same speaker [33\u201335].", "startOffset": 81, "endOffset": 93}, {"referenceID": 28, "context": "It has been suggested, for example, that listeners rely on ratios among formants [28, 29, 32] or on comparisons of formants among different vowels from the same speaker [33\u201335].", "startOffset": 81, "endOffset": 93}, {"referenceID": 29, "context": "It has been suggested, for example, that listeners rely on ratios among formants [28, 29, 32] or on comparisons of formants among different vowels from the same speaker [33\u201335].", "startOffset": 169, "endOffset": 176}, {"referenceID": 30, "context": "It has been suggested, for example, that listeners rely on ratios among formants [28, 29, 32] or on comparisons of formants among different vowels from the same speaker [33\u201335].", "startOffset": 169, "endOffset": 176}, {"referenceID": 31, "context": "It has been suggested, for example, that listeners rely on ratios among formants [28, 29, 32] or on comparisons of formants among different vowels from the same speaker [33\u201335].", "startOffset": 169, "endOffset": 176}, {"referenceID": 7, "context": "[9] observed that /\u00c7/ and /\u00e6/ moved toward the interior.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] focused on native American English speakers though they only specify that", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] observed that the F1-F2 distance between the /i/-/I/ pair did not change (or hypo-articulated, depending on the measure) from ADS to IDS.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "We will first evaluate learning performance under several learning models: logistic regression [9], support vector machines (SVM) with linear kernels, expectation-maximization on Gaussian mixture models (GMM) [8], and the Dirichlet process Gaussian Mixture model (DPGMM; the learner model outlined above, and used as the basis for generating the teaching data).", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "We will first evaluate learning performance under several learning models: logistic regression [9], support vector machines (SVM) with linear kernels, expectation-maximization on Gaussian mixture models (GMM) [8], and the Dirichlet process Gaussian Mixture model (DPGMM; the learner model outlined above, and used as the basis for generating the teaching data).", "startOffset": 209, "endOffset": 212}, {"referenceID": 32, "context": "We used the scikit-learn [36] implementation for each algorithm except DPGMM, which we implemented using the standard sequential Gibbs sampling algorithm [37, Algorithm 3] coupled with intermittent split-merge transitions [38], which improves mixing by allowing the Markov Chain to more easily move between modes in the probability distribution.", "startOffset": 25, "endOffset": 29}, {"referenceID": 34, "context": "We used the scikit-learn [36] implementation for each algorithm except DPGMM, which we implemented using the standard sequential Gibbs sampling algorithm [37, Algorithm 3] coupled with intermittent split-merge transitions [38], which improves mixing by allowing the Markov Chain to more easily move between modes in the probability distribution.", "startOffset": 222, "endOffset": 226}, {"referenceID": 0, "context": "As an example, the partition [1, 2, 3, 3], of four data into three categories implies that datum one belongs to category one, datum two belongs to category two, and data three and four belong to category three.", "startOffset": 29, "endOffset": 41}, {"referenceID": 1, "context": "As an example, the partition [1, 2, 3, 3], of four data into three categories implies that datum one belongs to category one, datum two belongs to category two, and data three and four belong to category three.", "startOffset": 29, "endOffset": 41}, {"referenceID": 2, "context": "As an example, the partition [1, 2, 3, 3], of four data into three categories implies that datum one belongs to category one, datum two belongs to category two, and data three and four belong to category three.", "startOffset": 29, "endOffset": 41}, {"referenceID": 2, "context": "As an example, the partition [1, 2, 3, 3], of four data into three categories implies that datum one belongs to category one, datum two belongs to category two, and data three and four belong to category three.", "startOffset": 29, "endOffset": 41}, {"referenceID": 35, "context": "ARI is an adjusted-for-chance version of the Rand Index [40], which is a normalized sum of the number of pairs of data points that are assigned to the same category in U and the same category in V, and the number of data points that are assigned to different categories in U and different categories in V.", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "However, hypo-articulation such as observed between /I/ and /i/ by Cristia & Seidl [7], and increases in variance of categories such as /\u00e6/ and /\u00c7/ observed by McMurray et al.", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "[9], have been argued to impede learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Specifically, models assume that infants either know the number of phonemes in their language a priori [8] or that the data they receive is accompanied by correct labels [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "Specifically, models assume that infants either know the number of phonemes in their language a priori [8] or that the data they receive is accompanied by correct labels [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 20, "context": "[24] data, which do not include many of the interior and rhotic vowels use in other studies [7, 9].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[24] data, which do not include many of the interior and rhotic vowels use in other studies [7, 9].", "startOffset": 92, "endOffset": 98}, {"referenceID": 7, "context": "[24] data, which do not include many of the interior and rhotic vowels use in other studies [7, 9].", "startOffset": 92, "endOffset": 98}, {"referenceID": 7, "context": "[9], nor did /i/ and /I/ exhibit F1-F2 hypoarticulation as reported by Cristia & Seidl [7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[9], nor did /i/ and /I/ exhibit F1-F2 hypoarticulation as reported by Cristia & Seidl [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 36, "context": "Indeed, research has shown that IDS improves word segmentation [41], word recognition [42], and label learning [43].", "startOffset": 63, "endOffset": 67}, {"referenceID": 37, "context": "Indeed, research has shown that IDS improves word segmentation [41], word recognition [42], and label learning [43].", "startOffset": 86, "endOffset": 90}, {"referenceID": 38, "context": "Indeed, research has shown that IDS improves word segmentation [41], word recognition [42], and label learning [43].", "startOffset": 111, "endOffset": 115}, {"referenceID": 18, "context": "Computational research has sought to answer the question of how people solve inference problems that are computationally intractable, positing that people use approximations [21].", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "[24] were measured only from words beginning with an \u2018h\u2019 and ending with a \u2018d\u2019 e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "48], may in fact be consistent with pedagogical manipulation (as explicitly suggested by Lindblom [46], Jaeger [49]).", "startOffset": 98, "endOffset": 102}, {"referenceID": 43, "context": "48], may in fact be consistent with pedagogical manipulation (as explicitly suggested by Lindblom [46], Jaeger [49]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 44, "context": "Related research indicates that speakers adapt when their communications are unsuccessful [50\u201352].", "startOffset": 90, "endOffset": 97}, {"referenceID": 45, "context": "Related research indicates that speakers adapt when their communications are unsuccessful [50\u201352].", "startOffset": 90, "endOffset": 97}, {"referenceID": 19, "context": "Increasingly, research has highlighted ways in which other people may affect learning [22, 23, 53, 54].", "startOffset": 86, "endOffset": 102}, {"referenceID": 46, "context": "Increasingly, research has highlighted ways in which other people may affect learning [22, 23, 53, 54].", "startOffset": 86, "endOffset": 102}, {"referenceID": 47, "context": "Increasingly, research has highlighted ways in which other people may affect learning [22, 23, 53, 54].", "startOffset": 86, "endOffset": 102}], "year": 2016, "abstractText": "Infant-directed speech (IDS) has distinctive properties that differ from adultdirected speech (ADS). Why it has these properties \u2013 and whether they are intended to facilitate language learning \u2013 is matter of contention. We argue that much of this disagreement stems from lack of a formal, guiding theory of how phonetic categories should best be taught to infant-like learners. In the absence of such a theory, researchers have relied on intuitions about learning to guide the argument. We use a formal theory of teaching, validated through experiments in other domains, as the basis for a detailed analysis of whether IDS is well-designed for teaching phonetic categories. Using the theory, we generate ideal data for teaching phonetic categories in English. We qualitatively compare the simulated teaching data with human IDS, finding that the teaching data exhibit many features of IDS, including some that have been taken as evidence IDS is not for teaching. The simulated data reveal potential pitfalls for experimentalists exploring the role of IDS in language learning. Focusing on different formants and phoneme sets leads to different conclusions, and the benefit of the teaching data to learners is not apparent until a sufficient number of examples have been provided. Finally, we investigate transfer of IDS to learning ADS. The teaching data improves classification of ADS data, but only for the learner they were generated to teach; not universally across all classes of learner. This research offers a theoretically-grounded framework that empowers experimentalists to systematically evaluate whether IDS is for teaching.", "creator": "LaTeX with hyperref package"}}}