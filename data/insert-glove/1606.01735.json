{"id": "1606.01735", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Integrated perception with recurrent multi-task neural networks", "abstract": "Modern discriminative scroller predictors have 12lb been kikutani shown to 60-inch match temo natural intelligences mammy in agovv specific azrael perceptual tasks hagai in ferolito image scary classification, object duisberg and hornibrook part gev101 detection, fourways boundary extraction, polovchak etc. kablan However, a aznavour major advantage khanim that drom natural \u010dovi\u0107 intelligences metroplex still have is berretti that vacuole they work tater well knuckled for \\ energoatom emph {all} perceptual 71.55 problems thaa together, solving them phasor efficiently and coherently wieferich in rewind an \\ wackenhut emph {kelli integrated yovel manner }. whitridge In order to kommunalreformen capture nooteboom some kerbino of these advantages nirupa in brevard machine x-machine perception, mykola we ask 168th two questions: whether fttp deep personne neural networks can smugglers learn digit universal image bobbling representations, kokhav useful allelic not nonnative only mushkoh for a lewknor single task crivelli but eressa for all of stereotypes them, 1902-1904 and how the 600-billion solutions miette to lain\u00e9 the different tasks can setiawan be tahitians integrated in this framework. owens We tung answer laliberte by ndikumana proposing crear a xanders new 3,029 architecture, katinka which we suze call \\ biel emph {multinet }, in greasers which maze not only bush-cheney deep pcpaul image ahcccs features are 2,236 shared between tasks, 0535 but where etoh tasks wlan can interact in a recurrent manner by carparelli encoding 350-page the results of viagens their autechre analysis dunloy in a common shared representation arrius of the unitrin data. In this annona manner, schepisi we show that the skyros performance bitche of sasso individual zig tasks bais in standard cagan benchmarks can naqshbandi be ritika improved first by udall sharing features chave between them boorg and guti then, ch\u00f6gyam more snowsuits significantly, rafowicz by integrating their solutions in the '83 common representation.", "histories": [["v1", "Mon, 6 Jun 2016 13:27:25 GMT  (511kb,D)", "https://arxiv.org/abs/1606.01735v1", "9 pages, 3 figures, 2 tables"], ["v2", "Tue, 29 Nov 2016 14:38:00 GMT  (558kb,D)", "http://arxiv.org/abs/1606.01735v2", "9 pages, 3 figures, 2 tables"]], "COMMENTS": "9 pages, 3 figures, 2 tables", "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["hakan bilen", "andrea vedaldi"], "accepted": true, "id": "1606.01735"}, "pdf": {"name": "1606.01735.pdf", "metadata": {"source": "CRF", "title": "Integrated Perception with Recurrent Multi-Task Neural Networks", "authors": ["Hakan Bilen", "Andrea Vedaldi"], "emails": ["hbilen@robots.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Natural perception can extract complete interpretations of sensory data in a coherent and efficient manner. By contrast, machine perception remains a collection of disjoint algorithms, each solving specific information extraction sub-problems. Recent advances such as modern convolutional neural networks have dramatically improved the performance of machines in individual perceptual tasks, but it remains unclear how these could be integrated in the same seamless way as natural perception does.\nIn this paper, we consider the problem of learning data representations for integrated perception. The first question we ask is whether it is possible to learn universal data representations that can be used to solve all sub-problems of interest. In computer vision, fine-tuning or retraining has been show to be an effective method to transfer deep convolutional networks between different tasks [9, 29]. Here we show that, in fact, it is possible to learn a single, shared representation that performs well on several sub-problems simultaneously, often as well or even better than specialised ones.\nA second question, complementary to the one of feature sharing, is how different perceptual subtasks should be combined. Since each subtask extracts a partial interpretation of the data, the problem is to form a coherent picture of the data as a whole. We consider an incremental interpretation scenario, where subtasks collaborate in parallel or sequentially in order to gradually enrich a shared interpretation of the data, each contributing its own \u201cdimension\u201d to it. Informally, many computer vision systems operate in this stratified manner, with different modules running in parallel or in sequence (e.g. object detection followed by instance segmentation). The question is how this can be done end-to-end and systematically.\nIn this paper, we develop an architecture, multinet (Fig. 1), that provides an answer to such questions. Multinet builds on the idea of a shared representation, called an integration space, which reflects both\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 6.\n01 73\n5v 2\n[ st\nat .M\nL ]\n2 9\nN ov\n2 01\nthe statistics extracted from the data as well as the result of the analysis carried by the individual subtasks. As a loose metaphor, one can think the integration space as a \u201ccanvas\u201d which is progressively updated with the information obtained by solving sub-problems. The representation distills this information and makes it available for further task resolution, in a recurrent configuration.\nMultinet has several advantages. First, by learning the latent integration space automatically, synergies between tasks can be discovered automatically. Second, tasks are treated in a symmetric manner, by associating to each of them encoder, decoder, and integrator functions, making the system modular and easily extensible to new tasks. Third, the architecture supports incremental understanding because tasks contribute back to the latent representation, making their output available to other tasks for further processing. Finally, while multinet is applied here to a image understanding setting, the architecture is very general and could be applied to numerous other domains as well.\nThe new architecture is described in detail in section 2 and an instance specialized for computer vision applications is given in section 3. The empirical evaluation in section 4 demonstrates the benefits of the approach, including that sharing features between different tasks is not only economical, but also sometimes better for accuracy, and that integrating the outputs of different tasks in the shared representation yields further accuracy improvements. Section 5 summarizes our findings."}, {"heading": "1.1 Related work", "text": "Multiple task learning (MTL): Multitask learning [5, 25, 1] methods have been studied over two decades by the machine learning community. The methods are based on the key idea that the tasks share a common low-dimensional representation which is jointly learnt with the task specific parameters. While MLT trains many tasks in parallel, Mitchell and Thrun [18] propose a sequential transfer method called Explanation-Based Neural Nets (EBNN) which exploits previously learnt domain knowledge to initialise or constraint the parameters of the current task. Breiman and Freidman [3] devise a hybrid method that first learns separate models and then improves their generalisation by exploiting the correlation between the predictions.\nMulti-task learning in computer vision: MTL has been shown to improve results in many computer vision problems. Typically, researchers incorporate auxiliary tasks into their target tasks, jointly train them in parallel and achieve performance gains in object tracking [30], object detection [11], facial landmark detection [31]. Differently, Dai et al. [8] propose multi-task network cascades in which convolutional layer parameters are shared between three tasks and the tasks are predicted sequentially. Unlike [8], our method can train multiple tasks in parallel and does not require a specification of task execution.\nRecurrent networks: Our work is also related to recurrent neural networks (RNN) [22] which has been successfully used in language modelling [17], speech recognition [13], hand-written recognition [12], semantic image segmentation [20] and human pose estimation [2]. Related to our work, Carreira et al. [4] propose an iterative segmentation model that progressively updates an initial\nsolution by feeding back error signal. Najibi et al. [19] propose an efficient grid based object detector that iteratively refine the predicted object coordinates by minimising the training error. While these methods [4, 19] are also based on an iterative solution correcting mechanism, our main goal is to improve generalisation performance for multiple tasks by sharing the previous predictions across them and learning output correlations."}, {"heading": "2 Method", "text": "In this section, we first introduce the multinet architecture for integrated multi-task prediction (section 2.1) and then we discuss ordinary multi-task prediction as a special case of multinet (section 2.2)."}, {"heading": "2.1 Multinet: integrated multiple-task prediction", "text": "We propose a recurrent neural network architecture (Fig. 1 and 2) that can address simultaneously multiple data labelling tasks. For symmetry, we drop the usual distinction between input and output spaces and consider insteadK label spacesX\u03b1, \u03b1 = 0, 1, . . . ,K. A label in the \u03b1-th space is denoted by the symbol x\u03b1 \u2208 X\u03b1. In the following, \u03b1 = 0 is used for the input (e.g. an image) of the network and is not inferred, whereas x1, . . . ,xK are labels estimated by the neural network (e.g. an object class, location, and parts). One reason why it is useful to keep the notation symmetric is because it is possible to ground any label x\u03b1 and treat it as an input instead.\nEach task \u03b1 is associated to a corresponding encoder function \u03c6\u03b1enc, which maps the label x \u03b1 to a vectorial representation r\u03b1 \u2208 R\u03b1 given by\nr\u03b1 = \u03c6\u03b1enc(x \u03b1). (1)\nEach task has also a decoder function \u03c8\u03b1dec going in the other direction, from a common representation space h \u2208 H to the label x\u03b1: x\u03b1 = \u03c8\u03b1dec(h). (2) The information r0, r1, . . . , r\u03b1 extracted from the data and the different tasks by the encoders is integrated in the shared representation h by using an integrator function \u0393. Since this update operation is incremental, we associate to it an iteration number t = 0, 1, 2, . . . . By doing so, the update equation can be written as\nht+1 = \u0393(ht, r 0, r1t , . . . , r K t ). (3)\nNote that, in the equation above, r0 is constant as the corresponding variable x0 is the input of the network, which is grounded and not updated.\nOverall, a task \u03b1 is specified by the triplet T \u03b1 = (X\u03b1, \u03c6\u03b1enc, \u03c8\u03b1dec) and by its contribution to the update rule eq. 3. Full task modularity can be achieved by decomposing the integrator function as a sequence of task-specific updates ht+1 = \u0393K(\u00b7, rKt ) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03931(ht, r1t ), such that each task is a quadruplet (X\u03b1, \u03c6\u03b1enc, \u03c8\u03b1dec,\u0393\u03b1), but this option is not investigated further here. Given tasks T \u03b1, \u03b1 = 1, . . . ,K, several variants of the recurrent architecture are possible. A natural one is to process tasks sequentially, but this has the added complication of having to choose a particular order and may in any case be suboptimal; instead, we propose to update all the task at each recurrent iteration, as follows:\nt = 0 Ordinary multi-task prediction. At the first iteration, the measurement x0 is acquired and the shared representation h is initialized as h0 = \u03c60enc(x\n0) = \u0393(\u2217, r0, \u2217, . . . , \u2217). The symbol \u2217 denotes the initial value of a variable (often zero in practice). Given h0, the output x\u03b10 = \u03c8 \u03b1 dec(h0) = (\u03c8 \u03b1 dec \u25e6 \u03c60enc)(x0) for each task is computed. This step corresponds to ordinary multi-task prediction, as discussed later (section 2.2).\nt > 0 Iterative updates. Each task \u03b1 = 1, . . . ,K is re-encoded using equations r\u03b1t = \u03c6\u03b1enc(x\u03b1t ), the shared representation is updated using ht+1 = \u0393(ht, r0, r1t , . . . , r K t ), and the labels are\npredicted again using x\u03b1t+1 = \u03c8 \u03b1 dec(ht+1).\nThe idea of feeding back the network output for further processing exists in several existing recurrent architectures [16, 24]; however, in these cases it is used to process sequential data, passing back the output obtained from the last process element in the sequence; here, instead, the feedback is used to integrate different and complementary labelling tasks. Our model is also reminiscent of encoder/decoder architectures [15, 21, 28]; however, in our case the encoder and decoder functions are associated to the output labels rather than to the input data."}, {"heading": "2.2 Ordinary multi-task learning", "text": "Ordinarily, multiple-task learning [5, 25, 1] is based on sharing features or parameters between different tasks. Multinet reduces to ordinary multi-task learning when there is no recurrence. At the first iteration t = 0, in fact, multinet simply evaluatesK predictor functions \u03c81dec\u25e6\u03c60enc, . . . , \u03c8Kdec\u25e6\u03c60enc, one for each task, which share the common subnetwork \u03c60enc.\nWhile multi-task learning from representation sharing is conceptually simple, it is practically important because it allows learning a universal representation function \u03c60enc which works well for all tasks simultaneously. The possibility of learning such a polyvalent representation, which can only be verified empirically, is a non-trivial and useful fact. In particular, in our experiments in image understanding (section 4), we will see that, for certain image analysis tasks, it is not only possible and efficient to learn such a shared representation, but that in some cases feature sharing can even improve the performance in the individual sub-problems."}, {"heading": "3 A multinet for classification, localization, and part detection", "text": "In this section we instantiate multinet for three complementary tasks in computer vision: object classification, object detection, and part detection. The main advantage of multinet compared to ordinary multi-task prediction is that, while sharing parameters across related tasks may improve generalization [5], it is not enough to capture correlations in the task input spaces. For example, in our computer vision application ordinary multi-task prediction would not be able to ensure that the detected parts are contained within a detected object. Multinet can instead capture interactions between the different labels and potentially learn to enforce such constraints. The latter is done in a soft and distributed manner, by integrating back the output of the individual tasks in the shared representation.\nNext, we discuss in some detail the specific architecture components used in our application. As a starting point we consider a standard CNN for image classification. While more powerful networks exist, we choose here a good performing model which is at the same time reasonably efficient to train and evaluate, namely the VGG-M-1024 network of [6]. This model is pre-trained for image classification from the ImageNet ILSVRC 2012 data [23] and was extended in [11] to object detection; here we follow such blueprints, and in particular the Fast R-CNN method of [11], to design the subnetworks for the three tasks. These components are described in some detail below, first focusing on the components corresponding to ordinary multi-task prediction, and then moving to the ones used for multiple task integration.\nOrdinary multiple-task components. The first several layers of the VGG-M network can be grouped in five convolutional sections, each comprising linear convolution, a non-linear activation function and, in some cases, max pooling and normalization. These are followed by three fullyconnected sections, which are the same as the convolutional ones, but with filter support of the same size as the corresponding input. The last layer is softmax and computes a posterior probability vector over the 1,000 ImageNet ILSVRC classes.\nVGG-M is adapted for the different tasks as follows. For clarity, we use symbolic names for the tasks rather than numeric indexes, and consider \u03b1 \u2208 {img, cls, det, part} instead of \u03b1 \u2208 {0, 1, 2, 3}. The five convolutional sections of VGG-M are used as the image encoder \u03c6imgenc and hence compute the initial value h0 of the shared representation. Cutting VGG-M at the level of the last convolutional layer is motivated by the fact that the fully-connected layers remove or at least dramatically blur spatial information, whereas we would like to preserve it for object and part localization. Hence, the shared representation is a tensor h \u2208 RH\u00d7W\u00d7C , where H \u00d7W are the spatial dimensions and C is the number of feature channels as determined by the VGG-M configuration (see section 4).\nNext, \u03c6imgenc is branched off in three directions, choosing a decoder \u03c8\u03b1dec for each task: image classification (\u03b1 = cls), object detection (\u03b1 = det), and part detection (\u03b1 = part). For the image classification branch, we choose \u03c6\u03b1enc as the rest of the original VGG-M network for image classification. In other words, the decoder function \u03c8clsdec for the image-level labels is initialized to be the same as the fully-connected layers of the original VGG-M, such that \u03c6VGG-Menc = \u03c8 cls dec \u25e6 \u03c6 img enc . There are however two differences. The first is the last fully-connected layer is reshaped and reinitialized randomly to predict a different number C of possible objects instead of the 1,000 ImageNet classes. The second difference is that the final output is a vector of binary probabilities obtained using sigmoid instead of a softmax.\nThe object and part detection decoders are instead based on the Fast R-CNN architecture [11], and classify individual image regions as belonging to one of the object classes (part types) or background. To do so, the Selective Search Windows (SSW) method [26] is used to generate a shortlist of M region (bounding box) proposals B(ximg) = {b1, . . . ,bM} from image ximg; this set is inputted to the spatial pyramid pooling (SPP) layer [14, 11] \u03c8SPPdec (h,B(ximg)), which extracts subsets of the feature map h in correspondence of each region using max pooling. The object detection decoder (and similarly for the part detector) is then given by \u03c8detdec(h) = \u03c8dec det(\u03c8SPPdec (h,B(ximg)) where \u03c8dec det contains fully connected layers initialized in the same manner as the classification decoder above (hence, before training one also has \u03c6VGG-Menc = \u03c8dec\ndet \u25e6 \u03c6imgenc ). The exception is once more the last layer, reshaped and reinitialized as needed, whereas softmax is still used as regions can have only one class.\nSo far, we have described the image encored \u03c6imgenc and the decoder branches \u03c8clsdec, \u03c8 det dec and \u03c8 part dec for the three tasks. Such components are sufficient for ordinary multi-task learning, corresponding to the initial multinet iteration. Next, we specify the components that allow to iterate multinet several times.\nRecurrent components: integrating multiple tasks. For task integration, we need to construct the encoder functions \u03c6clsenc, \u03c6 det enc and \u03c6 part enc for each task as well as the integrator function \u0393. While several constructions are possible, here we experiment with simple ones.\nIn order to encode the image label xcls, the encoder rcls = \u03c6clsenc(x cls) takes the vector of Ccls binary probabilities xcls \u2208 RCcls , one for each of the Ccls possible object classes, and broadcasts the corresponding values to all H \u00d7W spatial locations (u, v) in h. Formally rcls \u2208 RH\u00d7W\u00d7Ccls and\n\u2200u, v, c : rclsuvc = xclsc . Encoding the object detection label xdet is similar, but reflects the geometric information captured by such labels. In particular, each bounding box bm of the M extracted by SSW is associated to a vector of Ccls + 1 probabilities (one for each object class plus one more for background) xdetm \u2208 RC\ncls+1. This is decoded in a heat map rcls \u2208 RH\u00d7W\u00d7(Ccls+1) by max pooling across boxes:\n\u2200u, v, c : rclsuvc = max { xdetmc, \u2200m : (u, v) \u2208 bm } \u222a {0}.\nThe part label xpart is encoded in an entirely analogous manner.\nLastly, we need to construct the integrator function \u0393. We experiment with two simple designs. The first one simply stacks evidence from the different sources: h = stack(rimg, rcls, rdet, rpart). Then the update equation is given by\nht = \u0393(ht\u22121, r img, rclst , r det t , r part t ) = stack(r img, rclst , r det t , r part t ). (4)\nNote that this formulation requires modifying the first fully-connected layers of each decoder \u03c8\u0302clsdec, \u03c8\u0302detdec and \u03c8\u0302 part dec as the shared representation h has now C+2C cls +Cpart +2 channels instead of just C\nas for the original VGG-M architecture. This is done by initializing randomly additional dimensions in the linear maps.\nWe also experiment with a second update equation\nht = \u0393(ht\u22121, r img, rclst , r det t , r part t ) = ReLU(A \u2217 stack(ht\u22121, rcls, rclst , rdett , r part t )) (5)\nwhere A \u2208 R1\u00d71\u00d7(2C+2Ccls+Cpart+2)\u00d7C is a filter bank whose purpose is to reduce the stacked representation back to the original C channels. This is a useful design as it maintains the same representation dimensionality regardless of the number of tasks added. However, due to the compression, it may perform less well."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Implementation details and training", "text": "The image encoder \u03c6imgenc is initialized from the pre-trained VGG-M model using sections conv1 to conv5. If the input to the network is an RGB image ximg \u2208 RH img\u00d7W img\u00d73, then, due to downsampling, the spatial dimension H \u00d7W \u00d7 C of rimg = \u03c6imgenc (ximg) are H \u2248 H img/16 and W \u2248 W img/16. The number of feature channels is C = 512. As noted above, the decoders contain respectively subnetworks \u03c8deccls, \u03c8decdet, and \u03c8decpart comprising layers fc6 and fc7 from VGG-M, followed by a randomly-initialized linear predictor with output dimension equal to, respectively, Ccls, Ccls + 1, and Cpart + 1. Max pooling in SPP is performed in a grid of 6\u00d7 6 spatial bins as in [14, 11]. The task encoders \u03c6clsenc, \u03c6 det enc, \u03c6 part enc are given in section 2 and contain no parameter.\nFor training, each task is associated with a corresponding loss function. For the classification task, the objective is to minimize the sum of negative posterior log-probabilities of whether the image contains a certain object type or not (this allows different objects to be present in a single image). Combined with the fact that the classification branch uses sigmoid, this is the same as binary logistic regression. For the object and part detection tasks, decoders are optimized to classify the target regions as one of the Ccls or Cpart classes or background (unlike image-level labels, classes in region-level labels are mutually exclusive). Furthermore, we also train a branch performing bounding box refinement to improve the fit of the selective search region as proposed by [11].\nThe fully connected layers used for softmax classification and bounding-box regression in object and part detection tasks are initialized from zero-mean Gaussian distributions with 0.01 and 0.001 standard deviations respectively. The fully connected layers used for object classification task and the adaptation layer A (see eq. 5) are initialized with zero-mean Gaussian with 0.01 standard deviation.\nAll layers use a learning rate of 1 for filters and 2 for biases. We used SGD to optimize the parameters with a learning rate of 0.001 for 6 epochs and lower it to 0.0001 for another 6 epochs. We observe that running two iterations of recursion is sufficient to reach 99% of the performance, although marginal gains are possible with more. We use the publicly available CNN toolbox MatConvNet [27] in our experiments."}, {"heading": "4.2 Results", "text": "In this section, we describe and discuss experimental results of our models in two benchmarks.\nPASCAL VOC 2010 [10] and Parts [7]: The dataset contains 4998 training and 5105 validation images for 20 object categories and ground truth bounding box annotations for target categories. We use the PASCAL-Part dataset [7] to obtain bounding box annotations of object parts which consists of 193 annotated part categories such as aeroplane engine, bicycle back-wheel, bird left-wing, person right-upper-leg. After removing annotations that are smaller than 20 pixels on one side and the categories with less than 50 training samples, the number of part categories reduces to 152. The dataset provides annotations for only training and validation splits, thus we train our models in the train split and report results in the validation split for all the tasks. We follow the standard PASCAL VOC evaluation and report average precision (AP) and AP at 50% intersection-over-union (IoU) of the detected boxes with the ground ones for object classification and detection respectively. For the part detection, we follow [7] and report AP at a more relaxed 40% IoU threshold. The results for the tasks are reported in Table 1.\nIn order to establish the first baseline, we train an independent network for each task. Each network is initialized with the VGG-M model, the last classification and regression layers are initialized with random noise and all the layers are fine-tuned for the respective task. For object and part detection, we use our implementation of Fast-RCNN [11]. Note that, for consistency between the baselines and our method, minimum dimension of each image is scaled to be 600 pixels for all the tasks including object classification. An SPP layer is employed to scale the feature map into 6\u00d7 6 dimensionality. For the second baseline, we train a multi-task network that shares the convolutional layers across the tasks (this setting is called ordinary multi-task prediction in section 2.1). We observe in Table 1 that the multi-task model performs comparable or better than the independent networks, while being more efficient due to the shared convolutional computations. Since the training images are the same in all cases, this shows that just combining multiple labels together improves efficiency and in some cases even performance.\nFinally we test the full multinet model for two settings defined as update rules (1) and (2) corresponding to eq. 4 and 5 respectively. We first see that both models outperforms the independent networks and multi-task network as well. This is remarkable because our model consists of smaller number of parameters than the sum of three independent networks and yet our best model (update 1) consistently outperforms them by roughly 1.5 points in mean AP. Furthermore, multinet improves over the ordinary multi-task prediction by exploiting the correlations in the solutions of the individual tasks. In addition, we observe that update (1) performs better than update (2) that constraints the shared representation space to 512 dimensions regardless of the number of tasks, as it can be expected due to the larger capacity. Nevertheless, even with the bottleneck we observe improvements compared to ordinary multi-task prediction.\nWe also run a test case to verify whether multinet learns to mix information extracted by the various tasks as presumed. To do so, we exploit the predictions performed by these task in will be able to improve more with ground truth labels during test time. At test time we ground the classification label rcls in the first iteration of multinet to the ground truth class labels and we read the predictions after one iteration. The performances expectedly in the three tasks improve to 90.1, 58.9 and 39.2 respectively. This shows that, the feedback on the class information has a strong effect on class prediction itself, and a more modest but nevertheless significant effect on the other tasks as well.\nPASCAL VOC 2007 [10]: The dataset consists of 2501 training, 2510 validation, and 5011 test images containing bounding box annotations for 20 object categories. There is no part annotations available for this dataset, thus, we exclude the part detection task and run the same baselines and our best model for object classification and detection. The results are reported for the test split and depicted in Table 2. Note that our RCNN for the individual networks obtains the same detection score\nin [11]. In parallel to the former results, our method consistently outperforms both the baselines in classification and detection tasks."}, {"heading": "5 Conclusions", "text": "In this paper, we have presented multinet, a recurrent neural network architecture to solve multiple perceptual tasks in an efficient and coordinated manner. In addition to feature and parameter sharing, which is common to most multi-task learning methods, multinet combines the output of the different tasks by updating a shared representation iteratively.\nOur results are encouraging. First, we have shown that such architectures can successfully integrate multiple tasks by sharing a large subset of the data representation while matching or even outperforming specialised network. Second, we have shown that the iterative update of a common representation is an effective method for sharing information between different tasks which further improve performance."}, {"heading": "Acknowledgments", "text": "This work acknowledges the support of the ERC Starting Grant Integrated and Detailed Image Understanding (EP/L024683/1)."}], "references": [{"title": "A model of inductive bias learning", "author": ["J. Baxter"], "venue": "J. Artif. Intell. Res.(JAIR), 12(149-198):3", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Recurrent human pose estimation", "author": ["V. Belagiannis", "A. Zisserman"], "venue": "arXiv preprint arXiv:1605.02914", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Predicting multivariate responses in multiple linear regression", "author": ["L. Breiman", "J.H. Friedman"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 59(1):3\u201354", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Human pose estimation with iterative error feedback", "author": ["J. Carreira", "P. Agrawal", "K. Fragkiadaki", "J. Malik"], "venue": "CVPR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, 28(1)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "BMVC", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Detect what you can: Detecting and representing objects using holistic models and body parts", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun", "A.L. Yuille"], "venue": "CVPR, pages 1971\u20131978", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Instance-aware semantic segmentation via multi-task network cascades", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "CVPR", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "CoRR, abs/1310.1531", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "The PASCAL Visual Object Classes (VOC) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 88(2):303\u2013338", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "ICCV", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fern\u00e1ndez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(5):855\u2013868", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "ICASSP, pages 6645\u20136649. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV, pages 346\u2013361", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u2013 1780", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Statistical Language Models Based on Neural Networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Ph. D. thesis, Brno University of Technology", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Explanation-based neural network learning for robot control", "author": ["T.M. Mitchell", "S.B. Thrun"], "venue": "NIPS, pages 287\u2013287", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "G-cnn: an iterative grid based object detector", "author": ["M. Najibi", "M. Rastegari", "L.S. Davis"], "venue": "CVPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent convolutional neural networks for scene parsing", "author": ["P.H.O. Pinheiro", "R. Collobert"], "venue": "arXiv preprint arXiv:1306.2795", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M.A. Ranzato", "F.J. Huang", "Y. Boureau", "Y. LeCun"], "venue": "CVPR, pages 1\u20138", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling, 5(3):1", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1988}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "S. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "F.F. Li"], "venue": "IJCV", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, pages 3104\u20133112", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "editors", "author": ["S. Thrun", "L. Pratt"], "venue": "Learning to Learn. Kluwer Academic Publishers", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Segmentation as selective search for object recognition", "author": ["K. van de Sande", "J. Uijlings", "T. Gevers", "A. Smeulders"], "venue": "In ICCV,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "ICML, pages 1096\u20131103. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "CoRR, abs/1311.2901", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust visual tracking via structured multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "IJCV, 101(2):367\u2013383", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "ECCV, pages 94\u2013108. Springer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "In computer vision, fine-tuning or retraining has been show to be an effective method to transfer deep convolutional networks between different tasks [9, 29].", "startOffset": 150, "endOffset": 157}, {"referenceID": 28, "context": "In computer vision, fine-tuning or retraining has been show to be an effective method to transfer deep convolutional networks between different tasks [9, 29].", "startOffset": 150, "endOffset": 157}, {"referenceID": 4, "context": "Multiple task learning (MTL): Multitask learning [5, 25, 1] methods have been studied over two decades by the machine learning community.", "startOffset": 49, "endOffset": 59}, {"referenceID": 24, "context": "Multiple task learning (MTL): Multitask learning [5, 25, 1] methods have been studied over two decades by the machine learning community.", "startOffset": 49, "endOffset": 59}, {"referenceID": 0, "context": "Multiple task learning (MTL): Multitask learning [5, 25, 1] methods have been studied over two decades by the machine learning community.", "startOffset": 49, "endOffset": 59}, {"referenceID": 17, "context": "While MLT trains many tasks in parallel, Mitchell and Thrun [18] propose a sequential transfer method called Explanation-Based Neural Nets (EBNN) which exploits previously learnt domain knowledge to initialise or constraint the parameters of the current task.", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "Breiman and Freidman [3] devise a hybrid method that first learns separate models and then improves their generalisation by exploiting the correlation between the predictions.", "startOffset": 21, "endOffset": 24}, {"referenceID": 29, "context": "Typically, researchers incorporate auxiliary tasks into their target tasks, jointly train them in parallel and achieve performance gains in object tracking [30], object detection [11], facial landmark detection [31].", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Typically, researchers incorporate auxiliary tasks into their target tasks, jointly train them in parallel and achieve performance gains in object tracking [30], object detection [11], facial landmark detection [31].", "startOffset": 179, "endOffset": 183}, {"referenceID": 30, "context": "Typically, researchers incorporate auxiliary tasks into their target tasks, jointly train them in parallel and achieve performance gains in object tracking [30], object detection [11], facial landmark detection [31].", "startOffset": 211, "endOffset": 215}, {"referenceID": 7, "context": "[8] propose multi-task network cascades in which convolutional layer parameters are shared between three tasks and the tasks are predicted sequentially.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Unlike [8], our method can train multiple tasks in parallel and does not require a specification of task execution.", "startOffset": 7, "endOffset": 10}, {"referenceID": 21, "context": "Recurrent networks: Our work is also related to recurrent neural networks (RNN) [22] which has been successfully used in language modelling [17], speech recognition [13], hand-written recognition [12], semantic image segmentation [20] and human pose estimation [2].", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "Recurrent networks: Our work is also related to recurrent neural networks (RNN) [22] which has been successfully used in language modelling [17], speech recognition [13], hand-written recognition [12], semantic image segmentation [20] and human pose estimation [2].", "startOffset": 140, "endOffset": 144}, {"referenceID": 12, "context": "Recurrent networks: Our work is also related to recurrent neural networks (RNN) [22] which has been successfully used in language modelling [17], speech recognition [13], hand-written recognition [12], semantic image segmentation [20] and human pose estimation [2].", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "Recurrent networks: Our work is also related to recurrent neural networks (RNN) [22] which has been successfully used in language modelling [17], speech recognition [13], hand-written recognition [12], semantic image segmentation [20] and human pose estimation [2].", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "Recurrent networks: Our work is also related to recurrent neural networks (RNN) [22] which has been successfully used in language modelling [17], speech recognition [13], hand-written recognition [12], semantic image segmentation [20] and human pose estimation [2].", "startOffset": 230, "endOffset": 234}, {"referenceID": 1, "context": "Recurrent networks: Our work is also related to recurrent neural networks (RNN) [22] which has been successfully used in language modelling [17], speech recognition [13], hand-written recognition [12], semantic image segmentation [20] and human pose estimation [2].", "startOffset": 261, "endOffset": 264}, {"referenceID": 3, "context": "[4] propose an iterative segmentation model that progressively updates an initial", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] propose an efficient grid based object detector that iteratively refine the predicted object coordinates by minimising the training error.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "While these methods [4, 19] are also based on an iterative solution correcting mechanism, our main goal is to improve generalisation performance for multiple tasks by sharing the previous predictions across them and learning output correlations.", "startOffset": 20, "endOffset": 27}, {"referenceID": 18, "context": "While these methods [4, 19] are also based on an iterative solution correcting mechanism, our main goal is to improve generalisation performance for multiple tasks by sharing the previous predictions across them and learning output correlations.", "startOffset": 20, "endOffset": 27}, {"referenceID": 15, "context": "The idea of feeding back the network output for further processing exists in several existing recurrent architectures [16, 24]; however, in these cases it is used to process sequential data, passing back the output obtained from the last process element in the sequence; here, instead, the feedback is used to integrate different and complementary labelling tasks.", "startOffset": 118, "endOffset": 126}, {"referenceID": 23, "context": "The idea of feeding back the network output for further processing exists in several existing recurrent architectures [16, 24]; however, in these cases it is used to process sequential data, passing back the output obtained from the last process element in the sequence; here, instead, the feedback is used to integrate different and complementary labelling tasks.", "startOffset": 118, "endOffset": 126}, {"referenceID": 14, "context": "Our model is also reminiscent of encoder/decoder architectures [15, 21, 28]; however, in our case the encoder and decoder functions are associated to the output labels rather than to the input data.", "startOffset": 63, "endOffset": 75}, {"referenceID": 20, "context": "Our model is also reminiscent of encoder/decoder architectures [15, 21, 28]; however, in our case the encoder and decoder functions are associated to the output labels rather than to the input data.", "startOffset": 63, "endOffset": 75}, {"referenceID": 27, "context": "Our model is also reminiscent of encoder/decoder architectures [15, 21, 28]; however, in our case the encoder and decoder functions are associated to the output labels rather than to the input data.", "startOffset": 63, "endOffset": 75}, {"referenceID": 4, "context": "Ordinarily, multiple-task learning [5, 25, 1] is based on sharing features or parameters between different tasks.", "startOffset": 35, "endOffset": 45}, {"referenceID": 24, "context": "Ordinarily, multiple-task learning [5, 25, 1] is based on sharing features or parameters between different tasks.", "startOffset": 35, "endOffset": 45}, {"referenceID": 0, "context": "Ordinarily, multiple-task learning [5, 25, 1] is based on sharing features or parameters between different tasks.", "startOffset": 35, "endOffset": 45}, {"referenceID": 4, "context": "The main advantage of multinet compared to ordinary multi-task prediction is that, while sharing parameters across related tasks may improve generalization [5], it is not enough to capture correlations in the task input spaces.", "startOffset": 156, "endOffset": 159}, {"referenceID": 5, "context": "While more powerful networks exist, we choose here a good performing model which is at the same time reasonably efficient to train and evaluate, namely the VGG-M-1024 network of [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 22, "context": "This model is pre-trained for image classification from the ImageNet ILSVRC 2012 data [23] and was extended in [11] to object detection; here we follow such blueprints, and in particular the Fast R-CNN method of [11], to design the subnetworks for the three tasks.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "This model is pre-trained for image classification from the ImageNet ILSVRC 2012 data [23] and was extended in [11] to object detection; here we follow such blueprints, and in particular the Fast R-CNN method of [11], to design the subnetworks for the three tasks.", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "This model is pre-trained for image classification from the ImageNet ILSVRC 2012 data [23] and was extended in [11] to object detection; here we follow such blueprints, and in particular the Fast R-CNN method of [11], to design the subnetworks for the three tasks.", "startOffset": 212, "endOffset": 216}, {"referenceID": 10, "context": "The object and part detection decoders are instead based on the Fast R-CNN architecture [11], and classify individual image regions as belonging to one of the object classes (part types) or background.", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "To do so, the Selective Search Windows (SSW) method [26] is used to generate a shortlist of M region (bounding box) proposals B(ximg) = {b1, .", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": ",bM} from image ximg; this set is inputted to the spatial pyramid pooling (SPP) layer [14, 11] \u03c8SPP dec (h,B(ximg)), which extracts subsets of the feature map h in correspondence of each region using max pooling.", "startOffset": 86, "endOffset": 94}, {"referenceID": 10, "context": ",bM} from image ximg; this set is inputted to the spatial pyramid pooling (SPP) layer [14, 11] \u03c8SPP dec (h,B(ximg)), which extracts subsets of the feature map h in correspondence of each region using max pooling.", "startOffset": 86, "endOffset": 94}, {"referenceID": 13, "context": "Max pooling in SPP is performed in a grid of 6\u00d7 6 spatial bins as in [14, 11].", "startOffset": 69, "endOffset": 77}, {"referenceID": 10, "context": "Max pooling in SPP is performed in a grid of 6\u00d7 6 spatial bins as in [14, 11].", "startOffset": 69, "endOffset": 77}, {"referenceID": 10, "context": "Furthermore, we also train a branch performing bounding box refinement to improve the fit of the selective search region as proposed by [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 26, "context": "We use the publicly available CNN toolbox MatConvNet [27] in our experiments.", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "PASCAL VOC 2010 [10] and Parts [7]: The dataset contains 4998 training and 5105 validation images for 20 object categories and ground truth bounding box annotations for target categories.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "PASCAL VOC 2010 [10] and Parts [7]: The dataset contains 4998 training and 5105 validation images for 20 object categories and ground truth bounding box annotations for target categories.", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "We use the PASCAL-Part dataset [7] to obtain bounding box annotations of object parts which consists of 193 annotated part categories such as aeroplane engine, bicycle back-wheel, bird left-wing, person right-upper-leg.", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "For the part detection, we follow [7] and report AP at a more relaxed 40% IoU threshold.", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "For object and part detection, we use our implementation of Fast-RCNN [11].", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "PASCAL VOC 2007 [10]: The dataset consists of 2501 training, 2510 validation, and 5011 test images containing bounding box annotations for 20 object categories.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "in [11].", "startOffset": 3, "endOffset": 7}], "year": 2016, "abstractText": "Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for all perceptual problems together, solving them efficiently and coherently in an integrated manner. In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call multinet, in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation.", "creator": "LaTeX with hyperref package"}}}