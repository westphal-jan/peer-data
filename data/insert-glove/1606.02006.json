{"id": "1606.02006", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Incorporating Discrete Translation Lexicons into Neural Machine Translation", "abstract": "Neural orilla machine kutesa translation (NMT) often extradicted makes mistakes kapsabet in translating low - frequency content qbr words depressor that cuernavaca are essential venetiaan to sloves understanding freyja the \u03b2-carotene meaning of extol the sentence. We propose fencer a bodrogi method to alleviate sempervirens this pi04 problem digel by augmenting 22-strong NMT st.-pierre systems jfarrelldenverpost.com with discrete economicos translation zitel lexicons that efficiently cicutoxin encode translations odsal of mateelong these ainley low - frequency magsi words. We describe parineeta a mayuree method bxf5 to nho calculate calumpit the lexicon reciprocated probability of eisele the cantabile next word gid in the translation dream candidate maht by using 324.4 the attention furring vector p\u00e2r\u00e2ul of constitute the NMT elsmere model dolben to drewery select singularly which source faridah word lexical probabilities lazuli the model should mesonephros focus jihai on. lobdell We test octoberfest two methods .527 to physiological combine fledermaus this kempo probability o'donnel with serco the kafe standard NMT probability: (3,666 1) snowblower using it sanxia as a 80.7 bias, and (rippled 2) linear 2229 interpolation. egginton Experiments on trotman two corpora 830 show an improvement spitze of 2. suckow 0 - 2. 1986-1994 3 BLEU propelled and squatted 0. 13 - lewinksy 0. 44 NIST score, folmer and faster mlp convergence six-year-old time.", "histories": [["v1", "Tue, 7 Jun 2016 02:40:42 GMT  (157kb)", "https://arxiv.org/abs/1606.02006v1", "Submitted to EMNLP 2016"], ["v2", "Wed, 5 Oct 2016 02:46:39 GMT  (158kb)", "http://arxiv.org/abs/1606.02006v2", "Accepted at EMNLP 2016"]], "COMMENTS": "Submitted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["philip arthur", "graham neubig", "satoshi nakamura 0001"], "accepted": true, "id": "1606.02006"}, "pdf": {"name": "1606.02006.pdf", "metadata": {"source": "CRF", "title": "Incorporating Discrete Translation Lexicons into Neural Machine Translation", "authors": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "emails": ["philip.arthur.om0@is.naist.jp", "gneubig@cs.cmu.edu", "s-nakamura@is.naist.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n02 00\n6v 2\n[ cs\n.C L\n] 5\nO ct"}, {"heading": "1 Introduction", "text": "Neural machine translation (NMT, \u00a72; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks. NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016).\nOne feature of NMT systems is that they treat each word in the vocabulary as a vector of\n1Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html\ncontinuous-valued numbers. This is in contrast to more traditional SMT methods such as phrase-based machine translation (PBMT; Koehn et al. (2003)), which represent translations as discrete pairs of word strings in the source and target languages. The use of continuous representations is a major advantage, allowing NMT to share statistical power between similar words (e.g. \u201cdog\u201d and \u201ccat\u201d) or contexts (e.g. \u201cthis is\u201d and \u201cthat is\u201d). However, this property also has a drawback in that NMT systems often mistranslate into words that seem natural in the context, but do not reflect the content of the source sentence. For example, Figure 1 is a sentence from our data where the NMT system mistakenly translated \u201cTunisia\u201d into the word for \u201cNorway.\u201d This variety of error is particularly serious because the content words that are often mistranslated by NMT are also the words that play a key role in determining the whole meaning of the sentence.\nIn contrast, PBMT and other traditional SMT methods tend to rarely make this kind of mistake. This is because they base their translations on discrete phrase mappings, which ensure that source words will be translated into a target word that has\nbeen observed as a translation at least once in the training data. In addition, because the discrete mappings are memorized explicitly, they can be learned efficiently from as little as a single instance (barring errors in word alignments). Thus we hypothesize that if we can incorporate a similar variety of information into NMT, this has the potential to alleviate problems with the previously mentioned fatal errors on low-frequency words.\nIn this paper, we propose a simple, yet effective method to incorporate discrete, probabilistic lexicons as an additional information source in NMT (\u00a73). First we demonstrate how to transform lexical translation probabilities (\u00a73.1) into a predictive probability for the next word by utilizing attention vectors from attentional NMT models (Bahdanau et al., 2015). We then describe methods to incorporate this probability into NMT, either through linear interpolation with the NMT probabilities (\u00a73.2.2) or as the bias to the NMT predictive distribution (\u00a73.2.1). We construct these lexicon probabilities by using traditional word alignment methods on the training data (\u00a74.1), other external parallel data resources such as a handmade dictionary (\u00a74.2), or using a hybrid between the two (\u00a74.3).\nWe perform experiments (\u00a75) on two EnglishJapanese translation corpora to evaluate the method\u2019s utility in improving translation accuracy and reducing the time required for training."}, {"heading": "2 Neural Machine Translation", "text": "The goal of machine translation is to translate a sequence of source words F = f |F |\n1 into a sequence of\ntarget words E = e|E| 1\n. These words belong to the source vocabulary Vf , and the target vocabulary Ve respectively. NMT performs this translation by calculating the conditional probability pm(ei|F, e i\u22121 1\n) of the ith target word ei based on the source F and the preceding target words ei\u22121\n1 . This is done by en-\ncoding the context \u3008F, ei\u22121 1\n\u3009 a fixed-width vector \u03b7i, and calculating the probability as follows:\npm(ei|F, e i\u22121 1 ) = softmax(Ws\u03b7i + bs), (1)\nwhere Ws and bs are respectively weight matrix and bias vector parameters.\nThe exact variety of the NMT model depends on how we calculate \u03b7i used as input. While there\nare many methods to perform this modeling, we opt to use attentional models (Bahdanau et al., 2015), which focus on particular words in the source sentence when calculating the probability of ei. These models represent the current state of the art in NMT, and are also convenient for use in our proposed method. Specifically, we use the method of Luong et al. (2015a), which we describe briefly here and refer readers to the original paper for details.\nFirst, an encoder converts the source sentence F into a matrix R where each column represents a single word in the input sentence as a continuous vector. This representation is generated using a bidirectional encoder\n\u2212\u2192r j = enc(embed(fj), \u2212\u2192r j\u22121) \u2190\u2212r j = enc(embed(fj), \u2190\u2212r j+1)\nrj = [ \u2190\u2212r j ; \u2212\u2192r j ].\nHere the embed(\u00b7) function maps the words into a representation (Bengio et al., 2003), and enc(\u00b7) is a stacking long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Sutskever et al., 2014). Finally we concatenate the two vectors \u2212\u2192r j and\n\u2190\u2212r j into a bidirectional representation rj . These vectors are further concatenated into the matrix R where the jth column corresponds to rj .\nNext, we generate the output one word at a time while referencing this encoded input sentence and tracking progress with a decoder LSTM. The decoder\u2019s hidden state hi is a fixed-length continuous vector representing the previous target words ei\u22121\n1 ,\ninitialized as h0 = 0. Based on this hi, we calculate a similarity vector \u03b1i, with each element equal to\n\u03b1i,j = sim(hi, rj). (2)\nsim(\u00b7) can be an arbitrary similarity function, which we set to the dot product, following Luong et al. (2015a). We then normalize this into an attention vector, which weights the amount of focus that we put on each word in the source sentence\nai = softmax(\u03b1i). (3)\nThis attention vector is then used to weight the encoded representation R to create a context vector ci for the current time step\nc = Ra.\nFinally, we create \u03b7i by concatenating the previous hidden state hi\u22121 with the context vector, and performing an affine transform\n\u03b7i = W\u03b7[hi\u22121; ci] + b\u03b7,\nOnce we have this representation of the current state, we can calculate pm(ei|F, e i\u22121 1\n) according to Equation (1). The next word ei is chosen according to this probability, and we update the hidden state by inputting the chosen word into the decoder LSTM\nhi = enc(embed(ei),hi\u22121). (4)\nIf we define all the parameters in this model as \u03b8, we can then train the model by minimizing the negative log-likelihood of the training data\n\u03b8\u0302 = argmin \u03b8\n\u2211\n\u3008F, E\u3009\n\u2211\ni\n\u2212 log(pm(ei|F, e i\u22121 1 ; \u03b8))."}, {"heading": "3 Integrating Lexicons into NMT", "text": "In \u00a72 we described how traditional NMT models calculate the probability of the next target word pm(ei|e i\u22121 1\n, F ). Our goal in this paper is to improve the accuracy of this probability estimate by incorporating information from discrete probabilistic lexicons. We assume that we have a lexicon that, given a source word f , assigns a probability pl(e|f) to target word e. For a source word f , this probability will generally be non-zero for a small number of translation candidates, and zero for the majority of words in VE . In this section, we first describe how we incorporate these probabilities into NMT, and explain how we actually obtain the pl(e|f) probabilities in \u00a74."}, {"heading": "3.1 Converting Lexicon Probabilities into Conditioned Predictive Proabilities", "text": "First, we need to convert lexical probabilities pl(e|f) for the individual words in the source sentence F to a form that can be used together with pm(ei|e i\u22121 1\n, F ). Given input sentence F , we can construct a matrix in which each column corresponds to a word in the input sentence, each row corresponds to a word in the VE , and the entry corresponds to the appropriate lexical probability:\nLF =\n\n  pl(e = 1|f1) \u00b7 \u00b7 \u00b7 pl(e = 1|f|F |) ... . . . ...\npl(e = |Ve||f1) \u00b7 \u00b7 \u00b7 pl(e = |Ve||f|F |)\n\n  .\nThis matrix can be precomputed during the encoding stage because it only requires information about the source sentence F .\nNext we convert this matrix into a predictive probability over the next word: pl(ei|F, e i\u22121 1\n). To do so we use the alignment probability a from Equation (3) to weight each column of the LF matrix:\npl(ei|F, e i\u22121 1\n) = LFai = \n  pl(e = 1|f1) \u00b7 \u00b7 \u00b7 plex(e = 1|f|F |) ... . . . ...\npl(e = Ve|f1) \u00b7 \u00b7 \u00b7 plex(e = Ve|f|F |)\n\n \n\n  ai,1 ...\nai,|F |\n\n  .\nThis calculation is similar to the way how attentional models calculate the context vector ci, but over a vector representing the probabilities of the target vocabulary, instead of the distributed representations of the source words. The process of involving ai is important because at every time step i, the lexical probability pl(ei|e i\u22121 1\n, F ) will be influenced by different source words."}, {"heading": "3.2 Combining Predictive Probabilities", "text": "After calculating the lexicon predictive probability pl(ei|e i\u22121 1\n, F ), next we need to integrate this probability with the NMT model probability pm(ei|e i\u22121 1\n, F ). To do so, we examine two methods: (1) adding it as a bias, and (2) linear interpolation."}, {"heading": "3.2.1 Model Bias", "text": "In our first bias method, we use pl(\u00b7) to bias the probability distribution calculated by the vanilla NMT model. Specifically, we add a small constant \u01eb to pl(\u00b7), take the logarithm, and add this adjusted log probability to the input of the softmax as follows:\npb(ei|F, e i\u22121 1 ) = softmax(Ws\u03b7i + bs+\nlog(pl(ei|F, e i\u22121 1 ) + \u01eb)).\nWe take the logarithm of pl(\u00b7) so that the values will still be in the probability domain after the softmax is calculated, and add the hyper-parameter \u01eb to prevent zero probabilities from becoming \u2212\u221e after taking the log. When \u01eb is small, the model will be more heavily biased towards using the lexicon, and when \u01eb is larger the lexicon probabilities will be given less weight. We use \u01eb = 0.001 for this paper."}, {"heading": "3.2.2 Linear Interpolation", "text": "We also attempt to incorporate the two probabilities through linear interpolation between the standard NMT probability model probability pm(\u00b7) and the lexicon probability pl(\u00b7). We will call this the linear method, and define it as follows:\npo(ei|F, e i\u22121 1\n) = \n \npl(ei = 1|F, e i\u22121 1 ) pm(e = 1|F, e i\u22121 1\n) ...\n... pl(ei = |Ve||F, e i\u22121 1 ) pm(e = |Ve||F, e i\u22121 1 )\n\n \n[\n\u03bb\n1\u2212 \u03bb\n]\n,\nwhere \u03bb is an interpolation coefficient that is the result of the sigmoid function \u03bb = sig(x) = 1\n1+e\u2212x .\nx is a learnable parameter, and the sigmoid function ensures that the final interpolation level falls between 0 and 1. We choose x = 0 (\u03bb = 0.5) at the beginning of training.\nThis notation is partly inspired by Allamanis et al. (2016) and Gu et al. (2016) who use linear interpolation to merge a standard attentional model with a \u201ccopy\u201d operator that copies a source word as-is into the target sentence. The main difference is that they use this to copy words into the output while our method uses it to influence the probabilities of all target words."}, {"heading": "4 Constructing Lexicon Probabilities", "text": "In the previous section, we have defined some ways to use predictive probabilities pl(ei|F, e i\u22121 1\n) based on word-to-word lexical probabilities pl(e|f). Next, we define three ways to construct these lexical probabilities using automatically learned lexicons, handmade lexicons, or a combination of both."}, {"heading": "4.1 Automatically Learned Lexicons", "text": "In traditional SMT systems, lexical translation probabilities are generally learned directly from parallel data in an unsupervised fashion using a model such as the IBM models (Brown et al., 1993; Och and Ney, 2003). These models can be used to estimate the alignments and lexical translation probabilities pl(e|f) between the tokens of the two languages using the expectation maximization (EM) algorithm.\nFirst in the expectation step, the algorithm estimates the expected count c(e|f). In the maximiza-\ntion step, lexical probabilities are calculated by dividing the expected count by all possible counts:\npl,a(e|f) = c(f, e) \u2211\ne\u0303 c(f, e\u0303) ,\nThe IBM models vary in level of refinement, with Model 1 relying solely on these lexical probabilities, and latter IBM models (Models 2, 3, 4, 5) introducing more sophisticated models of fertility and relative alignment. Even though IBM models also occasionally have problems when dealing with the rare words (e.g. \u201cgarbage collecting\u201d effects (Liang et al., 2006)), traditional SMT systems generally achieve better translation accuracies of lowfrequency words than NMT systems (Sutskever et al., 2014), indicating that these problems are less prominent than they are in NMT.\nNote that in many cases, NMT limits the target vocabulary (Jean et al., 2015) for training speed or memory constraints, resulting in rare words not being covered by the NMT vocabulary VE . Accordingly, we allocate the remaining probability assigned by the lexicon to the unknown word symbol \u3008unk\u3009:\npl,a(e = \u3008unk\u3009|f) = 1\u2212 \u2211\ni\u2208Ve\npl,a(e = i|f). (5)"}, {"heading": "4.2 Manual Lexicons", "text": "In addition, for many language pairs, broadcoverage handmade dictionaries exist, and it is desirable that we be able to use the information included in them as well. Unlike automatically learned lexicons, however, handmade dictionaries generally do not contain translation probabilities. To construct the probability pl(e|f), we define the set of translations Kf existing in the dictionary for particular source word f , and assume a uniform distribution over these words:\npl,m(e|f) =\n{\n1\n|Kf | if e \u2208 Kf 0 otherwise .\nFollowing Equation (5), unknown source words will assign their probability mass to the \u3008unk\u3009 tag."}, {"heading": "4.3 Hybrid Lexicons", "text": "Handmade lexicons have broad coverage of words but their probabilities might not be as accurate as the\nlearned ones, particularly if the automatic lexicon is constructed on in-domain data. Thus, we also test a hybrid method where we use the handmade lexicons to complement the automatically learned lexicon.2 3 Specifically, inspired by phrase table fill-up used in PBMT systems (Bisazza et al., 2011), we use the probability of the automatically learned lexicons pl,a by default, and fall back to the handmade lexicons pl,m only for uncovered words:\npl,h(e|f) =\n{\npl,a(e|f) if f is covered pl,m(e|f) otherwise (6)"}, {"heading": "5 Experiment & Result", "text": "In this section, we describe experiments we use to evaluate our proposed methods."}, {"heading": "5.1 Settings", "text": "Dataset: We perform experiments on two widelyused tasks for the English-to-Japanese language pair: KFTT (Neubig, 2011) and BTEC (Kikui et al., 2003). KFTT is a collection of Wikipedia article about city of Kyoto and BTEC is a travel conversation corpus. BTEC is an easier translation task than KFTT, because KFTT covers a broader domain, has a larger vocabulary of rare words, and has relatively long sentences. The details of each corpus are depicted in Table 1.\nWe tokenize English according to the Penn Treebank standard (Marcus et al., 1993) and lowercase,\n2Alternatively, we could imagine a method where we combined the training data and dictionary before training the word alignments to create the lexicon. We attempted this, and results were comparable to or worse than the fill-up method, so we use the fill-up method for the remainder of the paper.\n3While most words in the Vf will be covered by the learned lexicon, many words (13% in experiments) are still left uncovered due to alignment failures or other factors.\nand tokenize Japanese using KyTea (Neubig et al., 2011). We limit training sentence length up to 50 in both experiments and keep the test data at the original length. We replace words of frequency less than a threshold u in both languages with the \u3008unk\u3009 symbol and exclude them from our vocabulary. We choose u = 1 for BTEC and u = 3 for KFTT, resulting in |Vf | = 17.8k, |Ve| = 21.8k for BTEC and |Vf | = 48.2k, |Ve| = 49.1k for KFTT. NMT Systems: We build the described models using the Chainer4 toolkit. The depth of the stacking LSTM is d = 4 and hidden node size h = 800. We concatenate the forward and backward encodings (resulting in a 1600 dimension vector) and then perform a linear transformation to 800 dimensions.\nWe train the system using the Adam (Kingma and Ba, 2014) optimization method with the default settings: \u03b1 = 1e\u22123, \u03b21 = 0.9, \u03b22 = 0.999, \u01eb = 1e\u22128. Additionally, we add dropout (Srivastava et al., 2014) with drop rate r = 0.2 at the last layer of each stacking LSTM unit to prevent overfitting. We use a batch size of B = 64 and we run a total of N = 14 iterations for all data sets. All of the experiments are conducted on a single GeForce GTX TITAN X GPU with a 12 GB memory cache.\nAt test time, we use beam search with beam size b = 5. We follow Luong et al. (2015b) in replacing every unknown token at position i with the target token that maximizes the probability pl,a(ei|fj). We choose source word fj according to the highest alignment score in Equation (3). This unknown word replacement is applied to both baseline and proposed systems. Finally, because NMT models tend to give higher probabilities to shorter sentences (Cho et al., 2014), we discount the probability of \u3008EOS\u3009 token by 10% to correct for this bias. Traditional SMT Systems: We also prepare two traditional SMT systems for comparison: a PBMT system (Koehn et al., 2003) using Moses5 (Koehn et al., 2007), and a hierarchical phrase-based MT system (Chiang, 2007) using Travatar6 (Neubig, 2013), Systems are built using the default settings, with models trained on the training data, and weights tuned on the development data. Lexicons: We use a total of 3 lexicons for the\n4http://chainer.org/index.html 5http://www.statmt.org/moses/ 6http://www.phontron.com/travatar/\nproposed method, and apply bias and linear method for all of them, totaling 6 experiments. The first lexicon (auto) is built on the training data using the automatically learned lexicon method of \u00a74.1 separately for both the BTEC and KFTT experiments. Automatic alignment is performed using GIZA++ (Och and Ney, 2003). The second lexicon (man) is built using the popular English-Japanese dictionary Eijiro7 with the manual lexicon method of \u00a74.2. Eijiro contains 104K distinct word-to-word translation entries. The third lexicon (hyb) is built by combining the first and second lexicon with the hybrid method of \u00a74.3. Evaluation: We use standard single reference BLEU-4 (Papineni et al., 2002) to evaluate the translation performance. Additionally, we also use NIST (Doddington, 2002), which is a measure that puts a particular focus on low-frequency word strings, and thus is sensitive to the low-frequency words we are focusing on in this paper. We measure the statistical significant differences between systems using paired bootstrap resampling (Koehn, 2004) with 10,000 iterations and measure statistical significance at the p < 0.05 and p < 0.10 levels.\nAdditionally, we also calculate the recall of rare words from the references. We define \u201crare words\u201d as words that appear less than eight times in the target training corpus or references, and measure the percentage of time they are recovered by each translation system."}, {"heading": "5.2 Effect of Integrating Lexicons", "text": "In this section, we first a detailed examination of the utility of the proposed bias method when used\n7http://eijiro.jp\nwith the auto or hyb lexicons, which empirically gave the best results, and perform a comparison among the other lexicon integration methods in the following section. Table 2 shows the results of these methods, along with the corresponding baselines.\nFirst, compared to the baseline attn, our bias method achieved consistently higher scores on both test sets. In particular, the gains on the more difficult KFTT set are large, up to 2.3 BLEU, 0.44 NIST, and 30% Recall, demonstrating the utility of the proposed method in the face of more diverse content and fewer high-frequency words.\nCompared to the traditional pbmt systems hiero, particularly on KFTT we can see that the proposed method allows the NMT system to exceed the traditional SMT methods in BLEU. This is despite the fact that we are not performing ensembling, which has proven to be essential to exceed traditional systems in several previous works (Sutskever\net al., 2014; Luong et al., 2015a; Sennrich et al., 2016). Interestingly, despite gains in BLEU, the NMT methods still fall behind in NIST score on the KFTT data set, demonstrating that traditional SMT systems still tend to have a small advantage in translating lower-frequency words, despite the gains made by the proposed method.\nIn Table 3, we show some illustrative examples where the proposed method (auto-bias) was able to obtain a correct translation while the normal attentional model was not. The first example is a mistake in translating \u201cextramarital affairs\u201d into the Japanese equivalent of \u201csoccer,\u201d entirely changing the main topic of the sentence. This is typical of the errors that we have observed NMT systems make (the mistake from Figure 1 is also from attn, and was fixed by our proposed method). The second example demonstrates how these mistakes can then affect the process of choosing the remaining words, propagating the error through the whole sentence.\nNext, we examine the effect of the proposed method on the training time for each neural MT method, drawing training curves for the KFTT data in Figure 2. Here we can see that the proposed bias training methods achieve reasonable BLEU scores in the upper 10s even after the first iteration. In contrast, the baseline attn method has a BLEU score of around 5 after the first iteration, and takes significantly longer to approach values close to its maximal\naccuracy. This shows that by incorporating lexical probabilities, we can effectively bootstrap the learning of the NMT system, allowing it to approach an appropriate answer in a more timely fashion.8\nIt is also interesting to examine the alignment vec-\n8Note that these gains are despite the fact that one iteration of the proposed method takes a longer (167 minutes for attn vs. 275 minutes for auto-bias) due to the necessity to calculate and use the lexical probability matrix for each sentence. It also takes an additional 297 minutes to train the lexicon with GIZA++, but this can be greatly reduced with more efficient training methods (Dyer et al., 2013).\ntors produced by the baseline and proposed methods, a visualization of which we show in Figure 3. For this sentence, the outputs of both methods were both identical and correct, but we can see that the proposed method (right) placed sharper attention on the actual source word corresponding to content words in the target sentence. This trend of peakier attention distributions in the proposed method held throughout the corpus, with the per-word entropy of the attention vectors being 3.23 bits for auto-bias, compared with 3.81 bits for attn, indicating that the auto-bias method places more certainty in its attention decisions."}, {"heading": "5.3 Comparison of Integration Methods", "text": "Finally, we perform a full comparison between the various methods for integrating lexicons into the translation process, with results shown in Table 4. In general the bias method improves accuracy for the auto and hyb lexicon, but is less effective for the man lexicon. This is likely due to the fact that the manual lexicon, despite having broad coverage, did not sufficiently cover target-domain words (coverage of unique words in the source vocabulary was 35.3% and 9.7% for BTEC and KFTT respectively).\nInterestingly, the trend is reversed for the linear method, with it improving man systems,\nbut causing decreases when using the auto and hyb lexicons. This indicates that the linear method is more suited for cases where the lexicon does not closely match the target domain, and plays a more complementary role. Compared to the log-linear modeling of bias, which strictly enforces constraints imposed by the lexicon distribution (Klakow, 1998), linear interpolation is intuitively more appropriate for integrating this type of complimentary information.\nOn the other hand, the performance of linear interpolation was generally lower than that of the bias method. One potential reason for this is the fact that we use a constant interpolation coefficient that was set fixed in every context. Gu et al. (2016) have recently developed methods to use the context information from the decoder to calculate the different interpolation coefficients for every decoding step, and it is possible that introducing these methods would improve our results."}, {"heading": "6 Additional Experiments", "text": "To test whether the proposed method is useful on larger data sets, we also performed follow-up experiments on the larger Japanese-English ASPEC dataset (Nakazawa et al., 2016) that consist of 2 million training examples, 63 million tokens, and 81,000 vocabulary size. We gained an improvement in BLEU score from 20.82 using the attn baseline to 22.66 using the auto-bias proposed method. This experiment shows that our method scales to larger datasets."}, {"heading": "7 Related Work", "text": "From the beginning of work on NMT, unknown words that do not exist in the system vocabulary have been focused on as a weakness of these systems. Early methods to handle these unknown words replaced them with appropriate words in the target vocabulary (Jean et al., 2015; Luong et al., 2015b) according to a lexicon similar to the one used in this work. In contrast to our work, these only handle unknown words and do not incorporate information from the lexicon in the learning procedure.\nThere have also been other approaches that incorporate models that learn when to copy words as-is into the target language (Allamanis et al., 2016; Gu\net al., 2016; Gu\u0308lc\u0327ehre et al., 2016). These models are similar to the linear approach of \u00a73.2.2, but are only applicable to words that can be copied asis into the target language. In fact, these models can be thought of as a subclass of the proposed approach that use a lexicon that assigns a all its probability to target words that are the same as the source. On the other hand, while we are simply using a static interpolation coefficient \u03bb, these works generally have a more sophisticated method for choosing the interpolation between the standard and \u201ccopy\u201d models. Incorporating these into our linear method is a promising avenue for future work.\nIn addition Mi et al. (2016) have also recently proposed a similar approach by limiting the number of vocabulary being predicted by each batch or sentence. This vocabulary is made by considering the original HMM alignments gathered from the training corpus. Basically, this method is a specific version of our bias method that gives some of the vocabulary a bias of negative infinity and all other vocabulary a uniform distribution. Our method improves over this by considering actual translation probabilities, and also considering the attention vector when deciding how to combine these probabilities.\nFinally, there have been a number of recent works that improve accuracy of low-frequency words using character-based translation models (Ling et al., 2015; Costa-Jussa\u0300 and Fonollosa, 2016; Chung et al., 2016). However, Luong and Manning (2016) have found that even when using character-based models, incorporating information about words allows for gains in translation accuracy, and it is likely that our lexicon-based method could result in improvements in these hybrid systems as well."}, {"heading": "8 Conclusion & Future Work", "text": "In this paper, we have proposed a method to incorporate discrete probabilistic lexicons into NMT systems to solve the difficulties that NMT systems have demonstrated with low-frequency words. As a result, we achieved substantial increases in BLEU (2.0-2.3) and NIST (0.13-0.44) scores, and observed qualitative improvements in the translations of content words.\nFor future work, we are interested in conducting the experiments on larger-scale translation tasks. We\nalso plan to do subjective evaluation, as we expect that improvements in content word translation are critical to subjective impressions of translation results. Finally, we are also interested in improvements to the linear method where \u03bb is calculated based on the context, instead of using a fixed value."}, {"heading": "Acknowledgment", "text": "We thank Makoto Morishita and Yusuke Oda for their help in this project. We also thank the faculty members of AHC lab for their supports and suggestions.\nThis work was supported by grants from the Ministry of Education, Culture, Sport, Science, and Technology of Japan and in part by JSPS KAKENHI Grant Number 16H05873."}], "references": [{"title": "A convolutional attention network for extreme summarization of source code", "author": ["Miltiadis Allamanis", "Hao Peng", "Charles Sutton."], "venue": "Proceedings of the 33th International Conference on Machine Learning (ICML).", "citeRegEx": "Allamanis et al\\.,? 2016", "shortCiteRegEx": "Allamanis et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 4th International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "Journal of Machine Learning Research, pages 1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Fill-up versus interpolation methods for phrasebased SMT adaptation", "author": ["Arianna Bisazza", "Nick Ruiz", "Marcello Federico."], "venue": "Proceedings of the 2011 International Workshop on Spoken Language Translation (IWSLT), pages 136\u2013143.", "citeRegEx": "Bisazza et al\\.,? 2011", "shortCiteRegEx": "Bisazza et al\\.", "year": 2011}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer."], "venue": "Computational Linguistics, pages 263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Computational Linguistics, pages 201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of the Workshop on Syntax and Structure in Statistical Translation (SSST), pages", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1693\u20131703.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 357\u2013361.", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.Juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["George Doddington."], "venue": "Proceedings of the Second International Conference on Human Language Technology Research, pages 138\u2013145.", "citeRegEx": "Doddington.,? 2002", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "A simple, fast, and effective reparameterization of IBM model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A. Gers", "J\u00fcrgen A. Schmidhuber", "Fred A. Cummins."], "venue": "Neural Computation, pages 2451\u20132471.", "citeRegEx": "Gers et al\\.,? 2000", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Incorporating copying mechanism in sequenceto-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1631\u20131640.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 140\u2013149.", "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? 2016", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "KyungHyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Creating corpora for speech-to-speech translation", "author": ["Gen-ichiro Kikui", "Eiichiro Sumita", "Toshiyuki Takezawa", "Seiichi Yamamoto."], "venue": "8th European Conference on Speech Communication and Technology, EU-", "citeRegEx": "Kikui et al\\.,? 2003", "shortCiteRegEx": "Kikui et al\\.", "year": 2003}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Log-linear interpolation of language models", "author": ["Dietrich Klakow."], "venue": "Proceedings of the 5th International Conference on Speech and Language Processing (ICSLP).", "citeRegEx": "Klakow.,? 1998", "shortCiteRegEx": "Klakow.", "year": 1998}, {"title": "Statistical phrase-based translation", "author": ["Phillip Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 48\u2013", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Alignment by agreement", "author": ["Percy Liang", "Ben Taskar", "Dan Klein."], "venue": "Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 104\u2013111.", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."], "venue": "CoRR.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1054\u20131063.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics (ACL) and the", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The Kyoto free translation task", "author": ["Graham Neubig"], "venue": null, "citeRegEx": "Neubig.,? \\Q2011\\E", "shortCiteRegEx": "Neubig.", "year": 2011}, {"title": "Travatar: A forest-to-string", "author": ["Graham Neubig"], "venue": null, "citeRegEx": "Neubig.,? \\Q2013\\E", "shortCiteRegEx": "Neubig.", "year": 2013}, {"title": "Bleu: A method for automatic", "author": ["Jing Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2002\\E", "shortCiteRegEx": "Zhu.", "year": 2002}], "referenceMentions": [{"referenceID": 25, "context": "NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016).", "startOffset": 201, "endOffset": 245}, {"referenceID": 15, "context": "Neural machine translation (NMT, \u00a72; Kalchbrenner and Blunsom (2013), Sutskever et al.", "startOffset": 37, "endOffset": 69}, {"referenceID": 15, "context": "Neural machine translation (NMT, \u00a72; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al.", "startOffset": 37, "endOffset": 94}, {"referenceID": 4, "context": "(2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks.", "startOffset": 62, "endOffset": 82}, {"referenceID": 20, "context": "This is in contrast to more traditional SMT methods such as phrase-based machine translation (PBMT; Koehn et al. (2003)), which represent translations as discrete pairs of word strings in the source and target languages.", "startOffset": 100, "endOffset": 120}, {"referenceID": 1, "context": "1) into a predictive probability for the next word by utilizing attention vectors from attentional NMT models (Bahdanau et al., 2015).", "startOffset": 110, "endOffset": 133}, {"referenceID": 1, "context": "While there are many methods to perform this modeling, we opt to use attentional models (Bahdanau et al., 2015), which focus on particular words in the source sentence when calculating the probability of ei.", "startOffset": 88, "endOffset": 111}, {"referenceID": 1, "context": "While there are many methods to perform this modeling, we opt to use attentional models (Bahdanau et al., 2015), which focus on particular words in the source sentence when calculating the probability of ei. These models represent the current state of the art in NMT, and are also convenient for use in our proposed method. Specifically, we use the method of Luong et al. (2015a), which we describe briefly here and refer readers to the original paper for details.", "startOffset": 89, "endOffset": 380}, {"referenceID": 2, "context": "Here the embed(\u00b7) function maps the words into a representation (Bengio et al., 2003), and enc(\u00b7) is a stacking long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 14, "context": ", 2003), and enc(\u00b7) is a stacking long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Sutskever et al., 2014).", "startOffset": 79, "endOffset": 156}, {"referenceID": 11, "context": ", 2003), and enc(\u00b7) is a stacking long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Sutskever et al., 2014).", "startOffset": 79, "endOffset": 156}, {"referenceID": 25, "context": "sim(\u00b7) can be an arbitrary similarity function, which we set to the dot product, following Luong et al. (2015a). We then normalize this into an attention vector, which weights the amount of focus that we put on each word in the source sentence", "startOffset": 91, "endOffset": 112}, {"referenceID": 0, "context": "This notation is partly inspired by Allamanis et al. (2016) and Gu et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 0, "context": "This notation is partly inspired by Allamanis et al. (2016) and Gu et al. (2016) who use linear interpolation to merge a standard attentional model with a \u201ccopy\u201d operator that copies a source word as-is into the target sentence.", "startOffset": 36, "endOffset": 81}, {"referenceID": 4, "context": "In traditional SMT systems, lexical translation probabilities are generally learned directly from parallel data in an unsupervised fashion using a model such as the IBM models (Brown et al., 1993; Och and Ney, 2003).", "startOffset": 176, "endOffset": 215}, {"referenceID": 22, "context": "\u201cgarbage collecting\u201d effects (Liang et al., 2006)), traditional SMT systems generally achieve better translation accuracies of lowfrequency words than NMT systems (Sutskever et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 15, "context": "Note that in many cases, NMT limits the target vocabulary (Jean et al., 2015) for training speed or memory constraints, resulting in rare words not being covered by the NMT vocabulary VE .", "startOffset": 58, "endOffset": 77}, {"referenceID": 3, "context": "2 3 Specifically, inspired by phrase table fill-up used in PBMT systems (Bisazza et al., 2011), we use the probability of the automatically learned lexicons pl,a by default, and fall back to the handmade lexicons pl,m only for uncovered words:", "startOffset": 72, "endOffset": 94}, {"referenceID": 27, "context": "Dataset: We perform experiments on two widelyused tasks for the English-to-Japanese language pair: KFTT (Neubig, 2011) and BTEC (Kikui et al.", "startOffset": 104, "endOffset": 118}, {"referenceID": 17, "context": "Dataset: We perform experiments on two widelyused tasks for the English-to-Japanese language pair: KFTT (Neubig, 2011) and BTEC (Kikui et al., 2003).", "startOffset": 128, "endOffset": 148}, {"referenceID": 18, "context": "We train the system using the Adam (Kingma and Ba, 2014) optimization method with the default settings: \u03b1 = 1e\u22123, \u03b21 = 0.", "startOffset": 35, "endOffset": 56}, {"referenceID": 6, "context": "Finally, because NMT models tend to give higher probabilities to shorter sentences (Cho et al., 2014), we discount the probability of \u3008EOS\u3009 token by 10% to correct for this bias.", "startOffset": 83, "endOffset": 101}, {"referenceID": 20, "context": "Traditional SMT Systems: We also prepare two traditional SMT systems for comparison: a PBMT system (Koehn et al., 2003) using Moses5 (Koehn et al.", "startOffset": 99, "endOffset": 119}, {"referenceID": 5, "context": ", 2007), and a hierarchical phrase-based MT system (Chiang, 2007) using Travatar6 (Neubig, 2013), Systems are built using the default settings, with models trained on the training data, and weights tuned on the development data.", "startOffset": 51, "endOffset": 65}, {"referenceID": 28, "context": ", 2007), and a hierarchical phrase-based MT system (Chiang, 2007) using Travatar6 (Neubig, 2013), Systems are built using the default settings, with models trained on the training data, and weights tuned on the development data.", "startOffset": 82, "endOffset": 96}, {"referenceID": 16, "context": "We train the system using the Adam (Kingma and Ba, 2014) optimization method with the default settings: \u03b1 = 1e\u22123, \u03b21 = 0.9, \u03b22 = 0.999, \u01eb = 1e\u22128. Additionally, we add dropout (Srivastava et al., 2014) with drop rate r = 0.2 at the last layer of each stacking LSTM unit to prevent overfitting. We use a batch size of B = 64 and we run a total of N = 14 iterations for all data sets. All of the experiments are conducted on a single GeForce GTX TITAN X GPU with a 12 GB memory cache. At test time, we use beam search with beam size b = 5. We follow Luong et al. (2015b) in replacing every unknown token at position i with the target token that maximizes the probability pl,a(ei|fj).", "startOffset": 36, "endOffset": 568}, {"referenceID": 9, "context": "Additionally, we also use NIST (Doddington, 2002), which is a measure that puts a particular focus on low-frequency word strings, and thus is sensitive to the low-frequency words we are focusing on in this paper.", "startOffset": 31, "endOffset": 49}, {"referenceID": 21, "context": "We measure the statistical significant differences between systems using paired bootstrap resampling (Koehn, 2004) with 10,000 iterations and measure statistical significance at the p < 0.", "startOffset": 101, "endOffset": 114}, {"referenceID": 10, "context": "It also takes an additional 297 minutes to train the lexicon with GIZA++, but this can be greatly reduced with more efficient training methods (Dyer et al., 2013).", "startOffset": 143, "endOffset": 162}, {"referenceID": 19, "context": "Compared to the log-linear modeling of bias, which strictly enforces constraints imposed by the lexicon distribution (Klakow, 1998), linear interpolation is intuitively more appropriate for integrating this type of complimentary information.", "startOffset": 117, "endOffset": 131}, {"referenceID": 12, "context": "Gu et al. (2016) have recently developed methods to use the context information from the decoder to calculate the different interpolation coefficients for every decoding step, and it is possible that introducing these methods would improve our results.", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "Early methods to handle these unknown words replaced them with appropriate words in the target vocabulary (Jean et al., 2015; Luong et al., 2015b) according to a lexicon similar to the one used in this work.", "startOffset": 106, "endOffset": 146}, {"referenceID": 26, "context": "Early methods to handle these unknown words replaced them with appropriate words in the target vocabulary (Jean et al., 2015; Luong et al., 2015b) according to a lexicon similar to the one used in this work.", "startOffset": 106, "endOffset": 146}, {"referenceID": 23, "context": "Finally, there have been a number of recent works that improve accuracy of low-frequency words using character-based translation models (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 136, "endOffset": 208}, {"referenceID": 8, "context": "Finally, there have been a number of recent works that improve accuracy of low-frequency words using character-based translation models (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 136, "endOffset": 208}, {"referenceID": 7, "context": "Finally, there have been a number of recent works that improve accuracy of low-frequency words using character-based translation models (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 136, "endOffset": 208}, {"referenceID": 7, "context": ", 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016). However, Luong and Manning (2016) have found that even when using character-based models, incorporating information about words allows for gains in translation accuracy, and it is likely that our lexicon-based method could result in improvements in these hybrid systems as well.", "startOffset": 41, "endOffset": 96}], "year": 2016, "abstractText": "Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.1", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}