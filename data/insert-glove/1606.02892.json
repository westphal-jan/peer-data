{"id": "1606.02892", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Linguistic Input Features Improve Neural Machine Translation", "abstract": "identically Neural machine love translation dobi has recently achieved yucca impressive results, nasrudin while tuita using little miwok in tembec the kopechne way of cassella external linguistic information. sandhill In this 2,501 paper self-released we show valdiserri that the choden strong learning capability bakti of neural MT models gynaecology does sent-off not guruvayoor make 23-percent linguistic 1908-09 features 245-pound redundant; they can macroinvertebrates be jellyvision easily incorporated torricelli to thi\u00e8s provide deuel further tavoy improvements in fiddled performance. We generalize the absolut embedding layer mugyenyi of bassoon the 220.1 encoder in threat the encima attentional khajimba encoder - - decoder xiaoxu architecture brisa to support shizuoka the inclusion wlef of irishness arbitrary maguelone features, alanine in sluiter addition to the baseline word feature. darzi We 19th-20th add richeza morphological jain features, part - of - pre-historic speech scheinberg tags, and syntactic matraca dependency renascent labels positon as input tundo features to English & 43-foot lt; - & replay gt; German neural conduce machine translation systems. In countermoves experiments on inapt WMT16 training avaz and test zitko sets, we s\u00f6ren find jugular that 5,500-square linguistic communicants input esdp features improve tottering model chilliest quality busselton according qureshi to horwich three metrics: freinademetz perplexity, tkeshelashvili BLEU and agazzi CHRF3.", "histories": [["v1", "Thu, 9 Jun 2016 10:12:36 GMT  (32kb)", "https://arxiv.org/abs/1606.02892v1", "accepted to WMT16"], ["v2", "Mon, 27 Jun 2016 23:11:51 GMT  (32kb)", "http://arxiv.org/abs/1606.02892v2", "WMT16 final version; new EN-RO results"]], "COMMENTS": "accepted to WMT16", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rico sennrich", "barry haddow"], "accepted": false, "id": "1606.02892"}, "pdf": {"name": "1606.02892.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rico.sennrich@ed.ac.uk,", "bhaddow@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n02 89\n2v 2\n[ cs\n.C L\n] 2\n7 Ju\nn 20\n16"}, {"heading": "1 Introduction", "text": "Neural machine translation has recently achieved impressive results (Bahdanau et al., 2015; Jean et al., 2015), while learning from raw, sentence-aligned parallel text and using little in the way of external linguistic information.3 However, we hypothesize that various levels of linguistic annotation can be valuable for neural machine translation. Lemmatisation can reduce data sparseness, and allow inflectional variants of\n1https://github.com/rsennrich/nematus 2https://github.com/rsennrich/wmt16-scripts 3Linguistic tools are most commonly used in preprocess-\ning, e.g. for Turkish segmentation (G\u00fcl\u00e7ehre et al., 2015).\nthe same word to explicitly share a representation in the model. Other types of annotation, such as parts-of-speech (POS) or syntactic dependency labels, can help in disambiguation. In this paper we investigate whether linguistic information is beneficial to neural translation models, or whether their strong learning capability makes explicit linguistic features redundant.\nLet us motivate the use of linguistic features using examples of actual translation errors by neural MT systems. In translation out of English, one problem is that the same surface word form may be shared between several word types, due to homonymy or word formation processes such as conversion. For instance, close can be a verb, adjective, or noun, and these different meanings often have distinct translations into other languages. Consider the following English\u2192German example:\n1. We thought a win like this might be close.\n2. Wir dachten, dass ein solcher Sieg nah sein k\u00f6nnte.\n3. *Wir dachten, ein Sieg wie dieser k\u00f6nnte schlie\u00dfen.\nFor the English source sentence in Example 1 (our translation in Example 2), a neural MT system (our baseline system from Section 4) mistranslates close as a verb, and produces the German verb schlie\u00dfen (Example 3), even though close is an adjective in this sentence, which has the German translation nah. Intuitively, partof-speech annotation of the English input could disambiguate between verb, noun, and adjective meanings of close.\nAs a second example, consider the following German\u2192English example:\n4. Gef\u00e4hrlich ist die Route aber dennoch . dangerous is the route but still .\n5. However the route is dangerous .\n6. *Dangerous is the route , however .\nGerman main clauses have a verb-second (V2) word order, whereas English word order is generally SVO. The German sentence (Example 4; English reference in Example 5) topicalizes the predicate gef\u00e4hrlich \u2019dangerous\u2019, putting the subject die Route \u2019the route\u2019 after the verb. Our baseline system (Example 6) retains the original word order, which is highly unusual in English, especially for prose in the news domain. A syntactic annotation of the source sentence could support the attentional encoder-decoder in learning which words in the German source to attend (and translate) first.\nWe will investigate the usefulness of linguistic features for the language pair German\u2194English, considering the following linguistic features:\n\u2022 lemmas\n\u2022 subword tags (see Section 3.2)\n\u2022 morphological features\n\u2022 POS tags\n\u2022 dependency labels\nThe inclusion of lemmas is motivated by the hope for a better generalization over inflectional variants of the same word form. The other linguistic features are motivated by disambiguation, as discussed in our introductory examples."}, {"heading": "2 Neural Machine Translation", "text": "We follow the neural machine translation architecture by Bahdanau et al. (2015), which we will briefly summarize here.\nThe neural machine translation system is implemented as an attentional encoder-decoder network with recurrent neural networks.\nThe encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1, ..., xm) and calculates a forward sequence of hidden states ( \u2212\u2192 h 1, ..., \u2212\u2192 h m), and a backward sequence ( \u2190\u2212 h 1, ..., \u2190\u2212 hm). The hidden states \u2212\u2192 h j and \u2190\u2212 h j are concatenated to obtain the annotation vector hj . The decoder is a recurrent neural network that predicts a target sequence y = (y1, ..., yn). Each word yi is predicted based on a recurrent hidden state si, the previously predicted word yi\u22121, and\na context vector ci. ci is computed as a weighted sum of the annotations hj . The weight of each annotation hj is computed through an alignment model \u03b1ij , which models the probability that yi is aligned to xj . The alignment model is a singlelayer feedforward neural network that is learned jointly with the rest of the network through backpropagation.\nA detailed description can be found in (Bahdanau et al., 2015), although our implementation is based on a slightly modified form of this architecture, released for the dl4mt tutorial4. Training is performed on a parallel corpus with stochastic gradient descent. For translation, a beam search with small beam size is employed."}, {"heading": "2.1 Adding Input Features", "text": "Our main innovation over the standard encoderdecoder architecture is that we represent the encoder input as a combination of features (Alexandrescu and Kirchhoff, 2006).\nWe here show the equation for the forward states of the encoder (for the simple RNN case; consider (Bahdanau et al., 2015) for GRU):\n\u2212\u2192 h j = tanh( \u2212\u2192 WExj + \u2212\u2192 U \u2212\u2192 h j\u22121) (1)\nwhere E \u2208 Rm\u00d7Kx is a word embedding matrix, \u2212\u2192 W \u2208 Rn\u00d7m, \u2212\u2192 U \u2208 Rn\u00d7n are weight matrices, with m and n being the word embedding size and number of hidden units, respectively, and Kx being the vocabulary size of the source language.\nWe generalize this to an arbitrary number of features |F |:\n\u2212\u2192 h j = tanh( \u2212\u2192 W (\n|F |n\nk=1\nEkxjk) + \u2212\u2192 U \u2212\u2192 h j\u22121) (2)\nwhere \u2016 is the vector concatenation, Ek \u2208 R mk\u00d7Kk are the feature embedding matrices, with \u2211|F | k=1mk = m, and Kk is the vocabulary size of the kth feature. In other words, we look up separate embedding vectors for each feature, which are then concatenated. The length of the concatenated vector matches the total embedding size, and all other parts of the model remain unchanged."}, {"heading": "3 Linguistic Input Features", "text": "Our generalized model of the previous section supports an arbitrary number of input features.\n4https://github.com/nyu-dl/dl4mt-tutorial\nIn this paper, we will focus on a number of well-known linguistic features. Our main empirical question is if providing linguistic features to the encoder improves the translation quality of neural machine translation systems, or if the information emerges from training encoderdecoder models on raw text, making its inclusion via explicit features redundant. All linguistic features are predicted automatically; we use Stanford CoreNLP (Toutanova et al., 2003; Minnen et al., 2001; Chen and Manning, 2014) to annotate the English input for English\u2192German, and ParZu (Sennrich et al., 2013) to annotate the German input for German\u2192English. We here discuss the individual features in more detail."}, {"heading": "3.1 Lemma", "text": "Using lemmas as input features guarantees sharing of information between word forms that share the same base form. In principle, neural models can learn that inflectional variants are semantically related, and represent them as similar points in the continuous vector space (Mikolov et al., 2013). However, while this has been demonstrated for high-frequency words, we expect that a lemmatized representation increases data efficiency; lowfrequency variants may even be unknown to wordlevel models. With character- or subword-level models, it is unclear to what extent they can learn the similarity between low-frequency word forms that share a lemma, especially if the word forms are superficially dissimilar. Consider the following two German word forms, which share the lemma liegen \u2018lie\u2019:\n\u2022 liegt \u2018lies\u2019 (3.p.sg. present)\n\u2022 l\u00e4ge \u2018lay\u2019 (3.p.sg. subjunctive II)\nThe lemmatisers we use are based on finite-state methods, which ensures a large coverage, even for infrequent word forms. We use the Zmorge analyzer for German (Schmid et al., 2004; Sennrich and Kunz, 2014), and the lemmatiser in the Stanford CoreNLP toolkit for English (Minnen et al., 2001)."}, {"heading": "3.2 Subword Tags", "text": "In our experiments, we operate on the level of subwords to achieve open-vocabulary translation with a fixed symbol vocabulary, using a segmentation based on byte-pair encoding (BPE)\n(Sennrich et al., 2016c). We note that in BPE segmentation, some symbols are potentially ambiguous, and can either be a separate word, or a subword segment of a larger word. Also, text is represented as a sequence of subword units with no explicit word boundaries, but word boundaries are potentially helpful to learn which symbols to attend to, and when to forget information in the recurrent layers. We propose an annotation of subword structure similar to popular IOB format for chunking and named entity recognition, marking if a symbol in the text forms the beginning (B), inside (I), or end (E) of a word. A separate tag (O) is used if a symbol corresponds to the full word."}, {"heading": "3.3 Morphological Features", "text": "For German\u2192English, the parser annotates the German input with morphological features. Different word types have different sets of features \u2013 for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect \u2013 and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature value."}, {"heading": "3.4 POS Tags and Dependency Labels", "text": "In our introductory examples, we motivated POS tags and dependency labels as possible disambiguators. Each word is associated with one POS tag, and one dependency label. The latter is the label of the edge connecting a word to its syntactic head, or \u2019ROOT\u2019 if the word has no syntactic head."}, {"heading": "3.5 On Using Word-level Features in a Subword Model", "text": "We segment rare words into subword units using BPE. The subword tags encode the segmentation of words into subword units, and need no further modification. All other features are originally word-level features. To annotate the segmented source text with features, we copy the word\u2019s feature value to all its subword units. An example is shown in Figure 1."}, {"heading": "4 Evaluation", "text": "We evaluate our systems on the WMT16 shared translation task English\u2194German. The parallel training data consists of about 4.2 million sentence pairs.\nTo enable open-vocabulary translation, we encode words via joint BPE5 (Sennrich et al., 2016c), learning 89 500 merge operations on the concatenation of the source and target side of the parallel training data. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via BLEU and perplexity on a validation set (newstest2013).\nFor neural MT, perplexity is a useful measure of how well the model can predict a reference translation given the source sentence. Perplexity is thus a good indicator of whether input features provide any benefit to the models, and we report the best validation set perplexity of each experiment. To evaluate whether the features also increase translation performance, we report casesensitive BLEU scores with mteval-13b.perl on two test sets, newstest2015 and newstest2016. We also report CHRF3 (Popovic\u0301, 2015), a character ngram F3 score which was found to correlate well with human judgments, especially for translations out of English (Stanojevic\u0301 et al., 2015).6 The two metrics may occasionally disagree, partly because they are highly sensitive to the length of the output. BLEU is precision-based, whereas CHRF3 considers both precision and recall, with a bias for recall. For BLEU, we also report whether differences between systems are statistically significant according to a bootstrap resampling significance test (Riezler and Maxwell, 2005).\nWe train models for about a week, and report\n5https://github.com/rsennrich/subword-nmt 6We use the re-implementation included with the subword\ncode\nresults for an ensemble of the 4 last saved models (with models saved every 12 hours). The ensemble serves to smooth the variance between single models.\nDecoding is performed with beam search with a beam size of 12.\nTo ensure that performance improvements are not simply due to an increase in the number of model parameters, we keep the total size of the embedding layer fixed to 500. Table 1 lists the embedding size we use for linguistic features \u2013 the embedding layer size of the word-level feature varies, and is set to bring the total embedding layer size to 500. If we include the lemma feature, we roughly split the embedding vector one-to-two between the lemma feature and the word feature. The table also shows the network vocabulary size; for all features except lemmas, we can represent all feature values in the network vocabulary \u2013 in the case of words, this is due to BPE segmentation. For lemmas, we choose the same vocabulary size as for words, replacing rare lemmas with a special UNK symbol.\nSennrich et al. (2016b) report large gains from using monolingual in-domain training data, auto-\nmatically back-translated into the source language to produce a synthetic parallel training corpus. We use the synthetic corpora produced in these experiments7 (3.6\u20134.2 million sentence pairs), and we trained systems which include this data to compare against the state of the art. We note that our experiments with this data entail a syntactic annotation of automatically translated data, which may be a source of noise. For the systems with synthetic data, we double the training time to two weeks.\nWe also evaluate linguistic features for the lower-resourced translation direction English\u2192Romanian, with 0.6 million sentence pairs of parallel training data, and 2.2 million sentence pairs of synthetic parallel data. We use the same linguistic features as for English\u2192German. We follow Sennrich et al. (2016a) in the configuration, and use dropout for the English\u2192Romanian systems. We drop out full words (both on the source and target side) with a probability of 0.1. For all other layers, the dropout probability is set to 0.2."}, {"heading": "4.1 Results", "text": "Table 2 shows our main results for German\u2192English, and English\u2192German. The baseline system is a neural MT system with only one input feature, the (sub)words themselves. For both translation directions, linguistic features improve the best perplexity on the development data (47.3 \u2192 46.2, and 54.9 \u2192 52.9, respectively). For German\u2192English, the linguistic features lead to an increase of 1.5 BLEU (31.4\u219232.9) and 0.5 CHRF3 (58.0 \u2192 58.5), on the newstest2016 test set. For English\u2192German, we observe improvements of 0.6 BLEU (27.8 \u2192 28.4) and 1.2 CHRF3 (56.0 \u2192 57.2).\nTo evaluate the effectiveness of different linguistic features in isolation, we performed contrastive experiments in which only a single feature was added to the baseline. Results are shown in Table 3. Unsurprisingly, the combination of all features (Table 2) gives the highest improvement, averaged over metrics and test sets, but most features are beneficial on their own. Subword tags give small improvements for English\u2192German, but not for German\u2192English. All other features outperform the baseline in terms of perplexity, and yield significant improvements in BLEU on at least\n7The corpora are available at http://statmt.org/rsennrich/wmt16_backtranslations/\none test set. The gain from different features is not fully cumulative; we note that the information encoded in different features overlaps. For instance, both the dependency labels and the morphological features encode the distinction between German subjects and accusative objects, the former through different labels (subj and obja), the latter through grammatical case (nominative and accusative).\nWe also evaluated adding linguistic features to a stronger baseline, which includes synthetic parallel training data. In addition, we compare our neural systems against phrase-based (PBSMT) and syntax-based (SBSMT) systems by (Williams et al., 2016), all of which make use of linguistic annotation on the source and/or target side. Results are shown in Table 4. For German\u2192English, we observe similar improvements in the best development perplexity (45.2 \u2192 44.1), test set BLEU (37.5\u219238.5) and CHRF3 (62.2 \u2192 62.8). Our test set BLEU is on par to the best submitted system to this year\u2019s WMT 16 shared translation task, which is similar to our baseline MT system, but which also uses a right-to-left decoder for reranking (Sennrich et al., 2016a). We expect that linguistic input features and bidirectional decoding are orthogonal, and that we could obtain further improvements by combining the two.\nFor English\u2192German, improvements in development set perplexity carry over (49.7 \u2192 48.4), but we see only small, non-significant differences in BLEU and CHRF3. While we cannot clearly account for the discrepancy between perplexity and translation metrics, factors that potentially lower the usefulness of linguistic features in this setting are the stronger baseline, trained on more data, and the low robustness of linguistic tools in the annotation of the noisy, synthetic data sets. Both our baseline neural MT systems and the systems with linguistic features substantially outperform phrase-based and syntax-based systems for both translation directions.\nIn the previous tables, we have reported the best perplexity. To address the question about the randomness in perplexity, and whether the best perplexity just happened to be lower for the systems with linguistic features, we show perplexity on our development set as a function of training time for different systems (Figure 2). We can see that perplexity is consistently lower for the systems\ntrained with linguistic features.\nTable 5 shows results for a lower-resourced language pair, English\u2192Romanian. With linguistic features, we observe improvements of 1.0 BLEU over the baseline, both for the systems trained on parallel data only (23.8\u219224.8), and the systems which use synthetic training data (28.2\u219229.2). According to BLEU, the best submission to WMT16 was a system combination by Peter et al. (2016). Our best system is competitive with this submission.\nTable 6 shows translation examples of our baseline, and the system augmented with linguistic features. We see that the augmented neural MT systems, in contrast to the respective baselines, successfully resolve the reordering for the German\u2192English example, and the disambiguation of close for the English\u2192German example."}, {"heading": "5 Related Work", "text": "Linguistic features have been used in neural language modelling (Alexandrescu and Kirchhoff, 2006), and are also used in other tasks for which neural models have recently been employed, such as syntactic parsing (Chen and Manning, 2014). This paper addresses the question whether linguistic features on the source side are beneficial for neural machine translation. On the target side, linguistic features are harder to obtain for a generation task such as machine translation, since this would require incremental parsing of the hypotheses at test time, and this is possible future work.\nAmong others, our model incorporates information from a dependency annotation, but is still a sequence-to-sequence model. Eriguchi et al. (2016) propose a tree-to-sequence model whose encoder computes vector representations for each phrase in the source tree. Their focus is on exploiting the (unlabelled) structure of a syntactic annotation, whereas we are focused on the disambiguation power of the functional dependency labels.\nFactored translation models are often used in phrase-based SMT (Koehn and Hoang, 2007) as a means to incorporate extra linguistic information. However, neural MT can provide a much more flexible mechanism for adding such information. Because phrase-based models cannot easily generalize to new feature combinations, the individual models either treat each feature combination as an atomic unit, resulting in data sparsity, or assume independence between features, for instance by having separate language models for words and POS tags. In contrast, we exploit the strong generalization ability of neural networks, and expect that even new feature combinations, e.g. a word that appears in a novel syntactic function, are handled gracefully.\nOne could consider the lemmatized representation of the input as a second source text, and perform multi-source translation (Zoph and Knight, 2016). The main technical difference is that in our approach, the encoder and attention layers are shared between features, which we deem appropriate for the types of features that we tested."}, {"heading": "6 Conclusion", "text": "In this paper we investigate whether linguistic input features are beneficial to neural machine translation, and our empirical evidence suggests that this is the case.\nWe describe a generalization of the encoder in the popular attentional encoder-decoder architecture for neural machine translation that allows for the inclusion of an arbitrary number of input features. We empirically test the inclusion of various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels, and morphological features, into English\u2194German, and English\u2192Romanian neural MT systems. Our experiments show that the linguistic features yield improvements over our baseline, resulting in improvements on newstest2016 of 1.5 BLEU for German\u2192English, 0.6 BLEU for English\u2192German, and 1.0 BLEU for English\u2192Romanian.\nIn the future, we expect several developments that will shed more light on the usefulness of linguistic (or other) input features, and whether they will establish themselves as a core component of neural machine translation. On the one hand, the machine learning capability of neural architectures is likely to increase, decreasing the benefit provided by the features we tested. On the other hand, there is potential to explore the inclusion of novel features for neural MT, which might prove to be even more helpful than the ones we investigated, and the features we investigated may prove especially helpful for some translation settings, such as very low-resourced settings and/or translation settings with a highly inflected source language."}, {"heading": "Acknowledgments", "text": "This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), and 644402 (HimL)."}], "references": [{"title": "Factored Neural Language Models", "author": ["Alexandrescu", "Katrin Kirchhoff"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL,", "citeRegEx": "Alexandrescu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Alexandrescu et al\\.", "year": 2006}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Kazuma Hashimoto", "author": ["Akiko Eriguchi"], "venue": "and Yoshimasa Tsuruoka.", "citeRegEx": "Eriguchi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Holger Schwenk", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u00efc Barrault", "HueiChi Lin", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "G\u00fcl\u00e7ehre et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Roland Memisevic", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho"], "venue": "and Yoshua Bengio.", "citeRegEx": "Jean et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Factored Translation Models", "author": ["Koehn", "Hoang2007] Philipp Koehn", "Hieu Hoang"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learn-", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Wen-tau Yih", "author": ["Tomas Mikolov"], "venue": "and Geoffrey Zweig.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Carroll", "author": ["Guido Minnen", "John A"], "venue": "and Darren Pearce.", "citeRegEx": "Minnen et al.2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "The QT21/HimL Combined Machine Translation System", "author": ["Allauzen", "Lauriane Aufrant", "Franck Burlot", "Elena Knyazeva", "Thomas Lavergne", "Fran\u00e7ois Yvon", "Marcis Pinnis"], "venue": "In Proceedings of the First Conference on Machine Translation (WMT16),", "citeRegEx": "Allauzen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Allauzen et al\\.", "year": 2016}, {"title": "chrF: character ngram F-score for automatic MT evaluation", "author": ["Maja Popovi\u0107"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Popovi\u0107.,? \\Q2015\\E", "shortCiteRegEx": "Popovi\u0107.", "year": 2015}, {"title": "On Some Pitfalls in Automatic Evaluation and Significance Testing for MT", "author": ["Riezler", "Maxwell2005] Stefan Riezler", "John T. Maxwell"], "venue": "In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-", "citeRegEx": "Riezler et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2005}, {"title": "Arne Fitschen", "author": ["Helmut Schmid"], "venue": "and Ulrich Heid.", "citeRegEx": "Schmid et al.2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Zmorge: A German Morphological Lexicon Extracted from Wiktionary", "author": ["Sennrich", "Kunz2014] Rico Sennrich", "Beat Kunz"], "venue": "In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Sennrich et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2014}, {"title": "Martin Volk", "author": ["Rico Sennrich"], "venue": "and Gerold Schneider.", "citeRegEx": "Sennrich et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "2016a. Edinburgh Neural Machine Translation Systems for WMT 16", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the First Conference on Machine Translation", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Philipp Koehn", "author": ["Milo\u0161 Stanojevi\u0107", "Amir Kamran"], "venue": "and Ond\u0159ej Bojar.", "citeRegEx": "Stanojevi\u0107 et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Manning", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D"], "venue": "and Yoram Singer.", "citeRegEx": "Toutanova et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Barry Haddow", "author": ["Philip Williams", "Rico Sennrich", "Maria Nadejde", "Matthias Huck"], "venue": "and Ond\u0159ej Bojar.", "citeRegEx": "Williams et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "2016", "author": ["Barret Zoph", "Kevin Knight"], "venue": "Multi-Source Neural Translation. In NAACL HLT", "citeRegEx": "Zoph and Knight2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder\u2013decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-ofspeech tags, and syntactic dependency labels as input features to English\u2194German and English\u2192Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An opensource implementation of our neural MT system is available1 , as are sample files and configurations2 .", "creator": "LaTeX with hyperref package"}}}