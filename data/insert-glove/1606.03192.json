{"id": "1606.03192", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "PSDVec: a Toolbox for Incremental and Scalable Word Embedding", "abstract": "isuru PSDVec asia is a Python / Perl tertullian toolbox that stans learns word embeddings, 25-49 i. e. the mapping of 2251 words in plesiosaurus a chorzow natural u27a4 language thinners to odebrecht continuous 16-29 vectors which 17.58 encode the semantic / syntactic regularities grawer between vaginally the nanny words. PSDVec othello implements a assn. word snrnas embedding golestan learning method based cauchi on inuvialuit a weighted serigraphs low - eb-5 rank positive semidefinite approximation. podbrozny To scale up yashin the panufnik learning gunns process, we normales implement a 26-12 blockwise grane online learning jave algorithm to 45.13 learn the 94.61 embeddings incrementally. This cottonmouths strategy greatly reduces the mayorov learning b\u00e2rzava time haegglund of srdjan word afar embeddings on a tucumcari large 60-a vocabulary, and xiaosi can learn the embeddings kilbarchan of new sharavathi words without alenitchev re - schnegg learning jocularity the nairn whole vocabulary. benchings On 9 double-double word danum similarity / huilin analogy benchmark theodolite sets and 2 fan-club Natural Language unaware Processing (NLP) untying tasks, bubnov PSDVec rockett produces correctly embeddings wotzel that has 27.47 the reinder best mylon average roquefeuil performance among charadriiformes popular ostrzesz\u00f3w word embedding hamedan tools. PSDVec provides couric a milli new mellat option 18-pound for murmu NLP agos practitioners.", "histories": [["v1", "Fri, 10 Jun 2016 05:55:58 GMT  (28kb)", "http://arxiv.org/abs/1606.03192v1", "12 pages, accepted by Neurocomputing, Software Track on Original Software Publications"]], "COMMENTS": "12 pages, accepted by Neurocomputing, Software Track on Original Software Publications", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shaohua li", "jun zhu", "chunyan miao"], "accepted": false, "id": "1606.03192"}, "pdf": {"name": "1606.03192.pdf", "metadata": {"source": "CRF", "title": "PSDVec: a Toolbox for Incremental and Scalable Word Embedding", "authors": ["Shaohua Li", "Jun Zhu", "Chunyan Miao"], "emails": ["shaohua@gmail.com", "dcszj@tsinghua.edu.cn", "ascymiao@ntu.edu.sg"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n03 19\n2v 1\n[ cs\n.C L\n] 1\nPSDVec is a Python/Perl toolbox that learns word embeddings, i.e. the mapping of words in a natural language to continuous vectors which encode the semantic/syntactic regularities between the words. PSDVec implements a word embedding learning method based on a weighted low-rank positive semidefinite approximation. To scale up the learning process, we implement a blockwise online learning algorithm to learn the embeddings incrementally. This strategy greatly reduces the learning time of word embeddings on a large vocabulary, and can learn the embeddings of new words without re-learning the whole vocabulary. On 9 word similarity/analogy benchmark sets and 2 Natural Language Processing (NLP) tasks, PSDVec produces embeddings that has the best average performance among popular word embedding tools. PSDVec provides a new option for NLP practitioners.\nKeywords: word embedding, matrix factorization, incremental learning"}, {"heading": "1. Introduction", "text": "Word embedding has gained popularity as an important unsupervised Natural Language Processing (NLP) technique in recent years. The task of word embedding is to derive a set of vectors in a Euclidean space corresponding to words which best fit certain statistics derived from a corpus. These vec-\n\u2217Corresponding author. Email addresses: shaohua@gmail.com (Shaohua Li), dcszj@tsinghua.edu.cn (Jun\nZhu), ascymiao@ntu.edu.sg (Chunyan Miao)\nPreprint submitted to Neurocomputing June 13, 2016\ntors, commonly referred to as the embeddings, capture the semantic/syntactic regularities between the words. Word embeddings can supersede the traditional one-hot encoding of words as the input of an NLP learning system, and can often significantly improve the performance of the system.\nThere are two lines of word embedding methods. The first line is neural word embedding models, which use softmax regression to fit bigram probabilities and are optimized with Stochastic Gradient Descent (SGD). One of the best known tools is word2vec1 [10]. The second line is low-rank matrix factorization (MF)-based methods, which aim to reconstruct certain bigram statistics matrix extracted from a corpus, by the product of two low rank factor matrices. Representative methods/toolboxes include Hyperwords2 [4, 5], GloVe3 [11], Singular4 [14], and Sparse5 [2]. All these methods use two different sets of embeddings for words and their context words, respectively. SVD based optimization procedures are used to yield two singular matrices. Only the left singular matrix is used as the embeddings of words. However, SVD operates on G\u22a4G, which incurs information loss in G, and may not correctly capture the signed correlations between words. An empirical comparison of popular methods is presented in [5].\nThe toolbox presented in this paper is an implementation of our previous work [9]. It is a new MF-based method, but is based on eigendecomposition instead. This toolbox is based on [9], where we estabilish a Bayesian generative model of word embedding, derive a weighted low-rank positive semidefinite approximation problem to the Pointwise Mutual Information (PMI) matrix, and finally solve it using eigendecomposition. Eigendecomposition avoids the information loss in based methods, and the yielded embeddings are of higher quality than SVD-based methods. However eigendecomposition is known to be difficult to scale up. To make our method scalable to large vocabularies, we exploit the sparsity pattern of the weight matrix and implement a divide-and-conquer approximate solver to find the embeddings incrementally.\nOur toolbox is named Positive-Semidefinite Vectors (PSDVec). It offers the following advantages over other word embedding tools:\n1https://code.google.com/p/word2vec/ 2https://bitbucket.org/omerlevy/hyperwords 3http://nlp.stanford.edu/projects/glove/ 4https://github.com/karlstratos/singular 5https://github.com/mfaruqui/sparse-coding\n1. The incremental solver in PSDVec has a time complexity O(cd2n) and space complexity O(cd), where n is the total number of words in a vocabulary, d is the specified dimensionality of embeddings, and c \u226a n is the number of specified core words. Note the space complexity does not increase with the vocabulary size n. In contrast, other MF-based solvers, including the core embedding generation of PSDVec, are of O(n3) time complexity and O(n2) space complexity. Hence asymptotically, PSDVec takes about cd2/n2 of the time and cd/n2 of the space of other MF-based solvers6;\n2. Given the embeddings of an original vocabulary, PSDVec is able to learn the embeddings of new words incrementally. To our best knowledge, none of other word embedding tools provide this functionality; instead, new words have to be learned together with old words in batch mode. A common situation is that we have a huge general corpus such as English Wikipedia, and also have a small domain-specific corpus, such as the NIPS dataset. In the general corpus, specific terms may appear rarely. It would be desirable to train the embeddings of a general vocabulary on the general corpus, and then incrementally learn words that are unique in the domain-specific corpus. Then this feature of incremental learning could come into play;\n3. On word similarity/analogy benchmark sets and common Natural Language Processing (NLP) tasks, PSDVec produces embeddings that has the best average performance among popular word embedding tools;\n4. PSDVec is established as a Bayesian generative model [9]. The probabilistic modeling endows PSDVec clear probabilistic interpretation, and the modular structure of the generative model is easy to customize and extend in a principled manner. For example, global factors like topics can be naturally incorporated, resulting in a hybrid model [8] of word embedding and Latent Dirichlet Allocation [1]. For such extensions, PSDVec would serve as a good prototype. While in other methods, the regression objectives are usually heuristic, and other factors are difficult to be incorporated.\n6Word2vec adopts an efficient SGD sampling algorithm, whose time complexity is only O(kL), and space complexity O(n), where L is the number of word occurrences in the input corpus, and k is the number of negative sampling words, typically in the range 5 \u223c 20."}, {"heading": "2. Problem and Solution", "text": "PSDVec implements a low-rank MF-based word embedding method. This method aims to fit the PMI(si, sj) = log P (si,sj)\nP (si)P (sj) using v\u22a4sjvsi, where P (si)\nand P (si, sj) are the empirical unigram and bigram probabilities, respectively, and vsi is the embedding of si. The regression residuals PMI(si, sj)\u2212 v \u22a4\nsj vsi are penalized by a monotonic transformation f(\u00b7) of P (si, sj), which implies that, for more frequent (therefore more important) bigram si, sj, we expect it is better fitted. The optimization objective in the matrix form is\nV \u2217 = argmin\nV\n||G\u2212 V \u22a4V ||f(H) +\nW\u2211\ni=1\n\u00b5i\u2016vsi\u2016 2 2, (1)\nwhere G is the PMI matrix, V is the embedding matrix, H is the bigram probabilities matrix, || \u00b7 ||f(H) is the f(H)-weighted Frobenius-norm, and \u00b5i are the Tikhonov regularization coefficients. The purpose of the Tikhonov regularization is to penalize overlong embeddings. The overlength of embeddings is a sign of overfitting the corpus. Our experiments showed that, with such regularization, the yielded embeddings perform better on all tasks.\n(1) is to find a weighted low-rank positive semidefinite approximation to G. Prior to computing G, the bigram probabilities {P (si, sj)} are smoothed using Jelinek-Mercer Smoothing.\nA Block Coordinate Descent (BCD) algorithm [13] is used to approach (1), which requires eigendecomposition of G. The eigendecomposition requires O(n3) time and O(n2) space, which is difficult to scale up. As a remedy, we implement an approximate solution that learns embeddings incrementally. The incremental learning proceeds as follows:\n1. Partition the vocabulary S intoK consecutive groups S1, \u00b7 \u00b7 \u00b7 ,Sk. Take K = 3 as an example. S1 consists of the most frequent words, referred to as the core words, and the remaining words are noncore words;\n2. Accordingly partition G into K \u00d7K blocks as\n\n\nG11 G12 G13 G21 G22 G23 G31 G32 G33\n\n .\nPartition f(H) in the same way. G11, f(H)11 correspond to core-core\nbigrams (consisting of two core words). Partition V into\n(\n\ufe38\ufe37\ufe37\ufe38\nS1\nV 1 \ufe38\ufe37\ufe37\ufe38\nS2\nV 2 \ufe38\ufe37\ufe37\ufe38\nS3\nV 3\n)\n;\n3. For core words, set \u00b5i = 0, and solve argminV ||G11 \u2212 V \u22a4 1V 1||f(H1) using eigendecomposition, obtaining core embeddings V \u22171;\n4. Set V 1 = V \u2217 1, and find V \u2217 2 that minimizes the total penalty of the 12-th\nand 21-th blocks (the 22-th block is ignored due to its high sparsity):\nargmin V 2 \u2016G12 \u2212 V \u22a4 1V 2\u2016 2 f(H)12 + \u2016G21 \u2212 V \u22a4 2V 1\u2016 2 f(H)21\n+ \u2211\nsi\u2208S2\n\u00b5i\u2016vsi\u2016 2.\nThe columns in V 2 are independent, thus for each vsi, it is a separate weighted ridge regression problem, which has a closed-form solution [9];\n5. For any other set of noncore words Sk, find V \u2217 k that minimizes the\ntotal penalty of the 1k-th and k1-th blocks, ignoring all other kj-th and jk-th blocks;\n6. Combine all subsets of embeddings to form V \u2217. Here V \u2217 = (V \u22171,V \u2217 2,V \u2217 3)."}, {"heading": "3. Software Architecture and Functionalities", "text": "Our toolbox consists of 4 Python/Perl scripts: extractwiki.py, gramcount.pl, factorize.py and evaluate.py. Figure 1 presents the overall architecture.\n1. extractwiki.py first receives a Wikipedia snapshot as input; it then removes non-textual elements, non-English words and punctuation; after converting all letters to lowercase, it finally produces a clean stream of English words;\n2. gramcount.pl counts the frequencies of either unigrams or bigrams in a word stream, and saves them into a file. In the unigram mode (-m1), unigrams that appear less than certain frequency threshold are discarded. In the bigram mode (-m2), each pair of words in a text window (whose size is specified by -n) forms a bigram. Bigrams starting with the same leading word are grouped together in a row, corresponding to a row in matrices H and G; 3. factorize.py is the core module that learns embeddings from a bigram frequency file generated by gramcount.pl. A user chooses to split the vocabulary into a set of core words and a few sets of noncore words. factorize.py can: 1) in function we factorize EM(), do BCD on the PMI submatrix of core-core bigrams, yielding core embeddings ; 2) given the core embeddings obtained in 1), in block factorize(), do a weighted ridge regression w.r.t. noncore embeddings to fit the PMI submatrices of core-noncore bigrams. The Tikhonov regularization coefficient \u00b5i for a whole noncore block can be specified by -t. A good rule-of-thumb for setting \u00b5i is to increase \u00b5i as the word frequencies decrease, i.e., give more penalty to rarer words, since the corpus contains insufficient information of them; 4. evaluate.py evaluates a given set of embeddings on 7 commonly used testsets, including 5 similarity tasks and 2 analogy tasks."}, {"heading": "4. Implementation and Empirical Results", "text": ""}, {"heading": "4.1. Implementation Details", "text": "The Python scripts use Numpy for the matrix computation. Numpy automatically parallelizes the computation to fully utilize a multi-core machine.\nThe Perl script gramcount.pl implements an embedded C++ engine to speed up the processing with a smaller memory footprint."}, {"heading": "4.2. Empirical results", "text": "Our competitors include: word2vec, PPMI and SVD in Hyperwords, GloVe, Singular and Sparse. In addition, to show the effect of Tikhonov regularization on \u201cPSDVec\u201d, evaluations were done on an unregularized PSDVec (by passing \u201c-t 0\u201d to factorize.py), denoted as PSD-unreg. All methods were trained on an 12-core Xeon 3.6GHz PC with 48 GB of RAM.\nWe evaluated all methods on two types of testsets. The first type of testsets are shipped with our toolkit, consisting of 7 word similarity tasks\nand 2 word analogy tasks (Luong\u2019s Rare Words is excluded due to many rare words contained). 7 out of the 9 testsets are used in [5]. The hyperparameter settings of other methods and evaluation criteria are detailed in [5, 14, 2]. The other 2 tasks are TOEFL Synonym Questions (TFL) [3] and Rubenstein & Goodenough (RG) dataset [12]. For these tasks, all 7 methods were trained on the Apri 2015 English Wikipedia. All embeddings except \u201cSparse\u201d were 500 dimensional. \u201cSparse\u201d needs more dimensionality to cater for vector sparsity, so its dimensionality was set to 2500. It used the embeddings of word2vec as the input. In analogy tasks Google and MSR, embeddings were evaluated using 3CosMul [6]. The embedding set of PSDVec for these tasks contained 180,000 words, which was trained using the blockwise online learning procedure described in Section 5, based on 25,000 core words.\nThe second type of testsets are 2 practical NLP tasks for evaluating word embedding methods as used in [15], i.e., Named Entity Recognition (NER) and Noun Phrase Chunking (Chunk). Following settings in [15], the embeddings for NLP tasks were trained on Reuters Corpus, Volume 1 [7], and the embedding dimensionality was set to 50 (\u201cSparse\u201d had a dimensionality of 500). The embedding set of PSDVec for these tasks contained 46,409 words, based on 15,000 core words.\nTable 1 above reports the performance of 7 methods on 11 tasks. The\nlast column reports the average score. \u201cPSDVec\u201d performed stably across the tasks, and achieved the best average score. On the two analogy tasks, \u201cword2vec\u201d performed far better than all other methods (except \u201cSparse\u201d, as it was derived from \u201cword2vec\u201d), the reason for which is still unclear. On NLP tasks, most methods achieved close performance. \u201cPSDVec\u201d outperformed \u201cPSD-unreg\u201d on all tasks.\nTo compare the efficiency of each method, we presented the training time of different methods across 2 training corpora in Table 2. Please note that the ratio of running time is determined by a few factors together: the ratio of vocabulary sizes (180000/46409 \u2248 4), the ratio of vector lengths (500/50 = 10), the language efficiency, and the algorithm efficiency. We were most interested in the algorithm efficiency. To reduce the effect of different language efficiency of different methods, we took the ratio of the two training time to measure the scalability of each algorithm.\nFrom Table 2, we can see that \u201cPSDVec\u201d exhibited a competitive absolute speed, considering the inefficiency of Python relative to C/C++. The scalability of \u201cPSDVec\u201d ranked the second best, worse than \u201cSingular\u201d and better than \u201cword2vec\u201d.\nThe reason that \u201cPPMI\u201d and \u201cSVD\u201d (based on \u201cPPMI\u201d) were so slow is that \u201chyperwords\u201d employs an external sorting command, which is extremely slow on large files. The reason for the poor scalability of \u201cSparse\u201d is unknown.\nTable 3 shows the time and space efficiency of the incremental learning (\u201cPSD-noncore\u201d for noncore words) and MF-based learning (\u201cPSD-core\u201d\nfor core words) on two corpora. The memory is halved using incremental learning, and is constant as the vocabulary size increases. Remind that the asymptotic per-word time complexity of \u201cPSD-noncore\u201d is cd2/\u00b5n2 of that of \u201cPSD-core\u201d, in which typically \u00b5 > 20. As embedding dimensionality d on Wikipedia is 10 times of that on RCV1, the speedup rate on the Wikipedia corpus is only around 1/12 of that on the RCV1 corpus7."}, {"heading": "5. Illustrative Example: Training on English Wikipedia", "text": "In this example, we train embeddings on the English Wikipedia snapshot in April 2015. The training procedure goes as follows:\n1. Use extractwiki.py to cleanse aWikipedia snapshot, and generate cleanwiki.txt, which is a stream of 2.1 billion words; 2. Use gramcount.plwith cleanwiki.txt as input, to generate top1grams-wiki.txt; 3. Use gramcount.pl with top1grams-wiki.txt and cleanwiki.txt as input, to generate top2grams-wiki.txt; 4. Use factorize.py with top2grams-wiki.txt as input, to obtain 25000 core embeddings, saved into 25000-500-EM.vec; 5. Use factorize.py with top2grams-wiki.txt and 25000-500-EM.vec as input, and Tikhonov regularization coefficient set to 2, to obtain 55000 noncore embeddings. The word vectors of totally 80000 words is saved into 25000-80000-500-BLKEM.vec; 6. Repeat Step 5 twice with Tikhonov regularization coefficient set to 4 and 8, respectively, to obtain extra 50000 \u00d7 2 noncore embeddings. The word vectors are saved into 25000-130000-500-BLKEM.vec and 25000-180000-500-BLKEM.vec, respectively; 7. Use evaluate.py to test 25000-180000-500-BLKEM.vec.\n7According to the expression cd2/\u00b5n2, the speedup rate on Wikipedia should be 1/60 of that on RCV1. But some common overhead of Numpy matrix operations is more prominent on the smaller matrices when d is small, which reduces the speedup rate on smaller d. Hence the ratio of the two speedup rates is 1/12 in practice."}, {"heading": "6. Conclusions", "text": "We have developed a Python/Perl toolkit PSDVec for learning word embeddings from a corpus. This open-source cross-platform software is easy to use, easy to extend, scales up to large vocabularies, and can learn new words incrementally without re-training the whole vocabulary. The produced embeddings performed stably on various test tasks, and achieved the best average score among 7 state-of-the-art methods."}, {"heading": "Acknowledgements", "text": "This research is supported by the National Research Foundation Singapore under its Interactive Digital Media (IDM) Strategic Research Programme."}], "references": [{"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Sparse overcomplete word vector representations", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T Dumais"], "venue": "Psychological review,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Neural word embeddings as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of NIPS 2014,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan"], "venue": "In Proceedings of CoNLL-2014,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Topic embedding: a continuous representation of documents", "author": ["Shaohua Li", "Tat-Seng Chua", "Jun Zhu", "Chunyan Miao"], "venue": "In Proceedings of the The 54th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "A generative word embedding model and its low rank positive semidefinite solution", "author": ["Shaohua Li", "Jun Zhu", "Chunyan Miao"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empirical Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1965}, {"title": "Weighted low-rank approximations", "author": ["Nathan Srebro", "Tommi Jaakkola"], "venue": "In Proceedings of ICML 2003,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Karl Stratos", "Michael Collins", "Daniel Hsu"], "venue": "In Proceedings of ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}], "referenceMentions": [{"referenceID": 9, "context": "One of the best known tools is word2vec [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "Representative methods/toolboxes include Hyperwords [4, 5], GloVe [11], Singular [14], and Sparse [2].", "startOffset": 52, "endOffset": 58}, {"referenceID": 4, "context": "Representative methods/toolboxes include Hyperwords [4, 5], GloVe [11], Singular [14], and Sparse [2].", "startOffset": 52, "endOffset": 58}, {"referenceID": 10, "context": "Representative methods/toolboxes include Hyperwords [4, 5], GloVe [11], Singular [14], and Sparse [2].", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Representative methods/toolboxes include Hyperwords [4, 5], GloVe [11], Singular [14], and Sparse [2].", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "Representative methods/toolboxes include Hyperwords [4, 5], GloVe [11], Singular [14], and Sparse [2].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "An empirical comparison of popular methods is presented in [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "The toolbox presented in this paper is an implementation of our previous work [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "This toolbox is based on [9], where we estabilish a Bayesian generative model of word embedding, derive a weighted low-rank positive semidefinite approximation problem to the Pointwise Mutual Information (PMI) matrix, and finally solve it using eigendecomposition.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "PSDVec is established as a Bayesian generative model [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "For example, global factors like topics can be naturally incorporated, resulting in a hybrid model [8] of word embedding and Latent Dirichlet Allocation [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "For example, global factors like topics can be naturally incorporated, resulting in a hybrid model [8] of word embedding and Latent Dirichlet Allocation [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 12, "context": "A Block Coordinate Descent (BCD) algorithm [13] is used to approach (1), which requires eigendecomposition of G.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "The columns in V 2 are independent, thus for each vsi, it is a separate weighted ridge regression problem, which has a closed-form solution [9]; 5.", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "7 out of the 9 testsets are used in [5].", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "The hyperparameter settings of other methods and evaluation criteria are detailed in [5, 14, 2].", "startOffset": 85, "endOffset": 95}, {"referenceID": 13, "context": "The hyperparameter settings of other methods and evaluation criteria are detailed in [5, 14, 2].", "startOffset": 85, "endOffset": 95}, {"referenceID": 1, "context": "The hyperparameter settings of other methods and evaluation criteria are detailed in [5, 14, 2].", "startOffset": 85, "endOffset": 95}, {"referenceID": 2, "context": "The other 2 tasks are TOEFL Synonym Questions (TFL) [3] and Rubenstein & Goodenough (RG) dataset [12].", "startOffset": 52, "endOffset": 55}, {"referenceID": 11, "context": "The other 2 tasks are TOEFL Synonym Questions (TFL) [3] and Rubenstein & Goodenough (RG) dataset [12].", "startOffset": 97, "endOffset": 101}, {"referenceID": 5, "context": "In analogy tasks Google and MSR, embeddings were evaluated using 3CosMul [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 14, "context": "The second type of testsets are 2 practical NLP tasks for evaluating word embedding methods as used in [15], i.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "Following settings in [15], the embeddings for NLP tasks were trained on Reuters Corpus, Volume 1 [7], and the embedding dimensionality was set to 50 (\u201cSparse\u201d had a dimensionality of 500).", "startOffset": 22, "endOffset": 26}, {"referenceID": 6, "context": "Following settings in [15], the embeddings for NLP tasks were trained on Reuters Corpus, Volume 1 [7], and the embedding dimensionality was set to 50 (\u201cSparse\u201d had a dimensionality of 500).", "startOffset": 98, "endOffset": 101}], "year": 2016, "abstractText": "PSDVec is a Python/Perl toolbox that learns word embeddings, i.e. the mapping of words in a natural language to continuous vectors which encode the semantic/syntactic regularities between the words. PSDVec implements a word embedding learning method based on a weighted low-rank positive semidefinite approximation. To scale up the learning process, we implement a blockwise online learning algorithm to learn the embeddings incrementally. This strategy greatly reduces the learning time of word embeddings on a large vocabulary, and can learn the embeddings of new words without re-learning the whole vocabulary. On 9 word similarity/analogy benchmark sets and 2 Natural Language Processing (NLP) tasks, PSDVec produces embeddings that has the best average performance among popular word embedding tools. PSDVec provides a new option for NLP practitioners.", "creator": "LaTeX with hyperref package"}}}