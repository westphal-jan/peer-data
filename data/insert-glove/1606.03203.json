{"id": "1606.03203", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Causal Bandits: Learning Good Interventions via Causal Inference", "abstract": "typographical We hadra study manisaspor the problem off-duty of using causal bfu models forty-seven to improve the rate at 4.48 which galva good tipico interventions can takraw be learned 40-ton online in 100.23 a albiston stochastic environment. Our 182.2 formalism ruxu combines htpc multi - arm mustika bandits and yus causal inference ammoniac to model a novel type scapularis of bandit gurian feedback borei that kongsvinger is hjelmeset not breaux exploited trickles by juli existing approaches. sooka We honko propose annelise a new algorithm that exploits protuberances the hi-md causal bogdonoff feedback kariba and prove a bound yfz on its swinson simple tioga regret that is araucanian strictly sinnathuray better (75-a in otniel all quantities) tetrahedron than algorithms that do not use the additional iker causal .262 information.", "histories": [["v1", "Fri, 10 Jun 2016 06:19:32 GMT  (1110kb,D)", "http://arxiv.org/abs/1606.03203v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["finnian lattimore", "tor lattimore", "mark d reid"], "accepted": true, "id": "1606.03203"}, "pdf": {"name": "1606.03203.pdf", "metadata": {"source": "CRF", "title": "Causal Bandits: Learning Good Interventions via Causal Inference", "authors": ["Finnian Lattimore", "Mark D. Reid"], "emails": ["finn.lattimore@gmail.com", "tor.lattimore@gmail.com", "mark.reid@anu.edu.au"], "sections": [{"heading": null, "text": "We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information."}, {"heading": "1 Introduction", "text": "Medical drug testing, policy setting, and other scientific processes are commonly framed and analysed in the language of sequential experimental design and, in special cases, as bandit problems (Robbins, 1952; Chernoff, 1959). In this framework, single actions (also referred to as interventions) from a pre-determined set are repeatedly performed in order to evaluate their effectiveness via feedback from a single, real-valued reward signal. We propose a generalisation of the standard model by assuming that, in addition to the reward signal, the learner observes the values of a number of covariates drawn from a probabilistic causal model (Pearl, 2000). Causal models are commonly used in disciplines where explicit experimentation may be difficult such as social science, demography and economics. For example, when predicting the effect of changes to childcare subsidies on workforce participation, or school choice on grades. Results from causal inference relate observational distributions to interventional ones, allowing the outcome of an intervention to be predicted without explicitly performing it. By exploiting the causal information we show, theoretically and empirically, how non-interventional observations can be used to improve the rate at which high-reward actions can be identified.\nThe type of problem we are concerned with is best illustrated with an example. Consider a farmer wishing to optimise the yield of her crop. She knows that crop yield is only affected by temperature, a particular soil nutrient, and moisture level but the precise effect of their combination is unknown. In each season the farmer has enough time and money to intervene and control at most one of these variables: deploying shade or heat lamps will set the temperature to be low or high; the nutrient can be added or removed through a choice of fertilizer; and irrigation or rain-proof covers will keep the soil wet or dry. When not intervened upon, the temperature, soil, and moisture vary naturally from season to season due to weather conditions and these are all observed along with the final crop yield at the end of each season. How might the farmer best experiment to identify the single, highest yielding intervention in a limited number of seasons?\nar X\niv :1\n60 6.\n03 20\n3v 1\n[ st\nat .M\nL ]\n1 0\nContributions We take the first step towards formalising and solving problems such as the one above. In \u00a72 we formally introduce causal bandit problems in which interventions are treated as arms in a bandit problem but their influence on the reward \u2014 along with any other observations \u2014 is assumed to conform to a known causal graph. We show that our causal bandit framework subsumes the classical bandits (no additional observations) and contextual stochastic bandit problems (observations are revealed before an intervention is chosen) before focusing on the case where, like the above example, observations occur after each intervention is made.\nOur focus is on the simple regret, which measures the difference between the return of the optimal action and that of the action chosen by the algorithm after T rounds. In \u00a73 we analyse a specific family of causal bandit problems that we call parallel bandit problems in which N factors affect the reward independently and there are 2N possible interventions. We propose a simple causal best arm identification algorithm for this problem and show that up to logarithmic factors it enjoys minimax optimal simple regret guarantees of \u0398\u0303( \u221a m/T ) where m depends on the causal model and may\nbe much smaller than N . In contrast, existing best arm identification algorithms suffer \u2126( \u221a N/T ) simple regret (Thm. 4 by Audibert and Bubeck (2010)). This shows theoretically the value of our framework over the traditional bandit problem. Experiments in \u00a75 further demonstrate the value of causal models in this framework.\nIn the general casual bandit problem interventions and observations may have a complex relationship. In \u00a74 we propose a new algorithm inspired by importance-sampling that a) enjoys sub-linear regret equivalent to the optimal rate in the parallel bandit setting and b) captures many of the intricacies of sharing information in a causal graph in the general case. As in the parallel bandit case, the regret guarantee scales like O( \u221a m/T ) where m depends on the underlying causal structure, with smaller values corresponding to structures that are easier to learn. The value of m is always less than the number of interventionsN and in the special case of the parallel bandit (where we have lower bounds) the notions are equivalent.\nRelated Work As alluded to above, causal bandit problems can be treated as classical multi-armed bandit problems by simply ignoring the causal model and extra observations and applying an existing best-arm identification algorithm with well understood simple regret guarantees (Jamieson et al., 2014). However, as we show in \u00a73, ignoring the extra information available in the non-intervened variables yields sub-optimal performance.\nA well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context.\nThere have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph. As we discuss in \u00a76, the parallel bandit problem can be captured in this framework, however the regret bounds are not optimal in our setting. They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009). The partial monitoring approach taken by Wu et al. (2015) could be applied (up to modifications for the simple regret) to the parallel bandit, but the resulting strategy would need to know the likelihood of each factor in advance, while our strategy learns this online. Yu and Mannor (2009) utilize extra observations to detect changes in the reward distribution, whereas we assume fixed reward distributions and use extra observations to improve arm selection. Avner et al. (2012) analyse bandit problems where the choice of arm to pull and arm to receive feedback on are decoupled. The main difference from our present work is our focus on simple regret and the more complex information linking rewards for different arms via causal graphs. To the best of our knowledge, our paper is the first to analyse simple regret in bandit problems with extra post-action feedback.\nTwo pieces of recent work also consider applying ideas from causal inference to bandit problems. Bareinboim et al. (2015) demonstrate that in the presence of confounding variables the value that a variable would have taken had it not been intervened on can provide important contextual information. Their work differs in many ways. For example, the focus is on the cumulative regret and the context is observed before the action is taken and cannot be controlled by the learning agent.\nOrtega and Braun (2014) present an analysis and extension of Thompson sampling assuming actions are causal interventions. Their focus is on causal induction (i.e., learning an unknown causal model) instead of exploiting a known causal model. Combining their handling of causal induction with our analysis is left as future work.\nThe truncated importance weighted estimators used in \u00a74 have been studied before in a causal framework by Bottou et al. (2013), where the focus is on learning from observational data, but not controlling the sampling process. They also briefly discuss some of the issues encountered in sequential design, but do not give an algorithm or theoretical results for this case."}, {"heading": "2 Problem Setup", "text": "We now introduce a novel class of stochastic sequential decision problems which we call causal bandit problems. In these problems, rewards are given for repeated interventions on a fixed causal model Pearl (2000). Following the terminology and notation in Koller and Friedman (2009), a causal model is given by a directed acyclic graph G over a set of random variables X = {X1, . . . , XN} and a joint distribution P over X that factorises over G. We will assume each variable only takes on a finite number of distinct values. An edge from variable Xi to Xj is interpreted to mean that a change in the value of Xi may directly cause a change to the value of Xj . The parents of a variable Xi, denoted PaXi , is the set of all variables Xj such that there is an edge from Xj to Xi in G. An intervention or action (of size n), denoted do(X = x), assigns the values x = {x1, . . . , xn} to the corresponding variables X = {X1, . . . , Xn} \u2282 X with the empty intervention (where no variable is set) denoted do(). The intervention also \u201cmutilates\u201d the graph G by removing all edges from Pai to Xi for each Xi \u2208X . The resulting graph defines a probability distribution P {Xc|do(X = x)} over Xc := X \u2212X . Details can be found in Chapter 21 of Koller and Friedman (2009). A learner for a casual bandit problem is given the casual model\u2019s graph G and a set of allowed actions A. One variable Y \u2208 X is designated as the reward variable and takes on values in {0, 1}. We denote the expected reward for the action a = do(X = x) by \u00b5a := E [Y |do(X = x)] and the optimal expected reward by \u00b5\u2217 := maxa\u2208A \u00b5a. The causal bandit game proceeds over T rounds. In round t, the learner intervenes by choosing at = do(Xt = xt) \u2208 A based on previous observations. It then observes sampled values for all non-intervened variables Xct drawn from P {X c t |do(Xt = xt)}, including the reward Yt \u2208 {0, 1}. After T observations the learner outputs an estimate of the optimal action a\u0302\u2217T \u2208 A based on its prior observations.\nThe objective of the learner is to minimise the simple regret RT = \u00b5\u2217 \u2212 E [ \u00b5a\u0302\u2217T ] . This is sometimes refered to as a \u201cpure exploration\u201d (Bubeck et al., 2009) or \u201cbest-arm identification\u201d problem (Gabillon et al., 2012) and is most appropriate when, as in drug and policy testing, the learner has a fixed experimental budget after which its policy will be fixed indefinitely.\nAlthough we will focus on the intervene-then-observe ordering of events within each round, other scenarios are possible. If the non-intervened variables are observed before an intervention is selected our framework reduces to stochastic contextual bandits, which are already reasonably well understood (Agarwal et al., 2014). Even if no observations are made during the rounds, the causal model may still allow offline pruning of the set of allowable interventions thereby reducing the complexity.\nWe note that classical K-armed stochastic bandit problem can be recovered in our framework by considering a simple causal model with one edge connecting a single variable X that can take on K values to a reward variable Y \u2208 {0, 1} where P {Y = 1|X} = r(X) for some arbitrary but unknown, real-valued function r. The set of allowed actions in this case isA = {do(X = k) : k \u2208 {1, . . . ,K}}. Conversely, any causal bandit problem can be reduced to a classical stochastic |A|-armed bandit problem by treating each possible intervention as an independent arm and ignoring all sampled values for the observed variables except for the reward. Intuitively though, one would expect to perform better by making use of the extra structure and observations."}, {"heading": "3 Regret Bounds for Parallel Bandit", "text": "In this section we propose and analyse an algorithm for achieving the optimal regret in a natural special case of the causal bandit problem which we call the parallel bandit. It is simple enough to admit a thorough analysis but rich enough to model the type of problem discussed in \u00a71, including\nthe farming example. It also suffices to witness the regret gap between algorithms that make use of causal models and those which do not.\nThe causal model for this class of problems has N binary variables {X1, . . . , XN} where each Xi \u2208 {0, 1} are independent causes of a reward variable Y \u2208 {0, 1}, as shown in Figure 1a. All variables are observable and the set of allowable actions are all size 0 and size 1 interventions: A = {do()} \u222a {do(Xi = j) : 1 \u2264 i \u2264 N and j \u2208 {0, 1}} In the farming example from the introduction, X1 might represent temperature (e.g., X1 = 0 for low and X1 = 1 for high). The interventions do(X1 = 0) and do(X1 = 1) indicate the use of shades or heat lamps to keep the temperature low or high, respectively.\nIn each round the learner either purely observes by selecting do() or sets the value of a single variable. The remaining variables are simultaneously set by independently biased coin flips. The value of all variables are then used to determine the distribution of rewards for that round. Formally, when not intervened upon we assume that each Xi \u223c Bernoulli(qi) where q = (q1, . . . , qN ) \u2208 [0, 1]N so that qi = P {Xi = 1}. The value of the reward variable is distributed as P {Y = 1|X} = r(X) where r : {0, 1}N \u2192 [0, 1] is an arbitrary, fixed, and unknown function. In the farming example, this choice of Y models the success or failure of a seasons crop, which depends stochastically on the various environment variables.\nThe Parallel Bandit Algorithm The algorithm operates as follows. For the first T/2 rounds it chooses do() to collect observational data. As the only link from each X1, . . . , XN to Y is a direct, causal one, P {Y |do(Xi = j)} = P {Y |Xi = j}. Thus we can create good estimators for the returns of the actions do(Xi = j) for which P {Xi = j} is large. The actions for which P {Xi = j} is small may not be observed (often) so estimates of their returns could be poor. To address this, the remaining T/2 rounds are evenly split to estimate the rewards for these infrequently observed actions. The difficulty of the problem depends on q and, in particular, how many of the variables are unbalanced (i.e., small qi or (1\u2212 qi)). For \u03c4 \u2208 [2...N ] let I\u03c4 = { i : min {qi, 1\u2212 qi} < 1\u03c4 } . Define\nm(q) = min {\u03c4 : |I\u03c4 | \u2264 \u03c4} .\nAlgorithm 1 Parallel Bandit Algorithm 1: Input: Total rounds T and N . 2: for t \u2208 1, . . . , T/2 do 3: Perform empty intervention do() 4: Observe Xt and Yt 5: for a = do(Xi = x) \u2208 A do 6: Count times Xi = x seen: Ta = \u2211T/2 t=1 1{Xt,i = x}\n7: Estimate reward: \u00b5\u0302a = 1Ta \u2211T/2 t=1 1{Xt,i = x}Yt\n8: Estimate probabilities: p\u0302a = 2TaT , q\u0302i = p\u0302do(Xi=1) 9: Compute m\u0302 = m(q\u0302) and A = { a \u2208 A : p\u0302a \u2264 1m\u0302 } .\n10: Let TA := T2|A| be times to sample each a \u2208 A. 11: for a = do(Xi = x) \u2208 A do 12: for t \u2208 1, . . . , TA do 13: Intervene with a and observe Yt 14: Re-estimate \u00b5\u0302a = 1TA \u2211TA t=1 Yt 15: return estimated optimal a\u0302\u2217T \u2208 arg maxa\u2208A \u00b5\u0302a\nI\u03c4 is the set of variables considered unbalanced and we tune \u03c4 to trade off identifying the low probability actions against not having too many of them, so as to minimize the worst-case simple regret. When q = ( 12 , . . . , 1 2 ) we have m(q) = 2 and when q = (0, . . . , 0) we havem(q) = N . We do not assume that q is known, thus Algorithm 1 also utilizes the samples captured during the observational phase to estimate m(q). Although very simple, the following two theorems show that this algorithm is effectively optimal.\nTheorem 1. Algorithm 1 satisfies\nRT \u2208 O\n(\u221a m(q)\nT log\n( NT\nm\n)) .\nTheorem 2. For all T , q and all strategies, there exists a reward function such that\nRT \u2208 \u2126\n(\u221a m(q)\nT\n) .\nThe proofs of Theorems 1 and 2 may be found in Sections 7 and 8 respectively. By utilizing knowledge of the causal structure, Algorithm 1 effectively only has to explore the m(q) \u2019difficult\u2019 actions. Standard multi-armed bandit algorithms must explore all 2N actions and thus achieve regret \u2126( \u221a N/T ). Since m is typically much smaller than N , the new algorithm can significantly outperform classical bandit algorithms in this setting. In practice, you would combine the data from both phases to estimate rewards for the low probability actions. We do not do so here as it slightly complicates the proofs and does not improve the worst case regret."}, {"heading": "4 Regret Bounds for General Graphs", "text": "We now consider the more general problem where the graph structure is known, but arbitrary. For general graphs, P {Y |Xi = j} 6= P {Y |do(Xi = j)} (correlation is not causation). However, if all the variables are observable, any causal distribution P {X1...XN |do(Xi = j)} can be expressed in terms of observational distributions via the truncated factorization formula Pearl (2000).\nP {X1...XN |do(Xi = j)} = \u220f k 6=i P {Xk| PaXk} \u03b4(Xi \u2212 j) ,\nwhere PaXk denotes the parents of Xk and \u03b4 is the dirac delta function. We could naively generalize our approach for parallel bandits by observing for T/2 rounds, applying the truncated product factorization to write an expression for each P {Y |a} in terms of observational quantities and explicitly playing the actions for which the observational estimates were poor. However, it is no longer optimal to ignore the information we can learn about the reward for intervening on one variable from rounds in which we act on a different variable. Consider the graph in Figure 1c and suppose each variable deterministically takes the value of its parent, Xk = Xk\u22121 for k \u2208 2, . . . , N and P {X1} = 0. We can learn the reward for all the interventions do(Xi = 1) simultaneously by selecting do(X1 = 1), but not from do(). In addition, variance of the observational estimator for a = do(Xi = j) can be high even if P {Xi = j} is large. Given the causal graph in Figure 1b, P {Y |do(X2 = j)} = \u2211 X1\nP {X1}P {Y |X1, X2 = j}. Suppose X2 = X1 deterministically, no matter how large P {X2 = 1} is we will never observe (X2 = 1, X1 = 0) and so cannot get a good estimate for P {Y |do(X2 = 1)}. To solve the general problem we need an estimator for each action that incorporates information obtained from every other action and a way to optimally allocate samples to actions. To address this difficult problem, we assume the conditional interventional distributions P {PaY |a} (but not P {Y |a}) are known. These could be estimated from experimental data on the same covariates but where the outcome of interest differed, such that Y was not included, or similarly from observational data subject to identifiability constraints. Of course this is a somewhat limiting assumption, but seems like a natural place to start. The challenge of estimating the conditional distributions for all variables in an optimal way is left as an interesting future direction. Let \u03b7 be a distribution on available interventions a \u2208 A so \u03b7a \u2265 0 and \u2211 a\u2208A \u03b7a = 1. Define Q = \u2211 a\u2208A \u03b7a P {PaY |a} to be the mixture distribution over the interventions with respect to \u03b7. Algorithm 2 General Algorithm\nInput: T , \u03b7 \u2208 [0, 1]A, B \u2208 [0,\u221e)A for t \u2208 {1, . . . , T} do\nSample action at from \u03b7 Do action at and observe Xt and Yt\nfor a \u2208 A do\n\u00b5\u0302a = 1\nT T\u2211 t=1 YtRa(Xt)1{Ra(Xt) \u2264 Ba}\nreturn a\u0302\u2217T = arg maxa \u00b5\u0302a\nOur algorithm samples T actions from \u03b7 and uses them to estimate the returns \u00b5a for all a \u2208 A simultaneously via a truncated importance weighted estimator. Let PaY (X) denote the realization of the variables in X that are parents of Y and define Ra(X) =\nP{PaY (X)|a} Q{PaY (X)}\n\u00b5\u0302a = 1\nT T\u2211 t=1 YtRa(Xt)1{Ra(Xt) \u2264 Ba} ,\nwhere Ba \u2265 0 is a constant that tunes the level of truncation to be chosen subsequently. The truncation introduces a bias in the estimator, but simultaneously chops the potentially heavy tail that is so detrimental to its concentration guarantees.\nThe distribution over actions, \u03b7 plays the role of allocating samples to actions and is optimized to minimize the worst-case simple regret. Abusing notation we define m(\u03b7) by\nm(\u03b7) = max a\u2208A\nEa [\nP {PaY (X)|a} Q {PaY (X)}\n] , where Ea is the expectation with respect to P {.|a}\nWe will show shortly that m(\u03b7) is a measure of the difficulty of the problem that approximately coincides with the version for parallel bandits, justifying the name overloading.\nTheorem 3. If Algorithm 2 is run with B \u2208 RA given by Ba = \u221a m(\u03b7)T log(2T |A|) .\nRT \u2208 O\n(\u221a m(\u03b7)\nT log (2T |A|)\n) .\nThe proof is in Section 9. Note the regret has the same form as that obtained for Algorithm 1, with m(\u03b7) replacing m(q). Algorithm 1 assumes only the graph structure and not knowledge of the conditional distributions on X . Thus it has broader applicability to the parallel graph than the generic algorithm given here. We believe that Algorithm 2 with the optimal choice of \u03b7 is close to minimax optimal, but leave lower bounds for future work.\nChoosing the Sampling Distribution Algorithm 2 depends on a choice of sampling distribution Q that is determined by \u03b7. In light of Theorem 3 a natural choice of \u03b7 is the minimiser of m(\u03b7).\n\u03b7\u2217 = arg min \u03b7 m(\u03b7) = arg min \u03b7 max a\u2208A\nEa [\nP {PaY (X)|a}\u2211 b\u2208A \u03b7b P {PaY (X)|b} ] \ufe38 \ufe37\ufe37 \ufe38\nm(\u03b7)\n.\nSince the mixture of convex functions is convex and the maximum of a set of convex functions is convex, we see that m(\u03b7) is convex (in \u03b7). Therefore the minimisation problem may be tackled using standard techniques from convex optimisation. An interpretation of m(\u03b7\u2217) is the minimum achievable worst-case variance of the importance weighted estimator. In the experimental section we present some special cases, but for now we give two simple results. The first shows that |A| serves as an upper bound on m(\u03b7\u2217). Proposition 4. m(\u03b7\u2217) \u2264 |A|. Proof. By definition, m(\u03b7\u2217) \u2264 m(\u03b7) for all \u03b7. Let \u03b7a = 1/|A| \u2200a.\nm(\u03b7) = max a\nEa [\nP {PaY (X)|a} Q {PaY (X)}\n] \u2264 max a Ea [ P {PaY (X)|a} \u03b7a P {PaY (X)|a} ] = max a Ea [ 1 \u03b7a ] = |A|\nThe second observation is that, in the parallel bandit setting, m(\u03b7\u2217) \u2264 2m(q). This is easy to see by letting \u03b7a = 1/2 for a = do() and \u03b7a = 1{P {Xi = j} \u2264 1/m(q)} /2m(q) for the actions corresponding to do(Xi = j), and applying an argument like that for Proposition 4. The proof is in Section 9.1. Remark 5. The choice of Ba given in Theorem 3 is not the only possibility. As we shall see in the experiments, it is often possible to choose Ba significantly larger when there is no heavy tail and this can drastically improve performance by eliminating the bias. This is especially true when the ratio Ra is never too large and Bernstein\u2019s inequality could be used directly without the truncation. For another discussion see the article by Bottou et al. (2013) who also use importance weighted estimators to learn from observational data."}, {"heading": "5 Experiments", "text": "We compare Algorithms 1 and 2 with Successive Elimination on the parallel bandit problem under a variety of conditions, including where the importance weighted estimator used by Algorithm 2 is not\n0 10 20 30 40 50 m(q)\n0.0\n0.1\n0.2\n0.3\nRe gr et Algorithm 1 Algorithm 2 Successive Rejects\n(a) Simple regret vs m(q) for fixed horizon T = 400 and number of variables N = 50\ntruncated, which is justified in this setting by Remark 5. Throughout we use a model in which Y depends only on a single variable X1 (this is unknown to the algorithms). Yt \u223c Bernoulli( 12 + \u03b5) if X1 = 1 and Yt \u223c Bernoulli( 12 \u2212 \u03b5\n\u2032) otherwise, where \u03b5\u2032 = q1\u03b5/(1\u2212 q1). This leads to an expected reward of 12 + \u03b5 for do(X1 = 1), 1 2 \u2212 \u03b5\n\u2032 for do(X1 = 0) and 12 for all other actions. We set qi = 0 for i \u2264 m and 12 otherwise. Note that changing m and thus q has no effect on the reward distribution. We compare the performance of the Algorithm 1, which is specific to the parallel problem, but does not require knowledge of q, with that of Algorithm 2 and the Successive Reject algorithm of Audibert and Bubeck (2010). For each experiment, we show the average regret over 10,000 simulations with error bars displaying three standard errors.\nIn figure 2a we fix the number of variables N and the horizon T and compare the performance of the algorithms as m increases. The regret for the Successive Reject algorithm is constant as it depends only on the reward distribution and has no knowledge of the causal structure. For the causal algorithms it increases approximately with \u221a m. As m approaches N , the gain the causal algorithms obtain from knowledge of the structure is outweighed by fact they do not leverage the observed rewards to focus sampling effort on actions with high pay-offs.\nFigure 2b demonstrates the performance of the algorithms in the worst case environment for standard bandits, where the gap between the optimal and sub-optimal arms, \u03b5 = \u221a N/(8T ) , is just too small to be learned. This gap is learn-able by the causal algorithms, for which the worst case \u03b5 depends on m N . In figure 2c we fix N and \u03b5 and observe that, for sufficiently large T , the regret decays exponentially. The decay constant is larger for the causal algorithms as they have observed a greater effective number of samples for a given T .\nFor the parallel bandit problem, the regression estimator used in the specific algorithm outperforms the truncated importance weighted estimator in the more general algorithm, despite the fact the specific algorithm must estimate q from the data. This is an interesting phenomenon that has been noted before in off-policy evaluation where the regression (and not the importance weighted) estimator is known to be minimax optimal asymptotically (Li et al., 2014)."}, {"heading": "6 Discussion & Future Work", "text": "Algorithm 2 for general causal bandit problems estimates the reward for all allowable interventions a \u2208 A over T rounds by sampling and applying interventions from a distribution \u03b7. Theorem 3 shows that this algorithm has (up to log factors) simple regret that is O( \u221a m(\u03b7)/T ) where the parameter m(\u03b7) measures the difficulty of learning the causal model and is always less than N . The value of m(\u03b7) is a uniform bound on the variance of the reward estimators \u00b5\u0302a and, intuitively, problems where all variables\u2019 values in the causal model \u201coccur naturally\u201d when interventions are sampled from \u03b7 will have low values of m(\u03b7).\nThe main practical drawback of Algorithm 2 is that both the estimator \u00b5\u0302a and the optimal sampling distribution \u03b7\u2217 (i.e., the one that minimises m(\u03b7)) require knowledge of the conditional distributions P {PaY |a} for all a \u2208 A. In contrast, in the special case of parallel bandits, Algorithm 1 uses the do() action to effectively estimate m(\u03b7) and the rewards then re-samples the interventions with\nvariances that are not bound by m\u0302(\u03b7). Despite these extra estimates, Theorem 2 shows that this approach is optimal (up to log factors). Finding an algorithm that only requires the causal graph and lower bounds for its simple regret in the general case is left as future work.\nMaking Better Use of the Reward Signal Existing algorithms for best arm identification are based on \u201csuccessive rejection\u201d (SR) of arms based on UCB-like bounds on their rewards (Even-Dar et al., 2002). In contrast, our algorithms completely ignore the reward signal when developing their arm sampling policies and only use the rewards when estimating \u00b5\u0302a. Incorporating the reward signal into our sampling techniques or designing more adaptive reward estimators that focus on high reward interventions is an obvious next step. This would likely improve the poor performance of our causal algorithm relative to the sucessive rejects algorithm for large m, as seen in Figure 2a. For the parallel bandit the required modifications should be quite straightforward. The idea would be to adapt the algorithm to essentially use successive elimination in the second phase so arms are eliminated as soon as they are provably no longer optimal with high probability. In the general case a similar modification is also possible by dividing the budget T into phases and optimising the sampling distribution \u03b7, eliminating arms when their confidence intervals are no longer overlapping. Note that these modifications will not improve the minimax regret, which at least for the parallel bandit is already optimal. For this reason we prefer to emphasize the main point that causal structure should be exploited when available. Another observation is that Algorithm 2 is actually using a fixed design, which in some cases may be preferred to a sequential design for logistical reasons. This is not possible for Algorithm 1, since the q vector is unknown.\nCumulative Regret Although we have focused on simple regret in our analysis, it would also be natural to consider the cumulative regret. In the case of the parallel bandit problem we can slightly modify the analysis from (Wu et al., 2015) on bandits with side information to get near-optimal cumulative regret guarantees. They consider a finite-armed bandit model with side information where in reach round the learner chooses an action and receives a Gaussian reward signal for all actions, but with a known variance that depends on the chosen action. In this way the learner can gain information about actions it does not take with varying levels of accuracy. The reduction follows by substituting the importance weighted estimators in place of the Gaussian reward. In the case that q is known this would lead to a known variance and the only (insignificant) difference is the Bernoulli noise model. In the parallel bandit case we believe this would lead to near-optimal cumulative regret, at least asymptotically.\nThe parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0.\nCausal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem.\nPartially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al. (2014) could potentially be incorporated into a causal bandit algorithm. In particular, Hu et al. (2014) show that only O (log log n) multi-variable interventions are required on average to recover a causal graph over n variables once purely observational data is used to recover the \u201cessential\ngraph\u201d. Simultaneously learning a completely unknown causal model while estimating the rewards of interventions without a large observational dataset would be much more challenging."}, {"heading": "7 Proof of Theorem 1", "text": "Assume without loss of generality that q1 \u2264 q2 \u2264 . . . \u2264 qN \u2264 1/2. The assumption is non-restrictive since all variables are independent and permutations of the variables can be pushed to the reward function. The proof of Theorem 1 requires some lemmas. Lemma 6. Let i \u2208 {1, . . . , N} and \u03b4 > 0. Then\nP { |q\u0302i \u2212 qi| \u2265 \u221a 6qi T log 2 \u03b4 } \u2264 \u03b4 .\nProof. By definition, q\u0302i = 2T \u2211T/2 t=1 Xt,i, whereXt,i \u223c Bernoulli(qi). Therefore from the Chernoff bound (see equation 6 in Hagerup and R\u00fcb (1990)),\nP {|q\u0302i \u2212 qi| \u2265 \u03b5} \u2264 2e\u2212 T\u03b52 6qi\nLetting \u03b4 = 2e\u2212 T\u03b52 6qi and solving for \u03b5 completes the proof.\nLemma 7. Let X1, X2 . . . , be a sequence of random variables with Xi \u2208 [0, 1] and E[Xi] = p and \u03b4 \u2208 [0, 1]. Then\nP { \u2203t \u2265 n0 : \u2223\u2223\u2223\u2223\u22231t t\u2211\ns=1\nXs \u2212 p \u2223\u2223\u2223\u2223\u2223 \u2265 \u221a 2 n0 log 2 \u03b4 } \u2264 4\u03b4 .\nProof. For \u03b4 \u2265 1/4 the result is trivial. Otherwise by Hoeffding\u2019s bound and the union bound:\nP { \u2203t \u2265 n0 : \u2223\u2223\u2223\u2223\u22231t t\u2211\ns=1\nXs \u2212 p \u2223\u2223\u2223\u2223\u2223 \u2265 \u221a 2 n0 log 2 \u03b4 } \u2264 \u221e\u2211 t=n0 P {\u2223\u2223\u2223\u2223\u22231t t\u2211 s=1 Xs \u2212 p \u2223\u2223\u2223\u2223\u2223 \u2265 \u221a 2 n0 log 2 \u03b4 }\n\u2264 2 \u221e\u2211 t=n0 exp ( \u2212 t n0 log 2 \u03b4 ) \u2264 4\u03b4 .\nLemma 8. Let \u03b4 \u2208 (0, 1) and assume T \u2265 48m log 2N\u03b4 . Then\nP {2m(q)/3 \u2264 m(q\u0302) \u2264 2m(q)} \u2265 1\u2212 \u03b4 .\nProof. Let F be the event that there exists and 1 \u2264 i \u2264 N for which |q\u0302i \u2212 qi| \u2265 \u221a\n6qi T log 2N \u03b4 .\nThen by the union bound and Lemma 6 we have P {F} \u2264 \u03b4. The result will be completed by showing that when F does not hold we have 2m(q)/3 \u2264 m(q\u0302) \u2264 2m(q). From the definition of m(q) and our assumption on q we have for i > m that qi \u2265 qm \u2265 1/m and so by Lemma 6 we have\n3 4 \u2265 1 2 +\n\u221a 3\nT log\n2N\n\u03b4 \u2265 qi + \u221a 6qi T log 2N \u03b4 \u2265 q\u0302i\n\u2265 qi \u2212 \u221a\n6qi T log 2N \u03b4 \u2265 qi \u2212\n\u221a qi\n8m \u2265 1 2m .\nTherefore by the pigeonhole principle we have m(q\u0302) \u2264 2m. For the other direction we proceed in a similar fashion. Since the failure event F does not hold we have for i \u2264 m that\nq\u0302i \u2264 qi + \u221a\n6qi T log 2N \u03b4 \u2264 1 m\n( 1 + \u221a 1\n8\n) \u2264 3\n2m .\nTherefore m(q\u0302) \u2265 2m(q)/3 as required.\nProof of Theorem 1. Let \u03b4 = m = m(q)/N . Then by Lemma 8 we have\nP {2m/3 \u2264 m(q\u0302) \u2264 2m} \u2265 1\u2212 \u03b4 . Recall that A = {a \u2208 A : p\u0302a \u2264 1/m(q\u0302)}. Then for a \u2208 A the algorithm estimates \u00b5a from T/(2m(q\u0302)) \u2265 T/(4m) samples. Therefore by Hoeffding\u2019s inequality and the union bound we have\nP { \u2203a \u2208 A : |\u00b5a \u2212 \u00b5\u0302a| \u2265 \u221a 8m\nT log\n2N\n\u03b4\n} \u2264 \u03b4 .\nFor arms not in a we have p\u0302a \u2265 1/m(q\u0302) \u2265 1/(2m). Therefore if a = do(Xi = j), then\np\u0302a = 2\nT T/2\u2211 t=1 1{Xi = j} \u2265 1 2m .\nTherefore \u2211T/2 t=1 1{Xt,i = j} \u2265 T/4m and by Lemma 7 we have\nP  T/2\u2211 t=1 1{Xi = j} \u2265 T 4m and |\u00b5\u0302a \u2212 \u00b5a| \u2265 \u221a 8m T log 2N \u03b4  \u2264 4\u03b4/N . Therefore with probability at least 1\u2212 6\u03b4 we have\n(\u2200a \u2208 A) |\u00b5\u0302a \u2212 \u00b5a| \u2264 \u221a 8m\nT log\nN\n\u03b4 = \u03b5 .\nIf this occurs, then\n\u00b5a\u0302\u2217T \u2265 \u00b5\u0302a\u0302\u2217T \u2212 \u03b5 \u2265 \u00b5\u0302a\u2217 \u2212 \u03b5 \u2265 \u00b5a\u2217 \u2212 2\u03b5 . Therefore\n\u00b5\u2217 \u2212 E[\u00b5a\u0302\u2217T ] \u2264 6\u03b4 + \u03b5 \u2264 6m\nT +\n\u221a 32m\nT log\nNT\nm ,\nwhich completes the result."}, {"heading": "8 Proof of Theorem 2", "text": "We follow a relatively standard path by choosing multiple environments that have different optimal arms, but which cannot all be statistically separated in T rounds. Assume without loss of generality that q1 \u2264 q2 \u2264 . . . \u2264 qN \u2264 1/2. For each i define reward function ri by\nr0(X) = 1\n2 ri(X) = { 1 2 + \u03b5 if Xi = 1 1 2 otherwise ,\nwhere 1/4 \u2265 \u03b5 > 0 is some constant to be chosen later. We abbreviate RT,i to be the expected simple regret incurred when interacting with the environment determined by q and ri. Let Pi be the corresponding measure on all observations over all T rounds and Ei the expectation with respect to Pi. By Lemma 2.6 by Tsybakov (2008) we have"}, {"heading": "P0 {a\u0302\u2217T = a\u2217}+ Pi {a\u0302\u2217T 6= a\u2217} \u2265 exp (\u2212KL(P0,Pi)) ,", "text": "where KL(P0,Pi) is the KL divergence between measures P0 and Pi. Let Ti(T ) =\u2211T t=1 1{at = do(Xi = 1)} be the total number of times the learner intervenes on variable i by setting it to 1. Then for i \u2264 m we have qi \u2264 1/m and the KL divergence between P0 and Pi may be bounded using the telescoping property (chain rule) and by bounding the local KL divergence by the \u03c7-squared distance as by Auer et al. (1995). This leads to\nKL(P0,Pi) \u2264 6\u03b52E0 [ T\u2211 t=1 1{Xt,i = 1} ] \u2264 6\u03b52 (E0Ti(T ) + qiT ) \u2264 6\u03b52 ( E0Ti(T ) + T m ) .\nDefine set A = {i \u2264 m : E0Ti(T ) \u2264 2T/m}. Then for i \u2208 A and choosing \u03b5 = min { 1/4, \u221a m/(18T ) } we have\nKL(P0,Pi) \u2264 18T\u03b52\nm = 1 . Now \u2211m i=1 E0Ti(T ) \u2264 T , which implies that |A| \u2265 m/2. Therefore\u2211\ni\u2208A Pi {a\u0302\u2217T 6= a} \u2265 \u2211 i\u2208A exp (\u2212KL(P0,Pi))\u2212 1 \u2265 |A| e \u2212 1 \u2265 m 2e \u2212 1 .\nTherefore there exists an i \u2208 A such that Pi {a\u0302\u2217T 6= a\u2217} \u2265 m 2e\u22121 m . Therefore if \u03b5 < 1/4 we have\nRT,i \u2265 1\n2 P {a\u0302\u2217T 6= a\u2217|i} \u03b5 \u2265\nm 2e \u2212 1\n2m\n\u221a m\n18T .\nOtherwise m \u2265 18T so \u221a m/T = \u2126(1) and\nRT,i \u2265 1\n2 P {a\u0302\u2217T 6= a\u2217|i} \u03b5 \u2265\n1\n4\nm 2e \u2212 1\n2m \u2208 \u2126(1)\nas required."}, {"heading": "9 Proof of Theorem 3", "text": "Proof. First note that Xt, Yt are sampled from Q. We define Za(Xt) = YtRa(Xt)1{Ra(Xt) \u2264 Ba} and abbreviate Zat = Za(Xt), Rat = Ra(Xt) and P {.|a} = Pa {.}. By definition we have |Zat| \u2264 Ba and\nVarQ[Zat] \u2264 EQ[Z2at] \u2264 EQ[R2at] = Ea[Rat] = Ea [\nPa {PaY (X)} Q {PaY (X)}\n] \u2264 m(\u03b7) .\nChecking the expectation we have\nEQ[Zat] = Ea [Y 1{Rat \u2264 Ba}] = EaY \u2212 Ea [Y 1{Rat > Ba}] = \u00b5a \u2212 \u03b2a ,\nwhere\n0 \u2264 \u03b2a = Ea[Y 1{Rat > Ba}] \u2264 Pa {Rat > Ba}\nis the negative bias. The bias may be bounded in terms of m(\u03b7) via an application of Markov\u2019s inequality.\n\u03b2a \u2264 Pa {Rat > Ba} \u2264 Ea[Rat] Ba \u2264 m(\u03b7) Ba .\nLet \u03b5a > 0 be given by\n\u03b5a =\n\u221a 2m(\u03b7)\nT log (2T |A|) + 3Ba T log (2T |A|) .\nThen by the union bound and Bernstein\u2019s inequality P {exists a \u2208 A : |\u00b5\u0302a \u2212 EQ[Zat]| \u2265 \u03b5a} \u2264 \u2211 a\u2208A P {|\u00b5\u0302a \u2212 EQ[Zat]| \u2265 \u03b5a} \u2264 1 T .\nLet I = a\u0302\u2217T be the action selected by the algorithm, a \u2217 = arg maxa\u2208A \u00b5a be the true optimal action and recall that EQ[Zat] = \u00b5a \u2212 \u03b2a. Assuming the above event does not occur we have,\n\u00b5I \u2265 \u00b5\u0302I \u2212 \u03b5I \u2265 \u00b5\u0302a\u2217 \u2212 \u03b5I \u2265 \u00b5\u2217 \u2212 \u03b5a\u2217 \u2212 \u03b5I \u2212 \u03b2a\u2217 .\nBy the definition of the truncation we have\n\u03b5a \u2264 (\u221a 2 + 3 )\u221am(\u03b7)\nT log (2T |A|)\nand\n\u03b2a \u2264 \u221a m(\u03b7)\nT log (2T |A|) .\nTherefore for C = \u221a 2 + 4 we have\nP { \u00b5I \u2265 \u00b5\u2217 \u2212 C \u221a m(\u03b7)\nT log (2T |A|) } \u2264 1 T .\nTherefore\n\u00b5\u2217 \u2212 E[\u00b5I ] \u2264 C \u221a m(\u03b7)\nT log (2T |A|) + 1 T\nas required.\n9.1 Relationship between m(\u03b7) and m(q)\nProposition 9. In the parallel bandit setting, m(\u03b7\u2217) \u2264 2m(q).\nProof. Recall that in the parallel bandit setting,\nA = {do()} \u222a {do(Xi = j) : 1 \u2264 i \u2264 N and j \u2208 {0, 1}}\nLet:\n\u03b7a = 1 { P {Xi = j} < 1\nm(q)\n} 1\n2m(q) for a \u2208 do(Xi = j)\nLet D = \u2211 a\u2208do(Xi=j) \u03b7a. From the definition of m(q),\u2211\na\u2208do(Xi=j)\n1 { P {Xi = j} < 1\nm(q)\n} \u2264 m(q) =\u21d2 D \u2264 1\n2\nLet \u03b7a = 12 + (1\u2212D) for a = do() such that \u2211 a\u2208A \u03b7a = 1\nRecall that,\nm(\u03b7) = max a\nEa [\nP {PaY (X)|a} Q {PaY (X)}\n]\nWe now show that our choice of \u03b7 ensures Ea [ P{PaY (X)|a} Q{PaY (X)} ] \u2264 2m(q) for all actions a.\nFor the actions a : \u03b7a > 0, ie do() and do(Xi = j) : P {Xi = j} < 1m(q) ,\nEa [\nP {X1...XN |a}\u2211 b \u03b7b P {X1...XN |b}\n] \u2264 Ea [ P {X1...XN |a} \u03b7a P {X1...XN |a} ] = Ea [ 1 \u03b7a ] \u2264 2m(q)\nFor the actions a : \u03b7a = 0, ie do(Xi = j) : P {Xi = j} \u2265 1m(q) ,\nEa [\nP {X1...XN |a}\u2211 b \u03b7b P {X1...XN |b}\n] \u2264Ea [ 1{Xi = j} \u220f k 6=i P {Xk}\n(1/2 +D) \u220f k P {Xk} ] =Ea [ 1{Xi = j}\n(1/2 +D) P {Xi = j}\n] \u2264 Ea [ 1{Xi = j}\n(1/2)(1/m(q))\n] \u2264 2m(q)\nTherefore m(\u03b7\u2217) \u2264 m(\u03b7) \u2264 2m(q) as required."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["A. Agarwal", "D. Hsu", "S. Kale", "J. Langford", "L. Li", "R.E. Schapire"], "venue": null, "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Online learning with feedback graphs: Beyond bandits", "author": ["N. Alon", "N. Cesa-Bianchi", "O. Dekel", "T. Koren"], "venue": "In COLT,", "citeRegEx": "Alon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2015}, {"title": "Best arm identification in multi-armed bandits. In COLT, pages 13\u2013p", "author": ["Audibert", "J.-Y", "S. Bubeck"], "venue": null, "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "Proceedings of IEEE 36th Annual Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Decoupling exploration and exploitation in multi-armed bandits", "author": ["O. Avner", "S. Mannor", "O. Shamir"], "venue": "In ICML,", "citeRegEx": "Avner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Avner et al\\.", "year": 2012}, {"title": "Bandits with unobserved confounders: A causal approach", "author": ["E. Bareinboim", "A. Forney", "J. Pearl"], "venue": "In NIPS,", "citeRegEx": "Bareinboim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bareinboim et al\\.", "year": 2015}, {"title": "Partial monitoring-classification, regret bounds, and algorithms", "author": ["G. Bart\u00f3k", "D.P. Foster", "D. P\u00e1l", "A. Rakhlin", "C. Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2014}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["L. Bottou", "J. Peters", "J. Quinonero-Candela", "D.X. Charles", "D.M. Chickering", "E. Portugaly", "D. Ray", "P. Simard", "E. Snelson"], "venue": null, "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In ALT,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Sequential design of experiments", "author": ["H. Chernoff"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Chernoff,? \\Q1959\\E", "shortCiteRegEx": "Chernoff", "year": 1959}, {"title": "Causal Discovery as a Game", "author": ["F. Eberhardt"], "venue": "In NIPS Causality: Objectives and Assessment,", "citeRegEx": "Eberhardt,? \\Q2010\\E", "shortCiteRegEx": "Eberhardt", "year": 2010}, {"title": "On the number of experiments sufficient and in the worst case necessary to identify all causal relations among n variables", "author": ["F. Eberhardt", "C. Glymour", "R. Scheines"], "venue": "In UAI", "citeRegEx": "Eberhardt et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Eberhardt et al\\.", "year": 2005}, {"title": "Pac bounds for multi-armed bandit and markov decision processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Computational Learning Theory,", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric"], "venue": "In NIPS,", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "A guided tour of chernoff bounds", "author": ["T. Hagerup", "C. R\u00fcb"], "venue": "Information processing letters,", "citeRegEx": "Hagerup and R\u00fcb,? \\Q1990\\E", "shortCiteRegEx": "Hagerup and R\u00fcb", "year": 1990}, {"title": "Two optimal strategies for active learning of causal models from interventional data", "author": ["A. Hauser", "P. B\u00fchlmann"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Hauser and B\u00fchlmann,? \\Q2014\\E", "shortCiteRegEx": "Hauser and B\u00fchlmann", "year": 2014}, {"title": "Randomized experimental design for causal graph discovery", "author": ["H. Hu", "Z. Li", "A.R. Vetta"], "venue": "In NIPS,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "lil\u2019UCB: An optimal exploration algorithm for multi-armed bandits", "author": ["K. Jamieson", "M. Malloy", "R. Nowak", "S. Bubeck"], "venue": "In COLT,", "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["T. Koc\u00e1k", "G. Neu", "M. Valko", "R. Munos"], "venue": "In NIPS,", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Langford and Zhang,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang", "year": 2008}, {"title": "On minimax optimal offline policy evaluation", "author": ["L. Li", "R. Munos", "C. Szepesvari"], "venue": "arXiv preprint arXiv:1409.3653", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Generalized thompson sampling for sequential decision-making and causal inference", "author": ["P.A. Ortega", "D.A. Braun"], "venue": "Complex Adaptive Systems Modeling,", "citeRegEx": "Ortega and Braun,? \\Q2014\\E", "shortCiteRegEx": "Ortega and Braun", "year": 2014}, {"title": "Causality: models, reasoning and inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q2000\\E", "shortCiteRegEx": "Pearl", "year": 2000}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Introduction to nonparametric estimation", "author": ["A.B. Tsybakov"], "venue": null, "citeRegEx": "Tsybakov,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov", "year": 2008}, {"title": "Online Learning with Gaussian Payoffs and Side Observations", "author": ["Y. Wu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Piecewise-stationary bandit problems with side observations", "author": ["J.Y. Yu", "S. Mannor"], "venue": "In ICML,", "citeRegEx": "Yu and Mannor,? \\Q2009\\E", "shortCiteRegEx": "Yu and Mannor", "year": 2009}, {"title": "1{at = do(Xi = 1)} be the total number of times the learner intervenes on variable i by setting it to 1. Then for i \u2264 m we have qi \u2264 1/m and the KL divergence between P0 and Pi may be bounded using the telescoping property (chain rule) and by bounding the local KL divergence by the \u03c7-squared distance", "author": ["Auer"], "venue": null, "citeRegEx": "Auer,? \\Q1995\\E", "shortCiteRegEx": "Auer", "year": 1995}], "referenceMentions": [{"referenceID": 24, "context": "Medical drug testing, policy setting, and other scientific processes are commonly framed and analysed in the language of sequential experimental design and, in special cases, as bandit problems (Robbins, 1952; Chernoff, 1959).", "startOffset": 194, "endOffset": 225}, {"referenceID": 9, "context": "Medical drug testing, policy setting, and other scientific processes are commonly framed and analysed in the language of sequential experimental design and, in special cases, as bandit problems (Robbins, 1952; Chernoff, 1959).", "startOffset": 194, "endOffset": 225}, {"referenceID": 23, "context": "We propose a generalisation of the standard model by assuming that, in addition to the reward signal, the learner observes the values of a number of covariates drawn from a probabilistic causal model (Pearl, 2000).", "startOffset": 200, "endOffset": 213}, {"referenceID": 17, "context": "Related Work As alluded to above, causal bandit problems can be treated as classical multi-armed bandit problems by simply ignoring the causal model and extra observations and applying an existing best-arm identification algorithm with well understood simple regret guarantees (Jamieson et al., 2014).", "startOffset": 277, "endOffset": 300}, {"referenceID": 6, "context": "(2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph.", "startOffset": 79, "endOffset": 100}, {"referenceID": 8, "context": "They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009).", "startOffset": 90, "endOffset": 111}, {"referenceID": 11, "context": "Related Work As alluded to above, causal bandit problems can be treated as classical multi-armed bandit problems by simply ignoring the causal model and extra observations and applying an existing best-arm identification algorithm with well understood simple regret guarantees (Jamieson et al., 2014). However, as we show in \u00a73, ignoring the extra information available in the non-intervened variables yields sub-optimal performance. A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al.", "startOffset": 278, "endOffset": 547}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention.", "startOffset": 114, "endOffset": 136}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al.", "startOffset": 114, "endOffset": 635}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al.", "startOffset": 114, "endOffset": 656}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph. As we discuss in \u00a76, the parallel bandit problem can be captured in this framework, however the regret bounds are not optimal in our setting. They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009). The partial monitoring approach taken by Wu et al. (2015) could be applied (up to modifications for the simple regret) to the parallel bandit, but the resulting strategy would need to know the likelihood of each factor in advance, while our strategy learns this online.", "startOffset": 114, "endOffset": 1141}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph. As we discuss in \u00a76, the parallel bandit problem can be captured in this framework, however the regret bounds are not optimal in our setting. They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009). The partial monitoring approach taken by Wu et al. (2015) could be applied (up to modifications for the simple regret) to the parallel bandit, but the resulting strategy would need to know the likelihood of each factor in advance, while our strategy learns this online. Yu and Mannor (2009) utilize extra observations to detect changes in the reward distribution, whereas we assume fixed reward distributions and use extra observations to improve arm selection.", "startOffset": 114, "endOffset": 1374}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph. As we discuss in \u00a76, the parallel bandit problem can be captured in this framework, however the regret bounds are not optimal in our setting. They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009). The partial monitoring approach taken by Wu et al. (2015) could be applied (up to modifications for the simple regret) to the parallel bandit, but the resulting strategy would need to know the likelihood of each factor in advance, while our strategy learns this online. Yu and Mannor (2009) utilize extra observations to detect changes in the reward distribution, whereas we assume fixed reward distributions and use extra observations to improve arm selection. Avner et al. (2012) analyse bandit problems where the choice of arm to pull and arm to receive feedback on are decoupled.", "startOffset": 114, "endOffset": 1565}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph. As we discuss in \u00a76, the parallel bandit problem can be captured in this framework, however the regret bounds are not optimal in our setting. They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009). The partial monitoring approach taken by Wu et al. (2015) could be applied (up to modifications for the simple regret) to the parallel bandit, but the resulting strategy would need to know the likelihood of each factor in advance, while our strategy learns this online. Yu and Mannor (2009) utilize extra observations to detect changes in the reward distribution, whereas we assume fixed reward distributions and use extra observations to improve arm selection. Avner et al. (2012) analyse bandit problems where the choice of arm to pull and arm to receive feedback on are decoupled. The main difference from our present work is our focus on simple regret and the more complex information linking rewards for different arms via causal graphs. To the best of our knowledge, our paper is the first to analyse simple regret in bandit problems with extra post-action feedback. Two pieces of recent work also consider applying ideas from causal inference to bandit problems. Bareinboim et al. (2015) demonstrate that in the presence of confounding variables the value that a variable would have taken had it not been intervened on can provide important contextual information.", "startOffset": 114, "endOffset": 2078}, {"referenceID": 7, "context": "The truncated importance weighted estimators used in \u00a74 have been studied before in a causal framework by Bottou et al. (2013), where the focus is on learning from observational data, but not controlling the sampling process.", "startOffset": 106, "endOffset": 127}, {"referenceID": 8, "context": "This is sometimes refered to as a \u201cpure exploration\u201d (Bubeck et al., 2009) or \u201cbest-arm identification\u201d problem (Gabillon et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 13, "context": ", 2009) or \u201cbest-arm identification\u201d problem (Gabillon et al., 2012) and is most appropriate when, as in drug and policy testing, the learner has a fixed experimental budget after which its policy will be fixed indefinitely.", "startOffset": 45, "endOffset": 68}, {"referenceID": 0, "context": "If the non-intervened variables are observed before an intervention is selected our framework reduces to stochastic contextual bandits, which are already reasonably well understood (Agarwal et al., 2014).", "startOffset": 181, "endOffset": 203}, {"referenceID": 19, "context": "In these problems, rewards are given for repeated interventions on a fixed causal model Pearl (2000). Following the terminology and notation in Koller and Friedman (2009), a causal model is given by a directed acyclic graph G over a set of random variables X = {X1, .", "startOffset": 88, "endOffset": 101}, {"referenceID": 16, "context": "Following the terminology and notation in Koller and Friedman (2009), a causal model is given by a directed acyclic graph G over a set of random variables X = {X1, .", "startOffset": 42, "endOffset": 69}, {"referenceID": 16, "context": "Following the terminology and notation in Koller and Friedman (2009), a causal model is given by a directed acyclic graph G over a set of random variables X = {X1, . . . , XN} and a joint distribution P over X that factorises over G. We will assume each variable only takes on a finite number of distinct values. An edge from variable Xi to Xj is interpreted to mean that a change in the value of Xi may directly cause a change to the value of Xj . The parents of a variable Xi, denoted PaXi , is the set of all variables Xj such that there is an edge from Xj to Xi in G. An intervention or action (of size n), denoted do(X = x), assigns the values x = {x1, . . . , xn} to the corresponding variables X = {X1, . . . , Xn} \u2282 X with the empty intervention (where no variable is set) denoted do(). The intervention also \u201cmutilates\u201d the graph G by removing all edges from Pai to Xi for each Xi \u2208X . The resulting graph defines a probability distribution P {X|do(X = x)} over X := X \u2212X . Details can be found in Chapter 21 of Koller and Friedman (2009). A learner for a casual bandit problem is given the casual model\u2019s graph G and a set of allowed actions A.", "startOffset": 42, "endOffset": 1048}, {"referenceID": 23, "context": "XN |do(Xi = j)} can be expressed in terms of observational distributions via the truncated factorization formula Pearl (2000).", "startOffset": 113, "endOffset": 126}, {"referenceID": 7, "context": "For another discussion see the article by Bottou et al. (2013) who also use importance weighted estimators to learn from observational data.", "startOffset": 42, "endOffset": 63}, {"referenceID": 21, "context": "This is an interesting phenomenon that has been noted before in off-policy evaluation where the regression (and not the importance weighted) estimator is known to be minimax optimal asymptotically (Li et al., 2014).", "startOffset": 197, "endOffset": 214}, {"referenceID": 12, "context": "Making Better Use of the Reward Signal Existing algorithms for best arm identification are based on \u201csuccessive rejection\u201d (SR) of arms based on UCB-like bounds on their rewards (Even-Dar et al., 2002).", "startOffset": 178, "endOffset": 201}, {"referenceID": 26, "context": "In the case of the parallel bandit problem we can slightly modify the analysis from (Wu et al., 2015) on bandits with side information to get near-optimal cumulative regret guarantees.", "startOffset": 84, "endOffset": 101}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen.", "startOffset": 103, "endOffset": 142}, {"referenceID": 18, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen.", "startOffset": 103, "endOffset": 142}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al.", "startOffset": 104, "endOffset": 578}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret.", "startOffset": 104, "endOffset": 599}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0. Causal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem. Partially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al.", "startOffset": 104, "endOffset": 2139}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0. Causal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem. Partially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al.", "startOffset": 104, "endOffset": 2157}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0. Causal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem. Partially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al.", "startOffset": 104, "endOffset": 2185}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0. Causal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem. Partially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al. (2014) could potentially be incorporated into a causal bandit algorithm.", "startOffset": 104, "endOffset": 2203}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0. Causal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem. Partially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al. (2014) could potentially be incorporated into a causal bandit algorithm. In particular, Hu et al. (2014) show that only O (log log n) multi-variable interventions are required on average to recover a causal graph over n variables once purely observational data is used to recover the \u201cessential", "startOffset": 104, "endOffset": 2301}], "year": 2016, "abstractText": "We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information.", "creator": "LaTeX with hyperref package"}}}