{"id": "1606.03207", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Deep CNNs along the Time Axis with Intermap Pooling for Robustness to Spectral Variations", "abstract": "Convolutional neural networks (formula CNNs) with pottuvil convolutional and pooling mahasaya operations furbies along the buttercup frequency 750 axis gaviria have engelberger been fabolous proposed istrabadi to attain wethered invariance pilati to frequency castagnoli shifts of cities/abc features. beaverhead However, carby this attu is 18-kilometer inappropriate with regard bsn to the dinon fact schwarmerei that turnabout acoustic 58.65 features vary in garas frequency. omitting In this exquisite paper, we contend that convolution orekhov along the time basesgioglu axis actuelles is tenille more effective. mid-1st We also propose the niland addition bacala of azzai an halfords intermap pooling (IMP) adygeya layer roots to duboc deep CNNs. In this forestlands layer, mangope filters narrowness in bosic each group extract mothballs common but spectrally variant features, mittweida then morula the ladders layer claudia pools the senthil feature gabin maps +.03 of cattleman each group. As equinix a skalmierzyce result, the linsey proposed IMP CNN can injury-free achieve bandhs insensitivity to spectral dispater variations annan characteristic acrolepiidae of andross different tschula speakers and utterances. kinboshi The someday effectiveness of the IMP lhr CNN taqiabad architecture is bhati demonstrated on molokans several familiarize LVCSR tasks. barnabei Even 0.045 without speaker tsugio adaptation abscesses techniques, the urner architecture kanakas achieved a WER tanapag of varg 12. 7% on peada the moer SWB 8:55 part lipovac of modula-3 the mashups Hub5 ' 2000 evaluation mirrabooka test set, 1.66 which wikileaks is yacob competitive misuzu with anseong other state - wu-tang of - the - culloden art methods.", "histories": [["v1", "Fri, 10 Jun 2016 06:44:21 GMT  (8488kb)", "https://arxiv.org/abs/1606.03207v1", "5 pagegs, 4 figures, 5 tables"], ["v2", "Tue, 12 Jul 2016 07:23:53 GMT  (9113kb)", "http://arxiv.org/abs/1606.03207v2", "Submitted to IEEE Signal Processing Letters"]], "COMMENTS": "5 pagegs, 4 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["hwaran lee", "geonmin kim", "ho-gyeong kim", "sang-hoon oh", "soo-young lee"], "accepted": false, "id": "1606.03207"}, "pdf": {"name": "1606.03207.pdf", "metadata": {"source": "CRF", "title": "Deep CNNs along the Time Axis with Intermap Pooling for Robustness to Spectral Variations", "authors": ["Hwaran Lee", "Geonmin Kim", "Ho-Gyeong Kim", "Sang-Hoon Oh", "Soo-Young Lee"], "emails": ["sylee}@kaist.ac.kr)", "shoh@mokwon.ac.kr)"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n03 20\n7v 2\n[ cs\n.C L\n] 1\n2 Ju\nl 2 01\n6 1\nIndex Terms\u2014intermap pooling layer, convolutional neural networks, acoustic modeling\nI. INTRODUCTION\nACOUSTIC modeling with deep learning has demon-strated remarkable performance improvements in automatic speech recognition [1]\u2013[4]. Deep neural networks (DNNs) are trained to label each frame of processed speech data with the state of a hidden Markov model (HMM). However, there is a difficulty due to the fact that acoustic features vary widely in frequency and articulation rate depending on harmonics of the vocal tract and characteristic speaking styles.\nEfforts to effectively handle these variations can be categorized into feature-level and model-level approaches. Amongst feature-level approaches, speaker-adapted methods such as fMLLR [5] have been proposed. Acoustic features concatenated with i-vectors, which represent speaker information, also have been employed as input for DNNs [6], [7]. Model-level approaches have employed hybrid NN-HMM systems with convolutional neural networks (CNNs) [8]\u2013[10] and recurrent neural networks (RNNs) [11]\u2013[13].\nIn particular, CNNs have advantages in terms of capturing local features through weight sharing while remaining robust to slight translations of these features through pooling.\nThis work was supported by the ICT R& D program of MSIP IITP. [R012615-1117, Core technology development of the spontaneous speech dialogue processing for the language learning]\nH. Lee, G. Kim, H.-G. Kim and S.-Y. Lee are with the School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon 305-701, Korea. (e-mail: {hwaran.lee, gmkim90, hogyeong, sylee}@kaist.ac.kr)\nS.-H Oh is with the Division of Information and Communication Convergence Engineering, Mokwon University, Daejeon, 302-318, Korea. (e-mail: shoh@mokwon.ac.kr)\nStructural advantages of CNNs enable the modeling of speech data without feature-level engineering, such as spectrograms or mel filter-banks. Previous researchers introduced time-delayed neural networks (TDNNs), which are CNNs with convolutions along the time axis to learn the temporal dynamics of features [14]\u2013[16]. Other researchers have applied convolutions along the frequency axis to attain invariance to frequency-shifts [8], [17]. However, acoustic features of speech vary in frequency, so that weight sharing along the frequency axis may not be appropriate. The limited weight sharing method in which weights are convolved only within a subsection of frequencybands has been employed in efforts to overcome this problem [10]. However, settling on appropriate band divisions and filters will require further work.\nOne limitation common to the preceding approaches is that most of them have employed only one or two convolution and pooling layers. Another limitation is that the relationship or topography of filters trained in supervised learning has not been intensively investigated. For unsupervised feature extraction, previous researchers imposed sparsity terms over small groups or neighborhoods in feature maps of image [18]\u2013 [20] and speech data [21], [22]. They attained topographicallyorganized maps of smoothly varying oriented edge filters or tonotopic disordered topography of spectrotemporal features, such as those found in the primary visual or auditory cortex (V1, A1) respectively.\nIn this paper, we argue that convolution along the time axis is more effective than along the frequency axis for acoustic models. In order that the network learns temporal dynamics adequately, we increase the depth of convolution layers that have small filters. Instead of frequency-axis convolution and pooling, we propose the addition of a convolutional maxout layer, namely an intermap pooling (IMP) layer in order to increase robustness to spectral variations. Previously, a convolutional maxout network has been proposed [23], however, it applied convolutions along the frequency axis. We show that the IMP CNNs with the time convolution reduce the word error rates more. As a result, the IMP CNNs can both model temporal dynamics and remain robust to spectral variations."}, {"heading": "II. CONVOLUTION NEURAL NETWORKS", "text": "CNNs consist of the alternation of convolution and pooling layers, and fully connected layers in the top-most layer. Let H\n(l) stand for input to the lth convolution layer having K filters, with the kth convolution filter denoted W(l)\nk \u2208 RM\u00d7N\u00d7G\nwith M and N denoting a filters height and width respectively, and G designating the number of feature maps of the input.\n2 A bias term b(l) k\nis shared inside the kth feature map. Thus, from any input H(l), the output H\u0303(l+1) \u2208 RI\u00d7J\u00d7K can be calculated as\nH\u0303 (l+1) (i,j,k) = f((H(l) \u2217W (l) k )(i,j) + b (l) k )\n(for 1 \u2264 i \u2264 I, 1 \u2264 j \u2264 J), (1)\nwhere I and J are height and width of each output feature map, and f is a non-linear function such as sigmoid or rectified linear unit (ReLU).\nAn intramap pooling layer, typically called \u201cmax-pooling\u201d, propagates the maximum value from each sub-region in each feature map. For non-overlapping sub-regions with height p and width q, the output from this pooling layer is given by\nH (l) (i,j,k)\n= max \u03b1=\u2212p+1,...,0 \u03b2=\u2212q+1,...,0\nH\u0303 (l) (ip+\u03b1,jq+\u03b2,k) . (2)\nIntramap pooling layers have blurring effects on feature maps, with the result being that the CNN is more robust to locally translated features.\nIII. INTERMAP POOLING LAYERS\nThere are several categories of acoustic features such as harmonics, formants, and on/offsets (i.e., start and end points of speech). Spectral variations of acoustic features appear as shifts in the frequency axis over time (spectro-temporal modulation). In order to ensure the robustness of our model to spectral variations, we propose the addition of a convolutional maxout layer, the intermap pooling (IMP) layer. Like the maxout networks [24], this layer groups the filters, and pools the feature maps inside a group.\nSpecifically, an intermap pooling layer partitions feature maps into a set of groups. Then each group propagates the maximum activation value at each position. Formally, the output of the kth group consisting of r consecutive feature maps is given by\nH (l) (i,j,k)\n= max \u03b3=\u2212r+1,...,0\nH\u0303 (l) (i,j,kr+\u03b3) . (3)\nThe structural comparison of intermap and intramap pooling layers is shown in Fig.1. Note that the method pursued in this paper does not introduce any additional learning terms except for the intermap grouping of filters.\nThe central idea to the IMP CNN is that the filters in each group learn common but spectrally variant features, such as frequency-shifted harmonics, and the pooled feature map is invariant to those feature variations within the group. The pooled feature maps H(l) are representative of the feature maps in each group. Through supervised learning, the pooled feature maps become discriminative of features for recognizing phonemes. For a phoneme, since spectral variations among different speakers and utterances are not discriminative information, the individual filters in a group spontaneously represent common but spectrally variant features, even though the layer does not ensure this."}, {"heading": "IV. DEEP CNN ARCHITECTURE FOR ACOUSTIC MODELING", "text": "Since short-term temporal dynamics are shared within every frame of a given speech sample, sharing filters along the\nConvolution layer Intramap pooling layer (max-pooling 1:2:1)\nIntermap pooling layer (max-pooling 1:1:2)\nInput 40x21x1\nConv. Filters\n40x4x1 @ 128\nFeature maps\n1x20x128\nPooled feature maps\n1x20x64\nPooled feature maps\n1x10x128\nF re\nq u\ne n\nc y\nb a\nn d\ns\nContext windowed frames\nFig. 1. An illustration of a convolution layer followed by an intramap pooling layer or an intermap pooling layer. The sizes of convolution filters and feature maps are denoted as \u2018(the freq. axis)\u00d7(the time axis)\u00d7(the feature axis)\u2019. The number of filters is denoted after \u2018@\u2019. The pooling size is denoted as \u2018(the freq. axis):(the time axis):(the feature axis)\u2019. The convolution input is padded with zeros at both ends in the time axis in order to preserve frame length after convolutions.\ntime axis is reasonable. However, sharing filters along the frequency axis may not be suitable, because features within lower frequency-band regions are significantly different from those in the higher regions. Instead of convolution along the frequency axis, our architecture employs an intermap pooling layer following the first convolution layer. This approach demonstrates robustness not only to frequency-shifted features but also to spectro-temporally distorted features. Moreover, it does not require engineered efforts to consider the varying characteristics of different frequency-bands.\nA sufficiently deep depth of convolution and pooling layers is necessary to precisely represent complex acoustic features with temporal and spectral variations. Individual frames also should be labeled as minutely as the number of HMM states, which is more than thousands in tri-phone modeling. However, context windowed inputs are too tiny (e.g., 21 frames) and stacking multiple intramap pooling layers decreases the feature map size in proportion to the pooling size, thereby restricting the depth of CNNs. Since, previous researchers have chosen large convolution filter and intramap pooling sizes, sufficient increases in depths of CNNs have not been realized.\nAs illustrated in Fig.1, the IMP CNN architecture applies convolution and intramap pooling layers only along the time axis. The pooling size of the intramap pooling layers is small so that it does not decrease temporal resolution much. Furthermore, motivated by the performance of very deep CNNs [25], we inserted convolution layers with small filters (of size 1x3) between two intramap pooling layers. The combination of filters before pooling layers increases non-linearity, and this results in a network that has rich feature expressions."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Experiments setup", "text": "We conducted experiments using the 300 hour SwitchboardI Release 2 (SWBD) dataset [26] which is conversational telephone speech task as well as the Wall Street Journal (WSJ) corpus [27] and Aurora4 database which are read speech. We used the 81-hour training dataset (SI-284) of the WSJ corpus. The Aurora4 database is a subset of the WSJ in which clean utterances are added with different noise types and/or convolved with microphone distortions. The following results\n3 pool, 1:2:1\n1x2x512 @512 1x3x256 @512\npool, 1:2:1\n1x3x256 @256 1x3x128 @256\npool, 1:2:1\n1x3x128 @128 40x4x1@128\nFC 1024x4096\nInput 40x21x1\nOutput\n# HMM states FC 4096x8786 FC 4096x4096\npool, 2:1:1\n2x1x512 @512 3x1x256 @512\npool, 2:1:1\n3x1x256 @256 3x1x128 @256\npool, 3:1:1\n3x1x128 @128 5x21x1@128\nFC 1024x4096\nInput 40x21x1\nOutput\n# HMM states FC 4096x8786 FC 4096x4096\npool, 1:2:1\n1x3x128 @128\n40x4x1@512 Input 40x21x1\npool, 1:1:4 (IMP)\nCNN 9L (!me) 9L-IMP(512,4) CNN 9L (frequency)\nFC 1024x1024\nFC 1024x3454 FC 1024x1024\nFC 1024x512\nFC 1024x2024 FC 512x1024\nFC layers for WSJ\nFC layers\nfor Aurora4\nFig. 2. Configurations of CNN architectures for SWBD. The ReLU nonlinearity function is used on the top of every activation. We made 6, 9, 12, and 15 layers by excluding or repeating the blue colored layers. For WSJ and Aurora4 datasets, the fully connected layers have smaller hidden neurons, and remainder are the same.\nare for the trained IMP CNN on the multi-conditioned training dataset.\nThe raw speech signal is processed via short-time Fourier transform (STFT) with a 25ms Hamming window and 10ms window shifts. We used 40-dimensional log-mel filter bank features without the energy coefficient, and concatenated frames with a context window size of 21 (\u00b110 frames) to feed them into networks as inputs. We trained the GMM-HMM system over fMLLR features. The forced alignment of each frame by the GMM-HMM baseline system is the target label of the neural networks.\nAfter random initialization of weights and biases from the Gaussian distribution N (0,0.01) and N (0,0.5) respectively, the CNNs were optimized by the stochastic gradient descent (SGD) method. In particular, for CNNs deeper than 9-layers, we faced with infeasible training, because each layer backpropagates errors by multiplying its small initial weights, resulting in vanishing gradients. Therefore, we increased the standard deviation (\u03c3) of the Gaussian distribution in lower layers. Each layer is trained with a momentum of 0.9, an L2-decay term of 0.0005, and mini-batch size of 512. After one epoch of training, the trained model is accepted if the validation cost decreases. Otherwise, the trained model is rejected and training starts again from the latest accepted model with a halved learning rate. The initial learning rate is 0.01, and the training stops after 50 epochs. Our implementation is developed upon the KALDI toolkit [28].\nFor the SWBD task, we decode speech using a trigram language model (LM) of 30k vocabularies which is trained on 3M words, and then we rescore the decoding results using 4- gram LM which is trained on Fisher English Part 1 transcripts [29]. For the WSJ and Aurora4 corpus, we used a 146K word extended dictionary and the trigram pruned language model which is exactly the same as the \u2018s5 \u2019recipe in the KALDI."}, {"heading": "B. Convolution axis and depth of CNNs", "text": "Fig. 3 shows the decoding results of CNNs on SWBD evaluation sets with various depths, from 6 layers up to 15 layers (configurations are described in Fig. 2). Deeper CNNs produced lower WERs, with the 15-layer CNN achieving a WER of 12.8% for SWB and 18.6% for total evaluation\n(a) SWB 6L 9L 12L 15L\nW E\nR (%\n)\n12.5\n13\n13.5\n14\n14.5\n15\n15.5 16 log-mel (time) log-mel (freq) fMLLR (time)\n(b) Total 6L 9L 12L 15L\n18\n18.5\n19\n19.5\n20\n20.5\n21\n21.5\nFig. 3. Decoding results of CNNs on the Switchboard evaluation sets (Hub5\u20192000). CNNs with different convolution axis (time, frequency) and input features (log-mel, fMLLR) are compared. (a) Switchboard subset (b) Total (SWB and CallHome).\nsets. Moreover, it is validated that convolution along the time axis always outperforms convolution along the frequency axis. Furthermore, CNNs trained over log-mel features had lower WER as fMLLR features when CNN has more than 9 layers. These results show that weight sharing along the time axis more effectively reduces the WER, and that increased nonlinearity obviates preprocessing for speaker adaptation.\nC. IMP CNNs\nDecoding results of IMP-CNNs with different numbers of maps and pooling sizes are compared in Table I. We further investigated an intermap pooling layer in which groups overlap (IMPO) each other with a stride of one. All CNNs with an intermap pooling layer performed better than the 9-layer CNN without. Especially, the \u20189L-IMP(512, 4)\u2019 CNN performs the best with a WER of 12.7% for SWB test set, showing a 3.78% relative improvement over the 9-layer CNN. Note Table II that when the IMP layer is applied to CNNs along the frequency axis or over fMLLR features, performance declines. In addition, IMP CNN performed well on the WSJ and Aurora4 corpus as shown in TableIII and IV, respectively. It\n4"}, {"heading": "9L 3.36 7.05 8.14 18.05 11.58", "text": "is remarkable that IMP layers contribute robustness to spectral variations in both clean and noisy conditions."}, {"heading": "D. Analysis on learnt filters", "text": "Learnt filters of the first convolution layer are visualized in Fig.4 (a). There are five categories of spectrotemporal features in the filters. (1) Harmonic features are narrow in the low frequency-region and (2) broad in the high frequency-region. (3) The on/off-set detecting filters are temporally selective, but are also sensitive to several frequencies. (4) The features of Gabor-like filters are centered on some frequency-bands, which presumably detect formants. (5) The features of formant changes are directional diagonal lines, spectrotemporal modulations, in the middle frequency-bands. Note that different features appear in different frequency-bands, and that local features of each type have different bandwidth sizes.\nThe trained filters in each group of the intermap pooling layer are presented in Fig.4 (b). Importantly, most filters in a group belong to a common category. For example, the filters in harmonic extractor and formant change detector groups have marginally shifted features on the frequency axis. This figure verifies that the intermap pooling lead the filters of a group to extract common but spectrally variant features, although there are no additional architectural constraints to guarantee this.\nThe consecutive trained filters of the IMPO layer are drawn in Fig. 4(c). The filters form a 1-dimensional topological map, where neighboring filters respond to similar spectrotemporal features. Along the topological map axis, filters appear discontinuously between feature categories, reflecting the fact that feature categories become definitely distinguishable to the system as it is trained. In recent neurophysiological studies, there is consensus that multiple tonotopic maps exist in the human auditory system [30]. However, few studies suggest that this topography includes other sound features, such as temporal, spectral, and joint modulations [31]\u2013[33]. The trained topography may provide a clue as to how human auditory neurons organize to efficiently process information in A1."}, {"heading": "E. Comparison of the IMP CNN", "text": "For comparison, we trained max-out networks that have 7 layers with 2,000 hidden neurons and 400 groups on both fMLLR and filter-bank features. The comparison of\nthe decoding results is summarized in Table V. The \u20189LIMP(512, 4)\u2019IMP CNN improved on the GMM-HMM baseline (19.5%) and the max-out network (14.6%), demonstrating a 34.87% and 13.01% relative improvement respectively. Also, it performs on par with a 15-layer CNN, i.e. a non-IMP CNN with six additional convolution layers. Finally, the IMP CNN is compared with the TDNN [16] and the CNNs which employed 2-dimensional convolutions [34], [35]. Note that we only compare other previous results without any sequence training such as sMBR. Even though our deep CNN did not use any speaker adaptation techniques, it yielded a comparative word error rate simply by employing intermap pooling and by increasing depths."}, {"heading": "VI. CONCLUSION", "text": "In this paper, the present experiments demonstrate that convolution along the time axis is more effective than along the frequency axis when processing speech. Depth in convolution layers is crucial for the sufficient representation of the complex temporal dynamics inherent in the acoustic features of speech. In order to achieve greater robustness to spectral variations in speech recognition, we proposed the addition of intermap pooling (IMP) to CNNs. Through visualization of the trained filters, we verified that filters grouped together learn similar spectrotemporal features and form a topological map. In the end, even without any speaker adaptation techniques, the proposed IMP CNN delivered competitive performance on the Switchboard, WSJ, and Aurora4 databases.\n5"}], "references": [{"title": "Acoustic Modeling using Deep Belief Networks", "author": ["A.-R. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "Audio, Speech, Lang. Process. IEEE Trans., vol. 20, no. 1, pp. 14\u201322, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Large vocabulary continuous speech recognition with context-dependent DBN-HMMs", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "INTER- SPEECH, pp. 4688\u20134691, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning of speech features for improved phonetic recognition", "author": ["J. Lee", "S.-Y. Lee"], "venue": "Acoust. Speech Signal Process. (ICASSP), 2011 IEEE Int. Conf. on., no. August, pp. 1249\u20131252, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A.-R. Mohamed", "N. Jaitly", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved feature processing for Deep Neural Networks", "author": ["S.P. Rath", "D. Povey", "K. Vesel", "J.H. Cernock"], "venue": "INTERSPEECH, pp. 1\u20135, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "2013 IEEE Work. Autom. Speech Recognit. Underst., pp. 55\u201359, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving DNN Speaker Independence With I-Vector Inputs", "author": ["A. Senior", "I. Lopez-Moreno"], "venue": "2014 IEEE Int. Conf. Acoust. Speech Signal Process., pp. 225\u2013229, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A.-R. Mohamed", "H. Jiang", "G. Penn"], "venue": "IEEE Int. Conf. Acoust. Speech Signal Process., 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu"], "venue": "INTERSPEECH, no. August, pp. 3366\u20133370, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional Neural Networks for Speech Recognition", "author": ["O. Abdel-Hamid", "A.-R. Mohamed", "H. Jiang", "L. Deng", "G. Penn", "D. Yu"], "venue": "Audio, Speech, Lang. Process. IEEE/ACM Trans., vol. 22, no. 10, pp. 1533\u2013 1545, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech Recognition With Deep Recurrent Neural Networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Acoust. Speech Signal Process. (ICASSP), 2013 IEEE Int. Conf. on. IEEE, no. 3, pp. 6645\u20136649, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Hybrid speech recognition with Deep Bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A.R. Mohamed"], "venue": "Autom. Speech Recognit. Underst. (ASRU), 2013 IEEE Work. on. IEEE, pp. 273\u2013278, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling Has", "author": ["F. Beaufays", "H. Sak", "A. Senior"], "venue": "INTERSPEECH, no. September, pp. 338\u2013342, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G.E. Hinton", "K. Shikano", "K.J. Lang"], "venue": "Acoust. Speech Signal Process. IEEE Trans., vol. 37, no. 3, pp. 328\u2013339, 1989.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks.", "author": ["H. Lee", "P. Pham", "Y. Largman", "A. Ng"], "venue": "Adv. Neural Inf. Process. Syst., pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "A time delay neural network architecture for efficient modeling of long temporal contexts", "author": ["V. Peddinti", "D. Povey", "S. Khudanpur"], "venue": "INTER- SPEECH, pp. 2\u20136, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Convolutional neural networks for LVCSPR", "author": ["T.N. Sainath", "A.-R. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "Acoust. Speech Signal Process. (ICASSP), 2013 IEEE Int. Conf. on., pp. 10\u201314, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Emergence of phase and shift invariant features by decomposition of natural images into independent feature subspaces", "author": ["A. Hyv\u00e4rinen", "H. Patrik"], "venue": "Neural Comput., vol. 12, no. 7, pp. 1705\u2014-1720, 2000.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Topographic independent component analysis", "author": ["A. Hyv\u00e4rinen", "P.O. Hoyer", "M. Inki"], "venue": "Neural Comput., vol. 13, no. 7, pp. 1527\u20131558, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning Invariant Features through Topographic Filter Maps", "author": ["K. Kavukcuoglu", "M. Ranzato", "R. Fergus", "Y. Le-Cun"], "venue": "Comput. Vis. Pattern Recognit., pp. 1605\u20131612, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning self-organized topology-preserving complex speech features at primary auditory cortex", "author": ["T. Kim", "S.-Y. Lee."], "venue": "Neurocomputing, vol. 65-66, pp. 793\u2013800, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "The topographic unsupervised learning of natural sounds in the auditory cortex", "author": ["H. Terashima", "M. Okada"], "venue": "Adv. Neural Inf. Process. Syst., pp. 1\u20139, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional maxout neural networks for low-resource speech recognition", "author": ["M. Cai", "Y. Shi", "J. Kang", "J. Liu", "T. Su"], "venue": "Proc. 9th Int. Symp. Chinese Spok. Lang. Process. ISC SLP 2014, pp. 133\u2013137, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout Networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv Prepr., pp. 1319\u20131327, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recoginition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv Prepr., pp. 1\u201314, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "SWITCHBOARD telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "Proc. 1992 IEEE Int. Conf. Acoust. Speech, Signal Process., vol. 1, pp. 517\u2013520, 1992.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1992}, {"title": "The Design for the Wall Street Journalbased CSR Corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proc. Work. Speech Nat. Languae, 1994.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1994}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "Autom. Speech Recognit. Underst. (ASRU), 2011 IEEE Work. on. IEEE, pp. 1\u20134, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "The Fisher corpus: a Resource for the Next Generations of Speech-to-Text", "author": ["C. Cieri", "D. Miller", "K. Walker"], "venue": "Proc. Lr., vol. 4, pp. 69\u201371, 2004.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Tonotopic mapping of human auditory cortex", "author": ["M. Saenz", "D.R. Langers"], "venue": "Hear. Res., vol. 307, pp. 42\u201352, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Orthogonal acoustic dimensions de fi ne auditory fi eld maps in human cortex", "author": ["B. Barton", "J.H. Venezia", "K. Saberi", "G. Hickok", "A.A. Brewer"], "venue": "Proc. Natl. Acad. Sci., 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Spatial representations of temporal and spectral sound cues in human auditory cortex", "author": ["M. Herdener", "F. Esposito", "K. Scheffler", "P. Schneider", "N.K. Logothetis", "K. Uludag", "C. Kayser"], "venue": "Cortex, vol. 49, no. 10, pp. 2822\u20132833, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Encoding of Natural Sounds at Multiple Spectral and Temporal Resolutions in the Human Auditory Cortex", "author": ["R. Santoro", "M. Moerel", "F. De Martino", "R. Goebel", "K. Ugurbil", "E. Yacoub", "E. Formisano"], "venue": "PLoS Comput. Biol., vol. 10, no. 1, p. e1003412, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "The IBM 2015 English Conversational Telephone Speech Recognition System", "author": ["G. Saon", "H.-K.J. Kuo", "S. Rennie", "M. Picheny"], "venue": "Interspeech 2015, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep multilingual convolutional neural networks for LVCSR", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "pp. 2\u20136, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "ACOUSTIC modeling with deep learning has demonstrated remarkable performance improvements in automatic speech recognition [1]\u2013[4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "ACOUSTIC modeling with deep learning has demonstrated remarkable performance improvements in automatic speech recognition [1]\u2013[4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "Amongst feature-level approaches, speaker-adapted methods such as fMLLR [5] have been proposed.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Acoustic features concatenated with i-vectors, which represent speaker information, also have been employed as input for DNNs [6], [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "Acoustic features concatenated with i-vectors, which represent speaker information, also have been employed as input for DNNs [6], [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "Model-level approaches have employed hybrid NN-HMM systems with convolutional neural networks (CNNs) [8]\u2013[10] and recurrent neural networks (RNNs) [11]\u2013[13].", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "Model-level approaches have employed hybrid NN-HMM systems with convolutional neural networks (CNNs) [8]\u2013[10] and recurrent neural networks (RNNs) [11]\u2013[13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Model-level approaches have employed hybrid NN-HMM systems with convolutional neural networks (CNNs) [8]\u2013[10] and recurrent neural networks (RNNs) [11]\u2013[13].", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "Model-level approaches have employed hybrid NN-HMM systems with convolutional neural networks (CNNs) [8]\u2013[10] and recurrent neural networks (RNNs) [11]\u2013[13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "Previous researchers introduced time-delayed neural networks (TDNNs), which are CNNs with convolutions along the time axis to learn the temporal dynamics of features [14]\u2013[16].", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "Previous researchers introduced time-delayed neural networks (TDNNs), which are CNNs with convolutions along the time axis to learn the temporal dynamics of features [14]\u2013[16].", "startOffset": 171, "endOffset": 175}, {"referenceID": 7, "context": "Other researchers have applied convolutions along the frequency axis to attain invariance to frequency-shifts [8], [17].", "startOffset": 110, "endOffset": 113}, {"referenceID": 16, "context": "Other researchers have applied convolutions along the frequency axis to attain invariance to frequency-shifts [8], [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "The limited weight sharing method in which weights are convolved only within a subsection of frequencybands has been employed in efforts to overcome this problem [10].", "startOffset": 162, "endOffset": 166}, {"referenceID": 17, "context": "For unsupervised feature extraction, previous researchers imposed sparsity terms over small groups or neighborhoods in feature maps of image [18]\u2013 [20] and speech data [21], [22].", "startOffset": 141, "endOffset": 145}, {"referenceID": 19, "context": "For unsupervised feature extraction, previous researchers imposed sparsity terms over small groups or neighborhoods in feature maps of image [18]\u2013 [20] and speech data [21], [22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 20, "context": "For unsupervised feature extraction, previous researchers imposed sparsity terms over small groups or neighborhoods in feature maps of image [18]\u2013 [20] and speech data [21], [22].", "startOffset": 168, "endOffset": 172}, {"referenceID": 21, "context": "For unsupervised feature extraction, previous researchers imposed sparsity terms over small groups or neighborhoods in feature maps of image [18]\u2013 [20] and speech data [21], [22].", "startOffset": 174, "endOffset": 178}, {"referenceID": 22, "context": "Previously, a convolutional maxout network has been proposed [23], however, it applied convolutions along the frequency axis.", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "Like the maxout networks [24], this layer groups the filters, and pools the feature maps inside a group.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "Furthermore, motivated by the performance of very deep CNNs [25], we inserted convolution layers with small filters (of size 1x3) between two intramap pooling layers.", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "We conducted experiments using the 300 hour SwitchboardI Release 2 (SWBD) dataset [26] which is conversational telephone speech task as well as the Wall Street Journal (WSJ) corpus [27] and Aurora4 database which are read speech.", "startOffset": 82, "endOffset": 86}, {"referenceID": 26, "context": "We conducted experiments using the 300 hour SwitchboardI Release 2 (SWBD) dataset [26] which is conversational telephone speech task as well as the Wall Street Journal (WSJ) corpus [27] and Aurora4 database which are read speech.", "startOffset": 181, "endOffset": 185}, {"referenceID": 27, "context": "Our implementation is developed upon the KALDI toolkit [28].", "startOffset": 55, "endOffset": 59}, {"referenceID": 28, "context": "For the SWBD task, we decode speech using a trigram language model (LM) of 30k vocabularies which is trained on 3M words, and then we rescore the decoding results using 4gram LM which is trained on Fisher English Part 1 transcripts [29].", "startOffset": 232, "endOffset": 236}, {"referenceID": 29, "context": "In recent neurophysiological studies, there is consensus that multiple tonotopic maps exist in the human auditory system [30].", "startOffset": 121, "endOffset": 125}, {"referenceID": 30, "context": "However, few studies suggest that this topography includes other sound features, such as temporal, spectral, and joint modulations [31]\u2013[33].", "startOffset": 131, "endOffset": 135}, {"referenceID": 32, "context": "However, few studies suggest that this topography includes other sound features, such as temporal, spectral, and joint modulations [31]\u2013[33].", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "MFCC + i-vectors TDNN 4L [16] 12.", "startOffset": 25, "endOffset": 29}, {"referenceID": 33, "context": "VTL-warped log-mel CNN 8L (2conv+6fc) [34] 12.", "startOffset": 38, "endOffset": 42}, {"referenceID": 34, "context": "6 CNN 13L (10conv+3fc) [35] 11.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "Finally, the IMP CNN is compared with the TDNN [16] and the CNNs which employed 2-dimensional convolutions [34], [35].", "startOffset": 47, "endOffset": 51}, {"referenceID": 33, "context": "Finally, the IMP CNN is compared with the TDNN [16] and the CNNs which employed 2-dimensional convolutions [34], [35].", "startOffset": 107, "endOffset": 111}, {"referenceID": 34, "context": "Finally, the IMP CNN is compared with the TDNN [16] and the CNNs which employed 2-dimensional convolutions [34], [35].", "startOffset": 113, "endOffset": 117}], "year": 2016, "abstractText": "Convolutional neural networks (CNNs) with convolutional and pooling operations along the frequency axis have been proposed to attain invariance to frequency shifts of features. However, this is inappropriate with regard to the fact that acoustic features vary in frequency. In this paper, we contend that convolution along the time axis is more effective. We also propose the addition of an intermap pooling (IMP) layer to deep CNNs. In this layer, filters in each group extract common but spectrally variant features, then the layer pools the feature maps of each group. As a result, the proposed IMP CNN can achieve insensitivity to spectral variations characteristic of different speakers and utterances. The effectiveness of the IMP CNN architecture is demonstrated on several LVCSR tasks. Even without speaker adaptation techniques, the architecture achieved a WER of 12.7% on the SWB part of the Hub5\u20192000 evaluation test set, which is competitive with other state-of-the-art methods.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}