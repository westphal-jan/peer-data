{"id": "1606.04422", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge", "abstract": "2640 We boyuk propose inauthenticity real logic: a uniform pro-rector framework for three-wheel integrating automatic macumba learning and 1,792 reasoning. Real stfu logic is resonances defined \u00f6stg\u00f6ta on a full wijnbergen first - order language ukrain where formulas stillingfleet have hawn truth - fairburn value in the interval [x-seven 0, zak\u0142ady 1] laying and 2,725 semantics 1389 defined concretely best-studied on the donlevy domain donington of overreliance real rainstorms numbers. rocklike Logical constants are interpreted as (154.3 feature) mustela vectors sixty-year of katu real 124.38 numbers. mansiones Real christie logic da\u00df promotes noordwijk a 6-0-1 well - may-treanor founded integration schisano of chastising deductive reasoning on piako knowledge - re-equipping bases with efficient, webster-ashburton data - driven cumberland relational 1,384 machine learning. government-wide We show ponzio how castroville Real Logic agc can be implemented in deep dedica Tensor rmp Neural Networks osid with the 28.28 use chaubey of Google ' s TensorFlow primitives. 50-member The mohale paper concludes with brickworks experiments sunburst on higa a agriculture-based simple but moke representative dorinda example dolore of abani knowledge completion.", "histories": [["v1", "Tue, 14 Jun 2016 15:25:28 GMT  (29kb)", "http://arxiv.org/abs/1606.04422v1", "12 pages, 2 figs, 1 table, 27 references"], ["v2", "Thu, 7 Jul 2016 12:28:57 GMT  (30kb)", "http://arxiv.org/abs/1606.04422v2", "12 pages, 2 figs, 1 table, 27 references"]], "COMMENTS": "12 pages, 2 figs, 1 table, 27 references", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO cs.NE", "authors": ["luciano serafini", "artur d'avila garcez"], "accepted": false, "id": "1606.04422"}, "pdf": {"name": "1606.04422.pdf", "metadata": {"source": "CRF", "title": "Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge", "authors": ["Luciano Serafini", "Artur d\u2019Avila Garcez"], "emails": ["serafini@fbk.eu", "a.garcez@city.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n04 42\n2v 1\n[ cs\n.A I]\n1 4\nJu n\nKeywords: Knowledge Representation, Relational Learning, Tensor Networks, NeuralSymbolic Computation, Data-driven Knowledge Completion."}, {"heading": "1 Introduction", "text": "The recent availability of large-scale data combining multiple data modalities, such as image, text, audio and sensor data, has opened up various research and commercial opportunities, underpinned by machine learning methods and techniques [5, 12, 17, 18]. In particular, recent work in machine learning has sought to combine logical services, such as knowledge completion, approximate inference, and goal-directed reasoning with data-driven statistical and neural network-based approaches. We argue that there are great possibilities for improving the current state of the art in machine learning and artificial intelligence (AI) thought the principled combination of knowledge representation, reasoning and learning. Guha\u2019s recent position paper [15] is a case in point, as it advocates a new model theory for real-valued numbers. In this paper, we take inspiration from such recent work in AI, but also less recent work in the area of neuralsymbolic integration [8, 10, 11] and in semantic attachment and symbol grounding [4] to achieve a vector-based representation which can be shown adequate for integrating machine learning and reasoning in a principled way.\n\u22c6 The first author acknowledges the Mobility Program of FBK, for supporting a long term visit at City University London. He also acknowledges NVIDIA Corporation for supporting this research with the donation of a GPU.\nThis paper proposes a framework called logic tensor networks (LTN) which integrates learning based on tensor networks [25] with reasoning using first-order manyvalued logic [6], all implemented in TENSORFLOWTM [13]. This enables, for the first time, a range of knowledge-based tasks using rich (full first-order logic (FOL)) knowledge representation to be combined with data-driven, efficient machine learning based on the manipulation of real-valued vectors. Given data available in the form of realvalued vectors, logical soft and hard constraints and relations which apply to certain subsets of the vectors can be compactly specified in first-order logic. Reasoning about such constraints can help improve learning, and learning from new data can revise such constraints thus modifying reasoning. An adequate vector-based representation of the logic, first proposed in this paper, enables the above integration of learning and inference, as detailed in what follows.\nWe are interested in providing a computationally adequate approach to implementing learning and reasoning in an integrated way within an idealized agent. This agent has to manage knowledge about an unbounded, possibly infinite, set of objects O = {o1, o2, . . . }. Some of the objects are associated with a set of quantitative attributes, represented by an n-tuple of real values G(oi) \u2208 Rn, which we call grounding. For example, a person may have a grounding into a 4-tuple containing some numerical representation of the person\u2019s name, her height, weight, and number of friends in some social network. Object tuples can participate in a set of relations R = {R1, . . . , Rk}, with Ri \u2286 O\u03b1(Ri), where \u03b1(Ri) denotes the arity of relation Ri. We presuppose the existence of a latent (unknown) relation between the above numerical properties, i.e. groundings, and partial relational structure R on O. Starting from this partial knowledge, an agent is required to: (i) infer new knowledge about the relational structure on the objects of O; (ii) predict the numerical properties or the class of the objects in O.\nClasses and relations are not normally independent. For example, it may be the case that if an object x is of class C, C(x), and it is related to another object y through relation R(x, y) then this other object y should be in the same class C(y). In logic: \u2200x\u2203y((C(x) \u2227 R(x, y)) \u2192 C(y)). Whether or not C(y) holds will depend on the application: through reasoning, one may derive C(y) where otherwise there might not have been evidence of C(y) from training examples only; through learning, one may need to revise such a conclusion once training examples to the contrary become available. The vectorial representation proposed in this paper permits both reasoning and learning as exemplified above and detailed in the next section.\nThe above forms of reasoning and learning are integrated in a unifying framework, implemented within tensor networks, and exemplified in relational domains combining data and relational knowledge about the objects. It is expected that, through an adequate integration of numerical properties and relational knowledge, differently from the immediate related literature [9, 2, 1], the framework introduced in this paper will be capable of combining in an effective way full first-order logical inference on open domains with efficient relational multi-class learning using tensor networks.\nThe main contribution of this paper is two-fold. It introduces a novel framework for the integration of learning and reasoning which can take advantage of the representational power of full (multi-valued) first-order logic, and it instantiates the framework using tensor networks into an efficient implementation which shows that the proposed\nvector-based representation of the logic offers an adequate mapping between symbols and their real-world manifestations, which is appropriate for both rich inference and learning from examples.\nThe paper is organized as follows. In Section 2, we define Real Logic. In Section 3, we propose the Learning-as-Inference framework. In Section 4, we instantiate the framework by showing how Real Logic can be implemented in Tensor Neural Networks leading to Logic Tensor Networks (LTN). Section 5 contains an example of how LTN handles knowledge completion using (possibly inconsistent) data and knowledge from the well-known smokers and friends experiment. Section 6 discusses related work, and Section 7 concludes the paper and discusses directions for future work."}, {"heading": "2 Real Logic", "text": "We start from a first order languageL, whose signature contains a set C of constant symbols, a set F of functional symbols, and a set P of predicate symbols. The sentences of L are used to express relational knowledge, e.g. the atomic formulaR(o1, o2) states that objects o1 and o2 are related to each other through binary relation R; \u2200xy.(R(x, y) \u2192 R(y, x)) states that R is a reflexive relation, where x and y are variables; \u2203y.R(o1, y) states that there is an (unknown) object which is related to object o1 throughR. For simplicity, we assume that all sentences of L are in prenex conjunctive, skolemised normal form [16], e.g. a sentence \u2200x(A(x) \u2192 \u2203yR(x, y)) is transformed into an equivalent clause \u00acA(x) \u2228R(x, f(x)), where f is a new function symbol.\nAs for the semantics of L, we deviate from the standard abstract semantics of FOL, and we propose a concrete semantics with sentences interpreted as tuples of real numbers. To emphasise the fact that L is interpreted in a \u201creal\u201d world we use the term (semantic) grounding, denoted by G, instead of the more standard interpretation1.\n\u2013 G associates an n-tuple of real numbers G(t) to any closed term t of L; intuitively G(t) is the set of numeric features of the object denoted by t. \u2013 G associates a real number in the interval [0, 1] to each clause \u03c6 of L. Intuitively, G(\u03c6) represents one\u2019s confidence in the truth of \u03c6; the higher the value, the higher the confidence.\nA grounding is specified only for the elements of the signature of L. The grounding of terms and clauses is defined inductively, as follows.\nDefinition 1. A grounding G for a first order language L is a function from the signature of L to the real numbers that satisfies the following conditions:\n1. G(c) \u2208 Rn for every constant symbol c \u2208 C; 2. G(f) \u2208 Rn\u00b7\u03b1(f) \u2212\u2192 Rn for every f \u2208 F ; 3. G(P ) \u2208 Rn\u00b7\u03b1(R) \u2212\u2192 [0, 1] for every P \u2208 P;\n1 In logic, the term \u201cgrounding\u201d indicates the operation of replacing the variables of a term/formula with constants, or terms that do not contains other variables. To avoid confusion, we use the synonym \u201cinstantiation\u201d for this sense.\nA grounding G is inductively extended to all the closed terms and clauses, as follows:\nG(f(t1, . . . , tm)) = G(f)(G(t1), . . . ,G(tm))\nG(P (t1, . . . , tm)) = G(P )(G(t1), . . . ,G(tm))\nG(\u00acP (t1, . . . , tm)) = 1\u2212 G(P (t1, . . . , tm))\nG(\u03c61 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u03c6k) = \u00b5(G(\u03c61), . . . ,G(\u03c6k))\nwhere \u00b5 is an s-norm operator (the co-norm operator associated some t-norm operator).2\nExample 1. Suppose that O = {o1, o2, o3} is a set of documents defined on a finite dictionary D = {w1, ..., wn} of n words. Let L be the language that contains the binary function symbol concat(x, y) denoting the document resulting from the concatenation of documents x with y. Let L contain also the binary predicate Sim which is supposed to be true if document x is deemed to be similar to document y. An example of grounding is the one that associates to each document its bag-of-words vector [7]. As a consequence, a natural grounding of the concat function would be the sum of the vectors, and of the Sim predicate, the cosine similarity between the vectors. More formally:\n\u2013 G(oi) = \u3008noiw1 , . . . , n oi wn \u3009, where ndw is the number of occurrences of word w in document d; \u2013 if v,u \u2208 Rn, G(concat)(u,v) = u+ v; \u2013 if v,u \u2208 Rn, G(Sim)(u,v) = u\u00b7v||u||||v|| .\nFor instance, if the three documents are o1 = \u201cJohn studies logic and plays football\u201d, o2 = \u201cMary plays football and logic games\u201d, o3 = \u201cJohn and Mary play football and study logic together\u201d, and W={John, Mary, and, football, game, logic, play, study, together} then the following are examples of the grounding of terms, atomic forulas and clauses.\nG(o1) = \u30081, 0, 1, 1, 0, 1, 1, 1, 0\u3009\nG(o2) = \u30080, 1, 1, 1, 1, 1, 1, 0, 0\u3009\nG(o3) = \u30081, 1, 2, 1, 0, 1, 1, 1, 1\u3009\nG(concat(o1, o2)) = G(o1) + G(o2) = \u30081, 1, 2, 2, 1, 2, 2, 1, 0\u3009\nG(Sim(concat(o1, o2), o3) = G(concat(o1, o2)) \u00b7 G(o3)\n||G(concat(o1, o2))|| \u00b7 ||G(o3)|| \u2248\n13\n14.83 \u2248 0.88\nG(Sim(o1, o3) \u2228 Sim(o2, o3)) = \u00b5max(G(Sim(o1, o3),G(Sim(o2, o3)) \u2248 0.86"}, {"heading": "3 Learning as approximate satisfiability", "text": "We start by defining ground theory and their satisfiability.\n2 Examples of t-nors are Lukasiewicz, product, and Go\u0308del. Lukasiewicz s-norm is defined as \u00b5Luk(x, y) = min(x+ y, 1); Product s-norm is defined as \u00b5Pr(x, y) = x+ y\u2212 x \u00b7 y; Go\u0308del s-norm is defined as \u00b5max(x, y) = max(x, y)\nDefinition 2 (Satisfiability). Let \u03c6 be a closed clause in L, G a grounding, and v \u2264 w \u2208 [0, 1]. We say that G satisfies \u03c6 in the confidence interval [v, w], written G |=wv \u03c6, if v \u2264 G(\u03c6) \u2264 w.\nA partial grounding, denoted by G\u0302, is a grounding that is defined on a subset of the signature of L. A grounded theory is a set of clauses in the language of L and partial grounding G\u0302.\nDefinition 3 (Grounded Theory). A grounded theory (GT) is a pair \u3008K, G\u0302\u3009 where K is a set of pairs \u3008[v, w], \u03c6(x)\u3009, where \u03c6(x) is a clause of L containing the set x of free variables, and [v, w] \u2286 [0, 1] is an interval contained in [0, 1], and G\u0302 is a partial grounding.\nDefinition 4 (Satisfiability of a Grounded Theory). A GT \u3008K, G\u0302\u3009 is satisfiabile if there exists a grounding G, which extends G\u0302 such that for all \u3008[v, w], \u03c6(x)\u3009 \u2208 K and any tuple t of closed terms, G |=wv \u03c6(t).\nFrom the previous definiiton it follows that checking if a GT \u3008K, G\u0302\u3009 is satisfiable amounts to seaching for an extension of the partial grounding G\u0302 in the space of all possible groundings, such that all the instantiations of the clauses in K are satisfied w.r.t. the specified interval. Clearly this is unfeasible from a practical point of view. As is usual, we must restrict both the space of grounding and clause instantiations. Let us consider each in turn: To check satisfiability on a subset of all the functions on real numbers, recall that a grounding should capture a latent correlation between the quantitative attributes of an object and its relational properties3. In particular, we are interested in searching within a specific class of functions, in this paper based on tensor networks, although other family of functions can be considered. To limit the number of clause instantiations, which in general might be infinite since L admits function symbols, the usual approach is to consider the instantiations of each clause up to a certain depth [3].\nWhen a grounded theory \u3008K, G\u0302\u3009 is inconsitent, that is, there is no grounding G that satisfies it, we are interested in finding a grounding which satisfies as much as possible of \u3008K, G\u0302\u3009. For any \u3008[v, w], \u03c6\u3009 \u2208 K we want to find a grounding G that minimizes the satisfiability error. An error occurs when a groundingG assigns a value G(\u03c6) to a clause \u03c6 which is outside the interval [v, w] prescribed by K. The measure of this error can be defined as the minimal distance between the points in the interval [v, w] and G(\u03c6):\nLoss(G, \u3008[v, w], \u03c6\u3009) = |x\u2212 G(\u03c6)|, v \u2264 x \u2264 w (1)\nNotice that if G(\u03c6) \u2208 [v, w], Loss(G, \u03c6) = 0. The above gives rise to the following definition of approximate satisfiability w.r.t. a family G of grounding functions on the language L.\n3 For example, whether a document is classified as from the field of Artificial Intelligence (AI) depends on its bag-of-words grounding. If the language L contains the unary predicate AI(x) standing for x is a paper about AI then the grounding of AI(x), which is a function from bag-of-words vectors to [0,1], should assign values close to 1 to the vectors which are close semantically to AI . Furthermore, if two vectors are similar (e.g. according to the cosine similarity measure) then their grounding should be similar.\nDefinition 5 (Approximate satisfiability). Let \u3008K, G\u0302\u3009 be a grounded theory and K0 a finite subset of the instantiations of the clauses in K, i.e.\nK0 \u2286 {\u3008[v, w], \u03c6(t)\u3009} | \u3008[v, w], \u03c6(x)\u3009 \u2208 K and t is any n-tuple of closed terms.}\nLet G be a family of grounding functions. We define the best satisfiability problem as the problem of finding an extensions G\u2217 of G\u0302 in G that minimizes the satisfiability error on the set K0, that is:\nG\u2217 = argmin G\u0302\u2286G\u2208G\n\u2211\n\u3008[v,w],\u03c6(t)\u3009\u2208K0\nLoss(G, \u3008[v, w], \u03c6(t)\u3009)"}, {"heading": "4 Implementing Real Logic in Tensor Networks", "text": "Specific instances of Real Logic can be obtained by selectiong the space G of groundings and the specific s-norm for the interpretation of disjunction. In this section, we describe a realization of real logic where G is the space of real tensor transformations of order k (where k is a parameter). In this space, function symbols are interpreted as linear transformations. More precisely, if f is a function symbol of arity m and v1, . . . ,vm \u2208 R\nn are real vectors corresponding to the grounding of m terms then G(f)(v1, . . . ,vm) can be written as:\nG(f)(v1, . . . ,vm) = Mfv +Nf\nfor some n\u00d7mn matrix Mf and n-vector Nf , where v = \u3008v1, . . . ,vn\u3009. The grounding of m-ary predicate P , G(P ), is defined as a generalization of the neural tensor network [25] (which has been shown effective at performing the task of knowledge completion in the presence of simple logical constraints), as a function from Rmn to [0, 1], as follows:\nG(P ) = \u03c3 ( uTP tanh ( v TW [1:k] P v + VPv +BP ))\n(2)\nwhere W [1:k]P is a 3-D tensor in R mn\u00d7mn\u00d7k, VP is a matrix in Rk\u00d7mn, and BP is a vector in Rk, and \u03c3 is the sigmoid function. With this encoding, the grounding (i.e. truth-value) of a clause can be determined by a neural network which first computes the grounding of the literals contained in the clause, and then combines them using the specific s-norm. An example of a tensor network for the clause \u2200x, y(P (x, y) \u2192 A(y)), equivalent to \u00acP (x, y)\u2228A(y), is shown in Figure 1. In this tensor network architecture, W\u2217, V\u2217, B\u2217 and u\u2217, with \u2217 \u2208 {P,A}, are parameters to be learned by minimizing the loss function or, equivalently, maximizing the satisfiability of \u2200x, y(P (x, y) \u2192 A(y))."}, {"heading": "5 An Example of Knowledge Completion", "text": "Logic Tensor Networks have been implemented as a Python library called ltn using Google\u2019s TENSORFLOWTM . To test our idea, in this section we use the well-known\nfriends and smokers4 example [24] to illustrate the task of knowledge completion in ltn. There are 14 people divided into two groups {a, b, . . . , h} and {i, j, . . . , n}. Within each group of people we have complete knowledge of their smoking habits. In the first group, we have complete knowledge of who has and does not have cancer. In the second group, this is not known for any of the persons. Knowledge about the friendship relation is complete within each group only if symmetry of friendship is assumed. Otherwise, it is imcomplete in that it may be known that, e.g., a is a friend of b, but not known whether b is a friend of a. Finally, there is also general knowledge about smoking, friendship and cancer, namely, that smoking causes cancer, friendship is normally a symmetric and anti-reflexive relation, everyone has a friend, and that smoking propagates (either actively or passively) among friends. All this knowledge can be represented by the knowledge-bases shown in Figure 2.\nThe facts contained in the knowledge-bases should have different degrees of truth, and this is not known. Otherwise, the combined knowledge-base would be inconsistent (it would deduce e.g. S(b) and \u00acS(b)). Our main task is to complete the knowledgebase (KB), that is: (i) find the degree of truth of the facts contained in KB, (ii) find a truth-value for all the missing facts, e.g. C(i), (iii) find the grounding of each constant symbol a, ..., n.5 To answer (i)-(iii), we use ltn to find a grounding that best approximates the complete KB. We start by assuming that all the facts contained in the knowledge-base are true (i.e. have degree of truth 1.0). To show the role of background knolwedge in the learning-inference process, we run two experiments. In the\n4 Normally, a probabilistic approach is taken to solve this problem, and one that requires instantiating all clauses to remove variables, essentially turning the problem into a propositional one; ltn takes a different approach. 5 Notice how no grounding is provided about the signature of the knowledge-base.\nfirst (exp1), we seek to complete KB consisting of only factual knowledge: Kexp1 = KSFCa...h \u222aK SF i...n. In the second (exp1), we include background knowledge, that is: Kexp2 = Kexp1 \u222a K SFC .\nWe confgure the network as follows: each constant (person, in this case) 30 real features. the number of layers k in the tensor network equal to 10, and the regularization parameter6 \u03bb = 1\u221210. We choose the Lukasiewicz t-norm7, and use the harmonic mean as aggregation operator. An estimation of the optimal grounding is obtained by 5,000 runs the RMSProp optimisation algorithm [26] available in TENSORFLOWTM .\nThe results of the two experiments are reported in Table 1. For readability, we use boldface for truth-values greater than 0.5. The truth-values of the facts listed in a knowledge-base are highlighted with the same background color of the knowledgebase in Figure 2. The values with white background are the result of the knowledge completion produced by the LTN learning-inference procedure. To evaluate the quality of the results, one has to check whether (i) the truth-values of the facts listed in a KB are indeed close to 1.0, and (ii) the truth-values associated with knowledge completion correspond to expectation. An initial analysis shows that the LTN associated with Kexp1 produces the same facts as Kexp1 itself. In other words, the LTN fits the data. However, the LTN also learns to infer additional positive and negative facts about F and C not derivable from Kexp1 by pure logical reasoning; for example: F (c, b), F (g, b) and \u00acF (b, a). These facts are derived by exploiting similarities between the groundings of the constants generated by the LTN. For instance, G(c) and G(g) happen to present a high cosine similarity measure. As a result, facts about the friendship relations of c affect the friendship relations of g and vice-versa, for instance F (c, b) and F (g, b). The\n6 A smoth factor \u03bb||\u2126||22 is added to the loss to limit the size of parameters. 7 \u00b5(a, b) = min(1, a+ b)\nThe results of the second experiment show that more facts can be learned with the inclusion of background knowledge. For example, the LTN now predicts that C(i) and C(n) are true. Similarly, from the symmetry of the friendship relation, the LTN concludes that m is a friend of i, as expected. In fact, all the axioms in the generic background knowledge KSFC are satisfied with a degree of satisfiability higher than 90%, apart from the smoking causes cancer axiom - which is responsible for the classical inconsistency since in the data f and g smoke and do not have cancer -, which has a degree of satisfiability of 77%."}, {"heading": "6 Related work", "text": "In his recent note, [15], Guha advocates the need for a new model theory for distributed representations (such as those based on embeddings). The note sketches a proposal, where terms and (binary) predicates are all interpreted as points/vectors in an n-dimensional real space. The computation of the truth-value of the atomic formulae P (t1, . . . , tn) is obtained by comparing the projections of the vector associated to each ti with that associated to Pi. Real logic shares with [15] the idea that terms must be interpreted in a geometric space. It has, however, a different (and more general) interpretation of functions and predicate symbols. Real logic is more general because\nthe semantics proposed in [15] can be implemented within an ltn with a single layer (k = 1), since the operation of projection and comparison necessary to compute the truth-value of P (t1, . . . , tm) can be encoded within an nm \u00d7 nm matrix W with the constraint that \u3008G(t1), . . . ,G(tn)\u3009 T W \u3008G(t1), . . . ,G(tn)\u3009 \u2264 \u03b4, which can be encoded easily in ltn. Real logic is orthogonal to the approach taken by (Hybrid) Markov Logic Networks (MLNs) and its variations [24, 27, 22]. In MLNs, the level of truth of a formula is determined by the number of models that satisfy the formula: the more models, the higher the degree of truth. Hybrid MLNs introduce a dependency from the real features associated to constants, which is given, and not learned. In real logic, instead, the level of truth of a complex formula is determined by (fuzzy) logical reasoning, and the relations between the features of different objects is learned through error minimization. Another difference is that MLNs work under the closed world assumption, while Real Logic is open domain. Much work has been done also on neuro-fuzzy approaches [19]. These are essentially propositional while real logic is first-order.\nBayesian logic (BLOG) [20] is open domain, and in this respect is similar to real logic and LTNs. But, instead of taking an explicit probabilistic approach, LTNs draw from the efficient approach used by tensor networks for knowledge graphs, as already discussed. LTNs can have a probabilistic interpretation but this is not a requirement. Other statistical AI and probabilistic approaches such as lifted inference fall in this same category, including probabilistic variations of inductive logic programming (ILP) [23], which are normally restricted to Horn clauses. Metainterpretive ILP [21], together with BLOG, seems closer to LTNs in what concerns the knowledge representation language, but without exploring tensor networks and its benefits in terms of computational efficiency.\nFinally, related work in the domain of neural-symbolic computing and neural network fibring [10] has sought to combine neural networks with ILP to gain efficiency [14] and other forms of knowledge representation, such as propositional modal logic and logic programming. The above are tightly-coupled neural-symbolic approaches. In contrast, LTNs use a richer FOL language, exploit the benefits of knowledge compilation and tensor networks, and might even offer an adequate representation of equality in logic, not considered before in the context of neural-symbolic integration. Experimental evaluations and comparison with other neural-symbolic approaches are desirable, including the latest developments in the field, a good snapshot of which can be found in [1]."}, {"heading": "7 Conclusion and future work", "text": "We have proposed real logic: a uniform framework for learning and reasoning. Approximate satisfiability is defined as a learning task with both knowledge and data being mapped onto real-valued vectors. With an inference-as-learning approach, relational knowledge constraints and state-of-the-art data-driven approaches can be integrated. We showed how real logic can be implemented in deep tensor networks, which we call Logic Tensor Networks (LTNs), and applied efficiently to knowledge completion and data prediction tasks. As future work, we will make the implementation of LTN, written\nin TENSORFLOWTM , available to the community so that LTN can be applied to largescale experiments and relational learning benchmarks, and so that comparisons can be drawn with systems from statistical relational learning, neural-symbolic computing, and (probabilistic) inductive logic programming."}], "references": [{"title": "Random satisfiability. In Handbook of Satisfiability, pages 245\u2013270", "author": ["Dimitris Achlioptas"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "A (somewhat) new solution to the variable binding problem", "author": ["Leon Barrett", "Jerome Feldman", "Liam MacDermed"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "An Introduction to Many-Valued and Fuzzy Logic: Semantics, Algebras, and Derivation Systems", "author": ["M. Bergmann"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "From machine learning to machine reasoning", "author": ["L\u00e9on Bottou"], "venue": "Technical report,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Neural-symbolic learning and reasoning (dagstuhl seminar 14381)", "author": ["Artur S. d\u2019Avila Garcez", "Marco Gori", "Pascal Hitzler", "Lu\u0131\u0301s C. Lamb"], "venue": "Dagstuhl Reports,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Neural-Symbolic Cognitive Reasoning", "author": ["Artur S. d\u2019Avila Garcez", "Lu\u0131\u0301s C. Lamb", "Dov M. Gabbay"], "venue": "Cognitive Technologies. Springer,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Bridging logic and kernel machines", "author": ["Michelangelo Diligenti", "Marco Gori", "Marco Maggini", "Leonardo Rigutini"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver"], "venue": "search. Nature,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "TensorFlow: Large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi"], "venue": "on heterogeneous systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Fast relational learning using bottom clause propositionalization with artificial neural networks", "author": ["Manoel V.M. Fran\u00e7a", "Gerson Zaverucha", "Artur S. d\u2019Avila Garcez"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Towards a model theory for distributed representations", "author": ["Ramanathan Guha"], "venue": "In 2015 AAAI Spring Symposium Series,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Logic in Computer Science: Modelling and Reasoning About Systems", "author": ["Michael Huth", "Mark Ryan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "The vision of autonomic", "author": ["Jeffrey O. Kephart", "David M. Chess"], "venue": "computing. Computer,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["Douwe Kiela", "L\u00e9on Bottou"], "venue": "In Proceedings of EMNLP", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Neural Networks and Fuzzy Systems: A Dynamical Systems Approach to Machine Intelligence", "author": ["Bart Kosko"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "BLOG: probabilistic models with unknown objects", "author": ["Brian Milch", "Bhaskara Marthi", "Stuart J. Russell", "David Sontag", "Daniel L. Ong", "Andrey Kolobov"], "venue": "Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Meta-interpretive learning of higher-order dyadic datalog: predicate invention revisited", "author": ["Stephen H. Muggleton", "Dianhuan Lin", "Alireza Tamaddoni-Nezhad"], "venue": "Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Learning relational sum-product networks", "author": ["Aniruddh Nath", "Pedro M. Domingos"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Statistical Relational Artificial Intelligence: Logic, Probability, and Computation. Synthesis Lectures on Artificial Intelligence and Machine Learning", "author": ["Luc De Raedt", "Kristian Kersting", "Sriraam Natarajan", "David Poole"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Reasoning With Neural Tensor Networks For Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Lecture 6.5 - RMSProp, COURSERA: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": "Technical report,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Hybrid markov logic networks", "author": ["Jue Wang", "Pedro M. Domingos"], "venue": "In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "The recent availability of large-scale data combining multiple data modalities, such as image, text, audio and sensor data, has opened up various research and commercial opportunities, underpinned by machine learning methods and techniques [5, 12, 17, 18].", "startOffset": 240, "endOffset": 255}, {"referenceID": 9, "context": "The recent availability of large-scale data combining multiple data modalities, such as image, text, audio and sensor data, has opened up various research and commercial opportunities, underpinned by machine learning methods and techniques [5, 12, 17, 18].", "startOffset": 240, "endOffset": 255}, {"referenceID": 14, "context": "The recent availability of large-scale data combining multiple data modalities, such as image, text, audio and sensor data, has opened up various research and commercial opportunities, underpinned by machine learning methods and techniques [5, 12, 17, 18].", "startOffset": 240, "endOffset": 255}, {"referenceID": 15, "context": "The recent availability of large-scale data combining multiple data modalities, such as image, text, audio and sensor data, has opened up various research and commercial opportunities, underpinned by machine learning methods and techniques [5, 12, 17, 18].", "startOffset": 240, "endOffset": 255}, {"referenceID": 12, "context": "Guha\u2019s recent position paper [15] is a case in point, as it advocates a new model theory for real-valued numbers.", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "In this paper, we take inspiration from such recent work in AI, but also less recent work in the area of neuralsymbolic integration [8, 10, 11] and in semantic attachment and symbol grounding [4] to achieve a vector-based representation which can be shown adequate for integrating machine learning and reasoning in a principled way.", "startOffset": 132, "endOffset": 143}, {"referenceID": 7, "context": "In this paper, we take inspiration from such recent work in AI, but also less recent work in the area of neuralsymbolic integration [8, 10, 11] and in semantic attachment and symbol grounding [4] to achieve a vector-based representation which can be shown adequate for integrating machine learning and reasoning in a principled way.", "startOffset": 132, "endOffset": 143}, {"referenceID": 8, "context": "In this paper, we take inspiration from such recent work in AI, but also less recent work in the area of neuralsymbolic integration [8, 10, 11] and in semantic attachment and symbol grounding [4] to achieve a vector-based representation which can be shown adequate for integrating machine learning and reasoning in a principled way.", "startOffset": 132, "endOffset": 143}, {"referenceID": 1, "context": "In this paper, we take inspiration from such recent work in AI, but also less recent work in the area of neuralsymbolic integration [8, 10, 11] and in semantic attachment and symbol grounding [4] to achieve a vector-based representation which can be shown adequate for integrating machine learning and reasoning in a principled way.", "startOffset": 192, "endOffset": 195}, {"referenceID": 21, "context": "This paper proposes a framework called logic tensor networks (LTN) which integrates learning based on tensor networks [25] with reasoning using first-order manyvalued logic [6], all implemented in TENSORFLOW [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 3, "context": "This paper proposes a framework called logic tensor networks (LTN) which integrates learning based on tensor networks [25] with reasoning using first-order manyvalued logic [6], all implemented in TENSORFLOW [13].", "startOffset": 173, "endOffset": 176}, {"referenceID": 10, "context": "This paper proposes a framework called logic tensor networks (LTN) which integrates learning based on tensor networks [25] with reasoning using first-order manyvalued logic [6], all implemented in TENSORFLOW [13].", "startOffset": 208, "endOffset": 212}, {"referenceID": 6, "context": "It is expected that, through an adequate integration of numerical properties and relational knowledge, differently from the immediate related literature [9, 2, 1], the framework introduced in this paper will be capable of combining in an effective way full first-order logical inference on open domains with efficient relational multi-class learning using tensor networks.", "startOffset": 153, "endOffset": 162}, {"referenceID": 13, "context": "For simplicity, we assume that all sentences of L are in prenex conjunctive, skolemised normal form [16], e.", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "An example of grounding is the one that associates to each document its bag-of-words vector [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "To limit the number of clause instantiations, which in general might be infinite since L admits function symbols, the usual approach is to consider the instantiations of each clause up to a certain depth [3].", "startOffset": 204, "endOffset": 207}, {"referenceID": 21, "context": "The grounding of m-ary predicate P , G(P ), is defined as a generalization of the neural tensor network [25] (which has been shown effective at performing the task of knowledge completion in the presence of simple logical constraints), as a function from R to [0, 1], as follows:", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "An estimation of the optimal grounding is obtained by 5,000 runs the RMSProp optimisation algorithm [26] available in TENSORFLOW .", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "In his recent note, [15], Guha advocates the need for a new model theory for distributed representations (such as those based on embeddings).", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "Real logic shares with [15] the idea that terms must be interpreted in a geometric space.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "the semantics proposed in [15] can be implemented within an ltn with a single layer (k = 1), since the operation of projection and comparison necessary to compute the truth-value of P (t1, .", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "Real logic is orthogonal to the approach taken by (Hybrid) Markov Logic Networks (MLNs) and its variations [24, 27, 22].", "startOffset": 107, "endOffset": 119}, {"referenceID": 19, "context": "Real logic is orthogonal to the approach taken by (Hybrid) Markov Logic Networks (MLNs) and its variations [24, 27, 22].", "startOffset": 107, "endOffset": 119}, {"referenceID": 16, "context": "Much work has been done also on neuro-fuzzy approaches [19].", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "Bayesian logic (BLOG) [20] is open domain, and in this respect is similar to real logic and LTNs.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "Other statistical AI and probabilistic approaches such as lifted inference fall in this same category, including probabilistic variations of inductive logic programming (ILP) [23], which are normally restricted to Horn clauses.", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "Metainterpretive ILP [21], together with BLOG, seems closer to LTNs in what concerns the knowledge representation language, but without exploring tensor networks and its benefits in terms of computational efficiency.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "Finally, related work in the domain of neural-symbolic computing and neural network fibring [10] has sought to combine neural networks with ILP to gain efficiency [14] and other forms of knowledge representation, such as propositional modal logic and logic programming.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "Finally, related work in the domain of neural-symbolic computing and neural network fibring [10] has sought to combine neural networks with ILP to gain efficiency [14] and other forms of knowledge representation, such as propositional modal logic and logic programming.", "startOffset": 163, "endOffset": 167}], "year": 2017, "abstractText": "We propose real logic: a uniform framework for integrating automatic learning and reasoning. Real logic is defined on a full first-order language where formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as (feature) vectors of real numbers. Real logic promotes a well-founded integration of deductive reasoning on knowledge-bases with efficient, data-driven relational machine learning. We show how Real Logic can be implemented in deep Tensor Neural Networks with the use of Google\u2019s TENSORFLOW primitives. The paper concludes with experiments on a simple but representative example of knowledge completion.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}