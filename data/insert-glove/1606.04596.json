{"id": "1606.04596", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Semi-Supervised Learning for Neural Machine Translation", "abstract": "pbms While end - yoseph to - korisa end boilermaker neural machine translation (brusy NMT) has made remarkable shawangunks progress mandula recently, NMT li\u00e8pvre systems only rely jurczynski on parallel nagorno-karabakh corpora fuggers for parameter anhydrides estimation. arrowe Since ollamh parallel farentino corpora are usually 1-pound limited in bilotti quantity, 1.22 quality, and sanshin coverage, serotina especially for low - hedemora resource genex languages, djindjic it is llegaron appealing stickwork to exploit monolingual corpora to blind improve cpk NMT. We sparseness propose 303,000 a semi - dmitrijs supervised mashriqi approach step-free for vieweg training NMT loughgall models maldon on duiker the winless concatenation nouwen of inter-city labeled (labaki parallel corpora) and fonterra unlabeled (.704 monolingual gro corpora) negligible data. periyar The central idea is to pade reconstruct nurhan the jondalar monolingual tailbone corpora zhenyu using an autoencoder, ajc.com in which displeasing the time-traveler source - ashish to - wersching target and target - to - oneximbank source ap-7 translation galerija models 30-4 serve as so\u015bnie the transposase encoder thuringia and decoder, respectively. abdoh Our metroliners approach can tricolore not only exploit schwall the boonma monolingual gumming corpora of darbuka the 25-6 target yogiji language, dc3 but non-parliamentary also of qb3 the idta source suwon language. thiers Experiments buttu on horia the Chinese - English jorgic dataset show ettaba that our approach clearness achieves significant improvements over state - zehi of - the - elizondo art 199.7 SMT lekhak and NMT frias systems.", "histories": [["v1", "Wed, 15 Jun 2016 00:22:27 GMT  (223kb,D)", "https://arxiv.org/abs/1606.04596v1", "Accepted for publication in the Proceedings of ACL 2016"], ["v2", "Wed, 10 Aug 2016 19:08:20 GMT  (223kb,D)", "http://arxiv.org/abs/1606.04596v2", "Accepted for publication in the Proceedings of ACL 2016. A typo in the references was corrected"], ["v3", "Sat, 10 Dec 2016 20:02:52 GMT  (223kb,D)", "http://arxiv.org/abs/1606.04596v3", "Corrected a typo"]], "COMMENTS": "Accepted for publication in the Proceedings of ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yong cheng", "wei xu 0005", "zhongjun he", "wei he", "hua wu", "maosong sun", "yang liu 0005"], "accepted": true, "id": "1606.04596"}, "pdf": {"name": "1606.04596.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Learning for Neural Machine Translation", "authors": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "emails": ["chengyong3001@gmail.com", "weixu@tsinghua.edu.cn", "hua}@baidu.com", "sms@tsinghua.edu.cn", "liuyang2011@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model-\n\u2217 Yang Liu is the corresponding author.\ning long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015).\nHowever, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unfortunately, parallel corpora are usually only available for a handful of resourcerich languages and restricted to limited domains such as government documents and news reports. In contrast, SMT is capable of exploiting abundant target-side monolingual corpora to boost fluency of translations. Therefore, the unavailability of large-scale, high-quality, and wide-coverage parallel corpora hinders the applicability of NMT.\nAs a result, several authors have tried to use abundant monolingual corpora to improve NMT. Gulccehre et al. (2015) propose two methods, which are referred to as shallow fusion and deep fusion, to integrate a language model into NMT. The basic idea is to use the language model to score the candidate words proposed by the translation model at each time step or concatenating the hidden states of the language model and the decoder. Although their approach leads to significant improvements, one possible downside is that the network architecture has to be modified to integrate the language model.\nAlternatively, Sennrich et al. (2015) propose two approaches to exploiting monolingual corpora that is transparent to network architectures. The first approach pairs monolingual sentences with dummy input. Then, the parameters of encoder\nar X\niv :1\n60 6.\n04 59\n6v 3\n[ cs\n.C L\n] 1\n0 D\nec 2\n01 6\nand attention model are fixed when training on these pseudo parallel sentence pairs. In the second approach, they first train a nerual translation model on the parallel corpus and then use the learned model to translate a monolingual corpus. The monolingual corpus and its translations constitute an additional pseudo parallel corpus. Similar ideas have also been suggested in conventional SMT (Ueffing et al., 2007; Bertoldi and Federico, 2009). Sennrich et al. (2015) report that their approach significantly improves translation quality across a variety of language pairs.\nIn this paper, we propose semi-supervised learning for neural machine translation. Given labeled (i.e., parallel corpora) and unlabeled (i.e., monolingual corpora) data, our approach jointly trains source-to-target and target-to-source translation models. The key idea is to append a reconstruction term to the training objective, which aims to reconstruct the observed monolingual corpora using an autoencoder. In the autoencoder, the source-to-target and target-to-source models serve as the encoder and decoder, respectively. As the inference is intractable, we propose to sample the full search space to improve the efficiency. Specifically, our approach has the following advantages:\n1. Transparent to network architectures: our approach does not depend on specific architectures and can be easily applied to arbitrary end-to-end NMT systems.\n2. Both the source and target monolingual corpora can be used: our approach can benefit NMT not only using target monolingual corpora in a conventional way, but also the monolingual corpora of the source language.\nExperiments on Chinese-English NIST datasets show that our approach results in significant improvements in both directions over state-of-the-art SMT and NMT systems."}, {"heading": "2 Semi-Supervised Learning for Neural Machine Translation", "text": ""}, {"heading": "2.1 Supervised Learning", "text": "Given a parallel corpus D = {\u3008x(n),y(n)\u3009}Nn=1, the standard training objective in NMT is to maximize the likelihood of the training data:\nL(\u03b8) = N\u2211\nn=1\nlogP (y(n)|x(n);\u03b8), (1)\nwhere P (y|x;\u03b8) is a neural translation model and \u03b8 is a set of model parameters. D can be seen as labeled data for the task of predicting a target sentence y given a source sentence x.\nAs P (y|x;\u03b8) is modeled by a single, large neural network, there does not exist a separate target language model P (y;\u03b8) in NMT. Therefore, parallel corpora have been the only resource for parameter estimation in most existing NMT systems. Unfortunately, even for a handful of resource-rich\nlanguages, the available domains are unbalanced and restricted to government documents and news reports. Therefore, the availability of large-scale, high-quality, and wide-coverage parallel corpora becomes a major obstacle for NMT."}, {"heading": "2.2 Autoencoders on Monolingual Corpora", "text": "It is appealing to explore the more readily available, abundant monolingual corpora to improve NMT. Let us first consider an unsupervised setting: how to train NMT models on a monolingual corpus T = {y(t)}Tt=1?\nOur idea is to leverage autoencoders (Vincent et al., 2010; Socher et al., 2011): (1) encoding an observed target sentence into a latent source sentence using a target-to-source translation model and (2) decoding the source sentence to reconstruct the observed target sentence using a source-to-target model. For example, as shown in Figure 1(b), given an observed English sentence \u201cBush held a talk with Sharon\u201d, a target-to-source translation model (i.e., encoder) transforms it into a Chinese translation \u201cbushi yu shalong juxing le huitan\u201d that is unobserved on the training data (highlighted in grey). Then, a source-to-target translation model (i.e., decoder) reconstructs the observed English sentence from the Chinese translation.\nMore formally, let P (y|x;\u2212\u2192\u03b8) and P (x|y;\u2190\u2212\u03b8 ) be source-to-target and target-to-source translation models respectively, where \u2212\u2192 \u03b8 and \u2190\u2212 \u03b8 are corresponding model parameters. An autoencoder aims to reconstruct the observed target sentence via a latent source sentence:\nP (y\u2032|y;\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 ) = \u2211 x P (y\u2032,x|y;\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 )\n= \u2211 x\nP (x|y;\u2190\u2212\u03b8 )\ufe38 \ufe37\ufe37 \ufe38 encoder P (y\u2032|x;\u2212\u2192\u03b8 )\ufe38 \ufe37\ufe37 \ufe38 decoder , (2)\nwhere y is an observed target sentence, y\u2032 is a copy of y to be reconstructed, and x is a latent source sentence.\nWe refer to Eq. (2) as a target autoencoder. 1 Likewise, given a monolingual corpus of source language S = {x(s)}Ss=1, it is natural to introduce a source autoencoder that aims at reconstructing\n1Our definition of auotoencoders is inspired by Ammar et al. (2014). Note that our autoencoders inherit the same spirit from conventional autoencoders (Vincent et al., 2010; Socher et al., 2011) except that the hidden layer is denoted by a latent sentence instead of real-valued vectors.\nthe observed source sentence via a latent target sentence:\nP (x\u2032|x;\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 ) = \u2211 y P (x\u2032,y|x;\u2190\u2212\u03b8 )\n= \u2211 y\nP (y|x;\u2212\u2192\u03b8 )\ufe38 \ufe37\ufe37 \ufe38 encoder P (x\u2032|y;\u2190\u2212\u03b8 )\ufe38 \ufe37\ufe37 \ufe38 decoder . (3)\nPlease see Figure 1(a) for illustration."}, {"heading": "2.3 Semi-Supervised Learning", "text": "As the autoencoders involve both source-to-target and target-to-source models, it is natural to combine parallel corpora and monolingual corpora to learn birectional NMT translation models in a semi-supervised setting.\nFormally, given a parallel corpus D = {\u3008x(n),y(n)\u3009}Nn=1 , a monolingual corpus of target language T = {y(t)}Tt=1, and a monolingual corpus of source language S = {x(s)}Ss=1, we introduce our new semi-supervised training objective as follows:\nJ( \u2212\u2192 \u03b8 , \u2190\u2212 \u03b8 )\n= N\u2211\nn=1 logP (y(n)|x(n);\u2212\u2192\u03b8 )\ufe38 \ufe37\ufe37 \ufe38 source-to-target likelihood\n+ N\u2211 n=1\nlogP (x(n)|y(n);\u2190\u2212\u03b8 )\ufe38 \ufe37\ufe37 \ufe38 target-to-source likelihood\n+\u03bb1 T\u2211 t=1\nlogP (y\u2032|y(t);\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 )\ufe38 \ufe37\ufe37 \ufe38 target autoencoder\n+\u03bb2 S\u2211 s=1\nlogP (x\u2032|x(s);\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 )\ufe38 \ufe37\ufe37 \ufe38 source autoencoder , (4)\nwhere \u03bb1 and \u03bb2 are hyper-parameters for balancing the preference between likelihood and autoencoders.\nNote that the objective consists of four parts: source-to-target likelihood, target-to-source likelihood, target autoencoder, and source autoencoder. In this way, our approach is capable of exploiting abundant monolingual corpora of both source and target languages.\nThe optimal model parameters are given by\n\u2212\u2192 \u03b8 \u2217 = argmax\n{ N\u2211\nn=1\nlogP (y(n)|x(n);\u2212\u2192\u03b8 ) +\n\u03bb1 T\u2211 t=1 logP (y\u2032|y(t);\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 ) +\n\u03bb2 S\u2211 s=1 logP (x\u2032|x(s);\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 )\n} (5)\n\u2190\u2212 \u03b8 \u2217 = argmax\n{ N\u2211\nn=1\nlogP (x(n)|y(n);\u2190\u2212\u03b8 ) +\n\u03bb1 T\u2211 t=1 logP (y\u2032|y(t);\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 ) +\n\u03bb2 S\u2211 s=1 logP (x\u2032|x(s);\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 )\n} (6)\nIt is clear that the source-to-target and target-tosource models are connected via the autoencoder and can hopefully benefit each other in joint training."}, {"heading": "2.4 Training", "text": "We use mini-batch stochastic gradient descent to train our joint model. For each iteration, besides the mini-batch from the parallel corpus, we also construct two additional mini-batches by randomly selecting sentences from the source and target monolingual corpora. Then, gradients are collected from these mini-batches to update model parameters.\nThe partial derivative of J( \u2212\u2192 \u03b8 , \u2190\u2212 \u03b8 ) with respect\nto the source-to-target model \u2212\u2192 \u03b8 is given by\n\u2202J( \u2212\u2192 \u03b8 , \u2190\u2212 \u03b8 )\n\u2202 \u2212\u2192 \u03b8\n= N\u2211 n=1 \u2202 logP (y(n)|x(n);\u2212\u2192\u03b8 ) \u2202 \u2212\u2192 \u03b8\n+\u03bb1 T\u2211 t=1 \u2202 logP (y\u2032|y(t);\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 ) \u2202 \u2212\u2192 \u03b8\n+\u03bb2 S\u2211 s=1 \u2202 logP (x\u2032|x(s);\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 ) \u2202 \u2212\u2192 \u03b8 . (7)\nThe partial derivative with respect to \u2190\u2212 \u03b8 can be calculated similarly. Unfortunately, the second and third terms in Eq. (7) are intractable to calculate due to the exponential search space. For example, the derivative in\nthe third term in Eq. (7) is given by\u2211 x\u2208X (y) P (x|y; \u2190\u2212 \u03b8 )P (y\u2032|x;\u2212\u2192\u03b8 )\u2202 logP (y\n\u2032|x;\u2212\u2192\u03b8 ) \u2202\u2212\u2192\u03b8\u2211\nx\u2208X (y) P (x|y; \u2190\u2212 \u03b8 )P (y\u2032|x;\u2212\u2192\u03b8 )\n. (8)\nIt is prohibitively expensive to compute the sums due to the exponential search space of X (y).\nAlternatively, we propose to use a subset of the full space X\u0303 (y) \u2282 X (y) to approximate Eq. (8): \u2211 x\u2208X\u0303 (y) P (x|y; \u2190\u2212 \u03b8 )P (y\u2032|x;\u2212\u2192\u03b8 )\u2202 logP (y\n\u2032|x;\u2212\u2192\u03b8 ) \u2202\u2212\u2192\u03b8\u2211\nx\u2208X\u0303 (y) P (x|y; \u2190\u2212 \u03b8 )P (y\u2032|x;\u2212\u2192\u03b8 )\n. (9)\nIn practice, we use the top-k list of candidate translations of y as X\u0303 (y). As |X\u0303 (y)| X |(y)|, it is possible to calculate Eq. (9) efficiently by enumerating all candidates in X\u0303 (y). In practice, we find this approximation results in significant improvements and k = 10 seems to suffice to keep the balance between efficiency and translation quality."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Setup", "text": "We evaluated our approach on the ChineseEnglish dataset.\nAs shown in Table 1, we use both a parallel corpus and two monolingual corpora as the training set. The parallel corpus from LDC consists of 2.56M sentence pairs with 67.53M Chinese words and 74.81M English words. The vocabulary sizes of Chinese and English are 0.21M and 0.16M, respectively. We use the Chinese and English parts of the Xinhua portion of the GIGAWORD corpus as the monolingual corpora. The Chinese monolingual corpus contains 18.75M sentences with 451.94M words. The English corpus contains 22.32M sentences with 399.83M words. The vocabulary sizes of Chinese and English are 0.97M and 1.34M, respectively.\nFor Chinese-to-English translation, we use the NIST 2006 Chinese-English dataset as the validation set for hyper-parameter optimization and model selection. The NIST 2002, 2003, 2004, and 2005 datasets serve as test sets. Each Chinese sentence has four reference translations. For English-to-Chinese translation, we use the NIST datasets in a reverse direction: treating the first English sentence in the four reference translations as a source sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script.\nWe compared our approach with two state-ofthe-art SMT and NMT systems:\n1. MOSES (Koehn et al., 2007): a phrase-based SMT system;\n2. RNNSEARCH (Bahdanau et al., 2015): an attention-based NMT system.\nFor MOSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models.\nFor RNNSEARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words.\nOn top of RNNSEARCH, our approach is capable of training bidirectional attention-based neural translation models on the concatenation of parallel and monolingual corpora. The sample size k is set to 10. We set the hyper-parameter \u03bb1 = 0.1 and\n\u03bb2 = 0 when we add the target monolingual corpus, and \u03bb1 = 0 and \u03bb2 = 0.1 for source monolingual corpus incorporation. The threshold of gradient clipping is set to 0.05. The parameters of our model are initialized by the model trained on parallel corpus."}, {"heading": "3.2 Effect of Sample Size k", "text": "As the inference of our approach is intractable, we propose to approximate the full search space with the top-k list of candidate translations to improve efficiency (see Eq. (9)).\nFigure 2 shows the BLEU scores of various settings of k over time. Only the English monolingual corpus is appended to the training data. We observe that increasing the size of the approximate search space generally leads to improved BLEU scores. There are significant gaps between k = 1 and k = 5. However, keeping increasing k does not result in significant improvements and decreases the training efficiency. We find that k = 10 achieves a balance between training efficiency and translation quality. As shown in Figure 3, similar findings are also observed on the English-to-Chinese validation set. Therefore, we set k = 10 in the following experiments."}, {"heading": "3.3 Effect of OOV Ratio", "text": "Given a parallel corpus, what kind of monolingual corpus is most beneficial for improving translation quality? To answer this question, we investigate the effect of OOV ratio on translation quality, which is defined as\nratio = \u2211 y\u2208yJy /\u2208 VDtK |y| , (10)\nwhere y is a target-language sentence in the monolingual corpus T , y is a target-language word in y, VDt is the vocabulary of the target side of the parallel corpus D.\nIntuitively, the OOV ratio indicates how a sentence in the monolingual resembles the parallel corpus. If the ratio is 0, all words in the monolingual sentence also occur in the parallel corpus.\nFigure 4 shows the effect of OOV ratio on the Chinese-to-English validation set. Only English monolingual corpus is appended to the parallel corpus during training. We constructed four monolingual corpora of the same size in terms of sentence pairs. \u201c0% OOV\u201d means the OOV ratio is 0% for all sentences in the monolingual corpus. \u201c10% OOV\u201d suggests that the OOV ratio is\nno greater 10% for each sentence in the monolingual corpus. We find that using a monolingual corpus with a lower OOV ratio generally leads to higher BLEU scores. One possible reason is that low-OOV monolingual corpus is relatively easier to reconstruct than its high-OOV counterpart and results in better estimation of model parameters.\nFigure 5 shows the effect of OOV ratio on the English-to-Chinese validation set. Only English monolingual corpus is appended to the parallel corpus during training. We find that \u201c0% OOV\u201d still achieves the highest BLEU scores."}, {"heading": "3.4 Comparison with SMT", "text": "Table 2 shows the comparison between MOSES and our work. MOSES used the monolingual corpora as shown in Table 1: 18.75M Chinese sentences and 22.32M English sentences. We find that exploiting monolingual corpora dramatically improves translation performance in both Chinese-to-English and English-to-Chinese directions.\nRelying only on parallel corpus, RNNSEARCH outperforms MOSES trained also only on parallel corpus. But the capability of making use of abundant monolingual corpora enables MOSES to achieve much higher BLEU scores than RNNSEARCH only using parallel corpus.\nInstead of using all sentences in the monolingual corpora, we constructed smaller monolingual corpora with zero OOV ratio: 2.56M Chinese sentences with 47.51M words and 2.56M English English sentences with 37.47M words. In other words, the monolingual corpora we used in the experiments are much smaller than those used by MOSES.\nBy adding English monolingual corpus, our approach achieves substantial improvements over RNNSEARCH using only parallel corpus (up to +4.7 BLEU points). In addition, significant improvements are also obtained over MOSES using both parallel and monolingual corpora (up to +3.5 BLEU points).\nAn interesting finding is that adding English monolingual corpora helps to improve English-toChinese translation over RNNSEARCH using only parallel corpus (up to +3.2 BLEU points), suggesting that our approach is capable of improving NMT using source-side monolingual corpora.\nIn the English-to-Chinese direction, we obtain similar findings. In particular, adding Chi-\nnese monolingual corpus leads to more benefits to English-to-Chinese translation than adding English monolingual corpus. We also tried to use both Chinese and English monolingual corpora through simply setting all the \u03bb to 0.1 but failed to obtain further significant improvements.\nTherefore, our findings can be summarized as follows:\n1. Adding target monolingual corpus improves over using only parallel corpus for source-totarget translation;\n2. Adding source monolingual corpus also improves over using only parallel corpus for source-to-target translation, but the improvements are smaller than adding target monolingual corpus;\n3. Adding both source and target monolingual corpora does not lead to further significant improvements."}, {"heading": "3.5 Comparison with Previous Work", "text": "We re-implemented Sennrich et al. (2015)\u2019s method on top of RNNSEARCH as follows:\n1. Train the target-to-source neural translation model P (x|y;\u2190\u2212\u03b8 ) on the parallel corpusD = {\u3008x(n),y(n)\u3009}Nn=1. 2. The trained target-to-source model \u2190\u2212 \u03b8 \u2217\nis used to translate a target monolingual corpus T = {y(t)}Tt=1 into a source monolingual corpus S\u0303 = {x\u0303(t)}Tt=1.\n3. The target monolingual corpus is paired with its translations to form a pseudo parallel corpus, which is then appended to the original parallel corpus to obtain a larger parallel corpus: D\u0303 = D \u222a \u3008S\u0303, T \u3009.\n4. Re-train the the source-to-target neural translation model on D\u0303 to obtain the final model parameters \u2212\u2192 \u03b8 \u2217 .\nTable 3 shows the comparison results. Both the two approaches use the same parallel and monolingual corpora. Our approach achieves significant improvements over Sennrich et al. (2015) in both Chinese-to-English and English-to-Chinese directions (up to +1.8 and +1.0 BLEU points). One possible reason is that Sennrich et al. (2015) only use the pesudo parallel corpus for parameter estimation for once (see Step 4 above) while our approach enables source-to-target and targetto-source models to interact with each other iteratively on both parallel and monolingual corpora.\nTo some extent, our approach can be seen as an iterative extension of Sennrich et al. (2015)\u2019s approach: after estimating model parameters on the pseudo parallel corpus, the learned model parameters are used to produce a better pseudo parallel corpus. Table 4 shows example Viterbi translations on the Chinese monolingual corpus over iterations:\nx\u2217 = argmax x\n{ P (y\u2032|x;\u2212\u2192\u03b8 )P (x|y;\u2190\u2212\u03b8 ) } . (11)\nWe observe that the quality of Viterbi translations generally improves over time."}, {"heading": "4 Related Work", "text": "Our work is inspired by two lines of research: (1) exploiting monolingual corpora for machine translation and (2) autoencoders in unsupervised and semi-supervised learning."}, {"heading": "4.1 Exploiting Monolingual Corpora for Machine Translation", "text": "Exploiting monolingual corpora for conventional SMT has attracted intensive attention in recent years. Several authors have introduced transductive learning to make full use of monolingual corpora (Ueffing et al., 2007; Bertoldi and Federico, 2009). They use an existing translation model to translate unseen source text, which can be paired with its translations to form a pseudo parallel corpus. This process iterates until convergence. While Klementiev et al. (2012) propose an approach to estimating phrase translation probabilities from monolingual corpora, Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrieval techniques. Another important line of research is to treat translation on monolingual corpora as a decipherment problem (Ravi and Knight, 2011; Dou et al., 2014).\nClosely related to Gulccehre et al. (2015) and Sennrich et al. (2015), our approach focuses on learning birectional NMT models via autoencoders on monolingual corpora. The major advantages of our approach are the transparency to network architectures and the capability to exploit both source and target monolingual corpora."}, {"heading": "4.2 Autoencoders in Unsupervised and Semi-Supervised Learning", "text": "Autoencoders and their variants have been widely used in unsupervised deep learning ((Vincent et al., 2010; Socher et al., 2011; Ammar et al., 2014), just to name a few). Among them, Socher et al. (2011)\u2019s approach bears close resemblance to our approach as they introduce semi-supervised recursive autoencoders for sentiment analysis. The difference is that we are interested in making a better use of parallel and monolingual corpora while they concentrate on injecting partial supervision to conventional unsupervised autoencoders. Dai and Le (2015) introduce a sequence autoencoder to reconstruct an observed sequence via RNNs. Our approach differs from sequence autoencoders in that we use bidirectional translation models as encoders and decoders to enable them to interact within the autoencoders."}, {"heading": "5 Conclusion", "text": "We have presented a semi-supervised approach to training bidirectional neural machine translation models. The central idea is to introduce autoencoders on the monolingual corpora with source-totarget and target-to-source translation models as encoders and decoders. Experiments on ChineseEnglish NIST datasets show that our approach leads to significant improvements.\nAs our method is sensitive to the OOVs present in monolingual corpora, we plan to integrate Jean et al. (2015)\u2019s technique on using very large vocabulary into our approach. It is also necessary to further validate the effectiveness of our approach on more language pairs and NMT architectures. Another interesting direction is to enhance the connection between source-to-target and target-tosource models (e.g., letting the two models share the same word embeddings) to help them benefit more from interacting with each other."}, {"heading": "Acknowledgements", "text": "This work was done while Yong Cheng was visiting Baidu. This research is supported by the 973 Program (2014CB340501, 2014CB340505), the National Natural Science Foundation of China (No. 61522204, 61331013, 61361136003), 1000 Talent Plan grant, Tsinghua Initiative Research Program grants 20151080475 and a Google Faculty Research Award. We sincerely thank the viewers for their valuable suggestions."}], "references": [{"title": "Conditional random field autoencoders for unsupervised structred prediction", "author": ["Waleed Ammar", "Chris Dyer", "Noah Smith."], "venue": "Proceedings of NIPS 2014.", "citeRegEx": "Ammar et al\\.,? 2014", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Domain adaptation for statistical machine translation with monolingual resources", "author": ["Nicola Bertoldi", "Marcello Federico."], "venue": "Proceedings of WMT.", "citeRegEx": "Bertoldi and Federico.,? 2009", "shortCiteRegEx": "Bertoldi and Federico.", "year": 2009}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer."], "venue": "Computational Linguisitics.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of ACL.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunhyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of SSST-8.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Semisupervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le."], "venue": "Proceedings of NIPS.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Beyond parallel data: Joint word alignment and decipherment improves machine translation", "author": ["Qing Dou", "Ashish Vaswani", "Kevin Knight."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Dou et al\\.,? 2014", "shortCiteRegEx": "Dou et al\\.", "year": 2014}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulccehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Gulccehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulccehre et al\\.", "year": 2015}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Computational Linguisitics.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1993", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1993}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Sebastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Toward statistical machine translation without paralel corpora", "author": ["Alexandre Klementiev", "Ann Irvine", "Chris CallisonBurch", "David Yarowsky."], "venue": "Proceedings of EACL.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz J. Och", "Daniel Marcu."], "venue": "Proceedings of NAACL.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Och."], "venue": "Proceedings of ACL.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a methof for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of ACL.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Deciphering foreign language", "author": ["Sujith Ravi", "Kevin Knight."], "venue": "Proceedings of ACL.", "citeRegEx": "Ravi and Knight.,? 2011", "shortCiteRegEx": "Ravi and Knight.", "year": 2011}, {"title": "Improving nerual machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv:1511.06709 [cs.CL].", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric Huang", "Andrew Ng", "Christopher Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Srilm - am extensible language modeling toolkit", "author": ["Andreas Stolcke."], "venue": "Proceedings of ICSLP.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Trasductive learning for statistical machine translation", "author": ["Nicola Ueffing", "Gholamreza Haffari", "Anoop Sarkar."], "venue": "Proceedings of ACL.", "citeRegEx": "Ueffing et al\\.,? 2007", "shortCiteRegEx": "Ueffing et al\\.", "year": 2007}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Autoine Manzagol."], "venue": "Journal of Machine Learning", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Learning a phrase-based translation model from monolingual data with application to domain adaptation", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "Proceedings of ACL.", "citeRegEx": "Zhang and Zong.,? 2013", "shortCiteRegEx": "Zhang and Zong.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 233, "endOffset": 312}, {"referenceID": 21, "context": "End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 233, "endOffset": 312}, {"referenceID": 1, "context": "End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 233, "endOffset": 312}, {"referenceID": 3, "context": "Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model-", "startOffset": 128, "endOffset": 182}, {"referenceID": 13, "context": "Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model-", "startOffset": 128, "endOffset": 182}, {"referenceID": 4, "context": "Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model-", "startOffset": 128, "endOffset": 182}, {"referenceID": 9, "context": "ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al.", "startOffset": 93, "endOffset": 169}, {"referenceID": 5, "context": "ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al.", "startOffset": 93, "endOffset": 169}, {"referenceID": 21, "context": "ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al.", "startOffset": 93, "endOffset": 169}, {"referenceID": 1, "context": ", 2014) and attention mechanisms (Bahdanau et al., 2015).", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": "This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 168, "endOffset": 247}, {"referenceID": 21, "context": "This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 168, "endOffset": 247}, {"referenceID": 1, "context": "This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 168, "endOffset": 247}, {"referenceID": 1, "context": ", 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unfortunately, parallel corpora are usually only available for a handful of resourcerich languages and restricted to limited domains such as government documents and news reports. In contrast, SMT is capable of exploiting abundant target-side monolingual corpora to boost fluency of translations. Therefore, the unavailability of large-scale, high-quality, and wide-coverage parallel corpora hinders the applicability of NMT. As a result, several authors have tried to use abundant monolingual corpora to improve NMT. Gulccehre et al. (2015) propose two methods, which are referred to as shallow fusion and deep fusion, to integrate a language model into NMT.", "startOffset": 34, "endOffset": 988}, {"referenceID": 1, "context": ", 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unfortunately, parallel corpora are usually only available for a handful of resourcerich languages and restricted to limited domains such as government documents and news reports. In contrast, SMT is capable of exploiting abundant target-side monolingual corpora to boost fluency of translations. Therefore, the unavailability of large-scale, high-quality, and wide-coverage parallel corpora hinders the applicability of NMT. As a result, several authors have tried to use abundant monolingual corpora to improve NMT. Gulccehre et al. (2015) propose two methods, which are referred to as shallow fusion and deep fusion, to integrate a language model into NMT. The basic idea is to use the language model to score the candidate words proposed by the translation model at each time step or concatenating the hidden states of the language model and the decoder. Although their approach leads to significant improvements, one possible downside is that the network architecture has to be modified to integrate the language model. Alternatively, Sennrich et al. (2015) propose two approaches to exploiting monolingual corpora that is transparent to network architectures.", "startOffset": 34, "endOffset": 1509}, {"referenceID": 22, "context": "Similar ideas have also been suggested in conventional SMT (Ueffing et al., 2007; Bertoldi and Federico, 2009).", "startOffset": 59, "endOffset": 110}, {"referenceID": 2, "context": "Similar ideas have also been suggested in conventional SMT (Ueffing et al., 2007; Bertoldi and Federico, 2009).", "startOffset": 59, "endOffset": 110}, {"referenceID": 2, "context": ", 2007; Bertoldi and Federico, 2009). Sennrich et al. (2015) report that their approach significantly improves translation quality across a variety of language pairs.", "startOffset": 8, "endOffset": 61}, {"referenceID": 23, "context": "Let us first consider an unsupervised setting: how to train NMT models on a monolingual corpus T = {y}t=1? Our idea is to leverage autoencoders (Vincent et al., 2010; Socher et al., 2011): (1) encoding an observed target sentence into a latent source sentence using a target-to-source translation model and (2) decoding the source sentence to reconstruct the observed target sentence using a source-to-target model.", "startOffset": 144, "endOffset": 187}, {"referenceID": 19, "context": "Let us first consider an unsupervised setting: how to train NMT models on a monolingual corpus T = {y}t=1? Our idea is to leverage autoencoders (Vincent et al., 2010; Socher et al., 2011): (1) encoding an observed target sentence into a latent source sentence using a target-to-source translation model and (2) decoding the source sentence to reconstruct the observed target sentence using a source-to-target model.", "startOffset": 144, "endOffset": 187}, {"referenceID": 23, "context": "Note that our autoencoders inherit the same spirit from conventional autoencoders (Vincent et al., 2010; Socher et al., 2011) except that the hidden layer is denoted by a latent sentence instead of real-valued vectors.", "startOffset": 82, "endOffset": 125}, {"referenceID": 19, "context": "Note that our autoencoders inherit the same spirit from conventional autoencoders (Vincent et al., 2010; Socher et al., 2011) except that the hidden layer is denoted by a latent sentence instead of real-valued vectors.", "startOffset": 82, "endOffset": 125}, {"referenceID": 0, "context": "Our definition of auotoencoders is inspired by Ammar et al. (2014). Note that our autoencoders inherit the same spirit from conventional autoencoders (Vincent et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 16, "context": "The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.", "startOffset": 47, "endOffset": 70}, {"referenceID": 1, "context": "RNNSEARCH (Bahdanau et al., 2015): an attention-based NMT system.", "startOffset": 10, "endOffset": 33}, {"referenceID": 15, "context": "For MOSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003).", "startOffset": 196, "endOffset": 207}, {"referenceID": 20, "context": "We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models.", "startOffset": 25, "endOffset": 40}, {"referenceID": 14, "context": "We follow Luong et al. (2015) to address rare words.", "startOffset": 10, "endOffset": 30}, {"referenceID": 1, "context": "RNNSEARCH is an attention-based neural machine translation system (Bahdanau et al., 2015).", "startOffset": 66, "endOffset": 89}, {"referenceID": 18, "context": "Table 3: Comparison with Sennrich et al. (2015). Both Sennrich et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 18, "context": "Table 3: Comparison with Sennrich et al. (2015). Both Sennrich et al. (2015) and our approach build on top of RNNSEARCH to exploit monolingual corpora.", "startOffset": 25, "endOffset": 77}, {"referenceID": 18, "context": "Table 3: Comparison with Sennrich et al. (2015). Both Sennrich et al. (2015) and our approach build on top of RNNSEARCH to exploit monolingual corpora. The BLEU scores are case-insensitive. \u201c*\u201d: significantly better than Sennrich et al. (2015) (p < 0.", "startOffset": 25, "endOffset": 244}, {"referenceID": 18, "context": "Table 3: Comparison with Sennrich et al. (2015). Both Sennrich et al. (2015) and our approach build on top of RNNSEARCH to exploit monolingual corpora. The BLEU scores are case-insensitive. \u201c*\u201d: significantly better than Sennrich et al. (2015) (p < 0.05); \u201c**\u201d: significantly better than Sennrich et al. (2015) (p < 0.", "startOffset": 25, "endOffset": 311}, {"referenceID": 18, "context": "We re-implemented Sennrich et al. (2015)\u2019s method on top of RNNSEARCH as follows:", "startOffset": 18, "endOffset": 41}, {"referenceID": 18, "context": "Our approach achieves significant improvements over Sennrich et al. (2015) in both Chinese-to-English and English-to-Chinese directions (up to +1.", "startOffset": 52, "endOffset": 75}, {"referenceID": 18, "context": "Our approach achieves significant improvements over Sennrich et al. (2015) in both Chinese-to-English and English-to-Chinese directions (up to +1.8 and +1.0 BLEU points). One possible reason is that Sennrich et al. (2015) only use the pesudo parallel corpus for parameter estimation for once (see Step 4 above) while our approach enables source-to-target and targetto-source models to interact with each other iteratively on both parallel and monolingual corpora.", "startOffset": 52, "endOffset": 222}, {"referenceID": 18, "context": "To some extent, our approach can be seen as an iterative extension of Sennrich et al. (2015)\u2019s approach: after estimating model parameters on the pseudo parallel corpus, the learned model parameters are used to produce a better pseudo parallel corpus.", "startOffset": 70, "endOffset": 93}, {"referenceID": 22, "context": "Several authors have introduced transductive learning to make full use of monolingual corpora (Ueffing et al., 2007; Bertoldi and Federico, 2009).", "startOffset": 94, "endOffset": 145}, {"referenceID": 2, "context": "Several authors have introduced transductive learning to make full use of monolingual corpora (Ueffing et al., 2007; Bertoldi and Federico, 2009).", "startOffset": 94, "endOffset": 145}, {"referenceID": 17, "context": "Another important line of research is to treat translation on monolingual corpora as a decipherment problem (Ravi and Knight, 2011; Dou et al., 2014).", "startOffset": 108, "endOffset": 149}, {"referenceID": 7, "context": "Another important line of research is to treat translation on monolingual corpora as a decipherment problem (Ravi and Knight, 2011; Dou et al., 2014).", "startOffset": 108, "endOffset": 149}, {"referenceID": 2, "context": ", 2007; Bertoldi and Federico, 2009). They use an existing translation model to translate unseen source text, which can be paired with its translations to form a pseudo parallel corpus. This process iterates until convergence. While Klementiev et al. (2012) propose an approach to estimating phrase translation probabilities from monolingual corpora, Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrieval techniques.", "startOffset": 8, "endOffset": 258}, {"referenceID": 2, "context": ", 2007; Bertoldi and Federico, 2009). They use an existing translation model to translate unseen source text, which can be paired with its translations to form a pseudo parallel corpus. This process iterates until convergence. While Klementiev et al. (2012) propose an approach to estimating phrase translation probabilities from monolingual corpora, Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrieval techniques.", "startOffset": 8, "endOffset": 373}, {"referenceID": 8, "context": "Closely related to Gulccehre et al. (2015) and Sennrich et al.", "startOffset": 19, "endOffset": 43}, {"referenceID": 8, "context": "Closely related to Gulccehre et al. (2015) and Sennrich et al. (2015), our approach focuses on learning birectional NMT models via autoencoders on monolingual corpora.", "startOffset": 19, "endOffset": 70}, {"referenceID": 23, "context": "Autoencoders and their variants have been widely used in unsupervised deep learning ((Vincent et al., 2010; Socher et al., 2011; Ammar et al., 2014), just to name a few).", "startOffset": 85, "endOffset": 148}, {"referenceID": 19, "context": "Autoencoders and their variants have been widely used in unsupervised deep learning ((Vincent et al., 2010; Socher et al., 2011; Ammar et al., 2014), just to name a few).", "startOffset": 85, "endOffset": 148}, {"referenceID": 0, "context": "Autoencoders and their variants have been widely used in unsupervised deep learning ((Vincent et al., 2010; Socher et al., 2011; Ammar et al., 2014), just to name a few).", "startOffset": 85, "endOffset": 148}, {"referenceID": 0, "context": ", 2011; Ammar et al., 2014), just to name a few). Among them, Socher et al. (2011)\u2019s approach bears close resemblance to our approach as they introduce semi-supervised recursive autoencoders for sentiment analysis.", "startOffset": 8, "endOffset": 83}, {"referenceID": 0, "context": ", 2011; Ammar et al., 2014), just to name a few). Among them, Socher et al. (2011)\u2019s approach bears close resemblance to our approach as they introduce semi-supervised recursive autoencoders for sentiment analysis. The difference is that we are interested in making a better use of parallel and monolingual corpora while they concentrate on injecting partial supervision to conventional unsupervised autoencoders. Dai and Le (2015) introduce a sequence autoencoder to reconstruct an observed sequence via RNNs.", "startOffset": 8, "endOffset": 432}, {"referenceID": 10, "context": "As our method is sensitive to the OOVs present in monolingual corpora, we plan to integrate Jean et al. (2015)\u2019s technique on using very large vocabulary into our approach.", "startOffset": 92, "endOffset": 111}], "year": 2016, "abstractText": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semisupervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourceto-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.", "creator": "TeX"}}}