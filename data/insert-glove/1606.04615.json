{"id": "1606.04615", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Deep Reinforcement Learning With Macro-Actions", "abstract": "kedem Deep 0100 reinforcement bitanga learning seitz has auto-maker been doobie shown i\u0142\u00f3w to melendy be a powerful framework gamlet for learning tetrachords policies aurora from quantrill complex high - tutankhamun dimensional sensory immodest inputs lannes to riskiness actions seawind in przedmie\u015bcie complex tasks, such slapton as sprayberry the ethnomusicologist Atari domain. muhyadin In this 74-year paper, we explore happonen output petitjean representation aprica modeling in vet the form serkes of temporal abstraction venturestar to raita improve girdles convergence sharh and petrobas reliability styponias of martika deep reinforcement news.com learning approaches. mcquivey We concentrate on macro - actions, and telepathic evaluate these viceroyal on hkgcc different grebenkina Atari lobov 2600 bionix games, singlehandedly where we bormio show capitated that they yield magnetic significant 88.10 improvements in learning speed. dearman Additionally, we show that ciccolo they can even achieve better wieger scores than garnacha DQN. We offer braathen analysis trusses and 17.84 explanation for both convergence sotillo and final devvarman results, revealing 1,050 a problem deep RL kitting approaches have with bvd sparse qutb-ud-din reward magnetohydrodynamic signals.", "histories": [["v1", "Wed, 15 Jun 2016 01:57:40 GMT  (525kb,D)", "http://arxiv.org/abs/1606.04615v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["ishan p durugkar", "clemens rosenbaum", "stefan dernbach", "sridhar mahadevan"], "accepted": false, "id": "1606.04615"}, "pdf": {"name": "1606.04615.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning with Macro-Actions", "authors": ["Ishan P. Durugkar"], "emails": ["idurugkar@cs.umass.edu", "cgbr@cs.umass.edu", "dernbach@cs.umass.edu", "mahadeva@cs.umass.edu"], "sections": [{"heading": "1 Introduction and Related Work", "text": "Since the groundbreaking results shown by Deep Q-Learning in [10, 11] for learning to play games from the Atari Learning Environment [4], there has been extensive research on deep reinforcement learning. Deep Q-learning in particular seeks to approximate the Q-values [21] using deep networks, such as deep convolutional neural networks [10]. There has also been work on modifying the loss used to train the network using more consistent operators [5], or ensuring better target estimation [19]. Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15]. In this paper, we focus on the second category of improvements.\nIn reinforcement learning (RL), temporal abstraction [14] denotes the use of hierarchical multi-step actions that the agent can take in the possible addition to the available primitive actions. In terms of Markov decision processes (MDPs), a well-studied framework for analyzing reinforcement learning algorithms, temporal abstraction allows executing multi-step actions, represented by entire paths in the MDP. An MDP that has been extended to allow modeling such paths is sometimes referred to as a semi-MDP (SMDP) [16]. Since one cannot consider all possible abstractions\u2013the number of such abstractions grows exponentially with their length\u2013learning useful (or meaningful) abstractions has long been an important problem in reinforcement learning [3].\nSome recent work on hierarchical models [8, 1] aim to achieve hierarchical behavior using closed loop control, or options [17]. The drawback of these methods is that they require external supervision, either by explicitly specifying the mapping from states to options [1] or by specifying possible sub-goals [8].\nar X\niv :1\n60 6.\n04 61\n5v 1\n[ cs\n.L G\n] 1\nAnother possible set of abstractions are state-independent, deterministic sequences of actions, called macro-actions or macros [7]. A macro can be chosen as if it was an atomic action, and the agent will then (deterministically) follow the sequence of actions predefined by the macro. Consequently, intermittent states are visited, but are non-actionable by the agent. However, any reward following from the visited state is still accumulated.\nWe explore the effect temporal abstraction has on deep RL by testing the effectiveness of using macros in addition to basic actions. Specifically we extend the Deep Q-Network [11] to use macro actions. It is important to note that even though our evaluation is based on the the original DQN architecture, our technique does not affect the remaining architecture, and can be paired with other improvements as mentioned above. We evaluate the effectiveness of macros, and contrast different approaches to construct them. We then illustrate how a reinforcement learner with access to macros interacts with deep networks, showing how macros can improve reasoning on deep network input representations."}, {"heading": "2 Hierarchical Reinforcement Learning", "text": "Hierarchical RL in general can be motivated along many lines. Among those are a (potential) solution to the \u201ccurse of dimensionality\u201d\u2013the problem that solving a problem becomes exponentially harder with its size. Here, a hierarchical RL agent can offer an elegant divide-and-conquer approach by separating the problem into smaller sub-problems. Further advantages are architectural; a hierarchical learner can add to parametrizable actions, to continuous action- and state spaces and to generalizability of policies. We will discuss the benefits particular to macros in section five.\nAn important distinction in hierarchical RL is between open loop and closed loop models. Closed loop models can be loosely related to a functional perspective: \u201chit your opponent\u201d or \u201cgo find the key, and open that door\u201d. In this case, the path is not described by its internal structure, but by it\u2019s terminal states instead. In terms of temporal abstraction, this problem is most closely modeled by options [17], i.e. stochastic policies defined over a subset of the (S)MDP. However, hierarchies of this kind are difficult to learn, since they require both a composition of atomic actions and a decomposition of a larger goal into smaller subgoals (an illustration of manual goal-decomposition is given by [8]).\nOpen loop models can be related to a more structural perspective: \u201cmove five steps left, three up, and punch\u201d would replace \u201chit the opponent\u201d. This approach is both more and less useful than the first. Once successfully learned, closed loop strategies can be much more useful than open loop strategies, since they are more general.On the other hand, viable sub-goals are hard to define in a general way for an agent to recognize on its own. Generally some sort of supervision is provided to define these sub-goals. In this work, we concentrate on open loop models in the form of macros exclusively.\nNotation and Definitions \u03c0 Policy mapping from states to a probability distribution over actions.\nS,A, s, a Sets of states and actions and an individual state and action, respectively. R(s, a), rt Reward of taking action a in state s and a specific reward received at time t, respectively P (s\u2032|s, a) Transition probability from state s to s\u2032 when taking action a V (s) Value of state s. \u03b3 Discount factor used to determine the effect of future reward.\nQ(s, a) State-action (or Q) function, determining the value of taking action a in state s. M, |M|, `,mi Macro-specific variables (the set of macros, the size of the set, the length of a macro, and the ith macro)"}, {"heading": "2.1 Background", "text": "Hierarchical RL is generaly modeled on semi-MDPs (SMDP). While in an MDP, each state transition is assumed to occur at uniform time intervals (encoding a one-step path), an SMDP is a generalization of an MDP that adds a random variable encoding the time difference in between successive states (thereby possibly encoding paths of arbitrary length). This random variable can be subject to certain additional constraints\u2013it can be real-valued, discrete or subject to any other restrictions defined by the\ndomain. In our case, the domain, the ALE [4], is a discrete time step system that can be interacted with every 1/15 s.1\nAn important consequence of changing an MDP to an SMDP is that the fixed point of the value function for a given policy changes:\nfrom V\u03c0(s) = R(s, a\u03c0) + \u03b3 \u2211\ns\u2032\nP (s\u2032|s, a\u03c0)V\u03c0(s\u2032) (1)\nto V\u03c0(s) = R(s, a\u03c0) + \u2211\ns\u2032,\u03c4\n\u03b3\u03c4P (s\u2032, \u03c4 |s, a\u03c0)V\u03c0(s\u2032) (2)\nwhere \u03c4 represents the possible duration of time taken by a temporally extended action. Consequently, the Q-learning update underlying many deep reinforcement learning algorithms changes\nfrom Qk+1(s, a) = Qk(s, a) + \u03b1k [ rk + \u03b3 amaxQk(s \u2032, a\u2032) ]\n(3)\nto Qk+1(s, a) = Qk(s, a) + \u03b1k [ rt+1 + \u03b3rt+2 + ...\n+ \u03b3\u03c4\u22121rt+\u03c4 + \u03b3 \u03c4 a maxQk(s\u2032, a\u2032)\n] (4)\nwhere rt+1, rt+1, ..., rt+\u03c4 are the rewards received in-between actionable states."}, {"heading": "2.2 Macros", "text": "Macro-actions or macros are predefined sequences of actions that will be taken deterministically [7]. UsingM to refer to the list of all macros, mi to refer to the ith macro within this list and `i to the length of the ith macro, we can then define a macro as:\nmi = \u3008ai,1, ..., ai,`i\u3009 Here, ai,x can be any of the actions available to the agent, generally including other macros although we do not explore this direction in this paper. Additionally, we use |M| to refer to the number of macros.\nAs an example consider the Atari game \u201cBoxing\u201d. In our experiments, we find that sequences of the kind \u201cup, up, punch, down, down\u201d, representing moving in for a punch and pulling out are fixed sequences that can be particularly useful.\nSince we restrict our discussion to the Atari domain for simplicity, and this domain is deterministic, we can make the following observations:\n1. The states Si = \u3008ss, ss+1, ..., ss+`i\u3009 visited by executing macro mi in state ss, can be determined by the sequence of actions in the macro.\n2. Consequently, the terminal state ss+`i is the last state visited, and is determined by the sequence of actions.\n3. The cumulative reward collected is the sum of the rewards of taking the actions defined in the macro, beginning from the original state: Rss,mi = \u2211`i k=1 r(ai,k, ss+k)\nWhile we will discuss the most prominent benefits macros offer later, we want to point out their cost here: Adding macros increases the size of the action space, which worsens the effect of high dimensionality. Using macros also means the agent skips decision making at certain states, which could lead to a sub-optimal policy."}, {"heading": "3 Learning Macros", "text": "In this section, we highlight the different challenges in choosing useful macros, and our modifications to DQN to address them. We also discuss different approaches to vary the shape of the macros, i.e. the sequence of actions contained. While the aim is to learn macros, we include another approach that isu meant as a baseline for later analysis.\n1The internal clock of the ALE runs at 60 frames per second, and each action is active for 4 frames, this extension through time could already be considered a case of temporal abstraction. However, since our learning agent considers the ALE to be a \u201cblack box\u201d, we consider the 1/15 s interactions to be atomic.\nAlgorithm 1 Changes to the DQN incorporating macros repeat \u00b7 Follow the DQN algorithm. if epoch = in K then \u00b7 Compute the new list of macros by the designated policy; if longer than |M |, cut off the last macros. If shorter, fill the list with empty macros and disable them as actions. \u00b7 Add the macros as actions to the DQN algorithm. \u00b7 Reset the observed action sequences.\nend if until epoch = maxNoEpoch\nWhen deciding on open loop sequences of actions, the major parameters to attend to are the length of the macros, the number of macros to use and the exact sequence of actions that make up these macros. Shorter and lower number of macros are not much different from atomic actions, while longer macros might delay the agent\u2019s responses. A higher number of macros will increase the number of actions the agent has to learn over. Once these parameters are decided, we turn to the problem of what actions these macros should be made up of.\nWe test our hypothesis on the Atari [4] domain using an agent similar to the DQN [11]. The input to the agent is the screen image from the ALE, taken in grayscale and resized to an (84\u00d7 84) frame. As in the original DQN, we take 4 consecutive frames to denote one state. These frames are then analyzed by a deep neural net structurally the same as the original DQN, 3 convolution layers, one fully connected layer and one output layer, with corresponding layer sizes except for the output layer. The updates are computed using the variation to RMSProp [18] used by [11]. The only structural difference between the DQN of [11] and our model is that the output layer is expanded to include the macros.\nTo account for the temporal aspect of actions that last longer, when saving transitions in the replay memory we also include \u03c4 , or how much time each transition took, and accumulate rewards as Rt = rt+1, rt+2, ..., rt+\u03c4 . This \u03c4 is then used while calculating the Bellman error. To incorporate the macros, we modify the network by expanding the output layer. The size of the output layer is taken to be |A|+ |M|. We initialize |M| = |A|. If we learn new macros, and the set of macros is smaller than the size of the output layer, we ignore extraneous outputs while choosing the next macro to execute, or while choosing the best Q-value at the next state when calculating the target for the Bellman error. This does not affect the backpropagation of error at any step since the error is only propagated via the output associated with the action taken during that transition. This allows us to change the size of the macros online. We replace macros at certain epochs with increasing gaps in between these replacements, to give the agent more time to exploit this new action-space.\nSince modifying macro assignment with respect to the output layer might mean that an output no longer takes the same sequence of actions it did earlier, the Value for that output needs to be learnt again, and the weights need to be adjusted accordingly. We do this by increasing the exploration factor to = 0.5 and allowing it to decay in accordance with DQN, rather than reinitializing the weights and having the agent learn them from scratch again.\nWe exclude the possibility of macros containing macros, but consider first-order abstractions (i.e. macros built on atomic actions) exclusively, which we append to the list of atomic actions."}, {"heading": "3.1 Repetition of Actions", "text": "In this method, each macro is composed of an atomic action repeated a specified number of times, with a macro corresponding to each action. In effect, this amounts to making the actions more granular, or taking larger steps with the same action. These macros take advantage of the fact that the time step of each action in the Atari domain may be smaller than is actually required to take optimal decisions.\n|M| : chosen as a parameter `i : chosen as a parameter ai,k : ai, ai \u2208 A\nAlgorithm 2 Selecting macros by execution frequency at the end of an epoch in k Initialize an object to count action sequence occurrences, O for i from 0 to |A| \u2212 ` do\nIncrement the counter in O of the sequence A[i : i+ `] by 1 end for Rank all sequences by their occurrence in O Initialize the set of macrosM to contain the highest-occurring sequence repeat\nTake the next highest occurring sequence from O, s\u2217 if the longest common subsequence between s\u2217 and all sequences inM is less than \u03c9` then\nAppend s\u2217 toM end if\nuntil |M| is reached or all sequences in O are exhausted"}, {"heading": "3.2 Frequency", "text": "Here, the macros are chosen by picking sub-sequences of actions of specified length that are repeated the most often during the trajectories since the last update of the list of macros, as long as they are sufficiently different. Difference is measured by the length of the longest common subsequence (lcs) in between two macros. If the lcs between a macro candidate and any other macro is longer than a percentage \u03c9 of the total length of the macros `, then the candidate is dropped. See Algorithm 2 for a more detailed explanation.\n|M| : chosen as a parameter `i : chosen as a parameter \u03c9 : chosen as a parameter\n. . . ai,0, ai+1,1, . . . , ai+n,` . . . is the ith most repeated sequence by the agent if sufficiently different from the [0, i\u2212 1]th most repeated sequences. Sequences repeated most often by the agent can be assumed to be useful to the agent given the current policy."}, {"heading": "4 Experimental Results", "text": "We evaluate this technique on the Arcade Learning Environment [4], a reinforcement learning interface to Atari 2600 games.\nFor our evaluation, we train agents following Algorithm 1. We have trained the agents on 8 games. To showcase the speed of convergence using macros, we show the performance of the agent after 50 million frames. We compare this behavior to our DQN implementation based on [11] trained for 100 million frames, i.e. twice the training period. Testing is done after each epoch (1 million frames) for 200,000 frames to evaluate the agent\u2019s performance. To showcase stability and performance, the performance scores are averaged over 5 epochs, and variance is shown over these as well.\nTo learn macros based on frequency, we allow as much as \u03c9 = 80% overlap and we update these macros with newer learned ones at larger and larger intervals (in epochs 6,13,25 and 50). This is because as an agent starts converging on a policy, changing the macros assigned to the network outputs tends to degrade performance before the agent can relearn the Q-values for the new macros. Larger intervals let the agent acclimatize to the new action-space, and converge on a policy. This can be seen from Figure 1b and 1d, where the red dashed lines indicate epochs where the agent learns new macros. At these points, the performance of the agent tends to go down, and the variance increases, until the macros converge as well .\nThere is some variance in our baseline as compared to [11]. One of the primary reasons is that we are showing the mean of average scores per episode over 5 epochs, whereas in the original paper the result shown was average over the best scores achieved. The results are shown in Table 1.\nThe main advantage we gain from macros is improved convergence times. This is evident from the results shown after training for 50 epochs. The plots over 10 trials of pong and Enduro also show the convergence and stability for both action repetition and frequency analysis based macros."}, {"heading": "5 Analysis", "text": "There are three different measures of performance we can evaluate macros on: scores achieved, the (arguably) most important measure; speed of convergence; and variance of scores over some epochs. To explain the respective performance of macro-actions, we offer and evaluate three hypotheses. The first is the impact macros have on exploration, the second is the impact they have on error-propagation and the third is the approximation precision of the deep representation, with its possible challenge to greedy algorithms (like the Q-learning algorithm used here).\nFrom the results, it can be seen that an agent with access to useful macros performs better than an agent with access only to atomic actions. We analyze this behavior in the context of both speed of convergence and the score achieved."}, {"heading": "5.1 Explanatory Hypotheses", "text": "As mentioned previously, there are two major established benefits to macros which we will discuss now. The first is related to the exploration-exploitation dilemma in reinforcement learning, i.e. the dilemma in between exploiting already gained knowledge and trying something new. Exploratory moves using macros offer larger exploration in a directed manner. Considering that the games in the Atari domain are very complex and consist of millions of states, more efficient exploration should hence offer convergence benefits.\nThe second potential benefit macros offer is the changed propagation of rewards throughout the SMDP. This change stems from macros receiving the cumulative reward along the states visited, thereby propagating rewards from states up to ` steps farther ahead in one iteration; in the MDP setting, this takes up to ` times more iterations, since the reward is only propagated one state in each update. This implies that the Bellman error is equally propagated much faster, allowing for fewer iterations until convergence."}, {"heading": "5.2 Convergence", "text": "Considering that rate of convergence is macros\u2019 most reliable benefit, it does not surprise that the same holds for macros used in the Atari domain. However, there are two potential reasons, first the exploratory bias and second the faster propagation of values. While it is near impossible to perfectly quantify the impact each of these two reasons has, we can still draw some conclusions on how the different macros behave. To do so, we contrasted the performance of the well-defined macros introduced previously with macros randomly populated. These do offer better exploration, since they will force the agent to explore states and regions further away. However, they do not offer substantially better convergence than the pure DQN approach and perform significantly worse than both manual macros and learned macros. Since they do not necessarily offer a higher \u201cspread\u201d over the MDP the propagation of rewards may be less, this suggests that faster convergence may be more heavily influenced by the propagation of values than by the macros\u2019 exploratory bias.\nAdditionally, we can infer that learning macros does indeed learn useful macros, since it offers convergence behavior similar to manual macros, considerably outperforming the basic DQN."}, {"heading": "5.3 Variance", "text": "As can be seen in the results, the final variance of the scores achieved is consistently lower when using macros (for Qbert in particular, the variance is dramatically smaller, while achieving higher final scores). We believe that this results from a combination of better exploration and better propagation of rewards. Using atomic actions only, the agent may not consistently explore areas with higher Q-values, which may lead the learner for finite training time to learn different policies. Higher exploration, and better propagation of the rewards, will make this less likely, since the learner will explore more distant states, and will have states with Q-values differing to choose from. Conclusively, macros lead to a policy where the agent has high confidence for the best action, leading to more stable policies."}, {"heading": "5.4 Scores Achieved", "text": "General observations An agent with access to macros does better on 6 out of the 7 games tested on than an agent with access to only atomic actions. The only game in which the behavior is at or below par with atomic actions is Breakout.\nThis is a surprising result, since coarse control in general should not yield better results. After all, an agent using atomic actions can always choose the same actions as enforced by a macro. We believe that this follows from a third benefit macros offer when combined with a deep network, larger resistance to error when approximating the Q-values of a state. Most games in the Atari domain offer sparse rewards (reward feedback is available after hundreds or in some cases even thousands of frames). Considering discounting and the ability to correct mistakes in intermediate states, this entails that states farther away from a potential reward will have Q-values within a very small \u03b4 around their average. This line of thought becomes obvious when looking at the plots in Figure 2. Figure 2b shows that the \u201cconfidence\u201d of the Q-learner, in terms of difference of the Q-values the greedy argmax chooses from, is orders of magnitude higher when using macros. Figure 2a shows the absolute values for reference. Please note that the behavior shown is for a policy learned in 100 million steps in the DQN, but only in 50 million for the macro case.\nFurther, the Q-value estimates of the network will contain some inherent approximation error. This error combined with the the underlying actual Q-values being similar means that the greedy argmax of a Q-learner will in some cases not be able to pick out the best action. Macros, with steps that change the target states more substantially, will therefore offer Q-value approximations that are more spread out, superseding the approximation error. This leads to more reliable decision making for the Q-learner, which eventually results in better policies learned. This relates macros to advantage learning, since they, too, increase what is known as the action gap [5, 2].\nAdditionally, this benefit is not exclusive to the reinforcement learning part of the deep RL architecture, but affects the neural network that is used to learn a useful representation of the state of the game as well. The network has to learn to differentiate between and meaningfully represent the changes in the state every time an action is taken. When the agent takes temporally extended actions, the network is given the opportunity to learn changes that affect the state in a more noticeable manner. This means that the network can more quickly encode these changes, and more of the training bandwidth is expended in tuning the agent policy.\nApproach-specific observations Considering their respective results, it is interesting how well the simple repetition of actions performs. Being considerably worse in Breakout is most likely a consequence of Breakout\u2019s requirement for fine-grade control. Temporal abstraction as provided by extended sequences of singular actions interferes with this process. However, it performs best for Ms Pacman. The argument to be made here is that Ms Pacman repeats actions by itself (i.e. once \u201cup\u201d is pressed, the agent will move up until it hits a wall). Any fine interference, either by macros of multiple actions, or by atomic actions for exploration, will interrupt this process.\nIt is equally interesting how little influence learning the macros by evaluating their frequency has. Intuitively, this would lead to macros that model specific strategies. However, this only holds true for Boxing, where the agent indeed learns strategies that resemble patterns such as \u201cgo in, punch, and leave\u201d."}, {"heading": "6 Conclusion and Future Work", "text": "We have shown how macro-actions can improve the convergence times and scores of a DQN agent on the Atari domain. Not only do macros improve the efficiency of the agent\u2019s training, but also help the agent achieve better scores on six out of the seven games we tested.\nWe have analyzed the reasons why macros can improve the policies learned by a deep reinforcement learning agent. Useful macros are likely to accelerate the learning of the agent and encourage discovery of optimum policies. Better macro discovery techniques would be a viable next step in this direction. Considering the different aspects of learning macros, we should ideally develop a technique that can not only learn the actions to be taken, but can learn their optimal length and quantity as well.\nAs open loop policies, macros take actions in a specified sequence without looking at the underlying state once it is initiated. While this means that the agent does not need to take decisions as frequently,\nand learns quicker, macros can be seen as a architecture too rigid for many problems. It would hence be interesting to develop a similar way of applying closed loop models, such as options, to a deep reinforcement learning agent and to compare its performance on the Atari domain."}], "references": [{"title": "Classifying options for deep reinforcement learning", "author": ["Kai Arulkumaran", "Nat Dilokthanakul", "Murray Shanahan", "Anil Anthony Bharath"], "venue": "arXiv preprint arXiv:1604.08153,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Reinforcement learning in continuous time: Advantage updating", "author": ["Leemon C Baird III"], "venue": "In Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Andrew G. Barto", "Sridhar Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["Marc G Bellemare", "Georg Ostrovski", "Arthur Guez", "Philip S Thomas", "R\u00e9mi Munos"], "venue": "arXiv preprint arXiv:1512.04860,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Deep Recurrent Q-Learning for Partially Observable MDPs", "author": ["Matthew J. Hausknecht", "Peter Stone"], "venue": "CoRR, abs/1507.06527,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Hierarchical solution of Markov decision processes using macro-actions", "author": ["Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Deep exploration via bootstrapped dqn", "author": ["Ian Osband", "Charles Blundell", "Alexander Pritzel", "Benjamin Van Roy"], "venue": "arXiv preprint arXiv:1602.04621,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Temporal abstraction in reinforcement learning", "author": ["Doina Precup"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S. Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Since the groundbreaking results shown by Deep Q-Learning in [10, 11] for learning to play games from the Atari Learning Environment [4], there has been extensive research on deep reinforcement learning.", "startOffset": 61, "endOffset": 69}, {"referenceID": 10, "context": "Since the groundbreaking results shown by Deep Q-Learning in [10, 11] for learning to play games from the Atari Learning Environment [4], there has been extensive research on deep reinforcement learning.", "startOffset": 61, "endOffset": 69}, {"referenceID": 3, "context": "Since the groundbreaking results shown by Deep Q-Learning in [10, 11] for learning to play games from the Atari Learning Environment [4], there has been extensive research on deep reinforcement learning.", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "Deep Q-learning in particular seeks to approximate the Q-values [21] using deep networks, such as deep convolutional neural networks [10].", "startOffset": 133, "endOffset": 137}, {"referenceID": 4, "context": "There has also been work on modifying the loss used to train the network using more consistent operators [5], or ensuring better target estimation [19].", "startOffset": 105, "endOffset": 108}, {"referenceID": 18, "context": "There has also been work on modifying the loss used to train the network using more consistent operators [5], or ensuring better target estimation [19].", "startOffset": 147, "endOffset": 151}, {"referenceID": 19, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 154, "endOffset": 165}, {"referenceID": 5, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 154, "endOffset": 165}, {"referenceID": 12, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 154, "endOffset": 165}, {"referenceID": 4, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 224, "endOffset": 231}, {"referenceID": 18, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 224, "endOffset": 231}, {"referenceID": 8, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 314, "endOffset": 321}, {"referenceID": 11, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 314, "endOffset": 321}, {"referenceID": 14, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 378, "endOffset": 382}, {"referenceID": 13, "context": "In reinforcement learning (RL), temporal abstraction [14] denotes the use of hierarchical multi-step actions that the agent can take in the possible addition to the available primitive actions.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "An MDP that has been extended to allow modeling such paths is sometimes referred to as a semi-MDP (SMDP) [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 2, "context": "Since one cannot consider all possible abstractions\u2013the number of such abstractions grows exponentially with their length\u2013learning useful (or meaningful) abstractions has long been an important problem in reinforcement learning [3].", "startOffset": 228, "endOffset": 231}, {"referenceID": 7, "context": "Some recent work on hierarchical models [8, 1] aim to achieve hierarchical behavior using closed loop control, or options [17].", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "Some recent work on hierarchical models [8, 1] aim to achieve hierarchical behavior using closed loop control, or options [17].", "startOffset": 40, "endOffset": 46}, {"referenceID": 16, "context": "Some recent work on hierarchical models [8, 1] aim to achieve hierarchical behavior using closed loop control, or options [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "The drawback of these methods is that they require external supervision, either by explicitly specifying the mapping from states to options [1] or by specifying possible sub-goals [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "The drawback of these methods is that they require external supervision, either by explicitly specifying the mapping from states to options [1] or by specifying possible sub-goals [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 6, "context": "Another possible set of abstractions are state-independent, deterministic sequences of actions, called macro-actions or macros [7].", "startOffset": 127, "endOffset": 130}, {"referenceID": 10, "context": "Specifically we extend the Deep Q-Network [11] to use macro actions.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "In terms of temporal abstraction, this problem is most closely modeled by options [17], i.", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "However, hierarchies of this kind are difficult to learn, since they require both a composition of atomic actions and a decomposition of a larger goal into smaller subgoals (an illustration of manual goal-decomposition is given by [8]).", "startOffset": 231, "endOffset": 234}, {"referenceID": 3, "context": "In our case, the domain, the ALE [4], is a discrete time step system that can be interacted with every 1/15 s.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "Macro-actions or macros are predefined sequences of actions that will be taken deterministically [7].", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "We test our hypothesis on the Atari [4] domain using an agent similar to the DQN [11].", "startOffset": 36, "endOffset": 39}, {"referenceID": 10, "context": "We test our hypothesis on the Atari [4] domain using an agent similar to the DQN [11].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "The updates are computed using the variation to RMSProp [18] used by [11].", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "The updates are computed using the variation to RMSProp [18] used by [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "The only structural difference between the DQN of [11] and our model is that the output layer is expanded to include the macros.", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "We evaluate this technique on the Arcade Learning Environment [4], a reinforcement learning interface to Atari 2600 games.", "startOffset": 62, "endOffset": 65}, {"referenceID": 10, "context": "We compare this behavior to our DQN implementation based on [11] trained for 100 million frames, i.", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "There is some variance in our baseline as compared to [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "This relates macros to advantage learning, since they, too, increase what is known as the action gap [5, 2].", "startOffset": 101, "endOffset": 107}, {"referenceID": 1, "context": "This relates macros to advantage learning, since they, too, increase what is known as the action gap [5, 2].", "startOffset": 101, "endOffset": 107}], "year": 2016, "abstractText": "Deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks, such as the Atari domain. In this paper, we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. We concentrate on macro-actions, and evaluate these on different Atari 2600 games, where we show that they yield significant improvements in learning speed. Additionally, we show that they can even achieve better scores than DQN. We offer analysis and explanation for both convergence and final results, revealing a problem deep RL approaches have with sparse reward signals.", "creator": "LaTeX with hyperref package"}}}