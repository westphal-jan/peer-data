{"id": "1606.04624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Finite-time Analysis for the Knowledge-Gradient Policy", "abstract": "elementaries We r\u00f8yken consider sequential decision hater problems in which gerik we adaptively adjustable choose one reines of finitely 400s many dooks alternatives iye and queued observe aliaga a marly stochastic reward. We offer montri a korvald new perspective of cybersex interpreting acers Bayesian devenish ranking and 48.44 selection ulvaeus problems a26 as bundock adaptive towanda stochastic multi - citied set peeved maximization skycam problems and derive pavlin the first newsworthiness finite - de-registered time bound okhla of the pyshkin knowledge - gradient policy baehring for pc1 adaptive 31m submodular embryologist objective nr1 functions. In seehausen addition, schodack we ujiri introduce the 50-euro concept of prior - buyo optimality and hi-5 provide tapani another insight s-300s into the hemorrhage performance of 63.79 the grenz knowledge gradient policy parchments based on the submodular boned assumption on 23.69 the value of mistakes information. 38.69 We demonstrate junnar submodularity for the two - alternative case and provide gm other tu\u1ea5n conditions for more twal general problems, rickshaw bringing solazyme out belmadi the issue ulick and fpp importance denish of submodularity in harrier learning problems. falconer Empirical leaned experiments leite are unreasoning conducted to further illustrate the haldon finite time platos behavior murrey of katsuta the knowledge gradient panh policy.", "histories": [["v1", "Wed, 15 Jun 2016 02:44:02 GMT  (2199kb,D)", "http://arxiv.org/abs/1606.04624v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yingfei wang", "warren powell"], "accepted": false, "id": "1606.04624"}, "pdf": {"name": "1606.04624.pdf", "metadata": {"source": "CRF", "title": "Finite-time Analysis for the Knowledge-Gradient Policy", "authors": ["Yingfei Wang", "Warren Powell"], "emails": ["yingfei@cs.princeton.edu", "powell@princeton.edu"], "sections": [{"heading": "1 Introduction", "text": "We consider sequential decision problems in which at each time step, we choose one of finitely many alternatives and observe a random reward. The rewards are independent of each other and follow some unknown probability distribution. One goal can be to identify the alternative with the best expected performance within a limited measurement budget, which is the objective of Bayesian ranking and selection problems. Ranking and selection problems are examples of sequential decision making problems with partial information that address the exploration-exploitation trade-off. Since the learner does not know the true distribution of each alternative, it needs to explore the\n\u2217Department of Computer Science, Princeton University, Princeton, NJ 08540, USA, yingfei@cs.princeton.edu \u2020Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08540, USA, powell@princeton.edu\nar X\niv :1\n60 6.\n04 62\n4v 1\n[ cs\n.L G\n] 1\n5 Ju\nn 20\nchoices that might give good rewards in the future as well as exploit the alternatives that appear to be better based on previous observations.\nRanking and selection problems arise in many settings. We may have to choose a type of material that has the best performance, the features in a laptop or car that produce the highest sales, or the molecular combination that produces the most effective drug. Often, the cost of a measurement may be substantial. Laboratory or field experiments may take a day or several weeks. For this reason, we assume we have a limited budget for making measurements.\nRaiffa and Schlaifer established the Bayesian framework for R&S problems [33]. Several two-stage and sequential procedures exist for selecting the best alternative. Branke et. al made a thorough comparison of several fully sequential sampling procedures [6]. They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10]. Another single-step Bayesian look-ahead policy first introduced by [19] and then further studied by [15] is called the \u201cknowledge-gradient policy\u201d (KG). It chooses to measure the alternative that maximizes the single-period expected value of information. Whereas the above mentioned policies assumed an independent normal or one-dimensional Wiener process prior on the alternatives\u2019 true means, Frazier et. al modified the knowledge-gradient policy to handle correlated multivariate normal belief on the mean values of these rewards [13].\nA similar field is the multi-armed bandit problem, which were originally studied under Bayesian assumptions [17]. A widely used class of policies for multi-armed bandit problems is called upper confidence bounding policies (UCB). Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7]. By contrast, knowledge gradient policies, which enjoy some nice theoretical properties, have never been characterized by the type of regret bounds for which UCB policies are famous.\nThis paper makes the following contributions: We first establish the connection between Bayesian ranking and selection problem and adaptive stochastic multi-set function maximization problems where each multi-set corresponds to a set of selected alternatives. The multi-set representation captures our ability to evaluate the same alternative more than once. This new perspective offers a new line of analysis for the properties of value-of-information policies. We derive the first finite-time bound for the knowledge gradient policy for R&S problems under the assumption that the utility function is adaptive submodular. However, pathwise adaptive submodularity can fail in offline learning settings when the utility function itself involves a maximum. To this end, instead of the pathwise behavior analyses of the utility function, we further study its average behavior by taking expectations over the observations given any fixed sample allocation, resulting in a well-known quantity: the value of information. As a result, we introduce the concept of the prior-value of a policy and analyze the prior-optimality of the KG policy to provide another insight into its performance based on the submodular assumption\nof the value of information that is weaker than adaptive submodularity. To accomplish this, we build on the general structure of the analysis of greedy algorithms given in [31] and [18]. We demonstrate submodularity for the two-alternative case and provide other conditions for more general problems, filling in a gap in the analysis of the knowledge gradient policy. Finally, we propose experiments to illustrate our theoretical analysis on the finite time behavior of the knowledge gradient policy. We further compare the KG policy with other policies with or without theoretical guarantees. Aside from the fact that the KG policy performs competitively with or significantly better than other policies especially in early iterations, we draw the conclusion that there is no universal best policy for all problem classes, which means that theoretical guarantees are not by themselves reliable indicators of which policy is best for a particular problem class and empirical experiments are needed to better understand their finite time performance.\nThis paper is organized as follows. In section 2, we lay out the mathematical models for Bayesian ranking and selection problems. In section 3, we describe the knowledge gradient policies. In section 4, we provide finite-time analyses of the knowledge gradient policy from two directions: the posterior optimality and the prior optimality. In section 5, we analyze the submodularity of the two-alternative case and provide other conditions for more general problems, bringing out the issue and importance of submodularity in leaning problems. Finally, in section 6 we present finite-time performance results and analyses of various policies for R&S problems."}, {"heading": "2 Ranking and Selection Problems", "text": "Suppose we have a collection X of M alternatives (where M might be quite large), each of which can be measured sequentially to estimate its unknown mean \u00b5x. We assume normally distributed measurement noise with known variance \u03c32W . We first introduce the model for independent normal beliefs. We begin with a normally distributed Bayesian prior belief on the sampling means that is independent across alternatives, \u00b5x \u223c N (\u03b80x, \u03c30x). At the nth iteration, we use some measurement policy \u03c0 to choose one alternative xn and observe W n+1xn \u223c N (\u00b5xn , \u03c3W ).\nFor convenience, we introduce the \u03c3-algebras Fn for any n = 0, 1, ..., N \u2212 1 which is formed by the previous n measurement choices and outcomes, x0,W 1, ..., xn\u22121,W n. We define \u03b8nx = E[\u00b5x|Fn] and (\u03c3nx)2 = Var[\u00b5x|Fn]. Then conditionally on Fn, \u00b5x \u223c N (\u03b8nx , \u03c3nx). Let \u03b2nx = 1(\u03c3nx )2 be the conditional precision of \u00b5x and our state of knowledge be Sn = (\u03b8nx , \u03b2 n x )x\u2208X . We will use Fn and Sn interchangeably. After the nth measurement we update our beliefs using Bayes\u2019 rule:\n\u03b8n+1x =\n{ \u03b2nx \u03b8 n x+\u03b2 WWn+1\n\u03b2n+\u03b2W if xn = x\n\u03b8nx otherwise, \u03b2n+1x = { \u03b2n + \u03b2W if xn = x \u03b2nx otherwise,\nwhere \u03b2W = 1/\u03c32W .\nWe may impose correlated beliefs between alternatives in order to strengthen the effect of each measurement. Starting from a prior distribution N (\u03b80,\u03a30) and after measurement W n+1 of alternative x, a posterior distribution on the beliefs are calculated by:\n\u03b8n+1 = \u03a3n+1 ( (\u03a3n)\u22121 \u03b8n + \u03b2WW n+1ex ) , (1)\n\u03a3n+1 = ( (\u03a3n)\u22121 + \u03b2W exe T x )\u22121 , (2)\nwhere ex is the vector with 1 in the entry corresponding to alternative x and 0 elsewhere. Sn = (\u03b8n,\u03a3n) is then our state of knowledge in this case.\nA decision function X\u03c0(Sn) is defined as a mapping from the knowledge state to X . We refer to the decision function X\u03c0 and the policy \u03c0 interchangeably.\nIf we are limited to N measurements, the objective is to maximize the expected reward of the final recommended alternative:\nmax \u03c0\u2208\u03a0\nE [\u00b5x\u03c0 ] , (3)\nwhere x\u03c0 = arg maxx\u2208X \u03b8 N x and x n = X\u03c0(Sn) for 0 \u2264 n < N ."}, {"heading": "3 Knowledge Gradient", "text": "For R&S problems, the knowledge gradient is a policy that at the nth iteration chooses its (n + 1)st measurement from X to maximize the single-period expected increase in value [15, 13]. To be more specific, the value of being in state Sn is maxx\u2208X \u03b8 n x . If we choose to measure xn = x right now, allowing us to observe W n+1x , then we transition to a new state of knowledge Sn+1 = (\u03b8n+1,\u03a3n+1). At iteration n, \u03b8n+1x is a random variable since we do not yet know what W n+1 is going to be. We would like to choose x at iteration n which maximizes the expected value of maxx\u2208X \u03b8 n+1 x . We can think of this as choosing an alternative to maximize the incremental value, given by\n\u03bdKG,nx = E[max x\u2032 \u03b8n+1x\u2032 \u2212max x\u2032 \u03b8nx\u2032|xn = x, Sn]. (4)\nThe knowledge gradient policy XKG(Sn) is defined by\nXKG(Sn) = arg max x\u2208X \u03bdKG,nx . (5)\nThe knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].\nThe knowledge gradient policy has some nice properties. For Bayesian ranking and selection problems, the knowledge gradient policy is optimal (by definition) if the measurement budget N = 1. The knowledge gradient is guaranteed to find the best alternative as the measurement budget N tends to infinity. If there are only two choices,\nthe knowledge gradient policy is optimal for any measurement budget. The knowledge gradient policy is the only stationary policy that is both myopically and asymptotically optimal. However, the KG has not enjoyed the finite-time bounds that have been popular in the UCB policies."}, {"heading": "4 Finite-time Analysis of the Knowledge Gradient", "text": "Policy\nWe follow the general structure of the analysis of greedy approximation [31] to develop the first finite-time bound for the knowledge gradient policy for R&S problems as follows. In Section 4.1, by interpreting the Bayesian R&S problems as the adaptive stochastic multi-set maximization problems, we show that the KG policy inherits precisely the performance guarantees of the greedy algorithm for classic submodular maximization problems if the utility function is adaptive submodular. We theoretically analyze the adaptive submodular assumption and point out that it can fail in the ranking and selection problems. In such cases, instead of the pathwise behavior analyses of the utility function, we study its average behavior by taking expectation over the observations in Section 4.2. In Section 4.3, we analyze the prior-optimality which provides another insight into the performance of the KG policy based on the submodularity of a wellunderstood quantity: value of information.\nIt is important to note that both the submodular maximization reduction and the theoretical analyses on the prior-optimality are not limited to the specific setup of Gaussian noise in observations and Gaussian prior structure. The theoretical guarantees are more generally applicable to any prior and measurement noise model as long as the adaptive submodular assumption or the submodular value of information assumption holds."}, {"heading": "4.1 The Reduction of R&S to Adaptive Stochastic Multi-set", "text": "Maximization\nWe first introduce the adaptive stochastic maximization problem. Let E be a finite set of items. Each item e \u2208 E maps to a random outcome of a measurement \u03a6(e) in a set O of possible values. We define a realization as a function \u03c6 : E 7\u2192 O representing the observation of each item in the ground set. Under Bayesian interpretation, we assume that there is a known prior probability distribution p(\u03c6) := P(\u03a6 = \u03c6) over all possible realizations. The adaptive stochastic optimization problem consists of sequentially picking an item e \u2208 E, revealing its outcome \u03a6(e) and picking the next item. After each pick, the observations so far can be represented as a partial realization \u03c8. A partial realization \u03c8 is consistent with realization \u03c6, denoted as \u03c6 \u223c \u03c8, if all the items selected in \u03c8 have the same outcomes as in \u03c6. We use dom(\u03c8) to refer to the items observed\nin \u03c8. We use the notation Z\u03c0(\u03c6) to denote the set of items chosen by policy \u03c0 under realization \u03c6.\nWe wish to maximize some utility function f : 2E \u00d7 OE 7\u2192 R that depends on which items we pick and which states they are in. The expected utility of a policy \u03c0 is favg(\u03c0) := E [ f ( Z\u03c0(\u03a6),\u03a6 )] where the expectation is taken over the prior distribution p(\u03c6). The goal of adaptive stochastic set maximization problem is to find an optimal policy \u03c0\u2217 that maximizes its expected utility under a cardinality constraint,\n\u03c0\u2217 \u2208 arg max \u03c0 favg(\u03c0), subject to |Z\u03c0(\u03c6)| \u2264 N,\nwhere N is the measurement budget. It is not obvious to treat the ranking and selection problem in an adaptive stochastic multi-set maximization way of thinking. To see this, define the ground set E = X . The outcomes are real numbers with O = R. Each alternative e = x can be selected multiple times. After each selection, its random outcome \u03a6(e) = Wx \u2208 O is revealed.\nSince the true values \u00b5x are random variables, we can let \u03d5 be a sample realization of the truth with a (correlated) prior distribution p(\u03d5) = N (\u03b80,\u03a30). We use the notation \u03c6 \u2208 \u03a6 to denote an realization of the random observations in our problem. The prior probability distribution over the realizations is determined by p(\u03d5) and the noise distribution N (0, \u03c3W ). For example, if in the ranking and selection problems each alternative can only be selected once, \u03c6 : E 7\u2192 O. For multi-selections, one way of defining the realization is by first making replicas of each item to construct E \u2032 and then selecting each e\u2032 \u2208 E \u2032 at most once.\nConsider any sampling allocation z = (zx)x\u2208X , by which we measure alternative x for zx \u2208 N times. We use Z to represent its corresponding multi-set. We use Z\u03c0(\u03c6) : \u03a6 7\u2192 (X \u00d7N) to refer to the alternatives selected by \u03c0 under realization \u03c6. Let \u03b8n be our vector of estimates of the means after n measurements according to allocation Z under realization \u03c6, where |Z| = n. \u03b8n can be obtained according to the updating equation (1) and (2), and does not depend on the order of the allocations. It can thus be denoted as \u03b8n(Z, \u03c6) : (X \u00d7 N) \u00d7 \u03a6 7\u2192 RM . The next lemma states the equivalence of E[\u00b5x\u03c0 ] and E[maxx \u03b8Nx ]. Hence, the utility function f : (X \u00d7 N) \u00d7 \u03a6 7\u2192 R can be defined as maxx \u03b8 n x(Z, \u03c6) and favg(\u03c0) := E [ maxx \u03b8 N x ( Z\u03c0(\u03a6),\u03a6 )] . The R&S objective (3) can then be re-written as \u03c0\u2217 \u2208 arg max\n\u03c0 favg(\u03c0), subject to |Z\u03c0(\u03c6)| \u2264 N.\nLemma 4.1 ([32]). Let \u03c0 be a policy, and let x\u03c0 = arg maxx \u03b8 N x be the alternative selected by the policy. Then E[\u00b5x\u03c0 ] = E[max\nx \u03b8Nx ].\nThe definition of the knowledge gradient \u03bdKG,nx coincides with the Conditional Expected Marginal Benefit \u2206(e|\u03c8) defined by [18]:\n\u2206(e|\u03c8) := E [ f ( dom(\u03c8) \u222a {e},\u03a6 ) \u2212 f ( dom(\u03c8),\u03a6 ) |\u03a6 \u223c \u03c8 ] .\nThe knowledge gradient policy is thus in fact the adaptive greedy policy with uniform item costs, with a slight difference in the ability of selecting each item more than once. We generalize the definition of adaptive monotonicity and adaptive submodularity for set functions given by [18] to multi-set functions as follows.\nDefinition 4.2 (Adaptive Monotonicity). A function f : (X \u00d7N)\u00d7\u03a6 7\u2192 R is adaptive monotone with respect to distribution p(\u03c6) if the conditional expected marginal benefit of any item is nonnegative: for all \u03c8 and all x \u2208 X .\n\u2206(x|\u03c8) \u2265 0.\nDefinition 4.3 (Adaptive Submodularity). A function f : (X\u00d7N)\u00d7\u03a6 7\u2192 R is adaptive submodular with respect to distribution p(\u03c6) if for all \u03c8 and \u03c8\u2032 such that dom(\u03c8) \u2286 dom(\u03c8\u2032) and both \u03c8, \u03c8\u2032 are consistent with some realization \u03c6 (i.e. \u03c8 \u2286 \u03c8\u2032), we have the conditional expected marginal benefit of any fixed item x \u2208 X does not increase as more items are selected and observed,\n\u2206(x|\u03c8) \u2265 \u2206(x|\u03c8\u2032).\nLet \u03c0\u2217 be the optimal policy to R&S problems. If f := maxx \u03b8 n x(Z, \u03c6) is adaptive\nmonotone and adaptive submodular with respect to the prior distribution p(\u03c6), then\nfavg(KG) > (1\u2212 e\u22121)favg(\u03c0\u2217).\nWe next show that the instances generated by ranking and selection problems are adaptive monotone.\nLemma 4.4. In ranking and selection problems, the utility function maxx \u03b8x is adaptive monotone with any Gaussian prior.\nProof. For any \u03c8, let n = |\u03c8|. Then for any item x \u2208 X , \u2206(x|\u03c8) can be rewritten as E[maxx\u2032 \u03b8n+1x\u2032 \u2212 maxx\u2032 \u03b8nx\u2032 |xn = x,Fn] = \u03bdKG,nx . Since for any x, \u03b8n+1x = \u03b8nx + \u03c3\u0303(\u03a3n, xn)Zn+1, where \u03c3\u0303(\u03a3, x) = \u03a3ex\u221a\n1/\u03b2W+\u03a3xx and the random variable Zn+1 is standard\nnormal when conditioned on Fn [13]. Hence we have E[\u03b8n+1x\u2032 |xn = x,Fn] = \u03b8nx\u2032 for any x\u2032. By Jensen\u2019s inequality, we have \u2206(x|\u03c8) = \u03bdKG,nx \u2265 0.\nEven though intuition suggests that the utility function should be adaptive submodular in the amount of information collected, as we collect more information it is natural to expect that the marginal value of this information should decrease, yet it is not always the case as shown in the next lemma. The proof can be found in Appendix A.1.\nLemma 4.5. For any independent normal prior distribution p(\u03d5) and nondegenerated noise distribution (i.e. \u03c3W 6= 0), there exists \u03c8, \u03c8\u2032 and x \u2208 X such that \u03c8 \u2286 \u03c8\u2032 and \u2206(x|\u03c8) < \u2206(x|\u03c8\u2032).\nIt can be seen that the adaptive submodular assumption can fail in the ranking and selection problems with the special utility function f = maxx \u03b8 n x(Z, \u03c6) that involves maximization itself. Hence, instead of the above pathwise behavior analyses of the utility function, we would like to study its average behavior by taking the expectation over the observations given any fixed sample allocation Z in the next section."}, {"heading": "4.2 The Value of Information", "text": "We define the pathwise value of information v\u0302(Z, \u03c6) as the incremental improvement over the best expected value that can be obtained without measurement, which is maxx\u2208X \u03b8 0 x,\nv\u0302(Z, \u03c6) := max x\u2208X \u03b8nx(Z, \u03c6)\u2212max x\u2208X \u03b80x.\nThe value of information v(Z) is then defined to be\nv(Z) := E\u03a6[v\u0302(Z,\u03a6)],\nwhere the expectation is taken over the prior distribution p(\u03c6). The value of information has a long history spanning the literatures of several disciplines. Stigler considers the value of information in economics when buyers search for the best price [35]. Howard laid the groundwork for the value of information in a decision-theoretic context and spawned a great deal of work in this area [22]. Yokota and Thompson gives a first comprehensive review of value of information analyses related to health risk management [37]. Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].\nSince the value of information is a multi-set function, we first generalize the definitions and properties of submodular set functions described by [31] to submodular multi-set functions.\nDefinition 4.6. Given a finite set E, a real-valued function g on the set of multi-sets over E is called submodular if for all multi-sets S and T whose elements belong to E,\n\u03c1x(S) \u2265 \u03c1x(T ),\u2200S \u2286 T,\u2200x \u2208 E,\nwhere \u03c1x(S) , g(S \u222a {x}) \u2212 g(S) is the incremental value of adding element x to the multi-set S.\nProposition 1. Each of the following statements is equivalent and defines a submodular multi-set function (S pathwiseand T are multi-sets on E, x, y \u2208 E):\n1. \u03c1x(S) \u2265 \u03c1x(T ),\u2200S \u2286 T and \u2200x.\n2. \u03c1x(S) \u2265 \u03c1x(S \u222a {y}),\u2200S, x, y.\n3. g(T ) \u2264 g(S) + \u2211 x\u2208T\u2212S \u03c1x(S)\u2212 \u2211 x\u2208S\u2212T \u03c1x(S \u222a T \u2212 {x}),\u2200S, T .\n4. g(T ) \u2264 g(S) + \u2211\nx\u2208T\u2212S \u03c1x(S),\u2200S \u2286 T .\nThis proposition follows from a similar proof of Proposition 2.1 in [31]. For completeness we provide the proof in Appendix A.2.\nIt is obvious that if \u03b8nx(Z, \u03c6) is adaptive monotone or adaptive submodular with respect to p(\u03c6), then so does v\u0302(Z, \u03c6). It is also easy to show that if \u03b8nx(Z, \u03c6) is adaptive monotone or adaptive submodular with respect to p(\u03c6), then by the law of total expectation, i.e. E[E[U |V ]] = E[U ] for any random variables U and V , the value of information v(Z) is monotone or submodular. We close this section by showing the monotonicity of the multi-set function v and leave the analyses of submodularity in Section 5.\nLemma 4.7. (Monotonicity of the value of information) For any sampling allocation Z1 and Z2, if Z1 \u2286 Z2, then v(Z1) \u2264 v(Z2).\nProof. We prove the monotonicity of v by showing v(Z) \u2264 v(Z \u222a {xn+1}) for any allocation Z (with \u2211 x\u2208X zx = n) and any additional measurement x\nn+1. By the tower property,\nv(Z \u222a {xn+1})\u2212 v(Z) = E\u03a6[v\u0302(Z \u222a {xn+1})]\u2212 E[v\u0302(Z)] = E\u03a6[max\nx\u2208X \u03b8n+1x (Z \u222a {xn+1})]\u2212 E[max x\u2208X \u03b8nx(Z)]\n= E\u03a6[E[max x \u03b8n+1x (Z \u222a {xn+1})\u2212max x \u03b8nx(Z)|\u03a6 \u223c \u03c8Z ]] = E\u03a6[\u03bdKG,nx ],\nwhere \u03c8Z is the partial realization with dom(\u03c8Z) = Z. The lemma follows from the adaptive monotonicity, \u03bdKG,nx \u2265 0."}, {"heading": "4.3 Guarantees on the Prior-optimality of the Knowledge Gra-", "text": "dient Policy\nThere are two ways to evaluate the value of a policy. The first, which we call the posterior view, conditions on the allocation Z = Z\u03c0(\u03a6) that would have occurred under policy \u03c0 for each sample path \u03c6 \u2208 \u03a6. This is the more conventional approach for evaluating policies. The second, which we call the prior view, starts by characterizing the value of an arbitrary allocation Z (before we have seen any sample realizations).\nMore formally, the classical way to estimate the value of a policy is to calculate the incremental improvement over what we could do before we collect any information, is given by\nf \u2032avg(\u03c0) = E[f(Z\u03c0(\u03a6),\u03a6)]\u2212max x \u03b80x.\nWe let P(\u03c0 Z) be the probability that policy \u03c0 produces allocation Z. Since with a fixed budget of N measurements, there are only finite choices of possible allocations, using the tower property, we can condition on the allocation Z\u03c0 = Z which gives us\nf \u2032avg(\u03c0) = \u2211 Z\u2208ZN P(\u03c0 Z) ( E[max x \u03b8nx(Z \u03c0(\u03a6),\u03a6)|Z\u03c0 = Z]\u2212max x \u03b80x ) .\nWe note that in this method for evaluating a policy (which is the standard method), we only consider allocations Z that are actually produced by policy \u03c0 for the outcomes in \u03c6. This approach makes it much more difficult to understand the relationship between the allocation Z and the value of a policy.\nFor this reason, we adopt a different method of evaluating a policy which we term the prior view. Since this idea is new, we define it formally as follows\nDefinition 4.8 (The prior-value of a policy). Let Zn be the set of all possible allocations with a limited budget n. The value of a policy \u03c0 with N measurements is defined as\nF \u03c0 = \u2211 Z\u2208ZN P(\u03c0 Z) ( E\u03a6[max x \u03b8nx(Z,\u03a6)]\u2212max x \u03b80x ) =\n\u2211 Z\u2208ZN P(\u03c0 Z)v(Z).\nIn this view, we use the prior probability of an outcome p(\u03c6) instead of the posterior p(\u03c6|Z\u03c0(\u03c6) = Z) which is conditioned on an allocation Z. The value of this approach is that it writes the value of a policy directly as a function of v(Z), making it easier to study the effect of the properties of v(Z) on the value of a policy. Intuitively, since a policy could generate different allocations Z for different sample realizations, it is natural to define the value of a policy \u03c0 as the weighted sum of the expected value of information based on all possible allocations Z and the weight should be the probability of occurrence of Z based on policy \u03c0.\nWe make the following assumption which is weaker than the adaptive submodularity assumption and will analyze it further in Section 5.\nAssumption 1. The value of information v is a submodular multi-set function on the set of alternatives X with respect to the prior distribution p(\u03c6).\nLet \u03c0\u2217 be the optimal sequential policy under a budget of N measurements in the sense that the prior-value of \u03c0\u2217 is the largest. We call it prior-optimality. In what follows, we first bound KG\u2019s sub-prior-optimality in Proposition 4.12:\nF \u03c0 \u2217 \u2264 FKG[n]@\u03c0\u2217 \u2264 FKG[n\u22121] +N(FKG[n] \u2212 FKG[n\u22121]), n = 1, 2, ..., N.\nThen we derive the worst-case bound for the KG policy in Theorem 4.14:\nFKG F \u03c0\u2217 \u2265 1\u2212 (N \u2212 1 N )N \u2265 e\u2212 1 e \u2248 0.632.\nBesides the posterior optimality bound obtained from adaptive stochastic multi-set maximization, the prior-optimality provides another insight into the performance of the KG policy based on a well-understood quantity: value of information.\nDefinition 4.9 (Policy concatenation). [18] A concatenated policy \u03c0 = \u03c01@\u03c02 is constructed by running \u03c01 to completion, and then running policy \u03c02 from a fresh start ignoring all the information collected while running \u03c01.\nTo be more specific, suppose \u03c0i has a budget of ni, i = 1, 2, the first phase is to run \u03c01 for n1 iterations starting from S\n0 and we get a sample realization including decisions and their corresponding measurements. The second phase is to run \u03c02 for n2 measurements starting from S0 and we get another sample realization. Thus the sample realization of the concatenated process is all the decisions and their corresponding measurements collected in two phases. Note here, when running the second policy, we ignore all the information collected during running the first one, but when calculating the value of \u03c01@\u03c02, F \u03c01@\u03c02 , we use all the information collected in two phases.\nDefinition 4.10 (Policy truncation). [18] For a policy \u03c0, define the j-truncation \u03c0[j] of \u03c0 as the policy that runs exactly (j + 1) steps under \u03c0\u2019s decision rule and \u03c0{j} as the single step policy that randomly chooses an alternative according to the probability distribution of policy \u03c0\u2019s decision for the (j + 1)-th step.\nWe now show that the value of \u03c01 is no larger than the value of \u03c01@\u03c02.\nLemma 4.11. F \u03c01 \u2264 F \u03c02@\u03c01 for all policies \u03c01 and \u03c02 under any prior and probability distribution that describes a measurement.\nProof. We first show that F \u03c01@\u03c02 = F \u03c02@\u03c01 . In a concatenated policy, the two phases are independent since no information is shared among the two phases. Hence for a given allocation pair (Z1, Z2) where Z1 \u2208 Zn1 , Z2 \u2208 Zn2 , we have\nP(\u03c01@\u03c02 (Z1, Z2)) = P(\u03c01 Z1)P(\u03c02 Z2) = P(\u03c02 Z2)P(\u03c01 Z1) = P(\u03c02@\u03c01 (Z2, Z1)).\nF \u03c01@\u03c02 = F \u03c02@\u03c01 follows immediately from taking the sum over all possible pairs of (Z1, Z2)) such that Z2 \u222a Z1 = Z for any fixed allocation Z.\nTherefore F \u03c01 \u2264 F \u03c01@\u03c02 holds if and only if F \u03c01 \u2264 F \u03c02@\u03c01 . We then finish this proof by showing F \u03c01 \u2264 F \u03c01@\u03c02 . We write F \u03c01@\u03c02 \u2212 F \u03c01 as a telescoping sequence\nF \u03c01@\u03c02 \u2212 F \u03c01 = \u2211\nZ\u2208Zn1+n2\nv(Z)P(\u03c01@\u03c02 Z)\u2212 \u2211\nZ1\u2208Zn1 v(Z1)P(\u03c01 Z1)\n= \u2211\nZ\u2208Zn1+n2 \u2211 Z1\u222aZ2=Z v(Z)P(\u03c01 Z1)P(\u03c02 Z2)\n\u2212 \u2211\nZ1\u2208Zn1 \u2211 Z2\u2208Zn2 v(Z1)P(\u03c01 Z1)P(\u03c02 Z2)\n= \u2211\nZ1\u2208Zn1 \u2211 Z2\u2208Zn2 [ v(Z1 \u222a Z2)\u2212 v(Z1) ] P(\u03c01 Z1)P(\u03c02 Z2)\n\u2265 0,\nwhere the second equality holds due to the same reason as in the proof above for F \u03c01@\u03c02 = F \u03c02@\u03c01 and the third equality is just the same summation in different orders. The last inequality holds because of the monotonicity of multi-set function v.\nBased on the monotonicity of v and a similar argument as in Proposition 4.11, F is non-decreasing with respect to the number of measurements. Thus the more measurements, the better the policy. Hence \u03c0\u2217 has exactly N measurements. We have the following sub-optimality bound on KG\u2019s prior-value. For a proof see Appendix A.3.\nProposition 4.12. Let \u03c1KG,n = FKG [n] \u2212 FKG[n\u22121] , then\nF \u03c0 \u2217 \u2264 F KG[n\u22121]@\u03c0\u2217 \u2264 F KG[n\u22121] +N\u03c1KG,n\n= n\u22121\u2211 i=0 \u03c1KG,i +N\u03c1KG,n, n = 0, 1, ..., N \u2212 1. (6)\nWe now derive a bound for the adaptive greedy policy by applying linear programming to the problem of minimizing F KG\nF\u03c0\u2217 subject to the inequalities (6), which is a worst-\ncase analysis. The following lemma states the linear program and its solution. We use it afterwards to establish the bounds.\nLemma 4.13. Given N \u2208 Z+, consider the following linear program\nmin N\u22121\u2211 i=0 ai,\nt\u22121\u2211 i=0 ai +Nat \u2265 1, t = 0, 1, ..., N \u2212 1.\nThen under these N constraints, min \u2211N\u22121\ni=0 ai = 1\u2212 \u03b1N , where \u03b1 = N\u22121 N .\nThe proof of this lemma can be found in [31]. We have the following results, which generalizes the classic result of the greedy algorithm that achieves (1 \u2212 1/e)-approximation to prior-optimality for ranking and selection problems.\nTheorem 4.14. Assume we have a budget of N measurements. Let \u03c0\u2217 denote the optimal sequential policy for the ranking and selection problem, then we have\nFKG F \u03c0\u2217 \u2265 1\u2212 (N \u2212 1 N )N .\nProof. By Proposition 4.12, we have F \u03c0 \u2217 \u2264 \u2211n\u22121 i=0 \u03c1\nKG,i + N\u03c1KG,n, n = 0, 1, ..., N \u2212 1. Divide by F \u03c0 \u2217 on both sides of this inequality, we have\n1 \u2264 n\u22121\u2211 i=0 \u03c1KG,i F \u03c0\u2217 +N \u03c1KG,n F \u03c0\u2217 , n = 0, 1, ..., N \u2212 1.\nLet ai = \u03c1KG,i\nF\u03c0\u2217 , and then these inequalities are identical to the constraints in Lemma\n4.13. We notice that\nmin N\u22121\u2211 i=0 ai = min N\u22121\u2211 i=0 \u03c1KG,i F \u03c0\u2217 \u2264 N\u22121\u2211 i=0 \u03c1KG,i F \u03c0\u2217 = FKG F \u03c0\u2217 .\nBy Lemma 4.13, we have min \u2211N\u22121\ni=0 ai = 1\u2212\u03b1N , so FKG F\u03c0\u2217 \u2265 1\u2212\u03b1N = 1\u2212(N\u22121 N )N ."}, {"heading": "5 Analysis of Submodularity of the Value of Infor-", "text": "mation\nThe finite-time bounds obtained in the previous sections assume that the value of information is submodular. In general, submodularity does not hold for arbitrary value functions. In this section, we analyze the submodularity of the two-alternative case for independent beliefs.\nWhile submodularity is a property for multi-set functions, we can extend it to any continuous function by making it possible for the increment to take any positive value. It could be easily extended to any continuous function. This allows us to use results from real analysis to study submodularity.\nDefinition 5.1. A function f : Rn 7\u2192 R is submodular if for all x, y \u2208 Rn, xi \u2264 yi and \u03b4 \u2208 Rn+,\nf(x+ \u03b4)\u2212 f(x) \u2265 f(y + \u03b4)\u2212 f(y).\nWe show that submodularity of C2 functions is directly related to its second derivatives and cross-derivatives (the proof is given in Appendix A.4):\nTheorem 5.2. C2 function f: Rn \u2192 R is submodular if and only if every element of its Hessian is non-positive.\nThe concavity of the value of information has been studied extensively by [14]. In this section, we only study the cross-derivatives of the value of information.\nLet M = 2 and the measurement allocation z = (z1, z2). The value of information\nv(z) = s(z)f(\u2212 |\u03b8 0 1\u2212\u03b802 | s(z) ), where s(z) = \u221a \u03c3\u030321(z1) + \u03c3\u0303 2 2(z2), \u03c3\u0303 2 i (zi) =\n\u03c32,0i zi\n\u03c32W /\u03c3 2,0 i +zi\n, f(a) =\na\u03a6(a) + \u03c6(a), \u03a6 and \u03c6 are the standard normal cumulative distribution and density respectively [14].\nAlthough the value of information is not concave in general in the two-alternative case, v is concave on the region where all zi\u2019s are large enough (see Theorem 2 in [14]).\nWe directly calculate the first derivative and cross-derivative of v as\n\u2202v \u2202z1 = \u03c3\u03031(z1)\u03c3\u0303\n\u2032 1(z1)\ns(z)\n[ f(\u2212|\u03b8\n0 1 \u2212 \u03b802| s(z)\n) + |\u03b801 \u2212 \u03b802| \u03a6(\u2212 |\u03b8 0 1\u2212\u03b802 | s(z) )\ns(z)\n] ,\n\u22022v\n\u2202z1\u2202z2 = \u03c3\u03031(z1)\u03c3\u0303\n\u2032 1(z1)\u03c3\u03032(z2)\u03c3\u0303 \u2032 2(z2) s3(z) \u03c6(\u2212|\u03b8 0 1 \u2212 \u03b802| s(z) )\n( |\u03b801 \u2212 \u03b802|2\n\u03c3\u030321(z1) + \u03c3\u0303 2 2(z2)\n\u2212 1 ) .\nTheorem 5.3. The value of information is submodular when M = 2 and \u03b801 = \u03b8 0 2.\nProof. Concavity of v(z) is proven in Remark 2 by [14]. Since \u03b801 = \u03b8 0 2, |\u03b801\u2212 \u03b802| = 0 and thus \u2202 2v\n\u2202z1\u2202z2 \u2264 0. Therefore, v is submodular in this case.\n\u22022v \u2202z1\u2202z2 \u2264 0 is equivalent to |\u03b801 \u2212 \u03b802|2 \u2264 \u03c3\u030321(z1) + \u03c3\u030322(z2). Rewriting this inequality, we get\n1 1\n\u03c32,01 + z1 \u03c32W\n+ 1\n1 \u03c32,02 + z2 \u03c32W\n\u2264 \u03c32,01 + \u03c3 2,0 2 \u2212 |\u03b801 \u2212 \u03b802|2. (7)\nWe need \u03c32,01 +\u03c3 2,0 2 \u2212 |\u03b801\u2212 \u03b802|2 \u2265 0, which can be achieved by setting our prior variance large enough or using a uniform prior over all alternatives. This is very reasonable when we have very little information about our problem domain.\nInequality equation (7) defines a region in the z1\u2212 z2 plane. Specifically, this region has the hyperbolic line 11\n\u03c3 2,0 1 + z1 \u03c32 W + 11 \u03c3 2,0 2 + z2 \u03c32 W = \u03c32,01 + \u03c3 2,0 2 \u2212 |\u03b801 \u2212 \u03b802|2 as its boundary and\ncontains infinity. In particular, when z1 and z2 are large enough (or equivalently when our measurement is accurate enough), the value of information is submodular.\nSince there is no closed-form expression for the value of information under arbitrary allocations, we cannot verify submodularity in a simple way for problems with more than two alternatives and for correlated beliefs. Instead, it can be checked using numerical approximation and is easy to guarantee by running repeated experiments and averaging to reduce measurement noise. A necessary condition is the concavity of the value of information for measuring a fixed alternative x for n times, which can be checked exactly.\nIntuitively, we may expect that the marginal value of information should decline as we make more observations. But it is not always the case. It is shown that the value of information for measuring a single alternative may form an S-curve which is concave when there are many measurements, but may be convex at the beginning [14]. The S-curve behavior arises when the measurement noise is large and thus a single measurement simply contains too little information, leading to algorithmic difficulties and apparent paradoxes. This issue is not related to any specific policy, but rather is an inherent property of learning problems. Although the value of information is not necessarily concave, it can be made concave by measuring each alternative enough times or (equivalently) using sufficiently precise measurements."}, {"heading": "6 Computational Experiments", "text": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3]. But none of these bounds are tight in finite time and different bounds can be based on different metrics. Hence, empirical experiments are needed to better understand the finite time performance of each policy. To this end, we propose experiments to illustrate the finite time behavior of both KG and other optimal learning policies. We consider the following learning settings that arise a lot in black box Bayesian optimization.\nEqual-prior: M = 100. The true values \u00b5x are uniformly distributed over [0, 60] and measurement noise \u03c3W = 100. \u03b8 0 x = 30 and \u03c3 0 x = 10 for every x.\nAsymmetric unimodular function (AUF): x is a controllable parameter ranging from 21 to 120. The objective function is F (x, \u03be) = \u03b81 min(x, \u03be) \u2212 \u03b82x, where \u03b81, \u03b82 and the distribution of the random variable \u03be are all unknown. The aim is to solve maxx EF (x, \u03be) while learning \u03b81, \u03b82 and the parameters that determine the distribution of \u03be. The true distribution of \u03be is taken as a normal distribution with mean 60 and standard deviation 18 (corresponding to a 30% noise ratio).\nGoldstein-Price\u2019s function with additive noise:\nf(x, y, \u03c6) = [1 + (x+ y + 1)2(19\u2212 14x+ 3x2 \u2212 14y + 6xy + 3y2)] \u00b7 [30 + (2x\u2212 3y)2(18\u2212 32x+ 12x2 + 48y \u2212 36xy + 27y2)] + \u03c6,\nwhere \u22123 \u2264 x \u2264 3, \u22123 \u2264 y \u2264 3 and are uniformly discretized into 13 \u00d7 13 alternatives. In order to obtain the prior distribution, we follow [24] and [23] to use Latin hypercube designs for initial fit. For independent beliefs, we adopt a uniform prior with the same mean value \u03b80x and standard deviation \u03c3 0 x for all alternatives. For correlated beliefs, we use a constant mean value \u03b80x for all alternatives and a prior covariance matrix of the form\n\u03a30xx\u2032 = \u03c3e \u2212\n\u2211d i=1 \u03bbi(xi\u2212x\u2032i)2 ,\nwhere each arm x is a d-dimensional vector and \u03c3, \u03bbi are constant. We adopt the rule of thumb by [24] for the default number (10 \u00d7 p) of points, where p is the number of parameters to be estimated. In addition, as suggested by [23], to estimate the random errors, after the first 10 \u00d7 p points are evaluated, we add one replicate at each of the locations where the best p responses are found. Maximum likelihood estimation is then used to estimate the parameters based on the points in the initial design.\nThe policies considered in this section is described as follows. EXPL: A pure exploration strategy that tests each alternative equally often. EXPT: A pure exploitation strategy, XEXPT,n(Sn) = arg maxx \u00b5\u0302 n x. Interval Estimation (IE): [25]\nX IE,n(Sn) = arg max x \u03b8nx + z\u03b1/2\u03c3 n x .\nKriging: [23] Let x\u2217 = arg maxx(\u03b8 n x + \u03c3 n x), then\nXKriging,n(Sn) = arg max x (\u03b8nx \u2212 \u03b8nx\u2217)\u03a6( \u03b8nx \u2212 \u03b8nx\u2217 \u03c3nx ) + \u03c3nx\u03c6( \u03b8nx \u2212 \u03b8nx\u2217 \u03c3nx ),\nwhere \u03c6 and \u03a6 are the standard normal density and cumulative distribution functions. UCB-E: [2]\nXUCB-E,n(Sn) = arg max x \u00b5\u0302nx +\n\u221a \u03b1\nNnx ,\nwhere \u00b5\u0302nx, N n x are the sample mean of \u00b5x and number of times x has been measured up to time n. The quantity \u00b5\u03020x is initialized by measuring each alternative once. SR: [2] Let A1 = X , log(M) = 12 + \u2211M i=2 1 i ,\nnm = \u2308 1\nlog(M) n\u2212M M + 1\u2212m\n\u2309 .\nFor each phase m = 1, ...,M \u2212 1:\n1. For each x \u2208 Am, select alternative x for nm \u2212 nm\u22121 rounds.\n2. Let Am+1 = Am \\ arg minx\u2208Am \u00b5\u0302x."}, {"heading": "6.1 Finite Time Performance of Different Policies", "text": "Although the theoretical analysis in the previous section is to bound the performance of the knowledge gradient policy to the optimal policy (in theory), the optimal sequential policy is impossible to find in practice. To this end, we compare the value of KG to the expected value of the best alternative maxx \u00b5x. Define the opportunity cost (OC\n\u03c0) of any policy \u03c0 at any time step n as:\nOC\u03c0 = max x \u00b5x \u2212 \u00b5x\u0303n ,\nwhere x\u0303n = arg maxx \u03b8 n x . We illustrate the finite time behavior of the KG policy under Equal-prior and AUF with independent normal beliefs. We run KG and calculate the opportunity cost ratio = maxx \u00b5x\u2212\u00b5x\u0303n\nmaxx \u00b5x in each iteration. We report the mean with 90%\nconfidence interval averaged over 1000 experiments in Figure 1. We next compare the performance of KG, IE with tuning, UCB-E with tuning, SR, EXPL and EXPT. Figure 2 shows the performance in problem classes AUF and Goldstein with independent beliefs under a measurement budget five times the number of alternatives. We run each policy for 1000 times. In each run, we pre-generate all the observations and share across different policies. We illustrate in the first column of Figure 2 the mean opportunity cost and the standard deviation of each policy over 1000 runs after the measurement budget is exhausted.\nIn order to give a comprehensive comparison based on different metrics, we also calculate the probability that the final recommendation of each policy is the optimal one and the probability that the opportunity cost of each policy is the lowest, as illustrated in the figures on the right hand side of Figure 2.\nThe three criteria characterize the behavior of policies from different perspectives. One observation is that there is no universal best policy for all problem classes or under all criteria, which means that theoretical guarantees are not by themselves reliable indicators of which policy is best for a particular problem class.\nWe also exploit correlated beliefs between alternatives in order to strengthen the effect of each measurement so that one measurement of some alternative can provide information for other alternatives.\nFirst, we present the OC of different policies after each iteration under AUF (\u03b82 = 0.5\u03b81) in Figure 3. We tune z\u03b1 for IE and \u03b1 for UCB for N = 400 measurements and the optimal values are z\u03b1 = 0.969 and \u03b1 = 6.657. Since UCB-E needs to measure each alternative once, we omit the OC for its first 100 (which is the number of alternatives) steps. KG uses independent beliefs while KGCB, IE and Kriging start from MLE fitted correlated beliefs. When incorporating correlated beliefs, a measurement of one alternative tells us something about other alternatives. As a result, KGCB learns faster\nthan KG."}, {"heading": "7 Conclusion", "text": "In this paper, we offer a new perspective of interpreting ranking and selection problems as adaptive stochastic multi-set maximization problems. We present the first finitetime bounds for the knowledge gradient on both the posterior optimality and the prior optimality. The prior view provides a cleaner relationship between the performance of the policy and the sample taken, making it possible to relate the value of information to the submodularity of the sample. We analyze the submodularity of the two-alternative case and provide other conditions for more general problems, bringing out the issue and importance of submodularity in leaning problems. We propose experiments to further illustrate the finite time behavior of the knowledge gradient policy as well as other policies with or without theoretical guarantees."}, {"heading": "A Proofs", "text": "A.1 Proof of Lemma 4.5\nFor any \u03c8 with |\u03c8| = n, we consider the resulting knowledge state Sn = (\u03b8nx , \u03b2nx )x\u2208X . Since \u03c3W 6= 0, there exists such \u03c8 that maxx \u03b8nx > maxx 6=x\u2032 \u03b8nx with positive probability. Now consider another realization \u03c8\u2032 with dom(\u03c8\u2032) = dom(\u03c8) \u222a {x2}, where x2 is the second largest alternative of \u03b8nx . We denote the observation of x2 in \u03c8\n\u2032 as W2 and the resulting Sn+1 as (\u03b8n+1x , \u03b2 n+1 x )x\u2208X according to Bayes\u2019 rule. The knowledge gradient \u2206(x|\u03c8) = \u03bdKG,nx can be analytically expressed by\n\u03bdKG,nx = \u03c3\u0303 n xf(\u03b6 n x ),\nwhere \u03c3\u0303nx = \u221a (\u03b2nx ) \u22121 \u2212 (\u03b2nx + \u03b2W )\u22121, \u03b6nx = \u2212 \u2223\u2223\u2223 \u03b8nx\u2212maxx\u2032 6=x \u03b8nx\u2032\u03c3\u0303nx \u2223\u2223\u2223 and f(\u03b6) = \u03b6\u03a6(\u03b6) + \u03c6(\u03b6). \u03a6(\u03b6) and \u03c6(\u03b6) are, respectively, the cumulative standard normal distribution the standard normal density [15]. We first notice that f \u2032(\u03b6) = \u03a6(\u03b6) \u2265 0 for any \u03b6 \u2208 R so that f(\u03b6) is non-decreasing. We next compare \u03bdKG,nx1 and \u03bd KG,n+1 x1 for x1 = arg maxx \u03b8 n x . According to Bayes\u2019 rule, the precision \u03b2 of x2 changes only when x2 is measured. So we have \u03c3\u0303nx = \u03c3\u0303 n+1 x . Similarly we have all the \u03b8 n+1 x unchanged except for alternative x2. By some algebra, it can be shown that for any W2 such that \u03b8 n x2 < W2 \u2264 \u03b2nx2 \u03b2W (\u03b8nx1\u2212\u03b8 n x2 )+\u03b8nx1 , we have \u03bdKG,nx1 < \u03bd KG,n+1 x1 . Since \u03b8nx1 > \u03b8 n x2\nby construction, such W2 can be obtained with positive probability.\nA.2 Proof of Proposition 1\nIn this appendix, we prove the properties of submodular multi-set functions. We prove the equivalence by showing 2)\u21d2 1)\u21d2 3)\u21d2 4)\u21d2 2).\n\u2022 2)\u21d2 1). Take S \u2286 T and T \u2212S = {x1, x2, ..., xr}. Then from 3) we have \u03c1x(S) \u2265 \u03c1x(S\u222a{x1}), \u03c1x(S\u222a{x1}) \u2265 \u03c1x(S\u222a{x1, x2}),..., \u03c1x(S\u222a{x1, x2, ..., xr\u22121}) \u2265 \u03c1x(T ). Summing these r inequalities yields 1).\n\u2022 1) \u21d2 3). For arbitrary S and T with T \u2212 S = {x1, x2, ..., xr} and S \u2212 T = {y1, y2, ..., yq}, from 1) we have\ng(S \u222a T )\u2212 g(S) = r\u2211 t=1 [g(S \u222a {x1, ..., xt})\u2212 g(S \u222a {x1, ..., xt\u22121})]\n= r\u2211 t=1 \u03c1xt(S \u222a {x1, ..., xt\u22121})\n\u2264 r\u2211 t=1 \u03c1xt(S) = \u2211 x\u2208T\u2212S \u03c1x(S). (8)\nAnd\ng(S \u222a T )\u2212 g(T ) = q\u2211 t=1 [g(T \u222a {y1, ..., yt})\u2212 g(T \u222a {y1, ..., yt\u22121})]\n= q\u2211 t=1 \u03c1yt(T \u222a {y1, ..., yt} \u2212 {yt}})\n\u2265 q\u2211 t=1 \u03c1yt(T \u222a S \u2212 {yt}) = \u2211 x\u2208S\u2212T \u03c1x(S \u222a T \u2212 {x}). (9)\nSubtracting equation (9) from equation (8) we get 3).\n\u2022 3)\u21d2 4). If S \u2286 T , S \u2212 T = \u2205, and therefore the last term in 3) vanishes.\n\u2022 4)\u21d2 2). Substitute T = S \u222a {x, y} into 4) to obtain\ng(S \u222a {x, y}) \u2264 g(S) + \u03c1x(S) + \u03c1y(S) = \u03c1x(S) + g(S \u222a {y}).\nRearrange this inequality, we get\n\u03c1x(S \u222a {y}) = g(S \u222a {x, y})\u2212 g(S \u222a {y} \u2264 \u03c1x(S).\nA.3 Proof of Proposition 4.12\nLet z\u2217(Z, \u03c0,\u03a6) be the next adaptive greedy choice that maximizes the expected marginal increment given that policy \u03c0 has generated Z. We first show that\nF \u03c02@\u03c01 \u2264 F \u03c02 + n1 \u2211 Z\u2208Zn P(\u03c02 Z) ( E [ v\u0302(Z \u222a {z\u2217(Z, \u03c02,\u03a6)},\u03a6) ] \u2212 v(Z) ) for all policies \u03c01 with a measurement budget n1 and \u03c02 with a budget n2 under any prior and probability distribution that describes a measurement.\nProof. Let \u03c0[j] denote the first j measurement decisions under some policy \u03c0. First of all we break F \u03c02@\u03c01 \u2212 F \u03c02 into n1 consecutive differences,\nF \u03c02@\u03c01 \u2212 F \u03c02 = n1\u2211 j=1 ( F \u03c02@\u03c0 [j] 1 \u2212 F \u03c02@\u03c0 [j\u22121] 1 ) .\nSimilar to what we did in the last lemma, for each difference we have\nF \u03c02@\u03c0 [j] 1 \u2212 F \u03c02@\u03c0 [j\u22121] 1 = \u2211\nZ1\u2208Zn2+j P(\u03c02@\u03c0[j]1 Z1)v(Z1)\u2212 \u2211 Z2\u2208Zn2+j\u22121 P(\u03c02@\u03c0[j\u22121]1 Z2)v(Z2)\n= \u2211\nZ1\u2208Zn2+j \u2211 Z2\u2208Zn2+j\u22121,Z2\u222aZ3=Z1 P(\u03c02@\u03c0[j\u22121]1 Z2)P(\u03c0 {j} 1 Z3|\u03c02@\u03c0 [j\u22121] 1 Z2)v(Z1)\n\u2212 \u2211\nZ2\u2208Zn2+j\u22121 \u2211 Z3\u2208Z1 P(\u03c02@\u03c0[j\u22121]1 Z2)P(\u03c0 {j} 1 Z3|\u03c02@\u03c0 [j\u22121] 1 Z2)v(Z2)\n= \u2211\nZ2\u2208Zn2+j\u22121 \u2211 Z3\u2208Z1 P(\u03c02@\u03c0[j\u22121]1 Z2)P(\u03c0 {j} 1 Z3|\u03c02@\u03c0 [j\u22121] 1 Z2) ( v(Z2 \u222a Z3)\u2212 v(Z2) ) .\nNow we consider all possible pair (Z4, Z5) such that Z4 \u2208 Zn2 , Z5 \u2208 Zj\u22121 and Z4\u222aZ5 = Z2. Notice that the policy \u03c02@\u03c0 [j] 1 employs a fresh start at the time n2, therefore the events before and after time n2 are independent. Then we have\u2211 Z2\u2208Zn2+j\u22121 \u2211 Z3\u2208Z1 P(\u03c02@\u03c0[j\u22121]1 Z2)P(\u03c0 {j} 1 Z3|\u03c02@\u03c0 [j\u22121] 1 Z2) ( v(Z2 \u222a Z3)\u2212 v(Z2)\n) =\n\u2211 Z2\u2208Zn2+j\u22121 \u2211 Z4\u222aZ5=Z2 \u2211 Z3\u2208Z1 P(\u03c02 Z4)P(\u03c0[j\u22121]1 Z5)P(\u03c0 {j} 1 Z3|\u03c02@\u03c0 [j\u22121] 1 Z2)\n\u00d7 ( v(Z2 \u222a Z3)\u2212 v(Z2) ) .\nBased on the submodular property of function v, we have\nv(Z2 \u222a Z3)\u2212 v(Z2) \u2264 v(Z4 \u222a Z3)\u2212 v(Z4).\nThen from the definition of z\u2217, we have\nv(Z4 \u222a Z3)\u2212 v(Z4) = E[v\u0302(Z4 \u222a Z3,\u03a6)\u2212 v\u0302(Z4,\u03a6)] = E\u03a6 [ E[v\u0302(Z4 \u222a Z3,\u03a6)\u2212 v\u0302(Z4,\u03a6)|Z\u03c02(\u03a6) = Z4] ] \u2264 E\u03a6 [ E[v\u0302(Z4 \u222a {z\u2217(Z4, \u03c02,\u03a6)},\u03a6)\u2212 v\u0302(Z4,\u03a6)|Z\u03c02(\u03a6) = Z4]\n] = E\u03a6[v\u0302(Z4 \u222a {z\u2217(Z4, \u03c02,\u03a6)},\u03a6)]\u2212 v(Z4).\nCombining the last two inequalities, we have\u2211 Z2\u2208Zn2+j\u22121 \u2211 Z4\u222aZ5=Z2 \u2211 Z3\u2208Z1 P(\u03c02 Z4)P(\u03c0[j\u22121]1 Z5)P(\u03c0 {j} 1 Z3|\u03c02@\u03c0 [j\u22121] 1 Z2)\n\u00d7 ( v(Z2 \u222a Z3)\u2212 v(Z2) ) \u2264\n\u2211 Z2\u2208Zn2+j\u22121 \u2211 Z4\u222aZ5=Z2 \u2211 Z3\u2208Z1 P(\u03c02 Z4)P(\u03c0[j\u22121]1 Z5)P(\u03c0 {j} 1 Z3|\u03c02@\u03c0 [j\u22121] 1 Z2)\n\u00d7 ( Ev\u0302(Z4 \u222a {z\u2217(Z4, \u03c02,\u03a6)},\u03a6)\u2212 v(Z4) ) =\n\u2211 Z2\u2208Zn2+j\u22121 \u2211 Z4\u222aZ5=Z2 P(\u03c02 Z4)P(\u03c0[j\u22121]1 Z5) ( Ev\u0302(Z4 \u222a {z\u2217(Z4, \u03c02,\u03a6)},\u03a6)\u2212 v(Z4) ) =\n\u2211 Z4\u2208Zn2 \u2211 Z5\u2208Zj\u22121 P(\u03c02 Z4)P(\u03c0[j\u22121]1 Z5) ( Ev\u0302(Z4 \u222a {z\u2217(Z4, \u03c02,\u03a6)},\u03a6)\u2212 v(Z4) ) =\n\u2211 Z4\u2208Zn2 P(\u03c02 Z4) ( Ev\u0302(Z4 \u222a {z\u2217(Z4, \u03c02,\u03a6)},\u03a6)\u2212 v(Z4) ) ,\nand this ends the proof.\nSet \u03c01 = \u03c0 \u2217 and \u03c02 = KG [n\u22121] in Lemma 4.11 and the above proposition then what left to show is that\nFKG [n] \u2212 FKG[n\u22121] \u2265 \u2211 Z\u2208Zn P(\u03c02 Z) ( Ev\u0302(Z \u222a {z\u2217(Z,KG[n\u22121],\u03a6)},\u03a6)\u2212 v(Z) ) .\nFrom the definition, the left hand side of the last equation:\nFKG [n] \u2212 FKG[n\u22121] = \u2211 Z1\u2208Zn+1 P(KG Z1)v(Z1)\u2212 \u2211 Z2\u2208Zn P(KG Z2)v(Z2)\n= \u2211 Z2\u2208Zn \u2211 Z3\u2208Z1 P(KG Z2)P(KG Z3|KG Z2)v(Z2 \u222a Z3)\n\u2212 \u2211 Z2\u2208Zn P(KG Z2)v(Z2).\nNow it is enough to show that\u2211 Z3\u2208Z1 P(KG Z3|KG Z2)v(Z2 \u222a Z3)\u2212 v(Z2)\n\u2265 Ev\u0302(Z2 \u222a {z\u2217(Z2,KG[n\u22121],\u03a6)},\u03a6)\u2212 v(Z2).\nWe could group together the partial realizations \u03c8 that lead to the same single step optimal decision z\u2217(Z2,KG\n[n\u22121],\u03a6), and then the last inequality follows from the adaptive greedy nature of the KG policy.\nA.4 Proof of Theorem 5.2\nFirst of all, we consider the case when f is a two dimensional function and the four points we pick form a rectangle. Assume f(x, y) is submodular. For any given point (x0, y0), we have f(x0+t+s, y0)\u2212f(x0+t, y0) \u2264 f(x0+s, y0)\u2212f(x0, y0) and f(x0+t, y0)\u2212f(x0, y0) \u2264 f(x0 + t, y0 + s) \u2212 f(x0, y0 + s) for any s, t > 0. From the first inequality we get fxx(x0, y0) \u2264 0 directly. From the second inequality, we have fx(x0, y0) \u2264 fx(x0, y0 + s), and finally fx,y(x0, y0) \u2264 0. On the other hand, if we have fxy \u2264 0, fxx \u2264 0, for any (x, y), then due to the fact that f(x0+t, y0+s)\u2212f(x0+t, y0)\u2212 ( f(x0, y0+s)\u2212f(x0, y0)\n) =\u222b x0+t\nx0 \u222b y0+s y0 fxy(u, v)dudv \u2264 0, f(x0+t+s, y0)\u2212f(x0+t, y0)\u2212 ( f(x0+s, y0)\u2212f(x0, y0) ) = stfxx(x0 + \u03be, y0) \u2264 0, we obtain the submodularity. We next consider the general case when f is n dimensional and the four points only form a parallelogram. Since the difference between the two marginal values can be decomposed into summation of several marginal value differences whose reference points form rectangles that parallel to coordinate planes, the result for the general case is straightforward from the two dimensional case."}], "references": [{"title": "Sample mean based index policies with o(logn) regret for the multiarmed bandit problem", "author": ["R. Agrawal"], "venue": "Adv. in Appl. Probab., ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "S", "author": ["J.-Y. Audibert"], "venue": "Bubeck, et al., Best arm identification in multi-armed bandits, COLT 2010-Proceedings, ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits", "author": ["J.-Y. Audibert", "R. Munos", "C. Szepesv\u00e1ri"], "venue": "Theor. Comput. Sci., 410 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Mach. Learn., 47 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimal learning for sequential sampling with non-parametric beliefs", "author": ["E. Barut", "W.B. Powell"], "venue": "J. Global Optim., ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Selecting a selection procedure", "author": ["J. Branke", "S.E. Chick", "C. Schmidt"], "venue": "Manag. Sci., 53 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Bandits with heavy tail", "author": ["S. Bubeck", "N. Cesa-Bianchi", "G. Lugosi"], "venue": "arXiv preprint arXiv:1209.1727, ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "G", "author": ["O. Capp\u00e9", "A. Garivier", "O.-A. Maillard", "R. Munos"], "venue": "Stoltz, et al., Kullback\u2013leibler upper confidence bounds for optimal sequential allocation, Ann. Statist., 41 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A gradient approach for smartly allocating computing budget for discrete event simulation", "author": ["C.-H. Chen", "H.-C. Chen", "L. Dai"], "venue": "28th Proc. Winter Simul., IEEE Computer Society", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient dynamic simulation allocation in ordinal optimization", "author": ["C.-H. Chen", "D. He", "M. Fu"], "venue": "IEEE Trans. Automat. Control, 51 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Simulation budget allocation for further enhancing the efficiency of ordinal optimization", "author": ["C.-H. Chen", "J. Lin", "E. Y\u00fccesan", "S.E. Chick"], "venue": "Discrete Event Dyn. Syst., 10 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "New two-stage and sequential procedures for selecting the best simulated system", "author": ["S.E. Chick"], "venue": "Oper. Res., 49 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "The knowledge-gradient policy for correlated normal beliefs", "author": ["P. Frazier", "W. Powell", "S. Dayanik"], "venue": "INFORMS J. Comput., 21 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Paradoxes in learning and the marginal value of information", "author": ["P.I. Frazier", "W.B. Powell"], "venue": "Decis. Anal., 7 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "A knowledge-gradient policy for sequential information collection", "author": ["P.I. Frazier", "W.B. Powell", "S. Dayanik"], "venue": "SIAM J. Control Optim., 47 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Bandit processes and dynamic allocation indices", "author": ["J.C. Gittins"], "venue": "J. R. Stat. Soc. Ser. B. Stat. Methodol., ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1979}, {"title": "Adaptive submodularity: A new approach to active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "COLT,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Bayesian look ahead one-stage sampling allocations for selection of the best population", "author": ["S.S. Gupta", "K.J. Miescke"], "venue": "J. Statist. Plann. Inference, 54 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "G", "author": ["I. Guttman"], "venue": "C. Tiao, et al., A bayesian approach to some best population problems, Ann. Math. Statist., 35 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1964}, {"title": "Opportunity cost and OCBA selection procedures in ordinal optimization for a fixed number of alternative systems", "author": ["D. He", "S.E. Chick", "C.-H. Chen"], "venue": "IEEE Trans. Syst. , Man, and Cybern., Part C: Applications and Reviews, 37 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Information value theory", "author": ["R. Howard"], "venue": "IEEE Trans Syst. Sci. and Cybern., 2 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1966}, {"title": "Global optimization of stochastic black-box systems via sequential kriging meta-models", "author": ["D. Huang", "T.T. Allen", "W.I. Notz", "N. Zeng"], "venue": "J. Global Optim., 34 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D.R. Jones", "M. Schonlau", "W.J. Welch"], "venue": "J. Global Optim., 13 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning and classifying under hard budgets", "author": ["A. Kapoor", "R. Greiner"], "venue": "Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Regret bounds for sleeping experts and bandits", "author": ["R. Kleinberg", "A. Niculescu-Mizil", "Y. Sharma"], "venue": "Mach. Learn., 80 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Adv. Appl. Math., 6 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1985}, {"title": "Hierarchical knowledge gradient for sequential sampling", "author": ["M.R. Mes", "W.B. Powell", "P.I. Frazier"], "venue": "J. Mach. Learn. Res., 12 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "The knowledge-gradient algorithm for sequencing experiments in drug discovery", "author": ["D.M. Negoescu", "P.I. Frazier", "W.B. Powell"], "venue": "INFORMS J. Comput., 23 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of approximations for maximizing submodular set functions", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Math. Program., 14 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1978}, {"title": "Optimal learning", "author": ["W.B. Powell", "I.O. Ryzhov"], "venue": "vol. 841", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Applied statistical decision theory", "author": ["H. Raiffa", "R. Schlaifer"], "venue": "Harvard Business School Publications, ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1961}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "arXiv preprint arXiv:0912.3995, ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "The economics of information", "author": ["G.J. Stigler"], "venue": "J. political Econ., ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1961}, {"title": "The knowledge gradient for sequential decision making with stochastic binary feedbacks", "author": ["Y. Wang", "C. Wang", "W. Powell"], "venue": "33rd Proc. ICML", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Value of information literature analysis: a review of applications in health risk management", "author": ["F. Yokota", "K.M. Thompson"], "venue": "Med. Decis. Mak., 24 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 30, "context": "Raiffa and Schlaifer established the Bayesian framework for R&S problems [33].", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "al made a thorough comparison of several fully sequential sampling procedures [6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].", "startOffset": 66, "endOffset": 77}, {"referenceID": 10, "context": "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].", "startOffset": 66, "endOffset": 77}, {"referenceID": 19, "context": "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].", "startOffset": 66, "endOffset": 77}, {"referenceID": 11, "context": "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].", "startOffset": 196, "endOffset": 200}, {"referenceID": 17, "context": "Another single-step Bayesian look-ahead policy first introduced by [19] and then further studied by [15] is called the \u201cknowledge-gradient policy\u201d (KG).", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "Another single-step Bayesian look-ahead policy first introduced by [19] and then further studied by [15] is called the \u201cknowledge-gradient policy\u201d (KG).", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "al modified the knowledge-gradient policy to handle correlated multivariate normal belief on the mean values of these rewards [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "A similar field is the multi-armed bandit problem, which were originally studied under Bayesian assumptions [17].", "startOffset": 108, "endOffset": 112}, {"referenceID": 25, "context": "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].", "startOffset": 131, "endOffset": 148}, {"referenceID": 0, "context": "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].", "startOffset": 131, "endOffset": 148}, {"referenceID": 3, "context": "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].", "startOffset": 131, "endOffset": 148}, {"referenceID": 24, "context": "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].", "startOffset": 131, "endOffset": 148}, {"referenceID": 6, "context": "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].", "startOffset": 131, "endOffset": 148}, {"referenceID": 28, "context": "To accomplish this, we build on the general structure of the analysis of greedy algorithms given in [31] and [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "To accomplish this, we build on the general structure of the analysis of greedy algorithms given in [31] and [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "For R&S problems, the knowledge gradient is a policy that at the nth iteration chooses its (n + 1)st measurement from X to maximize the single-period expected increase in value [15, 13].", "startOffset": 177, "endOffset": 185}, {"referenceID": 12, "context": "For R&S problems, the knowledge gradient is a policy that at the nth iteration chooses its (n + 1)st measurement from X to maximize the single-period expected increase in value [15, 13].", "startOffset": 177, "endOffset": 185}, {"referenceID": 27, "context": "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].", "startOffset": 113, "endOffset": 121}, {"referenceID": 33, "context": "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].", "startOffset": 113, "endOffset": 121}, {"referenceID": 26, "context": "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].", "startOffset": 139, "endOffset": 146}, {"referenceID": 4, "context": "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].", "startOffset": 139, "endOffset": 146}, {"referenceID": 28, "context": "We follow the general structure of the analysis of greedy approximation [31] to develop the first finite-time bound for the knowledge gradient policy for R&S problems as follows.", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "1 ([32]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "The definition of the knowledge gradient \u03bd x coincides with the Conditional Expected Marginal Benefit \u2206(e|\u03c8) defined by [18]: \u2206(e|\u03c8) := E [ f ( dom(\u03c8) \u222a {e},\u03a6 ) \u2212 f ( dom(\u03c8),\u03a6 ) |\u03a6 \u223c \u03c8 ] .", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "We generalize the definition of adaptive monotonicity and adaptive submodularity for set functions given by [18] to multi-set functions as follows.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "Since for any x, \u03b8 x = \u03b8 x + \u03c3\u0303(\u03a3, x)Z, where \u03c3\u0303(\u03a3, x) = \u03a3ex \u221a 1/\u03b2+\u03a3xx and the random variable Z is standard normal when conditioned on F [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 32, "context": "Stigler considers the value of information in economics when buyers search for the best price [35].", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "Howard laid the groundwork for the value of information in a decision-theoretic context and spawned a great deal of work in this area [22].", "startOffset": 134, "endOffset": 138}, {"referenceID": 34, "context": "Yokota and Thompson gives a first comprehensive review of value of information analyses related to health risk management [37].", "startOffset": 122, "endOffset": 126}, {"referenceID": 30, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 241, "endOffset": 260}, {"referenceID": 23, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 241, "endOffset": 260}, {"referenceID": 8, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 241, "endOffset": 260}, {"referenceID": 11, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 241, "endOffset": 260}, {"referenceID": 14, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 241, "endOffset": 260}, {"referenceID": 28, "context": "Since the value of information is a multi-set function, we first generalize the definitions and properties of submodular set functions described by [31] to submodular multi-set functions.", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "1 in [31].", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "[18] A concatenated policy \u03c0 = \u03c01@\u03c02 is constructed by running \u03c01 to completion, and then running policy \u03c02 from a fresh start ignoring all the information collected while running \u03c01.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] For a policy \u03c0, define the j-truncation \u03c0 of \u03c0 as the policy that runs exactly (j + 1) steps under \u03c0\u2019s decision rule and \u03c0{j} as the single step policy that randomly chooses an alternative according to the probability distribution of policy \u03c0\u2019s decision for the (j + 1)-th step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "The proof of this lemma can be found in [31].", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "The concavity of the value of information has been studied extensively by [14].", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "The value of information v(z) = s(z)f(\u2212 |\u03b8 0 1\u2212\u03b8 2 | s(z) ), where s(z) = \u221a \u03c3\u0303 1(z1) + \u03c3\u0303 2 2(z2), \u03c3\u0303 2 i (zi) = \u03c3 i zi \u03c32 W /\u03c3 2,0 i +zi , f(a) = a\u03a6(a) + \u03c6(a), \u03a6 and \u03c6 are the standard normal cumulative distribution and density respectively [14].", "startOffset": 242, "endOffset": 246}, {"referenceID": 13, "context": "Although the value of information is not concave in general in the two-alternative case, v is concave on the region where all zi\u2019s are large enough (see Theorem 2 in [14]).", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "Concavity of v(z) is proven in Remark 2 by [14].", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "It is shown that the value of information for measuring a single alternative may form an S-curve which is concave when there are many measurements, but may be convex at the beginning [14].", "startOffset": 183, "endOffset": 187}, {"referenceID": 25, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 169, "endOffset": 189}, {"referenceID": 7, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 169, "endOffset": 189}, {"referenceID": 31, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 169, "endOffset": 189}, {"referenceID": 3, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 169, "endOffset": 189}, {"referenceID": 2, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 169, "endOffset": 189}, {"referenceID": 22, "context": "In order to obtain the prior distribution, we follow [24] and [23] to use Latin hypercube designs for initial fit.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "In order to obtain the prior distribution, we follow [24] and [23] to use Latin hypercube designs for initial fit.", "startOffset": 62, "endOffset": 66}, {"referenceID": 22, "context": "We adopt the rule of thumb by [24] for the default number (10 \u00d7 p) of points, where p is the number of parameters to be estimated.", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "In addition, as suggested by [23], to estimate the random errors, after the first 10 \u00d7 p points are evaluated, we add one replicate at each of the locations where the best p responses are found.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Kriging: [23] Let x\u2217 = arg maxx(\u03b8 n x + \u03c3 n x), then X(S) = arg max x (\u03b8 x \u2212 \u03b8 x\u2217)\u03a6( \u03b8 x \u2212 \u03b8 x\u2217 \u03c3n x ) + \u03c3 x\u03c6( \u03b8 x \u2212 \u03b8 x\u2217 \u03c3n x ),", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "UCB-E: [2] X(S) = arg max x \u03bc\u0302x + \u221a \u03b1 Nn x ,", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "SR: [2] Let A1 = X , log(M) = 12 + \u2211M i=2 1 i ,", "startOffset": 4, "endOffset": 7}, {"referenceID": 14, "context": "\u03a6(\u03b6) and \u03c6(\u03b6) are, respectively, the cumulative standard normal distribution the standard normal density [15].", "startOffset": 105, "endOffset": 109}], "year": 2016, "abstractText": "We consider sequential decision problems in which we adaptively choose one of finitely many alternatives and observe a stochastic reward. We offer a new perspective of interpreting Bayesian ranking and selection problems as adaptive stochastic multi-set maximization problems and derive the first finite-time bound of the knowledge-gradient policy for adaptive submodular objective functions. In addition, we introduce the concept of prior-optimality and provide another insight into the performance of the knowledge gradient policy based on the submodular assumption on the value of information. We demonstrate submodularity for the two-alternative case and provide other conditions for more general problems, bringing out the issue and importance of submodularity in learning problems. Empirical experiments are conducted to further illustrate the finite time behavior of the knowledge gradient policy.", "creator": "LaTeX with hyperref package"}}}