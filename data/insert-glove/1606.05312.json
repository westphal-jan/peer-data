{"id": "1606.05312", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Successor Features for Transfer in Reinforcement Learning", "abstract": "Transfer 77-77 in demilitarised reinforcement kutz learning kanyenda refers ozzy to callaway the irrevocably notion capes that generalization 34:48 should remodeler occur http://www.noaa.gov not trials only picado within a housedress task but 1935 also across abebooks tasks. jaja Our focus is blanketing on transplantations transfer vlahov where the caymanian reward brandao functions neo-impressionism vary berlinetta across 3,142 tasks while the environment ' banxquote s kiyosato dynamics remain barzun the makeable same. The method r\u00edmur we tokimasa propose d'alton rests hafsid on apollon two imura key worton ideas: \" successor features, \" a yadu value function barham representation that bienal decouples gracen the taulava dynamics 502nd of upupidae the ll\u0177n environment aquisition from binoculars the rewards, bangabhaban and \" mechitza generalized buddhadharma policy pingpu improvement, \" a cavaillon generalization of subordinate dynamic programming ' s seahorse policy kulthum improvement step high-angle that horna considers a set 930,000 of policies non-u.s. rather depressor than a single one. insurability Put together, the two ideas c-max lead collectively to 120.90 an flirtatiousness approach 30-meter that lhota integrates seamlessly within the reinforcement learning radiophone framework permissions and allows biedrins transfer to semi-rural take place between abide tasks without any hammar restriction. The proposed nyserda method glycol also provides evangel performance guarantees deylaman for the addict transferred policy palaly even peru before any learning replacements has elita taken place. We derive liebenfels two netanyahu theorems that anastasiya set our approach in firm theoretical ground and 213.9 present experiments provider that show that it aker successfully promotes matti transfer in practice.", "histories": [["v1", "Thu, 16 Jun 2016 18:45:32 GMT  (84kb,D)", "http://arxiv.org/abs/1606.05312v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["andr\\'e barreto", "r\\'emi munos", "tom schaul", "david silver"], "accepted": true, "id": "1606.05312"}, "pdf": {"name": "1606.05312.pdf", "metadata": {"source": "CRF", "title": "Successor Features for Transfer in Reinforcement Learning", "authors": ["Andr\u00e9 Barreto", "R\u00e9mi Munos", "Tom Schaul", "David Silver"], "emails": ["andrebarreto@google.com", "munos@google.com", "schaul@google.com", "davidsilver@google.com"], "sections": [{"heading": "1 Introduction", "text": "Reinforcement learning (RL) provides a framework for the development of situated agents that learn how to behave while interacting with the environment [21]. The basic RL loop is defined in an abstract way so as to capture only the essential aspects of such an interaction: an agent receives observations and selects actions to maximize a reward signal. This setup is generic enough to describe tasks of different levels of complexity that may unroll at distinct time scales. For example, in the task of driving a car, an action can be to turn the wheel, to make a right turn, or to drive to a given location.\nClearly, from the point of view of the designer it is desirable to describe a task at the highest level of abstraction possible. However, by doing so one may overlook behavioral patterns and inadvertently make the task more difficult than it really is. The action of driving to a location clearly encompasses the action of making a right turn, which in turn encompasses the action of turning the wheel. In learning how to drive an agent should be able to identify and to exploit such interdependencies. More generally, the agent should be able to break a task in smaller subtasks and use knowledge accumulated in any subset of those to speed up learning in related tasks. This process of leveraging knowledge acquired in one task to improve performance on another task is usually referred to as transfer.\nar X\niv :1\n60 6.\n05 31\n2v 1\n[ cs\n.A I]\nTransfer in reinforcement learning can be defined in many different ways, but in general it refers to the notion that generalization should occur not only within a task but also across tasks [23]. In this paper we look at one specific type of transfer, namely, when the subtasks involved correspond to different reward functions defined in the same environment. This setup is flexible enough to allow transfer to happen at different levels. In particular, since rewards are a generic device to define the agent\u2019s objective, by appropriately defining them one can induce different task decompositions. For instance, the type of hierarchical decomposition involved in the driving example above can be induced by changing the frequency at which rewards are delivered to the agent: a positive reinforcement can be given after each maneuver that is well executed or only at the final destination. It is not difficult to see that one can also decompose a task in subtasks that are fairly independent of each other or whose dependency is strictly temporal (that is, when the tasks must be executed in a certain order but no single task is clearly \u201ccontained\u201d within another).\nThese types of task decomposition potentially allow the agent to tackle more complex problems than would be possible were the tasks modeled as a single monolithic challenge. However, in order to exploit this structure to its full extent the agent should have an explicit mechanism to promote transfer between tasks. Ideally, we want a transfer approach to have two important properties. First, the flow of information between tasks should not be dictated by a rigid diagram that reflects the relationship between the tasks themselves, such as hierarchical or temporal dependencies. On the contrary, information should be exchanged between tasks whenever useful. Second, rather than being posed as a separate problem, transfer should be integrated into the RL framework as much as possible, preferably in a way that is almost transparent to the agent.\nIn this paper we propose an approach to implement transfer that has the two properties above. Our method builds on two basic ideas that complement each other. The first one is a generalization of a concept proposed by Dayan [7] called successor representation. As the name suggests, in this representation scheme each state is described by a prediction about the future occurrence of all other states under a fixed policy. We present a generalization of Dayan\u2019s idea which extends the original scheme to continuous spaces and also facilitates the incorporation of function approximation. We call the resulting scheme successor features. As will be shown, successor features lead to a representation of the value function that naturally decouples the dynamics of the environment from the rewards, which makes them particularly suitable for transfer.\nIn order to actually put transfer into effect with successor features, we present two theoretical results that provide the foundation of our approach. The first one is a generalization of Bellman\u2019s [4] classic policy improvement theorem that extends the original result from one to multiple decision policies. This result shows how knowledge about a set of tasks can be transferred to a new task in a way that is completely integrated with reinforcement learning. It also provides performance guarantees on the new task before any learning has taken place. The second theoretical result is a theorem that formalizes the notion that an agent should be able to perform well on a task if it has seen a similar task before\u2014something clearly desirable in the context of transfer. Combined, the two results above not only set our approach in firm ground but also outline the mechanics of how to actually implement transfer. We build on this knowledge to propose a concrete method and evaluate it in experiments that illustrate the benefits of transfer in practice."}, {"heading": "2 Background and Problem Formulation", "text": "We consider the framework of RL outlined in the introduction: an agent interacts with an environment and selects actions in order to maximize the expected amount of reward received in the long run [21]. As usual, we assume that this interaction can be modeled as a Markov decision process (MDP, Puterman, [17]). An MDP is defined as a tuple M \u2261 (S,A, p, r, \u03b3). The sets S and A are the state and action spaces, respectively; here we assume that S and A are finite whenever such an assumption facilitates the presentation, but most of the ideas readily extend to continuous spaces. For each s \u2208 S and a \u2208 A the function p(\u00b7|s, a) gives the next-state distribution upon taking action a in state s. We will often refer to p(\u00b7|s, a) as the dynamics of the MDP. The reward received at transition s a\u2212\u2192 s\u2032 is given by r(s, a, s\u2032); usually one is interested in the expected reward resulting from the execution of a in s, which is given by r(s, a) = ES\u2032\u223cp(\u00b7|s,a)[r(s, a, S\u2032)]. The discount factor \u03b3 \u2208 [0, 1) gives smaller weights to rewards received further in the future.\nThe goal of the agent in RL is to find a policy \u03c0\u2014a mapping from states to actions\u2014that maximizes the expected discounted sum of rewards, also called the return Rt = \u2211\u221e i=0 \u03b3\nirt+i+1. One way to address this problem is to use methods derived from dynamic programming (DP), which heavily rely on the concept of a value function [17]. The action-value function of a policy \u03c0 is defined as\nQ\u03c0(s, a) \u2261 E\u03c0 [Rt|St = s,At = a] , (1)\nwhere E\u03c0[\u00b7] denotes expected value when following policy \u03c0. Once the action-value function of a particular policy \u03c0 is known, we can derive a new policy \u03c0\u2032 which is greedy with respect to Q\u03c0(s, a), that is, \u03c0\u2032(s) \u2208 argmaxaQ\u03c0(s, a). Policy \u03c0\u2032 is guaranteed to be at least as good as (if not better than) policy \u03c0. These two steps, policy evaluation and policy improvement, define the basic mechanics of RL algorithms based on DP; under certain conditions their successive application leads to an optimal policy \u03c0\u2217 that maximizes the expected return from every state in S [21]. As mentioned, in this paper we are interested in the problem of transfer. Here we adopt the following definition: given two sets of tasks T and T \u2032 such that T \u2282 T \u2032, after being exposed to T \u2032 the agent should perform no worse, and preferably better, than it would had it been exposed to T only. Note that T can be the empty set. In this paper a task will be a specific reward function r(s, a) for a given MDP. In Section 4 we will revisit this definition and make it more formal, and we will also clarify the measure used to compare performance. Before doing that, though, we will present a core concept for this paper whose interest is not restricted to transfer learning."}, {"heading": "3 Successor Features", "text": "In this section we present the concept that will serve as a cornerstone for the rest of the paper. We start by presenting a simple reward model and then show how it naturally leads to a generalization of Dayan\u2019s [7] successor representation (SR).\nSuppose that the one-step expected reward associated with state-action pair (s, a) is given by\nr(s, a) = \u03c6(s, a)>w, (2)\nwhere \u03c6(s, a) \u2208 Rd are features of (s, a) and w \u2208 Rd are weights. Supposing that (2) is true is not restrictive since we are not making any assumptions about \u03c6(s, a): if we have \u03c6i(s, a) = r(s, a) for some i, for example, we can clearly recover any reward function exactly. To simplify the notation, let \u03c6t = \u03c6(st, at). Then, by simply rewriting the definition of the action-value function in (1) we have\nQ\u03c0(s, a) = E\u03c0 [ rt+1 + \u03b3rt+2 + \u03b3 2rt+3 + ... |St = s,At = a ]\n= E\u03c0 [ \u03c6>t+1w + \u03b3\u03c6 > t+2w + \u03b3 2\u03c6>t+3w + ... |St = s,At = a ]\n= E\u03c0 [\u2211\u221e\ni=t\u03b3 i\u2212t\u03c6i+1 |St = s,At = a\n]> w = \u03c8\u03c0(s, a)>w. (3)\nWe call \u03c8\u03c0(s, a) \u2261 E\u03c0[ \u221e\u2211 i=t \u03b3i\u2212t\u03c6i+1|St = s,At = a] the successor features (SFs) of (s, a) under \u03c0.\nThe ith component of \u03c8\u03c0(s, a) gives the discounted sum of \u03c6i when following policy \u03c0 starting from (s, a). In the particular case where S andA are finite and \u03c6 is a tabular representation of S \u00d7A\u2014that is, \u03c6(s, a) is a \u201cone-hot\u201d vector in R|S||A|\u2014\u03c8\u03c0(s, a) is the discounted sum of occurrences of each state-action pair under \u03c0. This is essentially the concept of SR extended from the space S to the set S\u00d7A [7]. One of the points here is precisely to generalize SR to be used with function approximation, but the exercise of deriving the concept as above provides insights already in the tabular case. To see why this is so, note that in the tabular case the entries of w \u2208 R|S||A| are the function r(s, a) and suppose that r(s, a) 6= 0 in only a small subsetW \u2282 S \u00d7 A. From (2) and (3), it is clear that the cardinality ofW , and not of S \u00d7A, is what effectively defines the dimension of the representation \u03c8\u03c0 , since there is is no point in having d > |W|. Although this fact is hinted at in Dayan\u2019s [7] paper, it becomes much more apparent when we look at SR as a particular case of SFs.\nSFs extend Dayan\u2019s [7] SR in two ways. First, the concept readily applies to continuous state and action spaces without any modification. Second, by explicitly casting (2) and (3) as inner products involving feature vectors, SFs make it evident how to incorporate function approximation, since these vectors can clearly be learned from data. For reasons that will become apparent shortly, in this paper\nwe are mostly interested in learning w and \u03c8\u03c0(s, a). The extension to the scenario where \u03c6(s, a) must also be learned should not be difficult, though, and will also be discussed.\nThe SFs \u03c8\u03c0 are a way of summarizing the dynamics induced by \u03c0 in a given environment. As shown in (3), this allows for a modular representation of Q\u03c0 in which the MDP\u2019s dynamics are decoupled from its rewards, which are captured by w. One potential benefit of having such a decoupled representation is that only the relevant module must be relearned when either the dynamics or the reward changes. We come back to this point after describing how exactly each module can be learned.\nThe representation in (3) requires two components to be learned, w and \u03c8\u03c0. Since the latter is the expected discounted sum of \u03c6 under \u03c0, we either know \u03c6 or must learn it as well. Note that r(s, a) \u2248 \u03c6(s, a)>w is a supervised learning problem, so one can resort to one of the many wellunderstood techniques from the field to learn w (and potentially \u03c6, too)[8]. As for \u03c8\u03c0, we note that\n\u03c8\u03c0(s, a) = \u03c6t+1 + \u03b3E \u03c0[\u03c8\u03c0(St+1, \u03c0(St+1)) |St = s,At = a], (4)\nthat is, SFs satisfy a Bellman equation in which \u03c6i play the role of rewards\u2014something also noted by Dayan [7] regarding SR. Therefore, in principle any RL method can be used to compute \u03c8\u03c0 [21, 6].1\nNow that we have described how to learn \u03c8\u03c0 and w, we can resume the discussion on the potential benefits of doing so. Suppose that we have learned the value function of a given policy \u03c0 using the scheme shown in (3). It should be clear that whenever the reward function r(s, a) changes we only need to learn a new w. Similarly, whenever the dynamics of the MDP p(\u00b7|s, a) change we can relearn \u03c8\u03c0 while retaining in w the information that has remained the same. But SFs may be useful even if we restrict ourselves to the setting usually considered in RL, in which r(s, a) and p(\u00b7|s, a) are fixed. Note that the dynamics that determine Q\u03c0, and thus \u03c8\u03c0, depend on both p(\u00b7|s, a) and \u03c0. Hence, even when the former is fixed, the possibility of only relearning \u03c8\u03c0 may be advantageous when \u03c0 is changing, as is the case in the usual RL loop, since information regarding the reward function is preserved in w. This helps explain the good performance of SR in Dayan\u2019s [7] experiments and may also serve as an argument in favor of adopting SFs as a general approximation scheme for RL. However, in this paper we focus on a scenario where the decoupled value-function approximation provided by SFs is exploited to its full extent, as we discuss next."}, {"heading": "4 Transfer Via Successor Features", "text": "In this section we return to our discussion about transfer in RL. As described, we are interested in the scenario where all components of an MDP are fixed, except for the reward function. One way of formalizing this model is through (2): if we suppose that \u03c6 \u2208 Rd is fixed, any w \u2208 Rd gives rise to a new MDP. Based on this observation, we define\nM\u03c6(S,A, p, \u03b3) \u2261 {M(S,A, p, r, \u03b3) | r(s, a) = \u03c6(s, a)>w, with w \u2208 Rd}, (5)\nthat is,M\u03c6 is the set of MDPs induced by \u03c6 through all possible instantiations of w. Since what differentiates the MDPs inM\u03c6 is essentially the agent\u2019s goal, we will refer to Mi \u2208M\u03c6 as a task. The assumption is that we are interested in solving (a subset of) the tasks in the environmentM\u03c6. Unlike (2), which is not restrictive at all, supposing that a family of tasks of interest fit in the definition (5) will in general be a restrictive assumption.2 Despite the fact that similar assumptions have been made in the literature [1], we now describe some illustrative examples that suggest that our formulation ofM\u03c6 is a natural way of modeling some scenarios of interest. Perhaps the best way to motivate (5) is to note that some aspects of real environments which we clearly associate with specific features change their appeal over time. Think for example how much the desirability of water or food changes depending on whether an animal is thirsty or hungry. One way to model this type of preference shifting, which should probably occur with some types of\n1Yao et al. [25] discuss the properties of (3) when w and \u03c8\u03c0 are approximations learned in one specific way. However, the ideas presented here are not tightly coupled with any formulation of the two learning sub-problems.\n2It is not restrictive when d is greater than or equal to the number of MDPs we are interested in or when d \u2265 |S||A|. In the first case we can simply make the ith dimension of \u03c6(s, a) equal to the reward function of the ith MDP. As for the second case, if we can afford to use SR\u2014that is, if d = |S||A|\u2014M\u03c6 will include all possible reward functions over S \u00d7A.\nartificial agents as well, is to suppose that the vector w appearing in (2) reflects the taste of the agent at any given point in time. For a more concrete example, imagine that the agent\u2019s goal is to produce and sell a combination of goods whose production line is relatively stable but whose prices vary considerably over time. In this case updating the price of the products corresponds to picking a new w. Another intuitive example, which we will explore in the experimental section, is to imagine that the agent is navigating in a fixed environment but the goal location changes from time to time.\nIn all the examples above it is desirable for the agent to build on previous experience to improve its performance on a new setup. More concretely, if the agent knows good policies for the set of tasks M\u2261 {M1,M2, ...,Mn}, with Mi \u2208M\u03c6, it should be able to leverage this knowledge somehow to improve its behavior on a new task Mn+1\u2014that is, it should perform better than it would had it been exposed to only a subset of the original tasks,M\u2032 \u2282M. Here we assess the performance of an agent on Mn+1 based on the value function of the policy computed by the agent after receiving the new wn+1 but before any learning has taken place in Mn+1.3 More precisely, suppose that an agent agt has performed a number of transitions in each one of the tasks Mi \u2208M\u2032. Based on this experience and on the new wn+1, agt computes a policy \u03c0\u2032 that will define its initial behavior in Mn+1. Now, if we repeat the experience replacingM\u2032 withM, the value of the resulting policy \u03c0 should be such that Q\u03c0(s, a) \u2265 Q\u03c0\u2032(s, a) for all s \u2208 S and all a \u2208 A. Now that our setup is clear we can start to describe our solution for the transfer problem described above. We do so in two stages. First, we present a generalization of DP\u2019s notion of policy improvement whose interest may go beyond the current work. We then show how SFs can be used to implement this generalized form of policy improvement in an efficient and elegant way."}, {"heading": "4.1 Generalized Policy Improvement", "text": "One of the key results in DP is Bellman\u2019s [4] policy improvement theorem. Basically, the theorem states that acting greedily with respect to a policy\u2019s value function gives rise to another policy whose performance is no worse than the former\u2019s. This is the driving force behind DP, and any RL algorithm that uses the notion of a value function is exploiting Bellman\u2019s result in one way or another.\nIn this section we extend the policy improvement theorem to the scenario where the new policy is to be computed based on the value functions of a set of policies. We show that this extension can be done in a very natural way, by simply acting greedily with respect to the maximum over the value functions available. Our result is summarized in the theorem below. Theorem 1. (Generalized Policy Improvement) Let \u03c01, \u03c02, ..., \u03c0n be n decision policies and let Q\u0303\u03c01 , Q\u0303\u03c02 , ..., Q\u0303\u03c0n be approximations of their respective action-value functions such that\n|Q\u03c0i(s, a)\u2212 Q\u0303\u03c0i(s, a)| \u2264 for all s \u2208 S, a \u2208 A, and i \u2208 {1, 2, ..., n}. (6)\nDefine \u03c0(s) \u2208 argmaxa maxi Q\u0303\u03c0i(s, a). Then,\nQ\u03c0(s, a) \u2265 max i Q\u03c0i(s, a)\u2212 2 1\u2212 \u03b3 (7)\nfor any s \u2208 S and any a \u2208 A, where Q\u03c0 is the action-value function of \u03c0.\nThe proofs of our theoretical results are in Appendix A. As one can see, our theorem covers the case in which the policies\u2019 value functions are not computed exactly, either because function approximation is used or because some exact algorithm has not be run to completion. This error is captured by in (6), which of course re-appears as a \u201cpenalty\u201d term in the lower bound (7). Such a penalty is inherent to the presence of approximation in RL, and in fact it is identical to the penalty incurred in the single-policy case (see e.g. Bertsekas and Tsitsiklis\u2019s Proposition 6.1 [5]).\nIn order to contextualize our result within the broader scenario of DP, suppose for a moment that = 0. In this case Theorem 1 states that \u03c0 will perform no worse than all of the policies \u03c01, \u03c02, ..., \u03c0n. This is interesting because in general maxiQ\u03c0i\u2014the function used to induce \u03c0\u2014is not the value function of any particular policy. It is not difficult to see that \u03c0 will be strictly better than all previous policies if argmaxi maxa Q\u0303\n\u03c0i(s, a) \u2229 argmaxi maxa Q\u0303\u03c0i(s\u2032, a) = \u2205 for any s, s\u2032 \u2208 S , that is, if no 3Of course wn+1 can, and will be, learned, as discussed in Section 4.2 and illustrated in Section 5. Here\nthough we assume that wn+1 is given to make the definition of our performance criterion as clear as possible.\nsingle policy dominates all other policies. If one policy does dominate all others, Theorem 1 reduces to the original policy improvement theorem. Note that this will always be the case if one of the optimal policies \u03c0\u2217 belongs to the set {\u03c01, \u03c02, ..., \u03c0n}. If we consider the usual DP loop, in which policies of increasing performance are computed in sequence, our result is not of much use because the most recent policy will always dominate all others. Another way of putting it is to say that after Theorem 1 is applied once adding the resulting \u03c0 to the set {\u03c01, \u03c02, ..., \u03c0n} will reduce the next improvement step to standard policy improvement, and thus the policies \u03c01, \u03c02, ..., \u03c0n can be simply discarded.\nThere are however two situations in which our result may be of interest. One is when we have many policies \u03c0i being evaluated in parallel. In this case Theorem 1 provides a principled strategy for combining these policies. This could be used for example as an initialization scheme for policy iteration when it is possible to evaluate many policies simultaneously. This possibility may also be useful in RL when one consider the multi-agent setting. The other situation in which our result may be useful is when the underlying MDP changes, as we discuss next."}, {"heading": "4.2 Generalized Policy Improvement with Successor Features", "text": "We start this section by extending our notation slightly to make it easier to refer to the quantities involved in transfer learning. Let Mi be a task inM\u03c6 defined by wi \u2208 Rd. We will use \u03c0\u2217i to refer to an optimal policy of MDP Mi and use Q \u03c0\u2217 i i to refer to its value function. The value function of \u03c0 \u2217 i when executed in Mj \u2208M\u03c6 will be denoted by Q \u03c0\u2217 i j .\nSuppose now that an agent agt has computed optimal policies for the tasks M1,M2, ...,Mn \u2208M\u03c6. Suppose further that when exposed to a new task Mn+1 the agent computes Q \u03c0\u2217 i n+1\u2014the value functions of the policies \u03c0\u2217i under the new reward function induced by wn+1. In this case, applying Theorem 1 to the newly-computed set of value functions {Q\u03c0 \u2217 1 n+1, Q \u03c0\u22172 n+1, ..., Q \u03c0\u2217n n+1} will give rise to a policy that performs at least as well as a policy computed based on any subset of the set above, including the empty set (except of course in the unlikely event that one starts with a randomlygenerated policy that performs well). Thus, this strategy satisfies our definition of successful transfer.\nThere is a caveat, though. Why would one waste time computing the value functions of \u03c0\u22171 , \u03c0 \u2217 2 , ..., \u03c0\u2217n, whose performance in Mn+1 may be mediocre, if the same amount of resources can be allocated to compute a sequence of n policies with increasing performance? This is where SFs come into play. Suppose that we have learned the functions Q\u03c0\u2217i using the approximation scheme shown in (3). Now, if the reward changes to rn+1(s, a) = \u03c6(s, a)>wn+1, as long as we have wn+1 we can compute the new value function of \u03c0\u2217i by simply making Q \u03c0\u2217 i n+1(s, a) = \u03c8 \u03c0\u2217 i (s, a)>wn+1. This reduces the computation of all Q \u03c0\u2217 i n+1 to the much simpler supervised learning problem of computing wn+1.\nOnce Q \u03c0\u2217 i n+1 have been computed, we can apply Theorem 1 to derive a policy \u03c0 whose performance on Mn+1 is no worse than the performance of \u03c0\u22171 , \u03c0 \u2217 2 , ..., \u03c0 \u2217 n on the same task. A question that arises in this case is whether we can provide stronger guarantees on the performance of \u03c0 by exploiting the structure shared by the tasks inM\u03c6. The following theorem answers this question in the affirmative.\nTheorem 2. Let Mi \u2208 M\u03c6 and let Q \u03c0\u2217 j i be the value function of an optimal policy of Mj \u2208 M\u03c6 when executed in Mi. Given approximations {Q\u0303 \u03c0\u22171 i , Q\u0303 \u03c0\u22172 i , ..., Q\u0303\n\u03c0\u2217n i } such that\u2223\u2223\u2223Q\u03c0\u2217ji (s, a)\u2212 Q\u0303\u03c0\u2217ji (s, a)\u2223\u2223\u2223 \u2264 for all s \u2208 S, a \u2208 A, and j \u2208 {1, 2, ..., n}, (8)\nlet \u03c0(s) \u2208 argmaxa maxj Q\u0303 \u03c0\u2217 j\ni (s, a). Finally, let \u03c6max = maxs,a ||\u03c6(s, a)||, where || \u00b7 || is the norm induced by the inner product adopted. Then,\nQ \u03c0\u2217 i i (s, a)\u2212Q \u03c0 i (s, a) \u2264 2\n1\u2212 \u03b3 (\u03c6max minj ||wi \u2212wj ||+ ) . (9)\nNote that we used \u201cMi\u201d rather than \u201cMn+1\u201d in the theorem\u2019s statement to remove any suggestion of order among the tasks. This also makes it explicit that the result also applies when i \u2208 {1, 2, ..., n}. Theorem 2 is a specialization of Theorem 1 for the case where the set of value functions used to\ncompute \u03c0 are associated with tasks in the form of (5). As such, it provides stronger guarantees than its precursor: instead of comparing the performance of \u03c0 with that of the previously-computed policies \u03c0j , Theorem 2 quantifies the loss incurred by following \u03c0 as opposed to of one of Mi\u2019s optimal policies.\nAs shown in (9), the loss Q \u03c0\u2217 i i (s, a)\u2212Q\u03c0i (s, a) is upper-bounded by two terms. As before, 2 /(1\u2212\u03b3) is a \u201cpenalty\u201d term that shows up in the bound due to the use of approximations Q\u0303 \u03c0\u2217 j\ni instead of the true value functions Q \u03c0\u2217 j\ni . The term 2\u03c6maxminj ||wi \u2212 wj ||/(1 \u2212 \u03b3) is of more interest here because it reflects the structure ofM\u03c6. This term is a multiple of the distance between wi, the vector describing the task we are currently interested in, and the closest wj for which we have computed a policy. This formalizes the intuition that the agent should perform well in task wi if it has solved a similar task before. More generally, the term in question relates the concept of distance in Rd with difference in performance inM\u03c6, which allows for interesting extrapolations. For example, if we assume that the tasks wj are sampled from a distribution over Rd, it might be possible to derive probabilistic performance guarantees whose probability of failure goes to zero as n\u2192\u221e. Although Theorem 2 is inexorably related to the characterization ofM\u03c6 in (5), it does not depend on the definition of SFs in any way. Here SFs are the mechanism used to efficiently apply the protocol suggested by Theorem 2. When SFs are used the value function approximations are given by Q\u0303 \u03c0\u2217 j\ni (s, a) = \u03c8\u0303 \u03c0\u2217 j (s, a)>w\u0303i. The modules \u03c8\u0303 \u03c0\u2217 j are computed and stored when the agent is learning the tasks Mj ; when faced with a new task Mi the agent computes an approximation of wi, which is a supervised learning problem, and then uses the policy \u03c0 defined in Theorem 2 to learn \u03c8\u0303\u03c0 \u2217 i . Note that we do not assume that either \u03c8\u03c0 \u2217 j or wi is computed exactly: the effect of errors in \u03c8\u0303 \u03c0\u2217 j and w\u0303i in the approximation of Q \u03c0\u2217 j\ni (s, a) is accounted for by the term appearing in (8). As shown in (9), if is small and the agent has seen enough tasks the performance of \u03c0 on Mi should already be good, which suggests that it will also speed up the process of learning \u03c8\u0303\u03c0 \u2217 i . In the next section we verify empirically how these effects manifest in practice."}, {"heading": "5 Experiments", "text": "In this section we use experiments to illustrate how the transfer promoted by the combination of generalized policy iteration and SFs actually takes place in practice. In order to do so we introduce a generalized version of a classic RL task known as the \u201cpuddle world\u201d [20]. The puddle world is a simple two-dimensional problem with a goal position and two elliptical \u201cpuddles,\u201d one vertical and one horizontal [20]. The four actions available move the agent up, down, left, or right. An action fails with probability 0.1, in which case an action selected uniformly at random is executed. The objective is to reach the goal while avoiding the puddles along the way.\nWe generalized the puddle world task by letting the position of the puddles and of the goal state to change at arbitrary time steps. More specifically, we implemented the task as a 15 \u00d7 15 grid and restricted the position of the elements to a subset of the cells: the goal is only allowed to be in one of the four corners and the two puddles are restricted to the set {3, 5, 7, 9, 11} \u00d7 {3, 5, 7, 9, 11}. This gives rise to 2500 possible configurations of the task, which is our setM\u03c6. Following (5), the reward function for the ith task was defined as r(s, a) = \u03c6(s, a)>wi. Here \u03c6(s, a) is a binary vector in R54 that indicates whether the state that (s, a) most likely leads to corresponds to a puddle or a goal. Specifically, if the ith entry of \u03c6(s, a) is associated with, say, one of the possible 25 locations of the horizontal puddle, it will be equal to 1 if and only if (s, a) has as its most likely outcome the state at that location. The goal and puddles that are present in the ith task are indicated by three nonzero elements in wi: a +1 entry associated with the goal and a \u22121 entry associated with each puddle. We focus on the online RL scenario where the agent must learn while interacting with the environment M\u03c6. The task changes at every k transitions, with a new wi selected uniformly at random from the set described above. We adopted Watkins and Dayan\u2019s [24] Q-learning as our basic algorithm and combined it with different representation schemes to show their potential for transfer. In particular, we compared four versions of Q-learning: using a tabular representation (QL), using a tabular representation that is reinitialized to zero whenever the task changes (QLR), using SR, and using SFs. All versions of the algorithm used an -greedy policy to explore the environment, with = 0.15 [21].\nThe SR and SF agents were implemented in the following way. The action-value function was represented as Q\u0303\u03c0(s, a) = \u03c8\u0303 \u03c0\ni (s, a) >w\u0303, where \u03c8\u0303\n\u03c0 i (s, a) is associated with the i th task. Both\n\u03c8\u0303 \u03c0\ni and w\u0303 were learned online. The former was learned using temporal-difference updates to solve (4) [21], while the latter was learned as a least-squares minimization of the difference between the two sides of (2). Every time the task changed the current \u03c8\u0303 \u03c0 i (s, a) was stored, a new \u03c8\u0303 \u03c0\ni+1(s, a) was created, and w was reinitialized to 0. The agent followed a 0.15-greedy policy with respect to argmaxa maxj\u2208{1,2,...,i+1} \u03c8\u0303 \u03c0 j (s, a) >w\u0303 (hence the policy \u03c0 that induces (4) was constantly changing). In the case of SR, \u03c8\u0303 \u03c0\ni (s, a) were vectors in R|S||A|, as usual (here |S||A| = 900). The SF agent received with each (s, a) the corresponding vector \u03c6(s, a). So, in this case \u03c8\u0303 \u03c0\ni (s, a) \u2208 R54. The results of our experiments are shown in Figure 1. Several interesting observations can be made regarding the figure. First, note that QLR is unable to learn the task. If we compare it with QL, the difference in performance suggests that in this environment starting from an actual value function Q\u03c0(s, a) leads to better results than starting from a function that is zero everywhere, regardless of the policy \u03c0 associated with Q\u03c0(s, a). Also note that there is a clear improvement on the algorithms\u2019 performance as the interval k to change the reward increases, which is not surprising. However, the most important point to be highlighted here is that both SR and SF significantly outperform the standard version of Q-learning. This is an illustration of our theoretical results and a demonstration that the proposed approach is indeed able to successfully transfer knowledge across tasks. Finally, note that SF outperforms SR by a considerable margin. This is also expected: since the former uses a vector w that is in R54 rather than in R|S||A|, the nonzero elements of this vector will be updated whenever the agent encounters a puddle or a goal, regardless of the specific (s, a) pair that lead to that state. This shows how, unlike its precursor, SFs allows for generalization."}, {"heading": "6 Related Work", "text": "In this paper we present SFs, a representation scheme for value functions, and show how they provide a natural framework for implementing transfer in RL. Both representation and transfer are active areas of research in RL; in what follows we briefly describe the previous work we consider to be more closely related to ours and take advantage of the relevant connections to point out some interesting directions for future research.\nWhen it comes to representation, a lot of effort in previous research has been directed towards the development of methods to automatically compute good features to represent the value function [14, 10, 16, 15]. Many of these approaches build on the fact that, when S is finite, the value function of a policy \u03c0 is given by v\u03c0 = \u2211\u221e i=0(\u03b3P\n\u03c0)ir\u03c0, where p\u03c0ij = p(sj |si, \u03c0(si)) and r\u03c0i = r(si, \u03c0(si)). The idea is to exploit the structure in the definition of v\n\u03c0 to replace the set of vectors (\u03b3P\u03c0)ir\u03c0 \u2208 R|S|, which is infinite, by a properly defined basis whose cardinality is preferably smaller than |S|. If we adopt the reward model in (2), we can rewrite the previous expression as v\u03c0 = \u2211\u221e i=0(\u03b3P \u03c0)i\u03a6\u03c0w, where \u03a6 is a vector in R|S|\u00d7d whose ith row is \u03c6(si, \u03c0(si)). If we\nthen define \u03a8\u03c0 = \u2211\u221e i=0(\u03b3P\n\u03c0)i\u03a6\u03c0, it should be clear that the ith row of \u03a8\u03c0 is \u03c8\u03c0(si, \u03c0(si)). This shows that \u03a8\u03c0 arises as a natural basis when (2) is adopted, which is neither very surprising nor very useful, since \u03a8\u03c0 lives in R|S|\u00d7d. What is perhaps more interesting is the observation that, since the definitions of v\u03c0 and \u03a8\u03c0 are very similar, the methods cited above could in principle also be used to find good features to represent \u03a8\u03c0 itself.\nAlthough the methods above decouple the construction of features from the actual RL problem, it is also possible to tackle both problems concomitantly, using general nonlinear function approximators to incrementally learn \u03c8\u03c0(s, a) [12]. Another interesting possibility is the definition of a clear protocol to also learn \u03c6(s, a), which is closely related to the problem known as \u201cmulti-task feature learning\u201d [1]. Here again the use of nonlinear approximators may be useful, since with them it may be possible to embed an arbitrary family of MDPs into a modelM\u03c6 with the structure shown in (5) [11]. Still on the subject of representation, a scheme that also relates to SFs is Littman et al.\u2019s [9] predictive state representation (PSR). PSRs are similar to SFs in the sense that they also have a prediction at the core of their representation. Unlike the latter, though, the former tries to use such predictions to summarize the dynamics of the entire environment rather than of a single policy \u03c0. A scheme that is perhaps closer to SFs is the value function representation sometimes adopted in inverse RL [13]. The scenario considered in this case is considerably different, though, since the focus is in finding a w that induces a predefined policy \u03c0.\nWe now turn our attention to previous work related to the use of SFs for transfer. As mentioned in the introduction, the problem of transfer has many definitions in the literature [23]. When we focus on the scenario considered here, in which the agent must perform well on a family of MDPs that differ only in the reward function, two approaches are possible. One of them is to learn a model of the MDPs\u2019 dynamics [3]. Another alternative, which is more in-line with our approach, is to summarize the experience using policies or value functions\u2014which in some sense represent a \u201cpartial model\u201d of the environment. Among these, Schaul et al.\u2019s [18] universal value function approximators (UVFAs) are particularly relevant to our work. UVFAs extend the notion of value function to also include as an argument a representation of a goal. We note that the function maxj \u03c8\u0303 \u03c0\u2217 j (s, a)>w used in our generalized policy improvement framework can be seen as a function of s, a, and w\u2014the latter a generic way of representing a \u201cgoal.\u201d Thus, in some sense the approximation scheme proposed here is a UVFA, in which w corresponds to the learned goal embedding.\nAs discussed, one possible interpretation of the scenario studied here is that there is one main task that has been decomposed in many sub-tasks. This view of transfer highlights an interesting connection between our approach and temporal abstraction. In fact, if we look at \u03c8\u03c0 as instances of Sutton et al.\u2019s [22] options, acting greedily with respect to the maximum over their value functions corresponds in some sense to planning at a higher level of temporal abstraction. This is the view adopted by Yao et al. [25], whose universal option model closely resembles our approach in some aspects. The main difference is that, unlike in our method, in Yao et al.\u2019s [25] approach options are not used to learn new options."}, {"heading": "7 Conclusion", "text": "This paper builds on two concepts, both of which are generalizations of previous ideas. The first one is SFs, a generalization of Dayan\u2019s [7] SR that extends the original definition from discrete to continuous spaces and also facilitates the incorporation of function approximation. The second concept is generalized policy improvement, formalized in Theorem 1. As the name suggests, this result extends Bellman\u2019s [4] classic policy improvement theorem from a single to multiple policies.\nAlthough SFs and generalized policy improvement are of interest on their own, in this paper we focus on their combination to induce transfer. The resulting framework is an elegant extension of DP\u2019s basic setting that provides a solid foundation for transfer in RL. We derived a theoretical result, Theorem 2, that formalizes the intuition that an agent should perform well on a novel task if it has seen a similar task before. We also illustrated how this effect manifests in practice using experiments.\nWe believe the ideas presented in this paper lay out a general framework for transfer in RL. By specializing the basic components presented here one can build on our results to derive agents able to perform well across a wide variety of tasks, and thus handle environments of considerable complexity."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Joseph Modayil and Hado van Hasselt for the invaluable discussions during the development of the ideas described in this paper. We also thank Peter Dayan, Matt Botvinick, Marc Bellemare, and Guy Lever for all the excellent comments. Finally, we thank Dan Horgan and Alexander Pritzel for their help with the experiments."}, {"heading": "A Proofs", "text": "Theorem 1. (Generalized Policy Improvement) Let \u03c01, \u03c02, ..., \u03c0n be n decision policies and let Q\u0303\u03c01 , Q\u0303\u03c02 , ..., Q\u0303\u03c0n be approximations of their respective action-value functions such that\n|Q\u03c0i(s, a)\u2212 Q\u0303\u03c0i(s, a)| \u2264 for all s \u2208 S, a \u2208 A, and i \u2208 {1, 2, ..., n}. Define\n\u03c0(s) \u2208 argmax a max i Q\u0303\u03c0i(s, a).\nThen,\nQ\u03c0(s, a) \u2265 max i Q\u03c0i(s, a)\u2212 2 1\u2212 \u03b3\nfor any s \u2208 S and any a \u2208 A, where Q\u03c0 is the action-value function of \u03c0.\nProof. To simplify the notation, let Qmax(s, a) = maxiQ\n\u03c0i(s, a) and Q\u0303max(s, a) = maxi Q\u0303\u03c0i(s, a). We start by noting that for any s \u2208 S and any a \u2208 A the following holds: |Qmax(s, a)\u2212 Q\u0303max(s, a)| = |max\ni Q\u03c0i(s, a)\u2212max i Q\u0303\u03c0i(s, a)| \u2264 max i |Q\u03c0i(s, a)\u2212 Q\u0303\u03c0i(s, a)| \u2264 .\nFor all s \u2208 S, a \u2208 A, and i \u2208 {1, 2, ..., n} we have T\u03c0Q\u0303max(s, a) = r(s, a) + \u03b3 \u2211 s\u2032 p(s\u2032|s, a)Q\u0303max(s\u2032, \u03c0(s\u2032))\n= r(s, a) + \u03b3 \u2211 s\u2032 p(s\u2032|s, a) max b Q\u0303max(s \u2032, b)\n\u2265 r(s, a) + \u03b3 \u2211 s\u2032 p(s\u2032|s, a) max b Qmax(s \u2032, b)\u2212 \u03b3\n\u2265 r(s, a) + \u03b3 \u2211 s\u2032 p(s\u2032|s, a)Qmax(s\u2032, \u03c0i(s\u2032))\u2212 \u03b3\n\u2265 r(s, a) + \u03b3 \u2211 s\u2032 p(s\u2032|s, a)Q\u03c0i(s\u2032, \u03c0i(s\u2032))\u2212 \u03b3 = T\u03c0iQ\u03c0i(s, a)\u2212 \u03b3 = Q\u03c0i(s, a)\u2212 \u03b3 .\nSince T\u03c0Q\u0303max(s, a) \u2265 Q\u03c0i(s, a)\u2212 \u03b3 for any i, it must be the case that\nT\u03c0Q\u0303max(s, a) \u2265 max i Q\u03c0i(s, a)\u2212 \u03b3\n= Qmax(s, a)\u2212 \u03b3 \u2265 Q\u0303max(s, a)\u2212 \u2212 \u03b3 .\nLet e(s, a) = 1 for all s, a \u2208 S \u00d7 A. It is well known that T\u03c0(Q\u0303max(s, a) + ce(s, a)) = T\u03c0Q\u0303max(s, a) + \u03b3c for any c \u2208 R. Using this fact together with the monotonicity and contraction properties of the Bellman operator T\u03c0 , we have\nQ\u03c0(s, a) = lim k\u2192\u221e (T\u03c0)kQ\u0303max(s, a)\n\u2265 Q\u0303max(s, a)\u2212 1 + \u03b3\n1\u2212 \u03b3\n\u2265 Qmax(s, a)\u2212 \u2212 1 + \u03b3\n1\u2212 \u03b3 .\nLemma 1. Let \u03b4ij = maxs,a |ri(s, a)\u2212 rj(s, a)|. Then,\nQ \u03c0\u2217i i (s, a)\u2212Q \u03c0\u2217j i (s, a) \u2264 2\u03b4ij 1\u2212 \u03b3 .\nProof. To simplify the notation, let Qji (s, a) \u2261 Q \u03c0\u2217j i (s, a). Then,\nQii(s, a)\u2212Q j i (s, a) = Q i i(s, a)\u2212Q j j(s, a) +Q j j(s, a)\u2212Q j i (s, a)\n\u2264 |Qii(s, a)\u2212Q j j(s, a)|+ |Q j j(s, a)\u2212Q j i (s, a)|. (10)\nOur strategy will be to bound |Qii(s, a)\u2212Q j j(s, a)| and |Q j j(s, a)\u2212Q j i (s, a)|. Note that |Qii(s, a)\u2212 Qjj(s, a)| is the difference between the value functions of two MDPs with the same transition function but potentially different rewards. Let \u2206ij = maxs,a |Qii(s, a)\u2212Q j j(s, a)|. Then, 4\n|Qii(s, a)\u2212Q j j(s, a)| = \u2223\u2223\u2223\u2223\u2223ri(s, a) + \u03b3\u2211 s\u2032 p(s\u2032|s, a) max b Qii(s \u2032, b)\u2212 rj(s, a)\u2212 \u03b3 \u2211 s\u2032 p(s\u2032|s, a) max b Qjj(s \u2032, b) \u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223ri(s, a)\u2212 rj(s, a) + \u03b3\u2211 s\u2032 p(s\u2032|s, a) ( max b Qii(s \u2032, b)\u2212max b Qjj(s \u2032, b) )\u2223\u2223\u2223\u2223\u2223 \u2264 |ri(s, a)\u2212 rj(s, a)|+ \u03b3\n\u2211 s\u2032 p(s\u2032|s, a) \u2223\u2223\u2223\u2223maxb Qii(s\u2032, b)\u2212maxb Qjj(s\u2032, b) \u2223\u2223\u2223\u2223 \u2264 |ri(s, a)\u2212 rj(s, a)|+ \u03b3\n\u2211 s\u2032 p(s\u2032|s, a) max b \u2223\u2223\u2223Qii(s\u2032, b)\u2212Qjj(s\u2032, b)\u2223\u2223\u2223 \u2264 \u03b4ij + \u03b3\u2206ij . (11)\nSince (11) is valid for any s, a \u2208 S \u00d7A, we have shown that \u2206ij \u2264 \u03b4ij + \u03b3\u2206ij . Solving for \u2206ij we get\n\u2206ij \u2264 1\n1\u2212 \u03b3 \u03b4ij . (12)\n4We follow the steps of Strehl and Littman [19].\nWe now turn our attention to |Qjj(s, a) \u2212 Q j i (s, a)|. Following the previous steps, define \u2206\u2032ij = maxs,a |Qii(s, a)\u2212Q j i (s, a)|. Then,\n|Qjj(s, a)\u2212Q j i (s, a)| = \u2223\u2223\u2223\u2223\u2223rj(s, a) + \u03b3\u2211 s\u2032 p(s\u2032|s, a)Qjj(s \u2032, \u03c0\u2217j (s \u2032))\u2212 ri(s, a)\u2212 \u03b3 \u2211 s\u2032 p(s\u2032|s, a)Qji (s \u2032, \u03c0\u2217j (s \u2032)) \u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223ri(s, a)\u2212 rj(s, a) + \u03b3\u2211 s\u2032 p(s\u2032|s, a) ( Qjj(s \u2032, \u03c0\u2217j (s \u2032))\u2212Qji (s \u2032, \u03c0\u2217j (s \u2032)) )\u2223\u2223\u2223\u2223\u2223\n\u2264 |ri(s, a)\u2212 rj(s, a)|+ \u03b3 \u2211 s\u2032 p(s\u2032|s, a) \u2223\u2223\u2223Qjj(s\u2032, \u03c0\u2217j (s\u2032))\u2212Qji (s\u2032, \u03c0\u2217j (s\u2032))\u2223\u2223\u2223 \u2264 \u03b4ij + \u03b3\u2206\u2032ij .\nSolving for \u2206\u2032ij , as above, we get\n\u2206\u2032ij \u2264 1\n1\u2212 \u03b3 \u03b4ij . (13)\nPlugging (12) and (13) back in (10) we get the desired result.\nTheorem 2. Let Mi \u2208 M\u03c6 and let Q \u03c0\u2217j i be the value function of an optimal policy of Mj \u2208 M\u03c6 when executed in Mi. Given the set {Q\u0303 \u03c0\u22171 i , Q\u0303 \u03c0\u22172 i , ..., Q\u0303\n\u03c0\u2217n i } such that\u2223\u2223\u2223Q\u03c0\u2217ji (s, a)\u2212 Q\u0303\u03c0\u2217ji (s, a)\u2223\u2223\u2223 \u2264 for all s \u2208 S, a \u2208 A, and j \u2208 {1, 2, ..., n},\nlet \u03c0(s) \u2208 argmax\na max j Q\u0303 \u03c0\u2217j i (s, a).\nFinally, let \u03c6max = maxs,a ||\u03c6(s, a)||, where || \u00b7 || is the norm induced by the inner product adopted. Then,\nQ\u2217i (s, a)\u2212Q\u03c0i (s, a) \u2264 2\n1\u2212 \u03b3 (\u03c6max minj ||wi \u2212wj ||+ ) .\nProof. The result is a direct application of Theorem 1 and Lemma 1. For any j \u2208 {1, 2, ..., n}, we have\nQ\u2217i (s, a)\u2212Q\u03c0i (s, a) \u2264 Q\u2217i (s, a)\u2212Q \u03c0\u2217j i (s, a) + 2\n1\u2212 \u03b3 (Theorem 1)\n\u2264 2 1\u2212 \u03b3 maxs,a |ri(s, a)\u2212 rj(s, a)|+ 2 1\u2212 \u03b3 (Lemma 1) = 2\n1\u2212 \u03b3 maxs,a |\u03c6(s, a)>wi \u2212 \u03c6(s, a)>wj |+\n2\n1\u2212 \u03b3\n= 2\n1\u2212 \u03b3 maxs,a |\u03c6(s, a)>(wi \u2212wj)|+\n2\n1\u2212 \u03b3\n\u2264 2 1\u2212 \u03b3 maxs,a ||\u03c6(s, a)|| ||wi \u2212wj ||+ 2 1\u2212 \u03b3 (Cauchy-Schwarz\u2019s inequality) = 2\u03c6max 1\u2212 \u03b3 ||wi \u2212wj ||+ 2 1\u2212 \u03b3 ."}], "references": [{"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Effective control knowledge transfer through learning skill and representation hierarchies", "author": ["Mehran Asadi", "Manfred Huber"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "A comparison of direct and model-based reinforcement learning", "author": ["Christopher G. Atkeson", "J. Santamaria"], "venue": "In Proceedings of the IEEE International Conference on Robotics and Automation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Dynamic Programming", "author": ["Richard E. Bellman"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1957}, {"title": "Technical update: Least-squares temporal difference learning", "author": ["Justin A. Boyan"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Improving generalization for temporal difference learning: The successor representation", "author": ["Peter Dayan"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Predictive representations of state", "author": ["Michael L. Littman", "Richard S. Sutton", "Satinder Singh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes", "author": ["Sridhar Mahadevan", "Mauro Maggioni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Ng", "Stuart Russell"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Analyzing feature generation for value-function approximation", "author": ["Ronald Parr", "Christopher Painter-Wakefield", "Lihong Li", "Michael Littman"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning", "author": ["Ronald Parr", "Lihong Li", "Gavin Taylor", "Christopher Painter-Wakefield", "Michael L. Littman"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "An analysis of laplacian methods for value function approximation in MDPs", "author": ["Marek Petrik"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Markov Decision Processes\u2014Discrete Stochastic Dynamic Programming", "author": ["Martin L. Puterman"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Universal Value Function Approximators", "author": ["Tom Schaul", "Daniel Horgan", "Karol Gregor", "David Silver"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Alexander L. Strehl", "Michael L. Littman"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["Richard S. Sutton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Between MDPs and semi-MDPs: a framework for temporal abstraction in reinforcement learning", "author": ["Richard S. Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E. Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}], "referenceMentions": [{"referenceID": 19, "context": "Reinforcement learning (RL) provides a framework for the development of situated agents that learn how to behave while interacting with the environment [21].", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "Transfer in reinforcement learning can be defined in many different ways, but in general it refers to the notion that generalization should occur not only within a task but also across tasks [23].", "startOffset": 191, "endOffset": 195}, {"referenceID": 5, "context": "The first one is a generalization of a concept proposed by Dayan [7] called successor representation.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "The first one is a generalization of Bellman\u2019s [4] classic policy improvement theorem that extends the original result from one to multiple decision policies.", "startOffset": 47, "endOffset": 50}, {"referenceID": 19, "context": "We consider the framework of RL outlined in the introduction: an agent interacts with an environment and selects actions in order to maximize the expected amount of reward received in the long run [21].", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "As usual, we assume that this interaction can be modeled as a Markov decision process (MDP, Puterman, [17]).", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "One way to address this problem is to use methods derived from dynamic programming (DP), which heavily rely on the concept of a value function [17].", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "These two steps, policy evaluation and policy improvement, define the basic mechanics of RL algorithms based on DP; under certain conditions their successive application leads to an optimal policy \u03c0\u2217 that maximizes the expected return from every state in S [21].", "startOffset": 257, "endOffset": 261}, {"referenceID": 5, "context": "We start by presenting a simple reward model and then show how it naturally leads to a generalization of Dayan\u2019s [7] successor representation (SR).", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "This is essentially the concept of SR extended from the space S to the set S\u00d7A [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "Although this fact is hinted at in Dayan\u2019s [7] paper, it becomes much more apparent when we look at SR as a particular case of SFs.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "SFs extend Dayan\u2019s [7] SR in two ways.", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "Note that r(s, a) \u2248 \u03c6(s, a)>w is a supervised learning problem, so one can resort to one of the many wellunderstood techniques from the field to learn w (and potentially \u03c6, too)[8].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "that is, SFs satisfy a Bellman equation in which \u03c6i play the role of rewards\u2014something also noted by Dayan [7] regarding SR.", "startOffset": 107, "endOffset": 110}, {"referenceID": 19, "context": "Therefore, in principle any RL method can be used to compute \u03c8 [21, 6].", "startOffset": 63, "endOffset": 70}, {"referenceID": 4, "context": "Therefore, in principle any RL method can be used to compute \u03c8 [21, 6].", "startOffset": 63, "endOffset": 70}, {"referenceID": 5, "context": "This helps explain the good performance of SR in Dayan\u2019s [7] experiments and may also serve as an argument in favor of adopting SFs as a general approximation scheme for RL.", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "2 Despite the fact that similar assumptions have been made in the literature [1], we now describe some illustrative examples that suggest that our formulation ofM is a natural way of modeling some scenarios of interest.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "One of the key results in DP is Bellman\u2019s [4] policy improvement theorem.", "startOffset": 42, "endOffset": 45}, {"referenceID": 18, "context": "In order to do so we introduce a generalized version of a classic RL task known as the \u201cpuddle world\u201d [20].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "The puddle world is a simple two-dimensional problem with a goal position and two elliptical \u201cpuddles,\u201d one vertical and one horizontal [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "15 [21].", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The former was learned using temporal-difference updates to solve (4) [21], while the latter was learned as a least-squares minimization of the difference between the two sides of (2).", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "When it comes to representation, a lot of effort in previous research has been directed towards the development of methods to automatically compute good features to represent the value function [14, 10, 16, 15].", "startOffset": 194, "endOffset": 210}, {"referenceID": 8, "context": "When it comes to representation, a lot of effort in previous research has been directed towards the development of methods to automatically compute good features to represent the value function [14, 10, 16, 15].", "startOffset": 194, "endOffset": 210}, {"referenceID": 14, "context": "When it comes to representation, a lot of effort in previous research has been directed towards the development of methods to automatically compute good features to represent the value function [14, 10, 16, 15].", "startOffset": 194, "endOffset": 210}, {"referenceID": 13, "context": "When it comes to representation, a lot of effort in previous research has been directed towards the development of methods to automatically compute good features to represent the value function [14, 10, 16, 15].", "startOffset": 194, "endOffset": 210}, {"referenceID": 10, "context": "Although the methods above decouple the construction of features from the actual RL problem, it is also possible to tackle both problems concomitantly, using general nonlinear function approximators to incrementally learn \u03c8(s, a) [12].", "startOffset": 230, "endOffset": 234}, {"referenceID": 0, "context": "Another interesting possibility is the definition of a clear protocol to also learn \u03c6(s, a), which is closely related to the problem known as \u201cmulti-task feature learning\u201d [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 9, "context": "Here again the use of nonlinear approximators may be useful, since with them it may be possible to embed an arbitrary family of MDPs into a modelM with the structure shown in (5) [11].", "startOffset": 179, "endOffset": 183}, {"referenceID": 7, "context": "\u2019s [9] predictive state representation (PSR).", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "A scheme that is perhaps closer to SFs is the value function representation sometimes adopted in inverse RL [13].", "startOffset": 108, "endOffset": 112}, {"referenceID": 21, "context": "As mentioned in the introduction, the problem of transfer has many definitions in the literature [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "One of them is to learn a model of the MDPs\u2019 dynamics [3].", "startOffset": 54, "endOffset": 57}, {"referenceID": 16, "context": "\u2019s [18] universal value function approximators (UVFAs) are particularly relevant to our work.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "\u2019s [22] options, acting greedily with respect to the maximum over their value functions corresponds in some sense to planning at a higher level of temporal abstraction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "The first one is SFs, a generalization of Dayan\u2019s [7] SR that extends the original definition from discrete to continuous spaces and also facilitates the incorporation of function approximation.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "As the name suggests, this result extends Bellman\u2019s [4] classic policy improvement theorem from a single to multiple policies.", "startOffset": 52, "endOffset": 55}], "year": 2016, "abstractText": "Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. Our focus is on transfer where the reward functions vary across tasks while the environment\u2019s dynamics remain the same. The method we propose rests on two key ideas: \u201csuccessor features,\u201d a value function representation that decouples the dynamics of the environment from the rewards, and \u201cgeneralized policy improvement,\u201d a generalization of dynamic programming\u2019s policy improvement step that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows transfer to take place between tasks without any restriction. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice.", "creator": "LaTeX with hyperref package"}}}