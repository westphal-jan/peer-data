{"id": "1606.05597", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Adding Context to Concept Trees", "abstract": "molvi Concept Trees are a type of saqar database that can emile organise pietrangelo arbitrary textual ljuba information using a very simple 95.74 rule. motacilla Each counter-attacked tree 15.51 tries 1985-1991 to syomin represent dodsley a roca single muire cohesive concept panyangara and mounth the trees roderigo can link with each goislard other for rvers navigation gedmonson and fanara semantic purposes. The trees are grabow therefore geocaching a type a\u00e7ores of re-equipment semantic network threated and tasters would fur-bearing benefit rodrigue from having a jugraj consistent agur level of context for evropa each 76-centimeter of npcs the nodes. The chislehurst Concept Tree nodes impersonates have budennovsk a jijiga mathematical basis allowing ideogram for a consistent 3.55 build antenor process. These would waterkloof represent nouns 2,123 or ungated verbs in a tourab text gendarmes sentence, for 1621 example. bolong New to the design can then lontra be lists malam of descriptive elements horeb for each 4-martin of the kemeys nodes. mallow The descriptors barclay can bidding also be manville weighted, feherty but do not have to follow swell the 1malaysia strict camels counting rule of greatness the meadmore tree mandela nodes. cz\u0142opa With 1739 the 18.07 new descriptive fsny layers, s.dollars a pieni\u0119\u017cno much \u00f6hringen richer type of 73.56 knowledge can ammunitions be accentuates achieved pierrick and still reasoned over automatically. The linking myoclonic structure cand.jur of earthrace the 22-meter licas network 18-64 is very kuu relevant to building weidner the ksa concept trees now and forms akitaka the scroungers basis hallgren for jumps their construction. husar The garrisoning concept touriga tree - tudhaliya symbolic neural gorecki network flaget relation is nam\u0131k also latently extended double-elimination further.", "histories": [["v1", "Fri, 17 Jun 2016 17:32:11 GMT  (522kb)", "http://arxiv.org/abs/1606.05597v1", null], ["v2", "Tue, 12 Sep 2017 10:58:32 GMT  (747kb)", "http://arxiv.org/abs/1606.05597v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kieran greer"], "accepted": false, "id": "1606.05597"}, "pdf": {"name": "1606.05597.pdf", "metadata": {"source": "CRF", "title": "Adding Context to Concept Trees", "authors": ["Kieran Greer"], "emails": [], "sections": [{"heading": null, "text": "using a very simple rule. Each tree tries to represent a single cohesive concept and the trees can link with each other for navigation and semantic purposes. The trees are therefore a type of semantic network and would benefit from having a consistent level of context for each of the nodes. The Concept Tree nodes have a mathematical basis allowing for a consistent build process. These would represent nouns or verbs in a text sentence, for example. New to the design can then be lists of descriptive elements for each of the nodes. The descriptors can also be weighted, but do not have to follow the strict counting rule of the tree nodes. With the new descriptive layers, a much richer type of knowledge can be achieved and still reasoned over automatically. The linking structure of the licas network is very relevant to building the concept trees now and forms the basis for their construction. The concept tree - symbolic neural network relation is also extended further.\nKeywords: Concept, tree, context, link, semantic, self-organise."}, {"heading": "1 Introduction", "text": "The term \u2018concept base\u2019 has been used previously ([6] or [11], for example) and has been adopted in [4] to describe a database of heterogeneous sources or arbitrary concepts. They would be more similar to an OO database than a relational one, for example. The term \u2018concept\u2019 can be used to describe a single value or a complex entity equally and so the concept base can store information from any kind of data source. If the information is arbitrary, it is probably the case that some level of structure must first be added to the information, before it can be consistently processed, data mined, or reasoned over. A formal and mathematical process for doing this was described in [3], where tree-like structures could be built to represent the more arbitrary concept groups that are presented. A concept tree is essentially and AND/OR graph of related concepts in the sense that\nvertically, all nodes in a path would be included, but different sibling branches can be selected. The mathematical rule that was introduced allows for a consistent structure across all of the trees, giving each tree structure more integrity. Even if the concept that it represents is quite abstract, the integrity would mean that it can be used or shared between different scenarios. So the tree structure is the basic building block for the database and can also be normalised. This paper extends the initial concept tree structure, by adding layers of descriptive elements to each node. The tree nodes form a more permanent structure, but descriptors can put them into context. This context can help with a more dynamic type of reasoning and the descriptors can also form associations. To keep the structure flexible, the dynamic linking mechanism [5] can be used to link descriptors. The linking structure of the licas [5][7] network is very relevant in building the concept trees now and forms the basis for their construction. As part of a more complete system, the relation with the symbolic neural network [4] is developed again in the following sections.\nThe rest of this paper is organised as follows: section 2 reviews concept trees and introduces the new descriptor layers. Section 3 gives some related work. Section 4 describes a possible query language and process, while section 5 describes the computer program and tests that have been written. Finally, section 6 gives some conclusions on the work."}, {"heading": "2 Concept Trees", "text": "The concept trees have been described previously in [3] and again in [2]. They start with a base node that is extended by branches of related nodes. A strict frequency count rule does not allow a branch node to have a higher count than the parent node. This is actually the case if the tree is always updated starting from the base, but might not be the case if the tree can be updated from other places. As part of the construction process therefore, trees can be split into two or more, where links between the parts can be maintained, to help with navigation between related concepts. The paper [3] gives a more formal set of rules that would help to build the concept base in a normalised way and also how to update and re-structure it. Figure 1 is an example of what a concept base might look like with 3 trees.\nBecause \u2018drank milk\u2019 is important to more than one tree, it is a separate tree that gets linked to (key M) by the other concepts, for example.\nIf a tree node always links to the base of another tree, then similarities with the pointerbased system in computer programming languages can be made and might help with understanding. The linking mechanisms are mostly dynamic, where the frequency counts are already that way and so there is probably no reason to replace that with the 3-level linking mechanism [5]. Links between trees could use the linking mechanism however, because the path description can store key values, for example. For the descriptor layers, the original linking mechanism with its 3-levels can again be used."}, {"heading": "2.1 Adding New Trees", "text": "New trees can be created from text streams that are parsed into word sequences that can then build the tree structures. There are different ways to parse the text stream, including a statistical evaluation first of the most popular words, to be used as base nodes; or possibly simply parsing each sentence separately and adding as is. Natural language is already highly structured and intelligent and so parsing and using as is would pass some of that structure onto the concept base. When a new tree instance is added, some terms might overlap with\nexisting trees. If only one word term overlaps, then the trees should probably not be merged but separate instances created. So one question is whether two word or three word sequences should be mapped to an existing tree or whether separate instances should always be created. Merging trees as part of the first construction stage would require a parameter to be fine-tuned, but is ultimately what would be desired. The trees can still be removed or merged during periodic database updates however. A slow decrement of unused trees for example, can eventually remove them."}, {"heading": "2.2 Contextual Descriptors", "text": "A new suggestion for this paper is that the tree nodes should only be nouns or verbs. These are more solid and static real-world concepts, as opposed to the more transitory descriptive ones. The nouns and verbs would therefore make up the static tree structure. Each node would then be allowed to have any number of descriptors \u2013 adjectives for nouns and adverbs for verbs. For simplicity and a clear process, the descriptors can link only to other descriptors and do not determine new structure themselves. They would also be moved or re-structured with the parent concept.\nAs shown in Figure 1, there is a black cat, but the cat could also be white, tabby, or whatever. Also, a thirsty boy, who might also be hungry. So this type of context could change the meaning completely and could result in a requirement for different trees, to link up different sets of concepts. If the descriptor or context nodes are more transitory, then they do not have to follow the strict counting rule of the parent concepts and so they can build more arbitrary relations with other descriptors in the same tree or across trees. Although, a descriptor should still not have a larger count than its parent, but might have over other descriptors in other tree places. Figure 2 is a version that has added layers of descriptors for each node. In that figure, the black lines are the tree structure, the blue lines are from the concept to its descriptors and the dashed lines link descriptors with each other."}, {"heading": "3 Related Work", "text": "This paper is based on the earlier papers by the author [2]-[5]. In particular, [2] and [3] describe the concept trees and symbolic neural network working together, while [4] describes the symbolic neural network and the query language that is to be used. The book [5] writes about the original linking ideas, including the permanent and dynamic links that are now part of the concept base. The software has been written with the help of the licas system [7] and also OpenNLP [9] and WordNet [1][8]. Two earlier examples of other concept databases include [6] and [11]. As the concept base is also a type of semantic network, one reference for that would be [10]."}, {"heading": "4 Query language", "text": "A query can take the form of a tokenised list of words. This can be read as natural language from a text sequence, for example. If plain text is read, then some pre-processing can help. As with constructing the concept base, the more common words or punctuation can be\nremoved first, where the licas [7] classes can help with that. OpenNLP [9] was then used to parse a sentence into associated word phrases. Each word phrase would then be presented as a single tree structure to the concept base. To recognise the word types in the group, WordNet [1][8] was used. Nouns and verbs need to be recognised first. Then, if a noun has an adjective either side of it, it can be added as a descriptor for the noun. If a verb has an adverb either side of it, it can be added as a descriptor for the verb. There is some ambiguity, because the descriptor might belong to the word before or after it, but the database update process would help to confirm what is correct. For example, add the descriptor anywhere it may be relevant and it will only be updated and subsequently used if it is in fact returned as part of a search process.\nWith tokenised word sequences, a Horn clause [4] might be an appropriate query construct and would keep things relatively simple, although, changing the minor features in the implementation can change what a query result would be. In that paper, each clause part was an attribute with a related value. Here it is a tree concept with a related descriptor. Each query part could therefore be represented by a noun or verb and related descriptor, where the query process would try to find all trees that match with the terms. If a slot is empty, then a returned tree can make a suggestion for the missing information. Ordering of the clauses might not be possible if strict ordering is not maintained in the trees. But natural language still uses natural word groups, where each phrase should map to distinct tree parts. If the descriptors link with other descriptors, then cycles in the search paths can be found \u2013 from concept to concept and from a concept descriptor back to an earlier concept descriptor. Otherwise, paths that simply traverse all of the terms can be found and links between the trees that satisfy the query should be favoured. There could then be different solution sets that could be measured as more or less correct."}, {"heading": "4.1 Symbolic Neural Network", "text": "The concept trees have also been used as part of a cognitive model. In that model, they provide a more static and knowledge-based structure but link-up with a more dynamic type of neural network. The other half of the design is the symbolic neural network of [4], where that paper also mentions the query language. That model was more autonomous and\nexperience-based, but it was able to use the same type of query construct, with the idea that if all of the clause parts are found, they can be grouped or combined into a top global node that is representative of them. For example, food items can be grouped into a recipe. So while the research of this paper is to link-up the separate tree parts for multiple query constraints, that could be passed over to the symbolic network that then re-groups those into a global concept with some significance or meaning. This naturally joins the two types of construct together and there are even small or local circuits (cycles) in the trees themselves that could help to confirm the correct paths, for example."}, {"heading": "5 Computer Program and Test Scenarios", "text": "A test program has been written in the Java programming language. It does not implement the concept base in full, with regard to the re-structuring rules, but it allows for trees to be added and then searched over. The program can read a text document, parse it into sentences, remove the additional formatting and then use OpenNLP to construct each phrase or word group. WordNet then helps to parse this into concepts with related descriptors. Each phrase is then presented to the concept base and added as a tree structure. A similar process can be used to update the concept base, so that links can be created. Another set of text sentences, for example, can be parsed in the same way and used to search over the concept base trees. Sets of trees that satisfy the criteria can be retrieved and related links updated. This is still a query process, but the process that a human might use would ask for some information or knowledge to be returned. Therefore, after some links have been established, a query construct can be executed and can search the base in the same manner. But instead of updating links, it can fill in any missing slots in the query clauses, thereby providing new information for the user. If different sets of trees return full answers, then some reasoning might be required, but there are a lot of numerical values to suggest what answers might be more reliable.\nParsing text documents gives a variable list of terms to add, but they can still be added as each instance and rely on linking to make them reliable or permanent. If a node or descriptor is incorrect or unsuitable, it will probably not be used as part of a query result\nand so it will not get linked to. This also allows for a small amount of ambiguity in the sentence parsing, because the subsequent link updates can help to remove the error. So the descriptor can be added both to any noun or verb that comes before and/or after it in the parsed term list, if there is any confusion. The sentence parsing necessarily repeats word terms, where an N-Gram style of checking might be possible but it is not used as part of the current implementation. The extra coding and fine tuning of parameters are not required for a first test version."}, {"heading": "5.1 Example Scenario", "text": "A very simple example would be parsing the sentence \u2018Jack wore a white shirt and blue trousers\u2019. If this was added to the concept base and a query executed asking about [shirt-\nwhite]  [trousers-?], then the tree structure would return [shirt-white]  [trousers-blue], suggesting that the trousers are blue. Equally however, a question like \u2018was something blue\nworn with a white shirt?\u2019, or [shirt-white]  [?-blue], could be asked and would not necessarily require a search of the whole database. If descriptors link with each other, then a blue descriptor that links with the white descriptor of the shirt node can indicate a concept node that might contain relevant information."}, {"heading": "6 Conclusions", "text": "The concept trees have been enhanced with layers of descriptors. This provides a richer level of knowledge and would make them more useful in practice. The query structure of a Horn clause [4] also looks appropriate, at least for a more simple solution and the idea of an attribute name and value is the same as the concept name and descriptor of this paper. The concept tree \u2013 neural network structure of [2] or [3] has also been developed as it becomes clear that the two parts work very well together. It has always been suggested that positive feedback would be a good way of updating links dynamically. In [5] it was from queries over a distributed network and it is a similar scenario in this paper. A human-based decision to approve a query result can add intelligence that is missing in the system. This update or reinforcement process then provides an automatic mechanism for the symbolic neural network to re-cluster the concept groups back into some type of larger global concept. As\nthe global concept can still be arbitrary its actual use may be questionable, but it still represents something that has been queried for and the references will link to the base tree concepts that probably do represent something more specific. The symbolic neural network nodes can also trigger each other, to bring in other groups of related nodes. Therefore, lots of trees that might not be known to be related, can be grouped by the neural network, retrieved and reasoned over as part of an automatic process."}], "references": [{"title": "ed.) WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "New Ideas for Brain Modelling", "author": ["K. Greer"], "venue": "Intelligent Systems in Science and Information", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Concept Trees: Building Dynamic Concepts from Semi-Structured Data using Nature-Inspired Methods", "author": ["K. Greer"], "venue": "Studies in Fuzziness and Soft Computing, Springer-Verlag, Germany,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Symbolic Neural Networks for Clustering Higher-Level Concepts", "author": ["K. Greer"], "venue": "NAUN International Journal of Computers, Issue 3,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Thinking Networks \u2013 the Large and Small of it: Autonomic and Reasoning Processes for Information Networks, published with LuLu.com, 2008, ISBN: 1440433275", "author": ["K. Greer"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "ConceptBase - A Deductive Object Base Manager", "author": ["M. Jarke", "S. Eherer", "R. Gallersdorfer", "M.A. Jeusfeld", "M. Staudt"], "venue": "Journal on Intelligent Information Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "WordNet: A Lexical Database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Principles of Semantic Networks: Explorations in the representation of knowledge", "author": ["J.F. Sowa"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Automatic Construction of a Lexical Attribute Knowledge Base", "author": ["J. Zhao", "Y. Gao", "H. Liu", "R. Lu"], "venue": "KSEM", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "The term \u2018concept base\u2019 has been used previously ([6] or [11], for example) and has been adopted in [4] to describe a database of heterogeneous sources or arbitrary concepts.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "The term \u2018concept base\u2019 has been used previously ([6] or [11], for example) and has been adopted in [4] to describe a database of heterogeneous sources or arbitrary concepts.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "The term \u2018concept base\u2019 has been used previously ([6] or [11], for example) and has been adopted in [4] to describe a database of heterogeneous sources or arbitrary concepts.", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "A formal and mathematical process for doing this was described in [3], where tree-like structures could be built to represent the more arbitrary concept groups that are presented.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "To keep the structure flexible, the dynamic linking mechanism [5] can be used to link descriptors.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "The linking structure of the licas [5][7] network is very relevant in building the concept trees now and forms the basis for their construction.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "As part of a more complete system, the relation with the symbolic neural network [4] is developed again in the following sections.", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "The concept trees have been described previously in [3] and again in [2].", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "The concept trees have been described previously in [3] and again in [2].", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "The paper [3] gives a more formal set of rules that would help to build the concept base in a normalised way and also how to update and re-structure it.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "The linking mechanisms are mostly dynamic, where the frequency counts are already that way and so there is probably no reason to replace that with the 3-level linking mechanism [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "This paper is based on the earlier papers by the author [2]-[5].", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "This paper is based on the earlier papers by the author [2]-[5].", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "In particular, [2] and [3] describe the concept trees and symbolic neural network working together, while [4] describes the symbolic neural network and the query language that is to be used.", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "In particular, [2] and [3] describe the concept trees and symbolic neural network working together, while [4] describes the symbolic neural network and the query language that is to be used.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "In particular, [2] and [3] describe the concept trees and symbolic neural network working together, while [4] describes the symbolic neural network and the query language that is to be used.", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "The book [5] writes about the original linking ideas, including the permanent and dynamic links that are now part of the concept base.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "The software has been written with the help of the licas system [7] and also OpenNLP [9] and WordNet [1][8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "The software has been written with the help of the licas system [7] and also OpenNLP [9] and WordNet [1][8].", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": "Two earlier examples of other concept databases include [6] and [11].", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "Two earlier examples of other concept databases include [6] and [11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "As the concept base is also a type of semantic network, one reference for that would be [10].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "To recognise the word types in the group, WordNet [1][8] was used.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "To recognise the word types in the group, WordNet [1][8] was used.", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "With tokenised word sequences, a Horn clause [4] might be an appropriate query construct and would keep things relatively simple, although, changing the minor features in the implementation can change what a query result would be.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "The other half of the design is the symbolic neural network of [4], where that paper also mentions the query language.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "The query structure of a Horn clause [4] also looks appropriate, at least for a more simple solution and the idea of an attribute name and value is the same as the concept name and descriptor of this paper.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "The concept tree \u2013 neural network structure of [2] or [3] has also been developed as it becomes clear that the two parts work very well together.", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "The concept tree \u2013 neural network structure of [2] or [3] has also been developed as it becomes clear that the two parts work very well together.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "In [5] it was from queries over a distributed network and it is a similar scenario in this paper.", "startOffset": 3, "endOffset": 6}], "year": 2016, "abstractText": "Concept Trees are a type of database that can organise arbitrary textual information using a very simple rule. Each tree tries to represent a single cohesive concept and the trees can link with each other for navigation and semantic purposes. The trees are therefore a type of semantic network and would benefit from having a consistent level of context for each of the nodes. The Concept Tree nodes have a mathematical basis allowing for a consistent build process. These would represent nouns or verbs in a text sentence, for example. New to the design can then be lists of descriptive elements for each of the nodes. The descriptors can also be weighted, but do not have to follow the strict counting rule of the tree nodes. With the new descriptive layers, a much richer type of knowledge can be achieved and still reasoned over automatically. The linking structure of the licas network is very relevant to building the concept trees now and forms the basis for their construction. The concept tree symbolic neural network relation is also extended further.", "creator": "Microsoft\u00ae Word 2010"}}}