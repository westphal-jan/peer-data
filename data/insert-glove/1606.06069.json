{"id": "1606.06069", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Relative Natural Gradient for Learning Large Complex Models", "abstract": "Fisher information and crecio natural winterberry gradient cojones provided deep storebrand insights destreza and powerful a-like tools cafta to apostolates artificial neural s\u00f3l networks. However related lamborghini analysis fardon becomes bols\u00f3n more scripture and more plus-9 difficult as schmid the upal learner ' s cptm structure turns 68.19 large rushforth and complex. buckinghamshire This carasso paper makes a 108.5 preliminary hamidullah step towards webseries a garsse new direction. We extract wanhua a 12pm local component of immorally a witzleben large neuron system, foolproof and seatbacks defines one-electron its relative Fisher confindustria information metric koriya that now-obsolete describes ouse accurately this small component, and is maniram invariant to verdin the other esh parts ideological of the pancalli system. This concept is important because the arbuzov geometry zacharski structure is bidmead much simplified curdles and it kaliyan can be easily riggi applied 123-year to dniepropetrovsk guide philoctetes the learning of neural boliviana networks. cs We imbibers provide an dubailand analysis on prefatory a filin list of portis commonly ferrybridge used components, and 5-2-1 demonstrate limos how replayed to use esquivel this stir-fried concept mauduit to submerged further improve optimization.", "histories": [["v1", "Mon, 20 Jun 2016 11:36:40 GMT  (540kb,D)", "http://arxiv.org/abs/1606.06069v1", "24 pages, 5 figures"]], "COMMENTS": "24 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ke sun", "frank nielsen"], "accepted": false, "id": "1606.06069"}, "pdf": {"name": "1606.06069.pdf", "metadata": {"source": "CRF", "title": "Relative Natural Gradient for Learning Large Complex Models", "authors": ["Ke Sun", "Frank Nielsen"], "emails": ["sunk.edu@gmail.com", "Frank.Nielsen@acm.org"], "sections": [{"heading": "1 Introduction", "text": "The Fisher Information Metric (FIM) I(\u0398) = (Iij) of a statistical parametric model p(x |\u0398) of order D is defined by a D \u00d7D positive semidefinite (psd) matrix (I(\u0398) 0) with coefficients\nIij = Ep [ \u2202l\n\u2202\u0398i\n\u2202l\n\u2202\u0398j\n] , (1)\nwhere l(\u0398) denotes the log-likelihood function ln p(x |\u0398). Under light regularity conditions, Equation (1) can be rewritten equivalently as\nIij = \u2212Ep [ \u22022l\n\u2202\u0398i\u2202\u0398j\n] = 4 \u222b \u2202 \u221a p(x |\u0398) \u2202\u0398i \u2202 \u221a p(x |\u0398) \u2202\u0398j dx. (2)\nFor regular natural exponential families (NEFs) l(\u0398) = \u0398\u1d40t(x)\u2212 F (\u0398) (log-linear models with sufficient statistics t(x)), the FIM is I(\u0398) = \u22072F (\u0398) 0, the Hessian of the moment generating function (mgf). Although exponential families can approximate arbitrarily any smooth density (Cobb et al., 1983), the mgf may not be available in closed-form nor computationally tractable (Montanari, 2015). Besides the fact that learning machines usually have often singularities (Watanabe, 2009) (|I(\u0398)| = 0, not full rank) characterized by plateaux in gradient learning, computing/estimating the FIM of a large learning system is very challenging due to the finiteness of data, and the large number D(D+1)2 of matrix coefficients to evaluate. Moreover,\nar X\niv :1\n60 6.\n06 06\n9v 1\n[ cs\n.L G\n] 2\n0 Ju\nn 20\ngradient descent techniques require to invert this large matrix and to tune the learning rate. The FIM is not invariant and depends on the parameterization: I\u0398(\u0398) = J\u1d40I\u039b(\u039b)J where J is the Jacobian matrix Jij = \u2202\u039bi\u2202\u0398j . Therefore one may ponder whether we can always find a suitable parameterization that yields a diagonal FIM that is straightforward to invert. This fundamental problem of parameter orthogonalization was first investigated by Jeyffreys (1961) for decorrelating the estimation of the parameters of interest from the nuisance parameters. Fisher diagonalization yields parameter orthogonalization (Cox and Reid, 1987), and prove useful when estimating \u0398\u0302 using MLE that is asymptotically normally distributed, \u0398\u0302n = G(\u0398, I\u22121(\u0398)/ \u221a n), where G(\u03b81,\u03b82) denotes a univariate or multivariate Gaussian distribution with mean \u03b81 and variance \u03b82, and efficient since the variance of the estimator matches the Cram\u00e9r-Rao lower bound. Using the chain rule of differentiation of calculus, this amounts to find a suitable parameterization \u2126 = \u2126(\u0398) satisfying \u2211\ni,j\nE\n[ \u22022\n\u2202\u0398i\u2202\u0398j \u2202l(x; \u03b8) ] \u2202\u0398i \u2202\u2126k \u2202\u0398j \u2202\u2126l = 0, \u2200k 6= l.\nThus in general, we end up with ( D 2 ) = D(D\u22121)2 (non-linear) partial differential equations to\nsatisfy (Huzurbazar, 1950). Therefore, in general there is no solution when ( D 2 ) > D, that is when D > 3. When D = 2, the single differential equations is usually solvable and tractable, and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the location-scale families { 1\u03c3p0( x\u2212\u00b5 \u03c3 )} that include the Gaussian family and the Cauchy family. Sometimes, the structure of the differential equation system yields a solution: For example, Jeffreys (1961) reported a parameter orthogonalization for Pearson\u2019s distributions of type I which is of order D = 4. Cox and Reid (1987) further investigate this topic with application to conditional inference, and provide examples (including the Weibull distribution).\nFrom the viewpoint of geometry, the FIM induces a Riemannian manifold with metric tensor g(\u0398) = I(\u0398). When the FIM may be degenerate, this yields a pseudo-Riemannian manifold (Thomas, 2014). In differential geometry, orthogonalization amounts to transform the square length infinitesimal element gijd\u0398i\u0398j of a Riemannian geometry into an orthogonal system \u03c9 with matching square length infinitesimal element \u2126iid\u2126id\u2126j . However, such a global orthogonal metric does not exist (Huzurbazar, 1950) when D > 3 for an arbitrary metric tensor although interesting Riemannian parameterization structures may be derived in Riemannian 4D geometry (Grant and Vickers, 2009). For NEFs, the FIM can be made block-diagonal easily by using the mixed coordinate system (Amari, 2016) (\u03981:k,Hk+1:D), where H = Ep[t(x)] = \u2207F (\u0398) is the moment parameter, for any k \u2208 {1, ..., D\u22121}, where v[b:e] denotes the subvector (vb, ..., ve)\u1d40 of v. The geometry of NEFs is a dually flat structure (Amari, 2016) induced by the convex mgf, the potential function. It defines a dual affine coordinate systems ei = \u2202i = \u2202\u2202Hi and ej = \u2202 j = \u2202\u2202\u0398j that are orthogonal: \u3008e i, ej\u3009 = \u03b4ij , where \u03b4ij = 1 iff i = j and \u03b4ij = 0 otherwise. Those dual affine coordinate systems are defined up to an affine invertible transformation A: \u0398\u0303 = A\u0398 + b, H\u0303 = A\u22121H + c, where b and c are constants. In particular, for any order-2 NEF (D = 2), we can always obtain two mixed parameterizations (\u03981, H2) or (H1,\u03982).\nThe FIM g(\u0398) or I(\u0398) by definition is an expectation. If its Hessian form in eq. (2) is computed based on a set of empirical observations {xk}, as\ng\u0304(\u0398) = \u2212 \u2202 2lk\n\u2202\u0398i\u2202\u0398j , (3)\nwhere \u201c \u00b7 \u201d denotes the sample average over {xk}, the resulting metric is called the observed FIM (Efron and Hinkley, 1978). It is useful when the underlying distribution is not available.\nWhen the number of observations increases, the observed FIM becomes more and more close to the FIM.\nPast works on FIM-based approaches mainly focus on how to approximate the global FIM into a block diagonal form using the gradient of the cost function (Roux et al., 2008; Martens, 2010; Pascanu and Bengio, 2014; Martens and Grosse, 2015). This global approach faces the analytical complexity of learning systems. The approximation error increases as the system scales up and as complex and dynamic structures emerge.\nThis work aims at a different local approach. The idea is to accurately describe the information geometry in a local subsystem of the big learning system, which is invariant to the scaling up and structural change of the global system, so that the local machinery, including optimization, can be discussed regardless of the other parts.\nFor this purpose, a novel concept, Relative Fisher Information Metric (RFIM), is defined. Unlike the traditional geometric view of a high-dimensional parameter manifold, RFIMs defines multiple projected low-dimensional geometry of subsystems. This geometry is correlated to the parameters beyond the subsystem and is therefore considered dynamic. It can be used to characterize the efficiency of a local learning process. Taking this stance has potentials in deep learning because a big learning system can be decomposed into many local components, i.e. layers. This paper will make clear the concept of RFIM, provide proof-of-concept experiments, and discuss its theoretical advantages.\nThe paper is organized as follows. Sec. 2 reviews natural gradient within the context of MultiLayer Perceptrons (MLP). Sec. 3 presents the concept of RFIM, and gives detailed formulations of several commonly used subsystems. Sec. 4 discusses the advantages of using RFIM as compared to FIM. Sec. 5 shows how to use the RFIMs given by Sec. 3 to optimize neural networks, with an algorithm framework and several proof-of-concept experiments. Sec. 6 concludes this work and further hint at perspectives."}, {"heading": "2 Natural Gradient of Neural Networks", "text": "Consider a MLP as depicted in fig. 1, whose statistical model is the following conditional distribution\np(y |x,\u0398) = \u2211\nh1,\u00b7\u00b7\u00b7 ,hL\u22121\np(y |hL\u22121,\u03b8L) \u00b7 \u00b7 \u00b7 p(h2 |h1,\u03b82)p(h1 |x,\u03b81), (4)\nwhere the often intractable sum over h1, \u00b7 \u00b7 \u00b7 ,hL\u22121 can be get rid off by deteriorating p(h1 |x,\u03b81), \u00b7 \u00b7 \u00b7 , p(hL\u22121 |hL\u22122,\u03b8L\u22121) to Dirac\u2019s deltas \u03b4, and let merely the last layer p(y |hL\u22121,\u03b8L) be stochastic. Note that Restricted Boltzmann Machines (Nair and Hinton, 2010; Montavon and M\u00fcller, 2012) (RBMs), and dropout (Wager et al., 2013) do consider h to be stochastic.\nThe tensor metric of the neuromanifold (Amari, 1995)M\u0398, consisting of all MLPs with the same architecture but different parameter values, is locally defined by the FIM. Because that a MLP corresponds to a conditional distribution, its FIM by eq. (1) is a function of the input x. By taking an empirical average over the input samples {xi}, the FIM of a MLP has the following expression\ng(\u0398) = 1\nn n\u2211 i=1 Ep(y |xi,\u0398) [ \u2202li \u2202\u0398 \u2202li \u2202\u0398\u1d40 ] = \u2212 1 n n\u2211 i=1 Ep(y |xi,\u0398) [ \u22022li \u2202\u0398\u2202\u0398\u1d40 ] , (5)\nwhere li(\u0398) = ln p(y |xi, \u0398) denotes the conditional log-likelihood function wrt xi.\nJust like a Mahalanobis metric, g(\u0398) can be used to measure the distance between two neural networks locally around \u0398 \u2208M\u0398. A learning step makes a tiny movement \u03b4\u0398 onM\u0398 from \u0398 to \u0398 + \u03b4\u0398. According to the FIM, the infinitesimal square distance\n\u3008\u03b4\u0398, \u03b4\u0398\u3009g(\u0398) = \u03b4\u0398\u1d40g(\u0398)\u03b4\u0398 = 1\nn n\u2211 i=1 Ep(y |xi,\u0398) [ \u03b4\u0398\u1d40 \u2202li \u2202\u0398 ]2 (6)\nmeasures how much \u03b4\u0398 (with a radius constraint) is statistically along \u2202l\u2202\u0398 , or equivalently how much \u03b4\u0398 affects intrinsically the conditional distribution p(y |x, \u0398).\nConsider the negative log-likelihood function L(\u0398) = \u2212 \u2211 i ln p(yi |xi,\u0398) wrt the observed pairs {(xi,yi)}, we try to minimize the loss while maintaining a small cost, measured geometrically by the square distance \u3008\u03b4\u0398, \u03b4\u0398\u3009g(\u0398) onM\u0398. At \u0398t \u2208M\u0398, the target is to minimize wrt \u03b4\u0398\nL(\u0398t + \u03b4\u0398) + 1\n2\u03b3 \u3008\u03b4\u0398, \u03b4\u0398\u3009g(\u0398t) \u2248 L(\u0398t) + \u03b4\u0398\n\u1d40 5 L(\u0398t) + 1\n2\u03b3 \u03b4\u0398\u1d40g(\u0398t)\u03b4\u0398, (7)\nwhere \u03b3 > 0 is a learning rate. The optimal solution of the above eq. (7) gives a learning step\n\u03b4\u0398t = \u2212\u03b3g\u22121(\u0398t)5 L(\u0398t).\nIn this update procedure, the term g\u22121(\u0398t) 5 L(\u0398t) replaces the role of the usual gradient 5L(\u0398t) and is called the natural gradient (Amari, 1997).\nAlthough the FIM depends on the chosen parameterization, the natural gradient is invariant to reparametrization. Let \u039b be another coordinate system and J be the Jacobian matrix \u0398\u2192 \u039b. Then we have\ng\u22121(\u0398)5 L(\u0398) = (J\u1d40g(\u039b)J)\u22121 J\u1d40 5 L(\u039b) = J\u22121g\u22121(\u039b)5 L(\u039b). (8)\nThe left-hand-side and right-hand-side of eq. (8) correspond to exactly the same infinitesimal movement alongM\u0398. However, as the learning rate \u03b3 is not infinitesimal in practice, natural gradient descent actually depends on the coordinate system in practice. Other intriguing properties of natural gradient optimization lie in being free from getting trapped in plateaus of the error surface, and attaining Fisher efficiency in online learning (see Sec. 4 (Amari, 1998)).\nFor sake of simplicity, we omit to discuss the case when the FIM is singular. That is, the set of parameters \u0398s with zero metric. This set of parameters forms an analytic variety (Watanabe, 2009), and technically the MLP as a statistical model is said non-regular (and the parameter \u0398 is not identifiable). The natural gradient has been extended (Thomas, 2014) to cope with singular FIMs having positive semi-definite matrices by taking the Moore-Penrose pseudo-inverse (that coincides with the inverse matrix for full rank matrices).\nIn the family of 2nd-order optimization methods, a line can be drawn from natural gradient from the Newton methods, e.g. (Martens, 2010), by checking whether the computation of the Hessian term depends on the cost function. Taking a MLP as an example, the computation of the FIM does not need the given {yi}, but averages over all possible y generated according to {xi} and the current model \u0398. One advantage of natural gradient is that the FIM is guaranteed to be psd while the Hessian may not. We refer the reader to related references (Amari et al., 2000) for more details.\nBonnabel (Bonnabel, 2013) proposed to use the Riemannian exponential map to define a step gradient descent, thus ensuring to stay on the manifold for any chosen learning rate. Convergence is proven for Hadamard manifolds (of negative curvatures). However, it is not mathematically tractable to express the exponential map of hierarchical model manifolds."}, {"heading": "3 Relative Fisher Information Metric of Subsystems", "text": "In general, for large parametric matrices, it is impossible to diagonalize or decorrelate all the parameters as mentioned above, so that we split instead all random variables in the learning system into three parts \u03b8f , \u03b8, h. The reference, \u03b8f , consists of the majority of the random variables that are considered fixed. \u03b8 is the internal parameters of a subsystem wrt its structure. The response h is the interface of this subsystem to the rest of the learning system. This h usually carries sample-wise information to be summarized into the internal parameters \u03b8, so it is like \u201cpseudo-observations\u201d, or hidden variables, to the subsystem. Once \u03b8f is given, the subsystem can characterized by the conditional distribution p(h |\u03b8,\u03b8f ). We made the following definition.\nDefinition 1 (RFIM). Given \u03b8f , the RFIM 1 of \u03b8 wrt h is\ngh (\u03b8 |\u03b8f ) def = Ep(h | \u03b8, \u03b8f )\n[ \u2202\n\u2202\u03b8 ln p(h |\u03b8, \u03b8f )\n\u2202\n\u2202\u03b8\u1d40 ln p(h |\u03b8, \u03b8f )\n] , (9)\nor simply gh (\u03b8), corresponding to the estimation of \u03b8 based on observations of h given \u03b8f .\nWhen we choose h to be the observables, usually denoted by x, and choose \u03b8 to be all free parameters \u0398 in the learning system, then RFIM becomes FIM: g(\u0398) = gx(\u0398). What is novel is that we can choose the response h to be other than the raw observables to compute Fisher informations of subsystems, specially dynamically during the learning of machines. To see the meaning of RFIM, similar to eq. (6), the infinitesimal square distance\n\u3008\u03b4\u03b8, \u03b4\u03b8\u3009gh(\u03b8) = Ep(h | \u03b8, \u03b8f ) [ \u03b4\u03b8\u1d40 \u2202\n\u2202\u03b8 ln p(h |\u03b8, \u03b8f )\n]2 (10)\nmeasures how much \u03b4\u03b8 impacts intrinsically the conditional distribution featuring the subsystem. We also have the following proposition, following straightforwardly from definition 1.\n1We use the same term \u201crelative FIM\u201d (Zegers, 2015) with a different definition.\np(y |\u0398,x) = \u2211 h1 \u2211 h2 p(h1 |\u03b81,x) p(h2 |\u03b82,h1) p(y |\u03b83,h2)\nProposition 2. If \u03b81 consists of a subset of \u03b82 so that \u03b82 = (\u03b81, \u03b8\u03031), thenM\u03b81 with the metric gh (\u03b81) has the same Riemannian geometry with a sub-manifold ofM\u03b82 with the metric gh (\u03b82), when \u03b8\u03031 is given.\nWhen the response h is chosen, then different splits of (\u03b8,\u03b8f ) correspond to the same ambient geometry. In fact, the particular case of a mixed coordinate system (that is not an affine coordinate system) induces in information geometry (Amari, 2016) a dual pair of orthogonal e- and morthogonal foliations. Our splits in RFIMs consider non-orthogonal foliations that provide the factorization decompositions of the whole manifold into submanifolds, that are the leaves of the foliation (Amari and Nagaoka, 2000).\nFigure 2 shows the traditional global geometry of a learning system, as compared to the information geometry of subsystems defined by RFIMs. The red arrows means that the pointed geometry structure is dynamic and varies with the reference variable. In MLPs, the subsystems, i.e. layers, are supervised. Their reference \u03b8f also carries sample-wise information.\nOne should not confuse RFIM with the diagonal blocks of FIM. Both their meaning and expression are different. RFIM is computed by integrating out hidden variables, or the output of subsystems. FIM is always computed by integrating out the observables.\nIn the following we analyze accurately several commonly used RFIMs. Note that, the FIMs of small parametric structures such as single neurons have been studied for a long time (Amari, 1997). Although with similar expressions, we are looking at a component embedded in a large system rather than a small single component system. These are two different concepts, and only the former can be used to guide large learning systems."}, {"heading": "3.1 RFIMs of One Neuron", "text": "We start from the RFIM of single neuron models."}, {"heading": "3.1.1 Hyperbolic tangent activation", "text": "Consider a neuron with input x, weights w, a hyperbolic tangent activation function, and a stochastic output y \u2208 {\u22121, 1}, given by\np(y = 1) = 1 + tanh(w\u1d40x\u0303)\n2 , tanh(t) = exp(t)\u2212 exp(\u2212t) exp(t) + exp(\u2212t) . (11)\nFor convenience, throughout this paper x\u0303 = (x\u1d40, 1)\u1d40 denotes the augmented vector of x (homogeneous coordinates) so that w\u1d40x\u0303 contains a bias term, and a general linear transformation can be written simply as Ax\u0303. By definition 1 and some simple analysis 2, we get\ngy(w |x) = \u03bdtanh(w,x)x\u0303x\u0303\u1d40, \u03bdtanh(w,x) = 1\u2212 tanh2(w\u1d40x\u0303). (12)"}, {"heading": "3.1.2 Sigmoid activation", "text": "Similarly, the RFIM of a neuron with input x, weights w, a sigmoid activation function, and a stochastic binary output y \u2208 {0, 1}, where\np(y = 1) = sigm(w\u1d40x\u0303), sigm(t) = 1\n1 + exp(\u2212t) , (13)\nis given by\ngy(w |x) = \u03bdsigm(w,x)x\u0303x\u0303\u1d40, \u03bdsigm(w,x) = sigm (w \u1d40x\u0303) [ 1\u2212 sigm (w\u1d40x\u0303) ] . (14)\nA neuron with sigmoid activation but continuous output was discussed earlier (Amari, 1997)."}, {"heading": "3.1.3 Parametric Rectified Linear Unit", "text": "Another commonly used activation function is Parametric Rectified Linear Unit (PReLU) (He et al., 2015), which includes Rectified Linear Unit (ReLU) (Nair and Hinton, 2010) as a special case. To compute the RFIM, we formulate PReLU into a conditional distribution given by\np(y |w,x) = G(y | relu(w\u1d40x\u0303), \u03c32), relu(t) = { t if t \u2265 0 \u03b9t if t < 0. (0 \u2264 \u03b9 < 1) (15)\nWhen \u03b9 = 0, eq. (15) becomes ReLU. By definition 1, the corresponding RFIM is\ngy(w |x) =  1 \u03c32 x\u0303x\u0303 \u1d40 if w\u1d40x\u0303 > 0 undefined if w\u1d40x\u0303 = 0 \u03b92\n\u03c32 x\u0303x\u0303 \u1d40 if w\u1d40x\u0303 < 0\n(16)\nThis RFIM is discontinuous at w\u1d40x\u0303 = 0. To obtain a smoother RFIM, a trick is to replace relu(w\u1d40x\u0303) on the left-hand-side of eq. (15) with relu\u03c9(w\u1d40x\u0303), where\nrelu\u03c9(t) = \u03c9 ln\n( exp ( \u03b9t\n\u03c9\n) + exp ( t\n\u03c9\n)) , (17)\n2See the appendix for detailed derivations.\nand \u03c9 > 0 is a hyper-parameter so that lim\u03c9\u21920+ relu\u03c9 = relu. Then, PReLU\u2019s RFIM is given by\ngy(w |x) = \u03bdrelu(w,x)x\u0303x\u0303\u1d40,\n\u03bdrelu(w,x) = 1\n\u03c32\n[ \u03b9+ (1\u2212 \u03b9)sigm ( 1\u2212 \u03b9 \u03c9 w\u1d40x\u0303 )]2 , (18)\nwhich is simply a smoothed version of eq. (16). If we set empirically \u03c3 = 1, \u03b9 = 0, then \u03bdrelu(w,x) = sigm\n2 (\n1 \u03c9w\n\u1d40x\u0303 ) is close to 1 when w\u1d40x\u0303 > 0, and is close to 0 otherwise."}, {"heading": "3.1.4 Exponential Linear Unit", "text": "A stochastic exponential linear unit (ELU) (Clevert et al., 2015) with \u03b1 > 0 is p(y |w, x) = G(y | elu(w\u1d40x\u0303), \u03c32), elu(t) = { t if t \u2265 0 \u03b1 (exp(t)\u2212 1) if t < 0. (19)\nIts RFIM is given by\ngy(w |x) = \u03bdelu(w,x)x\u0303x\u0303\u1d40,\n\u03bdelu(w,x) =\n{ 1 \u03c32 if w\n\u1d40x\u0303 \u2265 0 \u03b12 \u03c32 exp (2w \u1d40x\u0303) if w\u1d40x\u0303 < 0.\n(20)\nThe coefficient function \u03bdelu(w,x) is continuous but non-differentiable at w\u1d40x\u0303 = 0."}, {"heading": "3.1.5 A generic expression of one-neuron RFIMs", "text": "Denote f \u2208 {tanh, sigm, relu, elu} to be an element-wise nonlinear activation function. By eqs. (12), (14) and (18), the RFIMs of single neurons have a common form\ngy(w |x) = \u03bdf (w,x)x\u0303x\u0303\u1d40, (21)\nwhere \u03bdf (w,x) is a positive coefficient with large values in the linear region, or the effective learning zone of the neuron."}, {"heading": "3.2 RFIM of One Layer", "text": "A linear layer with input x, connection weights W = [w1, \u00b7 \u00b7 \u00b7 ,wDy ], and stochastic output y can be represented by p(y |W ,x) = G(y |W \u1d40x\u0303, \u03c32I), where I is the identity matrix, and \u03c3 is the scale of the observation noise. We vectorize W by stacking its columns {wi}, then gy(W |x) is a tensor of size (Dx + 1)Dy \u00d7 (Dx + 1)Dy, where D denotes the dimension of the corresponding variable. Fortunately, due to conditional independence of y\u2019s dimensions given W and x, the RFIM has a simple block diagonal form, given by\ngy(W |x) = 1 \u03c32 diag [x\u0303x\u0303\u1d40, \u00b7 \u00b7 \u00b7 , x\u0303x\u0303\u1d40] , (22)\nwhere diag(\u00b7) means the (block) diagonal matrix constructed by the given matrix entries. A nonlinear layer increments a linear layer by adding an element-wise activation function applied on W \u1d40w\u0303, and randomized wrt the choice of the activation (Bernoulli for tanh and sigm; Gaussian for relu). By definition 1, its RFIM is given by\ngy (W |x) = diag [ \u03bdf (w1,x)x\u0303x\u0303\u1d40, \u00b7 \u00b7 \u00b7 , \u03bdf (wm,x)x\u0303x\u0303\u1d40 ] , (23)\nwhere \u03bdf (wi,x) depends on the activation function f as discussed in Subsec. 3.1. A softmax layer, which often appears as the last layer of a MLP, is given by y \u2208 {1, . . . ,m}, where p(y) = \u03b7y =\nexp(wyx\u0303)\u2211m i=1 exp(wix\u0303) . (24)\nIts RFIM is not block diagonal any more but given by\ngy(W ) =  (\u03b71 \u2212 \u03b721)x\u0303x\u0303\u1d40 \u2212\u03b71\u03b72x\u0303x\u0303\u1d40 \u00b7 \u00b7 \u00b7 \u2212\u03b71\u03b7mx\u0303x\u0303\u1d40 \u2212\u03b72\u03b71x\u0303x\u0303\u1d40 (\u03b72 \u2212 \u03b722)x\u0303x\u0303\u1d40 \u00b7 \u00b7 \u00b7 \u2212\u03b72\u03b7mx\u0303x\u0303\u1d40 ... ... . . . ...\n\u2212\u03b7m\u03b71x\u0303x\u0303\u1d40 \u2212\u03b7m\u03b72x\u0303x\u0303\u1d40 \u00b7 \u00b7 \u00b7 (\u03b7m \u2212 \u03b72m)x\u0303x\u0303\u1d40  . (25) Notice that its i\u2019th diagonal block (\u03b7i \u2212 \u03b72i )x\u0303x\u0303\u1d40 resembles the RFIM of a single sigm neuron."}, {"heading": "3.3 RFIM of Two Layers", "text": "By eq. (23), the one-layer RFIM is a product metric (Jost, 2011) and does not consider the inter-neuron correlations. We must look at a larger subsystem to obtain such correlations. Consider a two-layer model with stochastic output y around the mean vector f (C\u1d40h), where h = f (W \u1d40x), as shown in fig. 3. For simplicity, we ignore inter-layer correlations between the first layer and the second layer and focus on the inter-neuron correlations within the first layer. To do this, both x and C are considered as references to compute the RFIM of W . By definition 1, gy(W |x,C) = [Gij ]Dh\u00d7Dh and each block\nGij = Dy\u2211 l=1 cilcjl\u03bdf (cl,h)\u03bdf (wi,x)\u03bdf (wj ,x)x\u0303x\u0303 \u1d40, \u22001 \u2264 i \u2264 Dh, \u22001 \u2264 j \u2264 Dh. (26)\nConsider the computational difficulty of eq. (26) that is only listed here as an analytical contribution with possible empirical extensions."}, {"heading": "4 Advantages of RFIM", "text": "This section discusses theoretical advantages of RFIM. Consider wlog a MLP with one Bernoulli output y, whose mean \u00b5 is a deterministic function depending on the input x and the network\nparameters \u0398. By Sec. 2, the FIM of the MLP can be computed as\ng(\u0398) = \u00b5i \u2202 ln\u00b5i \u2202\u0398 \u2202 ln\u00b5i \u2202\u0398\u1d40 + (1\u2212 \u00b5i) \u2202 ln(1\u2212 \u00b5i) \u2202\u0398 \u2202 ln(1\u2212 \u00b5i) \u2202\u0398\u1d40 = 1 \u00b5i(1\u2212 \u00b5i) \u2202\u00b5i \u2202\u0398 \u2202\u00b5i \u2202\u0398\u1d40 . (27)\nTherefore rank(g(\u0398)) \u2264 n, as each sample contributes maximally 1 to the rank of g(\u0398). A small diagonal block of g(\u0398), representing one layer, is likely to have a rank much lower than the sample size. If the number of parameters is greater than the sample size, which can be achieved especially with deep learning (Szegedy et al., 2015), then g(\u0398) is guaranteed to be singular. All methods trying to approximate FIM suffers from this problem and should use proper regularizations. Comparatively, RFIM decomposes the network in a layer-wise manner. In each layer h = f(W \u1d40x), by eq. (23), rank(gh(W )) \u2264 nDh, which is an achievable upper bound. Therefore, RFIM is expected to have a much higher rank than FIM. Higher rank means less singularity, and more information is captured, and easier to reparameterize to achieve good optimization properties. Essentially, RFIM integrates the internal stochasticity (Bengio, 2013) of the neural system by considering the output of each layer as random variables. In theory, the computation of FIM should also consider stochastic neurons. However it requires to marginalize the joint distribution of h1, h2, \u00b7 \u00b7 \u00b7 , y. This makes the already infeasible computation even more difficult.\nRFIM is accurate, for that the geometry of \u03b8 is defined wrt to its direct response h in the system, or adjacent nodes in a graphical model. By the example in Sec. 2 and eq. (9), the RFIM gy(\u03b8L) is exactly the corresponding block in the FIM I(\u0398), because they both investigate how \u03b8L affects the last layer hL\u22121 \u2192 y. They start to diverge from the second last layer. To compute the geometry of \u03b8L\u22121, RFIM looks at how \u03b8L\u22121 affects the local mapping hL\u22121 \u2192 hL. Intuitively, this local relationship can be more reliably measured regardless of the rest of the system. In contrast, FIM examines how \u03b8L\u22121 affects the global mapping hL\u22121 \u2192 y. This task is much more difficult, because it must consider the correlation between different layers. This is hard to perform without approximation techniques. As a commonly used approximation, the block diagonalized version of FIM will ignore such correlations and loose accuracy.\nThe measurement of RFIM makes it possible to maintain global stability of the learning system by balancing different subsystems. Consider two connected subsystems with internal parameters \u03b81 and \u03b82 and corresponding responses h1 and h2. A learning step is given by \u03b81 \u2190 \u03b81 + \u03b4\u03b81 and \u03b82 \u2190 \u03b82 + \u03b4\u03b82, with \u2016\u03b4\u03b81\u2016 \u2264 \u03b3 and \u2016\u03b4\u03b82\u2016 \u2264 \u03b3 constrained by a pre-chosen maximum radius \u03b3. To balance the system, we constrain gh1(\u03b81) and gh2(\u03b82) to have similar scales, e.g. by normalizing their largest eigenvalue to 1. This can be done through reparametrization tricks. Then the intrinsic changes, or variations, of the responses h1 and h2 also have similar scales and is upper bounded. Note that the responses h1 and h2 are the interfaces of subsystems. For example, in the MLP shown in fig. 2, the response h1 in subsystem 1 becomes the reference in subsystem 2. By bounding the variation of h1, the Riemannian metric gh2(\u03b82 |h1) will present less variations during learning."}, {"heading": "5 Relative Natural Gradient Descent (RNGD)", "text": "The traditional non-parametric way of applying natural gradient requires to re-calculate the FIM and solving a large linear system in each learning step. Besides the huge computational cost, it meets some difficulties. For example, in an online learning scenario, a mini batch of samples cannot faithfully reflect the \u201ctrue\u201d geometry, which has to integrate the risk of sample variations. That is, the FIM of a mini batch is likely to be singular or with bad conditions.\nA recent series of efforts (Montavon and M\u00fcller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach of applying natural gradient, which memorizes\nand learns a geometry. For example, natural neural networks (Desjardins et al., 2015) augment each layer with a redundant linear layer, and let these linear layers to parametrize the geometry of the neural manifold.\nBy dividing the learning system into subsystems, RFIM makes this parametric approach much more implementable. The memory complexity of storing the Riemannian metric has been reduced from O(#\u03982) to O( \u2211 i #\u03b8 2 i ), where each \u03b8i corresponds to a subsystem, and #\u0398 means the dimensionality of \u0398. The computational complexity has been reduced from O(#\u0398%) (% \u2248 2.373, Williams (2012)) to O( \u2211 i #\u03b8 % i ). Approximation techniques of FIM to improve these complexities can be applied to RFIM. Optimization based on RFIM will be called Relative Natural Gradient Descent (RNGD). In the following of this section, we present two examples of RNGD. The objective is to demonstrate its advantages and mechanisms.\n5.1 RNGD with a Single sigm Neuron The first experiment is a single neuron model to implement logistic regression (Minka, 2003). The focus of the experiment is to demonstrate the mechanisms of RNGD and to show on a small scale model the improvement made by RNGD over feature whitening techniques, so that we can expect the same improvement on a larger model.\nTo classify a sample set {(xi, yi)} with features x and class labels y \u2208 {0, 1}, a statistical model is assumed as\np(y = 1) = sigm(\u03b8\u1d40z),\nz = ((x\u2212 a)\u1d40A\u1d40, 1)\u1d40. (28)\nIn eq. (28), \u03b8 is the canonical parameters or the link weights of the neuron. A and a are for feature whitening (Montavon and M\u00fcller, 2012; Desjardins et al., 2015). They are precomputed and fixed during learning, so that the transformed samples {A(xi \u2212 a)} have zero mean and unit covariance except singular dimensions. The learning cost function is the average cross entropy\nL(\u03b8) = \u2212yi ln sigm(\u03b8\u1d40zi)\u2212 (1\u2212 yi) ln [1\u2212 sigm(\u03b8\u1d40zi)], (29)\nwhose gradient is simply 5L = (sigm(\u03b8\u1d40zi)\u2212 yi) zi. To apply RNGD, by eq. (14), we have gy(\u03b8) = \u03bdsigm (\u03b8, zi) ziz \u1d40 i . In each learning step, we update \u03b8 based on\n\u03b8new \u2190 \u03b8old \u2212 \u03b3(gy(\u03b8) + I)\u22121 5 L, (30)\nwhere \u03b3 is a learning rate, and \u03b5 > 0 is a hyper-parameter to guarantee that (gy(\u03b8) + I) is invertible. We choose empirically to be \u03b5tr(gy(\u03b8))/D, where \u03b5 = 10\u22122.\nBased on a gradient descent optimizer with constant learning rate and momentum, we compare four different methods:\n1. GD fixes A = I, a = 0, and applies gradient descent;\n2. WhiteGD performs feature whitening by pre-computing A and a as well as gradient descent;\n3. NGD fixes A = I, a = 0, and updates \u03b8 based on eq. (30);\n4. WhiteNGD performs both feature whitening and the updating scheme in eq. (30).\nBased on the MNIST dataset (LeCun et al., 1998), fig. 4 shows their learning curves on binary classification problems \u201c3\u201d vs \u201c5\u201d and \u201c4\u201d vs \u201c9\u201d on two different training sizes. For each method, the best curve which achieved the minimal training error after 100 epochs is shown. The configuration\ngrid is given as follows. The candidate learning rate is in the range {10\u22122, 10\u22121, 1, 10, 100}. The candidate momentum is in the range {0, 0.8}.\nIn different cases, WhiteNGD and NGD can consistently reach deeper in the error surface within a reasonable number of epochs as compared to WhiteGD and GD. Among all methods, WhiteNGD performs best, which demonstrates the dependency of natural gradient and RFIM on the coordinate system. In a whitened coordinate system, RFIM is expected to have better conditions in average, and therefore leading to better optimization. Intuitively, in eq. (14), the term \u03bdsigm (\u03b8,x) serves as a \u201cselector\u201d, highlighting a subset of {xi} in the linear region of the perceptron. The updating rule in eq. (30) will zoom in and de-correlate the sample variance in this region, and let the classifier focus on the discriminative samples.\nGradient descent depends much more on the choice of the coordinate system. There is a\nsignificant improvement from GD to WhiteGD. A whitened coordinate system allows larger learning rates. In the first several epochs, WhiteGD can even learn faster than NGD and WhiteNGD, as shown in fig. 4c.\n5.2 RNGD with a relu MLP The good performance of batch normalization (BN) (Ioffe and Szegedy, 2015) can be explained using RFIM. Basically, BN uses an inter-sample normalization layer to transform the input of each layer, denoted as x, to be zero mean and unit variance and thus reduces \u201cinternal covariate shift\u201d. In a typical case, above this normalization layer is a linear layer given by y = W \u1d40x. By eq. (22), if x is normalized, then the diagonal entries of gy(W ) becomes uniform. Therefore the geometry of the parameter manifold is conditioned. In this case, BN helps to condition the RFIM of a linear layer.\nThis subsection looks at a larger subsystem consisting of a linear layer plus a nonlinear activation layer above it, given by y = f(W \u1d40x). By eq. (23), its RFIM is diag [ \u03bdf (w1,x)x\u0303x\u0303 \u1d40, \u00b7 \u00b7 \u00b7 , \u03bdf (wm,x)x\u0303x\u0303\u1d40 ]. To perform RNGD, one need to update this layer by\nwnew1 \u2190 wold1 \u2212 \u03bdf (w1,xi)x\u0303ix\u0303 \u1d40 i + I\n\u22121 \u2202E\n\u2202w1 ,\n\u00b7 \u00b7 \u00b7\nwnewm \u2190 woldm \u2212 \u03bdf (wm,xi)x\u0303ix\u0303 \u1d40 i + I\n\u22121 \u2202E\n\u2202wm , (31)\nwhere E is the cost function, and > 0 is a hyper parameter to avoid singularity. However, this update requires solving many linear subsystems and is too expensive to compute. Moreover, we only have a mini batch which contains not enough information to compute the RFIM. To tackle these difficulties, we maintain an exponentially moving average of Gl, the RFIM of the l\u2019th neuron in this layer. Initially, Gl is initialized to identity. At each iteration, it is updated by\nGnewl \u2190 \u03bbGoldl + (1\u2212 \u03bb)\u03bdf (wl,xi)x\u0303ix\u0303 \u1d40 i + I, (32)\nwhere the average is taken over all samples in this mini batch, and \u03bb is a decaying rate. Every T iterations, we recompute G\u22121l based on the most current Gl, and store the resulting G \u22121 l . In the next T iterations, this G\u22121l will remain constant and be used as an approximation of inverse RFIM. Then, the updating rule of the layer is given by\nwnew1 \u2190 wold1 \u2212G\u221211 \u2202E \u2202w1 \u00b7 \u00b7 \u00b7 wnewm \u2190 woldm \u2212G\u22121m \u2202E \u2202wm . (33)\nFor the input layer which scales with the number of input features, and the final soft-max layer, we apply instead the RFIM of the corresponding linear layer for the consideration of the computational efficiency.\nWe implemented the proposed method using TensorFlow (Abadi et al., 2015) and applied it to classify MNIST digits. The network has shape 784-64-64-64-10, with relu activation units, a final soft-max layer, and uses average cross-entropy as the cost function.\nFigure 5 shows the learning curves of different methods. SGD is stochastic gradient descent. ADAM is the Adam optimizer (Kingma and Ba, 2014). PLAIN means a plain MLP without batch normalization. BNA and BNB are two different implementations of BN, depending on whether BN is performed right before (BNB) or right after (BNA) the activation of the hidden units. They both use a re-scaling parameter to ensure enough flexibility of the parametric structure (Ioffe and Szegedy, 2015). The epsilon parameter of both BNA and BNB is set to 10\u22123. For RNGD, we\nset empirically T = 100 and \u03bb = 0.995. The right table shows the mean and standard deviation of the \u03c4 -sharp ratio, defined as the training cost over the last \u03c4M mini-batch iterations, where M = 55000 is the total number of iterations.\nWe can see that RNGD can significantly improve the learning curve over the other methods and achieve a smaller sharp ratio. The mechanism is similar to the first experiment. By eq. (18), \u03bdrelu(wi,x) is approximately binary, emphasizing such informative samples with w\u1d40i x\u0303 > 0, which are the ones contributing to the learning of wi with non-zero gradient values. Each output neuron has a different subset of informative samples. While BN normalizes the layer input x regardless of the output neurons, RNGD normalizes x differently wrt different output, so that the informative samples for each output neuron is centered and decorrelated.\nAs shown in fig. 5, RNGD appears to have a larger variation during learning. This is because of the non-smooth update of the inverse metric. One can however see from the right table that the variation is actually not much, as the y-axis is in log-scale.\nBy the results of RNGD, it is clear that this MLP structure overfits the input data. It is a fundamental trade-off between fitting the input data and generalizing. This objective of this experiment is mainly focused on improving learning optimization. The overfitting can be tackled by early stopping.\nRNGD\u2019s computational time per each iteration is much more than the other methods. In our experiments on a GPU instance of Amazon EC2, RNGD costs around half a minute per each epoch, while the other methods only costs seconds. On CPUs RNGD is even more time consuming. This is both due to the inefficiency of our implementation and its algorithm complexity. To seek efficient RNGD implementations is left for future work."}, {"heading": "6 Conclusion and discussion", "text": "We propose to investigate local structures of large learning systems using the new concept of Relative Fisher Information Matrix (RFIM). The key advantage of this approach is that the local dynamics can be analyzed in an accurate way without approximation. We present a core list of such local structures in neuron networks, and give their corresponding RFIMs. This list of recipes can be used to provide guiding principles to neuron networks. As a first example, we demonstrated how to apply single neuron RFIM and one layer RFIM to improve the learning curve by using the relative natural gradient descent.\nOur work applies to mirror descent as well since natural gradient is related to mirror descent (Raskutti and Mukherjee, 2015) as follows: In mirror descent, given a strictly convex distance function D(\u00b7, \u00b7) in the first argument (playing the role of the proximity function), we express the gradient descent step as :\n\u0398t+1 = arg min \u0398 {\u0398>\u2207F (\u0398t) +\n1 \u03b3 D(\u0398,\u0398t)}.\nWhen D(\u0398,\u0398\u2032) is chosen as a Bregman divergence BF ((\u0398,\u0398\u2032) = F (\u0398)\u2212F (\u0398\u2032)\u2212(\u0398\u2212\u0398\u2032)>\u2207F (\u0398), it has been proved that the mirror descent on the \u0398-parameterization is equivalent (Raskutti and Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (\u22072F (\u0398)) parameterized by the dual coordinate system H = \u2207F (\u0398). In general, to perform a Riemannian gradient descent for minimizing a real-valued function f(\u0398) on the manifold, one needs to choose a proper metric tensor given in matrix form G(\u0398). Thomas (2014) constructed a toy example showing that the natural gradient may diverge while the ordinary gradient (for G = I, the identity matrix) converges. Recently, Thomas et al. (2016) proposed a new kind of descent method based on what they called the Energetic Natural Gradient that generalizes the natural gradient. The energy distance DE(p(\u03981), p(\u03982))2 = E[2dp(\u03981)(X,Y )\u2212 dp(\u03981)(X,X\n\u2032) \u2212 dp(\u03981)(Y, Y \u2032)] where X,X \u2032 \u223c p(\u03981) and Y, Y \u2032 \u223c p(\u03982), where dp(\u03981)(\u00b7, \u00b7) is a distance metric over the support. Using a Taylor\u2019s expansion on their energy distance, they get the Energy Information Matrix (in a way similar to recovering the FIM from a Taylor\u2019s expansion of any f -divergence like the Kullback-Leibler divergence). Their idea is to incorporate prior knowledge on the structure of the support (observation space) to define energy distance. Twisting the geometry of the support (say, Wassertein\u2019s optimal transport) with the geometry of the parametric distributions (Fisher-Rao geodesic distances) is indeed important (Chizat et al., 2015). In information geometry, invariance on the support is provided by a Markov morphism that is a probabilistic mapping of support to itself (\u010cencov, 1982). Markov morphism include deterministic transformation of a random variable by a statistic. It is well-known that IT (\u0398) \u227a IX(\u0398) with equality iff. T = T (X) is a sufficient statistic of X. Thus to get the same invariance for the energy distance (Thomas et al., 2016), one shall further require dp(\u0398)(T (X), T (Y )) = dp(\u0398)(X,Y ).\nIn the foreseeable future, we believe that Relative Fisher Information Metrics (RFIMs) will provide a sound methodology to build further efficient learning techniques in deep learning.\nOur implementation is available at https://www.lix.polytechnique.fr/~nielsen/RFIM."}, {"heading": "A Non-linear Activation Functions", "text": "By definition,\ntanh(t) def = exp(t)\u2212 exp(\u2212t) exp(t) + exp(\u2212t) , (34)\nand\nsech(t) def =\n2\nexp(t) + exp(\u2212t) . (35)\nIt is easy to verify that\nsech2(t) = [1 + tanh(t)] [1\u2212 tanh(t)] = 1\u2212 tanh2(t). (36)\nBy eq. (34),\ntanh\u2032(t) = exp(t) + exp(\u2212t) exp(t) + exp(\u2212t) \u2212 exp(t)\u2212 exp(\u2212t) [exp(t) + exp(\u2212t)]2 [exp(t)\u2212 exp(\u2212t)]\n= [exp(t) + exp(\u2212t)]2 \u2212 [exp(t)\u2212 exp(\u2212t)]2\n[exp(t) + exp(\u2212t)]2 =\n4\n[exp(t) + exp(\u2212t)]2 = sech2(t). (37)\nBy definition,\nsigm(t) def =\n1\n1 + exp(\u2212t) . (38)\nTherefore\nsigm\u2032(t) = \u2212 1 [1 + exp(\u2212t)]2 (\u2212 exp(\u2212t)) = exp(\u2212t) [1 + exp(\u2212t)]2 = sigm(t) [1\u2212 sigm(t)] . (39)\nBy definition,\nrelu\u03c9(t) def = \u03c9 ln ( exp ( \u03b9t\n\u03c9\n) + exp ( t\n\u03c9\n)) , (40)\nwhere \u03c9 > 0 and 0 \u2264 \u03b9 < 1. Then,\nrelu\u2032\u03c9(t) = \u03c9 1 exp ( \u03b9t \u03c9 ) + exp ( t \u03c9 ) ( \u03b9 \u03c9 exp ( \u03b9t \u03c9 ) + 1 \u03c9 exp ( t \u03c9 )) = \u03b9 exp ( \u03b9t \u03c9 ) + exp ( t \u03c9 ) exp ( \u03b9t \u03c9 ) + exp ( t \u03c9\n) = \u03b9+ (1\u2212 \u03b9) exp ( t \u03c9 ) exp ( \u03b9t \u03c9 ) + exp ( t \u03c9\n) = \u03b9+ (1\u2212 \u03b9) 1\nexp ( (\u03b9\u2212 1) t\u03c9 ) + 1\n= \u03b9+ (1\u2212 \u03b9)sigm (\n1\u2212 \u03b9 \u03c9 t\n) . (41)\nBy definition,\nelu(t) = { t if t \u2265 0 \u03b1 (exp(t)\u2212 1) if t < 0. (42)\nTherefore elu\u2032(t) = { 1 if t \u2265 0 \u03b1 exp(t) if t < 0. (43)\nB A Single tanh Neuron Consider a neuron with parameters w and a Bernoulli output y \u2208 {+,\u2212}, p(y = +) = p+, p(y = \u2212) = p\u2212, and p+ + p\u2212 = 1. By the definition of RFIM, we have\ngy(w) = p+ \u2202 ln p+\n\u2202w\n\u2202 ln p+\n\u2202w\u1d40 + p\u2212\n\u2202 ln p\u2212\n\u2202w\n\u2202 ln p\u2212\n\u2202w\u1d40\n= 1 p+ \u2202p+ \u2202w \u2202p+ \u2202w\u1d40 + 1 p\u2212 \u2202p\u2212 \u2202w \u2202p\u2212 \u2202w\u1d40 . (44)\nSince p+ + p\u2212 = 1,\n\u2202p+ \u2202w + \u2202p\u2212 \u2202w = 0. (45)\nTherefore, the RFIM of a Bernoulli neuron has the general form\ngy(w) =\n( 1\np+ +\n1\np\u2212\n) \u2202p+\n\u2202w\n\u2202p+ \u2202w\u1d40 = 1 p+p\u2212 \u2202p+ \u2202w \u2202p+ \u2202w\u1d40 . (46)\nA single tanh neuron with stochastic output y \u2208 {\u22121, 1} is given by\np(y = \u22121) = 1\u2212 \u00b5(x) 2 , (47)\np(y = 1) = 1 + \u00b5(x)\n2 , (48)\n\u00b5(x) = tanh(w\u1d40x\u0303). (49)\nBy eq. (46),\ngy(w) = 1\n1\u2212\u00b5(x) 2 1+\u00b5(x) 2\n( 1\n2\n\u2202\u00b5\n\u2202w\n)( 1\n2\n\u2202\u00b5\n\u2202w\u1d40 ) = 1\n(1\u2212 \u00b5(x)) (1 + \u00b5(x)) [ 1\u2212 \u00b52(x) ]2 x\u0303x\u0303\u1d40\n= [ 1\u2212 \u00b52(x) ] x\u0303x\u0303\u1d40\n= [ 1\u2212 tanh2(w\u1d40x\u0303) ] x\u0303x\u0303\u1d40\n= sech2(w\u1d40x\u0303)x\u0303x\u0303\u1d40. (50)\nAn alternatively analysis is given as follows. By eqs. (47) to (49),\np(y = \u22121) = exp(\u2212w \u1d40x\u0303)\nexp(w\u1d40x) + exp(\u2212w\u1d40x) , (51)\np(y = 1) = exp(w\u1d40x\u0303)\nexp(w\u1d40x\u0303) + exp(\u2212w\u1d40x\u0303) . (52)\nThen,\ngy(w) = Ey\u223cp(y |x) ( \u2212\u2202 2 ln p(y)\n\u2202w\u2202w\u1d40 ) = \u22022\n\u2202w\u2202w\u1d40 ln [exp(w\u1d40x\u0303) + exp(\u2212w\u1d40x\u0303)] (first linear term vanishes)\n= \u2202\n\u2202w\u1d40 [ exp(w\u1d40x\u0303)\u2212 exp(\u2212w\u1d40x\u0303) exp(w\u1d40x\u0303) + exp(\u2212w\u1d40x\u0303) ] x\u0303\n= \u2202\n\u2202w\u1d40 tanh(w\u1d40x\u0303)x\u0303\n= sech2(w\u1d40x\u0303)x\u0303x\u0303\u1d40. (53)\nThe intuitive meaning of gy(w) is a weighted covariance to emphasize such \u201cinformative\u201d x\u2019s that\n\u2022 are in the linear region of tanh\n\u2022 contain \u201cambiguous\u201d samples\nWe will need at least dim(w) samples to make gy(w) full rank.\nC A Single sigm Neuron A single sigm neuron is given by\np(y = 0) = 1\u2212 \u00b5(x), (54) p(y = 1) = \u00b5(x), (55)\n\u00b5(x) = sigm(w\u1d40x\u0303). (56)\nBy eq. (46),\ngy(w) = 1\np(y = 0)p(y = 1)\n\u2202p(y = 1)\n\u2202w\n\u2202p(y = 1)\n\u2202w\u1d40\n= 1 \u00b5(x)(1\u2212 \u00b5(x)) \u2202\u00b5 \u2202w \u2202\u00b5 \u2202w\u1d40\n= 1\n\u00b5(x)(1\u2212 \u00b5(x)) \u00b52(x)(1\u2212 \u00b5(x))2x\u0303x\u0303\u1d40\n= \u00b5(x)(1\u2212 \u00b5(x))x\u0303x\u0303\u1d40\n= sigm(w\u1d40x\u0303) [1\u2212 sigm(w\u1d40x\u0303)] x\u0303x\u0303\u1d40. (57)\nD A Single relu Neuron Consider a single neuron with Gaussian output p(y |w,x) = G(y |\u00b5(w,x), \u03c32). Then\ngy(w |x) = Ep(y |w,x) [ \u2202 lnG(y |\u00b5, \u03c32)\n\u2202w\n\u2202 lnG(y |\u00b5, \u03c32) \u2202w\u1d40 ] = Ep(y |w,x) [ \u2202\n\u2202w\n( \u2212 1\n2\u03c32 (y \u2212 \u00b5)2\n) \u2202\n\u2202w\u1d40\n( \u2212 1\n2\u03c32 (y \u2212 \u00b5)2 )] = Ep(y |w,x) [( \u2212 1 \u03c32 (\u00b5\u2212 y) )2 \u2202\u00b5 \u2202w \u2202\u00b5 \u2202w\u1d40 ]\n= 1\n\u03c34 Ep(y |w,x) (\u00b5\u2212 y)\n2 \u2202\u00b5\n\u2202w\n\u2202\u00b5\n\u2202w\u1d40\n= 1 \u03c32 \u2202\u00b5 \u2202w \u2202\u00b5 \u2202w\u1d40 . (58)\nA single relu neuron is given by\n\u00b5(w,x) = relu\u03c9(w \u1d40x\u0303). (59)\nBy eqs. (41) and (58),\ngy(w) = 1\n\u03c32\n[ \u03b9+ (1\u2212 \u03b9)sigm ( 1\u2212 \u03b9 \u03c9 w\u1d40x\u0303 )]2 x\u0303x\u0303\u1d40. (60)\nE A Single elu Neuron Similar to the analysis in appendix D, a single elu neuron is given by\n\u00b5(w,x) = elu(w\u1d40x\u0303). (61)\nBy eq. (43), \u2202\u00b5\n\u2202w = { x\u0303 if w\u1d40x\u0303 \u2265 0 \u03b1 exp(w\u1d40x\u0303)x\u0303 if w\u1d40x\u0303 < 0. (62)\nBy eq. (58),\ngy(w) =\n{ 1 \u03c32 x\u0303x\u0303\n\u1d40 if w\u1d40x\u0303 \u2265 0 1 \u03c32 (\u03b1 exp(w \u1d40x\u0303)) 2 x\u0303x\u0303\u1d40 if w\u1d40x\u0303 < 0.\n(63)"}, {"heading": "F RFIM of a Linear Layer", "text": "Consider a linear layer\np(y) = G ( y |W \u1d40x\u0303, \u03c32I ) , (64)\nwhere W = (w1, \u00b7 \u00b7 \u00b7 ,wDy ). By the definition of multivariate Gaussian distribution,\nln p(y) = \u22121 2 ln 2\u03c0 \u2212 Dy 2 ln\u03c32 \u2212 1 2\u03c32 Dy\u2211 i=1 (yi \u2212w\u1d40i x\u0303) 2 . (65)\nTherefore,\n\u2200i, \u2202 \u2202wi ln p(y) = \u2212 1 \u03c32 (w\u1d40i x\u0303\u2212 yi) x\u0303. (66)\nTherefore,\n\u2200i,\u2200j \u2202 \u2202wi ln p(y) \u2202 \u2202w\u1d40j ln p(y) = 1 \u03c34 (yi \u2212w\u1d40i x\u0303)\n( yj \u2212w\u1d40j x\u0303 ) x\u0303x\u0303\u1d40. (67)\nW is vectorized by stacking its columns {wi} Dy i=1. In the followingW will be used interchangeably to denote either the matrix or its vector form. Correspondingly, the RFIM gy(W ) has Dy \u00d7Dy blocks, where the off-diagonal blocks are\n\u2200i 6= j, Ep(y)\n( \u2202\n\u2202wi ln p(y)\n\u2202\n\u2202w\u1d40j ln p(y)\n) = 1\n\u03c34 Ep(y)\n[ (yi \u2212w\u1d40i x\u0303) ( yj \u2212w\u1d40j x\u0303 )] x\u0303x\u0303\u1d40 = 0, (68)\nand the diagonal blocks are \u2200i, Ep(y) ( \u2202\n\u2202wi ln p(y)\n\u2202\n\u2202w\u1d40i ln p(y)\n) = 1\n\u03c34 Ep(y) (yi \u2212w\u1d40i x\u0303) 2 x\u0303x\u0303\u1d40 =\n1\n\u03c32 x\u0303x\u0303\u1d40. (69)\nIn summary,\ngy(W ) = 1\n\u03c32 diag [x\u0303x\u0303\u1d40, \u00b7 \u00b7 \u00b7 , x\u0303x\u0303\u1d40] . (70)"}, {"heading": "G RFIM of a Non-Linear Layer", "text": "The statistical model of a non-linear layer is\np(y |W ,x) = Dy\u220f i=1 p(yi |wi,x). (71)\nThen,\nln p(y |W ,x) = Dy\u2211 i=1 ln p(yi |wi,x). (72)\nTherefore,\n\u22022\n\u2202W \u2202W \u1d40 ln p(y |W ,x) =\n \u22022 \u2202w1\u2202w \u1d40 1 ln p(y1 |w1,x) . . .\n\u22022\n\u2202wDy\u2202w \u1d40 Dy\nln p(yDy |wDy ,x)\n . (73)\nTherefore RFIM gy(W ) is a block-diagonal matrix, with the i\u2019th block given by\n\u2212Ep(y |W ,x) [ \u22022\n\u2202wi\u2202w \u1d40 i\nln p(yi |wi,x) ] = \u2212Ep(yi |wi,x) [ \u22022\n\u2202wi\u2202w \u1d40 i\nln p(yi |wi,x) ] , (74)\nwhich is simply the single neuron RFIM of the i\u2019th neuron."}, {"heading": "H RFIM of a Softmax Layer", "text": "Recall that\n\u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 ,m} , p(y = i) = exp(wix\u0303)\u2211m i=1 exp(wix\u0303) . (75)\nThen\n\u2200i, ln p(y = i) = wix\u0303\u2212 ln m\u2211 i=1 exp(wix\u0303). (76)\nHence \u2200i, \u2200j, \u2202 ln p(y = i)\n\u2202wj = \u03b4ijx\u0303\u2212 exp(wjx\u0303)\u2211m i=1 exp(wix\u0303) x\u0303, (77)\nwhere \u03b4ij = 1 if and only if i = j and \u03b4ij = 0 otherwise. Then\n\u2200i, \u2200j, \u2200k, \u2202 2 ln p(y = i)\n\u2202wj\u2202w \u1d40 k = \u2212\u03b4jk exp(wjx\u0303)\u2211m i=1 exp(wix\u0303) x\u0303x\u0303\u1d40 + exp(wjx\u0303) ( \u2211m i=1 exp(wix\u0303)) 2 exp(wkx\u0303)x\u0303x\u0303 \u1d40\n= (\u2212\u03b4jk\u03b7j + \u03b7j\u03b7k) x\u0303x\u0303\u1d40. (78)\nThe right-hand-side of eq. (78) does not depend on i. Therefore\ngy(W ) =  (\u03b71 \u2212 \u03b721)x\u0303x\u0303\u1d40 \u2212\u03b71\u03b72x\u0303x\u0303\u1d40 \u00b7 \u00b7 \u00b7 \u2212\u03b71\u03b7mx\u0303x\u0303\u1d40 \u2212\u03b72\u03b71x\u0303x\u0303\u1d40 (\u03b72 \u2212 \u03b722)x\u0303x\u0303\u1d40 \u00b7 \u00b7 \u00b7 \u2212\u03b72\u03b7mx\u0303x\u0303\u1d40 ... ... . . . ...\n\u2212\u03b7m\u03b71x\u0303x\u0303\u1d40 \u2212\u03b7m\u03b72x\u0303x\u0303\u1d40 \u00b7 \u00b7 \u00b7 (\u03b7m \u2212 \u03b72m)x\u0303x\u0303\u1d40\n . (79)"}, {"heading": "I RFIM of Two layers", "text": "Consider a two layer structure, where the output y satisfies a multivariate Bernoulli distribution with independent dimensions. By a similar analysis to appendix B, we have\ngy(W ) = Dy\u2211 l=1 \u03bdf (cl,h) \u2202c\u1d40l h \u2202W \u2202c\u1d40l h \u2202W \u1d40 . (80)\nIt can be written block by block as gy(W ) = [Gij ]Dh\u00d7Dh , where each block Gij means the correlation between the i\u2019th hidden neuron with weights wi and the j\u2019th hidden neuron with\nweights wj . By eq. (80),\nGij = Dy\u2211 l=1 \u03bdf (cl,h) \u2202c\u1d40l h \u2202wi \u2202c\u1d40l h \u2202w\u1d40j = Dy\u2211 l=1 \u03bdf (cl,h) \u2202cilhi \u2202wi \u2202cjlhj \u2202w\u1d40j\n= Dy\u2211 l=1 \u03bdf (cl,h)cilcjl \u2202hi \u2202wi \u2202hj \u2202w\u1d40j = Dy\u2211 l=1 \u03bdf (cl,h)cilcjl (\u03bdf (wi,x)x\u0303) (\u03bdf (wj ,x)x\u0303 \u1d40)\n= Dy\u2211 l=1 cilcjl\u03bdf (cl,h)\u03bdf (wi,x)\u03bdf (wj ,x)x\u0303x\u0303 \u1d40. (81)\nThe proof of the other case, where two relu layers have stochastic output y satisfying a multivariate Gaussian distribution with independent dimensions, is very similar and is omitted."}], "references": [{"title": "Our implementation is available at https://www.lix.polytechnique.fr/~nielsen/RFIM", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "Neural learning in structured parameter spaces \u2013 natural Riemannian gradient", "author": ["S. Amari"], "venue": "In NIPS", "citeRegEx": "Amari.,? \\Q1997\\E", "shortCiteRegEx": "Amari.", "year": 1997}, {"title": "Natural gradient works efficiently in learning", "author": ["S. Amari"], "venue": "Neural Comput.,", "citeRegEx": "Amari.,? \\Q1998\\E", "shortCiteRegEx": "Amari.", "year": 1998}, {"title": "Information Geometry and its Applications, volume 194 of Applied Mathematical Sciences", "author": ["S. Amari"], "venue": null, "citeRegEx": "Amari.,? \\Q2016\\E", "shortCiteRegEx": "Amari.", "year": 2016}, {"title": "Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs", "author": ["S. Amari", "H. Nagaoka"], "venue": "AMS and OUP,", "citeRegEx": "Amari and Nagaoka.,? \\Q2000\\E", "shortCiteRegEx": "Amari and Nagaoka.", "year": 2000}, {"title": "Adaptive method of realizing natural gradient learning for multilayer perceptrons", "author": ["S. Amari", "H. Park", "K. Fukumizu"], "venue": "Neural Comput.,", "citeRegEx": "Amari et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Amari et al\\.", "year": 2000}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Y. Bengio"], "venue": "CoRR, abs/1305.2982,", "citeRegEx": "Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Bengio.", "year": 2013}, {"title": "Stochastic gradient descent on Riemannian manifolds", "author": ["S. Bonnabel"], "venue": "IEEE Trans. Automat. Contr.,", "citeRegEx": "Bonnabel.,? \\Q2013\\E", "shortCiteRegEx": "Bonnabel.", "year": 2013}, {"title": "Statistical decision rules and optimal inference, volume 53 of Translations of Mathematical Monographs", "author": ["N.N. \u010cencov"], "venue": "American Mathematical Society,", "citeRegEx": "\u010cencov.,? \\Q1982\\E", "shortCiteRegEx": "\u010cencov.", "year": 1982}, {"title": "An Interpolating Distance between Optimal Transport and Fisher-Rao", "author": ["L. Chizat", "B. Schmitzer", "G. Peyr\u00e9", "F.-X. Vialard"], "venue": "ArXiv e-prints,", "citeRegEx": "Chizat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chizat et al\\.", "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["D. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "CoRR, abs/1511.07289,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Estimation and moment recursion relations for multimodal distributions of the exponential family", "author": ["L. Cobb", "P. Koppstein", "N.H. Chen"], "venue": "JASA, 78(381):124\u2013130,", "citeRegEx": "Cobb et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Cobb et al\\.", "year": 1983}, {"title": "Parameter orthogonality and approximate conditional inference", "author": ["D.R. Cox", "N. Reid"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Cox and Reid.,? \\Q1987\\E", "shortCiteRegEx": "Cox and Reid.", "year": 1987}, {"title": "Natural neural networks", "author": ["G. Desjardins", "K. Simonyan", "R. Pascanu", "K. Kavukcuoglu"], "venue": "In NIPS", "citeRegEx": "Desjardins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2015}, {"title": "Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information", "author": ["B. Efron", "D.V. Hinkley"], "venue": null, "citeRegEx": "Efron and Hinkley.,? \\Q1978\\E", "shortCiteRegEx": "Efron and Hinkley.", "year": 1978}, {"title": "Block diagonalization of four-dimensional metrics", "author": ["J.D. Grant", "J. Vickers"], "venue": "Classical and Quantum Gravity,", "citeRegEx": "Grant and Vickers.,? \\Q2009\\E", "shortCiteRegEx": "Grant and Vickers.", "year": 2009}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Probability distributions and orthogonal parameters", "author": ["V.S. Huzurbazar"], "venue": "In Mathematical Proceedings of the Cambridge Philosophical Society,", "citeRegEx": "Huzurbazar.,? \\Q1950\\E", "shortCiteRegEx": "Huzurbazar.", "year": 1950}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In ICML; JMLR: W&CP", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Theory of Probability", "author": ["H. Jeffreys"], "venue": null, "citeRegEx": "Jeffreys.,? \\Q1961\\E", "shortCiteRegEx": "Jeffreys.", "year": 1961}, {"title": "Riemannian Geometry and Geometric Analysis", "author": ["J. Jost"], "venue": "Springer, 6th edition,", "citeRegEx": "Jost.,? \\Q2011\\E", "shortCiteRegEx": "Jost.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "In ICML, pages 735\u2013742,", "citeRegEx": "Martens.,? \\Q2010\\E", "shortCiteRegEx": "Martens.", "year": 2010}, {"title": "Optimizing neural networks with Kronecker-factored approximate curvature", "author": ["J. Martens", "R. Grosse"], "venue": "In ICML; JMLR: W&CP", "citeRegEx": "Martens and Grosse.,? \\Q2015\\E", "shortCiteRegEx": "Martens and Grosse.", "year": 2015}, {"title": "A comparison of numerical optimizers for logistic regression", "author": ["T.P. Minka"], "venue": "Technical report,", "citeRegEx": "Minka.,? \\Q2003\\E", "shortCiteRegEx": "Minka.", "year": 2003}, {"title": "Computational implications of reducing data to sufficient statistics", "author": ["A. Montanari"], "venue": "Electron. J. Statist.,", "citeRegEx": "Montanari.,? \\Q2015\\E", "shortCiteRegEx": "Montanari.", "year": 2015}, {"title": "Deep Boltzmann machines and the centering trick", "author": ["G. Montavon", "K.R. M\u00fcller"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Montavon and M\u00fcller.,? \\Q2012\\E", "shortCiteRegEx": "Montavon and M\u00fcller.", "year": 2012}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Pascanu and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu and Bengio.", "year": 2014}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["T. Raiko", "H. Valpola", "Y. LeCun"], "venue": "In AISTATS; JMLR W&CP", "citeRegEx": "Raiko et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2012}, {"title": "The information geometry of mirror descent", "author": ["G. Raskutti", "S. Mukherjee"], "venue": "ISBN 978-3-319-25039-7", "citeRegEx": "Raskutti and Mukherjee.,? \\Q2015\\E", "shortCiteRegEx": "Raskutti and Mukherjee.", "year": 2015}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["N.L. Roux", "P. Manzagol", "Y. Bengio"], "venue": "In NIPS", "citeRegEx": "Roux et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2008}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "GeNGA: A generalization of natural gradient ascent with positive and negative convergence results", "author": ["P. Thomas"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Thomas.,? \\Q2014\\E", "shortCiteRegEx": "Thomas.", "year": 2014}, {"title": "Energetic natural gradient descent", "author": ["P. Thomas", "B.C. da Silva", "C. Dann", "E. Brunskill"], "venue": "In Proceedings of the 33st International Conference on Machine Learning", "citeRegEx": "Thomas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2016}, {"title": "Dropout training as adaptive regularization", "author": ["S. Wager", "S. Wang", "P.S. Liang"], "venue": "In NIPS", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Algebraic Geometry and Statistical Learning Theory, volume 25 of Cambridge Monographs on Applied and Computational Mathematics", "author": ["S. Watanabe"], "venue": null, "citeRegEx": "Watanabe.,? \\Q2009\\E", "shortCiteRegEx": "Watanabe.", "year": 2009}, {"title": "Multiplying matrices faster than coppersmith-Winograd", "author": ["V.V. Williams"], "venue": "In Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Williams.,? \\Q2012\\E", "shortCiteRegEx": "Williams.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Although exponential families can approximate arbitrarily any smooth density (Cobb et al., 1983), the mgf may not be available in closed-form nor computationally tractable (Montanari, 2015).", "startOffset": 77, "endOffset": 96}, {"referenceID": 25, "context": ", 1983), the mgf may not be available in closed-form nor computationally tractable (Montanari, 2015).", "startOffset": 83, "endOffset": 100}, {"referenceID": 36, "context": "Besides the fact that learning machines usually have often singularities (Watanabe, 2009) (|I(\u0398)| = 0, not full rank) characterized by plateaux in gradient learning, computing/estimating the FIM of a large learning system is very challenging due to the finiteness of data, and the large number D(D+1) 2 of matrix coefficients to evaluate.", "startOffset": 73, "endOffset": 89}, {"referenceID": 12, "context": "Fisher diagonalization yields parameter orthogonalization (Cox and Reid, 1987), and prove useful when estimating \u0398\u0302 using MLE that is asymptotically normally distributed, \u0398\u0302n = G(\u0398, I\u22121(\u0398)/ \u221a n), where G(\u03b81,\u03b82) denotes a univariate or multivariate Gaussian distribution with mean \u03b81 and variance \u03b82, and efficient since the variance of the estimator matches the Cram\u00e9r-Rao lower bound.", "startOffset": 58, "endOffset": 78}, {"referenceID": 17, "context": "Thus in general, we end up with ( D 2 ) = D(D\u22121) 2 (non-linear) partial differential equations to satisfy (Huzurbazar, 1950).", "startOffset": 106, "endOffset": 124}, {"referenceID": 33, "context": "When the FIM may be degenerate, this yields a pseudo-Riemannian manifold (Thomas, 2014).", "startOffset": 73, "endOffset": 87}, {"referenceID": 17, "context": "However, such a global orthogonal metric does not exist (Huzurbazar, 1950) when D > 3 for an arbitrary metric tensor although interesting Riemannian parameterization structures may be derived in Riemannian 4D geometry (Grant and Vickers, 2009).", "startOffset": 56, "endOffset": 74}, {"referenceID": 15, "context": "However, such a global orthogonal metric does not exist (Huzurbazar, 1950) when D > 3 for an arbitrary metric tensor although interesting Riemannian parameterization structures may be derived in Riemannian 4D geometry (Grant and Vickers, 2009).", "startOffset": 218, "endOffset": 243}, {"referenceID": 3, "context": "For NEFs, the FIM can be made block-diagonal easily by using the mixed coordinate system (Amari, 2016) (\u03981:k,Hk+1:D), where H = Ep[t(x)] = \u2207F (\u0398) is the moment parameter, for any k \u2208 {1, .", "startOffset": 89, "endOffset": 102}, {"referenceID": 3, "context": "The geometry of NEFs is a dually flat structure (Amari, 2016) induced by the convex mgf, the potential function.", "startOffset": 48, "endOffset": 61}, {"referenceID": 12, "context": "Thus in general, we end up with ( D 2 ) = D(D\u22121) 2 (non-linear) partial differential equations to satisfy (Huzurbazar, 1950). Therefore, in general there is no solution when ( D 2 ) > D, that is when D > 3. When D = 2, the single differential equations is usually solvable and tractable, and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the location-scale families { 1 \u03c3p0( x\u2212\u03bc \u03c3 )} that include the Gaussian family and the Cauchy family.", "startOffset": 107, "endOffset": 355}, {"referenceID": 12, "context": "Thus in general, we end up with ( D 2 ) = D(D\u22121) 2 (non-linear) partial differential equations to satisfy (Huzurbazar, 1950). Therefore, in general there is no solution when ( D 2 ) > D, that is when D > 3. When D = 2, the single differential equations is usually solvable and tractable, and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the location-scale families { 1 \u03c3p0( x\u2212\u03bc \u03c3 )} that include the Gaussian family and the Cauchy family. Sometimes, the structure of the differential equation system yields a solution: For example, Jeffreys (1961) reported a parameter orthogonalization for Pearson\u2019s distributions of type I which is of order D = 4.", "startOffset": 107, "endOffset": 608}, {"referenceID": 9, "context": "Cox and Reid (1987) further investigate this topic with application to conditional inference, and provide examples (including the Weibull distribution).", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "where \u201c \u00b7 \u201d denotes the sample average over {xk}, the resulting metric is called the observed FIM (Efron and Hinkley, 1978).", "startOffset": 98, "endOffset": 123}, {"referenceID": 31, "context": "Past works on FIM-based approaches mainly focus on how to approximate the global FIM into a block diagonal form using the gradient of the cost function (Roux et al., 2008; Martens, 2010; Pascanu and Bengio, 2014; Martens and Grosse, 2015).", "startOffset": 152, "endOffset": 238}, {"referenceID": 22, "context": "Past works on FIM-based approaches mainly focus on how to approximate the global FIM into a block diagonal form using the gradient of the cost function (Roux et al., 2008; Martens, 2010; Pascanu and Bengio, 2014; Martens and Grosse, 2015).", "startOffset": 152, "endOffset": 238}, {"referenceID": 28, "context": "Past works on FIM-based approaches mainly focus on how to approximate the global FIM into a block diagonal form using the gradient of the cost function (Roux et al., 2008; Martens, 2010; Pascanu and Bengio, 2014; Martens and Grosse, 2015).", "startOffset": 152, "endOffset": 238}, {"referenceID": 23, "context": "Past works on FIM-based approaches mainly focus on how to approximate the global FIM into a block diagonal form using the gradient of the cost function (Roux et al., 2008; Martens, 2010; Pascanu and Bengio, 2014; Martens and Grosse, 2015).", "startOffset": 152, "endOffset": 238}, {"referenceID": 27, "context": "Note that Restricted Boltzmann Machines (Nair and Hinton, 2010; Montavon and M\u00fcller, 2012) (RBMs), and dropout (Wager et al.", "startOffset": 40, "endOffset": 90}, {"referenceID": 26, "context": "Note that Restricted Boltzmann Machines (Nair and Hinton, 2010; Montavon and M\u00fcller, 2012) (RBMs), and dropout (Wager et al.", "startOffset": 40, "endOffset": 90}, {"referenceID": 35, "context": "Note that Restricted Boltzmann Machines (Nair and Hinton, 2010; Montavon and M\u00fcller, 2012) (RBMs), and dropout (Wager et al., 2013) do consider h to be stochastic.", "startOffset": 111, "endOffset": 131}, {"referenceID": 1, "context": "In this update procedure, the term g(\u0398t) 5 L(\u0398t) replaces the role of the usual gradient 5L(\u0398t) and is called the natural gradient (Amari, 1997).", "startOffset": 131, "endOffset": 144}, {"referenceID": 2, "context": "4 (Amari, 1998)).", "startOffset": 2, "endOffset": 15}, {"referenceID": 36, "context": "This set of parameters forms an analytic variety (Watanabe, 2009), and technically the MLP as a statistical model is said non-regular (and the parameter \u0398 is not identifiable).", "startOffset": 49, "endOffset": 65}, {"referenceID": 33, "context": "The natural gradient has been extended (Thomas, 2014) to cope with singular FIMs having positive semi-definite matrices by taking the Moore-Penrose pseudo-inverse (that coincides with the inverse matrix for full rank matrices).", "startOffset": 39, "endOffset": 53}, {"referenceID": 22, "context": "(Martens, 2010), by checking whether the computation of the Hessian term depends on the cost function.", "startOffset": 0, "endOffset": 15}, {"referenceID": 5, "context": "We refer the reader to related references (Amari et al., 2000) for more details.", "startOffset": 42, "endOffset": 62}, {"referenceID": 7, "context": "Bonnabel (Bonnabel, 2013) proposed to use the Riemannian exponential map to define a step gradient descent, thus ensuring to stay on the manifold for any chosen learning rate.", "startOffset": 9, "endOffset": 25}, {"referenceID": 3, "context": "In fact, the particular case of a mixed coordinate system (that is not an affine coordinate system) induces in information geometry (Amari, 2016) a dual pair of orthogonal e- and morthogonal foliations.", "startOffset": 132, "endOffset": 145}, {"referenceID": 4, "context": "Our splits in RFIMs consider non-orthogonal foliations that provide the factorization decompositions of the whole manifold into submanifolds, that are the leaves of the foliation (Amari and Nagaoka, 2000).", "startOffset": 179, "endOffset": 204}, {"referenceID": 1, "context": "Note that, the FIMs of small parametric structures such as single neurons have been studied for a long time (Amari, 1997).", "startOffset": 108, "endOffset": 121}, {"referenceID": 1, "context": "A neuron with sigmoid activation but continuous output was discussed earlier (Amari, 1997).", "startOffset": 77, "endOffset": 90}, {"referenceID": 16, "context": "3 Parametric Rectified Linear Unit Another commonly used activation function is Parametric Rectified Linear Unit (PReLU) (He et al., 2015), which includes Rectified Linear Unit (ReLU) (Nair and Hinton, 2010) as a special case.", "startOffset": 121, "endOffset": 138}, {"referenceID": 27, "context": ", 2015), which includes Rectified Linear Unit (ReLU) (Nair and Hinton, 2010) as a special case.", "startOffset": 53, "endOffset": 76}, {"referenceID": 10, "context": "4 Exponential Linear Unit A stochastic exponential linear unit (ELU) (Clevert et al., 2015) with \u03b1 > 0 is", "startOffset": 69, "endOffset": 91}, {"referenceID": 20, "context": "(23), the one-layer RFIM is a product metric (Jost, 2011) and does not consider the inter-neuron correlations.", "startOffset": 45, "endOffset": 57}, {"referenceID": 32, "context": "If the number of parameters is greater than the sample size, which can be achieved especially with deep learning (Szegedy et al., 2015), then g(\u0398) is guaranteed to be singular.", "startOffset": 113, "endOffset": 135}, {"referenceID": 6, "context": "Essentially, RFIM integrates the internal stochasticity (Bengio, 2013) of the neural system by considering the output of each layer as random variables.", "startOffset": 56, "endOffset": 70}, {"referenceID": 26, "context": "A recent series of efforts (Montavon and M\u00fcller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach of applying natural gradient, which memorizes", "startOffset": 27, "endOffset": 99}, {"referenceID": 29, "context": "A recent series of efforts (Montavon and M\u00fcller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach of applying natural gradient, which memorizes", "startOffset": 27, "endOffset": 99}, {"referenceID": 13, "context": "A recent series of efforts (Montavon and M\u00fcller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach of applying natural gradient, which memorizes", "startOffset": 27, "endOffset": 99}, {"referenceID": 13, "context": "For example, natural neural networks (Desjardins et al., 2015) augment each layer with a redundant linear layer, and let these linear layers to parametrize the geometry of the neural manifold.", "startOffset": 37, "endOffset": 62}, {"referenceID": 13, "context": "For example, natural neural networks (Desjardins et al., 2015) augment each layer with a redundant linear layer, and let these linear layers to parametrize the geometry of the neural manifold. By dividing the learning system into subsystems, RFIM makes this parametric approach much more implementable. The memory complexity of storing the Riemannian metric has been reduced from O(#\u0398) to O( \u2211 i #\u03b8 2 i ), where each \u03b8i corresponds to a subsystem, and #\u0398 means the dimensionality of \u0398. The computational complexity has been reduced from O(#\u0398) (% \u2248 2.373, Williams (2012)) to O( \u2211 i #\u03b8 % i ).", "startOffset": 38, "endOffset": 571}, {"referenceID": 24, "context": "1 RNGD with a Single sigm Neuron The first experiment is a single neuron model to implement logistic regression (Minka, 2003).", "startOffset": 112, "endOffset": 125}, {"referenceID": 26, "context": "A and a are for feature whitening (Montavon and M\u00fcller, 2012; Desjardins et al., 2015).", "startOffset": 34, "endOffset": 86}, {"referenceID": 13, "context": "A and a are for feature whitening (Montavon and M\u00fcller, 2012; Desjardins et al., 2015).", "startOffset": 34, "endOffset": 86}, {"referenceID": 18, "context": "2 RNGD with a relu MLP The good performance of batch normalization (BN) (Ioffe and Szegedy, 2015) can be explained using RFIM.", "startOffset": 72, "endOffset": 97}, {"referenceID": 0, "context": "We implemented the proposed method using TensorFlow (Abadi et al., 2015) and applied it to classify MNIST digits.", "startOffset": 52, "endOffset": 72}, {"referenceID": 21, "context": "ADAM is the Adam optimizer (Kingma and Ba, 2014).", "startOffset": 27, "endOffset": 48}, {"referenceID": 18, "context": "They both use a re-scaling parameter to ensure enough flexibility of the parametric structure (Ioffe and Szegedy, 2015).", "startOffset": 94, "endOffset": 119}, {"referenceID": 30, "context": "Our work applies to mirror descent as well since natural gradient is related to mirror descent (Raskutti and Mukherjee, 2015) as follows: In mirror descent, given a strictly convex distance function D(\u00b7, \u00b7) in the first argument (playing the role of the proximity function), we express the gradient descent step as :", "startOffset": 95, "endOffset": 125}, {"referenceID": 30, "context": "When D(\u0398,\u0398\u2032) is chosen as a Bregman divergence BF ((\u0398,\u0398\u2032) = F (\u0398)\u2212F (\u0398\u2032)\u2212(\u0398\u2212\u0398\u2032)>\u2207F (\u0398), it has been proved that the mirror descent on the \u0398-parameterization is equivalent (Raskutti and Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (\u2207F (\u0398)) parameterized by the dual coordinate system H = \u2207F (\u0398).", "startOffset": 171, "endOffset": 201}, {"referenceID": 9, "context": "Twisting the geometry of the support (say, Wassertein\u2019s optimal transport) with the geometry of the parametric distributions (Fisher-Rao geodesic distances) is indeed important (Chizat et al., 2015).", "startOffset": 177, "endOffset": 198}, {"referenceID": 8, "context": "In information geometry, invariance on the support is provided by a Markov morphism that is a probabilistic mapping of support to itself (\u010cencov, 1982).", "startOffset": 137, "endOffset": 151}, {"referenceID": 34, "context": "Thus to get the same invariance for the energy distance (Thomas et al., 2016), one shall further require dp(\u0398)(T (X), T (Y )) = dp(\u0398)(X,Y ).", "startOffset": 56, "endOffset": 77}, {"referenceID": 28, "context": "When D(\u0398,\u0398\u2032) is chosen as a Bregman divergence BF ((\u0398,\u0398\u2032) = F (\u0398)\u2212F (\u0398\u2032)\u2212(\u0398\u2212\u0398\u2032)>\u2207F (\u0398), it has been proved that the mirror descent on the \u0398-parameterization is equivalent (Raskutti and Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (\u2207F (\u0398)) parameterized by the dual coordinate system H = \u2207F (\u0398). In general, to perform a Riemannian gradient descent for minimizing a real-valued function f(\u0398) on the manifold, one needs to choose a proper metric tensor given in matrix form G(\u0398). Thomas (2014) constructed a toy example showing that the natural gradient may diverge while the ordinary gradient (for G = I, the identity matrix) converges.", "startOffset": 172, "endOffset": 555}, {"referenceID": 28, "context": "When D(\u0398,\u0398\u2032) is chosen as a Bregman divergence BF ((\u0398,\u0398\u2032) = F (\u0398)\u2212F (\u0398\u2032)\u2212(\u0398\u2212\u0398\u2032)>\u2207F (\u0398), it has been proved that the mirror descent on the \u0398-parameterization is equivalent (Raskutti and Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (\u2207F (\u0398)) parameterized by the dual coordinate system H = \u2207F (\u0398). In general, to perform a Riemannian gradient descent for minimizing a real-valued function f(\u0398) on the manifold, one needs to choose a proper metric tensor given in matrix form G(\u0398). Thomas (2014) constructed a toy example showing that the natural gradient may diverge while the ordinary gradient (for G = I, the identity matrix) converges. Recently, Thomas et al. (2016) proposed a new kind of descent method based on what they called the Energetic Natural Gradient that generalizes the natural gradient.", "startOffset": 172, "endOffset": 730}], "year": 2016, "abstractText": "Fisher information and natural gradient provided deep insights and powerful tools to artificial neural networks. However related analysis becomes more and more difficult as the learner\u2019s structure turns large and complex. This paper makes a preliminary step towards a new direction. We extract a local component of a large neuron system, and defines its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system. This concept is important because the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks. We provide an analysis on a list of commonly used components, and demonstrate how to use this concept to further improve optimization.", "creator": "LaTeX with hyperref package"}}}