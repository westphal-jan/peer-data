{"id": "1606.06368", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings", "abstract": "44.51 Can hawi we malygin train a system avoth that, on any new input, either says \" don ' palai t know \" rigney or qaf makes 250-seat a prediction that unswervingly is guaranteed alpern to c\u0101rv\u0101ka be adjourned correct? lucjan We answer najara the mcfly question gna in the sportsgirl affirmative internationalized provided our model family loyals is venation well - hanohano specified. Specifically, we aechmea introduce aggrandizing the unanimity principle: only silwad predict when 10:20 all exchange models collectivised consistent with the siquiera training peruna data camerons predict hardouin-mansart the glenmalure same westenra output. metagene We operationalize kulai this vistica principle for semantic parsing, roshal the electroluminescence task 20.48 of coastway mapping utterances sundov to gavins logical binaisa forms. 15,000-strong We develop a simple, rijksmonument efficient method grrrr that reasons over tcni the infinite intermittently set alaixys of kartheiser all daubeny consistent himeko models lower-level by only abide checking flat-four two of the models. We prove that kebri our method helsingin obtains looy 100% 130-kilometer precision chinchillas even ibama with a curve modest tearfund amount mma of horsting training data console from a possibly wimereux adversarial 71,953 distribution. Empirically, travelport we nasalized demonstrate dogface the www.tamilnet.com effectiveness of our dismaying approach squirrelfish on the kaloor standard GeoQuery dataset.", "histories": [["v1", "Mon, 20 Jun 2016 23:59:25 GMT  (4978kb)", "https://arxiv.org/abs/1606.06368v1", "ACL 2016"], ["v2", "Thu, 23 Jun 2016 07:33:01 GMT  (242kb,D)", "http://arxiv.org/abs/1606.06368v2", "ACL 2016, Removed the duplicate author name of the previous version"]], "COMMENTS": "ACL 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL", "authors": ["fereshte khani", "martin c rinard", "percy liang"], "accepted": true, "id": "1606.06368"}, "pdf": {"name": "1606.06368.pdf", "metadata": {"source": "CRF", "title": "Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings", "authors": ["Fereshte Khani", "Martin Rinard", "Percy Liang"], "emails": ["fereshte@cs.stanford.edu", "rinard@lcs.mit.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "If a user asks a system \u201cHow many painkillers should I take?\u201d, it is better for the system to say \u201cdon\u2019t know\u201d rather than making a costly incorrect prediction. When the system is learned from data, uncertainty pervades, and we must manage this uncertainty properly to achieve our precision requirement. It is particularly challenging since training inputs might not be representative of test inputs due to limited data, covariate shift (Shimodaira, 2000), or adversarial filtering (Nelson et al., 2009; Mei and Zhu, 2015). In this unforgiving setting, can we still train a system that is guaranteed to either abstain or to make the correct prediction?\nOur present work is motivated by the goal of\nbuilding reliable question answering systems and natural language interfaces. Our goal is to learn a semantic mapping from examples of utterancelogical form pairs (Figure 1). More generally, we assume the input x is a bag (multiset) of source atoms (e.g., words {area, of,Ohio}), and the output y is a bag of target atoms (e.g., predicates {area,OH}). We consider learning mappings M that decompose according to the multiset sum: M(x) = ]s\u2208xM(s) (e.g., M({Ohio}) = {OH}, M({area,of,Ohio}) = {area,OH}). The main challenge is that an individual training example (x, y) does not tell us which source atoms map to which target atoms.1\nHow can a system be 100% sure about something if it has seen only a small number of possibly non-representative examples? Our approach is based on what we call the unanimity principle (Section 2.1). LetM be a model family that contains the true mapping from inputs to outputs. Let C be the subset of mappings that are consistent\n1A semantic parser further requires modeling the context dependence of words and the logical form structure joining the predicates. Our framework handles these cases with a different choice of source and target atoms (see Section 4.2).\nar X\niv :1\n60 6.\n06 36\n8v 2\n[ cs\n.L G\n] 2\n3 Ju\nn 20\n16\nwith the training data. If all mappings M \u2208 C unanimously predict the same output on a test input, then we return that output; else we return \u201cdon\u2019t know\u201d (see Figure 1). The unanimity principle provides robustness to the particular input distribution, so that we can tolerate even adversaries (Mei and Zhu, 2015), provided the training outputs are still mostly correct.\nTo operationalize the unanimity principle, we need to be able to efficiently reason about the predictions of all consistent mappings C. To this end, we represent a mapping as a matrixM , whereMst is number of times target atom t (e.g., OH) shows up for each occurrence of the source atom s (e.g., Ohio) in the input. We show that unanimous prediction can be performed by solving two integer linear programs. With a linear programming relaxation (Section 3), we further show that checking unanimity over C can be done very efficiently without any optimization but rather by checking the predictions of just two random mappings, while still guaranteeing 100% precision with probability 1 (Section 3.2).\nWe further relax the linear program to a linear system, which gives us a geometric view of the unanimity: We predict on a new input if it can be expressed as a \u201clinear combination\u201d of the training inputs. As an example, suppose we are given training data consisting of (CI) cities in Iowa, (CO) cities in Ohio, and (AI) area of Iowa (Figure 1). We can compute (AO) area of Ohio by analogy: (AO) = (CO) - (CI) + (AI). Other reasoning patterns fall out from more complex linear combinations.\nWe can handle noisy data (Section 3.4) by asking for unanimity over additional slack variables. We also show how the linear algebraic formulation enables other extensions such as learning from denotations (Section 5.1), active learning (Section 5.2), and paraphrasing (Section 5.3). We validate our methods in Section 4. On artificial data generated from an adversarial distribution with noise, we show that unanimous prediction obtains 100% precision, whereas point estimates fail. On GeoQuery (Zelle and Mooney, 1996), a standard semantic parsing dataset, where our model assumptions are violated, we still obtain 100% precision. We were able to reach 70% recall on recovering predicates and 59% on full logical forms."}, {"heading": "2 Setup", "text": "We represent an input x (e.g., area of Ohio) as a bag (multiset) of source atoms and an output y (e.g., area(OH)) as a bag of target atoms. In the simplest case, source atoms are words and target atoms are predicates\u2014see Figure 2(top) for an example.2 We assume there is a true mapping M\u2217 from a source atom s (e.g., Ohio) to a bag of target atoms t = M\u2217(s) (e.g., {OH}). Note that M\u2217 can also map a source atom s to no target atoms (M\u2217(of) = {}) or multiple target atoms (M\u2217(grandparent) = {parent,parent}). We extend M\u2217 to bag of source atoms via multiset sum: M\u2217(x) = ]s\u2208xM\u2217(s).\nOf course, we do not know M\u2217 and must estimate it from training data. Our training examples are input-output pairs D = {(x1, y1), . . . , (xn, yn)}. For now, we assume that there is no noise so that yi =M\u2217(xi); Section 3.4 shows how to deal with noise. Our goal is to output a mapping M\u0302 that maps each input x to either a bag of target atoms or \u201cdon\u2019t know.\u201d We say that M\u0302 has 100% precision if M\u0302(x) = M\u2217(x) whenever M\u0302(x) is not \u201cdon\u2019t know.\u201d The chief difficulty is that the source atoms xi and the target atoms yi are unaligned. While we could try to infer the alignment, we will show that it is unnecessary for obtaining 100% precision."}, {"heading": "2.1 Unanimity principle", "text": "LetM be the set of mappings (which contains the true mapping M\u2217). Let C be the subset of map-\n2Our semantic parsing experiments (Section 4.2) use more complex source and target atoms to capture some context and structure.\npings consistent with the training examples.\nC def= {M \u2208M |M(xi) = yi, \u2200i = 1, . . . , n} (1)\nFigure 2 shows the four mappings consistent with the training set in our running example. Let F be the set of safe inputs, those on which all mappings in C agree:\nF def= {x : |{M(x) :M \u2208 C}| = 1}. (2)\nThe unanimity principle defines a mapping M\u0302 that returns the unanimous output on F and \u201cdon\u2019t know\u201d on its complement. This choice obtains the following strong guarantee:\nProposition 1. For each safe input x \u2208 F , we have M\u0302(x) =M\u2217(x). In other words, M obtains 100% precision.\nFurthermore, M\u0302 obtains the best possible recall given this model family subject to 100% precision, since for any x 6\u2208 F there are at least two possible outputs generated by consistent mappings, so we cannot safely guess one of them."}, {"heading": "3 Linear algebraic formulation", "text": "To solve the learning problem laid out in the previous section, let us recast the problem in linear algebraic terms. Let ns (nt) be the number of source (target) atom types. First, we can represent the bag x (y) as a ns-dimensional (nt-dimensional) row vector of counts; for example, the vector\nform of \u201carea of Ohio\u201d is\nar ea\nof O hi\no\nci tie\ns\nin Io w\na\n[ ]1 1 1 0 0 0 .\nWe represent the mapping M as a non-negative integer-valued matrix, whereMst is the number of times target atom t appears in the bag that source atom s maps to (Figure 3). We also encode the n\ntraining examples as matrices: S is an n\u00d7 ns matrix where the i-th row is xi; T as an n\u00d7nt matrix where the i-th row is yi. Given these matrices, we can rewrite the set of consistent mappings (2) as:\nC = {M \u2208 Zns\u00d7nt\u22650 : SM = T}. (3)\nSee Figure 3 for the matrix formulation of S and T , along with one possible consistent mapping M for our running example."}, {"heading": "3.1 Integer linear programming", "text": "Finding an element of C as defined in (3) corresponds to solving an integer linear program (ILP), which is NP-hard in the worst case, though there exist relatively effective off-the-shelf solvers such as Gurobi. However, one solution is not enough. To check whether an input x is in the safe set F (2), we need to check whether all mappings M \u2208 C predict the same output on x; that is, xM is the same for all M \u2208 C.\nOur insight is that we can check whether x \u2208 F by solving just two ILPs. Recall that we want to know if the output vector xM can be different for different M \u2208 C. To do this, we pick a random vector v \u2208 Rnt , and consider the scalar projection xMv. The first ILP maximizes this scalar and the second one minimizes it. If both ILPs return the same value, then with probability 1, we can conclude that xM is the same for all mappingsM \u2208 C and thus x \u2208 F . The following proposition formalizes this:\nProposition 2. Let x be any input. Let v \u223c N (0, Int\u00d7nt) be a random vector. Let a = minM\u2208C xMv and b = maxM\u2208C xMv. With probability 1, a = b iff x \u2208 F .\nProof. If x \u2208 F , there is only one output xM , so a = b. If x 6\u2208 F , there exists two M1,M2 \u2208 C for which xM1 6= xM2. Then w def = x(M1 \u2212\nM2) \u2208 R1\u00d7nt is nonzero. The probability ofwv = 0 is zero because the space orthogonal to w is a (nt\u22121)-dimensional space while v is drawn from a nt-dimensional space. Therefore, with probability 1, xM1v 6= xM2v. Without loss of generality, a \u2264 xM1v < xM2v \u2264 b, so a 6= b."}, {"heading": "3.2 Linear programming", "text": "Proposition 2 requires solving two non-trivial ILPs per input at test time. A natural step is to relax the integer constraint so that we solve two LPs instead.\nCLP def = {M \u2208 Rns\u00d7nt\u22650 | SM = T} (4)\nFLP def = {x : |{M(x) :M \u2208 CLP}| = 1}. (5)\nThe set of consistent mappings is larger (CLP \u2287 C), so the set of safe inputs is smaller (FLP \u2286 F). Therefore, if we predict only on FLP, we still maintain 100% precision, although the recall could be lower.\nNow we will show how to exploit the convexity of CLP (unlike C) to avoid solving any LPs at test time at all. The basic idea is that if we choose two mappingsM1,M2 \u2208 CLP \u201crandomly enough\u201d, whether xM1 = xM2 is equivalent to unanimity over CLP. We could try to sample M1,M2 uniformly from CLP, but this is costly. We instead show that \u201cless random\u201d choice suffices. This is formalized as follows:\nProposition 3. Let X be a finite set of test inputs. Let d be the dimension of CLP. LetM1 be any mapping in CLP, and let vec(M2) be sampled from a\nproper density over a d-dimensional ball lying in CLP centered at vec(M1). Then, with probability 1, for all x \u2208 X , xM1 = xM2 implies x \u2208 FLP.\nProof. We will prove the contrapositive. If x 6\u2208 FLP, then xM is not the same for all M \u2208 CLP. Without loss of generality, assume not all M \u2208 CLP agree on the i-th component of xM . Note that (xM)i = tr(Meix), which is the inner product of vec(M) and vec(eix). Since (xM)i is not the same for all M \u2208 CLP and CLP is convex, the projection of CLP onto vec(eix) must be a one-dimensional polytope. For both vec(M1) and vec(M2) to have the same projection on vec(eix), they would have to both lie in a (d \u2212 1)-dimensional polytope orthogonal to vec(eix). Since vec(M2) is sampled from a proper density over a d-dimensional ball, this has probability 0.\nAlgorithm. We now provide an algorithm to find two points p1, p2 inside a general ddimensional polytope P = {p : Ap \u2264 b} satisfying the conditions of Proposition 3, where for clarity we have simplified the notation from vec(Mi) to pi and CLP to P .\nWe first find a point p1 in the relative interior of P , which consists of points for which the fewest number of inequalities j are active (i.e., ajp = bj). We can achieve this by solving the following LP from Freund et al. (1985):\nmax 1>\u03be s.t. Ap+ \u03be \u2264 \u03b1b, 0 \u2264 \u03be \u2264 1, \u03b1 \u2265 1. (6)\nHere, \u03bej is a lower bound on the slack of inequality j, and \u03b1 scales up the polytope so that all the \u03bej that can be positive are exactly 1 in the optimum solution. Importantly, if \u03bej = 0, constraint j is always active for all solutions p \u2208 P . Let (p\u2217, \u03be\u2217, \u03b1\u2217) be an optimal solution to the LP. Then define A1 as the submatrix of A containing rows j for which \u03be\u2217j = 1, and A0 consist of the remaining rows for which \u03be\u2217j = 0.\nThe above LP gives us p1 = p\u2217/\u03b1\u2217, which lies in the relative interior of P (see Figure 4). To obtain p2, define a radius R def = (\u03b1maxj:\u03be\u2217j=1 \u2016aj\u20162) \u22121. Let the columns of matrix N form an orthonormal basis of the null space of A0. Sample v from a unit d-dimensional ball centered at 0, and set p2 = p1 +RNv.\nTo show that p2 \u2208 P : First, p2 satisfies the always-active constraints j, a>j (p1 +RNv) = bj ,\nAlgorithm 1 Our linear programming approach. procedure TRAIN Input: Training examples Output: Generic mappings (M1,M2)\nDefine CLP as explained in (4). Compute M1 and a radius R by solving an LP (6). Sample M2 from a ball with radius R around M1. return (M1,M2)\nend procedure\nprocedure TEST Input: input x, mappings (M1,M2) Output: A guaranteed correct y or \u201cdon\u2019t know\u201d\nCompute y1 = xM1 and y2 = xM2. if y1 = y2 then return y1 else return \u201cdon\u2019t know\u201d end if\nend procedure\nby definition of null space. For non-active j, the LP ensures that a>j p1 + \u03b1\n\u22121 \u2264 bj , which implies a>j (p1 +RNv) \u2264 bj .\nAlgorithm 1 summarizes our overall procedure: At training time, we solve a single LP (6) and draw a random vector to obtain M1,M2 satisfying Proposition 3. At test time, we simply apply M1 and M2, which scales only linearly with the number of source atoms in the input."}, {"heading": "3.3 Linear system", "text": "To obtain additional intuition about the unanimity principle, let us relax CLP (4) further by removing the non-negativity constraint, which results in a linear system. Define the relaxed set of consistent mappings to be all the solutions to the linear system and the relaxed safe set accordingly:\nCLS def = {M \u2208 Rns\u00d7nt | SM = T} (7)\nFLS def = {x : |{M(x) :M \u2208 CLS}| = 1}. (8)\nNote that CLS is an affine subspace, so each M \u2208 CLS can be expressed as M0 + BA, where M0 is an arbitrary solution,B is a basis for the null space of S and A is an arbitrary matrix. Figure 5 presents the linear system for four training examples. In the rare case that S has full column rank (if we have many training examples), then the left inverse of S exists, and there is exactly one consistent mapping, the true one (M\u2217 = S\u2020T ), but we do not require this.\nLet\u2019s try to explore the linear algebraic structure in the problem. Intuitively, if we know area of Ohio maps to area(OH) and Ohio maps to OH, then we should conclude area of maps to area by subtracting the second example from the first. The following proposition formalizes and generalizes this intuition by characterizing the relaxed safe set:\nProposition 4. The vector x is in row space of S iff x \u2208 FLS.\nS\ufe37 \ufe38\ufe38 \ufe37\nar ea of O hi\no\nci tie\ns\nin Io w a[ ] area of Iowa 1 1 0 0 0 1 +1 cities in Ohio 0 0 1 1 1 0 +1 cities in Iowa 0 0 0 1 1 1 \u22121\n[ ]area of Ohio 1 1 1 0 0 0\nT\ufe37 \ufe38\ufe38 \ufe37\na r e a c i t y O H\nI A[ ]\narea(IA) 1 0 0 1 +1 city(OH) 0 1 1 0 +1 city(IA) 0 1 0 1 \u22121\n[ ]area(OH) 1 0 1 0\nFigure 6: Under the linear system relaxation, we can predict the target atoms for the new input area of Ohio by adding and subtracting training examples (rows of S and T ).\nProof. If x is in the row space of S, we can write x as a linear combination of S for some coefficients \u03b1 \u2208 Rn: x = \u03b1>S. Then for all M \u2208 CLS, we have SM = T , so xM = \u03b1>SM = \u03b1>T , which is the unique output3 (See Figure 6). If x \u2208 FLS is safe, then there exists a y such that for all M \u2208 CLS, xM = y. Recall that each element of CLS can be decomposed into M0 +BA. For x(M0 +BA) to be the same for each A, x should be orthogonal to each column of B, a basis for the null space of S. This means that x is in the row space of S.\nIntuitively, this proposition says that stitching new inputs together by adding and subtracting existing training examples (rows of S) gives you exactly the relaxed safe set FLS.\nNote that relaxations increases the set of consistent mappings (CLS \u2287 CLP \u2287 C), which has the contravariant effect of shrinking the safe set (FLS \u2286 FLP \u2286 F). Therefore, using the relaxation (predicting when x \u2208 FLS) still preserves 100% precision."}, {"heading": "3.4 Handling noise", "text": "So far, we have assumed that our training examples are noiseless, so that we can directly add the\n3 There might be more than one set of coefficients (\u03b11, \u03b12) for writing x. However, they result to a same output: \u03b1>1 S = \u03b1>2 S =\u21d2 \u03b1>1 SM = \u03b1>2 SM =\u21d2 \u03b1>1 T = \u03b1>2 T .\nconstraint SM = T . Now assume that an adversary has made at most nmistakes additions to and deletions of target atoms across the examples in T , but of course we do not know which examples have been tainted. Can we still guarantee 100% precision?\nThe answer is yes for the ILP formulation: we simply replace the exact match condition (SM = T ) with a weaker one: \u2016SM\u2212T\u20161 \u2264 nmistakes (*). The result is still an ILP, so the techniques from Section 3.1 readily apply. Note that as nmistakes increases, the set of candidate mappings grows, which means that the safe set shrinks.\nUnfortunately, this procedure is degenerate for linear programs. If the constraint (*) is not tight, thenM+E also satisfies the constraint for any matrix E of small enough norm. This means that the consistent mappings CLP will be full-dimensional and certainly not be unanimous on any input.\nAnother strategy is to remove examples from the dataset if they could be potentially noisy. For each training example i, we run the ILP (*) on all but the i-th example. If the i-th example is not in the resulting safe set (2), we remove it. This procedure produces a noiseless dataset, on which we can apply the noiseless linear program or linear system from the previous sections."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Artificial data", "text": "We generated a true mapping M\u2217 from 50 source atoms to 20 target atoms so that each source atom maps to 0\u20132 target atoms. We then created 120 training examples and 50 test examples, where the length of every input is between 5 and 10. The source atoms are divided into 10 clusters, and each input only contains source atoms from one cluster.\nFigure 7a shows the results for F (integer linear programming), FLP (linear programming), and\nFLS (linear system). All methods attain 100% precision, and as expected, relaxations lead to lower recall, though they all can reach 100% recall given enough data.\nComparison with point estimation. Recall that the unanimity principle M\u0302 reasons over the entire set of consistent mappings, which allows us to be robust to changes in the input distribution, e.g., from training set attacks (Mei and Zhu, 2015). As an alternative, consider computing the point estimateMp that minimizes \u2016SM\u2212T\u201622 (the solution is given by Mp = S\u2020T ). The point estimate, by minimizing the average loss, implicitly assumes i.i.d. examples. To generate output for input x we compute y = xMp and round each coordinate yt to the closest integer. To obtain a precision-recall tradeoff, we set a threshold and if for all target atoms t, the interval [yt\u2212 , yt+ ) contains an integer, we set yt to that integer; otherwise we report \u201cdon\u2019t know\u201d for input x.\nTo compare unanimous prediction M\u0302 and point estimation Mp, for each f \u2208 {0.2, 0.5, 0.7}, we randomly generate 100 subsampled datasets consisting of an f fraction of the training examples. For Mp, we sweep across {0.0, 0.1, . . . , 0.5} to obtain a ROC curve. In Figure 7c(left/right), we select the distribution that results in the maximum/minimum difference between F1(M\u0302) and F1(Mp) respectively. As shown, M\u0302 has always 100% precision, while Mp can obtain less 100% precision over its full ROC curve. An adversary can only hurt the recall of unanimous prediction.\nNoise. As stated in Section 3.4, our algorithm has the ability to guarantee 100% precision even when the adversary can modify the outputs. As we increase the number of predicate additions/deletions (nmistakes), Figure 7b shows that precision remains at 100%, while recall naturally decreases in response to being less confident about\nthe training outputs."}, {"heading": "4.2 Semantic parsing on GeoQuery", "text": "We now evaluate our approach on the standard GeoQuery dataset (Zelle and Mooney, 1996), which contains 880 utterances and their corresponding logical forms. The utterances are questions related to the US geography, such as: \u201cwhat river runs through the most states\u201d.\nWe use the standard 600/280 train/test split (Zettlemoyer and Collins, 2005). After replacing entity names by their types4 based on the standard entity lexicon, there are 172 different words and 57 different predicates in this dataset.\nHandling context. Some words are polysemous in that they map to two predicates: in \u201clargest river\u201d and \u201clargest city\u201d, the word largest maps to longest and biggest, respectively. Therefore, instead of using words as source atoms, we\n4If an entity name has more than one type we replace it by concatenating all of its possible types.\nuse bigrams, so that each source atom always maps to the same target atoms.\nReconstructing the logical form. We define target atoms to include more information than just the predicates, which enables us to reconstruct logical forms from the predicates. We use the variable-free functional logical forms (Kate et al., 2005), in which each target atom is a predicate conjoined with its argument order (e.g., loc 1 or loc 2). Table 1 shows two different choices of target atoms. At test time, we search over all possible \u201ccompatible\u201d ways of combining target atoms into logical forms. If there is exactly one, then we return that logical form and abstain otherwise. We call a predicate combination \u201ccompatible\u201d if it appears in the training set.\nWe put a \u201cnull\u201d word at the end of each sentence, and collapsed the loc and traverse predicates. To deal with noise, we minimized \u2016SM \u2212 T\u20161 over real-valued mappings and removed any example (row) with non-zero residual. We perform all experiments using the linear system relaxation. Training takes under 30 seconds.\nFigure 8 shows precision and recall as a function of the number of the training examples. We obtain 70% recall over predicates on the test examples. 84% of these have a unique compatible way of combining target atoms into a logical form, which results in a 59% recall on logical forms.\nThough our modeling assumptions are incorrect for real data, we were still able to get 100% precision for all training examples. Interestingly, the linear system (which allows negative mappings) helps model GeoQuery dataset better than the linear program (which has a non-negativity constraint). There exists a predicate all:e in GeoQuery that is in every sentence unless the ut-\nterance contains a proper noun. With negative mappings, null maps to all:e, while each proper noun maps to its proper predicate minus all:e.\nThere is a lot of work in semantic parsing that tackles the GeoQuery dataset (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011), and the state-of-the-art is 91.1% precision and recall (Liang et al., 2011). However, none of these methods can guarantee 100% precision, and they perform more feature engineering, so these numbers are not quite comparable. In practice, one could use our unanimous prediction approach in conjunction with others: For example, one could run a classic semantic parser and simply certify 59% of the examples to be correct with our approach. In critical applications, one could use our approach as a first-pass filter, and fall back to humans for the abstentions."}, {"heading": "5 Extensions", "text": ""}, {"heading": "5.1 Learning from denotations", "text": "Up until now, we have assumed that we have input-output pairs. For semantic parsing, this means annotating sentences with logical forms (e.g., area of Ohio to area(OH)) which is very expensive. This has motivated previous work to learn from question-answer pairs (e.g., area of Ohio to 44825) (Liang et al., 2011). This provides weaker supervision: For example, 44825 is the area of Ohio (in squared miles), but it is also the zip code of Chatfield. So, the true output could be either area(OH) or zipcode(Chatfield).\nIn this section, we show how to handle this form of weak supervision by asking for unanimity over additional selection variables. Formally, we have D = {(x1, Y1), . . . , (xn, Yn)} as a set of training examples, here each Yi consists of ki candidate outputs for xi. In this case, the unknowns are the mappingM as before along with a selection vector \u03c0i, which specifies which of the ki outputs in Yi is equal to xiM . To implement the unanimity prin-\nciple, we need to consider the set of all consistent solutions (M,\u03c0).\nWe construct an integer linear program as follows: Each training example adds a constraint that the output of it should be exactly one of its candidate output. For the i-th example, we form a matrix Ti \u2208 Rki\u00d7nt with all the ki candidate outputs. Formally we want xiM = \u03c0iTi. The entire ILP is:\n\u2200i, xiM = \u03c0iTi \u2200i, \u2211 j \u03c0ij = 1\n\u03c0,M \u2265 0 Given a new input x, we return the same output if xM is same for all consistent solutions (M,\u03c0). Note that we can effectively \u201cmarginalize out\u201d \u03c0. We can also relax this ILP into an linear program following Section 3.2."}, {"heading": "5.2 Active learning", "text": "A side benefit of the linear system relaxation (Section 3.3) is that it suggests an active learning procedure. The setting is that we are given a set of inputs (the matrix S), and we want to (adaptively) choose which inputs (rows of S) to obtain the output (corresponding row of T ) for.\nProposition 4 states that under the linear system formulation, the set of safe inputs FLS is exactly the same as the row space of S. Therefore, if we ask for an input that is already in the row space of S, this will not affect FLS at all. The algo-\nrithm is then simple: go through our training inputs x1, . . . , xn one by one and ask for the output only if it is not in the row space of the previouslyadded inputs x1, . . . , xi\u22121.\nFigure 9 shows the recall when we choose examples to be linearly independent in this way in comparison to when we choose examples randomly. The active learning scheme requires half as many labeled examples as the passive scheme to reach the same recall. In general, it takes rank(S) \u2264 n examples to obtain the same recall as having labeled all n examples. Of course, the precision of both systems is 100%."}, {"heading": "5.3 Paraphrasing", "text": "Another side benefit of the linear system relaxation (Section 3.3) is that we can easily partition the safe set FLS (8) into subsets of utterances which are paraphrases of each other. Two utterances are paraphrase of each other if both map to the same logical form, e.g., \u201cTexas\u2019s capital\u201d and \u201ccapital of Texas\u201d. Given a sentence x \u2208 FLS, our goal is to find all of its paraphrases in FLS.\nAs explained in Section 3.3, we can represent each input x as a linear combination of S for some coefficients \u03b1 \u2208 Rn: x = \u03b1>S. We want to find all x\u2032 \u2208 FLS such that x\u2032 is guaranteed to map to the same output as x. We can represent x\u2032 = \u03b2>S for some coefficients \u03b2 \u2208 Rn. The outputs for x and x\u2032 are thus \u03b1>T and \u03b2>T , respectively. Thus we are interested in \u03b2\u2019s such that \u03b1>T = \u03b2>T , or in other words, \u03b1 \u2212 \u03b2 is in the null space of T>. Let B be a basis for the null space of T>. We can then write \u03b1\u2212 \u03b2 = Bv for some v. Therefore, the set of paraphrases of x \u2208 FLS are:\nParaphrases(x) def= {(\u03b1\u2212Bv)>S : v \u2208 Rn}. (9)"}, {"heading": "6 Discussion and related work", "text": "Our work is motivated by the semantic parsing task (though it can be applied to any set-to-set prediction task). Over the last decade, there has been much work on semantic parsing, mostly focusing on learning from weaker supervision (Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al., 2013; Pasupat and Liang, 2015), and applying semantic parsing to other tasks (Matuszek et al., 2012; Kushman and Barzilay, 2013; Artzi and Zettlemoyer, 2013). How-\never, only Popescu et al. (2003) focuses on precision. They also obtain 100% precision, but with a hand-crafted system, whereas we learn a semantic mapping.\nThe idea of computing consistent hypotheses appears in the classic theory of version spaces for binary classification (Mitchell, 1977) and has been extended to more structured settings (Vanlehn and Ball, 1987; Lau et al., 2000). Our version space is used in the context of the unanimity principle, and we explore a novel linear algebraic structure. Our \u201csafe set\u201d of inputs appears in the literature as the complement of the disagreement region (Hanneke, 2007). They use this notion for active learning, whereas we use it to support unanimous prediction.\nThere is classic work on learning classifiers that can abstain (Chow, 1970; Tortorella, 2000; Balsubramani, 2016). This work, however, focuses on the classification setting, whereas we considered more structured output settings (e.g., for semantic parsing). Another difference is that we operate in a more adversarial setting by leaning on the unanimity principle.\nAnother avenue for providing user confidence is probabilistic calibration (Platt, 1999), which has been explored more recently for structured prediction (Kuleshov and Liang, 2015). However, these methods do not guarantee precision for any training set and test input.\nIn summary, we have presented the unanimity principle for guaranteeing 100% precision. For the task of learning semantic mappings, we leveraged the linear algebraic structure in our problem to make unanimous prediction efficient. We view our work as a first step in learning reliable semantic parsers. A natural next step is to explore our framework with additional modeling improvements\u2014especially in dealing with context, structure, and noise.\nReproducibility. All code, data, and experiments for this paper are available on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0x593676a278fc4e5abe2d8bac1e3df486/.\nAcknowledgments. We would like to thank the anonymous reviewers for their helpful comments. We are also grateful for a Future Of Life Research Award and NSF grant CCF-1138967, which supported this work."}], "references": [{"title": "Bootstrapping semantic parsers from conversations", "author": ["Artzi", "Zettlemoyer2011] Y. Artzi", "L. Zettlemoyer"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Artzi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2011}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics (TACL), 1:49\u201362", "author": ["Artzi", "Zettlemoyer2013] Y. Artzi", "L. Zettlemoyer"], "venue": null, "citeRegEx": "Artzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2013}, {"title": "Learning to abstain from binary prediction. arXiv preprint arXiv:1602.08151", "author": ["A. Balsubramani"], "venue": null, "citeRegEx": "Balsubramani.,? \\Q2016\\E", "shortCiteRegEx": "Balsubramani.", "year": 2016}, {"title": "Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Berant et al.2013] J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Largescale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL)", "author": ["Cai", "Yates2013] Q. Cai", "A. Yates"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "On optimum recognition error and reject tradeoff", "author": ["C.K. Chow"], "venue": "IEEE Transactions on Information", "citeRegEx": "Chow.,? \\Q1970\\E", "shortCiteRegEx": "Chow.", "year": 1970}, {"title": "Identifying the set of always-active constraints in a system of linear inequalities by a single linear program", "author": ["R.M. Freund", "R. Roundy", "M.J. Todd"], "venue": null, "citeRegEx": "Freund et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1985}, {"title": "Confidence driven unsupervised semantic parsing", "author": ["R. Reichart", "J. Clarke", "D. Roth"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Goldwasser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwasser et al\\.", "year": 2011}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Hanneke.,? \\Q2007\\E", "shortCiteRegEx": "Hanneke.", "year": 2007}, {"title": "Learning to transform natural to formal languages", "author": ["Kate et al.2005] R.J. Kate", "Y.W. Wong", "R.J. Mooney"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Kate et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kate et al\\.", "year": 2005}, {"title": "Calibrated structured prediction", "author": ["Kuleshov", "Liang2015] V. Kuleshov", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Kuleshov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kuleshov et al\\.", "year": 2015}, {"title": "Using semantic unification to generate regular expressions from natural language", "author": ["Kushman", "Barzilay2013] N. Kushman", "R. Barzilay"], "venue": "In Human Language Technology and North American Association", "citeRegEx": "Kushman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kushman et al\\.", "year": 2013}, {"title": "Inducing probabilistic CCG grammars from logical form with higher-order unification", "author": ["L. Zettlemoyer", "S. Goldwater", "M. Steedman"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2010}, {"title": "Version space algebra and its application to programming by demonstration", "author": ["T.A. Lau", "P. Domingos", "D.S. Weld"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Lau et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lau et al\\.", "year": 2000}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang et al.2011] P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["Matuszek et al.2012] C. Matuszek", "N. FitzGerald", "L. Zettlemoyer", "L. Bo", "D. Fox"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Matuszek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners. In Association for the Advancement of Artificial Intelligence (AAAI)", "author": ["Mei", "Zhu2015] S. Mei", "X. Zhu"], "venue": null, "citeRegEx": "Mei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "Version spaces: A candidate elimination approach to rule learning", "author": ["T.M. Mitchell"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Mitchell.,? \\Q1977\\E", "shortCiteRegEx": "Mitchell.", "year": 1977}, {"title": "Misleading learners: Co-opting your spam filter", "author": ["Nelson et al.2009] B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I. Rubinstein", "U. Saini", "C. Sutton", "J. Tygar", "K. Xia"], "venue": "In Machine learning in cyber trust,", "citeRegEx": "Nelson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nelson et al\\.", "year": 2009}, {"title": "Compositional semantic parsing on semistructured tables. In Association for Computational Linguistics (ACL)", "author": ["Pasupat", "Liang2015] P. Pasupat", "P. Liang"], "venue": null, "citeRegEx": "Pasupat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pasupat et al\\.", "year": 2015}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Platt.,? \\Q1999\\E", "shortCiteRegEx": "Platt.", "year": 1999}, {"title": "Towards a theory of natural language interfaces to databases", "author": ["Popescu et al.2003] A. Popescu", "O. Etzioni", "H. Kautz"], "venue": "In International Conference on Intelligent User Interfaces (IUI),", "citeRegEx": "Popescu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Popescu et al\\.", "year": 2003}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Shimodaira.,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira.", "year": 2000}, {"title": "An optimal reject rule for binary classifiers", "author": ["F. Tortorella"], "venue": "In Advances in Pattern Recognition,", "citeRegEx": "Tortorella.,? \\Q2000\\E", "shortCiteRegEx": "Tortorella.", "year": 2000}, {"title": "A version space approach to learning context-free grammars", "author": ["Vanlehn", "Ball1987] K. Vanlehn", "W. Ball"], "venue": "Machine learning,", "citeRegEx": "Vanlehn et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Vanlehn et al\\.", "year": 1987}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Wong", "Mooney2007] Y.W. Wong", "R.J. Mooney"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Wong et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2007}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["Zelle", "Mooney1996] M. Zelle", "R.J. Mooney"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Zelle et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Zelle et al\\.", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Zettlemoyer", "Collins2005] L.S. Zettlemoyer", "M. Collins"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 22, "context": "It is particularly challenging since training inputs might not be representative of test inputs due to limited data, covariate shift (Shimodaira, 2000), or adversarial filtering (Nelson et al.", "startOffset": 133, "endOffset": 151}, {"referenceID": 18, "context": "It is particularly challenging since training inputs might not be representative of test inputs due to limited data, covariate shift (Shimodaira, 2000), or adversarial filtering (Nelson et al., 2009; Mei and Zhu, 2015).", "startOffset": 178, "endOffset": 218}, {"referenceID": 6, "context": "We can achieve this by solving the following LP from Freund et al. (1985):", "startOffset": 53, "endOffset": 74}, {"referenceID": 9, "context": "We use the variable-free functional logical forms (Kate et al., 2005), in which each target atom is a predicate conjoined with its argument order (e.", "startOffset": 50, "endOffset": 69}, {"referenceID": 12, "context": "There is a lot of work in semantic parsing that tackles the GeoQuery dataset (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011), and the state-of-the-art is 91.", "startOffset": 77, "endOffset": 201}, {"referenceID": 14, "context": "There is a lot of work in semantic parsing that tackles the GeoQuery dataset (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011), and the state-of-the-art is 91.", "startOffset": 77, "endOffset": 201}, {"referenceID": 14, "context": "1% precision and recall (Liang et al., 2011).", "startOffset": 24, "endOffset": 44}, {"referenceID": 14, "context": ", area of Ohio to 44825) (Liang et al., 2011).", "startOffset": 25, "endOffset": 45}, {"referenceID": 14, "context": "Over the last decade, there has been much work on semantic parsing, mostly focusing on learning from weaker supervision (Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al.", "startOffset": 120, "endOffset": 223}, {"referenceID": 7, "context": "Over the last decade, there has been much work on semantic parsing, mostly focusing on learning from weaker supervision (Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al.", "startOffset": 120, "endOffset": 223}, {"referenceID": 3, "context": ", 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al., 2013; Pasupat and Liang, 2015), and applying semantic parsing to other tasks (Matuszek et al.", "startOffset": 101, "endOffset": 168}, {"referenceID": 15, "context": ", 2013; Pasupat and Liang, 2015), and applying semantic parsing to other tasks (Matuszek et al., 2012; Kushman and Barzilay, 2013; Artzi and Zettlemoyer, 2013).", "startOffset": 79, "endOffset": 159}, {"referenceID": 3, "context": ", 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al., 2013; Pasupat and Liang, 2015), and applying semantic parsing to other tasks (Matuszek et al., 2012; Kushman and Barzilay, 2013; Artzi and Zettlemoyer, 2013). However, only Popescu et al. (2003) focuses on precision.", "startOffset": 123, "endOffset": 333}, {"referenceID": 17, "context": "The idea of computing consistent hypotheses appears in the classic theory of version spaces for binary classification (Mitchell, 1977) and has been extended to more structured settings (Vanlehn and Ball, 1987; Lau et al.", "startOffset": 118, "endOffset": 134}, {"referenceID": 13, "context": "The idea of computing consistent hypotheses appears in the classic theory of version spaces for binary classification (Mitchell, 1977) and has been extended to more structured settings (Vanlehn and Ball, 1987; Lau et al., 2000).", "startOffset": 185, "endOffset": 227}, {"referenceID": 8, "context": "Our \u201csafe set\u201d of inputs appears in the literature as the complement of the disagreement region (Hanneke, 2007).", "startOffset": 96, "endOffset": 111}, {"referenceID": 5, "context": "There is classic work on learning classifiers that can abstain (Chow, 1970; Tortorella, 2000; Balsubramani, 2016).", "startOffset": 63, "endOffset": 113}, {"referenceID": 23, "context": "There is classic work on learning classifiers that can abstain (Chow, 1970; Tortorella, 2000; Balsubramani, 2016).", "startOffset": 63, "endOffset": 113}, {"referenceID": 2, "context": "There is classic work on learning classifiers that can abstain (Chow, 1970; Tortorella, 2000; Balsubramani, 2016).", "startOffset": 63, "endOffset": 113}, {"referenceID": 20, "context": "Another avenue for providing user confidence is probabilistic calibration (Platt, 1999), which has been explored more recently for structured prediction (Kuleshov and Liang, 2015).", "startOffset": 74, "endOffset": 87}], "year": 2016, "abstractText": "Can we train a system that, on any new input, either says \u201cdon\u2019t know\u201d or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is wellspecified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.", "creator": "LaTeX with hyperref package"}}}