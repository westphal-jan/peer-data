{"id": "1606.07149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "An Approach to Stable Gradient Descent Adaptation of Higher-Order Neural Units", "abstract": "Stability alleviate evaluation derngate of a crianlarich weight - update 800-billion system of whiffle higher - radif order neural banaz units (rispoli HONUs) anar with uniting polynomial aggregation na\u0163ional of neural kalib inputs (also known citadels as alumina classes jailbroken of 37-36 polynomial neural networks) for adaptation neagle of pedaled both feedforward and wintringham recurrent pre-1989 HONUs kadet by fashir a gradient kilgen descent method floatplanes is ildar introduced. An escrowed essential core of brickle the gruda approach is based on spectral push-button radius gewurztraminers of 104.64 a syed weight - bomoh update neugebauer system, and it allows stability monitoring yevgen and askew its charisteas maintenance d.k.p. at every tubman adaptation karradah step individually. aint Assuring stability subhead of paladino the weight - decimating update leitz system (at cornacchia every single adaptation step) naturally results car in blabbing adaptation stability god\u00f3 of seller the whole 60.6 neural afpentertainment architecture reniban that venz adapts serbia to 1607 target haste data. As an 56-47 aside, the stuntman used senior-level approach highlights the begrudges fact verone that ifereimi the weight apnewsnow optimization of HONU is kiemas a sina linear problem, so barno the futch proposed approach samaroo can be generally bdd extended to kosten any neural wathan architecture .8 that is linear in stobbe its cavernosa adaptable 1984/85 parameters.", "histories": [["v1", "Thu, 23 Jun 2016 01:07:27 GMT  (807kb)", "http://arxiv.org/abs/1606.07149v1", "2016, 13 pages"]], "COMMENTS": "2016, 13 pages", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CE cs.LG cs.SY", "authors": ["ivo bukovsky", "noriyasu homma"], "accepted": false, "id": "1606.07149"}, "pdf": {"name": "1606.07149.pdf", "metadata": {"source": "CRF", "title": "TNNLS-2015-P-4534.R2 1 Abstract\u2014 Stability evaluation of a weight-update system of higher-order neural units (HONUs) with polynomial aggregation of neural inputs (also known as classes of polynomial neural networks) for adaptation of both feedforward and recurrent HONUs by a gradient descent method is introduced. An essential core of the approach is based on spectral radius of a weight-update", "authors": [], "emails": ["(Ivo.Bukovsky@fs.cvut.cz).", "(homma@ieee.org)."], "sections": [{"heading": null, "text": "higher-order neural units (HONUs) with polynomial aggregation of neural inputs (also known as classes of polynomial neural networks) for adaptation of both feedforward and recurrent HONUs by a gradient descent method is introduced. An essential core of the approach is based on spectral radius of a weight-update system, and it allows stability monitoring and its maintenance at every adaptation step individually. Assuring stability of the weight-update system (at every single adaptation step) naturally results in adaptation stability of the whole neural architecture that adapts to target data. As an aside, the used approach highlights the fact that the weight optimization of HONU is a linear problem, so the proposed approach can be generally extended to any neural architecture that is linear in its adaptable parameters.\nIndex Terms\u2014 gradient descent, higher-order neural unit,\npolynomial neural network, spectral radius, stability 1\nI. INTRODUCTION\nIGHER-ORDER neural units (HONUs) with a polynomial weighting aggregation of neural inputs are known as a\nfundamental class of polynomial neural networks (PNNs). We may recall that polynomial feedforward neural networks \u201care attractive due to the reliable theoretical results for their universal approximation abilities according to the Weierstrass theorem [1] and for their generalization power measured by the Vapnik-Chervonenkis (VC) dimension [2]\u201d (cited from [3] with re-numbered references). Work [4] evaluated the computing ability of several types of HONNs by using pseudo-dimensions and VC dimensions [5] and higher-order neural networks (HONNs) were used as a universal approximator. Basically, both the PNNs and HONNs represent the same style of computation in artificial neural networks where neurons involve polynomials, or the neurons are polynomials themselves, or where synaptic connections between neurons involve\nManuscript received in February, 2015. This work was supported in parts by the specific research funding of Czech Technical University in Prague, by grants No. SGS15/189/OHK2/3T/12 and No. SGS12/177/OHK2/3T/12, and also by JSPS KAKENHI Grants No. 25293258 and No. 26540112.\nIvo Bukovsky is with CTU in Prague, Dpt. of Inst. and Control Eng., Adaptive Signal Processing & Informatics Computational Centre (ASPICC), FME, 166 07 Prague, Czech Republic (Ivo.Bukovsky@fs.cvut.cz).\nNoriyasu Homma is with the Dpt. of Radiological Imaging and Informatics, Tohoku University Graduate School of Medicine, and also with the Intelligent Biomedical System Engineering Lab., Graduate School of Biomedical Engineering, Tohoku University, 980-8575 Sendai, Japan, (homma@ieee.org).\npublications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13]. For more recent works and applications of HONN and PNN we can refer to [14] and the particular focus on a quadratic neural unit (QNU) using matrix notation with upper triangular weight matrix can be found in [15]\u2013[18] and [19]. Significant and most recent publications devoted to PNN concepts are the works [3], [20] [21] while most recent works that are framed within HONNs can be found in [22], where some modifications of HONU are introduced in order to cope with the curse of dimensionality of HONU for higher polynomial orders. Other interesting earlier-appearing neural network architectures are product neural units [23] and later logarithmic neural networks [24]. Another nonconventional neural units are continuous time-delay dynamic neural units and higher-order time-delay neural units that have adaptable time delays in neural synapses and in state feedbacks of individual neurons as introduced in [25]; a similar fuzzy-network oriented concept appeared in parallel also in [26]. Another work focusing on various types of neural transfer functions can be found in review [27]. The optimization of neural weights of conventional neural networks is a nonlinear problem, such as for layered networks with hidden neurons with sigmoid output functions. Then nonlinear approaches for stability evaluation that are based on Lyapunov approach or energetic approaches are commonly adopted. Mostly, those techniques are sophisticated and require significant and time demanding (thus costly) effort from users who are not true experts in the field of neural networks. On the other hand, some newer HONU models were proposed for effective computation, learning and configuration in [4] [18] [28]\u2013[34] but they may still suffer from or do not take care of the local minima problem.\nTo improve the learning of nonlinear adaptive models with gradient descent based learning, we propose a novel approach\nH\nCopyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nTNNLS-2015-P-4534.R2\n2\nefficient both in simple stability evaluation and in stability maintenance of GD adaptation and that principally avoids the local minima problems for a given training data set due to in-parameter-linearity of HONUs. The proposed approach recalls that the weight optimization of HONUs, nonlinear input-output mapping models, is a linear problem that theoretically implies the existence of only a unique (global) minimum.\nAs a minor contribution of this paper, the flattened representation of HONU using 1-D long-vector operations is shown, so the need for multidimensional arrays of weights for HONU is avoided. The introduced long-vector-operator approach also simplifies direct weight calculation of static HONU of an arbitrary polynomial order r by the least square method (LSM), i.e., by variations of Wiener-Hopf equation and points to its connotation to Levenberg-Marquardt (L-M) algorithm for HONU.\nThe main contribution of this paper is that the adaptation stability evaluation is based on the fact that optimization of weights of HONUs is a linear problem; thus, the evaluation of maximum eigenvalue (spectral radius) can be used to assess the stability of the neural weight system. Based on that principle, the nonlinear extension for stability monitoring and maintenance of static HONU as well as recurrent HONU is proposed. This is the novel approach to evaluation and maintenance of stability of GD adapted HONUs. In principle, the derived stability condition enables gradient adaptation of HONUs be stabilized via time-varying learning rates at every sampling moment. We also discuss the effect of data normalization (more precisely of scaling down the data magnitude), and we show the relationship of the scaling factor to the magnitude of learning rate (in respect to GD adaptation stability). Moreover, our achievements might bring novel research directions for HONU when considering the adaptive learning rate modifications of gradient descent as in [35]. In connection to that, adaptable learning rate modifications for HONU are recalled, and the proposed adaptation stability condition is discussed in connotation to them.\nThe paper is organized as follows. Subsection II.A introduces the flattening operator (a long-vector operator) approach for HONU, and thus it also reveals the linear optimization nature of HONU. Then, the operator approach is used to derive a stability condition of weight-update system of static HONUs in subsection II.B and of recurrent HONUs in II.C, II.D derives the relationship of data normalization with the change of learning rate for GD. Correspondingly, section III experimentally supports the theoretical derivations, and it also discusses possible extensions of adaptive-learning-rate principles of linear filters [35]\u2013[38] to HONU. The derived adaptation stability rule with adaptive multiple learning rates of static HONUs is demonstrated on the example of up to fifth polynomial order HONU for hyperchaotic Chua\u2019s time series prediction, and the rule of dynamic HONU is demonstrated on chaotic Mackey-Glass time series prediction. Also, subsection III.D computationally demonstrates the relationship of\ndecreasing magnitude of data with the decreasing of the learning rate. Basically, we adopt the following standard notation of variables: small caps as \u201cx\u201d for a scalar, bolt \u201cx\u201d as for a vector,\nand bolt capital \u201cX\u201d as for a matrix. Lower indexes as in \u201cxi\u201d or \u201cwi,j\u201c denotes the position within a vector or an array, and upper T is for the transposition. If a discrete time index is necessary to be shown, it comes as \u201ck\u201d in round brackets such as x(k), y denotes neural output, and yp is for a training target. The meaning of other symbols is given at their first appearance.\nII. INTRODUCED APPROACHES"}, {"heading": "A. Operators for HONU", "text": "We start with recalling static QNU [15]\u2013[17], [25] as a fundamental second-order HONU or as a fundamental class of PNN as follows\n( ) ( ) ( ) , 0 0\n, where 1 n n\nk k ki j i j\ni j i\ny x x w x\u22c5 \u22c5\n= =\n= =\u2211\u2211 , (1)\nwhere y is neural output, n is the number of neural inputs and w stands for neural weights. Let r denote a polynomial order of HONU. Adopting the matrix formulation of QNU from [15], the vector of neural inputs (for al HONU) and the weight array (for QNU) is as follows\n0\n( )1 ( )\n( )\n1\n, k k\nkn\nx\nx\nx =     =       x \u22ee\n0,0 0,1 0,\n1,1 1,\n,\n0\n0 0\nn\nn\nn n\nw w w\nw w\nw      =        W \u22ef \u22ef \u22ee \u22f1 \u22f1 \u22ee \u22ef , (2)\nwhere W is a weight matrix (2-D array for QNU) and x(k) is input vector at time k. Next we drop the time indexing (k) when\nunnecessary, so x=x(k) and xi=xi(k). Generally, for HONU of order r>2 (r=3 for cubic polynomial, r=4 for 4th order polynomial, \u2026), such that\n( ) , , 0 0 ...\n, where 1 n n n\ni j i j\ni j i\ny x x x w x\n= =\n= \u22c5 =\u2211\u2211 \u2211 \u2026 \u2026\u2026 \u22ef , (3)\nthe weight W is understood as a higher-dimensional array (3-D, 4-D,\u2026). In Section II, we derive formulation (13) that is a 1-D array alternative to (1) (3) that allows the gradient descent stability condition of HONU be effectively derived and that allows connotations to adaptive learning rates of linearly aggregated filters as summarized in [36] (Appendix K). Next, it will be useful to introduce long-vector operators row r () and col r () for any polynomial order r. As r=2 in case of QNU, the operators row r () and col r () work as follows\n2 2 T 2 2 0 0 1( ) ( ) [ ... ] r r i j nrow row x x x x x x = == =x x , (4)\n2 2 T 2 2 0 0 1 T ( ) ( ) [ ... ] r r i j ncol col x x x x x x = == =x x , (5) and, e.g., for r=3, the row3(x) would be a row vector as follows\n3( ) [{ } ; 0 , , ]rrow x x x i n j i n j ni j \u03ba\u03ba = = = = =x \u2026 \u2026 \u2026 , (6)\nwhere operator row r () generates a row vector of all r-order\nCopyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nTNNLS-2015-P-4534.R2\n3\ncorrelated elements of vector x, and regardless the dimensionality of vector x, it is apparent that\n( )( ) ( ) T r r col row x=x . (7)\nLetting N denote the total number of training input patterns, then an n\u00d7N matrix X consisting of all instances of the input vector x that was defined in (2) is defined as follows\n( 1) ( 2) ( )[ ]k k k N= = ==X x x x\u2026 , (8)\nand when applying the operators defined in (4) (5), X yields\n( ) ( ) ( ) ( )( 1) ( 2) ( )[ ],r r r rk k k Ncol col col col= = ==X x x x\u2026 (9)\n( )\n( )\n( )\n( )\n( )( )\n( 1)\nT( 2)\n( )\n,\nr k\nr kr r\nr k N\nrow\nrow row col\nrow\n=\n=\n=\n     \n= =        \nx\nx X X\nx\n\u22ee\n(10)\nwhere the row indexes of X stand for input variables (first row is a bias x0=1), while its column indexes correspond to the sample index k (i.e. discrete time index). In general, the application of the colr(X) or rowr(X) operators on the matrix of input patterns X means their application to individual input vectors x(k) of matrix X as shown in (10). Regarding the neural weights, we can benefit also from the long-column-vector operator or the long-row-vector one for multidimensional weight arrays of HONN (PNN). Notice that the weight matrix of QNU in (1) is a 2-dimensional array, and it would become an r-dimensional array of neural weights for an r-order neural unit. Therefore, we introduce another, yet compatible, functionality of operators col() and row(); this time it is the conversion of multidimensional arrays of neural weights into their long-column-vector or long-row-vector representation. Then for a weight matrix W, e.g. for QNU as in (2), the long-vector operators col() and row() work as follows\n( )\n( ) ( )( )\n0,0 0,1 , ,\nT\n[ ],i j n nrow w w w w\ncol row\n=\n=\nW\nW W\n\u2026 \u2026\n(11)\nFor clarity of further text, we drop the index of polynomial order r, and we also drop the use of round brackets in the operators, so the further notation will be simplified as follows\n( ) ( )\n( ) ( )\n( ) ( )\n, ,\n, ,\n, .\nr r\nr r\ncol row\ncol row\ncol row\n= =\n= =\n= =\ncolx x rowx x\ncolX X rowX X\ncolW W rowW W\n(12)\nThen in general, an individual output of HONU for any given order r can be calculated by vector multiplication with the use of the above introduced operators as follows\ny = \u22c5 = \u22c5rowx colW rowW colx , (13)\nwhere \u201c\u00b7\u201d stands for vector or matrix multiplication and the neural output can be calculated for all time instances by matrix multiplication as follows\nTor= \u22c5 = \u22c5y rowX colW y rowW colX , (14)\nwhere y is (N\u00d71) vector of neural outputs. Because we substitute y and rowX or colXT with measured training data, the\noptimization of weights in colW (or rowW) clearly represents a linear set of equations to be solved. Further, we recall the weight calculation for the above static HONU (14) using least squares with the introduced operators. Assume N input vectors in matrix X as defined in (8), where row indexes stand for input variables and column indexes stand for sampled input patterns. Let yp=[ yp(1), yp(2), \u2026, yp(N)]T denote the (N\u00d71) vector of targets for input patterns X. For general polynomial order r, we express the square error criteria Q between neural outputs and targets using (14) as follows\n( ) 2 ( ) ( )\n1\nT( ) ( )\nN\nk kp\nk\nQ y y\n=\n= \u2212\n= \u2212 \u22c5 \u2212 \u22c5\n\u2211\np py rowX colW y rowX colW\n. (15)\nTo calculate weights by the classical least square method (LSM), we solve the set of equations /Q\u2202 \u2202 =colW 0 , where 0 is\nzero vector with its length as the total number of weights, and that represents the set of equations in a simplified notation as\nT ( ) ( ) \u2202 \u2212 \u22c5\n\u2212 \u22c5 \u22c5 = \u2202\np\np\ny rowX colW y rowX colW 0\ncolW , (16)\n(16) can then be rewritten in a matrix way as\nT\nT\n( ) ( )\n.\n\u2212 \u22c5 \u22c5 \u2212 =\n\u2212 \u22c5 + \u22c5 \u22c5 =\np\np\ny rowW colX rowX\ny rowX rowW colX rowX 0 (17)\nThus we arrive to the LSM calculation of weights by a variation of the Wiener-Hopf equation for HONU and for arbitrarily polynomial order r in a long-row-vector form as\n( ) 1T \u2212= \u22c5 \u22c5 \u22c5prowW y rowX colX rowX , (18)\nor alternatively in a long-column-vector as\n( ) 1\u2212= \u22c5 \u22c5 \u22c5 pcolW colX rowX colX y . (19)\nThe above formulas for direct calculation of weights by least square method (LSM) imply the existence of a unique solution, i.e., a unique global minimum. Of course, to acquire and select the optimum training data with enough of nonlinearly independent training patterns that would result in correct calculation of weights by LSM is another issue. Then it is practical to notice that we may comfortably derive the Levenberg-Marquardt (L-M) weight-update algorithm for static HONU, e.g. in its simplest form, as follows\n1 1\n\u00b5\n\u2212  \n= \u22c5 + \u22c5 \u22c5 \u22c5    \u0394colW colX rowX I colX e , (20)\nwhere e is the standard vector (or matrix) of neural output errors, \u03bc is learning rate (smaller \u03bc results in finer weight updates), and rowX=colXT already represents the Jacobian matrix. For automated retraining techniques, it can help to estimate or to try to calculate the weights by LSM (18) or (19) and then to apply L-M algorithm. However, we focus on the stability of gradient descent learning for static and recurrent HONU further in this paper. In this subsection we defined two operators row() and col() for neural architectures with higher-order polynomial aggregation of neural inputs. The functionality of the operators slightly differs when applied to the\nCopyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nTNNLS-2015-P-4534.R2\n4\nvector of neural inputs or to the matrix (or multidimensional array) of neural weights. We then used these operators to derive the neural weights using the least square method and thus the existence of single (global) minimum of weight optimization of HONU and the clear relationship of the operators to L-M algorithm was shown. Next, we will use the introduced operators for HONU for evaluation and maintenance of stability of weight-update system at every gradient-descent adaptation step of both static as well as recurrent HONUs."}, {"heading": "B. Weight-Update Stability of Static HONU", "text": "The operator approach introduced above can be used for stability evaluation and stability maintenance of weight updates for both static HONU updated by the gradient descent and for recurrent HONU updated by its recurrent version also known as RTRL [39]. We derive the approach for stability evaluation for static HONU in this subsection first. The output of static HONU at discrete time samples k is given in (13). The weight-update system by fundamental gradient descent learning rule for update of all the weights of HONU at sampling time k may be given as\n( )( 1) ( )k k p y y y\u00b5+ \u2202\n= + \u22c5 \u2212 \u22c5 \u2202 colW colW colW , (21)\nwhere yp is the target, y is neural output, \u03bc is the learning rate (scalar), and\n0,0 0,1 ,\nT\nn n\ny y y y\nw w w\n \u2202 \u2202 \u2202 \u2202 =  \n\u2202 \u2202 \u2202 \u2202  colW \u2026 . (22)\nWhen the neural output y is expressed according to (13) and considering (4)(5)(11)(12), then the derivative of neural output with respect to a single general weight of QNU is as follows\n( ) i j\nij ij ij\ny x x\nw w w\n\u22c5\u2202\u2202 \u2202 = = \u22c5 = \u22c5\n\u2202 \u2202 \u2202\nrowx colW colW rowx , (23)\nthen the neural weight-update system for a weight of static HONU is as\n( )( 1) ( ) ( ) ,k k kij ij p i jw w y x x\u00b5+ = + \u22c5 \u2212 \u22c5 \u22c5 \u22c5rowx colW (24) and considering (11), a column weight update formula for all weights can be expressed as follows\n( )( 1) ( ) ( ) .k k kpy\u00b5+ = + \u22c5 \u2212 \u22c5 \u22c5colW colW rowx colW colx (25) To proceed further, we expand (25) with consideration of proper vector dimensionality as follows\n( )( 1) ( ) ( ) ( ) ( )\n( ) ( ) .\nk k kp\nk kp\nk k p\ny\ny\ny\n\u00b5\n\u00b5 \u00b5\n\u00b5 \u00b5\n+\n=\n= + \u22c5 \u22c5 \u2212 \u22c5\n= + \u22c5 \u22c5 \u2212 \u22c5 \u22c5 \u22c5\n\u2212 \u22c5 \u22c5 \u22c5 + \u22c5 \u22c5\ncolW colW colx rowx colW\ncolW colx colx rowx colW\ncolW colx rowx colW colx\n(26)\nThen we separate the parts of the weight-update rule as\n( )( 1) ( )k k py\u00b5 \u00b5+ = \u2212 \u22c5 \u22c5 \u22c5 + \u22c5 \u22c5colW I colx rowx colW colx (27)\nwhere I is (nw\u00d7nw) identity matrix, where nw is the total number of all weights (also the number of rows of colW(k)). Let\u2019s denote the long vector multiplication term as follows\n= \u22c5S colx rowx . (28)\nConsidering that colx=rowxT are external inputs and that yp is the training target, we clearly see from (27) that the stability\ncondition of the weight-update system of static HONU, as of a linear discrete-time system, is at each time k as follows ( ) 1\u03c1 \u00b5\u2212 \u22c5 \u2264I S . (29) where \u03c1(.) is spectral radius, and I is an identity matrix of diagonal length equal to the number of neural weights.\nTo improve the adaptation stability of static HONU, we can\nupdate the learning rate \u03bc and observe its impact on the spectral radius \u03c1(.). Naturally and instead of single \u03bc, we can introduce time varying individual learning rates for each weight via diagonal matrix M as\n( )( ) ( ) ( ) ( )0,0 0,1 ,k k k kn ndiag \u00b5 \u00b5 \u00b5= =M M \u2026 , (30) so the weight-update system becomes\n( )( 1) ( ) ( ) ( ) ,k k k k py+ = \u2212 \u22c5 \u22c5 + \u22c5 \u22c5colW \u0399 M S colW M colx (31)\nand the learning rates on diagonal of M are ordered accordingly such as the weights in colW or rowW. The stability of the weight-update system of static HONU at every adaptation step is then classically resulting from (31) as ( )( ) 1k\u03c1 \u2212 \u22c5 \u2264I \u039c S , (32) where S is defined in (28) and the time-indexed learning rate matrix M=M(k) indicates that we can stabilize the adaption via time varying learning rates, so (32) is a starting point for developing novel adaptive learning rate algorithms for HONU, e.g., starting with inspiration from works [35], [36] this time for HONU (and other nonlinear models that are linear in their parameters). Also, the condition (32) explains why the normalization of input data affects the gradient descent adaptation stability because high magnitude input data results in large S (defined in (28)) and that requires small learning rates to approach condition (32) (see II.D)."}, {"heading": "C. Weight-Update Stability of Recurrent HONU", "text": "Recurrent HONU feeds its step delayed neural output back to its input. The individual weight update of recurrent HONU by fundamental gradient descent (RTRL) can then be given using the above introduced operators and for any polynomial order as follows\n( )\n( 1) ( ), , , ,\n( ) ( ) ( ) ( )\n, ,\n,\nk ki j i j\nk ns k n k kp s\ni j\nw w\ny y\nw \u00b5\n+\n+ +\n= +\n\u2202 + \u22c5 \u2212 \u22c5 \u22c5\n\u2202 rowx colW\n\u2026 \u2026\n\u2026\n(33)\nwhere ns is the discrete prediction interval, and the individual derivatives of neural output are for recurrent HONU as follows\n( ) ( ) ( )\n( ) ( )\n, , ,\n, k n k ks k k\ni j i j i j\ny\nw w w +  \u2202 \u2202 \u2202  = +  \u2202 \u2202 \u2202  rowx colW colW rowx (34)\nwhere weight indexing is shown as if for QNU, and here ( ) ,k i jw\u2202 \u2202 \u2260rowx 0 (contrary to static HONU, see (23))\nbecause the neural input x of recurrent architecture is concatenated with delayed neural outputs, and it can be expressed for all derivatives of neural output in a long-column vector (considering (22) and (23)) as\n( ) ( ) ( ) ( ) k n ks k k y +\u2202 \u2202 \n= +  \u2202 \u2202 \nrowx colW colx\ncolW colW , (35)\nCopyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nTNNLS-2015-P-4534.R2\n5\nwhere when considering (4) for QNU\n2 2 0 0 1\n0,0 0,0 0,00,0\n2 2 0 0 1\n0,1 0,1 0,1 0,1\n2 2 0 0 1\n, , , ,\n( )\n( )\n( )\nn\nn\nn n n\nn n n n n n\nx x x x\nw w ww\nx x x x\nw w w w\nx x x x w\nw w w  \u2202 \u2202 \u2202\u2202      \u2202 \u2202 \u2202 \u2202      \u2202  \u2202 \u2202 \u2202  \u2202  \u2202= = = \u2202 \u2202 \u2202   \u2202          \u2202  \u2202 \u2202 \u2202   \u2202   \u2202 \u2202 \u2202   rowx rowx rowx J colW rowx \u2026 \u2026 \u22ee \u22ee \u2026 , (36)\nwhere J represents the recurrently calculated Jacobian matrix\nwith dimensions nw\u00d7nw, where nw is the total number of weights, which is also equal to the number of elements of rowx or colW. Let us denote J\u03b6,\u03b7 the element of Jacobian J in \u03b6th row and \u03b7th column. In case of QNU (r=2), J\u03b6,\u03b7 corresponds to partial derivative of qth element of vector rowx that correspond to the second-order polynomial correlation of ith and jth neural input x, and the neural output partial derivative by a single weight can be calculated as\n( )\n( ) ( ) ( ), : ,\nk ns k k ki j\ni j\ny x x\nw \u03b6\n+\u2202 = \u22c5 +\n\u2202 J colW , (37)\nwhere , :\u03b6J is the \u03b6 th row of Jacobian J that corresponds to the\nposition of weight wi,j in colW (and also rowW, see (11)) and it is evaluated as\n( ) 2 2\n0 10 , :\n, , , ,\n( ) ( )n\ni j i j i j i j\nx xx x\nw w w w \u03b6\n \u2202\u2202 \u2202\u2202  = =\n\u2202 \u2202 \u2202 \u2202  \nrowx J \u2026 , (38)\nwhere its each element is for QNU as follows\n( ) ( )\n( ) ( ),\n, ,\nk k k k i j i j x x J x x w w \u03c4 \u03c5 \u03b6 \u03b7 \u03c5 \u03c4+ \u2202 \u2202 = \u2202 \u2202 . (39)\nObviously in RTRL, ( ) / ,k i jx w\u03c4\u2202 \u2202 are calculated recurrently if\nx\u03c4 corresponds to the tapped delayed feedback of neural output,\nand ( ) / , 0k i jx w\u03c4\u2202 \u2202 = if x\u03c4 corresponds to the external input or to\na bias x0=1. According to (33)-(36) and with correct left-side matrix multiplication, we arrive to RTRL update rule for recurrent HONU of general polynomial order r that considers matrix dimensions for multiplications as\n( )\n( )\n( ) ( 1) ( ) ( ) ( )\n( ) ( ) ( )\nk ns k k n k kp s\nk k k np s\ny y\ny\n\u00b5\n\u00b5 \u00b5 \u00b5\n+ + +\n+\n\u2202 = \u22c5 \u22c5 \u2212 \u22c5\n\u2202\n= + \u22c5 \u2212 \u22c5 \u22c5 \u22c5 \u22c5+\ncolW rowx colW colW\nI R S colW colx\n(40)\nwhere the detailed derivation is shown in appendix and where\n( ) ( ) ( )k n k kp s= y +\u22c5 \u2212 \u22c5 \u22c5R J J colW rowx . (41)\nAgain we can introduce a diagonal matrix of learning rates M instead of a single \u03bc and separate the parts of the update recurrent system as follows\n( )( )( 1) ( ) ( ) ( ) .k k k k np sy+ += + \u22c5 \u2212 \u22c5 \u22c5 \u22c5+colW I M R S colW M colx (42) Then the stability condition for adaptation of recurrent HONU by RTRL technique via evaluation of spectral radius \u03c1() is as follows\n( )( )( ) 1k\u03c1 + \u22c5 \u2212 \u2264I M R S\n(43)\nwhere S is defined in (28) and the time indexing of the learning rate matrix M(k) indicates the time variability of individual learning rates. The condition (43) allows us to evaluate and maintain the stability of the update weight system of recurrent HONU at every sampling time k. Also, resetting Jacobian J (36) to zero may be occasionally considered and that results in eliminating term R. Note that when resetting the Jacobian to zero matrix J=0, the condition for stability of the weight-update system of recurrent HONU (43) yields the stability condition of static HONU (32), and thus the stability of neural weights becomes independent from the actual weights themselves. The proper investigation of the J-reset effect to learning of recurrent HONU and the very rigorous development and analysis of sophisticated techniques for adaptive learning rates, such as based on works [35]\u2013[38] exceeds the limits for this paper; however, the operator approach and the stability conditions of HONU allows us to propose most straightforward connotations to adaptive learning rates techniques for HONU and we introduce them in subsections III.B and III.C."}, {"heading": "D. Data Normalization vs. Learning Rate", "text": "The effect of normalization of learning rate can be viewed as the alternative to normalization of magnitude of training data. Let \u03b1 denote the scaling factor of input data as follows\n( ) 0 11 ; 0 T nx x x\u03b1 \u03b1 \u03b1 \u03b1= = \u22c5 \u22c5 >  x \u2026 .\n(44)\nFor the example of static QNU (HONU with r=2) it yields\n( ) ( )( 2)\n( ) ( ) 2 2 1\n( )\n[1 ]\nr\nsign i sign j i j n\nrow\nx x x x\n\u03b1 \u03b1\n\u03b1 \u03b1 \u03b1\n=\n+\n=\n= \u22c5 \u22c5 \u22c5 \u22c5\nx rowx\n\u2026 \u2026 (45)\nand for a general order of HONU\n( ) { }( ) ( ) ( )( ) 1r sign i sign j i jrow x x\u03b1 \u03b1 + + = \u22c5 \u22c5  x \u22ef \u22ef . (46)\nThen it is apparent from the stability condition of static HONU with involvement of scaling factor \u03b1 as\n( ) ( )( ) 1\u03c1 \u00b5 \u03b1 \u03b1\u2212 \u22c5 \u22c5 \u2264I colx rowx , (47) that variation of input data magnitudes and scaling data with \u03b1 can have up-to 2r-power stronger influence to adaptation stability than decreasing the learning rate, and it can be stated as\n2 ; where 0 1r\u03b1 \u00b5 \u00b5 \u03b1\u223c < \u2264 \u2264 . (48)\nExperimental comparison of scaling the learning rate versus\ntraining data is shown in section III.D."}, {"heading": "III. EXPERIMENTS AND EXTENSIONS", "text": ""}, {"heading": "A. Static HONU (r=3)", "text": "In this subsection, we present the results achieved with the proposed operator approach for weight calculation of static HONU by the least square method as derived in subsection II.A. The results support the existence of a unique minimum due to polynomial nonlinearity of HONU. As a benchmark we chose a variation of famous system from [40] as\nCopyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nTNNLS-2015-P-4534.R2\n6\n( ) 3\n( 1) ( ) 2( ) , 1\nk true\nk ktrue k\ntrue\ny y u\ny + = + + (49)\nwhere the training output patterns yp were obtained as the true values ytrue with additive measurement noise as\n( ) ( ) ( )k k kp truey y \u03b5= + , (50)\nwhere u and \u03b5 are white noise signals, independent of each other, with unit variances and zero means and signal to noise ratio was calculated commonly as\n2\n10 2\nE[ ] 10 log [ ]\nE[ ] true\ny SNR dB\n\u03b5 =\n  \u22c5  \n  . (51)\nTo use the static HONU as a one-step predictor of time series (49) with noise is a suitable task, and weights can be found directly by the least square method even with high noise training data of SNR = 4.83 [dB]. The HONU was trained for first 300 samples and tested on next 700 samples (Fig. 1). Mean absolute error (MAE) of neural output and true signal was 0.43 while the MAE of neural output and noisy training data was 1.691 demonstrating that even if the noise of training data was high, HONU (especially and naturally best for r=3) learns the governing laws, approximates the original signal, and tends to reject the noise as seen in Fig. 1.\nFig. 2 then shows the neural weights directly calculated by the least squares method, and the weights correspond to relevant polynomial terms containing y(k) and u(k) (eg. w3,3,3, see (49)), while terms with y(k-1) are suppressed by neural weights that resulted very near to zero (Fig. 2). The results demonstrated the functionality of the proposed operators for LSM weight calculation from the above subsection II.A of static HONU of polynomial order r=3. The good quality of nonlinear approximation and the capability of HONU to extract major governing laws from noised training data were also shown. In the next experimental part, we demonstrate the main contribution of this paper, i.e., the results of stability evaluation of weight updates by the gradient descent (GD) method for the recurrent HONU.\nThis subsection demonstrated good approximation capability of HONU and good extraction of the governing laws even by direct calculation with least squares method (18) or (19), i.e., variations of Wiener-Hopf equations,), which implies the principal existence of single (unique) minima of HONU for a given training data set because HONU are nonlinear models but they are linear in parameters. Next we draw extensions for adaptive learning rates of static and later of recurrent HONUs."}, {"heading": "B. Adaptive Learning Rate of Static HONU", "text": "In this subsection, we introduce the connotations of static HONUs to adaptive learning rate techniques that are well known in literature for adaptive filters with linear aggregation of neural inputs [35], [36], [41]\u2013[44].\nSingle Learning Rate\nThe in-parameter-linearity of HONUs allows us to draw a parallel between HONUs and linearly aggregated (FIR) filters for the learning rate adaptive techniques; i.e., by simple comparison of the long-vector operator notation of HONUs (13) with linearly aggregated FIR filters form (e.g.[36], p.279), we see that rowx(k) plays the role of x(k) and the colW(k) plays the role of w(k). With those substitutions, we can adapt the learning rate by the classical normalized least mean square (NLMS) algorithm [41], so the adaptive learning rate \u03b7 for static HONUs yields\n( ) 2\n( ) 2\nk\nk\n\u00b5 \u03b7\n\u03b5 = +rowx (52)\nwere \u03b5 is the regularization term for zero\u2013close input vector. Concluding our experience, we may recommend to use also the square of the Euclidean norm, so (52) yields\n( ) ( ) ( ) k k k\n\u00b5 \u03b7\n\u03b5\u22c5 = +rowx colx , (53)\nand that displayed improved stability and faster convergence in our experiments. The straight explanation for this is as follows. Contrary to (52), the squared-norm normalization in (48) more aggressively contributes to stability condition (29) by suppressing learning rates when the norm of neural inputs exceeds unit, i.e. ||rowx(k)||>1 that explains better stability (higher learning rate can be used) than with (52). On the other hand, the squared-norm in (48) is naturally more progressive for\nCopyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nTNNLS-2015-P-4534.R2\n7\naccelerating adaptation when ||.||<1 that explains faster convergence of GD when normalized with (53). If we introduce\n\u00b5= \u2212 \u22c5A I S according to (29), we can normalize the unique\nlearning rate \u00b5 using the Frobenius norm of A because it reflects the deviations of spectral radius from a unit and thus it reflects the weight-update stability, so (52) yields\n( ) 2\n2\nk \u00b5\n\u03b7 \u03b5 = +A . (54)\nPerformance comparison of GD with (52)\u2013(56) and other algorithms for static HONU is demonstrated in Fig. 3. Furthemore and similarly to substitution as made in (52) for static HONUs, we may also implement the Benveniste\u2019s learning rate updates based on [42], algorithm by Farhang and Ang [43], Mathews\u2019 algorithm [44], and generalized normalized gradient descent algorithm (GNGD) of Mandic [35] (as summarized in [36] (Appendix K). We recently showed these extensions for HONU and compared their performance for chaotic time series in [45]. They are recalled in Tab. 1.\nMultiple Learning Rates\nAs indicated in subsection II.B and as it also resulted from our experiments, it appeared more efficient when we used individual learning rates for each weight and normalize them individually. We propose the following algorithm. When we redefine A=(I-M(k)\u00b7S) according to (32) for multiple learning rates in M(k), we can contribute to the stability by normalization of individual learning rates in M(k) by Euclidean norm of corresponding rows in A, because individual learning rates in M(k) multiply only corresponding rows in S and thus they affect only the corresponding rows in A. Therefore the adaptive learning rate can be normalized as\n2\n2\n( )\n,:\nq kq\nq\n\u00b5 \u03b7\n\u03b5 = +A , (55)\nwhere q is a position index of a weight in colW and it also indexes the corresponding learning rate in diagonal matrix of learning rates M, and Aq,: is qth row of matrix A. Again, we may use the squared Euclidean norm of the rows, so (55) yields\n( )\n,: ,:( ) T\nq kq\nq q\n\u00b5 \u03b7\n\u03b5 = +A A , (56)\nAgain, the modification of normalization algorithm (56) performed faster convergence than (52)\u2013(55) (provided manually optimized \u03b5 that was easy to be found \u03b5=1). Again, the squared norm in (56) is more aggressive than the norm itself, and the individual learning rates are normalized so they equally contribute to stability. Practically, we have not found too significant difference in performance between (52)\u2013(56), because tuning of \u03b5 plays its role that we do not focus in this paper. Anyhow, we found the above adaptive learning rate techniques (52)\u2013(56) very useful for static HONU, and they are in clear connotation to the weight update stability (29). Yet it appeared in experiments that the normalization for static HONU by (56) is more efficient (faster and maintaining spectral radius\nclosest to 1) than the above mentioned options (52)\u2013(55). To us, it practically appeared that the normalizing approaches (52)\u2013(56) are for static HONU superior both in speed and in maintaining convergence for long adaptation runs (or for many epochs), while the gradient based learning rate adaptive techniques, adopted for HONU, tend to require an early stopping and that is a well-known issue.\nCopyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nTNNLS-2015-P-4534.R2\n8\nAn explanation can be that the normalizing approaches contribute to the derived stability conditions of weight updates (29) or (43), while the gradient adaptive learning rate techniques do not consider the stability maintenance so straightforwardly. The performance of the learning with adaptive learning rates (56) and showing the stability condition (32) for static HONUs of order r=2,3,4,5 for prediction of hyperchaotic Chua\u2019s time series [46]\u2013[48] is shown in Fig. 3."}, {"heading": "C. Recurrent HONU", "text": "In this part, we demonstrate the validity of the introduced\nweight-update stability condition of recurrent HONU (43).\nStability Monitoring\nLet us use recurrent QNU (HONU r=2) for a long-term prediction of the MacKey-Glass time series in chaotic mode [49], [50] that is given as\n( ) 1 10( ( )( ) ( ) )0.2 1 0.1t tt tx x x x\u03c4\u03c4 \u2212 \u2212= \u2212 \u22c5 + \u2212\u027a , (57)\nwhere \u03c4=17 and the time series was obtained with 1 sampling per second. Configuration of HONU as a nonlinear recurrent predictor was the prediction time ns=11 steps (seconds) and the input of HONU included bias x0=1 and 10 tapped delayed feedbacks and 7 most recently measured values. Fig. 4 shows the later epoch of stable adaptation of recurrent HONU being trained according to the gradient descent learning rule and using the operator approach as derived in subsection II.C. The adaptation in Fig. 4 was stable because of a sufficiently small learning rate, and the occasional violations of stability condition (43) in the bottom plot of Fig. 4 spontaneously diminished and have not resulted in instability of recurrent HONU. The example of unstable adaptation of recurrent HONU is given in Fig. 5 where the weight update becomes unstable before k=700 and which appears as oscillations of neural output (top plot in Fig. 5) and thus as oscillations of error (middle plot\nFig. 5). Importantly, the stability condition (43) (bottom plot in Fig. 5) became significantly violated before neural output oscillations appeared and this is well apparent from detail in Fig. 6. We can see in bottom plots of Fig. 5 and Fig. 6 that the weight system returns to stability again after k=717 (spectral radius returned very close to one, 1\u03c1 \u2248 (43)). This was\nmaintained by the simplest way that the proposed approach offers, i.e., if the spectral radius (43) exceeded a predefined threshold (here \u03c1>1.05), we decreased the learning rate (here we used a single learning rate for all weights 0.6\u00b5 \u00b5\u2190 \u22c5 ) and we\nreset the Jacobian to zeros and recurrently calculated gradients that was shown in (39). As mentioned already, the more advanced stability maintenance can be carried out by introducing individual learning rates (30)(32) for each weight and to optimize their magnitudes with respect to stability condition for static HONU according to (32) and with respect to stability condition for recurrent HONU according to (43) (exploring this deserves further research and exceeds this paper)."}, {"heading": "D. Data Normalization vs. Learning Rate", "text": "To computationally verify the derivation in subsection II.D, normally distributed zero-mean data of unit standard deviation were used as original input data into vector x of various length n for static QNU. The effect of scaling factor \u03b1 versus the one of the learning rate \u03bc on adaptation stability (32) and thus confirming the relationship (48) is demonstrated via Fig. 7 to Fig. 10. For data with larger variance of magnitude, Fig. 7\u2013Fig. 10 imply that adaptive \u03bc requires be adapted within much wider interval, approximately ( )1E 4 , 1\u00b5 \u2208 \u2212 , of values rather than\nwhen data are normalized (scaled down), approximately as ( )1E 1 , 1\u00b5 \u2208 \u2212 .\nCopyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nTNNLS-2015-P-4534.R2\n9\n0.0 0.1 0.2 0.3 0.4 0.5\n10^0\nn=2\nn=3\nn=4 n=30\n( )( ) )( vector scaled-re of lengths sfor variou)( xxSI \u03b1\u03b1\u03c1 \u2212\n\u03b1\n\u03c1\nFig. 7: Spectral radius \u03c1 of static QNU as the function of both the number of inputs n and the scaling\u2013down factor \u03b1. 10^-5 10^-4 10^-3 10^-2 10^-1 mu\n10^0\n10^1\nm u\nn=30\nn=2\n( )( ) for various lengths of ; 1, 1x\u03c1 \u00b5 \u03c3\u2212 \u22c5 = =I S x x x\n\u00b5\n\u03c1\nFig. 8: Spectral radius \u03c1 of static QNU as the function of both the number of inputs n and the learning rate \u03bc.\nCopyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nTNNLS-2015-P-4534.R2\n10"}, {"heading": "IV. DISCUSSION", "text": "Besides the good quality of nonlinear approximation, the first example in subsection III.A demonstrated that weights of static HONU can be calculated by the least mean square (LMS) approach using the introduced operators, and it also supports the fact of existence of a single minimum for weight optimization of HONU; the linear nature of the weight optimization task is apparent already from neural output equations of HONU (15) and linear problems have only a single solution. Although the weight optimization by LMS for the benchmark in subsection III.A was a suitable task for static HONU, it is known that adaptation by GD becomes nontrivial task for this benchmark because the weight-update system by GD becomes unstable and requires the control of magnitude of learning rate (several approaches, but not for HONU, to prevent instability and to improve convergence are known [35], [37], [38]).\nAs an aside, it was recalled that optimization of HONU features a unique minimum for weight optimization with a given training data that contains enough of linearly independent training patterns (equal to or more than the number of neural weights) and demonstrated on benchmark data. The proposed approach with HONU is helpful for many identification and control applications. Also the struggle with overfitting can be relieved from local minima issue introduced by the neural architecture itself and thus the effort to reach good generalization of nonlinear model can be focused primarily to\nproper data processing and to finding appropriate input configuration. We practically observed in [51], [52] that weight convergence of static HONU using L-M algorithm was very rapid and required a very few epoch in comparison to conventional multilayered perceptron networks. Moreover, HONUs that were trained from various initial weights had almost identical outputs compared to various instances of trained MLP networks whose outputs were different for the same input patterns when trained from different initial weights and for the same configurations [51], [52]. This can be reasoned in principle by the above recalled linear nature of the weight optimization of static HONU that implies the existence of a single minimum for a particular input configuration and for a given training data set, while conventional MLP suffers from local minima. The introduced operator approach and online weight-update stability evaluation of a gradient descent method is applicable to any neural architecture that is linear in its parameters if the neural output can be expressed as by (14) where colX or rowX may also include other nonlinear terms than the multiplicative ones as in case of HONU in this paper.\nFor the introduced adaptation stability of HONU, we also derived and experimentally showed in subsections II.D and III.D that scaling of the training data by a factor \u03b1 has up-to 2r-power stronger effect to adaptation stability (of rth polynomial HONU with up to 30 inputs) than the variation of its learning rate \u03bc.\nThe requirement for larger interval of \u03bc implies a possible need for its faster adaptation for un-normalized data, while the adaptation of \u03bc does not have to be so fast when data are normalized.\nAs regards the estimation of time complexity in sense of required computational operations per one sample time, the output of r-th order HONU with n inputs is calculated as vector multiplication \u22c5rowW colx , where both vectors have length\n( )w nn r= , and because each element of colx is made of r-th order polynomial terms, the computational complexity of\nHONU is ( )( ) ( )O O wnr r nr\u22c5 = \u22c5 . For the case of static HONU with the introduced weight update stability, the stability condition (32) requires the Frobenius norm calculation of w wn n\u00d7 matrix S. Thus, the weight update stability of static\nHONU (32) results in major time complexity of ( )2O wn . For the case of dynamical HONU, the weight update stability condition (43) requires computation of matrix R that involves matrix multiplication of two matrices each of w wn n\u00d7 elements (41), thus the time complexity estimation can be increased to\n( )3O wn . When a true spectral radius shall be calculated instead of a matrix norm, the time complexity of HONU with weight update stability would approximately increase up to\n( )3O wn for static HONU and to ( )4O wn for recurrent HONU. From the practical point of view and based on our\nCopyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nTNNLS-2015-P-4534.R2\n11\nobservations; however, it should be mentioned that HONUs can be found useful and efficient esp. for small network problems, i.e. for up to 30 to 50 inputs and many problems can be sufficiently solved with HONU of order 3r \u2264 . For such\nproblems, the time complexity of the introduced algorithm shall not be a practical issue with nowadays hardware."}, {"heading": "V. CONCLUSIONS", "text": "Using the introduced long-vector notation, the approach to the gradient descent adaptation stability of static and recurrent HONUs of a general polynomial order r was introduced via monitoring of the spectral radius of the weight-update systems at every adaptation step. In experiments, the method was verified as the adaptation instability was detected well before the prediction error divergence became visually clear. Due to in-parameter linearity of HONU, adaptive learning rate techniques for HONU were adopted as known from the linear adaptive filters, and the adaptation stability monitoring was applied to HONUs as well. Also, it was derived and experimentally shown that scaling-down of the training data by a factor \u03b1 takes up-to 2r-power stronger influence to adaptation stability rather than the decrease of the learning rate itself. This implies the importance of training data normalization, esp., for adaptive learning rate techniques.\nBy the presented approaches, HONUs are highlighted as neural architectures that offer adjustable strong nonlinear input-output mapping models with linear optimization nature (thus without local minima issues for a given training data set), and we propose a novel yet comprehensible approach toward stability of the gradient descent weight-update system that can be useful in prediction, control and system monitoring tasks.\nSPECIAL THANKS\nSpecial thanks belongs to Madan M. Gupta from the Intelligent System Research Laboratory of the University of Saskatchewan for founding and forming the author\u2019s collaborative research on HONU since 2003 and for long-term support and advices.\nThe authors also thank to Jiri F\u00fcrst from Dpt. of Technical Mathematics, Faculty of Mech. Eng. CTU in Prague, for consultations.\nSpecial thanks belongs to The Matsumae International Foundation (Tokyo, Japan) that funded the first author\u2019s cooperation with colleagues at Tohoku University in Japan in 2009, and this cooperation is still vitally continuing.\nThe authors would like to thank the anonymous reviewers and the Editor in Chief for their insightful and constructive remarks."}], "references": [{"title": "The Stone-Weierstrass theorem and its application to neural networks", "author": ["N.E. Cotter"], "venue": "IEEE Trans. Neural Netw., vol. 1, no. 4, pp. 290\u2013295, Dec. 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Quantifying Generalization in Linearly Weighted Neural Networks", "author": ["S.B. Holden", "M. Anthony"], "venue": "Complex Syst., vol. 8, pp. 8\u201391, 1993.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning polynomial feedforward neural networks by genetic programming and backpropagation", "author": ["N.Y. Nikolaev", "H. Iba"], "venue": "IEEE Trans. Neural Netw., vol. 14, no. 2, pp. 337\u2013350, Mar. 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Adaptive neural output feedback tracking control for a class of uncertain discrete-time nonlinear systems", "author": ["Y.-J. Liu", "C.L.P. Chen", "G.-X. Wen", "S. Tong"], "venue": "IEEE Trans. Neural Netw. Publ. IEEE Neural Netw. Counc., vol. 22, no. 7, pp. 1162\u20131167, Jul. 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "On the Complexity of Computing and Learning with Multiplicative Neural Networks", "author": ["M. Schmitt"], "venue": "Neural Comput., vol. 14, no. 2, pp. 241\u2013301, Feb. 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Polynomial Theory of Complex Systems", "author": ["A.G. Ivakhnenko"], "venue": "IEEE Trans. Syst. Man Cybern., vol. SMC-1, no. 4, pp. 364\u2013378, jen 1971.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1971}, {"title": "The pi-sigma network: an efficient higher-order neural network for pattern classification and function approximation", "author": ["Y. Shin", "J. Ghosh"], "venue": ", IJCNN-91-Seattle International Joint Conference on Neural Networks, 1991, 1991, vol. i, pp. 13\u201318 vol.1.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Correlations in high dimensional or asymmetric data sets: Hebbian neuronal processing", "author": ["W.R. Softky", "D.M. Kammen"], "venue": "Neural Netw., vol. 4, no. 3, pp. 337\u2013347, 1991.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning Higher Order Correlations", "author": ["J.G. Taylor", "S. Coombes"], "venue": "Neural Netw, vol. 6, no. 3, pp. 423\u2013427, Mar. 1993.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Conventional modeling of the multilayer perceptron using polynomial basis functions", "author": ["M.S. Chen", "M.T. Manry"], "venue": "IEEE Trans. Neural Netw. Publ. IEEE Neural Netw. Counc., vol. 4, no. 1, pp. 164\u2013166, 1993.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Pattern recognition properties of various feature spaces for higher order neural networks", "author": ["W.A.C. Schmidt", "J.P. Davis"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 15, no. 8, pp. 795\u2013801, Aug. 1993.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "High-order neural network structures for identification of dynamical systems", "author": ["E.B. Kosmatopoulos", "M.M. Polycarpou", "M.A. Christodoulou", "P.A. Ioannou"], "venue": "IEEE Trans. Neural Netw., vol. 6, no. 2, pp. 422\u2013431, Mar. 1995.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "A framework for improved training of Sigma-Pi networks", "author": ["M. Heywood", "P. Noakes"], "venue": "IEEE Trans. Neural Netw., vol. 6, no. 4, pp. 893\u2013903, Jul. 1995.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "On-line identification of nonlinear systems using Volterra polynomial basis function neural networks", "author": ["G.P. Liu", "V. Kadirkamanathan", "S.A. Billings"], "venue": "Neural Netw., vol. 11, no. 9, pp. 1645\u20131657, Dec. 1998.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Static and dynamic neural networks: from fundamentals to advanced theory", "author": ["M.M. Gupta"], "venue": "New York: Wiley,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Development of quadratic neural unit with applications to pattern classification", "author": ["S. Redlapalli", "M.M. Gupta", "K.-Y. Song"], "venue": "Fourth International Symposium on Uncertainty Modeling and Analysis, 2003. ISUMA 2003, 2003, pp. 141\u2013146.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Quadratic and cubic neural units for identification and fast state feedback control of unknown non-linear dynamic systems", "author": ["I. Bukovsky", "S. Redlapalli", "M.M. Gupta"], "venue": "2003, pp. 330\u2013334.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Correlative type higher-order neural units with applications", "author": ["M.M. Gupta"], "venue": "IEEE International Conference on Automation and Logistics, 2008. ICAL 2008, 2008, pp. 715\u2013718.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Foundation and Classification of Nonconventional Neural Units and Paradigm of Nonsynaptic Neural Interaction", "author": ["Ivo Bukovsky", "Jiri Bila", "Madan M. Gupta", "Zeng-Guang Hou", "Noriyasu Homma"], "venue": "Discoveries and Breakthroughs in Cognitive Informatics and Natural Intelligence, IGI Global, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive learning of polynomial networks genetic programming, backpropagation and Bayesian methods", "author": ["N.Y. Nikolaev", "H. Iba"], "venue": "New York: Springer,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Artificial Higher Order Neural Networks for Economics and Business", "author": ["Z. Ming"], "venue": "IGI Global,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Higher-Order Computational Model for Novel Neurons", "author": ["B.K. Tripathi"], "venue": "High Dimensional Neurocomputing, vol. 571, New Delhi: Springer India, 2015, pp. 79\u2013103.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Product Units: A Computationally Powerful and Biologically Plausible Extension to Backpropagation Networks", "author": ["R. Durbin", "D. Rumelhart"], "venue": "Neural Comput., vol. 1, no. 1, pp. 133\u2013142, Mar. 1989.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1989}, {"title": "Foundation and Classification of Nonconventional Neural Units and Paradigm of Nonsynaptic Neural Interaction", "author": ["Ivo Bukovsky", "Jiri Bila", "Madan M. Gupta", "Zeng-Guang Hou", "Noriyasu Homma"], "venue": "Discoveries and Breakthroughs in Cognitive Informatics and Natural Intelligence, IGI Global, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Time Delay Dynamic Fuzzy Networks for Time Series Prediction", "author": ["Y. Oysal"], "venue": "Computational Science \u2013 ICCS 2005, V. S. Sunderam, G. D. van Albada, P. M. A. Sloot, and J. J. Dongarra, Eds. Springer Berlin Heidelberg, 2005, pp. 775\u2013782.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Survey of Neural Transfer Functions", "author": ["W. Duch", "N. Jankowski"], "venue": "Neural Comput. Surv., vol. 2, pp. 163\u2013213, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Time series prediction with single multiplicative neuron model", "author": ["R.N. Yadav", "P.K. Kalra", "J. John"], "venue": "Appl. Soft Comput., vol. 7, no. 4, pp. 1157\u20131163, Aug. 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "A New Higher-order Binary-input Neural Unit: Learning and Generalizing Effectively via Using Minimal Number of Monomials", "author": ["E. Sahin"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Low-Complexity Nonlinear Adaptive Filter Based on a Pipelined Bilinear Recurrent Neural Network", "author": ["H. Zhao", "X. Zeng", "Z. He"], "venue": "IEEE Trans. Neural Netw., vol. 22, no. 9, pp. 1494\u20131507, Sep. 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Pipelined Chebyshev Functional Link Artificial Recurrent Neural Network for Nonlinear Adaptive Filter", "author": ["H. Zhao", "J. Zhang"], "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern., vol. 40, no. 1, pp. 162\u2013172, Feb. 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "A novel nonlinear adaptive filter using a pipelined second-order Volterra recurrent neural network", "author": ["H. Zhao", "J. Zhang"], "venue": "Neural Netw., vol. 22, no. 10, pp. 1471\u20131483, Dec. 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Pipelined Recurrent Fuzzy Neural Networks for Nonlinear Adaptive Speech Prediction", "author": ["D.G. Stavrakoudis", "J.B. Theocharis"], "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern., vol. 37, no. 5, pp. 1305\u20131320, Oct. 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptively Combined FIR and Functional Link Artificial Neural Network Equalizer for Nonlinear Communication Channel", "author": ["H. Zhao", "J. Zhang"], "venue": "IEEE Trans. Neural Netw., vol. 20, no. 4, pp. 665\u2013674, Apr. 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "A generalized normalized gradient descent algorithm", "author": ["D.P. Mandic"], "venue": "IEEE Signal Process. Lett., vol. 11, no. 2, pp. 115\u2013118, Feb. 2004.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Complex Valued Nonlinear Adaptive Filters: Noncircularity, Widely Linear and Neural Models", "author": ["D.P. Mandic", "V.S.L. Goh"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Robust Regularization for Normalized LMS Algorithms", "author": ["Y.-S. Choi", "H.-C. Shin", "W.-J. Song"], "venue": "IEEE Trans. Circuits Syst. II Express Briefs, vol. 53, no. 8, pp. 627\u2013631, Aug. 2006.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust Adaptive Gradient-Descent Training Algorithm for Recurrent Neural Networks in Discrete Time Domain", "author": ["Q. Song", "Y. Wu", "Y.C. Soh"], "venue": "IEEE Trans. Neural Netw., vol. 19, no. 11, pp. 1841\u20131853, Nov. 2008.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1841}, {"title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks", "author": ["R.J. Williams", "D. Zipser"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1989}, {"title": "Identification and control of dynamical systems using neural networks", "author": ["K.S. Narendra", "K. Parthasarathy"], "venue": "IEEE Trans. Neural Netw., vol. 1, no. 1, pp. 4\u201327, Mar. 1990.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1990}, {"title": "Adaptive signal processing", "author": ["B. Widrow", "S.D. Stearns"], "venue": "Englewood Cliffs, N.J: Prentice-Hall,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1985}, {"title": "A new class of gradient adaptive step-size LMS algorithms", "author": ["W.-P. Ang", "B. Farhang-Boroujeny"], "venue": "IEEE Trans. Signal Process., vol. 49, no. 4, pp. 805\u2013810, Apr. 2001.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}, {"title": "A stochastic gradient adaptive filter with gradient adaptive step size", "author": ["V.J. Mathews", "Z. Xie"], "venue": "IEEE Trans. Signal Process., vol. 41, no. 6, pp. 2075\u20132087, Jun. 1993.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning entropy for novelty detection a cognitive approach for adaptive filters", "author": ["I. Bukovsky", "C. Oswald", "M. Cejnek", "P.M. Benes"], "venue": "Sensor Signal Processing for Defence (SSPD), 2014, 2014, pp. 1\u20135.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Hyperchaotic behaviour of two bi-directionally coupled Chua\u2019s circuits", "author": ["B. Cannas", "S. Cincotti"], "venue": "Int. J. Circuit Theory Appl., vol. 30, no. 6, pp. 625\u2013637, 2002.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "Adaptive Evaluation of Complex Dynamical Systems Using Low-Dimensional Neural Architectures", "author": ["I. Bukovsky", "J. Bila"], "venue": "Advances in Cognitive Informatics and Cognitive Computing, vol. 323, Y. Wang, D. Zhang, and W. Kinsner, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2010, pp. 33\u201357.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Entropy: Multiscale Measure for Incremental Learning", "author": ["I. Bukovsky"], "venue": "Entropy, vol. 15, no. 10, pp. 4159\u20134187, Sep. 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Oscillation and chaos in physiological control systems", "author": ["M.C. Mackey", "L. Glass"], "venue": "Science, vol. 197, no. 4300, pp. 287\u2013289, Jul. 1977.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1977}, {"title": "Real-Time Identification and Forecasting of Chaotic Time Series Using Hybrid Systems of Computational Intelligence", "author": ["Y. Bodyanskiy", "V. Kolodyazhniy"], "venue": "Integration of Fuzzy Logic and Chaos Theory, D. Z. Li, P. W. A. Halang, and P. G. Chen, Eds. Springer Berlin Heidelberg, 2006, pp. 439\u2013480.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "Quadratic neural unit and its network in validation of process data of steam turbine loop and energetic boiler", "author": ["I. Bukovsky", "M. Lepold", "J. Bila"], "venue": "2010, pp. 1\u20137.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "Quadratic neural unit is a good compromise between linear models and neural networks for industrial applications", "author": ["I. Bukovsky", "N. Homma", "L. Smetana", "R. Rodriguez", "M. Mironovova", "S. Vrana"], "venue": "2010 9th IEEE International Conference on Cognitive Informatics (ICCI), 2010, pp. 556\u2013560.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "We may recall that polynomial feedforward neural networks \u201care attractive due to the reliable theoretical results for their universal approximation abilities according to the Weierstrass theorem [1] and for their generalization power measured by the Vapnik-Chervonenkis (VC) dimension [2]\u201d (cited from [3] with re-numbered references).", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "We may recall that polynomial feedforward neural networks \u201care attractive due to the reliable theoretical results for their universal approximation abilities according to the Weierstrass theorem [1] and for their generalization power measured by the Vapnik-Chervonenkis (VC) dimension [2]\u201d (cited from [3] with re-numbered references).", "startOffset": 285, "endOffset": 288}, {"referenceID": 2, "context": "We may recall that polynomial feedforward neural networks \u201care attractive due to the reliable theoretical results for their universal approximation abilities according to the Weierstrass theorem [1] and for their generalization power measured by the Vapnik-Chervonenkis (VC) dimension [2]\u201d (cited from [3] with re-numbered references).", "startOffset": 302, "endOffset": 305}, {"referenceID": 3, "context": "Work [4] evaluated the computing ability of several types of HONNs by using pseudo-dimensions and VC dimensions [5] and higher-order neural networks (HONNs) were used as a universal approximator.", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "Work [4] evaluated the computing ability of several types of HONNs by using pseudo-dimensions and VC dimensions [5] and higher-order neural networks (HONNs) were used as a universal approximator.", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 157, "endOffset": 160}, {"referenceID": 7, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 379, "endOffset": 382}, {"referenceID": 8, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 420, "endOffset": 423}, {"referenceID": 9, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 474, "endOffset": 478}, {"referenceID": 10, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 561, "endOffset": 565}, {"referenceID": 11, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 653, "endOffset": 657}, {"referenceID": 12, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 721, "endOffset": 725}, {"referenceID": 13, "context": "For more recent works and applications of HONN and PNN we can refer to [14] and the particular focus on a quadratic neural unit (QNU) using matrix notation with upper triangular weight matrix can be found in [15]\u2013[18] and [19].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "For more recent works and applications of HONN and PNN we can refer to [14] and the particular focus on a quadratic neural unit (QNU) using matrix notation with upper triangular weight matrix can be found in [15]\u2013[18] and [19].", "startOffset": 208, "endOffset": 212}, {"referenceID": 17, "context": "For more recent works and applications of HONN and PNN we can refer to [14] and the particular focus on a quadratic neural unit (QNU) using matrix notation with upper triangular weight matrix can be found in [15]\u2013[18] and [19].", "startOffset": 213, "endOffset": 217}, {"referenceID": 18, "context": "For more recent works and applications of HONN and PNN we can refer to [14] and the particular focus on a quadratic neural unit (QNU) using matrix notation with upper triangular weight matrix can be found in [15]\u2013[18] and [19].", "startOffset": 222, "endOffset": 226}, {"referenceID": 2, "context": "Significant and most recent publications devoted to PNN concepts are the works [3], [20] [21] while most recent works that are framed within HONNs can be found in [22], where some modifications of HONU are introduced in order to cope with the curse of dimensionality of HONU for higher polynomial orders.", "startOffset": 79, "endOffset": 82}, {"referenceID": 19, "context": "Significant and most recent publications devoted to PNN concepts are the works [3], [20] [21] while most recent works that are framed within HONNs can be found in [22], where some modifications of HONU are introduced in order to cope with the curse of dimensionality of HONU for higher polynomial orders.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Significant and most recent publications devoted to PNN concepts are the works [3], [20] [21] while most recent works that are framed within HONNs can be found in [22], where some modifications of HONU are introduced in order to cope with the curse of dimensionality of HONU for higher polynomial orders.", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "Significant and most recent publications devoted to PNN concepts are the works [3], [20] [21] while most recent works that are framed within HONNs can be found in [22], where some modifications of HONU are introduced in order to cope with the curse of dimensionality of HONU for higher polynomial orders.", "startOffset": 163, "endOffset": 167}, {"referenceID": 22, "context": "Other interesting earlier-appearing neural network architectures are product neural units [23] and later logarithmic neural networks [24].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Another nonconventional neural units are continuous time-delay dynamic neural units and higher-order time-delay neural units that have adaptable time delays in neural synapses and in state feedbacks of individual neurons as introduced in [25]; a similar fuzzy-network oriented concept appeared in parallel also in [26].", "startOffset": 238, "endOffset": 242}, {"referenceID": 24, "context": "Another nonconventional neural units are continuous time-delay dynamic neural units and higher-order time-delay neural units that have adaptable time delays in neural synapses and in state feedbacks of individual neurons as introduced in [25]; a similar fuzzy-network oriented concept appeared in parallel also in [26].", "startOffset": 314, "endOffset": 318}, {"referenceID": 25, "context": "Another work focusing on various types of neural transfer functions can be found in review [27].", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "On the other hand, some newer HONU models were proposed for effective computation, learning and configuration in [4] [18] [28]\u2013[34] but they may still suffer from or do not take care of the local minima problem.", "startOffset": 113, "endOffset": 116}, {"referenceID": 17, "context": "On the other hand, some newer HONU models were proposed for effective computation, learning and configuration in [4] [18] [28]\u2013[34] but they may still suffer from or do not take care of the local minima problem.", "startOffset": 117, "endOffset": 121}, {"referenceID": 26, "context": "On the other hand, some newer HONU models were proposed for effective computation, learning and configuration in [4] [18] [28]\u2013[34] but they may still suffer from or do not take care of the local minima problem.", "startOffset": 122, "endOffset": 126}, {"referenceID": 32, "context": "On the other hand, some newer HONU models were proposed for effective computation, learning and configuration in [4] [18] [28]\u2013[34] but they may still suffer from or do not take care of the local minima problem.", "startOffset": 127, "endOffset": 131}, {"referenceID": 33, "context": "Moreover, our achievements might bring novel research directions for HONU when considering the adaptive learning rate modifications of gradient descent as in [35].", "startOffset": 158, "endOffset": 162}, {"referenceID": 33, "context": "Correspondingly, section III experimentally supports the theoretical derivations, and it also discusses possible extensions of adaptive-learning-rate principles of linear filters [35]\u2013[38] to HONU.", "startOffset": 179, "endOffset": 183}, {"referenceID": 36, "context": "Correspondingly, section III experimentally supports the theoretical derivations, and it also discusses possible extensions of adaptive-learning-rate principles of linear filters [35]\u2013[38] to HONU.", "startOffset": 184, "endOffset": 188}, {"referenceID": 14, "context": "Operators for HONU We start with recalling static QNU [15]\u2013[17], [25] as a fundamental second-order HONU or as a fundamental class of PNN as follows", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "Operators for HONU We start with recalling static QNU [15]\u2013[17], [25] as a fundamental second-order HONU or as a fundamental class of PNN as follows", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "Operators for HONU We start with recalling static QNU [15]\u2013[17], [25] as a fundamental second-order HONU or as a fundamental class of PNN as follows", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "Adopting the matrix formulation of QNU from [15], the vector of neural inputs (for al HONU) and the weight array (for QNU) is as follows", "startOffset": 44, "endOffset": 48}, {"referenceID": 34, "context": "In Section II, we derive formulation (13) that is a 1-D array alternative to (1) (3) that allows the gradient descent stability condition of HONU be effectively derived and that allows connotations to adaptive learning rates of linearly aggregated filters as summarized in [36] (Appendix K).", "startOffset": 273, "endOffset": 277}, {"referenceID": 37, "context": "Weight-Update Stability of Static HONU The operator approach introduced above can be used for stability evaluation and stability maintenance of weight updates for both static HONU updated by the gradient descent and for recurrent HONU updated by its recurrent version also known as RTRL [39].", "startOffset": 287, "endOffset": 291}, {"referenceID": 33, "context": ", starting with inspiration from works [35], [36] this time for HONU (and other nonlinear models that are linear in their parameters).", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": ", starting with inspiration from works [35], [36] this time for HONU (and other nonlinear models that are linear in their parameters).", "startOffset": 45, "endOffset": 49}, {"referenceID": 33, "context": "The proper investigation of the J-reset effect to learning of recurrent HONU and the very rigorous development and analysis of sophisticated techniques for adaptive learning rates, such as based on works [35]\u2013[38] exceeds the limits for this paper; however, the operator approach and the stability conditions of HONU allows us to propose most straightforward connotations to adaptive learning rates techniques for HONU and we introduce them in subsections III.", "startOffset": 204, "endOffset": 208}, {"referenceID": 36, "context": "The proper investigation of the J-reset effect to learning of recurrent HONU and the very rigorous development and analysis of sophisticated techniques for adaptive learning rates, such as based on works [35]\u2013[38] exceeds the limits for this paper; however, the operator approach and the stability conditions of HONU allows us to propose most straightforward connotations to adaptive learning rates techniques for HONU and we introduce them in subsections III.", "startOffset": 209, "endOffset": 213}, {"referenceID": 0, "context": "[1 ] r", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "As a benchmark we chose a variation of famous system from [40] as This is the author's version of an article that has been published in this journal.", "startOffset": 58, "endOffset": 62}, {"referenceID": 33, "context": "Adaptive Learning Rate of Static HONU In this subsection, we introduce the connotations of static HONUs to adaptive learning rate techniques that are well known in literature for adaptive filters with linear aggregation of neural inputs [35], [36], [41]\u2013[44].", "startOffset": 237, "endOffset": 241}, {"referenceID": 34, "context": "Adaptive Learning Rate of Static HONU In this subsection, we introduce the connotations of static HONUs to adaptive learning rate techniques that are well known in literature for adaptive filters with linear aggregation of neural inputs [35], [36], [41]\u2013[44].", "startOffset": 243, "endOffset": 247}, {"referenceID": 39, "context": "Adaptive Learning Rate of Static HONU In this subsection, we introduce the connotations of static HONUs to adaptive learning rate techniques that are well known in literature for adaptive filters with linear aggregation of neural inputs [35], [36], [41]\u2013[44].", "startOffset": 249, "endOffset": 253}, {"referenceID": 41, "context": "Adaptive Learning Rate of Static HONU In this subsection, we introduce the connotations of static HONUs to adaptive learning rate techniques that are well known in literature for adaptive filters with linear aggregation of neural inputs [35], [36], [41]\u2013[44].", "startOffset": 254, "endOffset": 258}, {"referenceID": 34, "context": "[36], p.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "With those substitutions, we can adapt the learning rate by the classical normalized least mean square (NLMS) algorithm [41], so the adaptive learning rate \u03b7 for static HONUs yields", "startOffset": 120, "endOffset": 124}, {"referenceID": 40, "context": "Furthemore and similarly to substitution as made in (52) for static HONUs, we may also implement the Benveniste\u2019s learning rate updates based on [42], algorithm by Farhang and Ang [43], Mathews\u2019 algorithm [44], and generalized normalized gradient descent algorithm (GNGD) of Mandic [35] (as summarized in [36] (Appendix K).", "startOffset": 180, "endOffset": 184}, {"referenceID": 41, "context": "Furthemore and similarly to substitution as made in (52) for static HONUs, we may also implement the Benveniste\u2019s learning rate updates based on [42], algorithm by Farhang and Ang [43], Mathews\u2019 algorithm [44], and generalized normalized gradient descent algorithm (GNGD) of Mandic [35] (as summarized in [36] (Appendix K).", "startOffset": 205, "endOffset": 209}, {"referenceID": 33, "context": "Furthemore and similarly to substitution as made in (52) for static HONUs, we may also implement the Benveniste\u2019s learning rate updates based on [42], algorithm by Farhang and Ang [43], Mathews\u2019 algorithm [44], and generalized normalized gradient descent algorithm (GNGD) of Mandic [35] (as summarized in [36] (Appendix K).", "startOffset": 282, "endOffset": 286}, {"referenceID": 34, "context": "Furthemore and similarly to substitution as made in (52) for static HONUs, we may also implement the Benveniste\u2019s learning rate updates based on [42], algorithm by Farhang and Ang [43], Mathews\u2019 algorithm [44], and generalized normalized gradient descent algorithm (GNGD) of Mandic [35] (as summarized in [36] (Appendix K).", "startOffset": 305, "endOffset": 309}, {"referenceID": 42, "context": "We recently showed these extensions for HONU and compared their performance for chaotic time series in [45].", "startOffset": 103, "endOffset": 107}, {"referenceID": 42, "context": "1: Extensions of adaptive learning rates for HONU [45]; the adaptive learning rate then still can be used in stability conditions (29)", "startOffset": 50, "endOffset": 54}, {"referenceID": 39, "context": "NLMS [41] ( ) ( ) ( ) 2 ( ) 2 T k k k p k p e \u03bc \u03b5 \u22c5 \u22c5 \u2212 \u2212 \u2206 = + w colx colx", "startOffset": 5, "endOffset": 9}, {"referenceID": 33, "context": "GNGD [35] T ( ) ( 1) ( ) ( 1) ( 1) ( ) 2 2 ( 1) ( ) 2 k k k p k p k k", "startOffset": 5, "endOffset": 9}, {"referenceID": 35, "context": "RR\u2013NLMS [37] [ ( ) ( 1) min", "startOffset": 8, "endOffset": 12}, {"referenceID": 40, "context": "Farhang\u2019s & Ang\u2019s [43] ( ) ( 1) ( 1) ( 1) ; 0 , 1 k k k k p e \u03b7 \u03b7 \u2212 + \u2212 \u22c5 \u2212 \u2212 = \u22c5 \u2208 \u03b3 \u03b3 colx", "startOffset": 18, "endOffset": 22}, {"referenceID": 41, "context": "Mathew\u2019s [44] T ( 1) ( ) ( ) ( 1) ( ) ( 1)", "startOffset": 9, "endOffset": 13}, {"referenceID": 43, "context": "The performance of the learning with adaptive learning rates (56) and showing the stability condition (32) for static HONUs of order r=2,3,4,5 for prediction of hyperchaotic Chua\u2019s time series [46]\u2013[48] is shown in Fig.", "startOffset": 193, "endOffset": 197}, {"referenceID": 45, "context": "The performance of the learning with adaptive learning rates (56) and showing the stability condition (32) for static HONUs of order r=2,3,4,5 for prediction of hyperchaotic Chua\u2019s time series [46]\u2013[48] is shown in Fig.", "startOffset": 198, "endOffset": 202}, {"referenceID": 46, "context": "Stability Monitoring Let us use recurrent QNU (HONU r=2) for a long-term prediction of the MacKey-Glass time series in chaotic mode [49], [50] that is given as ( ) 1 10 ( ( ) ( ) ( ) ) 0.", "startOffset": 132, "endOffset": 136}, {"referenceID": 47, "context": "Stability Monitoring Let us use recurrent QNU (HONU r=2) for a long-term prediction of the MacKey-Glass time series in chaotic mode [49], [50] that is given as ( ) 1 10 ( ( ) ( ) ( ) ) 0.", "startOffset": 138, "endOffset": 142}, {"referenceID": 33, "context": "A was a suitable task for static HONU, it is known that adaptation by GD becomes nontrivial task for this benchmark because the weight-update system by GD becomes unstable and requires the control of magnitude of learning rate (several approaches, but not for HONU, to prevent instability and to improve convergence are known [35], [37], [38]).", "startOffset": 326, "endOffset": 330}, {"referenceID": 35, "context": "A was a suitable task for static HONU, it is known that adaptation by GD becomes nontrivial task for this benchmark because the weight-update system by GD becomes unstable and requires the control of magnitude of learning rate (several approaches, but not for HONU, to prevent instability and to improve convergence are known [35], [37], [38]).", "startOffset": 332, "endOffset": 336}, {"referenceID": 36, "context": "A was a suitable task for static HONU, it is known that adaptation by GD becomes nontrivial task for this benchmark because the weight-update system by GD becomes unstable and requires the control of magnitude of learning rate (several approaches, but not for HONU, to prevent instability and to improve convergence are known [35], [37], [38]).", "startOffset": 338, "endOffset": 342}, {"referenceID": 48, "context": "We practically observed in [51], [52] that weight convergence of static HONU using L-M algorithm was very rapid and required a very few epoch in comparison to conventional multilayered perceptron networks.", "startOffset": 27, "endOffset": 31}, {"referenceID": 49, "context": "We practically observed in [51], [52] that weight convergence of static HONU using L-M algorithm was very rapid and required a very few epoch in comparison to conventional multilayered perceptron networks.", "startOffset": 33, "endOffset": 37}, {"referenceID": 48, "context": "Moreover, HONUs that were trained from various initial weights had almost identical outputs compared to various instances of trained MLP networks whose outputs were different for the same input patterns when trained from different initial weights and for the same configurations [51], [52].", "startOffset": 279, "endOffset": 283}, {"referenceID": 49, "context": "Moreover, HONUs that were trained from various initial weights had almost identical outputs compared to various instances of trained MLP networks whose outputs were different for the same input patterns when trained from different initial weights and for the same configurations [51], [52].", "startOffset": 285, "endOffset": 289}], "year": 2016, "abstractText": "Stability evaluation of a weight-update system of higher-order neural units (HONUs) with polynomial aggregation of neural inputs (also known as classes of polynomial neural networks) for adaptation of both feedforward and recurrent HONUs by a gradient descent method is introduced. An essential core of the approach is based on spectral radius of a weight-update system, and it allows stability monitoring and its maintenance at every adaptation step individually. Assuring stability of the weight-update system (at every single adaptation step) naturally results in adaptation stability of the whole neural architecture that adapts to target data. As an aside, the used approach highlights the fact that the weight optimization of HONU is a linear problem, so the proposed approach can be generally extended to any neural architecture that is linear in its adaptable parameters.", "creator": "Appligent AppendPDF Pro 5.5"}}}