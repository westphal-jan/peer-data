{"id": "1606.08513", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "SelQA: A New Benchmark for Selection-based Question Answering", "abstract": "pavlides This carbonite paper underwrote presents weissglas a krzaklewski new dataset bruderhof to ehara benchmark turpitude selection - artform based question answering. 7/6 Our dataset contains contexts drawn papillons from the najeeb ten untreatable most 5:06 prevalent topics in the English clarksburg Wikipedia. bayport For the nacha generation of a large, effluents diverse, tiggy and challenging detain dataset, a deet new annotation scheme is nfc proposed. Our loughton annotation 6:3 scheme merletti involves farrait a gherardesca series 132-pound of crowdsourcing jingshi tasks that baltusrol can 48.45 be bolotowsky easily buckwild followed by harringtons any researcher. Several systems are trentham compared on blue the tasks bh of answer bernward sentence senter selection q. and gilgun answer 720 triggering, kalamandalam providing hatt strong baseline results for future coin work slamed to cantate improve upon. We mid-1500s hope lensed that chrysanthemum providing 1,936 a betterman large mozi corpus maisan will enable researchers to work towards more effective karuppu open - domain archbishop-elector question self-release answering.", "histories": [["v1", "Mon, 27 Jun 2016 23:48:16 GMT  (291kb,D)", "https://arxiv.org/abs/1606.08513v1", null], ["v2", "Wed, 12 Oct 2016 16:36:02 GMT  (334kb,D)", "http://arxiv.org/abs/1606.08513v2", null], ["v3", "Fri, 28 Oct 2016 01:20:19 GMT  (335kb,D)", "http://arxiv.org/abs/1606.08513v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tomasz jurczyk", "michael zhai", "jinho d choi"], "accepted": false, "id": "1606.08513"}, "pdf": {"name": "1606.08513.pdf", "metadata": {"source": "CRF", "title": "SelQA: A New Benchmark for Selection-based Question Answering", "authors": ["Tomasz Jurczyk", "Michael Zhai", "Jinho D. Choi"], "emails": ["tomasz.jurczyk@emory.edu", "michael.zhai@emory.edu", "jinho.choi@emory.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nSelection-based question answering is the task of selecting a segment of text, or interchangeably a context, from a provided set of contexts that best answers a posed question. Let us define a context, as a single document section, a group of contiguous sentences, or a single sentence. Selection-based question answering is subdivided into answer sentence selection and answer triggering. Answer sentence selection is defined as ranking sentences that answer a question higher than the irrelevant sentences where there is at least a single sentence that answers the question in a provided set of candidate sentences. Answer triggering is defined as selecting any number (n >= 0) of sentences from a set of candidate sentences that answers a question where the set of candidate sentences may or may not contain sentences that answer the question. Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language. However, most of these datasets are constrained in the number of examples and scope of topics. We attempt to mitigate these limitations to allow for a more through reading comprehension evaluation of open-domain question answering systems.\nThis paper presents a new corpus with annotated question answering examples of various topics drawn from Wikipedia. An effective annotation scheme is proposed to create a large corpus that is both challenging and realistic. Questions are additionally annotated with its topic, type, and paraphrase that enable comprehensive analyses of system performance\non the answer sentence selection and answer triggering tasks. Two recent state-of-the-art systems based on convolutional and recurrent neural networks are implemented to analyze this corpus and to provide strong baseline measures for future work. In addition, our systems are evaluated on another dataset, WikiQA [2], for a fair comparison to previous work. Our analysis suggests extensive ways of evaluating selectionbased question answering, providing meaningful benchmarks to question answering systems. The contributions of this work include:1\n\u2022 Creating a new corpus for answer sentence selection and answer triggering (Section III). \u2022 Developing QA systems using the latest advances in neural networks (Section IV). \u2022 Analyzing various aspects of selection-based question answering (Section V)."}, {"heading": "II. RELATED WORK", "text": "The TREC QA competition datasets have been a popular choice for evaluating answer sentence selection.2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation. This dataset, known as QASent, has been used as the standard benchmark for answer sentence selection although it is rather small (277 questions with manually picked answer contexts). [2] introduced a lager dataset, WikiQA, consisting of questions collected from the user logs of the Bing search engine. Our corpus is similar to WikiQA but covers more diverse topics, consists of a larger number of questions (about 6 times larger for answer sentence selection and 2.5 times larger for answer triggering), and makes use of more contexts by extracting contexts from the entire article instead of from only the abstract. [3] distributed another dataset, InsuranceQA, including questions in the insurance domain. WikiQA introduced the task of answer triggering and was the only answer triggering dataset. Our corpus provides a new automatically generated answer triggering dataset.\nDue to increasing complexity in question answering, deep learning has become a popular trend in solving difficult problems. [7] proposed a convolutional neural network with a\n1All our work will be publicly available on GitHub. 2http://trec.nist.gov/data/qa.html\nar X\niv :1\n60 6.\n08 51\n3v 3\n[ cs\n.C L\n] 2\n8 O\nct 2\n01 6\nsingle convolution layer, average pooling and logistic regression at the end for factoid question answering. Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings. Our recurrent neural network models with attention are based on established state-of-the-art systems for answer sentence selection [16], [17]."}, {"heading": "III. CORPUS", "text": "Our annotation scheme provides a framework for any researcher to create a large, diverse, pragmatic, and challenging dataset for answer sentence selection and answer triggering, while maintaining a low cost using crowdsourcing."}, {"heading": "A. Data Collection", "text": "A total of 486 articles are uniformly sampled from the following 10 topics of the English Wikipedia, dumped on August, 2014:\nArts, Country, Food, Historical Events, Movies, Music, Science, Sports, Travel, TV.\nThese are the most prevalent topics categorized by DBPedia.3 The original data is preprocessed into smaller chunks. First, each article is divided into sections using the section boundaries provided in the original dump.4 Each section is segmented into sentences by the open-source toolkit, NLP4J.5 In our corpus, documents refer to individual sections in the Wikipedia articles.\n3http://dbpedia.org 4https://dumps.wikimedia.org/enwiki 5https://github.com/emorynlp/nlp4j"}, {"heading": "B. Annotation Scheme", "text": "Four annotation tasks are conducted in sequence on Amazon Mechanical Turk for answer sentence selection (Tasks 1-4), and a single task is conducted for answer triggering using only Elasticsearch (Task 5; see Figure 1 for the overview).\nTask 1\nApproximately two thousand sections are randomly selected from the 486 articles in Section III-A. All the selected sections consist of 3 to 25 sentences; we found that annotators experienced difficulties accurately and timely annotating longer sections. For each section, annotators are instructed to generate a question that can be answered in one or more sentences in the provided section, and select the corresponding sentence or sentences that answer the question. The annotators are provided with the instructions, the topic, the article title, the section title, and the list of numbered sentences in the section (Table II).\nTask 2\nAnnotators are asked to create another set of \u22482K questions from the same selected sections excluding the sentences selected as answers in Task 1. The goal of Task 2 is to generate questions that can be answered from sentences different from those used to answer questions generated in the Task 1. The annotators are provided with the same information as in Task 1, except that the sentences used as the answer contexts in Task 1 are crossed out (line 1 in Table II). Annotators are instructed not to use these sentences to generate new questions.\nTask 3\nAlthough our instruction encourages the annotators to create questions in their own words, annotators will generate questions with some lexical overlap with the corresponding contexts. The intention of this task is to mitigate the effects of annotators\u2019 tendency to generating questions with similar vocabulary and phrasing to answer contexts. This is a necessary step in creating a corpus that evaluates reading comprehension rather than ability to model word co-occurrences. The annotators are provided with the previously generated questions and answer contexts and are instructed to paraphrase these questions using different terms.\nTask 4\nMost questions generated by Tasks 1-3 are of high quality, that is they can be answered by a human when given the corresponding contexts; however, there are some questions that are ambiguous in meaning and difficult for humans to answer correctly. These difficult questions often incorrectly assume that the related sections are provided with the questions. For instance, it is impossible to answer the question from Task 3.1 in Table II unless the related section is provided with the question. These ambiguous questions are sent back to the annotators for revision.\nElasticsearch is used to find ambiguous questions,6 a Lucenebased open-source search engine. First, an inverted index of 8,481 sections is built, where each section is considered a document. Each question is queried to this search engine. If the answer context is not included within the top 5 sections in the search result, the question is considered \u2018suspicious\u2019 although it may not be ambiguous. Among 7,904 questions generated by Tasks 1-3, 1,338 of them are found to be suspicious. These questions are sent to the annotators, and rephrased by the annotators if deemed necessary.\nTask 5\nBy using the previously generated answer sentence selection data, the answer triggering corpus can be automatically generated again using Elasticsearch. To generate answer contexts for answer triggering, all 14M sections from the entire English\n6www.elastic.co/products/elasticsearch\nWikipedia are indexed, and each question from Tasks 1-4 is queried. Every sentence in the top 5 highest scoring sections from Elasticsearch are collected as candidates, which may or may not include the answer context that resolves the question."}, {"heading": "C. Corpus Analysis", "text": "The entire annotation took about 130 hours, costing $770 in total; each mturk job took on average approximately 1 minute and costed about \u00a210. A total of 7,904 questions were generated from Tasks 1-4, where 92.2% of them found their answers in single sentences. It is clear that Task 3 was effective in reducing the percentage of overlapping words between question and answer pairs (about 4%; \u2126f in Table III). The questions from Task 3 can be used to develop paraphrasing models as well. Multiple pilot studies on different tasks were conducted to analyze quality and cost; Tasks 1-4 were proved to be the most effective in the pilot studies. Following [18], we paid incentives to those who submitted outstanding work, which improved the overall quality of our annotation.\nOur corpus could be compared to WikiQA that was created with the intent of providing a challenging dataset for selectionbased question answering [2]. Questions in this dataset were collected from the user logs of the Bing search engine, and associated with the specific sections in Wikipedia, namely the first sections known as the abstracts. We aim to provide a similar yet more exhaustive dataset by broadening the scope to all sections. A notable difference was found between these two corpora for overlapping words (about 11% difference), which was expected due to the artificial question generation in\nour scheme. Although questions taken from the search queries are more natural, real search queries are inacessible to most researchers. The new annotation scheme proposed here can prove useful for researchers needing to create a corpus for selection-based QA.\nOur answer triggering dataset contains 5 times more answer candidates per question than WikiQA because WikiQA includes only sections clicked on by users. Manual selection is eliminated from our framework, making our corpus more practical. In WikiQA, 40.76% of the questions have corresponding answer contexts for answer triggering, as compared to 39.25% in ours."}, {"heading": "IV. SYSTEMS", "text": "Two models using convolutional neural networks are developed, one is our replication of the best model in [2], and the other is an improved model using subtree matching (Section IV-A). Two more models using recurrent neural networks are developed, one is our replication of the attentive pooling model in [17], and the other is a simpler model using one-way attention (Section IV-B). These are inspired by the latest state-of-the-art approaches, providing sensible evaluations."}, {"heading": "A. Convolutional Neural Networks", "text": "Our CNN model is motivated by [2]. First, a convolutional layer is applied on the image of text using the hyperbolic tangent activation function. The image consists of rows standing for consecutive words in two sentences, the question (q) and the answer candidate (a), where the words are represented by their embeddings [19]. For our experiments, we use the image of 80 rows (40 for question and answer, respectively). If any of the question or answer is longer than 40 tokens, the rest is being cut from the input. Next, the max pooling is applied,7 and the sentence vectors for q and a are generated. Unlike [2] who performed the dot product between these two vectors, we added another hidden layer to learn their weights. Finally, the sigmoid activation function is applied and the entire network is trained using the binary cross-entropy. Next, we use a logistic regression model, where the CNN score from the output layer is used as one of the features.\n7We also experimented with the average pooling as [2], which led to a marginally lower accuracy.\nOther features in the logistic regression are the number of overlapping words between q and a, say \u2126, \u2126 normalized by the IDF, and the question length. While the logistic regression model could be merged directly with our CNN model, it has been empirically shown that it is more effective to construct this last phase as a separate model.\nInput: T : a set of co-occurring words between a question and answer. Dq, Da: sets of slices for a question and answer. fm: a metrics function. fc: a comparator function. Output: Sdep: A triplet of dependency similarity. Sdep \u2190 [0, 0, 0]; foreach word woi in T do\npqi \u2190getParent(D q i ) pai \u2190getParent(Dai ) Sdep[0]\u2190 Sdep[0] + fc(pqi , pai ) Sqi \u2190getSiblings(D q i ) Sai \u2190getSiblings(Dai ) vals\u2190 [] foreach sibling sqj in S q i do\nforeach sibling sak in Sai do vals.append(fc(s q j , s a k))\nend end Sdep[1]\u2190 Sdep[1] + fm(vals) Cqi \u2190getChildren(D q i ) Cai \u2190getChildren(Dai ) vals\u2190 [] foreach child cqj in C q i do\nforeach child cak in Cai do vals.append(fc(c q j , c a k))\nend end Sdep[2]\u2190 Sdep[2] + fm(vals)\nend Algorithm 1: Algorithm of our subtree matching mechanism\nFor the answer sentence selection task, the predictions for each question are treated as a ranking and the MAP and MRR scores are being calculated (Section V-B). On the other hand, in the answer triggering task (Section V-C) a threshold is applied on each predicted question by the logistic regression; the candidate with the highest score is considered the answer if it is above the threshold found during development; otherwise, the model assumes no existence of the answer context in this document for that question. Figure 2 shows the overview of our CNN and LR model.\nSubtree Matching: We propose a subtree matching mechanism for measuring the contextual similarity between two sentences. All sentences are automatically parsed by the NLP4J dependency parser [20]. First, a set of co-occurring words between q and a, say T , is created. For each woi \u2208 T , woi \u2019s parents (pqi , p a i ), siblings (S q i , S a i ), and children (C q i , C a i ) are extracted from the dependency slices of q and a. When the word-forms are used as the comparator, fc(x, y) returns 1 if x and y have the same form; otherwise, 0. When the word embeddings are used as the comparator, fc(x, y) returns the cosine similarity between x and y. The function fm takes a list of scores and returns either the sum, avg, or max of the scores. Finally, the triplet Sdep is used as the additional features to the logistic regression model. Algorithm 1 presents the entire process in detail. Although our subtree matching mechanism adds only 3 more features, our experiments show significant performance gains for both the answer sentence selection and answer triggering strengthening our hypothesis that to solve question answering problems more effectively, deeper contextual similarity is required."}, {"heading": "B. Recurrent Neural Network", "text": "Our RNN model is based on the bidirectional Long ShortTerm Memory (LSTM) using attentive pooling introduced by [17], except that our network uses a gated recurrent unit (GRU; [21]) instead of LSTM. From our preliminary experiments, we found that GRU converged faster than LSTM while achieving similar performance for these tasks. Let wqi \u2208 q, waj \u2208 a, where q is the question and a is the answer candidate, and e(w) returns the embedding of a word w. Embeddings are encoded by a single bidirectional GRU g that consists of the forward (\u2212\u2192g ) and the backward (\u2190\u2212g ) GRUs, each with h hidden units. Given w, g outputs the vector concatenation of the hidden states of \u2212\u2192g and \u2190\u2212g :\ng(e(w)) = \u2212\u2192g (e(w))||\u2190\u2212g (e(w))\nLet c = 2 \u00b7 h represent the dimensionality of the output of g. Then, sentence embedding matrices Q \u2208 R|q|\u00d7c and A \u2208 R|a|\u00d7c are generated by g as Qi = g(e(wqi )) and Aj = g(e(w a j )).\nBoth the attentive pooling and one-way attention models below are trained by minimizing the pairwise hinge ranking loss. In addition, RMSProp is used for the optimization and the `2 weight penalty is applied on all parameters except for embeddings. All network parameters except the embeddings are initialized using orthogonal initialization.\nAttentive Pooling: Attentive Pooling (AP) is a frameworkindependent two-way attention mechanism that jointly learns a similarity measure between q and a. AP learns the similarity measure over the hidden states of q and a. The AP matrix H \u2208 R|q|\u00d7|a| has a bilinear form and is followed by a hyperbolic tangent non-linearity, where U \u2208 Rc\u00d7c:\nH = tanh(QTUA)\nThe importance vectors hq \u2208 R|q| and ha \u2208 R|a| are generated from the column-wise and row-wise max pooling over H , respectively:\n[hq]j = max i\u2208[1,|q|] [Hj,i]\nThe normalized attention vectors \u03c3q and \u03c3a are created by applying the softmax activation function on hq and ha:\n\u03c3q = exp([hq]j)\u2211\ni\u2208[1,|q|] exp([hq]i)\nThe final representations rq = Q\u03c3q and ra = A\u03c3a for q and a are created using the dot products of the sentence representations and their corresponding attention vectors. The score is computed for each pair using cosine similarity:\ns(q, a) = rqT ra\n\u2016rq\u2016\u2016ra\u2016\nOne-Way Attention: Our one-way attention model is a simplified version of the attentive pooling model above, which is most similar to the global attention model introduced by [22]. We did not use the one-way attention from [16] to avoid deviating the attention mechanism significantly. Replacing Q with Q|q|, the last hidden state of g, H becomes the importance vector h. Again, we create the normalized attention vector \u03c3a by applying the softmax activation function. The final representations are rq = Q|q| and ra = A\u03c3a."}, {"heading": "V. EXPERIMENTS", "text": "Our systems are evaluated for the answer sentence selection (Section V-B) and answer triggering (Section V-C) tasks on both WikiQA and our corpus."}, {"heading": "A. SelQA: Selection-based QA Corpus", "text": "Table IV shows the distributions of our corpus, called SelQA. Our corpus is split into training (70%), development (10%), and evaluation (20%) sets. The answer triggering data (AT) is significantly larger than the answer sentence selection data (ASS), due to the extra sections added by Task 5 (Section III-B)."}, {"heading": "B. Answer Sentence Selection", "text": "Table V shows results from ours and the previous approaches on WikiQA. Two metrics are used, mean average precision (MAP) and mean reciprocal rank (MRR), for the evaluation of this task. CNN0 is our replication of the best model in [2]. CNN1 and CNN2 are the CNN models using the subtree matching in Section IV-A, where the comparator of fc is either the word form or the word embedding respectively, and fm = avg. The subtree matching models consistently outperforms the baseline model. Note that among the three metrics of fm, avg, sum, and max, avg outperformed the others in our experiments for the answer sentence selection task although no significant differences were found. RNN0 and RNN1 are the RNN models using the one-way attention and the attentive pooling in Section IV-B. Note that RNN1 converged much faster than RNN0 at the same learning rate and fixed number of parameters in our experiments, implying that two-way attention assists with optimization.\nIt is interesting to see how CNN1 and RNN0 outperform CNN2 and RNN1 respectively on the development set, but not on the evaluation set. This result may be explained by the larger percentage of overlapping words in the development set, enabling the simpler models perform more effectively.\nTable VI shows the results achieved by our models on SelQA. CNN2 outperforms the other CNN models, indicating the power of subtree matching coupled with word embeddings. RNN1 outperforms RNN0, indicating the importance of attention over the questions. Unlike the results on WikiQA in Table V, CNN2 and RNN0 show the best performance on both the development and evaluation sets, implying the robustness of these models on our corpus.\nTable VII shows the MRR scores from our models on SelQA with respect to different topics. All models show strength on topics such as \u2018Country\u2019 and \u2018Historical Events\u2019, which is comprehensible since questions in these topics tend to be deterministic. On the other hand, most models show weakness on topics such as \u2018TV\u2019, \u2018Arts\u2019, or \u2018Music\u2019. This may be due to the fact that not many overlapping words are found between question and answer pairs in these documents, which also consist of many segments caused by bullet points.\nTable VIII shows comparisons between questions from Tasks 1 and 2 (original) and Task 3 (paraphrase) in Section III-B. As expected, noticeable performance drop is found for the paraphrased questions, which have much fewer overlapping words to the answer contexts than the original questions.\nTable IX shows the MRR scores with respect to question types. The CNN models show strength on the \u2018who\u2019 type, whereas the RNN models show strength on the \u2018when\u2019 type. Each model varies on showing their weakness, which we will explore in the future. Finally, Figure 4 shows the performance difference with respect to question and section lengths. All models except for RNN0 tend to perform better as questions become longer. This makes sense since longer questions are usually more informative. On the other hand, models generally perform worse as sections become longer, which also makes sense because the models have to select the answer contexts from larger pools."}, {"heading": "C. Answer Triggering", "text": "Due to the nature of answer triggering, metrics used for evaluating answer sentence selection are not used here, because those metrics assume that models are always provided with contexts including the answers. Broadly speaking, the answer sentence selection task is a raking problem, while answer triggering is a binary classification task with additional constraints. Thus, the F1-score on the question level was proposed by [2] as the evaluation for this task, which we follow.\nTable X shows the answer triggering results on WikiQA. Note that RNN0 using one-way attention was dropped for these experiments because it did not show comparable performance against the others for this task. Interestingly, the CNN model\nwith fm = max outperformed the other metrics for answer triggering, although avg was found to be the most effective for answer sentence selection. The CNN subtree matching models consistently gave over 2% improvements to the baseline model.\nIn addition, CNN3 was experimented by retraining word embeddings (emb+), which performed slightly worse on the development set, but gave another 1.68% improvement on the evaluation set.8 RNN1 showed a very similar result to [2], which was surprising since it performed so much better for answer sentence selection. This can be due to a lack of hyper-parameter optimization, which we leave as a future work.\nTable XI shows the answer triggering results on SelQA. Unlike the results on WikiQA (Table X), CNN2 outperforms CNN3 on our corpus. On the other hand, RNN1 shows a similar score to [2] as it does on WikiQA. CNN2 using subtree matching gives over a 5% improvement to the baseline model, which is significant.\nTable XII shows the accuracies on SelQA with respect to different topics. The accuracy is measured on the subset of questions that contain at least one answer among candidates; the top ranked sentence is taken and checked for the correct answer. Similar to answer sentence selection, CNN2 stills\n8Retraining word embeddings was not found to be useful for answer sentence selection.\nshows strength on topics such as \u2018Country\u2019 and \u2018Historical Events\u2019, but the trend is not as clear for the other models. The worst performing topics are \u2018TV\u2019, \u2018Music\u2019 and \u2018Art\u2019. Such a noticeable difference might be caused by the unusual semantic sentence constructions of the text. Sections in these categories often contain listings, bullet-pointed texts etc., which is problematic for the models to properly take care of. How to correctly understand and solve question from such context will be a challenge to the future systems. Also, interestingly, the standard deviation is much smaller for RNN1 (3.9%) compared to the CNN models (10-12%) although RNN1\u2019s overall performance is lower.\nTable XIII shows the accuracies on SelQA with respect to paraphrasing, which is similar to the trend found in Table VIII for answer sentence selection.\nTable XIV shows the accuracies on SelQA with respect to question types. Interestingly, each model shows different strength on different types, which may suggest a possibility of an ensemble model. Finally, Figure 5 shows the performance difference with respect to question and section lengths for the answer triggering task. All the models tend to perform better as questions become longer. Similarly as in the answer sentence selection task, since longer questions are more informative, it is\nunderstandable. Interestingly, once the section becomes longer, the accuracy increases. We hypothesize that such a behavior might be caused by the fact that it is easier for the models to decide whether the context of the section is the same as the context of the question when there is more information (sentences) in the section. Thus, this phenomenon is related to the task of answer triggering, where the model not only choose the sentence with the answer, but must decide if the context matches first."}, {"heading": "VI. CONCLUSION", "text": "In this paper we present a new benchmark for two major question answering tasks: answer sentence selection and answer triggering. Several systems using neural networks are developed for the analysis of our corpus. Our analysis shows different aspects about the current QA approaches, beneficial for further enhancement.\nResearchers devoted to relatively small datasets reveal useful characteristics of the question answering tasks. Techniques that result in improvements on smaller datasets are often significantly diminished with larger datasets. Current hardware trends and the availability of larger datasets make large scale question answering more accessible.\nWe plan to continue our work on providing large scale corpora for open-domain question answering. Also, we intend to continue working towards providing context-aware frameworks for question answering."}, {"heading": "ACKNOWLEDGEMENT", "text": "We gratefully acknowledge the support from Infosys Ltd. Any contents in this material are those of the authors and do not necessarily reflect the views of Infosys Ltd."}], "references": [{"title": "What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA", "author": ["M. Wang", "N.A. Smith", "T. Mitamura"], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, ser. EMNLP-CoNLL\u201907, 2007, pp. 22\u201332.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "WIKIQA: A Challenge Dataset for Open-Domain Question Answering", "author": ["Y. Yang", "W.-t. Yih", "C. Meek"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP\u201915, 2015, pp. 2013\u20132018.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Applying Deep Learning to Answer Selection: A Study and An Open Task", "author": ["M. Feng", "B. Xiang", "M.R. Glass", "L. Wang", "B. Zhou"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding, 2015, pp. 813\u2013820.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Learning for Answer Sentence Selection", "author": ["L. Yu", "K.M. Hermann", "P. Blunsom", "S. Pulman"], "venue": "Proceedings of the NIPS Deep Learning Workshop, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering", "author": ["D. Wang", "E. Nyberg"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ser. ACL\u201915, 2015, pp. 707\u2013712.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, ser. SIGIR \u201915, 2015, pp. 373\u2013382.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["L. Yu", "K.M. Hermann", "P. Blunsom", "S. Pulman"], "venue": "arXiv preprint arXiv:1412.1632, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["M. Iyyer", "J. Boyd-Graber", "L. Claudino", "R. Socher", "H. Daum\u00e9 III"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 633\u2013644.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Question answering over freebase with multi-column convolutional neural networks", "author": ["L. Dong", "F. Wei", "M. Zhou", "K. Xu"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, 2015, pp. 260\u2013269.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs", "author": ["W. Yin", "H. Sch\u00fctze", "B. Xiang", "B. Zhou"], "venue": "arXiv preprint arXiv:1512.05193, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic parsing for single-relation question answering", "author": ["W.-t. Yih", "X. He", "C. Meek"], "venue": "Proceedings of ACL, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["P. Blunsom", "E. Grefenstette", "N. Kalchbrenner"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions", "author": ["M. Heilman", "N.A. Smith"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, ser. HLT\u201910, 2010, pp. 1011\u20131019.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and Question Answering", "author": ["M. Wang", "C. Manning"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, ser. COLING\u201910, 2010, pp. 1164\u20131172.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic Feature Engineering for Answer Selection and Extraction", "author": ["A. Severyn", "A. Moschitti"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP\u201913, 2013, pp. 458\u2013467.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "LSTM-based Deep Learning Models for Non-factoid Answer Selection", "author": ["M. Tan", "B. Xiang", "B. Zhou"], "venue": "arXiv, vol. arXiv:1511.04108, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Attentive pooling networks", "author": ["C.N. d. Santos", "M. Tan", "B. Xiang", "B. Zhou"], "venue": "CoRR, vol. abs/1602.03609, 2016. [Online]. Available: http://arxiv.org/abs/1602.03609", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Incentivize High Quality Crowdwork", "author": ["C.-J. Ho", "A. Slivkins", "S. Suri", "J.W. Vaughan"], "venue": "Proceedings of the 24th World Wide Web Conference, ser. WWW\u201915, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Proceedings of Advances in Neural Information Processing Systems 26, ser. NIPS\u201913, 2013, pp. 3111\u20133119.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Transition-based Dependency Parsing with Selectional Branching", "author": ["J.D. Choi", "A. McCallum"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ser. ACL\u201913, 2013, pp. 1052\u20131062.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP\u201914, 2014, pp. 1724\u20131734.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["T. Luong", "H. Pham", "C.D. Manning"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, ser. EMNLP\u201915, 2015, pp. 1412\u20131421.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Variational Inference for Text Processing", "author": ["Y. Miao", "L. Yu", "P. Blunsom"], "venue": "arXiv, vol. arXiv:1511.06038, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs", "author": ["W. Yin", "H. Sch\u00fctze", "B. Xiang", "B. Zhou"], "venue": "arXiv, vol. arXiv:1512.05193, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Sentence Similarity Learning by Lexical Decomposition and Composition", "author": ["Z. Wang", "H. Mi", "A. Ittycheriah"], "venue": "arXiv, vol. arXiv:1602.07019, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "In addition, our systems are evaluated on another dataset, WikiQA [2], for a fair comparison to previous work.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 8, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 9, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 10, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 11, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 1, "context": "[2] introduced a lager dataset, WikiQA, consisting of questions collected from the user logs of the Bing search engine.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] distributed another dataset, InsuranceQA, including questions in the insurance", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] proposed a convolutional neural network with a", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 267, "endOffset": 271}, {"referenceID": 13, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 273, "endOffset": 277}, {"referenceID": 14, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 279, "endOffset": 283}, {"referenceID": 15, "context": "Our recurrent neural network models with attention are based on established state-of-the-art systems for answer sentence selection [16], [17].", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "Our recurrent neural network models with attention are based on established state-of-the-art systems for answer sentence selection [16], [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "Following [18], we paid incentives to those who submitted outstanding work, which improved the overall quality of our annotation.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "Our corpus could be compared to WikiQA that was created with the intent of providing a challenging dataset for selectionbased question answering [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "Two models using convolutional neural networks are developed, one is our replication of the best model in [2], and the other is an improved model using subtree matching (Section IV-A).", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "Two more models using recurrent neural networks are developed, one is our replication of the attentive pooling model in [17], and the other is a simpler model using one-way attention (Section IV-B).", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "Our CNN model is motivated by [2].", "startOffset": 30, "endOffset": 33}, {"referenceID": 18, "context": "The image consists of rows standing for consecutive words in two sentences, the question (q) and the answer candidate (a), where the words are represented by their embeddings [19].", "startOffset": 175, "endOffset": 179}, {"referenceID": 1, "context": "Unlike [2] who performed the dot product between these two vectors, we added another hidden layer to learn their weights.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "7We also experimented with the average pooling as [2], which led to a marginally lower accuracy.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "append(fc(s q j , s a k)) end end Sdep[1]\u2190 Sdep[1] + fm(vals) C i \u2190getChildren(D q i ) C i \u2190getChildren(D i ) vals\u2190 [] foreach child cqj in C q i do foreach child ck in C i do vals.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "append(fc(s q j , s a k)) end end Sdep[1]\u2190 Sdep[1] + fm(vals) C i \u2190getChildren(D q i ) C i \u2190getChildren(D i ) vals\u2190 [] foreach child cqj in C q i do foreach child ck in C i do vals.", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "append(fc(c q j , c a k)) end end Sdep[2]\u2190 Sdep[2] + fm(vals) end Algorithm 1: Algorithm of our subtree matching mechanism", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "append(fc(c q j , c a k)) end end Sdep[2]\u2190 Sdep[2] + fm(vals) end Algorithm 1: Algorithm of our subtree matching mechanism", "startOffset": 47, "endOffset": 50}, {"referenceID": 19, "context": "All sentences are automatically parsed by the NLP4J dependency parser [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "Our RNN model is based on the bidirectional Long ShortTerm Memory (LSTM) using attentive pooling introduced by [17], except that our network uses a gated recurrent unit (GRU; [21]) instead of LSTM.", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "Our RNN model is based on the bidirectional Long ShortTerm Memory (LSTM) using attentive pooling introduced by [17], except that our network uses a gated recurrent unit (GRU; [21]) instead of LSTM.", "startOffset": 175, "endOffset": 179}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "We did not use the one-way attention from [16] to avoid deviating the attention mechanism significantly.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "CNN0 is our replication of the best model in [2].", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "[2] - 65.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] - 68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] - 68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] - 69.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] - 70.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Thus, the F1-score on the question level was proposed by [2] as the evaluation for this task, which we follow.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "[2] - - - 27.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "8 RNN1 showed a very similar result to [2], which was surprising since it performed so much better for answer sentence selection.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "On the other hand, RNN1 shows a similar score to [2] as it does on WikiQA.", "startOffset": 49, "endOffset": 52}], "year": 2016, "abstractText": "This paper presents a new selection-based question answering dataset, SelQA. The dataset consists of questions generated through crowdsourcing and sentence length answers that are drawn from the ten most prevalent topics in the English Wikipedia. We introduce a corpus annotation scheme that enhances the generation of large, diverse, and challenging datasets by explictly aiming to reduce word co-occurrences between the question and answers. Our annotation scheme is composed of a series of crowdsourcing tasks with a view to more effectively utilize crowdsourcing in the creation of question answering datasets in various domains. Several systems are compared on the tasks of answer sentence selection and answer triggering, providing strong baseline results for future work to improve upon.", "creator": "LaTeX with hyperref package"}}}