{"id": "1606.08561", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "Estimating the class prior and posterior from noisy positives and unlabeled data", "abstract": "m45 We develop dumber a classification algorithm conakry for estimating posterior innisfallen distributions from 31.19 positive - jonestown unlabeled data, gabab that is 133.5 robust 488,000 to 1204 noise barraud in 77-62 the 243.5 positive labels hardstyle and effective bankverein for formula high - dimensional data. frogging In recent years, carparks several algorithms amplio have korytko been proposed to sidime learn 123-mile from positive - mckart unlabeled mcbay data; warhammer however, many nineteenth-century of senecas these televising contributions remain theoretical, performing jcr poorly 60.63 on real ximena high - kadosh dimensional court data that wrex is gree typically pictum contaminated invitrogen with noise. fritzsch We milov build 105.78 on this previous nicer work to develop two practical 4,590 classification algorithms that corporations explicitly model the ringfort noise rico in the labaton positive condense labels teddybears and utilize lagg-3 univariate akseki transforms built on 6-foot-3 discriminative .383 classifiers. toch We baalbeck prove 46.59 that these univariate uniparental transforms preserve inocentes the class prior, metagenomics enabling textus estimation in pergamino the univariate space and avoiding loyalties kernel density hulu estimation for colley high - dubonnet dimensional spoiler data. The theoretical ebershoff development and odysseus both melcher parametric garotinho and shifters nonparametric semi-fictional algorithms proposed mubarakmand here constitutes promover an important surf step towards people/km wide - satanta spread elected use of 161st robust pedicle classification algorithms for positive - unlabeled dayu data.", "histories": [["v1", "Tue, 28 Jun 2016 05:29:25 GMT  (80kb,D)", "http://arxiv.org/abs/1606.08561v1", null], ["v2", "Tue, 31 Jan 2017 19:25:14 GMT  (81kb,D)", "http://arxiv.org/abs/1606.08561v2", "Fixed a typo in the MSGMM update equations in the appendix. Other minor changes"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shantanu jain", "martha white", "predrag radivojac"], "accepted": true, "id": "1606.08561"}, "pdf": {"name": "1606.08561.pdf", "metadata": {"source": "CRF", "title": "Estimating the class prior and posterior from noisy positives and unlabeled data", "authors": ["Shantanu Jain", "Martha White", "Predrag Radivojac"], "emails": ["predrag}@indiana.edu"], "sections": [{"heading": "1 Introduction", "text": "Access to positive, negative and unlabeled examples is a standard assumption for most semisupervised binary classification techniques. In many domains, however, a sample from one of the classes (say, negatives) may not be available, leading to the setting of learning from positive and unlabeled data (Denis et al., 2005). Positive-unlabeled learning often emerges in sciences and commerce where an observation of a positive example (say, that a protein catalyzes reactions or that a Facebook user likes a particular product) is usually reliable. Here, however, the absence of a positive observation cannot be interpreted as a negative example. In molecular biology, for example, an attempt to label a data point as positive (say, that a protein is an enzyme) may be unsuccessful for a variety of experimental and biological reasons, whereas in social networks an explicit dislike of a product may not be possible. Both scenarios lead to a situation where negative examples cannot be actively collected.\nIf the class priors in the unlabeled data are known, positive-unlabeled learning can straightforwardly translate into learning of non-traditional classifiers (Elkan and Noto, 2008); i.e., classifiers that discriminate between labeled and unlabeled data. These non-traditional classifiers can then be used to compute the outputs of traditional classifiers; i.e., those that discriminate between positive and negative data (Jain et al., 2016). Under mild assumptions, even when the class priors are unknown, there exists a monotonic relationship between the outputs of these classifiers (Jain et al., 2016) and, hence, the models trained for information retrieval and ranking generally do not suffer when trained on labeled vs. unlabeled data. Unfortunately, the monotonic relationship is non-linear, suggesting that in the absence of class prior knowledge, a non-traditional classifier that learns posterior class distributions cannot be used to infer the posterior distributions of the traditional classifiers. Therefore, a fundamental yet ill-posed problem in these situations is that of learning class priors of positive and negative examples in unlabeled data.\nar X\niv :1\n60 6.\n08 56\n1v 1\n[ st\nat .M\nL ]\n2 8\nJu n\nClass prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Jain et al., 2016). Application of these algorithms to real data, however, is limited in that none of the proposed algorithms simultaneously deals with noise in the labels and practical estimation for high-dimensional data. Much of the theory on learning class priors rely on the assumption that either the distribution of positives is known or that the positive sample is clean. In practice, however, labeled data sets contain class-label noise, where an unspecified amount of negative examples contaminates the positive sample. This is a realistic scenario in experimental sciences where technological advances enabled generation of high-throughput data at a cost of occasional errors. One example for this comes from the studies of proteins using analytical chemistry technology; i.e., mass spectrometry. For example, in the process of peptide identification (Steen and Mann, 2004), bioinformatics methods are usually set to report results with specified false discovery rate thresholds (e.g., 1%). Unfortunately, statistical assumptions in these experiments are sometimes violated thereby leading to substantial noise in reported results, as in the case of identifying protein post-translational modifications. Similar amounts of noise might appear in social networks such as Facebook, where some users select \u2018like\u2019, even when they do not actually like a particular post. Further, the only approach that does consider similar such noise (Scott et al., 2013) requires density estimation, which is known to be problematic for high-dimensional data.\nIn this work, we propose the first classification algorithm, with class prior estimation, designed particularly for high-dimensional data with noise in the labeling of positive data. We first formalize the problem of class prior estimation from noisy positive and unlabeled data. We extend the existing identifiability theory for class prior estimation from positive-unlabeled data to this noise setting. We then show that we can practically estimate class priors and the posterior distributions by first transforming the input space to a univariate space, where density estimation is reliable. We prove that these transformations preserve class priors and show that they correspond to training a nontraditional classifier. We derive a parametric algorithm and a nonparametric algorithm to learn the class priors. Finally, we carry out experiments on synthetic and real-life data and provide evidence that the new approaches are sound and effective."}, {"heading": "2 Problem formulation", "text": "Consider a binary classification problem of mapping an input spaceX to an output spaceY = {0, 1}. Let f be the true distribution of inputs. It can be represented as the following mixture\nf(x) = \u03b1f1(x) + (1\u2212 \u03b1)f0(x), (1)\nwhere x \u2208 X , y \u2208 Y , fy are distributions over X for the positive (y = 1) and negative (y = 0) class, respectively; and \u03b1 \u2208 [0, 1) is the class prior or the proportion of the positive examples in f . We will refer to a sample from f as unlabeled data.\nLet now g be the distribution of inputs for the labeled data. Because the labeled sample contains some mislabeled examples, the corresponding distribution is also a mixture of f1 and a small proportion, say 1\u2212 \u03b2, of f0. That is,\ng(x) = \u03b2f1(x) + (1\u2212 \u03b2)f0(x), (2)\nwhere \u03b2 \u2208 (0, 1]. Observe that both mixtures have the same components but different mixing proportions. The simplest scenario is that the mixing components f0 and f1 correspond to the classconditional distributions p(x|Y = 0) and p(x|Y = 1), respectively. However, our approach also permits transformations of the input space X thus resulting in a more general setup. The objective of this work is to study the estimation of the class prior \u03b1 = p(Y = 1) and propose practical algorithms for estimating \u03b1. The efficacy of this estimation is clearly tied to \u03b2, where as \u03b2 gets smaller, the noise in the positive labels becomes larger. We will discuss identifiability of \u03b1 and \u03b2 and give a practical algorithm for estimating \u03b1 (and \u03b2). We will then use these results to estimate the posterior distribution of the class variable, p(y|x), although the labeled set does not contain any negative examples."}, {"heading": "3 Identifiability", "text": "The class prior is identifiable if there is a unique class prior for a given pair (f, g). Much of the identifiability characterization in this section has already been considered as the case of asymmetric noise (Scott et al., 2013); see Section 7 on related work. We recreate these results here, with the aim to introduce required notation, to highlight several important results for later algorithm development and to include a few missing results needed for our approach. Though the proof techniques are themselves quite different and could be of interest, we include them in the appendix due to space.\nThere are typically two aspects to address with identifiability. First, one needs to determine if a problem is identifiable, and, second, if it is not, propose a canonical form that is identifiable. In this section we will see that class prior is not identifiable in general because f0 can be a mixture containing f1 and vice versa. To ensure identifiability, it is necessary to choose a canonical form that prefers a class prior that makes the two components as different as possible; this canonical form was independently introduced as the mutual irreducibility principle (Scott et al., 2013) or the max-canonical form (Jain et al., 2016).\nWe discuss identifiability in terms of measures. Let \u00b5, \u03bd, \u00b50 and \u00b51 be probability measures defined on some \u03c3-algebra A on X , corresponding to f , g, f0 and f1, respectively. It follows that\n\u00b5 = \u03b1\u00b51 + (1\u2212 \u03b1)\u00b50 (3) \u03bd = \u03b2\u00b51 + (1\u2212 \u03b2)\u00b50 (4)\nConsider a family of pairs of mixtures having the same components:\nF(\u03a0) = {(\u00b5, \u03bd) : \u00b5 = \u03b1\u00b51 + (1\u2212 \u03b1)\u00b50, \u03bd = \u03b2\u00b51 + (1\u2212 \u03b2)\u00b50, (\u00b50, \u00b51) \u2208 \u03a0, 0 \u2264 \u03b1 < \u03b2 \u2264 1}, where \u03a0 is some set of pairs of probability measures defined on A. The family is parametrized by the quadruple (\u03b1, \u03b2, \u00b50, \u00b51). The condition \u03b2 > \u03b1 means that \u03bd has a greater proportion of \u00b51 compared to \u00b5. This is consistent with our assumption that the labeled sample mainly contains positives. The most general choice for \u03a0 is\n\u03a0all = Pall \u00d7 Pall \\ { (\u00b5, \u00b5) : \u00b5 \u2208 Pall } ,\nwhere Pall is the set of all probability measures defined on A and { (\u00b5, \u00b5) : \u00b5 \u2208 Pall }\nis the set of pairs that are equal. Removing equal pairs prevents \u00b5 and \u03bd from being identical.\nWe now define the maximum proportion of a component \u03bb1 in a mixture \u03bb, which is used in the results below and to specify the criterion that enables identifiability; more specifically\na\u03bb1\u03bb = max { \u03b1 : \u03bb = \u03b1\u03bb1 + (1\u2212 \u03b1)\u03bb0, \u03bb0 \u2208 Pall, \u03b1 \u2208 [0, 1] } . (5)\nOf particular interest is the case when a\u03bb1\u03bb = 0, which should be read as \u201c\u03bb is not a mixture containing \u03bb1\u201d. We finally define the set all possible (\u03b1, \u03b2) that generate \u00b5 and \u03bd when (\u00b50, \u00b51) varies in \u03a0:\nA+(\u00b5, \u03bd,\u03a0) = {(\u03b1, \u03b2) : \u00b5 = \u03b1\u00b51 + (1\u2212 \u03b1)\u00b50, \u03bd = \u03b2\u00b51 + (1\u2212 \u03b2)\u00b50, (\u00b50, \u00b51) \u2208 \u03a0, 0 \u2264 \u03b1 < \u03b2 \u2264 1}. If A+(\u00b5, \u03bd,\u03a0) is a singleton set for all (\u00b5, \u03bd) \u2208 F(\u03a0), then F(\u03a0) is identifiable in (\u03b1, \u03b2). First, we show that the most general choice for \u03a0, \u03a0all, leads to unidentifiability (Lemma 1). Fortunately, however, by choosing\n\u03a0res = { (\u00b50, \u00b51) \u2208 \u03a0all : a\u00b51\u00b50 = 0, a\u00b50\u00b51 = 0 }\nas \u03a0, we do obtain identifiability (Theorem 1). In words, \u03a0res contains pairs of distributions, where each distribution in a pair cannot be expressed as a mixture containing the other. The proofs of the results below are in the Appendix.\nLemma 1 (Unidentifiability) Given a pair of mixtures (\u00b5, \u03bd) \u2208 F(\u03a0all), let parameters (\u03b1, \u03b2, \u00b50, \u00b51) generate (\u00b5, \u03bd) and \u03b1+ = a\u03bd\u00b5, \u03b2 + = a\u00b5\u03bd . It follows that\n1. There is a one-to-one relation between (\u00b50, \u00b51) and (\u03b1, \u03b2) and\n\u00b50 = \u03b2\u00b5\u2212 \u03b1\u03bd \u03b2 \u2212 \u03b1 , \u00b51 = (1\u2212 \u03b1)\u03bd \u2212 (1\u2212 \u03b2)\u00b5 \u03b2 \u2212 \u03b1 . (6)\n2. Both expressions on the right-hand side of Equation 6 are well defined probability measures if and only if \u03b1/\u03b2 \u2264 \u03b1+ and (1\u2212\u03b2)/(1\u2212\u03b1) \u2264 \u03b2+.\n3. A+(\u00b5, \u03bd,\u03a0all) = {(\u03b1, \u03b2) : \u03b1/\u03b2 \u2264 \u03b1+, (1\u2212\u03b2)/(1\u2212\u03b1) \u2264 \u03b2+}. 4. F(\u03a0all) is unidentifiable in (\u03b1, \u03b2); i.e., (\u03b1, \u03b2) is not uniquely determined from (\u00b5, \u03bd). 5. F(\u03a0all) is unidentifiable in both \u03b1 and \u03b2; i.e., Individually, \u03b1 and \u03b2 are not uniquely\ndetermined from (\u00b5, \u03bd).\nObserve that the definition of a\u03bb1\u03bb and \u00b5 6= \u03bd imply \u03b1+ < 1 and, consequently, any (\u03b1, \u03b2) \u2208 A+(\u00b5, \u03bd,\u03a0all) satisfies \u03b1 < \u03b2, as expected.\nTheorem 1 (Identifiablity) Given (\u00b5, \u03bd) \u2208 F(\u03a0all), let \u03b1+ = a\u03bd\u00b5 and \u03b2+ = a\u00b5\u03bd . Let \u00b5\u22170 = (\u00b5\u2212\u03b1+\u03bd)/(1\u2212\u03b1+), \u00b5\u22171 = (\u03bd\u2212\u03b2 +\u00b5)/(1\u2212\u03b2+) and\n\u03b1\u2217 = \u03b1 +(1\u2212\u03b2+)/(1\u2212\u03b1+\u03b2+), \u03b2\u2217 = (1\u2212\u03b2 +)/(1\u2212\u03b1+\u03b2+). (7)"}, {"heading": "It follows that", "text": "1. (\u03b1\u2217, \u03b2\u2217, \u00b5\u22170, \u00b5 \u2217 1) generate (\u00b5, \u03bd)\n2. (\u00b5\u22170, \u00b5 \u2217 1) \u2208 \u03a0res and consequently, \u03b1\u2217 = a \u00b5\u22171 \u00b5 , \u03b2\u2217 = a \u00b5\u22171 \u03bd .\n3. F(\u03a0res) contains all pairs of mixtures in F(\u03a0all). 4. A+(\u00b5, \u03bd,\u03a0res) = {(\u03b1\u2217, \u03b2\u2217)}. 5. F(\u03a0res) is identifiable in (\u03b1, \u03b2); i.e., (\u03b1, \u03b2) is uniquely determined from (\u00b5, \u03bd).\nWe refer to the expressions of \u00b5 and \u03bd as mixtures of components \u00b50 and \u00b51 as a max-canonical form when (\u00b50, \u00b51) is picked from \u03a0res. This form enforces that \u00b51 is not a mixture containing \u00b50 and vice versa, which leads to \u00b50 and \u00b51 having maximum separation, while still generating \u00b5 and \u03bd. Each pair of distributions in F(\u03a0res) is represented in this form. Identifiability of F(\u03a0res) in (\u03b1, \u03b2), precisely, A+(\u00b5, \u03bd,\u03a0res) = {(\u03b1\u2217, \u03b2\u2217)} tells that (\u03b1\u2217, \u03b2\u2217) is the only pair of mixing proportions that can appear in a max-canonical form of \u00b5 and \u03bd. Moreover, Theorem 1 statement 1 and Lemma 1 statement 1 imply that the max-canonical form is unique and completely specified by (\u03b1\u2217, \u03b2\u2217, \u00b5\u22170, \u00b5 \u2217 1), with \u03b1\n\u2217 < \u03b2\u2217 following from Equation 7. Thus, using F(\u03a0res) to model the unlabeled and labeled data distributions makes estimation of not only \u03b1, the class prior, but also \u03b2, \u00b50, \u00b51 a well-posed problem. Moreover, due to Theorem 1 statement 3, there is no loss in the modeling capability by using F(\u03a0res) instead of F(\u03a0all). Overall, identifiability, absence of loss of modeling capability and maximum separation between \u00b50 and \u00b51 combine to justify estimating \u03b1\u2217 as the class prior."}, {"heading": "4 Univariate Transformation", "text": "The theory and algorithms for class prior estimation are agnostic to the dimensionality of the data; in practice, however, this dimensionality can have important consequences. Parametric Gaussian mixture models trained via expectation-maximization (EM) are known to strongly suffer from colinearity in high-dimensional data. Nonparametric (kernel) density estimation is also known to have curse-of-dimensionality issues, both in theory (Liu et al., 2007) and in practice (Scott, 2008).\nWe address the curse of dimensionality by transforming the data to a single dimension. The transformation \u03c4 : X \u2192 R, surprisingly, is simply an output of a non-traditional classifier trained to separate labeled sample, L, from unlabeled sample, U . The transform is similar to that in (Jain et al., 2016), except that it is not required to be calibrated like a posterior distribution; as shown below, a good ranking function is sufficient. First, however, we introduce notation and formalize the data generation steps (Figure 1).\nLet X be a random variable taking values in X , capturing the true distribution of inputs, \u00b5, and Y be an unobserved random variable taking values in Y , giving the true class of the inputs. It follows that X|Y = 0 and X|Y = 1 are distributed according to \u00b50 and \u00b51, respectively. Let S be a selection random variable, whose value in S = {0, 1, 2} determines the sample to which an input x is added (Figure 1). When S = 1, x is added to the noisy labeled sample; when S = 0, x is added to the unlabeled sample; and when S = 2, x is not added to either of the samples. It follows that\nXu = X|S = 0 and X l = X|S = 1 are distributed according to \u00b5 and \u03bd, respectively. We make the following assumptions which are consistent with the statements above:\np(y|S = 0) = p(y), (8) p(y = 1|S = 1) = \u03b2, (9) p(x|s, y) = p(x|y). (10)\nAssumptions 8 and 9 tell that the proportion of positives in the unlabeled sample and the labeled sample matches the true proportion in \u00b5 and \u03bd, respectively. Assumption 10 tells that the distribution of the positive inputs (and the negative inputs) in both the unlabeled and the labeled samples is equal and unbiased. Lemma 2 gives the implications of these assumptions. Statement 3 is particularly interesting and perhaps counter-intuitive as it tells that with non-zero probability some inputs need to be dropped.\nLemma 2 Let X , Y and S be random variables taking values in X , Y and S, respectively, and Xu = X|S = 0 and X l = X|S = 1. For measures \u00b5, \u03bd, \u00b50, \u00b51, satisfying Equations 3 and 4 and \u00b51 6= \u00b50, let \u00b5, \u00b50, \u00b51 give the distribution of X , X|Y = 0 and X|Y = 1, respectively. If X,Y and S satisfy assumptions 8, 9 and 10, then\n1. X is independent of S = 0; i.e., p(x|S = 0) = p(x) 2. Xu and X l are distributed according to \u00b5 and \u03bd, respectively. 3. p(S = 2) 6= 0.\nThe proof is in the Appendix. Next, we highlight the conditions under which the score function \u03c4 preserves \u03b1\u2217. Observing that S serves as the pseudo class label for labeled vs. unlabeled classification as well, we first give an expression for the posterior:\n\u03c4p(x) = p(S = 1|x, S \u2208 {0, 1}), \u2200x \u2208 X . (11)\nTheorem 2 (\u03b1\u2217-preserving transform) Let random variables X,Y, S,Xu, X l and measures \u00b5, \u03bd, \u00b50, \u00b51 be as defined in Lemma 2. Let \u03c4p be the posterior as defined in Equation 11 and \u03c4 = H \u25e6 \u03c4p, where H is a 1-to-1 function on [0, 1] and \u25e6 is the composition operator. Assume\n1. (\u00b50, \u00b51) \u2208 \u03a0res, 2. Xu, X l are continuous with densities f, g, respectively, 3. \u00b5\u03c4 , \u03bd\u03c4 , \u00b5\u03c41 are the measures corresponding to \u03c4(Xu), \u03c4(X l), \u03c4(X1), respectively, 4. (\u03b1+, \u03b2+, \u03b1\u2217, \u03b2\u2217) = (a\u03bd\u00b5, a \u00b5 \u03bd , a \u00b51 \u00b5 , a \u00b51 \u03bd ) and (\u03b1 + \u03c4 , \u03b2 + \u03c4 , \u03b1 \u2217 \u03c4 , \u03b2 \u2217 \u03c4 ) = (a \u03bd\u03c4 \u00b5\u03c4 , a \u00b5\u03c4 \u03bd\u03c4 , a \u00b5\u03c41 \u00b5\u03c4 , a \u00b5\u03c41 \u03bd\u03c4 ).\nThen (\u03b1+\u03c4 , \u03b2 + \u03c4 , \u03b1 \u2217 \u03c4 , \u03b2 \u2217 \u03c4 ) = (\u03b1 +, \u03b2+, \u03b1\u2217, \u03b2\u2217) and so \u03c4 is an \u03b1\u2217-preserving transformation.\nMoreover, \u03c4p can also be used to compute the true posterior probability:\np(Y = 1|x) = \u03b1 \u2217(1\u2212 \u03b1\u2217) \u03b2\u2217 \u2212 \u03b1\u2217\n( p(S = 0)\np(S = 1)\n\u03c4p(x) 1\u2212 \u03c4p(x) \u2212 1\u2212 \u03b2 \u2217 1\u2212 \u03b1\u2217 ) . (12)\nThe proof is in the Appendix. Theorem 2 shows that using any function \u03c4 , that can be expressed as a composition of a one-to-one function, H , defined on [0, 1], with \u03c4p preserves \u03b1\u2217. Trivially, \u03c4p itself is one such function. We emphasize, however, that \u03b1\u2217-preservation is not limited by the efficacy of the calibration algorithm; uncalibrated scoring that ranks inputs as \u03c4p(x) does, also preserves \u03b1\u2217. Theorem 2 further demonstrates how the true posterior, p(Y = 1|x), can be recovered from \u03c4p by plugging in estimates of \u03c4p, p(S=0)/p(S=1), \u03b1\u2217 and \u03b2\u2217 in Equation 12. The posterior probability \u03c4p can be estimated directly by using a probabilistic classifier or by calibrating a classifier\u2019s score (Platt, 1999; Niculescu-Mizil and Caruana, 2005); |U |/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric approaches for estimation of \u03b1\u2217 and \u03b2\u2217."}, {"heading": "5 Algorithms", "text": "In this section, we derive a parametric and a nonparametric algorithm to estimate \u03b1\u2217 and \u03b2\u2217 from the unlabeled sample, U = {Xui }, and the noisy positive sample, L = {X li}. In theory, both approaches can handle multivariate samples; in practice, however, to circumvent the curse of dimensionality, we exploit the theory of \u03b1\u2217-preserving univariate transforms to transform the samples.\nParametric approach. The parametric approach is derived by modeling each sample as a two component Gaussian mixture, sharing the same components but having different mixing proportions:\nXui \u223c \u03b1N (u1,\u03a31) + (1\u2212 \u03b1)N (u0,\u03a30) X li \u223c \u03b2N (u1,\u03a31) + (1\u2212 \u03b2)N (u0,\u03a30)\nwhere u1, u0 \u2208 Rd and \u03a31,\u03a30 \u2208 Sd++, the set of all d\u00d7d positive definite matrices. The algorithm is an extension to the EM approach for Gaussian mixture models (GMMs) where, instead of estimating the parameters of a single mixture, the parameters of both mixtures (\u03b1, \u03b2, u0, u1,\u03a30,\u03a31) are estimated simultaneously by maximizing the combined likelihood over both U and L. This approach, that we refer to as a multi-sample GMM (MSGMM), exploits the constraint that the two mixtures share the same components. The update rules and their derivation are given in the Appendix.\nNonparametric approach. Our nonparametric strategy directly exploits the results of Lemma 1 and Theorem 1 stating the direct connection between (\u03b1+ = a\u03bd\u00b5, \u03b2 + = a\u00b5\u03bd ) and (\u03b1 \u2217, \u03b2\u2217). Therefore, for a two-component mixture sample, M , and a sample from one of the components, C, it only requires an algorithm to estimate the maximum proportion of C in M . For this purpose, we use the AlphaMax algorithm (Jain et al., 2016), briefly summarized in the Appendix. Specifically, our two-step approach for estimating \u03b1\u2217 and \u03b2\u2217 is as follows: (i) Estimate \u03b1+ and \u03b2+ as outputs of AlphaMax(U,L) and AlphaMax(L,U), respectively; (ii) Estimate (\u03b1\u2217, \u03b2\u2217) from the estimates of (\u03b1+, \u03b2+) by applying Equation 7. We refer to our nonparametric algorithm as AlphaMax-N."}, {"heading": "6 Empirical investigation", "text": "In this section we systematically evaluate the new algorithms in a controlled, synthetic setting as well as on a variety of data sets from the UCI Machine Learning Repository (Lichman, 2013).\nExperiments on synthetic data: We start by evaluating all algorithms in a univariate setting where both mixing proportions, \u03b1 and \u03b2, are known. We generate unit-variance Gaussian and unit-scale Laplace distributed i.i.d. samples and explore impact on the accuracy of estimation of mixing proportion, size of the component sample, and separation and overlap between the mixing components. The class prior \u03b1 was varied from {0.05, 0.25, 0.50, 0.75, 0.95} and the noise component \u03b2 from {1.00, 0.95, 0.75}. The size of the labeled sample L was varied from {100, 1000}, whereas the size of the unlabeled sample U was fixed at 10000.\nExperiments on real-life data: We considered twelve real-life data sets from the UCI Machine Learning Repository. To adjust these data to our problems, categorical features were transformed into numerical using sparse binary representation, the regression data sets were transformed into classification based on mean of the target variable, and the multi-class classification problems were converted into binary problems by combining classes. In each data set, a subset of positive and negative examples was randomly selected to provide a labeled sample while the remaining data (without class labels) were used as unlabeled data. The size of the labeled sample was kept at 1000 (or 100 for small data sets) and the maximum size of unlabeled data was set 10000.\nAlgorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm (Elkan and Noto, 2008) as well as the noiseless version of AlphaMax (Jain et al., 2016). There are several versions of the Elkan-Noto estimator and each can use any underlying classifier. We used the e1 alternative estimator combined with the ensembles of 100 two-layer feed-forward neural networks, each with five hidden units. This same classifier was used as a class-prior preserving transformation that created an input to the AlphaMax algorithms. The algorithm proposed by du Plessis and Sugiyama (2014) minimizes the same objective as the e1 Elkan-Noto estimator and, thus, was not implemented. It is important to mention that neither Elkan-Noto nor AlphaMax algorithms were developed to handle noisy labeled samples. In addition, the theory behind the Elkan-Noto estimator restricts its use to class-conditional distributions with non-overlapping supports.\nEvaluation: All experiments were repeated 50 times to be able to draw conclusions with statistical significance. In real-life data, the labeled sample was created randomly by choosing an appropriate number of positive and negative examples to satisfy the condition for \u03b2 and the size of the labeled sample, while the remaining data was used to determine \u03b1 in the unlabeled sample. Therefore, the class prior in the unlabeled data varies with the selection of the noise parameter \u03b2. The mean absolute difference between the true and estimated class priors was used as a performance measure. The best performing algorithm on each data set was determined by multiple hypothesis testing using the P-value of 0.05 and Bonferroni correction.\nResults: The comprehensive results for synthetic data drawn from univariate Gaussian and Laplace distributions are shown in Appendix (Table 2). In these experiments no transformation was applied prior to running any of the algorithms. As expected, the results show excellent performance of the MSGMM model on the Gaussian data. These results significantly degrade on Laplace-distributed data, suggesting sensitivity to the underlying assumptions. On the other hand, AlphaMax-N was accurate over all data sets and also robust to noise. These results suggest that new parametric and nonparametric algorithms perform well in these controlled settings.\nTable 1 shows the results on twelve real data sets. Here, AlphaMax and AlphaMax-N algorithms demonstrate significant robustness to noise, although the parametric version MSGMM was competitive in some cases. On the other hand, the Elkan-Noto algorithm expectedly degrades with noise. Finally, we investigated the practical usefulness of the \u03b1\u2217-preserving transform. Table 3 (Appendix) shows the results of AlphaMax-N and MSGMM on the real data sets, with and without using the transform. Because of computational and numerical issues, we reduced the dimensionality by using principal component analysis (the original data caused matrix singularity issues for MSGMM and density estimation issues for AlphaMax-N). MSGMM deteriorates significantly without the transform, whereas AlphaMax-N preserves some signal for the class prior. AlphaMax-N with the transform, however, shows superior performance on most data sets."}, {"heading": "7 Related work", "text": "Class prior estimation in a semi-supervised setting including positive-unlabeled learning, has been extensively discussed previously; see Saerens et al. (2002); Cortes et al. (2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al. (2016) and references therein. Recently, a general setting for label noise has also been introduced, called the mutual contamination model. The aim under this model is to estimate multiple unknown base distributions, using multiple random samples that are composed of different convex combinations of those base distributions (Katz-Samuels and Scott, 2016). The setting of asymmetric label noise is a subset of this more general setting, treated under general conditions by Scott et al. (2013), and previously investigated under a more restrictive setting as co-training (Blum and Mitchell, 1998). A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise. Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class. As the most related work, though Scott et al. (2013) did not explicitly treat the positive-unlabeled learning with noisy positives, their formulation can incorporate this setting by using \u03c00 = \u03b1 and \u03b2 = 1 \u2212 \u03c01. The theoretical and algorithmic treatment, however, is very different. Their focus is on identifiability and analyzing convergence rates and statistical properties, assuming access to some \u03ba\u2217 function which can obtain proportions\nbetween samples. They do not explicitly address issues with high-dimensional data, nor focus on algorithms to obtain \u03ba\u2217. In contrast, we focus primarily on the univariate transformation to handle high-dimensional data and practical algorithms for estimating \u03b1\u2217. Supervised learning used for class prior-preserving transformation provides a rich set of techniques to address high-dimensional data."}, {"heading": "8 Conclusion", "text": "In this paper, we developed a practical algorithm for classification of positive-unlabeled data with noise in the labeled data set. In particular, we focused on a strategy for high-dimensional data, providing a univariate transform that reduces the dimension of the data, preserves the class prior so that estimation in this reduced space remains valid and is then further useful for classification. This approach provides a simple algorithm that simultaneously improves estimation of the class prior and provides a resulting classifier. We derived a parametric and a nonparametric algorithm and then demonstrated performance on a wide variety of learning scenarios and data sets. To the best of our knowledge, this algorithm represents one of the first practical and easy-to-use approaches to learning with high-dimensional positive-unlabeled data with noise in the labels."}, {"heading": "It follows that", "text": "1. (\u03b1\u2217, \u03b2\u2217, \u00b5\u22170, \u00b5 \u2217 1) generate (\u00b5, \u03bd)\n2. (\u00b5\u22170, \u00b5 \u2217 1) \u2208 \u03a0res and consequently, \u03b1\u2217 = a \u00b5\u22171 \u00b5 , \u03b2\u2217 = a \u00b5\u22171 \u03bd .\n3. F(\u03a0res) contains all pairs of mixtures in F(\u03a0all). 4. A+(\u00b5, \u03bd,\u03a0res) = {(\u03b1\u2217, \u03b2\u2217)}. 5. F(\u03a0res) is identifiable in (\u03b1, \u03b2); i.e., (\u03b1, \u03b2) is uniquely determined from (\u00b5, \u03bd).\nProof: Throughout the proof, we use an alternate characterization of a\u03bb1\u03bb given by Theorem 3 (statement 1) as\na\u03bb1\u03bb = inf R(\u03bb, \u03bb1),\nwhere R(\u03bb, \u03bb1) = {\u03bb(A)/\u03bb1(A) : A \u2208 A, \u03bb1(A) > 0}. Statement 1: First, we show that \u00b5\u22170, \u00b5 \u2217 1 are well defined probability measure. \u03b1+ 6= 1, \u03b2+ 6= 1: Suppose \u03b1+ = 1. It follows that \u00b5 = \u03bd and consequently, from Equation 6, \u00b50 = \u00b51 = \u03bd. However, \u00b50 6= \u00b51 because they are picked from \u03a0all. Thus \u03b1+ 6= 1 by contradiction. Similarly \u03b2+ 6= 1. Thus the denominator in R.H.S of \u00b5\u22170 and \u00b5\u22171 is not 0. Probability measure: By definition \u03b1+ = inf R(\u00b5, \u03bd). Thus \u03b1+ \u2264 \u00b5(A)/\u03bd(A) when \u03bd(A) > 0. Consequently, \u00b5(A) \u2212 \u03b1+\u03bd(A) \u2265 0. The inequality is trivially true when \u03bd(A) = 0. Thus \u00b5(A) \u2212 \u03b1+\u03bd(A) \u2265 0 for all A \u2208 A. Hence \u00b5\u22170 is a probability measure. Similarly \u00b5\u22171 is also a probability measure. Second, we show that (\u03b1\u2217, \u03b2\u2217, \u00b5\u22170, \u00b5 \u2217 1) generate \u00b5, \u03bd. (\u03b1\u2217, \u03b2\u2217, \u00b5\u22170, \u00b5 \u2217 1) \u2192 (\u00b5, \u03bd): Observe that \u00b5\u22170, \u00b5\u22171 can also be expressed as \u00b5\u22170 = \u03b2\u2217\u00b5\u2212\u03b1\u2217\u03bd \u03b2\u2217\u2212\u03b1\u2217 and \u00b5\u22171 = (1\u2212\u03b1\u2217)\u03bd\u2212(1\u2212\u03b2\u2217)\u00b5\n\u03b2\u2217\u2212\u03b1\u2217 . Moreover, after some algebraic manipulation of equations labeled 7, \u03b1\u2217/\u03b2\u2217 = \u03b1+ and (1\u2212\u03b2\u2217)/(1\u2212\u03b1\u2217) = \u03b2+ can be derived. Thus, from Lemma 1 (statements 1 and 3), (\u03b1\u2217, \u03b2\u2217, \u00b5\u22170, \u00b5 \u2217 1) generate \u00b5, \u03bd. Statement 2: a \u00b5\u22170 \u00b5\u22171 = 0 : Suppose, for some > 0 and\n\u03bd(A)\u2212 \u03b2+\u00b5(A) \u2265 (\u00b5(A)\u2212 \u03b1+\u03bd(A)) (for all A \u2208 A) \u21d2 (1 + \u03b1+)\u03bd(A) \u2265 ( + \u03b2+)\u00b5(A) \u21d2 \u03bd(A)/\u00b5(A) \u2265 ( +\u03b2+)/(1+ \u03b1+) (when \u00b5(A) > 0)\nThus ( +\u03b2+)/(1+ \u03b1+) is a lower bound toR(\u03bd, \u00b5) and consequently, \u03b2+ \u2265 ( +\u03b2+)/(1+ \u03b1+). However, because \u03b1+\u03b2+ < 1, ( +\u03b2+)/(1+ \u03b1+) > ( \u03b1+\u03b2++\u03b2+)/(1+ \u03b1+) = \u03b2+. This is a contradiction. Thus for all > 0 there exists some A \u2208 A such that \u03bd(A ) \u2212 \u03b2+\u00b5(A ) < (\u00b5(A ) \u2212 \u03b1+\u03bd(A )). We now divide the inequality on both sides by \u00b5(A ) \u2212 \u03b1+\u03bd(A ), observing that the divisor is strictly greater than 0 because \u03bd(A )\u2212 \u03b2+\u00b5(A ) \u2265 0 as \u00b5\u22171 is a probability measure. Thus\n(\u03bd(A )\u2212\u03b2+\u00b5(A ))/(\u00b5(A )\u2212\u03b1+\u03bd(A )) <\n\u21d2 \u00b5\u22171(A )/\u00b5\u22170(A ) < (1\u2212\u03b1+)/(1\u2212\u03b2+). Because the choice of is arbitrary and \u03b2+ 6= 1, any lower bound of R(\u00b5\u22171, \u00b5\u22170) cannot be greater than 0. Thus a\u00b5 \u2217 0\n\u00b5\u22171 = 0.\na \u00b5\u22171 \u00b5\u22170 = 0 : Suppose, for some > 0\n\u00b5(A)\u2212 \u03b1+\u03bd(A) \u2265 (\u03bd(A)\u2212 \u03b2+\u00b5(A)) (for all A \u2208 A) \u21d2 (1 + \u03b2+)\u00b5(A) \u2265 ( + \u03b1+)\u03bd(A) \u21d2 \u00b5(A)/\u03bd(A) \u2265 ( +\u03b1+)/(1+ \u03b2+) (when \u03bd(A) > 0)\nThus ( +\u03b1+)/(1+ \u03b2+) is a lower bound toR(\u00b5, \u03bd) and consequently, \u03b1+ \u2265 ( +\u03b1+)/(1+ \u03b2+). However, because \u03b1+\u03b2+ < 1, ( +\u03b1+)/(1+ \u03b2+) > ( \u03b2+\u03b1++\u03b1+)/(1+ \u03b2+) = \u03b1+. This is a contradiction. Thus for all > 0 there exists some A \u2208 A such that \u00b5(A ) \u2212 \u03b1+\u03bd(A ) < (\u03bd(A ) \u2212 \u03b2+\u00b5(A )). We now divide the inequality on both sides by \u03bd(A ) \u2212 \u03b2+\u00b5(A ), observing that the divisor is strictly greater than 0 because \u00b5(A )\u2212 \u03b1+\u03bd(A ) \u2265 0 as \u00b5\u22170 is a probability measure. Thus\n(\u00b5(A )\u2212\u03b1+\u03bd(A ))/(\u03bd(A )\u2212\u03b2+\u00b5(A )) <\n\u21d2 \u00b5\u22170(A )/\u00b5\u22171(A ) < (1\u2212\u03b2+)/(1\u2212\u03b1+).\nBecause the choice of is arbitrary and \u03b1+ 6= 1, any lower bound of R(\u00b5\u22170, \u00b5\u22171) cannot be greater than 0. Thus a\u00b5 \u2217 1\n\u00b5\u22170 = 0.\nSummarizing, a\u00b5 \u2217 0\n\u00b5\u22171 = 0 and a\u00b5\n\u2217 1\n\u00b5\u22170 = 0; consequently, (\u00b5\u22170, \u00b5 \u2217 1) \u2208 \u03a0res. Because \u00b5\u22170 is not a mixture\ncontaining \u00b5\u22171 and \u00b5 = \u03b1 \u2217\u00b5\u22171 + (1 \u2212 \u03b1\u2217)\u00b5\u22170, \u03b1\u2217 = a \u00b5\u22171 \u00b5 from Theorem 3 (statement 2). Similarly, \u03b2\u2217 = a \u00b5\u22171 \u03bd . This concludes the proof of statement 2. Statement 3: Because statements 1 and 2 are true for any (\u00b5, \u03bd) \u2208 F(\u03a0all), statement 3 is true. Statement 4: It follows from statements 1 and 2, that (\u03b1\u2217, \u03b2\u2217) \u2208 A+(\u00b5, \u03bd,\u03a0res). To show that A+(\u00b5, \u03bd,\u03a0res) contains no other element, we give a proof by contradiction. Suppose (\u03b1, \u03b2) \u2208 A+(\u00b5, \u03bd,\u03a0res) and (\u03b1, \u03b2) 6= (\u03b1\u2217, \u03b2\u2217). Let (\u03b1, \u03b2, \u00b50, \u00b51) generate (\u00b5, \u03bd), for some (\u00b50, \u00b51) \u2208 \u03a0res. First, we show that \u03b1+ = \u03b1/\u03b2 and \u03b2+ = (1\u2212\u03b2)/(1\u2212\u03b1): \u03b1+ = \u03b1/\u03b2: Suppose for some 0 < < (\u03b2\u2212\u03b1)/\u03b2(1\u2212\u03b2) and all A \u2208 A where \u03bd(A) > 0,\n\u00b5(A)/\u03bd(A) \u2265 \u03b1/\u03b2 +\n\u21d2 \u03b1+ (1\u2212 \u03b1) \u00b50(A)/\u00b51(A) \u03b2 + (1\u2212 \u03b2)\u00b50(A)/\u00b51(A) \u2265 \u03b1+ \u03b2 \u03b2\n\u21d2 \u00b50(A)/\u00b51(A) \u2265 \u03b22 /(\u03b2\u2212\u03b1\u2212 \u03b2(1\u2212\u03b2)) \u21d2 a\u00b51\u00b50 \u2265 \u03b2 2 /(\u03b2\u2212\u03b1\u2212 \u03b2(1\u2212\u03b2)) > 0\nHowever, this is a contradiction because (\u00b50, \u00b51) \u2208 \u03a0res. Thus for every 0 < < (\u03b2\u2212\u03b1)/\u03b2(1\u2212\u03b2) there exists some A \u2208 A with \u03bd(A ) > 0 such that \u00b5(A )/\u03bd(A ) < \u03b1/\u03b2 + . Thus \u03b1+ < \u03b1/\u03b2 + , Because can be made arbitrarily small, \u03b1+ \u2264 \u03b1/\u03b2. However, because A+(\u00b5, \u03bd,\u03a0res) \u2286 A+(\u00b5, \u03bd,\u03a0all), (\u03b1, \u03b2) also belongs to A+(\u00b5, \u03bd,\u03a0all) and consequently \u03b1+ \u2265 \u03b1/\u03b2 from Lemma 1 (statement 3). Thus \u03b1+ = \u03b1/\u03b2. \u03b2+ = (1\u2212\u03b2)/(1\u2212\u03b1): The proof is similar to \u03b1+ = \u03b1/\u03b2 \u2014supposing \u03bd(A)/\u00b5(A) \u2265 (1\u2212\u03b2)/(1\u2212\u03b1)+ for some > 0 and all A \u2208 A with \u00b5(A) > 0, reaching a contradiction and following the subsequent steps. \u03b1+ = \u03b1/\u03b2, \u03b2+ = (1\u2212\u03b2)/(1\u2212\u03b1) and Equation 7 implies \u03b1 = \u03b1\u2217 and \u03b2 = \u03b2\u2217, which contradicts our assumption. Hence (\u03b1\u2217, \u03b2\u2217) is the only element in A+(\u00b5, \u03bd,\u03a0res), which proves statement 4. Statement 5: Statement 5 follows by observing that A+(\u00b5, \u03bd,\u03a0res) is a singleton set.\nLemma 2 Let X , Y and S be random variables taking values in X , Y and S , respectively, and Xu = X|S = 0 and X l = X|S = 1. For measures \u00b5, \u03bd, \u00b50, \u00b51, satisfying Equations 3 and 4 and \u00b51 6= \u00b50, let \u00b5, \u00b50, \u00b51 give the distribution of X , X|Y = 0 and X|Y = 1, respectively. If X,Y and S satisfy assumptions 8, 9 and 10, then\n1. X is independent of S = 0; i.e., p(x|S = 0) = p(x) 2. Xu and X l are distributed according to \u00b5 and \u03bd, respectively. 3. p(S = 2) 6= 0.\nProof: Observe that\np(x|S = 0) = p(x, Y = 1|S = 0) + p(x, Y = 0|S = 0) = p(Y = 1|S = 0)p(x|Y = 1, s = 0) + p(Y = 0|S = 0)p(x|Y = 0, s = 0) = p(Y = 1)p(x|Y = 1) + p(Y = 0)p(x|Y = 0) (from assumptions 8 and 10) = p(x).\nThus X is independent of S = 0. This proves statement 1. From statement 1, Xu has the same distribution as X , which is \u00b5. Now,\np(x|S = 1) = p(x, Y = 1|S = 1) + p(x, Y = 0|S = 1) = p(Y = 1|S = 1)p(x|Y = 1, s = 1) + p(Y = 0|S = 1)p(x|Y = 0, s = 1) = \u03b2p(x|Y = 1) + (1\u2212 \u03b2)p(x|Y = 0). (from assumptions 9 and 10)\nThus the distribution of X|S = 1 is \u03b2\u00b51 + (1\u2212 \u03b2)\u00b50, which is \u03bd. This proves statement 2. Now,\np(S = 2|x) = 1\u2212 p(S = 0|x)\u2212 p(S = 1|x)\n= 1\u2212 p(S = 0)\u2212 p(x|S = 1) p(x) p(S = 1) (because S = 0 and X are independent)\nThe probability p(S = 2|x) is independent of x only if p(x|S=1)/p(x) is a constant with respect to x. Let p(x|S=1)/p(x) = c, where c is some constant. Integrating over x on both sides gives \u222b X p(x|S =\n1)dx = c \u222b X p(x)dx. Since both integrals are 1, it follows that c = 1. Thus p(x|S=1)/p(x) = 1, which implies \u00b5 = \u03bd, i.e., the labeled and unlabeled samples have the same distribution. However, this implies \u00b51 = \u00b50, which contradicts the assumption. Therefore, S = 2 is not independent of X .\nTheorem 2 (\u03b1\u2217-preserving transform) Let random variables X,Y, S,Xu, X l and measures \u00b5, \u03bd, \u00b50, \u00b51 be as defined in Lemma 2. Let \u03c4p be the posterior as defined in Equation 11 and \u03c4 = H \u25e6 \u03c4p, where H is a 1-to-1 function on [0, 1] and \u25e6 is the composition operator. Assume\n1. (\u00b50, \u00b51) \u2208 \u03a0res, 2. Xu, X l are continuous with densities f, g, respectively, 3. \u00b5\u03c4 , \u03bd\u03c4 , \u00b5\u03c41 are the measures corresponding to \u03c4(Xu), \u03c4(X l), \u03c4(X1), respectively, 4. (\u03b1+, \u03b2+, \u03b1\u2217, \u03b2\u2217) = (a\u03bd\u00b5, a \u00b5 \u03bd , a \u00b51 \u00b5 , a \u00b51 \u03bd ) and (\u03b1 + \u03c4 , \u03b2 + \u03c4 , \u03b1 \u2217 \u03c4 , \u03b2 \u2217 \u03c4 ) = (a \u03bd\u03c4 \u00b5\u03c4 , a \u00b5\u03c4 \u03bd\u03c4 , a \u00b5\u03c41 \u00b5\u03c4 , a \u00b5\u03c41 \u03bd\u03c4 ).\nThen (\u03b1+\u03c4 , \u03b2 + \u03c4 , \u03b1 \u2217 \u03c4 , \u03b2 \u2217 \u03c4 ) = (\u03b1 +, \u03b2+, \u03b1\u2217, \u03b2\u2217) and so \u03c4 is an \u03b1\u2217-preserving transformation.\nMoreover, \u03c4p can also be used to compute the true posterior probability:\np(Y = 1|x) = \u03b1 \u2217(1\u2212 \u03b1\u2217) \u03b2\u2217 \u2212 \u03b1\u2217\n( p(S = 0)\np(S = 1)\n\u03c4p(x) 1\u2212 \u03c4p(x) \u2212 1\u2212 \u03b2 \u2217 1\u2212 \u03b1\u2217 ) . (12)\nProof: First we prove that (\u03b1+, \u03b2+) = (\u03b1+\u03c4 , \u03b2+\u03c4 ). To this end, we expand \u03c4p(x) as follows\n\u03c4p(x) = p(S = 1, x, S \u2208 {0, 1})\np(x, S \u2208 {0, 1})\n= p(S = 1, x)\np(x, S = 0) + p(x, S = 1)\n= p(x|S = 1)p(S = 1)\np(x|S = 0)p(S = 0) + p(x|S = 1)p(S = 1) (13)\n= p(S = 1)\np(x|S=0) p(x|S=1)p(S = 0) + p(S = 1)\n= p(S = 1)\nf(x) g(x)p(S = 0) + p(S = 1)\n; (14)\nthe last step is justified because f(x) = p(X = x|S = 0) and g(x) = p(X = x|S = 1). For oneto-one functions G1(t) = p(S=1) tp(S=0)+p(S=1) and G2(t) =\np(S=1) 1 t p(S=0)+p(S=1) defined on R+ \u222a {0,\u221e}, we apply Theorem 4 with\n\u2022 Xu, X l and H \u25e6G1 playing the role of X , X1 and G, respectively. Now, \u03c4 = H \u25e6 \u03c4p\n= H \u25e6G1 \u25e6 \u03c4d (from Equation 14) Because H and G1 are both one-to-one functions, H \u25e6 G1 is one-to-one as well. Thus all the conditions of Theorem 4 is satisfied and consequently, \u03b1+ = \u03b1+\u03c4 .\n\u2022 Xu, X l and H \u25e6G2 playing the role of X1, X and G, respectively. Now, \u03c4 = H \u25e6 \u03c4p\n= H \u25e6G2 \u25e6 \u03c4d (from Equation 14) Because H and G2 are both one-to-one functions, H \u25e6 G2 is one-to-one as well. Thus all the conditions of Theorem 4 is satisfied and consequently, \u03b2+ = \u03b2+\u03c4 .\nNow, from Theorem 1 statement 2 and Equation 7\n\u03b1\u2217 = \u03b1 +(1\u2212\u03b2+)/(1\u2212\u03b1+\u03b2+)\n\u03b2\u2217 = (1\u2212\u03b2 +)/(1\u2212\u03b1+\u03b2+)\n\u03b1\u2217\u03c4 = \u03b1 + \u03c4 (1\u2212\u03b2 + \u03c4 )/(1\u2212\u03b1+\u03c4 \u03b2 + \u03c4 ) \u03b2\u2217\u03c4 = (1\u2212\u03b2 + \u03c4 )/(1\u2212\u03b1+\u03c4 \u03b2 + \u03c4 ).\nand thus (\u03b1\u2217\u03c4 , \u03b2 \u2217 \u03c4 ) = (\u03b1 \u2217, \u03b2\u2217).\nNext we prove Equation 12. Let c = p(S=1)/p(S=0). Rearranging the terms of Equation 13,\n\u03c4(x) 1\u2212 \u03c4(x) = p(x|S = 1)p(S = 1) p(x|S = 0)p(S = 0)\n= p(S = 1)\np(S = 0)\n( p(x, Y = 1|S = 1) + p(x, Y = 1|Y = 0)\np(x)\n) (from Lemma 2 statement 1)\n= p(S = 1)\np(S = 0)\n( p(Y = 1|S = 1)p(x|Y = 1, S = 1)\np(x) + p(Y = 0|S = 1)p(x|Y = 0, S = 1)) p(x) ) = p(S = 1)\np(S = 0)\n( p(Y = 1|S = 1)p(x|Y = 1)\np(x) + p(Y = 0|S = 1)p(x|Y = 0) p(x) ) (from assumption 10)\n= 1\nc\n( p(Y = 1|S = 1)\np(Y = 1) p(Y = 1|x) + p(Y = 0|S = 1) p(Y = 0) p(Y = 0|x) ) = 1\nc\n( \u03b2\u2217\n\u03b1\u2217 p(Y = 1|x) + 1\u2212 \u03b2 \u2217 1\u2212 \u03b1\u2217 (1\u2212 p(Y = 1|x)) )\n= 1\nc ( 1\u2212 \u03b2\u2217 1\u2212 \u03b1\u2217 + ( \u03b2\u2217 \u03b1\u2217 \u2212 1\u2212 \u03b2 \u2217 1\u2212 \u03b1\u2217 ) p(Y = 1|x) ) .\nRearranging the terms,\np(Y = 1|x) = \u03b1 \u2217(1\u2212 \u03b1\u2217) \u03b2\u2217 \u2212 \u03b1\u2217\n( c\u03c4(x)\n1\u2212 \u03c4(x) \u2212 1\u2212 \u03b2\u2217 1\u2212 \u03b1\u2217\n) .\nTheorem 3 Let \u00b5, \u00b51 and \u00b50 be three measures defined on A such that \u00b5 = \u03b1\u00b51 + (1 \u2212 \u03b1)\u00b50 for some \u03b1 \u2208 [0, 1]. Define\nR(\u00b5, \u00b51) = {\u00b5(A)/\u00b51(A) : A \u2208 A, \u00b51(A) > 0}."}, {"heading": "It follows that", "text": "1. a\u00b51\u00b5 = inf R(\u00b5, \u00b51).\n2. If a\u00b51\u00b50 = 0, then \u03b1 = a \u00b51 \u00b5 .\nProof: The proof follows from Lemma 4 and Theorem 3 of Jain et al. (2016).\nTheorem 4 (Restatement of Theorem 9 in Jain et al. (2016)) LetX andX1 be random variables with densities f and f1 and measures \u00b5 and \u00b51 respectively. For R + = R+\u222a{0,\u221e} and an abstract space X\u03c4 , given any one-to-one function G : R + \u2192 X\u03c4 , define function \u03c4 : X \u2192 X\u03c4\n\u03c4 = G \u25e6 \u03c4d,\nwhere\n\u03c4d(x) = { f(x)/f1(x) if f1(x) > 0 \u221e if f1(x) = 0.\nLet \u00b5\u03c4 and \u00b5\u03c41 be the measures for the random variables \u03c4(X), \u03c4(X1) respectively for \u03c3-algebra"}, {"heading": "A\u03c4 on X\u03c4 . Then a\u00b51\u00b5 = a\u00b5\u03c41\u00b5\u03c4 .", "text": ""}, {"heading": "B MSGMM", "text": "Let U = {Xui } and L = {X li} be the unlabeled sample and the noisy positive sample, respectively. The parametric approach is derived by modeling each sample as a two component Gaussian mixture, sharing the same components but having different mixing proportions:\nXui \u223c \u03b1N (u1,\u03a31) + (1\u2212 \u03b1)N (u0,\u03a30) X li \u223c \u03b2N (u1,\u03a31) + (1\u2212 \u03b2)N (u0,\u03a30)\nwhere u1, u0 \u2208 Rd and \u03a31,\u03a30 \u2208 Sd++, the set of all d\u00d7 d positive definite matrices. The algorithm is an extension to the EM approach for Gaussian mixture models (GMMs) where, instead of estimating the parameters of a single mixture, the parameters of both mixtures (\u03b1, \u03b2, u0, u1,\u03a30,\u03a31) are estimated simultaneously by maximizing the combined likelihood over both U and L. This approach, that we refer to as a multi-sample GMM (MSGMM), exploits the constraint that the two mixtures share the same components. To derive the update equations, we introduce missing variables Wui ,W l j that give the true class of the ith and jth example in U and L, respectively. The variables Wui ,W l j are Bernoulli distributed; i.e., Wui \u223c Bernoulli(\u03b1) and W lj \u223c Bernoulli(\u03b2). For\nW = {Wui }|U |i=1, V = {W lj} |L| j=1\nthe quartet (U,L,W, V ) forms the observed and unobserved variables in the EM framework. The complete data log-likelihood, llC is given by,\nllC = \u2211|U | i=1W u i log[\u03b1\u03c61(x u i )] + (1\u2212Wui ) log[(1\u2212 \u03b1)\u03c60(xui )]\n+ \u2211|L| i=1W l i log [ \u03b2\u03c61(x l i) ] + (1\u2212W li ) log [ (1\u2212 \u03b2)\u03c60(xli) ] ,\nwhere \u03c6i is the density of N (ui,\u03a3i). Our goal is to maximize E[llC ]. To do so we take the conditional expectation of llC with respect to W and V given U and L. For\nw\u0304ui = E[W u i |Xui = xui ] =\n\u03b1\u03c61(x u i )\n\u03b1\u03c61(xui ) + (1\u2212\u03b1)\u03c60(x u i ) ,\nw\u0304li = E[W l i |X li = xli] =\n\u03b2\u03c61(x l i)\n\u03b2\u03c61(xli) + (1\u2212\u03b2)\u03c60(x l i) ,\nwe obtain\nE[llC ] = |U |\u2211 i=1 w\u0304ui log[\u03b1\u03c61(x u i )] + (1\u2212 w\u0304ui ) log[(1\u2212 \u03b1)\u03c60(xui )]\n+ |L|\u2211 i=1 w\u0304li log [ \u03b2\u03c61(x l i) ] + (1\u2212w\u0304li) log [ (1\u2212\u03b2)\u03c60(xli) ]\nwhich up to constants, which are ignored in the optimization, can be explicitly written as\nE[llC ] = |U |\u2211 i=1 w\u0304ui [ log\u03b1\u2212 1 2 log |\u03a31|\u2212(xui \u2212u1)T\u03a3\u221211 (xui \u2212u1) ]\n+ |U |\u2211 i=1 (1\u2212w\u0304ui ) [ log(1\u2212\u03b1)\u2212 1 2 log |\u03a30|\u2212(xui \u2212u0)T\u03a3\u221210 (xui \u2212u0) ]\n+ |L|\u2211 i=1 w\u0304li [ log \u03b2\u2212 1 2 log |\u03a31|\u2212(xli\u2212u1)T\u03a3\u221211 (xli\u2212u1) ]\n+ |L|\u2211 i=1 (1\u2212w\u0304li) [ log(1\u2212\u03b2)\u2212 1 2 log |\u03a30|\u2212(xli\u2212u0)T\u03a3\u221210 (xli\u2212u0) ] .\nFinally, we get the parameter update equations by maximizing E[llC ] with respect to (\u03b1, \u03b2, u0, u1,\u03a30,\u03a31):\n\u03b1\u2190 1/|U |\u2211|U |i=1 w\u0304ui \u03b2 \u2190 1/|L|\u2211|L|j=1 w\u0304lj u1 \u2190 \u2211|U | i=1 w\u0304 u i x u i + \u2211|L| j=1 w\u0304 l jx l j\u2211|U |\ni=1 w\u0304 u i + \u2211|L| j=1 w\u0304 l j\nu0 \u2190 \u2211|U | i=1(1\u2212 w\u0304ui )xui + \u2211|L| j=1(1\u2212 w\u0304lj)xlj\u2211|U |\ni=1(1\u2212 w\u0304ui ) + \u2211|L| j=1(1\u2212 w\u0304lj)\n\u03a31 \u2190 \u2211|U | i=1 w\u0304 u i (x u i \u2212 u0)(xui \u2212 u0)T + \u2211|L| j=1 w\u0304 l j(x l j \u2212 u0)(xlj \u2212 u0)T\u2211|U |\ni=1 w\u0304 u i + \u2211|L| j=1 w\u0304 l j\n\u03a30 \u2190 \u2211|U | i=1(1\u2212 w\u0304ui )(xui \u2212 u0)(xui \u2212 u0)T + \u2211|L| j=1(1\u2212 w\u0304lj)(xlj \u2212 u0)(xlj \u2212 u0)T\u2211|U |\ni=1(1\u2212 w\u0304ui ) + \u2211|L| j=1(1\u2212 w\u0304lj)\nThe update rules reduce to the standard GMM when the labeled sample is not provided. Further generalization to more than two samples and/or mixing components is straightforward."}, {"heading": "C AlphaMax", "text": "For a mixture sample M and a component sample C AlphaMax(M,C) estimates the maximum proportion of C in M . AlphaMax is based on the constrained maximization of the log likelihood of samples M and C, derived using nonparametric estimates of their densities m and c, respectively. We list the main steps of AlphaMax below.\n1. Estimate c nonparameterically as c\u0302 using sample C. Obtain the weights, vi, and components \u03bai from nonparametric density estimation of m as a k-component mixture, m\u0302(x) =\u2211k i=1 vi\u03bai(x), using M .\n2. Construct two density functions c\u0303(\u00b7|\u03c9) and m\u0303(\u00b7|\u03c9) from vi, \u03bai and c\u0302 parameterized by a k-dimensional weight vector \u03c9 = [\u03c9i], 0 \u2264 \u03c9i \u2264 1, which re-weights components \u03bai:\nc\u0303(x|\u03c9) = \u2211k i=1 \u03c9ivi\u03bai(x)\u2211k\ni=1 \u03c9ivi ,\nm\u0303(x|\u03c9) = (\nk\u2211 i=1 \u03c9ivi\n) c\u0302(x) + ( 1\u2212\nk\u2211 i=1 \u03c9ivi\n) c\u0303(x|1\u2212 \u03c9);\n3. Maximize the log likelihood of M and C constructed with m\u0303 and c\u0303 under the constraint\u2211k i=1 \u03c9ivi = r for many values of r equispaced in [0, 1].\nllr = max w.r.t. \u03c9 \u2211 x\u2208M log m\u0303(x|\u03c9) + \u2211 x\u2208C log c\u0303(x|\u03c9),\nsubject to \u2211k i=1 \u03c9ivi = r,\n0 \u2264 vi \u2264 1, i = 1, . . . , k.\n4. Estimate the maximum proportion of c in m, acm (minor abuse of notation 1), as the x-\ncoordinate of the elbow in the llr versus r graph.\nThe densities m\u0303(\u00b7|\u03c9) and c\u0303(\u00b7|\u03c9) are constructed to approximate m and c. The efficacy of the approximation depends on the value of \u03c9; there exists \u03c9 such that m\u0303(\u00b7|\u03c9) and c\u0303(\u00b7|\u03c9) are good approximations provided \u2211k i=1 \u03c9ivi \u2264 acm, however, the approximation deteriorates progressively, even\nwith the optimum \u03c9, as \u2211k i=1 \u03c9ivi moves beyond a c m. This suggests that the graph of llr versus r should be approximately a flat line from 0 to acm and decrease progressively beyond a c m exposing an elbow at acm, which is detected in the last step. The pseudo code for elbow detection is provided in (Jain et al., 2016).\nOur implementation uses histograms as the nonparametric method to obtain m\u0302 and c\u0302. The bin-width is chosen to cover the component sample\u2019s (after the transformation) range and reveal the shape of its distribution, using the default option in Matlab\u2019s histogram function. More bins with the same bin-width are subsequently added to cover the mixture sample\u2019s range."}, {"heading": "D Results for synthetic data", "text": ""}, {"heading": "E Results for multivariate AlphaMax-N and MSGMM", "text": "To demonstrate the efficacy of class-prior preserving transform, we implemented the multivariate versions of AlphaMax-N and MSGMM and evaluated them on the twelve real data sets without applying the transform. There were significant stability and computational issues related to the highdimensional nature of the data sets. MSGMM was numerically instable because of singular/nearlysingular covariance matrix; AlphaMax-N became computationally demanding because the number of bins (for histogram based density estimation) grow exponentially with the dimension, resulting into a large parameter vector \u03c9 and consequently, a large optimization problem, even after removing the zero-count bins. This is expected, as density estimation for multivariate data is known to be problematic, which is one of the main reasons for introducing our transform. To make estimation feasible under these stability and computational issues, we used dimensionality reduction. Though not all data sets posed the same level of difficulty, to have a standard approach and permit effective density estimation, we used the top three principal components, obtained via principal component analysis on the z-score normalized data (mixture and component samples combined), as input to the two algorithms. We also attempted using top k principal components that preserve 75 percent of the total variance, however, for some of the data sets, the dimension was still too high.\nIn the same manner as in the univariate case, we used histograms in the multivariate implementation of AlphaMax-N. The bin-width for a dimension was selected to minimize the asymptotic mean integrated squared error (AMISE) with a normal reference rule, using the component sample, C. The formula for the bin-width of dimension k is given by:\nbk = 3.5\u03c3k|C|\u22121/(2+d), where d is the total number of dimensions, \u03c3k is the standard deviation of the kth dimension and |C| is the component sample size. Bins were added to cover the range of entire data, mixture and component combined and empty bins were removed to reduce the size of the optimization problem.\nTable 3 contains the results of AlphaMax-N and MSGMM on the real-life data sets, using the top three principal components under column headings AlphaMax-NM (M for multi-dimensional) and\n1acm is a minor abuse of notation; replacing the densities with the underlying measures makes the notation correct\nMSGMM, respectively. The results of AlphaMax-N and MSGMM-T with the class-prior preserving transform are also provided for comparison. Notice that, though AlphaMax-NM (without transform) performs well, AlphaMax-N (with transform) is significantly better in terms of estimation error, despite having a lower computational cost. Also notice the deterioration in the performance of MSGMM (without transform) compared to MSGMM-T (with transform). In conclusion, the results illustrate the usefulness of the class-prior preserving transform."}], "references": [{"title": "High breakdown mixture discriminant analysis", "author": ["S. Bashir", "E.M. Carter"], "venue": "J Multivar Anal,", "citeRegEx": "Bashir and Carter.,? \\Q2005\\E", "shortCiteRegEx": "Bashir and Carter.", "year": 2005}, {"title": "Semi-supervised novelty detection", "author": ["G. Blanchard", "G. Lee", "C. Scott"], "venue": "J Mach Learn Res,", "citeRegEx": "Blanchard et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2010}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In Proceedings of the 11th Annual Conference on Computational Learning Theory, COLT", "citeRegEx": "Blum and Mitchell.,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "Robust supervised classification with mixture models: learning from data with uncertain labels", "author": ["C. Bouveyron", "S. Girard"], "venue": "Pattern Recognit,", "citeRegEx": "Bouveyron and Girard.,? \\Q2009\\E", "shortCiteRegEx": "Bouveyron and Girard.", "year": 2009}, {"title": "Sample selection bias correction theory", "author": ["C. Cortes", "M. Mohri", "M. Riley", "A. Rostamizadeh"], "venue": "In Proceedings of the 19th International Conference on Algorithmic Learning Theory, ALT", "citeRegEx": "Cortes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2008}, {"title": "Learning from positive and unlabeled examples", "author": ["F. Denis", "R. Gilleron", "F. Letouzey"], "venue": "Theor Comput Sci,", "citeRegEx": "Denis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Denis et al\\.", "year": 2005}, {"title": "Class prior estimation from positive and unlabeled data", "author": ["M.C. du Plessis", "M. Sugiyama"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "Plessis and Sugiyama.,? \\Q2014\\E", "shortCiteRegEx": "Plessis and Sugiyama.", "year": 2014}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["C. Elkan", "K. Noto"], "venue": "In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Elkan and Noto.,? \\Q2008\\E", "shortCiteRegEx": "Elkan and Noto.", "year": 2008}, {"title": "High-breakdown linear discriminant analysis", "author": ["D.M. Hawkins", "G.J. McLachlan"], "venue": "J Am Stat Assoc,", "citeRegEx": "Hawkins and McLachlan.,? \\Q1997\\E", "shortCiteRegEx": "Hawkins and McLachlan.", "year": 1997}, {"title": "Nonparametric semi-supervised learning of class proportions", "author": ["S. Jain", "M. White", "M.W. Trosset", "P. Radivojac"], "venue": "arXiv preprint arXiv:1601.01944,", "citeRegEx": "Jain et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2016}, {"title": "A mutual contamination analysis of mixed membership and partial label models", "author": ["J. Katz-Samuels", "C. Scott"], "venue": "arXiv preprint arXiv:1602.06235,", "citeRegEx": "Katz.Samuels and Scott.,? \\Q2016\\E", "shortCiteRegEx": "Katz.Samuels and Scott.", "year": 2016}, {"title": "Estimating a kernel Fisher discriminant in the presence of label noise", "author": ["N.D. Lawrence", "B. Scholkopf"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "Lawrence and Scholkopf.,? \\Q2001\\E", "shortCiteRegEx": "Lawrence and Scholkopf.", "year": 2001}, {"title": "Sparse nonparametric density estimation in high dimensions using the rodeo", "author": ["H. Liu", "J.D. Lafferty", "L.A. Wasserman"], "venue": "In Proceedings of the 10th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Random classification noise defeats all convex potential boosters", "author": ["P.M. Long", "R.A. Servedio"], "venue": "Mach Learn,", "citeRegEx": "Long and Servedio.,? \\Q2010\\E", "shortCiteRegEx": "Long and Servedio.", "year": 2010}, {"title": "Noise tolerance under risk minimization", "author": ["N. Manwani", "P.S. Sastry"], "venue": "IEEE T Cybern,", "citeRegEx": "Manwani and Sastry.,? \\Q2013\\E", "shortCiteRegEx": "Manwani and Sastry.", "year": 2013}, {"title": "Obtaining calibrated probabilities from boosting", "author": ["A. Niculescu-Mizil", "R. Caruana"], "venue": "In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Niculescu.Mizil and Caruana.,? \\Q2005\\E", "shortCiteRegEx": "Niculescu.Mizil and Caruana.", "year": 2005}, {"title": "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods, pages 61\u201374", "author": ["J.C. Platt"], "venue": null, "citeRegEx": "Platt.,? \\Q1999\\E", "shortCiteRegEx": "Platt.", "year": 1999}, {"title": "Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure", "author": ["M. Saerens", "P. Latinne", "C. Decaestecker"], "venue": "Neural Comput,", "citeRegEx": "Saerens et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Saerens et al\\.", "year": 2002}, {"title": "Classification with asymmetric label noise: consistency and maximal denoising", "author": ["C. Scott", "G. Blanchard", "G. Handy"], "venue": "J Mach Learn Res W&CP,", "citeRegEx": "Scott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Scott et al\\.", "year": 2013}, {"title": "The curse of dimensionality and dimension reduction. Multivariate Density Estimation: Theory, Practice, and Visualization, pages", "author": ["D.W. Scott"], "venue": null, "citeRegEx": "Scott.,? \\Q2008\\E", "shortCiteRegEx": "Scott.", "year": 2008}, {"title": "The ABC\u2019s (and XYZ\u2019s) of peptide sequencing", "author": ["H. Steen", "M. Mann"], "venue": "Nat Rev Mol Cell Biol,", "citeRegEx": "Steen and Mann.,? \\Q2004\\E", "shortCiteRegEx": "Steen and Mann.", "year": 2004}, {"title": "Theorem 4 (Restatement of Theorem 9 in Jain et al. (2016)) LetX andX1 be random variables with densities f and f1 and measures \u03bc and \u03bc1", "author": ["Jain"], "venue": null, "citeRegEx": "Jain,? \\Q2016\\E", "shortCiteRegEx": "Jain", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "In many domains, however, a sample from one of the classes (say, negatives) may not be available, leading to the setting of learning from positive and unlabeled data (Denis et al., 2005).", "startOffset": 166, "endOffset": 186}, {"referenceID": 7, "context": "If the class priors in the unlabeled data are known, positive-unlabeled learning can straightforwardly translate into learning of non-traditional classifiers (Elkan and Noto, 2008); i.", "startOffset": 158, "endOffset": 180}, {"referenceID": 9, "context": ", those that discriminate between positive and negative data (Jain et al., 2016).", "startOffset": 61, "endOffset": 80}, {"referenceID": 9, "context": "Under mild assumptions, even when the class priors are unknown, there exists a monotonic relationship between the outputs of these classifiers (Jain et al., 2016) and, hence, the models trained for information retrieval and ranking generally do not suffer when trained on labeled vs.", "startOffset": 143, "endOffset": 162}, {"referenceID": 1, "context": "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.", "startOffset": 146, "endOffset": 228}, {"referenceID": 18, "context": "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.", "startOffset": 146, "endOffset": 228}, {"referenceID": 9, "context": "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.", "startOffset": 146, "endOffset": 228}, {"referenceID": 7, "context": ", 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Jain et al., 2016).", "startOffset": 38, "endOffset": 129}, {"referenceID": 9, "context": ", 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Jain et al., 2016).", "startOffset": 38, "endOffset": 129}, {"referenceID": 20, "context": "For example, in the process of peptide identification (Steen and Mann, 2004), bioinformatics methods are usually set to report results with specified false discovery rate thresholds (e.", "startOffset": 54, "endOffset": 76}, {"referenceID": 18, "context": "Further, the only approach that does consider similar such noise (Scott et al., 2013) requires density estimation, which is known to be problematic for high-dimensional data.", "startOffset": 65, "endOffset": 85}, {"referenceID": 18, "context": "Much of the identifiability characterization in this section has already been considered as the case of asymmetric noise (Scott et al., 2013); see Section 7 on related work.", "startOffset": 121, "endOffset": 141}, {"referenceID": 18, "context": "To ensure identifiability, it is necessary to choose a canonical form that prefers a class prior that makes the two components as different as possible; this canonical form was independently introduced as the mutual irreducibility principle (Scott et al., 2013) or the max-canonical form (Jain et al.", "startOffset": 241, "endOffset": 261}, {"referenceID": 9, "context": ", 2013) or the max-canonical form (Jain et al., 2016).", "startOffset": 34, "endOffset": 53}, {"referenceID": 12, "context": "Nonparametric (kernel) density estimation is also known to have curse-of-dimensionality issues, both in theory (Liu et al., 2007) and in practice (Scott, 2008).", "startOffset": 111, "endOffset": 129}, {"referenceID": 19, "context": ", 2007) and in practice (Scott, 2008).", "startOffset": 24, "endOffset": 37}, {"referenceID": 9, "context": "The transform is similar to that in (Jain et al., 2016), except that it is not required to be calibrated like a posterior distribution; as shown below, a good ranking function is sufficient.", "startOffset": 36, "endOffset": 55}, {"referenceID": 16, "context": "The posterior probability \u03c4p can be estimated directly by using a probabilistic classifier or by calibrating a classifier\u2019s score (Platt, 1999; Niculescu-Mizil and Caruana, 2005); |U |/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric approaches for estimation of \u03b1\u2217 and \u03b2\u2217.", "startOffset": 130, "endOffset": 178}, {"referenceID": 15, "context": "The posterior probability \u03c4p can be estimated directly by using a probabilistic classifier or by calibrating a classifier\u2019s score (Platt, 1999; Niculescu-Mizil and Caruana, 2005); |U |/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric approaches for estimation of \u03b1\u2217 and \u03b2\u2217.", "startOffset": 130, "endOffset": 178}, {"referenceID": 9, "context": "For this purpose, we use the AlphaMax algorithm (Jain et al., 2016), briefly summarized in the Appendix.", "startOffset": 48, "endOffset": 67}, {"referenceID": 7, "context": "Algorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm (Elkan and Noto, 2008) as well as the noiseless version of AlphaMax (Jain et al.", "startOffset": 87, "endOffset": 109}, {"referenceID": 9, "context": "Algorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm (Elkan and Noto, 2008) as well as the noiseless version of AlphaMax (Jain et al., 2016).", "startOffset": 155, "endOffset": 174}, {"referenceID": 6, "context": "The algorithm proposed by du Plessis and Sugiyama (2014) minimizes the same objective as the e1 Elkan-Noto estimator and, thus, was not implemented.", "startOffset": 29, "endOffset": 57}, {"referenceID": 10, "context": "The aim under this model is to estimate multiple unknown base distributions, using multiple random samples that are composed of different convex combinations of those base distributions (Katz-Samuels and Scott, 2016).", "startOffset": 186, "endOffset": 216}, {"referenceID": 2, "context": "(2013), and previously investigated under a more restrictive setting as co-training (Blum and Mitchell, 1998).", "startOffset": 84, "endOffset": 109}, {"referenceID": 13, "context": "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.", "startOffset": 165, "endOffset": 216}, {"referenceID": 14, "context": "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.", "startOffset": 165, "endOffset": 216}, {"referenceID": 8, "context": "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.", "startOffset": 233, "endOffset": 287}, {"referenceID": 0, "context": "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.", "startOffset": 233, "endOffset": 287}, {"referenceID": 11, "context": "Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class.", "startOffset": 95, "endOffset": 153}, {"referenceID": 3, "context": "Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class.", "startOffset": 95, "endOffset": 153}, {"referenceID": 5, "context": "Class prior estimation in a semi-supervised setting including positive-unlabeled learning, has been extensively discussed previously; see Saerens et al. (2002); Cortes et al.", "startOffset": 138, "endOffset": 160}, {"referenceID": 0, "context": "(2002); Cortes et al. (2008); Elkan and Noto (2008); Blanchard et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2002); Cortes et al. (2008); Elkan and Noto (2008); Blanchard et al.", "startOffset": 8, "endOffset": 52}, {"referenceID": 0, "context": "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 0, "context": "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al.", "startOffset": 31, "endOffset": 76}, {"referenceID": 0, "context": "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al. (2016) and references therein.", "startOffset": 31, "endOffset": 96}, {"referenceID": 0, "context": "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al. (2016) and references therein. Recently, a general setting for label noise has also been introduced, called the mutual contamination model. The aim under this model is to estimate multiple unknown base distributions, using multiple random samples that are composed of different convex combinations of those base distributions (Katz-Samuels and Scott, 2016). The setting of asymmetric label noise is a subset of this more general setting, treated under general conditions by Scott et al. (2013), and previously investigated under a more restrictive setting as co-training (Blum and Mitchell, 1998).", "startOffset": 31, "endOffset": 583}, {"referenceID": 0, "context": "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise. Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class. As the most related work, though Scott et al. (2013) did not explicitly treat the positive-unlabeled learning with noisy positives, their formulation can incorporate this setting by using \u03c00 = \u03b1 and \u03b2 = 1 \u2212 \u03c01.", "startOffset": 263, "endOffset": 611}, {"referenceID": 9, "context": "Proof: The proof follows from Lemma 4 and Theorem 3 of Jain et al. (2016).", "startOffset": 55, "endOffset": 74}, {"referenceID": 9, "context": "Theorem 4 (Restatement of Theorem 9 in Jain et al. (2016)) LetX andX1 be random variables with densities f and f1 and measures \u03bc and \u03bc1 respectively.", "startOffset": 39, "endOffset": 58}, {"referenceID": 9, "context": "The pseudo code for elbow detection is provided in (Jain et al., 2016).", "startOffset": 51, "endOffset": 70}], "year": 2017, "abstractText": "We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data. In recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise. We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers. We prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data. The theoretical development and both parametric and nonparametric algorithms proposed here constitutes an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data.", "creator": "LaTeX with hyperref package"}}}