{"id": "1607.00036", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes", "abstract": "In this paper, hydaspes we extend neural Turing machine (NTM) brickbats into a dynamic re-integrated neural ghannouchi Turing mikell machine (D - letellier NTM) by introducing wilda a nov. trainable alemka memory 11-july addressing pppp scheme. pkg This scheme maintains 20-strikeout for ashmore each nicodemo memory cell ogino two blinkers separate nigrita vectors, relating content travelin and huichol address csat vectors. deletion This donnchad allows the nine D - dyula NTM repacholi to learn a 10.5-meter wide folie variety of cooperstown location - based addressing strategies knockoff including khirad both frogmore linear and mandipaka nonlinear ones. streicher We ivanovich implement marocco the 115.92 D - NTM with clarity both teimour soft, differentiable combed and canadia hard, non - savvy differentiable read / supportable write mechanisms. 99w We sell-out investigate the 1747 mechanisms and 1/0 effects for learning to read clooney and sust write mate to a leena memory stuckist through experiments piggish on Facebook bAbI tasks using both carpodacus a 864,000 feedforward midi and suomussalmi GRU - controller. patronyms The marshalled D - cocke NTM vitarich is 88-ball evaluated horiguchi on a set camporese of scholte the ringo Facebook alakrana bAbI hadep tasks canelli and 0147 shown to marchione outperform s-adenosyl NTM and LSTM baselines.", "histories": [["v1", "Thu, 30 Jun 2016 20:45:12 GMT  (173kb,D)", "http://arxiv.org/abs/1607.00036v1", "13 pages, 2 figures"], ["v2", "Fri, 17 Mar 2017 05:56:48 GMT  (406kb,D)", "http://arxiv.org/abs/1607.00036v2", "13 pages, 3 figures"]], "COMMENTS": "13 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["caglar gulcehre", "sarath chandar", "kyunghyun cho", "yoshua bengio"], "accepted": false, "id": "1607.00036"}, "pdf": {"name": "1607.00036.pdf", "metadata": {"source": "CRF", "title": "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes", "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "emails": ["name.lastname@umontreal.ca", "name.lastname@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Designing general-purpose learning algorithms is one of the long-standing goals of artificial intelligence. Despite the success of deep learning in this area (see, e.g., [1],) there are still a set of complex tasks that are not well addressed by conventional neural networks. Those tasks often require a neural network to be equipped with an explicit, external memory in which a larger, potentially unbounded, set of facts need to be stored. They include, but are not limited to, episodic question-answering [2, 3, 4], compact algorithms [5] and video caption generation [6].\nRecently two promising approaches based on neural networks to this type of tasks have been proposed. Memory networks [2] explicitly store all the facts, or information, available for each episode in an external memory (as continuous vectors) and use the attentional mechanism to index them when returning an output. On the other hand, neural Turing machines (NTM, [7]) read each fact in an episode and decides whether to read, write the fact or do both to the external, differentiable memory.\nA crucial difference between these two models is that the memory network does not have a mechanism to modify the content of the external memory, while the NTM does. In practice, this leads to easier learning in the memory network, which in turn resulted in it being used more in real tasks [8, 9]. On the contrary, the NTM has mainly been tested on a series of small-scale, carefully-crafted tasks such as copy and associative recall. The NTM, however is more expressive, precisely because it can store and modify the internal state of the network as it processes an episode.\nThe original NTM supports two modes of addressing (which can be used simultaneously.) They are contentbased and location-based addressing. We notice that the location-based strategy is based on linear addressing. The distance between each pair of consecutive memory cells is fixed to a constant. We address this limitation, in this paper, by introducing a learnable address vector for each memory cell of the NTM with least recently used memory addressing mechanism, and we call this variant a dynamic neural Turing machine (D-NTM).\nWe evaluate the proposed D-NTM on the full set of Facebook bAbI task [2] using either soft, differentiable attention or hard, non-differentiable attention [10] as an addressing strategy. Our experiments reveal that\nar X\niv :1\n60 7.\n00 03\n6v 1\n[ cs\n.L G\n] 3\n0 Ju\nn 20\nit is possible to use the hard, non-differentiable attention mechanism, and in fact, the D-NTM with the hard attention and GRU controller outperforms the one with the soft attention.\nOur Contributions\n1. We propose a generalization of Neural Turing Machine called a dynamic neural Turing machine (D-NTM) which employs learnable, location-based addressing.\n2. We demonstrate the application of neural Turing machines for a more complicated real task: episodic question-answering.\n3. We propose to use the hard attention mechanism and empirically show that it outperforms the soft attention based addressing.\n4. We propose a curriculum strategy for our model with the feedforward controller and discrete attention that improves our results significantly."}, {"heading": "2 Dynamic Neural Turing Machine", "text": "The proposed dynamic neural Turing machine (D-NTM) extends the neural Turing machine (NTM, [7]) which has a modular design. The NTM consists of two main modules, a controller and, a memory. The controller, which is often implemented as a recurrent neural network, issues a command to the memory so as to read, write to and erase a subset of memory cells. Although the memory was originally envisioned as an integrated module, it is not necessary, and the memory may be an external, black box [10]."}, {"heading": "2.1 Controller", "text": "At each time step t, the controller (1) receives an input value xt, (2) addresses and reads mt from a portion of the memory, (3) erases/writes a portion of the memory, (4) updates its own hidden state ht, and (5) outputs a value yt (if needed.) In this paper, we both use a gated recurrent unit (GRU, [11]) and a feedforward-controller to implement the controller such that\nht = GRU(xt,ht\u22121,mt) (1)\nor for a feedforward-controller\nht = \u03c3(xt,mt). (2)"}, {"heading": "2.2 Memory", "text": "We use a rectangular matrix M \u2208 RN\u00d7dh to denote N memory cells. Unlike the original NTM, we partition each memory cell vector into two parts:\nM = [A;C] .\nThe first part A is a learnable address matrix, and the second C a content matrix. In other words, each memory cell mi is now\nmi = [ai;ci] .\nThe address part ai is considered a model parameter that is updated during training. During inference, the address part is not overwritten by the controller and remains constant. On the other hand, the content part ci is both read and written by the controller both during training and inference. At the beginning of each episode, the content part C is refreshed to be an all-zero matrix.\nThis introduction of the learnable address portion for each memory cell allows the model to learn sophisticated location-based addressing strategies."}, {"heading": "2.3 Memory Addressing", "text": "Memory addressing in the D-NTM is equivalent to computing an N-dimensional address vector. The D-NTM computes three such vectors for respectively reading wt, erasing et and writing ut. Specifically for writing, the controller further computes a new content vector ct based on its current hidden state ht.\nReading With the read vector wt, the memory content is retrieved by mt = wtMt\u22121, (3)\nwhere wt is a row vector.\nErasing and Writing Given the erase, write and content vectors (etj, utj, and ctj respectively), the memory matrix is updated by\nmtj = (1\u2212 etjutj)mt\u22121j + u t jc t, (4)\nwhere the subscript j in mtj denotes the j-th element of the memory matrix M t and it is a vector."}, {"heading": "2.4 Learning", "text": "Once the proposed D-NTM is executed, it returns the output distribution p(y|x1, . . . ,xT ). As a result, we define a cost function as the negative log-likelihood:\nC(\u03b8) = 1\nN N\u2211 n=1 \u2212 log p(yn|xn1 , . . . ,xnT ), (5)\nwhere \u03b8 is a set of all the parameters. As the proposed D-NTM, just like the original NTM, is fully differentiable end-to-end, we can compute the gradient of this cost function using backpropagation and learn the parameters of the model with a gradient-based optimization algorithm, such as stochastic gradient descent, to train it end-to-end.\nNo Operation (NOP) As found in [12], an additional NOP action might be beneficial for the controller not to access the memory once in a while. We model this situation by designating one memory cell as a NOP cell. Reading or writing from this memory cell is ignored."}, {"heading": "3 Addressing Mechanism", "text": ""}, {"heading": "3.1 Address Vectors", "text": "Each of the address vectors (read, write and erase) is computed in an identical manner which we describe here.\nFirst, the controller computes a key vector: kt = W>k h t + btk, where Wk and bk are the parameters for this specific head (either read, write or erase.) Also, the sharpening factor \u03b2t is computed:\n\u03b2t = softplus(u>\u03b2 h t + b\u03b2).\nu\u03b2 and b\u03b2 are similarly the head parameters.\nThe address vector is then computed by zti = \u03b2 tK ( kt,mti ) (6)\nwti = exp(zti)\u2211 j exp(z t j) , (7)\nwhere the similarity functionK is defined as\nK (x,y) = x \u00b7 y\n(||x||||y||+ ) ."}, {"heading": "3.2 Multi-step Addressing", "text": "At each time-step, controller may require more than one-step in order to access to the memory. The original NTM addresses this by implementing multiple sets of read, erase and write heads. In this paper, we explore an option of allowing each head to operate more than once at each time step, similar to the multi-hop mechanism from the end-to-end memory network [13]."}, {"heading": "3.3 Dynamic Least Recently Used Addressing", "text": "We introduce a memory addressing schema that can learn to put more emphasis on the least recently used (LRU) memory locations. As observed in [14], we find it more easier to learn pure content-based addressing by using a LRU addressing.\nTo learn a LRU based addressing, first we compute the exponentially moving averages of the logits (zt) as vt, vt \u2190 0.1vt\u22121 + 0.9zt. We rescale the accumulated vt with \u03b3t, such that the controller adjusts the influence of how much the previously written memory locations should effect the attention weights of a particular time-step. Next, we subtract vt from zt in order to reduce the weights of previously read or written memory locations. \u03b3t is a shallow MLP with a scalar output and it is conditioned on the hidden state of the controller. \u03b3t is parametrized with the parameters u\u03b3 and b\u03b3,\n\u03b3t = sigmoid(u\u03b3ht + b\u03b3), (8) wt \u2190 softmax(zt \u2212 \u03b3tvt\u22121). (9)\nThis scheme has an effect of increasing the weights of the least recently used read and write weights. The magnitude of this reduction is being learned and adjusted with \u03b3t. Our LRU addressing is dynamic due to the model\u2019s ability to switch between pure content-based addressing and LRU. During the training, we do not backpropagate through vt."}, {"heading": "4 Regularizing Dynamic Neural Turing Machines", "text": "Read-Write Consistency Regularizer When the controller of D-NTM is a powerful recurrent neural network, it is important to regularize training of the D-NTM so as to avoid suboptimal solutions in which the D-NTM ignores the memory and works as a simple recurrent neural network.\nOne such suboptimal solution we have observed in our preliminary experiments with the proposed D-NTM is that the D-NTM uses the address part A of the memory matrix simply as an additional weight matrix, rather than as a means to accessing the content part C. We found that this pathological case can be effectively avoided by encouraging the read head to point to a memory cell which has also been pointed by the write head. This can be implemented as the following regularization term:\nRrw(w,u) = \u03bb T\u2211 t\u2032=1 ||1\u2212 ( 1 t\u2032 t\u2032\u2211 t=1 ut) >wt\u2032||22\nIn the equations above, ut is the write and wt is the read weights.\nNext Input Prediction as Regularization Temporal structure is a strong signal that should be exploited by the controller based on a recurrent neural network. We exploit this structure by letting the controller predict the input in the future. We maximize the predictability of the next input by the controller during training. This is equivalent to minimizing the following regularizer:\nRpred(W) = \u2212 log p(fBOWt+1 |ft,wt,ut,Mt;W)).\nWe found this regularizer to be effective in our preliminary experiments and use it throughout the experiments."}, {"heading": "5 Generating Discrete Address Vectors", "text": "In this section, we briefly describe the hard attention based addressing strategy.\nDiscrete Addressing Let us use w to denote an address vector (either read, write or erase) at time t. By definition in Eq. (6), every element in this address vector is positive and sums up to one. In other words, we can treat this vector as the probabilities of a categorical distribution C(w) with dim(w) choices:\np(j) = wj,\nwhere wj is the j-th element of w. We can readily sample from this categorical distribution and form an one-hot vector w\u0303 such that\nw\u0303k = I(k = j),\nwhere j \u223c C(w), and I is an indicator function.\nTraining We use this sampling-based strategy for all the heads during training. This clearly makes the use of backpropagation infeasible to compute the gradient, as the sampling procedure is not differentiable. Thus, we use REINFORCE [15] together with the three variance reduction techniques\u2013global baseline, input-dependent baseline and variance normalization\u2013 suggested in [16].\nLet us defineR(x) = log p(y|x1, . . . ,xT ) as a reward. We first center and re-scale the reward by\nR\u0303(x) = R(x)\u2212 b\u221a \u03c32 + ,\nwhere b and \u03c3 is running average and standard deviation ofR(x). We can further center it for each input x separately, i.e.,\nR\u0303(x)\u2190 R\u0303(x)\u2212 b(x), where b(x) is computed by a baseline network which takes as input x and predicts its estimated reward. The baseline network is trained to minimize the Huber loss [17] between the true reward R\u0303(x)\u2217 and the predicted reward b(x). We use the Huber loss, which is defined by\nH\u03b4(x) = { x2 for |x| \u2264 \u03b4, \u03b4(2|x| \u2212 \u03b4), otherwise,\ndue to its robustness. As a further measure to reduce the variance, we regularize the negative entropy of all those category distributions to facilitate a better exploration during training [18].\nThen, the cost function for each training example is approximated as\nCn(\u03b8) =\u2212 log p(y|x1:T , w\u03031:J , u\u03031:J , e\u03031:J)\n\u2212 J\u2211 j=1 R\u0303(xn)(log p(w\u0303j|x1:T ) + log p(u\u0303j|x1:T ) + log p(e\u0303j|x1:T ))\n+ \u03bbH J\u2211 j=1 (H(wj|x1:T ) +H(uj|x1:T ) +H(ej|x1:T )),\nwhere J is the number of addressing steps, \u03bbH is the entropy regularization coefficient, andH denotes the entropy.\nInference Once training is over, we switch to a deterministic strategy. We simply choose an element of w with the largest value to be the index of the target memory cell, such that\nw\u0303k = I(k = argmax(w)).\nCurriculum Learning for the Discrete Attention Training discrete attention with feed-forward controller and REINFORCE is challenging. We propose to use a curriculum strategy for training with the discrete attention in order to tackle this problem. For each minibatch, we sample \u03c0 from a binomial distribution with the probability pt, \u03c0t \u223c Bin(pt). The model will either use the discrete or the soft-attention based on the \u03c0t. We start the training procedure with p0 = 1 and during the training pt is annealed to 0 by setting pt = p 0\n\u221a 1+t .\nWe can rewrite the weights wt as in Equation 5, where it is expressed as the combination of soft attention weights w\u0304t and discrete attention weights w\u0303t with\u03c0t being a binary variable that chooses to use one of them,\nwt \u2190 \u03c0tw\u0304t + (1\u2212 \u03c0t)w\u0303t .\nBy using this curriculum learning strategy, at the beginning of the training, the model learns to use the memory mainly with the soft attention. As we anneal the pt, the model will rely more on the discrete attention."}, {"heading": "6 Related Work", "text": "A recurrent neural network (RNN), which is used as a controller in the proposed D-NTM, has an implicit memory in the form of recurring hidden states. Even with this implicit memory, a vanilla RNN is however known to have difficulties in storing information for long time-spans [19]. Long short-term memory (LSTM, [20]) and gated recurrent units (GRU, [11]) have been found to address this issue. However all these models based solely on RNNs have been found to be limited when they are used to solve, e.g., algorithmic tasks and episodic question-answering.\nIn addition to the finite random access memory of the neural Turing machine, based on which the D-NTM is designed, other data structures have been proposed as external memory for neural networks. In [21, 22, 12], a continuous, differentiable stack was proposed. In [5, 10], grid and tape storages are used. These approaches differ from the NTM in that their memory is unbounded and can grow indefinitely. On the other hand, they are often not randomly accessible.\nMemory networks [2] form another family of neural networks with external memory. In this class of neural networks, information is stored explicitly as it is (in the form of its continuous representation) in the memory, without being erased or modified during an episode. Memory networks and their variants have been applied to various tasks successfully [13, 8, 9, 23].\nAnother related family of models is the attention-based neural networks. Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].\nThe latter two, the memory network and attention-based networks, are however clearly distinguishable from the D-NTM by the fact that they do not modify the content of the memory."}, {"heading": "7 Episodic Question-Answering: bAbI Tasks", "text": "We evaluate the proposed D-NTM on the recently proposed episodic question-answering task called Facebook bAbI [27]. We use the dataset with 10k training examples per sub-task provided by Facebook.1\nFor each episode, the D-NTM reads a sequence of factual sentences followed by a question, all of which are given as natural language sentences. The D-NTM is expected to store and retrieve relevant information in the memory in order to answer the question based on the presented facts. The computational complexity of the D-NTM on each episode is linear with respect to the number of facts, as the size of the memory is constant. This is unlike the memory network which requires quadratic time, as it scans through all the facts at each time step.\n1 https://research.facebook.com/researchers/1543934539189348"}, {"heading": "7.1 Model and Training Details", "text": "We use the same hyperparameters for all the tasks for a given model.\nFact Representation We use a recurrent neural network with GRU units to encode a variable-length fact into a fixed-size vector representation. This allows the D-NTM to exploit the word ordering in each fact, unlike when facts are encoded as bag-of-words vectors.\nController We experiment with both a recurrent and feedforward neural network as the controller that generates the read and write weights. The controller has 180 units. We train our feed-forward controller using noisy-tanh activation function [28] since we were experiencing training difficulties with sigmoid and tanh activation functions. We use both single-step and three-steps addressing with our GRU controller.\nMemory The memory contains 120 memory cells. Each memory cell consists of a 16-dimensional address part and 28-dimensional content part.\nTraining We set aside a random 10% of the training examples as a validation set for each sub-task and use it for early-stopping and hyperparameter search. We train one D-NTM for each sub-task, using Adam [29] with its learning rate set to 0.003 and 0.007 respectively for GRU and Feedforward controller. The size of each minibatch is 160, and each minibatch is constructed uniform-randomly from the training set."}, {"heading": "7.2 Goals", "text": "The goal of this experiment is three-fold. First, we present for the first time the performance of a memory-based network that can both read and write dynamically on the Facebook bAbI tasks. We aim to understand whether a model that has to learn to write an incoming fact to the memory, rather than storing it as it is, is able to work well, and to do so, we compare both the original NTM and proposed D-NTM against an LSTM-RNN.\nSecond, we investigate the effect of having to learn how to write. The fact that the NTM needs to learn to write likely has adverse effect on the overall performance, when compared to, for instance, end-to-end memory networks (MemN2N, [13]) and dynamic memory network (DMN+, [23]) both of which simply store the incoming facts as they are. We quantify this effect in this experiment. Lastly, we show the effect of the proposed learnable addressing scheme.\nWe further explore the effect of using a feedforward controller instead of the GRU controller. In addition to the explicit memory, the GRU controller can use its own internal hidden state as the memory. On the other hand, the feedforward controller must solely rely on the explicit memory, as it is the only memory available."}, {"heading": "7.3 Results and Analysis", "text": "In Table 1, we first observe that the NTMs are indeed capable of solving this type of episodic question-answering better than the vanilla LSTM-RNN. Although the availability of explicit memory in the NTM has already suggested this result, we note that this is the first time neural Turing machines have been used in this specific task.\nAll the variants of NTM with the GRU controller outperform the vanilla LSTM-RNN. However, not all them perform equally well. First, it is clear that the proposed dynamic NTM (D-NTM) using the GRU controller outperforms the original NTM with the GRU controller (LBA NTM, CBA NTM vs. Soft D-NTM, Hard D-NTM). As discussed earlier, the learnable addressing scheme of the D-NTM allows the controller to access the memory slots by location in a potentially nonlinear way. We expect it to help with tasks that have non-trivial access patterns, and as anticipated, we see a large gain with the D-NTM over the original NTM in the tasks of, for instance, 12 - Conjunction and 17 - Positional Reasoning.\nAmong the recurrent variants of the proposed D-NTM, we notice significant improvements by using discrete addressing over using soft addressing. We conjecture that this is due to certain types of tasks that require precise/sharp retrieval of a stored fact, in which case soft addressing is in disadvantage over discrete addressing. This is evident from the observation that the D-NTM with discrete addressing significantly outperforms that with soft addressing in the tasks of 8 - Lists/Sets and 11 - Basic Coreference. Furthermore, this is in line with an earlier observation in [18], where discrete addressing was found to generalize better in the task of image caption generation.\nIn Table 2, we observe that the D-NTM with the feedforward controller and discrete attention performs worse than LSTM and D-NTM with soft-attention. However, when the proposed curriculum strategy from Sec. 5 is used, the average test error drops from 68.30 to 37.79.\nWe empirically found training of the feedforward controller more difficult than that of the recurrent controller. We train our feedforward controller based models four times longer (in terms of the number of updates) than the recurrent controller based ones in order to ensure that they are converged for most of the tasks. On the other hand, the models trained with the GRU controller overfit on bAbI tasks very quickly. For example, on tasks 3 and 16 the feedforward controller based model underfits (i.e., high training loss) at the end of the training, whereas with the same number of units the model with the GRU controller can overfit on those tasks after 3,000 updates only.\nWhen our results are compared to the variants of the memory network [2] (MemN2N and DMN+), we notice a significant performance gap. We attribute this gap to the difficulty in learning to manipulate and store a complex input."}, {"heading": "8 Conclusion and Future Work", "text": "In this paper we extend neural Turing machines (NTM) by introducing a learnable addressing scheme which allows the NTM to be capable of performing highly nonlinear location-based addressing. This extension, to which we refer by dynamic NTM (D-NTM), is extensively tested with various configurations, including different addressing mechanisms (soft vs. discrete) and different number of addressing steps, on the Facebook bAbI tasks. This is the first time an NTM-type model was tested on this task, and we observe that the NTM, especially the proposed D-NTM, performs better than vanilla LSTM-RNN. Furthermore, the experiments revealed that the discrete, hard addressing works better than the soft addressing with the GRU controller, and our analysis reveals that this is the case when the task requires precise retrieval of memory content.\nOur experiments showed that the NTM-based models are weaker than other variants of memory networks which do not learn but have an explicit mechanism of storing incoming facts as they are. We conjecture that this is due to the difficulty in learning how to write, manipulate and delete the content of memory. Despite this difficulty, we find the NTM-based approach, such as the proposed D-NTM, to be a better, future-proof approach, because it can scale to a much longer horizon (where it becomes impossible to explicitly store all the experiences.)\nThe success of both the learnable address and the discrete addressing scheme suggests two future research directions. First, we should try both of these schemes in a wider array of memory-based models, as they are not specific to the neural Turing machines. Second, the proposed D-NTM needs to be evaluated on a diverse set of applications, such as text summarization [30], visual question-answering [31] and machine translation, in order to make an even more concrete conclusion."}, {"heading": "Acknowledgements", "text": "The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Qu\u00e9bec, Compute Canada, Samsung, the Canada Research Chairs and CIFAR. We would also like to thank the developers of Theano 2, for developing such a powerful tool for scientific computing [32]. KC thanks Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016). We would like to thank Li Yao and Julian Serban for the discussions and help."}, {"heading": "9 Appendix", "text": ""}, {"heading": "9.1 Visualization of Discrete Attention", "text": "We visualize the attention of D-NTM with GRU controller with hard attention in Figure 9.1. From this example, we can see that D-NTM has learned to find the correct supporting fact even without any supervision for the particular story in the visualization."}, {"heading": "9.2 Learning Curves for the Recurrent Controller", "text": "In Figure 9.2, we compare the learning curves of the soft and discrete attention D-NTM model with recurrent controller on Task 1. Surprisingly, the discrete attention D-NTM converges faster than the soft-attention model. The main difficulty of learning soft-attention is due to the fact that learning to write with soft-attention can be challenging."}, {"heading": "9.3 Training with Soft-attention and Testing with Hard-attention", "text": "In Table 3, we provide results investigating the effects of using hard attention model at the test-time for a model trained with feed-forward controller and soft attention. Discrete\u2217 D-NTM model bootstraps the discrete attention with the soft attention, using the curriculum method that we have introduced in Section 5. Discrete\u2020 D-NTM model is the soft-attention model which uses hard-attention at the test time. We observe that the Discrete\u2020 D-NTM model which is trained with soft-attention outperforms Discrete D-NTM model."}, {"heading": "9.4 D-NTM with BoW Fact Representation", "text": "In Table 4, we provide results for D-NTM using BoW with positional encoding (PE) [13] as the representation of the input facts. The facts representations are provided as an input to the GRU controller. In agreement to our results with the GRU fact representation, with the BoW fact representation we observe improvements with multi-step of addressing over single-step and discrete addressing over soft-addressing."}], "references": [{"title": "Deep learning. Book in preparation for MIT Press, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "URL http://www.deeplearningbook.org", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1506.03340,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1511.02301,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus"], "venue": "arXiv preprint arXiv:1511.07275,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1506.02075,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "author": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston"], "venue": "CoRR, abs/1511.06931,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "CoRR, abs/1505.00521,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "One-shot learning with memory-augmented neural networks", "author": ["Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy Lillicrap"], "venue": "arXiv preprint arXiv:1605.06065,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Robust estimation of a location parameter", "author": ["Peter J. Huber"], "venue": "Ann. Math. Statist., 35(1):73\u2013101,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1964}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "In Proceedings Of The International Conference on Representation Learning (ICLR 2015),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "The neural network pushdown automaton: Architecture, dynamics and training", "author": ["Guo-Zheng Sun", "C. Lee Giles", "Hsing-Hen Chen"], "venue": "In Adaptive Processing of Sequences and Data Structures, International Summer School on Neural Networks,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings Of The International Conference on Representation Learning (ICLR 2015),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "In Proceedings Of The Conference on Empirical Methods for Natural Language Processing", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1506.07503,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Noisy activation functions", "author": ["Caglar Gulcehre", "Marcin Moczulski", "Misha Denil", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.00391,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "VQA: visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In 2015 IEEE International Conference on Computer Vision,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": ", [1],) there are still a set of complex tasks that are not well addressed by conventional neural networks.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "They include, but are not limited to, episodic question-answering [2, 3, 4], compact algorithms [5] and video caption generation [6].", "startOffset": 66, "endOffset": 75}, {"referenceID": 2, "context": "They include, but are not limited to, episodic question-answering [2, 3, 4], compact algorithms [5] and video caption generation [6].", "startOffset": 66, "endOffset": 75}, {"referenceID": 3, "context": "They include, but are not limited to, episodic question-answering [2, 3, 4], compact algorithms [5] and video caption generation [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "They include, but are not limited to, episodic question-answering [2, 3, 4], compact algorithms [5] and video caption generation [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "On the other hand, neural Turing machines (NTM, [7]) read each fact in an episode and decides whether to read, write the fact or do both to the external, differentiable memory.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "In practice, this leads to easier learning in the memory network, which in turn resulted in it being used more in real tasks [8, 9].", "startOffset": 125, "endOffset": 131}, {"referenceID": 7, "context": "In practice, this leads to easier learning in the memory network, which in turn resulted in it being used more in real tasks [8, 9].", "startOffset": 125, "endOffset": 131}, {"referenceID": 8, "context": "We evaluate the proposed D-NTM on the full set of Facebook bAbI task [2] using either soft, differentiable attention or hard, non-differentiable attention [10] as an addressing strategy.", "startOffset": 155, "endOffset": 159}, {"referenceID": 5, "context": "The proposed dynamic neural Turing machine (D-NTM) extends the neural Turing machine (NTM, [7]) which has a modular design.", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "Although the memory was originally envisioned as an integrated module, it is not necessary, and the memory may be an external, black box [10].", "startOffset": 137, "endOffset": 141}, {"referenceID": 9, "context": ") In this paper, we both use a gated recurrent unit (GRU, [11]) and a feedforward-controller to implement the controller such that", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "No Operation (NOP) As found in [12], an additional NOP action might be beneficial for the controller not to access the memory once in a while.", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "In this paper, we explore an option of allowing each head to operate more than once at each time step, similar to the multi-hop mechanism from the end-to-end memory network [13].", "startOffset": 173, "endOffset": 177}, {"referenceID": 12, "context": "As observed in [14], we find it more easier to learn pure content-based addressing by using a LRU addressing.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "Thus, we use REINFORCE [15] together with the three variance reduction techniques\u2013global baseline, input-dependent baseline and variance normalization\u2013 suggested in [16].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "Thus, we use REINFORCE [15] together with the three variance reduction techniques\u2013global baseline, input-dependent baseline and variance normalization\u2013 suggested in [16].", "startOffset": 165, "endOffset": 169}, {"referenceID": 15, "context": "The baseline network is trained to minimize the Huber loss [17] between the true reward R\u0303(x)\u2217 and the predicted reward b(x).", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "As a further measure to reduce the variance, we regularize the negative entropy of all those category distributions to facilitate a better exploration during training [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 17, "context": "Even with this implicit memory, a vanilla RNN is however known to have difficulties in storing information for long time-spans [19].", "startOffset": 127, "endOffset": 131}, {"referenceID": 18, "context": "Long short-term memory (LSTM, [20]) and gated recurrent units (GRU, [11]) have been found to address this issue.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "Long short-term memory (LSTM, [20]) and gated recurrent units (GRU, [11]) have been found to address this issue.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "In [21, 22, 12], a continuous, differentiable stack was proposed.", "startOffset": 3, "endOffset": 15}, {"referenceID": 20, "context": "In [21, 22, 12], a continuous, differentiable stack was proposed.", "startOffset": 3, "endOffset": 15}, {"referenceID": 10, "context": "In [21, 22, 12], a continuous, differentiable stack was proposed.", "startOffset": 3, "endOffset": 15}, {"referenceID": 3, "context": "In [5, 10], grid and tape storages are used.", "startOffset": 3, "endOffset": 10}, {"referenceID": 8, "context": "In [5, 10], grid and tape storages are used.", "startOffset": 3, "endOffset": 10}, {"referenceID": 11, "context": "Memory networks and their variants have been applied to various tasks successfully [13, 8, 9, 23].", "startOffset": 83, "endOffset": 97}, {"referenceID": 6, "context": "Memory networks and their variants have been applied to various tasks successfully [13, 8, 9, 23].", "startOffset": 83, "endOffset": 97}, {"referenceID": 7, "context": "Memory networks and their variants have been applied to various tasks successfully [13, 8, 9, 23].", "startOffset": 83, "endOffset": 97}, {"referenceID": 21, "context": "Memory networks and their variants have been applied to various tasks successfully [13, 8, 9, 23].", "startOffset": 83, "endOffset": 97}, {"referenceID": 22, "context": "Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].", "startOffset": 152, "endOffset": 160}, {"referenceID": 23, "context": "Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].", "startOffset": 152, "endOffset": 160}, {"referenceID": 24, "context": "Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].", "startOffset": 181, "endOffset": 185}, {"referenceID": 1, "context": "Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].", "startOffset": 217, "endOffset": 220}, {"referenceID": 16, "context": "Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].", "startOffset": 250, "endOffset": 254}, {"referenceID": 25, "context": "We evaluate the proposed D-NTM on the recently proposed episodic question-answering task called Facebook bAbI [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "We train our feed-forward controller using noisy-tanh activation function [28] since we were experiencing training difficulties with sigmoid and tanh activation functions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 27, "context": "We train one D-NTM for each sub-task, using Adam [29] with its learning rate set to 0.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "The fact that the NTM needs to learn to write likely has adverse effect on the overall performance, when compared to, for instance, end-to-end memory networks (MemN2N, [13]) and dynamic memory network (DMN+, [23]) both of which simply store the incoming facts as they are.", "startOffset": 168, "endOffset": 172}, {"referenceID": 21, "context": "The fact that the NTM needs to learn to write likely has adverse effect on the overall performance, when compared to, for instance, end-to-end memory networks (MemN2N, [13]) and dynamic memory network (DMN+, [23]) both of which simply store the incoming facts as they are.", "startOffset": 208, "endOffset": 212}, {"referenceID": 16, "context": "Furthermore, this is in line with an earlier observation in [18], where discrete addressing was found to generalize better in the task of image caption generation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "Second, the proposed D-NTM needs to be evaluated on a diverse set of applications, such as text summarization [30], visual question-answering [31] and machine translation, in order to make an even more concrete conclusion.", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "Second, the proposed D-NTM needs to be evaluated on a diverse set of applications, such as text summarization [30], visual question-answering [31] and machine translation, in order to make an even more concrete conclusion.", "startOffset": 142, "endOffset": 146}], "year": 2016, "abstractText": "In this paper we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both soft, differentiable and hard, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of the Facebook bAbI tasks and shown to outperform NTM and LSTM baselines.", "creator": "LaTeX with hyperref package"}}}