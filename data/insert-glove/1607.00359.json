{"id": "1607.00359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Moving Toward High Precision Dynamical Modelling in Hidden Markov Models", "abstract": "therry Hidden iecc Markov dialing Model (HMM) is 1,493 often ecgric regarded as lightvessel the dynamical hugman model of gigantism choice in noster many zisco fields woudenberg and applications. lunatic It is sensacion also at guigal the gandhar heart amortize of most grosmont state - m\u00f6r\u00f6n of - sherwoods the - diffracted art marchwood speech shtool recognition systems since terza the narnia 70 ' tenax s. 724,000 However, from lebeouf Gaussian brave mixture tehachapi models tagge HMMs (GMM - qiliang HMM) korak to 1177 deep 3,095 neural pelly network HMMs (39-foot DNN - HMM ), the mthembu underlying jeffree Markovian vopat chain of nexavar state - of - tosteson the - devilbiss art connett models o.s.k. did not changed much. The \" all-seated left - suffern to - right \" topology 47.89 is ulo mostly l'orange always employed kabadi because populaire very mirecourt few deqin other alternatives exist. In this enc paper, chippenham we propose that 34.3 finely - hinshaw tuned mahtani HMM topologies dimbleby are bill-clattering essential for jujhar precise kyivstar temporal modelling passes and that visitscotland this sumbana approach should luse be investigated akande in state - greenwalt of - kurlovich the - harming art five-year-old HMM ill-treatment system. brechner As such, murud we kanout\u00e9 propose a proof - kiyohara of - eyed concept quelled framework 2/0 for thief learning fearn efficient topologies lisse by roorda pruning burlakov down complex generic models. dynes Speech recognition pajama experiments that were 646-4850 conducted indicate lotuses that complex heterostructure time dependencies ellwood can be chippie better learned by redbirds this approach keese than washerwoman with jijia classical \" left - to - regreso right \" ustedes models.", "histories": [["v1", "Fri, 1 Jul 2016 19:20:50 GMT  (87kb,D)", "http://arxiv.org/abs/1607.00359v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["s\\'ebastien gagnon", "jean rouat"], "accepted": false, "id": "1607.00359"}, "pdf": {"name": "1607.00359.pdf", "metadata": {"source": "CRF", "title": "Moving Toward High Precision Dynamical Modelling in Hidden Markov Models", "authors": ["S\u00e9bastien Gagnon"], "emails": ["sebastien.gagnon2@usherbrooke.ca,", "jean.rouat@usherbrooke.ca"], "sections": [{"heading": null, "text": "Hidden Markov Model (HMM) is often regarded as the dynamical model of choice in many fields and applications. It is also at the heart of most state-of-theart speech recognition systems since the 70\u2019s. However, from Gaussian mixture models HMMs (GMMHMM) to deep neural network HMMs (DNN-HMM), the underlying Markovian chain of state-of-the-art models did not changed much. The \u201cleft-to-right\u201d topology is mostly always employed because very few other alternatives exist. In this paper, we propose that finely-tuned HMM topologies are essential for precise temporal modelling and that this approach should be investigated in state-of-the-art HMM system. As such, we propose a proof-of-concept framework for learning efficient topologies by pruning down complex generic models. Speech recognition experiments that were conducted indicate that complex time dependencies can be better learned by this approach than with classical \u201cleft-to-right\u201d models."}, {"heading": "1 Introduction", "text": "The correctness of a hidden Markov model\u2019s (HMM) topology can strongly influence the model accuracy of a HMM systems, especially for signals with high\n\u2217Thanks to S. Brodeur and S. Wood, for fruitful discussions and to J.-F. Duval for his insightful review. Partial Funding by NSERC discovery \u2020J. Rouat and S. Gagnon are with the NECOTIS research group, dept. of ECE, Univ. de Sherbrooke, Qc., Canada, J1K 2R1\ndynamic variability. This graphical architecture is usually hand-designed to a simple and generic form (usually shared across all classes), whereas constructing precisely tuned class representations can be challenging.\nIn the 70\u2019s a \u201cleft-to-right\u201d topology was first proposed for speech modelling, meaning that feature changes through time always flowed in a specific sequential order [1]. It is however simplifying considering that spontaneous speech dynamics are known to be very variable [2]. Up to these days, most state-ofthe-art ASR systems such as deep neural networksHMMs (DNN-HMMs, [3]) are still based on that architecture.\nAs states of HMMs encode static feature space distributions, simple HMM topologies can only model coarse dynamics. A dynamic process constructed from static events is as detailed as the number of such events. Too little precision results in an underfitted model with low discriminative power in classification systems. Too much precision, however, can lead to an overfitted model without generalizing power [4], making it unable to recognize anything but the training signals. In model selection, as discussed in [4], the key is to balance precision and generalization for maximum performance.\nRobustness, the ability of a system to tolerate recording environment changes, is also to be considered and seems to be strongly related to a model\u2019s generalizing power [5]. As such, improving model precision would decrease robustness. Online adaptation techniques, however, can easily compensate for\nar X\niv :1\n60 7.\n00 35\n9v 1\n[ cs\n.C L\n] 1\nJ ul\nsuch a drawback. Based on a more comprehensive approach, these methods deal with unseen noise by modifying model statistics at testing time [6, 7, 8]. Performance of such systems in noisy conditions are quite remarkable.\nEven if \u201cleft-to-right\u201d topologies are good for speech signals, some datasets with higher temporal complexity need more precise architectures. The fact, however, is that topologies are usually handdesigned and kept simple, following Occam\u2019s razor principle [9]. Therefore, the main goal of this work is to provide a framework to automatically learn precise HMM topologies from data.\n\u201cLeft-to-right\u201d HDP-HMM [10], a recently developed technique, is one example of such a framework; it is however unbound by Occam\u2019s razor principle or any strong underfitting/overfitting criterion. The expected size of the learned topology being dependant on a concentration parameter chosen a priori, the designer has indirect control over the degree of temporal precision. Furthermore, the model is allowed to both increase or decrease its size according to a Dirichlet process and is thus unconstrained to follow Occam\u2019s principle [9]. On the other hand, this pioneer work allow for the development of HMM speech models without any a priori knowledge of the dynamics, something that is not possible with other approaches.\nWhile adept at learning complex topologies from data, \u201cleft-to-right\u201d HDP-HMM is oblivious to recognition accuracy and how the architecture influences it. Like most HMM training procedures, this is caused by a mismatch between training and decoding alignement methods, i.e. forward-backward vs Viterbi. In [10], for example, some learned monophone topologies allow decoded paths to be as short as 2 time frames long whereas in standard monophone systems the shortest path is 3 time frames. According to our preliminary experiments, this tend to generate insertion errors when decoding, enough to significantly lower accuracy as defined by the word error rate (WER) standard. As shown in TIMIT benchmarks listed in [11], considering insertion errors always significantly decreases the recognition performance of a system. Thus, while useful in approximating the topology needed for each class, \u201cleft-toright\u201d HDP-HMM procedure alone is not ideal for\nour intended goal. Conventional approaches to dynamics encoding with HMMs usually substitute topology learning with transition probabilities estimation. In speech recognition, this is the most popular paradigm: generic \u201cleft-to-right\u201d architectures are adapted to target signal\u2019s dynamics by tuning a few persistent parameters. In [12], such an approach is attempted on a complex generic topology with improved additive noise robustness. However, clean speech performance are not reported, which may suggests that precision has not improved. In fact, improved robustness can be linked to a greater generalizing power, as explored in [5].\nThese results might be explained by an intrinsic problem of HMMs, the imbalance between the dynamic ranges of the transition and emission probabilities. Exposed by Rabiner and Huang in [13], this phenomenon is at the root of the popular thought that transition probabilities are almost useless. It is even a common practice for designers to implement HMM ASR systems with untrained transitions, because the loss in performance is fairly small. Explained in [13] as a lack of pervasive discriminative power of the transition probabilities in path decoding, we conceptualize its effects as rendering equiprobable all transitions that leave the same state. Thus, tuning transition probabilities cannot be a good substitution to complex topology learning.\nIn this work, we first analyse the effects of the imbalance phenomenon. We show that all paths leaving the same state are effectively equiprobable in the standard TIMIT monophone recognition experiment. Thus, topology learning is shown essential for precise dynamics modelling, for which we then propose a simple and accessible framework. Assuming that HMM spoken word models in conventional ASR systems are closer to underfitting than overfitting (a reasoning we based on [2]), we propose to use model flattening [12] in conjunction with transitions pruning to extract precise class topologies. Flattening is the process of transforming a simple \u201cleft-to-right\u201d Gaussian mixture model -HMM (GMM-HMM) into an equivalent complex HMM with single Gaussian emission models. Using transitions pruning to reduce the flatten model complexity then reveals a more precise dy-\nnamic model while still following Occam\u2019s razor principle. We finally demonstrate that with the same number of emission model parameters, our technique clearly outperforms the classic \u201cleft-to-right\u201d topology on clean word recognition tasks."}, {"heading": "2 Transition and Emission", "text": "Probabilities Imbalance\nAs discussed in [13], transition probabilities may not play a significant role in path decoding (using Viterbi); recognition could be entirely independent of them. Then, all transitions leaving the same state could be considered effectively equiprobable during path decoding. To the knowledge of the authors, this phenomenon has not been quantitatively documented for speech recognition. In the token passing implementation of the Viterbi algorithm, the state s(t+ 1) occupied at time t+ 1 is given by [14]:\ns(t+ 1) = argmax k=1,2,...,N [as(t)\u2192kbk(Ot+1)] (1)\nWhere N is the total number of emitting states in the model. Let there be a distinction between zero and non-zero transition probabilities:\n\u2200(i, k) \u2208 [1, ..., N ]; ai\u2192k 6= 0\u2192 k \u2208 \u03d5i (2)\nWere \u03d5i regroups all non-zero transitions leaving state i. Thus, (1) can be reformulated in the following fashion:\ns(t+ 1) = argmax k\u2208\u03d5s(t) [as(t)\u2192kbk(Ot+1)] (3)\nEquation (3) only takes into account states that are linked by a non-zero transition from s(t). Formula (3) can be formulated as follows:\ns(t+ 1) = i if\nbi(Ot+1) bk(Ot+1) > as(t)\u2192k as(t)\u2192i ,\u2200(i, k) \u2208 \u03d5s(t), k 6= i (4)\nLets define: \u03b2 = bi(Ot+1)\nbk(Ot+1) \u03b1 =\nas(t)\u2192k\nas(t)\u2192i\nWe defined \u03b1 and \u03b2 as ratios of probabilities to isolate the respective transition and emission discriminability forces. The variance of these variables, respectively the transition and emission discriminability coefficients, give a good estimate of their dynamic ranges. To evaluate them, we conducted an experiment on the TIMIT training set with conventional 5- states (3 emitting states) \u201cleft-to-right\u201d monophonic models with Gaussian mixture models (GMMs) of 16 components on each state. The procedure is done in 2 steps for each training utterance: first, using the appropriate models (listed in the signal\u2019s label) an ideal path is computed with the forward-backward algorithm [15]. Then, for each transition taken in the decoded path the \u03b1 and \u03b2 values are computed. The variances are then estimated across all the training set. The path alignment method used here, forwardbackward, is not equivalent to (4). In fact, Token passing does not take the backward probability into account and is therefore less optimal. This was done purposefully to favor high emission probabilities in an effort to minimize discriminative power. One must understand that strong model mismatch comes from emission probability values being several orders of magnitude different from one state to another, which far more happens in low probabilities (in mismatched dynamics). In other words, the emission discriminability coefficient calculated is minimized to a level unattainable in practical applications.\n\u03c3(ln(\u03b1)) = 0.80 \u03c3(ln(\u03b2)) = 193.24\nWhere \u03c3(i) is the variance of i. In the linear domain, the standard deviation of \u03b2 is roughly 440,000 times larger than \u03b1. Thus, the transition probabilities are in some sense binary variables, i.e. they are, or not, members of \u03d5s(t) in (4).\nThis is because emission probabilities have a nearinfinite dynamic range, while transition probability do not, for any given s(t). In fact, considering how this problem is exposed in [13], we infer that only in topologies with states of near-infinite branching ratios may this imbalance vanishes. It is therefore safe to assume that all discrete topologies considered in this work are equally affected by this imbalance."}, {"heading": "3 Pruning", "text": "Encoding acoustic dynamic properties in a generic HMM model is to change its topology, i.e. by activating or deactivating transitions. A deactivated transition has a probability value of 0 and is therefore not involved in (4).\nLearning the topology can be done in 3 ways: either \u201cgrowing\u201d from a simple prototype model (ex. [16]), \u201cpruning\u201d from a complex generic model (ex. [17]) or a mixture of both (ex. [10]). With \u201cgrowing\u201d techniques, i.e. increasing the model\u2019s complexity, an almost groundless guess must be made to determine how the expansion is done. This is very much subject to human error.\nOn the other hand, \u201cpruning\u201d processes are much more reliable as one removes only the paths that are not often visited. Mak and Chan [17], for example, have successfully used pruning on a \u201cleft-toright\u201d topology with long range transitions. When compared with an unpruned system, they obtained a significant improvement on the accuracy in a clean word recognition task. Our work follows that line of thinking and implements pruning instead of other alternatives."}, {"heading": "4 Proposed System", "text": ""}, {"heading": "4.1 Integration of the Pruning Module and Threshold Optimization", "text": "A modification of the standard HMM training procedure is proposed for increasing the temporal modelling precision. Fig. 1 illustrates the full proposed system. The pruning (step #5) is done by comparing each individual transition probability with a threshold value, :\nif ai\u2192j > then keep, else ai\u2192j = 0 (5)\nSince the value of is unknown, optimization steps are required during training to find (using the loop step #6 in Fig. 1). The very simple optimization process is designed as such to exploit the steady relation between pruning threshold and performance, thus avoiding unpredictable local minimums."}, {"heading": "4.2 Initialization by Model Flattening", "text": "To maximize the beneficial impact of transition pruning we work on a complex prototype model (high number of states and transitions). However, this can be difficult since the transition parameter space is larger than with simple \u201cleft-to-right\u201d topologies. Furthermore, if it is improper, alternate paths tend to die off during training (i.e. very low occupancy probability) to only favour a single path through the topology. This effectively returns the model to a long \u201cleft-to-right\u201d chain with mediocre performance. Thus, the initialization of a complex HMM model is an important aspect of this work, for which the flattening technique presented in [12] is used. Since a multi-gaussian mixture model can be viewed as an HMM of single-gaussian states, the flattening consists in replacing the states of a \u201cleft-to-right\u201d GMMHMM by their respective HMM\u2019s form. This effectively flattens the representation to a lattice of monogaussian densities, as illustrated in Fig. 2. The final form of a trained \u201cleft-to-right\u201d model is already finetuned to the acoustic properties of the modelled class, it is therefore an ideal configuration."}, {"heading": "4.3 Feedback of Emission Models", "text": "To further increase the recognition performance of the proposed system, the emission models of the pruned HMMs are fed back to the initialization step (link between steps #8 and #3 in Fig.1):\n\u2200i \u2208 [1, ..., N ]; bprei (\u00b7)\u2190 b post i (\u00b7) (6)\nWhere bprei (\u00b7) and b post i (\u00b7) are respectively the emission distributions of the state i at initialization and after training. As complex models require more care to train adequately because of their vast transition parameter space, this step is added to allow slower convergence. We observed experimentally that with about 10 iterations of this \u201cfeedback\u201d process (step #8 in Fig. 1) the recognition performance on the training set seems to saturate. While this step is not required to beat baseline accuracies on clean word recognition, on average it increases performance even further (Table 1). However, it also sharply increases the computational effort required for training."}, {"heading": "5 Experimental Framework", "text": "To evaluate the temporal modelling precision, we chose to view it in terms of recognition accuracy. Our premise is that classical \u201cleft-to-right\u201d modelling of speech dynamics is closer to underfitting than overfitting and therefore improved precision should yield better results. According to [2], this is most likely true for spoken word models. Furthermore, clean speech accuracies (the tested signal was recorded in the same conditions as the training signals) are the main focus since, as explored in [5], noisy recognition seems to deal more with generalizing power than precision.\nFor all conducted tests, we want any increase in recognition accuracy to be entirely attributed to the higher temporal modelling precision. This is done by ensuring that the total amount of Gaussian distributions for each HMM was the same, i.e. the summed amount of GMM mixture components across each model is identical for both the reference and the proposed systems.\nFirst, large dictionary speech recognition applicability is evaluated with a word recognition task with\nmonophonic models on TIMIT. The baseline \u201cleftto-right\u201d 5-states (3 emitting states) models have 10 Gaussian mixture components per state. The proposed system uses flatten versions of the \u201cleft-toright\u201d models and are thus 30 states long with singleGaussian GMMs.\nNext we tested on the Aurora-2 digits word classification task. Since monophonic models should not posses much temporal structure, by linguistic definition, a word recognition task is thought to be more representative of the difference in temporal modelling precision (word models encode much richer dynamics than monophones). The baseline \u201cleft-to-right\u201d HMM models used for the Aurora-2 tasks are 18-states long (16 emitting states) with 3- Gaussian GMMs. The proposed models are 50-states long (48 emitting states) with 1-Gaussian GMM per state. Training is done on the Aurora-2 clean speech \u201cTRAIN\u201d corpus. Noisy recognition tasks are also performed to evaluate the robustness of the technique.\nSince the clean speech recognition accuracies of the reference \u201cleft-to-right\u201d GMM-HMMs on this last dataset are very high (>99%), improvements may not be significant. As a result, we generated 4 new versions of the Aurora-2 digits dataset, each of them convoluted with a different real world reverberation impulse response (IR) taken from the Openair IR database [18]. The IRs are chosen on the basis of their uniqueness. We selected \u201cMaes Howe\u201d, \u201cFalkland Palace Royal Tennis Court\u201d, \u201cPurnode\u2019s Tunnel\u201d and \u201cTyndall Bruce Monument\u201d. For these experiments, both systems are trained on a convoluted version of the \u201cTRAIN\u201d corpus. Clean and noisy testing datasets are also convoluted in the same fashion, which means the latter is corrupted by IRs with its additive noise.\nWord error rates (WER) are computed in the standard fashion, taking into account insertion and deletion errors. For every one of the 3 Aurora-2 sets (A, B, and C), presented WER values are averaged over all additive noise types. Furthermore, noisy tests are averaged over SNRs 20, 15, 10, 5 and 0 dB."}, {"heading": "6 Results and Discussion", "text": "Table 1 shows significant WER reductions with the proposed approach in clean word recognition. When trained and tested in reverberated environments without additive noise, performance are also significantly improved on average. Since reverberation adds time-dependencies to signals and thus increases their temporal complexity, this demonstrates that temporal precision has indeed been increased. However, the proposed system is also less robust to additive noise. This may be explained by a loss of generalizing power caused by the improved precision [5]. To our knowledge, the poor performance with the TIMIT database (shown in Table 2) are best explained by the fact that monophonic HMMs have linguistically little to no temporal structure, as the name implies. Hence, a model specially designed to encode complex temporal behaviours is unsuited to this recognition task.\nWe thus see that monophone-based speech recognition does not seem to profit from models with higher temporal precision while word-based classification does. As discussed in [2], this indicates that a high temporal variability exists at the syllable level. Considering this, we suggest that the proposed approach should perform better in large dictionnary systems with classes representing linguistic units of increased length, such as triphones [11].\nThe proposed proof-of-concept framework could be\nimproved in a number of ways for hypothetical increased performances. First, a less trivial optimization algorithm (see Fig.1) could be used in the pruning iterative mechanism. This could yield very precise pruning strengths leading to high recognition accuracy.\nBetter initialization models and methods could also greatly benefit our solution. As it was discussed earlier, complex and finely-tuned models are very sensitive to their initialization conditions and as such, there is much work to be done in optimizing them. Finally, the proposed framework can also be coupled with complementary state-of-the-art techniques that implement better emission models such as DNNHMMs [3] and online model adaptation [7]."}], "references": [{"title": "Continuous speech recognition by statistical methods", "author": ["F. Jelinek"], "venue": "Proceedings of the IEEE, vol. 64, no. 4, pp. 532 \u2013 56, 1976/04.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1976}, {"title": "Speaking in shorthand-a syllablecentric perspective for understanding pronunciation variation", "author": ["S. Greenberg"], "venue": "Speech Communication, vol. 29, no. 2-4, pp. 159 \u2013 76, 1999/11.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 20, no. 1, pp. 14 \u2013 22, 2012/01.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "On over-fitting in model selection and subsequent selection bias in performance evaluation", "author": ["Gavin C. Cawley", "Nicola L.C. Talbot"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 2079 \u2013 2107, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "A study on the generalization capability of acoustic models for robust speech recognition", "author": ["Xiong Xiao", "Jinyu Li", "Eng Siong Chng", "Haizhou Li", "Chin-Hui Lee"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 18, no. 6, pp. 1158 \u2013 69, 2010/08.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Noise adaptive training for robust automatic speech recognition", "author": ["Ozlem Kalinli", "Michael L. Seltzer", "Jasha Droppo", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 18, no. 8, pp. 1889 \u2013 1901, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1889}, {"title": "High-performance HMM adaptation with joint compensation of additive and convolutive distortions via vector taylor series", "author": ["Jinyu Li", "Li Deng", "Dong Yu", "Yifan Gong", "A. Acero"], "venue": "2007 IEEE Workshop on Automatic Speech Recognition and Understanding, Piscataway, NJ, USA, 2007//, pp. 65 \u2013 70.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving robustness of deep neural network acoustic models via speech separation and joint adaptive training", "author": ["A. Narayanan", "DeLiang Wang"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 23, no. 1, pp. 92 \u2013 101, 2015/01.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A left-to-right HDP-HMM with HDPM emissions", "author": ["A.H.H.N. Torbati", "J. Picone", "M. Sobel"], "venue": "2014 48th Annual Conference on Information Sciences and Systems (CISS), Piscataway, NJ, USA, 2014, p. 6 pp.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Stranded gaussian mixture hidden markov models for robust speech recognition", "author": ["Yong Zhao", "Biing-Hwang Juang"], "venue": "Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2012), Piscataway, NJ, USA, 2012, pp. 4301 \u2013 4.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Hidden markov models for speech recognition - strengths and limitations", "author": ["L.R. Rabiner", "B.H. Juang"], "venue": "Speech Recognition and Understanding. Recent Advances, Trends and Applica- 7  tions. Proceedings of the NATO Advanced Study Institute, pp. 3 \u2013 29, 1992.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "Token passing: a simple conceptual model for connected speech recognition systems", "author": ["S.J. Young", "N.H. Russell", "J.H.S. Thornton"], "venue": "1989.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, no. 2, pp. 257 \u2013 86, 1989/02/.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1989}, {"title": "Automatic generation of non-uniform hmm structures based on variational bayesian approach", "author": ["T. Jitsuhiro", "S. Nakamura"], "venue": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing, Piscataway, NJ, USA, 2004, vol. vol.1, pp. 805 \u2013 8.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Pruning hidden markov models with optimal brain surgeon", "author": ["B. Mak", "Kin-Wah Chan"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 13, no. 5, pp. 993 \u2013 1003, Sept. 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Open acoustic impulse response (open air) library", "author": ["D. Murphy", "S. Shelley"], "venue": ". 8", "citeRegEx": "18", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "In the 70\u2019s a \u201cleft-to-right\u201d topology was first proposed for speech modelling, meaning that feature changes through time always flowed in a specific sequential order [1].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "It is however simplifying considering that spontaneous speech dynamics are known to be very variable [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "Up to these days, most state-ofthe-art ASR systems such as deep neural networksHMMs (DNN-HMMs, [3]) are still based on that architecture.", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Too much precision, however, can lead to an overfitted model without generalizing power [4], making it unable to recognize anything but the training signals.", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "In model selection, as discussed in [4], the key is to balance precision and generalization for maximum performance.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "Robustness, the ability of a system to tolerate recording environment changes, is also to be considered and seems to be strongly related to a model\u2019s generalizing power [5].", "startOffset": 169, "endOffset": 172}, {"referenceID": 5, "context": "Based on a more comprehensive approach, these methods deal with unseen noise by modifying model statistics at testing time [6, 7, 8].", "startOffset": 123, "endOffset": 132}, {"referenceID": 6, "context": "Based on a more comprehensive approach, these methods deal with unseen noise by modifying model statistics at testing time [6, 7, 8].", "startOffset": 123, "endOffset": 132}, {"referenceID": 7, "context": "Based on a more comprehensive approach, these methods deal with unseen noise by modifying model statistics at testing time [6, 7, 8].", "startOffset": 123, "endOffset": 132}, {"referenceID": 8, "context": "\u201cLeft-to-right\u201d HDP-HMM [10], a recently developed technique, is one example of such a framework; it is however unbound by Occam\u2019s razor principle or any strong underfitting/overfitting criterion.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "In [10], for example, some learned monophone topologies allow decoded paths to be as short as 2 time frames long whereas in standard monophone systems the shortest path is 3 time frames.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "In [12], such an approach is attempted on a complex generic topology with improved additive noise robustness.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "In fact, improved robustness can be linked to a greater generalizing power, as explored in [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 10, "context": "Exposed by Rabiner and Huang in [13], this phenomenon is at the root of the popular thought that transition probabilities are almost useless.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "Explained in [13] as a lack of pervasive discriminative power of the transition probabilities in path decoding, we conceptualize its effects as rendering equiprobable all transitions that leave the same state.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "Assuming that HMM spoken word models in conventional ASR systems are closer to underfitting than overfitting (a reasoning we based on [2]), we propose to use model flattening [12] in conjunction with transitions pruning to extract precise class topologies.", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "Assuming that HMM spoken word models in conventional ASR systems are closer to underfitting than overfitting (a reasoning we based on [2]), we propose to use model flattening [12] in conjunction with transitions pruning to extract precise class topologies.", "startOffset": 175, "endOffset": 179}, {"referenceID": 10, "context": "As discussed in [13], transition probabilities may not play a significant role in path decoding (using Viterbi); recognition could be entirely independent of them.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "In the token passing implementation of the Viterbi algorithm, the state s(t+ 1) occupied at time t+ 1 is given by [14]:", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "The procedure is done in 2 steps for each training utterance: first, using the appropriate models (listed in the signal\u2019s label) an ideal path is computed with the forward-backward algorithm [15].", "startOffset": 191, "endOffset": 195}, {"referenceID": 10, "context": "In fact, considering how this problem is exposed in [13], we infer that only in topologies with states of near-infinite branching ratios may this imbalance vanishes.", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "[16]), \u201cpruning\u201d from a complex generic model (ex.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17]) or a mixture of both (ex.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Mak and Chan [17], for example, have successfully used pruning on a \u201cleft-toright\u201d topology with long range transitions.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "Thus, the initialization of a complex HMM model is an important aspect of this work, for which the flattening technique presented in [12] is used.", "startOffset": 133, "endOffset": 137}, {"referenceID": 1, "context": "According to [2], this is most likely true for spoken word models.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Furthermore, clean speech accuracies (the tested signal was recorded in the same conditions as the training signals) are the main focus since, as explored in [5], noisy recognition seems to deal more with generalizing power than precision.", "startOffset": 158, "endOffset": 161}, {"referenceID": 15, "context": "As a result, we generated 4 new versions of the Aurora-2 digits dataset, each of them convoluted with a different real world reverberation impulse response (IR) taken from the Openair IR database [18].", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "This may be explained by a loss of generalizing power caused by the improved precision [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "As discussed in [2], this indicates that a high temporal variability exists at the syllable level.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Finally, the proposed framework can also be coupled with complementary state-of-the-art techniques that implement better emission models such as DNNHMMs [3] and online model adaptation [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "Finally, the proposed framework can also be coupled with complementary state-of-the-art techniques that implement better emission models such as DNNHMMs [3] and online model adaptation [7].", "startOffset": 185, "endOffset": 188}], "year": 2016, "abstractText": "Hidden Markov Model (HMM) is often regarded as the dynamical model of choice in many fields and applications. It is also at the heart of most state-of-theart speech recognition systems since the 70\u2019s. However, from Gaussian mixture models HMMs (GMMHMM) to deep neural network HMMs (DNN-HMM), the underlying Markovian chain of state-of-the-art models did not changed much. The \u201cleft-to-right\u201d topology is mostly always employed because very few other alternatives exist. In this paper, we propose that finely-tuned HMM topologies are essential for precise temporal modelling and that this approach should be investigated in state-of-the-art HMM system. As such, we propose a proof-of-concept framework for learning efficient topologies by pruning down complex generic models. Speech recognition experiments that were conducted indicate that complex time dependencies can be better learned by this approach than with classical \u201cleft-to-right\u201d models.", "creator": "LaTeX with hyperref package"}}}