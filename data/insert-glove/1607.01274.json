{"id": "1607.01274", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2016", "title": "Temporal Topic Analysis with Endogenous and Exogenous Processes", "abstract": "sidransky We consider the problem of mandalika modeling limpar temporal textual codirector data 3450 taking australia/new endogenous ferley and exogenous processes into 8,480 account. Such text paxil documents hartlepool arise in faulhaber real world sstl applications, hells including job belew advertisements and hdw economic direct-entry news yellow-crowned articles, prioritising which miz are 18-25 influenced houck by discala the 39-06 fluctuations of luso the general tuttle economy. We propose a hierarchical epigraphs Bayesian teruya topic riqueza model which imposes sundarban a \" group - abounds correlated \" hierarchical lgv structure on the evolution of to-2 topics downbeats over investigates time grego incorporating heartiest both processes, netweaver and show that this model ikk\u014d-ikki can saytiev be estimated from olympiodorus Markov feni chain Monte Carlo 40,250 sampling methods. adilabad We further magical demonstrate zlatko that abubakari this north-eastwards model pavlovi\u0107 captures the intrinsic relationships between the topic distribution hocker and well-preserved the time - dependent lalu factors, dourado and 1.30 compare pulumur its weight-saving performance terrarum with kirchen latent chile Dirichlet nicoll allocation (upgrades LDA) and two i.r.a. other related models. chekhov The indo-aryans model gp9 is applied hunch to qilada two karytaina collections 21/22 of eadulf documents gald\u00f3s to illustrate its zeev empirical chloroquine performance: online reichling job advertisements from eastville DirectEmployers Association and cetatea journalists ' taradale postings pavkovic on certificated BusinessInsider. com.", "histories": [["v1", "Mon, 4 Jul 2016 01:16:55 GMT  (347kb,D)", "http://arxiv.org/abs/1607.01274v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["baiyang wang", "diego klabjan"], "accepted": true, "id": "1607.01274"}, "pdf": {"name": "1607.01274.pdf", "metadata": {"source": "META", "title": "Temporal Topic Analysis with Endogenous and Exogenous Processes", "authors": ["Baiyang Wang", "Diego Klabjan"], "emails": ["baiyang@u.northwestern.edu", "d-klabjan@northwestern.edu"], "sections": [{"heading": "1 Introduction", "text": "Many organizations nowadays provide portals for job posting and job search, such as glassdoor.com from Glassdoor, indeed.com from Recruit, and my.jobs from DirectEmployers Association. Our work is inspired by data collected from the portal my.jobs, a website where job seekers can apply to the posted job openings through a provided link. The data collected from the website includes user clickstreams (users create accounts on the site) and attributes of job advertisements, such as their description, location, company name, and posted date.\nIn this paper, we investigate the relationship between economic fluctuations and the related changes in job advertisements, which can reveal the economic conditions of different time periods. More generally, this question is about the influence of any exogenous process on textual data with temporal dimensions. We adopt the perspective that the documents are organized into a certain number of topics, and study the impact of the exogenous process on the topic distribution, i.e. the relative topic proportions. Given a corpus of text\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ndocuments with time stamps and a related exogenous process, the problem is to find a relationship between the topics discussed and the exogenous process. This setting is natural in an economic context; for instance, changes in macroeconomic indicators have an impact on government reports and Wall Street Journal news articles. Meanwhile, we also notice that for most temporal documents, the topic proportions change over time, which indicates an endogenous process of topic evolution.\nWith the goal of establishing topic dependency on the endogenous and exogenous processes, LDA-type topic models are especially suitable. The latent Dirichlet allocation (LDA) (Blei, Ng, and Jordan 2003) is the original model. Since then, a large number of variants have been proposed, many of which can be found in Blei (2011).\nMeanwhile, there has been relatively limited discussion on modeling time-dependent documents when there are relevant simultaneous exogenous processes. Many timedependent topic models without the exogenous component have been proposed, such as the Topics over Time (ToT) model (Wang and McCallum 2006) and the dynamic topic model (DTM) (Blei and Lafferty 2006), to name a few. However, to the best of our knowledge, none of these papers incorporate the effect of exogenous processes. On the other hand, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2015) considers the effect of metadata, i.e. the attributes specified for each document, on the topic distribution. While STM can be applied for mining time-dependent textual data with exogenous covariates, it does not explicitly consider the time factor or the endogenous topic evolution processes of time-stamped documents.\nOur approach to this problem is to incorporate both endogenous and exogenous processes into a topic model. For the endogenous part of our paper, we impose a Markovian structure on the topic distribution over time, similar to Blei and Lafferty (2006) and Dubey et al. (2014). For the exogenous process, we incorporate it into the topic distribution in each period, adjusting the endogenous topic evolution process. In this way, our model is essentially a stick-breaking truncation of a \u201dgroup-correlated\u201d hierarchical Dirichlet process. Our model has the following contributions: (i) it addresses the question of measuring the influence of exogenous processes on the topics in related documents, (ii) it incorporates both endogenous and exogenous aspects,\nar X\niv :1\n60 7.\n01 27\n4v 1\n[ cs\n.C L\n] 4\nJ ul\n2 01\n6\nand (iii) it demonstrates that text mining can also have useful implications in the realm of economics, which, from the authors\u2019 perspective, is a relatively new finding.\nSection 2 offers a brief review on the topic modeling techniques related to our model. Section 3 develops our hierarchical Bayesian model and describes how to make posterior inferences with a variant of the Markov chain Monte Carlo (MCMC) technique. Section 4 studies the online job advertisements from DirectEmployers Association and journalists\u2019 postings in finance on BusinessInsider.com with our proposed method, providing a comparison of performance with the standard LDA and STM. Section 5 suggests possible directions for the future and concludes the paper."}, {"heading": "2 Review of Time-Dependent Topic Modeling", "text": "We first introduce the standard model of LDA (Blei, Ng, and Jordan 2003). Suppose that there is a collection of documents di, i = 1, . . . , N and words {xi,j}Jij=1 within each document di indexed by a common dictionary containing V words, where N is the number of documents, and Ji is the number of words in di. The LDA model is as follows,{\n\u03b8i iid\u223c Dir(\u03b1), \u03c6k iid\u223c Dir(\u03b2), zi,j |\u03b8i iid\u223c Cat(\u03b8i), xi,j |zi,j \u223c Cat(\u03c6zi,j ). (1)\nHere i = 1, . . . , N , j = 1, . . . , Ji, k = 1, . . . ,K; \u03b8i is the length-K per-document topic distribution for di, \u03c6k is the length-V per-topic word distribution for the k-th topic, zji is the topic for the j-th word in di, and K is the number of topics. Dir(\u00b7) denotes the Dirichlet distribution and Cat(\u00b7) denotes the categorical distribution, a special case of the multinomial distribution when nobs = 1.\nThe Dirichlet process is a class of randomized probability measures and can be applied for non-parametric modeling of mixture models. Denoting the concentration parameter by \u03b1 and the mean probability measure by H , a realization G from the Dirichlet process can be written as G \u223c DP (\u03b3,H). With the stick-breaking notation (Sethuraman 1994), we have\nG = \u221e\u2211 k=1 bk\u03b4\u03d5k , (2)\nwhere \u03b4\u03d5k is a \u201cdelta\u201d probability measure with all the probability mass placed at \u03d5k, \u03d5k iid\u223c H , bk = b\u2032k \u220fk\u22121 i=1 (1 \u2212 b\u2032i), b \u2032 k\niid\u223c Beta(1, \u03b3), k = 1, 2, \u00b7 \u00b7 \u00b7 . We write b = (b1, b2, . . .) \u223c Stick(\u03b3). More properties of the Dirichlet process can be found in Ferguson (1973).\nA hierarchical Dirichlet process (HDP) was proposed in the context of text modeling by Teh et al. (2005). The following hierarchical structure is assumed, G0|\u03b3 \u223c DP (\u03b3,H), G1, . . . , GN |(\u03b1,G)\niid\u223c DP (\u03b1,G), \u03c6i,j |Gi iid\u223c Gi, xi,j \u223c Cat(\u03c6i,j). (3)\nHere i = 1, . . . , N , j = 1, . . . , Ji. The length-V random vectors G0, G1, . . . , GN are \u201crandom word distributions,\u201d each of which is a draw from a Dirichlet process in (3). Moreover, each draw from a random word distribution is a length-V fixed vector \u03c6i,j ; it is the word distribution for xi,j . The posterior inference can be achieved by different strategies of Gibbs sampling.\nThere are mainly two approaches in the literature of measuring endogenous topic evolution processes. One approach is to impose a finite mixture structure on the topic distribution: a dynamic hierarchical Dirichlet process (dHDP) (Ren, Dunson, and Carin 2008) was proposed by adding a temporal dimension, and its variation was further applied on topic modeling with a stick-breaking truncation of Dirichlet processes (Pruteanu-Malinici et al. 2010). The other approach imposes a Markovian structure. For instance, the dynamic topic model (DTM) (Blei and Lafferty 2006) is as follows, \u03c6t,k|\u03c6t\u22121,k \u223c N(\u03c6t\u22121,k, \u03c32I), \u03b1t|\u03b1t\u22121 \u223c N(\u03b1t\u22121, \u03b42I), \u03b8t,i|\u03b1t iid\u223c N(\u03b1t, a2I), zt,i,j |\u03b8t,i\niid\u223c Cat(exp(\u03b8t,i)), xt,i,j |zt,i,j \u223c Cat(exp(\u03c6t,zt,i,j )).\n(4)\nHere t = 1, . . . , T , i = 1, . . . , Nt, j = 1, . . . , Jt,i, k = 1, . . . ,K (t \u2265 2 for the first two equations); T is the number of time periods, Nt is the number of documents in the t-th period, and Jt,i is the number of words in the i-th document in the t-th period; the rest are similarly defined as in LDA. One major difference between DTM and LDA is that the topic distributions \u03b8t,i and word distributions \u03c6t,k are in log-scale in DTM. A variational Kalman filtering was proposed for the posterior inference. As this Markovian approach is simpler for both interpretation and posterior inference, we apply a more generalized version of it to specify the endogenous process in our model.\nThe structural topic model (STM) (Roberts, Stewart, and Airoldi 2015) measures the effect of metadata of each document with the logistic normal distribution. Their model for each document di is as follows, \u03b8i|(Xi\u03b3,\u03a3) \u223c LogisticNormal(Xi\u03b3,\u03a3), p(\u03c6i,k) \u221d exp(m+ \u03bak + \u03bagi + \u03bakgi), zi,j |\u03b8i iid\u223c Cat(\u03b8i), xi,j |zi,j \u223c Cat(\u03c6i,zi,j ), (5)\nwhere i = 1, . . . , N , j = 1, . . . , Ji; Xi is the metadata matrix, \u03b3 is a coefficient vector, \u03a3 is the covariance matrix,\u03c6i,k is the word distribution for di and the k-th topic,m is a baseline log-word distribution, \u03bak, and \u03bagi and \u03bakgi are the topic, group, and interaction effects; the rest are defined similarly to LDA. This model explicitly considers exogenous factors, and can be applied to find the relationship between topic distributions and exogenous processes. Below we adopt a slightly more general approach, incorporating both endogenous and exogenous factors."}, {"heading": "3 Model and Algorithm", "text": ""}, {"heading": "3.1 Motivation: A Group-Correlated Hierarchical Dirichlet Process", "text": "We formulate our problem as follows: we are given time periods t = 1, . . . , T , documents from each period dt,i, i = 1, . . . , Nt, t = 1, . . . , T , and the indices of words {xt,i,j} Jt,i j=1 within each document dt,i from the first word to the last. The words are indexed by a dictionary containing V words in total. We begin with a hierarchical Dirichlet process in time 1: let G1|\u03b3 \u223c DP (\u03b3,H), G1i|(\u03b11, G1) \u223c DP (\u03b11, G1), where G1 is a baseline random word distribution for time 1, and G1i is the random word distribution for document d1i. For G2, . . . GT , we have the following Markovian structure,\np(Gt)|Gt\u22121 \u221d exp[\u2212d(Gt, Gt\u22121)], t = 2, . . . , T. (6)\nHere d(\u00b7, \u00b7) is some distance between two probability measures. This completes our endogenous process. To take an exogenous process {yt}Tt=1 into account, we assume the following\nG\u0303t =M(Gt,yt), t = 1, . . . , T, (7)\nwhereM maps the endogenous baseline random word distribution Gt to the realized baseline random word distribution G\u0303t for time t, considering the influence of {yt}Tt=1. Therefore, we further assume that each per-document random word distribution Gt,i is sampled with mean G\u0303t rather than Gt. The final model is as follows, G1|\u03b3 \u223c DP (\u03b3,H), p(Gt)|Gt\u22121 \u221d exp[\u2212d(Gt, Gt\u22121)], G\u0303t =M(Gt,yt), Gt,i|(\u03b1t, G\u0303t)\niid\u223c DP (\u03b1t, G\u0303t), \u03c6t,i,j |Gt,i iid\u223c Gt,i, xt,i,j \u223c Cat(\u03c6t,i,j).\n(8)\nHere t = 1, . . . , T , i = 1, . . . , Nt, j = 1, . . . , Jt,i (t \u2265 2 for the first line). Throughout this paper, our model is fully conditional on {yt}Tt=1, i.e. we assume {yt}Tt=1 to be fixed; this has an intuitive explanation, as our temporal documents represent a very small portion of the underlying environment, i.e. the exogenous process, so their influence on {yt}Tt=1 is almost negligible."}, {"heading": "3.2 A Group-Correlated Temporal Topic Model: Stick-Breaking Truncation", "text": "Below we consider a stick-breaking truncation of the model above, since posterior inference of the exact model can be intricate. With the stick-breaking expression of G1 in (8), we have  \u03c61,\u03c62, . . . , iid\u223c H,\n\u03c01 = (\u03c011, \u03c012, . . .) \u223c Stick(\u03b3), G1 = \u2211\u221e k=1 \u03c01k\u03b4\u03c6k .\n(9)\nHere we set d(\u00b7, \u00b7) = +\u221e if the two probability measures have different supports; this necessitates that all periods share the same topics. Our intent is that the topics should remain the same to investigate their relationships with endogenous and exogenous processes; otherwise, changes in topics can blur the relationships and possibly result in overfitting. We apply the total variation distance d(p, q) = \u03bb \u00b7 \u222b |p\u2212 q|d\u00b5 with \u03bb > 0, although many others can also be applied and lead to, for instance, a log-normal model in DTM, or a normal model (Dubey et al. 2014; Zhang, Kim, and Xing 2015). We have the following, Gt = \u2211\u221e k=1 \u03c0tk\u03b4\u03c6k , \u03c0tk = \u03c0t\u22121 k + Lap(\u03bb),\n\u03c0t = (\u03c0t1, \u03c0t2, . . .).\n(10)\nHere t = 2, . . . , T , k = 1, 2, . . ., and Lap(\u03bb) denotes a Laplacian distribution with scale parameter \u03bb. For the exogenous part, we consider specifying the relationship between \u03c0t and \u03c0\u0303t = (\u03c0\u0303t1, \u03c0\u0303t2, . . .) such that G\u0303t =\u2211\u221e k=1 \u03c0\u0303tk\u03b4\u03c6k , Gt,i = \u2211\u221e k=1 \u03b8t,i,k\u03b4\u03c6k . We let \u03c0\u0303t = \u03c0t + \u03b7 \u00b7 yt, t = 1, . . . , T, 1\u2032 \u00b7 \u03b7 = 0. (11) Here \u03b7 is a K \u00d7 p matrix which indicates the relationship between the topic distribution \u03c0\u0303t and the length-p vector yt. However, we notice that \u03c0\u0303t and \u03c0t are of infinite length, which creates difficulty in our inference. Therefore we adopt a stick-breaking truncation approach, i.e. we only consider {\u03c6k}Kk=1 in our model; the probability weights for {\u03c6k}\u221ek=K+1 in \u03c0t will be added into \u03c0tK . We note that a number of papers in topic modeling have put this approach into practice (Pruteanu-Malinici et al. 2010; Wang, Paisley, and Blei 2011).\nIt has been shown (Pruteanu-Malinici et al. 2010) that when the truncation levelK is large, we may as well replace the distribution of \u03c01 with \u03c01 \u223c Dir(\u03b3\u03c00), where \u03b3 = 1, \u03c00 = (1/K, . . . , 1/K). We also let H = Dir(\u03b2, . . . , \u03b2) as in the paper by Teh et al. (2005). We summarize our model, \u03c61, . . . ,\u03c6K iid\u223c Dir(\u03b2, . . . , \u03b2), \u03c01 \u223c Dir(\u03b3\u03c00), \u03c0tk = \u03c0t\u22121 k + Lap(\u03bb), \u03c0\u0303t = \u03c0t + \u03b7 \u00b7 yt, \u03b8t,i|(\u03b1t, \u03c0\u0303t)\niid\u223c Dir(\u03b1t\u03c0\u0303t), zt,i,j |\u03b8t,i iid\u223c Cat(\u03b8t,i), xt,i,j \u223c Cat(\u03c6zt,i,j ).\n(12)\nHere t = 1, . . . , T , i = 1, . . . , Nt, j = 1, . . . , Jt,i (t \u2265 2 for the second line). The last two lines above are derived as in Teh et al. (2005). We note that here \u03c6k is the per-topic word distribution, \u03b8t,i is the per-document topic distribution, and zt,i,j is the actual topic for each word; they have the same meaning as in LDA.\nWe name our model a \u201cgroup-correlated temporal topic model\u201d (GCLDA). Here a \u201cgroup\u201d stands for all the documents within the same time period. We use the term \u201dcorrelated\u201d because the baseline topic distributions {\u03c0t}Tt=1 for each period, controlling for {yt}Tt=1, are endogenously correlated; meanwhile, the realized baseline topic distributions {\u03c0\u0303t}Tt=1 for each period are also correlated with the given exogenous process {yt}Tt=1."}, {"heading": "3.3 Sampling the posterior: An MCMC Approach", "text": "Direct estimation of the Bayesian posterior is often intractable since the closed-form expression, if it exists, can be difficult to integrate and thus, many approaches to approximate the posterior have been proposed. Monte Carlo methods, which draw a large number of samples from the posterior as its approximation, are particularly helpful. In this paper, we adopt the Markov chain Monte Carlo (MCMC) approach which constructs samples from a Markov chain and is asymptotically exact. Below we provide the Metropoliswithin-Gibbs sampling approach tailored to our situation, which is a variant of the general MCMC approach. It only requires specifying the full conditionals of the unknown variables, which is covered below.\nWe consider sampling the following variables Z = {zt,i,j} Jt,i j=1 Nt i=1 T t=1, {\u03b1t}Tt=1, {\u03c0\u0303t}Tt=1, \u03b7, \u03bb. We integrate out {\u03b8t,i}Nti=1 Tt=1 and {\u03c6k}Kk=1 to speed up calculation. Following Griffiths and Steyvers (2004), conditioning on all other variables listed for sampling,\np(zt,i,j = k|rest) \u221d (C(\u22121)t,i,k + \u03b1t\u03c0\u0303tk) C\n(\u22121) xt,i,k,k + \u03b2 C (\u22121) k + V \u03b2 . (13)\nHere C(\u22121)t,i,k is the count of elements inZ\\{zt,i,j} which belong to dt,i and has values equal to k; C (\u22121) xt,i,k,k\nis the count of elements in Z\\{zt,i,j} whose values are k and corresponding words are xt,i,j ; C (\u22121) k is the count of elements in Z\\{zt,i,j} whose values are k. Also following Griffiths and Steyvers (2004), we have for \u03b1t and \u03c0\u0303t\np(\u03b1t, \u03c0\u0303t|rest) \u221d p(Z|\u03b1t, \u03c0\u0303t)p(\u03c01,...,T |\u03b3,\u03c00)p(\u03b1t) \u221d [\n\u0393(\u03b1t)\u220f k \u0393(\u03b1t\u03c0\u0303tk) ]Nt Nt\u220f i=1 \u220f k \u0393(Ct,i,k + \u03b1t\u03c0\u0303tk) \u0393(Jt,i + \u03b1t)\n\u00d7 exp [\u2212\u03bb (1t<T \u00b7 \u2016\u03c0t+1 \u2212 \u03c0t\u20161 + 1t>1 \u00b7 \u2016\u03c0t \u2212 \u03c0t\u22121\u20161)] \u00d7 p(\u03c01|\u03b3\u03c00)p(\u03b1t). (14)\nHere the difference between Ct,i,k and C (\u22121) t,i,k is to replace Z\\{zt,i,j} with Z. We also view \u03c0t, \u03c0tk, etc. as functions of the other parameters; specifically, \u03c0tk = \u03c0\u0303tk \u2212 \u03b7kyt, where \u03b7k is the k-th row of \u03b7. This involves a transformation of variables; however, the related Jacobian determinant det(J) = 1, so (14) is still valid. For parameter \u03b7, we have\np(\u03b7|rest) \u221d T\u220f t=2 p(\u03c0t|\u03c0t\u22121, \u03bb) \u00b7 p(\u03b7)\n\u221d exp ( \u2212\u03bb \u00b7\nT\u2211 t=2 \u2016\u03c0t \u2212 \u03c0t\u22121\u20161\n) p(\u03b7). (15)\nFinally, for parameter \u03bb, we have\np(\u03bb|rest) \u221d T\u220f t=2 p(\u03c0t|\u03c0t\u22121, \u03bb) \u00b7 p(\u03bb)\n\u221d \u03bb(T\u22121)K exp ( \u2212\u03bb \u00b7\nT\u2211 t=2 \u2016\u03c0t \u2212 \u03c0t\u22121\u20161\n) p(\u03bb). (16)\nWe note that (13) and (16) are full conditionals, and we can easily derive the full conditionals of \u03b1t, \u03c0tk, and \u03b7k from (14) and (15). Since each zt,i,j |rest has a categorical distribution, and \u03bb|rest has a Gamma distribution with a conjugate prior, they can be updated with Gibbs updates. For \u03b1t, \u03c0tk, and \u03b7k, we replace a Gibbs update with a Metropolis update. Specifically, suppose we know p(par|rest) up to a multiplicative constant, where par is any length-1 parameter. We also assume par = par(r) at the r-th iteration. Then at the (r + 1)-th iteration,\nparnew \u223c q(\u00b7|par(r)),\npar(r+1) = { parnew with prob. P, par(r) with prob. 1\u2212 P, (17)\nwhere P = min{p(parnew|rest)/p(par(r)|rest), 1}, and q(\u00b7|\u00b7) is a known conditional probability distribution such that q(x|y) = q(y|x).\nThis completes our sampling and posterior inference. For the theoretical convergence properties of Metropolis-withinGibbs samplers, the reader can refer to Robert and Casella (2004) and Roberts and Rosenthal (2006)."}, {"heading": "4 Case Studies", "text": "The proposed model, \u201cGCLDA,\u201d is demonstrated on two data sets: (1) online job advertisements from my.jobs from February to September in 2014, and (2) journalists\u2019 postings in 2014 in the \u201cFinance\u201d section in BusinessInsider.com, an American business and technology news website. Our algorithm has been implemented in Java, and we compare GCLDA with LDA, ToT, and STM."}, {"heading": "4.1 Experiment Settings", "text": "We initialize the hyperparameters of LDA as follows: \u03b1 = (50/K, . . . , 50/K), \u03b2 = (0.01, . . . , 0.01), according to a rule of thumb which has been carried out in Berry and Kogan (2010) and Sridhar (2015). For ToT, we use the same \u03b1 and \u03b2 and linearly space the timestamps to make computation feasible. For GCLDA, we let \u03b3 = 1,\u03c00 = (1/K, . . . , 1/K), \u03b1t iid\u223c \u0393(1, 1), p(\u03b7) \u221d e\u22120.01 \u2211 |\u03b7k|, \u03bb \u223c \u0393(1, 1), \u03b2 = 0.01. We carry out the Metropolis-within-Gibbs algorithm as described in Section 3.3 for GCLDA, and run 5,000 iterations of the Markov chain with 1,000 burn-in samples for GCLDA, LDA, and ToT; for LDA, we apply the collapsed Gibbs sampling as in Griffiths and Steyvers (2004). The number of topics is set to K = 50 for both data sets. For STM, we apply the \u201cSpectral\u201d initialization (Roberts, Stewart, and Tingley 2015) together with other default settings in the R package stm. We perform data cleaning, remove the stopwords, stem the documents, and keep most frequent V words in each study. For the job advertisements, V = 2,000 and covers 96.2% of all words with repetition, which means that the choice of words in job advertisements is quite narrow; for the journalists\u2019 postings, V = 3,000 and covers 93.9% of all words with repetition.\nWe use perplexity to compare the difference of the prediction power between LDA, ToT, and GCLDA. The perplexity for Ntest held-out documents given the training data D is defined as\nperp = exp { \u2212 \u2211Ntest i=1 log p(dtest,i|D)\u2211Ntest\ni=1 ntest,i\n} (18)\nwhere dtest,i represents the i-th held-out document, and ntest,i is the number of words in dtest,i. We expect the perplexity to be small when a model performs well, since this means that under the estimated model, the probability of a word in the testing documents being written a priori is large. We apply the \u201cLeft-to-right\u201d algorithm (Wallach et al. 2009) and apply point estimates for \u201c\u03a6\u201d and \u201c\u03b1m\u201d using the training data, as suggested in Section 3 in the same paper."}, {"heading": "4.2 My.jobs: Online Job Advertisements", "text": "The number of online job advertisements on my.jobs from February to September in 2014 amounts to 17,147,357 in total, and the number of advertisements each day varies greatly. Therefore, we gather a stratified sample of 44,660 advertisements with a roughly equal number of samples for each day, so that we have sampled 0.26% of all the documents in total. The training data set consists of 40,449 advertisements, and the testing data set consists of 4,211 advertisements (9.4% of the sample). For the exogenous variable {yt}Tt=1, we use the standardized Consumer Price Index from February to September in 2014, so that p = 1, and T = 8.\nFigure 1 implies that GCLDA better predicts the words in the new documents in terms of perplexity. This is due to the fact that the introduction of both endogenous and exogenous processes allows us to make more accurate inference on the topic distributions of the documents in a given period of time. The standard errors for the perplexity in each period are also shown; we can observe that the difference is quite significant.\nThe 20 most common topics are presented in Figure 2. The only axis, the x-axis, represents the degree of correlation \u03c1 = \u03b7/\u03c0 for all topics, i.e. the percent change in the topic proportion given one unit change in the exogenous covariate. Here \u03c0 and \u03b7 denote the related component of \u2211 \u03c0t/T and \u03b7 for each topic. Table 1 lists the highest probability words sorted by their probabilities from high to low inside the five topics with highest \u03c1 in Figure 2.\nA number of facts can be inferred from Figure 2. The topics with positive \u03c1 are those that have a positive correlation with the growth of the CPI in 2014. We can observe that two of them are supported by the U.S. government spending, namely \u201cequal opportunity\u201d and \u201chealth care,\u201d the latter of which is probably related to the Affordable Care Act programs. This suggests that there is a causal relationship between the increase in government spending and the increase in the number of jobs in these categories, and the former was also an underlying factor in the growth of the CPI in 2014. We also observe that \u201csoftware\u201d and \u201csecretarial\u201d were moving in the same direction of CPI, while some traditional higher-paid job categories, such as engineering and marketing, were not. This partly agrees with some news articles in 2014 in that while the labor market was recovering, there was relatively lower growth in traditional higher-paid job categories (Lowrey 2014; Coy 2014).\nTherefore, we posit that GCLDA could be helpful in identifying topics in temporal documents which are closely related to an exogenous process. The changes in the topic proportions may be caused by the exogenous process, or its underlying factors, as is illustrated by this example, where more demand of goods increases the CPI and creates more jobs in certain categories. The other way around is also possible; changes in an exogenous process are caused by certain kinds of news, as is demonstrated in the next example. Such relationships require a more case-specific examination.\nWe also compare our method with STM. Below is the STM counterpart of Figure 2. We can observe that the topics and correlation scores from GCLDA seem to be more timerelated and tend to be more informative of the labor market during the period."}, {"heading": "4.3 BusinessInsider.com: Financial News Articles", "text": "We consider all contributions in the \u201cFinance\u201d section of BusinessInsider.com on all trading days in 2014. There are 15,659 articles in total, which are divided into a training data set containing 12,527 articles and a testing data set containing 3,132 articles (20% of all articles). We increase the proportion of testing documents and let T = 252 (all trading days) in order to create a more challenging scenario for GCLDA. We apply the daily price of the Chicago Board Options Exchange Market Volatility Index (VIX) as the exogenous process, measuring the volatility of the U.S. financial market. The other settings are the same as those in Section 4.2. We provide an analysis of the perplexity of LDA, GCLDA, and ToT in Figure 4. The lines are smoothed by LOESS with a span of 0.2, as there are large fluctuations in perplexity from day to day.\nAgain we observe that GCLDA generates a lower perplexity for the testing documents over time, therefore improving the fitting of the topic model. Also, the perplexities obtained from LDA and ToT are largely the same.\nFrom Figure 5 and Table 2 below, the topics that are strongly positively correlated with the VIX are generally short-term news, such as stock market news and announcements from central banks, as in the topics \u201cFED\u201d (the Federal Reserve), \u201crevenue,\u201d and \u201cstock market,\u201d which are indeed closely related to changes in the stock market. From our analysis, the drop in oil price and the instability in Russia and Ukraine were also major causes of fluctuations in the stock market in 2014. On the other hand, we observe that news about longer-term economic trends is not positively correlated with the VIX, such as \u201ccompanies\u201d and \u201clabor market.\u201d These are generally consistent with our understanding of the stock market.\nIn this example, we demonstrate an application of GCLDA for finding documents that are major contributors to changes in an exogenous process during a period of time. We can easily estimate the topic distribution for each document, and therefore, we can select the news articles that are mostly related to the stock market. Such a direction could possibly evoke future research. We also note that in this example, the causal relationship between the topic distribution and the stock market is bidirectional; news can change the stock market, and vice versa.\nWe also compare our method with STM, with Figure 6 being the STM counterpart of Figure 5. Again we observe that the topics and correlation scores from GCLDA seem to be more time-related and tend to be more informative of the stock market during the period. These findings assert our view that GCLDA improves the structure of the topic model and makes it more time-dependent."}, {"heading": "5 Conclusion", "text": "We have developed a temporal topic model which analyzes time-stamped text documents with known exogenous processes. Our new model, GCLDA, takes both endogenous and exogenous processes into account, and applies Markov chain Monte Carlo sampling for calibration. We have demonstrated that this model better fits temporal documents in terms of perplexity, and extracts well information from job advertisements and financial news articles. We suggest that a possible direction for the future could be analyzing the contents of temporal documents so that they could predict the trends of related exogenous processes."}, {"heading": "Acknowledgments", "text": "This research was conducted in collaboration with the Workforce Science Project of the Searle Center for Law, Regulation and Economic Growth at Northwestern University. We are indebted to Deborah Weiss, Director, Workforce Science Project, for introducing us to the subject of workforce and providing guidance. We are also very grateful for the help and data from DirectEmployers Association."}], "references": [{"title": "Text Mining: Applications and Theory", "author": ["M. Berry", "J. Kogan"], "venue": "Chichester, UK: Wiley.", "citeRegEx": "Berry and Kogan,? 2010", "shortCiteRegEx": "Berry and Kogan", "year": 2010}, {"title": "Dynamic topic models", "author": ["D. Blei", "J. Lafferty"], "venue": "23rd International Conference on Machine Learning.", "citeRegEx": "Blei and Lafferty,? 2006", "shortCiteRegEx": "Blei and Lafferty", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research 3:993\u2013 1022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "America\u2019s low-paying recovery: More jobs than ever, worse wages", "author": ["P. Coy"], "venue": "Bloomberg Business. www.bloomberg.com/bw/articles/2014-08-11/reportnew-jobs-in-u-dot-s-dot-offer-lower-wages-than-beforerecession.", "citeRegEx": "Coy,? 2014", "shortCiteRegEx": "Coy", "year": 2014}, {"title": "Dependent nonparametric trees for dynamic hierarchical clustering", "author": ["A. Dubey", "Q. Ho", "S. Williamson", "E. Xing"], "venue": "Advances in Neural Information Processing Systems 28. MIT Press.", "citeRegEx": "Dubey et al\\.,? 2014", "shortCiteRegEx": "Dubey et al\\.", "year": 2014}, {"title": "Bayesian analysis of some nonparametric problems", "author": ["T. Ferguson"], "venue": "Annals of Statistics 1:209\u2013230.", "citeRegEx": "Ferguson,? 1973", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Finding scientific topics", "author": ["T. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences 101:5228\u20135235.", "citeRegEx": "Griffiths and Steyvers,? 2004", "shortCiteRegEx": "Griffiths and Steyvers", "year": 2004}, {"title": "Recovery has created far more low-wage jobs than better-paid ones", "author": ["A. Lowrey"], "venue": "The New York Times. www.nytimes.com/2014/04/28/business/economy/recoveryhas-created-far-more-low-wage-jobs-than-better-paidones.html.", "citeRegEx": "Lowrey,? 2014", "shortCiteRegEx": "Lowrey", "year": 2014}, {"title": "Hierarchical bayesian modeling of topics in time-stamped documents", "author": ["I. Pruteanu-Malinici", "L. Ren", "J. Paisley", "E. Wang", "L. Carin"], "venue": "IEEE Trans. Pattern Analysis Machine Intelligence 32:996\u20131011.", "citeRegEx": "Pruteanu.Malinici et al\\.,? 2010", "shortCiteRegEx": "Pruteanu.Malinici et al\\.", "year": 2010}, {"title": "The dynamic hierarchical dirichlet process", "author": ["L. Ren", "D. Dunson", "L. Carin"], "venue": "25th International Conference on Machine Learning.", "citeRegEx": "Ren et al\\.,? 2008", "shortCiteRegEx": "Ren et al\\.", "year": 2008}, {"title": "Monte Carlo Statistical Methods", "author": ["C. Robert", "G. Casella"], "venue": "New York: Springer, 2nd ed. edition.", "citeRegEx": "Robert and Casella,? 2004", "shortCiteRegEx": "Robert and Casella", "year": 2004}, {"title": "Harris recurrence of metropolis-within-gibbs and trans-dimensional markov chains", "author": ["G. Roberts", "J. Rosenthal"], "venue": "The Annals of Applied Probability 16:2123\u20132139.", "citeRegEx": "Roberts and Rosenthal,? 2006", "shortCiteRegEx": "Roberts and Rosenthal", "year": 2006}, {"title": "A model of text for experimentation in the social sciences", "author": ["M. Roberts", "B. Stewart", "E. Airoldi"], "venue": "scholar.harvard.edu/files/bstewart/files/stm.pdf.", "citeRegEx": "Roberts et al\\.,? 2015", "shortCiteRegEx": "Roberts et al\\.", "year": 2015}, {"title": "Navigating the local modes of big data: The case of topic models", "author": ["M. Roberts", "B. Stewart", "D. Tingley"], "venue": "Data Analytics in Social Science, Government, and Industry. New York: Cambridge University Press. forthcoming.", "citeRegEx": "Roberts et al\\.,? 2015", "shortCiteRegEx": "Roberts et al\\.", "year": 2015}, {"title": "A constructive definition of dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica 4:639\u2013650.", "citeRegEx": "Sethuraman,? 1994", "shortCiteRegEx": "Sethuraman", "year": 1994}, {"title": "Unsupervised topic modeling for short texts using distributed representations of words", "author": ["V. Sridhar"], "venue": "Proceedings of NAACL-HLT 2015.", "citeRegEx": "Sridhar,? 2015", "shortCiteRegEx": "Sridhar", "year": 2015}, {"title": "Hierarchical dirichlet processes", "author": ["Y. Teh", "M. Jordan", "M. Beal", "D. Blei"], "venue": "Journal of the American Statistical Association 101:1566\u20131582.", "citeRegEx": "Teh et al\\.,? 2005", "shortCiteRegEx": "Teh et al\\.", "year": 2005}, {"title": "Evaluation methods for topic models", "author": ["H. Wallach", "I. Murray", "R. Salakhutdinov", "D. Mimno"], "venue": "26th International Conference on Machine Learning.", "citeRegEx": "Wallach et al\\.,? 2009", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Topics over time: A non-markov continuous-time model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.", "citeRegEx": "Wang and McCallum,? 2006", "shortCiteRegEx": "Wang and McCallum", "year": 2006}, {"title": "Online variational inference for the hierarchical dirichlet process", "author": ["C. Wang", "J. Paisley", "D. Blei"], "venue": "Artificial Intelligence and Statistics.", "citeRegEx": "Wang et al\\.,? 2011", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Dynamic topic modeling for monitoring market competition from online text and image data", "author": ["H. Zhang", "G. Kim", "E. Xing"], "venue": "21st ACM SIGKDD Conference on knowledge Discovery and Data Mining.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Many timedependent topic models without the exogenous component have been proposed, such as the Topics over Time (ToT) model (Wang and McCallum 2006) and the dynamic topic model (DTM) (Blei and Lafferty 2006), to name a few.", "startOffset": 125, "endOffset": 149}, {"referenceID": 1, "context": "Many timedependent topic models without the exogenous component have been proposed, such as the Topics over Time (ToT) model (Wang and McCallum 2006) and the dynamic topic model (DTM) (Blei and Lafferty 2006), to name a few.", "startOffset": 184, "endOffset": 208}, {"referenceID": 1, "context": "For the endogenous part of our paper, we impose a Markovian structure on the topic distribution over time, similar to Blei and Lafferty (2006) and Dubey et al.", "startOffset": 118, "endOffset": 143}, {"referenceID": 1, "context": "For the endogenous part of our paper, we impose a Markovian structure on the topic distribution over time, similar to Blei and Lafferty (2006) and Dubey et al. (2014). For the exogenous process, we incorporate it into the topic distribution in each period, adjusting the endogenous topic evolution process.", "startOffset": 118, "endOffset": 167}, {"referenceID": 14, "context": "With the stick-breaking notation (Sethuraman 1994), we have", "startOffset": 33, "endOffset": 50}, {"referenceID": 5, "context": "More properties of the Dirichlet process can be found in Ferguson (1973).", "startOffset": 57, "endOffset": 73}, {"referenceID": 16, "context": "A hierarchical Dirichlet process (HDP) was proposed in the context of text modeling by Teh et al. (2005). The following hierarchical structure is assumed, \uf8f4\uf8f2\uf8f4\uf8f3 G0|\u03b3 \u223c DP (\u03b3,H), G1, .", "startOffset": 87, "endOffset": 105}, {"referenceID": 8, "context": "One approach is to impose a finite mixture structure on the topic distribution: a dynamic hierarchical Dirichlet process (dHDP) (Ren, Dunson, and Carin 2008) was proposed by adding a temporal dimension, and its variation was further applied on topic modeling with a stick-breaking truncation of Dirichlet processes (Pruteanu-Malinici et al. 2010).", "startOffset": 315, "endOffset": 346}, {"referenceID": 1, "context": "For instance, the dynamic topic model (DTM) (Blei and Lafferty 2006) is as follows, \uf8f1\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 \u03c6t,k|\u03c6t\u22121,k \u223c N(\u03c6t\u22121,k, \u03c3I), \u03b1t|\u03b1t\u22121 \u223c N(\u03b1t\u22121, \u03b4I), \u03b8t,i|\u03b1t iid \u223c N(\u03b1t, aI), zt,i,j |\u03b8t,i iid \u223c Cat(exp(\u03b8t,i)), xt,i,j |zt,i,j \u223c Cat(exp(\u03c6t,zt,i,j )).", "startOffset": 44, "endOffset": 68}, {"referenceID": 4, "context": "We apply the total variation distance d(p, q) = \u03bb \u00b7 \u222b |p\u2212 q|d\u03bc with \u03bb > 0, although many others can also be applied and lead to, for instance, a log-normal model in DTM, or a normal model (Dubey et al. 2014; Zhang, Kim, and Xing 2015).", "startOffset": 188, "endOffset": 234}, {"referenceID": 8, "context": "We note that a number of papers in topic modeling have put this approach into practice (Pruteanu-Malinici et al. 2010; Wang, Paisley, and Blei 2011).", "startOffset": 87, "endOffset": 148}, {"referenceID": 8, "context": "It has been shown (Pruteanu-Malinici et al. 2010) that when the truncation levelK is large, we may as well replace the distribution of \u03c01 with \u03c01 \u223c Dir(\u03b3\u03c00), where \u03b3 = 1, \u03c00 = (1/K, .", "startOffset": 18, "endOffset": 49}, {"referenceID": 8, "context": "We note that a number of papers in topic modeling have put this approach into practice (Pruteanu-Malinici et al. 2010; Wang, Paisley, and Blei 2011). It has been shown (Pruteanu-Malinici et al. 2010) that when the truncation levelK is large, we may as well replace the distribution of \u03c01 with \u03c01 \u223c Dir(\u03b3\u03c00), where \u03b3 = 1, \u03c00 = (1/K, . . . , 1/K). We also let H = Dir(\u03b2, . . . , \u03b2) as in the paper by Teh et al. (2005). We summarize our model, \uf8f1\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 \u03c61, .", "startOffset": 88, "endOffset": 417}, {"referenceID": 16, "context": "The last two lines above are derived as in Teh et al. (2005). We note that here \u03c6k is the per-topic word distribution, \u03b8t,i is the per-document topic distribution, and zt,i,j is the actual topic for each word; they have the same meaning as in LDA.", "startOffset": 43, "endOffset": 61}, {"referenceID": 6, "context": "Following Griffiths and Steyvers (2004), conditioning on all other variables listed for sampling,", "startOffset": 10, "endOffset": 40}, {"referenceID": 6, "context": "Also following Griffiths and Steyvers (2004), we have for \u03b1t and \u03c0\u0303t p(\u03b1t, \u03c0\u0303t|rest) \u221d p(Z|\u03b1t, \u03c0\u0303t)p(\u03c01,.", "startOffset": 15, "endOffset": 45}, {"referenceID": 10, "context": "For the theoretical convergence properties of Metropolis-withinGibbs samplers, the reader can refer to Robert and Casella (2004) and Roberts and Rosenthal (2006).", "startOffset": 103, "endOffset": 129}, {"referenceID": 10, "context": "For the theoretical convergence properties of Metropolis-withinGibbs samplers, the reader can refer to Robert and Casella (2004) and Roberts and Rosenthal (2006).", "startOffset": 103, "endOffset": 162}, {"referenceID": 0, "context": "01), according to a rule of thumb which has been carried out in Berry and Kogan (2010) and Sridhar (2015).", "startOffset": 64, "endOffset": 87}, {"referenceID": 0, "context": "01), according to a rule of thumb which has been carried out in Berry and Kogan (2010) and Sridhar (2015). For ToT, we use the same \u03b1 and \u03b2 and linearly space the timestamps to make computation feasible.", "startOffset": 64, "endOffset": 106}, {"referenceID": 6, "context": "3 for GCLDA, and run 5,000 iterations of the Markov chain with 1,000 burn-in samples for GCLDA, LDA, and ToT; for LDA, we apply the collapsed Gibbs sampling as in Griffiths and Steyvers (2004). The number of topics is set to K = 50 for both data sets.", "startOffset": 163, "endOffset": 193}, {"referenceID": 17, "context": "We apply the \u201cLeft-to-right\u201d algorithm (Wallach et al. 2009) and apply point estimates for \u201c\u03a6\u201d and \u201c\u03b1m\u201d using the training data, as suggested in Section 3 in the same paper.", "startOffset": 39, "endOffset": 60}, {"referenceID": 7, "context": "This partly agrees with some news articles in 2014 in that while the labor market was recovering, there was relatively lower growth in traditional higher-paid job categories (Lowrey 2014; Coy 2014).", "startOffset": 174, "endOffset": 197}, {"referenceID": 3, "context": "This partly agrees with some news articles in 2014 in that while the labor market was recovering, there was relatively lower growth in traditional higher-paid job categories (Lowrey 2014; Coy 2014).", "startOffset": 174, "endOffset": 197}], "year": 2016, "abstractText": "We consider the problem of modeling temporal textual data taking endogenous and exogenous processes into account. Such text documents arise in real world applications, including job advertisements and economic news articles, which are influenced by the fluctuations of the general economy. We propose a hierarchical Bayesian topic model which imposes a \u201dgroupcorrelated\u201d hierarchical structure on the evolution of topics over time incorporating both processes, and show that this model can be estimated from Markov chain Monte Carlo sampling methods. We further demonstrate that this model captures the intrinsic relationships between the topic distribution and the time-dependent factors, and compare its performance with latent Dirichlet allocation (LDA) and two other related models. The model is applied to two collections of documents to illustrate its empirical performance: online job advertisements from DirectEmployers Association and journalists\u2019 postings on BusinessInsider.com.", "creator": "TeX"}}}