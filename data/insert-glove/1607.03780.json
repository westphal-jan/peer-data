{"id": "1607.03780", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2016", "title": "A Vector Space for Distributional Semantics for Entailment", "abstract": "joyner Distributional semantics goverment creates milkers vector - fakie space rushailo representations tchenguiz that freixenet capture antisemitism many forms of semantic chieftain similarity, ribe but epauletted their 89.65 relation to semantic lavard entailment has opened been barrau less akademischer clear. antiqua We propose anoxia a worksite vector - space model 53-lap which glafcos provides desafio a secreta formal aeroports foundation for underpay a distributional word-play semantics of snail entailment. Using inconsolable a correlational mean - jemaah field approximation, noapara we develop icehawks approximate inference molotschna procedures 5,000-man and zorn entailment conversationally operators menasseh over vectors enseigne of probabilities of features orison being plunderer known (refundings versus benzion unknown ). We use this framework masted to reinterpret an existing tailors distributional - semantic model (Word2Vec) litespeed as pathological approximating an entailment - ecclesiastics based model glusker of the distributions of words in contexts, tomassi thereby predicting mangurian lexical euro400 entailment hartpury relations. paika In small-business both unsupervised queretaro and semi - supervised 2401 experiments superioris on hyponymy labit detection, jutra we whupped get substantial katende improvements greylock over hydrophones previous nebraska-lincoln results.", "histories": [["v1", "Wed, 13 Jul 2016 15:08:26 GMT  (58kb,D)", "http://arxiv.org/abs/1607.03780v1", "To appear in Proc. 54th Annual Meeting of the Association Computational Linguistics (ACL 2016)"]], "COMMENTS": "To appear in Proc. 54th Annual Meeting of the Association Computational Linguistics (ACL 2016)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["james henderson", "diana nicoleta popa"], "accepted": true, "id": "1607.03780"}, "pdf": {"name": "1607.03780.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Nicoleta Popa"], "emails": ["james.henderson@xrce.xerox.com", "diana.popa@xrce.xerox.com"], "sections": [{"heading": null, "text": "Distributional semantics creates vectorspace representations that capture many forms of semantic similarity, but their relation to semantic entailment has been less clear. We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. Using a mean-field approximation, we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known (versus unknown). We use this framework to reinterpret an existing distributionalsemantic model (Word2Vec) as approximating an entailment-based model of the distributions of words in contexts, thereby predicting lexical entailment relations. In both unsupervised and semi-supervised experiments on hyponymy detection, we get substantial improvements over previous results."}, {"heading": "1 Introduction", "text": "Modelling entailment is a fundamental issue in computational semantics. It is also important for many applications, for example to produce abstract summaries or to answer questions from text, where we need to ensure that the input text entails the output text. There has been a lot of interest in modelling entailment in a vector-space, but most of this work takes an empirical, often ad-hoc, approach to this problem, and achieving good results has been difficult (Levy et al., 2015). In this work, we propose a new framework for modelling entailment in a vector-space, and illustrate its effective-\n\u2217This work was partially supported by French ANR grant CIFRE N 1324/2014.\nness with a distributional-semantic model of hyponymy detection.\nUnlike previous vector-space models of entailment, the proposed framework explicitly models what information is unknown. This is a crucial property, because entailment reflects what information is and is not known; a representation y entails a representation x if and only if everything that is known given x is also known given y. Thus, we model entailment in a vector space where each dimension represents something we might know. As illustrated in Table 1, knowing that a feature f is true always entails knowing that same feature, but never entails knowing that a different feature g is true. Also, knowing that a feature is true always entails not knowing anything (unk), since strictly less information is still entailment, but the reverse is never true. Table 1 also illustrates that knowing that a feature f is false (\u00acf ) patterns exactly the same way as knowing that an unrelated feature g is true. This illustrates that the relevant dichotomy for entailment is known versus unknown, and not true versus false.\nPrevious vector-space models have been very successful at modelling semantic similarity, in particular using distributional semantic models (e.g. (Deerwester et al., 1990; Schu\u0308tze, 1993; Mikolov et al., 2013a)). Distributional semantics uses the distributions of words in contexts to induce vectorspace embeddings of words, which have been\nar X\niv :1\n60 7.\n03 78\n0v 1\n[ cs\n.C L\n] 1\n3 Ju\nl 2 01\n6\nshown to be useful for a wide variety of tasks. Two words are predicted to be similar if the dot product between their vectors is high. But the dot product is an anti-symmetric operator, which makes it more natural to interpret these vectors as representing whether features are true or false, whereas the dichotomy known versus unknown is asymmetric. We surmise that this is why distributional semantic models have had difficulty modelling lexical entailment (Levy et al., 2015).\nTo develop a vector-space model of whether features are known or unknown, we start with discrete binary vectors, where 1 means known and 0 means unknown. Entailment between these discrete binary vectors can be calculated by independently checking each dimension. But as soon as we try to do calculations with distributions over these vectors, we need to deal with the case where the features are not independent. For example, if feature f has a 50% chance of being true and a 50% chance of being false, we can\u2019t assume that there is a 25% chance that both f and \u00acf are known. This simple case of mutual exclusion is just one example of a wide range of constraints between features which we need to handle in semantic models. These constraints mean that the different dimensions of our vector space are not independent, and therefore exact models are not factorised. Because the models are not factorised, exact calculations of entailment and exact inference of vectors are intractable.\nMean-field approximations are a popular approach to efficient inference for intractable models. In a mean-field approximation, distributions over binary vectors are represented using a single probability for each dimension. These vectors of real values are the basis of our proposed vector space for entailment.\nIn this work, we propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. This framework is derived from a mean-field approximation to entailment between binary vectors, and includes operators for measuring entailment between vectors, and procedures for inferring vectors in an entailment graph. We validate this framework by using it to reinterpret existing Word2Vec (Mikolov et al., 2013a) word embedding vectors as approximating an entailment-based model of the distribution of words in contexts. This reinterpretation allows us to use existing word embeddings as an un-\nsupervised model of lexical entailment, successfully predicting hyponymy relations using the proposed entailment operators in both unsupervised and semi-supervised experiments."}, {"heading": "2 Modelling Entailment in a Vector Space", "text": "To develop a model of entailment in a vector space, we start with the logical definition of entailment in terms of vectors of discrete known features: y entails x if and only if all the known features in x are also included in y. We formalise this relation with binary vectors x, y where 1 means known and 0 means unknown, so this discrete entailment relation (y\u21d2x) can be defined with the binary formula:\nP ((y\u21d2x) | x, y) = \u220f k (1\u2212 (1\u2212yk)xk)\nGiven prior probability distributions P (x), P (y) over these vectors, the exact joint and marginal probabilities for an entailment relation are:\nP (x, y, (y\u21d2x)) = P (x) P (y) \u220f k (1\u2212(1\u2212yk)xk)\nP ((y\u21d2x)) = EP (x)EP (y) \u220f k (1\u2212(1\u2212yk)xk) (1)\nWe cannot assume that the priors P (x) and P (y) are factorised, because there are many important correlations between features and therefore we cannot assume that the features are independent. As discussed in Section 1, even just representing both a feature f and its negation \u00acf requires two different dimensions k and k\u2032 in the vector space, because 0 represents unknown and not false. Given valid feature vectors, calculating entailment can consider these two dimensions separately, but to reason with distributions over vectors we need the prior P (x) to enforce the constraint that xk and xk\u2032 are mutually exclusive. In general, such correlations and anti-correlations exist between many semantic features, which makes inference and calculating the probability of entailment intractable.\nTo allow for efficient inference in such a model, we propose a mean-field approximation. This in effect assumes that the posterior distribution over vectors is factorised, but in practice this is a much weaker assumption than assuming the prior is factorised. The posterior distribution has less uncertainty and therefore is influenced less by nonfactorised prior constraints. By assuming a factorised posterior, we can then represent distributions over feature vectors with simple vectors of\nprobabilities of individual features (or as below, with their log-odds). These real-valued vectors are the basis of the proposed vector-space model of entailment.\nIn the next two subsections, we derive a meanfield approximation for inference of real-valued vectors in entailment graphs. This derivation leads to three proposed vector-space operators for approximating the log-probability of entailment, summarised in Table 2. These operators will be used in the evaluation in Section 5. This inference framework will also be used in Section 3 to model how existing word embeddings can be mapped to vectors to which the entailment operators can be applied."}, {"heading": "2.1 A Mean-Field Approximation", "text": "A mean-field approximation approximates the posterior P using a factorised distribution Q. First of all, this gives us a concise description of the posterior P (x| . . .) as a vector of continuous values Q(x=1), where Q(x=1)k = Q(xk=1) \u2248 EP (x|...)xk = P (xk=1| . . .) (i.e. the marginal probabilities of each bit). Secondly, as is shown below, this gives us efficient methods for doing approximate inference of vectors in a model.\nFirst we consider the simple case where we want to approximate the posterior distribution P (x, y|y\u21d2x). In a mean-field approximation, we want to find a factorised distribution Q(x, y) which minimises the KL-divergence DKL(Q(x, y)||P (x, y|y\u21d2x)) with the true distribution P (x, y|y\u21d2x).\nL = DKL(Q(x, y)||P (x, y|(y\u21d2x))) \u221d \u2211 x Q(x) log Q(x, y) P (x, y, (y\u21d2x))\n= \u2211 k EQ(xk) logQ(xk) + \u2211 k EQ(yk) logQ(yk)\n\u2212 EQ(x) logP (x)\u2212 EQ(y) logP (y) \u2212 \u2211 k EQ(xk)EQ(yk) log(1\u2212(1\u2212yk)xk)\nIn the final equation, the first two terms are the negative entropy of Q, \u2212H(Q), which acts as a maximum entropy regulariser, the final term enforces the entailment constraint, and the middle two terms represent the prior for x and y. One approach (generalised further in the next subsection) to the prior terms \u2212EQ(x) logP (x) is to bound them by assuming P (x) is a function in the exponential family, giving us:\nEQ(x) logP (x) \u2265 EQ(x) log exp(\n\u2211 k \u03b8 x kxk)\nZ\u03b8 = \u2211 k EQ(xk)\u03b8 x kxk \u2212 logZ\u03b8\nwhere the logZ\u03b8 is not relevant in any of our inference problems and thus will be dropped below.\nAs typically in mean-field approximations, inference ofQ(x) andQ(y) can\u2019t be done efficiently with this exact objective L, because of the nonlinear interdependence between xk and yk in the last term. Thus, we introduce two approximations to L, one for use in inferring Q(x) given Q(y) (forward inference), and one for the reverse inference problem (backward inference). In both cases, the approximation is done with an application of Jensen\u2019s inequality to the log function, which gives us an upper bound on L, as is standard practice in mean-field approximations. For forward inference:\nL \u2264\u2212H(Q)\u2212Q(xk=1)\u03b8xk \u2212 EQ(yk)\u03b8 y kyk (2)\n\u2212Q(xk=1) logQ(yk=1) )\nwhich we can optimise for Q(xk=1):\nQ(xk=1) = \u03c3( \u03b8 x k + logQ(yk=1) ) (3)\nwhere \u03c3() is the sigmoid function. The sigmoid function arises from the entropy regulariser, making this a specific form of maximum entropy model. And for backward inference:\nL \u2264\u2212H(Q)\u2212 EQ(xk)\u03b8 x kxk \u2212Q(yk=1)\u03b8 y k (4)\n\u2212 (1\u2212Q(yk=1)) log(1\u2212Q(xk=1)) )\nwhich we can optimise for Q(yk=1):\nQ(yk=1) = \u03c3( \u03b8 y k \u2212 log(1\u2212Q(xk=1)) ) (5)\nNote that in equations (2) and (4) the final terms, Q(xk=1) logQ(yk=1) and (1\u2212Q(yk=1)) log(1\u2212Q(xk=1)) respectively, are approximations to the log-probability of the entailment. We define two vector-space operators, <\u00a9 and >\u00a9, to be these same approximations.\nlogQ(y\u21d2x) \u2248 \u2211 k EQ(xk) log(EQ(yk)(1\u2212 (1\u2212yk)xk))\n= Q(x=1) \u00b7 logQ(y=1) \u2261 X<\u00a9Y\nlogQ(y\u21d2x) \u2248 \u2211 k EQ(yk) log(EQ(xk)(1\u2212 (1\u2212yk)xk))\n= (1\u2212Q(y=1)) \u00b7 log(1\u2212Q(x=1)) \u2261 Y >\u00a9X\nWe parametrise these operators with the vectors X,Y of log-odds of Q(x), Q(y), namely X = log Q(x=1)Q(x=0) = \u03c3\n-1(Q(x=1)). The resulting operator definitions are summarised in Table 2.\nAlso note that the probability of entailment given in equation (1) becomes factorised when we replace P with Q. We define a third vector-space operator, \u21d2\u0303, to be this factorised approximation, also shown in Table 2."}, {"heading": "2.2 Inference in Entailment Graphs", "text": "In general, doing inference for one entailment is not enough; we want to do inference in a graph of entailments between variables. In this section we generalise the above mean-field approximation to entailment graphs.\nTo represent information about variables that comes from outside the entailment graph, we assume we are given a prior P (x) over all variables xi in the graph. As above, we do not assume that this prior is factorised. Instead we assume that the prior P (x) is itself a graphical model which can be approximated with a mean-field approximation.\nGiven a set of variables xi each representing vectors of binary variables xik, a set of entailment relations r = {(i, j)|(xi\u21d2xj)}, and a set of negated entailment relations r\u0304 = {(i, j)|(xi /\u21d2xj)}, we can write the joint posterior probability as:\nP (x, r, r\u0304) = 1\nZ P (x) \u220f i ( ( \u220f\nj:r(i,j) \u220f k P (xik\u21d2xjk|xik, xjk))\n( \u220f\nj:r\u0304(i,j) (1\u2212 \u220f k P (xik\u21d2xjk|xik, xjk))) )\nWe want to find a factorised distribution Q that minimises L = DKL(Q(x)||P (x|r, r\u0304)). As above, we bound this loss for each element Xik=\u03c3\n-1(Q(xik=1)) of each vector we want to infer, using analogous Jensen\u2019s inequalities for the terms involving nodes i and j such that r(i, j) or r(j, i). For completeness, we also propose similar\ninequalities for nodes i and j such that r\u0304(i, j) or r\u0304(j, i), and bound them using the constants\nCijk \u2265 \u220f k\u2032 6=k (1\u2212\u03c3(\u2212Xik\u2032)\u03c3(Xjk\u2032)).\nTo represent the prior P (x), we use the terms\n\u03b8ik(Xi\u0304k) \u2264 log EQ(xi\u0304k)P (xi\u0304k, xik=1)\n1\u2212 EQ(xi\u0304k)P (xi\u0304k, xik=1) where xi\u0304k is the set of all xi\u2032k\u2032 such that either i\n\u2032 6=i or k\u2032 6=k. These terms can be thought of as the logodds terms that would be contributed to the loss function by including the prior\u2019s graphical model in the mean-field approximation.\nNow we can infer the optimal Xik as: Xik = \u03b8ik(Xi\u0304k) + \u2211 j:r(i,j) \u2212 log \u03c3(\u2212Xjk) (6)\n+ \u2211 j:r(j,i) log \u03c3(Xjk) + \u2211 j:r\u0304(j,i) log 1\u2212Cijk\u03c3(Xjk) 1\u2212Cijk\n+ \u2211 j:r\u0304(i,j) \u2212 log 1\u2212Cijk\u03c3(\u2212Xjk) 1\u2212Cijk\nIn summary, the proposed mean-field approximation does inference in entailment graphs by iteratively re-estimating each Xi as the sum of: the prior log-odds, \u2212 log \u03c3(\u2212Xj) for each entailed variable j, and log \u03c3(Xj) for each entailing variable j.1 This inference optimises Xi<\u00a9Xj for each entailing j plus Xi >\u00a9Xj for each entailed j, plus a maximum entropy regulariser on Xi. Negative entailment relations, if they exist, can also be incorporated with some additional approximations. Complex priors can also be incorporated through their log-odds, simulating the inclusion of the prior within the mean-field approximation.\nGiven its dependence on mean-field approximations, it is an empirical question to what extent we should view this model as computing real entailment probabilities and to what extent we should view it as a well-motivated non-linear mapping for which we simply optimise the input-output behaviour (as for neural networks (Henderson and Titov, 2010)). In Sections 3 and 5 we argue for the former (stronger) view."}, {"heading": "3 Interpreting Word2Vec Vectors", "text": "To evaluate how well the proposed framework provides a formal foundation for the distributional semantics of entailment, we use it to re-interpret an\n1It is interesting to note that \u2212 log \u03c3(\u2212Xj) is a nonnegative transform of Xj , similar to the ReLU nonlinearity which is popular in deep neural networks (Glorot et al., 2011). log \u03c3(Xj) is the analogous non-positive transform.\nexisting model of distributional semantics in terms of semantic entailment. There has been a lot of work on how to use the distribution of contexts in which a word occurs to induce a vector representation of the semantics of words. In this paper, we leverage this previous work on distributional semantics by re-interpreting a previous distributional semantic model and using this understanding to map its vector-space word embeddings to vectors in the proposed framework. We then use the proposed operators to predict entailment between words using these vectors. In Section 5 below, we evaluate these predictions on the task of hyponymy detection. In this section we motivate three different ways to interpret the Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) distributional semantic model as an approximation to an entailment-based model of the semantic relationship between a word and its context.\nDistributional semantics learns the semantics of words by looking at the distribution of contexts in which they occur. To model this relationship, we assume that the semantic features of a word are (statistically speaking) redundant with those of its context words, and consistent with those of its context words. We model these properties using a hidden vector which is the consistent unification of the features of the middle word and the context. In other words, there must exist a hidden vector which entails both of these vectors, and is consistent with prior constraints on vectors. We split this into two steps, inference of the hidden vector Y from the middle vector Xm, context vectors Xc and prior, and computing the log-probability (7) that this hidden vector entails the middle and context vectors:\nmax Y\n(logP (y, y\u21d2xm, y\u21d2xc)) (7)\nWe interpret Word2Vec\u2019s Skip-Gram model as learning its context and middle word vectors so that the log-probability of this entailment is high for the observed context words and low for other (sampled) context words. The word embeddings produced by Word2Vec are only related to the vectors Xm assigned to the middle words; context vectors are computed but not output. We model the context vectors X \u2032c as combining (as in equation (5)) information about a context word itself with information which can be inferred from this word given the prior, X \u2032c = \u03b8c \u2212 log \u03c3(\u2212Xc).\nThe numbers in the vectors output by Word2Vec\nare real numbers between negative infinity and infinity, so the simplest interpretation of them is as the log-odds of a feature being known. In this case we can treat these vectors directly as theXm in the model. The inferred hidden vector Y can then be calculated using the model of backward inference from the previous section."}, {"heading": "Y = \u03b8c \u2212 log \u03c3(\u2212Xc)\u2212 log \u03c3(\u2212Xm)", "text": "= X \u2032c \u2212 log \u03c3(\u2212Xm)\nSince the unification Y of context and middle word features is computed using backward inference, we use the backward-inference operator >\u00a9 to calculate how successful that unification was. This gives us the final score:\nlogP (y, y\u21d2xm, y\u21d2xc) \u2248 Y >\u00a9Xm + Y >\u00a9Xc +\u2212\u03c3(\u2212Y )\u00b7\u03b8c = Y >\u00a9Xm +\u2212\u03c3(\u2212Y )\u00b7X \u2032c\nThis is a natural interpretation, but it ignores the equivalence in Word2Vec between pairs of positive values and pairs of negative values, due to its use of the dot product. As a more accurate interpretation, we interpret each Word2Vec dimension as specifying whether its feature is known to be true or known to be false. Translating this Word2Vec vector into a vector in our entailment vector space, we get one copy Y + of the vector representing known-to-be-true features and a second negated duplicate Y \u2212 of the vector representing known-to-be-false features, which we concatenate to get our representation Y .\nY + = X \u2032c \u2212 log \u03c3(\u2212Xm) Y \u2212 = \u2212X \u2032c \u2212 log \u03c3(Xm) logP (y, y\u21d2xm, y\u21d2xc) \u2248 Y + >\u00a9Xm +\u2212\u03c3(\u2212Y +)\u00b7X \u2032c\n+ Y \u2212 >\u00a9(\u2212Xm) +\u2212\u03c3(\u2212Y \u2212)\u00b7(\u2212X \u2032c)\nAs a third alternative, we modify this latter interpretation with some probability mass reserved for unknown in the vicinity of zero. By subtracting 1 from both the original and negated copies of each dimension, we get a probability of unknown of 1\u2212\u03c3(Xm\u22121)\u2212 \u03c3(\u2212Xm\u22121). This gives us: Y + = X \u2032c \u2212 log \u03c3(\u2212(Xm\u22121)) Y \u2212 = \u2212X \u2032c \u2212 log \u03c3(\u2212(\u2212Xm\u22121)) logP (y, y\u21d2xm, y\u21d2xc) \u2248 Y + >\u00a9(Xm\u22121) +\u2212\u03c3(\u2212Y +)\u00b7X \u2032c\n+ Y \u2212 >\u00a9(\u2212Xm\u22121)) +\u2212\u03c3(\u2212Y \u2212)\u00b7(\u2212X \u2032c)\nTo understand better the relative accuracy of these three interpretations, we compared the training gradient which Word2Vec uses to train its middle-word vectors to the training gradient for each of these interpretations. We plotted these gradients for the range of values typically found in Word2Vec vectors for both the middle vector and the context vector. Figure 1 shows three of these plots. As expected, the second interpretation is more accurate than the first because its plot is antisymmetric around the diagonal, like the Word2Vec gradient. In the third alternative, the constant 1 was chosen to optimise this match, producing a close match to the Word2Vec training gradient, as shown in Figure 1 (Word2Vec versus Unk dup).\nThus, Word2Vec can be seen as a good approximation to the third model, and a progressively worse approximation to the second and first models. Therefore, if the entailment-based distributional semantic model we propose is accurate, then we would expect the best accuracy in hyponymy detection using the third interpretation of Word2Vec vectors, and progressively worse accuracy for the other two interpretations. As we will see in Section 5, this prediction holds."}, {"heading": "4 Related Work", "text": "There has been a significant amount of work on using distributional-semantic vectors for hyponymy detection, using supervised, semi-supervised or unsupervised methods (e.g. (Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)). Because our main concern is modelling entailment within a vector space, we do not do a thorough comparison to models which use mea-\nsures computed outside the vector space (e.g. symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)). We also limit our evaluation of lexical entailment to hyponymy, not including other related lexical relations (cf. (Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics. We are also not concerned with models or evaluations which require supervised learning about individual words, instead limiting ourselves to semisupervised learning where the words in the training and test sets are disjoint.\nFor these reasons, in our evaluations we replicate the experimental setup of Weeds et al. (2014), for both unsupervised and semi-supervised models. Within this setup, we compare to the results of the models evaluated by Weeds et al. (2014) and to previously proposed vector-space operators. This includes one vector space operator for hyponymy which doesn\u2019t have trained parameters, proposed by Rei and Briscoe (2014), called weighted cosine. The dimensions of the dot product (normalised to make it a cosine measure) are weighted to put more weight on the larger values in the entailed (hypernym) vector.\nWe base this evaluation on the Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) distributional semantic model and its publicly available word embeddings. We choose it because it is popular, simple, fast, and its embeddings have been derived from a very large corpus. Levy and Goldberg (2014) showed that it is closely related to the previous PMI-based distributional semantic models (e.g. (Turney and Pantel, 2010)).\nThe most similar previous work, in terms of motivation and aims, is that of Vilnis and McCallum (2015). They also model entailment directly using a vector space, without training a classifier. But instead of representing words as a point in a vector space (as in this work), they represent words as a Gaussian distribution over points in a vector space. This allows them to represent the extent to which a feature is known versus unknown as the\namount of variance in the distribution for that feature\u2019s dimension. While nicely motivated theoretically, the model appears to be more computationally expensive than the one proposed here, particularly for inferring vectors. They do make unsupervised predictions of hyponymy relations with their learned vector distributions, using KL-divergence between the distributions for the two words. They evaluate their models on the hyponymy data from (Baroni et al., 2012). As discussed further in section 5.2, our best models achieve non-significantly better average precision than their best models.\nThe semi-supervised model of Kruszewski et al. (2015) also models entailment in a vector space, but they use a discrete vector space. They train a mapping from distributional semantic vectors to Boolean vectors such that feature inclusion respects a training set of entailment relations. They then use feature inclusion to predict hyponymy, and other lexical entailment relations. This approach is similar to the one used in our semisupervised experiments, except that their discrete entailment prediction operator is very different from our proposed entailment operators."}, {"heading": "5 Evaluation", "text": "To evaluate whether the proposed framework is an effective model of entailment in vector spaces, we apply the interpretations from Section 3 to publicly available word embeddings and use them to predict the hyponymy relations in a benchmark dataset. This framework predicts that the more accurate interpretations of Word2Vec result in more accurate unsupervised models of hyponymy. We evaluate on detecting hyponymy relations between words because hyponymy is the canonical type of lexical entailment; most of the semantic features of a hypernym (e.g. \u201canimal\u201d) must be included in the semantic features of the hyponym (e.g. \u201cdog\u201d). We evaluate in both a fully unsupervised setup and a semi-supervised setup."}, {"heading": "5.1 Hyponymy with Word2Vec Vectors", "text": "For our evaluation on hyponymy detection, we replicate the experimental setup of Weeds et al. (2014), using their selection of word pairs2 from the BLESS dataset (Baroni and Lenci, 2011).3\n2https://github.com/SussexCompSem/ learninghypernyms\n3Of the 1667 word pairs in this data, 24 were removed because we do not have an embedding for one of the words.\nThese noun-noun word pairs include positive hyponymy pairs, plus negative pairs consisting of some other hyponymy pairs reversed, some pairs in other semantic relations, and some random pairs. Their selection is balanced between positive and negative examples, so that accuracy can be used as the performance measure. For their semisupervised experiments, ten-fold cross validation is used, where for each test set, items are removed from the associated training set if they contain any word from the test set. Thus, the vocabulary of the training and testing sets are always disjoint, thereby requiring that the models learn about the vector space and not about the words themselves. We had to perform our own 10-fold split, but apply the same procedure to filter the training set.\nWe could not replicate the word embeddings used in Weeds et al. (2014), so instead we use publicly available word embeddings.4 These vectors were trained with the Word2Vec software applied to about 100 billion words of the Google-News dataset, and have 300 dimensions.\nThe hyponymy detection results are given in Table 3, including both unsupervised (upper box) and semi-supervised (lower box) experiments. We report two measures of performance, hyponymy detection accuracy (50% Acc) and direction classification accuracy (Dir Acc). Since all the operators only determine a score, we need to choose a threshold to get detection accuracies. Given that the proportion of positive examples in the dataset has been artificially set at 50%, we threshold each model\u2019s score at the point where the proportion of positive examples output is 50%, which we call \u201c50% Acc\u201d. Thus the threshold is set after seeing the testing inputs but not their target labels.\nDirection classification accuracy (Dir Acc) indicates how well the method distinguishes the relative abstractness of two nouns. Given a pair of nouns which are in a hyponymy relation, it classifies which word is the hypernym and which is the hyponym. This measure only considers positive examples and chooses one of two directions, so it is inherently a balanced binary classification task. Classification is performed by simply comparing the scores in both directions. If both directions produce the same score, the expected random accuracy (50%) is used.\nAs representative of previous work, we report\n4https://code.google.com/archive/p/ word2vec/\nthe best results from Weeds et al. (2014), who try a number of unsupervised and semi-supervised models, and use the same testing methodology and hyponymy data. However, note that their word embeddings are different. For the semisupervised models, Weeds et al. (2014) trains classifiers, which are potentially more powerful than our linear vector mappings. We also compare the proposed operators to the dot product (dot),5 vector differences (dif ), and the weighted cosine of Rei and Briscoe (2014) (weighted cos), all computed with the same word embeddings as for the proposed operators.\nIn Section 3 we argued for three progressively more accurate interpretations of Word2Vec vectors in the proposed framework, the log-odds interpretation (log-odds >\u00a9), the negated duplicate interpretation (dup >\u00a9), and the negated duplicate interpretation with unknown around zero (unk dup >\u00a9). We also evaluate using the factorised calculation of entailment (log-odds \u21d2\u0303, unk dup \u21d2\u0303), and the backward-inference entailment operator (log-odds <\u00a9), neither of which match the proposed interpre-\n5We also tested the cosine measure, but results were very slightly worse than dot.\ntations. For the semi-supervised case, we train a linear vector-space mapping into a new vector space, in which we apply the operators (mapped operators). All these results are discussed in the next two subsections."}, {"heading": "5.2 Unsupervised Hyponymy Detection", "text": "The first set of experiments evaluate the vectorspace operators in unsupervised models of hyponymy detection. The proposed models are compared to the dot product, because this is the standard vector-space operator and has been shown to capture semantic similarity very well. However, because the dot product is a symmetric operator, it always performs at chance for direction classification. Another vector-space operator which has received much attention recently is vector differences. This is used (with vector sum) to perform semantic transforms, such as \u201cking - male + female = queen\u201d, and has previously been used for modelling hyponymy (Vylomova et al., 2015; Weeds et al., 2014). For our purposes, we sum the pairwise differences to get a score which we use for hyponymy detection.\nFor the unsupervised results in the upper box of table 3, the best unsupervised model of Weeds et al. (2014), and the operators dot, dif and weighted cos all perform similarly on accuracy, as does the log-odds factorised entailment calculation (logodds \u21d2\u0303). The forward-inference entailment operator (log-odds <\u00a9) performs above chance but not well, as expected given the backward-inferencebased interpretation of Word2Vec vectors. By definition, dot is at chance for direction classification, but the other models all perform better, indicating that all these operators are able to measure relative abstractness. As predicted, the >\u00a9 operator performs significantly better than all these results on accuracy, as well as on direction classification, even assuming the log-odds interpretation of Word2Vec vectors.\nWhen we move to the more accurate interpretation of Word2Vec vectors as specifying both original and negated features (dup >\u00a9), we improve (non-significantly) on the log-odds interpretation. Finally, the third and most accurate interpretation, where values around zero can be unknown (unk dup >\u00a9), achieves the best results in unsupervised hyponymy detection, as well as for direction classification. Changing to the factorised entailment operator (unk dup \u21d2\u0303) is worse but also signifi-\ncantly better than the other accuracies. To allow a direct comparison to the model of Vilnis and McCallum (2015), we also evaluated the unsupervised models on the hyponymy data from (Baroni et al., 2012). Our best model achieved 81% average precision on this dataset, non-significantly better than the 80% achieved by the best model of Vilnis and McCallum (2015)."}, {"heading": "5.3 Semi-supervised Hyponymy Detection", "text": "Since the unsupervised learning of word embeddings may reflect many context-word correlations which have nothing to do with hyponymy, we also consider a semi-supervised setting. Adding some supervision helps distinguish features that capture semantic properties from other features which are not relevant to hyponymy detection. But even with supervision, we still want the resulting model to be captured in a vector space, and not in a parametrised scoring function. Thus, we train mappings from the Word2Vec word vectors to new word vectors, and then apply the entailment operators in this new vector space to predict hyponymy. Because the words in the testing set are always disjoint from the words in the training set, this experiment measures how well the original unsupervised vector space captures features that generalise entailment across words, and not how well the mapping can learn about individual words.\nOur objective is to learn a mapping to a new vector space in which an operator can be applied to predict hyponymy. We train linear mappings for the >\u00a9 operator (mapped >\u00a9) and for vector differences (mapped dif ), since these were the best performing proposed operator and baseline operator, respectively, in the unsupervised experiments. We do not use the duplicated interpretations because these transforms are subsumed by the ability to learn a linear mapping.6 Previous work on using vector differences for semi-supervised hyponymy detection has used a linear SVM (Vylomova et al., 2015; Weeds et al., 2014), which is mathematically equivalent to our vector-differences model, except that we use cross entropy loss and they use a large-margin loss and SVM training.\nThe semi-supervised results in the bottom box of table 3 show a similar pattern to the unsupervised results.7 The >\u00a9 operator achieves the best\n6Empirical results confirm that this is in practice the case, so we do not include these results in the table.\n7It is not clear how to measure significance for crossvalidation results, so we do not attempt to do so.\ngeneralisation from training word vectors to testing word vectors. The mapped >\u00a9 model has the best accuracy, followed by the factorised entailment operator mapped \u21d2\u0303 and Weeds et al. (2014). Direction accuracies of all the proposed operators (mapped >\u00a9, mapped \u21d2\u0303, mapped <\u00a9) reach into the 90\u2019s. The dif operator performs particularly poorly in this mapped setting, perhaps because both the mapping and the operator are linear. These semisupervised results again support our distributionalsemantic interpretations of Word2Vec vectors and their associated entailment operator >\u00a9."}, {"heading": "6 Conclusion", "text": "In this work, we propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. We developed a mean-field approximation to probabilistic entailment between vectors which represent known versus unknown features. And we used this framework to derive vector operators for entailment and vector inference equations for entailment graphs. This framework allows us to reinterpret Word2Vec as approximating an entailment-based distributional semantic model of words in context, and show that more accurate interpretations result in more accurate unsupervised models of lexical entailment, achieving better accuracies than previous models. Semi-supervised evaluations confirm these results.\nA crucial distinction between the semisupervised models here and much previous work is that they learn a mapping into a vector space which represents entailment, rather than learning a parametrised entailment classifier. Within this new vector space, the entailment operators and inference equations apply, thereby generalising naturally from these lexical representations to the compositional semantics of multi-word expressions and sentences. Further work is needed to explore the full power of these abilities to extract information about entailment from both unlabelled text and labelled entailment data, encode it all in a single vector space, and efficiently perform complex inferences about vectors and entailments. This future work on compositional distributional semantics should further demonstrate the full power of the proposed framework for modelling entailment in a vector space."}], "references": [{"title": "How we blessed distributional semantic evaluation", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS \u201911, pages 1\u201310. Association for Computa-", "citeRegEx": "Baroni and Lenci.,? 2011", "shortCiteRegEx": "Baroni and Lenci.", "year": 2011}, {"title": "Entailment above the word level in distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Baroni et al\\.,? 2012", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman."], "venue": "Journal of the American Society for Information Science, 41(6):391\u2013407.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Learning semantic hierarchies: A continuous vector space approach", "author": ["Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, 23(3):461\u2013471.", "citeRegEx": "Fu et al\\.,? 2015", "shortCiteRegEx": "Fu et al\\.", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 315\u2013323.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Incremental sigmoid belief networks for grammar learning", "author": ["James Henderson", "Ivan Titov."], "venue": "Journal of Machine Learning Research, 11(Dec):3541\u20133570.", "citeRegEx": "Henderson and Titov.,? 2010", "shortCiteRegEx": "Henderson and Titov.", "year": 2010}, {"title": "Directional distributional similarity for lexical inference", "author": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-geffet."], "venue": "Natural Language Engineering, 16(4):359\u2013389.", "citeRegEx": "Kotlerman et al\\.,? 2010", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "Deriving boolean structures from distributional vectors", "author": ["Germn Kruszewski", "Denis Paperno", "Marco Baroni."], "venue": "Transactions of the Association for Computational Linguistics, 3:375\u2013388.", "citeRegEx": "Kruszewski et al\\.,? 2015", "shortCiteRegEx": "Kruszewski et al\\.", "year": 2015}, {"title": "Identifying hypernyms in distributional semantic spaces", "author": ["Alessandro Lenci", "Giulia Benotto."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics, SemEval \u201912, pages 75\u201379. Association for Computational Linguistics.", "citeRegEx": "Lenci and Benotto.,? 2012", "shortCiteRegEx": "Lenci and Benotto.", "year": 2012}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Focused entailment graphs for open ie propositions", "author": ["Omer Levy", "Ido Dagan", "Jacob Goldberger."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Automatic retrieval and clustering of similar words", "author": ["Dekang Lin."], "venue": "Proceedings of the 17th International Conference on Computational Linguistics Volume 2, COLING \u201998, pages 768\u2013774. Association for Computational Linguistics.", "citeRegEx": "Lin.,? 1998", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Ad-", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships", "author": ["Silvia Necsulescu", "Sara Mendes", "David Jurgens", "N\u00faria Bel", "Roberto Navigli."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and", "citeRegEx": "Necsulescu et al\\.,? 2015", "shortCiteRegEx": "Necsulescu et al\\.", "year": 2015}, {"title": "Looking for hyponyms in vector space", "author": ["Marek Rei", "Ted Briscoe."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 68\u201377, Ann Arbor, Michigan. Association for Computational Linguistics.", "citeRegEx": "Rei and Briscoe.,? 2014", "shortCiteRegEx": "Rei and Briscoe.", "year": 2014}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Stephen Roller", "Katrin Erk", "Gemma Boleda."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1025\u2013", "citeRegEx": "Roller et al\\.,? 2014", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte im Walde."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Santus et al\\.,? 2014", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Word space", "author": ["Hinrich Sch\u00fctze."], "venue": "Advances in Neural Information Processing Systems 5, pages 895\u2013902. Morgan Kaufmann.", "citeRegEx": "Sch\u00fctze.,? 1993", "shortCiteRegEx": "Sch\u00fctze.", "year": 1993}, {"title": "Experiments with three approaches to recognizing lexical entailment", "author": ["Peter D. Turney", "Saif M. Mohammad."], "venue": "CoRR, abs/1401.8269.", "citeRegEx": "Turney and Mohammad.,? 2014", "shortCiteRegEx": "Turney and Mohammad.", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "J. Artif. Int. Res., 37(1):141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Word representations via Gaussian embedding", "author": ["Luke Vilnis", "Andrew McCallum."], "venue": "Proceedings of the International Conference on Learning Representations 2015 (ICLR).", "citeRegEx": "Vilnis and McCallum.,? 2015", "shortCiteRegEx": "Vilnis and McCallum.", "year": 2015}, {"title": "Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning", "author": ["Ekaterina Vylomova", "Laura Rimell", "Trevor Cohn", "Timothy Baldwin."], "venue": "CoRR 2015.", "citeRegEx": "Vylomova et al\\.,? 2015", "shortCiteRegEx": "Vylomova et al\\.", "year": 2015}, {"title": "A general framework for distributional similarity", "author": ["Julie Weeds", "David Weir."], "venue": "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201903, pages 81\u2013", "citeRegEx": "Weeds and Weir.,? 2003", "shortCiteRegEx": "Weeds and Weir.", "year": 2003}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Julie Weeds", "David Weir", "Diana McCarthy."], "venue": "Proceedings of the 20th International Conference on Computational Linguistics, COLING \u201904, pages 1015\u20131021. Association for Computa-", "citeRegEx": "Weeds et al\\.,? 2004", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}, {"title": "Learning to distinguish hypernyms and co-hyponyms", "author": ["Julie Weeds", "Daoud Clarke", "Jeremy Reffin", "David Weir", "Bill Keller."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages", "citeRegEx": "Weeds et al\\.,? 2014", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "Learning term embeddings for hypernymy identification", "author": ["Zheng Yu", "Haixun Wang", "Xuemin Lin", "Min Wang."], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015. AAAI Press / International Joint", "citeRegEx": "Yu et al\\.,? 2015", "shortCiteRegEx": "Yu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "There has been a lot of interest in modelling entailment in a vector-space, but most of this work takes an empirical, often ad-hoc, approach to this problem, and achieving good results has been difficult (Levy et al., 2015).", "startOffset": 204, "endOffset": 223}, {"referenceID": 2, "context": "(Deerwester et al., 1990; Sch\u00fctze, 1993; Mikolov et al., 2013a)).", "startOffset": 0, "endOffset": 63}, {"referenceID": 19, "context": "(Deerwester et al., 1990; Sch\u00fctze, 1993; Mikolov et al., 2013a)).", "startOffset": 0, "endOffset": 63}, {"referenceID": 13, "context": "(Deerwester et al., 1990; Sch\u00fctze, 1993; Mikolov et al., 2013a)).", "startOffset": 0, "endOffset": 63}, {"referenceID": 11, "context": "We surmise that this is why distributional semantic models have had difficulty modelling lexical entailment (Levy et al., 2015).", "startOffset": 108, "endOffset": 127}, {"referenceID": 13, "context": "We validate this framework by using it to reinterpret existing Word2Vec (Mikolov et al., 2013a) word embedding vectors as approximating an entailment-based model of the distribution of words in contexts.", "startOffset": 72, "endOffset": 95}, {"referenceID": 5, "context": "Given its dependence on mean-field approximations, it is an empirical question to what extent we should view this model as computing real entailment probabilities and to what extent we should view it as a well-motivated non-linear mapping for which we simply optimise the input-output behaviour (as for neural networks (Henderson and Titov, 2010)).", "startOffset": 319, "endOffset": 346}, {"referenceID": 4, "context": "It is interesting to note that \u2212 log \u03c3(\u2212Xj) is a nonnegative transform of Xj , similar to the ReLU nonlinearity which is popular in deep neural networks (Glorot et al., 2011).", "startOffset": 153, "endOffset": 174}, {"referenceID": 13, "context": "In this section we motivate three different ways to interpret the Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) distributional semantic model as an approximation to an entailment-based model of the semantic relationship between a word and its context.", "startOffset": 75, "endOffset": 121}, {"referenceID": 14, "context": "In this section we motivate three different ways to interpret the Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) distributional semantic model as an approximation to an entailment-based model of the semantic relationship between a word and its context.", "startOffset": 75, "endOffset": 121}, {"referenceID": 27, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 15, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 23, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 26, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 3, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 16, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 12, "context": "symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al.", "startOffset": 24, "endOffset": 35}, {"referenceID": 24, "context": "symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al.", "startOffset": 69, "endOffset": 111}, {"referenceID": 25, "context": "symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al.", "startOffset": 69, "endOffset": 111}, {"referenceID": 6, "context": ", 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 8, "context": ", 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al.", "startOffset": 15, "endOffset": 40}, {"referenceID": 18, "context": ", 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al.", "startOffset": 75, "endOffset": 96}, {"referenceID": 3, "context": ", 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)).", "startOffset": 106, "endOffset": 165}, {"referenceID": 17, "context": ", 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)).", "startOffset": 106, "endOffset": 165}, {"referenceID": 1, "context": ", 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)).", "startOffset": 106, "endOffset": 165}, {"referenceID": 26, "context": "(Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics.", "startOffset": 0, "endOffset": 89}, {"referenceID": 23, "context": "(Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics.", "startOffset": 0, "endOffset": 89}, {"referenceID": 20, "context": "(Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics.", "startOffset": 0, "endOffset": 89}, {"referenceID": 10, "context": "(Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics.", "startOffset": 0, "endOffset": 89}, {"referenceID": 24, "context": "For these reasons, in our evaluations we replicate the experimental setup of Weeds et al. (2014), for both unsupervised and semi-supervised models.", "startOffset": 77, "endOffset": 97}, {"referenceID": 24, "context": "For these reasons, in our evaluations we replicate the experimental setup of Weeds et al. (2014), for both unsupervised and semi-supervised models. Within this setup, we compare to the results of the models evaluated by Weeds et al. (2014) and to previously proposed vector-space operators.", "startOffset": 77, "endOffset": 240}, {"referenceID": 16, "context": "This includes one vector space operator for hyponymy which doesn\u2019t have trained parameters, proposed by Rei and Briscoe (2014), called weighted cosine.", "startOffset": 104, "endOffset": 127}, {"referenceID": 13, "context": "We base this evaluation on the Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) distributional semantic model and its publicly available word embeddings.", "startOffset": 40, "endOffset": 86}, {"referenceID": 14, "context": "We base this evaluation on the Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) distributional semantic model and its publicly available word embeddings.", "startOffset": 40, "endOffset": 86}, {"referenceID": 21, "context": "(Turney and Pantel, 2010)).", "startOffset": 0, "endOffset": 25}, {"referenceID": 9, "context": "Levy and Goldberg (2014) showed that it is closely related to the previous PMI-based distributional semantic models (e.", "startOffset": 0, "endOffset": 25}, {"referenceID": 22, "context": "The most similar previous work, in terms of motivation and aims, is that of Vilnis and McCallum (2015). They also model entailment directly using a vector space, without training a classifier.", "startOffset": 76, "endOffset": 103}, {"referenceID": 1, "context": "They evaluate their models on the hyponymy data from (Baroni et al., 2012).", "startOffset": 53, "endOffset": 74}, {"referenceID": 7, "context": "The semi-supervised model of Kruszewski et al. (2015) also models entailment in a vector space, but they use a discrete vector space.", "startOffset": 29, "endOffset": 54}, {"referenceID": 0, "context": "(2014), using their selection of word pairs2 from the BLESS dataset (Baroni and Lenci, 2011).", "startOffset": 68, "endOffset": 92}, {"referenceID": 24, "context": "For our evaluation on hyponymy detection, we replicate the experimental setup of Weeds et al. (2014), using their selection of word pairs2 from the BLESS dataset (Baroni and Lenci, 2011).", "startOffset": 81, "endOffset": 101}, {"referenceID": 25, "context": "We could not replicate the word embeddings used in Weeds et al. (2014), so instead we use publicly available word embeddings.", "startOffset": 51, "endOffset": 71}, {"referenceID": 25, "context": "Table 3: Accuracies on the BLESS data from Weeds et al. (2014), for hyponymy detection (50% Acc) and hyponymy direction classification (Dir Acc), in the unsupervised (upper box) and semisupervised (lower box) experiments.", "startOffset": 43, "endOffset": 63}, {"referenceID": 23, "context": "the best results from Weeds et al. (2014), who try a number of unsupervised and semi-supervised models, and use the same testing methodology and hyponymy data.", "startOffset": 22, "endOffset": 42}, {"referenceID": 23, "context": "the best results from Weeds et al. (2014), who try a number of unsupervised and semi-supervised models, and use the same testing methodology and hyponymy data. However, note that their word embeddings are different. For the semisupervised models, Weeds et al. (2014) trains classifiers, which are potentially more powerful than our linear vector mappings.", "startOffset": 22, "endOffset": 267}, {"referenceID": 12, "context": "(2014) trains classifiers, which are potentially more powerful than our linear vector mappings. We also compare the proposed operators to the dot product (dot),5 vector differences (dif ), and the weighted cosine of Rei and Briscoe (2014) (weighted cos), all computed with the same word embeddings as for the proposed operators.", "startOffset": 72, "endOffset": 239}, {"referenceID": 23, "context": "This is used (with vector sum) to perform semantic transforms, such as \u201cking - male + female = queen\u201d, and has previously been used for modelling hyponymy (Vylomova et al., 2015; Weeds et al., 2014).", "startOffset": 155, "endOffset": 198}, {"referenceID": 26, "context": "This is used (with vector sum) to perform semantic transforms, such as \u201cking - male + female = queen\u201d, and has previously been used for modelling hyponymy (Vylomova et al., 2015; Weeds et al., 2014).", "startOffset": 155, "endOffset": 198}, {"referenceID": 25, "context": "For the unsupervised results in the upper box of table 3, the best unsupervised model of Weeds et al. (2014), and the operators dot, dif and weighted cos all perform similarly on accuracy, as does the log-odds factorised entailment calculation (logodds \u21d2\u0303).", "startOffset": 89, "endOffset": 109}, {"referenceID": 1, "context": "To allow a direct comparison to the model of Vilnis and McCallum (2015), we also evaluated the unsupervised models on the hyponymy data from (Baroni et al., 2012).", "startOffset": 141, "endOffset": 162}, {"referenceID": 21, "context": "To allow a direct comparison to the model of Vilnis and McCallum (2015), we also evaluated the unsupervised models on the hyponymy data from (Baroni et al.", "startOffset": 45, "endOffset": 72}, {"referenceID": 1, "context": "To allow a direct comparison to the model of Vilnis and McCallum (2015), we also evaluated the unsupervised models on the hyponymy data from (Baroni et al., 2012). Our best model achieved 81% average precision on this dataset, non-significantly better than the 80% achieved by the best model of Vilnis and McCallum (2015).", "startOffset": 142, "endOffset": 322}, {"referenceID": 23, "context": "6 Previous work on using vector differences for semi-supervised hyponymy detection has used a linear SVM (Vylomova et al., 2015; Weeds et al., 2014), which is mathematically equivalent to our vector-differences model, except that we use cross entropy loss and they use a large-margin loss and SVM training.", "startOffset": 105, "endOffset": 148}, {"referenceID": 26, "context": "6 Previous work on using vector differences for semi-supervised hyponymy detection has used a linear SVM (Vylomova et al., 2015; Weeds et al., 2014), which is mathematically equivalent to our vector-differences model, except that we use cross entropy loss and they use a large-margin loss and SVM training.", "startOffset": 105, "endOffset": 148}, {"referenceID": 24, "context": "The mapped > \u00a9 model has the best accuracy, followed by the factorised entailment operator mapped \u21d2\u0303 and Weeds et al. (2014). Direction accuracies of all the proposed operators (mapped > \u00a9, mapped \u21d2\u0303, mapped < \u00a9) reach into the 90\u2019s.", "startOffset": 105, "endOffset": 125}], "year": 2016, "abstractText": "Distributional semantics creates vectorspace representations that capture many forms of semantic similarity, but their relation to semantic entailment has been less clear. We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. Using a mean-field approximation, we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known (versus unknown). We use this framework to reinterpret an existing distributionalsemantic model (Word2Vec) as approximating an entailment-based model of the distributions of words in contexts, thereby predicting lexical entailment relations. In both unsupervised and semi-supervised experiments on hyponymy detection, we get substantial improvements over previous results.", "creator": "TeX"}}}