{"id": "1607.05271", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2016", "title": "A Semiparametric Model for Bayesian Reader Identification", "abstract": "demonetized We mechnikov study the od\u00e9on problem fori of identifying individuals based lipatti on their yearns characteristic gaze toucan patterns during animal-related reading of arbitrary text. khirbat The haseley motivation googie for this problem is -300 an unobtrusive biometric setting riffat in flavivirus which a reclined user is 95.32 observed 32.69 during access ruegamer to a gidari document, quarrel but no specific fabricator challenge protocol combined-cycle requiring marick the fractiousness user ' s time greenshank and gabfest attention vojvodina is four-page carried 2000-present out. urt Existing extrapolations models of 54,400 individual sippola differences unemotional in gaze control during banco reading are vasse either nahapana based on waiouru simple aggregate features prelature of aroldis eye 136.6 movements, bonf\u00e1 or rely on kankowski parametric ijazah density models 1853 to describe, laetus for instance, saccade walla amplitudes hrt or word sacatep\u00e9quez fixation durations. vizconde We develop transfrontier flexible sarioglu semiparametric thirdly models of eye movements theatine during reading kangana in which 200c densities 167-seat are adgonzalez inferred under a Gaussian process interference prior fossa centered at a parametric x-3 distribution vadra family that amfm is sohm expected yaobang to indignity approximate genovese the piggeries true distribution well. An empirical study process on moods reading regularising data concords from 251 individuals christmassy shows significant conductus improvements keidanren over the state of the kernis art.", "histories": [["v1", "Mon, 18 Jul 2016 14:46:05 GMT  (132kb,D)", "http://arxiv.org/abs/1607.05271v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ahmed abdelwahab", "reinhold kliegl", "niels landwehr"], "accepted": true, "id": "1607.05271"}, "pdf": {"name": "1607.05271.pdf", "metadata": {"source": "CRF", "title": "A Semiparametric Model for Bayesian Reader Identification", "authors": ["Ahmed Abdelwahab", "Reinhold Kliegl", "Niels Landwehr"], "emails": ["niels.landwehr}@uni-potsdam.de", "kliegl@uni-potsdam.de"], "sections": [{"heading": "1 Introduction", "text": "Eye-movement patterns during skilled reading consist of brief fixations of individual words in a text that are interleaved with quick eye movements called saccades that change the point of fixation to another word. Eye movements are driven both by low-level visual cues and high-level linguistic and cognitive processes related to text understanding; as a reflection of the interplay between vision, cognition, and motor control during reading they are frequently studied in cognitive psychology (Kliegl et al., 2006; Rayner, 1998). Computational models (Engbert et al., 2005; Reichle et al., 1998) as well\nas models based on machine learning (Matties and S\u00f8gaard, 2013; Hara et al., 2012) have been developed to study how gaze patterns arise based on text content and structure, facilitating the understanding of human reading processes.\nA central observation in these and earlier psychological studies (Huey, 1908; Dixon, 1951) is that eye movement patterns strongly differ between individuals. Holland et al. (2012) and Landwehr et al. (2014) have developed models of individual differences in eye movement patterns during reading, and studied these models in a biometric problem setting where an individual has to be identified based on observing her eye movement patterns while reading arbitrary text. Using eye movements during reading as a biometric feature has the advantage that it suffices to observe a user during a routine access to a device or document, without requiring the user to react to a specific challenge protocol. If the observed eye movement sequence is unlikely to be generated by an authorized individual, access can be terminated or an additional verification requested. This is in contrast to approaches where biometric identification is based on eye movements in response to an artificial visual stimulus, for example a moving (Kasprowski and Ober, 2004; Komogortsev et al., 2010; Rigas et al., 2012b; Zhang and Juhola, 2012) or fixed (Bednarik et al., 2005) dot on a computer screen, or a specific image stimulus (Rigas et al., 2012a).\nThe model studied by Holland & Komogortsev (2012) uses aggregate features (such as average fixation duration) of the observed eye movements. Landwehr et al. (2014) showed that readers can be identified more accurately with a model that captures aspects of individual-specific distributions over\nar X\niv :1\n60 7.\n05 27\n1v 1\n[ cs\n.L G\n] 1\n8 Ju\nl 2 01\neye movements, such as the distribution over fixation durations or saccade amplitudes for word refixations, regressions, or next-word movements. Some of these distributions need to be estimated from very few observations; a key challenge is thus to design models that are flexible enough to capture characteristic differences between readers yet robust to sparse data. Landwehr et al. (2014) used a fully parametric approach where all densities are assumed to be in the gamma family; gamma distributions were shown to approximate the true distribution of interest well for most cases (see Figure 1). This model is robust to sparse data, but might not be flexible enough to capture all differences between readers.\nThe model we study in this paper follows ideas developed by Landwehr et al. (2014), but employs more flexible semiparametric density models. Specifically, we place a Gaussian process prior over densities that concentrates probability mass on densities that are close to the gamma family. Given data, a posterior distribution over densities is derived. If data is sparse, the posterior will still be sharply peaked around distributions in the gamma family, reducing the effective capacity of the model and minimizing overfitting. However, given enough evidence in the data, the model will also deviate from the gamma-centered prior and represent any density function. Integrating over the space of densities weighted by the posterior yields a marginal likelihood for novel observations from which predictions are inferred. We empirically study this model in the same setting as studied by Landwehr et al. (2014), but using an order of magnitude more individuals. Identification error is reduced by more than a factor of three compared to the state of the art.\nThe rest of the paper is organized as follows. After defining the problem setting in Section 2, Section 3 presents the semiparametric probabilistic model. Section 4 discusses inference, Section 5 presents an empirical study on reader identification."}, {"heading": "2 Problem Setting", "text": "Assume R different readers, indexed by r \u2208 {1, . . . , R}, and letX = {X1, . . . ,Xn} denote a set of texts. Each r \u2208 R generates a set of eye movement patterns S(r) = {S(r)1 , . . . ,S (r) n } on X , by\nS (r) i \u223c p(S|Xi, r,\u0393)\nwhere p(S|Xi, r,\u0393) is a reader-specific distribution over eye movement patterns given a text Xi. Here, r is a variable indicating the reader generating the sequence, and \u0393 is a true but unknown model that defines all reader-specific distributions. We assume that \u0393 can be broken down into reader-specific models, \u0393 = (\u03b31, . . . ,\u03b3R), such that the distribution\np(S|Xi, r,\u0393) = p(S|Xi,\u03b3r) (1)\nis defined by the partial model \u03b3r. We aggregate the observations of all readers on the training data into a variable S(1:R) = (S(1), . . . ,S(R)).\nWe follow a Bayesian approach, defining a prior p(\u0393) over the joint model that factorizes into priors over reader-specific models, p(\u0393) = \u220fR r=1 p(\u03b3r). At test time, we observe novel eye movement patterns S\u0304 = {S\u03041, . . . , S\u0304m} on a novel set of texts X\u0304 = {X\u03041, . . . , X\u0304m} generated by an unknown reader r \u2208 R. We assume a uniform prior over readers, that is, each r \u2208 R is equally likely to be observed at test time. The goal is to infer the most likely reader to have generated the novel eye movement patterns. In a Bayesian setting, this means inferring the most likely reader given the training observations (X ,S(1:R)) and test observation (X\u0304 , S\u0304):\nr\u2217 = arg max r\u2208R\np(r|X\u0304 , S\u0304,X ,S(1:R)). (2)\nWe can rewrite Equation 2 to\nr\u2217 = arg max r\u2208R\np(S\u0304|r, X\u0304 ,X ,S(1:R)) (3)\n= arg max r\u2208R\n\u222b p(S\u0304|r, X\u0304 ,\u0393)p(\u0393|X ,S(1:R))d\u0393\n= arg max r\u2208R\n\u222b p(S\u0304|X\u0304 ,\u03b3r)p(\u03b3r|X ,S(r))d\u03b3r (4)\nwhere\np(S\u0304|X\u0304 ,\u03b3r) = m\u220f i=1 p(S\u0304i|X\u0304i,\u03b3r) (5)\np(\u03b3r|X ,S(r)) \u221d p(\u03b3r) n\u220f i=1 p(S (r) i |Xi,\u03b3r). (6)\nIn Equation 3 we exploit that readers are uniformly chosen at test time, and in Equation 4 we exploit the factorization p(\u0393) = \u220fR r=1 p(\u03b3r) of the prior, which together with Equation 1 entails a factorization p(\u0393|X ,S(1:R)) = \u220fR r=1 p(\u03b3r|X ,S(r)) of the\nposterior. Note that Equation 4 states that at test time we predict the reader r for which the marginal likelihood (that is, after integrating out the readerspecific model \u03b3r) of the test observations is highest. The next section discusses the reader-specific models p(S|X,\u03b3r) and prior distributions p(\u03b3r)."}, {"heading": "3 Probabilistic Model", "text": "The probabilistic model we employ follows the general structure proposed by Landwehr et al. (2014), but employs semiparametric density models and allows for fully Bayesian inference. To reduce notational clutter, let \u03b3 \u2208 {\u03b31, . . . ,\u03b3R} denote a particular reader-specific model, and let X \u2208 X denote a text. An eye movement pattern is a sequence S = ((s1, d1), . . . , (sT , dT )) of gaze fixations, consisting of a fixation position st (position in text that was fixated) and duration dt \u2208 R (length of fixation in milliseconds). In our experiments, individual sentences are presented in a single line on screen, thus we only model a horizontal gaze position st \u2208 R. We model p(S|X,\u03b3) as a dynamic process that successively generates fixation positions st and durations dt in S, reflecting how a reader generates a sequence of saccades in response to a text stimulus X: p(S|X,\u03b3) = p(s1, d1|X,\u03b3) T\u220f t=2 p(st, dt|st\u22121,X,\u03b3),\nwhere p(st, dt|st\u22121,X,\u03b3) models the generation of the next fixation position and duration given the old fixation position st\u22121. In the psychological literature, four different saccade types are distinguished: a reader can refixate the current word (refixation), fixate the next word in the text (next word movement), move the fixation to a word after the next word, that is, skip one or more words (forward skip), or regress to fixate a word occurring earlier in the text (regression), see, e.g., Heister et al. (2012). We observe empirically that for each saccade type, there is a characteristic distribution over saccade amplitudes and fixation durations, and that both approximately follow gamma distributions\u2014see Figure 1. We therefore model p(st, dt|st\u22121,X,\u03b3) using a mixture over distributions for the four different saccade types. At each time t, the model first draws a saccade type ut \u2208 {1, 2, 3, 4}, and then draws a saccade amplitude at and fixation duration dt from\ntype-specific distributions p(a|ut, st\u22121,X,\u03b3) and p(d|ut,\u03b3). More formally,\nut \u223c p(u|\u03c0) (7) at \u223c p(a|ut, st\u22121,X,\u03b1) (8) dt \u223c p(d|ut, \u03b4), (9)\nwhere \u03b3 = (\u03c0,\u03b1, \u03b4) is decomposed into components \u03c0, \u03b1, and \u03b4. Afterwards, the model updates the fixation position according to st = st\u22121 + at, concluding the definition of p(st, dt|st\u22121,X,\u03b3). Figure 2 shows a slice in the dynamical model.\nThe distribution p(u|\u03c0) over saccade types (Equation 7) is multinomial with parameter vector \u03c0 \u2208 R4. The distributions over amplitudes and durations (Equations 8 and 9) are modeled semiparametrically as discussed in the following subsections."}, {"heading": "3.1 Model of Saccade Amplitudes", "text": "We first discuss the amplitude model p(a|ut, st\u22121,X,\u03b1) (Equation 8). We first define a distribution p(a|ut,\u03b1) over amplitudes for saccade type ut, and subsequently discuss conditioning on the text X and old fixation position st\u22121, leading to p(a|ut, st\u22121,X,\u03b1). We define\np(a|ut = 1,\u03b1) =\n{ \u00b5\u03b11(a) : a > 0\n(1\u2212 \u00b5)\u03b1\u03041(\u2212a) : a \u2264 0 (10)\nwhere \u00b5 is a mixture weight and \u03b11, \u03b1\u03041 are densities defining the distribution over positive and negative amplitudes for the saccade type refixation, and\np(a|ut = 2,\u03b1) = \u03b12(a) (11) p(a|ut = 3,\u03b1) = \u03b13(a) (12) p(a|ut = 4,\u03b1) = \u03b14(\u2212a) (13)\nwhere \u03b12(a), \u03b13(a), and \u03b14(a) are densities defining the distribution over amplitudes for the remaining saccade types. Finally, the distribution\np(s1|X,\u03b1) = \u03b10(s1) (14)\nover the initial fixation position is given by another density function \u03b10. The variables \u00b5, \u03b10, \u03b11, \u03b1\u03041, \u03b12, \u03b13, and \u03b14 are aggregated into model component \u03b1. For resolving the most likely reader at test time (Equation 4), densities in \u03b1 will be integrated out under a prior based on Gaussian processes (Section 3.3) using MCMC inference (Section 4).\nGiven the old fixation position st\u22121, the text X, and the chosen saccade type ut, the amplitude is constrained to fall within a specific interval. For instance, for a refixation the amplitude has to be chosen such that the novel fixation position lies within the beginning and the end of the currently fixated word; a regression implies an amplitude that is negative and makes the novel fixation position lie before the beginning of the currently fixated word. These constraints imposed by the text structure define the conditional distribution p(a|ut, st\u22121,X,\u03b1).\nMore formally, p(a|ut, st\u22121,X,\u03b1) is the distribution p(a|ut,\u03b1) conditioned on a \u2208 [l, r], that is,\np(a|ut, st\u22121,X,\u03b1) = p(a|a \u2208 [l, r], ut,\u03b1),\nwhere l and r are the minimum and maximum amplitude consistent with the constraints. Recall that for a distribution over a continuous variable x given by density \u03b1(x), the distribution over x conditioned on x \u2208 [l, r] is given by the truncated density\n\u03b1(x|x \u2208 [l, r]) =\n{ \u03b1(x)\u222b r\nl \u03b1(x\u0304)dx\u0304 : x \u2208 [l, r]\n0 : x /\u2208 [l, r]. (15)\nWe derive p(a|ut, st\u22121,X,\u03b1) by truncating the distributions given by Equations 10 to 13 to the minimum and maximum amplitude consistent with the current fixation position st\u22121 and text X. Let w\u25e6l (w\u25e6r ) denote the position of the left-most (rightmost) character of the currently fixated word, and let w+l , w + r denote these positions for the next word in X. Let furthermore l\u25e6 = w\u25e6l \u2212 st\u22121, r\u25e6 = w\u25e6r \u2212 st\u22121, l+ = w+l \u2212 st\u22121, and r + = w+r \u2212 st\u22121. Then\np(a|ut = 1, st\u22121,X,\u03b1) ={ \u00b5\u03b11(a|a \u2208 [0, r\u25e6]) : a > 0 (1\u2212 \u00b5)\u03b1\u03041(\u2212a|a \u2208 [l\u25e6, 0]) : a \u2264 0\n(16)\np(a|ut = 2, st\u22121,X,\u03b1) =\u03b12(a|a\u2208 [l+, r+]) (17) p(a|ut = 3, st\u22121,X,\u03b1) =\u03b13(a|a\u2208 (r+,\u221e)) (18) p(a|ut = 4, st\u22121,X,\u03b1) =\u03b14(\u2212a|a\u2208 (\u2212\u221e, l\u25e6))\n(19)\ndefines the appropriately truncated distributions."}, {"heading": "3.2 Model of Fixation Durations", "text": "The model for fixation durations (Equation 9) is similarly specified by saccade type-specific densities,\np(d|ut = u, \u03b4) = \u03b4u(d) for u \u2208 {1, 2, 3, 4} (20)\nand a density for the initial fixation durations\np(d1|X, \u03b4) = \u03b40(d1) (21)\nwhere \u03b40, ..., \u03b44 are aggregated into model component \u03b4. Unlike saccade amplitude, the fixation duration is not constrained by the text structure and accordingly densities are not truncated. This concludes the definition of the model p(S|X,\u03b3)."}, {"heading": "3.3 Prior Distributions", "text": "The prior distribution over the entire model \u03b3 factorizes over the model components as\np(\u03b3|\u03bb, \u03c1, \u03ba) = (22)\np(\u03c0|\u03bb)p(\u00b5|\u03c1)p(\u03b1\u03041|\u03ba) 4\u220f i=0 p(\u03b1i|\u03ba) 4\u220f i=0 p(\u03b4i|\u03ba)\nwhere p(\u03c0) = Dir(\u03c0|\u03bb) is a symmetric Dirichlet prior and p(\u00b5) = Beta(\u00b5|\u03c1) is a Beta prior. The key challenge is to develop appropriate priors for the densities defining saccade amplitude (p(\u03b1\u03041|\u03ba), p(\u03b1i|\u03ba)) and fixation duration (p(\u03b4i|\u03ba)) distributions. Empirically, we observe that amplitude and duration distributions tend to be close to gamma distributions\u2014see the example in Figure 1.\nOur goal is to exploit the prior knowledge that distributions tend to be closely approximated by gamma distributions, but allow the model to deviate from the gamma assumption in case there is enough evidence in the data. To this end, we define a prior over densities that concentrates probability mass around the gamma family. For all densities f \u2208 {\u03b1\u03041, \u03b10, ..., \u03b14, \u03b40, ..., \u03b44}, we employ identical prior distributions p(f |\u03ba). Intuitively, the prior is given by first drawing a density function from the gamma family and then drawing the final density from a Gaussian process (with covariance function \u03ba) centered at this function. More formally, let\nG(x|\u03b7) = exp(\u03b7 Tu(x))\u222b\nexp(\u03b7Tu(x\u2032))dx\u2032 (23)\ndenote the gamma distribution in exponential family form, with sufficient statistics u(x) = (log(x), x)T and parameters \u03b7 = (\u03b71, \u03b72). Let p(\u03b7) denote a prior over the gamma parameters, and define\np(f |\u03ba) = \u222b p(\u03b7)p(f |\u03b7, \u03ba)d\u03b7 (24)\nwhere p(f |\u03b7, \u03ba) is given by drawing\ng \u223c GP(0, \u03ba) (25)\nfrom a Gaussian process prior GP(0, \u03ba) with mean zero and covariance function \u03ba, and letting\nf(x) = exp(\u03b7Tu(x) + g(x))\u222b\nexp(\u03b7Tu(x\u2032) + g(x\u2032))dx\u2032 . (26)\nNote that decreasing the variance of the Gaussian process means regularizing g(x) towards zero, and therefore Equation 26 towards Equation 23. This concludes the specification of the prior p(\u03b3|\u03bb, \u03c1, \u03ba).\nThe density model defined by Equations 24 to 26 draws on ideas from the large body of literature on GP-based density estimation, for example by Adams et al. (2009), Leonard (1978), or Tokdar et al. (2010), and semiparametric density estimation, e.g. as discussed by Yang (2009), Lenk (2003) or Hjort & Glad (1995). However, note that existing density estimation approaches are not applicable offthe-shelf as in our domain distributions are truncated differently at each observation due to constraints that arise from the way eye movements interact with the text structure (Equations 16 to 19)."}, {"heading": "4 Inference", "text": "To solve Equation 4, we need to integrate for each r \u2208 R over the reader-specific model \u03b3r. To reduce notational clutter, let \u03b3 \u2208 {\u03b31, . . . ,\u03b3R} denote a reader-specific model, and let S \u2208 {S(1), . . . ,S(R)} denote the eye movement observations of that reader on the training texts X . We approximate\u222b\np(S\u0304|X\u0304 ,\u03b3)p(\u03b3|X ,S)d\u03b3 \u2248 1 K K\u2211 k=1 p(S\u0304|X\u0304 ,\u03b3(k))\nby a sample \u03b3(1), . . . ,\u03b3(K) of models drawn by\n\u03b3(k) \u223c p(\u03b3|X ,S, \u03bb, \u03c1, \u03ba),\nwhere p(\u03b3|X ,S, \u03bb, \u03c1, \u03ba) is the posterior as given by Equation 6 but with the dependence on the prior hyperparameters \u03bb, \u03c1, \u03ba made explicit. Note that with X and S, all saccade types ut are observed. Together with the factorizing prior (Equation 22), this means that the posterior factorizes according to\np(\u03b3|X ,S, \u03bb, \u03c1, \u03ba) = p(\u03c0|X ,S, \u03bb)p(\u00b5|X ,S, \u03c1)\n\u00b7 p(\u03b1\u03041|X ,S, \u03ba) 4\u220f i=0 p(\u03b1i|X ,S, \u03ba) 4\u220f i=0 p(\u03b4i|X ,S, \u03ba)\nas is easily seen from the graphical model in Figure 2. Obtaining samples \u03c0(k) \u223c p(\u03c0|X ,S) and \u00b5(k) \u223c p(\u00b5|X ,S) is straightforward because their prior distributions are conjugate to the likelihood terms. Let now f \u2208 {\u03b1\u03041, \u03b10, ..., \u03b14, \u03b40, ..., \u03b44}\ndenote a particular density in the model. The posterior p(f |X ,S, \u03ba) is proportional to the prior p(f |\u03ba) (Equation 24) multiplied by the likelihood of all observations that are generated by this density, that is, that are generated according to Equation 14, 16, 17, 18, 19, 20, or 21. Let y = (y1, . . . , y|y|)T \u2208 R|y| denote the vector of all observations generated from density f , and let l = (l1, . . . , l|l|)\nT \u2208 R|l|, r = (r1, . . . , r|r|)T \u2208 R|r| denote the corresponding left and right boundaries of the truncation intervals (again see Equations 14 to 21), where for densities that are not truncated we take li = 0 and ri =\u221e throughout. Then the likelihood of the observations generated from f is\np(y|f, l, r) = |y|\u220f i=1 f(yi|yi \u2208 [li, ri]) (27)\nand the posterior over f is given by\np(f |X ,S, \u03ba) \u221d p(f |\u03ba)p(y|f, l, r). (28)\nNote that y, l and r are observable from X , S. We obtain samples from the posterior given by Equation 28 from a Metropolis-Hastings sampler that explores the space of densities f : R \u2192 R, generating density samples f (1), ..., f (K). A density f is given by a combination of gamma parameters \u03b7 \u2208 R2 and function g : R \u2192 R; specifically, f is obtained by multiplying the gamma distribution with parameters \u03b7 by exp(g) and normalizing appropriately (Equation 26). During sampling, we explicitly represent a density sample f (k) by its gamma parameters \u03b7(k) and function g(k). The proposal distribution of the Metropolis-Hastings sampler is\nq(\u03b7(k+1), g(k+1)|\u03b7(k), g(k)) = p(g(k+1)|\u03ba)N (\u03b7(k+1)|\u03b7(k), \u03c32I)\nwhere p(g(k+1)|\u03ba) is the probability of g(k+1) according to the GP prior GP(0, \u03ba) (Equation 25), and N (\u03b7(k+1)|\u03b7(k), \u03c32I) is a symmetric proposal that randomly perturbs the old state \u03b7(k) according to a Gaussian. In every iteration k a proposal \u03b7?, g? \u223c q(\u03b7, g|\u03b7(k), g(k)) is drawn based on the old state (\u03b7(k), g(k)). The acceptance probability is A(\u03b7?, g?|\u03b7(k), g(k)) = min(1, Q) with\nQ =\nq(\u03b7(k), g(k)|\u03b7?, g?)p(\u03b7?)p(g?|\u03ba)p(y|f?, l, r) q(\u03b7?, g?|\u03b7(k), g(k))p(\u03b7(k))p(g(k)|\u03ba), p(y|f (k), l, r) .\nHere, p(\u03b7?) is the prior probability of gamma parameters \u03b7? (Section 3.3) and p(y|f?, l, r) is given by Equation 27 where f? is obtained from \u03b7?, g? according to Equation 26. To compute the likelihood terms p(y|f (k), l, r) (Equation 27) and also to compute the likelihood of test data under a model (Equation 5), the density f : R \u2192 R needs to be evaluated. According to Equation 26, f is represented by parameter vector \u03b7 together with the nonparametric function g : R \u2192 R. As usual when working with distributions over functions in a Gaussian process framework, the function g only needs to be represented at those points for which we need to evaluate it. Clearly, this includes all observations of saccade amplitudes and fixation durations observed in the training and test set. However, we also need to evaluate the normalizer in Equation 26, and (for f \u2208 {\u03b11, \u03b1\u03041, \u03b12, \u03b13, \u03b14}) the additional normalizer required when truncating the distribution (see Equation 15). As these integrals are one-dimensional, they can be solved relatively accurately using numerical integration; we use 2-point Newton-Cotes quadrature. Newton-Cotes integration requires the evaluation (and thus representation) of g at an additional set of equally spaced supporting points.\nWhen the set of test observations S\u0304, X\u0304 is large, the need to evaluate p(S\u0304|X\u0304 ,\u03b3(k)) for all \u03b3k and all test observations leads to computational challenges. In our experiments, we use a heuristic to reduce computational load. While generating samples, densities are only represented at the training observations and the supporting points needed for NewtonCotes integration. We then estimate the mean of the posterior by \u03b3\u0302 = 1K \u2211K k=1 \u03b3\n(k), and approximate 1 K \u2211K k=1 p(S\u0304|X\u0304 ,\u03b3(k)) \u2248 p(S\u0304|X\u0304 , \u03b3\u0302). To evaluate p(S\u0304|X\u0304 , \u03b3\u0302), we infer the approximate value of the density \u03b3\u0302 at a test observation by linearly interpolating based on the available density values at the training observations and supporting points."}, {"heading": "5 Empirical Study", "text": "We conduct a large-scale study of biometric identification performance using the same setup as discussed by Landwehr et al. (2014) but a much larger set of individuals (251 rather than 20).\nEye movement records for 251 individuals are\nobtained from an EyeLink II system with a 500- Hz sampling rate (SR Research, Ongoode, Ontario, Canada) while reading sentences from the Potsdam Sentence Corpus (Kliegl et al., 2006). There are 144 sentences in the corpus, which we split into equally sized sets of training and test sentences. Individuals read between 100 and 144 sentences, the training (testing) observations for one individual are the observations on those sentences in the training (testing) set of sentences that the individual has read. Results are averaged over 10 random train-test splits.\nWe study the semiparametric model discussed in Section 3 with MCMC inference as presented in Section 4 (denoted Semiparametric). We employ a squared exponential covariance function \u03ba(x, x\u2032) = \u03b1 exp ( \u2212\u2016x\u2212x\n\u2032\u20162 2\u03c32\n) , where the multiplicative con-\nstant \u03b1 is tuned on the training data and the bandwidth \u03c3 is set to the average distance between points in the training data. The Beta and Dirichlet parameters \u03bb and \u03c1 are set to one (Laplace smoothing), the prior p(\u03b7) for the Gamma parameters is uninformative. We use backoff-smoothing as discussed by Landwehr et al. (2014). We initialize the sampler with the maximum-likelihood Gamma fit and perform 10.000 sampling iterations, 5000 of which are burn-in iterations. As a baseline, we study the model by Landwehr et al. (2014) (Landwehr et al.) and simplified versions proposed by them that only use\nsaccade type and amplitude (Landwehr et al. (TA) ) or saccade type (Landwehr et al. (T) ). We also study the weighted and unweighted version of the featurebased model of Holland & Komogortsev (2012) with a feature set adapted to the Potsdam Sentence Corpus data as described in Landwehr et al. (2014).\nWe first study multiclass identification accuracy. All test observations of one particular individual constitute one test example; the task is to infer the individual that has generated these test observations. Multiclass identification accuracy is the fraction of cases in which the correct individual is identified. Table 1 shows multiclass identification accuracy for all methods. We observe that Semiparametric outperforms Landwehr et al., reducing the error by more than a factor of three. Consistent with results reported in Landwehr et al. (2014), Holland & K.\n(unweighted) is less accurate than Landwehr et al., but more accurate than the simplified variants. We next study how the amount of data available at test time\u2014that is, the amount of time we can observe a reader before having to make a decision\u2014influences accuracy. Figure 3 (left) shows identification accuracy as a function of the fraction of test data available, obtained by randomly removing a fraction of sentences from the test set. We observe that identification accuracy steadily improves with more test observations for all methods. Figure 3 (right) shows identification accuracy when varying the number R of individuals that need to be distinguished. We randomly draw a subset of R individuals from the set of 251 individuals, and perform identification based on only these individuals. Results are averaged over 10 such random draws. As expected, accuracy improves if fewer individuals need to be distinguished.\nWe next study a binary setting in which for each individual and each set of test observations a decision has to be made whether or not the test observations have been generated by that individual. This\nsetting more closely matches typical use cases for the deployment of a biometric system. Let X\u0304 denote the text being read at test time, and let S\u0304 denote the observed eye movement sequences. Our model infers for each reader r \u2208 R the marginal likelihood p(S\u0304|r, X\u0304 ,X ,S(1:R)) of the eye movement observations under the reader-specific model (Equation 3). The binary decision is made by dividing this marginal likelihood by the average marginal likelihood assigned to the observations by all reader-specific models, and comparing the result to a threshold \u03c4 . Figure 4 shows the fraction of false accepts as a function of false rejects as the threshold \u03c4 is varied, averaged over all individuals. The Landwehr et al. model and variants also assign a reader-specific likelihood to novel test observations; we compute the same statistics again by normalizing the likelihood and comparing to a threshold \u03c4 . Finally, Holland & K. (unweighted) and Holland & K. (weighted) compute a similarity measure for each combination of individual and set of test observations, which we normalize and threshold analogously. We observe that Semiparametric accomplishes a false-reject rate of below 1% at virtually no false accepts; Landwehr et al. and variants tend to perform better than Holland & K. (unweighted) and Holland & K. (weighted) . Table 2 shows the area under the curve for this experiment and the different methods. Figure 5 shows the same experiment for different amounts of test observations and a selected subset of methods.\nTraining the joint model for all 251 individuals takes 46 hours on a single eight-core CPU; predicting the most likely individual to have generated a set of 72 test sentences takes less than 2 seconds."}, {"heading": "6 Conclusions", "text": "We have studied the problem of identifying readers unobtrusively during reading of arbitrary text. For fitting reader-specific distributions, we employ a Bayesian semiparametric approach that infers densities under a Gaussian process prior centered at the gamma family of distributions, striking a balance between robustness to sparse data and modeling flexibility. In an empirical study with 251 individuals, the model was shown to reduce identification error by more than a factor of three compared to earlier approaches to reader identification proposed by Landwehr et al. (2014) and Holland & Komogortsev (2012)."}], "references": [{"title": "Gaussian process density sampler", "author": ["Adams et al.2009] Ryan P. Adams", "Iain Murray", "David J.C"], "venue": "MaxKay", "citeRegEx": "Adams et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2009}, {"title": "Eyemovements as a biometric", "author": ["Tomi Kinnunen", "Andrei Mihaila", "Pasi Fr\u00e4nti"], "venue": "In Proceedings of the 14th Scandinavian Conference on Image Analysis", "citeRegEx": "Bednarik et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bednarik et al\\.", "year": 2005}, {"title": "Studies in the psychology of reading", "author": ["W. Robert Dixon"], "venue": "Univ. of Michigan Monographs in Education No", "citeRegEx": "Dixon.,? \\Q1951\\E", "shortCiteRegEx": "Dixon.", "year": 1951}, {"title": "SWIFT: A dynamical model of saccade generation during reading", "author": ["Engbert et al.2005] Ralf Engbert", "Antje Nuthmann", "Eike M. Richter", "Reinhold Kliegl"], "venue": "Psychological Review,", "citeRegEx": "Engbert et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Engbert et al\\.", "year": 2005}, {"title": "Predicting word fixations in text with a CRF model for capturing general reading strategies among readers", "author": ["Hara et al.2012] Tadayoshi Hara", "Daichi Mochihashi", "Yoshino Kano", "Akiko Aizawa"], "venue": "In Proceedings of the First Workshop on Eye-Tracking and Natu-", "citeRegEx": "Hara et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hara et al\\.", "year": 2012}, {"title": "Analysing large datasets of eye movements during reading", "author": ["Kay-Michael W\u00fcrzner", "Reinhold Kliegl"], "venue": "Visual word recognition", "citeRegEx": "Heister et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Heister et al\\.", "year": 2012}, {"title": "Nonparametric density estimation with a parametric start", "author": ["Hjort", "Glad1995] Nils L. Hjort", "Ingrid K. Glad"], "venue": null, "citeRegEx": "Hjort et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hjort et al\\.", "year": 1995}, {"title": "Biometric identification via eye movement scanpaths in reading", "author": ["Holland", "Komogortsev2012] Corey Holland", "Oleg V. Komogortsev"], "venue": "In Proceedings of the 2011 International Joint Conference on Biometrics", "citeRegEx": "Holland et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Holland et al\\.", "year": 2012}, {"title": "The psychology and pedagogy of reading", "author": ["Edmund B. Huey"], "venue": null, "citeRegEx": "Huey.,? \\Q1908\\E", "shortCiteRegEx": "Huey.", "year": 1908}, {"title": "Eye movements in biometrics", "author": ["Kasprowski", "Ober2004] Pawel Kasprowski", "Jozef Ober"], "venue": "In Proceedings of the 2004 International Biometric Authentication Workshop", "citeRegEx": "Kasprowski et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kasprowski et al\\.", "year": 2004}, {"title": "Tracking the mind during reading: The influence of past, present, and future words on fixation durations", "author": ["Antje Nuthmann", "Ralf Engbert"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "Kliegl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kliegl et al\\.", "year": 2006}, {"title": "Biometric identification via an oculomotor plant mathematical model", "author": ["Sampath Jayarathna", "Cecilia R. Aragon", "Mechehoul Mahmoud"], "venue": "In Proceedings of the 2010 Symposium on Eye-Tracking Research & Appli-", "citeRegEx": "Komogortsev et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Komogortsev et al\\.", "year": 2010}, {"title": "A model of individual differences in gaze control during reading", "author": ["Sebastian Arzt", "Tobias Scheffer", "Reinhold Kliegl"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods on Natural Language Processing", "citeRegEx": "Landwehr et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Landwehr et al\\.", "year": 2014}, {"title": "Bayesian semiparametric density estimation and model verification using a logistic-Gaussian process", "author": ["Peter J. Lenk"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Lenk.,? \\Q2003\\E", "shortCiteRegEx": "Lenk.", "year": 2003}, {"title": "Density estimation, stochastic processes and prior information", "author": ["Tom Leonard"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Leonard.,? \\Q1978\\E", "shortCiteRegEx": "Leonard.", "year": 1978}, {"title": "With blinkers on: robust prediction of eye movements across readers", "author": ["Matties", "S\u00f8gaard2013] Franz Matties", "Anders S\u00f8gaard"], "venue": "In Proceedings of the 2013 Conference on Empirical Natural Language Processing", "citeRegEx": "Matties et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Matties et al\\.", "year": 2013}, {"title": "Eye movements in reading and information processing: 20 years of research", "author": ["Keith Rayner"], "venue": "Psychological Bulletin,", "citeRegEx": "Rayner.,? \\Q1998\\E", "shortCiteRegEx": "Rayner.", "year": 1998}, {"title": "Toward a model of eye movement control in reading", "author": ["Alexander Pollatsek", "Donald L. Fisher", "Keith Rayner"], "venue": "Psychological Review,", "citeRegEx": "Reichle et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Reichle et al\\.", "year": 1998}, {"title": "Biometric identification based on the eye movements and graph matching techniques", "author": ["Rigas et al.2012a] Ioannis Rigas", "George Economou", "Spiros Fotopoulos"], "venue": "Pattern Recognition", "citeRegEx": "Rigas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rigas et al\\.", "year": 2012}, {"title": "Human eye movements as a trait for biometrical identification", "author": ["Rigas et al.2012b] Ioannis Rigas", "George Economou", "Spiros Fotopoulos"], "venue": "In Proceedings of the IEEE 5th International Conference on Biometrics: Theory, Applications and Systems", "citeRegEx": "Rigas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rigas et al\\.", "year": 2012}, {"title": "Bayesian density regression with logistic gaussian process and subspace projection", "author": ["Yu M. Zhuy", "Jayanta K. Ghoshz"], "venue": "Bayesian Analysis,", "citeRegEx": "Tokdar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tokdar et al\\.", "year": 2010}, {"title": "Penalized semiparametric density estimation", "author": ["Ying Yang"], "venue": "Statistics and Computing,", "citeRegEx": "Yang.,? \\Q2009\\E", "shortCiteRegEx": "Yang.", "year": 2009}, {"title": "On biometric verification of a user by means of eye movement data mining", "author": ["Zhang", "Juhola2012] Youming Zhang", "Martti Juhola"], "venue": "In Proceedings of the 2nd International Conference on Advances in Information Mining and Management", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Eye movements are driven both by low-level visual cues and high-level linguistic and cognitive processes related to text understanding; as a reflection of the interplay between vision, cognition, and motor control during reading they are frequently studied in cognitive psychology (Kliegl et al., 2006; Rayner, 1998).", "startOffset": 281, "endOffset": 316}, {"referenceID": 16, "context": "Eye movements are driven both by low-level visual cues and high-level linguistic and cognitive processes related to text understanding; as a reflection of the interplay between vision, cognition, and motor control during reading they are frequently studied in cognitive psychology (Kliegl et al., 2006; Rayner, 1998).", "startOffset": 281, "endOffset": 316}, {"referenceID": 3, "context": "Computational models (Engbert et al., 2005; Reichle et al., 1998) as well as models based on machine learning (Matties and S\u00f8gaard, 2013; Hara et al.", "startOffset": 21, "endOffset": 65}, {"referenceID": 17, "context": "Computational models (Engbert et al., 2005; Reichle et al., 1998) as well as models based on machine learning (Matties and S\u00f8gaard, 2013; Hara et al.", "startOffset": 21, "endOffset": 65}, {"referenceID": 4, "context": ", 1998) as well as models based on machine learning (Matties and S\u00f8gaard, 2013; Hara et al., 2012) have been developed to study how gaze patterns arise based on text content and structure, facilitating the understanding of human reading processes.", "startOffset": 52, "endOffset": 98}, {"referenceID": 8, "context": "A central observation in these and earlier psychological studies (Huey, 1908; Dixon, 1951) is that eye movement patterns strongly differ between individuals.", "startOffset": 65, "endOffset": 90}, {"referenceID": 2, "context": "A central observation in these and earlier psychological studies (Huey, 1908; Dixon, 1951) is that eye movement patterns strongly differ between individuals.", "startOffset": 65, "endOffset": 90}, {"referenceID": 11, "context": "This is in contrast to approaches where biometric identification is based on eye movements in response to an artificial visual stimulus, for example a moving (Kasprowski and Ober, 2004; Komogortsev et al., 2010; Rigas et al., 2012b; Zhang and Juhola, 2012) or fixed (Bednarik et al.", "startOffset": 158, "endOffset": 256}, {"referenceID": 1, "context": ", 2012b; Zhang and Juhola, 2012) or fixed (Bednarik et al., 2005) dot on a computer screen, or a specific image stimulus (Rigas et al.", "startOffset": 42, "endOffset": 65}, {"referenceID": 1, "context": "A central observation in these and earlier psychological studies (Huey, 1908; Dixon, 1951) is that eye movement patterns strongly differ between individuals. Holland et al. (2012) and Landwehr et al.", "startOffset": 78, "endOffset": 180}, {"referenceID": 1, "context": "A central observation in these and earlier psychological studies (Huey, 1908; Dixon, 1951) is that eye movement patterns strongly differ between individuals. Holland et al. (2012) and Landwehr et al. (2014) have developed models of individual differences in eye movement patterns during reading, and studied these models in a biometric problem setting where an individual has to be identified based on observing her eye movement patterns while reading arbitrary text.", "startOffset": 78, "endOffset": 207}, {"referenceID": 12, "context": "Landwehr et al. (2014) showed that readers can be identified more accurately with a model that captures aspects of individual-specific distributions over ar X iv :1 60 7.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "Landwehr et al. (2014) used a fully parametric approach where all densities are assumed to be in the gamma family; gamma distributions were shown to approximate the true distribution of interest well for most cases (see Figure 1).", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "Landwehr et al. (2014) used a fully parametric approach where all densities are assumed to be in the gamma family; gamma distributions were shown to approximate the true distribution of interest well for most cases (see Figure 1). This model is robust to sparse data, but might not be flexible enough to capture all differences between readers. The model we study in this paper follows ideas developed by Landwehr et al. (2014), but employs more flexible semiparametric density models.", "startOffset": 0, "endOffset": 428}, {"referenceID": 12, "context": "Landwehr et al. (2014) used a fully parametric approach where all densities are assumed to be in the gamma family; gamma distributions were shown to approximate the true distribution of interest well for most cases (see Figure 1). This model is robust to sparse data, but might not be flexible enough to capture all differences between readers. The model we study in this paper follows ideas developed by Landwehr et al. (2014), but employs more flexible semiparametric density models. Specifically, we place a Gaussian process prior over densities that concentrates probability mass on densities that are close to the gamma family. Given data, a posterior distribution over densities is derived. If data is sparse, the posterior will still be sharply peaked around distributions in the gamma family, reducing the effective capacity of the model and minimizing overfitting. However, given enough evidence in the data, the model will also deviate from the gamma-centered prior and represent any density function. Integrating over the space of densities weighted by the posterior yields a marginal likelihood for novel observations from which predictions are inferred. We empirically study this model in the same setting as studied by Landwehr et al. (2014), but using an order of magnitude more individuals.", "startOffset": 0, "endOffset": 1256}, {"referenceID": 12, "context": "The probabilistic model we employ follows the general structure proposed by Landwehr et al. (2014), but employs semiparametric density models and allows for fully Bayesian inference.", "startOffset": 76, "endOffset": 99}, {"referenceID": 5, "context": ", Heister et al. (2012). We observe empirically that for each saccade type, there is a characteristic distribution over saccade amplitudes and fixation durations, and that both approximately follow gamma distributions\u2014see Figure 1.", "startOffset": 2, "endOffset": 24}, {"referenceID": 0, "context": "The density model defined by Equations 24 to 26 draws on ideas from the large body of literature on GP-based density estimation, for example by Adams et al. (2009), Leonard (1978), or Tokdar et al.", "startOffset": 144, "endOffset": 164}, {"referenceID": 0, "context": "The density model defined by Equations 24 to 26 draws on ideas from the large body of literature on GP-based density estimation, for example by Adams et al. (2009), Leonard (1978), or Tokdar et al.", "startOffset": 144, "endOffset": 180}, {"referenceID": 0, "context": "The density model defined by Equations 24 to 26 draws on ideas from the large body of literature on GP-based density estimation, for example by Adams et al. (2009), Leonard (1978), or Tokdar et al. (2010), and semiparametric density estimation, e.", "startOffset": 144, "endOffset": 205}, {"referenceID": 0, "context": "The density model defined by Equations 24 to 26 draws on ideas from the large body of literature on GP-based density estimation, for example by Adams et al. (2009), Leonard (1978), or Tokdar et al. (2010), and semiparametric density estimation, e.g. as discussed by Yang (2009), Lenk (2003) or Hjort & Glad (1995).", "startOffset": 144, "endOffset": 278}, {"referenceID": 0, "context": "The density model defined by Equations 24 to 26 draws on ideas from the large body of literature on GP-based density estimation, for example by Adams et al. (2009), Leonard (1978), or Tokdar et al. (2010), and semiparametric density estimation, e.g. as discussed by Yang (2009), Lenk (2003) or Hjort & Glad (1995).", "startOffset": 144, "endOffset": 291}, {"referenceID": 0, "context": "The density model defined by Equations 24 to 26 draws on ideas from the large body of literature on GP-based density estimation, for example by Adams et al. (2009), Leonard (1978), or Tokdar et al. (2010), and semiparametric density estimation, e.g. as discussed by Yang (2009), Lenk (2003) or Hjort & Glad (1995). However, note that existing density estimation approaches are not applicable offthe-shelf as in our domain distributions are truncated differently at each observation due to constraints that arise from the way eye movements interact with the text structure (Equations 16 to 19).", "startOffset": 144, "endOffset": 314}, {"referenceID": 12, "context": "We conduct a large-scale study of biometric identification performance using the same setup as discussed by Landwehr et al. (2014) but a much larger set of individuals (251 rather than 20).", "startOffset": 108, "endOffset": 131}, {"referenceID": 10, "context": "obtained from an EyeLink II system with a 500Hz sampling rate (SR Research, Ongoode, Ontario, Canada) while reading sentences from the Potsdam Sentence Corpus (Kliegl et al., 2006).", "startOffset": 159, "endOffset": 180}, {"referenceID": 12, "context": "We use backoff-smoothing as discussed by Landwehr et al. (2014). We initialize the sampler with the maximum-likelihood Gamma fit and perform 10.", "startOffset": 41, "endOffset": 64}, {"referenceID": 12, "context": "We use backoff-smoothing as discussed by Landwehr et al. (2014). We initialize the sampler with the maximum-likelihood Gamma fit and perform 10.000 sampling iterations, 5000 of which are burn-in iterations. As a baseline, we study the model by Landwehr et al. (2014) (Landwehr et al.", "startOffset": 41, "endOffset": 267}, {"referenceID": 12, "context": "saccade type and amplitude (Landwehr et al. (TA) ) or saccade type (Landwehr et al. (T) ). We also study the weighted and unweighted version of the featurebased model of Holland & Komogortsev (2012) with a feature set adapted to the Potsdam Sentence Corpus data as described in Landwehr et al.", "startOffset": 28, "endOffset": 199}, {"referenceID": 12, "context": "saccade type and amplitude (Landwehr et al. (TA) ) or saccade type (Landwehr et al. (T) ). We also study the weighted and unweighted version of the featurebased model of Holland & Komogortsev (2012) with a feature set adapted to the Potsdam Sentence Corpus data as described in Landwehr et al. (2014).", "startOffset": 28, "endOffset": 301}, {"referenceID": 12, "context": "We observe that Semiparametric outperforms Landwehr et al., reducing the error by more than a factor of three. Consistent with results reported in Landwehr et al. (2014), Holland & K.", "startOffset": 43, "endOffset": 170}, {"referenceID": 12, "context": "In an empirical study with 251 individuals, the model was shown to reduce identification error by more than a factor of three compared to earlier approaches to reader identification proposed by Landwehr et al. (2014) and Holland & Komogortsev (2012).", "startOffset": 194, "endOffset": 217}, {"referenceID": 12, "context": "In an empirical study with 251 individuals, the model was shown to reduce identification error by more than a factor of three compared to earlier approaches to reader identification proposed by Landwehr et al. (2014) and Holland & Komogortsev (2012).", "startOffset": 194, "endOffset": 250}], "year": 2016, "abstractText": "We study the problem of identifying individuals based on their characteristic gaze patterns during reading of arbitrary text. The motivation for this problem is an unobtrusive biometric setting in which a user is observed during access to a document, but no specific challenge protocol requiring the user\u2019s time and attention is carried out. Existing models of individual differences in gaze control during reading are either based on simple aggregate features of eye movements, or rely on parametric density models to describe, for instance, saccade amplitudes or word fixation durations. We develop flexible semiparametric models of eye movements during reading in which densities are inferred under a Gaussian process prior centered at a parametric distribution family that is expected to approximate the true distribution well. An empirical study on reading data from 251 individuals shows significant improvements over the state of the art.", "creator": "LaTeX with hyperref package"}}}