{"id": "1607.06333", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2016", "title": "Uncovering Causality from Multivariate Hawkes Integrated Cumulants", "abstract": "tootsie We design forewarning a nicoline new reservations nonparametric nandana method 111.9 that allows one to estimate nadeau the 8,160 matrix of integrated kaula kernels barce of a bianconi multivariate sherone Hawkes heiress process. This matrix arpeggiated not arnhold only archtop encodes karembeu the tjp mutual durazno influences of each nodes ideapad of 18c the pervading process, tlacolula but surtr also duing disentangles tsvetaeva the causality fichtelgebirge relationships handlowy between them. luebbe Our approach is hayashi the first that leads to mid-1944 an janmabhoomi estimation woke of this matrix parenthetical without any parametric west-germany modeling and wrc estimation funcionarios of 2na the presiding kernels themselves. sonthaya A studeman consequence is that it can 108.32 give 24-11 an estimation of causality relationships jawal between uncials nodes (rapee or kuujjuaq users ), guangyun based 71-52 on khashan their linguistique activity eppy timestamps (on a social network 7,280 for instance ), mccloskey without tunb knowing zhangsun or estimating the shape d'oliveira of laborite the activities webgl lifetime. For higbie that purpose, we 6-17 introduce a moment counselling matching smolin method that fits the puttumatalan third - dansie order pallidum integrated brampton cumulants bonsu of daisen the cinebook process. tjallingii We skorokhod show nekron on numerical set-50 experiments beckler that i8 our approach dreco is indeed very robust gaither to the 3,888 shape of the kernels, gallops and cbot gives appealing results on 7-ranked the MemeTracker database.", "histories": [["v1", "Thu, 21 Jul 2016 14:19:23 GMT  (412kb,D)", "https://arxiv.org/abs/1607.06333v1", null], ["v2", "Thu, 13 Oct 2016 22:46:27 GMT  (288kb,D)", "http://arxiv.org/abs/1607.06333v2", null], ["v3", "Tue, 30 May 2017 00:06:14 GMT  (2023kb,D)", "http://arxiv.org/abs/1607.06333v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["massil achab", "emmanuel bacry", "st\u00e9phane ga\u00efffas", "iacopo mastromatteo", "jean-fran\u00e7ois muzy"], "accepted": true, "id": "1607.06333"}, "pdf": {"name": "1607.06333.pdf", "metadata": {"source": "CRF", "title": "Uncovering Causality from Multivariate Hawkes Integrated Cumulants", "authors": ["Massil Achab", "Emmanuel Bacry", "St\u00e9phane Gaiffas", "Iacopo Mastromatteo", "Jean-Fran\u00e7ois Muzy"], "emails": ["massil.achab@m4x.org"], "sections": [{"heading": null, "text": "Keywords. Hawkes Process, Causality Inference, Cumulants, Generalized Method of Moments"}, {"heading": "1 Introduction", "text": "In many applications, one needs to deal with data containing a very large number of irregular timestamped events that are recorded in continuous time. These events can reflect, for instance, the activity of users on a social network, see Subrahmanian et al. (2016), the high-frequency variations of signals in finance, see Bacry et al. (2015), the earthquakes and aftershocks in geophysics, see Ogata (1998), the crime activity, see Mohler et al. (2011) or the position of genes in genomics, see Reynaud-Bouret and Schbath (2010). The succession of the precise timestamps carries a great deal of information about the dynamics of the underlying systems. In this context, multidimensional counting processes based models play a paramount role. Within this framework, an important task is to recover the mutual influence of the nodes (i.e., the different components of the counting process), by leveraging on their timestamp patterns, see, for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou et al. (2013a); Gomez-Rodriguez et al. (2013); Farajtabar et al. (2015); Xu et al. (2016).\nConsider a set of nodes I = {1, . . . , d}. For each i \u2208 I , we observe a set Zi of events, where each \u03c4 \u2208 Zi labels the occurrence time of an event related to the activity of i. The events of all nodes can \u2217massil.achab@m4x.org\nar X\niv :1\n60 7.\n06 33\n3v 3\n[ st\nat .M\nL ]\n3 0\nM ay\nbe represented as a vector of counting processes N t = [N1t \u00b7 \u00b7 \u00b7Ndt ]>, where N it counts the number of events of node i until time t \u2208 R+, namely N it = \u2211 \u03c4\u2208Zi 1{\u03c4\u2264t}. The vector of stochastic intensities \u03bbt = [\u03bb 1 t \u00b7 \u00b7 \u00b7\u03bbdt ]> associated with the multivariate counting processN t is defined as\n\u03bbit = lim dt\u21920 P(N it+dt \u2212N it = 1|Ft) dt\nfor i \u2208 I , where the filtration Ft encodes the information available up to time t. The coordinate \u03bbit gives the expected instantaneous rate of event occurrence at time t for node i. The vector \u03bbt characterizes the distribution of N t, see Daley and Vere-Jones (2003), and patterns in the events time-series can be captured by structuring these intensities.\nThe Hawkes process introduced in Hawkes (1971) corresponds to an autoregressive structure of the intensities in order to capture self-excitation and cross-excitation of nodes, which is a phenomenon typically observed, for instance, in social networks, see for instance Crane and Sornette (2008). Namely, N t is called a Hawkes point process if the stochastic intensities can be written as\n\u03bbit = \u00b5 i + d\u2211 j=1 \u222b t 0 \u03c6ij(t\u2212 t\u2032)dN jt\u2032 ,\nwhere \u00b5i \u2208 R+ is an exogenous intensity and \u03c6ij are positive, integrable and causal (with support in R+) functions called kernels encoding the impact of an action by node j on the activity of node i. Note that when all kernels are zero, the process is a simple homogeneous multivariate Poisson process.\nMost of the litterature uses a parametric approach for estimating the kernels. With no doubt, the most popular parametrization form is the exponential kernel \u03c6ij(t) = \u03b1ij\u03b2ije\u2212\u03b2ijt because it definitely simplifies the inference algorithm (e.g., the complexity needed for computing the likelihood is much smaller). When d is large, in order to reduce the number of parameters, some authors choose to arbitrarily share the kernel shapes across the different nodes. Thus, for instance, in Yang and Zha (2013); Zhou et al. (2013b); Farajtabar et al. (2015), they choose \u03c6ij(t) = \u03b1ijh(t) with \u03b1ij \u2208 R+ quantifies the intensity of the influence of j on i and h(t) a (normalized) function that characterizes the time-profile of this influence and that is shared by all couples of nodes (i, j) (most often, it is chosen to be either exponential h(t) = \u03b2e\u2212\u03b2t or power law h(t) = \u03b2t\u2212(\u03b2+1)). Both approaches are, most of the time, highly non-realistic. On the one hand there is a priori no reason for assuming that the time-profile of the influence of a node j on a node i does not depend on the pair (i, j). On the other hand, assuming an exponential shape or a power law shape for a kernel arbitrarily imposes an event impact that is always instantly maximal and that can only decrease with time, while in practice, there may exist a latency between an event and its maximal impact.\nIn order to have more flexibility on the shape of the kernels, nonparametric estimation can be considered. Expectation-Maximization algorithms can be found in Lewis and Mohler (2011) (for d = 1) or in Zhou et al. (2013a) (d > 1). An alternative method is proposed in Bacry and Muzy (2016) where the nonparametric estimation is formulated as a numerical solving of a Wiener-Hopf equation. Another nonparametric strategy considers a decomposition of kernels on a dictionary of function h1, . . . , hK , namely \u03c6ij(t) = \u2211K k=1 a ij k hk(t), where the coefficients a ij k are estimated, see Hansen et al. (2015); Lemonnier and Vayatis (2014) and Xu et al. (2016), where group-lasso is used to induce a sparsity pattern on the coefficients aijk that is shared across k = 1, . . . ,K.\nSuch methods are heavy when d is large, since they rely on likelihood maximization or least squares minimization within an over-parametrized space in order to gain flexibility on the shape of the kernels. This is problematic, since the original motivation for the use of Hawkes processes is to estimate the influence and causality of nodes, the knowledge of the full parametrization of the model being of little\ninterest for causality purpose.\nOur paper solves this problem with a different and more direct approach. Instead of trying to estimate the kernels \u03c6ij , we focus on the direct estimation of their integrals. Namely, we want to estimate the matrixG = [gij ] where\ngij = \u222b +\u221e 0 \u03c6ij(u) du \u2265 0 for 1 \u2264 i, j \u2264 d. (1)\nAs it can be seen from the cluster representation of Hawkes processes (Hawkes and Oakes (1974)), this integral represents the mean total number of events of type i directly triggered by an event of type j, and then encodes a notion of causality. Actually, as detailed below (see Section 2.1), such integral can be related to the Granger causality (Granger (1969)).\nThe main idea of the method we developed in this paper is to estimate the matrixG directly using a matching cumulants (or moments) method. Apart from the mean, we shall use second and third-order cumulants which correspond respectively to centered second and third-order moments. We first compute an estimation M\u0302 of these centered moments M(G) (they are uniquely defined byG). Then, we look for a matrix G\u0302 that minimizes the L2 error \u2016M(G\u0302)\u2212 M\u0302\u20162. Thus the integral matrix G\u0302 is directly estimated without making hardly any assumptions on the shape the involved kernels. As it will be shown, this approach turns out to be particularly robust to the kernel shapes, which is not the case of all previous Hawkes-based approaches that aim causality recovery. We call this method NPHC (Non Parametric Hawkes Cumulant), since our approach is of nonparametric nature. We provide a theoretical analysis that proves the consistency of the NPHC estimator. Our proof is based on ideas from the theory of Generalized Method of Moments (GMM) but requires an original technical trick since our setting strongly departs from the standard parametric statistics with i.i.d observations. Note that moment and cumulant matching techniques proved particularly powerful for latent topic models, in particular Latent Dirichlet Allocation, see Podosinnikova et al. (2015). A small set of previous works, namely Da Fonseca and Zaatour (2014); A\u00eft-Sahalia et al. (2010), already used method of moments with Hawkes processes, but only in a parametric setting. Our work is the first to consider such an approach for a nonparametric counting processes framework.\nThe paper is organized as follows: in Section 2, we provide the background on the integrated kernels and the integrated cumulants of the Hawkes process. We then introduce the method, investigate its complexity and explain the consistency result we prove. In Section 3, we estimate the matrix of Hawkes kernels\u2019 integrals for various simulated datasets and for real datasets, namely the MemeTracker database and financial order book data. We then provide in Section 4 the technical details skipped in the previous parts and the proof of our consistency result. Section 5 contains concluding remarks."}, {"heading": "2 NPHC: The Non Parametric Hawkes Cumulant method", "text": "In this Section, we provide the background on integrals of Hawkes kernels and integrals of Hawkes cumulants. We then explain how the NPHC method enables estimatingG."}, {"heading": "2.1 Branching structure and Granger causality", "text": "From the definition of Hawkes process as a Poisson cluster process, see Jovanovic\u0301 et al. (2015) or Hawkes and Oakes (1974), gij can be simply interpreted as the average total number of events of node i whose direct ancestor is a given event of node j (by direct we mean that interactions mediated by any other intermediate event are not counted). In that respect,G not only describes the mutual influences between\nnodes, but it also quantifies their direct causal relationships. Namely, introducing the counting function N i\u2190jt that counts the number of events of i whose direct ancestor is an event of j, we know from Bacry et al. (2015) that\nE[dN i\u2190jt ] = g ijE[dN jt ] = g ij\u039bjdt, (2)\nwhere we introduced \u039bi as the intensity expectation, namely satisfying E[dN it ] = \u039bidt. Note that \u039bi does not depend on time by stationarity ofN t, which is known to hold under the stability condition \u2016G\u2016 < 1, where \u2016G\u2016 stands for the spectral norm ofG. In particular, this condition implies the non-singularity of Id \u2212G.\nSince the question of a real causality is too complex in general, most econometricians agreed on the simpler definition of Granger causality Granger (1969). Its mathematical formulation is a statistical hypothesis test: X causes Y in the sense of Granger causality if forecasting future values of Y is more successful while taking X past values into account. In Eichler et al. (2016), it is shown that for N t a multivariate Hawkes process, N jt does not Granger-cause N i t w.r.t N t if and only if \u03c6\nij(u) = 0 for u \u2208 R+. Since the kernels take positive values, the latter condition is equivalent to \u222b\u221e 0 \u03c6\nij(u)du = 0. In the following, we\u2019ll refer to learning the kernels\u2019 integrals as uncovering causality since each integral encodes the notion of Granger causality, and is also linked to the number of events directly caused from a node to another node, as described above at Eq. (2)."}, {"heading": "2.2 Integrated cumulants of the Hawkes process", "text": "A general formula for the integral of the cumulants of a multivariate Hawkes process is provided in Jovanovic\u0301 et al. (2015). As explained below, for the purpose of our method, we only need to consider cumulants up to the third order. Given 1 \u2264 i, j, k \u2264 d, the first three integrated cumulants of the Hawkes process can be defined as follows thanks to stationarity:\n\u039bidt = E(dN it ) (3)\nCijdt = \u222b \u03c4\u2208R ( E(dN itdN j t+\u03c4 )\u2212 E(dN it )E(dN j t+\u03c4 ) ) (4)\nKijkdt = \u222b \u222b \u03c4,\u03c4 \u2032\u2208R2 ( E(dN itdN j t+\u03c4dN k t+\u03c4 \u2032) + 2E(dN i t )E(dN j t+\u03c4 )E(dN k t+\u03c4 \u2032)\n\u2212 E(dN itdN j t+\u03c4 )E(dN k t+\u03c4 \u2032)\u2212 E(dN itdNkt+\u03c4 \u2032)E(dN j t+\u03c4 )\u2212 E(dN j t+\u03c4dN k t+\u03c4 \u2032)E(dN i t ) ) ,\n(5)\nwhere Eq. (3) is the mean intensity of the Hawkes process, the second-order cumulant (4) refers to the integrated covariance density matrix and the third-order cumulant (5) measures the skewness ofN t. Using the martingale representation from Bacry and Muzy (2016) or the Poisson cluster process representation from Jovanovic\u0301 et al. (2015), one can obtain an explicit relationship between these integrated cumulants and the matrixG. If one sets R = (Id \u2212G)\u22121, (6) straightforward computations (see Section 4) lead to the following identities:\n\u039bi = d\u2211\nm=1\nRim\u00b5m (7)\nCij = d\u2211 m=1 \u039bmRimRjm (8)\nKijk = d\u2211 m=1 (RimRjmCkm +RimCjmRkm + CimRjmRkm \u2212 2\u039bmRimRjmRkm). (9)\nEquations (8) and (9) are proved in Section 4. Our strategy is to use a convenient subset of Eqs. (3), (4) and (5) to define M , while we use Eqs. (7), (8) and (9) in order to construct the operator that maps a candidate matrix R to the corresponding cumulants M(R). By looking for R\u0302 that minimizes R 7\u2192 \u2016M(R) \u2212 M\u0302\u20162, we obtain, as illustrated below, good recovery of the ground truth matrix G using Equation (6).\nThe simplest case d = 1 has been considered in Hardiman and Bouchaud (2014), where it is shown that one can choose M = {C11} in order to compute the kernel integral. Eq. (8) then reduces to a simple second-order equation that has a unique solution inR (and consequently a uniqueG) that accounts for the stability condition (\u2016G\u2016 < 1).\nUnfortunately, for d > 1, the choice M = {Cij}1\u2264i\u2264j\u2264d is not sufficient to uniquely determine the kernels integrals. In fact, the integrated covariance matrix provides d(d+ 1)/2 independent coefficients, while d2 parameters are needed. It is straightforward to show that the remaining d(d\u2212 1)/2 conditions can be encoded in an orthogonal matrixO, reflecting the fact that Eq. (8) is invariant under the change R\u2192 OR, so that the system is under-determined.\nOur approach relies on using the third order cumulant tensorK = [Kijk] which contains (d3 + 3d2 + 2d)/6 > d2 independent coefficients that are sufficient to uniquely fix the matrixG. This can be justified intuitively as follows: while the integrated covariance only contains symmetric information, and is thus unable to provide causal information, the skewness given by the third order cumulant in the estimation procedure can break the symmetry between past and future so as to uniquely fixG. Thus, our algorithm consists of selecting d2 third-order cumulant components, namely M = {Kiij}1\u2264i,j\u2264d. In particular, we define the estimator ofR as R\u0302 \u2208 argminRL(R), where\nL(R) = (1\u2212 \u03ba)\u2016Kc(R)\u2212 K\u0302c\u201622 + \u03ba\u2016C(R)\u2212 C\u0302\u201622, (10)\nwhere \u2016 \u00b7\u20162 stands for the Frobenius norm,Kc = {Kiij}1\u2264i,j\u2264d is the matrix obtained by the contraction of the tensorK to d2 indices,C is the covariance matrix, while K\u0302c and C\u0302 are their respective estimators, see Equations (12), (13) below. It is noteworthy that the above mean square error approach can be seen as a peculiar Generalized Method of Moments (GMM), see Hall (2005). This framework allows us to determine the optimal weighting matrix involved in the loss function. However, this approach is unusable in practice, since the associated complexity is too high. Indeed, since we have d2 parameters, this matrix has d4 coefficients and GMM calls for computing its inverse leading to a O(d6) complexity. In this work, we use the coefficient \u03ba to scale the two terms, as\n\u03ba = \u2016K\u0302c\u201622\n\u2016K\u0302c\u201622 + \u2016C\u0302\u201622 ,\nsee Section 4.4 for an explanation about the link between \u03ba and the weighting matrix. Finally, the estimator ofG is straightforwardly obtained as\nG\u0302 = Id \u2212 R\u0302 \u22121 ,\nfrom the inversion of Eq. (6). Let us mention an important point: the matrix inversion in the previous formula is not the bottleneck of the algorithm. Indeed, its has a complexity O(d3) that is cheap compared to the computation of the cumulants when n = maxi |Zi| d, which is the typical scaling satisfied in applications. Solving the considered problem on a larger scale, say d 103, is an open question, even with state-of-the-art parametric and nonparametric approaches, see for instance Zhou et al. (2013a); Xu et al. (2016); Zhou et al. (2013b); Bacry and Muzy (2016), where the number of components d in experiments is always around 100 or smaller. Note that, actually, our approach leads to a much faster algorithm than the considered state-of-the-art baselines, see Tables 1\u20134 from Section 3 below."}, {"heading": "2.3 Estimation of the integrated cumulants", "text": "In this section we present explicit formulas to estimate the three moment-based quantities listed in the previous section, namely, \u039b, C and K. We first assume there exists H > 0 such that the truncation from (\u2212\u221e,+\u221e) to [\u2212H,H] of the domain of integration of the quantities appearing in Eqs. (4) and (5), introduces only a small error. In practice, this amounts to neglecting border effects in the covariance density and in the skewness density that is a good approximation if the support of the kernel \u03c6ij(t) is smaller than H and the spectral norm \u2016G\u2016 satisfies \u2016G\u2016 < 1. In this case, given a realization of a stationary Hawkes process {N t : t \u2208 [0, T ]}, as shown in Section 4, we can write the estimators of the first three cumulants (3), (4) and (5) as\n\u039b\u0302i = 1\nT \u2211 \u03c4\u2208Zi 1 = N iT T\n(11)\nC\u0302ij = 1\nT \u2211 \u03c4\u2208Zi ( N j\u03c4+H \u2212N j \u03c4\u2212H \u2212 2H\u039b\u0302 j )\n(12)\nK\u0302ijk = 1\nT \u2211 \u03c4\u2208Zi ( N j\u03c4+H \u2212N j \u03c4\u2212H \u2212 2H\u039b\u0302 j ) \u00b7 ( Nk\u03c4+H \u2212Nk\u03c4\u2212H \u2212 2H\u039b\u0302k ) \u2212 \u039b\u0302 i\nT \u2211 \u03c4\u2208Zj \u2211 \u03c4 \u2032\u2208Zk (2H \u2212 |\u03c4 \u2032 \u2212 \u03c4 |)+ + 4H2\u039b\u0302i\u039b\u0302j\u039b\u0302k. (13)\nLet us mention the following facts.\nBias. While the first cumulant \u039b\u0302i is an unbiased estimator of \u039bi, the other estimators C\u0302ij and K\u0302ijk introduce a bias. However, as we will show, in practice this bias is small and hardly affects numerical estimations (see Section 3). This is confirmed by our theoretical analysis, which proves that if H does not grow too fast compared to T , then these estimated cumulants are consistent estimators of the theoretical cumulants (see Section 2.6).\nComplexity. The computations of all the estimators of the first, second and third-order cumulants have complexity respectively O(nd), O(nd2) and O(nd3), where n = maxi |Zi|. However, our algorithm requires a lot less than that: it computes only d2 third-order terms, of the form K\u0302iij , leaving us with only O(nd2) operations to perform.\nSymmetry. While the values of \u039bi, Cij and Kijk are symmetric under permutation of the indices, their estimators are generally not symmetric. We have thus chosen to symmetrize the estimators by averaging their values over permutations of the indices. Worst case is for the estimator of Kc, which involves only an extra factor of 2 in the complexity."}, {"heading": "2.4 The NPHC algorithm", "text": "The objective to minimize in Equation (10) is non-convex. More precisely, the loss function is a polynomial ofR of degree 6. However, the expectations of cumulants \u039b andC defined in Eq. (4) and (5) that appear in the definition of L(R) are unknown and should be replaced with \u039b\u0302 and C\u0302. We denote L\u0303(R) the objective function, where the expectations of cumulants \u039bi and Cij have been replaced with their estimators in the right-hand side of Eqs. (8) and (9):\nL\u0303(R) = (1\u2212 \u03ba)\u2016R 2C\u0302 >\n+ 2[R (C\u0302 \u2212RL\u0302)]R> \u2212 K\u0302c\u201622 + \u03ba\u2016RL\u0302R> \u2212 C\u0302\u201622 (14)\nAs explained in Choromanska et al. (2015), the loss function of a typical multilayer neural network with simple nonlinearities can be expressed as a polynomial function of the weights in the network,\nwhose degree is the number of layers. Since the loss function of NPHC writes as a polynomial of degree 6, we expect good results using optimization methods designed to train deep multilayer neural networks. We used the AdaGrad from Duchi et al. (2011), a variant of the Stochastic Gradient Descent with adaptive learning rates. AdaGrad scales the learning rates coordinate-wise using the online variance of the previous gradients, in order to incorporate second-order information during training. The NPHC method is summarized schematically in Algorithm 1.\nAlgorithm 1 Non Parametric Hawkes Cumulant method Input: N t Output: G\u0302\n1: Estimate \u039b\u0302i, C\u0302ij , K\u0302iij from Eqs. (11, 12, 13) 2: Design L\u0303(R) using the computed estimators. 3: Minimize numerically L\u0303(R) so as to obtain R\u0302 4: Return G\u0302 = Id \u2212 R\u0302 \u22121 .\nOur problem being non-convex, the choice of the starting point has a major effect on the convergence. Here, the key is to notice that the matrices R that match Equation (8) writes C1/2OL\u22121/2, with L = diag(\u039b) andO an orthogonal matrix. Our starting point is then simply chosen by settingO = Id in the previous formula, leading to nice convergence results. Even though our main concern is to retrieve the matrixG, let us notice we can also obtain an estimation of the baseline intensities\u2019 from Eq. (3), which leads to \u00b5\u0302 = R\u0302 \u22121 \u039b\u0302. An efficient implementation of this algorithm with TensorFlow, see Abadi et al. (2016), is available on GitHub: https://github.com/achab/nphc."}, {"heading": "2.5 Complexity of the algorithm", "text": "Compared with existing state-of-the-art methods to estimate the kernel functions, e.g., the ordinary differential equations-based (ODE) algorithm in Zhou et al. (2013a), the Granger Causality-based algorithm in Xu et al. (2016), the ADM4 algorithm in Zhou et al. (2013b), and the Wiener-Hopf-based algorithm in Bacry and Muzy (2016), our method has a very competitive complexity. This can be understood by the fact that those methods estimate the kernel functions, while in NPHC we only estimate their integrals. The ODE-based algorithm is an EM algorithm that parametrizes the kernel function with M basis functions, each being discretized to L points. The basis functions are updated after solving M Euler-Lagrange equations. If n denotes the maximum number of events per component (i.e. n = max1\u2264i\u2264d |Zi|) then the complexity of one iteration of the algorithm isO(Mn3d2 +ML(nd+n2)). The Granger Causality-based algorithm is similar to the previous one, without the update of the basis functions, that are Gaussian kernels. The complexity per iteration is O(Mn3d2). The algorithm ADM4 is similar to the two algorithms above, as EM algorithm as well, with only one exponential kernel as basis function. The complexity per iteration is then O(n3d2). The Wiener-Hopf-based algorithm is not iterative, on the contrary to the previous ones. It first computes the empirical conditional laws on many points, and then invert the Wiener-Hopf system, leading to a O(nd2L+ d4L3) computation. Similarly, our method first computes the integrated cumulants, then minimize the objective function with Niter iterations, and invert the resulting matrix R\u0302 to obtain G\u0302. In the end, the complexity of the NPHC method is O(nd2 +Niterd3). According to this analysis, summarized in Table 1 below, one can see that in the regime n d, the NPHC method outperforms all the other ones."}, {"heading": "2.6 Theoretical guarantee: consistency", "text": "The NPHC method can be phrased using the framework of the Generalized Method of Moments (GMM). GMM is a generic method for estimating parameters in statistical models. In order to apply GMM,\nwe have to find a vector-valued function g(X, \u03b8) of the data, where X is distributed with respect to a distribution P\u03b80 , which satisfies the moment condition: E[g(X, \u03b8)] = 0 if and only if \u03b8 = \u03b80, where \u03b80 is the \u201cground truth\u201d value of the parameter. Based on i.i.d. observed copies x1, . . . , xn of X , the GMM method minimizes the norm of the empirical mean over n samples, \u2016 1n \u2211n i=1 g(xi, \u03b8)\u2016, as a function of \u03b8, to obtain an estimate of \u03b80. In the theoretical analysis of NPHC, we use ideas from the consistency proof of the GMM, but the proof actually relies on very different arguments. Indeed, the integrated cumulants estimators used in NPHC are not unbiased, as the theory of GMM requires, but asymptotically unbiased. Moreover, the setting considered here, where data consists of a single realization {N t} of a Hawkes process strongly departs from the standard i.i.d setting. Our approach is therefore based on the GMM idea but the proof is actually not using the theory of GMM.\nIn the following, we use the subscript T to refer to quantities that only depend on the process (Nt) in the interval [0, T ] (e.g., the truncation term HT , the estimated integrated covariance C\u0302T or the estimated kernel norm matrix G\u0302T ). In the next equation, stands for the Hadamard product and 2 stands for the entrywise square of a matrix. We denoteG0 = Id \u2212R\u221210 the true value ofG, and the R2d\u00d7d valued vector functions\ng0(R) =\n[ C \u2212RLR> Kc \u2212R 2C> \u2212 2[R (C \u2212RL)]R> ]\ng\u0302T (R) =\n[ C\u0302T \u2212RL\u0302TR>\nK\u0302cT \u2212R 2C\u0302 > T \u2212 2[R (C\u0302T \u2212RL\u0302T )]R> .\n]\nUsing these notations, L\u0303T (R) can be seen as the weighted squared Frobenius norm of g\u0302T (R). Moreover, when T \u2192 +\u221e, one has g\u0302T (R) P\u2192 g0(R) under the conditions of the following theorem, where P\u2192 stands for convergence in probability.\nTheorem 2.1 (Consistency of NPHC). Suppose that (Nt) is observed on R+ and assume that\n1. g0(R) = 0 if and only ifR = R0;\n2. R \u2208 \u0398, where \u0398 is a compact set;\n3. the spectral radius of the kernel norm matrix satisfies \u2016G0\u2016 < 1;\n4. HT \u2192\u221e and H2T /T \u2192 0.\nThen\nG\u0302T = Id \u2212 (\narg min R\u2208\u0398 L\u0303T (R)\n)\u22121 P\u2192 G0.\nThe proof of the Theorem is given in Section 4.5 below. Assumption 3 is mandatory for stability of the Hawkes process, and Assumptions 3 and 4 are sufficient to prove that the estimators of the integrated cumulants defined in Equations (11), (12) and (13) are asymptotically consistent. Assumption 2 is a very mild standard technical assumption allowing to prove consistency for estimators based on moments. Assumption 1 is a standard asymptotic moment condition, that allows to identity parameters from the integrated cumulants."}, {"heading": "3 Numerical Experiments", "text": "In this Section, we provide a comparison of NPHC with the state-of-the art, on simulated datasets with different kernel shapes, the MemeTracker dataset (social networks) and the order book dynamics dataset (finance).\nSimulated datasets. We simulated several datasets with Ogata\u2019s Thinning algorithm Ogata (1981) using the open-source library tick1, each corresponding to a shape of kernel: rectangular, exponential or power law kernel, see Figure 1 below.\nThe integral of each kernel on its support equals \u03b1, 1/\u03b2 can be regarded as a characteristic time-scale and \u03b3 is the scaling exponent for the power law distribution and a delay parameter for the rectangular one. We consider a non-symmetric block-matrix G to show that our method can effectively uncover causality between the nodes, see Figure 3. The matrix G has constant entries \u03b1 on the three blocks - \u03b1 = gij = 1/6 for dimension 10 and \u03b1 = gij = 1/10 for dimension 100 -, and zero outside. The two other parameters\u2019 values are the same for dimensions 10 and 100. The parameter \u03b3 is set to 1/2 on the three blocks as well, but we set three very different \u03b20, \u03b21 and \u03b22 from one block to the other, with ratio \u03b2i+1/\u03b2i = 10 and \u03b20 = 0.1. The number of events is roughly equal to 105 on average over the nodes. We ran the algorithm on three simulated datasets: a 10-dimensional process with rectangular kernels named Rect10, a 10-dimensional process with power law kernels named PLaw10 and a 100-dimensional process with exponential kernels named Exp100.\nMemeTracker dataset. We use events of the most active sites from the MemeTracker dataset2. This dataset contains the publication times of articles in many websites/blogs from August 2008 to April 2009, and hyperlinks between posts. We extract the top 100 media sites with the largest number of documents, with about 7 million of events. We use the links to trace the flow of information and establish an estimated ground truth for the matrixG. Indeed, when an hyperlink j appears in a post in website i, the link j can be regarded as a direct ancestor of the event. Then, Eq. (2) shows gij can be estimated by N i\u2190jT /N j T = #{links j \u2192 i}/N j T .\n1https://github.com/X-DataInitiative/tick 2https://www.memetracker.org/data.html\nOrder book dynamics. We apply our method to financial data, in order to understand the self and crossinfluencing dynamics of all event types in an order book. An order book is a list of buy and sell orders for a specific financial instrument, the list being updated in real-time throughout the day. This model has first been introduced in Bacry et al. (2016), and models the order book via the following 8-dimensional point process: Nt = (P (a) t , P (b) t , T (a) t , T (b) t , L (a) t , L (b) t , C (a) t , C (b) t ), where P\n(a) (resp. P (b)) counts the number of upward (resp. downward) price moves, T (a) (resp. T (b)) counts the number of market orders at the ask3 (resp. at the bid) that do not move the price, L(a) (resp. L(b)) counts the number of limit orders at the ask4 (resp. at the bid) that do not move the price, and C(a) (resp. C(b)) counts the number of cancel orders at the ask5 (resp. at the bid) that do not move the price. The financial data has been provided by QuantHouse EUROPE/ASIA, and consists of DAX future contracts between 01/01/2014 and 03/01/2014.\nBaselines. We compare NPHC to state-of-the art baselines: the ODE-based algorithm (ODE) by Zhou et al. (2013a), the Granger Causality-based algorithm (GC) by Xu et al. (2016), the ADM4 algorithm (ADM4) by Zhou et al. (2013b), and the Wiener-Hopf-based algorithm (WH) by Bacry and Muzy (2016).\nMetrics. We evaluate the performance of the proposed methods using the computing time, the Relative Error\nRelErr(A,B) = 1\nd2 \u2211 i,j |aij \u2212 bij | |aij | 1{aij 6=0} + |bij |1{aij=0}\nand the Mean Kendall Rank Correlation\nMRankCorr(A,B) = 1\nd d\u2211 i=1 RankCorr([ai\u2022], [bi\u2022]),\nwhere RankCorr(x, y) = 2d(d\u22121)(Nconcordant(x, y) \u2212Ndiscordant(x, y)) with Nconcordant(x, y) the number of pairs (i, j) satisfying xi > xj and yi > yj or xi < xj and yi < yj and Ndiscordant(x, y) the number of pairs (i, j) for which the same condition is not satisfied.\nNote that RankCorr score is a value between \u22121 and 1, representing rank matching, but can take smaller values (in absolute value) if the entries of the vectors are not distinct.\n3i.e. buy orders that are executed and removed from the list 4i.e. buy orders added to the list 5i.e. the number of times a limit order at the ask is canceled: in our dataset, almost 95% of limit orders are canceled before\nexecution.\nDiscussion. We perform the ADM4 estimation, with exponential kernel, by giving the exact value \u03b2 = \u03b20 of one block. Let us stress that this helps a lot this baseline, in comparison to NPHC where nothing is specified on the shape of the kernel functions. We used M = 10 basis functions for both ODE and GC algorithms, and L = 50 quadrature points for WH. We did not run WH on the 100-dimensional datasets, for computing time reasons, because its complexity scales with d4. We ran multi-processed versions of the baseline methods on 56 cores, to decrease the computing time.\nOur method consistently performs better than all baselines, on the three synthetic datasets, on MemeTracker and on the financial dataset, both in terms of Kendall rank correlation and estimation error. Moreover, we observe that our algorithm is roughly 50 times faster than all the considered baselines.\nOn Rect10, PLaw10 and Exp100 our method gives very impressive results, despite the fact that it does not uses any prior shape on the kernel functions, while for instance the ADM4 baseline do. On Figure 3, we observe that the matrix G\u0302 estimated with ADM4 recovers well the block for which \u03b2 = \u03b20, i.e. the value we gave to the method, but does not perform well on the two other blocks, while the matrix G\u0302 estimated with NPHC approximately reaches the true value for each of the three blocks. On these simulated datasets, NPHC obtains a comparable or slightly better Kendall rank correlation, but improves a lot the relative error.\nOn MemeTracker, the baseline methods obtain a high relative error between 9% and 19% while our method achieves a relative error of 7% which is a strong improvement. Moreover, NPHC reaches a much better Kendall rank correlation, which proves that it leads to a much better recovery of the relative order of estimated influences than all the baselines. Indeed, it has been shown in Zhou et al. (2013a) that kernels of MemeTracker data are not exponential, nor power law. This partly explains why our approach behaves\nbetter. On the financial data, the estimated kernel norm matrix obtained via NPHC, see Figure 3, gave some interpretable results (see also Bacry et al. (2016)):\n1. Any 2\u00d7 2 sub-matrix with same kind of inputs (i.e. Prices changes, Trades, Limits or Cancels) is symmetric. This shows empirically that ask and bid have symmetric roles.\n2. The prices are mostly cross-excited, which means that a price increase is very likely to be followed by a price decrease, and conversely. This is consistent with the wavy prices we observe on financial markets.\n3. The market, limit and cancel orders are strongly self-excited. This can be explained by the persistence of order flows, and by the splitting of meta-orders into sequences of smaller orders. Moreover, we observe that orders impact the price without changing it. For example, the increase of cancel orders at the bid causes downward price moves."}, {"heading": "4 Technical details", "text": "We show in this section how to obtain the equations stated above, the estimators of the integrated cumulants and the scaling coefficient \u03ba that appears in the objective function. We then prove the theorem of the paper.\n4.1 Proof of Equation (8)\nWe denote \u03bd(z) the matrix\n\u03bdij(z) = Lz ( t\u2192 E(dN iudN j u+t)\ndudt \u2212 \u039bi\u039bj\n) ,\nwhere Lz(f) is the Laplace transform of f , and \u03c8t = \u2211 n\u22651 \u03c6 (?n) t , where \u03c6 (?n) t refers to the n\nth autoconvolution of \u03c6t. Then we use the characterization of second-order statistics, first formulated in Hawkes (1971) and fully generalized in Bacry and Muzy (2016),\n\u03bd(z) = (Id + L\u2212z(\u03a8))L(Id + Lz(\u03a8))>,\nwhere Lij = \u039bi\u03b4ij with \u03b4ij the Kronecker symbol. Since Id +Lz(\u03a8) = (Id \u2212Lz(\u03a6))\u22121, taking z = 0 in the previous equation gives\n\u03bd(0) = (Id \u2212G)\u22121L(Id \u2212G>)\u22121, C = RLR>,\nwhich gives us the result since the entry (i, j) of the last equation gives Cij = \u2211\nm \u039b mRimRjm.\n4.2 Proof of Equation (9)\nWe start from Jovanovic\u0301 et al. (2015), cf. Eqs. (48) to (51), and group some terms:\nKijk = \u2211 m \u039bmRimRjmRkm\n+ \u2211 m RimRjm \u2211 n \u039bnRknL0(\u03c8mn)\n+ \u2211 m RimRkm \u2211 n \u039bnRjnL0(\u03c8mn)\n+ \u2211 m RjmRkm \u2211 n \u039bnRinL0(\u03c8mn).\nUsing the relations L0(\u03c8mn) = Rmn \u2212 \u03b4mn and Cij = \u2211 m \u039b mRimRjm, proves Equation (9)."}, {"heading": "4.3 Integrated cumulants estimators", "text": "For H > 0 let us denote \u2206HN it = N i t+H \u2212N it\u2212H . Let us first remark that, if one restricts the integration domain to (\u2212H,H) in Eqs. (4) and (5), one gets by permuting integrals and expectations:\n\u039bidt = E(dN it ) Cijdt = E ( dN it (\u2206HN j t \u2212 2H\u039bj) ) Kijkdt = E ( dN it (\u2206HN j t \u2212 2H\u039bj)(\u2206HNkt \u2212 2H\u039bk)\n) \u2212 dt\u039biE ( (\u2206HN j t \u2212 2H\u039bj)(\u2206HNkt \u2212 2H\u039bk) ) .\nThe estimators (11) and (12) are then naturally obtained by replacing the expectations by their empirical counterparts, notably\nE(dN itf(t)) dt \u2192 1 T \u2211 \u03c4\u2208Zi f(\u03c4).\nFor the estimator (13), we shall also notice that\nE((\u2206HN jt \u2212 2H\u039bj)(\u2206HNkt \u2212 2H\u039bk))\n= \u222b \u222b 1[\u2212H,H](t)1[\u2212H,H](t \u2032)Cjkt\u2212t\u2032dtdt \u2032\n= \u222b (2H \u2212 |t|)+Cjkt dt.\nWe estimate the last integral with the remark above."}, {"heading": "4.4 Choice of the scaling coefficient \u03ba", "text": "Following the theory of GMM, we denote m(X, \u03b8) a function of the data, where X is distributed with respect to a distribution P\u03b80 , which satisfies the moment conditions g(\u03b8) = E[m(X, \u03b8)] = 0 if and only if \u03b8 = \u03b80, the parameter \u03b80 being the ground truth. For x1, . . . , xN observed copies of X , we denote g\u0302i(\u03b8) = m(xi, \u03b8), the usual choice of weighting matrix is W\u0302N (\u03b8) = 1N \u2211N i=1 g\u0302i(\u03b8)g\u0302i(\u03b8)\n>, and the objective to minimize is then(\n1\nN N\u2211 i=1 g\u0302i(\u03b8)\n)( W\u0302N (\u03b81) )\u22121( 1 N N\u2211 i=1 g\u0302i(\u03b8) ) , (15)\nwhere \u03b81 is a constant vector. Instead of computing the inverse weighting matrix, we rather use its projection on {\u03b1Id : \u03b1 \u2208 R}. It can be shown that the projection choses \u03b1 as the mean eigenvalue of W\u0302N (\u03b81). We can easily compute the sum of its eigenvalues:\nTr(W\u0302N (\u03b81)) = 1\nN N\u2211 i=1 Tr(g\u0302i(\u03b81)g\u0302i(\u03b81)>) = 1 N N\u2211 i=1 Tr(g\u0302i(\u03b81)>g\u0302i(\u03b81)) = 1 N N\u2211 i=1 ||g\u0302i(\u03b81)||22.\nIn our case, g\u0302(R) = [ vec[K\u0302c \u2212Kc(R)], vec[C\u0302 \u2212C(R)] ]> \u2208 R2d2 . Considering a block-wise\nweighting matrix, one block for K\u0302c\u2212Kc(R) and the other for C\u0302\u2212C(R), the sum of the eigenvalues of the first block becomes \u2016K\u0302c \u2212Kc(R)\u201622, and \u2016C\u0302 \u2212C(R)\u201622 for the second. We compute the previous terms withR1 = 0. All together, the objective function to minimize is\n1\n\u2016K\u0302c\u201622 \u2016Kc(R)\u2212 K\u0302c\u201622 +\n1\n\u2016C\u0302\u201622 \u2016C(R)\u2212 C\u0302\u201622. (16)\nDividing this function by ( 1/\u2016K\u0302c\u201622 + 1/\u2016C\u0302\u201622 )\u22121\n, and setting \u03ba = \u2016K\u0302c\u201622/(\u2016K\u0302c\u201622 + \u2016C\u0302\u201622), we obtaind the loss function given in Equation (10)."}, {"heading": "4.5 Proof of the Theorem", "text": "The main difference with the usual Generalized Method of Moments, see Hansen (1982), relies in the relaxation of the moment conditions, since we have E[g\u0302T (\u03b80)] = mT 6= 0. We adapt the proof of consistency given in Newey and McFadden (1994).\nWe can relate the integral of the Hawkes process\u2019s kernels to the integrals of the cumulant densities, from Jovanovic\u0301 et al. (2015). Our cumulant matching method would fall into the usual GMM framework if we could estimate - without bias - the integral of the covariance on R, and the integral of the skewness on R2. Unfortunately, we can\u2019t do that easily. We can however estimate without bias \u222b fTt C ij t dt\nand \u222b fTt K ijk t dt with f\nT a compact supported function on [\u2212HT , HT ] that weakly converges to 1, with HT \u2212\u2192 \u221e. In most cases we will take fTt = 1[\u2212HT ,HT ](t). Denoting C\u0302\nij,(T ) the estimator of\u222b fTt C ij t dt, the term |E[C\u0302ij,(T )]\u2212Cij | = | \u222b fTt C ij t dt\u2212Cij | can be considered a proxy to the distance to the classical GMM. This distance has to go to zero to make the rest of GMM\u2019s proof work: the estimator C\u0302ij,(T ) is then asymptotically unbiased towards Cij when T goes to infinity."}, {"heading": "4.5.1 Notations", "text": "We observe the multivariate point process (N t) on R+, with Zi the events of the ith component. We will often write covariance / skewness instead of integrated covariance / skewness. In the rest of the document, we use the following notations. Hawkes kernels\u2019 integrals Gtrue = \u222b \u03a6tdt = ( \u222b \u03c6ijt dt)ij = Id \u2212 (Rtrue)\u22121\nTheoretical mean matrix L = diag(\u039b1, . . . ,\u039bd)\nTheoretical covariance C = RtrueL(Rtrue)>\nTheoretical skewness Kc = (Kiij)ij = (Rtrue) 2 C> + 2[Rtrue (C \u2212RtrueL)](Rtrue)> Filtering function fT \u2265 0 supp(fT ) \u2282 [\u2212HT , HT ] F T = \u222b fTs ds f\u0303 T t = f T \u2212t\nEvents sets Zi,T,1 = Zi \u2229 [HT , T +HT ] Zj,T,2 = Zj \u2229 [0, T + 2HT ]\nEstimators of the mean \u039b\u0302i = N iT+HT \u2212N iHT T \u039b\u0303 j = NjT+2HT T+2HT\nEstimator of the covariance C\u0302ij,(T ) = 1T \u2211 \u03c4\u2208Zi,T,1 (\u2211 \u03c4 \u2032\u2208Zj,T,2 f\u03c4 \u2032\u2212\u03c4 \u2212 \u039b\u0303jF T )\nEstimator of the skewness6\nK\u0302ijk,(T ) = 1\nT \u2211 \u03c4\u2208Zi,T,1  \u2211 \u03c4 \u2032\u2208Zj,T,2 f\u03c4 \u2032\u2212\u03c4 \u2212 \u039b\u0303jF T  \u2211 \u03c4 \u2032\u2032\u2208Zk,T,2 f\u03c4 \u2032\u2212\u03c4 \u2212 \u039b\u0303kF T  \u2212 \u039b\u0302 i\nT + 2HT \u2211 \u03c4 \u2032\u2208Zj,T,2  \u2211 \u03c4 \u2032\u2032\u2208Zk,T,2 (fT ? f\u0303T )\u03c4 \u2032\u2212\u03c4 \u2032\u2032 \u2212 \u039b\u0303k(F T )2 \n6When fTt = 1[\u2212HT ,HT ](t), we remind that (f T ? f\u0303T )t = (2HT \u2212 |t|)+. This leads to the estimator we showed in the\narticle.\nGMM related notations\n\u03b8 = R and \u03b80 = Rtrue g0(\u03b8) = vec [ C \u2212RLR>\nKc \u2212R 2C> \u2212 2[R (C \u2212RL)]R>\n] \u2208 R2d2\ng\u0302T (\u03b8) = vec\n[ C\u0302 (T ) \u2212RL\u0302R>\nK\u0302c (T ) \u2212R 2(C\u0302 (T ) )> \u2212 2[R (C\u0302 (T ) \u2212RL\u0302)]R>\n] \u2208 R2d2\nQ0(\u03b8) = g0(\u03b8) >Wg0(\u03b8) Q\u0302T (\u03b8) = g\u0302T (\u03b8) >W\u0302T g\u0302T (\u03b8)"}, {"heading": "4.5.2 Consistency", "text": "First, let\u2019s remind a useful theorem for consistency in GMM from Newey and McFadden (1994).\nTheorem 4.1. If there is a function Q0(\u03b8) such that (i) Q0(\u03b8) is uniquely maximized at \u03b80; (ii) \u0398 is compact; (iii) Q0(\u03b8) is continuous; (iv) Q\u0302T (\u03b8) converges uniformly in probability to Q0(\u03b8), then \u03b8\u0302T = arg max Q\u0302T (\u03b8) P\u2212\u2192 \u03b80.\nWe can now prove the consistency of our estimator.\nTheorem 4.2. Suppose that (Nt) is observed on R+, W\u0302T P\u2212\u2192W , and\n1. W is positive semi-definite and Wg0(\u03b8) = 0 if and only if \u03b8 = \u03b80,\n2. \u03b8 \u2208 \u0398, which is compact,\n3. the spectral radius of the kernel norm matrix satisfies ||\u03a6||\u2217 < 1, 4. \u2200i, j, k \u2208 [d], \u222b fTu C ij u du\u2192 \u222b Ciju du and \u222b fTu f T v K ijk u,vdudv \u2192 \u222b Kijku,vdudv,\n5. (F T )2/T P\u2212\u2192 0 and ||f ||\u221e = O(1).\nThen\n\u03b8\u0302T P\u2212\u2192 \u03b80.\nRemark 1. In practice, we use a constant sequence of weighting matrices: W\u0302T = Id.\nProof. Proceed by verifying the hypotheses of Theorem 2.1 from Newey and McFadden (1994). Condition 2.1(i) follows by (i) and by Q0(\u03b8) = [W 1/2g0(\u03b8)]>[W 1/2g0(\u03b8)] > 0 = Q0(\u03b80). Indeed, there exists a neighborhood N of \u03b80 such that \u03b8 \u2208 N\\{\u03b80} and g0(\u03b8) 6= 0 since g0(\u03b8) is a polynom. Condition 2.1(ii) follows by (ii). Condition 2.1(iii) is satisfied since Q0(\u03b8) is a polynom. Condition 2.1(iv) is harder to prove. First, since g\u0302T (\u03b8) is a polynom of \u03b8, we prove easily that E[sup\u03b8\u2208\u0398 |g\u0302T (\u03b8)|] <\u221e. Then, by \u0398 compact, g0(\u03b8) is bounded on \u0398, and by the triangle and Cauchy-Schwarz inequalities,\u2223\u2223Q\u0302T (\u03b8)\u2212Q0(\u03b8)\u2223\u2223\n\u2264 \u2223\u2223(g\u0302T (\u03b8)\u2212 g0(\u03b8))>W\u0302T (g\u0302T (\u03b8)\u2212 g0(\u03b8))\u2223\u2223\n+ \u2223\u2223g0(\u03b8)>(W\u0302T + W\u0302>T )(g\u0302T (\u03b8)\u2212 g0(\u03b8))\u2223\u2223+ \u2223\u2223g0(\u03b8)>(W\u0302T \u2212W )g0(\u03b8)\u2223\u2223 \u2264 \u2016g\u0302T (\u03b8)\u2212 g0(\u03b8)\u20162\u2016W\u0302T \u2016+ 2\u2016g0(\u03b8)\u2016\u2016g\u0302T (\u03b8)\u2212 g0(\u03b8)\u2016\u2016W\u0302T \u2016+ \u2016g0(\u03b8)\u20162\u2016W\u0302T \u2212W\u2016.\nTo prove sup\u03b8\u2208\u0398 \u2223\u2223Q\u0302T (\u03b8)\u2212Q0(\u03b8)\u2223\u2223 P\u2212\u2192 0, we should now prove that sup\u03b8\u2208\u0398\u2016g\u0302T (\u03b8)\u2212 g0(\u03b8)\u2016 P\u2212\u2192 0. By \u0398 compact, it is sufficient to prove that \u2016L\u0302\u2212L\u2016 P\u2212\u2192 0, \u2016C\u0302 (T ) \u2212C\u2016 P\u2212\u2192 0, and \u2016K\u0302c (T ) \u2212Kc\u2016 P\u2212\u2192 0.\nProof that \u2016L\u0302\u2212L\u2016 P\u2212\u2192 0\nThe estimator of L is unbiased so let\u2019s focus on the variance of L\u0302.\nE[(\u039b\u0302i \u2212 \u039bi)2] = E\n[( 1\nT \u222b T+HT HT (dN it \u2212 \u039bidt) )2]\n= 1\nT 2 \u222b T+HT HT \u222b T+HT HT E[(dN it \u2212 \u039bidt)(dN it\u2032 \u2212 \u039bidt\u2032)]\n= 1\nT 2 \u222b T+HT HT \u222b T+HT HT Ciit\u2032\u2212tdtdt \u2032\n\u2264 1 T 2 \u222b T+HT HT Ciidt = Cii T \u2212\u2192 0\nBy Markov inequality, we have just proved that \u2016L\u0302\u2212L\u2016 P\u2212\u2192 0.\nProof that \u2016C\u0302 (T ) \u2212C\u2016 P\u2212\u2192 0\nFirst, let\u2019s remind that E(C\u0302 (T ) ) 6= C. Indeed,\nE ( C\u0302ij,(T ) ) = E ( 1\nT \u222b T+HT HT dN it \u222b T+2HT 0 dN jt\u2032ft\u2032\u2212t \u2212 \u039b\u0302 i\u039b\u0303jF T ) = E ( 1\nT \u222b T+HT HT dN it \u222b T+2HT\u2212t \u2212t dN jt+sfs \u2212 \u039bi\u039bjF T ) + ij,T,HTF T\n= 1\nT \u222b T+HT HT \u222b HT \u2212HT fsE ( dN itdN j t+s \u2212 \u039bi\u039bjds ) + ij,T,HTF T\n= \u222b fsC ij s ds+ ij,T,HTF T\nNow,\nij,T,HT = E ( \u039bi\u039bj \u2212 \u039b\u0302i\u039b\u0303j )\n= \u2212 1 T 2 \u222b T+HT HT \u222b T+2HT 0 E ( dN itdN j t\u2032 \u2212 \u039b i\u039bjdtdt\u2032 )\n= \u2212 1 T 2 \u222b T+HT HT \u222b T+2HT 0 Cijt\u2212t\u2032dtdt \u2032\n= \u2212 1 T\n\u222b ( 1 + ( HT \u2212 |t|\nT\n)\u2212)+ Cijt dt\nSince f satisfies F T = o(T ), we have E(C\u0302 (T ) ) \u2212\u2192 C. It remains now to prove that \u2016C\u0302 (T ) \u2212 E(C\u0302 (T )\n)\u2016 P\u2212\u2192 0. Let\u2019s now focus on the variance of C\u0302ij,(T ) : V(C\u0302ij,(T )) = E ( (C\u0302ij,(T ))2 ) \u2212 E(C\u0302ij,(T ))2.\nNow, E ( (C\u0302ij,(T ))2 )\n= E  1 T 2 \u2211 (\u03c4,\u03b7,\u03c4 \u2032,\u03b7\u2032)\u2208(Zi,T,1)2\u00d7(Zj,T,2)2 (f\u03c4 \u2032\u2212\u03c4 \u2212 F T /(T + 2HT ))(f\u03b7\u2032\u2212\u03b7 \u2212 F T /(T + 2HT ))  = E ( 1\nT 2 \u222b t,s\u2208[HT ,T+HT ] \u222b t\u2032,s\u2032 dN itdN j t\u2032dN i sdN j s\u2032(ft\u2032\u2212t \u2212 F T /(T + 2HT ))(fs\u2032\u2212s \u2212 F T /(T + 2HT )) )\n= 1\nT 2 \u222b t,s\u2208[HT ,T+HT ] \u222b t\u2032,s\u2032\u2208[0,T+2HT ] E ( dN itdN j t\u2032dN i sdN j s\u2032 ) \u00b7 (ft\u2032\u2212t \u2212 F T /(T + 2HT ))(fs\u2032\u2212s \u2212 F T /(T + 2HT ))\nAnd,\nE(C\u0302ij,(T ))2\n= 1\nT 2 \u222b t,s\u2208[HT ,T+HT ] \u222b t\u2032,s\u2032\u2208[0,T+2HT ] E ( dN itdN j t\u2032 ) E ( dN isdN j s\u2032 ) \u00b7 (ft\u2032\u2212t \u2212 F T /(T + 2HT ))(fs\u2032\u2212s \u2212 F T /(T + 2HT ))\nThen, the variance involves the integration towards the difference of moments \u00b5r,s,t,u \u2212 \u00b5r,s\u00b5t,u. Let\u2019s write it as a sum of cumulants, since cumulants density are integrable.\n\u00b5r,s,t,u \u2212 \u00b5r,s\u00b5t,u = \u03bar,s,t,u + \u03bar,s,t\u03bau[4] + \u03bar,s\u03bat,u[3] + \u03bar,s\u03bat\u03bau[6] + \u03bar\u03bas\u03bat\u03bau \u2212 (\u03bar,s + \u03bar\u03bas)(\u03bat,u + \u03bat\u03bau) = \u03bar,s,t,u\n+ \u03bar,s,t\u03bau + \u03bau,r,s\u03bat + \u03bat,u,r\u03bas + \u03bas,t,u\u03bar\n+ \u03bar,t\u03bas,u + \u03bar,u\u03bas,t\n+ \u03bar,t\u03bas\u03bau + \u03bar,u\u03bas\u03bat + \u03bas,t\u03bar\u03bau + \u03bas,t\u03bar\u03bau\nIn the rest of the proof, we denote at = 1t\u2208[HT ,T+HT ], bt = 1t\u2208[0,T+2HT ], ct = 1t\u2208[\u2212HT ,HT ], gt = ft \u2212 1T+2HT F T Before starting the integration of each term, let\u2019s remark that:\n1. \u03a8t = \u2211 n\u22651 \u03a6 (?n) t \u2265 0 since \u03a6t \u2265 0.\n2. The regular parts ofCiju ,K ijk u,v (skewness density) andM ijkl u,v,w (fourth cumulant density) are positive\nas polynoms of integrals of \u03c8ab\u00b7 with positive coefficients. The integrals of the singular parts are positive as well.\n3. (a) \u222b atbt\u2032ft\u2032\u2212tdtdt \u2032 = TF T\n(b) \u222b atbt\u2032gt\u2032\u2212tdtdt \u2032 = 0\n(c) \u222b atbt\u2032 |gt\u2032\u2212t|dtdt\u2032 \u2264 2TF T\n4. \u2200t \u2208 R, at(b ? g\u0303)t = 0, where g\u0303s = g\u2212s.\nFourth cumulant We want here to compute \u222b \u03bai,j,i,jt,t\u2032,s,s\u2032atbt\u2032asbs\u2032gt\u2032\u2212tgs\u2032\u2212sdtdt\n\u2032dsds\u2032. We remark that |gt\u2032\u2212tgs\u2032\u2212s| \u2264 (||f ||\u221e(1 + 2HT /T ))2 \u2264 4||f ||2\u221e.\u2223\u2223\u2223 1 T 2 \u222b \u03bai,j,i,jt,t\u2032,s,s\u2032atbt\u2032asbs\u2032gt\u2032\u2212tgs\u2032\u2212sdtdt \u2032dsds\u2032 \u2223\u2223\u2223 \u2264 (2||f ||\u221e T )2 \u222b dtat \u222b dt\u2032bt\u2032 \u222b dsas \u222b ds\u2032bs\u2032M ijij t\u2032\u2212t,s\u2212t,s\u2032\u2212t\n\u2264 (\n2||f ||\u221e T\n)2 \u222b dtat \u222b dt\u2032bt\u2032 \u222b dsas \u222b dwM ijijt\u2032\u2212t,s\u2212t,w\n\u2264 (\n2||f ||\u221e T\n)2 \u222b dtat \u222b M ijiju,v,wdudvdw\n\u2264 4||f || 2 \u221e\nT M ijij \u2212\u2192 T\u2192\u221e 0\nThird \u00d7 First We have four terms, but only two different forms since the roles of (s, s\u2032) and (t, t\u2032) are symmetric. First form \u222b\n\u03bai,j,it,t\u2032,s\u039b jGtdt =\n\u039bj\nT 2\n\u222b \u03bai,j,it,t\u2032,satbt\u2032asbs\u2032gt\u2032\u2212tgs\u2032\u2212sdtdt \u2032dsds\u2032\n= \u039bj\nT 2\n\u222b \u03bai,j,it,t\u2032,satbt\u2032as(b ? g\u0303)sgt\u2032\u2212tdtdt \u2032ds\n= 0 since as(b ? g\u0303)s = 0\nSecond form\u2223\u2223\u2223 \u222b \u03bai,j,jt,t\u2032,s\u2032\u039biGtdt\u2223\u2223\u2223 = \u2223\u2223\u2223\u039biT 2 \u222b \u03bai,j,jt,t\u2032,s\u2032atbt\u2032asbs\u2032gt\u2032\u2212tgs\u2032\u2212sdtdt \u2032dsds\u2032 \u2223\u2223\u2223\n= \u2223\u2223\u2223\u039bi T 2 \u222b \u03bai,j,jt,t\u2032,s\u2032atbt\u2032gt\u2032\u2212tbs\u2032(a ? g)s\u2032dtdt \u2032ds\u2032 \u2223\u2223\u2223 \u2264 \u039b i\nT 2 2||f ||\u221e\n\u222b ds\u2032bs\u2032(a ? |g|)s\u2032 \u222b dtat \u222b dt\u2032bt\u2032K ijj t\u2032\u2212s\u2032,t\u2212s\u2032\n\u2264 4||f ||\u221eKijj\u039bi F T\nT \u2212\u2192 T\u2192\u221e 0\nSecond \u00d7 Second First form \u2223\u2223\u2223 \u222b \u03bai,it,s\u03baj,jt\u2032,s\u2032Gtdt\u2223\u2223\u2223 \u2264 2||f ||\u221eT 2 \u222b Ciit\u2212sC jj t\u2032\u2212s\u2032atbt\u2032 |gt\u2032\u2212t|asbs\u2032dtdt \u2032dsds\u2032\n\u2264 2||f ||\u221e T 2\nCiiCjj \u222b atbt\u2032 |gt\u2032\u2212t|dtdt\u2032\n\u2264 4||f ||\u221eCiiCjj F T\nT \u2212\u2192 T\u2192\u221e 0\nSecond form \u2223\u2223\u2223 \u222b \u03bai,jt,s\u2032\u03bai,jt\u2032,sGtdt\u2223\u2223\u2223 \u2264 4||f ||\u221e(Cij)2F TT \u2212\u2192T\u2192\u221e 0 Second \u00d7 First \u00d7 First First form \u222b\n\u03bai,jt,t\u2032\u039b i\u039bjGtdt =\n\u039bi\u039bj\nT 2\n\u222b \u03bai,jt,t\u2032atbt\u2032gt\u2032\u2212tdtdt \u2032 \u222b asbs\u2032gs\u2032\u2212sdsds \u2032 = 0\nSecond form \u222b \u03bai,it,s\u039b j\u039bjGtdt = ( \u039bj\nT\n)2 \u222b \u03bai,it,satbt\u2032gt\u2032\u2212tas(b ? g\u0303)sdtdt \u2032ds = 0\nWe have just proved that V(C\u0302 (T ) ) P\u2212\u2192 0. By Markov inequality, it ensures us that \u2016C\u0302 (T ) \u2212E(C\u0302 (T ) )\u2016 P\u2212\u2192 0, and finally that \u2016C\u0302 (T ) \u2212C\u2016 P\u2212\u2192 0.\nProof that \u2016K\u0302c (T ) \u2212Kc\u2016 P\u2212\u2192 0\nThe scheme of the proof is similar to the previous one. The upper bounds of the integrals involve the same kind of terms, plus the new term (F T )2/T that goes to zero thanks to the assumption 5 of the theorem."}, {"heading": "5 Conclusion", "text": "In this paper, we introduce a simple nonparametric method (the NPHC algorithm) that leads to a fast and robust estimation of the matrixG of the kernel integrals of a Multivariate Hawkes process that encodes Granger causality between nodes. This method relies on the matching of the integrated order 2 and order 3 empirical cumulants, which represent the simplest set of global observables containing sufficient information to recover the matrixG. Since this matrix fully accounts for the self- and cross- influences of the process nodes (that can represent agents or users in applications), our approach can naturally be used to quantify the degree of endogeneity of a system and to uncover the causality structure of a network.\nBy performing numerical experiments involving very different kernel shapes, we show that the baselines, involving either parametric or non-parametric approaches are very sensible to model misspecification, do not lead to accurate estimation, and are numerically expensive, while NPHC provides fast, robust and reliable results. This is confirmed on the MemeTracker database, where we show that NPHC outperforms classical approaches based on EM algorithms or the Wiener-Hopf equations. Finally, the NPHC algorithm provided very satisfying results on financial data, that are consistent with well-known stylized facts in finance."}, {"heading": "Acknowledgements", "text": "This work benefited from the support of the chair \u201cChanging markets\u201d, CMAP \u00c9cole Polytechnique and \u00c9cole Polytechnique fund raising - Data Science Initiative.\nThe authors want to thank Marcello Rambaldi for fruitful discussions on order book data\u2019s experiments."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Modeling financial contagion using mutually exciting jump processes", "author": ["Y. A\u00eft-Sahalia", "J. Cacho-Diaz", "R. JA Laeven"], "venue": "Technical report, National Bureau of Economic Research,", "citeRegEx": "A\u00eft.Sahalia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "A\u00eft.Sahalia et al\\.", "year": 2010}, {"title": "First- and second-order statistics characterization of hawkes processes and non-parametric estimation", "author": ["E. Bacry", "J.-F. Muzy"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Bacry and Muzy.,? \\Q2016\\E", "shortCiteRegEx": "Bacry and Muzy.", "year": 2016}, {"title": "Muzy. Hawkes processes in finance", "author": ["E. Bacry", "I. Mastromatteo", "J.-F"], "venue": "Market Microstructure and Liquidity,", "citeRegEx": "Bacry et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bacry et al\\.", "year": 2015}, {"title": "Estimation of slowly decreasing hawkes kernels: application to high-frequency order book dynamics", "author": ["E. Bacry", "T. Jaisson", "J.-F. Muzy"], "venue": "Quantitative Finance,", "citeRegEx": "Bacry et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bacry et al\\.", "year": 2016}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G. Ben Arous", "Y. LeCun"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Robust dynamic classes revealed by measuring the response function of a social system", "author": ["R. Crane", "D. Sornette"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Crane and Sornette.,? \\Q2008\\E", "shortCiteRegEx": "Crane and Sornette.", "year": 2008}, {"title": "Zaatour. Hawkes process: Fast calibration, application to trade clustering, and diffusive limit", "author": ["R.J. Da Fonseca"], "venue": "Journal of Futures Markets,", "citeRegEx": "Fonseca,? \\Q2014\\E", "shortCiteRegEx": "Fonseca", "year": 2014}, {"title": "An Introduction to the Theory of Point Processes Volume I: Elementary Theory and Methods", "author": ["D.J. Daley", "D. Vere-Jones"], "venue": "Springer Science & Business Media,", "citeRegEx": "Daley and Vere.Jones.,? \\Q2003\\E", "shortCiteRegEx": "Daley and Vere.Jones.", "year": 2003}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Graphical modeling for multivariate hawkes processes with nonparametric link functions", "author": ["M. Eichler", "R. Dahlhaus", "J. Dueck"], "venue": "Journal of Time Series Analysis, pages n/a\u2013n/a,", "citeRegEx": "Eichler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eichler et al\\.", "year": 2016}, {"title": "Coevolve: A joint point process model for information diffusion and network co-evolution", "author": ["M. Farajtabar", "Y. Wang", "M. Rodriguez", "S. Li", "H. Zha", "L. Song"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Farajtabar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Farajtabar et al\\.", "year": 2015}, {"title": "Modeling information propagation with survival theory", "author": ["M. Gomez-Rodriguez", "J. Leskovec", "B. Sch\u00f6lkopf"], "venue": "Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Gomez.Rodriguez et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gomez.Rodriguez et al\\.", "year": 2013}, {"title": "Investigating causal relations by econometric models and cross-spectral methods", "author": ["C.W.J. Granger"], "venue": "URL http://www.jstor", "citeRegEx": "Granger.,? \\Q1969\\E", "shortCiteRegEx": "Granger.", "year": 1969}, {"title": "Generalized Method of Moments", "author": ["A.R. Hall"], "venue": "Oxford university press,", "citeRegEx": "Hall.,? \\Q2005\\E", "shortCiteRegEx": "Hall.", "year": 2005}, {"title": "Large sample properties of generalized method of moments estimators", "author": ["L.P. Hansen"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "Hansen.,? \\Q1982\\E", "shortCiteRegEx": "Hansen.", "year": 1982}, {"title": "Lasso and probabilistic inequalities for multivariate point processes", "author": ["N.R. Hansen", "P. Reynaud-Bouret", "V. Rivoirard"], "venue": null, "citeRegEx": "Hansen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2015}, {"title": "Branching-ratio approximation for the self-exciting Hawkes process", "author": ["S.J. Hardiman", "J.-P. Bouchaud"], "venue": "Phys. Rev. E,", "citeRegEx": "Hardiman and Bouchaud.,? \\Q2014\\E", "shortCiteRegEx": "Hardiman and Bouchaud.", "year": 2014}, {"title": "Point spectra of some mutually exciting point processes", "author": ["A.G. Hawkes"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Hawkes.,? \\Q1971\\E", "shortCiteRegEx": "Hawkes.", "year": 1971}, {"title": "A cluster process representation of a self-exciting process", "author": ["A.G. Hawkes", "D. Oakes"], "venue": "Journal of Applied Probability,", "citeRegEx": "Hawkes and Oakes.,? \\Q1974\\E", "shortCiteRegEx": "Hawkes and Oakes.", "year": 1974}, {"title": "Cumulants of Hawkes point processes", "author": ["S. Jovanovi\u0107", "J. Hertz", "S. Rotter"], "venue": "Phys. Rev. E,", "citeRegEx": "Jovanovi\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jovanovi\u0107 et al\\.", "year": 2015}, {"title": "Nonparametric markovian learning of triggering kernels for mutually exciting and mutually inhibiting multivariate hawkes processes", "author": ["R. Lemonnier", "N. Vayatis"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Lemonnier and Vayatis.,? \\Q2014\\E", "shortCiteRegEx": "Lemonnier and Vayatis.", "year": 2014}, {"title": "A nonparametric em algorithm for multiscale hawkes processes", "author": ["E. Lewis", "G. Mohler"], "venue": "Journal of Nonparametric Statistics,", "citeRegEx": "Lewis and Mohler.,? \\Q2011\\E", "shortCiteRegEx": "Lewis and Mohler.", "year": 2011}, {"title": "Self-exciting point process modeling of crime", "author": ["G.O. Mohler", "M.B. Short", "P.J. Brantingham", "F.P. Schoenberg", "G.E. Tita"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Mohler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohler et al\\.", "year": 2011}, {"title": "Large sample estimation and hypothesis testing", "author": ["W. K Newey", "D. McFadden"], "venue": "Handbook of econometrics,", "citeRegEx": "Newey and McFadden.,? \\Q1994\\E", "shortCiteRegEx": "Newey and McFadden.", "year": 1994}, {"title": "On lewis\u2019 simulation method for point processes", "author": ["Y. Ogata"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Ogata.,? \\Q1981\\E", "shortCiteRegEx": "Ogata.", "year": 1981}, {"title": "Space-time point-process models for earthquake occurrences", "author": ["Y. Ogata"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Ogata.,? \\Q1998\\E", "shortCiteRegEx": "Ogata.", "year": 1998}, {"title": "Rethinking lda: moment matching for discrete ica", "author": ["A. Podosinnikova", "F. Bach", "S. Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Podosinnikova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Podosinnikova et al\\.", "year": 2015}, {"title": "Adaptive estimation for hawkes processes; application to genome analysis", "author": ["P. Reynaud-Bouret", "S. Schbath"], "venue": "The Annals of Statistics,", "citeRegEx": "Reynaud.Bouret and Schbath.,? \\Q2010\\E", "shortCiteRegEx": "Reynaud.Bouret and Schbath.", "year": 2010}, {"title": "Learning granger causality for hawkes processes", "author": ["H. Xu", "M. Farajtabar", "H. Zha"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Mixture of mutually exciting processes for viral diffusion", "author": ["S.-H. Yang", "H. Zha"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Yang and Zha.,? \\Q2013\\E", "shortCiteRegEx": "Yang and Zha.", "year": 2013}, {"title": "Learning triggering kernels for multi-dimensional hawkes processes", "author": ["K. Zhou", "H. Zha", "L. Song"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Learning social infectivity in sparse low-rank networks using multidimensional hawkes processes", "author": ["K. Zhou", "H. Zha", "L. Song"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "(2016), the high-frequency variations of signals in finance, see Bacry et al. (2015), the earthquakes and aftershocks in geophysics, see Ogata (1998), the crime activity, see Mohler et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 2, "context": "(2016), the high-frequency variations of signals in finance, see Bacry et al. (2015), the earthquakes and aftershocks in geophysics, see Ogata (1998), the crime activity, see Mohler et al.", "startOffset": 65, "endOffset": 150}, {"referenceID": 2, "context": "(2016), the high-frequency variations of signals in finance, see Bacry et al. (2015), the earthquakes and aftershocks in geophysics, see Ogata (1998), the crime activity, see Mohler et al. (2011) or the position of genes in genomics, see Reynaud-Bouret and Schbath (2010).", "startOffset": 65, "endOffset": 196}, {"referenceID": 2, "context": "(2016), the high-frequency variations of signals in finance, see Bacry et al. (2015), the earthquakes and aftershocks in geophysics, see Ogata (1998), the crime activity, see Mohler et al. (2011) or the position of genes in genomics, see Reynaud-Bouret and Schbath (2010). The succession of the precise timestamps carries a great deal of information about the dynamics of the underlying systems.", "startOffset": 65, "endOffset": 272}, {"referenceID": 2, "context": ", the different components of the counting process), by leveraging on their timestamp patterns, see, for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou et al.", "startOffset": 115, "endOffset": 137}, {"referenceID": 2, "context": ", the different components of the counting process), by leveraging on their timestamp patterns, see, for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou et al.", "startOffset": 115, "endOffset": 167}, {"referenceID": 2, "context": ", the different components of the counting process), by leveraging on their timestamp patterns, see, for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou et al.", "startOffset": 115, "endOffset": 192}, {"referenceID": 2, "context": ", the different components of the counting process), by leveraging on their timestamp patterns, see, for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou et al. (2013a); Gomez-Rodriguez et al.", "startOffset": 115, "endOffset": 213}, {"referenceID": 2, "context": ", the different components of the counting process), by leveraging on their timestamp patterns, see, for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou et al. (2013a); Gomez-Rodriguez et al. (2013); Farajtabar et al.", "startOffset": 115, "endOffset": 244}, {"referenceID": 2, "context": ", the different components of the counting process), by leveraging on their timestamp patterns, see, for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou et al. (2013a); Gomez-Rodriguez et al. (2013); Farajtabar et al. (2015); Xu et al.", "startOffset": 115, "endOffset": 270}, {"referenceID": 2, "context": ", the different components of the counting process), by leveraging on their timestamp patterns, see, for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou et al. (2013a); Gomez-Rodriguez et al. (2013); Farajtabar et al. (2015); Xu et al. (2016). Consider a set of nodes I = {1, .", "startOffset": 115, "endOffset": 288}, {"referenceID": 8, "context": "The vector \u03bbt characterizes the distribution of N t, see Daley and Vere-Jones (2003), and patterns in the events time-series can be captured by structuring these intensities.", "startOffset": 57, "endOffset": 85}, {"referenceID": 17, "context": "The Hawkes process introduced in Hawkes (1971) corresponds to an autoregressive structure of the intensities in order to capture self-excitation and cross-excitation of nodes, which is a phenomenon typically observed, for instance, in social networks, see for instance Crane and Sornette (2008).", "startOffset": 4, "endOffset": 47}, {"referenceID": 6, "context": "The Hawkes process introduced in Hawkes (1971) corresponds to an autoregressive structure of the intensities in order to capture self-excitation and cross-excitation of nodes, which is a phenomenon typically observed, for instance, in social networks, see for instance Crane and Sornette (2008). Namely, N t is called a Hawkes point process if the stochastic intensities can be written as", "startOffset": 269, "endOffset": 295}, {"referenceID": 22, "context": "Thus, for instance, in Yang and Zha (2013); Zhou et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 22, "context": "Thus, for instance, in Yang and Zha (2013); Zhou et al. (2013b); Farajtabar et al.", "startOffset": 23, "endOffset": 64}, {"referenceID": 10, "context": "(2013b); Farajtabar et al. (2015), they choose \u03c6ij(t) = \u03b1ijh(t) with \u03b1ij \u2208 R+ quantifies the intensity of the influence of j on i and h(t) a (normalized) function that characterizes the time-profile of this influence and that is shared by all couples of nodes (i, j) (most often, it is chosen to be either exponential h(t) = \u03b2e\u2212\u03b2t or power law h(t) = \u03b2t\u2212(\u03b2+1)).", "startOffset": 9, "endOffset": 34}, {"referenceID": 10, "context": "(2013b); Farajtabar et al. (2015), they choose \u03c6ij(t) = \u03b1ijh(t) with \u03b1ij \u2208 R+ quantifies the intensity of the influence of j on i and h(t) a (normalized) function that characterizes the time-profile of this influence and that is shared by all couples of nodes (i, j) (most often, it is chosen to be either exponential h(t) = \u03b2e\u2212\u03b2t or power law h(t) = \u03b2t\u2212(\u03b2+1)). Both approaches are, most of the time, highly non-realistic. On the one hand there is a priori no reason for assuming that the time-profile of the influence of a node j on a node i does not depend on the pair (i, j). On the other hand, assuming an exponential shape or a power law shape for a kernel arbitrarily imposes an event impact that is always instantly maximal and that can only decrease with time, while in practice, there may exist a latency between an event and its maximal impact. In order to have more flexibility on the shape of the kernels, nonparametric estimation can be considered. Expectation-Maximization algorithms can be found in Lewis and Mohler (2011) (for d = 1) or in Zhou et al.", "startOffset": 9, "endOffset": 1038}, {"referenceID": 10, "context": "(2013b); Farajtabar et al. (2015), they choose \u03c6ij(t) = \u03b1ijh(t) with \u03b1ij \u2208 R+ quantifies the intensity of the influence of j on i and h(t) a (normalized) function that characterizes the time-profile of this influence and that is shared by all couples of nodes (i, j) (most often, it is chosen to be either exponential h(t) = \u03b2e\u2212\u03b2t or power law h(t) = \u03b2t\u2212(\u03b2+1)). Both approaches are, most of the time, highly non-realistic. On the one hand there is a priori no reason for assuming that the time-profile of the influence of a node j on a node i does not depend on the pair (i, j). On the other hand, assuming an exponential shape or a power law shape for a kernel arbitrarily imposes an event impact that is always instantly maximal and that can only decrease with time, while in practice, there may exist a latency between an event and its maximal impact. In order to have more flexibility on the shape of the kernels, nonparametric estimation can be considered. Expectation-Maximization algorithms can be found in Lewis and Mohler (2011) (for d = 1) or in Zhou et al. (2013a) (d > 1).", "startOffset": 9, "endOffset": 1076}, {"referenceID": 2, "context": "An alternative method is proposed in Bacry and Muzy (2016) where the nonparametric estimation is formulated as a numerical solving of a Wiener-Hopf equation.", "startOffset": 37, "endOffset": 59}, {"referenceID": 2, "context": "An alternative method is proposed in Bacry and Muzy (2016) where the nonparametric estimation is formulated as a numerical solving of a Wiener-Hopf equation. Another nonparametric strategy considers a decomposition of kernels on a dictionary of function h1, . . . , hK , namely \u03c6ij(t) = \u2211K k=1 a ij k hk(t), where the coefficients a ij k are estimated, see Hansen et al. (2015); Lemonnier and Vayatis (2014) and Xu et al.", "startOffset": 37, "endOffset": 378}, {"referenceID": 2, "context": "An alternative method is proposed in Bacry and Muzy (2016) where the nonparametric estimation is formulated as a numerical solving of a Wiener-Hopf equation. Another nonparametric strategy considers a decomposition of kernels on a dictionary of function h1, . . . , hK , namely \u03c6ij(t) = \u2211K k=1 a ij k hk(t), where the coefficients a ij k are estimated, see Hansen et al. (2015); Lemonnier and Vayatis (2014) and Xu et al.", "startOffset": 37, "endOffset": 408}, {"referenceID": 2, "context": "An alternative method is proposed in Bacry and Muzy (2016) where the nonparametric estimation is formulated as a numerical solving of a Wiener-Hopf equation. Another nonparametric strategy considers a decomposition of kernels on a dictionary of function h1, . . . , hK , namely \u03c6ij(t) = \u2211K k=1 a ij k hk(t), where the coefficients a ij k are estimated, see Hansen et al. (2015); Lemonnier and Vayatis (2014) and Xu et al. (2016), where group-lasso is used to induce a sparsity pattern on the coefficients a k that is shared across k = 1, .", "startOffset": 37, "endOffset": 429}, {"referenceID": 17, "context": "As it can be seen from the cluster representation of Hawkes processes (Hawkes and Oakes (1974)), this integral represents the mean total number of events of type i directly triggered by an event of type j, and then encodes a notion of causality.", "startOffset": 53, "endOffset": 95}, {"referenceID": 13, "context": "1), such integral can be related to the Granger causality (Granger (1969)).", "startOffset": 40, "endOffset": 74}, {"referenceID": 16, "context": "As it will be shown, this approach turns out to be particularly robust to the kernel shapes, which is not the case of all previous Hawkes-based approaches that aim causality recovery. We call this method NPHC (Non Parametric Hawkes Cumulant), since our approach is of nonparametric nature. We provide a theoretical analysis that proves the consistency of the NPHC estimator. Our proof is based on ideas from the theory of Generalized Method of Moments (GMM) but requires an original technical trick since our setting strongly departs from the standard parametric statistics with i.i.d observations. Note that moment and cumulant matching techniques proved particularly powerful for latent topic models, in particular Latent Dirichlet Allocation, see Podosinnikova et al. (2015). A small set of previous works, namely Da Fonseca and Zaatour (2014); A\u00eft-Sahalia et al.", "startOffset": 131, "endOffset": 778}, {"referenceID": 6, "context": "A small set of previous works, namely Da Fonseca and Zaatour (2014); A\u00eft-Sahalia et al.", "startOffset": 41, "endOffset": 68}, {"referenceID": 1, "context": "A small set of previous works, namely Da Fonseca and Zaatour (2014); A\u00eft-Sahalia et al. (2010), already used method of moments with Hawkes processes, but only in a parametric setting.", "startOffset": 69, "endOffset": 95}, {"referenceID": 13, "context": "1 Branching structure and Granger causality From the definition of Hawkes process as a Poisson cluster process, see Jovanovi\u0107 et al. (2015) or Hawkes and Oakes (1974), gij can be simply interpreted as the average total number of events of node i whose direct ancestor is a given event of node j (by direct we mean that interactions mediated by any other intermediate event are not counted).", "startOffset": 26, "endOffset": 140}, {"referenceID": 13, "context": "1 Branching structure and Granger causality From the definition of Hawkes process as a Poisson cluster process, see Jovanovi\u0107 et al. (2015) or Hawkes and Oakes (1974), gij can be simply interpreted as the average total number of events of node i whose direct ancestor is a given event of node j (by direct we mean that interactions mediated by any other intermediate event are not counted).", "startOffset": 26, "endOffset": 167}, {"referenceID": 3, "context": "Namely, introducing the counting function N i\u2190j t that counts the number of events of i whose direct ancestor is an event of j, we know from Bacry et al. (2015) that E[dN i\u2190j t ] = g E[dN j t ] = g \u039bdt, (2) where we introduced \u039bi as the intensity expectation, namely satisfying E[dN i t ] = \u039bidt.", "startOffset": 141, "endOffset": 161}, {"referenceID": 3, "context": "Namely, introducing the counting function N i\u2190j t that counts the number of events of i whose direct ancestor is an event of j, we know from Bacry et al. (2015) that E[dN i\u2190j t ] = g E[dN j t ] = g \u039bdt, (2) where we introduced \u039bi as the intensity expectation, namely satisfying E[dN i t ] = \u039bidt. Note that \u039bi does not depend on time by stationarity ofN t, which is known to hold under the stability condition \u2016G\u2016 < 1, where \u2016G\u2016 stands for the spectral norm ofG. In particular, this condition implies the non-singularity of Id \u2212G. Since the question of a real causality is too complex in general, most econometricians agreed on the simpler definition of Granger causality Granger (1969). Its mathematical formulation is a statistical hypothesis test: X causes Y in the sense of Granger causality if forecasting future values of Y is more successful while taking X past values into account.", "startOffset": 141, "endOffset": 687}, {"referenceID": 3, "context": "Namely, introducing the counting function N i\u2190j t that counts the number of events of i whose direct ancestor is an event of j, we know from Bacry et al. (2015) that E[dN i\u2190j t ] = g E[dN j t ] = g \u039bdt, (2) where we introduced \u039bi as the intensity expectation, namely satisfying E[dN i t ] = \u039bidt. Note that \u039bi does not depend on time by stationarity ofN t, which is known to hold under the stability condition \u2016G\u2016 < 1, where \u2016G\u2016 stands for the spectral norm ofG. In particular, this condition implies the non-singularity of Id \u2212G. Since the question of a real causality is too complex in general, most econometricians agreed on the simpler definition of Granger causality Granger (1969). Its mathematical formulation is a statistical hypothesis test: X causes Y in the sense of Granger causality if forecasting future values of Y is more successful while taking X past values into account. In Eichler et al. (2016), it is shown that for N t a multivariate Hawkes process, N j t does not Granger-cause N i t w.", "startOffset": 141, "endOffset": 915}, {"referenceID": 18, "context": "2 Integrated cumulants of the Hawkes process A general formula for the integral of the cumulants of a multivariate Hawkes process is provided in Jovanovi\u0107 et al. (2015). As explained below, for the purpose of our method, we only need to consider cumulants up to the third order.", "startOffset": 30, "endOffset": 169}, {"referenceID": 2, "context": "Using the martingale representation from Bacry and Muzy (2016) or the Poisson cluster process representation from Jovanovi\u0107 et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 2, "context": "Using the martingale representation from Bacry and Muzy (2016) or the Poisson cluster process representation from Jovanovi\u0107 et al. (2015), one can obtain an explicit relationship between these integrated cumulants and the matrixG.", "startOffset": 41, "endOffset": 138}, {"referenceID": 16, "context": "The simplest case d = 1 has been considered in Hardiman and Bouchaud (2014), where it is shown that one can choose M = {C11} in order to compute the kernel integral.", "startOffset": 47, "endOffset": 76}, {"referenceID": 14, "context": "It is noteworthy that the above mean square error approach can be seen as a peculiar Generalized Method of Moments (GMM), see Hall (2005). This framework allows us to determine the optimal weighting matrix involved in the loss function.", "startOffset": 126, "endOffset": 138}, {"referenceID": 29, "context": "Solving the considered problem on a larger scale, say d 103, is an open question, even with state-of-the-art parametric and nonparametric approaches, see for instance Zhou et al. (2013a); Xu et al.", "startOffset": 167, "endOffset": 187}, {"referenceID": 28, "context": "(2013a); Xu et al. (2016); Zhou et al.", "startOffset": 9, "endOffset": 26}, {"referenceID": 28, "context": "(2013a); Xu et al. (2016); Zhou et al. (2013b); Bacry and Muzy (2016), where the number of components d in experiments is always around 100 or smaller.", "startOffset": 9, "endOffset": 47}, {"referenceID": 2, "context": "(2013b); Bacry and Muzy (2016), where the number of components d in experiments is always around 100 or smaller.", "startOffset": 9, "endOffset": 31}, {"referenceID": 5, "context": "(8) and (9): L\u0303(R) = (1\u2212 \u03ba)\u2016R \u0108 > + 2[R (\u0108 \u2212RL\u0302)]R> \u2212 K\u0302\u20162 + \u03ba\u2016RL\u0302R> \u2212 \u0108\u20162 (14) As explained in Choromanska et al. (2015), the loss function of a typical multilayer neural network with simple nonlinearities can be expressed as a polynomial function of the weights in the network,", "startOffset": 96, "endOffset": 122}, {"referenceID": 9, "context": "We used the AdaGrad from Duchi et al. (2011), a variant of the Stochastic Gradient Descent with adaptive learning rates.", "startOffset": 25, "endOffset": 45}, {"referenceID": 0, "context": "An efficient implementation of this algorithm with TensorFlow, see Abadi et al. (2016), is available on GitHub: https://github.", "startOffset": 67, "endOffset": 87}, {"referenceID": 28, "context": ", the ordinary differential equations-based (ODE) algorithm in Zhou et al. (2013a), the Granger Causality-based algorithm in Xu et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 12, "context": "(2013a), the Granger Causality-based algorithm in Xu et al. (2016), the ADM4 algorithm in Zhou et al.", "startOffset": 13, "endOffset": 67}, {"referenceID": 12, "context": "(2013a), the Granger Causality-based algorithm in Xu et al. (2016), the ADM4 algorithm in Zhou et al. (2013b), and the Wiener-Hopf-based algorithm in Bacry and Muzy (2016), our method has a very competitive complexity.", "startOffset": 13, "endOffset": 110}, {"referenceID": 2, "context": "(2013b), and the Wiener-Hopf-based algorithm in Bacry and Muzy (2016), our method has a very competitive complexity.", "startOffset": 48, "endOffset": 70}, {"referenceID": 29, "context": "Method Total complexity ODE Zhou et al. (2013a) O(NiterM(nd + L(nd+ n))) GC Xu et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 28, "context": "(2013a) O(NiterM(nd + L(nd+ n))) GC Xu et al. (2016) O(NiterMnd) ADM4 Zhou et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 28, "context": "(2013a) O(NiterM(nd + L(nd+ n))) GC Xu et al. (2016) O(NiterMnd) ADM4 Zhou et al. (2013b) O(Niternd) WH Bacry and Muzy (2016) O(ndL+ dL) NPHC O(nd +Niterd)", "startOffset": 36, "endOffset": 90}, {"referenceID": 2, "context": "(2013b) O(Niternd) WH Bacry and Muzy (2016) O(ndL+ dL) NPHC O(nd +Niterd)", "startOffset": 22, "endOffset": 44}, {"referenceID": 25, "context": "We simulated several datasets with Ogata\u2019s Thinning algorithm Ogata (1981) using the open-source library tick1, each corresponding to a shape of kernel: rectangular, exponential or power law kernel, see Figure 1 below.", "startOffset": 35, "endOffset": 75}, {"referenceID": 3, "context": "This model has first been introduced in Bacry et al. (2016), and models the order book via the following 8-dimensional point process: Nt = (P (a) t , P (b) t , T (a) t , T (b) t , L (a) t , L (b) t , C (a) t , C (b) t ), where P (a) (resp.", "startOffset": 40, "endOffset": 60}, {"referenceID": 28, "context": "We compare NPHC to state-of-the art baselines: the ODE-based algorithm (ODE) by Zhou et al. (2013a), the Granger Causality-based algorithm (GC) by Xu et al.", "startOffset": 80, "endOffset": 100}, {"referenceID": 12, "context": "(2013a), the Granger Causality-based algorithm (GC) by Xu et al. (2016), the ADM4 algorithm (ADM4) by Zhou et al.", "startOffset": 13, "endOffset": 72}, {"referenceID": 12, "context": "(2013a), the Granger Causality-based algorithm (GC) by Xu et al. (2016), the ADM4 algorithm (ADM4) by Zhou et al. (2013b), and the Wiener-Hopf-based algorithm (WH) by Bacry and Muzy (2016).", "startOffset": 13, "endOffset": 122}, {"referenceID": 2, "context": "(2013b), and the Wiener-Hopf-based algorithm (WH) by Bacry and Muzy (2016).", "startOffset": 53, "endOffset": 75}, {"referenceID": 31, "context": "Indeed, it has been shown in Zhou et al. (2013a) that kernels of MemeTracker data are not exponential, nor power law.", "startOffset": 29, "endOffset": 49}, {"referenceID": 3, "context": "On the financial data, the estimated kernel norm matrix obtained via NPHC, see Figure 3, gave some interpretable results (see also Bacry et al. (2016)): 1.", "startOffset": 131, "endOffset": 151}, {"referenceID": 17, "context": "Then we use the characterization of second-order statistics, first formulated in Hawkes (1971) and fully generalized in Bacry and Muzy (2016),", "startOffset": 81, "endOffset": 95}, {"referenceID": 2, "context": "Then we use the characterization of second-order statistics, first formulated in Hawkes (1971) and fully generalized in Bacry and Muzy (2016),", "startOffset": 120, "endOffset": 142}, {"referenceID": 20, "context": "2 Proof of Equation (9) We start from Jovanovi\u0107 et al. (2015), cf.", "startOffset": 38, "endOffset": 62}, {"referenceID": 15, "context": "5 Proof of the Theorem The main difference with the usual Generalized Method of Moments, see Hansen (1982), relies in the relaxation of the moment conditions, since we have E[\u011dT (\u03b80)] = mT 6= 0.", "startOffset": 93, "endOffset": 107}, {"referenceID": 15, "context": "5 Proof of the Theorem The main difference with the usual Generalized Method of Moments, see Hansen (1982), relies in the relaxation of the moment conditions, since we have E[\u011dT (\u03b80)] = mT 6= 0. We adapt the proof of consistency given in Newey and McFadden (1994).", "startOffset": 93, "endOffset": 264}, {"referenceID": 18, "context": "We can relate the integral of the Hawkes process\u2019s kernels to the integrals of the cumulant densities, from Jovanovi\u0107 et al. (2015). Our cumulant matching method would fall into the usual GMM framework if we could estimate - without bias - the integral of the covariance on R, and the integral of the skewness on R2.", "startOffset": 34, "endOffset": 132}, {"referenceID": 24, "context": "2 Consistency First, let\u2019s remind a useful theorem for consistency in GMM from Newey and McFadden (1994). Theorem 4.", "startOffset": 79, "endOffset": 105}, {"referenceID": 24, "context": "2 Consistency First, let\u2019s remind a useful theorem for consistency in GMM from Newey and McFadden (1994). Theorem 4.1. If there is a function Q0(\u03b8) such that (i) Q0(\u03b8) is uniquely maximized at \u03b80; (ii) \u0398 is compact; (iii) Q0(\u03b8) is continuous; (iv) Q\u0302T (\u03b8) converges uniformly in probability to Q0(\u03b8), then \u03b8\u0302T = arg max Q\u0302T (\u03b8) P \u2212\u2192 \u03b80. We can now prove the consistency of our estimator. Theorem 4.2. Suppose that (Nt) is observed on R+, \u0174T P \u2212\u2192W , and 1. W is positive semi-definite and Wg0(\u03b8) = 0 if and only if \u03b8 = \u03b80, 2. \u03b8 \u2208 \u0398, which is compact, 3. the spectral radius of the kernel norm matrix satisfies ||\u03a6||\u2217 < 1, 4. \u2200i, j, k \u2208 [d], \u222b fT u C ij u du\u2192 \u222b C u du and \u222b fT u f T v K ijk u,vdudv \u2192 \u222b K u,vdudv, 5. (F T )2/T P \u2212\u2192 0 and ||f ||\u221e = O(1). Then \u03b8\u0302T P \u2212\u2192 \u03b80. Remark 1. In practice, we use a constant sequence of weighting matrices: \u0174T = Id. Proof. Proceed by verifying the hypotheses of Theorem 2.1 from Newey and McFadden (1994). Condition 2.", "startOffset": 79, "endOffset": 942}], "year": 2017, "abstractText": "We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. As a consequence, it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process. A theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we show, on numerical experiments, that our approach is indeed very robust with respect to the shape of the kernels and gives appealing results on the MemeTracker database and on financial order book data.", "creator": "LaTeX with hyperref package"}}}