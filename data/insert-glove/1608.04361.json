{"id": "1608.04361", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Multi-way Monte Carlo Method for Linear Systems", "abstract": "We -2.5 study galax the Monte Carlo heydrich method for sugarcoating solving a linear system pnu of the sharaf form $ x = straightjacket H penhallow x + b $. kandji A sufficient condition for retinues the method owain to ferromagnet work is $ \\ | starodub H \\ | & aramaean lt; cubillas 1 $, torneo which pue greatly sexed limits demokratikong the mcmullan usability abramson of 71-60 this 108.34 method. We tucuman improve wycombe this broadheath condition 24.88 by amyl proposing traversi a isixhosa new fuxi multi - enthrone way 4,344 Markov random walk, brusqueness which goffs is pc-7 a kreisky generalization 1.3927 of the kokoschka standard bakhrom Markov khenpo random kushite walk. Under our new 430 framework nivelle we iar prove that the marathe necessary dakar and rool sufficient norbury condition icrisat for vient our method 23.39 to work 30/32 is 5-34 the wolfensohn spectral antholz radius $ \\ sikirica rho (snit H ^ {+} ) & lt; non-jewish 1 $, restructuration which viggen is windecker a bandai weaker requirement quillayute than $ \\ | etheric H \\ | & lt; 1 $. m-24 In almendralejo addition afrotropics to entertains solving more problems, our lothians new m-21 method can 25.56 work faster than the moshtarak standard algorithm. tulancingo In numerical marians experiments sokol on both synthetic macgregors and vedic real aliant world matrices, angawi we housesitter demonstrate orzysz the rionegro effectiveness of stinginess our drogheda new chayote method.", "histories": [["v1", "Mon, 15 Aug 2016 18:45:08 GMT  (230kb,D)", "http://arxiv.org/abs/1608.04361v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.AI", "authors": ["tao wu", "david f gleich"], "accepted": false, "id": "1608.04361"}, "pdf": {"name": "1608.04361.pdf", "metadata": {"source": "CRF", "title": "Multi-way Monte Carlo Method for Linear Systems", "authors": ["Tao Wu", "David F. Gleich"], "emails": ["wu577@purdue.edu", "dgleich@purdue.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n04 36\n1v 1\n[ cs\n.N A\n] 1\n5 A\nug 2\n01 6"}, {"heading": "1 Introduction", "text": "The Monte Carlo method [18] for solving a linear system uses a random walk to approximate the solution. This method has several advantages over traditional deterministic algorithms (e.g., Gaussian elimination and iterative methods) due to its unique characteristics. First, the Monte Carlo method can be highly effective when only modest accuracy is required, as is common for many problems on data such as PageRank computations [2]. Second, it is well-known that Monte Carlo algorithms are highly parallelizable [13, 5], thus they are ideal for modern paralleled computers or clusters. Third, Monte Carlo methods can identify only on a single component or a linear form of the solution, which is often all that is required in many applications [17]. Last but not least, Monte Carlo methods have advantages when dealing with large linear systems [11, 7] because they do not always require a full solution vector."}, {"heading": "1.1 The standard Monte Carlo method", "text": "Consider the following linear system: x = Hx + b\nwhere H \u2208 Rn\u00d7n and x,b \u2208 Rn and where our goal is to evaluate the functional \u3008h,x\u3009 = \u2211n\ni=1 hixi. We could then use this primitive to compute the solution by evaluating the functional for each standard basis vector to get each single component of the solution.\nIt is known that if the spectral radius \u03c1(H) < 1, then the Neumann Series \u2211\u221e\n`=0H `b will\nconverge to the solution vector x. The Monte Carlo method uses this observation to create a Markov random walk Xt on the state space S = {1, 2, \u00b7 \u00b7 \u00b7 , n} with initial probability Pr(X0 = i) = pi and transition probability Pr(X`+1 = j | X` = i) = Pi,j , s.t.hi 6= 0\u21d2 pi 6= 0 and Hi,j 6= 0\u21d2 Pi,j 6= 0.\nLet \u03bd be a realization of the random walk: k0 \u2192 k1 \u2192 k2 \u2192 \u00b7 \u00b7 \u00b7 \u2192 k` \u2192 \u00b7 \u00b7 \u00b7. A walk related weight and random variable can be calculated as\nW` = hk0Hk0,k1Hk1,k2 \u00b7 \u00b7 \u00b7Hk`\u22121,k` pk0Pk0,k1Pk1,k2 \u00b7 \u00b7 \u00b7Pk`\u22121,k` for ` = 0, 1, 2, \u00b7 \u00b7 \u00b7 and X(\u03bd) = \u221e\u2211 `=0 W`bk` .\nThen it can be shown (for instance [7]) that E[X] = \u3008h,x\u3009, and more specifically E[W`fk` ] = \u3008h,H `f\u3009.\nHowever the random walk model does not guarantee convergence [12, 18]. According to the law of large numbers, a necessary and sufficient condition to estimate E[X] using the empirical mean value of X is Var[X] <\u221e. Empirical studies [12, 3] show that it is easy to have Var[X] =\u221e even when the Neumann Series converges (i.e., \u03c1(H) < 1)."}, {"heading": "1.2 Our Contributions", "text": "In order to apply the Monte Carlo method, existing work [8, 16, 17] assumes \u2016H\u2016 < 1 (for the infinity norm \u2016H\u2016 = maxi \u2211 j |Hi,j |), which suffices to show Var[X] <\u221e, but which is a stronger condition than \u03c1(H) < 1. Although it is possible Var[X] <\u221e when \u2016H\u2016 \u2265 1, there is no easy way to check. To tackle this problem, we propose a multi-way Markov random walk which uses multiple transition matrices. At each step of the random walk, the transition matrix is constructed in a way akin to the Monte Carlo Almost Optimal (MAO) framework [8, 12]. We prove that under this type of random walk, the new method always converges when \u03c1(H+) < 1, where H+ is the nonnegative matrix as H+ij = |Hij |. In addition, our new framework has the tendency to get the result faster than the standard method. One downside to our approach is that is cannot be implemented in a purely local fashion akin to the standard Monte Carlo method as it requires global work to build the multi-way walk."}, {"heading": "1.3 Related Work", "text": "Research on Monte Carlo Methods for linear systems can be divided into two classes: direct methods and hybrid methods. Direct methods study the various techniques of using the Monte Carlo solvers themselves, for instances non-diagonal splitting [16] and relaxation parameters [7]. Hybrid methods [11, 10, 1] use Monte Carlo as a black box combined with iterative techniques. Examples of these works are Sequential Monte Carlo method [11] and synthetic-acceleration method [10]. Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].\nIn this paper, we focus on the direct Monte Carlo procedure. Our ideas can also be incorporated into the hybrid frameworks to better improve the performance."}, {"heading": "2 Multi-way Markov Random Walk", "text": "In this section we generalize the idea of random walk for estimating the functional to using a hypermatrix of transitions to compute the estimate. Then we analyze the convergence of the simulations based on the variance of the relevant random variable.\nWe use bold, upper-case letters such as A to denote matrices, and bold, lower-case letters such as x to denote vectors. Hypermatrices as in P are bold, underlined, upper-case letters. We use letters with subscripts of indices to denote elements xi of a vector and Ai,j of a matrix. For a mode-3 hypermatrix P , its elements are denoted by P (`)i,j ."}, {"heading": "2.1 Hypermatrix Transitions", "text": "Instead of using a fixed transition matrix P as in the classic Monte Carlo method in section 1, we allow the random walk to vary transition matrices with each step. An m\u2212way random walk can be interpreted as walking via m different transition matrices periodically in a round-robin way. Formally, we define a m\u2212way Markov random walk (p,P ) as Zt: k0 \u2192 k1 \u2192 k2 \u2192 \u00b7 \u00b7 \u00b7 \u2192 ki \u2192 \u00b7 \u00b7 \u00b7, where the initial probability follows p, and the transition probability follows a hypermatrix P :\nPr(k0 = i) = pi\nPr(k`+1 = j|k` = i) = P (mod(`,m)+1) i,j .\n(1)\nHere mod(`,m) denotes the remainder after dividing ` by m. For notation simplicity, we use P (`)i,j to denote P (mod(`\u22121,m)+1)i,j for ` = 1, 2, \u00b7 \u00b7 \u00b7."}, {"heading": "2.2 The Multi-way Monte Carlo Method", "text": "Our goal is to compute the functional \u3008h,x\u3009 where x is the solution of linear system x = Hx + b. Through the paper we have the basic assumption \u03c1(H) < 1. We also exclude the corner cases where h is a zero vector, or H has zero rows/columns. If we construct the initial probability such that hi 6= 0 \u21d2 pi 6= 0 and the transition hypermatrix P such that Hi,j 6= 0 \u21d2 P (`)i,j 6= 0, then we can define the related weights W` and the variable Z in a similar way with section 1, and formally:\nW` = hk0Hk0,k1Hk1,k2 \u00b7 \u00b7 \u00b7Hk`\u22121,k` pk0P (1) k0,k1 P (2) k1,k2 \u00b7 \u00b7 \u00b7P (`)k`\u22121,k` for ` = 0, 1, 2, \u00b7 \u00b7 \u00b7\nZ = \u221e\u2211 `=0 W`bk`\n(2)\nIt is worth noting the above definition of multi-way Markov random walk is a generalization of the standard Markov chain, which is the special case with m = 1.\nTheorem 2.1. For the linear system x = Hx + b, Z defined from (2) has the expected value E[Z] = \u3008h,x\u3009. Proof. We first prove that E[W`bk` ] = \u3008h,H\n`b\u3009 for all ` = 0, 1, 2, \u00b7 \u00b7 \u00b7. Then the convergence of the Neumann series will give us E[Z] = \u3008h,x\u3009.\nWe have E[W0bk0 ] = \u2211\npk0 6=0 hk0 pk0\nbk0pk0 = \u2211\nhk0 6=0 hk0bk0 = \u3008h,b\u3009. Similarly for the case of ` \u2265 1:\nE[W`bk` ] = \u2211 pk0 6=0 \u2211 P\n(1) k0,k1 6=0\n\u00b7 \u00b7 \u00b7 \u2211\nP (`) k`\u22121,k` 6=0\nhk0Hk0,k1Hk1,k2 \u00b7 \u00b7 \u00b7Hk`\u22121,k` pk0P (1) k0,k1 P (2) k1,k2 \u00b7 \u00b7 \u00b7P (`)k`\u22121,k` bk`pk0P (1) k0,k1 P (2) k1,k2 \u00b7 \u00b7 \u00b7P (`)k`\u22121,k`\n= \u2211\nhk0 6=0 \u2211 Hk0,k1 6=0 \u00b7 \u00b7 \u00b7 \u2211 Hk`\u22121,k` 6=0 hk0Hk0,k1Hk1,k2 \u00b7 \u00b7 \u00b7Hk`\u22121,k`bk`\n= n\u2211 k0=1 n\u2211 k1=1 \u00b7 \u00b7 \u00b7 n\u2211 k`=1 hk0Hk0,k1Hk1,k2 \u00b7 \u00b7 \u00b7Hk`\u22121,k`bk` = \u3008h,H `b\u3009\nSo E[Z] = \u2211\u221e `=0 E[W`bk` ] = \u3008h, \u2211\u221e `=0H `b\u3009 = \u3008h,x\u3009."}, {"heading": "2.3 Convergence Analysis", "text": "In order to statistically estimate E[Z], we need to ensure Var[Z] <\u221e. The following theorem reveals the explicit form of Var[Z] determined by h,b,H and the m\u2212way random walk (p,P ). Theorem 2.2. For the linear system x = Hx + b, if H and b are nonnegative, Z defined from (2) has variance\nVar[Z] = \u3008h\u0302, \u221e\u2211 i=0 H\u0303 i GDiag(b)(2Hx + b)\u3009 \u2212 \u3008h,x\u30092 (3)\nwhere Diag(b) is a diagonal matrix with diagonal entries equal to b, and h\u0302, H\u0303,G are defined as:\nh\u0302i = { h2i /pi if hi 6= 0 0 if hi = 0\nH\u0302 (`) i,j =\n{ H2i,j/P (`) i,j if Hi,j 6= 0\n0 if Hi,j = 0\nH\u0303 = H\u0302 (1) H\u0302 (2) \u00b7 \u00b7 \u00b7 H\u0302(m) G = I + H\u0302(1) + H\u0302(1)H\u0302(2) + \u00b7 \u00b7 \u00b7+ H\u0302(1)H\u0302(2) \u00b7 \u00b7 \u00b7 H\u0302(m\u22121)\nProof. Since Var[Z] = E[Z2]\u2212 (E[Z])2 = E[Z2]\u2212 \u3008h,x\u30092, we will focus on computing E[Z2]:\nE[Z2] = E[ \u221e\u2211 `=0 W 2` b 2 k` + 2 \u2211 r>` W`Wrbk`bkr ]\nSince all the intermediate terms are nonnegative, by Tonelli\u2019s theorem we can analyze the sum in pieces. We have E[W 20 b2k0 ] = \u2211\npk0 6=0 h2k0 p2k0\nb2k0pk0 = \u2211\nh\u0302k0 6=0 h\u0302k0b\n2 k0 = \u3008h\u0302,Diag(b)b\u3009, and when ` \u2265 1,\nE[W 2` b 2 k` ] = \u2211 pk0 6=0 \u2211 P\n(1) k0,k1 6=0\n\u00b7 \u00b7 \u00b7 \u2211\nP (`) k`\u22121,k` 6=0\n(hk0Hk0,k1Hk1,k2 \u00b7 \u00b7 \u00b7Hk`\u22121,k` pk0P (1) k0,k1 P (2) k1,k2 \u00b7 \u00b7 \u00b7P (`)k`\u22121,k` )2 b2k`pk0P (1) k0,k1 P (2) k1,k2 \u00b7 \u00b7 \u00b7P (`)k`\u22121,k`\n= \u2211\nh\u0302k0 6=0\n\u2211 H\u0302\n(1) k0,k1 6=0\n\u00b7 \u00b7 \u00b7 \u2211\nH\u0302 (`) k`\u22121,k` 6=0\nh\u0302k0H\u0302 (1) k0,k1 H\u0302 (2) k1,k2 \u00b7 \u00b7 \u00b7 H\u0302(`)k`\u22121,k`b 2 k`\n= \u3008h\u0302, H\u0302(1)H\u0302(2) \u00b7 \u00b7 \u00b7 H\u0302(`) Diag(b)b\u3009\n(4)\nApplying the above result from (4), we have\nE[ \u221e\u2211 `=0 W 2` b 2 k` ] = \u2329 h\u0302, ( I + \u221e\u2211 `=1 H\u0302 (1) H\u0302 (2) \u00b7 \u00b7 \u00b7 H\u0302(`) ) Diag(b)b \u232a\n= \u2329 h\u0302, ( G+ \u221e\u2211 `=m H\u0302 (1) H\u0302 (2) \u00b7 \u00b7 \u00b7 H\u0302(`) ) Diag(b)b \u232a\n= \u2329 h\u0302, ( G+ H\u0303(I + \u221e\u2211 `=1 H\u0302 (1) H\u0302 (2) \u00b7 \u00b7 \u00b7 H\u0302(`)) ) Diag(b)b \u232a = \u2329 h\u0302, \u221e\u2211 i=0 H\u0303 i GDiag(b)b\n\u232a (5)\nNext we compute the second part of E[Z2]:\nE[ \u2211 r>` W`Wrbk`bkr ] = E[ \u221e\u2211 `=0 W`bk`( \u221e\u2211 r=`+1 Wrbkr)]\n= \u221e\u2211 `=0 \u2211 pk06=0 \u00b7 \u00b7 \u00b7 \u2211\nP (`) k`\u22121,k 6\u0300=0\n(hk0Hk0,k1Hk1,k2\u00b7 \u00b7 \u00b7Hk`\u22121,k` pk0P (1) k0,k1 P (2) k1,k2 \u00b7 \u00b7 \u00b7P (`)k`\u22121,k` )2( \u221e\u2211 r=`+1 \u2211 P\n(`) k`,k`+1 6=0\n\u00b7 \u00b7 \u00b7 \u2211\nP (`) kr\u22121,kr 6=0\nHk`,k`+1\u00b7 \u00b7 \u00b7Hkr\u22121,kr P\n(`+1) k`,k`+1 \u00b7 \u00b7 \u00b7P (r)kr\u22121,kr\n\u00d7pk0P (1) k0,k1 P (2) k1,k2 \u00b7 \u00b7 \u00b7P (`)k`\u22121,k`P (`+1) k`,k`+1 \u00b7 \u00b7 \u00b7P (r)kr\u22121,krbk`bkr ) ,\n(here, we have extracted all the prefix terms in W` and Wr that are the same because r > `)\n= \u221e\u2211 `=0 \u2211 h\u0302k06=0 \u00b7 \u00b7 \u00b7 \u2211\nH\u0302 (`) k`\u22121,k 6\u0300=0\n( h\u0302k0H\u0302 (1) k0,k1 H\u0302 (2) k1,k2 \u00b7 \u00b7 \u00b7 H\u0302(`)k`\u22121,k` ) bk` ( \u221e\u2211 r=`+1 \u2211 Hk`,k`+16=0 \u00b7 \u00b7 \u00b7 \u2211 Hkr\u22121,kr6=0 Hk`,k`+1\u00b7 \u00b7 \u00b7Hkr\u22121,kr ) bkr\n= \u2329 h\u0302, \u221e\u2211 `=0 (H\u0302 (1) \u00b7 \u00b7 \u00b7 H\u0302(`)) Diag(b)( \u221e\u2211 r=`+1 Hr\u2212`b) \u232a\n(6)\n= \u2329 h\u0302, \u221e\u2211 `=0 (H\u0302 (1) \u00b7 \u00b7 \u00b7 H\u0302(`)) Diag(b)Hx \u232a = \u2329 h\u0302, \u221e\u2211 i=0 H\u0303 i GDiag(b)Hx \u232a .\nFor these final steps, we used the Neumann series to move to Hx and then used the periodicity to rewrite the expressions in terms of G. Now, combining the results from (5) and (6) we have:\nVar[Z] = \u2329 h\u0302, \u221e\u2211 i=0 H\u0303 i GDiag(b)b \u232a + 2 \u2329 h\u0302, \u221e\u2211 i=0 H\u0303 i GDiag(b)Hx \u232a \u2212 \u3008h,x\u30092\n= \u3008h\u0302, \u221e\u2211 i=0 H\u0303 i GDiag(b)(2Hx + b)\u3009 \u2212 \u3008h,x\u30092\nFor the general cases of H,b without the assumption of nonnegativity, if \u03c1(H\u0303) < 1, the above conclusion (i.e., equation (3)) still holds according to Fubini\u2019s Theorem.\nCombining both of these results, the following corollary is straightforward from the conclusion of Theorem 2.2.\nCorollary 2.3. For the linear system x = Hx + b, if the spectral radius \u03c1(H\u0303) < 1, then Var[Z] = \u3008h\u0302, (I \u2212 H\u0303)\u22121GDiag(b)(2Hx + b)\u3009 \u2212 \u3008h,x\u30092 <\u221e\nThe above analysis of Var[Z] shows that with the condition \u03c1(H\u0303) < 1, and by the law of large numbers we can estimate the value of \u3008h,x\u3009 from the variable Z. For the cases when \u03c1(H\u0303) \u2265 1, the following corollary shows that it is possible to have Var[Z] =\u221e. The essence of the idea and proof is just that we can construct a vector to touch the dominant eigenvector with eigenvalue \u2265 1.\nCorollary 2.4. Under the same assumptions with Theorem 2.2, if the spectral radius \u03c1(H\u0303) \u2265 1, and if G is full-rank, then there always exists some b,h \u2208 Rn such that Var[Z] =\u221e. (Note that for the standard Monte Carlo method (i.e., m = 1), since G = I, the method diverges for certain b,h.)\nProof. Let J denote the Jordan canonical form for matrix H\u0302 s.t. H\u0302 = PJP\u22121, where P = [p1,p2, \u00b7 \u00b7 \u00b7 ,pn] and pi for i = 1, 2, \u00b7 \u00b7 \u00b7 , n are the generalized eigenvectors. The diagonal entries of J are eigenvalues of H\u0302, and J is composed with Jordan blocks:\nJ =  J1 J2 . . .\nJp\n where J i =  \u03bbi 1 \u03bbi . . . . . . 1\n\u03bbi\n for i = 1, 2, \u00b7 \u00b7 \u00b7 , p. (7)\nThe power of J has the form: J j = Diag(J j1,J j 2, \u00b7 \u00b7 \u00b7 ,J j p), where each individual block J j i with\nsize s is:\nJ ji =  \u03bbji ( j 1 ) \u03bbj\u22121i \u00b7 \u00b7 \u00b7 ( j s\u22121 ) \u03bbj\u2212s\u22121i 0 \u03bbji \u00b7 \u00b7 \u00b7 ( j s\u22122 ) \u03bbj\u2212si\n0 . . . . . . 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u03bbji  for j > s. (8) So the upper right element ( j s\u22121 ) \u03bbj\u2212s\u22121i has the largest asymptotic value as j \u2192\u221e. Without a loss of generality, we can assume that J1,J2, \u00b7 \u00b7 \u00b7 ,Jp are sorted in the decending order of eigenvalues, and for the case of the equal eigenvalues, they are sorted in the decending order of block sizes. So let J1, \u00b7 \u00b7 \u00b7 ,Jk be the blocks with largestest eigenvalues (i.e., \u03bb1 = \u00b7 \u00b7 \u00b7 = \u03bbk \u2265 \u03bbk+1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbp) and they have the same size s.\nDenote y = P\u22121GDiag(b)(2Hx+b), and z(i) = J iy for i = 0, 1, 2, \u00b7 \u00b7 \u00b7 , n. Given ys, y2s \u00b7 \u00b7 \u00b7 , yks 6= 0, we have z(i)s = \u03bbi1ys(1 + o(1)), z (i) 2s = \u03bb i 1y2s(1 + o(1)), \u00b7 \u00b7 \u00b7 z (i) ks = \u03bb i 1yks(1 + o(1)), and z (i) r /z (i) 1 = o(1) for r 6= s, 2s, \u00b7 \u00b7 \u00b7 , ks as i \u2192 \u221e. If we select h s.t. \u3008h, ysps + y2sp2s + \u00b7 \u00b7 \u00b7 + ykspks\u3009 6= 0, then we have:\n\u3008h, \u221e\u2211 i=0 H\u0303 i GDiag(b)(2Hx + b)\u3009 = \u3008h, \u221e\u2211 i=0 P z(i)\u3009\n= \u221e\u2211 i=0 z (i) 1 \u3008h,p1\u3009+ \u221e\u2211 i=0 z (i) 2 \u3008h,p2\u3009+ \u00b7 \u00b7 \u00b7+ \u221e\u2211 i=0 z(i)n \u3008h,pn\u3009\n= ( \u221e\u2211 i=0 \u03bbi1\u3008h, ysps + y2sp2s + \u00b7 \u00b7 \u00b7+ ykspks\u3009 )( 1 + o(1) ) =\u221e\nSince ps,p2s, \u00b7 \u00b7 \u00b7 ,pks are linear independent, there always exists a vector h, s.t. \u3008h, ysps + y2sp2s + \u00b7 \u00b7 \u00b7+ ykspks\u3009 6= 0, given ys, y2s, \u00b7 \u00b7 \u00b7 , yks are not all zero. Next we prove that there always exists a vector x s.t. ys 6= 0. Let uT denote the s\u2212th row of P\u22121G, then ys can be calculated as:\nys = u T Diag(b)(2Hx + b)\n=uT Diag(x\u2212Hx)(x +Hx) = uT Diag(x)x\u2212 uT Diag(Hx)Hx (9)\nSo ys is a polynomial of x with coefficients coming from uT and H. If ys = 0 for all x \u2208 Rn, then all the coefficients from equation 9 are zero. Denote hi for i = 1, 2, \u00b7 \u00b7 \u00b7 , n are the columns of matrix H, then the coefficients of terms x21, x1x2, x1x3, \u00b7 \u00b7 \u00b7 , x1xn equaling zero gives us: u1 \u2212 \u2211n i=1 uiHi,1Hi,1 = 0 \u2212 \u2211n i=1 uiHi,1Hi,2 = 0 \u2212 \u2211n i=1 uiHi,1Hi,3 = 0 ... ... ... \u2212 \u2211n\ni=1 uiHi,1Hi,n = 0\n=\u21d2 HT Diag(u)h1 = (u1, 0, 0, \u00b7 \u00b7 \u00b7 , 0)T\nSimilarly by setting the coefficients of terms xix1, xix2, \u00b7 \u00b7 \u00b7 , xixn to zero, we have equation:\nHT Diag(u)hi = (0, \u00b7 \u00b7 \u00b7 , 0, ui, 0, \u00b7 \u00b7 \u00b7 , 0)T .\nAnd combining all together will get us HT Diag(u)H = Diag(u). Since uT is the first row of a full-rank matrix, it cannot be a vector of all zeros, and the spectral radius \u03c1(Diag(u)) \u2264 \u03c1(HT )\u03c1(Diag(u))\u03c1(H) < \u03c1(Diag(u)) gives us the contradiction. So ys cannot always be zero.\nIn this section we have seen that E[Z] = \u3008h,x\u3009, which provides us the potential to estimate the value of \u3008h,x\u3009 by simulating the value of Z. However whether it is feasible to apply Monte Carlo simulation depends on H\u0303 . If \u03c1(H\u0303) < 1, then Var[Z] <\u221e, so the simulation is guaranteed to converge. And if \u03c1(H\u0303) \u2265 1, the simulation tends to fail."}, {"heading": "3 Multi-way Monte Carlo Method", "text": "In this section we discuss the two aspects of applying Monte Carlo method based on the multi-way Markov random walk introduced in Section 2. First, we detail the construction of the transition hypermatrix P . Second, we give the error analysis regarding the truncation of the random walk, as well as the probable error."}, {"heading": "3.1 Transition Hypermatrix", "text": "In section 2 Corollary 2.3 and 2.4 indicate that the spectral radius of matrix H\u0303 is crucial to the variance Var[Z]. The matrix H\u0303 is determined by the transition hypermatrix P . Since it is usually computationally inefficient to directly compute the spectral radius of a matrix, the common practice is to find an upper-bound of \u03c1(H\u0303). The spectral radius of a matrix is bounded by any sub-multiplicative matrix norm. As before, we use the infinity norm \u2016 \u00b7 \u2016def= \u2016 \u00b7 \u2016\u221e in this paper. We first consider the case for the standard Markov random walk (i.e., m = 1), where H\u0303i,j = H2i,j/P (1) i,j . The following lemma [12] provides insight on how to assign the probability in terms of minimizing the norm.\nLemma 3.1. Let h = (h1, h2, \u00b7 \u00b7 \u00b7 , hn)T be a vector where at least one of its elements is non-zero: hk 6= 0 for some k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}. Let p = (p1, p2, \u00b7 \u00b7 \u00b7 , pn)T be a probability distribution vector. Then \u2211n i=1 h 2 i /pi \u2265 (\u2211n i=1 |hi| )2 , and the lower-bound is attained when pi = |hi|/ \u2211n r=1 |hr|.\nAccording to Lemma 3.1, the infinity norm:\n\u2016H\u0303\u2016 = max 1\u2264i\u2264n n\u2211 j=1 |H\u0303i,j | = max 1\u2264i\u2264n n\u2211 j=1 H2i,j P (1) i,j \u2265 max 1\u2264i\u2264n ( n\u2211 j=1 |Hi,j | )2 = (\u2016H\u2016)2.\nWhen P (1)i,j = |Hi,j |\u2211n\nk=1 |Hi,k| for all i, j = 1, 2, \u00b7 \u00b7 \u00b7 , n, the above lower-bound (\u2016H\u2016)2 is reached, making\nthis choice in some sense optimal. However, for a variety of problems, this choice is unlikely to result in a method that will have \u03c1(H\u0303) < 1. For a linear system Ax = b, we can rewrite it into x = Hx + b as H = I \u2212A. It is common to have \u03c1(H) be very close to 1 even with the help of preconditioners [3]. Since the infinity norm is generally a loose upper-bound for the spectral radius, \u2016H\u2016 > 1 is likely [3]. This inability of upper-bounding the spectral radius \u03c1(H\u0303) for the standard Markov random walk encourages us to explore the multi-way generality.\nWe describe the method for computing P for an m\u2212way Markov random walk in Algorithm 1, then we prove in Theorem 3.2 that it minimizes \u2016H\u0303\u2016.\nData: matrix H Result: transition hypermatrix P initialization \u03c9i = 1 for i = 1, 2, \u00b7 \u00b7 \u00b7 , n; for k = m : 1 do\n\u03b7i = \u2211n\n`=1 \u03c9`|Hi,`| for i = 1, 2, \u00b7 \u00b7 \u00b7 , n; P\n(k) i,j = \u03c9j |Hi,j |/\u03b7i for i, j = 1, 2, \u00b7 \u00b7 \u00b7 , n; \u03c9i = \u03b7i for i = 1, 2, \u00b7 \u00b7 \u00b7 , n\nend Algorithm 1: Compute transition hypermatrix\nIt is worth noting that Algorithm 1 only takes linear time in the number of non-zeros in the matrix H in each iteration. Also the output result of the transition hypermatrix is compatible with different values of m, which means that m does not need to be pre-selected to run the algorithm. In other words, we can stop the iteration anytime we want and still get the output hypermatrix for some smaller m. This is useful when we later discuss how to choose the value of m, as it turns out that we can set a criterion to stop the iteration. Lastly we see that the output transition hypermatrix P is only determined by H . So the procedure of computing P is similar to loading the matrix into the memory as they both only need to be done once for different problems (i.e., different h and b). On the other hand, this means that we need global computation to compute this sequence and this choice prohibits a purely local algorithm.\nTheorem 3.2. Let P be the output of Algorithm 1, then H\u0303 defined in Theorem 2.2 has reached its minimal infinity norm.\nProof. We use matrices P (i) for i = 1, 2, \u00b7 \u00b7 \u00b7 ,m to denote matrix slices of hypermatrices P from the output of Algorithm 1, and \u03b7(k)i for the value of \u03b7i at the kth iteration.\nWe first prove that the value of \u2016H\u0303\u2016 cannot be further decreased by changing P (m). Since H\u0303 is a nonnegative matrix, \u2016H\u0303\u2016 = \u2016H\u0303e\u2016 holds, where e \u2208 Rn and ei = 1 for all i = 1, 2, \u00b7 \u00b7 \u00b7 , n. The kth element of H\u0302 (m) e is \u2211n j=1H 2 k,j/P (m) k,j , and according to Lemma 3.1, this value is minimized\nwhen P (m)k,i = |Hk,i|/ \u2211n j=1 |Hk,j |, which is exactly the kth row of P (m) from the algorithm. Thus by changing P (m), we cannot decrease any elements of vector H\u0302 (m)\ne, and \u2016H\u0303e\u2016 will not decrease. Second we prove that ( (\u03b7\n(m) 1 ) 2, (\u03b7 (m) 2 ) 2, \u00b7 \u00b7 \u00b7 , (\u03b7(m)n )2 )T = H\u0302 (m) e. Because P (m) is constructed as\nP (m) k,i = |Hk,i|/ \u2211n j=1 |Hk,j |, which means the kth element of H\u0302 (m) e is ( \u2211n j=1 |Hk,j |)2 = (\u03b7 (m) k )\n2. Lastly we use mathematical induction and assume that we cannot decrease \u2016H\u0303\u2016 by changing\nP (`), and ( (\u03b7\n(`) 1 ) 2, (\u03b7 (`) 2 ) 2, \u00b7 \u00b7 \u00b7 , (\u03b7(`)n )2 )T = H\u0302 (`) \u00b7 \u00b7 \u00b7 H\u0302(m)e for r + 1 \u2264 ` \u2264 m. Then similarly we\nprove that the statement holds for P (r). We notice that the kth element of H\u0302 (r) H\u0302 (r+1) \u00b7 \u00b7 \u00b7 H\u0302(m)e is\n\u2211n j=1(Hk,j\u03b7 (r+1) j ) 2/P rk,j and it is minimized because P (r) is computed as\nP (r) i,j = \u03b7 (r+1) j |Hi,j |/ n\u2211 k=1 \u03b7 (r+1) k |Hi,k|. (10)\nSo no elements of the vector H\u0302 (r) H\u0302 (r+1) \u00b7 \u00b7 \u00b7 H\u0302(m)e will decrease in value and neither will norm \u2016H\u0303\u2016 if we change P (r). From formula (10) we can compute the kth element of H\u0302 (r) H\u0302\n(r+1) \u00b7 \u00b7 \u00b7 H\u0302(m)e as ( \u2211n\nj=1 \u03b7 (r+1) j |Hk,j |)2 = (\u03b7 (r) k ) 2. So we have proved that this induction statement also holds for ` = r. In conclusion the output hypermatrix P from Algorithm 1 will ensure \u2016H\u0303\u2016 to be minimized.\nThe standard 1\u2212way method can also be viewed as a special case of m\u2212way random walk, with the m transition matrices being the same. However the 1\u2212way method generally does not minimize \u2016H\u0303\u2016 in the m\u2212way setting as Algorithm 1 minimizes \u2016H\u0303\u2016. Formula (3) from Theorem 2.2 indicates the connection between the variance and the power series of H\u0303. Since \u2016H\u0303\u2016 is an upper-bound of \u03c1(H\u0303) and \u03c1(H\u0303) affects how big this power series will grow, we can see that the m\u2212way random walk with transition hypermatrix defined from Algorithm 1 has the tendency to decrease the variance compared to the standard 1\u2212way method. Although the above analysis does not ensure a smaller variance for the m\u2212way method, numerical experiments in both synthetic matrices and matrices in real applications support this conjecture. (See Section 4).\nNext we move to see how the spectral radius \u03c1(H\u0303) is related to the matrix H . In order to bound \u03c1(H\u0303), the standard Markov random walk requires \u2016H\u2016 < 1, which does not happen often from our early analysis. The following theorem states the necessary and sufficient condition for a m\u2212way Markov random walk to have \u03c1(H\u0303) < 1.\nTheorem 3.3. Let H+ denote the nonnegative matrix where H+i,j = |Hi,j |. There exists a m\u2212way Markov random walk transition hypermatrix P such \u2016H\u0303\u2016 < 1 if and only if \u03c1(H+) < 1.\nProof. If there exists a m\u2212way Markov random walk transition hypermatrix P such that \u2016H\u0303\u2016 < 1, without a loss of generality we assume P is the output from Algorithm 1 since Theorem 3.2 states that it minimize \u2016H\u0303\u2016. From the proof of Theorem 3.2 we have:\n\u2016H\u0303\u2016 = \u2016H\u0303e\u2016 = \u2016H\u0302(1)H\u0302(2) \u00b7 \u00b7 \u00b7 H\u0302(m)e\u2016 = \u2016 ( (\u03b7\n(1) 1 ) 2, (\u03b7 (1) 2 ) 2, \u00b7 \u00b7 \u00b7 , (\u03b7(1)n )2 )T \u2016. (11)\nAccording to the computing procedure of Algorithm 1 we have (\u03b7(`)1 , \u03b7 (`) 2 , \u00b7 \u00b7 \u00b7 , \u03b7 (`) n )T = (H +)me. So \u2016H\u0303\u2016 < 1 =\u21d2 \u2016(H+)me\u2016 < 1 =\u21d2 \u2016(H+)m\u2016 < 1 =\u21d2 \u03c1(H+) < 1.\nIf we have \u03c1(H+) < 1, from Gelfand\u2019s Formula, we have \u03c1(H+) = limk\u2192\u221e \u2016(H+)k\u2016 1/k. Then we can find a sufficient large number m s.t. for any k \u2265 m the inequality \u2016(H+)k\u20161/k < 1 holds. Let H\u0303 be the matrix based on the transition hypermatrix output from Algorithm 1. Based on the observation of (11), we have \u03c1(H+) < 1 =\u21d2 \u2016(H+)m\u2016 < 1 =\u21d2 \u03b7(1)i < 1, i = 1, 2, \u00b7 \u00b7 \u00b7 , n =\u21d2 \u2016H\u0303\u2016 < 1\nTheorem 3.3 creates an equivalent link between \u03c1(H+) < 1 and existence of m\u2212way Markov random walk such that \u2016H\u0303\u2016 < 1. However it does not guarantee the size of m. In another words one can always cook up some matrix H with \u03c1(H+) < 1 but make m arbitrarily large. Although these extreme cases are not our primary focus in this paper, we point it out for the discussion of the practical implementation of Algorithm 1. In order to find the transition hypermatrix P with \u2016H\u0303\u2016 < 1, we can set a threshold number \u03c6max, and let m grow until we have \u03b7i < 1 for all i = 1, 2, \u00b7 \u00b7 \u00b7 , n or m = \u03c6max. As stated before, we do not need to re-run the algorithm for different value of m, because the way Algorithm 1 computes the transition hypermatrix is compatible with different values of m."}, {"heading": "3.2 Random Walk Error Analysis", "text": "To practically estimate the value \u3008h,x\u3009 from simulating the value of Z, we need to truncate the multi-way Markov random walk in order for it to end after some large number of steps N . The practical solution [7, 3] to determine N is through the criterion: |WN | \u2264 |W0| where > 0 denotes some small number. For the case that the initial probability pi = |hi|/ \u2211n j=1 |hj |, we have W0 = \u2016h\u2016.\nWe notice thatWN is a random variable, and follow the similar analysis with that in Theorem 2.1, it is easy to see its expected value is \u3008h, (H+)Ne\u3009. So \u03c1(H+) < 1 is a necessary condition in order to determine the truncation number N . Here we can see that our m\u2212way Markov random walk has the minimal requirements on H , because \u03c1(H+) < 1 is required for all the Monte Carlo frameworks to be able to truncate the random walk, and yet we show that under this condition, our algorithm can always find a m\u2212way transition Hypermatrix to ensure \u2016H\u0303\u2016 < 1.\nThe following theorem justifies that the truncation procedure has little effect on the estimation result or the variance of the variable.\nTheorem 3.4. Let ZN denote the truncation value of Z after N steps of the random walk. Formally ZN = \u2211N `=0W`bk` with W`, ` = 0, 1, 2, \u00b7 \u00b7 \u00b7 , N defined in equation (2). If \u2016H\u0303\u2016 < 1 then ZN converges in probability to Z: ZN p\u2212\u2192 Z, and Var[ZN ] converges to Var[Z] as N \u2192\u221e.\nProof. From the definition of the variable Z = limN\u2192\u221e ZN , the conclusions can be easily verified.\nIn addition to the truncation, another error comes from the simulation procedure when using the empirical mean value of Z to estimate E[Z], formally we define the probable error as:\nr = sup { s : Pr ( |Z\u0304 \u2212 E[Z]| \u2265 s ) > 1\n2 } where Z\u0304 = \u2211M i=1 Z\n(i)/M denotes the mean value of M simulations Z(1), Z(2), \u00b7 \u00b7 \u00b7 , Z(M). There is a close link between the probable error and the variance of the random variable.\nAccording to Central Limit Theorem\n\u221a M ( |Z\u0304 \u2212 Z| ) d\u2212\u2192 N (0,Var[Z]) where N ( 0,Var[Z] ) denotes the normal distribution with zero mean and variance Var[Z], and the\nsymbol d\u2212\u2192 means convergence in distribution. When M is sufficiently large, r \u2248 0.6745 \u221a\nVar[Z]/M . The probable error is determined by the ratio of the variance to the number of simulations. If the variance is decreased by \u03be times, then it only require \u03be times fewer number of simulations to get to the same precision (i.e., probable error).\nBased on the above observations we can conduct numerical experiments to compare the variance between the standard Monte Carlo method and our multi-way Monte Carlo method, and the ratio between the variance can demonstrate how many times faster our new method can get."}, {"heading": "4 Numerical Experiments", "text": "In this section, we conduct numerical experiments to demonstrate the two key improvements from our new method 1. In section 4.1 we show that our new method can be applied solve more problems than the standard method. In section 4.2 we show that our new method can achieve a considerable\n1Codes for this paper are available at https://github.com/wutao27/multi-way-MC\nspeed-up. The testing methods are the standard 1\u2212way method, and our multi-way methods with m = 2, 3, 4, 5. In both experiments, synthetic matrices and real-world matrices are used.\nFor synthetic data, we generate the matrix H via Matlab command sprand(1000, 1000, 0.2), which outputs a 1, 000 by 1, 000 matrix with 0.2 of its entries being non-zeros, and each non-zero is a random number following uniform distribution between (0, 1). The synthetic matrices are rescaled to reach certain spectral radius required during experiments. Formally to get a spectral radius 0 < r < 1: H \u2190 rH/\u03c1(H). Each result is the average over 100 trials for the related problems.\nFor real world matrices, we focus on the Harwell-Boeing sparse matrix collection [9, 4]. The matrix H is constructed by a simple diagonal precondition on the original matrix A from the collection: H = I\u2212Diag(A)\u22121A. And for the test problems we only consider the matrices that have \u03c1(H+) < 1. In the interest of simplicity, we only use problems with fewer than 5,000 dimensions.\nIn both synthetic and real world experiments, vectors b,h are randomly generated with elements following uniform distribution between (0, 1)."}, {"heading": "4.1 The Number of Solvable Problems", "text": "We define the solvable problems as those with \u2016H\u0303\u2016 < 1, which is a sufficient condition that guarantees the convergence of the Monte Carlo methods. The ratio of solvable problems is the percentage of random problems that are solvable. Figure 1 shows the results as we vary the spectral radius. As we can see our multi-way methods can solve more problems than the standard method, which cannot guarantee any convergences when \u03c1(H+) \u2265 0.85. And when m increases, even more problems are solvable. We also find several real world matrices where the standard method fails to guarantee convergence but ours can. They are matrices fs_760_1, jpwh_991, nos7 from the Harwell-Boeing Collection and add32 from Hamm matrix group2."}, {"heading": "4.2 Algorithm Efficiency", "text": "We apply the conclusion in Theorem 3 to compute Var[Z] for all the testing methods. Then we compare Var[Z] for different methods. Formally we define speed-up times as Var[X]/Var[Z], where X and Z denote the variable from the standard 1\u2212way method and our method respectively. The\n2http://www.cise.ufl.edu/research/sparse/matrices/Hamm/index.html\nspeed-up times is an indicator for how much times faster our multi-way methods can get compared to the standard 1\u2212way method. Table 1 shows that we have considerable speed-up when applying our multi-way methods. Note that we only consider the problems with \u03c1(H\u0303) < 1 in order for Var[Z] <\u221e. In the Harwell-Boeing collection there are a few problems having H+e equal for each element, we exclude these matrices because the multi-way method is equivalent to the standard method, as we now show.\nWhen H+e is a vector with its elements being the same number, that is, H+e = \u03b3e, the multi-way method is equivalent to the standard method. This occurs because, as in Algorithm 1, the vectors \u03b7 = (H+)ke and \u03c9 = (H+)k\u22121e will have their elements be the same. So the transition matrices P (i) for i = 1, 2, \u00b7 \u00b7 \u00b7 ,m will also be the same. In our experiments on the Harwell-Boeing collection, we do find several matrices that have this property, so in this case our multi-way method is equivalent to the standard method.\nAmong our testing problems, there is only one 3 that our multi-way method can have a larger variance than that of the standard method. Actually we find the matrix H for this problem is outside our assumptions in this paper. We assume that H does not have zero row in order to assign transition probabilities for each state. For the corner case that H does have zero rows, the linear system can be easily adjusted by deleting the zero rows of H . For this testing problem H, the row sums of H+ distribute in a drastic way. Over half of the rows have sum values between 10\u221217 to 10\u22126, and quite a few \u201cbig\u201d rows have sums larger than 103. So this matrix have many rows that are nearly zero. For all the other testing problems, our multi-way method can achieve smaller variances than the standard method, and the speed-up times in shown in Table 1."}, {"heading": "5 Conclusion", "text": "In this paper we studied a generalization of Monte Carlo methods for linear systems. The generalization allows the Markov random walk to transition using a set of matrices. We derived the variance of the resulting estimator and construct the matrices in a way to attempt to produce a finite variance. The advantages of this new random walk procedures are two-fold. First it can solve more problems that the standard method fails to solve. Second our new method has the tendency to decrease the variance thus decrease the computations needed for estimate the solution. Numerical experiments on both synthetic and real world matrices confirm the superiority of our method in the above two aspects when comparing to the standard Monte Carlo method. An open problem suggested by our work is to get a purely local method that avoids the global work in building the sequence of adjacency matrices.\nAcknowledgements. This work was supported by NSF IIS-1422918, CAREER award CCF1149756, Center for Science of Information STC, CCF-093937; DOE award DE-SC0014543; and the DARPA SIMPLEX program."}], "references": [{"title": "Parallel hybrid Monte Carlo algorithms for matrix computations", "author": ["V. Alexandrov", "E. Atanassov", "I. Dimov", "S. Branford", "A. Thandavan", "C. Weihrauch"], "venue": "In International Conference on Computational Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Monte Carlo methods in PageRank computation: When one iteration is sufficient", "author": ["K. Avrachenkov", "N. Litvak", "D. Nemirovsky", "N. Osipova"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Analysis of Monte Carlo accelerated iterative methods for sparse linear systems", "author": ["M. Benzi", "T. Evans", "S. Hamilton", "M.L. Pasini", "S. Slattery"], "venue": "Technical Report Math/CS Technical Report TR-2016-002,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The university of florida sparse matrix collection", "author": ["T.A. Davis", "Y. Hu"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Scalar and parallel optimized implementation of the direct simulation Monte Carlo method", "author": ["S. Dietrich", "I.D. Boyd"], "venue": "Journal of Computational Physics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Parallel resolvent Monte Carlo algorithms for linear algebra problems", "author": ["I. Dimov", "V. Alexandrov", "A. Karaivanova"], "venue": "Mathematics and Computers in Simulation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "A new iterative Monte Carlo approach for inverse matrix problem", "author": ["I. Dimov", "T. Dimov", "T. Gurov"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "A new walk on equations Monte Carlo method for solving systems of linear algebraic equations", "author": ["I. Dimov", "S. Maire", "J.M. Sellier"], "venue": "Applied Mathematical Modelling,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Users\u2019 guide for the harwell-boeing sparse matrix collection", "author": ["I.S. Duff", "R.G. Grimes", "J.G. Lewis"], "venue": "(release i),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "A Monte Carlo syntheticacceleration method for solving the thermal radiation diffusion equation", "author": ["T.M. Evans", "S.W. Mosher", "S.R. Slattery", "S.P. Hamilton"], "venue": "Journal of Computational Physics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Sequential Monte Carlo techniques for the solution of linear systems", "author": ["J.H. Halton"], "venue": "Journal of Scientific Computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "Convergence analysis of Markov chain Monte Carlo linear solvers using Ulam-von Neumann algorithm", "author": ["H. Ji", "M. Mascagni", "Y. Li"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A parallel implementation of the direct simulation Monte Carlo method", "author": ["G. LeBeau"], "venue": "Computer Methods in Applied Mechanics and Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Parallel Monte Carlo Synthetic Acceleration methods for discrete transport problems", "author": ["S.R. Slattery"], "venue": "PhD thesis, University of Wisconsin Madison,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "A spectral analysis of the domain decomposed Monte Carlo method for linear systems", "author": ["S.R. Slattery", "T.M. Evans", "P.P. Wilson"], "venue": "Nuclear Engineering and Design,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Monte Carlo linear solvers with non-diagonal splitting", "author": ["A. Srinivasan"], "venue": "Mathematics and Computers in Simulation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "A Monte Carlo method for solving unsteady adjoint equations", "author": ["Q. Wang", "D. Gleich", "A. Saberi", "N. Etemadi", "P. Moin"], "venue": "Journal of Computational Physics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "A note on the inversion of matrices by random walks", "author": ["W. Wasow"], "venue": "Mathematical Tables and Other Aids to Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1952}], "referenceMentions": [{"referenceID": 17, "context": "The Monte Carlo method [18] for solving a linear system uses a random walk to approximate the solution.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "First, the Monte Carlo method can be highly effective when only modest accuracy is required, as is common for many problems on data such as PageRank computations [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 12, "context": "Second, it is well-known that Monte Carlo algorithms are highly parallelizable [13, 5], thus they are ideal for modern paralleled computers or clusters.", "startOffset": 79, "endOffset": 86}, {"referenceID": 4, "context": "Second, it is well-known that Monte Carlo algorithms are highly parallelizable [13, 5], thus they are ideal for modern paralleled computers or clusters.", "startOffset": 79, "endOffset": 86}, {"referenceID": 16, "context": "Third, Monte Carlo methods can identify only on a single component or a linear form of the solution, which is often all that is required in many applications [17].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "Last but not least, Monte Carlo methods have advantages when dealing with large linear systems [11, 7] because they do not always require a full solution vector.", "startOffset": 95, "endOffset": 102}, {"referenceID": 6, "context": "Last but not least, Monte Carlo methods have advantages when dealing with large linear systems [11, 7] because they do not always require a full solution vector.", "startOffset": 95, "endOffset": 102}, {"referenceID": 6, "context": "Then it can be shown (for instance [7]) that E[X] = \u3008h,x\u3009, and more specifically E[W`fk` ] = \u3008h,H f\u3009.", "startOffset": 35, "endOffset": 38}, {"referenceID": 11, "context": "However the random walk model does not guarantee convergence [12, 18].", "startOffset": 61, "endOffset": 69}, {"referenceID": 17, "context": "However the random walk model does not guarantee convergence [12, 18].", "startOffset": 61, "endOffset": 69}, {"referenceID": 11, "context": "Empirical studies [12, 3] show that it is easy to have Var[X] =\u221e even when the Neumann Series converges (i.", "startOffset": 18, "endOffset": 25}, {"referenceID": 2, "context": "Empirical studies [12, 3] show that it is easy to have Var[X] =\u221e even when the Neumann Series converges (i.", "startOffset": 18, "endOffset": 25}, {"referenceID": 7, "context": "2 Our Contributions In order to apply the Monte Carlo method, existing work [8, 16, 17] assumes \u2016H\u2016 < 1 (for the infinity norm \u2016H\u2016 = maxi \u2211 j |Hi,j |), which suffices to show Var[X] <\u221e, but which is a stronger condition than \u03c1(H) < 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 15, "context": "2 Our Contributions In order to apply the Monte Carlo method, existing work [8, 16, 17] assumes \u2016H\u2016 < 1 (for the infinity norm \u2016H\u2016 = maxi \u2211 j |Hi,j |), which suffices to show Var[X] <\u221e, but which is a stronger condition than \u03c1(H) < 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 16, "context": "2 Our Contributions In order to apply the Monte Carlo method, existing work [8, 16, 17] assumes \u2016H\u2016 < 1 (for the infinity norm \u2016H\u2016 = maxi \u2211 j |Hi,j |), which suffices to show Var[X] <\u221e, but which is a stronger condition than \u03c1(H) < 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 7, "context": "At each step of the random walk, the transition matrix is constructed in a way akin to the Monte Carlo Almost Optimal (MAO) framework [8, 12].", "startOffset": 134, "endOffset": 141}, {"referenceID": 11, "context": "At each step of the random walk, the transition matrix is constructed in a way akin to the Monte Carlo Almost Optimal (MAO) framework [8, 12].", "startOffset": 134, "endOffset": 141}, {"referenceID": 15, "context": "Direct methods study the various techniques of using the Monte Carlo solvers themselves, for instances non-diagonal splitting [16] and relaxation parameters [7].", "startOffset": 126, "endOffset": 130}, {"referenceID": 6, "context": "Direct methods study the various techniques of using the Monte Carlo solvers themselves, for instances non-diagonal splitting [16] and relaxation parameters [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 10, "context": "Hybrid methods [11, 10, 1] use Monte Carlo as a black box combined with iterative techniques.", "startOffset": 15, "endOffset": 26}, {"referenceID": 9, "context": "Hybrid methods [11, 10, 1] use Monte Carlo as a black box combined with iterative techniques.", "startOffset": 15, "endOffset": 26}, {"referenceID": 0, "context": "Hybrid methods [11, 10, 1] use Monte Carlo as a black box combined with iterative techniques.", "startOffset": 15, "endOffset": 26}, {"referenceID": 10, "context": "Examples of these works are Sequential Monte Carlo method [11] and synthetic-acceleration method [10].", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "Examples of these works are Sequential Monte Carlo method [11] and synthetic-acceleration method [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 5, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 67, "endOffset": 77}, {"referenceID": 13, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 67, "endOffset": 77}, {"referenceID": 0, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 67, "endOffset": 77}, {"referenceID": 16, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 102, "endOffset": 109}, {"referenceID": 1, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 102, "endOffset": 109}, {"referenceID": 11, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "The following lemma [12] provides insight on how to assign the probability in terms of minimizing the norm.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "It is common to have \u03c1(H) be very close to 1 even with the help of preconditioners [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Since the infinity norm is generally a loose upper-bound for the spectral radius, \u2016H\u2016 > 1 is likely [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "The practical solution [7, 3] to determine N is through the criterion: |WN | \u2264 |W0| where > 0 denotes some small number.", "startOffset": 23, "endOffset": 29}, {"referenceID": 2, "context": "The practical solution [7, 3] to determine N is through the criterion: |WN | \u2264 |W0| where > 0 denotes some small number.", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "For real world matrices, we focus on the Harwell-Boeing sparse matrix collection [9, 4].", "startOffset": 81, "endOffset": 87}, {"referenceID": 3, "context": "For real world matrices, we focus on the Harwell-Boeing sparse matrix collection [9, 4].", "startOffset": 81, "endOffset": 87}], "year": 2016, "abstractText": "We study the Monte Carlo method for solving a linear system of the form x = Hx + b. A sufficient condition for the method to work is \u2016H\u2016 < 1, which greatly limits the usability of this method. We improve this condition by proposing a new multi-way Markov random walk, which is a generalization of the standard Markov random walk. Under our new framework we prove that the necessary and sufficient condition for our method to work is the spectral radius \u03c1(H) < 1, which is a weaker requirement than \u2016H\u2016 < 1. In addition to solving more problems, our new method can work faster than the standard algorithm. In numerical experiments on both synthetic and real world matrices, we demonstrate the effectiveness of our new method. ar X iv :1 60 8. 04 36 1v 1 [ cs .N A ] 1 5 A ug 2 01 6", "creator": "LaTeX with hyperref package"}}}