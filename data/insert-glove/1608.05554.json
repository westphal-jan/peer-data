{"id": "1608.05554", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Learning to Start for Sequence to Sequence Architecture", "abstract": "The sequence wellsprings to :) sequence katas architecture is phonogram widely used weigman in co-owners the response chaliyar generation inicio and seifzadeh neural machine translation fox-owned to model 40.82 the potential cissoko relationship between two darzab sentences. It delbarton typically taylorcraft consists placegetters of bapat two cromdale parts: an theophoric encoder rosily that reads mackle from the draken source sentence adila and a decoder kidnapping that generates renew the target rugger sentence ingjald word vigilia by word husserl according handspring to the 700-point encoder ' m\u00e4r s output tutko and leandra the noncombatant last generated tullie word. himeji However, it faces to the seraf\u00edn cold arenys start \u00feorsteinn problem berro when generating sulpicians the tresgrandas first granting word as aeroportos there is priklopil no procedurally previous resoluteness word jer\u00f3nimo to refer. matthiesen Existing work mainly circulant use a kr\u00e1l special visy start symbol & byu-idaho lt; / whole-heartedly s & bakekang gt; takugin to ebersberg generate polan\u00f3w the first word. An 156-man obvious timelier drawback of these chemtura work veniamin is aldar that there hemis is not dekha a 100k learnable 38.04 relationship 15-to-20 between relying words and the start symbol. Furthermore, it asfaw may lochalsh lead to thornalley the error accumulation for decoding when 38a the first word is incorrectly exigency generated. begam In this paper, 40104 we symphoricarpos proposed narcotraffickers a dokki novel ringstead approach to learning to generate the cordiality first accumulate word in the muttaleb sequence toktogul to rahv sequence theed architecture rather than using the start vagni symbol. shama Experimental frel results plethodontidae on bromford the task of westerlo response scheps generation of gitega short ryberg text conversation show greenburg that waray the winces proposed garros approach outperforms snapfish the state - statt of - the - art bokhara approach in icons both of ohs the cchr automatic and 853,000 manual evaluations.", "histories": [["v1", "Fri, 19 Aug 2016 09:48:13 GMT  (231kb,D)", "http://arxiv.org/abs/1608.05554v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qingfu zhu", "weinan zhang", "lianqiang zhou", "ting liu"], "accepted": false, "id": "1608.05554"}, "pdf": {"name": "1608.05554.pdf", "metadata": {"source": "CRF", "title": "Learning to Start for Sequence to Sequence Architecture", "authors": ["Qingfu Zhu", "Weinan Zhang", "Lianqiang Zhou", "Ting Liu"], "emails": ["qfzhu@ir.hit.edu.cn", "wnzhang@ir.hit.edu.cn", "tliu@ir.hit.edu.cn", "tomcatzhou@tencent.com"], "sections": [{"heading": "1 Introduction", "text": "Recently, the sequence to sequence(Seq2Seq) architecture has gained great development as a general neural network method to model the potential relationship between two sequences. For the basic Seq2Seq model, each sequence is usually modeled by RNN, and the two RNNs for the source sequence and target sequence are called encoder and decoder respectively. The encoder reads from the source sentence and do some summarize. The decoder is actually a language model that produce words according to previously predicted words conditioned with the encoder\u2019s output(usually called the context vector). This indicates that when the decoder try to predict a word, the context vector and the word predicted at previous time are two necessary inputs that requires.\nSo here comes the initialization question: when producing the first word by the decoder, there is no previous predicted word to be referenced to. Typically, previous work use a start symbol \u201c</s>\u201d to generation the first word (Sutskever et al., 2014). While it is not suitable to introduce a start symbol as the first word varies from different sentences. Concretely, there is not a learnable conditional probability of words given start symbol. Meanwhile, the process of producing the first word and generating the rest words of a sentence are different so that they should be handled respectively. To address this issue, we proposed a novel approach to learning to generate the first word. In detail, we find two factors that impact the encoding and decoding process: one is the source sequence which can be expressed using the encoder\u2019s states. The other is the representation of candidate words, of which information are all contained in the embedding matrix. We thus introduce these variables to map the representation of the source sentence into a probability distribution over the word table, pick up the maximal dimension as the final result.\nThe contribution of this paper is as follows:\n\u2022 To the best of our knowledge, we are the first to proposed a novel approach to learning to generate the first word in Seq2Seq architecture.\nar X\niv :1\n60 8.\n05 55\n4v 1\n[ cs\n.C L\n] 1\n9 A\nug 2\n01 6\n\u2022 The proposed approach outperforms the state-of-the-art on the response generation of the short text conversation.\n\u2022 Besides the short text conversation task, the proposed approach is a general framework which can also adapt to other Seq2Seq learning applications."}, {"heading": "2 Background", "text": "From the perspective of probability, the Seq2Seq model maximize the probability of the target sequence conditioned with the source sequence during the training process, and search for a sequence that have a maximal conditioned probability given the source sequence during the predicting process. Due to that highly abstract attribute, lots of tasks such as Response Generation,Machine Translation and Question Answering can all be modeled using that architecture."}, {"heading": "2.1 RNN encoder-decoder", "text": "Typically, a sequence to sequence model consists of two parts: encoder and decoder, both of which are often implemented using a family of RNN, such as GRU (Cho et al., 2014a; Chung et al., 2014) and LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Graves, 2012a), so a seq2seq model is also called RNN encoder-decoder architecture.\nThe encoder is a normal RNN, which reads from a sequence of words and outputs their hidden states.These states are also called annotations denoted by H , and for each hidden state hi at time i, it is computed by its previous hidden state hi\u22121 and the word at time t:\nht = f(ht\u22121, xt); H = {h1, h2, h3, ..., hT } (1)\nHere, T is the length of the source sequence, f is a non-linear function. After that, the encoder computes a distributed representation using these hidden states as a summary(context vector) of the input sequence. The most simplest way is directly fetching the last one:\nc = q({h1, h2, h3, ..., hT }) (2)\nFor the decoder, the hidden state\u2019s calculation is quite similar, the only difference is that the sequence input xt is replaced by the word predicted at last time:\nst = f(st\u22121, yt\u22121) (3)\nIt should be noted that, the context vector c is used to initialize the hidden state of decoder to make sure that the decoder was conditioned with the encoder. Based on that, (Cho et al., 2014b) add the vector c as an extra input into the computation of the hidden state in decoder to make sure that every time step of the decoder can get full information of the context. In that way, the formula 3 should be updated to:\nst = f(st\u22121, yt\u22121, c) (4)\nThen, the word at time t can be predicted by mapping the st to a probability over the word table using the maxout activation(Goodfellow et al., 2013)."}, {"heading": "2.2 Attention Mechanism in Seq2Seq", "text": "In the basic architecture of the sequence to sequence, source sequence sent to the encoder is encoded into a dense, fixed-length vector. Considering that vector may not be able to contain all the useful information of the source sequence, thus becoming a bottleneck of the model,Bahdanau et al. (2014) add the attention mechanism to improve the Seq2Seq\u2019s performance.Compared with the basic architecture, which use the last hidden state as the context vector c, attention mechanism gives a weight to all the annotations, then use them to calculate a weighted sum as a new context vector. It should be noted that, in that way, the\nvector c is distinct for every time step in the decoder, because a time-related variable was involved during the computing, so here we denote the result as vector cj .\ncj = T\u2211 i=1 \u03b1ijhi (5)\nHere, cj is the context vector when we decode the j-th word in the decoder, and the weight \u03b1ij for the i-th annotation of encoder is computed by:\n\u03b1ij = a(sj\u22121, hi) (6)\nwhere a is a forward neural network. Intuitively, the vector sj\u22121 contains the context information of the response, so Formula 6 can be understanded as to calculate the similarity between that context and these encoder annotations, which can be also regarded as a weight."}, {"heading": "2.3 Initialization in Seq2Seq Learning", "text": "Initialization is such a small detail that can be ignored easily,sometimes. However, it is an important part of the model. In the encoder RNN, a state will be used to compute the state at next time(see Eq.1), and by this way, the initial state will have an indirectly influence on all the states next. The decoder RNN share the same situation. In addition, the decoder has an extra variable that should be initialized: predicted word at last time step, because we don\u2019t have that input for the first process of generation. Typically, we set the initial hidden state of encoder to an all zero vector, and people usually use the last hidden state of the encoder to initialize the decoder\u2019s first hidden states:\ns0 = \u03c3(WshT ) (7)\nwhere \u03c3 is a non-linear function, Ws is a trainable parameter. That is intuitively plausible because it describes the relation between the two sequences that the decoder is conditioned with the encoder. As to the previous generated word for first generation in the decoder, we manually set a start symbol to act as that role."}, {"heading": "3 Learning to start", "text": "In this section, we propose a new model to accomplish Seq2Seq\u2019s the initial prediction. We think that the method using a start symbol to predict the first word is not very suitable. First, the decoder RNN is essentially a language model (Mikolov et al., 2010), which use the previous predicted words to predict a new word, from the perspective of probability, it learns a conditional probability of word that given last predicted words. While the start symbol and the first predicted word do not have such association, because most words can be put at the first position of a sequence, there is not a learnable conditional probability, so the result of taking a start symbol may cause the model prefer to predict some high frequency words, which is also observed during other conversation models using this architecture (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015). We think the reason may lie in training samples started with these words takes a higher proportion, making the decoder learn a conditional probability that given the start symbol, these words\u2019 probabilities should be higher than others. Second, we suppose the process of predicting the first word and predicting a word according to its previous word should not be treated identically, using a same method to do the two works may not be a good choice. Third, the start symbol is involved in the calculation of decoder\u2019s hidden state(see Eq. 4), so introducing a start symbol that irrelevant of a sequence and no difference between all the source sequences may indirectly influence the prediction of the rest time steps.\nSo we propose a new method to relive the decoder from both predicting the first word and predicting word according to the last predicted word. In our model, the first word is predicted independently from the decoder. Inspired by the initialization of the hidden state of the decoder, we use the hidden state of encoder to calculate the probability of the first word using the formula below:\ny0 = \u03c3((\u03c3(Wic) + bi)E + be) (8)\nIn this formula, vector c is the context vector, here we directly use the last hidden state of the encoder, Wi is a matrix that can be trained in the model, E is the embedding matrix of the decoder, and bi, be are bias vectors, \u03c3 is a no-linear activation function, so in form, the formula is equal to below if we ignore all the bias:\ny0 = g(c, E) (9)\nIntuitively, this formula build a tie between the context vector and the embedding matrix of the decoder. The former contains information of the source sequence, and the latter contains information of all the candidate words to be predicted in the decoder. So the Wi matrix can be regarded as a similarity matrix used to compute the probability that how similar a word is to the source sequence, which indicates whether it is suitable to be predicted as the first word. Besides, by doing like this, the generation of the first word is decided only by the encoder\u2019s state. And without a start symbol\u2019s influence, the encoder\u2019s state can also be transferred to the decoder without any loss. And the rest process of prediction remains the same to the basic structure."}, {"heading": "4 Experiment Settings", "text": "To verify the effectiveness, our proposed model were tested in the task of response generation of short text conversation. As a kind of neural machine architecture, a big-data is always required to get a good performance. To achieve that, a dialogue set was crawled as the training set. And to be compared with, a basic kind of Seq2Seq architecture for response generation called hybrid model proposed by Shang et al. (2015) was implemented."}, {"heading": "4.1 Data", "text": "For the training process, Some one-round dialogue pairs was crawled from the Internet. For convenience, first sentence and the second sentence of one dialogue pair are denoted as post and response(Shang et al., 2015)respectively. The data set contains one million pairs, and about 35 thousands words. It should be noted that compared with the data used by Shang et al. (2015), this crawled data is a one-to-one data set, one post is corresponding to exact one response. While in the Shang et al. (2015) paper, they crawled some one-to-many data from microblog, then distributed all the responses to the its post. This is a creative way to build a big data set, while during our experiments, we found that the one-to-one data has a more rapid rate of convergence, so we created our own data set and trained models on it.Table 1 is an example of our data.\nAs for the test set, considering that one of our evaluation method\u2013Bleu, which will be introduced in detail in next section, should has more than one reference for every candidate, our one-to-one data is not very suitable, so we select 100 posts and their corresponding responses in Shang et al. (2015)\u2019s data-set to build our test set. Table 2 shows some statics of our the whole data set."}, {"heading": "4.2 Models", "text": "We trained two models. The first one is a basic Seq2Seq model for dialogue called Hybird Model(denoted as HYP)(Shang et al., 2015) , the other is what we have proposed(denoted as LTS). The encoder and decoder were both implemented using the GRU (Cho et al., 2014a; Chung et al., 2014). and we set our model\u2019s parameters reference to Shang et al. (2015). The hidden size in the encoder was set to 1024. And the embedding size was set to 500, all the embedding vectors were pre-trained using the training data (Mikolov et al., 2013a; Mikolov et al., 2013b).\nBesides, during the processing of training, we sent the data to the model using the mini-batch with a batch size of 100, and the RMSprop algorithm was used to update model\u2019s parameters. And we trained both models about 5 days. After that, we used the beam search algorithm to search for the N-best result of response for one paritular sentence (Graves, 2012b; Boulanger-Lewandowski et al., 2013)."}, {"heading": "5 Result", "text": "Until now, there is still not a uniform evaluation for response generation (Galley et al., 2015; Pietquin and Hastie, 2013; Schatzmann et al., 2005).First, we tested our model\u2019s performance using the some statistics of the the first predicted word. Second, we evaluate the complete response to see if our model can bring the Seq2Seq architecture improvement. to achieve that goal, we use two metrics: for one hand, we employed the wildly used automatic evaluation method\u2013blue (Papineni et al., 2002) in the area of machine translation, for the other hand, we employed the human annotation method."}, {"heading": "5.1 First Generated Word Evaluation", "text": "To evaluate the generation of the first word, two aspects are taken into consideration: the accuracy rate and the diversity. The dialogue pairs in the test set are denoted as test sample and reference respectively, and the sequence generated by the model is still denoted as response. As mentioned before, the test set is a one-to-many data set so each test sample corresponding to serval references. We define a set called Rset for every sample, each sample\u2019s R-set is composed by all the first words of that sample\u2019s references, during the test process, if the first word of sample\u2019s response fall into its R-set, then it will be marked as hit. And the accuracy is the ratio of hit samples over the whole test set. Furthermore, we considered such situation: some high-frequency words(derived from the training data) are so common that nearly all the R-sets consists at least one, so a sample will easily hit its R-set as well as its first generated word is such words, for example:\u2019I\u2019. So we further defined the accuracy without high-frequency words, denoted by accw-i,which takes such situation into consideration: if a hit word is one of top i high-frequency words, then this hit will be ignored. Particularly, accw-0 equals the basic accuracy that do not filter any high-frequency words.\nFrom the Figure 2, we can see that the LTS outperformed the HYP from the accw-2.We analyzed the results and find the most frequent word is a auxiliary word: \"\u4e86\", which seldom appear in the beginning of a sentence, so there is no change from the accw-0 to accw-1.When we ignored the second frequent word \"\u6211(I)\", the performance of the HYP descends rapidly,which can be observed from the accw-1 to accw-2. while the LTS has a more stable accuracy that do not depend the easily hit high-frequency words.\nAlso, we evaluate the initial prediction from the perspective of diversity. In fact the Table 2 has already reflected the diversity to some degree, which our model\u2019s stable accw-i shows that the generation of the first word is distributed fairly balance. While we still give another metric to evaluate it, we define the div-i which means the ratio of test samples whose response\u2019s first word fall into the top i frequent words.According to the definition we describe, we can see that the diversity declines with increasing div-i score.\nThe Figure 3 show us that rather than concentrate on some high-frequency words, our model prefer to predict more diversity ones. Noted that in HYP there is a sharp increasement from the div-1 to div-2, which indicates that lots of the samples\u2019s first generated word is the second frequent word, which also agrees the results of acc-i."}, {"heading": "5.2 Bleu Metric", "text": "We use this metric to evaluate the a model\u2019s complete response rather than the first word. which is proved to agree well with human judgement on response generation task (Sordoni et al., 2015; Li et al., 2015). And the result is given in the Table 3.\nFrom the Table 3, we can see that the LTS performs well than the HYP in Bleu-1 to Bleu-3. Through that table, we can also see that improvement on the Bleu-1 is not as significant as other two. We analyzed this situation and got an opinion, it may because the Bleu metric calculate overlap of n-grams between response and references, compared with other n-grams, the unigram is more easily to be matched making\nthe Bleu-1 not distinguished enough.Table 4 shows some results of the bleu evaluation, two models got similar Bleu-1 scores, while the Bleu-2 and Bleu-3 are much more strict metrics that can reflect the improvement more significantly, which also proves our analyzed mentioned before."}, {"heading": "5.3 Manual Evaluation", "text": "At the same time, we also tested our model adopting human annotation method. The evaluation metric is made reference to Shang (2015). We generated responses with HYP model and LTS respectively, then these responses together with their original questions are mixed up into a new file to make sure that labers can judge the result fairly. Three labers were involved to assigned a score to these responses in range of 0 to 2, and the score metric is as follows:\n0:This indicates a bad response. if a response has grammar,fluency mistakes, not logic consist or relevant with the original post in semantic, it should be assigned a 0 score.\n1:This means the response may not be a perfect one, but may be treated as a suitable response in some particular scenario, or it is a too general response, like \"I don\u2019t know\".\n2:This indicates a quite appropriate response, a response can be sorted to this category only when it is free of grammar and fluency errors and is independent of scenario.\nTable 5 show the annotation metric in more detail. The example one conflicts with the logic consistency principle, the post said he got a cough, while the response advised the antipyretics, which is not logical relevant. The example two\u2019s response is not semantic relevant to its post, so they got a score of zero. In the example three, the response can be seen as a suitable one but it is too general, so it only got a score of one. While the example four got the same score in a different way that its response strongly depend on a particular scenario that the author of the post must has exactly two dictionaries. The last example show a suitable response that free of the questions that mentioned before.\nThe human annotation result was used to compute these metrics for the two model respectively: mean score, ratios of different categories. From the table 4, we can see that the LTS outperforms the HYP in\nall metrics. Besides, we evaluated the consistent of different labelers using the Fleiss\u2019 kappa, which is listed the Agreement column. We can see that both of the two models\u2019 agreement all fall into the range of 0.2-0.4, which indicates that the result is a fair agreement one."}, {"heading": "6 Related Work", "text": ""}, {"heading": "6.1 Sequence to sequence for Machine Translation", "text": "Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al. (2014b),Gao et al. (2014). Besides, Cho et al. (2014b) added the vector c as an extra input to every time step of the decoder, by doing like this, all the steps not only the first one, can get full information of the context vector. Furthermore, Bahdanau et al. (2014) proposed a novel method to calculated a weighted sum of all the annotations of the encoder. This mechanism can be regarded as a kind of attention, which means when we decode a word we chose which part of the annotations should be paid more attention to."}, {"heading": "6.2 Sequence to sequence in Response Generation", "text": "General speaking, dialogue systems can be sorted into two classes (Serban et al., 2016): goal-driven represented by systems Ga\u0161ic\u0301 et al. (2013) and non-goal-driven systems. The neural networks methods are mainly used in the later, because a large scale of data is more easily to get in that area. Ritter et al. (2011) first combine micro-blogging data with the generative probabilistic models, then Shang et al. (2015) used this type of data on the Seq2Seq to build a short conservation machine. followed by Serban et al. (2016), who came up with the Hierarchical Nerual Network model, aiming to model the utterances and interactive structure to build a multi-round dialogue system.\nAt the same time, Banchs (2012) proposed methods using a different type of data,the movie dialogue. Based on that, Ameixa et al. (2014) find using the retrieal system and movie subtitles can also improvement the performance."}, {"heading": "7 Conclusions", "text": "In this paper, we proposed a new approach for the sequence to sequence model to generate the first word. Proved our proposed model can bring a promotion in both the accuracy and the diversity for the first word\u2019s generation, thus improving the whole performance of the generation. Experiments in the response generation tasked verified our model\u2019s effectiveness, while rather than a method for a specific task, our proposed method is a general framework, which can also used for other tasks."}], "references": [{"title": "Luke, i am your father: dealing with out-of-domain requests by using movies subtitles", "author": ["Ameixa et al.2014] David Ameixa", "Luisa Coheur", "Pedro Fialho", "Paulo Quaresma"], "venue": "In International Conference on Intelligent Virtual Agents,", "citeRegEx": "Ameixa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ameixa et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Movie-dic: a movie dialogue corpus for research and development", "author": ["Rafael E Banchs"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Banchs.,? \\Q2012\\E", "shortCiteRegEx": "Banchs.", "year": 2012}, {"title": "Audio chord recognition with recurrent neural networks", "author": ["Yoshua Bengio", "Pascal Vincent"], "venue": "In ISMIR,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho et al.2014a] Kyunghyun Cho", "Bart Van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "Computer Science", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["Cho et al.2014b] Kyunghyun Cho", "Bart Van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "Kyung Hyun Cho", "Yoshua Bengio"], "venue": "Eprint Arxiv", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Galley et al.2015] Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06863", "citeRegEx": "Galley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Gao et al.2014] Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng"], "venue": null, "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "On-line policy optimisation of bayesian spoken dialogue systems via human interaction", "author": ["Ga\u0161i\u0107 et al.2013] M Ga\u0161i\u0107", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve Young"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2013}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers et al.2000] Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Neural networks. In Supervised Sequence Labelling with Recurrent Neural Networks, pages", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055", "author": ["Li et al.2015] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A survey on metrics for the evaluation of user simulations. The knowledge engineering review, 28(01):59\u201373", "author": ["Pietquin", "Hastie2013] Olivier Pietquin", "Helen Hastie"], "venue": null, "citeRegEx": "Pietquin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pietquin et al\\.", "year": 2013}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011] Alan Ritter", "Colin Cherry", "William B Dolan"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["Kallirroi Georgila", "Steve Young"], "venue": "In 6th SIGdial Workshop on DISCOURSE and DIALOGUE", "citeRegEx": "Schatzmann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "Computer Science", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06714", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model. arXiv preprint arXiv:1506.05869", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "Typically, previous work use a start symbol \u201c</s>\u201d to generation the first word (Sutskever et al., 2014).", "startOffset": 80, "endOffset": 104}, {"referenceID": 6, "context": "1 RNN encoder-decoder Typically, a sequence to sequence model consists of two parts: encoder and decoder, both of which are often implemented using a family of RNN, such as GRU (Cho et al., 2014a; Chung et al., 2014) and LSTM (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 177, "endOffset": 216}, {"referenceID": 10, "context": ", 2014) and LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Graves, 2012a), so a seq2seq model is also called RNN encoder-decoder architecture.", "startOffset": 17, "endOffset": 85}, {"referenceID": 1, "context": "Considering that vector may not be able to contain all the useful information of the source sequence, thus becoming a bottleneck of the model,Bahdanau et al. (2014) add the attention mechanism to improve the Seq2Seq\u2019s performance.", "startOffset": 142, "endOffset": 165}, {"referenceID": 16, "context": "First, the decoder RNN is essentially a language model (Mikolov et al., 2010), which use the previous predicted words to predict a new word, from the perspective of probability, it learns a conditional probability of word that given last predicted words.", "startOffset": 55, "endOffset": 77}, {"referenceID": 26, "context": "While the start symbol and the first predicted word do not have such association, because most words can be put at the first position of a sequence, there is not a learnable conditional probability, so the result of taking a start symbol may cause the model prefer to predict some high frequency words, which is also observed during other conversation models using this architecture (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015).", "startOffset": 383, "endOffset": 448}, {"referenceID": 23, "context": "While the start symbol and the first predicted word do not have such association, because most words can be put at the first position of a sequence, there is not a learnable conditional probability, so the result of taking a start symbol may cause the model prefer to predict some high frequency words, which is also observed during other conversation models using this architecture (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015).", "startOffset": 383, "endOffset": 448}, {"referenceID": 25, "context": "And to be compared with, a basic kind of Seq2Seq architecture for response generation called hybrid model proposed by Shang et al. (2015) was implemented.", "startOffset": 118, "endOffset": 138}, {"referenceID": 25, "context": "For convenience, first sentence and the second sentence of one dialogue pair are denoted as post and response(Shang et al., 2015)respectively.", "startOffset": 109, "endOffset": 129}, {"referenceID": 25, "context": "For convenience, first sentence and the second sentence of one dialogue pair are denoted as post and response(Shang et al., 2015)respectively. The data set contains one million pairs, and about 35 thousands words. It should be noted that compared with the data used by Shang et al. (2015), this crawled data is a one-to-one data set, one post is corresponding to exact one response.", "startOffset": 110, "endOffset": 289}, {"referenceID": 25, "context": "For convenience, first sentence and the second sentence of one dialogue pair are denoted as post and response(Shang et al., 2015)respectively. The data set contains one million pairs, and about 35 thousands words. It should be noted that compared with the data used by Shang et al. (2015), this crawled data is a one-to-one data set, one post is corresponding to exact one response. While in the Shang et al. (2015) paper, they crawled some one-to-many data from microblog, then distributed all the responses to the its post.", "startOffset": 110, "endOffset": 416}, {"referenceID": 25, "context": "For convenience, first sentence and the second sentence of one dialogue pair are denoted as post and response(Shang et al., 2015)respectively. The data set contains one million pairs, and about 35 thousands words. It should be noted that compared with the data used by Shang et al. (2015), this crawled data is a one-to-one data set, one post is corresponding to exact one response. While in the Shang et al. (2015) paper, they crawled some one-to-many data from microblog, then distributed all the responses to the its post. This is a creative way to build a big data set, while during our experiments, we found that the one-to-one data has a more rapid rate of convergence, so we created our own data set and trained models on it.Table 1 is an example of our data. As for the test set, considering that one of our evaluation method\u2013Bleu, which will be introduced in detail in next section, should has more than one reference for every candidate, our one-to-one data is not very suitable, so we select 100 posts and their corresponding responses in Shang et al. (2015)\u2019s data-set to build our test set.", "startOffset": 110, "endOffset": 1070}, {"referenceID": 25, "context": "The first one is a basic Seq2Seq model for dialogue called Hybird Model(denoted as HYP)(Shang et al., 2015) , the other is what we have proposed(denoted as LTS).", "startOffset": 87, "endOffset": 107}, {"referenceID": 6, "context": "The encoder and decoder were both implemented using the GRU (Cho et al., 2014a; Chung et al., 2014).", "startOffset": 60, "endOffset": 99}, {"referenceID": 3, "context": "After that, we used the beam search algorithm to search for the N-best result of response for one paritular sentence (Graves, 2012b; Boulanger-Lewandowski et al., 2013).", "startOffset": 117, "endOffset": 168}, {"referenceID": 3, "context": "The encoder and decoder were both implemented using the GRU (Cho et al., 2014a; Chung et al., 2014). and we set our model\u2019s parameters reference to Shang et al. (2015). The hidden size in the encoder was set to 1024.", "startOffset": 61, "endOffset": 168}, {"referenceID": 7, "context": "Until now, there is still not a uniform evaluation for response generation (Galley et al., 2015; Pietquin and Hastie, 2013; Schatzmann et al., 2005).", "startOffset": 75, "endOffset": 148}, {"referenceID": 22, "context": "Until now, there is still not a uniform evaluation for response generation (Galley et al., 2015; Pietquin and Hastie, 2013; Schatzmann et al., 2005).", "startOffset": 75, "endOffset": 148}, {"referenceID": 19, "context": "to achieve that goal, we use two metrics: for one hand, we employed the wildly used automatic evaluation method\u2013blue (Papineni et al., 2002) in the area of machine translation, for the other hand, we employed the human annotation method.", "startOffset": 117, "endOffset": 140}, {"referenceID": 26, "context": "which is proved to agree well with human judgement on response generation task (Sordoni et al., 2015; Li et al., 2015).", "startOffset": 79, "endOffset": 118}, {"referenceID": 15, "context": "which is proved to agree well with human judgement on response generation task (Sordoni et al., 2015; Li et al., 2015).", "startOffset": 79, "endOffset": 118}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014).", "startOffset": 180, "endOffset": 203}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al.", "startOffset": 181, "endOffset": 298}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al.", "startOffset": 181, "endOffset": 322}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al. (2014b),Gao et al.", "startOffset": 181, "endOffset": 341}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al. (2014b),Gao et al. (2014). Besides, Cho et al.", "startOffset": 181, "endOffset": 359}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al. (2014b),Gao et al. (2014). Besides, Cho et al. (2014b) added the vector c as an extra input to every time step of the decoder, by doing like this, all the steps not only the first one, can get full information of the context vector.", "startOffset": 181, "endOffset": 388}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al. (2014b),Gao et al. (2014). Besides, Cho et al. (2014b) added the vector c as an extra input to every time step of the decoder, by doing like this, all the steps not only the first one, can get full information of the context vector. Furthermore, Bahdanau et al. (2014) proposed a novel method to calculated a weighted sum of all the annotations of the encoder.", "startOffset": 181, "endOffset": 602}, {"referenceID": 24, "context": "2 Sequence to sequence in Response Generation General speaking, dialogue systems can be sorted into two classes (Serban et al., 2016): goal-driven represented by systems Ga\u0161i\u0107 et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 7, "context": ", 2016): goal-driven represented by systems Ga\u0161i\u0107 et al. (2013) and non-goal-driven systems.", "startOffset": 44, "endOffset": 64}, {"referenceID": 7, "context": ", 2016): goal-driven represented by systems Ga\u0161i\u0107 et al. (2013) and non-goal-driven systems. The neural networks methods are mainly used in the later, because a large scale of data is more easily to get in that area. Ritter et al. (2011) first combine micro-blogging data with the generative probabilistic models, then Shang et al.", "startOffset": 44, "endOffset": 238}, {"referenceID": 7, "context": ", 2016): goal-driven represented by systems Ga\u0161i\u0107 et al. (2013) and non-goal-driven systems. The neural networks methods are mainly used in the later, because a large scale of data is more easily to get in that area. Ritter et al. (2011) first combine micro-blogging data with the generative probabilistic models, then Shang et al. (2015) used this type of data on the Seq2Seq to build a short conservation machine.", "startOffset": 44, "endOffset": 339}, {"referenceID": 7, "context": ", 2016): goal-driven represented by systems Ga\u0161i\u0107 et al. (2013) and non-goal-driven systems. The neural networks methods are mainly used in the later, because a large scale of data is more easily to get in that area. Ritter et al. (2011) first combine micro-blogging data with the generative probabilistic models, then Shang et al. (2015) used this type of data on the Seq2Seq to build a short conservation machine. followed by Serban et al. (2016), who came up with the Hierarchical Nerual Network model, aiming to model the utterances and interactive structure to build a multi-round dialogue system.", "startOffset": 44, "endOffset": 449}, {"referenceID": 1, "context": "At the same time, Banchs (2012) proposed methods using a different type of data,the movie dialogue.", "startOffset": 18, "endOffset": 32}, {"referenceID": 0, "context": "Based on that, Ameixa et al. (2014) find using the retrieal system and movie subtitles can also improvement the performance.", "startOffset": 15, "endOffset": 36}], "year": 2016, "abstractText": "The sequence to sequence architecture is widely used in the response generation and neural machine translation to model the potential relationship between two sentences. It typically consists of two parts: an encoder that reads from the source sentence and a decoder that generates the target sentence word by word according to the encoder\u2019s output and the last generated word. However, it faces to the \u201ccold start\u201d problem when generating the first word as there is no previous word to refer. Existing work mainly use a special start symbol \u201c</s>\u201d to generate the first word. An obvious drawback of these work is that there is not a learnable relationship between words and the start symbol. Furthermore, it may lead to the error accumulation for decoding when the first word is incorrectly generated. In this paper, we proposed a novel approach to learning to generate the first word in the sequence to sequence architecture rather than using the start symbol. Experimental results on the task of response generation of short text conversation show that the proposed approach outperforms the state-of-the-art approach in both of the automatic and manual evaluations.", "creator": "LaTeX with hyperref package"}}}