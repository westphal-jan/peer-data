{"id": "1608.05605", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Using Distributed Representations to Disambiguate Biomedical and Clinical Concepts", "abstract": "linksys In this glow paper, we report a stampings knowledge - based upper-body method mon. for ornate Word garuda Sense Disambiguation kena in akhmedov the 147.50 domains sifra of roh biomedical supatra and steffens clinical disseminates text. loung We ng5 combine pre-marital word representations created ruckauf on pauwels large corpora silvina with a small woodcarver number of atenza definitions metsing from robespierre the UMLS qotbi to create concept representations, mandak which we dohn then encuestas compare to representations 3048 of parsimony the inital context of kury ambiguous terms. ignorance Using balachandran no relational sarlahi information, unrolls we 1000-meter obtain youlan comparable telecast performance 387,000 to mcqueen previous approaches cyl on immeasurable the o.d. MSH - space-like WSD dataset, heze which is a well - known dataset in restructure the biomedical melmoth domain. Additionally, w-class our 3.01 method is sls fast and non-christian easy to tymoshchuk set waitressing up and extend dietzsch to other domains. Supplementary 25.43 materials, including urbain source code, wyandotte can be guatemalans found 41-15 at zymogen https: / / g400 github. com / clips / droves yarn", "histories": [["v1", "Fri, 19 Aug 2016 14:05:03 GMT  (51kb,D)", "http://arxiv.org/abs/1608.05605v1", "6 pages, 1 figure, presented at the 15th Workshop on Biomedical Natural Language Processing, Berlin 2016"]], "COMMENTS": "6 pages, 1 figure, presented at the 15th Workshop on Biomedical Natural Language Processing, Berlin 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["st\\'ephan tulkens", "simon \\v{s}uster", "walter daelemans"], "accepted": false, "id": "1608.05605"}, "pdf": {"name": "1608.05605.pdf", "metadata": {"source": "CRF", "title": "Using Distributed Representations to Disambiguate Biomedical and Clinical Concepts", "authors": ["St\u00e9phan TulkensF", "Simon \u0160usterF", "Walter DaelemansF", "FCLiPS \u2660CLCG"], "emails": ["firstname.lastname@uantwerpen.be"], "sections": [{"heading": null, "text": "In this paper, we report a knowledge-based method for Word Sense Disambiguation in the domains of biomedical and clinical text. We combine word representations created on large corpora with a small number of definitions from the UMLS to create concept representations, which we then compare to representations of the context of ambiguous terms. Using no relational information, we obtain comparable performance to previous approaches on the MSH-WSD dataset, which is a well-known dataset in the biomedical domain. Additionally, our method is fast and easy to set up and extend to other domains. Supplementary materials, including source code, can be found at https: //github.com/clips/yarn"}, {"heading": "1 Introduction", "text": "Word Sense Disambiguation (WSD) is a procedure in which an ambiguous term or concept is assigned a single sense appropriate for that context, and is an important step in the creation of a semantic representation of a document (Ide and Ve\u0301ronis, 1998). While performing WSD will benefit most natural language processing applications, disambiguation of concepts is a critical component of applications operating on clinical and biomedical text, in which the same word can denote differing concepts, and may thus elicit radically different responses.\nCompounding this problem of ambiguity is the fact that clinical text, in general, is noisier than other domains, and contains a large variety of abbreviations, some of which may be specific to a single hospital or physician. Additionally, there is a marked absence of large volumes of annotated clinical text, even for English, which presents a problem\nfor supervised approaches to Word Sense Disambiguation. For other languages, such as Dutch, there exist no freely available annotated corpora of clinical text.\nA first step towards solving this problem could be the use of distributed representations. Where a more traditional word representation, such as a TFIDF bag-of-words (BoW) representation, carries frequency information, distributed representations encode semantic information. A big advantage to using these representations is that they can be generated from large corpora of unlabeled text, and can be trained on very large corpora in a reasonable amount of time. These representations, especially when trained using neural architectures such as word2vec (Mikolov et al., 2013), have been shown to improve performance on a variety of tasks when compared to more traditional BoW representations.\nWe hypothesize that these kinds of distributional representations are well-suited for WSD in the clinical and biomedical domain because of the lack of training data, and the large terminological variety. We present a knowledge-based approach to Word Sense Disambiguation which creates concept representations by combining definitions from the Unified Medical Language System (UMLS) with distributed representations. We test our hypothesis on the MSH-WSD, which is a well-known dataset for WSD in the biomedical domain."}, {"heading": "2 Related Research", "text": "All knowledge-based methods we review use the Unified Medical Language System\u00ae (UMLS) Metathesaurus\u00ae (Bodenreider, 2004) as a knowledge base, possibly augmented with external sources, such as MeSH\u00ae-indexed abstracts. Generally speaking, the UMLS contains two separate information sources that are suitable for use in dis-\nar X\niv :1\n60 8.\n05 60\n5v 1\n[ cs\n.C L\n] 1\n9 A\nug 2\n01 6\nambiguation: the concept unique identifier (CUI), which is a unique label for each concept, and the semantic type (ST), which is a set of 135 broad labels such as \u201cAnimal\u201d or \u201cChemical\u201d. In general, a word is only considered disambiguated if the correct CUI can be selected; hence, as McInnes and Pedersen (2013) note, approaches based on semantic types are not able to disambiguate between approximately 12% of concepts, as some concepts with the same surface form have an identical ST, but a different CUI.\nIn terms of approaches using ST, Humphrey et al. (2006) create one vector for each semantic type by creating a BoW representation of all words that denote that semantic type. For each ambiguous term, a target word vector is created by taking a window of words from the right and left of the term. The concept which is associated with the ST with the lowest cosine distance is then taken to be the correct sense of the term. Similarly, Alexopoulou et al. (2009) create a method which finds the closest concept based on a combination of co-occurrence with other semantic types and ontological similarity through is-a relationships.\nClosest to our approach is the machine readable dictionary (MRD) approach (McInnes, 2008; Jimeno-Yepes et al., 2011), which uses definitions from the UMLS to create concept vectors by creating BoW representations of concepts using all definitions of the concept and those of related concepts. This BoW representation contains TF-IDF values where D is the number of concepts in which a word appears, thereby reducing the influence of general words which occur in many concepts. These representations are then compared to the vectorized contexts of the ambiguous terms using cosine distance. A refinement of MRD, called second-order co-occurrence MRD (2-MRD) (McInnes, 2008), replaces each word in a definition by a vector which contains TF-IDF values of co-occurrence counts, thereby associating each word with a context.\nMcInnes and Pedersen (2013) introduce UMLS::SenseRelate, an approach which is based on Pedersen et al. (2004)\u2019s WordNet::SenseRelate. In this system, each possible sense for an ambiguous term is assigned a distance-weighted score based on the concepts of the terms surrounding it, where the concepts of the surrounding terms are determined using UMLS::Similarity (McInnes et al., 2009).\nJimeno-Yepes and Berlanga (2015) present so-\ncalled step models, which calculate the probability of a word occurring with a certain concept by considering the number of times a word occurs in the definitions of that concept and its related concepts. It then steps through the UMLS-defined ontology of concepts, and refines the probabilities for each word and each concept based on the relations within the ontology.\nFinally, Chen et al. (2014) present an approach for general WSD which uses word embeddings coupled with WordNet (Fellbaum, 1998) as a resource to perform sense disambiguation, and which creates sense-specific word embeddings from these sense-disambiguated word representations."}, {"heading": "3 Materials", "text": ""}, {"heading": "3.1 Test Corpus", "text": "We use the MSH-WSD corpus (Jimeno-Yepes et al., 2011), which consists of a set of 203 ambiguous terms, each associated with multiple concepts, to evaluate our approach. Of the 203 terms in the corpus, 106 are regular terms, 88 are acronyms, and 9 can be acronyms and regular terms. For each of these concepts, up to 100 MeSH abstracts were retrieved, resulting in a set of 37,888 abstracts. In our approach, all abstracts were pre-processed using the tokenizer from the Pattern package (De Smedt and Daelemans, 2012), and all stop words were removed using the English stop word list from scikit-learn (Pedregosa et al., 2011)."}, {"heading": "3.2 Word vectors", "text": "We evaluate our approach using three sets of vectors: The first set was trained on a small set of Medline abstracts1, and a second set of vectors created on the entirety of the MIMIC-III corpus of clinical notes (Johnson et al., 2016). For both sets, we used the word2vec implementation from gensim (R\u030cehu\u030ar\u030cek and Sojka, 2010), using skipgram with negative sampling, a frequency cutoff\n1The specific IDs of these abstracts are available in the online appendix.\nof 5 and a negative sampling of 15. Additionally, we used a third set of vectors, available from the BioASQ organisers2, which was trained on a much larger set of Medline abstracts.3 The model statistics are visualized in Table 1."}, {"heading": "4 Approach", "text": "Similar to the 2-MRD approach detailed above, our approach creates concept vectors by replacing each word in every definition by the vector representation of that word. This creates an M \u00d7n matrix for each definition, where M is the dimensionality of the word vectors, and n the number of words contained in that definition. Following this, for each definition, we then obtain a single vector of dimensionality M by applying a compositional function to the matrix, thereby obtaining so-called definition vectors, which represent the entire meaning of the definition in one vector. Each concept can then be represented by a M\u00d7d matrix, where d is the number of definitions that a concept has in the UMLS. Finally, we apply a second composition function to this matrix, thereby obtaining a single vector of dimensionality M which represents the combined meaning of all definitions for that concept, i.e. a concept vector.\nFor each abstract in the test corpus, we first locate each ambiguous term through a simple lookup. For each located term in the abstract we create a vector representation by retrieving all words in a window of size w surrounding the ambiguous term, and replacing the words by their vectors. Note that this window does not include the ambiguous term itself. These collections of vectors are then combined into M -dimensional vectors using the same composition function as above. This is done separately for each term occurrence within a single document, creating a M \u00d7 x matrix, where x is the number of times the ambiguous term occurs in a single document. These are then combined in an M -dimensional term vector using the same composition we used for the concepts, above. A schematic representation of our model is given in Figure 1.\nBecause all concept and term vectors are created using the same distributed vectors and compositional functions, the vector space in which they are\n2Available on the BioASQ website. 3While we concede that the BioASQ corpora might contain abstracts from the MSH dataset, it does not contain any explicit labeled information that might be used in disambiguation.\nplaced is also comparable. Hence, for each ambiguous word we encounter, we can use the cosine distance between the abstract vector of the ambiguous utterance and each possible sense of that word to determine the correct sense. This makes our approach very similar to the Lesk family of approaches (Lesk, 1986).\nIn terms of composition function we experimented with elementwise multiplication, averaging and summation, all of which are unordered compositional functions (Mitchell and Lapata, 2008). In addition, it is worth noting that there\u2019s still a lively debate whether ordered composition actually leads to better results for estimating document-, or sentence-level meaning, when compared to unordered composition (Iyyer et al., 2015; Socher et al., 2013)."}, {"heading": "5 Results", "text": "The accuracy scores obtained by our models using the different word vectors are displayed in Table 2. med, mim and bio denote the vectors created on the small Medline corpus, the Mimic-III corpus and the BioASQ vectors, respectively. We consider both a constrained and an unconstrained version of the task. For each word, the constrained version of the task only considers the senses present\nin the MSH-WSD dataset as possible targets. The unconstrained version considers all concepts which are denoted by the ambiguous term in the 2015AB version of the UMLS as possible targets. The term cortex, for example, only has 2 concepts associated with it in the MSH-WSD dataset, while in the 2015AB UMLS release it can denote 5 separate concepts. Because the unconstrained version of the task considers all words, it therefore gives a better indication of real-life performance.\nAccuracy C and U denote that the scores were obtained in the constrained settings and unconstrained setting, respectively. All reported scores use a window size of 6, which was optimized on a randomly selected set of 20 terms from the MSH-WSD set. Varying the window size had negligible results: all window sizes over 6 had comparable results, and increasing the window size over 30 causes a (small) decline in results. This is in line with McInnes and Pedersen (2013), who report a positive effect of window size that quickly tapers off for window sizes > 10. Concerning the composition functions, summation and averaging as first and second order composition function worked best, while using element-wise multiplication did not work well in any case. Where possible, we display the selfreported scores from the relevant papers on the same dataset.\nA first thing to note is the large difference in accuracy when changing the set of word representations, especially the difference between the Medline vectors and the vectors derived from the Mimic-III corpus. It is currently unclear what causes these performance differences, although it is likely that the small vocabulary, caused by the noisiness of the clinical data in the MIMIC-III cor-\npus, reduces performance. Compared to previous approaches, our approach outperforms the MRD, 2-MRD, and UMLS::SenseRelate approaches, but does not manage to improve on the scores of the step models. Recall, however, that the step models largely rely on relationships in the UMLS ontology to estimate concept relatedness.\nTo compare how our models improved when including relation information, we also experimented with adding definitions of related concepts, i.e. concepts which had a sibling, parent or child relationship to each concept. In contrast to patterns observed in earlier work, this did not have a significant, and often a detrimental, effect on performance. Note that this makes our model entirely independent of the actual UMLS hierarchy, and more flexible as a result, as we only use the mappings from definition to CUI for disambiguation, and no other information, such as relations or semantic type. In addition, our system is also fast: on a consumer-grade laptop, our approach takes 10 seconds to vectorize and disambiguate all abstracts in the MSH dataset, not taking into account the time it takes to load the embeddings into memory.\nOur approach obtains an accuracy of > 90% on 103 terms, showing that it is able to disambiguate a large variety of terms. For some terms, however, the performance was below random guessing. These are shown in Table 3. The pattern of errors is quite clear: Our approach has trouble with disambiguation if the definitions of the concepts themselves are lexically very similar. As an example, on the term Hemlock our approach performs below chance level because one of the concepts denotes a family of poisonous plants, while the other reports a tree, also called hemlock, the description of which mentions that it is explicitly not poisonous. We expect these kinds of problems to be alleviated with the addition of more data."}, {"heading": "6 Conclusion and future work", "text": "In this paper we presented a novel approach to WSD in the biomedical domain which achieves comparable performance to existing methods without incorporating relational information from an\nontology. This makes the approach easily transferable to other languages, for which such ontologies might not exist, and to other domains. The large variation in accuracy when changing sets of word embeddings also raises interesting prospects for improvement; better word representations will lead to an improvement in our approach without modifying the approach itself. Additionally, we would like to experiment with different composition functions for composing the definition and concept vectors."}, {"heading": "Acknowledgments", "text": "Part of this research was carried out in the framework of the Accumulate IWT SBO project, funded by the government agency for Innovation by Science and Technology (IWT). We would also like to thank Elyne Scheurwegs for making the small set of Medline abstract available to us."}], "references": [{"title": "The unified medical language system (UMLS): integrating biomedical terminology", "author": ["Olivier Bodenreider"], "venue": "Nucleic acids research,", "citeRegEx": "Bodenreider.,? \\Q2004\\E", "shortCiteRegEx": "Bodenreider.", "year": 2004}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Word sense disambiguation by selecting the best semantic type based on Journal Descriptor Indexing", "author": ["Willie J Rogers", "Halil Kilicoglu", "Dina Demner-Fushman", "Thomas C Rindflesch"], "venue": null, "citeRegEx": "Humphrey et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Humphrey et al\\.", "year": 2006}, {"title": "Introduction to the special issue on word sense disambiguation: the state of the art", "author": ["Ide", "V\u00e9ronis1998] Nancy Ide", "Jean V\u00e9ronis"], "venue": null, "citeRegEx": "Ide et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Ide et al\\.", "year": 1998}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer et al.2015] Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": null, "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Knowledge based word-concept model estimation and refinement for biomedical text mining", "author": ["Jimeno-Yepes", "Rafael Berlanga"], "venue": "Journal of biomedical informatics,", "citeRegEx": "Jimeno.Yepes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jimeno.Yepes et al\\.", "year": 2015}, {"title": "Exploiting MeSH indexing in MEDLINE to generate a data set for word sense disambiguation", "author": ["Bridget T McInnes", "Alan R Aronson"], "venue": "BMC bioinformatics,", "citeRegEx": "Jimeno.Yepes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jimeno.Yepes et al\\.", "year": 2011}, {"title": "MIMICIII, a freely accessible critical care database", "author": ["Johnson et al.2016] AEW Johnson", "TJ Pollard", "L Shen", "L Lehman", "M Feng", "M Ghassemi", "B Moody", "P Szolovits", "LA Celi", "RG Mark"], "venue": "Scientific Data", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone", "author": ["Michael Lesk"], "venue": "In Proceedings of the 5th annual international conference on Systems documentation,", "citeRegEx": "Lesk.,? \\Q1986\\E", "shortCiteRegEx": "Lesk.", "year": 1986}, {"title": "Evaluating measures of semantic similarity and relatedness to disambiguate terms in biomedical text", "author": ["McInnes", "Pedersen2013] Bridget T McInnes", "Ted Pedersen"], "venue": "Journal of biomedical informatics,", "citeRegEx": "McInnes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McInnes et al\\.", "year": 2013}, {"title": "UMLS-Interface and UMLS-Similarity: open source software for measuring paths and semantic similarity", "author": ["Ted Pedersen", "Serguei VS Pakhomov"], "venue": "In AMIA Annual Symposium Proceedings,", "citeRegEx": "McInnes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McInnes et al\\.", "year": 2009}, {"title": "An unsupervised vector approach to biomedical term disambiguation: integrating UMLS and Medline", "author": ["Bridget T McInnes"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Lan-", "citeRegEx": "McInnes.,? \\Q2008\\E", "shortCiteRegEx": "McInnes.", "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of the Association for Computational Linguistics,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "WordNet:: Similarity: measuring the relatedness of concepts", "author": ["Siddharth Patwardhan", "Jason Michelizzi"], "venue": "In Demonstration papers at HLT-NAACL", "citeRegEx": "Pedersen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pedersen et al\\.", "year": 2004}, {"title": "Scikit-learn: Machine learning", "author": ["Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": null, "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["\u0158eh\u016f\u0159ek", "Sojka2010] Radim \u0158eh\u016f\u0159ek", "Petr Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "\u0158eh\u016f\u0159ek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "\u0158eh\u016f\u0159ek et al\\.", "year": 2010}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "Proceedings of the confer-", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "These representations, especially when trained using neural architectures such as word2vec (Mikolov et al., 2013), have been shown to improve performance on a variety of tasks when compared to more traditional BoW representations.", "startOffset": 91, "endOffset": 113}, {"referenceID": 0, "context": "All knowledge-based methods we review use the Unified Medical Language System\u00ae (UMLS) Metathesaurus\u00ae (Bodenreider, 2004) as a knowledge base, possibly augmented with external sources, such as MeSH\u00ae-indexed abstracts.", "startOffset": 101, "endOffset": 120}, {"referenceID": 2, "context": "In terms of approaches using ST, Humphrey et al. (2006) create one vector for each semantic type", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": "able dictionary (MRD) approach (McInnes, 2008; Jimeno-Yepes et al., 2011), which uses definitions from the UMLS to create concept vectors by creating BoW representations of concepts using all definitions of the concept and those of related concepts.", "startOffset": 31, "endOffset": 73}, {"referenceID": 6, "context": "able dictionary (MRD) approach (McInnes, 2008; Jimeno-Yepes et al., 2011), which uses definitions from the UMLS to create concept vectors by creating BoW representations of concepts using all definitions of the concept and those of related concepts.", "startOffset": 31, "endOffset": 73}, {"referenceID": 11, "context": "A refinement of MRD, called second-order co-occurrence MRD (2-MRD) (McInnes, 2008), replaces each word in a definition by a vector which contains TF-IDF values of co-occurrence counts, thereby associating each word with a context.", "startOffset": 67, "endOffset": 82}, {"referenceID": 10, "context": "where the concepts of the surrounding terms are determined using UMLS::Similarity (McInnes et al., 2009).", "startOffset": 82, "endOffset": 104}, {"referenceID": 1, "context": "Finally, Chen et al. (2014) present an approach for general WSD which uses word embeddings coupled with WordNet (Fellbaum, 1998) as a re-", "startOffset": 9, "endOffset": 28}, {"referenceID": 6, "context": "We use the MSH-WSD corpus (Jimeno-Yepes et al., 2011), which consists of a set of 203 ambiguous terms, each associated with multiple concepts,", "startOffset": 26, "endOffset": 53}, {"referenceID": 15, "context": "approach, all abstracts were pre-processed using the tokenizer from the Pattern package (De Smedt and Daelemans, 2012), and all stop words were removed using the English stop word list from scikit-learn (Pedregosa et al., 2011).", "startOffset": 203, "endOffset": 227}, {"referenceID": 7, "context": "We evaluate our approach using three sets of vectors: The first set was trained on a small set of Medline abstracts1, and a second set of vectors created on the entirety of the MIMIC-III corpus of clinical notes (Johnson et al., 2016).", "startOffset": 212, "endOffset": 234}, {"referenceID": 8, "context": "This makes our approach very similar to the Lesk family of approaches (Lesk, 1986).", "startOffset": 70, "endOffset": 82}, {"referenceID": 4, "context": "In addition, it is worth noting that there\u2019s still a lively debate whether ordered composition actually leads to better results for estimating document-, or sentence-level meaning, when compared to unordered composition (Iyyer et al., 2015; Socher et al., 2013).", "startOffset": 220, "endOffset": 261}, {"referenceID": 17, "context": "In addition, it is worth noting that there\u2019s still a lively debate whether ordered composition actually leads to better results for estimating document-, or sentence-level meaning, when compared to unordered composition (Iyyer et al., 2015; Socher et al., 2013).", "startOffset": 220, "endOffset": 261}, {"referenceID": 11, "context": "This is in line with McInnes and Pedersen (2013), who report a positive effect of window size that quickly tapers off for window sizes > 10.", "startOffset": 21, "endOffset": 49}], "year": 2016, "abstractText": "In this paper, we report a knowledge-based method for Word Sense Disambiguation in the domains of biomedical and clinical text. We combine word representations created on large corpora with a small number of definitions from the UMLS to create concept representations, which we then compare to representations of the context of ambiguous terms. Using no relational information, we obtain comparable performance to previous approaches on the MSH-WSD dataset, which is a well-known dataset in the biomedical domain. Additionally, our method is fast and easy to set up and extend to other domains. Supplementary materials, including source code, can be found at https: //github.com/clips/yarn", "creator": "LaTeX with hyperref package"}}}