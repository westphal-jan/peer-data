{"id": "1608.08724", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "A Programming Language With a POMDP Inside", "abstract": "We present POAPS, 1945-1950 a novel kima planning jaunty system for defining Partially oxycontin Observable Markov kassell Decision Processes (g60 POMDPs) swanepoel that ones abstracts clawfinger away from POMDP ameliorated details for the benefit of non - 85-82 expert practitioners. POAPS 6,500-seat includes nanya an 274.8 expressive adaptive tackiest programming language sunbed based johanssen on boc Lisp khaffaf that has see-saw constructs for choice points reformatory that can 206-448-8135 be tarento dynamically optimized. nudgee Non - alessandro experts ersatz can use murase our language to write dweikat adaptive programs truffles that hebbel have partially observable components without mizell needing zaozhuang to specify formula belief / tengah hidden states or arriva reason about probabilities. tivos POAPS is also skylark a o'rahilly compiler 99.51 that 98-run defines and performs hrdina the lumby transformation nagravision of 13,550 any program jilly written in our 1900s language undercity into beaven a POMDP with rubery control grimwade knowledge. tenian We 56-42 demonstrate phrasebook the snowsuit generality microclimates and niigata power skibby of unsurveyed POAPS in the rapidly 16.47 growing domain of pe\u00e7anha human computation mouthguards by describing whingeing its elt expressiveness vatnajokull and simplicity by polina writing proceeded several POAPS minsitry programs pashtun for common resting-place crowdsourcing crusading tasks.", "histories": [["v1", "Wed, 31 Aug 2016 04:25:45 GMT  (237kb,D)", "http://arxiv.org/abs/1608.08724v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.PL", "authors": ["christopher h lin", "mausam", "daniel s weld"], "accepted": false, "id": "1608.08724"}, "pdf": {"name": "1608.08724.pdf", "metadata": {"source": "CRF", "title": "A Programming Language With a POMDP Inside", "authors": ["Christopher H. Lin", "Daniel S. Weld"], "emails": ["chrislin@cs.washington.edu", "mausam@cse.iitd.ac.in", "weld@cs.washington.edu"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors I.2 [Artificial Intelligence]: Programming Languages and Software\nGeneral Terms Algorithms, Languages\nKeywords POMDPs, Planning, Decision Theory, Adaptive Programming, Crowdsourcing"}, {"heading": "1. INTRODUCTION", "text": "Although optimal decision-making is widely applicable across many aspects of human life, ultimately the ability to construct and use intelligent agents to make optimal decisions has been restricted to those who understand to some degree the theory of decision processes. Those who would greatly like access to such tools, like many in the crowdsourcing community [23] for example, must either resort to sub-optimal techniques that use approximate heuristics or hire a planning expert to formally define and solve their domain-specific problems.\nThis paper presents POAPS (Partially Observable Adaptive Programming System), which is a step toward bringing the power of decision theory to the masses. POAPS makes available the power of Partially-Observable Markov Decision Processes (POMDPs) to non-expert users through the interface of an adaptive programming\nlanguage that provides an abstraction over POMDPs so that nonexperts can write POMDPs without knowing anything about them.\nLike many previous approaches to proceduralizing decision processes[2, 5, 18], we create a language that includes choice points, which allow the system to make optimal decisions adaptively. However, unlike previous approaches, we do not expect the programmer to explicitly reason about the state space or world dynamics. Such an expectation for our target users is impractical. In an informal experiment, we recruited scientists unfamiliar with artificial intelligence and introduced them to (PO)MDPs. We then asked them to define a state space for some simple crowdsourcing problems. However, all of them were unable to define a satisfactory state space, let alone an entire POMDP. In particular, they had trouble grasping the meaning and mathematical formalism of a POMDP hidden \u201cstate.\u201d Therefore, the challenge is to create a language that can express all the details of POMDPs, yet hides these details from the programmer, but is still flexible enough to represent programs for a variety of scenarios.\nOne of our key contributions is a division of work between experts and non-experts that achieves our desiderata. POAPS asks experts to define primitives. A primitive consists of a function and a mathematical model of that function. The mathematical model describes some hidden state underlying the function and its dynamics. Non-expert programmers understand the primitives as procedures in terms of easily understood inputs and outputs and may not appreciate the hidden mathematical models. They can, however, compose the primitives into novel programs for their needs. POAPS then compiles their programs into completely new POMDPs.\nFor instance, suppose a user would like to write a program that uses crowdsourcing to label training data. An adaptive program that achieves this goal might be the following. For each datum, poll crowd workers for labels until the system is confident it can stop and return a label. For an adaptive program to make optimal decisions, it needs to both maintain some state that represents a current belief about what the correct label is, and know how to update this belief after every label observation. Instead of hiring a planning expert to handcraft a custom POMDP for this simple voting problem [8, 11], users, and in particular, non-experts, should be able to write a very simple program that abstracts away from state variables and probabilities: either ask another worker for another label and recurse, or return the label with the most number of votes. POAPS achieves this goal. Figure 1 shows a POAPS program for labeling (voting) that implements the algorithm we just described. It assumes there are two possible labels and reposes the problem as one of discovering if the first label is better than the second. Notice that the program makes no reference to any POMDP components in its definition. The user does not need to specify some hidden state that represents the correct answer. Instead, an expert has previously ar X iv :1\n60 8.\n08 72\n4v 1\n[ cs\n.A I]\n3 1\nA ug\n2 01\n6\n(define (vote-better? q a0 a1 c0 c1)\ndefined the primitive crowd-vote, which contains a mathematical model describing the dynamics of its inputs and ouputs, and we see that all the user needs to do in excess of providing the program logic is to use that primitive (by finding it in a library) and provide a choice point in the program. Then, POAPS will automatically determine the optimal branch to take at runtime.\nWe show the value of POAPS by writing many useful crowdsourcing POMDPs as POAPS programs. The simplicity of the language and programs leads us to believe that our system will be easily usable by non-experts.\nIn summary, our contributions are (1) a system that exploits a separation of experts and non-experts that allows non-experts to write POMDPs while being isolated from the mathematical description of the decision process, and (2) an implementation that will likely help non-expert POMDP practitioners in their ability to write decision processes in a variety of domains."}, {"heading": "2. RELATED WORK", "text": "Various languages have been proposed in the literature for representing POMDPs. Several of those are declarative representations, which ask the user to explicitly declare each component (state, actions, etc.) of a POMDP. Examples include Cassandra-style format 1, Probabilistic PDDL [24], and RDDL [20].\nSeveral procedural languages have also been developed including A2BL [22], ALisp [2] and concurrent ALisp [14], Hierarchical Abstract Machines (HAMs) [17], and Programmable Hierarchical Abstract Machines (PHAMs) [1]. All these representations allow a user to provide control knowledge in the form of a procedural program for an existing and explicitly specified MDP. In other words, when one writes an ALisp program, one must additionally explicitly specify an MDP that the program is tied to and constraining.\nDTGolog [5] is a situation calculus-based procedural language that can both define decision problems (by defining a set of axioms) and specify control. While this unification is useful for experts, users must still explicitly specify MDPs before they can write control programs. While non-expert users can write control policies for expert-written MDPs, they cannot write their own MDPs. Additionally, they must explicitly use the components of the MDP in their control policies.\nStochastic Programs (SP) [15] provide a language for experts to write world models and primitive actions and then compose those primitive actions to create control policies for the corresponding world model. While they do no consider non-expert use of the language, one can imagine non-experts using the primitive actions to create control policies. However, these non-expert users must explicitly use POMDP components. In particular, the primitives do not abstract away from the state space; they take state variables as arguments, and return state variables and observations. SP also does not allow for learning policies. The language only provides for specifying a complete control policy and evaluating its utility.\n1http://pomdp.org\nThe work on adaptive programs [18] allows non-expert users to quickly construct observable decision processes by writing programs that can contain optimizable choice points. However, construction of POMDPs still requires the user to explicitly model POMDP details like belief states and belief updates.\nThe key difference between our work and all the related work is that we do not ask the programmer to explicitly define or use POMDP components. Instead we leverage a division of work between experts and non-experts whereby non-experts can glue together expert-provided POMDP components to create entirely new POMDPS for their own problems."}, {"heading": "3. CROWDSOURCING BACKGROUND", "text": "We are motivated by and describe our system using many examples taken from the crowdsourcing literature. Crowdsourcing requesters, those who hire crowdsourced workers, often design workflows to complete their tasks. An example of a simple workflow is the labeling workflow we described in the Introduction (Figure 1). Another example of a workflow is the iterative improvement workflow [13]. Suppose a requester wants to generate some artifact, like a text description of an image. In the iterative improvement workflow, he first hires one worker to improve an existing caption. Then, he asks several workers to vote on whether the new caption is better than the old caption. Finally, he repeats this process until he is satisfied with the caption. Previous work has hand-crafted a POMDP for this particular workflow in order to make dynamic decisions like when to vote and when to stop, showing significant savings over static policies [8]. Our system would allow these requesters, who are likely not planning experts, to easily write a program for this workflow (and others) that implicitly defines a POMDP, which our system can then optimize and control."}, {"heading": "4. PRIMITIVES", "text": "In order to interpret a program like the one for iterative improvement as a POMDP, POAPS needs mathematical models for function calls, like the one that hires a worker to improve an artifact. POAPS asks experts to define primitives to bootstrap this process. A primitive is a ten-tuple \u3008D,R,\u2126, T ,O, I, C,DU ,RU ,F\u3009, where:\n\u2022 D = D1 \u00d7 . . .\u00d7Dn is a set of domain states.\n\u2022 R is a set of range states.\n\u2022 \u2126 is a set of observations.\n\u2022 T : D \u00d7R \u2192 [0, 1] is a transition function.\n\u2022 O : R\u00d7 \u2126\u2192 [0, 1] is an observation function.\n\u2022 I is an n-dimensional indicator vector indicating which of the Di are observable.\n\u2022 C : D \u2192 R+ is a cost function.\n\u2022 DU = D1U \u00d7 . . .\u00d7DnU is a set of user domain states.\n\u2022 RU is a set of user range states.\n\u2022 F : DU \u2192RU is a user function.\nAn expert defines all 10 of these components. Intuitively, a primitive is a function (the user function F : DU \u2192 RU ), and a model of that function (the rest of the components). We note that in the special case when D = R, \u3008D,R,\u2126, T ,O, C\u3009 is a one-action POMDP. The best way to understand the purpose of primitives is through an example. In particular, we discuss how we would define\n(define (improve text)\nthe primitive c-imp, which would be a function used in iterative improvement to improve an artifact.\nFirst, we define the function part. c-imp should take an artifact \u03b1 as input, call some crowdsourcing API, and return an improved artifact \u03b1\u2032. Therefore, we define the user function to be F(\u03b1 \u2208 DU ) = calltoAPI(\u03b1), where DU = RU are the set of all possible artifacts \u03b1 (e.g., the set of all strings). Now we want to define the model part of the primitive, which will track the quality of artifacts being input and output by F . We define D = R = [0, 1] to represent the hidden quality of the artifact. The transition function T needs to encode the probability of getting an artifact of quality q\u2032 if a worker improves an artifact of quality q. Therefore, we define T (q \u2208 D, q\u2032 \u2208 R) = P (q\u2032|q) using some conditional distribution like a Beta distribution. We set C to be the amount of money paid to a worker, which can be some constant like 5 cents. c-imp produces no observations so \u2126 is empty, and hence there is no observation function O. Finally, we set I = (0) indicating that D is not observable.\nThis primitive combines a model for the improvement of an artifact with a function that outputs an improvement of the artifact. We can view eachDiU as a model forDi andRU is a model forR. So, for a non-expert user who does not care about or cannot understand the model, a primitive is simply the functionF : DU \u2192RU . These non-experts can call primitives in their programs, and when they do so, they expect they are calling the function F ."}, {"heading": "5. THE LANGUAGE", "text": "We now describe the POAPS language, which users use to express adaptive programs using primitives. We define a POAPS program to be a function definition written in the POAPS language. The POAPS language is an extension of Lisp, because Lisp is both easy to write and easy to interpret. Following previous work [2, 5, 18], we add the special form (choose <exp0> <exp1> ... ).\nThe choose special form is a construct for dynamic execution. It takes a variable number of arguments, each of which is a Lisp S-expression. When used in a program, it describes a choice point in the program, meaning that at runtime, POAPS will dynamically decide the optimal argument expression to execute.\nA key contribution of POAPS is how function calls are interpreted. However, we first emphasize that for a non-expert user, POAPS behaves just as an ordinary programming language. When a non-expert user calls a primitive: (p arg0 . . . argn), the expression evaluates to F(argo, . . . , argn) where argi \u2208 DiU . A function call is just a function call, regardless of whether the function is a POAPS primitive or a user-defined function. Figure 2 shows a POAPS program that a crowdsourcing expert might write for improving a piece of text using crowdsourcing. It is a simplified version of iterative-improvement that removes voting.\nTo the non-expert user, the argument text is bound to a string,\u03b1. During execution, there are two execution paths. Suppose the program chooses the first path. When the string, \u03b1 \u2208 Du, is passed to c-imp, a primitive we described in the previous section, a function, F is called to hire a crowdworker to improve the string. c-imp re-\nturns the improved string \u03b1\u2032 \u2208 RU and the program recurses. If the program chooses the second execution path, the string is returned.\nHowever, the semantics of the POAPS language are more complex. The expert user understands that in POAPS, all variables are actually bound to two values, and thus all expressions evaluate to two values. The first value, the Normal value, is the usual value that the non-expert user sees and understands, and is the same as it would be in any other programming language. For example, text is bound to a string. The second value is a Poaps value that can be unobservable, and hence, represented by a distribution in our system. This value is the value of a state variable in the POMDP POAPS constructs. Let exp represent this possibly hidden POAPS value of some expression, exp.\nFor the expert user, calling a primitive is everything that it is for the non-expert user. However, the expert user knows that in addition to being bound to the Normal value, the result of an expression (p arg0 . . . argn) is bound to a POAPS value r \u2208 R with probability T ((arg0 . . . , argn), r), where argi \u2208 Di. Furthermore, when p is called, an observation o \u2208 \u2126 is produced with probability O(r, o). The POAPS agent reasons about the POAPS values in the program using observations in order to make decisions.\nConsider the program in Figure 2. The argument text is actually bound to two values. The first value, the string, is what the programmer cares about. The second POAPS value can be thought of as some unobservable measure of the quality of the text q \u2208 [0, 1]. The domain of this second value was implicitly specified by an expert when he defined the primitive c-imp. When c-imp is called, in addition to returning an improved string, a POAPS value q\u2032 \u2208 [0, 1] is also returned with probabilities defined by T . Then, the program recurses and text is now bound to both the new string and q\u2032. In this example, no observation is produced.\nWe emphasize that the expert, the program, and the POAPS agent, may not know the POAPS values. The POAPS values may be unobservable, so the best an expert and an agent can do is hold a belief about what they might be, using the observations as hints. Therefore, the next step in POAPS is to compile a POAPS program into a POMDP, and then solve the POMDP to generate a policy that controls the program based on the beliefs about the hidden values of the variables.\nAs another example, we provide a description of the voting program we present in Figure 1. The program uses three primitives: +, >, and crowd-vote. The POAPS values of q, a0, and a1 can be thought of as unobservable measures of the difficulty of the question and the quality of the two answers, respectively. The POAPS values of c0 and c1 are observed, and are the same as their Normal values. crowd-vote\u2019s range states are the same as its observations (R = \u2126), and its observation function is defined as O(r \u2208 R, \u03c9 \u2208 \u2126) = 1 if and only if r = \u03c9. So, when it is called, it returns a POAPS value with probability defined by T , and the observation it emits is the same POAPS value. The Normal value it returns is also the same as the POAPS value. The primitives + and > are defined in the expected way. 2\nOf course, POAPS programs do not restrict users to calling primitives. Users can also call their own user-defined functions. For example, they can use their program for voting (Figure 1) in a program for iterative-improvement (Figure 3). We note that in the program for voting, the operators > and + are primitives. When a function calls another user-defined function, the semantics are \u201ccall-\n2We note here that for planning purposes, the if construct in our language uses the POAPS value of its test expression to determine which branch to take instead of the Normal value. The next section will show how during execution we insert observations to tell our agent what branch was actually taken.\n(define (it-i image worse-text better-text)\nby-poaps-value.\u201d Quite simply, in contrast to the normal \u201ccall-byvalue\u201d semantics where only one value is copied and passed, in our language, both the normal value and POAPS value are copied and passed.\nWe now define a compiler for the POAPS programming language, which converts the language into a POMDP."}, {"heading": "6. THE COMPILER", "text": "Before we delve into the technical details of the compiler, we provide a high-level description of the process. The whole point of converting a POAPS adaptive program into a POMDP is to enable construction of an optimal policy for the program, but this requires an optimality criterion. Since optimality is different for every user, we need the flexibility to construct different utility functions or goals for individual users. In light of these challenges, we assume that executing a primitive incurs a cost defined by the primitive, but that an externally-provided and expert-defined mechanism for goal or utility elicitation (e.g. [6]) is used to guide the overall program objective. For example, consider the voting program of Figure 1. It might cost $0.05 to execute the crowd-vote primitive, but learning a given user\u2019s desired target accuracy in order to guide the execution of the program requires additional information. A reasonable goal elicitation module for a POMDP compiled from this program is one that simply asks the user for a desired accuracy and budget, and converts the desired accuracy into a goal belief state and the budget into a horizon to ensure no dead ends. Such a goal elicitation module could be used for any program that outputs \u201ccorrect answers\u201d and uses crowd-vote.\nAlternatively, users can forego providing their goals/utilities with an external mechanism and simply write goals into their programs. For example, they can simply write their own termination conditions that rely only on the visible parts of their programs. Whether or not we have elicited goals/utilities, our goal is to execute the branches that minimize the expected sum of costs.\nFrom our description of the POAPS language, we have a very natural, but unbounded, decision process that emerges. This decision problem can be posed as a history-based MDP. The state of the MDP consists of all the branches taken and observations received so far. An action in the MDP is choosing a branch in the program. Taking an action produces observations and costs, so the transition function (from a list of actions and observations to another list of actions and observations) is completely determined by the dynamics of the underlying primitives and our \u201ccall-by-poaps-value\u201d semantics.\nHowever, we do not want to define such an MDP because solution methods will not scale. Instead we define an equivalent POMDP. We now define the POAPS compiler that produces this POMDP by\ndescribing in detail the process that converts any POAPS program into a POMDP. Given an input program p, the compiler converts p into a POMDP (M \u25e6 S)(p) by the following steps:\n1. Define a set of states S(p) by statically analyzing p. Each state variable of S(p) will represent the POAPS value of some variable or expression in p or a function called by p. So, we call a state c \u2208 S(p) a control state, because it is the part of the state that determines what action should be taken.\n2. Construct a Hierarchical Abstract Machine (HAM) [17]M(p) by evaluating the program under a set of operational semantics. A HAM is a type of nondeterministic finite state machine. Each state m \u2208 M(p) will be a representation of the current program counter. In other words, it tells the agent where in the program it currently is. So, we call this state the machine state. This state will be fully observable, and provides information about what actions are available to take.\n3. Following the insights of [17], mergeM(p) with S(p) to create a POMDP (M\u25e6S)(p) with state space S\u0302 = StatesOf(M(p))\u00d7 S(p), and define the actions, transition function and observation function of (M \u25e6 S)(p) by traversing M(p) and applying a set of rules. Therefore, a single state in our POMDP will be a tuple (m, c), where one part of the state is the machine state, and the other part of the state is the control state. Finally, using a separate goal/utility elicitation module, integrate the goal or rewards into the POMDP."}, {"heading": "6.1 Step 1: Creation of a State Space S", "text": "First, we need to define the state variables for the arguments of p. Let X(p) = {arg1, . . . , argn} be the set of all arguments of p. In order to construct S(p), the compiler needs to know the state space of each argument. The state space of an argument argi is defined by the domain state space Di of the primitives that use argi 3.\nNext, we need to define state variables for all subexpressions in p. A program p in our language can be viewed as an evaluation tree of expressions. For example, Figure 4 shows the corresponding tree for the improve program (Figure 2). In order to remember all state necessary to control, we have a state variable for each subexpression in p. We denote this state space R(p).\nLet F (p) be the set of POAPS programs corresponding to userdefined functions that are called in p. Then, we abuse notation for ease of understanding, and recursively define S(p) = S(F (p)) \u00d7 Domain(X(p))\u00d7Domain(R(p)). Thus, the state space that we have constructed is a cross product of the state spaces of all the functions that p calls, the state spaces of all the arguments of p, and the state space which consists of all possible evaluations of every expression in p. This state space is the control state space. Since 3We assume that all the primitives that use a variable argi have the same state space Di. We can relax this assumption by using typing techniques.\nwe use Monte-Carlo solution methods to solve our POMDPs, we do not need to express the state in a closed, non-recursive form."}, {"heading": "6.2 Step 2: Construction of a HAM", "text": "The second step in the compilation process is to construct the machine state space by constructing a HAM [17]M(p) given p and S(p). The HAM\u2019s states will be used in our constructed POMDP as observable state variables that represent the current program counter. Each HAM state represents the evaluation of an expression. In other words, the HAM will be the part of the POMDP that says where in the evaluation tree we are for a program p.\nThe five types of states of a HAM are Action, Call, Choice, Start, and Stop. Call states represent a call to a user-defined function. They will execute the corresponding HAM. Choice states can transition to one of many HAMs. Stop states signify the end of execution of a HAM and return control to the next state of the parent calling HAM. Start states denote the initial HAM state. Action states represent the evaluation of a symbol or constant, or the execution of a primitive.\nFinally, we add a sixth type of state: an Observation State. Obs states do not represent the evaluation of any expression in p. These states will do nothing except emit an observation. These states are inserted after conditionals so that an agent can eliminate inconsistent beliefs. These states were not necessary in [17] because their world was fully observable.\nWe evaluate the program p to a HAM by using inference rules in the same way computer programs are evaluated by interpreters. We recursively evaluate subexpressions to HAMs using inferences rules and then combine them into larger and larger HAMs for each parent expression until we have a HAM for p.\nConsider the improve program in Figure 2. We first use an inference rule for define, which leads to using a choice inference rule. We provide a simplified version of the choice inference rule here.\nCHOICE H ; ei \u21d3Mi\nH ; (choose e1 \u00b7 \u00b7 \u00b7 en) \u21d3 Choice(M1, . . . ,Mn)\nThe rule says that if each expression ei evaluates to a HAM Mi under the heapH , then the expression (choose e1 \u00b7 \u00b7 \u00b7 en) evaluates to a HAM that contains a Choice node that can transition to any of the HAMs Mi. Thus, for the improve program, when we evaluate the (choose...) subexpression, there are two expressions that needs to be recursively evaluated. The result is the HAM in Figure 5.\nAfter we construct a HAM, we post-process by adding a Start state to the beginning and a Stop state to the end. Additionally, if we see any tail calls (Call states that are leaf nodes), we can add an edge from the call state to the beginning of the HAM, and change the semantics of that call state so that it transitions to the next state instead of executing another HAM as a subroutine."}, {"heading": "6.3 Step 3: Putting It All Together", "text": "Letting S(M(p)) denote the set of states of a HAM M(p), the\nstate space of the POMDP, (M \u25e6S)(p), that we construct is S\u0302(p) = S(p)\u00d7 StatesOf(M(p)).\nThe actions depend only on the current machine statem \u2208M(p), which is fully observable. In any machine state that is not a Choice state, the agent can only take one action. If the machine state is a Choice state, then the actions are the branches of the Choice state.\nWe define the transition function T (s, a, s\u2032) of (M \u25e6S)(p) such that values are passed around correctly between states to enforce\ncall-by-poaps-value semantics. The observation function O(s\u2032, o) is simple. Observations are only received in two scenarios. First, observations can be received when executing a primitive and they are defined by the primitive. Second, observations can be received when transitioning to a HAM observation state."}, {"heading": "7. OPTIMALITY", "text": "We now show that the POMDP we construct is correct, in that its optimal policies result in the optimal executions of its corresponding program.\nLEMMA 1. For any POMDP (M \u25e6 S)(p) for a program p, let C be the set of choice belief states, which are the belief states in which the machine component of every possible world state is a choice node. There exists a semi-MDP (m \u25e6 s)(p) with state space C, such that an optimal policy \u03c0 for (m \u25e6 s)(p) corresponds to an optimal policy \u03a0 for (M \u25e6 S)(p), in that \u03a0 simply augments \u03c0 by mapping belief states not in the domain of \u03c0 to their single, default actions.\nPROOF. Consider the belief-MDP that corresponds to (M\u25e6S)(p). Consider the states that are not choice beliefs. We can remove these states to produce an equivalent belief-Semi-MDP. The optimal policy for this belief-Semi-MDP is the same as the optimal policy for the belief-MDP, and thus the same as the optimal policy for (M \u25e6 S)(p).\nTHEOREM 1. LetM be the history-based MDP associated with a program p. Then an optimal policy for the POMDP we generate, (M \u25e6 S)(p), can be used as an optimal policy forM.\nPROOF. M and (m\u25e6s)(p) are stochastically bisimilar [10] (we map each history to its corresponding belief state), and the optimal policy of (m \u25e6 s)(p) corresponds to that of (M \u25e6 S)(p) (Lemma 1), so the optimal policy for (M \u25e6 S)(p) can be used as an optimal policy forM (Lemma 1).\nThis theorem also affirms that an optimal policy for our constructed POMDP is not just a \u201crecursively optimal\u201d [9] policy. For example, suppose a user has written a program p that calls some other userdefined program f that also calls some other user-defined program g. Then, the optimal policy\u2019s actions while in g consider not only the state local to g, but also the state local to f and the state local to p. In other words, an optimal policy for (M \u25e6 S)(p) does not solve lots of primitive POMDPs in isolation. Notably, a POMDP\nsolver that produces an optimal policy will update beliefs about state variables local to p even when observations are made about correlating state variables local to g."}, {"heading": "8. MONTE CARLO PLANNING", "text": "We solve the POMDP when a user runs a program. The POMDP that we construct can potentially have many unreachable states. Additionally, we do not want to construct the entire state space or the full matrix representation of the transition and observation functions, since these can be very large or infinite. Therefore, we choose to use online Monte-Carlo methods to solve the POMDP. While we try using a UCT-based solver, POMCP (without a rollout policy) 4 [21], we find that the value function can take an extraordinary amount of time to converge.\nInstead, we modify RTDP-Bel [3] to create C-RTDP, an algorithm similar to HAMQ-learning [17] that takes advantage of the fact that the actual complexity of the POMDP is determined by the number of choice points. C-RTDP modifies RTDP-Bel by only performing backups on choice beliefs. In the POMDP that we construct, all the states that have non-zero probability in a reachable belief state will always have the same machine state. A choice belief is one in which the machine component of every state is a choice node.\nWe fully specify our algorithm below. boa means the belief state given that the agent had belief state b, then took action a and received observations o. Coa is the expected cost of taking action a in belief b and receiving observations o.\nAlgorithm 1: C-RTDP (One simulation) Initialize belief b = b0 Sample s = (m, c) \u223c b if m is terminal then\nReturn end if m is a choice node then\nfor every action a do \u03a8 = \u222ai\u2208N{(o1, . . . , oi)|bo1,...,oia is choice belief} Q(a, b) = \u2211 \u03c8\u2208\u03a8 P (\u03c8|a)(C \u03c8 a + V (b \u03c8 a ))\na\u0302 = minaQ(a, b)\nend else\na\u0302 = default action end Update V (b) = Q(a\u0302, b) Sample s\u2032 \u223c T (s, a\u0302, s\u2032), o \u223c O(a\u0302, s\u2032) Update b0 = boa\u0302 and repeat\nWe can show that C-RTDP converges to the optimal policy:\nTHEOREM 2. For any POMDP (M \u25e6 S)(p) for a program p, C-RTDP will converge to the optimal policy with probability 1.\nPROOF. For a POMDP (M \u25e6 S)(p), C-RTDP solves the corresponding belief-Semi-MDP. Then by Lemma 1, we have that CRTDP solves (M \u25e6 S)(p).\nBy using a Monte-Carlo, online approach, we gain several advantages. Initializing the initial belief is easy. Suppose a user runs a program for a function f with arguments arg0 . . . , argn. None 4Since POAPS is a general representation language, the existence of a rollout policy cannot be assumed.\nof the expressions have been evaluated yet, so we only need to initialize our belief of the arguments argi. If argi is observable, as defined by the primitives that use it inside f , then we simply define the POAPS value of argi to be equal to the Normal value. Therefore, if a state is observable, its space can be infinite. If argi is unobservable, then we initialize a uniform belief over the state space defined by the primitives that use it."}, {"heading": "9. PROOF-OF-CONCEPT", "text": "As a proof of concept that POAPS can run programs, we implement the POAPS system and write the voting program from the introduction. We also implement a goal eliciation module. The primitive crowd-vote has a cost of 1 cent and we specify a goal accuracy of 90%. We run the program on Mechanical Turk with 1100 named entity recognition tasks. Each named entity recognition task begins by providing the worker with a body of text and an entity, like \u201cWashington led the troops into battle.\u201d Then it uses Wikification [16, 19] to find a set of possible Wikipedia articles describing the entity, such as \u201cWashington (state)\u201d and \u201cGeorge Washington,\u201d and asks workers to choose the article that best describes the entity. POAPS achieves an overall accuracy of 87.73% with an average cost of 4.33 cents. This result is consistent with those in [7], showing that our general purpose implementation can perform at par compared to an expert-written problem-specific POMDP, suggesting the value of our system to end-users."}, {"heading": "10. A LARGER EXAMPLE", "text": "Throughout this paper, we have expressed several practical crowdsourcing problems in our language. Note that all our programs have only used two non-trivial expert-defined primitives: c-imp and c-vote (trivial Lisp-primitives like + and > whose hidden behaviors are identical to their visible behaviors come with POAPS). We demonstrate the versatility of our paradigm by writing find-fix-verify [4], a more complex and popular workflow that can be used for crowdsourcing edits to text.\nFor example, given a piece of text as input (like a term paper), it first asks crowdworkers to find patches in the text that need attention. Then, given these subsets of text, it asks workers to revise them. Finally, given the revisions, it asks workers to verify that the revisions are better, through some voting mechanism.\nIn Figure 6, we show the POAPS program for find-fix-verify (ffv). In addition to using c-imp and c-vote, we only need to use one more non-trivial expert-defined primitive: c-find, which asks a worker to provide an interval of text that requires attention. However, note that we can potentially use only c-imp and c-vote by eliminating c-find and replacing it with c-vote where the possible answers are a set of intervals. These primitives provide all the information we need to construct the POMDP for the program. The domain state spaces of the primitives provides the state spaces of the POAPS values of the arguments to those primitives, and the transition functions describe the probabilities of the POAPS values of the returns.\nThe program also uses trivial string and list manipulation primitives like get-relevant-text, replace-text, and merge, which like + and >, do not need to be expertly defined. worse-text represents what we think is the worse-text and better-text represents what we think is the better text. We call ffv with worse-text bound to an empty string and better-text bound to the text we want to improve.\nThere are three choices. We can: 1) find mistakes in and fix the better text (find-fix and then recurse, or 2) verify which version of the text is better and recurse, or 3) return the better text.\n(define (ffv worse-text better-text)\n(choose\n(ffv better-text\n(find-fix better-text))\n(if (vote-better? \u2019which is better?\u2019 better-text\nworse-text 0 0)\n(ffv worse-text better-text)\n(ffv better-text worse-text))\nbetter-text))\n(define (find-fix text)\n(fix text (find text \u2019()))\n(define (fix text intervals)\n(choose\n(let ((next-int (choose intervals))\n(next-text (get-relevant-text text next-int))\n(better-text (c-imp next-text)))\n(fix (replace-text text next-int better-text)\nintervals))\ntext))\n(define (find text intervals)\nfind-fix first calls find to repeatedly ask workers to provide intervals in the text that require work. Then, it calls fix with those intervals and repeatedly asks workers to improve the text in those intervals.\nA simple goal elicitation module for this program could simply ask the user whether they would like a \u201cAlmost-Perfect\u201d text, an \u201cExcellent\u201d text, or a \u201cSatisfactory\u201d text. It would translate the choice into a goal belief on the hidden quality of the text, and then minimize the expected cost of achieving that goal.\nFor even more examples of programs we can write, please refer to the Appendix."}, {"heading": "11. CONCLUSION", "text": "We have presented POAPS, a system that provides a language for writing decision processes that provides an abstraction over POMDPs. Knowledge of POMDPs is not a prerequisite for being able to use decision-theory in everyday applications. In particular, the states and dynamics of POMDPs are hidden from users. We have shown how crowdsourcing experts can use POAPS to build and optimally control many of their workflows. We have also implemented POAPS and conducted a proof-of-concept experiment that shows that POAPS can run the voting program of Figure 1 and achieve results comparable to an expert-written problem-specific POMDP. The complete POAPS system that we have built will be available at the authors\u2019 websites."}, {"heading": "12. FUTURE WORK", "text": "Our work on POAPS is just the beginning. We imagine many future directions: 1) A key question is whether or not POAPS is easy to use by\npeople who are not planning experts. A comprehensive answer to this question requires a user study of our complete system.\n2) POAPS does not allow access to underlying POMDP details through its language. But ideally, we would like to allow users to modify whatever aspects they understand (e.g., a subset of state variables or costs). We imagine an extension of POAPS that allows users to work with POMDP/primitive components in their programs.\n3) POAPS allows users to specify hard constraints on policies. By writing an adaptive program, they are exactly specifying the policies that may be chosen by a planner. In other words, they not only specify a POMDP, but they also specify a partial policy on that POMDP, by limiting the actions that can be taken in a given state. For example, in the voting program (Figure 1), if an agent decides to stop asking the crowd for more votes, it can only return the answer that received more votes. It is not allowed to return the answer that received fewer votes. We imagine a non-trivial extension of POAPS that allows users to specify soft constraints, in the same way that UCT allows users to specify a rollout policy to guide search. In this framework, POAPS would be able to deviate from the program. For example, instead of always returning the answer with more votes, it might return the answer with fewer votes, because maybe the user did not foresee that sometimes the answer with fewer votes is more likely to be correct.\n4) POAPS assumes that all the primitives that use the same variable specify the same state space for that variable. Such a restriction makes life more difficult for non-experts. In particular, this assumption may lead to unforeseen crashing or unexpected behavior. However, we envision at least two methods for ameliorating these scenarios. The first is to type our language, making it impossible to write programs that would crash the compiler or planner. The second is to use polymorphic typing or subtyping so that primitives are more flexible.\n5) Solving large POMDPs is a hard problem, and POAPS creates large POMDPs. The scalability of our system is a weakness that we hope to address. One way to reduce the size of the POMDPs that POAPS creates is to use state abstraction. If we can analyze the programs to determine the states that are irrelevant for making decisions, we can eliminate them and significantly increase the size of the programs that we can write.\n6) While a procedural Lisp-like language is easy for us to compile and interpret, we believe that most users prefer a more imperative C-like language. Converting the POAPS language to one with a more familiar syntax should increase usability and adoption.\n7) Finally, POAPS assumes that experts either know, or can write down the models for their primitives. However, we can easily extend the language to allow experts to direct POAPS to learn the models using reinforcement learning, thereby reducing the amount of work that experts need to put into the system."}, {"heading": "13. REFERENCES", "text": "[1] D. Andre and S. J. Russell. Programmable reinforcement\nlearning agents. In NIPS, 2001. [2] D. Andre and S. J. Russell. State abstraction for\nprogrammable reinforcement learning agents. In AAAI, 2002. [3] A. G. Barto, S. J. Bradtke, and S. P. Singh. Learning to act\nusing real-time dynamic programming. Artificial Intelligence, 72:81\u2013138, 1995.\n[4] M. S. Bernstein, G. Little, R. C. Miller, B. Hartmann, M. S. Ackerman, D. R. Karger, D. Crowell, and K. Panovich. Soylent: A word processor with a crowd inside. In UIST, 2010.\n[5] C. Boutilier, R. Reiter, M. Soutchanski, and S. Thrun. Decision-theoretic, high-level agent programming in the situation calculus. In AAAI, 2000.\n[6] U. Chajewska, D. Koller, and R. Parr. Making rational decisions using adaptive utility elicitation. In AAAI, 2000.\n[7] P. Dai, C. H. Lin, Mausam, and D. S. Weld. Pomdp-based control of workflows for crowdsourcing. Artificial Intelligence, 202:52\u201385, 2013.\n[8] P. Dai, Mausam, and D. S. Weld. Decision-theoretic control of crowd-sourced workflows. In AAAI, 2010.\n[9] T. G. Dietterich. Hierarchical reinforcement learning with the maxq value function decomposition. Journal of Artificial Intelligence Research, 13:227\u2013303, 2000.\n[10] R. Givan, T. Dean, and M. Greig. Equivalence notions and model minimization in markov decision processes. Artificial Intelligence, 147:163\u2013223, 2003.\n[11] E. Kamar, S. Hacker, and E. Horvitz. Combining human and machine intelligence in large-scale crowdsourcing. In AAMAS, 2012.\n[12] C. H. Lin, Mausam, and D. S. Weld. Dynamically switching between synergistic workflows for crowdsourcing. In AAAI, 2012.\n[13] G. Little, L. B. Chilton, M. Goldman, and R. C. Miller. Turkit: tools for iterative tasks on mechanical turk. In KDD-HCOMP, pages 29\u201330, 2009.\n[14] B. Marthi, S. Russell, D. Latham, and C. Guestrin. Concurrent hierarchical reinforcement learning. In IJCAI, 2005.\n[15] D. McAllester. Bellman equations for stochastic programs, 1999.\n[16] D. Milne and I. H. Witten. Learning to link with wikipedia. In Proceedings of the ACM Conference on Information and Knowledge Management, 2008.\n[17] R. Parr and S. Russell. Reinforcement learning with hierarachies of machines. In NIPS, 1998.\n[18] J. Pinto, A. Fern, T. Bauer, and M. Erwig. Robust learning for adaptive programs by leveraging program structure. In ICMLA, 2010.\n[19] L. Ratinov, D. Roth, D. Downey, and M. Anderson. Local and global algorithms for disambiguation to wikipedia. In Proceedings of the Annual Meeting of the Association of Computational Linguistics, 2011.\n[20] S. Sanner. Relational dynamic influence diagram language (rddl): Language description. Technical report, NICTA and the Australian National University, 2011.\n[21] D. Silver and J. Veness. Monte-carlo planning in large pomdps. In NIPS, 2010.\n[22] C. Simpkins, S. Bhat, C. I. Jr., and M. Mateas. Towards adaptive programming: Integrating reinforcement learning into a programming language. In OOPSLA, 2008.\n[23] D. S. Weld, Mausam, and P. Dai. Human intelligence needs artificial intelligence. In HCOMP, 2011.\n[24] H. L. S. Younes and M. L. Littman. Ppddl1.0: The language for the probabilistic part of ipc-4. In IPC, 2004.\nAPPENDIX We have shown how to write a program that polls workers in order to find the best answer to some question. However, requesters can do better by asking the question in multiple ways [12]. Figure 7 shows how to write a voting program if you have two methods for asking the same question.\nWhile the primary goal of POAPS is to enable non-experts to write POMDPs, experts can also use POAPS to quickly build large and complex POMDPs. Figure 8 shows how one can use POAPS to write a goal-based rocksample. We note that the program we write constrains the possible policies to ones that are more likely to be optimal (though it may not include the most optimal policy).\n(define (m-vote q0 q1 a0 a1 c0 c1)\n(define (move start end)\n(if (= start end)\nend\n(choose(move (move-north start) end)\n(move (move-south start) end)\n(move (move-east start) end)\n(move (move-west start) end))))\n(define (r-s pos rocks exit-pos)\nmove-* and sample are the primitives that need to expertlydefined. These definitions boostrap the creation of the POMDP. For example, The POAPS value of each rock can be defined by the primitives as a pair, where the first element is a binary indicator of whether or not the rock is good, and the second element is the position of the rock. The Normal value of each rock can also be a pair, where the first element is a rock id and the second element is its position. Then, the behavior of the POAPS values of the return values of these primitives (and thereby all subexpressions that use the return values), are given by the expert-defined transition probabilities. find-good-rock is a user-defined function (not shown) that can be viewed as a generalization of the voting program. r-s is the rocksample program. It contains two choice points. The first choice is to simply move to the exit. The second choice is to first find a good rock, move to where the good rock is, sample it (a primitive), remove the rock from our list of rocks, and recurse.\nTo define the policy, we can write a goal elicitation module that asks the user how many rocks should be sampled before quitting. For example, if the user specifies that all rocks be sampled, then the agent should find the expected minimum cost policy to sample all the rocks, where the costs are given by the primitives."}], "references": [{"title": "Programmable reinforcement learning agents", "author": ["D. Andre", "S.J. Russell"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "State abstraction for programmable reinforcement learning agents", "author": ["D. Andre", "S.J. Russell"], "venue": "In AAAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Learning to act using real-time dynamic programming", "author": ["A.G. Barto", "S.J. Bradtke", "S.P. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Soylent: A word processor with a crowd inside", "author": ["M.S. Bernstein", "G. Little", "R.C. Miller", "B. Hartmann", "M.S. Ackerman", "D.R. Karger", "D. Crowell", "K. Panovich"], "venue": "In UIST,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Decision-theoretic, high-level agent programming in the situation calculus", "author": ["C. Boutilier", "R. Reiter", "M. Soutchanski", "S. Thrun"], "venue": "In AAAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Making rational decisions using adaptive utility elicitation", "author": ["U. Chajewska", "D. Koller", "R. Parr"], "venue": "In AAAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Pomdp-based control of workflows for crowdsourcing", "author": ["P. Dai", "C.H. Lin", "Mausam", "D.S. Weld"], "venue": "Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Decision-theoretic control of crowd-sourced workflows", "author": ["P. Dai", "Mausam", "D.S. Weld"], "venue": "In AAAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["T.G. Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Equivalence notions and model minimization in markov decision processes", "author": ["R. Givan", "T. Dean", "M. Greig"], "venue": "Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Combining human and machine intelligence in large-scale crowdsourcing", "author": ["E. Kamar", "S. Hacker", "E. Horvitz"], "venue": "In AAMAS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Dynamically switching between synergistic workflows for crowdsourcing", "author": ["C.H. Lin", "Mausam", "D.S. Weld"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Turkit: tools for iterative tasks on mechanical turk", "author": ["G. Little", "L.B. Chilton", "M. Goldman", "R.C. Miller"], "venue": "In KDD-HCOMP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Concurrent hierarchical reinforcement learning", "author": ["B. Marthi", "S. Russell", "D. Latham", "C. Guestrin"], "venue": "In IJCAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Bellman equations for stochastic programs", "author": ["D. McAllester"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Learning to link with wikipedia", "author": ["D. Milne", "I.H. Witten"], "venue": "In Proceedings of the ACM Conference on Information and Knowledge Management,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Reinforcement learning with hierarachies of machines", "author": ["R. Parr", "S. Russell"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Robust learning for adaptive programs by leveraging program structure", "author": ["J. Pinto", "A. Fern", "T. Bauer", "M. Erwig"], "venue": "In ICMLA,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Local and global algorithms for disambiguation to wikipedia", "author": ["L. Ratinov", "D. Roth", "D. Downey", "M. Anderson"], "venue": "In Proceedings of the Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Relational dynamic influence diagram language (rddl): Language description", "author": ["S. Sanner"], "venue": "Technical report, NICTA and the Australian National University,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Monte-carlo planning in large pomdps", "author": ["D. Silver", "J. Veness"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Towards adaptive programming: Integrating reinforcement learning into a programming language", "author": ["C. Simpkins", "S. Bhat", "C.I. Jr.", "M. Mateas"], "venue": "In OOPSLA,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Human intelligence needs artificial intelligence", "author": ["D.S. Weld", "Mausam", "P. Dai"], "venue": "In HCOMP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Ppddl1.0: The language for the probabilistic part of ipc-4", "author": ["H.L.S. Younes", "M.L. Littman"], "venue": "In IPC,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}], "referenceMentions": [{"referenceID": 22, "context": "Those who would greatly like access to such tools, like many in the crowdsourcing community [23] for example, must either resort to sub-optimal techniques that use approximate heuristics or hire a planning expert to formally define and solve their domain-specific problems.", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "Like many previous approaches to proceduralizing decision processes[2, 5, 18], we create a language that includes choice points, which allow the system to make optimal decisions adaptively.", "startOffset": 67, "endOffset": 77}, {"referenceID": 4, "context": "Like many previous approaches to proceduralizing decision processes[2, 5, 18], we create a language that includes choice points, which allow the system to make optimal decisions adaptively.", "startOffset": 67, "endOffset": 77}, {"referenceID": 17, "context": "Like many previous approaches to proceduralizing decision processes[2, 5, 18], we create a language that includes choice points, which allow the system to make optimal decisions adaptively.", "startOffset": 67, "endOffset": 77}, {"referenceID": 7, "context": "Instead of hiring a planning expert to handcraft a custom POMDP for this simple voting problem [8, 11], users, and in particular, non-experts, should be able to write a very simple program that abstracts away from state variables and probabilities: either ask another worker for another label and recurse, or return the label with the most number of votes.", "startOffset": 95, "endOffset": 102}, {"referenceID": 10, "context": "Instead of hiring a planning expert to handcraft a custom POMDP for this simple voting problem [8, 11], users, and in particular, non-experts, should be able to write a very simple program that abstracts away from state variables and probabilities: either ask another worker for another label and recurse, or return the label with the most number of votes.", "startOffset": 95, "endOffset": 102}, {"referenceID": 23, "context": "Examples include Cassandra-style format , Probabilistic PDDL [24], and RDDL [20].", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "Examples include Cassandra-style format , Probabilistic PDDL [24], and RDDL [20].", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "Several procedural languages have also been developed including ABL [22], ALisp [2] and concurrent ALisp [14], Hierarchical Abstract Machines (HAMs) [17], and Programmable Hierarchical Abstract Machines (PHAMs) [1].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "Several procedural languages have also been developed including ABL [22], ALisp [2] and concurrent ALisp [14], Hierarchical Abstract Machines (HAMs) [17], and Programmable Hierarchical Abstract Machines (PHAMs) [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "Several procedural languages have also been developed including ABL [22], ALisp [2] and concurrent ALisp [14], Hierarchical Abstract Machines (HAMs) [17], and Programmable Hierarchical Abstract Machines (PHAMs) [1].", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "Several procedural languages have also been developed including ABL [22], ALisp [2] and concurrent ALisp [14], Hierarchical Abstract Machines (HAMs) [17], and Programmable Hierarchical Abstract Machines (PHAMs) [1].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "Several procedural languages have also been developed including ABL [22], ALisp [2] and concurrent ALisp [14], Hierarchical Abstract Machines (HAMs) [17], and Programmable Hierarchical Abstract Machines (PHAMs) [1].", "startOffset": 211, "endOffset": 214}, {"referenceID": 4, "context": "DTGolog [5] is a situation calculus-based procedural language that can both define decision problems (by defining a set of axioms) and specify control.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "Stochastic Programs (SP) [15] provide a language for experts to write world models and primitive actions and then compose those primitive actions to create control policies for the corresponding world model.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "org The work on adaptive programs [18] allows non-expert users to quickly construct observable decision processes by writing programs that can contain optimizable choice points.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "Another example of a workflow is the iterative improvement workflow [13].", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "Previous work has hand-crafted a POMDP for this particular workflow in order to make dynamic decisions like when to vote and when to stop, showing significant savings over static policies [8].", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "\u2022 T : D \u00d7R \u2192 [0, 1] is a transition function.", "startOffset": 13, "endOffset": 19}, {"referenceID": 0, "context": "\u2022 O : R\u00d7 \u03a9\u2192 [0, 1] is an observation function.", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "We define D = R = [0, 1] to represent the hidden quality of the artifact.", "startOffset": 18, "endOffset": 24}, {"referenceID": 1, "context": "Following previous work [2, 5, 18], we add the special form (choose <exp0> <exp1> .", "startOffset": 24, "endOffset": 34}, {"referenceID": 4, "context": "Following previous work [2, 5, 18], we add the special form (choose <exp0> <exp1> .", "startOffset": 24, "endOffset": 34}, {"referenceID": 17, "context": "Following previous work [2, 5, 18], we add the special form (choose <exp0> <exp1> .", "startOffset": 24, "endOffset": 34}, {"referenceID": 0, "context": "The second POAPS value can be thought of as some unobservable measure of the quality of the text q \u2208 [0, 1].", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "When c-imp is called, in addition to returning an improved string, a POAPS value q\u2032 \u2208 [0, 1] is also returned with probabilities defined by T .", "startOffset": 86, "endOffset": 92}, {"referenceID": 5, "context": "[6]) is used to guide the overall program objective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Construct a Hierarchical Abstract Machine (HAM) [17]M(p) by evaluating the program under a set of operational semantics.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "Following the insights of [17], mergeM(p) with S(p) to cre-", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "The second step in the compilation process is to construct the machine state space by constructing a HAM [17]M(p) given p and S(p).", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "These states were not necessary in [17] because their world was fully observable.", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "M and (m\u25e6s)(p) are stochastically bisimilar [10] (we map each history to its corresponding belief state), and the optimal policy of (m \u25e6 s)(p) corresponds to that of (M \u25e6 S)(p) (Lemma 1), so the optimal policy for (M \u25e6 S)(p) can be used as an optimal policy forM (Lemma 1).", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "This theorem also affirms that an optimal policy for our constructed POMDP is not just a \u201crecursively optimal\u201d [9] policy.", "startOffset": 111, "endOffset": 114}, {"referenceID": 20, "context": "While we try using a UCT-based solver, POMCP (without a rollout policy) 4 [21], we find that the value function can take an extraordinary amount of time to converge.", "startOffset": 74, "endOffset": 78}, {"referenceID": 2, "context": "Instead, we modify RTDP-Bel [3] to create C-RTDP, an algorithm similar to HAMQ-learning [17] that takes advantage of the fact that the actual complexity of the POMDP is determined by the number of choice points.", "startOffset": 28, "endOffset": 31}, {"referenceID": 16, "context": "Instead, we modify RTDP-Bel [3] to create C-RTDP, an algorithm similar to HAMQ-learning [17] that takes advantage of the fact that the actual complexity of the POMDP is determined by the number of choice points.", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "\u201d Then it uses Wikification [16, 19] to find a set of possible Wikipedia articles describing the entity, such as \u201cWashington (state)\u201d and \u201cGeorge Washington,\u201d and asks workers to choose the article that best describes the entity.", "startOffset": 28, "endOffset": 36}, {"referenceID": 18, "context": "\u201d Then it uses Wikification [16, 19] to find a set of possible Wikipedia articles describing the entity, such as \u201cWashington (state)\u201d and \u201cGeorge Washington,\u201d and asks workers to choose the article that best describes the entity.", "startOffset": 28, "endOffset": 36}, {"referenceID": 6, "context": "This result is consistent with those in [7], showing that our general purpose implementation can perform at par compared to an expert-written problem-specific POMDP, suggesting the value of our system to end-users.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "We demonstrate the versatility of our paradigm by writing find-fix-verify [4], a more complex and popular workflow that can be used for crowdsourcing edits to text.", "startOffset": 74, "endOffset": 77}], "year": 2016, "abstractText": "We present POAPS, a novel planning system for defining PartiallyObservable Markov Decision Processes (POMDPs) that abstracts away from POMDP details for the benefit of non-expert practitioners. POAPS includes an expressive adaptive programming language based on Lisp that has constructs for choice points that can be dynamically optimized. Non-experts can use our language to write adaptive programs that have partially observable components without needing to specify belief/hidden states or reason about probabilities. POAPS is also a compiler that defines and performs the transformation of any program written in our language into a POMDP with control knowledge. We demonstrate the generality and power of POAPS in the rapidly growing domain of human computation by describing its expressiveness and simplicity by writing several POAPS programs for common crowdsourcing tasks.", "creator": "LaTeX with hyperref package"}}}