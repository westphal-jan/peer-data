{"id": "1608.08925", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Recursive Partitioning for Personalization using Observational Data", "abstract": "We jins study the problem vicarious of neua learning to trochlea choose granot from m discrete treatment tannaitic options (e. kuibyshev g. , saudati medical drugs) dreamworld the coldstone one violencia with peshmerga best vokkaliga causal honking effect jamaah for kenitzer a particular instance (e. kangavar g. , patient) incompletions characterized by an observation of covariates. The training mitogen-activated data filippis consists jesic of dizzily observations disclosures of covariates, 59.88 treatment, and zuoji the outcome of bessarion the pareil treatment. We outhouse recast coupeaux the problem katta of learning malleable to personalize lifeline from polur these observational armorial data as esmeijer a single twenty-four learning okolona task, mukachevo which 45.38 we impedances use bournazel to develop glucosidase four specific usages machine professore learning carcelle methods to helix-loop-helix directly gigon address sacr\u00e9 the personalization problem, biar two with marburg a 78.4 unique fallar interpretability kudisch property. kanas We bassford also show combtooth how pish to validate medium-scale personalization models wieschaus on observational gloxinia data, proposing 91kg the new coefficient feltex of christen personalization fauziya as pellnas a unitless measure 30-stock of effectiveness. shiyu We demonstrate demonstrating the power pollinators of the new wirtschaft methods 4-35 in schilit two specific personalized medicine and ericson policymaking applications and show sleater they provide atomizing a aloys significant stimac advantage schwenningen over standard merhav approaches.", "histories": [["v1", "Wed, 31 Aug 2016 16:20:59 GMT  (588kb,D)", "http://arxiv.org/abs/1608.08925v1", null], ["v2", "Wed, 15 Mar 2017 22:59:40 GMT  (570kb,D)", "http://arxiv.org/abs/1608.08925v2", null], ["v3", "Tue, 1 Aug 2017 13:12:19 GMT  (673kb,D)", "http://arxiv.org/abs/1608.08925v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["nathan kallus"], "accepted": true, "id": "1608.08925"}, "pdf": {"name": "1608.08925.pdf", "metadata": {"source": "META", "title": "Learning to Personalize from Observational Data", "authors": ["Nathan Kallus"], "emails": ["kallus@cornell.edu"], "sections": [{"heading": null, "text": "We study the problem of learning to choose from m discrete treatment options (e.g., medical drugs) the one with best causal e\u21b5ect for a particular instance (e.g., patient) characterized by an observation of covariates. The training data consists of observations of covariates, treatment, and the outcome of the treatment. We recast the problem of learning to personalize from these observational data as a single learning task, which we use to develop four specific machine learning methods to directly address the personalization problem, two with a unique interpretability property. We also show how to validate personalization models on observational data, proposing the new coe cient of personalization as a unitless measure of e\u21b5ectiveness. We demonstrate the power of the new methods in two specific personalized medicine and policymaking applications and show they provide a significant advantage over standard approaches."}, {"heading": "1 Introduction", "text": "Personalization is the problem of determining the best treatment option for a given instance. A treatment can, for example, be a movie recommendation, a display ad, or a pharmacological therapy, and an instance is usually an individual person. In this paper, we study the\nar X\niv :1\n60 8.\n08 92\n5v 1\n[ st\nat .M\nproblem learning how to personalize from observational data, which is an important problem in emergent contexts such as personalized medicine. In this and related contexts, experimentation can be prohibitively small-scale, costly, dangerous, and unethical in comparison to passive data collection, which can be potentially massive. Just last month, President Obama announced an initiative to sign up millions of volunteers to donate their medical and related data to personalized medicine research [1]. However, this data source and ones like it, such as hospitals\u2019 electronic medical records (EMR), are purely observational and non-experimental, where the isolated causal e\u21b5ect of a particular treatment is hidden by a myriad confounding factors and needs to be carefully mined out. Standard approaches to the problem that apply supervised learning naively fall short in this setting, as we show here for the case of personalization and as [2] recently showed for the case of estimating heterogeneous causal e\u21b5ects. The urgent methodological question that we address is how to adapt the success of supervised machine learning to the prescriptive purpose of learning how to personalize treatments for maximal causal e\u21b5ect based on observational data. To this end, based on a new theoretical characterization, we propose new learning algorithms as well as evaluation methods used for validation, selection, and tuning.\nSpecifically, we consider the problem of learning how to assign the best of m treatments to an instance, given an observation of associated baseline covariates x 2 Rd. An instance is characterized by the random variables X 2 Rd and Y (1), . . . , Y (m) 2 R, which denote the covariates and the m potential outcomes of applying each of the treatments [3, Chs. 1- 2]. We use the convention that smaller outcome is better. A personalization model is a map \u2327 : Rd ! [m] = {1, . . . , m} that, given an observation of covariates x, prescribes a treatment \u2327(x). Its (out-of-sample) personalization risk is its average causal e\u21b5ect in the population R(\u2327) = E [Y (\u2327(X))] (the expectation is taken with respect to the joint distribution of X, Y (1), . . . , Y (m)). The Bayes (i.e., optimal) risk is R\u21e4 = R(\u2327 \u21e4), where \u2327 \u21e4(x) 2 T \u21e4(x) = arg mint2[m] E [Y (t) | X = x]. The learning task is to train a personalization model \u2327\u0302n(\u00b7) on n data points: Sn = {(X1, T1, Y1), . . . , (Xn, Tn, Yn)} , where the observed outcome Yi = Yi(Ti) corresponds only to the treatment Ti administered. This data is observational : we may not control the historic administration of treatment (as we would in a controlled experiment) and the values Yi(t) for\nt 6= Ti are missing data. We assume the data is independent and identically distributed (iid) and let X, T, Y, Y (1), . . . , Y (m) represent a generic draw. Our second assumption about the data is unconfoundedness:\nAssumption 1. For each t 2 [m]: Y (t) is independent of T given X and T = t has positive probability given almost every X, i.e., Y (t) ? T | X and P (P (T = t | X) > 0) = 1.\nThis assumption is standard in causal inference for ensuring identifiability [4] and is closely related to the backdoor criterion [5]. It says that we measured the right covariates to separate the e\u21b5ect of the treatment itself from the e\u21b5ect of assignment. Under Asn. 1, the conditional causal e\u21b5ect of treatment is equal to regressing Y on X, T : E [Y (t) | X = x] = E [Y (t) | X = x, T = t] = E [Y | X = x, T = t] ."}, {"heading": "1.1 Standard Approach: Regress and Compare", "text": "Since the optimal model \u2327 \u21e4 chooses a treatment by minimizing among the m conditional means, one obvious approach to personalization is to estimate m regression functions, each fitted to the subset of the data that received each treatment, and then use these to predict the conditional means and pick the smallest prediction. For example, in medicine, there is a vast literature on predicting patient-specific responses to treatment [6, 7] and picking the best by comparing predictions [8, 9], although recent work [10] has looked at alternative approaches for the case of randomized, experimental data.\nThe same approach is also generally taken in the related contextual multi-arm bandit problem [11], where, starting from no data, at each step i = 1, . . . , n, we observe a context Xi, choose a treatment arm t 2 [m], and experience a penalty Yi(t). The target is to achieve least overall penalty (usually as regret) by e ciently learning the best arm for each context. The main di\u21b5erences to our problem are (1) we consider an o\u270fine learning problem and (2) our data is observational whereas the data in a contextual bandit is the result of repeated controlled experiments. In each of [11, 12, 13], the proposed solution is to fit m regression functions, and, for a new instance, predict m outcomes and pick the smallest prediction (subject to cleverly ensuring su cient exploration by, e.g., adding optimistic confidence bounds that vanish with n). The regression, assumed linear, is done using ordinary least\nsquares (OLS) as in [12], ridge regression as in [11], or LASSO as in [13].\nThe regress and compare (R&C) approach to personalization from observational data\ncan be summarized as:\n1. For each t 2 [m]: (a) Consider the t-treated data subset St,nt = {(Xi, Yi) : i 2 [n], Ti = t} of size nt =\nPn i=1 I [Ti = t].\n(b) Fit a regression model \u00b5\u0302t,nt(x) to the nt datapoints in St,nt of the response Y to\nregressors X, e.g., by OLS.1\n2. Personalize by choosing the best estimated prediction: \u2327\u0302R&Cn (x) = arg mint2[m] \u00b5\u0302t,nt(x).\nUnder unconfoundedness, if our regression estimator is consistent then so is R&C per-\nsonalization consistent as shown below. All proofs are given in Sec. 5.\nTheorem 1. If Asn. 1 holds and \u00b5\u0302t,nt(x) are pointwise consistent regressions, i.e., \u00b5\u0302t,nt(X) ! E [Y | X, T = t] almost surely (a.s.) 8t 2 [m], then \u2327\u0302R&Cn (X) 2 T \u21e4(X) eventually a.s.\nExamples of pointwise consistent regression estimators are k-nearest neighbors (kNN) and kernel regression (see [14] for details). If a linear model is well-specified, then OLS is also pointwise consistent. While asymptotically consistent, R&C is not e\u21b5ective personalization because it attempts to learn more than it needs to, splits the training data, and addresses estimation risk rather than personalization risk. In what follows, we present algorithms for personalization that treat it as a single learning task and we demonstrate that doing so can lead to better personalization in real problems."}, {"heading": "1.2 Other Related Problems and Approaches", "text": "In learning heterogeneous causal e\u21b5ects [2, 15], one is concerned with the case of observational data with two treatments, t = 1 (\u201cControl\u201d) and t = 2 (\u201cTreatment\u201d), and the estimation of the relative conditional average treatment e\u21b5ect (CATE), (x) = E [Y (2) Y (1) | X = x]. 1Note that running OLS on each data subset St,nt is exactly equivalent to running OLS on the entire dataset and including all interaction terms with the dummy variables I [Ti = t]. On the other hand, running OLS on the whole dataset with no such interaction terms will always result in an R&C model that assigns the same treatment to everyone and performs no personalization.\nUnder Asn. 1, CATE is the di\u21b5erence between two regression functions and one way to estimate it is by regressing outcome in each treatment population. When the conditioning variables in CATE are a proper subset of the covariates needed to satisfy Asn. 1, [15] proposed estimates based on propensity-score weighting and kernel regression. Recently, [2] developed the Causal Tree (CT), which adapts machine learning methods, namely recursive partitioning, to directly address the heterogeneous e\u21b5ects problem by using new estimates for the error of estimating (x) within a partition of X data and by leveraging an \u201chonest\u201d estimation method that splits the data into that used for partitioning and that used for e\u21b5ect estimation. For personalization, learning heterogeneous e\u21b5ects can be used to choose between two treatments by comparing an estimate of their relative CATE to zero. As a learning problem, however, this focuses on minimal estimation risk rather than personalization risk and deals with only two treatments. In a later section we propose one-vs-one and one-vs-all strategies for personalization using estimated heterogeneous e\u21b5ects and show it is consistent. We compare to this strategy applied to CT of [2] in our empirical investigation. In learning from logged bandit feedback [16, 17], one is concerned with learning a good policy for a contextual multi-arm bandit problem based on logged data from another, known policy, rather than online interactions. This problem di\u21b5ers from ours because it assumes the policy that generated the data is known and available, but is similar due to its o\u270fine learning setting. To address this problem, [16] develop the Policy Optimizer for Exponential Models (POEM). [17] propose an improved Normalized POEM (NPOEM). In a later section we propose an adaptation of these methods to our problem, to which we compare in our empirical investigation."}, {"heading": "2 Methods for Personalizing from Observational Data", "text": "In this section we present four new algorithms that tackle personalization directly as a single learning task."}, {"heading": "2.1 Recasting the Problem", "text": "The following results relate personalization risk to an accuracy score weighted by outcome and generalized propensity score (GPS). The GPS is Q = (T, X), where (t, x) = P (T = t | X = x) [18, Def. 2.1]. The GPS of subject i, Qi, is an unknown quantity given by taking the unknown (t, x) and plugging in the known variables Ti, Xi.\nTheorem 2. Under Asn. 1,\nR(\u2327) = E [I [T = \u2327(X)] Y/Q] . (1)\nVariants of (1) have appeared before. For m = 2 and randomized data ( (1, x) = \u21e1 constant), [10] has a special case of (1). Moreover, (1) is closely related to the CATE transformation of [15, eq. (2)] and [2] and to [16, eq. (1)].\nTheorem 3. For any \u2318 2 R, \u232b > 0,\n(R(\u2327) \u2318 E [(Y \u2318)/Q]) /\u232b = E  I [T 6= \u2327(X)] \u2318 Y\n\u232bQ\n, (2)\nTherefore, for any fixed \u2318, \u232b, minimizing the right-hand side of (2) is the same as minimizing R(\u2327). If we think of \u2327(x) as a classifier, the right-hand side is its weighted misclassification error (with non-negative weights if \u2318 is chosen large enough)."}, {"heading": "2.2 Support Vector Personalization", "text": "Support vector machine (SVM), also known as max-margin, is a popular classification algorithm [19, 20]. Using Thm. 3, we develop a related algorithm for personalization, which we call support vector personalization (SVP), based on a hinge-loss approximation to (2) and imputed GPS estimates.\nSVP seeks functions \u21e2\u03021, . . . , \u21e2\u0302m : Rd ! R such that the personalization model \u2327\u0302SVPn (x) 2 arg maxt2[m] \u21e2\u0302t(x) has small risk. For example, we can seek linear functions \u21e2\u0302Ti x, but more generally we can consider a kernelized rule by considering a positive definite kernel K : Rd\u21e5 Rd ! R [21] and its associated reproducing kernel Hilbert space H = closure(span K(x, \u00b7) : x 2 Rd ) and letting \u21e2\u0302i 2 H. Popular choices for kernel are linear\nK(x, x0) = xT x0, polynomial K(x, x0) = (1 + xT x0/ )s, and radial basis function (RBF) K(x, x0) = ekx x0k22/(2 2) (see [22] for more).\nLet \u2318 = maxi Yi and \u232b = maxi Yi mini Yi. Then Y i = (\u2318 Yi)/\u232b is in [0, 1]. Suppose we are given some estimates for the GPS of the data, Q\u0302i. An empirical estimate of the modified personalization risk as in (2) of the model \u2327\u0302SVPn (x) is\nR\u0302n(\u2327\u0302n) = 1\nn\nnX\ni=1\nY i Q\u0302i I [9t 6= Ti : \u21e2\u0302t(Xi) \u21e2\u0302Ti(Xi)] .\nWe would like to minimize this empirical risk. However, R\u0302n(\u2327\u0302n) is not convex in \u21e2t(Xi) making optimization di cult. The convex envelope of R\u0302n(\u2327\u0302n) is the following weighted hinge loss:\nR\u0302n(\u2327\u0302n)  1\nn\nnX\ni=1\nY i Q\u0302i max t2[m] (I [t 6= Ti] + \u21e2\u0302t(Xi) \u21e2\u0302Ti(Xi)) .\nThe SVP algorithm is given by minimizing this hinge loss plus a 1/C-Hilbert-norm-regularization (C > 0) of \u21e21, . . . , \u21e2m. By expressing the hinge with constraints and using the representer theorem [23] and the kernel Gram matrix Kij = K(Xi, Xj), we can write this problem as a convex quadratic program:\nmin \u21b52Rm\u21e5n, \u21e02Rn C\nnX\ni=1\nY i Q\u0302i \u21e0i + 1 2\nTX\nt=1\n\u21b5Tt K\u21b5t, (3)\ns.t. \u21e0i I[t 6= Ti] + \u21b5Tt Ki \u21b5TTiKi 8i 2 [n], t 2 [m].\nWhen weights Y i/Q\u0302i = 1 are all one and m = 2, the above is exactly equivalent to SVM as in Ch. 12 of [24] and when m 3 and weights are all one it is equivalent to the multiclass formulation of [25]. Note that, when the kernel is linear, we can reduce \u21b5 to an m \u21e5 ddimensional variable.\nThe SVP algorithm uses imputed estimates of the GPS, Q\u0302i. To estimate these, we use a probabilistic classification model \u0302n(t, x) fitted to the data {(X1, T1), . . . , (Xn, Tn)} and let Q\u0302i = \u0302n(Xi, Ti) (as done in [18]). Examples include logistic regression, kNN, and kernel regression (see [24] for more). We summarize the SVP algorithm as Alg. 1. The optimization step is solved using extensions to liblinear and libsvm by Chang et al. for weighted samples (www.csie.ntu.edu.tw/~cjlin/libsvmtools). For a linear kernel and m 2, the extension to liblinear is used. For a non-linear kernel and m = 2, the extension\nAlgorithm 1 Support Vector Personalization\ninput: Data Sn = {(X1, T1, Y1), . . . , (Xn, Tn, Yn)}, classification algorithm A, positive definite kernel K, coe cient C.\n1: Fit a probability model \u0302n(t, x) to the data {(X1, T1), . . . , (Xn, Tn)} using A. 2: Impute Q\u0302i = \u0302n(Ti, Xi). 3: Set \u2318 = maxi Yi, \u232b = maxi Yi mini Yi, and Y i = (\u2318 Yi)/\u232b. 4: Compute the Gram matrix Kij = K(Xi, Xj). 5: Find \u21b5\u0302 that optimizes problem (3).\noutput: A personalization model \u2327\u0302n(x) 2 arg maxt2[m] Pn i=1 \u21b5\u0302tiK(Xi, x).\nto libsvm is used. For a non-linear kernel and m 3, the quadratic program is solved with Gurobi (www.gurobi.com). Note that the SVP with a linear kernel searches over the same decision boundaries as could be generated by comparing linear regressions, but with the direct goal of minimizing personalization risk."}, {"heading": "2.3 Personalization Tree", "text": "Classification and regression trees (CART) are predictive models based on recursive partitioning: the covariate space is recursively partitioned by axis-aligned hyperplanes (x`  \u2713 for ` 2 [d] and \u2713 2 R) in order to minimize a within-partition impurity measure [26]. Impurities for classification include entropy and Gini and for regression include sum of squared errors, [2] gives impurities for estimating heterogeneous e\u21b5ects. Motivated by Thm. 2, we develop a recursive partitioning algorithm for personalization called personalization tree (PT), which has the special property of generating an interpretable model (see Fig. 1 and Sec. 4.1).\nThe PT algorithm is based on an impurity measure to address the personalization risk as reformulated in Thm. 2. When a partition is small enough, the GPS of the subjects in the partition can be well approximated by the fraction of subjects with the same treatment in the partition. (In fact, this is the same as the motivation for classification trees.) Consider a partition of the data consisting of k datapoints, S\u0303 = {(Xi1 , Ti1 , Yi1), . . . , (Xik , Tik , Yik)}. Using the above motivation, we would estimate Q\u0302ij = 1 k Pk `=1 I \u21e5 Tij = Ti` \u21e4 as the imputed GPS for the datapoints in this partition, which leads to the following estimate of personalization\nrisk in the partition, which we use as our impurity:\nIpers(S\u0303) = min t2[m]\nkX\nj=1\nI \u21e5 Tij = t \u21e4 Yij/Q\u0302ij = k min\nt2[m] Average{Yij : j 2 [k], Tij = t},\nwhere we have rewritten Ipers(S\u0303) equivalently as the total estimated outcomes if we assign the estimated best treatment to all subjects in the partition, where estimated outcomes are constant within-partition means. For these estimates to be defined, we need at least 1 subject for each treatment in the partition. For additional reliability, we require at least nmin-leaf subjects of each treatment in the partition. 2\nThe PT algorithm proceeds by recursively partitioning the dataset along axis-aligned cuts in order to reduce the total sum of impurities. The tuning parameters are nmin-leaf as above, #features number of features to sample, and max maximal depth of tree. We summarize the recursive subroutine as Alg. 2. The PT algorithm is given by passing the whole dataset Sn and initial depth = 0 to Alg. 2."}, {"heading": "2.4 Personalization Forest", "text": "Random forest, which applies bagging (bootstrap aggregating) to CART with a limited number of random features at each cut, is one of the most popular predictive models [27]. We can similarly bag many PTs to produce a personalization forest (PF). The corresponding PF algorithm is summarized in Alg. 3. Generally, #features is set to p d for su cient independence between trees.\n2An alternative appropriate for scarce data and large m allows for any number of subjects but chooses\nthe best treatment only from among those with at least nmin-leaf subjects in the partition.\nAlgorithm 2 Personalization Tree subroutine\ninput: Data part S\u0303 = {(Xi1 , Ti1 , Yi1), . . . , (Xik , Tik , Yik)}, current depth , tuning parameters nmin-leaf, max, #features.\n1: for ` 2 [d] do sort the data along x`: Xi\u21e1(`,1),`  \u00b7 \u00b7 \u00b7  Xi\u21e1(`,k),`. 2: Set \u2327\u0302S\u0303(x) = arg mint2[m] Pk j=1 I[Tij = t]Yij/ Pk j=1 I[Tij = t]. 3: if < max and mint2[m] Pk j=1 I[Tij = t] > nmin-leaf then 4: Set I? = 1, `? = 0, j? = 0. Draw `1, . . . , `#features at random from [d] without replacement. 5: for ` = `1, . . . , `#features do 6: Set kL1 = \u00b7 \u00b7 \u00b7 = kLm = 0, SL1 = \u00b7 \u00b7 \u00b7 = SLm = 0, kL = 0. 7: Set kRt = Pk j=1 I[Tij = t], SRt = Pk j=1 I[Tij = t]Yij for t 2 [m], kR = k. 8: for j 2 [k 1] do 9: Update kL+=1, kR =1, t = Ti\u21e1(`,j) , kLt +=1, kRt =1, SLt +=Yi\u21e1(`,j) , SRt =Yi\u21e1(`,j) .\n10: Set I = kL mint2[m] SLt /k L t + k R mint2[m] SRt /k R t . 11: if I < I? and mint2[m] min(kLt , k R t ) nmin-leaf then set I? = I, `? = `, j? = j. 12: end for 13: end for 14: if I? < 1 then 15: Let S\u0303L = {(Xi\u21e1(`?,1) , Ti\u21e1(`?,1) , Yi\u21e1(`?,1)), . . . , (Xi\u21e1(`?,j?) , Ti\u21e1(`?,j?) , Yi\u21e1(`?,j?))}. 16: Set \u2327\u0302S\u0303L = Alg. 2(S\u0303 L, + 1). 17: Let S\u0303R = {(Xi\u21e1(`?,j+1) , Ti\u21e1(`?,j?+1) , Yi\u21e1(`?,j?+1)), . . . , (Xi\u21e1(`?,k) , Ti\u21e1(`?,k) , Yi\u21e1(`?,k))}. 18: Set \u2327\u0302S\u0303R = Alg. 2(S\u0303 R, + 1). 19: Set \u2327\u0302S\u0303(x) = (if x`?  12(Xi\u21e1(`?,j),` + Xi\u21e1(`?,j+1),`) then \u2327\u0302S\u0303L(x) else \u2327\u0302S\u0303R(x)). 20: end if 21: end if\noutput: Personalization model \u2327\u0302S\u0303."}, {"heading": "2.5 Optimal Personalization Tree", "text": "One di culty with PT is that the impurity is linear, which may fault a greedy search. For classification trees, non-linear impurities like entropy are favored over linear impurities like\nAlgorithm 3 Personalization Forest\ninput: Data Sn = {(X1, T1, Y1), . . . , (Xn, Tn, Yn)}, parameters T , nmin-leaf, max, #features. 1: for j 2 [T ] do 2: Draw S\n(j) n = {(Xi1 , Ti1 , Yi1), . . . , (Xin , Tin , Yin)} at random from Sn with replacement.\n3: Set \u2327\u0302 (j) n = Alg. 2(S (j) n , 0, nmin-leaf, max, #features). 4: end for\noutput: Personalization model \u2327\u0302n(x) = mode{\u2327\u0302 (1)n (x), . . . , \u2327\u0302 (T )n (x)}.\naccuracy because they encourage cuts that, despite not improving prediction accuracy when majorities are unchanged, may lead to eventual cuts that do because they further refine the homogeneity (purity) of the partitions. To overcome this in PT, in this section we propose the optimal personalization tree (OPT) algorithm, which solves the global problem of finding partitions whose personalization impurities are small:\nmin R1t\u00b7\u00b7\u00b7tRL=Rd:(\u21e4)\nLX\np=1\nIpers({(Xi, Ti, Yi) : Xi 2 Rp}), (4)\nwhere (\u21e4) is the restriction that R1, . . . , RL be disjoint regions defined by the leaves of a binary decision tree. For classification and regression, there have been attempts at finding globally optimal trees [28] despite it being NP-hard [29] and recently [30] proposed a successful mixed integer programming (MIP) approach. Motivated by this success, we propose a MIP approach to the optimal personalization tree problem (4).\nWe consider a fixed binary tree structure on nodes 1, . . . , P . Let Ap \u21e2 [P ] be the unique path from the root to node p, i.e., its ancestors. For q 2 Ap, let Rpq = 1 if the right branch is taken to reach p from q, otherwise 0. Let L = {p 2 [P ] : p /2 A(q) 8q 2 [P ]} be the set of leaf nodes and let LC = [P ]\\L be the non-leaf nodes. Let Cp \u21e2 [d]\u21e5R be the finite set of potential cuts at each non-leaf node p 2 LC , where (`, \u2713) 2 Cp denotes that the cut x`  \u2713 is to be considered at node p. (Usually we take \u2713 to be the data midpoints along dimension `.) Let Y i = Yi minj2[n] Yj, Y max = maxi Y i, and M = Y max(maxt2[m] Pn\ni=1 I [Ti = t] |L| nmin-leaf). For a vector with index set C \u21e2 [d]\u21e5R, let i( , C) = P (`,\u2713)2C I [Xi,`  \u2713] `,\u2713. For p 2 LC , let kp = dlog2 |Cp|e and Zp 2 {0, 1}kp\u21e5|Cp| be such that (Zp)ij = 1 if bj/2ic is odd and\notherwise 0. Our MIP for OPT follows:\nmin\nnX\ni=1\nX p2L \u232bip (5a)\ns.t.w 2 [0, 1][n]\u21e5L, 2 {0, 1}L\u21e5m, \u00b5 2 RL+, \u232b 2 R[n]\u21e5L+ (5b)\np 2 [0, 1]Cp , p 2 {0, 1}kp , Zp p = p 8p 2 LC (5c) wip  Rpq + ( 1)Rpq i( q, Cq) 8i 2 [n], p 2 L, q 2 Ap (5d) wip 1 + X\nq2Ap (1 Rpq + ( 1)Rpq i( q, Cq)) 8i 2 [n], p 2 L (5e)\nX\ni:Ti=t\nwip nmin-leaf 8t 2 [m] (5f)\n\u232bip  Y maxwip, \u232bip  \u00b5p 8i 2 [n], p 2 L (5g) \u232bip \u00b5p Y max(1 wip) 8i 2 [n], p 2 L (5h) X t2[m] pt = 1 8p 2 L (5i)\nX\ni:Ti=t\n(\u232bip wipY i)  M(1 pt) 8p 2 L, t 2 [m] (5j)\nX\ni:Ti=t\n(\u232bip wipY i) M( pt 1) 8p 2 L, t 2 [m] (5k)\nProblem (5) has |L| m + Pp2LC log2 |Cp| binary variables. The variables p encode choice of cut at node p and constraint (5c) ensures only one is chosen (see [31]). The variable wip encodes membership of datapoint i to leaf p and constraints (5d-5e) enforce that wip is the product of indicators of whether Xi goes in the left or right branch of the ancestor nodes. Since these constraints are integral [32] we need not enfoce wip be binary. Constraint (5f) ensures at least nmin-leaf samples per leaf. The variable \u00b5p encodes the mean outcome of the prescribed treatment in leaf p and the variable \u232bip encodes its product with wip, as ensured by constraints (5g-5h). The variable pt encodes the choice of treatment t in leaf p and constraint (5i) ensures only one is chosen. Constraints (5j-5k) ensure the consistency between the choice of treatment pt and the mean outcome \u00b5p. We summarize the OPT algorithm for a complete binary tree in Alg. 4. We use Gurobi to solve MIP (5) and use PT as a heuristic warm start, randomly splitting leaf nodes at depth less than .\nAlgorithm 4 Optimal Personalization Tree (complete binary tree)\ninput: Data Sn = {(X1, T1, Y1), . . . , (Xn, Tn, Yn)}, parameters nmin-leaf, , #features, #cuts. 1: Set P = 2 , LC = [2 1], Ap = {bp/2jc : j 2 [ ]}, Rpq = 1 i\u21b5 bp/2 blog2(q)cc is odd. 2: for ` 2 [d] do sort the data along x`: Xi\u21e1(`,1),`  \u00b7 \u00b7 \u00b7  Xi\u21e1(`,k),`. 3: for p = 1, . . . , 2 1 do 4: Draw Fp \u21e2 [d] with |Fd| = #features. 5: Set Cp = {(`, Xi\u21e1(`,j),`+Xi\u21e1(`,j+1,`) 2\n) : ` 2 Fp, j = 1, d n 1#cuts e, . . . , n 1}. 6: end for 7: Find , that solve problem (5).\noutput: Personalization model \u2327\u0302n(x) that proceeds as follows: Set p = 1. for j 2 [ ] do set (`, \u2713) = inf{c 2 Cp : p,c = 1}, p = 2p + I [x` > \u2713]. return inf{t 2 [m] : pt = 1}."}, {"heading": "2.6 Adapting other approaches", "text": "As discussed earlier, methods that estimate CATE, notably [2]\u2019s CT, can be used to choose between two treatments by comparing (x) = E [Y | X = x, T = 2] E [Y | X = x, T = 1] to zero. However, such methods are directed at estimation rather than personalization and only address two treatments. To address the latter, we propose one-vs-all (1vA) and one-vs-one (1v1) strategies for personalization. For 1vA, for each t 2 [m] we learn an estimate \u0302tvAn (x) of tvA(x) = E [Y | X = x, T = t] E [Y | X = x, T 6= t] by applying a base algorithm (e.g., CT) to the modified dataset StvAn = {(Xi, 1 + I [Ti = t] , Yi) : i 2 [n]}; then we assign the treatment that does the best compared to the rest,\n\u2327\u0302 1vAn (x) 2 arg min t2[m] \u0302tvA(x).\nFor 1v1, for each t 6= s we learn an estimate \u0302tvsnt+ns(x) of tvs(x) = E [Y | X = x, T = t] E [Y | X = x, T = s] on the modified data subset Stvsnt+ns = {(Xi, 1 + I [Ti = t] , Yi) : Ti 2 {t, s}}; then we assign the treatment that does the best compared to the worst,\n\u2327\u0302 1v1-An (x) 2 arg min t2[m] min s2[m] \u0302tvsnt+ns(x),\nor the one that gets the most votes in one-to-one comparisons,\n\u2327\u0302 1v1-Bn (x) 2 arg max t2[m]\nX s 6=t I h \u0302tvsnt+ns(x) < 0 i .\nNote that 1vA and 1v1 with CT do not inherit trees\u2019 interpretability because the partitions of the 1v models may not overlap. Given pointwise consistent estimates of CATE, these are consistent:\nTheorem 4. Let Asn.1 hold. If \u0302tvAn (X) ! tvA(X) a.s. 8t 2 [m], then \u2327\u0302 1vAn (X) 2 T \u21e4(X) eventually a.s. If \u0302tvsnt+ns(X) ! tvs(X) a.s. 8t 6= s, then \u2327\u0302 1v1-An (X), \u2327\u0302 1v1-Bn (X) 2 T \u21e4(X) eventually a.s.\nPOEM and NPOEM [16, 17] solve the problem of learning from logged bandit feedback, assuming access to the logging policy that generated the data. To adapt these to personalizing from observational data, we propose to impute the logging policy using estimated GPS, i.e., pretend the data were generated by the policy that assigns t when context is x with probability \u0302n(t, x). We call these IPOEM and INPOEM."}, {"heading": "3 Validating Personalization using Observational Data", "text": "In this section we discuss how one can evaluate and validate personalization policies, such as the ones from the last section. Usually, a new policy would be evaluated using a randomized controlled trial, but these can be infeasibly costly. We consider how to evaluate a personalization policy using observational data. Such a dataset can be a subset removed from the training data either for the purpose of testing or for tuning and selection by (cross)validation. The di culty in using observational data is that if a policy prescribes any treatment \u2327(Xi) 6= Ti, then it is not immediately clear how to score this. Recent work [33] has looked at o\u270fine evaluation of contextual bandits with experimental data and showed that rejection sampling is su cient. A similar solution to evaluation with observational data is a combined rejection and importance sampling approach suggested by Thm. 2. If we had the GPS Qi, we could omit any datapoint where \u2327(Xi) 6= Ti while giving score Yi/Qi to each datapoint where the prescription matched the data, \u2327(Xi) = Ti. Per\nThm. 2 and the law of large numbers, this will provide a consistent estimate for out-ofsample personalization risk. However, not only does this throw away many datapoints, but to implement this in practice we would have to estimate the GPS from data. Estimating the GPS generally either relies heavily on model specification or, in non-parametric settings, can be biased and variable. This may be acceptable for training purposes, as in SVP, as it is already a blackbox. However, for evaluation, a more reliable estimate of risk is desirable for evidence of success.\nWe propose the use of a matched dataset for evaluation. Matching is a common tool for causal inference [34]. In our case, a matched dataset is a subset of the data where each subject is matched, based on a metric kx x0k, to m 1 subjects that received each of the treatments the subject did not. Their outcome is imputed as the counterfactual outcome of those treatments for the subject. All matched subjects are not used for training in order to avoid in-sample bias. Usually, Mahalanobis distance is used: ((x x0)\u2303\u0302 1(x x0))1/2 where \u2303\u0302 is the sample covariance. Note that due to personalization on X, matching on propensity scores alone is insu cient."}, {"heading": "3.1 Greedy Matching", "text": "The simplest way to extract a matched dataset of size ntest from Sn is to do so greedily : draw random i1, . . . , intest from [n] without replacement, for each j 2 [ntest] and t 2 [m], if t = Tij then set Y\u0302ijt = Yij and if t 6= Tij then find i 2 arg mini2[n]:Ti=t kXi Xijk (with replacement), let Y\u0302ijt = Yi and flag subject i, and finally remove all drawn and flagged subjects from training data. The imputed value for the unknown Yij(t) is Y\u0302ijt and our estimate for personalization risk of \u2327(x) is R\u0302(\u2327) = 1 ntest Pntest j=1 Y\u0302ij\u2327(Xij ). When matching is exact, i.e. Xi = Xij for all matches, this estimate is unbiased.\nTheorem 5. Under Asn. 1 and exact matching, E[R\u0302(\u2327)] = R(\u2327)."}, {"heading": "3.2 Optimal Matching", "text": "The greedy method for constructing a matched dataset is simple but it can be wasteful, limiting the amount of the data available for training. We may be able to do better for\ntesting and evaluation when m = 2. Consider the problem of finding the subset of the data with the closest matches. That is, finding i11, i12, . . . , inpair1, inpair2 with Tijt = t and minimal Pnpair\nj=1 kXij1 Xij2k, and using the pairs for imputations. This problem can be reduced to bipartite matching, which can be solved e ciently [35]. Consider the complete bipartite graph with left nodes being subjects with Ti = 1 and right nodes being subjects with Ti = 2 along with n npair dummy nodes. Put weight kXi Xjk on edges between datapoints and weight 0 on edges to dummy nodes. The subset of the data with the closest matches is given by the nodes incident to edges not incident to dummy nodes in the least-weight bipartite match. We extract these to construct a well-matched, economical test set with ntest = 2npair. Although this test set may be biased relative to the whole population (e.g., it may emphasize areas of treatment overlap), the corresponding risk estimate is unbiased conditioned on the test set, i.e., it corresponds to risk on an alternative population, which is often su cient for comparison and selection."}, {"heading": "3.3 Coe cient of Personalization", "text": "In prediction, the coe cient of determination R2 is a unitless quantity bounded by 1 that measures both how well data X predict outcomes Y and how well a predictive model leverages X. One way to interpret out-of-sample R2 is as the percent of the way that X and the model go from a no-X-data solution (Y \u2019s sample average) to perfect foresight (Y \u2019s realized value). Using this interpretation, we construct two analogous quantities for personalization, the 1st and 2nd coe cients of personalization:\nP1(\u2327) = 1 E[Y (\u2327(X))] E[mint2[m] Y (t)]\nmint2[m] E[Y (t)] E[mint2[m] Y (t)] ,\nP2(\u2327) = 1 E[Y (\u2327(X))] E[mint2[m] Y (t)]\nE[Y (T )] E[mint2[m] Y (t)] .\nThese are also analogous to the coe cient of prescription for conditional stochastic optimization [36]. The first measures the improvement toward perfect (prescient) personalization relative to no personalization at all and the second does relative to current practice or standard of care (whatever determined T in the data). They are unitless, bounded by 1.\nUsing a matched dataset, we can estimate these as:\nP\u03021(\u2327) = 1 \u2303ntestj=1 Y\u0302ij\u2327(Xij ) \u2303 ntest j=1 mint2[m] Y\u0302ijt\nmint2[m]\u2303 ntest j=1 Y\u0302ijt \u2303ntestj=1 mint2[m] Y\u0302ijt\n,\nP\u03022(\u2327) = 1 \u2303ntestj=1 Y\u0302ij\u2327(Xij ) \u2303 ntest j=1 mint2[m] Y\u0302ijt\n\u2303ntestj=1 Yij \u2303ntestj=1 mint2[m] Y\u0302ijt ."}, {"heading": "4 Empirical Investigation", "text": "We conclude with an empirical investigation of personalization."}, {"heading": "4.1 Personalized Warfarin Dosing", "text": "According to the International Warfarin Pharmacogenetics Consortium, \u201cwarfarin is the most widely used oral anticoagulant agent worldwide\u201d and finding the appropriate dose is both di cult and important \u201cbecause it can vary by a factor of 10 among patients\u201d and \u201cincorrect doses contribute to a high rate of adverse e\u21b5ects\u201d [37]. Currently, the common practice is to start a new patient at 35 mg/week and slowly adjust the dose [38]. We present an application of our methods to personalizing dosage based on data on 5410 warfarin patients collected by [37] (available at pharmgkb.org).\nThe baseline data collected on each patient include demographic characteristics (sex, ethnicity, age, weight, height, and smoker), reason for treatment (e.g., atrial fibrillation), current medications, co-morbidities (e.g., diabetes), genotype of two polymorphisms in CYP2C9, and genotype of seven single nucleotide polymorphisms (SNPs) in VKORC1. The correct stable therapeutic dose of warfarin, determined by adjustment over a few weeks, is recorded for each patient and segmented into three dose groups: low ( 21 mg/week, t = 1), medium (> 21, < 49 mg/week, t = 2), and high ( 49 mg/week, t = 3). The dataset was also studied in an online (bandit) setting in [13] where an R&C approach is analyzed.\nIn our experiment, we let Y (t) be 1 if the dose t is incorrect and otherwise 0. To generate observational data (where dosage is not revealed by experimentation), we consider T chosen based on body mass index (BMI): P (T = t | X = x) = e(t 2)(xBMI \u00b5BMI)/ BMI e (xBMI \u00b5BMI)/ BMI+1+e(xBMI \u00b5BMI)/ BMI , where \u00b5BMI and BMI are the sample mean and standard deviation of BMI. As an example, we run the PT algorithm with max = 5 on the whole data, generating the tree shown in Fig. 1. It is known that VKORC1 and CYP2C9 genotypes are strongly associated to warfarin dosage requirements [39]. PT is able to learn this relationship and it provides an e cient and interpretable dosing guideline where the e\u21b5ect of these genotypes is clear.\nTo assess the e ciency of various personalization algorithms, for each n = 100, 200, . . . , 2500, we consider 100 replications in which we randomly select n training subjects and ntest = 2500 test subjects (disjoint, without replacement). In each replication, we run 12 personalization algorithms and evaluate their risk on the test set (where full counterfactuals are available). We test standard R&C using four predictive models: OLS, logistic regression, CART (scikit-learn defaults), and kNN (k = bpnc). We compare these to our four direct personalization methods: SVP (linear kernel, C = 1, GPS imputed by crossvalidated `1-regularized multinomial regression using R package glmnet), PT (nmin-leaf = 20, max = 1, #features = d), PF (T = 500, nmin-leaf = 10, max = 1, #features = p d), and OPT (nmin-leaf = 20, #features = d, #cuts = 10, = 2 + I [n 300], MIP solve time limited to 1 hour). We also compare to our 1vA strategy using [2]\u2019s CT-A (adaptive) and CT-H (honest with 50-50 split) and to IPOEM and INPOEM (parameters tuned on 25% holdout validation set as in [16, 17]). Due to limited space, we focus on 1vA, which outperformed 1v1. We plot the average risk over replicates in Fig. 2 (note log scale). It is evident that R&C approaches make ine cient use of the available data by splitting it and learning more than is necessary. While eventually reaching low risk (< 0.4), R&C using OLS and logistic regression take much longer (n = 1300) to get there than our direct methods, which achieve low risk very quickly (n = 200) and near-optimal risk ( 0.36) soon after (n = 700). Nonparametric R&C (CART, kNN), IPOEM, and INPOEM converge slowly. 1vA with CT-A and CT-H o\u21b5ers competitive performance for moderate n, but fails to achieve near-optimal risk even at n = 2500. CT-A o\u21b5ers a small edge over CT-H, which can be attributed to CT-H\u2019s splitting of the training data \u2013 indeed, CT-H\u2019s primary advantage are correctly sized confidence intervals, which we do not use here. Among our direct methods, PF appears to work the best overall, for both small and large n, while SVP and PT achieve similar performance for n 2000. For smaller n, OPT outperforms PT (and PF for n = 100) attributed to OPT\u2019s ability to find the best simple tree to fit the scarce data. For larger n, the MIP becomes so large that Gurobi is hardly able to improve the PT warm start, which has very limited depth. Therefore, we see performance deteriorate. We conclude OPT is best either for small datasets or for finding models that are reasonably e cient while being exceedingly simple and interpretable (depth 2\u20133 compared to depth 9\u201313 for PT at n = 2500), albeit\nat computational cost. Our best out-of-sample risk is 0.356, which translates to P\u03021 = 0.22, P\u03022 = 0.47. That is, we go 22% (or, 47%) of the way from no personalization (or, standard of care) to perfect personalization."}, {"heading": "4.2 Personalized Job Training", "text": "We consider an application to personalized recommendations for a job training program. We use data from the National Supported Work Demonstration [40, 41] (available at users. nber.org/~rdehejia). The data includes 445 individuals, 185 of which received a job training program in 1976-77 (Ti = 1). The data includes information about age, education level, ethnicity, marital status, earnings in years 1974-75, and earnings in 1978. This data is the standard benchmark in evaluation of causal methodologies for estimating an average treatment e\u21b5ect [42]. We consider an alternative setting where we give a personalized recommendation as to whether to enroll in the job training program, assuming enrollment costs $2,000. Therefore, we let Yi equal 1978 earnings less $2,000 if Ti = 1.\nFrom the 445, we extract an optimal matched test set of 185/5 = 37 pairs (ntest = 74) perfectly matched in all covariates (0 within-pair di\u21b5erence). On the remaining n = 371 subjects, we train the same personalization models as above with the following changes: we omit logistic regression (outcomes are not binary), use RBF kernel for SVP, use nmin-leaf = 10 for PT and OPT and = 1 for PF, use = 4 for OPT and let the MIP solve for 24 hours. We plot the estimated average personalized net income (after enrollment costs) in Fig. 3. We see a clear benefit to our methods\u2019 direct targeting of personalization and that, with only two treatments, CT-A provides highly competitive performance. Average net income of 4904.5 due to PF translates to P\u03021 = 0.40, P\u03022 = 0.40, i.e., 40% of the way from either no personalization or the standard of care to perfect personalization."}, {"heading": "5 Proofs", "text": "Proof of Theorem 1. By Asn. 1, we have\nE [Y | X = x, T = t] = E [Y (T ) | X = x, T = t] (definition of Y = Y (T ))\n= E [Y (t) | X = x, T = t] (conditioned on T = t) = E [Y (t) | X = x] (Asn. 1).\nConsider a realization of the data and X = x where convergence occurs for all t 2 [m]. Let\n\u270f(x) = inf{\u21e3 : s 2 [m], \u21e3 = \u2713\nE [Y | X = x, T = s] min t2[m]\nE [Y | X = x, T = t] \u25c6 > 0},\nwhere inf(?) = 1. By assumption of convergence at this realization of the data and X = x, we have that eventually for all t 2 [m], |\u00b5\u0302t,nt(x) E [Y | X = x, T = t]| < \u270f(x)/2, at which point we must necessarily also have \u2327\u0302n(x) 2 arg mint2[m] E [Y | X = x, T = t] = arg mint2[m] E [Y (t) | X = x]. By assumption of pointwise consistency and because the intersection of finitely many a.s. events is a.s., the set of such realization of the data and X = x have probability 1.\nProof of Theorem 2. First note that, given any x with P (T = t | X = x) > 0, we have\nE [Y | X = x, T = t] = E [Y I [T = t] | X = x] P (T = t | X = x) = E  Y I [T = t] (t, x) | X = x\n= E  Y I [T = t] (T, X) | X = x = E  Y I [T = t] Q | X = x .\nTherefore, since P (T = t | X) > 0 almost surely,\nR(\u2327) = E [Y (\u2327(X))] = E [E [Y (\u2327(X)) | X]] (iterated expectations)\n= E [E [Y (\u2327(X)) | X, T = \u2327(X)]] (Asn. 1) = E [E [Y | X, T = \u2327(X)]] (definition of Y ) = E [E [Y I [T = \u2327(X)]/Q | X]] (above observation)\n= E [Y I [T = \u2327(X)]/Q] (iterated expectations) .\nProof Theorem 3. First note that, because P (T = \u2327(X) | X) > 0 almost surely, we have\n1 = E [I [T = \u2327(X)] | T = \u2327(X)] = E [E [I [T = \u2327(X)] | X, T = \u2327(X)]]\n= E \nE [I [T = \u2327(X)] | X] P (T = \u2327(X) | X) = E\n E \nI [T = \u2327(X)] (\u2327(X), X) | X\n= E  E \nI [T = \u2327(X)] Q | X\n= E \nI [T = \u2327(X)] Q .\nCombining this observation with Theorem 2, we have\nE[I[T 6= \u2327(X)] (\u2318 Y )/(\u232bQ)] = E [I [T = \u2327(X)] (Y \u2318)/(\u232bQ)] E [(Y \u2318)/(\u232bQ)]\n= E [I [T = \u2327(X)] Y/Q] /\u232b E [I [T = \u2327(X)] /Q] \u2318/\u232b E [(Y \u2318)/Q] /\u232b\n= R(\u2327)/\u232b \u2318/\u232b E [(Y \u2318)/Q] /\u232b.\nProof of Theorem 4. We start with 1vA. Restrict to x such that (s, x) > 0 8s (almost everywhere). Let \u00b5(t, x) = E [Y (t) | X = x]. Under Asn. 1,\ntvA(x) = E [Y | X = x, T = t] E [Y | X = x, T 6= t]\n= E [Y | X = x, T = t] X\ns 6=t E [Y | X = x, T = s] P (T = s | X = x, T 6= t)\n= \u00b5(t, x) X\ns 6=t (s, x)\u00b5(s, x)/\nX s 6=t (s, x).\nSince (s, x) > 0, it\u2019s clear that tvA(x)  svA(x) 8s if and only if \u00b5(t, x)  \u00b5(s, x) 8s. The rest of the proof for 1vA follows the same way as Thm. 1, showing that, under the assumption of pointwise consistent estimation, the estimation gap supt2[m] \u0302tvAn (x) tvA(x) is eventually smaller than half the decision gap,\n\u270f1vA(x) = inf{\u21e3 : s 2 [m], \u21e3 = \u2713 svA(x) min\nt2[m] tvA(x)\n\u25c6 > 0},\na.s. and for almost everywhere x.\nNext, we deal with 1v1-A. Fix x. Fix any tm 2 arg maxt2[m] \u00b5(t, x). Let tvmin(x) = mins 6=t tvs(x). If t, s 6= tm, then tvmin(x) svmin(x) = \u00b5(t, x) \u00b5(s, x). On the other hand, for any t 2 [m], we always have both \u00b5(t, x) \u00b5(tm, x)  0 and tvmin(x) tmvmin(x)  0.\nTherefore, we have\nt 2 arg min t2[m] \u00b5(t, x) () \u00b5(t, x) \u00b5(s, x)  0 8s 6= t () \u00b5(t, x) \u00b5(s, x)  0 8s 6= t, tm\n() tvmin(x) svmin(x)  0 8s 6= t, tm () tvmin(x) svmin(x)  0 8s 6= t () t 2 arg min t2[m] tvmin(x).\nLet \u0302tvminn (x) = mins 6=t \u0302 tvs nt+ns(x) and note that\nsup t2[m]\n\u0302tvminn (x) tvmin(x)  sup\nt2[m],s2[m]\n\u0302tvsnt+ns(x) tvs(x) ,\nwhich converges to zero under pointwise consistency. The rest of the proof for 1v1-A follows as above, showing that this estimation gap is eventually smaller than half the decision gap,\n\u270f1v1-A(x) = inf{\u21e3 : s 2 [m], \u21e3 = \u2713 svmin(x) min\nt2[m] tvmin(x)\n\u25c6 > 0},\na.s. and for almost everywhere x.\nNext, we deal with 1v1-B. Fix x and a realization of the data where convergence holds\nfor all t 6= s. Then, eventually \u0302tvsnt+ns(x) tvs(x)  | tvs(x)| /2 for all t 6= s such that tvs(x) 6= 0. That is, eventually I h \u0302tvsnt+ns(x) < 0 i = I [ tvs(x) < 0] for all t 6= s such that\ntvs(x) 6= 0. Restrict to such large enough n. Let kt(x) = P\nt 6=s I [ tvs(x) < 0], k\u0302t(x) =P t 6=s I h \u0302tvsnt+ns(x) < 0 i , and kmin(x) = arg mint2[m] \u00b5(t, x) . Then,\nt 2 arg min t2[m] \u00b5(t, x) () kt(x) = m kmin(x)\n() k\u0302t(x) m kmin(x) (= t 2 arg max t2[m]\nX s 6=t I h \u0302tvsnt+ns(x) < 0 i .\nProof of Theorem 5. By random sampling, (Xij , Tij , Yij(1), . . . , Yij(m)) are distributed iid as (X, T, Y (1), . . . , Y (m)) is in population. For j 2 [ntest], let ijt be ij\u2019s match for treatment t, or ij if Tij = t. Under exact matching, Yijt(1), . . . , Yijt(m) | Xji is distributed the same as\nYij(1), . . . , Yij(m) | Xji , Tji = t. By writing Y\u0302ijt = Yijt = Pm s=1 I [t = s] Yijs(s), we see that\nE[Y\u0302ij\u2327(Xij )] = E\n\" E \" mX\ns=1\nI \u21e5 s = \u2327(Xij) \u21e4 Yijs(s) | Xij ## (iterated expectation)\n= mX\ns=1\nE \u21e5 I \u21e5 s = \u2327(Xij) \u21e4 E \u21e5 Yijs(s) | Xji \u21e4\u21e4 (linearity)\n= mX\ns=1\nE \u21e5 I [s = \u2327(Xi)] E \u21e5 Yij(s) | Xi, Ti = s \u21e4\u21e4 (exact matching)\n= mX\ns=1\nE \u21e5 I [s = \u2327(Xi)] E \u21e5 Yij(s) | Xi \u21e4\u21e4 (Asn. 1)\n= E \" E \" mX\ns=1\nI [s = \u2327(Xi)] Yij(s) | Xi ##\n(linearity)\n= E \u21e5 Yij(\u2327(Xij)) \u21e4 (iterated expectation)"}], "references": [{"title": "Recursive partitioning for heterogeneous causal e\u21b5ects", "author": ["Susan Athey", "Guido Imbens"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Causal inference in statistics, social, and biomedical sciences", "author": ["Guido W Imbens", "Donald B Rubin"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "The central role of the propensity score in observational studies for causal e\u21b5ects", "author": ["Paul R Rosenbaum", "Donald B Rubin"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1983}, {"title": "Causality: models, reasoning, and inference", "author": ["Pearl Judea"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "A statistical model for predicting response of breast cancer patients to cytotoxic chemotherapy", "author": ["Michael L Feldstein", "Edwin D Savlov", "Russell Hilf"], "venue": "Cancer Res,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1978}, {"title": "A multivariate analysis of genomic polymorphisms: prediction of clinical outcome to 5fu/oxaliplatin combination chemotherapy in refractory colorectal cancer", "author": ["J Stoehlmacher", "DJ Park", "W Zhang", "D Yang", "S Groshen", "S Zahedy", "HJ Lenz"], "venue": "Brit J Cancer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Performance guarantees for individualized treatment rules", "author": ["Min Qian", "Susan A Murphy"], "venue": "Ann Stat,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Personalized diabetes management using electronic medical records", "author": ["Dimitris Bertsimas", "Nathan Kallus", "Alex Weinstein", "Daisy Zhuo"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Estimating individualized treatment rules using outcome weighted learning", "author": ["Yingqi Zhao", "Donglin Zeng", "A John Rush", "Michael R Kosorok"], "venue": "J Am Stat Assoc,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire"], "venue": "In WWW,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "A linear response bandit problem", "author": ["Alexander Goldenshluger", "Assaf Zeevi"], "venue": "Stoch Syst,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Online decision-making with high-dimensional covariates", "author": ["Hamsa Bastani", "Mohsen Bayati"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Strong laws of large numbers and nonparametric estimation", "author": ["Harro Walk"], "venue": "Recent Developments in Applied Probability and Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Estimating conditional average treatment e\u21b5ects", "author": ["Jason Abrevaya", "Yu-Chin Hsu", "Robert P Lieli"], "venue": "J Bus Econ Stat,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Counterfactual risk minimization: Learning from logged bandit feedback", "author": ["Adith Swaminathan", "Thorsten Joachims"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "The self-normalized estimator for counterfactual learning", "author": ["Adith Swaminathan", "Thorsten Joachims"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "The propensity score with continuous treatments", "author": ["Keisuke Hirano", "Guido W Imbens"], "venue": "Applied Bayesian modeling and causal inference from incomplete-data perspectives,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "A training algorithm for optimal margin classifiers", "author": ["Bernhard E Boser", "Isabelle M Guyon", "Vladimir N Vapnik"], "venue": "In COLT,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["Alain Berlinet", "Christine Thomas-Agnan"], "venue": "Kluwer Academic,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["Bernhard Scholkopf", "Alexander J Smola"], "venue": "MIT press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "A generalized representer theorem", "author": ["Bernhard Sch\u00f6lkopf", "Ralf Herbrich", "Alex J Smola"], "venue": "In COLT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "The elements of statistical learning", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Koby Crammer", "Yoram Singer"], "venue": "J Mach Learn Res,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Classification and Regression Trees", "author": ["Leo Breiman", "Jerome Friedman", "Charles Stone", "Richard Olshen"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1984}, {"title": "Constructing optimal binary decision trees is np-complete", "author": ["Laurent Hyafil", "Ronald L Rivest"], "venue": "Inform Process Lett,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1976}, {"title": "Incremental and encoding formulations for mixed integer programming", "author": ["Sercan Y\u0131ld\u0131z", "Juan Pablo Vielma"], "venue": "Oper Res Lett,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Network flows: theory, algorithms, and applications", "author": ["RK Ahuja", "TL Magnanti", "JB Orlin"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1993}, {"title": "Unbiased o\u270fine evaluation of contextual-bandit-based news article recommendation algorithms", "author": ["Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang"], "venue": "In WSDM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Imbens. Large sample properties of matching estimators for average treatment", "author": ["Alberto Abadie", "Guido W"], "venue": "e\u21b5ects. Econometrica,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "An n\u02c65/2 algorithm for maximum matchings in bipartite graphs", "author": ["John E Hopcroft", "Richard M Karp"], "venue": "SIAM J Comput,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1973}, {"title": "From predictive to prescriptive analytics", "author": ["Dimitris Bertsimas", "Nathan Kallus"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Practical tips for warfarin dosing and monitoring", "author": ["Amir Ja\u21b5er", "Lee Bragg"], "venue": "Clev Clin J Med,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Polymorphisms in the vkorc1 gene are strongly associated with warfarin dosage requirements in patients receiving anticoagulation", "author": ["Tao Li", "Leslie A Lange", "Xiangli Li", "Lisa Susswein", "Betsy Bryant", "Robb Malone", "Ethan M Lange", "Teng-Yi Huang", "Darrel W Sta\u21b5ord", "James P Evans"], "venue": "J Med Genet,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Evaluating the econometric evaluations of training programs with experimental data", "author": ["Robert J LaLonde"], "venue": "Am Econ Rev,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1986}, {"title": "Causal e\u21b5ects in nonexperimental studies: Reevaluating the evaluation of training programs", "author": ["Rajeev H Dehejia", "Sadek Wahba"], "venue": "J Am Stat Assoc,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1999}, {"title": "Propensity score-matching methods for nonexperimental causal studies", "author": ["Rajeev H Dehejia", "Sadek Wahba"], "venue": "Rev Econ Stat,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Standard approaches to the problem that apply supervised learning naively fall short in this setting, as we show here for the case of personalization and as [2] recently showed for the case of estimating heterogeneous causal e\u21b5ects.", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": "This assumption is standard in causal inference for ensuring identifiability [4] and is closely related to the backdoor criterion [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "This assumption is standard in causal inference for ensuring identifiability [4] and is closely related to the backdoor criterion [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 4, "context": "For example, in medicine, there is a vast literature on predicting patient-specific responses to treatment [6, 7] and picking the best by comparing predictions [8, 9], although recent work [10] has looked at alternative approaches for the case of randomized, experimental data.", "startOffset": 107, "endOffset": 113}, {"referenceID": 5, "context": "For example, in medicine, there is a vast literature on predicting patient-specific responses to treatment [6, 7] and picking the best by comparing predictions [8, 9], although recent work [10] has looked at alternative approaches for the case of randomized, experimental data.", "startOffset": 107, "endOffset": 113}, {"referenceID": 6, "context": "For example, in medicine, there is a vast literature on predicting patient-specific responses to treatment [6, 7] and picking the best by comparing predictions [8, 9], although recent work [10] has looked at alternative approaches for the case of randomized, experimental data.", "startOffset": 160, "endOffset": 166}, {"referenceID": 7, "context": "For example, in medicine, there is a vast literature on predicting patient-specific responses to treatment [6, 7] and picking the best by comparing predictions [8, 9], although recent work [10] has looked at alternative approaches for the case of randomized, experimental data.", "startOffset": 160, "endOffset": 166}, {"referenceID": 8, "context": "For example, in medicine, there is a vast literature on predicting patient-specific responses to treatment [6, 7] and picking the best by comparing predictions [8, 9], although recent work [10] has looked at alternative approaches for the case of randomized, experimental data.", "startOffset": 189, "endOffset": 193}, {"referenceID": 9, "context": "The same approach is also generally taken in the related contextual multi-arm bandit problem [11], where, starting from no data, at each step i = 1, .", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "In each of [11, 12, 13], the proposed solution is to fit m regression functions, and, for a new instance, predict m outcomes and pick the smallest prediction (subject to cleverly ensuring su cient exploration by, e.", "startOffset": 11, "endOffset": 23}, {"referenceID": 10, "context": "In each of [11, 12, 13], the proposed solution is to fit m regression functions, and, for a new instance, predict m outcomes and pick the smallest prediction (subject to cleverly ensuring su cient exploration by, e.", "startOffset": 11, "endOffset": 23}, {"referenceID": 11, "context": "In each of [11, 12, 13], the proposed solution is to fit m regression functions, and, for a new instance, predict m outcomes and pick the smallest prediction (subject to cleverly ensuring su cient exploration by, e.", "startOffset": 11, "endOffset": 23}, {"referenceID": 10, "context": "squares (OLS) as in [12], ridge regression as in [11], or LASSO as in [13].", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "squares (OLS) as in [12], ridge regression as in [11], or LASSO as in [13].", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "squares (OLS) as in [12], ridge regression as in [11], or LASSO as in [13].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "Examples of pointwise consistent regression estimators are k-nearest neighbors (kNN) and kernel regression (see [14] for details).", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "In learning heterogeneous causal e\u21b5ects [2, 15], one is concerned with the case of observational data with two treatments, t = 1 (\u201cControl\u201d) and t = 2 (\u201cTreatment\u201d), and the estimation of the relative conditional average treatment e\u21b5ect (CATE), (x) = E [Y (2) Y (1) | X = x].", "startOffset": 40, "endOffset": 47}, {"referenceID": 13, "context": "In learning heterogeneous causal e\u21b5ects [2, 15], one is concerned with the case of observational data with two treatments, t = 1 (\u201cControl\u201d) and t = 2 (\u201cTreatment\u201d), and the estimation of the relative conditional average treatment e\u21b5ect (CATE), (x) = E [Y (2) Y (1) | X = x].", "startOffset": 40, "endOffset": 47}, {"referenceID": 13, "context": "1, [15] proposed estimates based on propensity-score weighting and kernel regression.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Recently, [2] developed the Causal Tree (CT), which adapts machine learning methods, namely recursive partitioning, to directly address the heterogeneous e\u21b5ects problem by using new estimates for the error of estimating (x) within a partition of X data and by leveraging an \u201chonest\u201d estimation method that splits the data into that used for partitioning and that used for e\u21b5ect estimation.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "We compare to this strategy applied to CT of [2] in our empirical investigation.", "startOffset": 45, "endOffset": 48}, {"referenceID": 14, "context": "In learning from logged bandit feedback [16, 17], one is concerned with learning a good policy for a contextual multi-arm bandit problem based on logged data from another, known policy, rather than online interactions.", "startOffset": 40, "endOffset": 48}, {"referenceID": 15, "context": "In learning from logged bandit feedback [16, 17], one is concerned with learning a good policy for a contextual multi-arm bandit problem based on logged data from another, known policy, rather than online interactions.", "startOffset": 40, "endOffset": 48}, {"referenceID": 14, "context": "To address this problem, [16] develop the Policy Optimizer for Exponential Models (POEM).", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "[17] propose an improved Normalized POEM (NPOEM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "For m = 2 and randomized data ( (1, x) = \u21e1 constant), [10] has a special case of (1).", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "(2)] and [2] and to [16, eq.", "startOffset": 9, "endOffset": 12}, {"referenceID": 17, "context": "Support vector machine (SVM), also known as max-margin, is a popular classification algorithm [19, 20].", "startOffset": 94, "endOffset": 102}, {"referenceID": 18, "context": "For example, we can seek linear functions \u21e2\u0302i x, but more generally we can consider a kernelized rule by considering a positive definite kernel K : Rd\u21e5 R ! R [21] and its associated reproducing kernel Hilbert space H = closure(span K(x, \u00b7) : x 2 R ) and letting \u21e2\u0302i 2 H.", "startOffset": 158, "endOffset": 162}, {"referenceID": 19, "context": "K(x, x0) = x x0, polynomial K(x, x0) = (1 + x x0/ ), and radial basis function (RBF) K(x, x0) = ekx xk2/(2 2) (see [22] for more).", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "By expressing the hinge with constraints and using the representer theorem [23] and the kernel Gram matrix Kij = K(Xi, Xj), we can write this problem as a convex quadratic program:", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "12 of [24] and when m 3 and weights are all one it is equivalent to the multiclass formulation of [25].", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "12 of [24] and when m 3 and weights are all one it is equivalent to the multiclass formulation of [25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": ", (Xn, Tn)} and let Q\u0302i = \u0000\u0302n(Xi, Ti) (as done in [18]).", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "Examples include logistic regression, kNN, and kernel regression (see [24] for more).", "startOffset": 70, "endOffset": 74}, {"referenceID": 23, "context": "Classification and regression trees (CART) are predictive models based on recursive partitioning: the covariate space is recursively partitioned by axis-aligned hyperplanes (x` \uf8ff \u2713 for ` 2 [d] and \u2713 2 R) in order to minimize a within-partition impurity measure [26].", "startOffset": 261, "endOffset": 265}, {"referenceID": 0, "context": "Impurities for classification include entropy and Gini and for regression include sum of squared errors, [2] gives impurities for estimating heterogeneous e\u21b5ects.", "startOffset": 105, "endOffset": 108}, {"referenceID": 24, "context": "For classification and regression, there have been attempts at finding globally optimal trees [28] despite it being NP-hard [29] and recently [30] proposed a successful mixed integer programming (MIP) approach.", "startOffset": 124, "endOffset": 128}, {"referenceID": 25, "context": "The variables p encode choice of cut at node p and constraint (5c) ensures only one is chosen (see [31]).", "startOffset": 99, "endOffset": 103}, {"referenceID": 26, "context": "Since these constraints are integral [32] we need not enfoce wip be binary.", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "1: Set P = 2 , LC = [2 ], Ap = {bp/2jc : j 2 [ ]}, Rpq = 1 i\u21b5 bp/2 blog2(q)cc is odd.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "As discussed earlier, methods that estimate CATE, notably [2]\u2019s CT, can be used to choose between two treatments by comparing (x) = E [Y | X = x, T = 2] E [Y | X = x, T = 1] to zero.", "startOffset": 58, "endOffset": 61}, {"referenceID": 14, "context": "POEM and NPOEM [16, 17] solve the problem of learning from logged bandit feedback, assuming access to the logging policy that generated the data.", "startOffset": 15, "endOffset": 23}, {"referenceID": 15, "context": "POEM and NPOEM [16, 17] solve the problem of learning from logged bandit feedback, assuming access to the logging policy that generated the data.", "startOffset": 15, "endOffset": 23}, {"referenceID": 27, "context": "Recent work [33] has looked at o\u270fine evaluation of contextual bandits with experimental data and showed that rejection sampling is su cient.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "Matching is a common tool for causal inference [34].", "startOffset": 47, "endOffset": 51}, {"referenceID": 29, "context": "This problem can be reduced to bipartite matching, which can be solved e ciently [35].", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "These are also analogous to the coe cient of prescription for conditional stochastic optimization [36].", "startOffset": 98, "endOffset": 102}, {"referenceID": 31, "context": "Currently, the common practice is to start a new patient at 35 mg/week and slowly adjust the dose [38].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "The dataset was also studied in an online (bandit) setting in [13] where an R&C approach is analyzed.", "startOffset": 62, "endOffset": 66}, {"referenceID": 32, "context": "It is known that VKORC1 and CYP2C9 genotypes are strongly associated to warfarin dosage requirements [39].", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "We also compare to our 1vA strategy using [2]\u2019s CT-A (adaptive) and CT-H (honest with 50-50 split) and to IPOEM and INPOEM (parameters tuned on 25% holdout validation set as in [16, 17]).", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "We also compare to our 1vA strategy using [2]\u2019s CT-A (adaptive) and CT-H (honest with 50-50 split) and to IPOEM and INPOEM (parameters tuned on 25% holdout validation set as in [16, 17]).", "startOffset": 177, "endOffset": 185}, {"referenceID": 15, "context": "We also compare to our 1vA strategy using [2]\u2019s CT-A (adaptive) and CT-H (honest with 50-50 split) and to IPOEM and INPOEM (parameters tuned on 25% holdout validation set as in [16, 17]).", "startOffset": 177, "endOffset": 185}, {"referenceID": 33, "context": "We use data from the National Supported Work Demonstration [40, 41] (available at users.", "startOffset": 59, "endOffset": 67}, {"referenceID": 34, "context": "We use data from the National Supported Work Demonstration [40, 41] (available at users.", "startOffset": 59, "endOffset": 67}, {"referenceID": 35, "context": "This data is the standard benchmark in evaluation of causal methodologies for estimating an average treatment e\u21b5ect [42].", "startOffset": 116, "endOffset": 120}], "year": 2016, "abstractText": "We study the problem of learning to choose from m discrete treatment options (e.g., medical drugs) the one with best causal e\u21b5ect for a particular instance (e.g., patient) characterized by an observation of covariates. The training data consists of observations of covariates, treatment, and the outcome of the treatment. We recast the problem of learning to personalize from these observational data as a single learning task, which we use to develop four specific machine learning methods to directly address the personalization problem, two with a unique interpretability property. We also show how to validate personalization models on observational data, proposing the new coe cient of personalization as a unitless measure of e\u21b5ectiveness. We demonstrate the power of the new methods in two specific personalized medicine and policymaking applications and show they provide a significant advantage over standard approaches.", "creator": "LaTeX with hyperref package"}}}