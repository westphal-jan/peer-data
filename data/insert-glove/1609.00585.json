{"id": "1609.00585", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2016", "title": "Doubly stochastic large scale kernel learning with the empirical kernel map", "abstract": "single-sideband With the rise 349 of patience big sts-134 data sets, the 1.1530 popularity iod of waybourn kernel problematical methods nikolaenko declined and hrc neural 4.96 networks took adyghe over leominster again. korybut The careered main problem 94.04 with tollberg kernel guntur methods pinkwater is mios that the fogdoe kernel matrix streptavidin grows quadratically with lastuvka the number driveshaft of parisse data deucalion points. boltz Most aubameyang attempts to scale pirc up shnaider kernel methods solve 50.90 this problem 3,338 by chard\u00f3n discarding aparte data aoraki points formula or basis 7-10 functions of some approximation of the seagoing kernel nallur map. kamerun Here we present a simple mutolo yet mandane effective alphabetic alternative zaanun for scaling up kernel malankara methods that takes into harl account devkota the ibasis entire data set via doubly grizzles stochastic optimization proffit of the graman emprical kernel amorello map. The algorithm is cheuse straightforward langdon to implement, reavis in particular in restelo parallel fingal execution settings; drumbeat it ningguo leverages the birket full fava power and tryst versatility counter-attacking of readily classical kernel kursh functions gangitano without the need hashem to explicitly formulate a kernel map approximation. ajones We provide empirical evidence that the manipulable algorithm summarily works on sofiane large zinj data 88.64 sets.", "histories": [["v1", "Fri, 2 Sep 2016 13:20:06 GMT  (135kb,D)", "http://arxiv.org/abs/1609.00585v1", null], ["v2", "Wed, 14 Sep 2016 11:58:08 GMT  (132kb,D)", "http://arxiv.org/abs/1609.00585v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nikolaas steenbergen", "sebastian schelter", "felix bie{\\ss}mann"], "accepted": false, "id": "1609.00585"}, "pdf": {"name": "1609.00585.pdf", "metadata": {"source": "CRF", "title": "Doubly stochastic large scale kernel learning with the empirical kernel map", "authors": ["Nikolaas Steenbergen", "Sebastian Schelter", "Felix Biessmann"], "emails": ["nikolaas.steenbergen@dfki.de", "sebastian.schelter@tu-berlin.de", "felix.biessmann@tu-berlin"], "sections": [{"heading": "1 Introduction", "text": "When kernel methods [15, 19] were introduced in the machine learning community, they quickly gained popularity and became the gold standard in many applications. A reason for this was that kernel methods are powerful tools for modelling nonlinear dependencies. Even more importantly, kernel methods offer a convenient split between modelling the data with an expressive set of versatile kernel functions for all kinds of data types (e.g., graph data [19] or text data [13]), and the learning part, including both the learning paradigm (unsupervised vs. supervised) and the optimization.\nThe main drawback of kernel methods is that they require the computation of the kernel matrix K \u2208 RN\u00d7N , where N is the number of samples. For large data sets this kernel matrix can neither be computed nor stored in memory. Even worse, the learning part of kernel machines often has complexity O(N3). This renders standard formulations of kernel methods intractable for large data sets. When machine learning entered the era of web-scale data sets, artificial neural networks, enjoying learning complexities ofO(N), took over again, and have been dominating the top ranks of competitions, the press on machine learning and all major conferences since then. But the advantage of neural networks \u2013 or other nonlinear supervised algorithms that perform well on large data sets in many applications, such as Random Forests [3] \u2013 leaves many researchers with one question (see e.g., [14]): What if kernel methods could be trained on the same amounts of data that neural networks can be trained on?\nThere have been several attempts to scale up kernel machines, most of which fall into two main categories: a) approximations of the kernel map based on subsampling of Fourier basis functions (see [16]) or b) approximations of the kernel matrix based on subsampling data points (see [21]). While both of these are powerful methods which often achieve competitive performance, most ap-\nar X\niv :1\n60 9.\n00 58\n5v 1\n[ cs\n.L G\n] 2\nplications of these approximations solve the problem of scaling up kernel machines by discarding data points or Fourier basis functions from the computationally expensive part of the learning. We present a remarkably simple yet effective alternative of scaling up kernel methods that \u2013 in contrast to many previous approaches \u2013 allows us to make use of the entire data set.\nSimilar to [7] we propose a doubly stochastic approximation to scale up kernel methods. In contrast to their work however, who use an explicit approximation of the kernel map1, we propose to use an approximation of the empirical kernel map. While the optimization follows a similar schema, there is evidence suggesting that approximations of the explicit kernel map can result in lower performance [22]. The approach is called doubly stochastic because there are two sources of noise in the optimization: a) the first source samples random data points at which a noisy gradient of the dual coefficients is evaluated and b) the second source samples data points at which a noisy version of the empirical kernel map is evaluated. We propose a redundant data distribution scheme that allows for computing approximations that go beyond the block-diagonal of the full kernel matrix, as proposed in [8] for example. We perform experiments on synthetic data comparing the proposed approach with other approximations of the kernel map, and conduct experiments with a parallel implementation to show the scale-up behaviour on a large data set.\nIn the following, we give a short summary of the essentials on kernel machines, in subsection 2.1 we give a broad overview over other attempts to scale up kernel methods, section 3 outlines the main idea of the paper and section 4 describes our experiments."}, {"heading": "2 Kernel methods", "text": "This section summarizes some of the essentials of kernel machines. For the sake of presentation we only consider supervised learning and assume D-dimensional real-valued input data xi \u2208 RD and a corresponding boolean label yi \u2208 {\u22121, 1}. The key idea of kernel methods is that the function to be learned \u03c6\u2217(xi), evaluated at the ith data point xi, is modelled as a linear combination \u03b1 \u2208 RN of similarities between data point xi and data points xj , j = {1, 2, . . . , N}, both mapped to a potentially infinite dimensional kernel feature space S\n\u03c6\u2217(xi) = N\u2211 j=1 k(xi,xj)\u03b1j . (1)\nHere N again denotes the number of data points in the data set, \u03b1i denotes the i-th entry of \u03b1, and the kernel function k(., .) measures the similarity of data points in the kernel feature space by computing inner products in S\nk(xi,xj) = \u3008\u03c6(xi), \u03c6(xj)\u3009S . (2)\nKernel methods became popular as a nonlinear dependency between data points (and labels) in input space becomes linear in S. Taking the shortcut through k(., .), i.e., mapping data points to S and computing their inner products without ever formulating the mapping \u03c6 explicitely when learning \u03c6 is sometimes referred to as kernel trick. Methods that attempt to construct an explicit representation of \u03c6 are hence sometimes referred to as explicit kernel approximations [7].\nMost kernel machines then minimize a function E(y,x,\u03b1, k) which combines a loss function l(y,x,\u03b1, k) with a regularization term r(\u03b1) that controls the complexity of \u03c6\nE(y,x,\u03b1, k) = r(\u03b1) + l(y,x,\u03b1, k). (3)\nThe regularizer r(\u03b1) often takes the form of some Lp norm of the vector of dual coefficients \u03b1, where usually p = 2. A popular example of Equation 3 is that of the kernel support-vector machine (SVM) [6]: a hinge loss combined with a quadratic regularizer\nESVM = ||max (0,1\u2212 diag(y)K\u03b1) ||1+\u03bb||\u03b1||2, \u2202ESVM \u2202\u03b1\n= max ( 0, 1\u2212 ( \u03bb\u03b1\u2212 y>K )) (4)\n1We follow the convention that explicit kernel maps refer to a data independent kernel map approximation, see also section 2.\nwhere y \u2208 {\u22121, 1}N is a vector of concatenated labels and diag(.) a transformation of a vector into a diagonal matrix. Other examples of popular kernel methods include a least squares loss function combined with L2 regularization, also known as Kernel Ridge Regression and spectral decompositions of the kernel matrix such as kernel PCA [18]. We refer the interested reader to [19] for an overview."}, {"heading": "2.1 Large Scale Kernel Learning", "text": "Evaluating the empirical kernel map in Equation 2 for one data point comes at the cost of N evaluations of the kernel function, since the index j (which picks the data points that are used for expanding the kernel map) runs over all data points in the data set2 both for training and predicting. Computing the full gradient with respect to \u03b1 requires N evaluations, too, so the total complexity of computing the gradient of a kernel machine is in the order of O(N2). This is the reason why kernel methods became unattractive for large data sets \u2013 as other methods like linear models or neural networks only require training in O(N) time. We categorize attempts to scale up kernel methods into two classes: a) Reducing the number of data points when evaluating the empirical kernel map in Equation 2 and b) avoiding to evaluate the empirical kernel map altogether by using an explicit approximation of the kernel map. There are more sophisticated approaches within each of these categories that can give a quite significant speedup [12, 17]. We focus on a comparison between these two types of approximations. We emphasize however that many of these additional improvements also apply to the approach proposed in this manuscript and are likely to improve its convergence and runtime. In the following, we briefly survey research from both categories.\nEmpirical / implicit kernel maps: The first approach, reducing the number of data points when evaluating the kernel function, amounts to subsampling data points for computing the empirical kernel map in Equation 2. The data points used to compute the empirical kernel map are sometimes referred to as landmarks [10]. A prominent line of research in this direction follows what is commonly referred to as the Nystro\u0308m method [21]. The key idea here is to take a low-rank approximation of the kernel matrix computed on a randomly subsampled part of the data, instead of using the entire matrix. Other work in this direction aims at sparsifying the vector of dual coefficients. Another idea similar to our approach is the Naive Online Regularized Risk Minimization Algorithm (NORMA) [11], the Forgetron [9] or other work on online kernel learning. The authors propose ways of speeding up the sum computation by discarding data points from the sum computation in Equation 1; this is the key difference to the approach proposed here which follows a much simpler randomized scheme. Few of the above methods are simple to implement in a parallel or distributed setting. One recent approach is to distribute the data to different workers and solve the kernel problems independently on each worker [8]. This implicitly assumes however that the kernel matrix is a block diagonal matrix where the blocks on the diagonal are the kernels on each worker \u2013 all the rest of the kernel matrix is neglected.\nExplicit kernel maps: Recent approaches for large scale kernel learning avoid the computation of the kernel matrix by relying on explicit forms of the kernel function [16, 20]. The basic idea is that instead of using a kernel function k, (which implicitly projects the data to kernel feature space S and computes the inner products in that space in a single step), explicit kernel functions just perform the first step: mapping to kernel feature space with an approximation of the kernel map \u03c6(.). This has the advantage of being able to directly control the effective number of features. The model then simply learns a linear combination of these features. Explicit feature maps often express the kernel function as a set of Fourier basis functions. [7] provides a comprehensive overview of kernel functions and their explicit representations. [20] gives a more detailed explanation with graphical illustrations for a small set of kernel functions. In the context of large-scale kernel learning this method was popularized by Rahimi and Recht under the name of random kitchen sinks [16]. An important parameter choice in these approaches is the number of basis functions. This choice determines the accuracy of the approximation as well as the speed of the computations.\n2Actually the complexity is of O(ND) where D is the dimensionality of the data, but as this is constant given a data set, we omit this factor here.\nWhich approximation is better? Both approaches, implicit kernel maps and explicit kernel maps, are similar in that they approximate a mapping to a potentially infinite dimensional space S. The main difference is that for empirical kernel map approaches, the approximation samples data points (and in most cases simply discards a lot of data points), while in the case of explicit kernel map approximations the approximation samples random Fourier basis functions.\nIn practice there are many limitations on how much data can be acquired and processed efficiently. Furthermore, the type of data influences the performance of either approximation: When using the empirical kernel map on extremely sparse data, the empirical kernel function evaluated on a small subset of data points will return 0 in most cases \u2013 while Fourier bases with low frequencies will cover the space of the data much better.\nWhich of the two approximations is better in practice is likely to depend on the data. Empirical evidence suggests that the Nystro\u0308m approximation is better than random kitchen sinks [22]. The authors of [20] perform an extensive comparison of various explicit kernel map approximations and empirical kernel maps, highlighting the advantages in the empirical kernel map approach: Empirical kernel maps have the potential to model some parts of the input distribution better \u2013 but they have to be trained on data. This can be considered a disadvantage. Yet there could be scenarios in which learning the feature representation via empirical kernel maps gives performance gains. We are not aware of a concise comparison of the two approaches in a parallel setting. In our experimental section we provide a direct comparison between the two methods in which we keep the optimization part fixed and concentrate on the type of approximation."}, {"heading": "3 Doubly stochastic kernel learning", "text": "This section describes the learning approach to which we refer to as doubly stochastic empirical kernel learning (DSEKL). The key idea is that in each iteration a random sample I \u2286 {1, 2, . . . , N}, |I| = I of data points is chosen for computing the gradient of the dual coefficients \u03b1 and another (independent) random sample J \u2286 {1, 2, . . . , N}, |J | = J of data points is chosen for expanding the empirical kernel map k(., .). Note that this is very similar to Algorithm 1 and 2 in [7], except that instead of drawing random basis functions of the kernel function approximation, we sample random data points for expanding the empirical kernel map in Equation 1. If one were to compute the entire kernel matrix K \u2208 RN\u00d7N , this procedure would correspond to sampling a rectangular submatrix KI,J \u2208 RI\u00d7J . The number of data points J sampled for expanding the empirical kernel map as well as the number of data points I to compute the gradient are important parameters that determine the noise of the gradient of the dual coefficients and the noise of the empirical kernel map, respectively. The pseudocode in algorithm 1 summarizes the procedure, which alternates two steps: 1) sample a random submatrix of the kernel matrix and 2) take a gradient step along the direction of \u2202E(xi)\u2202\u03b1j , \u2200i \u2208 I, j \u2208 J , the gradient of E w.r.t. \u03b1 at indices J evaluated at data points I.\nAlgorithm 1 Doubly Stochastic Kernel Learning\nRequire: (xi, yi), i \u2208 {1, . . . , N},xi \u2208 RD, yi \u2208 {\u22121,+1}, Kernel k(., .) Ensure: Dual coefficients \u03b1\n# Initialize coefficients \u03b1, initialize counter i = 0 while Not Converged do t\u2190 t+ 1 # Sample indices I for gradient I \u223c unif(1, N) # Sample indices J for empirical kernel map J \u223c unif(1, N) # Compute Gradient \u2200j \u2208 J : gj \u2190 \u2211 i\u2208I \u2202E(xi) \u2202\u03b1j\n(see e.g. Equation 4) # Update weight vector \u2200j \u2208 J : \u03b1j \u2190 \u03b1j \u2212 1/t gj\nend while\nNote that in contrast to other kernel approximations, the memory footprint of this algorithm is rather low: While low-rank approximations need to store the low rank factors, algorithm 1 only requires us to store the dual coefficients \u03b1. We simply set the learning rate parameter to 1/t where t is the number of iterations. It is good practice to adjust that parameter according to some more sophisticated schedule. We emphasize the applicability of many standard methods to speed up convergence of stochastic gradient, e.g. through better control of the variance of the gradients."}, {"heading": "4 Experiments", "text": "This experimental section describes experiments on artificial data and on publicly available realworld data sets. All experiments use a support-vector machine with RBF kernel, for the sake of comparability. We performed experiments on a single machine with serial execution (see algorithm 1) as well as with a parallel shared-memory variant of our approach (see algorithm 2). We compared against the batch SVM implementation available in scikit learn [4]. We conducted experiments with serial execution for small-scale experiments, while we leveraged the parallel variant for larger scale experiments.\nHyperparameter optimization We tuned Hyperparameters with two-fold cross-validation and exhaustive grid search for all models; the reported accuracies were computed on a held out test set of the same size as the training set. We select hyperparameters for batch and SGD algorithms (the regularization parameter and RBF scale) from a logarithmic grid from 10\u22126 \u2212 106; The SGD approaches have additional parameters such as the step size (candidates were 10\u22124 \u2212 104) and the minibatch size I for computing the gradient. For doubly stochastic kernel learning and for random fourier features, there is the additional hyperparameter J , referring to the number of kernel expansion coefficients or random fourier features, respectively.\nComparisons with related methods We compared the proposed method with other kernel approximations as well as with batch kernel SVMs. We conducted comparisons with random kitchen sinks (RKS) where the number of basis functions matched the number of expansion coefficients J . In order to assess standard large-scale kernel approximations that only use a subset of data points, we also compared with a version in which we first draw one random sample from the data, and then train the algorithm with that subset only. While most of these methods that use just a subset of the data apply more sophisticated schemes for selecting that subset and smarter ways of extrapolating, we focused on the main difference here, which is training on a fixed random subset of the data.\nData sets In order to provide a qualitative comparison between the different kernel map approximations, we performed experiments on small synthetic data sets. We chose the XOR problem described in Figure 1 as a benchmark for nonlinear classification. We also performed experiments on a number of standard benchmark real world data sets available on the libsvm homepage3. Table 1 lists these data sets.\n3https://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/binary.html"}, {"heading": "4.1 Serial Execution", "text": "We ran small-scale experiments with a single threaded implementation of algorithm 1. We generated N = 100 data points according to the XOR problem, see Figure 1, and optimized all hyperparameters as described in section 4. Figure 2 shows comparisons of the proposed method with random kitchen sinks and a fixed random selection of data points, as well as with a batch setting. We plot the error on the test set varying I , the number of samples for computing the gradient, in Figure 2a and Figure 2b while keeping all other hyperparameters fixed. Figure 2c and Figure 2d show the error when varying J , the number of expansion coefficients. Note that with too few data points for computing the gradient or the expansion, both random kitchen sinks as well as a fixed sample of data points have an advantage over the doubly stochastic approach (Figure 2a and Figure 2c). As the number of data points in the gradient computation and the kernel map expansion increases however, doubly stochastic kernel learning achieves performance comparable to that of batch methods, indicated as dotted line (Figure 2b and Figure 2d).\nWe performed experiments on a number of standard benchmark real world data sets available on the libsvm homepage4. We compared to the batch version using serial execution and small data sets. We discuss experiments on a larger dataset, leveraging our parallel variant in subsection 4.2. For all experiments, we sampled min(1000, Ndataset) data points where Ndataset is the number of data points in the respective data set, and took half the data for training and half the data for testing, including hyperparameter optimization on the training set. We ran 10 repetitions of each experiment and show the test set error in Table 1. In all data sets investigated, the proposed doubly stochastic empirical kernel learning approach achieved errors comparable to that of a batch SVM. In cases where the batch SVM achieves perfect accuracy and DSEKL still resulted in a few errors, we emphasize that we conduct these comparisons to show that DSEKL has the potential to achieve performance comparable to that of batch methods. Refining the SGD optimization or running more iterations could further improve the performance, yet our main intention is to only provide a proof of concept for the doubly stochastic approach. Also note that the proposed DSEKL approach only uses a fraction of the data in each step. This allows for training on much larger data sets, which we discuss in the next section."}, {"heading": "4.2 Parallel Execution using a Shared-Memory Variant", "text": "This section describes the experiments performed using a parallel, shared-memory variant of our approach inspired by [1]. We list the pseudocode in algorithm 2. The difference to algorithm 1 is that we run multiple workers at the same time, and process multiple sample batches for the empirical kernel map per iteration to parallelize learning. We used sampling without replacement to generate the sample batches for the different workers.\n4https://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/binary.html\nAlgorithm 2 Parallel Shared-Memory Nonlinear Support-Vector Machine Require: sample size s, number of workers K\n1: # Initialize coefficients \u03b1 2: # Sample indices I(0), . . . , I(K) for gradient 3: # Sample indices J (0), . . . ,J (K) for empirical kernel map 4: G\u2190 I 5: while Not Converged do 6: for all I(0), . . . , I(K) do 7: for all J (0), . . . ,J (K) in parallel on worker k do 8: # Compute gradients as in Algorithm 1 9: \u2200j \u2208 J (k) : g(k)j \u2190 \u2211 i\u2208I(k) \u2202E(xi) \u2202\u03b1j\n10: # Aggregate inverse gradients for dampening updates of \u03b1 11: Gii \u2190 Gii + ( g (k) ji )2 for all i \u2208 I(k) and j \u2208 J (k) 12: end for 13: # Update weight vector 14: \u03b1\u2190 \u03b1\u2212G\u2212 12 \u2211 k g (k) 15: end for 16: end while\nWe ran our experiment on the covertype dataset5, consisting of 581,012 data points with 54 features. We drew samples of I = 10, 000 points for computing the gradient and J = 10, 000 for evaluating the empirical kernel map. For the sake of comparability with the results in [7], we set the regularization parameter \u03bb to 1/N , and fix the RBF scale to 1.0. We employ a learning rate of 1/i, where i is the number of epochs, i.e., passes through the entire data set. We stop the training process if the L2 norm of the weight change over one epoch is less than 1. We separate the entire data set into three random splits for training, validation during training and evaluation after convergence. For computing the error on the validation set during training, we hold back 1122 random samples. Additionally we hold back a separate random sample of 20,000 data points for the final evaluation after convergence. Figure 3a depicts the validation error after evaluating all J for one mini batch of I respectively, for about 3 passes through the whole dataset. After one pass through the data the validation error decreased from 51% to about 17%. After 54 epochs the algorithm converged, and the final error rate on the evaluation set was 13.34%. These results are comparable to [7] who report a test error of about 15% after one iteration.\nFigure 3b shows the speedup achieved through the usage of multiple cores in our shared-memory variant. Our python implementation of algorithm 2 runs on a 48 core machine (having 24 physical cores with hyperthreading) with 500 GB main memory. We recorded the runtime for processing a single batch I(k), for which the empirical kernel map is evaluated using all batches J (k), k : 1, . . . ,K in parallel on twice the full covertype dataset to ensure a full utilization of the machine. We measured speedups against the runtime on a single core and increase the parallelism by ten cores at a time. We observed a linear speedup until running with 20 cores, where we achieved a speedup of factor 16 compared to the runtime of only one core. After that, the speedup curve flattens out. We attribute this flattening to several overhead factors, such as resource-sharing from hyperthreading after exceeding the number of physical cores, as well as serialization costs caused by python\u2019s multithreading. Nevertheless, this experiment shows that our approach amends itself to a simple parallelization scheme, which has the potential for massive speedups."}, {"heading": "5 Conclusion", "text": "When modelling complex functions, the practitioner usually has three main options: Decision-Tree based methods, Neural Networks and Kernel methods. Decision-Tree based methods appear to dominate most Kaggle competitions, and in general give stunning performance on real-world data sets [5]. But when a modelling task goes beyond simple supervised settings, these methods might not be the first choice. Deep neural networks yield large performance improvements on many tasks and are successfully used for unsupervised learning as well \u2013 but they are often difficult to design and train. This is where kernel methods offer advantages: Instead of optimizing a network architecture one simply picks an off-the-shelf kernel function for a given type of data, and then one only needs to perform model selection over a handful of kernel parameters in order to tackle both unsupervised and supervised learning in a principled manner.\n5https://archive.ics.uci.edu/ml/datasets/Covertype\nWe have proposed a simple algorithm for scaling up kernel learning that is easy to implement and parallelize. Our results demonstrate that the proposed method achieves competitive performance on standard benchmarks. We hope complementing the existing methods for large scale kernel learning as well as other successful methods such as random forests and neural networks will ultimately help to better understand the strengths of the respective methods, independent of factors such as hardware and optimization procedures.\nOur experiments on artificial data suggest that there are conditions under which the empirical kernel map approach performs better than the explicit kernel map approximation, in agreement with previous results in [20]. Yet a direct comparison of our results with results obtained with explicit kernel maps as in [7] is difficult, due to the differences in the implementations. An important topic of future research will be to investigate when to prefer explicit kernel map approximations as in [16, 7] over the empirical kernel map approaches presented here. In terms of implementation however, applying the doubly stochastic empirical kernel map approach to more complex kernels might appear simpler than implementing a dedicated explicit kernel map approximation for every kernel function.\nFurthermore, we showed that a parallel variant of our algorithm is extremely simple to implement, achieves competitive performance on a large data set, and has the potential for massive speed ups. An interesting direction for the future would be to implement the doubly stochastic approach on graphics cards to leverage their potential for massive parallel computation and use the proposed approach in a streaming/online learning setting, similar to the approaches in [11, 9] but with a simpler, randomized scheme for reducing the cost of the empirical kernel map computation. Note that it is straightforward to combine the DSEKL approach with truncation schemes as in [11, 9] during or after convergence for speeding up predictions at test time.\nAnother interesting direction could be to explore a distributed variant of our algorithm. We found that in its presented form, our approach is not well suited for distributed data processing systems like Apache Spark [23] or Apache Flink [2] which use a shared-nothing architecture. This is due to the fact that a naive distributed execution of our algorithm would impose a too high amount of communication per iteration for aggregating the gradients over the network, as well as for redistributing the updated parameter vector. A variant that updates parameters locally on the slaves in the cluster, and only updates the global model from time to time (thereby reducing inter-machine communication) could be worth to look into."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Alekh Agarwal", "Olivier Chapelle", "Miroslav Dud\u0131\u0301k", "John Langford"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "The stratosphere platform for big data analytics", "author": ["Alexander Alexandrov", "Rico Bergmann", "Stephan Ewen", "Johann-Christoph Freytag", "Fabian Hueske", "Arvid Heise", "Odej Kao", "Marcus Leich", "Ulf Leser", "Volker Markl"], "venue": "The VLDB Journal\u2014The International Journal on Very Large Data Bases,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "API design for machine learning software: experiences from the scikit-learn project", "author": ["Lars Buitinck", "Gilles Louppe", "Mathieu Blondel", "Fabian Pedregosa", "Andreas Mueller", "Olivier Grisel", "Vlad Niculae", "Peter Prettenhofer", "Alexandre Gramfort", "Jaques Grobler", "Robert Layton", "Jake VanderPlas", "Arnaud Joly", "Brian Holt", "Ga\u00ebl Varoquaux"], "venue": "In ECML PKDD Workshop: Languages for Data Mining and Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["Rich Caruana", "Alexandru Niculescu-Mizil"], "venue": "In Proceedings of the ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, 20:273\u2013297", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Scalable kernel methods via doubly stochastic gradients", "author": ["Bo Dai", "Bo Xie", "Niao He", "Yingyu Liang", "Anant Raj", "Maria-Florina Balcan", "Le Song"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Distributed gaussian processes", "author": ["Marc Peter Deisenroth", "Jun Wei Ng"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "The forgetron: A kernel-based perceptron on a budget", "author": ["Ofer Dekel", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "SIAM J. Comput.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Fast prediction for large-scale kernel machines", "author": ["Cho-Jui Hsieh", "Si Si", "Inderjit S. Dhillon"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Online learning with kernels", "author": ["Jyrki Kivinen", "Alexander J. Smola", "Robert C. Williamson"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Fastfood - computing hilbert space expansions in loglinear time", "author": ["Quoc V. Le", "Tam\u00e1s Sarl\u00f3s", "Alexander J. Smola"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Text classification using string kernels", "author": ["Huma Lodhi", "John Shawe-Taylor", "Nello Cristianini", "Chris Watkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "How to scale up kernel methods to be as good as deep neural nets", "author": ["Zhiyun Lu", "Avner May", "Kuan Liu", "Alireza Bagheri Garakani", "Dong Guo", "Aur\u00e9lien Bellet", "Linxi Fan", "Michael Collins", "Brian Kingsbury", "Michael Picheny", "Fei Sha"], "venue": "CoRR, abs/1411.4000,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "An introduction to kernel-based learning algorithms", "author": ["Klaus-Robert M\u00fcller", "Sebastian Mika", "G Ratsch", "K Tsuda", "B Bernhard Sch\u00f6lkopf"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Less is more: Nystr\u00f6m computational regularization", "author": ["Alessandro Rudi", "Raffaello Camoriano", "Lorenzo Rosasco"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B Sch\u00f6lkopf", "A J Smola", "KR M\u00fcller"], "venue": "Neural Computation, 10(6):1299\u20131319", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Kernel methods for pattern analysis", "author": ["J Shawe-Taylor", "N Cristianini"], "venue": "Cambridge University Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": "In CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["Christopher K.I. Williams", "Matthias Seeger"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["Tianbao Yang", "Yu-feng Li", "Mehrdad Mahdavi", "Rong Jin", "Zhi-Hua Zhou"], "venue": "In NIPS", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["Matei Zaharia", "Mosharaf Chowdhury", "Tathagata Das", "Ankur Dave", "Justin Ma", "Murphy McCauley", "Michael J Franklin", "Scott Shenker", "Ion Stoica"], "venue": "In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "When kernel methods [15, 19] were introduced in the machine learning community, they quickly gained popularity and became the gold standard in many applications.", "startOffset": 20, "endOffset": 28}, {"referenceID": 17, "context": "When kernel methods [15, 19] were introduced in the machine learning community, they quickly gained popularity and became the gold standard in many applications.", "startOffset": 20, "endOffset": 28}, {"referenceID": 17, "context": ", graph data [19] or text data [13]), and the learning part, including both the learning paradigm (unsupervised vs.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": ", graph data [19] or text data [13]), and the learning part, including both the learning paradigm (unsupervised vs.", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": ", [14]): What if kernel methods could be trained on the same amounts of data that neural networks can be trained on?", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "There have been several attempts to scale up kernel machines, most of which fall into two main categories: a) approximations of the kernel map based on subsampling of Fourier basis functions (see [16]) or b) approximations of the kernel matrix based on subsampling data points (see [21]).", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "There have been several attempts to scale up kernel machines, most of which fall into two main categories: a) approximations of the kernel map based on subsampling of Fourier basis functions (see [16]) or b) approximations of the kernel matrix based on subsampling data points (see [21]).", "startOffset": 282, "endOffset": 286}, {"referenceID": 5, "context": "Similar to [7] we propose a doubly stochastic approximation to scale up kernel methods.", "startOffset": 11, "endOffset": 14}, {"referenceID": 20, "context": "While the optimization follows a similar schema, there is evidence suggesting that approximations of the explicit kernel map can result in lower performance [22].", "startOffset": 157, "endOffset": 161}, {"referenceID": 6, "context": "We propose a redundant data distribution scheme that allows for computing approximations that go beyond the block-diagonal of the full kernel matrix, as proposed in [8] for example.", "startOffset": 165, "endOffset": 168}, {"referenceID": 5, "context": "Methods that attempt to construct an explicit representation of \u03c6 are hence sometimes referred to as explicit kernel approximations [7].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "A popular example of Equation 3 is that of the kernel support-vector machine (SVM) [6]: a hinge loss combined with a quadratic regularizer", "startOffset": 83, "endOffset": 86}, {"referenceID": 16, "context": "Other examples of popular kernel methods include a least squares loss function combined with L2 regularization, also known as Kernel Ridge Regression and spectral decompositions of the kernel matrix such as kernel PCA [18].", "startOffset": 218, "endOffset": 222}, {"referenceID": 17, "context": "We refer the interested reader to [19] for an overview.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "There are more sophisticated approaches within each of these categories that can give a quite significant speedup [12, 17].", "startOffset": 114, "endOffset": 122}, {"referenceID": 15, "context": "There are more sophisticated approaches within each of these categories that can give a quite significant speedup [12, 17].", "startOffset": 114, "endOffset": 122}, {"referenceID": 8, "context": "The data points used to compute the empirical kernel map are sometimes referred to as landmarks [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "A prominent line of research in this direction follows what is commonly referred to as the Nystr\u00f6m method [21].", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "Another idea similar to our approach is the Naive Online Regularized Risk Minimization Algorithm (NORMA) [11], the Forgetron [9] or other work on online kernel learning.", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "Another idea similar to our approach is the Naive Online Regularized Risk Minimization Algorithm (NORMA) [11], the Forgetron [9] or other work on online kernel learning.", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "One recent approach is to distribute the data to different workers and solve the kernel problems independently on each worker [8].", "startOffset": 126, "endOffset": 129}, {"referenceID": 14, "context": "Explicit kernel maps: Recent approaches for large scale kernel learning avoid the computation of the kernel matrix by relying on explicit forms of the kernel function [16, 20].", "startOffset": 167, "endOffset": 175}, {"referenceID": 18, "context": "Explicit kernel maps: Recent approaches for large scale kernel learning avoid the computation of the kernel matrix by relying on explicit forms of the kernel function [16, 20].", "startOffset": 167, "endOffset": 175}, {"referenceID": 5, "context": "[7] provides a comprehensive overview of kernel functions and their explicit representations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[20] gives a more detailed explanation with graphical illustrations for a small set of kernel functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In the context of large-scale kernel learning this method was popularized by Rahimi and Recht under the name of random kitchen sinks [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 20, "context": "Empirical evidence suggests that the Nystr\u00f6m approximation is better than random kitchen sinks [22].", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "The authors of [20] perform an extensive comparison of various explicit kernel map approximations and empirical kernel maps, highlighting the advantages in the empirical kernel map approach: Empirical kernel maps have the potential to model some parts of the input distribution better \u2013 but they have to be trained on data.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "Note that this is very similar to Algorithm 1 and 2 in [7], except that instead of drawing random basis functions of the kernel function approximation, we sample random data points for expanding the empirical kernel map in Equation 1.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "We compared against the batch SVM implementation available in scikit learn [4].", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "2) around [1, 1]> and [\u22121,\u22121]>, and data points from the other class (red dots) are drawn from the same gaussian distribution centered around [1,\u22121]> and [\u22121, 1]>.", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "2) around [1, 1]> and [\u22121,\u22121]>, and data points from the other class (red dots) are drawn from the same gaussian distribution centered around [1,\u22121]> and [\u22121, 1]>.", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "This section describes the experiments performed using a parallel, shared-memory variant of our approach inspired by [1].", "startOffset": 117, "endOffset": 120}, {"referenceID": 5, "context": "For the sake of comparability with the results in [7], we set the regularization parameter \u03bb to 1/N , and fix the RBF scale to 1.", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "These results are comparable to [7] who report a test error of about 15% after one iteration.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "Decision-Tree based methods appear to dominate most Kaggle competitions, and in general give stunning performance on real-world data sets [5].", "startOffset": 138, "endOffset": 141}, {"referenceID": 18, "context": "Our experiments on artificial data suggest that there are conditions under which the empirical kernel map approach performs better than the explicit kernel map approximation, in agreement with previous results in [20].", "startOffset": 213, "endOffset": 217}, {"referenceID": 5, "context": "Yet a direct comparison of our results with results obtained with explicit kernel maps as in [7] is difficult, due to the differences in the implementations.", "startOffset": 93, "endOffset": 96}, {"referenceID": 14, "context": "An important topic of future research will be to investigate when to prefer explicit kernel map approximations as in [16, 7] over the empirical kernel map approaches presented here.", "startOffset": 117, "endOffset": 124}, {"referenceID": 5, "context": "An important topic of future research will be to investigate when to prefer explicit kernel map approximations as in [16, 7] over the empirical kernel map approaches presented here.", "startOffset": 117, "endOffset": 124}, {"referenceID": 9, "context": "An interesting direction for the future would be to implement the doubly stochastic approach on graphics cards to leverage their potential for massive parallel computation and use the proposed approach in a streaming/online learning setting, similar to the approaches in [11, 9] but with a simpler, randomized scheme for reducing the cost of the empirical kernel map computation.", "startOffset": 271, "endOffset": 278}, {"referenceID": 7, "context": "An interesting direction for the future would be to implement the doubly stochastic approach on graphics cards to leverage their potential for massive parallel computation and use the proposed approach in a streaming/online learning setting, similar to the approaches in [11, 9] but with a simpler, randomized scheme for reducing the cost of the empirical kernel map computation.", "startOffset": 271, "endOffset": 278}, {"referenceID": 9, "context": "Note that it is straightforward to combine the DSEKL approach with truncation schemes as in [11, 9] during or after convergence for speeding up predictions at test time.", "startOffset": 92, "endOffset": 99}, {"referenceID": 7, "context": "Note that it is straightforward to combine the DSEKL approach with truncation schemes as in [11, 9] during or after convergence for speeding up predictions at test time.", "startOffset": 92, "endOffset": 99}, {"referenceID": 21, "context": "We found that in its presented form, our approach is not well suited for distributed data processing systems like Apache Spark [23] or Apache Flink [2] which use a shared-nothing architecture.", "startOffset": 127, "endOffset": 131}, {"referenceID": 1, "context": "We found that in its presented form, our approach is not well suited for distributed data processing systems like Apache Spark [23] or Apache Flink [2] which use a shared-nothing architecture.", "startOffset": 148, "endOffset": 151}], "year": 2017, "abstractText": "With the rise of big data sets, the popularity of kernel methods declined and neural networks took over again. The main problem with kernel methods is that the kernel matrix grows quadratically with the number of data points. Most attempts to scale up kernel methods solve this problem by discarding data points or basis functions of some approximation of the kernel map. Here we present a simple yet effective alternative for scaling up kernel methods that takes into account the entire data set via doubly stochastic optimization of the emprical kernel map. The algorithm is straightforward to implement, in particular in parallel execution settings; it leverages the full power and versatility of classical kernel functions without the need to explicitly formulate a kernel map approximation. We provide empirical evidence that the algorithm works on large data sets.", "creator": "LaTeX with hyperref package"}}}