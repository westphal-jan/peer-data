{"id": "1609.01226", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2016", "title": "The Robustness of Estimator Composition", "abstract": "\u010doli\u0107 We ricardos formalize notions tarkwa of robustness for composite searles estimators via haircuts the brinster notion 84.15 of a lava-flooded breakdown point. curll A composite daofu estimator successively 114.7 applies 3,760 two (http://www.homedepot.com or more) kelson estimators: brownrigg on pisani data kawasme decomposed into antibiotic disjoint parts, calstart it applies dantewada the first estimator on crawler each part, two-pole then vernaculars the second estimator 98.82 on wanting the colasanto outputs of ensnaring the jahres first .244 estimator. And so ushiro on, blaffer if kosai the 44.50 composition jebril is of more than two estimators. Informally, zalamea the ideapad breakdown point coate is the 14n minimum fraction of data worby points which dvd-video if significantly dysplasia modified shinkolobwe will wisk also upl significantly montlu\u00e7on modify intraday the psicosis output aristolochia of nelson the estimator, drakoulias so it is typically seok desirable lockinge to b\u00e1novce have comiss\u00e3o a iv. large breakdown point. Our main result brunansky shows chichijima that, landestheater under mild conditions on the individual estimators, neurocrine the breakdown point of the medullary composite twinnings estimator is the emsa product of bairam the ahhs breakdown points of the 58.73 individual tams estimators. We delineating also demonstrate tinwood several shamanism scenarios, ranging 14-of-22 from laetrile regression to relocatable statistical maturely testing, donisthorpe where rigoletto this 232.2 analysis unesco is erturk easy rakaia to apply, useful ruxandra in understanding 16,667 worst calibration case manesar robustness, ehlvest and sheds tuntex powerful insights editura onto the 98.92 associated flc data crump analysis.", "histories": [["v1", "Mon, 5 Sep 2016 17:27:22 GMT  (62kb,D)", "http://arxiv.org/abs/1609.01226v1", "14 pages, 2 figures, 29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain"]], "COMMENTS": "14 pages, 2 figures, 29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["pingfan tang", "jeff m phillips"], "accepted": true, "id": "1609.01226"}, "pdf": {"name": "1609.01226.pdf", "metadata": {"source": "CRF", "title": "The Robustness of Estimator Composition", "authors": ["Pingfan Tang", "Jeff M. Phillips"], "emails": ["tang1984@cs.utah.edu", "jeffp@cs.utah.edu"], "sections": [{"heading": "1 Introduction", "text": "Robust statistical estimators [5, 7] (in particular, resistant estimators), such as the median, are an essential tool in data analysis since they are provably immune to outliers. Given data with a large fraction of extreme outliers, a robust estimator guarantees the returned value is still within the nonoutlier part of the data. In particular, the roll of these estimators is quickly growing in importance as the scale and automation associated with data collection and data processing becomes more commonplace. Artisanal data (hand crafted and carefully curated), where potential outliers can be removed, is becoming proportionally less common. Instead, important decisions are being made blindly based on the output of analysis functions, often without looking at individual data points and their effect on the outcome. Thus using estimators as part of this pipeline that are not robust are susceptible to erroneous and dangerous decisions as the result of a few extreme and rogue data points.\nAlthough other approaches like regularization and pruning a constant number of obvious outliers are common as well, they do not come with the important guarantees that ensure these unwanted outcomes absolutely cannot occur.\nIn this paper we initiate the formal study of the robustness of composition of estimators through the notion of breakdown points. These are especially important with the growth of data analysis pipelines where the final result or prediction is the result of several layers of data processing. When each layer in this pipeline is modeled as an estimator, then our analysis provides the first general robustness analysis of these processes.\nThe breakdown point [4, 3] is a basic measure of robustness of an estimator. Intuitively, it describes how many outliers can be in the data without the estimator becoming unreliable. However, the literature is full of slightly inconsistent and informal definitions of this concept. For example:\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 9.\n01 22\n6v 1\n[ cs\n.L G\n] 5\nS ep\n2 01\n\u2022 Aloupis [1] write \u201cthe breakdown point is the proportion of data which must be moved to infinity so that the estimator will do the same.\u201d\n\u2022 Huber and Ronchetti [8] write \u201cthe breakdown point is the smallest fraction of bad observations that may cause an estimator to take on arbitrarily large aberrant values.\"\n\u2022 Dasgupta, Kumar, and Srikumar [14] write \u201cthe breakdown point of an estimator is the largest fraction of the data that can be moved arbitrarily without perturbing the estimator to the boundary of the parameter space.\u201d\nAll of these definitions have similar meanings, and they are typically sufficient for the purpose of understanding a single estimator. However, they are not mathematically rigorous, and it is difficult to use them to discuss the breakdown point of composite estimators.\nComposition of Estimators. In a bit more detail (we give formal definitions in Section 2.1), an estimator E maps a data set to single value in another space, sometimes the same as a single data point. For instance the mean or the median are simple estimators on one-dimensional data. A composite E1-E2 estimator applies two estimators E1 and E2 on data stored in a hierarchy. Let P = {P1, P2, . . . , Pn} be a set of subdata sets, where each subdata set Pi = {pi,1, pi,2, . . . , pi,k} has individual data readings. Then the E1-E2 estimator reports E2(E1(P1), E1(P2), . . . , E1(Pn)), that is the estimator E2 applied to the output of estimator E1 on each subdata set."}, {"heading": "1.1 Examples of Estimator Composition", "text": "Composite estimators arise in many scenarios in data analysis.\nUncertain Data. For instance, in the last decade there has been increased focus on the study of uncertainty data [11, 9, 2] where instead of analyzing a data set, we are given a model of the uncertainty of each data point. Consider tracking the summarization of a group of n people based on noisy GPS measurements. For each person i we might get k readings of their location Pi, and use these k readings as a discrete probability distribution of where that person might be. Then in order to represent the center of this set of people a natural thing to do would be to estimate the location of each person as xi \u2190 E1(Pi), and then use these estimates to summarize the entire group E2(x1, x2, . . . , xn). Using the mean as E1 and E2 would be easy, but would be susceptible to even a single outrageous outlier (all people are in Manhattan, but a spurious reading was at (0, 0) lat-long, off the coast of Africa). An alternative is to use the L1-median for E1 and E2, that is known to have an optimal breakdown point of 0.5. But what is the breakdown point of the E1-E2 estimator?\nRobust Analysis of Bursty Behavior. Understanding the robustness of estimators can also be critical towards how much one can \u201cgame\u201d a system. For instance, consider a start-up media website that gets bursts of traffic from memes they curate. They publish a statistic showing the median of the top half of traffic days each month, and aggregate these by taking the median of such values over the top half of all months. This is a composite estimator, and they proudly claim, even through they have bursty traffic, it is robust (each estimator has a breakdown point of 0.25). If this composite estimator shows large traffic, should a potential buyer of this website by impressed? Is there a better, more robust estimator the potential buyer could request? If the media website can stagger the release of its content, how should they distribute it to maximize this composite estimator?\nPart of the Data Analysis Pipeline. This process of estimator composition is very common in broad data analysis literature. This arises from the idea of an \u201canalysis pipeline\u201d where at several stages estimators or analysis is performed on data, and then further estimators and analysis are performed downstream. In many cases a robust estimator like the median is used, specifically for its robustness properties, but there is no analysis of how robust the composition of these estimators is."}, {"heading": "1.2 Main Results", "text": "This paper initiates the formal and general study of the robustness of composite estimators.\n\u2022 In Subsection 2.1, we give two formal definitions of breakdown points which are both required to prove composition theorem. One variant of the definition closely aligns with other formalizations [4, 3], while another is fundamentally different.\n\u2022 The main result provides general conditions under which an E1-E2 estimator with breakdown points \u03b21 and \u03b22, has a breakdown point of \u03b21\u03b22 (Theorem 2 in Subsection 2.2).\n\u2022 Moreover, by showing examples where our conditions do not strictly apply, we gain an understanding of how to circumvent the above result. An example is in composite percentile estimators (e.g., E1 returns the 25th percentile, and E2 the 75th percentile of a ranked set). These composite estimators have larger breakdown point than \u03b21 \u00b7 \u03b22. \u2022 The main result can extended to multiple compositions, under suitable conditions, so for\ninstance anE1-E2-E3 estimator has a breakdown point of \u03b21\u03b22\u03b23 (Theorem 3 in Subsection 2.3). This implies that long analysis chains can be very suspect to a few carefully places outliers since the breakdown point decays exponentially in the length of the analysis chain.\n\u2022 In Section 3, we highlight several applications of this theory, including robust regression, robustness of p-values, a depth-3 composition, and how to advantageously manipulate the observation about percentile estimator composition. We demonstrate a few more applications with simulations in Section 4."}, {"heading": "2 Robustness of Estimator Composition", "text": ""}, {"heading": "2.1 Formal Definitions of Breakdown Points", "text": "In this paper, we give two definitions for the breakdown point: Asymptotic Breakdown Point and Asymptotic Onto-Breakdown Point. The first definition, Asymptotic Breakdown Point, is similar to the classic formal definitions in [4] and [3] (including their highly technical nature), although their definitions of the estimator are slightly different leading to some minor differences in special cases. However our second definition, Asymptotic Onto-Breakdown Point, is a structurally new definition, and we illustrate how it can result in significantly different values on some common and useful estimators. Our main theorem will require both definitions, and the differences in performance will lead to several new applications and insights.\nWe define an estimator E as a function from the collection of some finite subsets of a metric space (X , d) to another metric space (X \u2032, d\u2032):\nE : A \u2282 {X \u2282X | 0 < |X| <\u221e} 7\u2192X \u2032, (1)\nwhere X is a multiset. This means if x \u2208 X then x can appear more than once in X , and the multiplicity of elements will be considered when we compute |X|.\nFinite Sample Breakdown Point. For estimator E defined in (1) and positive integer n we define its finite sample breakdown point gE(n) over a set M as\ngE(n) = { max(M) if M 6= \u2205 0 if M = \u2205 (2)\nwhere for \u03c1(x\u2032, X) = maxx\u2208X d(x\u2032, x) is the distance from x\u2032 to the furthest point in X ,\nM = {m \u2208 [0, n] | \u2200X \u2208 A , |X| = n, \u2200 G1 > 0,\u2203 G2 = G2(X,G1) s.t. \u2200X \u2032 \u2208 A , if |X \u2032| = n and |{x\u2032 \u2208 X \u2032 | \u03c1(x\u2032, X) > G1}| \u2264 m then d\u2032(E(X), E(X \u2032)) \u2264 G2}.\n(3)\nFor an estimator E in (1) and X \u2208 A , the finite sample breakdown point gE(n) means if the number of unbounded points in X \u2032 is at most gE(n), then E(X \u2032) will be bounded. Lets break this definition down a bit more. The definition holds over all data sets X \u2208 A of size n, and for all values G1 > 0 and some value G2 defined as a function G2(X,G1) of the data set X and value G1. Then gE(n) is the maximum value m (over all X , G1, and G2 above) such that for all X \u2032 \u2208 A with |X \u2032| = n then |{x\u2032 \u2208 X \u2032 | \u03c1(x\u2032, X) > G1}| \u2264 m (that is at most m points are further than G1 from X) where the estimators are close, d\u2032(E(X), E(X \u2032)) \u2264 G2. For example, consider a point set X = {0, 0.15, 0.2, 0.25, 0.4, 0.55, 0.6, 0.65, 0.72, 0.8, 1.0} with n = 11 and median 0.55. If we set G1 = 3, then we can consider sets X \u2032 of size 11 with fewer than m points that are either greater than 3 or less than \u22122. This means in X \u2032 there are at most m points which are greater than 3 or less than \u22122, and all other n\u2212m points are in [\u22122, 3]. Under these conditions, we can (conservatively) set G2 = 4, and know that for values of m as 1, 2, 3, 4, or 5, then the median of X \u2032 must be between \u22123.45 and 4.55; and this holds no matter where we set those m points (e.g., at 20 or at 1000). This does not hold for m \u2265 6, so gE(11) = 5.\nAsymptotic Breakdown Point. If the limit limn\u2192\u221e gE(n)n exists, then we define this limit\n\u03b2 = lim n\u2192\u221e\ngE(n)\nn (4)\nas the asymptotic breakdown point, or breakdown point for short, of the estimator E. Remark 1. It is not hard to see that many common estimators satisfy the conditions. For example, the median, L1-median [1], and Siegel estimators [12] all have asymptotic breakdown points of 0.5.\nAsymptotic Onto-Breakdown Point. For an estimator E given in (1) and positive integer n, if\nM\u0303 = {0 \u2264 m \u2264 n | \u2200 X \u2208 A , |X| = n, \u2200 y \u2208X \u2032, \u2203 X \u2032 \u2208 A s.t. |X \u2032| = n, |X \u2229X \u2032| = n\u2212m,E(X \u2032) = y}\nis not empty, we define fE(n) = min(M\u0303). (5)\nThe definition of fE(n) implies, if we change fE(n) elements in X , we can make E become any value in X \u2032: it is onto. In contrast gE(n) only requires E(X \u2032) to become far from E(X), perhaps only in one direction. Then the asymptotic onto-breakdown point is defined as the following limit if it exists\nlim n\u2192\u221e\nfE(n)\nn . (6)\nRemark 2. For a quantile estimator E that returns a percentile other than the 50th, then limn\u2192\u221e gE(n) n 6= limn\u2192\u221e fE(n) n . For instance, if E returns the 25th percentile of a ranked set, setting only 25% of the data points to \u2212\u221e causes E to return \u2212\u221e; hence limn\u2192\u221e gE(n)n = 0.25. And while any value less than the original 25th percentile can also be obtained; to return a value larger than the largest element in the original set, at least 75% of the data must be modified, thus limn\u2192\u221e fE(n) n = 0.75.\nAs we will observe in Section 3, this nuance in definition regarding percentile estimators will allow for some interesting composite estimator design.\n2.2 Definition of E1-E2 Estimators, and their Robustness We consider the following two estimators:\nE1 : A1 \u2282 {X \u2282X1 | 0 < |X| <\u221e} 7\u2192X2, (7) E2 : A2 \u2282 {X \u2282X2 | 0 < |X| <\u221e} 7\u2192X \u20322 , (8)\nwhere any finite subset of E1(A1), the range of E1, belongs to A2. Suppose Pi \u2208 A1, |Pi| = k for i = 1, 2, \u00b7 \u00b7 \u00b7 , n and Pflat = ]ni=1Pi, where ] means if x appears n1 times in X1 and n2 times in X2 then x appears n1 + n2 times in X1 ]X2. We define\nE(Pflat) = E2 (E1(P1), E1(P2), \u00b7 \u00b7 \u00b7 , E1(Pn)) . (9)\nTheorem 1. Suppose gE1(k) and gE2(n) are the finite sample breakdown points of estimators E1 and E2 which are given by (7) and (8) respectively. If gE(nk) is the finite sample breakdown point of E given by (9), then we have gE2(n)gE1(k) \u2264 gE(nk). (10) and if\n\u03b21 = lim k\u2192\u221e\ngE1(k)\nk , \u03b22 = lim n\u2192\u221e\ngE2(n)\nn , \u03b2 = lim n,k\u2192\u221e\ngE(nk)\nnk\nand all exist, then \u03b21\u03b22 \u2264 \u03b2. (11)\nProof. For any fixed G1 > 0, and any subsets P \u20321, P \u2032 2, \u00b7 \u00b7 \u00b7 , P \u2032n \u2208 A1 satisfying |P \u20321| = |P \u20322| = \u00b7 \u00b7 \u00b7 = |P \u2032n| = k, and |{p\u2032 \u2208 P \u2032flat| \u03c1(p\u2032, Pflat) > G1}| \u2264 gE2(n)gE1(k) (12) where P \u2032flat = ]ni=1P \u2032i , we introduce the notation\nX = {E1(P1), E1(P2), \u00b7 \u00b7 \u00b7 , E1(Pn)}, X \u2032 = {E1(P \u20321), E1(P \u20322), \u00b7 \u00b7 \u00b7 , E1(P \u2032n)}.\nSo, in order to prove (10), we only need to bound E(P \u2032flat).\nWe define I1 = {1 \u2264 i \u2264 n| |{p\u2032 \u2208 P \u2032i | \u03c1(p\u2032, Pi) > G1}| > gE1(k)} (13)\nand then have |I1| \u2264 gE2(n). (14)\nOtherwise, since \u03c1(p\u2032, Pi) > G1 implies \u03c1(p\u2032, Pflat) > G1, from |I1| > gE2(n) and (13) we can obtain |{p\u2032 \u2208 P \u2032flat| \u03c1(p\u2032, Pflat) > G1}| > gE2(n)gE1(k) which is contradictory to (12).\nFor any i /\u2208 I1, we have |{p\u2032 \u2208 P \u2032i | \u03c1(p\u2032, Pi) > G1}| \u2264 gE1(k), so, from the definition of gE1(k) we know \u2203 Gi2 = Gi2(Pi, G1), s.t. d2(E1(P \u2032i ), E1(Pi)) \u2264 Gi2 \u2200 i /\u2208 I1. where d2 is the metric of space X2. Let\nG2 = max i/\u2208I1 Gi2 + max 1\u2264i,j\u2264n d2(E1(Pi), E1(Pj))\nthen we have \u03c1(E1(P \u2032 i ), X) \u2264 G2,\u2200 i /\u2208 I1. (15)\nDefining I2 = {1 \u2264 i \u2264 n | \u03c1(E1(P \u2032i ), X) > G2} from (15) we have I2 \u2282 I1, which implies |I2| \u2264 |I1| \u2264 gE2(n) by (14). Therefore, from the definition of gE2(n), we have\n\u2203 G3 = G3(X,G2) s.t. \u2016E(P \u2032flat)\u2212 E(Pflat)\u2016 = \u2016E2(X \u2032)\u2212 E2(X)\u2016 \u2264 G3,\nwhich implies (10), and (11) can be obtained from (10) directly. Thus, the proof is completed.\nRemark 3. Under the condition of Theorem 1, we cannot guarantee \u03b2 = \u03b21\u03b22. For example, suppose E1 andE2 take the 25th percentile and the 75th percentile of a ranked set of real numbers respectively. So, we have \u03b21 = \u03b22 = 14 . However, \u03b2 = 1 4 \u00b7 3 4 = 3 16 .\nIn fact, the limit of gE(nk)nk as n, k \u2192\u221e may even not exist. For example, suppose E1 takes the 25th percentile of a ranked set of real numbers. When n is odd E2 takes the the 25th percentile of a ranked set of n real numbers, and when n is even E2 takes the the 75th percentile of a ranked set of n real numbers. Thus, \u03b21 = \u03b22 = 14 , but gE(nk) \u2248 1 4nk if n is odd, and gE(nk) \u2248 1 4 \u00b7 3 4nk if n is even, which implies limn,k\u2192\u221e gE(nk)\nnk does not exist.\nTherefore, to guarantee \u03b2 exist and \u03b2 = \u03b21\u03b22, we introduce the definition of asymptotic ontobreakdown point in (6). As shown in Remark 2, the values of (4) and (6) may be not equal. However, with the condition of the asymptotic breakdown point and asymptotic onto-breakdown point of E1 being the same, we can finally state our desired clean result.\nTheorem 2. For estimators E1, E2 and E given by (7), (8) and (9) respectively, suppose gE1(k), gE2(n) and gE(nk) are defined by (2), and fE1(k) is defined by (5). Moreover, E1 is an onto function and for any fixed positive integer n we have\n\u2203 X \u2208 A2, |X| = n,G1 > 0, s.t. \u2200 G2 > 0,\u2203 X \u2032 \u2208 A2 satisfying |X \u2032| = n, |X \u2032 \\X| = gE2(n) + 1, and d\u20322(E2(X), E2(X \u2032)) > G2.\n(16)\nwhere d\u20322 is the metric of space X \u2032 2 .\nIf\n\u03b21 = lim k\u2192\u221e\ngE1(k)\nk = lim k\u2192\u221e\nfE1(k)\nk , and \u03b22 = lim n\u2192\u221e\ngE2(n)\nn (17)\nboth exist, then\n\u03b2 = lim n,k\u2192\u221e\ngE(nk)\nnk exists and \u03b2 = \u03b21\u03b22. (18)\nProof. For any fixed positive integer n, we can find X = {x1, x2, \u00b7 \u00b7 \u00b7 , xn} \u2208 A2, and G1 > 0 satisfying (16). Since E1 is an onto function, we can find Pflat = ]ni=1Pi such that Pi \u2208 A1 and E1(Pi) = xi for all 1 \u2264 i \u2264 n. From (16), we know for anyG2 > 0, we can findX \u2032 \u2208 A2 such that |X \u2032| = n, |X \u2032\\X| = gE2(n)+1 and\nd\u2032(E2(X), E2(X \u2032)) > G2.\nThis implies the number of different elements betweenX andX \u2032 is gE2(n)+1. For any x \u2032 i \u2208 X \u2032 \\X , we can find P \u2032i \u2208 A1 such that |P \u2032i | = k, E1(P \u2032i ) = x\u2032i and |P \u2032i \\ Pi| = fE1(k). So, we only need to change fE1(k)(gE2(n) + 1) points of Pflat, and then we can obtain P \u2032 flat such that |P \u2032flat \\ Pflat| = fE1(k)(gE2(n) + 1) and d \u2032(E(Pflat), E(P \u2032 flat)) > G2. This implies\ngE(nk) \u2264 fE1(k)(gE2(n) + 1). (19)\nTherefore, from Theorem 1 and (19) we have\ngE1(k)\nk\ngE2(n) n \u2264 gE(nk) nk \u2264 fE1(k) k (gE2(n) + 1) n . (20)\nLetting n and k go to infinity in (20), we obtain (18) from (17). Thus, the proof of this theorem is completed.\nRemark 4. Without the introduction of fE(n), we cannot even guarantee \u03b2 \u2264 \u03b21 or \u03b2 \u2264 \u03b22 only under the condition of Theorem 1, even if E1 and E2 are both onto functions. For example, for any P = {p1, p2, \u00b7 \u00b7 \u00b7 , pk} \u2282 R and X = {x1, x2, \u00b7 \u00b7 \u00b7 , xn} \u2282 R, we define E1(P ) = 1/median(P ) (if median(P ) 6= 0, otherwise define E1(P ) = 0) and E2(X) = median(y1, y2, \u00b7 \u00b7 \u00b7 , yn), where yi (1 \u2264 y \u2264 n) is given by yi = 1/xi (if xi 6= 0, otherwise define yi = 0). Since gE1(k) = gE2(n) = 0 for all n, k, we have \u03b21 = \u03b22 = 0. However, in order to make E2(E1(P1), E1(P2), \u00b7 \u00b7 \u00b7 , E1(Pn))\u2192 +\u221e, we need to make about n2 elements in {E(P1), E(P2), \u00b7 \u00b7 \u00b7 , E(Pn)} go to 0+. To make E1(Pi)\u2192 0+, we need to make about k2 points in Pi go to +\u221e. Therefore, we have gE(nk) \u2248 n 2 \u00b7 k 2 and \u03b2 = 14 ."}, {"heading": "2.3 Multi-level Composition of Estimators", "text": "To study the breakdown point of composite estimators with more than two levels, we introduce the following estimator:\nE3 : A3 \u2282 {X \u2282X \u20322 | 0 < |X| <\u221e} 7\u2192X \u20323 , (21)\nwhere any finite subset of E2(A2), the range of E2, belongs to A3. Suppose Pi,j \u2208 A1, |Pi,j | = k for i = 1, 2, \u00b7 \u00b7 \u00b7 , n, j = 1, 2, \u00b7 \u00b7 \u00b7 ,m and P jflat = ] n i=1Pi,j , Pflat = ]mj=1P j flat. We define\nE(Pflat) = E3 ( E2(P\u0303 1 flat), E2(P\u0303 2 flat), \u00b7 \u00b7 \u00b7 , E2(P\u0303mflat) ) , (22)\nwhere P\u0303 jflat = {E1(P1,j), E1(P2,j), \u00b7 \u00b7 \u00b7 , E1(Pn,j)}, for j = 1, 2, \u00b7 \u00b7 \u00b7 ,m. From Theorem 2, we can obtain the following theorem about the breakdown point of E in (22). Theorem 3. For estimators E1, E2, E3 and E given by (7), (8), (21) and (22) respectively, suppose gE1(k), gE2(n), gE3(m) and gE(mnk) are defined by (2), and fE1(k), fE2(n) are defined by (5). Moreover, E1 and E2 are both onto functions, and for any fixed positive integer m we have\n\u2203 X \u2208 A3, |X| = m,G1 > 0, s.t. \u2200 G2 > 0,\u2203 X \u2032 \u2208 A3 satisfying |X \u2032| = m, |X \u2032 \\X| = gE3(m) + 1, and d\u20323(E3(X), E3(X \u2032)) > G2.\nwhere d\u20323 is the metric of space X \u2032 3 . If\n\u03b21 = lim k\u2192\u221e\ngE1(k)\nk = lim k\u2192\u221e\nfE1(k)\nk , \u03b22 = lim n\u2192\u221e\ngE2(n)\nn = lim n\u2192\u221e\nfE2(n)\nn , (23)\nand \u03b23 = limm\u2192\u221e gE3 (m)\nm all exist, then\n\u03b2 = lim m,n,k\u2192\u221e\ngE(mnk)\nmnk exist and \u03b2 = \u03b21\u03b22\u03b23. (24)\nProof. We define an estimator E\u0303:\nE\u0303(P\u0303 jflat) = E2(E1(P1,j), E1(P2,j), \u00b7 \u00b7 \u00b7 , E1(Pn,j))\nfor j = 1, 2, \u00b7 \u00b7 \u00b7 ,m, and first prove the breakdown point of E\u0303 is \u03b2\u0303 = \u03b21\u03b22. For any fixed y \u2208 X \u20322 and X = {E1(P1), E1(P2), \u00b7 \u00b7 \u00b7 , E1(Pn)}, we can find X \u2032 \u2208 A2 such that |X \u2032| = n, |X\u2229X \u2032| = n\u2212fE2(n) andE2(X \u2032) = y. For any element y\u2032 \u2208 X \u2032\\(X\u2229X \u2032), we can find E1(Pi) \u2208 X \\ (X \u2229X \u2032) and P \u2032i \u2208 A1 such that |P \u2032i | = k, |Pi \u2229P \u2032i | = k\u2212 gE1(k) and E1(P \u2032i ) = y\u2032. This implies we can find a set P \u2032flat \u2282X1 such that |P \u2032flat| = nk, |Pflat \u2229 P \u2032flat| = nk \u2212 fE2(n)fE1(k) and E\u0303(P \u2032flat) = y, i.e. we only need to change fE2(n)fE1(k) points in Pflat, and E\u0303 can become any value. So, we have\nfE\u0303(nk) \u2264 fE2(n)fE1(k). (25)\nApplying Theorem 1 to E1 and E2, we obtain\ngE2(n)gE1(k) \u2264 gE\u0303(nk). (26)\nSince gE\u0303(nk) < fE\u0303(nk), from (25) and (26), we have\ngE2(n)\nn\ngE1(k) k \u2264 gE\u0303(nk) nk < fE\u0303(nk) nk \u2264 fE2(n) n fE1(k) k . (27)\nLetting n, k go to infinity in (27), from (23) we obtain the breakdown point of E\u0303 is\n\u03b2\u0303 = lim n,k\u2192\u221e\ngE\u0303(nk)\nnk = lim n,k\u2192\u221e\nfE\u0303(nk)\nnk = \u03b21\u03b22.\nSince E(Pflat) = E3(E\u0303(P\u0303 1flat), E\u0303(P\u0303 2 flat), \u00b7 \u00b7 \u00b7 , E\u0303(P\u0303mflat)), we apply Theorem 2 to E\u0303 and E3, and then obtain (24)."}, {"heading": "3 Applications", "text": "We next discuss several applications of our main theorems and observations. Applications 2 and 4 are direct applications of the easy to use theorems. Applications 1 and 3 take advantage of some of the nuances in definition, in particular the unexpected robustness of composing quantile estimators."}, {"heading": "3.1 Application 1 : Balancing Percentiles", "text": "For n companies, for simplicity, assume each company has k employees. We are interested in the income of the regular employees of all companies, not the executives who may have exorbitant pay. Let pi,j represents the income of the jth employee in the ith company. Set Pflat = ]ni=1Pi where the ith company has a set Pi = {pi,1, pi,2, \u00b7 \u00b7 \u00b7 , pi,k} \u2282 R and for notational convenience pi,1 \u2264 pi,2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 pi,k for i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}. Suppose the income data Pi of each company is preprocessed by a 45-percentile estimator E1 (median of lowest 90% of incomes), with breakdown point \u03b21 = 0.45. In theory E1(Pi) can better reflect the income of regular employees in a company, since there may be about 10% of employees in the management of a company and their incomes are usually much higher than that of common employees. So, the preprocessed data is X = {E1(P1), E1(P2), \u00b7 \u00b7 \u00b7 , E1(Pn)}. If we define E2(X) = median(X) and E(Pflat) = E2(X), then the breakdown point of E2 is \u03b22 = 0.5, and the breakdown points of E is \u03b2 = \u03b21\u03b22 = 0.225.\nHowever, if we use another E2, then E can be more robust. For example, for X = {x1, x2, \u00b7 \u00b7 \u00b7 , xn} where x1 \u2264 x2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 xn, we can define E2 as the 55-percentile estimator (median of largest 90% of incomes). In order to make E(Pflat) = E2(X) = E2(E1(P1), E1(P2), \u00b7 \u00b7 \u00b7 , E1(Pn)) go to infinity, we need to either move 55% points of X to \u2212\u221e or move 45% points of X to +\u221e. In either case, we need to move about 0.45 \u00b7 0.55nk points of Pflat to infinity. This means the breakdown point of E is \u03b2 = 0.45 \u00b7 0.55 = 0.2475 which is greater than 0.225. This example implies if we know how the raw data is preprocessed by estimator E1, we can choose a proper estimator E2 to make the E1-E2 estimator more robust."}, {"heading": "3.2 Application 2 : Regression of L1 Medians", "text": "Suppose we want to use linear regression to robustly predict the weight of a person from his or her height, and we have multiple readings of each person\u2019s height and weight. The raw data is Pflat = ]ni=1Pi where for the ith person we have a set Pi = {pi,1, pi,2, \u00b7 \u00b7 \u00b7 , pi,k} \u2282 R2 and pi,j = (xi,j , yi,j) for i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}, j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , k}. Here, xi,j and yi,j are the height and weight respectively of the ith person in their jth measurement.\nOne \u201crobust\u201d way to process this data, is to first pre-process each Pi with its L1-median [1]: (x\u0304i, y\u0304i)\u2190 E1(Pi), where E1(Pi) = L1-median(Pi) has breakdown point \u03b21 = 0.5. Then we could generate a linear model to predict weight y\u0302i = ax+b from the Siegel Estimator [12]: E2(Z) = (a, b), with breakdown point \u03b22 = 0.5. From Theorem 2 we immediately know the breakdown point of E(Pflat) = E2(E1(P1), E1(P2), \u00b7 \u00b7 \u00b7 , E1(Pn)) is \u03b2 = \u03b21\u03b22 = 0.5 \u00b7 0.5 = 0.25. Alternatively, taking the Siegel estimator of Pflat (i.e., returning E2(Pflat)) would have a much larger breakdown point of 0.5. So a seemingly harmless operation of normalizing the data with a robust estimator (with optimal 0.5 breakdown point) drastically decreases the robustness of the process."}, {"heading": "3.3 Application 3 : Significance Thresholds", "text": "Suppose we are studying the distribution of the wingspread of fruit flies. There are n = 500 flies, and the variance of the true wingspread among these flies is on the order of 0.1 units. Our goal is to estimate the 0.05 significance level of this distribution of wingspread among normal flies.\nTo obtain a measured value of the wingspread of the ith fly, denoted Fi, we measure the wingspread of ith fly k = 100 times independently, and obtain the measurement set Pi = {pi,1, pi,2, \u00b7 \u00b7 \u00b7 , pi,k}. The measurement is carried out by a machine automatically and quickly, which implies the variance of each Pi is typically very small, perhaps only 0.0001 units, but there are outliers in Pi with small chance due to possible machine malfunction. This malfunction may be correlated to individual flies because of anatomical issues, or it may have autocorrelation (the machine jams for a series of consecutive measurements).\nTo perform hypothesis testing we desire the 0.05 significance level, so we are interested in the 95th percentile of the set F = {F1, F2, \u00b7 \u00b7 \u00b7 , Fn}. So a post processing estimator E2 returns the 95th percentile of F and has a breakdown point of \u03b22 = 0.05 [6]. Now, we need to design an estimator E1 to process the raw data Pflat = ]ni=1Pi to obtain F = {F1, F2, \u00b7 \u00b7 \u00b7 , Fn}. For example, we can defineE1 as Fi = E1(Pi) = median(Pi) and estimator E as E(Pflat) = E2(E1(P1), E1(P2), \u00b7 \u00b7 \u00b7 , E1(Pn)). Then, the breakdown point ofE1 is 0.5. Since the breakdown point ofE2 is 0.05, the breakdown point of the composite estimator E is \u03b2 = \u03b21\u03b22 = 0.5 \u00b7 0.05 = 0.025. This means if the measurement machine malfunctioned only 2.5% of the time, we could have an anomalous significant level, leading to false discovery. Can we make this process more robust by adjusting E1?\nActually, yes!, we can use another pre-processing estimator to get a more robust E. Since the variance of each Pi is only 0.0001, we can let E1 return the 5th percentile of a ranked set of real numbers, then there is not much difference between E1(Pi) and the median of Pi. (Note: this introduces a small amount of bias that can likely be accounted for in other ways.) In order to make E(Pflat) = E2(F ) go to infinity we need to move 5% points of X to \u2212\u221e (causing E2 to give an anomalous value) or 95% points of X to +\u221e (causing many, 95%, of the E1 values, to give anomalous values). In either case, we need to move about 5% \u00b7 95% points of Pflat to infinity. So, the breakdown points of E is \u03b2 = 0.05 \u00b7 0.95 = 0.0475 which is greater than 0.025. That is, we can now sustain up to 4.75% of the measurement machine\u2019s reading to be anomalous, almost double than before, without leading to an anomalous significance threshold value.\nThis example implies if we know the post-processing estimator E2, we can choose a proper method to preprocess the raw data to make the E1-E2 estimator more robust.\nRemark 5. A further study would be required to use such a composite estimator in practice due some bias it introduces. To replicate the normalization process on new experimental data (e.g., on a new species with hypothesized long wingspread), we would probably need to make one of the following adjustments to the standard process of measuring the wingspread of the new species and directly comparing it to the significance threshold. (a) Also consider the 5th percentile of the experimental measurements (with breakdown point 0.05 instead of 0.5). (b) Adjust the significance level by\nroughly 0.0001 units (the variance over Pi) making it conservative with respect to the 5th percentile versus the 50th percentile decision of each fly\u2019s measurements, so the 50th percentile could be used on the new experimental data. Or, (c) use a different percentile (say the (95 + \u03b5)th percentile instead of 95th) to balance the bias in using the 5th percentile of measurements. In the specific scenario we describe, we believe option (b) may be a very acceptable option with little lack in precision (due to difference in variance 0.1 and 0.0001) but with large gain in robustness."}, {"heading": "3.4 Application 4 : 3-Level Composition", "text": "Suppose we want to use a single value to represent the temperature of the US in a certain day. There are m = 50 states in the country. Suppose each state has n = 100 meteorological stations, and the station i in state j measures the local temperature k = 24 times to get the data Pi,j = {ti,j,1, ti,j,2, \u00b7 \u00b7 \u00b7 , ti,j,k}. We define P jflat = ] n i=1Pi,j , Pflat = ]mj=1P j flat and\nE1(Pi,j) = median(Pi,j), E2(P j flat) = median (E1(P1,j), E1(P1,j), \u00b7 \u00b7 \u00b7 , E1(Pn,j))\nE(Pflat) = E3(E2(P 1 flat), E2(P 2 flat), \u00b7 \u00b7 \u00b7 , E2(Pmflat)) = median(E2(P 1flat), E2(P 2flat), \u00b7 \u00b7 \u00b7 , E2(Pmflat)).\nSo, the break down points of E1, E2 and E3 are \u03b21 = \u03b22 = \u03b23 = 0.5. From Theorem 3, we know the break down point of E is \u03b2 = \u03b21\u03b22\u03b23 = 0.125. Therefore, we know the estimator E is not very robust, and it may be not a good choice to use E(Pflat) to represent the temperature of the US in a certain day.\nThis example illustrates how the more times the raw data is aggregated, the more unreliable the final result can become."}, {"heading": "4 Simulations", "text": "We next describe a few more scenarios where our new theory on estimator composition is relevant. For these we simulate a couple of data sets to demonstrate how one might construct interesting algorithms from these ideas."}, {"heading": "4.1 Simulation 1 : Estimator Manipulation", "text": "In this simulation we actually construct a method to relocate an estimator by modifying the smallest number of points possible. We specifically target the L1-median of L1-medians since its somewhat non-trivial to solve for the new location of data points.\nIn particular, given a target point p0 \u2208 R2 and a set of nk points Pflat = ]ni=1Pi, where Pi = {pi,1, pi,2, \u00b7 \u00b7 \u00b7 , pi,k} \u2282 R2, we use simulation to show that we only need to change n\u0303k\u0303 points of Pflat, then we can get a new set P\u0303flat = ]ni=1P\u0303i such that median(median(P\u03031),median(P\u03032), \u00b7 \u00b7 \u00b7 ,median(P\u0303n)) = p0. Here, the \"median\" means L1-median, and\nn\u0303 = { 1 2n if n is even 1 2 (n+ 1) if n is odd , k\u0303 = { 1 2k if k is even 1 2 (k + 1) if k is odd .\nTo do this, we first show that, given k points S = {(xi, yi) | 1 \u2264 i \u2264 k} in R2, and a target point (x0, y0), we can change k\u0303 points of S to make (x0, y0) as the L1-median of the new set. As n and k grow, then n\u0303k\u0303/(nk) = 0.25 is the asymptotic breakdown point of this estimator, as a consequence of Theorem 2, and thus we may need to move this many points to get the result.\nIf (x0, y0) is the L1-median of the set {(xi, yi) | 1 \u2264 i \u2264 k}, then we have [13]: k\u2211\ni=1\nxi \u2212 x0\u221a (xi \u2212 x0)2 + (yi \u2212 y0)2 = 0, k\u2211 i=1 yi \u2212 y0\u221a (xi \u2212 x0)2 + (yi \u2212 y0)2 = 0. (28)\nWe define ~x = (x1, x2, \u00b7 \u00b7 \u00b7 , xk\u0303), ~y = (y1, y2, \u00b7 \u00b7 \u00b7 , yk\u0303) and\nh(~x, ~y) =\n( k\u2211\ni=1\nxi \u2212 x0\u221a (xi \u2212 x0)2 + (yi \u2212 y0)2\n)2 + ( k\u2211\ni=1\nyi \u2212 y0\u221a (xi \u2212 x0)2 + (yi \u2212 y0)2\n)2 ."}, {"heading": "5 8 3 4 (10.7631, 11.0663) (10.7025 11.0623)", "text": ""}, {"heading": "10 5 5 3 (-13.8252, -4.7462) (-13.8330, -4.7482)", "text": ""}, {"heading": "100 50 50 25 ( -14.0778, 18.3665) ( -14.0773, 18.3658)", "text": "Since (28) is the sufficient and necessary condition for L1-median, if we can find ~x and ~y such that h(~x, ~y) = 0, then (x0, y0) is the L1-median of the new set.\nSince\n\u2202xih(~x, ~y) =2 ( k\u2211\nj=1\nxj \u2212 x0\u221a (xj \u2212 x0)2 + (yj \u2212 y0)2 ) (yi \u2212 y0)2( (xi \u2212 x0)2 + (yi \u2212 y0)2 ) 3 2\n\u2212 2 ( k\u2211\nj=1\nyj \u2212 y0\u221a (xj \u2212 x0)2 + (yj \u2212 y0)2 ) (xi \u2212 x0)(yi \u2212 y0)( (xi \u2212 x0)2 + (yi \u2212 y0)2 ) 3 2 ,\n\u2202yih(~x, ~y) =\u2212 2 ( k\u2211\nj=1\nxj \u2212 x0\u221a (xj \u2212 x0)2 + (yj \u2212 y0)2 ) (xi \u2212 x0)(yi \u2212 y0)( (xi \u2212 x0)2 + (yi \u2212 y0)2 ) 3 2\n+ 2 ( k\u2211\nj=1\nyj \u2212 y0\u221a (xj \u2212 x0)2 + (yj \u2212 y0)2 ) (xi \u2212 x0)2( (xi \u2212 x0)2 + (yi \u2212 y0)2 ) 3 2 ,\nwe can use gradient descent to compute ~x, ~y to minimize h. For the input S = {(xi, yi)|1 \u2264 i \u2264 k}, we choose the initial value ~x0 = {x1, x2, \u00b7 \u00b7 \u00b7 , xk\u0303}, ~y0 = {y1, y2, \u00b7 \u00b7 \u00b7 , yk\u0303}, and then update ~x and ~y along the negative gradient direction of h, until the Euclidean norm of gradient is less than 0.00001.\nThe algorithm framework is then as follows, using the above gradient descent formulation at each step. We first compute the L1-median mi for each Pi, and then change n\u0303 points in {m1,m2, \u00b7 \u00b7 \u00b7 ,mn} to obtain {m\u20321,m\u20322, \u00b7 \u00b7 \u00b7 ,m\u2032n\u0303,mn\u0303+1, \u00b7 \u00b7 \u00b7 ,mn} such that median(m\u20321,m \u2032 2, \u00b7 \u00b7 \u00b7 ,m\u2032n\u0303,mn\u0303+1, \u00b7 \u00b7 \u00b7 ,mn) = p0. For each m\u2032i, we change k\u0303 points in Pi to obtain P\u0303i = {p\u2032i,1, p\u2032i,2, \u00b7 \u00b7 \u00b7 , p\u2032i,k\u0303, pi,k\u0303+1, \u00b7 \u00b7 \u00b7 , pi,k}\nsuch that median(P\u0303i) = m\u2032i. Thus, we have median ( median(P\u03031), \u00b7 \u00b7 \u00b7 ,median(P\u0303n\u0303),median(Pn\u0303+1), \u00b7 \u00b7 \u00b7 ,median(Pn) ) = p0. (29)\nTo show a simulation of this process, we use a uniform distribution to randomly generate nk points in the region [\u221210, 10] \u00d7 [\u221210, 10], and generate a target point p0 = (x0, y0) in the region [\u221220, 20]\u00d7 [\u221220, 20], and then use our algorithm to change n\u0303k\u0303 points in the given set, to make the new set satisfy (29). Table 1 shows the result of running this experiment for different n and k, where (x\u20320, y \u2032 0) is the median of medians for the new set obtained by our algorithm. It lists the various values n and k, the corresponding values n\u0303 and k\u0303 of points modified, and the target point and result of our algorithm. If we reduce the terminating condition, which means increasing the number of iteration, we can obtain a more accurate result, but only requiring the Euclidean norm of gradient to be less than 0.00001, we get very accurate results, within about 0.01 in each coordinate.\nWe illustrate the results of this process graphically for a couple of examples in Table 1; for the cases n = 5, k = 8, (x0, y0) = (0.9961, 1.0126) and n = 5, k = 8, (x0, y0) = (10.7631, 11.0663) These are shown in Figure 1 and Figure 2, respectively. In these two figures, the green star is the target point. Since n = 5, we use five different markers (circle, square, upward-pointing triangle,\ndownward-pointing triangle, and diamond) to represent five kinds of points. The given data Pflat are shown by black points and unfilled points. Our algorithm changes those unfilled points to the blue ones, and the green points are the medians of the new subsets. The red star is the median of medians for Pflat, and other red points are the median of old subsets. So, we only changed 12 points out of 40, and the median of medians for the new data set is very close to the target point."}, {"heading": "4.2 Simulation 2 : Router Monitoring", "text": "Suppose there are n = 100 routers in a network, and each router monitors a stream of length k = 1000. A router can use streaming algorithm to monitor a single percentile, for instance the frugal algorithm here [10] only needs a few bites per percentile maintained \u2013 it does not need to monitor all. We will consider monitoring the approximate median (50% percentile), 10% percentile, and 90% percentile of the stream, and sending these to a single command center. The command center will analyze these data to determine whether an attack occurs. In practice, command centers monitor much larger streams (values of k) and many more routers (values of n).\nWe use standard normal distribution to generate an array Si with 1000 entries to simulate the ith stream, and assume the routers use the estimator E1 to process streams, i.e. E1 returns the approximate 10% percentile, or 90% percentile, or the median of a stream. The command center uses the estimator E2 to process the gathered data S = (E1(S1), E1(S2), \u00b7 \u00b7 \u00b7 , E1(Sn)), and E2 can return the 10% percentile, or 90% percentile, or the median of S. In our simulation, we compute each of these quantities exactly. We use outliers in interval [100, 110] or [\u2212110,\u2212100] to simulate attacks. These values may represent some statistic deemed worth monitoring, say the packet length or header size after it has been appropriately normalized.\nWe choose n1 streams, and put k1 outliers from the same interval (all positive, or all negative) to each chosen stream. Table 2 shows the final output from command center for different combinations of estimators and outliers. The first column in Table 2 shows the proportion of outliers, which is equal to n1k1nk . For example, in the third row of the table, we choose 11 streams randomly and put 110 outliers drawn from [100,110] into each chosen stream, so the proportion of outliers is (11 \u00d7 110)/(100 \u00d7 1000) = 1.21%. When a value being monitored as a composite of various percentiles becomes very large (above 100, so not from the normal distribution) we mark it bold.\nIt is shown in Table 2 that for the case E1 : 10%, E2 : 10% and E1 : 90%, E2 : 90%, we can use 1.21% of outliers to change the output of E1-E2 estimator, since in this situation the breakdown point of E1-E2 estimator is 0.01. For the case E1 : 10%, E2 : 90% and E1 : 10%, E2 : 90%, we can use 10.01% of outliers to change the output of E1-E2 estimator, since in this situation the breakdown point of E1-E2 estimator is 0.09. When E1 and E2 both return the median of a data set, we can use 26.01% of outliers to change the output of E1-E2 estimator, since in this situation the breakdown point of E1-E2 estimator is 0.25.\nThis experiment illustrates how using various composite estimators with different percentiles can highlight various levels of potential distributed denial of service attacks. For instance, if only the E1 : 10%, E2 : 10% estimator is flagged, then we see a few routers have a few anomalous packets, and even though it is distributed to only about 10% of routers and 10% of data, we can observe it; but for the most part would be at most a warning. If E1 : 10%, E2 : 90% estimator or E1 : 50%, E2 : 50% estimator is flagged, it means at least 9% or 25% of the packets across all routers much be anomalous, and we may see a real DDS or an early sign of one. These are all conservative estimates. On the other hand, if at least 10% of the packets are modified on 10% of routers (not too much, perhaps as little as 1%), then the E1 : 10%, E2 : 10% estimator will definitely observe it. And if at least 10% of the packets are modified on 50% of the routers (over 5% of all packets), then an E1 : 10%, E2 : 50% estimator will definitely observe it. Further work is required to discover the best combination of percentiles to monitor, but using our observations about composite estimators suggests this approach which can monitor against various distributions of DDS attacks without only a few simple estimators, requiring a few bites each, at each router."}, {"heading": "5 Discussion", "text": "In this paper, we define the breakdown point of the composition of two or more estimators. These definitions are technical but necessary to understand the robustness of composite estimators; and they do not stray too far from prior formal definitions [4, 3]. Generally, the composition of two of\nmore estimators is less robust than each individual estimator. We highlight a few applications and believe many more exist. These results already provide important insights for complex data analysis pipelines common to large-scale automated data analysis. Moreover, these approaches provides worst case guarantees that are concrete about when outliers can or cannot create a problem, as opposed to some regularization-based approaches that just tend to work on most data.\nNext we will highlight a few more insights from this work, or discuss challenges for follow-on work.\nOn the dangers of composition. The common case of composing two estimators, each with breakdown point of 0.5 yields a composite estimator of 0.25. This means if the result is anomalous, at least 25% of the data must change, down from 50%. In other cases, the resulting composite estimator might yield an even smaller breakdown point of say 0.05. This seems like very bad news! But for large data sets, adversarially changing 5% of data is still a lot. For instance with 1 million data points, then 5% is 50, 000, which would still be an ominously difficult task to modify. So even a 0.05 or 0.01 breakdown point on large data is a useful barrier to manipulation (of the sort in our Simulation 1 below). On the other hand, repeated composition can quickly (exponentially) decrease the breakdown point until it is dangerously low; hence we believe this new theory will play an import role in understanding the robustness and security of long data analysis pipelines.\nRobustness and unbiasedness. In this paper, we focus exclusively on the robustness of estimators, but it is also important to aim for low-MSE or unbiasedness estimators. An interesting future direction is to design estimators that are both robust (including have large onto-breakdown points) as well as other properties. We lead this direction with a few points:\n\u2022 Composing two unbiased estimators will typically be unbiased (some care may be needed in weighting). \u2022 Robustness is a worst-case analysis (protecting against adversarial data) and its claims are often orthogonal to those about low-MSE. \u2022 Our analysis bounds the robustness of composition of any two (or more) estimators. So if other work independently shows low-MSE or low-bias properties, then we can immediately combine these works to show both.\nRemoving all subsets size k constraint. The restriction |Pi| = k (all subsets at the first level are the same size) is mainly for expositional convenience. Otherwise, there are some technical issues with reweighing points in Pflat and defining the limits. In fact, suppose |Pi| = ki for i = 1, 2, \u00b7 \u00b7 \u00b7 , n, Pflat = ]ni=1Pi, gE1(k1) \u2264 gE1(k2) \u2264 \u00b7 \u00b7 \u00b7 \u2264 gE1(kn), and\nE(Pflat) = E2 (E1(P1), E1(P2), \u00b7 \u00b7 \u00b7 , E1(Pn)) .\nThen using the method in the proof of Theorem 1, we can obtain a result similar :\ngE2 (n)\u2211 i=1 gE1(ki) \u2264 gE( n\u2211 i=1 ki) (30)\nwhich is a generalization of (10).\nFinite sampling breakdown point for composite estimators. Theorem 2 provides an asymptotic breakdown point for composite estimators. But for smaller data sets, a finite sample version is also useful and important. Equation (10) already gives a lower bound of the finite sample breakdown point of composite estimators. To get an upper bound on the finite sample vesion, we can modify Theorem 2, by adding a condition fE1(k) = gE1(k) + C where C is a positive constant. Then there is also an annoying off-by-one error on gE2 (see eq (20)), so the result would be something like\ngE1(k)gE2(n) \u2264 gE(nk) \u2264 (gE1(k) + C)(gE2(n) + 1),\nand it is not completely tight. We leave providing a tight bound (up to these constants) as an open question."}], "references": [{"title": "Geometric measures of data depth", "author": ["G. Aloupis"], "venue": "Data Depth: Robust Multivariate Analysis, Computational Geometry and Applications. AMS,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Approximation algorithms for clustering uncertain data", "author": ["G. Cormode", "A. McGregor"], "venue": "PODS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "The breakdown point: Examples and counterexamples", "author": ["P. Davies", "U. Gather"], "venue": "REVSTAT \u2013 Statitical Journal, 5:1\u201317,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "A general qualitative definition mof robustness", "author": ["F.R. Hampel"], "venue": "Annals of Mathematical Statistics, 42:1887\u2013 1896,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1971}, {"title": "Robust Statistics: The Approach Based on Influence Functions", "author": ["F.R. Hampel", "E.M. Ronchetti", "P.J. Rousseeuw", "W.A. Stahel"], "venue": "Wiley,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1986}, {"title": "Breakdown robustness of tests", "author": ["X. He", "D.G. Simplson", "S.L. Portnoy"], "venue": "Journal of the Maerican Statistical Association, 85:446\u2013452,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Robust Statistics", "author": ["P.J. Huber"], "venue": "Wiley,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1981}, {"title": "Breakdown point", "author": ["P.J. Huber", "E.M. Ronchetti"], "venue": "Robust Statistics, page 8. John Wiley & Sons, Inc.,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Geometric computation on indecisive points", "author": ["A.G. J\u00f8rgensen", "M. L\u00f6ffler", "J.M. Phillips"], "venue": "WADS,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Frugal streaming for estimating quantiles: One (or two) memory suffices", "author": ["S.M. Ma", "Qiang", "M. Sandler"], "venue": "arXiv preprint arXiv: 1407.1121,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Representing uncertain data: models, properties, and algorithms", "author": ["A.D. Sarma", "O. Benjelloun", "A. Halevy", "S. Nabar", "J. Widom"], "venue": "VLDBJ, 18:989\u20131019,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust regression using repeated medians", "author": ["A.F. Siegel"], "venue": "Biometrika, 82:242\u2013244,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1982}, {"title": "On the point for which the sum of the distances to n given points is minimum", "author": ["E. Weiszfeld", "F. Plastria"], "venue": "Annals of Operations Research, 167:7\u201341,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "The standard deviation", "author": ["A.H. Welsh"], "venue": "Aspects of Statistical Inference, page 245. Wiley-Interscience;,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 4, "context": "Robust statistical estimators [5, 7] (in particular, resistant estimators), such as the median, are an essential tool in data analysis since they are provably immune to outliers.", "startOffset": 30, "endOffset": 36}, {"referenceID": 6, "context": "Robust statistical estimators [5, 7] (in particular, resistant estimators), such as the median, are an essential tool in data analysis since they are provably immune to outliers.", "startOffset": 30, "endOffset": 36}, {"referenceID": 3, "context": "The breakdown point [4, 3] is a basic measure of robustness of an estimator.", "startOffset": 20, "endOffset": 26}, {"referenceID": 2, "context": "The breakdown point [4, 3] is a basic measure of robustness of an estimator.", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "\u2022 Aloupis [1] write \u201cthe breakdown point is the proportion of data which must be moved to infinity so that the estimator will do the same.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "\u201d \u2022 Huber and Ronchetti [8] write \u201cthe breakdown point is the smallest fraction of bad observations that may cause an estimator to take on arbitrarily large aberrant values.", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "\" \u2022 Dasgupta, Kumar, and Srikumar [14] write \u201cthe breakdown point of an estimator is the largest fraction of the data that can be moved arbitrarily without perturbing the estimator to the boundary of the parameter space.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "For instance, in the last decade there has been increased focus on the study of uncertainty data [11, 9, 2] where instead of analyzing a data set, we are given a model of the uncertainty of each data point.", "startOffset": 97, "endOffset": 107}, {"referenceID": 8, "context": "For instance, in the last decade there has been increased focus on the study of uncertainty data [11, 9, 2] where instead of analyzing a data set, we are given a model of the uncertainty of each data point.", "startOffset": 97, "endOffset": 107}, {"referenceID": 1, "context": "For instance, in the last decade there has been increased focus on the study of uncertainty data [11, 9, 2] where instead of analyzing a data set, we are given a model of the uncertainty of each data point.", "startOffset": 97, "endOffset": 107}, {"referenceID": 3, "context": "One variant of the definition closely aligns with other formalizations [4, 3], while another is fundamentally different.", "startOffset": 71, "endOffset": 77}, {"referenceID": 2, "context": "One variant of the definition closely aligns with other formalizations [4, 3], while another is fundamentally different.", "startOffset": 71, "endOffset": 77}, {"referenceID": 3, "context": "The first definition, Asymptotic Breakdown Point, is similar to the classic formal definitions in [4] and [3] (including their highly technical nature), although their definitions of the estimator are slightly different leading to some minor differences in special cases.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "The first definition, Asymptotic Breakdown Point, is similar to the classic formal definitions in [4] and [3] (including their highly technical nature), although their definitions of the estimator are slightly different leading to some minor differences in special cases.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "For example, the median, L1-median [1], and Siegel estimators [12] all have asymptotic breakdown points of 0.", "startOffset": 35, "endOffset": 38}, {"referenceID": 11, "context": "For example, the median, L1-median [1], and Siegel estimators [12] all have asymptotic breakdown points of 0.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "One \u201crobust\u201d way to process this data, is to first pre-process each Pi with its L1-median [1]: (x\u0304i, \u0233i)\u2190 E1(Pi), where E1(Pi) = L1-median(Pi) has breakdown point \u03b21 = 0.", "startOffset": 90, "endOffset": 93}, {"referenceID": 11, "context": "Then we could generate a linear model to predict weight \u0177i = ax+b from the Siegel Estimator [12]: E2(Z) = (a, b), with breakdown point \u03b22 = 0.", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "05 [6].", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "If (x0, y0) is the L1-median of the set {(xi, yi) | 1 \u2264 i \u2264 k}, then we have [13]:", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "A router can use streaming algorithm to monitor a single percentile, for instance the frugal algorithm here [10] only needs a few bites per percentile maintained \u2013 it does not need to monitor all.", "startOffset": 108, "endOffset": 112}, {"referenceID": 3, "context": "These definitions are technical but necessary to understand the robustness of composite estimators; and they do not stray too far from prior formal definitions [4, 3].", "startOffset": 160, "endOffset": 166}, {"referenceID": 2, "context": "These definitions are technical but necessary to understand the robustness of composite estimators; and they do not stray too far from prior formal definitions [4, 3].", "startOffset": 160, "endOffset": 166}], "year": 2016, "abstractText": "We formalize notions of robustness for composite estimators via the notion of a breakdown point. A composite estimator successively applies two (or more) estimators: on data decomposed into disjoint parts, it applies the first estimator on each part, then the second estimator on the outputs of the first estimator. And so on, if the composition is of more than two estimators. Informally, the breakdown point is the minimum fraction of data points which if significantly modified will also significantly modify the output of the estimator, so it is typically desirable to have a large breakdown point. Our main result shows that, under mild conditions on the individual estimators, the breakdown point of the composite estimator is the product of the breakdown points of the individual estimators. We also demonstrate several scenarios, ranging from regression to statistical testing, where this analysis is easy to apply, useful in understanding worst case robustness, and sheds powerful insights onto the associated data analysis.", "creator": "LaTeX with hyperref package"}}}