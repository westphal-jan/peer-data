{"id": "1609.02236", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2016", "title": "Latent Dependency Forest Models", "abstract": "Probabilistic hatoum modeling panter is one 118.91 of the 62.70 foundations of modern post-standard machine tuch\u00f3w learning mayow and tops-10 artificial intelligence. In this kretchmer paper, rimmer we confiding propose miankova a .253 novel type of probabilistic kisito models counterintuitive named karikoga latent dependency maddon forest bashkirian models (LDFMs ). goidelic A 131.9 LDFM timoleon models the dependencies amaranthaceae between random tenderize variables 53-43 with surfing a cozier forest structure that can calydon change sarzana dynamically based peak-time on abramovi\u0107 the zahawi variable 15-lap values. It katyr is clarithromycin therefore capable of modeling context - minnillo specific independence. We smara parameterize dreadful a sticking LDFM 175.8 using a beechy first - policlinico order u.s.-financed non - 1959-1963 projective dependency grammar. hearsts Learning smithee LDFMs microwaveable from data kathai can be formulated purely as pierre-paul a parameter al-jazari learning beightler problem, and hence stress the prima difficult problem re-appointment of pavlovsky model structure learning is lsu circumvented. Our experimental results parmelia show joest that composer LDFMs are competitive dyesebel with existing mactaquac probabilistic astafei models.", "histories": [["v1", "Thu, 8 Sep 2016 00:57:19 GMT  (276kb,D)", "https://arxiv.org/abs/1609.02236v1", "7 pages, 2 figures, conference"], ["v2", "Sun, 20 Nov 2016 15:51:35 GMT  (436kb,D)", "http://arxiv.org/abs/1609.02236v2", "10 pages, 3 figures, conference"]], "COMMENTS": "7 pages, 2 figures, conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["shanbo chu", "yong jiang", "kewei tu"], "accepted": true, "id": "1609.02236"}, "pdf": {"name": "1609.02236.pdf", "metadata": {"source": "CRF", "title": "Latent Dependency Forest Models", "authors": ["Shanbo Chu", "Yong Jiang", "Kewei Tu"], "emails": ["tukw}@shanghaitech.edu.cn"], "sections": [{"heading": "Introduction", "text": "Probabilistic modeling is one of the foundations of modern machine learning and artificial intelligence, which aims to compactly represent the joint probability distribution of random variables. The most widely used approach for probabilistic modeling is probabilistic graphical models. A probabilistic graphical model represents a probability distribution with a directed or undirected graph. It represents random variables with the nodes in the graph and uses the edges in the graph to encode the probabilistic relationships between random variables. However, traditional probabilistic graphical models have a number of limitations. First, inference takes exponential time in the worst case. Second, learning probabilistic graphical models (in particular, learning the model structures) is very difficult in general. Third, the dependencies between variables are fixed and therefore context-specific independence (CSI) (i.e., independency between variables that only holds given a specific assignment of certain variables) cannot be represented in the basic form of probabilistic graphical models.\nTo alleviate or solve the first two problems of probabilistic graphical models, a number of tractable probabilistic models are proposed. One example is mixtures of trees (MTs) (Meila and Jordan 2001), which represent a probability distribution with a finite mixture of tree distributions. Inference and learning of MTs are both tractable. In addition, certain CSI can be represented in MTs.\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nMore recently, sum-product networks (SPNs) (Poon and Domingos 2011) have been proposed as a type of very expressive tractable probabilistic models that subsume many previous probabilistic models as special cases. A SPN can be represented as a rooted directed acyclic graph with univariate distributions as leaves and sums and products as internal nodes. Exact inference in SPNs can be done in linear time with respect to the network size. However, the large number of latent sum and product nodes in SPNs makes learning (especially structure learning) of SPNs very challenging.\nIt has been shown that SPNs can be seen as an extension of probabilistic context-free grammars (PCFGs) and a special case of stochastic And-Or grammars (Poon and Domingos 2011; Tu 2016b). Based on this observation, we would like to draw a parallel between learning probabilistic models such as SPNs and unsupervised learning of probabilistic grammars. Learning the structure of a probabilistic model resembles learning the set of production rules of a grammar, while learning model parameters resembles learning grammar rule probabilities. From the unsupervised grammar learning literature, one can see that learning approaches based on PCFGs have not been very successful, while the state-of-the-art performance has mostly been achieved based on less expressive models such as dependency grammars (DGs) (Klein and Manning 2004; Headden III, Johnson, and McClosky 2009; Tu and Honavar 2012; Spitkovsky 2013). In comparison with PCFGs, DGs eliminate all the latent nodes (i.e., nonterminals) and therefore the difficult discrete optimization problem of structure learning can be easily converted into the more amenable continuous optimization problem of parameter learning. Inspired by this property of DGs, we propose latent dependency forest models (LDFMs) as a new type of probabilistic models. LDFMs encode the relations between variables with a dependency forest (a set of trees). A distribution over all possible dependency forests given the current assignment of variables is specified using a first-order nonprojective DG (McDonald and Satta 2007). The probability of a complete assignment can then be computed by adding up the weights of all the dependency forests. Figure 1 gives an example of using LDFMs to compute the joint probability of an assignment of two random variables. The number of possible forests grows exponentially with the number of variables, but the summation of the forest weights can still be computed tractably by utilizing the Matrix Tree Theorem ar X iv :1 60 9. 02 23 6v 2 [ cs\n.A I]\n2 0\nN ov\n2 01\n6\n\u00a0 \u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n\u00a0\n\u00a0 \u00a0\nDependency\u00a0Forest Weight\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 X2  T X1  F X2  T X1  F X2  T X2  TX1  F\nX1  F w x 2 T |ro ot w x 2 T |x 1 F\nw x 1 F|x\n2 T\nw1  wx1F|root wx2T |root\nroot\nw2  wx1F|root wx2T |x1F w3  wx1F|x2T wx2T |root\nwx1F|root\np(X1  F, X2  T )w1 w2 w3\nFigure 1: An example of using LDFMs to compute the joint probability of X1 = False and X2 = True. Left: all possible pairwise dependencies between the two variables and a root node (explained in the following section); each dependency has a weight. Right: all three possible dependency forests and their weights. Bottom: computing the joint probability.\n(MTT) (Tutte 1984). Compared with existing probabilistic models, LDFMs have the following features. Similar to SPNs, LDFMs model latent dependencies between random variables, i.e., the dependencies are dynamically determined based on the assignments of the random variables, and therefore LDFMs are capable of modeling CSI (we give an example of modeling CSI with LDFMs in the supplementary material). Unlike SPNs, there is no latent variable in LDFMs, resulting in easier learning. Similar to MTs, LDFMs assume the latent dependencies to form a forest structure; but unlike MTs that restrict the possible dependency structure to a small number of forest structures, LDFMs consider all possible forest structures at the same time. By parameterizing the model using a first-order non-projective DG, unnormalized joint probability computation can still be made tractable. Unlike most of the previous probabilistic models, learning LDFMs from data can be formulated purely as a parameter learning problem without the difficult step of structure learning, and therefore it can be handled by any continuous optimization approach such as the expectation-maximization (EM) algorithm."}, {"heading": "Related Work", "text": ""}, {"heading": "Probabilistic Modeling", "text": "Probabilistic modeling aims to compactly model a joint distribution over a set of random variables. Many different types of probabilistic models have been proposed in the literature. The most widely used probabilistic models are perhaps Bayesian networks (BNs) (Pearl 1988) and Markov networks (MNs) (Kindermann and Snell 1980). A BN models a set of random variables and their conditional dependencies with a directed acyclic graph (DAG). The nodes in the DAG represent the random variables and the edges represent the dependencies. The dependencies are specified between variables regardless of their assignments, so CSI is not representable. The inference of BNs is generally intractable: computing the exact probability of a marginal or conditional query is ]P-complete\n(Roth 1996). In addition, learning the structure of BNs from data is very challenging and finding the global optimum structure is known to be NP-hard (Chickering, Heckerman, and Meek 2004). A MN is similar to a BN in its representation of dependencies, the differences being that BNs are directed and acyclic, whereas MNs are undirected and may be cyclic. In general, exact inference of MNs is also ]P-complete (Roth 1996) and learning of MNs is hard.\nMixtures of trees (MTs) (Meila and Jordan 2001) represent joint probability distributions as finite mixtures of tree distributions. One can represent certain CSI in MTs by using different tree distributions to model different contexts. Like most mixture models, the EM algorithm can be used for the learning of MTs. The inference of MTs takes linear time in n (the number of variables) and each step of the EM learning algorithm takes quadratic time in n. The number of mixture components in a MT is usually set to a small number in practice. LDFMs can also be seen as a mixture of tree distributions, but unlike MTs, LDFMs consider all possible structures of tree distributions and resort to first-order nonprojective DGs to encode the mixture weight of each tree distribution.\nA sum-product network (SPN) (Poon and Domingos 2011) is a tractable deep probabilistic model represented as a rooted DAG with univariate distributions as leaves and sums and products as internal nodes. A sum node computes the weighted sum of its child nodes and a product node computes the product of its child nodes. The root of a SPN can represent different types of probability distributions according to different inputs. For example, when all the leaves are set to 1, the root represents the partition function; when the input is evidence, the root represents the unnormalized marginal probability of the evidence; and when the input is a complete assignment, the root represents the unnormalized joint probability. SPNs can also represent CSI by using different sub-SPNs to model distributions of variables under different contexts. Calculating the root value of a SPN is a bottom-up process with time complexity linear in the network size, so inference is tractable in terms of the network size. Structure learning of SPNs is still a challenging problem involving difficult discrete optimization and recently a variety of approaches have been proposed (Gens and Pedro 2013; Rooshenas and Lowd 2014; Adel, Balduzzi, and Ghodsi 2015). More recently, some subclasses of SPNs have been proposed (Rahman, Kothalkar, and Gogate 2014; Peharz, Gens, and Domingos 2014).\nFor most of the above models, learning involves identification of a good model structure, which is typically a difficult discrete optimization problem. Borrowing ideas from the unsupervised grammar learning literature, we formulate a LDFM as a first-order non-projective dependency \u201cgrammar\u201d on variable assignments and circumvent the structure learning step by including all possible \u201cgrammar rules\u201d in the model and then learning their parameters."}, {"heading": "Dependency Grammars", "text": "In natural language processing (NLP), dependency grammars (DGs) are a simple flexible mechanism for encoding words and their syntactic dependencies through directed graphs. In the directed graph derived from a sentence, each word is a\nnode and the dependency relationships between words are the edges. In a non-projective dependency graph, an edge can cross with other edges. Non-projectivity arises due to long distance dependencies or in languages with flexible order. In most cases, the dependency structure is assumed to be a directed tree. Figure 2 is an example non-projective dependency tree for the sentence A hearing is scheduled on the issue today. Each edge connects a word to its modifier and is labeled with the specific syntactic function of the dependency, e.g., SBJ for subject. Dependency parsing (finding the dependency trees) is an important task in NLP and McDonald et al. (McDonald et al. 2005) proposed an efficient parsing algorithm for first-order non-projective DGs by searching for maximum spanning trees (MSTs) in directed graphs. McDonald and Satta (McDonald and Satta 2007) appealed to Matrix Tree Theorem to make unsupervised learning feasible for first-order non-projective DGs via the EM algorithm.\nLDFMs can be seen as an extension of first-order nonprojective DGs for probabilistic modeling. There are two main differences between non-projective DGs and LDFMs. One difference is that LDFMs use the nodes in a dependency tree to represent the assignments of the variables, while nonprojective DGs use the nodes to represent the words in a sentence. The other difference is that when viewed as generative models, a DG always produces a valid sentence while a LDFM may produce invalid assignments of variables if there are conflicting or missing assignments."}, {"heading": "Latent Dependency Forest Models", "text": ""}, {"heading": "Basics", "text": "Let X = (X1, X2, . . . , Xn) be a set of random variables and x = (x1, x2, . . . , xn) be an assignment to the set of random variables. Given an assignment x, we construct a complete directed graph Gx = (Vx, Ex) such that, \u2022 Vx = {x0 = root, x1, . . . , xn} where x0 is a dummy root\nnode. \u2022 Ex = {(xi, xj)|i 6= j, 0 \u2264 i \u2264 n, 1 \u2264 j \u2264 n} Gx contains all possible pairwise dependencies between the variables under the current assignments x. We assume that the actual dependency relations between the variables always form a forest structure (a set of trees). By adding an edge from the dummy root node to the root of each tree of the dependency forest, we obtain a single dependency tree structure that is a directed spanning tree of the graph Gx rooted at x0. We denote this tree by T = (VT , ET ), where VT = Vx, ET \u2286 Ex.\nWe assume that the strength of each pairwise dependency is independent of any other dependencies and denote the dependency strength from node xi to node xj by edge weight wij . We can compute the strength or weight of a spanning tree T = (VT , ET ) as the product of the edge weights:\nw(T ) = \u220f\n(xi,xj)\u2208ET\nwij\nThe partition function is the sum over the weights of all possible dependency trees for a given assignment x, which represents the weight of the assignment. We denote this value as Zx.\nZx = \u2211\nT\u2208T (Gx)\nw(T ) = \u2211\nT\u2208T (Gx) \u220f (xi,xj)\u2208ET wij\nT (Gx) is the set of all possible dependency trees. The size of T (Gx) is exponential in n, but we can use Matrix Tree Theorem (Tutte 1984) to compute the partition function tractably. Matrix Tree Theorem (MTT). LetG be a graph with nodes V = {x0, x1, . . . , xn} and edges E. Define (Laplacian) matrix Q as a (n+1)\u00d7 (n+1) matrix indexed from 0 to n. For all i and j, define:\nQij =  \u2211 i\u2032 6=j,(xi\u2032 ,xj)\u2208E wi\u2032j if i = j\n\u2212wij if i 6= j, (xi, xj) \u2208 E\nIf the i-th row and column are removed from Q to produce the matrix Qi, then the sum of the weights of all the directed spanning trees rooted at node i is equal to the determinant of Qi.\nThus, to compute Zx, we construct matrix Q from graph Gx and compute the determinant of matrixQ0. The time complexity isO(n3) if we use LU decomposition for determinant calculation.\nBased on the framework introduced above, now we present two generative probabilistic models: LDFM and LDFM-S."}, {"heading": "LDFM", "text": "LDFM requires that the weight of each dependency (xi, xj) is the conditional probability of generating the variable Xj and setting its value to xj given the assignment Xi = xi or the root node. We denote this probability by wxj |xi . We impose the constraint 0 \u2264 wxj |xi \u2264 1 and the normalization condition \u2211 j 6=i \u2211 xj wxj |xi = 1 for each xi (or\u2211\nj 6=i \u222b xj wxj |xi = 1 for continuous variables).\nAn assignment x = (x1, x2, . . . , xn) is generated recursively in a top-down manner. First, we generate a dependency tree with n+ 1 nodes uniformly at random. We label the root node as x0. Then, starting from the root node, we recursively traverse the tree in pre-order; at each non-root node, we generate a \u3008variable, value\u3009 pair conditioned on the \u3008variable, value\u3009 pair of its parent node. The probability of generating an assignment x is:\np(x) = \u03b2n!Zx \u221d Zx\nwhere \u03b2 is the uniform probability of the tree structure, and \u03b2n! is a constant w.r.t. x. The derivation details can be found in the supplementary material.\nNote that, however, the above process may also generate invalid assignments. First, it allows multiple assignments of the same variable to be generated at different nodes, resulting in duplicate or conflicting assignments. Second, since there are only n non-root nodes, if some variables are assigned at multiple nodes, then there must be some other variables that are not assigned at all. One could modify the generation process to disallow invalid assignments, but that would break the assumed independence between the strength of pairwise dependencies, leading to intractable computation of the partition function Zx.\nSince we are only interested in the space of valid assignments (i.e., no duplicate or missing variable assignment), we define the joint probability of a valid assignment x as\n\u03c6(x) = p(x)\u2211\nx\u2208A p(x) = Zx \u03b3\nwhere A is the set of valid assignments and \u03b3 is the normalization factor. The joint probability is proportional to the partition function Zx, so we can use MTT to compute the unnormalized joint probability. However, the normalized joint probability is intractable to compute because computing the normalization factor \u03b3 is ]P-hard, which is similar to the case of Markov networks."}, {"heading": "LDFM-S", "text": "The assumption of uniformly distributed tree structures in LDFM can be unreasonable in many cases. In LDFM-S, instead of first uniformly sampling a tree structure and then instantiating the tree nodes, we generate the tree structure and the \u3008variable, value\u3009 pairs at the tree nodes simultaneously. Starting from the root node, we generate a tree structure in a top-down recursive manner: at each node xi, we keep sampling from the conditional distribution wxj |xi to generate new child nodes until a dummy stop node is generated; then for each child of xi, we recursively repeat this procedure. We denote the probability of generating a stop node given the assignment Xi = xi by ws|xi and require the normalization condition ws|xi + \u2211 j 6=i \u2211 xj wxj |xi = 1 for each xi (or\nws|xi + \u2211 j 6=i \u222b xj wxj |xi = 1 for continuous variables). It is easy to see that if ws|xi is larger, then xi is more likely to be a leaf node in the tree structure. The probability of generating an assignment x is:\np(x) = \u2211\nT\u2208T (Gx) \u220f (xi,xj)\u2208ET wxj |xi \u220f xi\u2208VT ws|xi = Zx \u220f xi\u2208Vx ws|xi\nThe joint probability of a valid assignment x is:\n\u03c6(x) = p(x)\u2211\nx\u2208A p(x) = Zx \u220f xi\u2208Vx ws|xi \u03b3\nwhere A is the set of valid assignments and \u03b3 is the normalization factor. Again, computing the unnormalized joint probability is tractable by using MTT, but computing the normalization factor is ]P-hard. Note that when all the stop weights ws|xi are equal, LDFM-S reduces to LDFM.\nInference\nIn probabilistic inference, the set of random variables are divided into query, evidence, and hidden variables and we want to compute the conditional probabilities of the query variables given the evidence variables. Inference of LDFMs in general can be shown to be ]P-hard, so we resort to Markov chain Monte Carlo (MCMC) for approximate inference.\nOne way to do MCMC is by Gibbs sampling, which resamples each of the query and hidden variables in turn according to its conditional distribution given the rest of the variables. After getting enough samples, we can compute the probability of a particular query as the fraction of the samples that match the query. At each resampling step, the conditional distribution is computed from the unnormalized joint probabilities, which takes O(n3) time and thus can be slow when n is large.\nFor more efficient sampling, we apply the idea of data augmentation (Tanner and Wong 1987) and simultaneously sample the latent dependency tree structure and the variable values. We name this method tree-augmented sampling. We first randomly initialize the values of the query and hidden variables as well as the dependency tree structure. At each MCMC step, we randomly pick a variable and simultaneously change its value and its parent node in the dependency tree (with the constraint that no loop is formed). Suppose variable Xi is picked, then the proposal probability of value xi and parent Xj is proportional to the dependency weight wxi|xj where xj is the value of Xj in the previous sample. It can be shown that the acceptance rate of the proposal is always one. The time complexity of each sampling step is O(n), which can be further reduced if the dependencies between variables are sparse. After getting enough samples, we estimate the query probability based on the statistics of the sampled variable values while disregarding the sampled dependency trees.\nLearning\nWe want to learn a LFDM form data where the dependency structure of each training instance is unknown. We avoid the difficult structure learning problem by assuming a complete LDFM structure, i.e., we assume that all the dependencies between \u3008variable, value\u3009 pairs are possible (having nonzero weights), rather than trying to identify a subset of possible dependencies. We then rely on parameter learning to specify the weights of all the dependencies. This strategy is quite different from that of learning other types of probabilistic models, as structure learning is unlikely to circumvent for most of them. For BNs, if we assume a complete structure (i.e., the skeleton being a complete graph), then the model size (in particular, sizes of conditional probability tables) becomes exponential in the number of random variables and learning becomes intractable. For SPNs, there is no general principle of constructing a \u201ccomplete\u201d structure; if we assume that there is at least one node for each possible scope (subset of random variables), then the model size is also exponential.\nOur learning objective function is the log-likelihood of the\nmodel. |D|\u2211 \u03b1=1 log p(x\u03b1) = |D|\u2211 \u03b1=1 logZx\u03b1 + constant\nwhere D = {x\u03b1}|D|\u03b1=1 is the training dataset. Note that we compute the likelihood based on p(x), the probability of generating an assignment, instead of \u03c6(x), the probability of a valid assignment. This makes our learning algorithm tractable and also encourages the learned model to be more likely to produce valid assignments.\nSince the dependency structure of each training sample is hidden, we can use the expectation-maximization (EM) algorithm for learning LDFM parameters. We uniformly initialize all the dependency weights, which we find to be the most effective initialization method.\nIn the E-step, we compute the partition functions and edge expectations of the training samples. For a training sample x, the edge expectation of (xi, xj) is defined as follows:\n\u3008(xi, xj)\u3009x = \u2211\nT\u2208T (Gx)\nw(T )\u00d7 I((xi, xj), T )\nwhere I((xi, xj), T ) is an indicator function which is one when (xi, xj) is in the tree T and zero otherwise. Following the work of McDonald and Satta (McDonald and Satta 2007), we can compute the edge expectations through matrix inversion. When i, j > 0,\n\u3008(xi, xj)\u3009x = wxj |xiZx[((Q 0)\u22121)jj \u2212 ((Q0)\u22121)ji]\nWhen i = 0 and j > 0,\n\u3008(x0, xj)\u3009x = wxj |x0Zx((Q 0)\u22121)jj\nIn the M-step, we update the parameters wxj |xi to maximize the log-likelihood subject to the constraints of\u2211 j 6=i \u2211 xj wxj |xi = 1 (for discrete variables) and wxj |xi \u2265 0. By solving the above constrained optimization problem with the Lagrange multiplier method, we can get:\nwxj |xi =\n\u2211|D| \u03b1=1\n1 Zx\u03b1 \u3008(xi, xj)\u3009x\u03b1\u2211|D|\n\u03b1=1 1\nZx\u03b1\n\u2211 j\u2032 6=i \u2211 xj\u2032 \u3008(xi, xj\u2032)\u3009x\u03b1\nFor continuous variables, we can assume a certain form of conditional distributions (e.g., a bivariate conditional normal distribution) and derive its optimal parameters in terms of the partition functions and edge expectations in a similar manner.\nIn addition to maximum likelihood estimaton, we may also run maximum a posteriori (MAP) estimation using EM with a prior over the parameters. We find that the modified Dirichlet prior (Tu 2016a), which is a strong sparsity prior, can sometimes significantly improve the learning results.\nExperiments We empirically evaluated the learning and inference of LDFMs on nine datasets and compared the performance against several popular probabilistic models including Bayesian networks (BNs), dependency networks (DNs), mixtures of trees (MTs), and sum-product networks (SPNs).\nTo produce our training and test data, we picked nine BNs that are frequently used in the BN learning literature from bnlearn1, a popular BN repository. For each BN, we sampled two training sets of 5000 and 500 instances, one validation set of 1000 instances, and one testing set of 1000 instances. All the random variables are discrete. Table 1 shows the statistics of each BN. It can be seen that many of the ground truth BNs are much more complicated than a tree structure and some have relatively high tree-width. Note that the way we produced our data actually gives BNs an advantage in our evaluation because the data never goes beyond the representational power of BNs. Nevertheless, as we will see, LDFMs and a few other models still outperform BNs on these datasets.\nWe learned the five types of models from the training data and evaluated them by their accuracy in query answering on the test data. For each test data sample, we randomly divided the variables into three subsets: query variables Q, evidence variables E, and hidden variables H . We then ran the learned model to compute the conditional probability p(Q = q|E = e), where q and e are the values that Q and E take in the test data sample. A model that is closer to the ground truth would produce a higher conditional probability on average. Four different proportions of dividing the query, evidence, and hidden variables (e.g., 30% query variables, 10% evidence variables, and 60% hidden variables) were used and for each proportion, a thousand query instances were generated from the test set. Following the evaluation metrics of the previous work (Rooshenas and Lowd 2014), we report the maximum of the conditional log-likelihood (CLL) and the conditional marginal log-likelihood (CMLL):\u2211\nXi\u2208Q log p(Xi = xi|E = e). We normalize CLL or\n1http://www.bnlearn.com/bnrepository/\nCMLL by the number of query variables to facilitate comparison across different query proportions.\nWe trained LDFM using EM and modified Dirichlet prior on each dataset and used the two MCMC approaches introduced in the Inference section to estimate the query probabilities. For the other four probabilistic models, we used the Libra toolkit (Lowd and Rooshenas 2015) to train them and tuned the hyperparameters of the training algorithms according to the query probabilities on the validation set. Specifically, Libra learns BNs with decision-tree CPDs (Chickering, Heckerman, and Meek 1997) which is an extension of plain BNs that is capable of modeling CSI; it learns DNs using an algorithm similar to (Heckerman et al. 2001); it uses the algorithm proposed in (Meila and Jordan 2001) to learn MTs; and it learns SPNs using direct and indirect variable interactions (Rooshenas and Lowd 2014). Note that in Libra, the input data to the MTs and SPNs training algorithms needs to be binary, so we binarized all the variables when training MTs and SPNs. After training the four probabilistic models, we again used Libra to do inference. For BNs and DNs, we used the Gibbs sampling algorithm implemented in Libra. For MTs and SPNs, Libra first converted the trained models to an equivalent arithmetic circuit (AC) and then used an exact AC inference method to do inference.\nWe report the evaluation results of two proportions in Table 2, and report the results of the other two proportions in the supplementary material. The evaluation results of LDFM-S are similar to the results of LDFM, and we report them in the supplementary material. It can be seen that LDFMs are competitive with the other probabilistic models and achieve the best results on most datasets. Comparing the performance\nof the two MCMC approaches of LDFMs, we see that Gibbs sampling achieves better overall results than tree-augmented sampling; however, in our experiments tree-augmented sampling is more than twenty times faster than Gibbs sampling on average. SPNs have very good performance on the larger training set , which verifies their effectiveness in probabilistic modeling compared with traditional approaches; however, their performance is not as good on the smaller training set suggesting that SPNs may require more data to learn than LDFMs. BNs perform worst on average even though the data was generated from ground truth BNs, which suggests that structure learning of BNs is still a very challenging problem.\nConclusion In this paper, we propose latent dependency forest models (LDFMs), a novel probabilistic model. A LDFM models the dependencies between random variables with a forest structure that can change dynamically based on the variable values. We define a LDFM as a generative model parameterized by a first-order non-projective DG. We propose two MCMC approaches, Gibbs sampling and tree-augmented sampling, for inference of LDFMs. Learning LDFMs from data can be formulated purely as a parameter learning problem, and hence the difficult problem of model structure learning is circumvented. We derive an EM algorithm for learning LDFMs. Our experimental results show that LDFMs are competitive with existing probabilistic models.\nAcknowledgments This work was supported by the National Natural Science Foundation of China (61503248)."}, {"heading": "Supplementary Material", "text": ""}, {"heading": "An Example of Using LDFMs to Model CSI", "text": "The assignments of the variables can influence the distribution over the dependency structures. In this way, LDFMs can model CSI to some extent. Here is an example of using LDFM-S to model three binary variables X1, X2 and X3.\nFigure 3 gives an example of using LDFM-S to model CSI. The conditional probabilities of the two variables X2 and X3 given X1 can be computed using the formula in the LDFM-S subsection and they are shown in Table 3. It can be seen that when X1 = T , X2 and X3 are strongly dependent; when X1 = F , they are only weakly dependent."}, {"heading": "The Derivation Details", "text": "We show the details of deriving the probability of generating an assignment x discussed in the LDFM subsection in the main text.\nwhere T\u0302 is the uniformly generated tree structure and M is a mapping from the n variables to the n nodes of the tree structure T\u0302 , \u03b2 is the constant value of p(T\u0302 ). \u03b2n! is a constant w.r.t. x. Here we have n! because for each spanning tree T of Gx, each permutation of the n variables is generated differently (i.e., corresponds to a different \u3008T,M\u3009 pair)."}, {"heading": "The Evaluation Results of LDFM-S", "text": "We report the results of LDFM-S and LDFM trained on the 5000-sample datasets and evaluated by using Gibbs sampling on two different proportions of dividing the query and evidence variables in Table 4. It can be seen that LDFM-S has similar performance to LDFM on most datasets, but achieves significantly better results on the Win95pts dataset and significantly worse results on the Water dataset. Therefore, it may depend on the dataset as to whether modeling distributions over tree structures is useful."}, {"heading": "More Evaluation Results", "text": "In the Experiments section in the main text, we report the evaluation results of two proportions of dividing the query and evidence variables (40% query, 30% evidence and 30% query, 20% evidence). In Table 5 we report the evaluation results of the other two proportions (30% query, 40% evidence and 20% query, 30% evidence).\nReferences [Adel, Balduzzi, and Ghodsi 2015] Adel, T.; Balduzzi, D.;\nand Ghodsi, A. 2015. Learning the structure of sum-product networks via an svd-based algorithm. In 31st Conference on Uncertainty in Artificial Intelligence (UAI).\nPh.D. Dissertation, Computer Science Department, Stanford University, Stanford, CA.\n[Tanner and Wong 1987] Tanner, M. A., and Wong, W. H. 1987. The calculation of posterior distributions by data augmentation. Journal of the American statistical Association 82(398):528\u2013540.\n[Tu and Honavar 2012] Tu, K., and Honavar, V. 2012. Unambiguity regularization for unsupervised learning of probabilistic grammars. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 1324\u20131334.\n[Tu 2016a] Tu, K. 2016a. Modified dirichlet distribution: Allowing negative parameters to induce stronger sparsity. In Conference on Empirical Methods in Natural Language Processing (EMNLP 2016), Austin, Texas, USA.\n[Tu 2016b] Tu, K. 2016b. Stochastic And-Or grammars: A unified framework and logic perspective. In IJCAI.\n[Tutte 1984] Tutte, W. T., ed. 1984. Graph Theory. Cambridge University Press."}], "references": [{"title": "Learning the structure of sum-product networks via an svd-based algorithm", "author": ["Balduzzi Adel", "T. Ghodsi 2015] Adel", "D. Balduzzi", "A. Ghodsi"], "venue": "In 31st Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Adel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Adel et al\\.", "year": 2015}, {"title": "and Pedro", "author": ["R. Gens"], "venue": "D.", "citeRegEx": "Gens and Pedro 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Satta", "author": ["R. McDonald"], "venue": "G.", "citeRegEx": "McDonald and Satta 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["McDonald"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "McDonald,? \\Q2005\\E", "shortCiteRegEx": "McDonald", "year": 2005}, {"title": "M", "author": ["M. Meila", "Jordan"], "venue": "I.", "citeRegEx": "Meila and Jordan 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning selective sum-product networks", "author": ["Gens Peharz", "R. Domingos 2014] Peharz", "R. Gens", "P. Domingos"], "venue": "In LTPM workshop", "citeRegEx": "Peharz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2014}, {"title": "and Domingos", "author": ["H. Poon"], "venue": "P.", "citeRegEx": "Poon and Domingos 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Cutset networks: A simple, tractable, and scalable approach for improving the accuracy of chow-liu trees", "author": ["Kothalkar Rahman", "T. Gogate 2014] Rahman", "P. Kothalkar", "V. Gogate"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Rahman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rahman et al\\.", "year": 2014}, {"title": "and Lowd", "author": ["A. Rooshenas"], "venue": "D.", "citeRegEx": "Rooshenas and Lowd 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "V", "author": ["Spitkovsky"], "venue": "I.", "citeRegEx": "Spitkovsky 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "W", "author": ["M.A. Tanner", "Wong"], "venue": "H.", "citeRegEx": "Tanner and Wong 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "and Honavar", "author": ["K. Tu"], "venue": "V.", "citeRegEx": "Tu and Honavar 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Modified dirichlet distribution: Allowing negative parameters to induce stronger sparsity", "author": ["K. Tu 2016a] Tu"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Tu,? \\Q2016\\E", "shortCiteRegEx": "Tu", "year": 2016}, {"title": "Stochastic And-Or grammars: A unified framework and logic perspective", "author": ["K. Tu 2016b] Tu"], "venue": null, "citeRegEx": "Tu,? \\Q2016\\E", "shortCiteRegEx": "Tu", "year": 2016}, {"title": "W", "author": ["Tutte"], "venue": "T., ed.", "citeRegEx": "Tutte 1984", "shortCiteRegEx": null, "year": 1984}], "referenceMentions": [], "year": 2016, "abstractText": "Probabilistic modeling is one of the foundations of modern machine learning and artificial intelligence. In this paper, we propose a novel type of probabilistic models named latent dependency forest models (LDFMs). A LDFM models the dependencies between random variables with a forest structure that can change dynamically based on the variable values. It is therefore capable of modeling context-specific independence. We parameterize a LDFM using a first-order non-projective dependency grammar. Learning LDFMs from data can be formulated purely as a parameter learning problem, and hence the difficult problem of model structure learning is circumvented. Our experimental results show that LDFMs are competitive with existing probabilistic models.", "creator": "LaTeX with hyperref package"}}}