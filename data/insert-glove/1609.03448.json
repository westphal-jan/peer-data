{"id": "1609.03448", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Learning Sparse Graphs Under Smoothness Prior", "abstract": "1041 In this paper, we weisselberg are godana interested ptn in azf learning the underlying graph rumoured structure behind training breisach data. Solving this basic pr\u00e9ludes problem is deslys essential flowerpots to carry out any kazatchkine graph signal processing or barnby machine learning task. svanholm To outfoxed realize this, kempthorne we francisca assume midem that the apsrtc data euro165 is elder-beerman smooth consumerism with racegoers respect wnu to the rangoni graph melecio topology, bial and nephridia we 194.3 parameterize the hardware-based graph topology 80.96 using 96.9 an edge sampling astaroth function. 34.76 That ataqatigiit is, the graph Laplacian bocek is chamaeleon expressed in terms madson of reanalyzed a hygrometer sparse edge selection trunnions vector, which provides multifactorial an lardil explicit zeine handle single-legged to control deniro the melville sparsity rumbia level of peregrina the graph. mdl We solve the twyford sparse graph learning problem bioshock given some training necropsies data anti-western in scarisbrick both 25-degree the 134.5 noiseless and noisy settings. Given fostoria the zohrab true smooth data, the linebaugh posed quatrefoil sparse graph learning assess problem disperser can be chiltan solved optimally and mandrel is based chafes on fawning simple beecroft rank ruy ordering. Given the noisy data, figure-of-eight we show that karlsberg the sakazaki joint kdom sparse graph russulales learning and itx denoising studii problem semmel can be simplified laflesche to mirman designing palaeontologists only the 1890-1891 sparse intersectionality edge hitlerian selection endopeptidase vector, 28.82 which airbender can stardock be solved fuckin using convex optimization.", "histories": [["v1", "Mon, 12 Sep 2016 15:31:20 GMT  (852kb)", "http://arxiv.org/abs/1609.03448v1", "ICASSP 2017 conference paper"]], "COMMENTS": "ICASSP 2017 conference paper", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sundeep prabhakar chepuri", "sijia liu", "geert leus", "alfred o hero iii"], "accepted": false, "id": "1609.03448"}, "pdf": {"name": "1609.03448.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sundeep Prabhakar Chepuri", "Sijia Liu", "Geert Leus"], "emails": ["g.j.t.leus}@tudelft.nl,", "hero}@umich.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n03 44\n8v 1\n[ cs\n.L G\n] 1\n2 Se\np 20\nIndex Terms\u2014 Graph Learning, graph signal processing, graph sparsification, topology inference, sparse sampling."}, {"heading": "1. INTRODUCTION", "text": "Graphs offer a way to describe and explain relationships in complex datasets, a central entity of modern data analysis, where data deluge is prominent [1\u20133]. In particular, the nodes of the graph denote the entities and the edges encode the pairwise relationship between these entities. Such entities are referred to as graph signals. Examples of such complex-structured data beyond traditional time-series include data residing on brain networks, gene networks, social networks, recommendation systems, transportation networks, and so on.\nHaving a good quality graph is central to any graph signal processing or machine learning task. In this paper, we are interested in the problem of learning the hidden graph topology behind the data. Due to the sheer quantity of data, we are motivated to select the simplest graphical models that adequately explain the data. In particular, we are interested in learning a sparse graph, i.e., a graph with a limited number of edges that adequately explains the input (or training) data. To realize this, we make a simple, but widely used assumption [1,4] that the data is smooth with respect to the discovered graph.\nThe contributions in this paper are threefold. First, we model the graph learning problem as an edge selection problem, where we parameterize the graph through a sparse edge sampling vector. In particular, the proposed model provides an elegant handle to control the number of edges, thus the graph sparsity. Second, for the case\nThis work is supported in part by the KAUST-MIT-TUD consortium under grant OSR-2015-Sensors-2700 and the US Army Research Office under grant W911NF-15-1-0479.\nwhen the true smooth graph signals are given, the graph learning problem can be solved optimally, and the solution is based on simple rank ordering. Finally, given the noisy graph signals, i.e., for the joint sparse graph learning and denoising problem, we provide a onestep solution based on convex optimization as well as an algorithm based on alternating minimization.\nThe problem of learning the graph Laplacian or the weighted adjacency matrix from smooth graph signals has been considered before [4,5]. Learning sparse graphs from the true graph signals, which is the problem we consider in Section 3, has been studied in [5]. There the graph learning problem is posed as a constrained optimization problem with the constraint set being the set of valid adjacency matrices, and the optimization problem is solved using an iterative primal-dual algorithm. In contrast, our modelling greatly simplifies the solution to simple rank ordering. Such a modelling is inspired from [6], where the problem to design edge weights that maximize the algebraic connectivity of the graph has been addressed. In [4], the joint graph learning and denoising problem has been addressed, i.e., the problem that we study in Section 4. An alternating minimization algorithm is proposed, alternating between graph learning and denoising, where the graph learning optimization problem involves a search over the space of all valid graph Laplacians. On the contrary, we show that this problem can be solved in one-step and it boils down to the design of a sparse edge sampling function.\nGraph topology identification is also investigated in [7] under the assumption that the eigenvectors of the graph Laplacian are known, which is a much stronger assumption. Although the eigenvectors can be computed from graph data (or the sample covariance matrix) when it is stationary with respect to the graph [8, 9], the graph signals need not always be vertex stationary. In any case, the estimated eigenvectors are not error free due to limited data records. In most of the existing approaches [4,5,7], graph sparsification is (or can be) achieved by penalizing the \u21131-norm of the graph Laplacian matrix, adjacency matrix or the shift operator, however, there is no explicit handle to control the number of edges, unlike the proposed approach. In a related line of research, [10, 11] investigate computing sparse graphs that approximate a given graph spectrally, which means that their Laplacian matrices have similar quadratic forms."}, {"heading": "2. PROBLEM SETUP", "text": "Consider a dataset with N real valued elements, which are defined on the vertices of an undirected graph G = (V, E), where the vertex set V = {v1, \u00b7 \u00b7 \u00b7 , vN} denotes the set of nodes, and the edge set E reveals the connection between the nodes. We refer to such datasets as graph signals. We assume that the length of the graph signals (thus the number of nodes), i.e., N is known. However, the edge set is not known. Therefore, we assume a complete graph G(V, E)\nas a candidate graph in which each node is connected to every other node with the number of edges |E| = M = N(N\u22121)/2, and aim to determine a subgraph of G by choosing a subset of edges, Es, from the edge set E of this candidate graph.\nAny undirected graph topology is basically determined by its graph Laplacian matrix, which essentially reveals the connectivity of the graph. Let us denote the graph Laplacian matrix (i.e., a symmetric matrix) of the complete graph by L \u2208 SN , where [L]i,j is nonzero, say equal to 1, only if i = j or (i, j) \u2208 E . The symmetric matrixL can be expressed in terms of the so-called incidence matrix, A = [a1, \u00b7 \u00b7 \u00b7 ,aM ] \u2208 R N\u00d7M as\nL = AAT =\nM\u2211\nm=1\nama T m,\nwhere the m-th column of A, i.e., am denotes a length-N edge vector with entries [am]i = 1, [am]j = \u22121, and it has zeros elsewhere, for an edge m connecting nodes i with j (more generally, am is determined only up to a sign).\nLet us now denote the subgraph Gs(V, Es) with the edge set Es \u2282 E such that |Es| = K \u226a M . We will refer to such a subgraph with K edges as a K-sparse graph. We connect such a K-sparse graph Gs to L through a sparse edge selection vector w = [w1, w2, \u00b7 \u00b7 \u00b7 , wM ]\nT \u2208 {0, 1}M , where wm = 1 if an edge belongs to the edge subset Es, and wm = 0 otherwise. In terms of w, |Es| = K means \u2016w\u20160 = K. (The notation \u2016w\u20160 counts the number of non-zero entries in w.) Finally, we can write the Laplacian matrix of the K-sparse graph, Ls, as a function of w as\nLs(w) =\nM\u2211\nm=1\nwmama T m. (1)\nIn what follows, we will optimally design the edge sampling function w to recover the graph that sufficiently explains the data."}, {"heading": "3. LEARNING FROM NOISELESS GRAPH SIGNALS", "text": "Let x = [x1, x2, \u00b7 \u00b7 \u00b7 , xN ]T \u2208 RN be a graph signal defined on the vertices V of a graph. The smoothness and the spectral content of the signal both depend on the underlying graph topology. The Laplacian quadratic form given by xTLs(w)x quantifies how smooth the graph signal x is with respect to the underlying graph [1]. In particular, the signal x is smoothest with respect to the graph with K edges for low values of xTLs(w)x."}, {"heading": "3.1. Problem statement: noiseless setting", "text": "Suppose we are given L graph signals denoted by the vectors {xk} L k=1, and they are collected in an N \u00d7 L matrix X = [x1, \u00b7 \u00b7 \u00b7 ,xL]. We are interested in recovering the graph Laplacian (in other words, the graph topology) under the prior information that the graph signals are smooth with respect to a K-sparse graph. More formally, we state the following.\nProblem 1. Given the graph signals {xk}Lk=1, determine a graph with K edges such that the graph signals have smooth variations on the resulting graph.\nMathematically, the above problem can be cast as the following optimization problem:\nargmin w\u2208W\n1\nL\nL\u2211\nk=1\nx T kLs(w)xk =\n1 L tr{XTLs(w)X}, (2)\nwhere W = {w \u2208 {0, 1}M | \u2016w\u20160 = K} is the constraint set that restricts the number of edges."}, {"heading": "3.2. Solver", "text": "Problem (2) is a cardinality constrained Boolean optimization problem, hence nonconvex. By recalling that Ls(w) = \u2211M m=1 wmama T m, we can express the cost function in (2) as a linear function in w, i.e., we have\n1 L tr { X T Ls(w)X } = M\u2211\nm=1\nwmtr { X T (amam T )X } . (3)\nIntroducing the length-M vector c = [c1, c2, . . . , cM ]T with cm = tr { XT (amam T )X } , we can write (2) as\nargmin w\u2208{0,1}M\nc T w s.to \u2016w\u20160 = K. (4)\nThe above Boolean linear programming problem admits an explicit solution and computing the optimal solution is straightforward. It is solved by sorting the entries of c in an ascending ordering. More specifically, the solution w will have entries equal to 1 at indices corresponding to the K smallest entries of c, and others are set to zero (ties may be broken arbitrarily). Computationally, the sorting algorithm costs O(K logK), and with a parallel implementation (e.g., on different processors), the computational complexity will be as low as O(K) [12, 13]. We give another interpretation of this result through the following remark.\nRemark 1. Let us suppose the graph signal is stochastic with covariance matrix Rx = E{xxT } \u2208 RN\u00d7N . Then, the solution to (4) would select K edges between those nodes having the highest crosscorrelation, i.e., it will add an edge between the ith and the jth node if the variables xi and xj are strongly correlated. To see this, we express the cost function in (2) as\nL\u22121tr{XTLs(w)X} = tr { Ls(w)R\u0302x }\n= M\u2211\nm=1\nwm(am T R\u0302xam)\nwhere R\u0302x = 1LXX T \u2208 RN\u00d7N is the sample data covariance matrix. Recalling the definition of am, it is easy to see that the term am\nT R\u0302xam = [R\u0302x]i,i + [R\u0302x]j,j \u2212 2[R\u0302x]i,j is small if the ith and jth nodes are highly correlated and we have sufficient samples to compute the sample covariance matrix.\nBy modelling the graph topology through an edge selection vector, the graph learning problem can be solved optimally using a simple and elegant solution with a controlled sparsity level, whereas optimizing directly the graph Laplacian [4] or the adjacency matrix [5] leads to a more complicated suboptimal solution with no explicit handle to control the graph sparsity."}, {"heading": "4. LEARNING FROM NOISY GRAPH SIGNALS", "text": "In many cases, we might not have access to the true graph signals. Suppose we observe a noisy version of the graph signal, xk, as\nyk = xk + nk \u2208 R N , (5)\nand we are given L such observations for k = 1, 2, . . . , L, where we assume that nk is zero-mean white Gaussian noise of variance \u03c32.\nTo recover xk based on the smoothness assumption, typically a leastsquares problem is solved with a Tikhonov regularization, xTk Lxk, to enforce the prior information that the noiseless graph signal xk is smooth with respect to the underlying graph. More specifically, the following optimization problem (assuming, for a moment that the graph, i.e., w is known) is solved [1]:\nargmin {xk} L\nk=1\n1\nL\nL\u2211\nk=1\n( \u2016yk \u2212 xk\u2016 2 2 + \u03b3x T k Ls(w)xk ) , (6)\nwhere the regularization parameter \u03b3 > 0 controls the amount of smoothness. This graph denoising problem has an explicit solution given by\nx\u0302k = [I + \u03b3Ls(w)] \u22121 yk, k = 1, \u00b7 \u00b7 \u00b7 , L."}, {"heading": "4.1. Problem statement: noisy setting", "text": "Having given the above denoising inference problem at hand, we will now formally state the problem of interest.\nProblem 2. Given the observations {yk} L k=1 that is related to the unknown graph signal xk as in (5), determine the K-sparse graph such that the estimate x\u0302k has the lowest possible estimation error, and it is smooth with respect to the recovered graph.\nSparse graph learning for the denoising inference problem can be mathematically formulated as follows:\nargmin {xk} L k=1 ,w\u2208W\n1\nL\nL\u2211\nk=1\n(\u2016yk \u2212 xk\u2016 2 2 + \u03b3 x T kLs(w)xk) (7)\nwhose solution is denoted as ({x\u0302k}Lk=1, w\u0302). This formulation is different from [4], as [4] solves an optimization problem over the space of all possible graph Laplacians (instead of parameterizing the graph with w \u2208 W) without sparsifying the graph. It can nevertheless be done through an extra \u21131-norm penalty term.\nThe above problem (7) is noncovex due to the Boolean and cardinality constraints on w and the coupling between the optimization variables in the second term of (7). We provide two methods to solve it. The first one is a straightforward approach based on alternating descent, while the second one is based on convex relaxation."}, {"heading": "4.2. Alternating minimization", "text": "The optimization problem (7) can be solved using alternating minimization with respect to {xk}Lk=1 and w. That is, given w, the problem in (7) reduces to a linear system in the unknown X, which admits a closed form solution; while given {xk}Lk=1, it reduces to a Boolean linear programming problem, which admits an analytical solution with respect to w based on rank ordering. These observations suggest an iterative alternating minimization algorithm yielding successive estimates of {xk}Lk=1 with fixed w, and alternately of w with fixed {xk}Lk=1. Specifically, with the iterate of w given per iteration i \u2265 0, i.e., w[i], we solve for X[i] using a matrix inversion as\nX [i] = Xmin(w[i])\nwith\nXmin(w) = argmin X\n\u2016Y \u2212X\u20162F + \u03b3 tr{X T Ls(w)X}\n= [I + \u03b3Ls(w)] \u22121 Y , (8)\nwhere Y = [y1,y2, \u00b7 \u00b7 \u00b7 ,yL] is the data matrix of size N \u00d7 L. Once X [i] is available, w[i+ 1] can be obtained by solving the Boolean linear program [cf. (4)]\nw[i+ 1] = argmin w\u2208{0,1}M\nM\u2211\nm=1\nwmcm[i+ 1] s.to \u2016w\u20160 = K,\nwhere cm[i+ 1] = tr { XT [i+ 1](amam T )X[i+ 1] }\n. In spite of the Boolean and cardinality constraints in the above problem, there exists a simple analytical solution for w[i + 1] based on sorting {cm[i+1]} M m=1, i.e., the solution w[i+1] will have entries equal to 1 at indices corresponding to the K smallest entries in {cm[i+1]}Mm=1 and zeros otherwise. The iterations are initialized at i = 0 by randomly generating w[i+1] from a uniform distribution over W . The above alternating minimization method is computationally very attractive, and consists of two simple known solutions per iteration. However, the algorithm converges only to a stationary point of (7), and it suffers from the choice of the initial estimate.\nThe algorithm proposed in [4] is also along the lines of alternating minimization, except that the graph learning step involves a complicated optimization over the space of all possible valid Laplacian matrices."}, {"heading": "4.3. Convex relaxation", "text": "To avoid the issues related to the initialization of the alternating minimization algorithm, in what follows we propose a one-step solution based on convex relaxation. We can rewrite the formulation in (7) alternatively as\nw\u0302 = argmin w\u2208W r(w); X\u0302 = Xmin(w\u0302) (9)\nwith\nr(w) = \u2016Y \u2212Xmin(w)\u2016 2 F + \u03b3 tr{X T min(w)Ls(w)Xmin(w)}\nand [I + \u03b3Ls(w)]Xmin(w) = Y . (10)\nThe computational complexity of solving the linear system of equations (10) decreases as the sparsity in w increases. Furthermore, the estimates {x\u0302k}Lk=1 and w\u0302 in (9) are still the same as in (7).\nPlugging the solution to (10) in r(w) and after some straightforward matrix algebra, we can express the regularized residual squared, r(w), as\nr(w) = tr { Y T [I + \u03b3Ls(w)] \u22121 Y }\n+ \u03b3tr { Y T Ls(w)Y } \u2212 \u2016Y \u20162F .\n(11)\nRelaxing the cardinality constraint \u2016w\u20160 = K with 1Tw = K and the Boolean constraints {0, 1}M with linear inequality constraints related to the box constraint [0, 1]M , the optimization problem (9) will be convex on w \u2208 [0, 1]M . To see this, we introduce a variable\nZ = Y T [I + \u03b3Ls(w)] \u22121 Y + \u03b3Y TLs(w)Y \u2208 R L\u00d7L\nand obtain a semidefinite program:\nargmin Z,w tr{Z}\ns.to\n[ Z \u2212 \u03b3Y TLs(w)Y Y T\nY I + \u03b3Ls(w)\n] 0L+N ,\n1 T w = K, 0 \u2264 wm \u2264 1, m = 1, 2, . . . ,M,\n(12)\nwith variables w and Z, and recall that Ls(w) = \u2211M m=1 wmama T m. A standard off-the-shelf solver can be used for solving the semidefinite program in (12). For large-scale problems, computationally cheaper first-order (and online) methods for solving (12) can be derived as the size of the linear matrix inequality in (12) depends on the size of the training data and the number of nodes."}, {"heading": "5. NUMERICAL RESULTS", "text": "We use temperature measurements collected across 32 weather stations in the French region of Brittany and the aim is to learn the graph that explains the observed data; see Fig. 1. There are 744 observations per weather station available, out of which we use L = 50 snapshots as the training set and the remaining ones as the evaluation set. One such observation (i.e., a graph signal) on a graph with N = 32 nodes is shown in Fig. 1, where the colored dots indicate different temperature readings. The convex optimization problems are solved using the CVX toolbox, which internally calls\nSDPT3 [14]. The candidate graph with N = 32 will have M = 496 edges, from which we aim to learn a subgraph with K = 110 edges.\nTo begin with, we consider the noiseless case, where the true graph signal is assumed to be known, and graph learning in this case amounts to solving a sorting problem. As shown in Fig. 1a, we can see that in the learnt graph with K = 110 edges, edges are present between nodes that share similar values. Although the proposed approach doesn\u2019t always (e.g., for low values of K) ensure a well-connected graph, it clusters entities (or correlated nodes) with similar values. Fig. 2a shows that the cost (i.e., smoothness) of the proposed closed-form sorting solution, which is optimal, is lower than the existing iterative solution [5].\nNext, we consider the noisy setting with the same training data as before, where we perform joint graph learning and denoising. In Fig. 1b, we show the learnt graph with K = 110 edges based on the convex relaxation approach explained in Sec. 4.3. In Fig. 2b, we evaluate the denoising performance based on the learnt graph using the evaluation set. In particular, we show the mean squared error for different values of the noise level, where the mean squared error is computed from 1000 independent Monte Carlo experiments. The one-step solution based on convex optimization (cf. Sec. 4.3) leads to a lower error as compared to the alternating minimization approaches, which in general converge only to a stationary point. This also holds for our method developed in Sec. 4.2, however, we stress the fact that the proposed alternating minimization (cf. Sec. 4.2) is computationally much less expensive (involving two simple known solutions per iteration) as compared to the iterative solution in [4]. The graph learnt under the noiseless setting does not perform well for denoising. Nevertheless, due its simple solution, it can be used to generate a base graph, which can be further refined for specific graph inference problems."}, {"heading": "6. CONCLUSIONS", "text": "We have studied the problem of learning a sparse graph that adequately explains the data under a smoothness prior. We model the graph learning problem as the design of a sparse edge sampling function. In other words, we express the graph Laplacian in terms of an edge selection vector. We have considered both the noiseless and noisy setting. In the noiseless setting, designing the edge selection vector is elegant, and it boils down to a simple low-complexity sorting problem. However, in the presence of noise, we propose a computationally cheap alternating minimization algorithm as well as a one-step convex relaxation based solution.\nSoftware and datasets to produce results of this paper can be downloaded from http://cas.et.tudelft.nl/\u02dcsundeep/sw/ icassp17Graphlearning.zip"}, {"heading": "7. REFERENCES", "text": "[1] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, \u201cThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains,\u201d IEEE Signal Process. Mag., vol. 30, no. 3, pp. 83\u201398, 2013.\n[2] A. Sandryhaila and J. M. Moura, \u201cBig data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure,\u201d IEEE Signal Process. Mag., vol. 31, no. 5, pp. 80\u201390, 2014.\n[3] K. Slavakis, G. Giannakis, and G. Mateos, \u201cModeling and optimization for big data analytics:(statistical) learning tools for our era of data deluge,\u201d IEEE Signal Process. Mag., vol. 31, no. 5, pp. 18\u201331, 2014.\n[4] X. Dong, D. Thanou, P. Frossard, and P. Vandergheynst, \u201cLearning laplacian matrix in smooth graph signal representations,\u201d arXiv preprint arXiv:1406.7842, 2014.\n[5] V. Kalofolias, \u201cHow to learn a graph from smooth signals,\u201d in Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, 2016, pp. 920\u2013929.\n[6] A. Ghosh and S. Boyd, \u201cGrowing well-connected graphs,\u201d in Proceedings of the 45th IEEE Conference on Decision and Control, 2006, pp. 6605\u20136611.\n[7] S. Segarra, A. G. Marques, G. Mateos, and A. Ribeiro, \u201cNetwork topology identification from spectral templates,\u201d arXiv preprint arXiv:1604.02610, 2016.\n[8] A. Marques, S. Segarra, G. Leus, and A. Ribeiro, \u201cStationary graph processes and spectral estimation,\u201d arXiv preprint arXiv:1603.04667, 2016.\n[9] S. P. Chepuri and G. Leus, \u201cSubsampling for graph power spectrum estimation,\u201d arXiv preprint arXiv:1603.03697, 2016.\n[10] D. A. Spielman and S.-H. Teng, \u201cSpectral sparsification of graphs,\u201d SIAM Journal on Computing, vol. 40, no. 4, pp. 981\u2013 1025, 2011.\n[11] J. Batson, D. A. Spielman, and N. Srivastava, \u201cTwiceramanujan sparsifiers,\u201d SIAM Journal on Computing, vol. 41, no. 6, pp. 1704\u20131721, 2012.\n[12] R. S. Blum and B. M. Sadler, \u201cEnergy efficient signal detection in sensor networks using ordered transmissions,\u201d IEEE Trans. Signal Process., vol. 56, no. 7, pp. 3229\u20133235, 2008.\n[13] S. P. Chepuri and G. Leus, \u201cSparse sensing for distributed detection,\u201d IEEE Trans. Signal Process., vol. 64, no. 6, pp. 1446\u2013 1460, 2015.\n[14] M. Grant and S. Boyd, \u201cCVX: Matlab software for disciplined convex programming, version 2.0 beta,\u201d http://cvxr.com/cvx, Sep. 2012."}], "references": [{"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Process. Mag., vol. 30, no. 3, pp. 83\u201398, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Big data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure", "author": ["A. Sandryhaila", "J.M. Moura"], "venue": "IEEE Signal Process. Mag., vol. 31, no. 5, pp. 80\u201390, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling and optimization for big data analytics:(statistical) learning tools for our era of data deluge", "author": ["K. Slavakis", "G. Giannakis", "G. Mateos"], "venue": "IEEE Signal Process. Mag., vol. 31, no. 5, pp. 18\u201331, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning laplacian matrix in smooth graph signal representations", "author": ["X. Dong", "D. Thanou", "P. Frossard", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1406.7842, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "How to learn a graph from smooth signals", "author": ["V. Kalofolias"], "venue": "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, 2016, pp. 920\u2013929.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Growing well-connected graphs", "author": ["A. Ghosh", "S. Boyd"], "venue": "Proceedings of the 45th IEEE Conference on Decision and Control, 2006, pp. 6605\u20136611.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Network topology identification from spectral templates", "author": ["S. Segarra", "A.G. Marques", "G. Mateos", "A. Ribeiro"], "venue": "arXiv preprint arXiv:1604.02610, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Stationary graph processes and spectral estimation", "author": ["A. Marques", "S. Segarra", "G. Leus", "A. Ribeiro"], "venue": "arXiv preprint arXiv:1603.04667, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Subsampling for graph power spectrum estimation", "author": ["S.P. Chepuri", "G. Leus"], "venue": "arXiv preprint arXiv:1603.03697, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral sparsification of graphs", "author": ["D.A. Spielman", "S.-H. Teng"], "venue": "SIAM Journal on Computing, vol. 40, no. 4, pp. 981\u2013 1025, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Twiceramanujan sparsifiers", "author": ["J. Batson", "D.A. Spielman", "N. Srivastava"], "venue": "SIAM Journal on Computing, vol. 41, no. 6, pp. 1704\u20131721, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Energy efficient signal detection in sensor networks using ordered transmissions", "author": ["R.S. Blum", "B.M. Sadler"], "venue": "IEEE Trans. Signal Process., vol. 56, no. 7, pp. 3229\u20133235, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse sensing for distributed detection", "author": ["S.P. Chepuri", "G. Leus"], "venue": "IEEE Trans. Signal Process., vol. 64, no. 6, pp. 1446\u2013 1460, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "CVX: Matlab software for disciplined convex programming, version 2.0 beta", "author": ["M. Grant", "S. Boyd"], "venue": "http://cvxr.com/cvx, Sep. 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Graphs offer a way to describe and explain relationships in complex datasets, a central entity of modern data analysis, where data deluge is prominent [1\u20133].", "startOffset": 151, "endOffset": 156}, {"referenceID": 1, "context": "Graphs offer a way to describe and explain relationships in complex datasets, a central entity of modern data analysis, where data deluge is prominent [1\u20133].", "startOffset": 151, "endOffset": 156}, {"referenceID": 2, "context": "Graphs offer a way to describe and explain relationships in complex datasets, a central entity of modern data analysis, where data deluge is prominent [1\u20133].", "startOffset": 151, "endOffset": 156}, {"referenceID": 0, "context": "To realize this, we make a simple, but widely used assumption [1,4] that the data is smooth with respect to the discovered graph.", "startOffset": 62, "endOffset": 67}, {"referenceID": 3, "context": "To realize this, we make a simple, but widely used assumption [1,4] that the data is smooth with respect to the discovered graph.", "startOffset": 62, "endOffset": 67}, {"referenceID": 3, "context": "The problem of learning the graph Laplacian or the weighted adjacency matrix from smooth graph signals has been considered before [4,5].", "startOffset": 130, "endOffset": 135}, {"referenceID": 4, "context": "The problem of learning the graph Laplacian or the weighted adjacency matrix from smooth graph signals has been considered before [4,5].", "startOffset": 130, "endOffset": 135}, {"referenceID": 4, "context": "Learning sparse graphs from the true graph signals, which is the problem we consider in Section 3, has been studied in [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "Such a modelling is inspired from [6], where the problem to design edge weights that maximize the algebraic connectivity of the graph has been addressed.", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "In [4], the joint graph learning and denoising problem has been addressed, i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Graph topology identification is also investigated in [7] under the assumption that the eigenvectors of the graph Laplacian are known, which is a much stronger assumption.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "Although the eigenvectors can be computed from graph data (or the sample covariance matrix) when it is stationary with respect to the graph [8, 9], the graph signals need not always be vertex stationary.", "startOffset": 140, "endOffset": 146}, {"referenceID": 8, "context": "Although the eigenvectors can be computed from graph data (or the sample covariance matrix) when it is stationary with respect to the graph [8, 9], the graph signals need not always be vertex stationary.", "startOffset": 140, "endOffset": 146}, {"referenceID": 3, "context": "In most of the existing approaches [4,5,7], graph sparsification is (or can be) achieved by penalizing the l1-norm of the graph Laplacian matrix, adjacency matrix or the shift operator, however, there is no explicit handle to control the number of edges, unlike the proposed approach.", "startOffset": 35, "endOffset": 42}, {"referenceID": 4, "context": "In most of the existing approaches [4,5,7], graph sparsification is (or can be) achieved by penalizing the l1-norm of the graph Laplacian matrix, adjacency matrix or the shift operator, however, there is no explicit handle to control the number of edges, unlike the proposed approach.", "startOffset": 35, "endOffset": 42}, {"referenceID": 6, "context": "In most of the existing approaches [4,5,7], graph sparsification is (or can be) achieved by penalizing the l1-norm of the graph Laplacian matrix, adjacency matrix or the shift operator, however, there is no explicit handle to control the number of edges, unlike the proposed approach.", "startOffset": 35, "endOffset": 42}, {"referenceID": 9, "context": "In a related line of research, [10, 11] investigate computing sparse graphs that approximate a given graph spectrally, which means that their Laplacian matrices have similar quadratic forms.", "startOffset": 31, "endOffset": 39}, {"referenceID": 10, "context": "In a related line of research, [10, 11] investigate computing sparse graphs that approximate a given graph spectrally, which means that their Laplacian matrices have similar quadratic forms.", "startOffset": 31, "endOffset": 39}, {"referenceID": 0, "context": "The Laplacian quadratic form given by xLs(w)x quantifies how smooth the graph signal x is with respect to the underlying graph [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 11, "context": ", on different processors), the computational complexity will be as low as O(K) [12, 13].", "startOffset": 80, "endOffset": 88}, {"referenceID": 12, "context": ", on different processors), the computational complexity will be as low as O(K) [12, 13].", "startOffset": 80, "endOffset": 88}, {"referenceID": 3, "context": "By modelling the graph topology through an edge selection vector, the graph learning problem can be solved optimally using a simple and elegant solution with a controlled sparsity level, whereas optimizing directly the graph Laplacian [4] or the adjacency matrix [5] leads to a more complicated suboptimal solution with no explicit handle to control the graph sparsity.", "startOffset": 235, "endOffset": 238}, {"referenceID": 4, "context": "By modelling the graph topology through an edge selection vector, the graph learning problem can be solved optimally using a simple and elegant solution with a controlled sparsity level, whereas optimizing directly the graph Laplacian [4] or the adjacency matrix [5] leads to a more complicated suboptimal solution with no explicit handle to control the graph sparsity.", "startOffset": 263, "endOffset": 266}, {"referenceID": 0, "context": ", w is known) is solved [1]:", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "This formulation is different from [4], as [4] solves an optimization problem over the space of all possible graph Laplacians (instead of parameterizing the graph with w \u2208 W) without sparsifying the graph.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "This formulation is different from [4], as [4] solves an optimization problem over the space of all possible graph Laplacians (instead of parameterizing the graph with w \u2208 W) without sparsifying the graph.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "The algorithm proposed in [4] is also along the lines of alternating minimization, except that the graph learning step involves a complicated optimization over the space of all possible valid Laplacian matrices.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "Relaxing the cardinality constraint \u2016w\u20160 = K with 1w = K and the Boolean constraints {0, 1} with linear inequality constraints related to the box constraint [0, 1] , the optimization problem (9) will be convex on w \u2208 [0, 1] .", "startOffset": 157, "endOffset": 163}, {"referenceID": 0, "context": "Relaxing the cardinality constraint \u2016w\u20160 = K with 1w = K and the Boolean constraints {0, 1} with linear inequality constraints related to the box constraint [0, 1] , the optimization problem (9) will be convex on w \u2208 [0, 1] .", "startOffset": 217, "endOffset": 223}, {"referenceID": 4, "context": "tr { X T L s ( ) } proposed (noiseless, optimal) primal-dual [5]", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "from [4] one-step cvx opt in Sec.", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": "The convex optimization problems are solved using the CVX toolbox, which internally calls SDPT3 [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": ", smoothness) of the proposed closed-form sorting solution, which is optimal, is lower than the existing iterative solution [5].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "2) is computationally much less expensive (involving two simple known solutions per iteration) as compared to the iterative solution in [4].", "startOffset": 136, "endOffset": 139}], "year": 2016, "abstractText": "In this paper, we are interested in learning the underlying graph structure behind training data. Solving this basic problem is essential to carry out any graph signal processing or machine learning task. To realize this, we assume that the data is smooth with respect to the graph topology, and we parameterize the graph topology using an edge sampling function. That is, the graph Laplacian is expressed in terms of a sparse edge selection vector, which provides an explicit handle to control the sparsity level of the graph. We solve the sparse graph learning problem given some training data in both the noiseless and noisy settings. Given the true smooth data, the posed sparse graph learning problem can be solved optimally and is based on simple rank ordering. Given the noisy data, we show that the joint sparse graph learning and denoising problem can be simplified to designing only the sparse edge selection vector, which can be solved using convex optimization.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}