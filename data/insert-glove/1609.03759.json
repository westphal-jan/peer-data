{"id": "1609.03759", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "3D Simulation for Robot Arm Control with Deep Q-Learning", "abstract": "Intelligent cigaritis control of silurians robotic droz arms henney has huge dozier potential over the heatherley coming 0-1 years, but as 2005-6 of now bigham will often fail to 96.81 adapt carafe when claydon presented with 149.6 new and power8 unfamiliar environments. polynya Recent whiteland trends to karhu solve this problem dworshak have mukamal seen a conocer shift to paleoindians end - to - figuera end diefenbach solutions somtas using deep shamayleh reinforcement noaki learning clew to moderniser learn policies tikhvinsky from rosencrans visual input, rather thomaselli than valvoline relying on spott a handcrafted, sylphs modular hentoff pipeline. Building upon the icemark recent success boxful of cocaleros deep Q - liscard networks, similiar we present an approach anthony which numeiri uses prompting three - consortiums dimensional simulations to 6:41 train a 92.60 7 - persepolis DOF 70-a robotic arm in christine a 2-7/8 robot hestrie arm control williamses task 54.07 without clarisse any 6,800 prior knowledge. 250th Policies ondes accept images kamps of agee the environment as input and tr3s output mccarrick motor actions. However, the 51-page high - dimensionality mizzenmast of razmi the edzell policies bundesbahn as moderated well as chuadanga the cockney large state space makes mickle policy sannyasi search difficult. This is overcome non-surgical by ensuring eiswein interesting :0 states are explored lpf via sub-species intermediate over-the-road rewards 9,350 that hotlines guide the reevaluated policy criss towards higher inward reward states. Our kershaw results demonstrate filarm\u00f3nica that maluti deep kanneh Q - networks combermere can gelfond be used to p-type learn weinblatt policies r\u00e1s for jerusalemites a zollverein task that involves creola locating laker a groark cube, grasping, lean-to and nemat then krumholz finally consignment lifting. sicignano The agent unicorn is hohenlohe-langenburg able jebe to learn to contaminants deal visi with gedeo a range of starting joint configurations sandokan and 36.65 starting cube lennox positions when tested flom in soegijapranata simulation. sarika Moreover, derlin we show cellucci that backstop policies trained morag via simulation have the potential to 4.375 be directly applied to mzila real - hernici world batyrov equivalents without any further laetitia training. aphrodisiac We byard believe that robot simulations can decrease the 8e dependency on bi-lo physical robots onboard and ultimately outposts improve 3,315 productivity of napaljarri training robot 27,838 control psoralea tasks.", "histories": [["v1", "Tue, 13 Sep 2016 10:40:24 GMT  (2177kb,D)", "https://arxiv.org/abs/1609.03759v1", null], ["v2", "Tue, 13 Dec 2016 16:09:17 GMT  (1214kb,D)", "http://arxiv.org/abs/1609.03759v2", "In NIPS 2016 Workshop: Deep Learning for Action and Interaction (this https URL)"]], "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["stephen james", "edward johns"], "accepted": false, "id": "1609.03759"}, "pdf": {"name": "1609.03759.pdf", "metadata": {"source": "CRF", "title": "3D Simulation for Robot Arm Control with Deep Q-Learning", "authors": ["Stephen James"], "emails": ["slj12@imperial.ac.uk", "e.johns@imperial.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Traditionally, robot arm control has been solved by hand-crafting solutions in a modular fashion, for example: 3D reconstruction, scene segmentation, object recognition, object pose estimation, robot pose estimation, and finally trajectory planning. However, this approach can result in loss of information between each of the modules resulting in accumulation of error, and it is not flexible for learning a range of tasks, due to the need for prior knowledge.\nA recent trend has seen robot arm control being learned directly from image pixels, in an end-to-end manner using deep reinforcement learning. This exploits the power of deep learning with its ability to learn complex functions from raw data, avoiding the need for hand-engineering cumbersome and complex modules. However, applying deep reinforcement learning to robot control is challenging due to the need for large amounts of training data to deal with the high dimensionality of policies and state space, which makes efficient policy search difficult.\nDeep reinforcement learning for robot manipulation often uses real-world robotic platforms, which usually require human interaction and long training times. We believe that a promising alternative solution is using simulation to train a controller, which can then be directly applied to real-world hardware, scaling up the available training data without the burden of requiring human interaction during the training process.\nBuilding upon the recent success of deep Q-networks (DQNs) [7], we present an approach that uses deep Q-learning and 3D simulations to train a 7-DOF robotic arm in a robot control task in an end-to-end manner, without any prior knowledge. Images are rendered of the virtual environment and passed through the network to produce motor actions. To ensure that the agent explores interesting states of this 3D environment, we use intermediate rewards that guide the policy to higher-reward states. As a test case for evaluation, our experiments focus on a task that involves locating a cube,\nar X\niv :1\n60 9.\n03 75\n9v 2\n[ cs\n.R O\n] 1\n3 D\nec 2\ngrasping it, and then finally lifting it off a table. Whilst this particular task is simpler than some others trained with real-world data, the novelty of end-to-end deep reinforcement learning requires evaluation of these initial experiments in order to set the scene for further, more complex applications in future work."}, {"heading": "2 Related work", "text": "Deep reinforcement learning enables policies to be learned by exploiting the function approximation powers of deep neural networks, to map observations directly to actions. Recently, state-of-the-art results have been achieved with deep Q-networks (DQN) [7], a novel variant of Q-learning that is used to play 49 Atari-2600 games using only pixels as input. This method was designed for use with discrete action spaces, and is adopted for our work. However, recent extensions adapted deep Q-learning for the continuous domain [15], allowing high dimensional action spaces such as that in physical control tasks.\nFor robot manipulation, recent work has also seen deep reinforcement learning used to learn visuallyguided robot arm controllers via convolutional neural networks [1, 3, 4]. Most notably, guided policy search [2, 1] has accomplished tasks such as block insertion into a shape sorting cube and screwing on a bottle cap, by using trajectory optimization to direct policy learning. While the tasks undertaken in these papers are impressive, they require significant human involvement for data collection. An alternative approach is to use large-scale self-supervision [8, 5, 9, 10], but in practice this is limited due to the expense of acquiring many robots for learning in parallel, or the time consuming nature if only a single robot is available.\nWe believe that training in simulation is a more scalable solution for the most complex manipulation tasks. Simulation has been used for a number of tasks in computer vision and robotics, including object recognition [11], semantic segmentation [16], robot grasping [12], and autonomous driving [17]. For robot arm control, it was recently shown that policies could be learned in simulation for a 2D target reaching task [6], but this failed to show any feasibility of transferring to the real world. To address this issue of transfer learning, a cross-domain loss was proposed in [13] to incorporate both simulated and real-world data within the same loss function. An alternative approach has made use of progressive neural networks [18], which ensure that information from simulation is not forgotten when further training is carried out on a real robot [14]. However, our approach differs in that we do not require any real-world training, and attempt to directly apply policies learned in simulation, over to a real robot."}, {"heading": "3 Approach", "text": "Our approach uses a DQN [7] which approximates the optimal action-value (or Q) function for a given input:\nQ\u2217(s, a) = max \u03c0 E\u03c0( \u221e\u2211 k=0 \u03b3krt+k|st = s, at = a). (1)\nHere, Q(s, a) defines the q-value of action a and state s. A policy \u03c0 determines which action to take in each state, and is typically the action associated with the highest q-value for that state. Q\u2217(s, a) is then the optimum policy, defined as that which has the greatest expected cumulative reward r when this policy is followed, over each time step k between the starting state at time t and some termination criteria (or infinite time). The discount factor \u03bb gives weight to short-term rewards and ensures convergence of Equation 1.\nThe Q-learning update rule uses the following loss function at iteration i:\nLi(\u03b8i) = E(s,a,r,s\u2032)\u223cD\n[ r + \u03b3max\na\u2032 Q(s\u2032, a\u2032; \u03b8\u2212i )\u2212Q(s, a; \u03b8i)\n]2 , (2)\nwhere \u03b8i are the parameters of the Q-network, and \u03b8\u2212i are the parameters used to compute the target.\nIn this paper, we use this approach to train a 3D simulation of a 7-DOF robotic arm in a control task. On each iteration of training, an image is taken of the virtual environment and passed through the network to produce motor actions. We define an episode as 1000 iterations or until the task has been completed. The task is considered completed when the arm has grasped the cube and lifted\nit to a height of 30cm. At the end of each episode, the scene is reset, and a new episode begins. We define the maximum number of iterations to be 1000 so that the agent does not waste time in episodes that cannot be completed successfully \u2014 for example, if the agent knocks or drops the cube out of its grasping range. We use an epsilon-greedy exploration approach which decides between a random action or the action corresponding the highest Q-value produced by the DQN. During training, epsilon is annealed over a period of 1 million episodes.\nOur state is represented by an image of the scene which includes both the arm and cube. We define a set of 14 actions that control the gripper and 6 joints of the arm. Each joint can be controlled by 2 actions \u2014 one action to rotate the joint by one degree clockwise, and another action to rotate one degree counter clockwise. The gripper follows similarly, with one action to open the gripper, and the other to close it. Our network, summarised in Figure 1, has 14 outputs corresponding to the Q-values for each action given the current state.\nDuring each of our experiments, we fix the discount rate at \u03b3 = 0.99 and set our replay memory size to 500, 000. Gradient descent is performed using the Adam optimisation algorithm [19] with the learning rate set at \u03b1 = 6\u00d7 10\u22126.\nDue to the size of the state space, we must engineer the simulator to produce rewards that encourage the agent to explore states around the cube. We use positional information to give the agent intermediate rewards based on the distance between the gripper and the cube. Summarised in Algorithm 1, rewards are given to the agent based on the exponential decaying distance from the gripper to the cube (e\u2212\u03b3\u00d7distance), with a reward of 1 while the cube is grasped, and then an additional reward based on the distance the cube is from the ground. During our experimentation, we defined \u03b3 = 0.25. When the arm lifts the cube high enough to complete the task, the agent receives a final reward of 100 for task completion.\nAlgorithm 1: Reward function if terminal then r = 100 else if target.grasped then r = 1 + target.position.y else r = e\u2212\u03b3\u00d7distance end"}, {"heading": "4 Experiments and results", "text": "During the course of this research, we used two versions of our simulator. The first version was built from primitive shapes and was used as an initial proof-of-concept to show that policies could be learned when trained and tested in simulation. We then turned our attention to evaluating the feasibility of transferring trained networks to the real world without any further training. In order for policy to have any chance of transferring, we needed to modify the simulation so that the virtual world would resembled the real world as much as possible. This version was then used to train one final network which would be tested in the real world.\nOur first experiment saw the agent learn to grasp and then lift the cube from a fixed starting position shown by image (a) in Figure 2. Interestingly, we observed the arm returning to grasp the cube if it was dropped during the lifting motion (due to a random \"open gripper\" action selected via\nthe epsilon-greedy method). This demonstrates the ability of the agent to recognise high rewards associated with grasping the object from a range of states, despite having never been trained for a re-grasping action.\nIn addition to observing the agent in action when being tested to get a good indication of whether the agent has begun to learn a set of high reward policies, we can also study the data collected along the learning process. Figure 3 shows us the cumulative successes after each episode, the average reward per episode, and the average maximal Q-value per episode. When the agent starts to succeed on a regular basis at 1800 episodes (Figure 3a), we see a greater frequency of increased average rewards (Figure 3b), along with a sharp increase in the average maximal Q-values (Figure 3c).\nFollowing this, we attempted to make our network learn to deal with a range of starting joint configurations and starting cube positions. At the start of each episode, the joint angles were set in a similar configuration to image (a) in Figure 2, but with variations of 20 degrees in each joint. During training, each time the agent was successful in completing the task, the cube was moved to a random location within a 200cm2 rectangular area. A comparison of this expriment and the previous are summarised in Table 1. Here, we see the importance of training for tolerance to varying initial conditions compared to training with a fixed initial condition, where the success rate jumps from 2% to 52%.\nFigure 4 shows a visualisation of the learned value function on a successful episode of 112 iterations (or frames). There are 5 images labelled from A to E which correspond to the frame numbers in the\ngraph. The frames from A to C show a steady increase in the Q-values which is a result of the agent getting closer to the cube. At frame C, the Q-values fluctuate briefly due to the network trying to determine whether or not it has grasped the cube. By the time we get to frame D, there has been a large jump in the Q-values where the agent seems certain that it has the cube. The Q-value then continues to rise as the height of the cube increases, and finally peaks as the agent expects to receive a large reward for task completion. The visualisation shows that the value function is able to evolve over time for a robot control task.\nTo explore direct transfer to a real robot, we first needed to ensure that the simulated world resembled the real-world as much as possible. For this, we set out a scene in the real world and then created a more visually-realistic simulation than the first attempt. Images (c) and (d) in Figure 2 show both the real-world scene and its simulated replica. We took the network trained on this simulation and ran it directly on the real-world version of the arm with epsilon fixed at 0.1. As we had hoped, the agent exhibited similar behaviour to its simulated counterpart, moving the gripper directly towards the cube \u2014 suggesting that transferring from simulation to real-world is possible.\nHowever, the action to close the gripper was in fact rarely chosen by our trained policy when applied to the real-world, and so the agent was unable to fully complete the task in one sequence. Instead, the agent typically progresses towards the cube and remains in nearby states. In order to test if the agent could complete the task had it closed its gripper, we started the agent in a position where it has the cube in its grasp. We re-ran the experiment from this starting state, and then the agent completed the task successfully by lifting the cube in an upwards direction. This illustrates that transferring control of the gripper is more challenging than that of the 6 joints on the arm, perhaps due to there being less room for error in a binary \"open or close\" action, compared to an action involving movement in continuous 3D space.\nFinally, observing the feature map activations in Figure 5 gives evidence that transfer is somewhat successful, by showing similar feature map activations for both simulation and the real world. All layers show very similar activations to the respective counterpart layers."}, {"heading": "5 Conclusions", "text": "The experiments and results in this paper present an investigation into training robot arm control using deep Q-learning via 3D simulation. Whilst the experimental set up is simple in our case, extension to more complex and useful tasks can now be treated as one of scaling up the training data, and increasing the accuracy of graphics and physics simulation. Given the scalability of simulation compared to real-world training as is typically performed elsewhere, the range of possible tasks which could be learned is now significantly greater than before, together with the capacity for generalisation to scenes with new layouts, colours and textures. This paper provides a strong foundation from which highly-scalable, end-to-end training of complex robot arm control, can now be fully investigated."}], "references": [{"title": "End-to-end training of deep visuomotor policies.", "author": ["Sergey Levine"], "venue": "Journal of Machine Learning Research", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Guided Policy Search.", "author": ["Sergey Levine", "Vladlen Koltun"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Deep Spatial Autoencoders for Visuomotor Learning.\" ICRA 2016", "author": ["Chelsea Finn"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning contact-rich manipulation skills with guided policy search.", "author": ["Sergey Levine", "Nolan Wagener", "Pieter Abbeel"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection.\" ISER 2016", "author": ["Sergey Levine"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control.", "author": ["Fangyi Zhang"], "venue": "ARAA", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Human-level control through deep reinforcement learning.", "author": ["Volodymyr Mnih"], "venue": "Nature", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours.", "author": ["Lerrel Pinto", "Abhinav Gupta"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep Reinforcement Learning for Robotic Manipulation.\" arXiv preprint 2016", "author": ["Shixiang Gu"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Deep Visual Foresight for Planning Robot Motion.\" arXiv preprint 2016", "author": ["Chelsea Finn", "Sergey Levine"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Pairwise Decomposition of Image Sequences for Active Multi-View Recognition.", "author": ["Edward Johns", "Stefan Leutenegger", "Andrew J. Davison"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep Learning a Grasp Function for Grasping under Gripper Pose Uncertainty.\" IROS 2016", "author": ["Edward Johns", "Stefan Leutenegger", "Andrew J. Davison"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Towards Adapting Deep Visuomotor Representations from Simulated to Real Environments.\" WARF 2016", "author": ["Eric Tzeng"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Sim-to-Real Robot Learning from Pixels with Progressive Nets.\" arXiv preprint 2016", "author": ["Andrei A. Rusu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Continuous control with deep reinforcement learning.\" ICLR 2016", "author": ["Timothy P. Lillicrap"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Understanding real world indoor scenes with synthetic data.\" CVPR 2016", "author": ["Ankur Handa"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes.", "author": ["German Ros"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Progressive neural networks.\" arXiv preprint 2016", "author": ["Andrei A. Rusu"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Adam: A method for stochastic optimization.", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "ICLR 2015", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Building upon the recent success of deep Q-networks (DQNs) [7], we present an approach that uses deep Q-learning and 3D simulations to train a 7-DOF robotic arm in a robot control task in an end-to-end manner, without any prior knowledge.", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "Recently, state-of-the-art results have been achieved with deep Q-networks (DQN) [7], a novel variant of Q-learning that is used to play 49 Atari-2600 games using only pixels as input.", "startOffset": 81, "endOffset": 84}, {"referenceID": 14, "context": "However, recent extensions adapted deep Q-learning for the continuous domain [15], allowing high dimensional action spaces such as that in physical control tasks.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "For robot manipulation, recent work has also seen deep reinforcement learning used to learn visuallyguided robot arm controllers via convolutional neural networks [1, 3, 4].", "startOffset": 163, "endOffset": 172}, {"referenceID": 2, "context": "For robot manipulation, recent work has also seen deep reinforcement learning used to learn visuallyguided robot arm controllers via convolutional neural networks [1, 3, 4].", "startOffset": 163, "endOffset": 172}, {"referenceID": 3, "context": "For robot manipulation, recent work has also seen deep reinforcement learning used to learn visuallyguided robot arm controllers via convolutional neural networks [1, 3, 4].", "startOffset": 163, "endOffset": 172}, {"referenceID": 1, "context": "Most notably, guided policy search [2, 1] has accomplished tasks such as block insertion into a shape sorting cube and screwing on a bottle cap, by using trajectory optimization to direct policy learning.", "startOffset": 35, "endOffset": 41}, {"referenceID": 0, "context": "Most notably, guided policy search [2, 1] has accomplished tasks such as block insertion into a shape sorting cube and screwing on a bottle cap, by using trajectory optimization to direct policy learning.", "startOffset": 35, "endOffset": 41}, {"referenceID": 7, "context": "An alternative approach is to use large-scale self-supervision [8, 5, 9, 10], but in practice this is limited due to the expense of acquiring many robots for learning in parallel, or the time consuming nature if only a single robot is available.", "startOffset": 63, "endOffset": 76}, {"referenceID": 4, "context": "An alternative approach is to use large-scale self-supervision [8, 5, 9, 10], but in practice this is limited due to the expense of acquiring many robots for learning in parallel, or the time consuming nature if only a single robot is available.", "startOffset": 63, "endOffset": 76}, {"referenceID": 8, "context": "An alternative approach is to use large-scale self-supervision [8, 5, 9, 10], but in practice this is limited due to the expense of acquiring many robots for learning in parallel, or the time consuming nature if only a single robot is available.", "startOffset": 63, "endOffset": 76}, {"referenceID": 9, "context": "An alternative approach is to use large-scale self-supervision [8, 5, 9, 10], but in practice this is limited due to the expense of acquiring many robots for learning in parallel, or the time consuming nature if only a single robot is available.", "startOffset": 63, "endOffset": 76}, {"referenceID": 10, "context": "Simulation has been used for a number of tasks in computer vision and robotics, including object recognition [11], semantic segmentation [16], robot grasping [12], and autonomous driving [17].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "Simulation has been used for a number of tasks in computer vision and robotics, including object recognition [11], semantic segmentation [16], robot grasping [12], and autonomous driving [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "Simulation has been used for a number of tasks in computer vision and robotics, including object recognition [11], semantic segmentation [16], robot grasping [12], and autonomous driving [17].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "Simulation has been used for a number of tasks in computer vision and robotics, including object recognition [11], semantic segmentation [16], robot grasping [12], and autonomous driving [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 5, "context": "For robot arm control, it was recently shown that policies could be learned in simulation for a 2D target reaching task [6], but this failed to show any feasibility of transferring to the real world.", "startOffset": 120, "endOffset": 123}, {"referenceID": 12, "context": "To address this issue of transfer learning, a cross-domain loss was proposed in [13] to incorporate both simulated and real-world data within the same loss function.", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "An alternative approach has made use of progressive neural networks [18], which ensure that information from simulation is not forgotten when further training is carried out on a real robot [14].", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "An alternative approach has made use of progressive neural networks [18], which ensure that information from simulation is not forgotten when further training is carried out on a real robot [14].", "startOffset": 190, "endOffset": 194}, {"referenceID": 6, "context": "Our approach uses a DQN [7] which approximates the optimal action-value (or Q) function for a given input: Q\u2217(s, a) = max \u03c0 E\u03c0( \u221e \u2211", "startOffset": 24, "endOffset": 27}, {"referenceID": 18, "context": "Gradient descent is performed using the Adam optimisation algorithm [19] with the learning rate set at \u03b1 = 6\u00d7 10\u22126.", "startOffset": 68, "endOffset": 72}], "year": 2016, "abstractText": "Recent trends in robot arm control have seen a shift towards end-to-end solutions, using deep reinforcement learning to learn a controller directly from raw sensor data, rather than relying on a hand-crafted, modular pipeline. However, the high dimensionality of the state space often means that it is impractical to generate sufficient training data with real-world experiments. As an alternative solution, we propose to learn a robot controller in simulation, with the potential of then transferring this to a real robot. Building upon the recent success of deep Q-networks, we present an approach which uses 3D simulations to train a 7-DOF robotic arm in a control task without any prior knowledge. The controller accepts images of the environment as its only input, and outputs motor actions for the task of locating and grasping a cube, over a range of initial configurations. To encourage efficient learning, a structured reward function is designed with intermediate rewards. We also present preliminary results in direct transfer of policies over to a real robot, without any further training.", "creator": "LaTeX with hyperref package"}}}