{"id": "1609.05123", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Learning Opposites Using Neural Networks", "abstract": "250 Many besik research altfest works bing have successfully ahmadova extended algorithms contritely such vervet as evolutionary algorithms, reinforcement agents mycale and pletikosa neural networks fola using \" 29.83 opposition - burster based learning \" (2.0-3 OBL ). Two types of the \" opposites \" have unformatted been frederiksen defined intension in fender the literature, bulin namely \\ ingratiation textit {branford type - c\u00e9l\u00e8bre I} text-to-speech and \\ 82-71 textit {samna type - f-box II }. surefire The former are chicago-kent linear in rocchi nature mangubat and straight applicable greetham to the 20-valve variable nopal space, non-liturgical hence easy to zaitsev calculate. worsdell On multi-threading the remanent other hand, seguenzia type - II eumenides opposites drive-in capture the \" biped oppositeness \" in the thoren output radm space. adheres In haggar fact, barton-upon-humber type - I opposites toxics are potassium-argon considered shearith a lutie special vert case of subdividing type - II opposites prakash where kili inputs and refill outputs have tarmiyah a asserts linear supercritical relationship. However, cleanflicks in anti-muslim many hirschman real - world baisalov problems, inputs and bangash outputs 768 do in fact exhibit corregidor a nonlinear pardos relationship. Therefore, type - hodja II enterobacteria opposites are otium expected shoka to hallmark be maurine better santangelo in sinew capturing the sense debacle of \" islamized opposition \" starsailor in conason terms resubmit of conspiring the tardy input - hakkinen output gierke relation. In afterburner the absence carnmoney of any knowledge ghozi about the problem contenting at guyomar hand, there cunoniaceae seems handgrenade to eimer be no netguide intuitive way 182.9 to calculate keoni the sigmar type - staufen II opposites. In doggart this lukes paper, mennonite we conexant introduce an sundin approach to (727) learn inuit type - II tyagi opposites from the padlock given thorbecke inputs exynos and mcvicar their outputs soferim using the scania-vabis artificial pagemill neural chimalpopoca networks (kaqchikel ANNs ). We first perform \\ emph {dongbang opposition mining} on bratslav the doubted sample data, iannello and melling then ingeniero use bushfield the mined budahn data to paparizou learn the relationship between prepares input $ iwakuma x $ and riverfront its opposite $ \\ 31.60 breve {x} $. apomixis We redlands have mrinal validated devil-may-care our algorithm diff\u00e9rance using serov various benchmark zuccarini functions mequon to barbuto compare thamarai it vire against an interned evolving unsportsmanlike fuzzy inference kutter approach that mircorp has breazell been shiners recently ois\u00edn introduced. nicasius The zwicker results show jotaro the aktiebolag better performance of liberal-minded a hamsters neural approach baj to izbasa learn the downie opposites. This will baathification create marq new defoor possibilities for obraztsova integrating rluipa oppositional schemes wetherill within existing orgyen algorithms aksener promising trajectory a potential increase in convergence speed sopa and / mesmero or accuracy.", "histories": [["v1", "Fri, 16 Sep 2016 16:19:56 GMT  (208kb,D)", "http://arxiv.org/abs/1609.05123v1", "To appear in proceedings of the 23rd International Conference on Pattern Recognition (ICPR 2016), Cancun, Mexico, December 2016"]], "COMMENTS": "To appear in proceedings of the 23rd International Conference on Pattern Recognition (ICPR 2016), Cancun, Mexico, December 2016", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["shivam kalra", "aditya sriram", "shahryar rahnamayan", "h r tizhoosh"], "accepted": false, "id": "1609.05123"}, "pdf": {"name": "1609.05123.pdf", "metadata": {"source": "CRF", "title": "LEARNING OPPOSITES USING NEURAL NETWORKS", "authors": ["Shivam Kalra", "Aditya Sriram", "Shahryar Rahnamayan", "H.R. Tizhoosh"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "A large number of problems in engineering and science are unapproachable with conventional schemes instead they are handled with intelligent stochastic techniques such as evolutionary, neural network, reinforcement and swarm-based algorithms. However, essential parameter for the end users of aforementioned intelligent algorithms is to yield the solutions within desirable accuracy in timely manner \u2013 which remains volatile and uncertain. Many heuristic methods exist to speed up the convergence rate of stochastic algorithms to enhance their viability for complex real-world problems. Opposite Based Computing (OBC) is one such heuristic method\nintroduced by Tizhoosh in [1]. The underlying idea is simultaneous consideration of guess and opposite guess, estimate and opposite estimate, parameter and opposite parameter & so on in order to make more educated decisions within the stochastic processes, that eventually results in yielding solutions quickly and accurately.\nIn essence, learning the relationship between an entity and its opposite entity for a given problem is a special case of apriori knowledge, which can be beneficial for computationally intelligent algorithms in stochastic setups. In context of machine learning algorithm, one may ask, why should effort be spent on extraction of the opposite relations when inputoutput relationship itself is not well defined? However, various research on this topic has shown that simultaneous analysis of entities and their opposites can accelerate the task in focus \u2013 since it allows the algorithm to harness the knowledge about symmetry in the solution domain thus allowing a better exploration of the solutions. Opposition-based Differential Evolution (ODE), however, seems to be the most successful oppositional inspired algorithm so far [2].\nThere have been two types of opposites defined in literature 1) type-I and 2) type-II. Generally, most learning algorithms have an objective function; mapping the relationship between inputs and their outputs \u2013 which may be known or unknown. In such scenario, type-I based learning algorithms deal with the relationship among input parameters, based on their values, without considering their relationship with the objective landscape. On contrary, type-II opposite requires a prior knowledge of the objective function. Until 2015, all papers published on using OBL employed the simple notion of type-I opposites which are conveniently, but naively defined on the input space only, making a latent linearity assumption about the problem domain. Tizhoosh and Rahnamayan [3] introduced the idea of \u201copposition mining\u201d and evolving rules to capture oppositeness in dynamic environments.\nThe paper is organized as follows: Section 2 provides a literature review on OBL. Section 3 introduces the idea to use artificial neural network (ANN) to learn the opposites, and provides an overview of type-I and type-II OBL. Finally, Section 5 provides experimental results and analysis and also a comparison of the proposed ANN approach with the evolving fuzzy inference systems, a method recently proposed in [3].\nar X\niv :1\n60 9.\n05 12\n3v 1\n[ cs\n.L G\n] 1\n6 Se\np 20\n16"}, {"heading": "2. BACKGROUND REVIEW", "text": "Roughly 10 years ago, the idea of opposition-based learning (OBL) was introduced as a generic framework to improve existing learning and optimization algorithms [1]. This approach has received a modest but growing attention by the research community resulting in improving diverse optimization and learning techniques published in several hundred papers [4]. A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].\nThe majority of learning algorithms are tailored toward approximating functions by arbitrary setting weights, activation functions, and the number of neurons in the hidden layer. The convergence would be significantly faster towards the optimal solution if these random initializations are close to the result [18]. On the contrary, if the initial estimates are far from the optimal solution \u2013 in the opposite corner of the search space, for instance, then convergence to the ideal solution will take considerably more time or can be left intractable [19]. Hence, there is a need to look simultaneously for a candidate solution in both current and opposite directions to increase the convergence speed \u2013 a learning mechanism denoted as \u201copposition-based learning\u201d [1]. The concept of OBL has touched upon the various existing algorithm, and it has proven to yield better results compared to the conventional method of determining the optimal solution. A detailed survey on applications of OBL in soft-computing is discussed by AlQuanaieer et al. in [4]. The paper discusses the integration of OBL when used for reinforcement learning, neural networks, optimization, fuzzy set theory and fuzzy c-mean clustering. A review on each algorithm states that applying OBL can be beneficial when applied in an effective manner when applications use optimization algorithm, learning algorithm, fuzzy sets and image processing.\nMany problems in optimization involve minimization or maximization of some scalar parameterized objective function, with respect to all its parameters. For such problems, OBL can be used as a heuristic technique to quickly converge to the solution within the search space by generating the candidates solutions that have \u201copposite-correlation\u201d instead of being entirely random.\nThe concept of \u201copposite-correlation\u201d can be discussed from type-I and type-II perspectives when an unknown function y = f(x1, x2, . . . , xn) needs to be learned or optimized by relying on some sample data alone.\nDefinition 1 Type-I opposite x\u0306I of input x is defined as x\u0306I = xmax +xmin\u2212x where xmax is the maximum and xmin is the minimum value of x.\nComputation of type-I opposites are easier due to its linear definition in the variable space. On the contrary, type-II\nopposition scheme requires to operate on the output space.\nDefinition 2 Type-II opposite x\u0306II of input x is defined as x\u0306II = {xi|y\u0306(xi) = ymin + ymax \u2212 y(xi)} where ymax is the maximum value, and ymin is the minimum value of y.\nType-II opposites may be difficult to incorporate in realworld problems because 1) their calculation may require apriori domain knowledge, and 2) the inverse of the function y, namely y\u0306, is not available when we are dealing with unknown functions y = f(x1, x2, . . . , xn). The focus of this paper is to develop a general framework to allow ANNs to learn the relationship among the inputs and their corresponding type-II opposites. Validation of the proposed approach is comprised of several benchmark functions. Validation results on the benchmark functions demonstrate the effectiveness of the proposed algorithm as an initial step for future developments in type-II OBL approximations using neural networks."}, {"heading": "3. THE IDEA", "text": "Type-II (or true) opposite of x, denoted with x\u0306II , is more intuitive when compared to type-I opposite in context of \u201cnon-linear\u201d functions. When looking at function y = f(x1, x2, ..., xn) in a typical machine-learning setup, one may receive the output values y for some input variables x1, x2, . . . , xn. However, the function y = f(\u00b7) itself is usually unknown otherwise there would be little justification for resorting to machine-learning tools. Instead, one has some sort of evaluation function g(\u00b7) (error, reward, fitness, etc.) that enables to assess the quality of any guess x\u03021, x\u03022, . . . , x\u0302n delivering an estimate y\u0302 of the true/desired output y.\nTizhoosh and Rahnamayan introduced the idea of opposition mining as an approach to approximate type-II opposites for training the learning algorithms using fuzzy inference systems (FIS) with evolving rules in [3]. Evolving FIS has received much attention lately [20, 21] which is now being used for modeling nonlinear dynamic systems [22] and image classification and segmentation [23, 24, 25]. However, learning opposites with evolving rules are observed to be sensitive to the parameters used and encounters difficulty in generalizing a large variety of data. The proposed method in this paper uses opposition mining for training artificial neural network to approximate the relationship between the input x and its type-II opposite x\u0306II . This methodology can, of course, be extended for various applications; hence, if a large training dataset is available, then one can apply them at once instead of incremental changes. A graphical representation of the typeII opposites, described in Definition 2, is shown in Fig. 1: 1) Given variable x, 2) The corresponding f(x) is calculated, 3) from which the opposite of f(x), namely of(x) is determined, and 4) the opposite(s) of x are found to be: ox1, ox2 and ox3. However, there are various challenges using this approach, which include: 1) the output range of y = f(\u00b7) may not be a-priori known, thereby an updated knowledge on the\noutput range of [ymin, ymax] is needed, 2) the precise output of the type-II opposite may not be present in the given data; thereby, a method to determine gaps in the dataset is needed to estimate the relationship between the input and the output, and 3) it is difficult to generalize over a high dimension/range of data. In this paper, we put forward a concise algorithm to learn opposites via neural networks \u2013 which, as outlined in Section 5, is proven to yield better results when compared to recently introduced FIS approach."}, {"heading": "4. LEARNING OPPOSITES", "text": "Neural networks with sufficient number of hidden layers can learn any bounded degree polynomials with good accuracy [26]. Therefore, ANNs make a good candidate for learning the nonlinear relationship between the inputs x and their typeII opposites x\u0306II .\nIn order to learn type-II opposites, we first need to sample the (quasi-)opposites from the given input-output data. The first stage of our algorithm is opposition mining, which provides the data that can be subsequently used by ANN for performing a nonlinear regression. One generally assumes that the more data is available the better the approximation becomes for x\u0306II = f(x) as we have more (quasi-)opposites for training the ANN. We assume that the range of input variable is known, xi \u2208 [ximin, ximax] but the range of output, yj \u2208 [yjmin, yjmax], may be apriori unknown. Since we are approximating type-II opposites, we need to generate the (quasi-)opposite data from given training data. Our approach consists of two distinct stages: Opposition Mining \u2013 The training data is sampled to establish the output boundaries. Depending on a specific oppositeness scheme, all data points in training data are processed to find (quasi-)opposites of each input. At the end of the opposite mining, we have a corresponding (quasi-)opposite (approximate of type-II opposite) for every input point in training data as outlined in Algorithm 1. There are different schemes for calculating the opposition. Given a sample of random variable x \u2208 [xmin, xmax] with mean x\u0304, the opposite of x\nAlgorithm 1 Opposition Mining Input: x inputs, y outputs & Ti opposition scheme Output: type-II opposites x\u0306II in same order as input x\n1: procedure OPPOSITION MINING(x, y, Ti) 2: ymax \u2190 max(y) 3: ymin \u2190 min(y) 4: y\u0304 \u2190 mean(y) 5: for i \u2208 [1, length(y)] do 6: yI \u2190 y(i) . Choosing opposition scheme based on value of Ti 7: if Ti = 0 then 8: oppY \u2190 ymax + ymin \u2212 yi 9: else if TI = 1 then\n10: oppY \u2190 ( yI +\nymax+ymin 2\n) %ymax\n11: else 12: oppY \u2190 2y\u0304 \u2212 yi 13: end if . In case oppY goes out of boundary range 14: if oppY /\u2208 [ymax, ymin] then 15: oppY \u2190 ymax + ymin \u2212 yi 16: end if . Getting index of element in y closest to oppY 17: oppYidx \u2190 argmin(|y \u2212 oppY |) 18: x\u0306II(i)\u2190 x(oppYidx) 19: end for return x\u0306II 20: end procedure\ncan be calculated as follows:\nT1 : x\u0306I = zmax + xmin \u2212 x (1)\nT2 : x\u0306I =\n( x+\nxmin + xmax 2\n) % xmax (2)\nT3 : x\u0306I = 2x\u0304\u2212 x (3)\nIn scheme T3, calculated opposite x\u0306I may go out of the boundaries of the variable range. Therefore, for the experiments purposes, we solved the boundary violation problem by switching the scheme to T1 whenever necessary (Algorithm 1, Line 14). It is important to note that, opposites calculated with any of the above schemes in output space when projected back on to the variable space are known as type-II (true) opposites.\nLearning the Opposites \u2013 ANN is employed to approximate the function x\u0306II = f(x) that maps input and its type-II opposite. The network is trained on the data collected from the opposition mining step, and can be retrained progressively as more data comes in or it can be used to predict the typeII opposites for a given input x. In the following sections, we report the results of some experiments to verify the accuracy of our algorithm, its superiority over existing FIS based technique and some discussion on the usefulness of type-II opposites for machine-learning algorithms."}, {"heading": "5. EXPERIMENTS AND RESULTS", "text": "We have performed two experiment series to test the various aspects of our algorithm including \u2013 opposition mining, learning type-II using ANN and prediction accuracy of the trained ANN model versus evolving fuzzy rules model. The experiments deal with the approximation accuracy of type-II opposites and application of type-II opposites for some standard optimization scenarios respectively. For comparing the approximation accuracy, we used the 8 benchmark functions for generating the data required for the opposition-mining and subsequent training of ANN and evolving fuzzy rules models; which are taken from [3]. It is important to note that, benchmarks function have been intentionally kept simple and mostly monotonic in defined ranges, in order avoid the surjective relationship of inputs and their (quasi-)opposites during the opposition mining stage, thus allowing to extract the most feasible patterns in the data to be used with learning algorithms. However, in order to learn the type-II opposites across any general non-monotonic functions, it would be required to decompose the function in question into monotonic piece-wise ranges and subsequently perform the type-II approximation procedure on each of the piece separately. We calculated the approximation error of our algorithm for every benchmark function against different oppositeness schemes. We compared the results against the recently published ones by approximating type-II using evolving fuzzy rules [3]."}, {"heading": "5.1. Comparing with Evolving Fuzzy Rules", "text": "The results for 8 benchmark functions (used in [3]) are summarized in Table 1. Green cells represent the performance of ANN is statistically significant with 95% confidence (unless otherwise stated), whereas red cells show the significance of evolving fuzzy rules based approach for the respective opposition scheme. Cells marked gray represent the best results achieved using any deployed method or opposition scheme for a given benchmark function. The error in approximation of the type-II opposite x\u0306\u2217II is inferred by comparing the value of the function at approximated opposite xII and true opposite value of the function y\u0306\u2217II at given input x. It is important to note that y\u0306\u2217II can be calculated if input x, opposition scheme Ti and function f are known:\nerror(x\u0306II) \u221d error(y\u0306II) = | \u02d8yII\u2217 \u2212 f(x\u0306II)| (4)\nThe results are reported in Table 1. Overall T1 seems to be the best oppositeness scheme. As long as ymax, ymin does not change for the given sample data, T1 and T2 schemes allow continuous training of ANN. The approximation seems to perform better overall except for the linear functions and functions with square root power. The ANN approach seems to generalize much better for the logarithm function at higher values of x where output changes much slower than change in x."}, {"heading": "5.2. Optimization Problems", "text": "In this experiment, we test three standard optimization functions which are commonly used in literature of global optimization \u2013 Ackely, Bulkin and Booth functions [3].\nAckley Function \u2013 The Ackley function is given by\nf(x1, x2) = 20\n( 1\u2212 exp ( \u22120.2 \u221a 0.5(x21 + x 2 2) )) \u2212\nexp ( 0.5 \u2217 (cos(2\u03c0x1) + cos(2\u03c0x2)) ) + exp(1)\nThe global minimum is 0 at (3, 0.5) with x1, x2 \u2208 [\u221235, 35]. Bulkin Function \u2013 The Bulkin function is given by\nf(x1, x2) = 100 \u221a ||x2 \u2212 0.01x21||+ 0.01||x1 + 10||\nThe global minimum is 0 at (\u221210, 0) with x1 \u2208 [\u221215,\u22125] and x2 \u2208 [\u22123, 3].\nBooth Function \u2013 The Booth function is given by\nf(x1, x2) = (x1 + 2x2 \u2212 7)2 + (2x1 + x2 \u2212 5)2\nThe global minimum is 0 at (1, 3) such that x1, X2 \u2208 [\u221210, 10].\nWe train ANN and evolving fuzzy rules method for type-II opposites using ns = 1000 samples from each of the benchmark function. This experiment enables to verify three majors points: 1.) test whether the fundamental statement of OBL holds, namely that \u2013 simultaneous consideration of a guess and opposite guess provides faster convergence to the solution in learning and optimization processes, 2.) test whether type-II opposites provides any advantage over type-I, and 3.) test if ANN based type-II approximation provides any superiority over recently introduced evolving fuzzy rule approach.\nTo conduct this experiment, we generate two random input samples xr1 and x r 2 and we calculate the error (distance from the global minimum). Then, we approximate the opposites x\u0306r1 and x\u0306 r 2 and calculate the error again. In order to find the solution, we chose the strategy that yields less error and continues the process. We should expect to have a reduction or no change in error at every iteration since we deliberately choose the outcome with the least error. We carry out the experiment for both type-I x\u0306rI and type-II opposites x\u0306rII generated by each of the candidate methods. By recording the average error after 0.1\u2217ns iterations for multiple runs of the experiments, we can test whether considering type-II opposite from either of the candidate methods have any statistical significance over type-I in yielding outcome closer to the global minimum. However, the focus is more on comparing the two candidate approaches for approximating type-II (proposed and evolving fuzzy rule-based), to see which one provides better estimates of type-II to bring the optimization process closer to the global optima. The results for Ackley, Booth, and Bulkin functions are shown in Table 2. The\nfirst column consists of random guesses, the second column contains type-II opposite guesses estimated by ANN, and the third column contains type-II opposite guesses using the FIS approach; the last column is type-I opposites. By performing the t-test, results of type-II opposites obtained by ANN are statistically significant compared to the FIS approach. TypeII is performing better except for the Booth function."}, {"heading": "6. CONCLUSION", "text": "Ten years since the introduction of opposition-based learning, the full potential of type-II opposites is still largely unknown. In this paper, we put forward a method for learning type-II opposites with ANNs. The core idea in this paper is to utilize the (quasi-)opposite data collected from opposition-mining to learn the relationship between input x and its type-II opposite x\u0306II using neural networks. We tested the proposed algorithms with various benchmark functions and compared it against the existing fuzzy rules-based approach. We showed the correctness of fundamental statement of OBL scheme by utilizing type-II opposites on three of the famous global optimization problems. One of the major hurdles for existing type-II approximation methods (including proposed in\nthis paper) is when the function in question is highly nonmonotonic or periodic in nature. In those circumstances, the relationship between x and x\u0306II becomes surjective, causing discontinuities in opposition mining. This makes it difficult for any learning algorithm difficult to fit such discontinuous data. There is a potential for improvement in future works where non-monotonic functions can be decomposed into monotonic piece-wise intervals; each of the intervals can then be trained separately."}, {"heading": "7. REFERENCES", "text": "[1] H. R. Tizhoosh, \u201cOpposition-based learning: a new scheme for machine intelligence,\u201d in null. IEEE, 2005, pp. 695\u2013701.\n[2] S. Rahnamayan, H. R. Tizhoosh, and M. Salama, \u201cOpposition-based differential evolution,\u201d Evolutionary Computation, IEEE Transactions on, vol. 12, no. 1, pp. 64\u201379, 2008.\n[3] H. R. Tizhoosh and S. Rahnamayan, \u201cLearning opposites with evolving rules,\u201d in Fuzzy Systems (FUZZ-\nIEEE), 2015 IEEE International Conference on. IEEE, 2015, pp. 1\u20138.\n[4] F. S. Al-Qunaieer, H. R. Tizhoosh, and S. Rahnamayan, \u201cOpposition based computinga survey,\u201d in Neural Networks (IJCNN), The 2010 International Joint Conference on. IEEE, 2010, pp. 1\u20137.\n[5] H. R. Tizhoosh, \u201cReinforcement learning based on actions and opposite actions,\u201d in Int. Conf. on Artificial Intelligence and Machine Learning, 2005, p. 9498.\n[6] M. Mahootchi, H. Tizhoosh, and K. Ponnambalam, \u201cOpposition-based reinforcement learning in the management of water resources,\u201d in Approximate Dynamic Programming and Reinforcement Learning, 2007. ADPRL 2007. IEEE International Symposium on. IEEE, 2007, pp. 217\u2013224.\n[7] H. R. Tizhoosh, \u201cOpposition-based reinforcement learning,\u201d JACIII, vol. 10, no. 4, pp. 578\u2013585, 2006.\n[8] M. Mahootchi, H. R. Tizhoosh, and K. Ponnambalam, \u201cOppositional extension of reinforcement learning techniques,\u201d Information Sciences, vol. 275, pp. 101\u2013114, 2014.\n[9] S. Rahnamayan, H. R. Tizhoosh, and M. Salama, \u201cOpposition-based differential evolution algorithms,\u201d in Evolutionary Computation, 2006. CEC 2006. IEEE Congress on. IEEE, 2006, pp. 2010\u20132017.\n[10] H. Salehinejad, S. Rahnamayan, and H. R. Tizhoosh, \u201cType-ii opposition-based differential evolution,\u201d in Evolutionary Computation (CEC), 2014 IEEE Congress on. IEEE, 2014, pp. 1768\u20131775.\n[11] S. Rahnamayan, H. R. Tizhoosh, and M. M. Salama, \u201cA novel population initialization method for accelerating evolutionary algorithms,\u201d Computers & Mathematics with Applications, vol. 53(10), pp. 1605\u20131614, 2007.\n[12] S. Rahnamayan and H. R. Tizhoosh, \u201cImage thresholding using micro opposition-based differential evolution (micro-ode),\u201d in IEEE World Congress on Computational Intelligence Evolutionary Computation, 2008, pp. 1409\u20131416.\n[13] C. Zhang, Z. Ni, Z. Wu, and L. Gu, \u201cA novel swarm model with quasi-oppositional particle,\u201d in Information Technology and Applications, 2009. IFITA\u201909. International Forum on, vol. 1, 2009, pp. 325\u2013330.\n[14] H. Jabeen, Z. Jalil, and A. R. Baig, \u201cOpposition based initialization in particle swarm optimization (o-pso),\u201d in Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers, 2009, pp. 2047\u20132052.\n[15] M. Kaucic, \u201cA multi-start opposition-based particle swarm optimization algorithm with adaptive velocity for bound constrained global optimization,\u201d Journal of Global Optimization, vol. 55(1), pp. 165\u2013188, 2013.\n[16] M. Rashid and A. R. Baig, \u201cImproved opposition-based pso for feedforward neural network training,\u201d in Information Science and Applications (ICISA), International Conference on, 2010, pp. 1\u20136.\n[17] M. Yaghini, M. M. Khoshraftar, and M. Fallahi, \u201cHiopga: a new hybrid metaheuristic algorithm to train feedforward neural networks for prediction,\u201d in Proceedings of the International Conference on Data Mining, 2011, pp. 18\u201321.\n[18] Q. Xu, L. Wang, N. Wang, X. Hei, and L. Zhao, \u201cA review of opposition-based learning from 2005 to 2012,\u201d Engineering Applications of Artificial Intelligence, vol. 29, pp. 1\u201312, 2014.\n[19] M. Ventresca and H. R. Tizhoosh, \u201cA diversity maintaining population-based incremental learning algorithm,\u201d Information Sciences, vol. 178(21), pp. 4038\u20134056, 2008.\n[20] P. Angelov, E. Lughofer, and X. Zhou, \u201cEvolving fuzzy classifiers using different model architectures,\u201d Fuzzy Sets and Systems, vol. 159(23), pp. 3160\u20133182, 2008.\n[21] P. P. Angelov and X. Zhou, \u201cEvolving fuzzy-rule-based classifiers from data streams,\u201d Fuzzy Systems, IEEE Transactions on, vol. 16, no. 6, pp. 1462\u20131475, 2008.\n[22] J.-C. de Barros and A. L. Dexter, \u201cOn-line identification of computationally undemanding evolving fuzzy models,\u201d Fuzzy sets and systems, vol. 158(18), pp. 1997\u2013 2012, 2007.\n[23] E. Lughofer, \u201cOn-line evolving image classifiers and their application to surface inspection,\u201d Image and Vision Computing, vol. 28(7), pp. 1065\u20131079, 2010.\n[24] A. A. Othman and H. R. Tizhoosh, \u201cEvolving fuzzy image segmentation,\u201d in Fuzzy Systems, IEEE International Conference on, 2011, pp. 1603\u20131609.\n[25] A. A. Othman, H. R. Tizhoosh, and F. Khalvati, \u201cEfisevolving fuzzy image segmentation,\u201d Fuzzy Systems, IEEE Transactions on, vol. 22(1), pp. 72\u201382, 2014.\n[26] A. Andoni, R. Panigrahy, G. Valiant, and L. Zhang, \u201cLearning polynomials with neural networks,\u201d in Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1908\u20131916."}], "references": [{"title": "Opposition-based learning: a new scheme for machine intelligence", "author": ["H.R. Tizhoosh"], "venue": "null. IEEE, 2005, pp. 695\u2013701.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Opposition-based differential evolution", "author": ["S. Rahnamayan", "H.R. Tizhoosh", "M. Salama"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 12, no. 1, pp. 64\u201379, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning opposites with evolving rules", "author": ["H.R. Tizhoosh", "S. Rahnamayan"], "venue": "Fuzzy Systems (FUZZ- 5  To appear in proceedings of the 23rd International Conference on Pattern Recognition (ICPR 2016), Cancun, Mexico, December 2016 IEEE), 2015 IEEE International Conference on. IEEE, 2015, pp. 1\u20138.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Opposition based computinga survey", "author": ["F.S. Al-Qunaieer", "H.R. Tizhoosh", "S. Rahnamayan"], "venue": "Neural Networks (IJCNN), The 2010 International Joint Conference on. IEEE, 2010, pp. 1\u20137.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Reinforcement learning based on actions and opposite actions", "author": ["H.R. Tizhoosh"], "venue": "Int. Conf. on Artificial Intelligence and Machine Learning, 2005, p. 9498.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Opposition-based reinforcement learning in the management of water resources", "author": ["M. Mahootchi", "H. Tizhoosh", "K. Ponnambalam"], "venue": "Approximate Dynamic Programming and Reinforcement Learning, 2007. AD- PRL 2007. IEEE International Symposium on. IEEE, 2007, pp. 217\u2013224.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Opposition-based reinforcement learning", "author": ["H.R. Tizhoosh"], "venue": "JACIII, vol. 10, no. 4, pp. 578\u2013585, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Oppositional extension of reinforcement learning techniques", "author": ["M. Mahootchi", "H.R. Tizhoosh", "K. Ponnambalam"], "venue": "Information Sciences, vol. 275, pp. 101\u2013114, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Opposition-based differential evolution algorithms", "author": ["S. Rahnamayan", "H.R. Tizhoosh", "M. Salama"], "venue": "Evolutionary Computation, 2006. CEC 2006. IEEE Congress on. IEEE, 2006, pp. 2010\u20132017.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Type-ii opposition-based differential evolution", "author": ["H. Salehinejad", "S. Rahnamayan", "H.R. Tizhoosh"], "venue": "Evolutionary Computation (CEC), 2014 IEEE Congress on. IEEE, 2014, pp. 1768\u20131775.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel population initialization method for accelerating evolutionary algorithms", "author": ["S. Rahnamayan", "H.R. Tizhoosh", "M.M. Salama"], "venue": "Computers & Mathematics with Applications, vol. 53(10), pp. 1605\u20131614, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Image thresholding using micro opposition-based differential evolution (micro-ode)", "author": ["S. Rahnamayan", "H.R. Tizhoosh"], "venue": "IEEE World Congress on Computational Intelligence Evolutionary Computation, 2008, pp. 1409\u20131416.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "A novel swarm model with quasi-oppositional particle", "author": ["C. Zhang", "Z. Ni", "Z. Wu", "L. Gu"], "venue": "Information Technology and Applications, 2009. IFITA\u201909. International Forum on, vol. 1, 2009, pp. 325\u2013330.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Opposition based initialization in particle swarm optimization (o-pso)", "author": ["H. Jabeen", "Z. Jalil", "A.R. Baig"], "venue": "Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers, 2009, pp. 2047\u20132052.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "A multi-start opposition-based particle swarm optimization algorithm with adaptive velocity for bound constrained global optimization", "author": ["M. Kaucic"], "venue": "Journal of Global Optimization, vol. 55(1), pp. 165\u2013188, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Improved opposition-based pso for feedforward neural network training", "author": ["M. Rashid", "A.R. Baig"], "venue": "Information Science and Applications (ICISA), International Conference on, 2010, pp. 1\u20136.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Hiopga: a new hybrid metaheuristic algorithm to train feedforward neural networks for prediction", "author": ["M. Yaghini", "M.M. Khoshraftar", "M. Fallahi"], "venue": "Proceedings of the International Conference on Data Mining, 2011, pp. 18\u201321.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "A review of opposition-based learning from 2005 to 2012", "author": ["Q. Xu", "L. Wang", "N. Wang", "X. Hei", "L. Zhao"], "venue": "Engineering Applications of Artificial Intelligence, vol. 29, pp. 1\u201312, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A diversity maintaining population-based incremental learning algorithm", "author": ["M. Ventresca", "H.R. Tizhoosh"], "venue": "Information Sciences, vol. 178(21), pp. 4038\u20134056, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Evolving fuzzy classifiers using different model architectures", "author": ["P. Angelov", "E. Lughofer", "X. Zhou"], "venue": "Fuzzy Sets and Systems, vol. 159(23), pp. 3160\u20133182, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Evolving fuzzy-rule-based classifiers from data streams", "author": ["P.P. Angelov", "X. Zhou"], "venue": "Fuzzy Systems, IEEE Transactions on, vol. 16, no. 6, pp. 1462\u20131475, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "On-line identification of computationally undemanding evolving fuzzy models", "author": ["J.-C. de Barros", "A.L. Dexter"], "venue": "Fuzzy sets and systems, vol. 158(18), pp. 1997\u2013 2012, 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "On-line evolving image classifiers and their application to surface inspection", "author": ["E. Lughofer"], "venue": "Image and Vision Computing, vol. 28(7), pp. 1065\u20131079, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Evolving fuzzy image segmentation", "author": ["A.A. Othman", "H.R. Tizhoosh"], "venue": "Fuzzy Systems, IEEE International Conference on, 2011, pp. 1603\u20131609.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Efisevolving fuzzy image segmentation", "author": ["A.A. Othman", "H.R. Tizhoosh", "F. Khalvati"], "venue": "Fuzzy Systems, IEEE Transactions on, vol. 22(1), pp. 72\u201382, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning polynomials with neural networks", "author": ["A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1908\u20131916. 6", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Opposite Based Computing (OBC) is one such heuristic method introduced by Tizhoosh in [1].", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Opposition-based Differential Evolution (ODE), however, seems to be the most successful oppositional inspired algorithm so far [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Tizhoosh and Rahnamayan [3] introduced the idea of \u201copposition mining\u201d and evolving rules to capture oppositeness in dynamic environments.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "Finally, Section 5 provides experimental results and analysis and also a comparison of the proposed ANN approach with the evolving fuzzy inference systems, a method recently proposed in [3].", "startOffset": 186, "endOffset": 189}, {"referenceID": 0, "context": "Roughly 10 years ago, the idea of opposition-based learning (OBL) was introduced as a generic framework to improve existing learning and optimization algorithms [1].", "startOffset": 161, "endOffset": 164}, {"referenceID": 3, "context": "This approach has received a modest but growing attention by the research community resulting in improving diverse optimization and learning techniques published in several hundred papers [4].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 115, "endOffset": 127}, {"referenceID": 5, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 115, "endOffset": 127}, {"referenceID": 6, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 115, "endOffset": 127}, {"referenceID": 7, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 115, "endOffset": 127}, {"referenceID": 8, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 153, "endOffset": 168}, {"referenceID": 9, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 153, "endOffset": 168}, {"referenceID": 10, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 153, "endOffset": 168}, {"referenceID": 11, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 153, "endOffset": 168}, {"referenceID": 12, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 190, "endOffset": 202}, {"referenceID": 13, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 190, "endOffset": 202}, {"referenceID": 14, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 190, "endOffset": 202}, {"referenceID": 15, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 224, "endOffset": 236}, {"referenceID": 16, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 224, "endOffset": 236}, {"referenceID": 17, "context": "A few algorithms have been reported to employ \u201coppositeness\u201d in their processing, including reinforcement learning [5, 6, 7, 8], evolutionary algorithms [9, 10, 11, 12], swarm-based methods [13, 14, 15], and neural networks [16, 17, 18].", "startOffset": 224, "endOffset": 236}, {"referenceID": 17, "context": "The convergence would be significantly faster towards the optimal solution if these random initializations are close to the result [18].", "startOffset": 131, "endOffset": 135}, {"referenceID": 18, "context": "On the contrary, if the initial estimates are far from the optimal solution \u2013 in the opposite corner of the search space, for instance, then convergence to the ideal solution will take considerably more time or can be left intractable [19].", "startOffset": 235, "endOffset": 239}, {"referenceID": 0, "context": "Hence, there is a need to look simultaneously for a candidate solution in both current and opposite directions to increase the convergence speed \u2013 a learning mechanism denoted as \u201copposition-based learning\u201d [1].", "startOffset": 207, "endOffset": 210}, {"referenceID": 3, "context": "in [4].", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "Tizhoosh and Rahnamayan introduced the idea of opposition mining as an approach to approximate type-II opposites for training the learning algorithms using fuzzy inference systems (FIS) with evolving rules in [3].", "startOffset": 209, "endOffset": 212}, {"referenceID": 19, "context": "Evolving FIS has received much attention lately [20, 21] which is now being used for modeling nonlinear dynamic systems [22] and image classification and segmentation [23, 24, 25].", "startOffset": 48, "endOffset": 56}, {"referenceID": 20, "context": "Evolving FIS has received much attention lately [20, 21] which is now being used for modeling nonlinear dynamic systems [22] and image classification and segmentation [23, 24, 25].", "startOffset": 48, "endOffset": 56}, {"referenceID": 21, "context": "Evolving FIS has received much attention lately [20, 21] which is now being used for modeling nonlinear dynamic systems [22] and image classification and segmentation [23, 24, 25].", "startOffset": 120, "endOffset": 124}, {"referenceID": 22, "context": "Evolving FIS has received much attention lately [20, 21] which is now being used for modeling nonlinear dynamic systems [22] and image classification and segmentation [23, 24, 25].", "startOffset": 167, "endOffset": 179}, {"referenceID": 23, "context": "Evolving FIS has received much attention lately [20, 21] which is now being used for modeling nonlinear dynamic systems [22] and image classification and segmentation [23, 24, 25].", "startOffset": 167, "endOffset": 179}, {"referenceID": 24, "context": "Evolving FIS has received much attention lately [20, 21] which is now being used for modeling nonlinear dynamic systems [22] and image classification and segmentation [23, 24, 25].", "startOffset": 167, "endOffset": 179}, {"referenceID": 2, "context": "Type-II opposites [Adopted from [3]].", "startOffset": 32, "endOffset": 35}, {"referenceID": 25, "context": "Neural networks with sufficient number of hidden layers can learn any bounded degree polynomials with good accuracy [26].", "startOffset": 116, "endOffset": 120}, {"referenceID": 2, "context": "For comparing the approximation accuracy, we used the 8 benchmark functions for generating the data required for the opposition-mining and subsequent training of ANN and evolving fuzzy rules models; which are taken from [3].", "startOffset": 220, "endOffset": 223}, {"referenceID": 2, "context": "We compared the results against the recently published ones by approximating type-II using evolving fuzzy rules [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "The results for 8 benchmark functions (used in [3]) are summarized in Table 1.", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "In this experiment, we test three standard optimization functions which are commonly used in literature of global optimization \u2013 Ackely, Bulkin and Booth functions [3].", "startOffset": 164, "endOffset": 167}], "year": 2016, "abstractText": "Many research works have successfully extended algorithms such as evolutionary algorithms, reinforcement agents and neural networks using \u201copposition-based learning\u201d (OBL). Two types of the \u201copposites\u201d have been defined in the literature, namely type-I and type-II. The former are linear in nature and applicable to the variable space, hence easy to calculate. On the other hand, type-II opposites capture the \u201coppositeness\u201d in the output space. In fact, type-I opposites are considered a special case of type-II opposites where inputs and outputs have a linear relationship. However, in many real-world problems, inputs and outputs do in fact exhibit a nonlinear relationship. Therefore, type-II opposites are expected to be better in capturing the sense of \u201copposition\u201d in terms of the input-output relation. In the absence of any knowledge about the problem at hand, there seems to be no intuitive way to calculate the type-II opposites. In this paper, we introduce an approach to learn type-II opposites from the given inputs and their outputs using the artificial neural networks (ANNs). We first perform opposition mining on the sample data, and then use the mined data to learn the relationship between input x and its opposite x\u0306. We have validated our algorithm using various benchmark functions to compare it against an evolving fuzzy inference approach that has been recently introduced. The results show the better performance of a neural approach to learn the opposites. This will create new possibilities for integrating oppositional schemes within existing algorithms promising a potential increase in convergence speed and/or accuracy.", "creator": "LaTeX with hyperref package"}}}