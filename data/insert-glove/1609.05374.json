{"id": "1609.05374", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2016", "title": "Online Learning of Combinatorial Objects via Extended Formulation", "abstract": "lentivirus The merola convex hull of $ mahaweli n $ - novarro symbol single-edged Huffman pbhi trees is fara known sidelnikov to pellorneidae have exponentially akritas many facets / constraints. e-sahaba This orvil makes the feasa standard on - 59-39 line half-open learning techniques palilula for learning boys-only Huffman trees impractical, since northen they ozdemir use 97.30 multiplicative cranley updates followed surp by ahuntsic-cartierville projections to satisfy all of the taklung constraints. lovable However, there beru are imron general lampshades extended formulation techniques that acclimatization encode navalar the stanleys convex hull of anifah Huffman anti-capitalist trees tablemates as a chulack polytope in eulogised a higher self-professed dimensional it\u014d space bibulus with aerofoil only ouzou polynomially p-39s many facets. ipsilateral This extended \u0101tman formulation ironclads methodology pre-sales can also hafun be used to encode the $ n $ - mabee element permutahedron in $ laggardly O (rgl n \\ nonofficial log 13:57 n) $ dimensions with only macedonian a glengormley polynomial grindley number saadia of facets. We develop piggybacking a general roadway technique olivio for converting warthogs these tyre extended formulations into 2-for-2 efficient 19-meter on - 10-piece line drewnowski algorithms with good rabinal relative loss bounds. The hoopoe resulting algorithms have fetac nearly blackfield the theikdi same elaheh regret bloedel bounds grodzisk as usborne state of the 1994-2007 art algorithms tsiolkovsky for nadon permutations, mesic and meffert are the manipulative first gucheng efficient yoani algorithms poignance for the healthfully on - line intercooler learning of offsides Huffman trees.", "histories": [["v1", "Sat, 17 Sep 2016 18:38:46 GMT  (26kb)", "http://arxiv.org/abs/1609.05374v1", null], ["v2", "Tue, 21 Feb 2017 19:51:28 GMT  (218kb,D)", "http://arxiv.org/abs/1609.05374v2", null], ["v3", "Thu, 6 Apr 2017 16:28:45 GMT  (262kb,D)", "http://arxiv.org/abs/1609.05374v3", null], ["v4", "Thu, 8 Jun 2017 01:49:06 GMT  (218kb,D)", "http://arxiv.org/abs/1609.05374v4", null], ["v5", "Mon, 30 Oct 2017 20:33:11 GMT  (40kb)", "http://arxiv.org/abs/1609.05374v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["holakou rahmanian", "s v n vishwanathan", "david p helmbold"], "accepted": false, "id": "1609.05374"}, "pdf": {"name": "1609.05374.pdf", "metadata": {"source": "CRF", "title": "Extended Formulation for Online Learning of Combinatorial Objects", "authors": ["Holakou Rahmanian"], "emails": ["holakou@ucsc.edu", "vishy@ucsc.edu", "dph@ucsc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n05 37\n4v 1\n[ cs\n.L G"}, {"heading": "1 Introductions", "text": "This paper introduces a general methodology for developing efficient and effective on-line learning algorithms over combinatorial structures. Examples include learning the best permutation of a set of elements for scheduling or assignment problems, or learning the best Huffman tree for compressing sequences of symbols. On-line learning algorithms are being successfully applied to an increasing variety of problems, so it is important to have good tools and techniques for creating good algorithms that match the particular problem at hand.\nThe on-line learning setting proceeds in a series of trials where the algorithm makes a prediction or takes an action associated with a combinatorial object in the space and then receives the loss of its choice in such a way that the loss of any of the possible combinatorial objects can be easily computed. The algorithm can then update its internal representation based on this feedback and the process moves on to the next trial. Unlike batch learning settings, there is no assumed distribution from which losses are randomly drawn, instead the losses are drawn adversarially. In general, an adversary can force arbitrarily large loss on the algorithm, so instead of measuring the algorithm\u2019s performance by the total loss incurred, the algorithm is measured by its regret, the amount of loss the algorithm incurs above that of the single best predictor in some comparator class. Usually the comparator class is the class of predictors defined by the combinatorial space being learned. To\nmake the setting concrete, consider the case of learning Huffman trees. In each trial, the algorithm would predict a Huffman tree, and then obtain a sequence of symbols to be encoded. The loss of the algorithm on that trial can be the inner product of any [0, 1]n loss vector and the sequence of code lengths of the symbols. Using frequencies as the loss vector, the loss will be the average code length of the sequence. Over a series of trials, the regret of the algorithm is the difference in combined average code lengths between the on-line algorithm and the single best Huffman tree chosen in hindsight. Therefore the regret of the algorithm can be viewed as the cost of not knowing the best combinatorial object ahead of time. With proper tuning, the regret is typically logarithmic in the number of combinatorial objects.\nOne way to create algorithms for these combinatorial problems is to use one of the well-known socalled \u201cexperts algorithms\u201d like weighted majority [15] or hedge [5] with each combinatorial object treated as an \u201cexpert\u201d. However, this requires explicitly keeping track of one weight for each of the exponentially many combinatorial objects, and thus results in an inefficient algorithm. Furthermore, it also causes an additional loss range factor in regret bounds as well. There has been much work on creating efficient algorithms that implicitly encode the weights over the set of combinatorial objects using a concise representations. For example, many distributions over the 2n subsets of n elements can be encoded by the probability of including each of the n elements. In addition to subsets, such work includes permutations [9, 21], paths [14, 19], and k-sets [20]. The component hedge algorithm of Koolen, Warmuth, and Kivinen [13] is a powerful generic technique when the implicit encodings are suitably simple.\nThe component hedge algorithm works by performing multiplicative updates on the parameters of its implicit representation. However, the implicit representation is typically constrained to lie in a convex polytope. Therefore Bregman projections are used after the update to return the implicit representation to the desired polytope. The technique for proving good relative loss bounds is to, on each trial, relate the excess loss of the algorithm to its movement (from both update and projection) towards the implicit representation of an arbitrary comparator in the class. Note that this process is only efficient when there are simply a small (polynomial) number of constraints on the implicit representations.\nMathematicians have studied the problem of concisely specifying the convex hulls of complicated combinatorial structures like permutations and trees using few constraints. They have developed a powerful technique of representing these polytopes as a linear projection of a higher-dimensional polyhedron using extended formulations so that the polytope description has way less (polynomial instead of exponential) constraints. Unfortunately component hedge techniques do not apply to these extended formulations.\nOur main contribution is a general methodology for creating efficient and effective on-line learning algorithms over combinatorial structures that exploit these extended formulations. This methodology creates new on-line leaning algorithms over structures like permutations that already have efficient algorithms, as well as efficient algorithms over structures like Huffman Trees that are not suitable for component hedge techniques and did not have efficient on-line learning algorithms.\nOur methodology with extended formulations includes a novel way of producing predictions directly from the algorithm\u2019s implicit representation. Previous techniques require that the algorithm perform a potentially expensive decomposition of its implicit representation into a convex combination of corners of its convex polytope, and then samples one of these corners to use as its prediction. In contrast, we can sample directly from the algorithm\u2019s implicit representation without require a decomposition step.\nThe remainder of the paper is organized as follows. In Section 2, we discuss existing work in area of online learning generally, and over structured concepts specifically, as well as extended formulation techniques. Section 3 explains the extended formulation used in our setting by reviewing the work by Kaibel and Pashkovich [12]. We then propose our algorithm along its technical steps in Section 4. Finally, Section 5 concludes with contrasting our approach to existing ones and describing directions of future work."}, {"heading": "2 Related Work", "text": "On-line learning is a rich and vibrant area, see [3] for a textbook treatment. The implicit representations for structured concepts (sometimes called \u2018indirect representations\u2019) have been used for a variety of problems [7, 8, 16, 18, 19]. In the case of permutations [9, 21] and component hedge [13] the implicit representations seem to be better matched with the combinatorial structure than the explicit representation, allowing not only decreased running time but also the proof of better bounds. Our more general methodology with extended formulations also has this advantage, although perhaps to a lesser extent than the permutation-specific algorithms.\nAs in [21], the loss family provided in our approach is linear over the first order representation of the objects [4]. Concretely, for permutations of n items, we work with vectors v \u2208 Rn in which each of the elements of {1, 2, . . . , n} appears exactly once. Also for Huffman trees of n symbols, we work with vectors v \u2208 Rn in which vi indicates the depth of the node corresponding to symbol i in the coding tree. In contrast, [9] works with the second order representation, and consequently losses, which is a more general loss family (See [21] for comparison).\nThere have been several works aimed at efficiently describing the polytope of different combinatorial objects like permutations [6] and Huffman trees [17]. Discovered by the combinatorial optimization community, extended formulation, on which our results rely, is a general methodology to nicely describe combinatorial polyhedra [11, 12]."}, {"heading": "3 Extended Formulation", "text": "There are many combinatorial objects whose polytope cannot be described other than using exponentially many facets in its original space (e.g. see [17]). In order to have more efficient algorithms, there have been several efforts in the field of combinatorial optimization towards describing these polytopes in some other spaces. In recent years, the concept of representing these polytopes as a linear projection of a higher-dimensional polyhedron \u2013 which is known as extended formulation \u2013 has received significant attention. There are many combinatorial objects whose associated polyhedra can be described using small numbers of facets by projections of higher dimensional polyhedra comparing to the original space. See [11] for some of the tools for constructing such extended formulations. In the following subsections, we first overview the work by Kaibel and Pashkovich [12], and then extract the formulation fitting to our methodology."}, {"heading": "3.1 Constructing Extended Formulation from Reflection Relations", "text": "One of the tools to construct polynomial size extended formulations is the framework developed by [12] using reflection relations. The basic idea is to start with a corner of the polytope (e.g. a permutation) and then create a sequence of reflections through hyperplanes (e.g. swapping a pair of elements) so that any corner of the polytope can be generated by applying a subsequence of the reflections to the canonical corner. The convex hull is then generated by allowing \u201cpartial reflections\u201d (i.e. containing the entire line segment connecting the original point and its reflection). Any point in the convex hull is then created by a sequence of partial reflections, and can be encoded by a sequence of variables indicating how much of each reflection was used. In fact, there is no need to start with a single corner, one could consider passing an entire polytope as an input through the sequence of (partial) reflections to generate a new polytope.\nThis framework provides an inductive construction of higher dimensional polytopes via sequence of reflection relations (see Theorem 1 in [12]). Concretely, let Pnobj be the polytope of a given combinatorial object of size n (i.e. the convex hull of H \u2013 the set of all vectors of the combinatorial object in Rn). The typical approach is to properly embed Pnobj \u2282 Rn into P\u0302nobj \u2282 Rn+1, and then feed it through an appropriate sequence of reflection relations as an input polytope in order to obtain an extended formulation for Pn+1obj \u2282 Rn+1. Theorem 1 in [12] provides the sufficient conditions for the correctness of this procedure.\nBy doing this inductive step, there will be additional variables and inequalities in the formulations. In fact, for each reflection relation, there will be one additional variable indicating the extent to which the reflection occurs, and two additional inequalities indicating the two extreme cases of complete reflection and remaining unchanged. Therefore, if polynomially many reflection relations\nare used to go from n to n + 1, then we can construct an extended formulation of polynomial size for Pnobj for all n."}, {"heading": "3.2 Extended Formulation of Objects Closed under Re-Ordering", "text": "Assume we want to construct an extended formulation for the class of combinatorial objects which is closed under any re-ordering. Note that Huffman trees and \u2013 trivially \u2013 permutations belong to such class of objects. In these cases, for reflection relations, one can use hyperplanes going through the origin with normal vector of form ek \u2212 e\u2113 which makes reflections as swapping kth and \u2113th elements. Now let us figure out the form of extended formulation in this particular case. First, we find the additional variable along with two additional inequalities associated with this reflection relation. Concretely, assume v \u2208 Rn is going through this reflection relation and v\u2032 \u2208 Rn is the output. Since v\u2032 must fall into the line segment connecting v and its reflection via the hyperplane, we have \u2203x \u2208 R v\u2032 \u2212 v = x (ek \u2212 e\u2113) and \u3008ek \u2212 e\u2113,v\u3009 \u2264 \u3008ek \u2212 e\u2113,v\u2032\u3009 \u2264 \u2212 \u3008ek \u2212 e\u2113,v\u3009. Using these two properties, one can obtain the relation between v\u2032 and v via a linear transformation given the additional variable x as well as the constraints enforced on x:\nv\u2032 = m x+ v, mi =\n\n\n\n1 i = k \u22121 i = \u2113 0 otherwise\n(1)\n0 \u2264 x \u2264 v\u2113 \u2212 vk1 (2)\nNotice that x indicates the value that is being swapped between kth and \u2113th elements which can go from zero (remaining unchanged) to the maximum swap capacity (complete swap). Also note that the sequence of reflection relations can be viewed as a sequence of comparators in a network [1]. In the case of permutations, the comparators associated with combined sequences of reflection relations form bubble-sort networks as in each inductive step a sequence of length n of corresponding comparators must bubble-out the largest element. For Huffman trees, however, the sequence of comparators used in the inductive step consists of two sweeps of the elements to bubble-out the two largest elements which are associated with the two deepest leaves in the tree [12]. Furthermore, since the construction of the polytope is inductive, the order of reflection relations is the opposite of the direction of the network of comparators. In fact, one can observe that the extended formulation of a Huffman tree/permutation is the swap values of the comparators in the sorting network. Moreover, a mixture of Huffman trees/permutations can be represented by partial swap values in the comparators (See Figure 1).\nNow we can express the extended formulation using the results above. Suppose we are using m reflection relations in total. Then starting from an anchor point c and applying Equation (1) successively, we obtain the affine transformation connecting the extended formulation space X and original space V :\nv = M x+ c, v, c \u2208 V \u2282 Rn, x \u2208 X \u2282 Rm, M \u2208 {\u22121, 0, 1}n\u00d7m\nThe anchor point c is [1, 2, . . . , n]T and [1, 2, . . . , n\u2212 2, n\u2212 1, n\u2212 1]T in case of permutation and Huffman tree, respectively. Also using all the inequalities from the reflection relations as in (2), the\n1v\u2113 \u2212 vk may already be in terms of previous variables in extended formulation.\nextended formulation space X can be described as X = {x \u2208 Rm : Ax \u2264 b and x \u2265 0} in which A \u2208 Rm\u00d7m and b \u2208 Rm. See the Appendix for some theoretical results of the structure of A and b. Note: In description above, for simplicity, we explained the basic inductive approach of building extended formulation for permutations and Huffman trees. However, one can build an extended formulation both for permutations and Huffman tree non-inductively and more efficiently. Concretely, for permutations one can construct an extended formulation based on an arbitrary sorting network. Similarly, by using an arbitrary sorting network along with some additional comparators and simple linear maps, an extended formulation for Huffman trees can also be built (See Theorem 7 in [12])."}, {"heading": "3.3 Combinatorial Polytope Description in Augmented Formulation", "text": "Even though the polytope can now be described using polynomial number of facets in extended formulation x \u2208 X , it is not natural to define linear loss over the elements of x. However, one can concatenate the original formulation v \u2208 V to the extended formulation x \u2208 X , so that it is possible to not only efficiently describe the polytope, but also provide meaningful losses such as average code length and sum of completion times in Huffman trees and permutations, respectively. So given a loss vector \u2113 \u2208 [0, 1]n defined over V , we can work with (V ,X ) as shown in the left column of Table 1.\nIn addition, in order to have the comfort of working with affine subspaces2, we add a positive slack vector \u03bb to turn all inequalities into equalities. As a result, we define the augmented formulation space W (See the right column of the Table 1). Observe that there exists a duality between x and \u03bb. Concretely, xi +\u03bbi indicates the swap value capacity at ith reflection relation and comparator. Also xi \u03bbi = 0 for all i \u2208 [m] for any arbitrary pure Huffman trees/permutations since either comparator either passes or swaps its input elements."}, {"heading": "4 Algorithm", "text": "In this section, we propose our algorithm Extended-Learn, discuss its technical steps, and prove its regret bounds. The main idea of our algorithm is to maintain a distribution over all instances of the objects by keeping track of an evolving point wt = (vt,xt,\u03bbt) in the augmented formulation space W through trials t = 1, . . . , T . The main structure of Extended-Learn is show in Algorithm 1. Similar to [13], our algorithm consists of three main technical parts:\n1. Prediction: Predict with an instance \u03b3t\u22121 \u2208 H such that E [ \u03b3t\u22121 ] = vt\u22121\n2. Update: Update the mixture wt\u22121 to w\u0302t\u22121 according to the incurred loss multiplicatively.\n3. Projection: Project the updated mixture w\u0302t\u22121 back to the polytope W and obtain wt.\nIn the prediction step, which is discussed in 4.1, we draw an instance probabilistically from the distribution latent in wt\u22121 \u2208 W such that it has the same expected value as vt\u22121 \u2208 V . For the update step, having defined Lt = (\u2113t,0,0), the updated w\u0302t\u22121 is obtained from a trade-off between the linear loss and the unnormalized relative entropy [13]. :\nw\u0302 t\u22121 = argmin\nw\u2208Rn+2m\n\u2206(w||wt\u22121) + \u03b7w \u00b7 Lt\n2The reader will see having affine subspaces allows us to take an efficient projection approach, namely iterative Bregman projections, in Section 4.2\nIt is fairly straight-forward to see:\n\u2200i \u2208 [n+ 2m], w\u0302t\u22121i = wt\u22121i e\u2212\u03b7 L t i \u2212\u2192\n\n \n \n\u2200i \u2208 [n], v\u0302t\u22121i = vt\u22121i e\u2212\u03b7 \u2113 t i \u2200i \u2208 [m], x\u0302t\u22121i = xt\u22121i \u2200i \u2208 [m], \u03bb\u0302t\u22121i = \u03bb\u0302t\u22121i\nIn the projection step, for which we propose an iterative approach in 4.2, we obtain wt, the Bregman projection of w\u0302t\u22121 back to the augmented formulation space W ,\nwt = argmin w\u2208W \u2206(w||w\u0302t\u22121)\nAlgorithm 1 Extended-Learn\n1: w0 \u2190 q \u2208 W \u2013 a proper prior distribution discussed in 4.3 2: For t = 1, . . . , T 3: wt\u22121 = (vt\u22121,xt\u22121,\u03bbt\u22121) \u2208 W 4: Execute Prediction(wt\u22121) and get a random instance \u03b3t\u22121 \u2208 H s.t. E [ \u03b3t\u22121 ] = vt\u22121 5: Incur a loss \u03b3t\u22121 \u00b7 \u2113t 6: Update w\u0302t\u22121 = (v\u0302t\u22121, x\u0302t\u22121, \u03bb\u0302 t\u22121 ): 7: Keep unchanged x\u0302t\u22121 \u2190 xt\u22121, \u03bb\u0302t\u22121 \u2190 \u03bbt\u22121 8: Update v\u0302t\u22121 as v\u0302t\u22121i \u2190 vt\u22121i e\u2212\u03b7 \u2113 t i 9: Execute Projection(w\u0302t\u22121) and obtain wt which is wt = argmin w\u2208W \u2206(w||w\u0302t\u22121)"}, {"heading": "4.1 Prediction", "text": "In this subsection, we describe how to probabilisticly predict with an instance such that it has the same expected value as the mixture vector of which we are keeping track. First we propose an algorithm for decomposing any mixture point into convex combination of instances. Despite its inefficiency, it leads us to another algorithm which does efficiently produce proper predictions.\nInefficient Decomposition The decomposition algorithm is shown in Algortihm 2. The main idea of the algorithm is to exploit the notion of partial swaps in the comparators corresponding to reflection relations in our extended formulation. In other words, at each comparator, we want to decompose based on the extent to which we used swap capacity. One can show that Algorithm 2, despite its inefficiency, results in valid convex combination of instances (See the Appendix for the proof):\nLemma 1. (i) Given (v,x,\u03bb), Algorithm 2 generates a decomposition set S of (\u03b3, p\u03b3) pairs with \u03b3 \u2208 H such that v = \u2211(\u03b3,p\u03b3 )\u2208S p\u03b3 \u03b3. (ii) The time complexity of Algorithm 2 is O(2 m).\nAlgorithm 2 Extended-Decomposition\n1: Input: (v,x,\u03bb) \u2208 W \u2286 Rn+2m 2: Output: A decomposition set S of (instance, probability)-pairs \u2013 (\u03b3, p\u03b3) 3: S \u2190 {(c, 1)} 4: for i = 1 to m do 5: if xi = 0 then 6: continue 7: else if \u03bbi = 0 then 8: Swap the associated coordinates for all s \u2208 S 9: else\n10: for (\u03b3, p\u03b3) \u2208 S do 11: pi \u2190 xixi+\u03bbi 12: Remove (\u03b3, p\u03b3) from S 13: Add (\u03b3, (1 \u2212 pi)p\u03b3) and (Mi(\u03b3), pip\u03b3) to S 3 14: end for 15: end if 16: end for 17: return S\nEfficient Prediction In order to achieve efficiency, despite the vastly common idea in the literature [9, 13, 20, 21], one can avoid decomposition and do prediction directly. To this purpose, in Algorithm 2, we replace the idea of partial swaps with probabilistic swaps. Algorithm 3 describes this idea in concrete terms. It can be shown (see the proof in the Appendix) that Algorithm 3 does proper prediction efficiently:\nLemma 2. (i) Given (v,x,\u03bb), the Algorithm 3 generates a \u03b3 \u2208 H such that E[\u03b3] = v. (ii) The time complexity of Algorithm 3 is O(m).\nAlgorithm 3 Extended-Prediction\n1: Input: (v,x,\u03bb) \u2208 W \u2286 Rn+2m 2: Output: A prediction \u03b3 \u2208 H 3: \u03b3 \u2190 c 4: for i = 1 to m do 5: if xi = 0 then 6: continue 7: else 8: \u03b3 \u2190 {\nMi(\u03b3) with probability xi/(xi + \u03bbi) \u03b3 with probability \u03bbi/(xi + \u03bbi)\n9: end if 10: end for 11: return S"}, {"heading": "4.2 Projection", "text": "Formally, the problem is to find the \u2206-projection of the multiplicatively-updatedp back onto set W :\nP\u2206W (p) = argmin q\u2208W \u2206(q||p) (3)\nIn which \u2206(\u00b7||\u00b7) is the unnormalized relative entropy. Observe that W is an intersection of affine subspaces presented by m + n equality constraints in Ax + \u03bb = b and Mx + c = v denoted by C1, . . . , Cm+n. Since the non-negativity constraints are already in the definition of \u2206, it is possible to solve (3) by simply using iterative \u2206-projections 4 [2]. Starting from p0 = p, we iteratively\n3Mi denotes the swap action associated with the ith reflection relation. 4In [9] Sinkhorn balancing is used for projection which is also a special case of iterative Bregman projection\ncompute: \u2200k > 0, pk = P\u2206Ck(pk\u22121) (4)\nin which the index k is (m + n)-periodic (i.e. \u2200k \u2208 N, Ck+m+n = Ck). It is proved [2] that pk converges to the unique solution of (3). In the Appendix, we discuss how one can project onto each hyperplane Ck for all k \u2208 [n+m]. Although the iterative projection is only guaranteed to enter the simplex in the limit, the approaches in [9, 13] provide ways to overcome this issue."}, {"heading": "4.3 Regret Bounds", "text": "Using the vectors wt \u2208 W produced by the algorithm, we can prove the following cumulative regret bound:\nLemma 3. E\n[\nT \u2211\nt=1\n\u03b3t\u22121 \u00b7 \u2113t ] \u2264 \u03b7min \u03b3\u2208H\n\u2211T t=1 \u03b3 \u00b7 \u2113t +\u2206(\u03b8||w0)\n1\u2212 e\u2212\u03b7 in which \u03b8 \u2208 W is the augmented\nformulation corresponding to the instance \u03b3 \u2208 V .\nThe proof is standard in the online learning literature (see, e.g., [13]) and is shown in the Appendix. Now, we prove that there exists a good initial point w0 in W such that the \u2206(\u03b8||w0) divergences are all appropriately bounded (shown in the Appendix).\nLemma 4. Assume we are working with m reflection relations. Also assume that associated network does not have any redundant comparator. Then given n \u2264 m \u2264 n2, there exists 5 q \u2208 W such that for all p \u2208 W , we have \u2206(p||q) \u2264 16mn logn.\nBy Lemmas 3 and 4, we can now show the main regret bound result (see the Appendix for the proof):\nTheorem 5. Assume the number of reflection relations used in the extended formulation is m \u2208 [n, n2] 6 and none of the corresponding comparators in the network are redundant. Then, given \u2113t \u2208 [0, 1]n for all t = 1 . . . T , we have:\nE\n[\nT \u2211\nt=1\n\u03b3t\u22121 \u00b7 \u2113t ] \u2264 \u03b7min \u03b3\u2208H\n\u2211T t=1 \u03b3 \u00b7 \u2113 t + 16mn logn\n1\u2212 e\u2212\u03b7\nFurthermore, in both cases of permutations and Huffman trees, by choosing a sorting network of size m = O(n log n) and tuning \u03b7 appropriately, the expected regret of Extended-Learn is at most O(n2 logn \u221a T )."}, {"heading": "5 Comparisons and Conclusion", "text": "Table 2 contains a comparison of the regret bounds for the new Extended-Learn algorithm, previous algorithms for permutations, and the hedge algorithm [5] which inefficiently maintains an explicit weight for each of the exponential in n logn permutations or trees. Using loss vectors from the general unit cube, for permutations we assume the scheduling loss from [21] that has range [0, n2] per trial. When compared with the state of the art implicit algorithms PermELearn [9] and PermutahedLearn [21], the general Extended-Learn methodology has a small additional regret bound penalty of \u221a\nlog(n). When compared with the generic explicit hedge algorithm (which is not computationally efficient), Extended-Learn has a better loss bound by a factor of \u221a n/ logn.\nWhen comparing Extended-Learn with explicit hedge on Huffman trees, we consider two loss regimes: (i) one where the loss vectors are from the general unit cube, and consequently, the pertrial losses are in [0, n2] (like permutations), (ii) and one where the loss vectors represent frequencies and lie on the unit simplex so the per-trial losses are in [0, n]. In the first case, Extended-Learn and Hedge have the same asymptotic bounds as with permutations. In the second case, the lower loss range favors Hedge, and the inefficient algorithm\u2019s bound is slightly better by a factor of \u221a logn.\n5The construction of q can be done as a pre-processing step and may depend on the network of comparators. For instance, if we use the bubble-sort sorting network in case of permutation, the average of the augmented formulation of the two permutation [1, 2, . . . , n] and [n, n\u2212 1, . . . , 1] will be a good choice.\n6We still have \u2206(p||q) = O(mn log n) and the same bounds asymptotically with m = poly(n).\nIn conclusion, we have presented a general methodology for creating on-line learning algorithms from generic extended formulation constructions. Because these extended formulations are designed to describe complex polytopes using a manageable number of constraints, they allow efficient learning on structures, like the polytope of Huffman trees, that previously could not be learned efficiently. Several areas for future work remain. We mentioned how to generate a good initial w0 for particular extended formulations. It would be nice to have a generic w0 generation method for an arbitrary extended formulation. Also recall that the iterative Bregman projections (see Section 4.2) may only enter the polytope W in the limit. Although this causes a negligible increase in regret with Sinkhorn balancing [9], it would be helpful to have a formal analysis of when Extended-Learn makes predictions using wt just outside the polytope. Finally, and most importantly, one can investigate extended formulations for other combinatorial objects and analyze the learning algorithms that result from the Extended-Learn methodology."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 Structure of Inequality Constraints in Extended Formulation", "text": "Lemma 6. If M is the matrix of affine transformation corresponding to m reflection relations, the we have: A = Tri(MTM) + I, b = \u2212MTc in which Tri(\u00b7) is a function over square matrices which zeros out the upper triangular part of the input including the diagonal.\nProof. Let vk be the vector in V after going through the kth reflection relation. Also denote the kth column of M by Mk. Observe that v0 = c and vk = c + \u2211k i=1 Mixi. Let Mk = er \u2212 es. Then, using (2), the inequality associated with the kth row of Ax \u2264 b will be obtained as below:\nxk \u2264 vk\u22121s \u2212 vk\u22121r = \u2212MTk vk\u22121 = \u2212MTk\n(\nc+\nk\u22121 \u2211\ni=1\nMixi\n)\n\u2212\u2192 xk + k\u22121 \u2211\ni=1\nMTk Mixi \u2264 \u2212MTk c = bk\nThus:\n\u2200 i, j \u2208 [m] Aij =\n\n\n\nMTi Mj i > j 1 i = j\n0 i < j\n, \u2200 k \u2208 [m] bk = \u2212MTk c\nwhich concludes the proof."}, {"heading": "6.2 Proof of Lemma 1", "text": "Proof. Let x = [x1x2 . . . xm]T . We prove that by the end of the ith iteration of ExtendedDecomposition, S is the correct decomposition for xi = [x1 . . . xi 0 . . . 0]. The lemma is proved as a result of this statement for i = m. We prove the statement by induction. The base case i = 0 is indeed true, since S is initialized to {(c, 1)} and we have v0 = Mx0 + c = c. Now consider that by the end of the (k \u2212 1)th iteration we have the right decompositon namely vk\u22121 = \u2211\n\u03b3 p\u03b3 \u03b3.\nAssume that the kth comparator is applied on rth and sth element. Thus the kth column of M will be Mk = er \u2212 es. Now, according to (2) the swap capacity at kth comparator is:\nxk + \u03bbk = v k\u22121 s \u2212 vk\u22121r =\n\u2211\n\u03b3\np\u03b3 (\u03b3s \u2212 \u03b3r) = \u2212 \u2211\n\u03b3\np\u03b3 M T k \u03b3 = \u2212MTk\n\u2211\n\u03b3\np\u03b3 \u03b3\nNow observe:\nvk = Mxk + c = xkMk +Mx k\u22121 + c = xkMk + v k\u22121 = xkMk + \u2211\n\u03b3\np\u03b3 \u03b3\n= pkMk (xk + \u03bbk) + \u2211\n\u03b3\np\u03b3 \u03b3 = \u2212 pkMkMTk \u2211\n\u03b3\np\u03b3 \u03b3 + \u2211\n\u03b3\np\u03b3 \u03b3\n= ( I \u2212 pkMkMTk )\n\u2211\n\u03b3\np\u03b3 \u03b3 = ((1\u2212 pk) I + pk Trs) \u2211\n\u03b3\np\u03b3 \u03b3\n= \u2211\n\u03b3\n(1\u2212 pk) p\u03b3 \u03b3 + pkp\u03b3 Trs \u03b3\nin which Trs is row-switching matrix that is obtained form switching rth and sth row from identity matrix. This concludes the inductive proof. It is easy to see that the time complexity of the algorithm is O(2m) since S may grow exponentially and we have to go through all elements of it in each iteration."}, {"heading": "6.3 Proof of Lemma 2", "text": "Proof. For each comparator i \u2208 [m] in the network, the Extended-Prediction algorithm defines the distribution below for the action of the comparator i:\nP (actioni) =\n{\npi actioni = swap 1\u2212 pi actioni = pass\nIt is easy to see from Lemma 1 that the distribution of a given instance is drawn as below:\nP (\u03b3) =\nm \u220f\ni=1\nP (actioni)\nThat is, the distribution over instances \u03b3 \u2208 H is decomposed into individual actions of swap/pass through the network of comparators independently. Thus one can draw an instance according to the distribution by simply doing independent Bernoulli trials associated with the comparators. It is also easy to see that the time complexity of the algorithm is O(m) since one just needs to do m Bernoulli trials."}, {"heading": "6.4 Projection onto Each Constraint", "text": "Each constraint of the polytope in the augmented formulation is of the form aTw = a0. Formally, the projection w\u2217 of a give point w to this constraint is solution to the following:\nargmin aTw\u2217=a0\n\u2211\ni\nw\u2217i log\n(\nw\u2217i wi\n)\n+ wi \u2212 w\u2217i\nUsing the method of Lagrange multipliers, we have:\nL(w\u2217, \u00b5) = \u2211\ni\nw\u2217i log\n(\nw\u2217i wi\n)\n+ wi \u2212 w\u2217i \u2212 \u00b5\n\n\n2m+n \u2211\nj=1\naiw \u2217 i \u2212 a0\n\n\n\u2202L\n\u2202w\u2217i = log\n(\nw\u2217i wi\n)\n\u2212 \u00b5ai = 0, \u2200i \u2208 [n+ 2m]\n\u2202L \u2202\u00b5 =\n2m+n \u2211\nj=1\nai w \u2217 i \u2212 a0 = 0\nReplacing \u03b2 = e\u2212\u00b5, we have w\u2217i = wi \u03b2 ai . By enforcing \u2202L \u2202\u00b5 = 0, one needs to find \u03b2 > 0 such that: n+2m \u2211\ni=1\nai wi \u03b2 ai \u2212 a0 = 0 (5)\nObserve that due to the structure of matrices M and A (see Lemma 6), ai \u2208 Z and ai \u2265 \u22121 for all i \u2208 [n + 2m], and furthermore a0 \u2265 0. Thus we can re-write the equation (5) as the polynomial below:\nf(\u03b2) = \u03b1k\u03b2 k + . . .+ \u03b12\u03b2 2 \u2212 \u03b11\u03b2 \u2212 \u03b10 = 0 in which all \u03b1i\u2019s are positive real numbers. Note that f(0) < 0 and f(\u03b2) \u2192 +\u221e as \u03b2 \u2192 +\u221e. Thus f(\u03b2) has at least one positive root. However, it can not have more than one positive roots and we can prove it by contradiction. Assume that there exist 0 < r1 < r2 such that f(r1) = f(r2) = 0. Since f is convex on positive real line, using Jensen\u2019s inequality, we can obtain the contradiction below:\n0 = f(r1) = f\n(\nr2 \u2212 r1 r2 \u00d7 0 + r1 r2 \u00d7 r2 ) < r2 \u2212 r1 r2 f(0) + r1 r2 f(r2) = r2 \u2212 r1 r2 f(0) < 0\nTherefore f has exactly one positive root which can be found by Newton\u2019s method starting from a sufficiently large initial point. 7"}, {"heading": "6.5 Proof of Lemma 3", "text": "Proof. Assuming w = (v,x,\u03bb), \u03b8 = (\u03b3,x\u03b8,\u03bb\u03b8) and L = (\u2113,0,0):\n(1\u2212 e\u2212\u03b7)vt\u22121 \u00b7 \u2113t = (1\u2212 e\u2212\u03b7)wt\u22121 \u00b7Lt\n\u2264 \u2211\ni\nwt\u22121i (1\u2212 e\u2212\u03b7 L t i )\n= \u2206(\u03b8||wt\u22121)\u2212\u2206(\u03b8||w\u0302t\u22121) + \u03b7 \u03b8 \u00b7 Lt\n\u2264 \u2206(\u03b8||wt\u22121)\u2212\u2206(\u03b8||wt) + \u03b7 \u03b3 \u00b7 \u2113t\nThe first inequality is obtained using 1 \u2212 e\u2212\u03b7x \u2265 (1 \u2212 e\u2212\u03b7)x for x \u2208 [0, 1] as done in [15]. The second inequality is a result of the Generalized Pythagorean Theorem [10], since wt is a Bregman projection of w\u0302t\u22121 into the convex set W which contains \u03b8. By summing over t = 1 . . . T and using the non-negativity of divergences, we obtain:\n(1\u2212 e\u2212\u03b7) T \u2211\nt=1\nvt\u22121 \u00b7 \u2113t \u2264 \u2206(\u03b8||w0)\u2212\u2206(\u03b8||wT ) + \u03b7 T \u2211\nt=1\n\u03b3 \u00b7 \u2113t\n\u2212\u2192 E [ T \u2211\nt=1\n\u03b3t\u22121 \u00b7 \u2113t ] \u2264 \u03b7min \u03b3\u2208H\n\u2211T t=1 \u03b3 \u00b7 \u2113t +\u2206(\u03b8||w0)\n1\u2212 e\u2212\u03b7\n7Note that if the constraint belongs to v = Mx + c, since all the coefficinets are in {\u22121, 0, 1}, the f will be quadratic and the positve root can be found through the closed form formula."}, {"heading": "6.6 Proof of Lemma 4", "text": "Proof. According to the definition:\n\u2206(p||q) = \u2211\ni\npi log pi qi + qi \u2212 pi\n= \u2211\ni\npi log pi \u2212 pi log qi + qi \u2212 pi\nWe will bound each term of the expression above. First observe that, given Sp = \u2211\ni pi, we have: \u2211\ni\npi Sp log pi Sp \u2264 log(n+ 2m)\n\u2212\u2192 \u2211\ni\npi log pi \u2264 Sp log(n+ 2m) + Sp logSp\nNote that for an arbitrary point (v,x,\u03bb) \u2208 W , for all i \u2208 [m], xi + \u03bbi \u2264 n because it indicates the maximum swap value at reflection relation i. Also since vi \u2264 n for all i \u2208 [n], thus Sq, Sp \u2264 n2 +mn \u2264 2mn. Therefore:\n\u2211\ni\npi log pi \u2264 Sp log(n+ 2m) + Sp log Sp\n\u2264 4mn logn+ 6mn logn = 10mn logn\nNow we only need to bound \u2211 i\u2212pi log qi. To do so, we choose q to be the average of some instances {\u03b8(j)}j\u2208J \u2282 W such that for all i \u2208 [n + 2m], qi is sufficiently large. Trivially v\u03b8 \u2265 1. Also note that, since we do not have any redundant comparator in our network, we can assume for all i \u2208 [m], there exists a witness instance in W such that xi \u2265 1. Same argument can be made for \u03bbi\u2019s. Now let q be the average of all these 2m instances. Therefore for all i \u2208 [n + 2m], qi \u2265 12m and consequently \u2212 log qi \u2264 log 2m.\nPutting everything together, we obtain:\n\u2206(p||q) = \u2211\ni\npi log pi \u2212 pi log qi + qi \u2212 pi\n\u2264 10mn logn+ 2mn log 2m+ 2mn \u2264 16mn logn"}, {"heading": "6.7 Proof of Theorem 5", "text": "Proof. The first part is the immediate consequence of Lemmas 3 and 4:\nE\n[\nT \u2211\nt=1\n\u03b3t\u22121 \u00b7 \u2113t ] \u2264 \u03b7min \u03b3\u2208H\n\u2211T t=1 \u03b3 \u00b7 \u2113t + 16mn logn\n1\u2212 e\u2212\u03b7\nLet Lbest = min \u03b3\u2208H\n\u2211T t=1 \u03b3 \u00b7 \u2113t. We can tune \u03b7 as instructed in Lemma 4 in [5]:\nE\n[\nT \u2211\nt=1\n\u03b3t\u22121 \u00b7 \u2113t ]\n\u2212 min \u03b3\u2208H\nT \u2211\nt=1\n\u03b3 \u00b7 \u2113t \u2264 \u221a\n2Lbest 16mn logn+ 16mn logn\nApplying Lbest \u2264 Tn2 and m = O(n log n) (by choosing an appropriate sorting network [1]) into inequality above, we will obtain the desired result."}], "references": [{"title": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming", "author": ["Lev M Bregman"], "venue": "USSR computational mathematics and mathematical physics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1967}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Cambridge university press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Group representations in probability and statistics", "author": ["Persi Diaconis"], "venue": "Lecture Notes-Monograph Series,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "Journal of computer and system sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Smallest compact formulation for the permutahedron", "author": ["Michel X Goemans"], "venue": "Mathematical Programming,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Direct and indirect algorithms for on-line learning of disjunctions", "author": ["David P Helmbold", "Sandra Panizza", "Manfred K Warmuth"], "venue": "Theoretical Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Predicting nearly as well as the best pruning of a decision tree", "author": ["David P Helmbold", "Robert E Schapire"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Learning permutations with exponential weights", "author": ["David P Helmbold", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Tracking the best linear predictor", "author": ["Mark Herbster", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Extended formulations in combinatorial optimization", "author": ["Volker Kaibel"], "venue": "arXiv preprint arXiv:1104.1023,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Constructing extended formulations from reflection relations", "author": ["Volker Kaibel", "Kanstantsin Pashkovich"], "venue": "In Facets of Combinatorial Optimization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Optimum follow the leader algorithm", "author": ["Dima Kuzmin", "Manfred K Warmuth"], "venue": "In Learning Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K Warmuth"], "venue": "Information and computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Efficient learning with virtual threshold gates", "author": ["Wolfgang Maass", "Manfred K Warmuth"], "venue": "Information and Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "On the convex hull of huffman trees", "author": ["Jean-Fran\u00e7ois Maurras", "Thanh Hai Nguyen", "Viet Hung Nguyen"], "venue": "Electronic Notes in Discrete Mathematics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Predicting nearly as well as the best pruning of a planar decision graph", "author": ["Eiji Takimoto", "Manfred K Warmuth"], "venue": "Theoretical Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Path kernels and multiplicative updates", "author": ["Eiji Takimoto", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Randomized online pca algorithms with regret bounds that are logarithmic in the dimension", "author": ["Manfred K Warmuth", "Dima Kuzmin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Online linear optimization over permutations", "author": ["Shota Yasutake", "Kohei Hatano", "Shuji Kijima", "Eiji Takimoto", "Masayuki Takeda"], "venue": "In Algorithms and Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "Abstract The convex hull of n-symbol Huffman trees is known to have exponentially many facets/constraints [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "One way to create algorithms for these combinatorial problems is to use one of the well-known socalled \u201cexperts algorithms\u201d like weighted majority [15] or hedge [5] with each combinatorial object treated as an \u201cexpert\u201d.", "startOffset": 147, "endOffset": 151}, {"referenceID": 3, "context": "One way to create algorithms for these combinatorial problems is to use one of the well-known socalled \u201cexperts algorithms\u201d like weighted majority [15] or hedge [5] with each combinatorial object treated as an \u201cexpert\u201d.", "startOffset": 161, "endOffset": 164}, {"referenceID": 7, "context": "In addition to subsets, such work includes permutations [9, 21], paths [14, 19], and k-sets [20].", "startOffset": 56, "endOffset": 63}, {"referenceID": 18, "context": "In addition to subsets, such work includes permutations [9, 21], paths [14, 19], and k-sets [20].", "startOffset": 56, "endOffset": 63}, {"referenceID": 11, "context": "In addition to subsets, such work includes permutations [9, 21], paths [14, 19], and k-sets [20].", "startOffset": 71, "endOffset": 79}, {"referenceID": 16, "context": "In addition to subsets, such work includes permutations [9, 21], paths [14, 19], and k-sets [20].", "startOffset": 71, "endOffset": 79}, {"referenceID": 17, "context": "In addition to subsets, such work includes permutations [9, 21], paths [14, 19], and k-sets [20].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "Section 3 explains the extended formulation used in our setting by reviewing the work by Kaibel and Pashkovich [12].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "2 Related Work On-line learning is a rich and vibrant area, see [3] for a textbook treatment.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "The implicit representations for structured concepts (sometimes called \u2018indirect representations\u2019) have been used for a variety of problems [7, 8, 16, 18, 19].", "startOffset": 140, "endOffset": 158}, {"referenceID": 6, "context": "The implicit representations for structured concepts (sometimes called \u2018indirect representations\u2019) have been used for a variety of problems [7, 8, 16, 18, 19].", "startOffset": 140, "endOffset": 158}, {"referenceID": 13, "context": "The implicit representations for structured concepts (sometimes called \u2018indirect representations\u2019) have been used for a variety of problems [7, 8, 16, 18, 19].", "startOffset": 140, "endOffset": 158}, {"referenceID": 15, "context": "The implicit representations for structured concepts (sometimes called \u2018indirect representations\u2019) have been used for a variety of problems [7, 8, 16, 18, 19].", "startOffset": 140, "endOffset": 158}, {"referenceID": 16, "context": "The implicit representations for structured concepts (sometimes called \u2018indirect representations\u2019) have been used for a variety of problems [7, 8, 16, 18, 19].", "startOffset": 140, "endOffset": 158}, {"referenceID": 7, "context": "In the case of permutations [9, 21] and component hedge [13] the implicit representations seem to be better matched with the combinatorial structure than the explicit representation, allowing not only decreased running time but also the proof of better bounds.", "startOffset": 28, "endOffset": 35}, {"referenceID": 18, "context": "In the case of permutations [9, 21] and component hedge [13] the implicit representations seem to be better matched with the combinatorial structure than the explicit representation, allowing not only decreased running time but also the proof of better bounds.", "startOffset": 28, "endOffset": 35}, {"referenceID": 18, "context": "As in [21], the loss family provided in our approach is linear over the first order representation of the objects [4].", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "As in [21], the loss family provided in our approach is linear over the first order representation of the objects [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 7, "context": "In contrast, [9] works with the second order representation, and consequently losses, which is a more general loss family (See [21] for comparison).", "startOffset": 13, "endOffset": 16}, {"referenceID": 18, "context": "In contrast, [9] works with the second order representation, and consequently losses, which is a more general loss family (See [21] for comparison).", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "There have been several works aimed at efficiently describing the polytope of different combinatorial objects like permutations [6] and Huffman trees [17].", "startOffset": 128, "endOffset": 131}, {"referenceID": 14, "context": "There have been several works aimed at efficiently describing the polytope of different combinatorial objects like permutations [6] and Huffman trees [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "Discovered by the combinatorial optimization community, extended formulation, on which our results rely, is a general methodology to nicely describe combinatorial polyhedra [11, 12].", "startOffset": 173, "endOffset": 181}, {"referenceID": 10, "context": "Discovered by the combinatorial optimization community, extended formulation, on which our results rely, is a general methodology to nicely describe combinatorial polyhedra [11, 12].", "startOffset": 173, "endOffset": 181}, {"referenceID": 14, "context": "see [17]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "See [11] for some of the tools for constructing such extended formulations.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "In the following subsections, we first overview the work by Kaibel and Pashkovich [12], and then extract the formulation fitting to our methodology.", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "1 Constructing Extended Formulation from Reflection Relations One of the tools to construct polynomial size extended formulations is the framework developed by [12] using reflection relations.", "startOffset": 160, "endOffset": 164}, {"referenceID": 10, "context": "This framework provides an inductive construction of higher dimensional polytopes via sequence of reflection relations (see Theorem 1 in [12]).", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "Theorem 1 in [12] provides the sufficient conditions for the correctness of this procedure.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "For Huffman trees, however, the sequence of comparators used in the inductive step consists of two sweeps of the elements to bubble-out the two largest elements which are associated with the two deepest leaves in the tree [12].", "startOffset": 222, "endOffset": 226}, {"referenceID": 10, "context": "Similarly, by using an arbitrary sorting network along with some additional comparators and simple linear maps, an extended formulation for Huffman trees can also be built (See Theorem 7 in [12]).", "startOffset": 190, "endOffset": 194}, {"referenceID": 7, "context": "Efficient Prediction In order to achieve efficiency, despite the vastly common idea in the literature [9, 13, 20, 21], one can avoid decomposition and do prediction directly.", "startOffset": 102, "endOffset": 117}, {"referenceID": 17, "context": "Efficient Prediction In order to achieve efficiency, despite the vastly common idea in the literature [9, 13, 20, 21], one can avoid decomposition and do prediction directly.", "startOffset": 102, "endOffset": 117}, {"referenceID": 18, "context": "Efficient Prediction In order to achieve efficiency, despite the vastly common idea in the literature [9, 13, 20, 21], one can avoid decomposition and do prediction directly.", "startOffset": 102, "endOffset": 117}, {"referenceID": 0, "context": "Since the non-negativity constraints are already in the definition of \u2206, it is possible to solve (3) by simply using iterative \u2206-projections 4 [2].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "In [9] Sinkhorn balancing is used for projection which is also a special case of iterative Bregman projection", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "It is proved [2] that pk converges to the unique solution of (3).", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "Although the iterative projection is only guaranteed to enter the simplex in the limit, the approaches in [9, 13] provide ways to overcome this issue.", "startOffset": 106, "endOffset": 113}, {"referenceID": 3, "context": "5 Comparisons and Conclusion Table 2 contains a comparison of the regret bounds for the new Extended-Learn algorithm, previous algorithms for permutations, and the hedge algorithm [5] which inefficiently maintains an explicit weight for each of the exponential in n logn permutations or trees.", "startOffset": 180, "endOffset": 183}, {"referenceID": 18, "context": "Using loss vectors from the general unit cube, for permutations we assume the scheduling loss from [21] that has range [0, n] per trial.", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "When compared with the state of the art implicit algorithms PermELearn [9] and PermutahedLearn [21], the general Extended-Learn methodology has a small additional regret bound penalty of \u221a", "startOffset": 71, "endOffset": 74}, {"referenceID": 18, "context": "When compared with the state of the art implicit algorithms PermELearn [9] and PermutahedLearn [21], the general Extended-Learn methodology has a small additional regret bound penalty of \u221a", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "The convex hull of n-symbol Huffman trees is known to have exponentially many facets/constraints [17]. This makes the standard on-line learning techniques for learning Huffman trees impractical, since they use multiplicative updates followed by projections to satisfy all of the constraints. However, there are general extended formulation techniques that encode the convex hull of Huffman trees as a polytope in a higher dimensional space with only polynomially many facets. This extended formulation methodology can also be used to encode the n-element permutahedron in O(n log n) dimensions with only a polynomial number of facets. We develop a general technique for converting these extended formulations into efficient on-line algorithms with good relative loss bounds. The resulting algorithms have nearly the same regret bounds as state of the art algorithms for permutations, and are the first efficient algorithms for the on-line learning of Huffman trees.", "creator": "LaTeX with hyperref package"}}}