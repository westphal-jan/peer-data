{"id": "1609.05518", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2016", "title": "Towards Deep Symbolic Reinforcement Learning", "abstract": "41.09 Deep reinforcement tiehi learning (78.7 DRL) parameswaran brings waywardness the power of moog deep neural noar networks to jamaar bear on the generic 945,000 task sabee of hoist trial - yazdani and - meriton error pyrophoric learning, and its effectiveness narrowcasting has been convincingly kingbird demonstrated gamine on hoysalas tasks siaolin such as Atari caldecott video candle games gatlinburg and malladi the cranium game sailboat of hvs Go. seiran However, milsap contemporary DRL shaef systems inherit eave a honesto number of shortcomings kapfenberger from inventing the morfessis current generation berloni of deep learning ohau techniques. For example, existentialists they require danae very fifth-highest large datasets intracytoplasmic to work wilkes-barre/scranton effectively, sudeley entailing that horfield they are heimer slow to rolande learn even theodosios when left-hander such lying-in datasets otmoor are caguas available. Moreover, guedj they lack vaigai the solomont ability nani to mitchelstown reason on an abstract hrdc level, ormsbee which makes 428th it picota difficult to albans implement esteros high - level pinelawn cognitive one-sixteenth functions angping such dan.bickley as foursomes transfer learoyd learning, disemboweled analogical reasoning, and preceding hypothesis - based coudenhove-kalergi reasoning. Finally, nsls their etu operation is largely opaque to harpertorch humans, psychoanalytical rendering damini them berge unsuitable for counterculture domains rahs in waterbody which ahmedpur verifiability ignoble is anata important. magha In 22-17 this kangchu paper, we bareness propose kreft an end - druten to - vlachos end thermophiles reinforcement learning wienermobile architecture hinga comprising a marshal neural back end oranges and naegle a .575 symbolic intersections front end with the potential holl to overcome nutfield each avaz of kyiv-mohyla these lonergans shortcomings. As proof - 12.54 of - concept, we present a preliminary bu\u00f1uel implementation of 0250 the bathymetry architecture and artkraft apply harzburg it to nozizwe several variants fireboxes of a simple temnospondyls video game. We sc-1 show ohman that browse the ogbeh resulting system - - banier though gipps just foulger a prototype - - shtalenkov learns effectively, gauntlett and, reorganise by semmering acquiring a set of 1971-1972 symbolic rules that are easily caipirinhas comprehensible to humans, irkut dramatically outperforms a conventional, fully neural DRL iligan system guyra on a noreaga stochastic negotiator variant anm of roecker the game.", "histories": [["v1", "Sun, 18 Sep 2016 17:28:22 GMT  (981kb,D)", "http://arxiv.org/abs/1609.05518v1", null], ["v2", "Sat, 1 Oct 2016 16:19:56 GMT  (981kb,D)", "http://arxiv.org/abs/1609.05518v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["marta garnelo", "kai arulkumaran", "murray shanahan"], "accepted": false, "id": "1609.05518"}, "pdf": {"name": "1609.05518.pdf", "metadata": {"source": "CRF", "title": "Towards Deep Symbolic Reinforcement Learning", "authors": ["Marta Garnelo", "Kai Arulkumaran"], "emails": ["garnelo@ic.ac.uk", "kailash.arulkumaran13@ic.ac.uk", "m.shanahan@ic.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Deep reinforcement learning (DRL), wherein a deep neural network (LeCun et al., 2015; Schmidhuber, 2015) is used as a function approximator within a reinforcement learning system (Sutton and Barto, 1998), has recently been shown to be effective in a number of domains, including Atari video games (Mnih et al., 2015), robotics (Levine et al., 2016), and the game of Go (Silver et al., 2016). DRL can be thought of as a step towards instantiating the formal characterisation of universal artificial intelligence presented by Hutter(Legg and Hutter, 2007), a theoretical framework for artificial general intelligence (AGI) founded on reinforcement learning. However, contemporary DRL systems suffer from a number of shortcomings. First, they inherit from deep learning the need for very large training sets, which entails that they learn very slowly. Second, they are brittle in the sense that a trained network that performs well on one task often performs very poorly on a new task, even if the new task is very similar to the one it was originally trained on. Third, they are strictly reactive, meaning that they do not use high-level processes such as planning, causal reasoning, or analogical reasoning to fully exploit the statistical regularities present in the training data. Fourth,\nar X\niv :1\n60 9.\n05 51\n8v 1\n[ cs\n.A I]\nthey are opaque. It is typically difficult to extract a humanly-comprehensible chain of reasons for the action choice the system makes. Each of these shortcomings is an active area of research in the DRL community, where data efficient learning (Assael et al., 2015; Gu et al., 2016), transfer learning (Parisotto et al., 2015; Arulkumaran et al., 2016; Barreto et al., 2016; Rusu et al., 2016), planning (Guo et al., 2014; Vezhnevets et al., 2016), and transparency (Zahavy et al., 2016) are all hot topics.\nHere we take a different approach. We propose a novel reinforcement learning architecture that addresses all of these issues at once in a principled way by combining neural network learning with aspects of classical symbolic AI, gaining the advantages of both methodologies without their respective disadvantages. Central to classical AI is the use of language-like propositional representations to encode knowledge. Thanks to their compositional structure, such representations are amenable to endless extension and recombination, an essential feature for the acquisition and deployment of high-level abstract concepts, which are key to general intelligence (McCarthy, 1987). Moreover, knowledge expressed in propositional form can be exploited by multiple high-level reasoning processes and has general-purpose application across multiple tasks and domains. Features such as these, derived from the benefits of human language, motivated several decades of research in symbolic AI.\nBut as an approach to general intelligence, classical symbolic AI has been disappointing. A major obstacle here is the symbol grounding problem (Harnad, 1990; Shanahan, 2005). The symbolic elements of a representation in classical AI \u2013 the constants, functions, and predicates \u2013 are typically hand-crafted, rather than grounded in data from the real world. Philosophically speaking, this means their semantics are parasitic on meanings in the heads of their designers rather than deriving from a direct connection with the world. Pragmatically, hand-crafted representations cannot capture the rich statistics of real-world perceptual data, cannot support ongoing adaptation to an unknown environment, and are an obvious barrier to full autonomy. By contrast, none of these problems afflict machine learning. Deep neural networks in particular have proven to be remarkably effective for supervised learning from large datasets using backpropagation (LeCun et al., 2015; Schmidhuber, 2015). Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013). The hybrid neural-symbolic reinforcement learning architecture we propose relies on a deep learning solution to the symbol grounding problem.\nAt the top level, the deep symbolic reinforcement learning architecture we propose is very simple (Fig. 1). It comprises a deep neural network back end, whose job is to transform raw perceptual data into a symbolic representation, which is fed to a symbolic front end whose task is action selection. The function computed by each half of the architecture is shaped by machine learning, so the system as a whole learns end-to-end with minimal assumptions made on the nature of the environment. The neural back end must learn a compositionally-structured compressed representation of the raw perceptual data, while the symbolic front end must learn a mapping from the resulting symbolic representation to actions that maximise expected reward over time.\nIn this paper we present one instantiation of this architecture as a proof-of-concept, and illustrate its effectiveness on several variants of a simple video game. This demonstrator system has many limitations and makes numerous simplifying assumptions that are not inherent in the larger proposal, but it illustrates the four fundamental principles of our architectural manifesto. (For a related set of desiderata see (Lake et al., 2016).)\n1) Conceptual abstraction. Determining that a new situation is similar or analogous to one (or several) encountered previously is an operation fundamental to general intelligence, and to reinforcement learning in particular. In a conventional DRL system, such as DQN (Mnih et al., 2015), this is achieved through the generalising capabilities of the neural network that approximates the Q function (or the value function or policy function, depending on the style of reinforcement learning in question). However, this low-level approach to establishing similarity relationships requires the gradual build-up of a statistical picture of the state space. The upshot is that while a novice human player will rapidly spot the high-level similarity between, say, the paddle and ball in Pong and the paddle and ball in Breakout, a conventional DRL system is blind to this. By contrast, the present architecture maps high-dimensional raw input into a lower-dimensional conceptual state space within which it is possible to establish similarity between states using symbolic methods that operate at a higher level of abstraction. This facilitates both data efficient learning and transfer learning as well as providing a foundation for other high-level cognitive processes such as planning, innovative problem solving, and communication with other agents (including humans).\n2) Compositional structure. To enable this sort of conceptual abstraction, a representational medium is required that has a compositional structure. That is to say it should comprise a set of elements that can be combined and recombined in an open-ended way. Classically, the theoretical foundation for such a representational medium is first-order logic, and the underlying language comprises predicates, quantifiers, constant symbols, function symbols, and boolean operators (McCarthy, 1987). (It should be noted that a fixed-size vector representation is inadequate for such a representational medium, because it can encode formulae of arbitrary length.) But the binary nature of classical logic makes it less well suited to dealing with the uncertainty inherent in real data than a Bayesian approach. To handle uncertainty, we propose probabilistic first-order logic for the semantic underpinnings of the low-dimensional conceptual state space representation into which the neural front end must map the system\u2019s high-dimensional raw input (Halpern, 1990).\n3) Common sense priors. Although our target is general intelligence, meaning the ability to achieve goals and perform tasks in a wide variety of domains, it is unrealistic to expect an end-to-end reinforcement learning system to succeed with no prior assumptions about the domain. For example, in most DRL systems that take visual input, spatial priors, such as the likelihood that similar 2D patterns will appear in different locations in the visual field, are implicit in the convolutional structure of the network (Bengio et al., 2013). But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014). Consisting mostly of empty space, it contains a variety of objects that tend to persist over time and have various attributes such as shape, colour, and texture (Shanahan, 1995). Objects frequently move, typically in continuous trajectories. Objects participate in a number of stereotypical events, such as starting to move or coming to a halt, appearing or disappearing, and coming into contact with other objects. These minimal assumptions and expectations can be built into the system by grafting a suitable ontology onto the underlying representational language, greatly reducing the learning workload and facilitating various forms of common sense reasoning.\n4) Causal reasoning. The present generation of DRL architectures eschews model-based reinforcement learning, ensuring that the resulting systems are purely reactive. By contrast, the architecture we propose attempts to discover the causal structure of the domain, and to encode this as a set of symbolic causal rules expressed in terms of the common sense ontology described above. These\ncausal rules enable conceptual abstraction. As already mentioned, the key to general intelligence is the ability to see that an ongoing situation is similar or analogous to a previously encountered situation or set of situations. A deep neural network that approximates the Q function in reinforcement learning can be thought of as carrying out analogical inference of this kind, but only at the most superficial, statistical level. To carry out analogical inference at a more abstract level, and thereby facilitate the transfer of expertise from one domain to another, the narrative structure of the ongoing situation needs to be mapped to the causal structure of a set of previously encountered situations. As well as maximising the benefit of past experience, this enables high-level causal reasoning processes to be deployed in action selection, such as planning, lookahead, and off-line exploration (imagination).\nOur implemented proof-of-concept system embodies each of these principles, albeit in a restricted form. The back end of the system learns to construct symbolic representations of sequences of game states, in which the flow of raw pixel data is encoded in a more conceptually abstract form, defined in terms of objects, their types, locations, and interactions. These representational elements can be joined in arbitrary combinations, yielding compositional structure. The ontology of this representational medium reflects common sense priors such as the tendency of objects to persist over time and the default assumption that objects that look alike behave in similar ways. Finally, the symbolic front end of the system learns effective policies because, in a rudimentary sense that is nevertheless inapplicable to conventional DRL systems, it \u201cunderstands\u201d the causal relations between its own actions, interactions among objects, and the acquisition of reward. However, in the present prototype the neural back end is in fact relatively shallow, while the symbolic front end carries out very little high-level reasoning. Consequently, it barely scratches the surface of what we believe is possible with the architecture we are proposing, and it should be regarded as just a first step towards a general purpose AI system that fully exploits the combined power of deep learning and symbolic reasoning."}, {"heading": "2 Experimental setup", "text": "As a benchmark for our prototype system, we implemented several variants of a simple game where the agent (shaped as a \u2018+\u2019) has to learn either to avoid or to collect objects depending on their shape. Once the agent reaches an object using one of four possible move actions (up, down, left, or right), this object disappears and the agent obtains either a positive or a negative reward. Encountering a circle (\u2018o\u2019) results in a negative reward while collecting a cross (\u2018x\u2019) yields a positive reward.\nWe applied the system to four variants of this game (figure 2) wherein the type of objects involved and their initial positions are as follows:\nVariant 1. In this environment there are only objects that return negative rewards (\u2018o\u2019) and they are positioned in a grid across the screen. This layout is the same for every new game. Encountering an object returns a score of -1 and at the beginning of the game the player is located in the middle of the board.\nVariant 2. The layout is the same as in version 1 but there are two types of objects. As before, circles give -1 points and we introduce crosses that return 1 points.\nVariant 3. As in version 1 this game only contains objects that return a negative reward. In order to increase the difficulty of the learning process however, the position of these objects is determined at random and changes at every new game.\nVariant 4. This version combines the randomness from environment 3 and the different object types from version 2."}, {"heading": "3 Methods", "text": "Our algorithm can be thought of as a pipeline comprising three stages: low-level symbol generation, representation building, and reinforcement learning."}, {"heading": "3.1 Low-level symbol generation", "text": "The goal of this first stage is to generate, in an unsupervised manner, a set of symbols that can be used to represent the objects in a scene. We use a convolutional neural network for this, since such networks are well-suited to feature extraction, especially from images. Specifically, we train a convolutional autoencoder on 5000 randomly generated images of varying numbers of game objects scattered across the screen.\nGiven the simplicity of the images, which consist of objects with three different geometric shapes (cross, plus sign, and circle) on a uniform background, we don\u2019t need to train a deep network to detect the different features for our current game benchmark. Our network consists of a 5x5 convolutional layer followed by a 2x2 pooling layer plus the corresponding decoding layers. The activations across features in the middle layer of the CNN are used directly for the detection of the objects in the scene.\nObject detection and characterisation. The next step of the symbol generation stage uses the salient regions of the convoluted image (figure 3). As shown by Li et al (Li et al., 2016), salient areas in an image will result in higher activations throughout the layers of a convolutional network. Given that our games are geometrically very simple, this property is enough to enable the extraction of the individual objects from any given frame. To do this, we first select, for each pixel, the feature with the highest activation. We then threshold these activation values, forming a list of those that are sufficiently salient. Ideally, each member of this list is a representative pixel for a single object. The objects identified this way are then assigned a symbolic type according to the geometric properties computed by the autoencoder. This is done by comparing the activation spectra of the salient pixels across features. In the current implementation, this comparison is carried out using the sum of the squared distances, which involves setting an ad hoc threshold for the maximal distance between two objects of the same type. In future work with richer environments, we anticipate using more sophisticated clustering algorithms as the state of the art in unsupervised learning advances.\nThe information extracted at this stage consists of a symbolic representation of the positions of salient objects in the frame along with their types. (See the left-hand table of figure 4.)"}, {"heading": "3.2 Representation building", "text": "Once we have extracted the low-level symbols from a single snapshot of the game we need to be able to track them across frames in order to observe and learn from their dynamics. To do this, we need to take account of the first common sense prior: object persistence across time. The concept of persistence we deploy is based on three measures combined into a single value. This value, which captures the likelihood that object it1 identified in one frame is the same as object i t+1 2 identified in the next frame, is a function of:\nSpatial proximity. We build in the notion of continuity by defining the likelihood to be inversely proportional to the distance between two objects in consecutive frames. We have\nLdist = 1\n1 + d\nwhere d is the Euclidean distance between two objects it1 and i t+1 2 in consecutive frames t and t+ 1 respectively. Distance, however, is not the sole determiner of an object\u2019s identity, given that objects can replace each other.\nType transitions. Given the types of two objects \u03c4(it1) = \u03c4i1 and \u03c4(i t+1 2 ) = \u03c4i2 in consecutive frames, we can determine the probability that they are the same object that has changed from one type to the other by learning a transition matrix T from previously observed frames. Given that this is a transition matrix it is already normalised to be a probability.\nLtrans = T\u03c4i1,\u03c4i2\nIn order to be able to describe all transitions, including the ones that correspond to objects appearing and disappearing, we introduce the object type 0. This type corresponds to \u2019non-existent\u2019. An object of type 1 that appears in frame t has thus carried out the transition 0 \u2192 1. If that object disappears later on it would transition 1 \u2192 0. This addition can only be carried out after the tracking step and once every object has been assigned its corresponding type in the previous time steps.\nNeighbourhood. The neighbourhood of an object will typically be similar from one frame to the next. This allows us to discriminate between objects that are spatially close but approaching from different directions. For now we just consider the difference in the number of neighbours, \u2206N between two objects, where we define a neighbour to be any object, in, within a distance dmax of another object i1. Future improvements could include considering the types of neighbours rather than just the number.\nLneigh = 1\n1 + \u2206N\nWe track an object by combining the three measures just described. We have\nL = w1Ldist + w2Ltrans + w3Lneigh\nwhere w1, ..., w3 are ad hoc weights. In future implementations these weights could be learned and set dynamically as the importance of each term varies during the learning process.\nAs a result of the tracking process, we acquire an additional attribute for each object in the scene: a unique identifier that labels it across time. This identifier is added to the symbolic representation of the ongoing game state. (See the middle table of figure 4.)\nSymbolic interactions and dynamics. So far, information about the game is expressed in terms of static frame-by-frame representations (albeit taking advantage of information in consecutive frames). But the final, reinforcement learning stage of the algorithm will require information about the dynamics of objects and their spatial interactions. This is obtained from the static representations constructed so far using two principles. First, we consider the difference between frames rather than working with single frames, thus moving to a temporally extended representation. Second, we represent the positions of objects relative to other objects rather than using absolute coordinates. Moreover, we only record relative positions of objects that lie within a certain maximum distance of each other. This approach is justified by the common sense prior that local relations between multiple objects are more relevant than the global properties of single objects.\nOf course, this assumption is not always valid, even with this game. As a result, the present system can arrive at a locally optimal policy that is inferior to the global optimum. So future implementations will have to handle this question of circumscribing relevance in a more nuanced way (Shanahan, 2016). But in general, the representational sparseness that results from adopting a locality assumption compensates for the loss of global optimality by allowing much faster training.\nWe now have a concise spatio-temporal representation of the game situation, one that captures not only what objects are in the scene along with their locations and types, but also what they are doing. In particular, it represents frame-to-frame interactions between objects, and the changes in type and relative position that result. (See the right-hand table in figure 4.) This is the input to reinforcement learning, the third and final stage of the pipeline."}, {"heading": "3.3 Reinforcement learning", "text": "The spatio-temporal representation constructed in stages one and two of the system pipeline can now be used to learn an effective policy for game play. At this point, the advantage of using the locality heuristic becomes clear. A representation that included all possible relations between objects would result in a very large state space and very long training times, with most states being visited infrequently. However, most interactions between objects are independent of each other, both in the real world and in this particular game. So instead of representing all relations in one global state, our representation comprises a set of localised representations. This results in a significantly reduced state space and enables fast generalisation across object types.\nIn order to implement this independence we train a separate Q function for each interaction between two object types. The main idea is to learn several Q functions for the different interactions and query those that are relevant for the current situation. Given the simplicity of the game and the reduced state space that results from the sparse symbolic representation we can approximate the optimal policy using tabular Q-learning. The update rule for the interaction between objects of types i and j is therefore\nQij(sijt , at)\u2190 Qij(s ij t , at) + \u03b1 ( rt+1 + \u03b3(max\na Qij(sijt+1, a)\u2212Qij(s ij t , at)) ) where \u03b1 is the learning rate, \u03b3 is the temporal discount factor, and each state sijt represents an interaction between object types i and j at time step t. In this case the interactions are changes in relative distance between the objects in question. The state space Sij thus describes the different possible relations between two objects of types i and j. Given that we have limited the interactions to a certain radius of proximity this state space is bounded but learning is not guaranteed to converge on a global optimum. Finally, in order to choose the next action we add up all Q values obtained from the currently relevant Q functions at the time step and pick the one that will return the highest reward overall.\nat+1 = arg max a ( \u2211 Q (Q(st+1, a))\nDuring training we use an -greedy exploration strategy with a 10% chance of choosing a different action at random.\nNote that, in the present benchmark games, the only moving object is the one the agent acts on directly (the plus sign), which ensures that every currently relevant Q function pertains to a possible agent action. In a more general implementation it will be necessary to automatically identify which objects in the world the agent controls directly (its own body, for example) and restrict the Q function accordingly."}, {"heading": "4 Results", "text": "Agents were trained in epochs of 100 time steps for a maximum of 1000 epochs. We trained 20 agents separately and tested them for 200 time steps on 10 games at every tenth epoch. The resulting average score is plotted in figure 5 for all four games. In all four cases the score increases within the first few hundred epochs and remains approximately constant for the remaining time.\nWhile plotting the average score in this way is a common way of visualising successful learning for this type of experiment, this measure can produce an incomplete characterisation of the learning process in environments with both positive and negative objects. There are two reasons for this. First, given that the scores returned by the objects can cancel each other out, there is more than one way to achieve any given final score. For example, collecting ten positive and nine negative objects will result in the same final score as only collecting one positive object. Yet in the first case the number of positive objects the agent collects is about 53% while in the second scenario it is 100%. Therefore, although they both have the same score, this percentage measure reveals that the agent in the latter case has learned to react to the different objects correctly while the agent in the former case collects items without regard for their type. Second, our agent only has a limited radius of view\nand can therefore get stuck in a location surrounded by negative objects. In this case the agent is forced to spend most of the test time avoiding these negative objects and won\u2019t obtain a high score. For these reasons we introduce a second measure: the percentage of positive objects collected of the total amount of objects encountered. Rather than measuring how well our agent performs on a global level, this measure shows whether or not the agent has learned to interact correctly with the individual objects at a local level.\nThe results for the two game variants that feature objects of two different types are shown in figure 6. As expected, about 50% of the objects that the agent initially collects are positive. As training continues the percentage increases to approximately 70% in both cases.\nFinally, we tested the transfer learning capabilities of our algorithm by training an agent only on games of the grid variant then testing it on games of the random variant. While this setup is similar to the experiments on the random game in the sense that the grid setup can be seen as one among the possible random initialisations, the difference lies in the fact that the agent is exposed to just this one type of environment during training, whereas in the random experiments the agent experiences numerous random variations. As shown in figure 7, the learning curve is comparable to, albeit a bit lower than in the totally random case."}, {"heading": "4.1 Comparison to DQN", "text": "Finally we compare our approach to DQN1. The environments that are suited the best are those with two types of objects as the initial score will be independent of the speed of the agent at the beginning given that they cancel each other out. Figure 6 shows the performance of DQN over time. It\u2019s important to note that our system\u2019s convolutional network was pre-trained on 5000 images, which corresponds to 50 epochs worth of frames. We don\u2019t include these in the plots because this pre-training is applicable to all the games and only has to be carried out once.\nThanks to the geometrical simplicity of the grid scenario, the DQN agent quickly learns to move down diagonally to collect only positive objects and avoid negative ones. As a result, the relative\n1For this comparison we used an open source implementation of DQN: https://github.com/Kaixhin/Atari\nnumber of objects with positive reward for this game variant reaches 100% after only a few hundred epochs of training, while our agent can only achieve 70%. On the other hand, when the objects are positioned at random, the DQN agent is not able to learn an effective policy within 1000 epochs, with the number of positive items collected fluctuating around 50%. So on this game variant, our agent\u2019s performance is markedly better than DQN\u2019s.\nThis is also the case for our final experiment where the agent is trained on the grid variant and tested on the random variant. While our agent rapidly attains a percentage of approximately 70%, and then fluctuates around that value, DQN is again unable to do better than chance after 1000 epochs. Although we haven\u2019t run the experiment long enough to confirm this, we hypothesise that, while DQN might eventually learn to play the random game effectively when trained on the same game, it will never achieve competence at the random game when trained on the grid setup."}, {"heading": "5 Discussion", "text": "We have proposed a hybrid neural-symbolic, end-to-end reinforcement learning architecture, and claimed that it addresses a number of drawbacks inherent in the current generation of DRL systems. To support this claim, we presented a simple prototype system conforming to the architecture and demonstrated it on several variants of a basic video game. Although the present system cannot learn a globally optimal policy for these games, it learns effectively in all of them. Moreover, even though the system is only a preliminary proof-of-concept with many limitations (to be discussed shortly), it dramatically outperforms DQN on the most difficult game variant, in which the initial placement of objects is random. In this game, DQN\u2019s performance didn\u2019t exceed chance level after 1000 epochs of training, while our system acquired an effective policy in just 200. We conjecture that DQN struggles with this game because it has to form a statistical picture of all possible object placements, which would require a much larger number of games. In contrast, thanks to the conceptual abstraction made possible by its symbolic front end, our system very quickly \u201cgets\u201d the game and forms a set of general rules that covers every possible initial configuration. This demonstration merely hints at the potential for a symbolic front end to promote data efficient learning, potential that we aim to exploit more fully in future work.\nOur proof-of-concept system also illustrates one aspect of the architecture\u2019s inherent capacity for transfer learning. After training, the unsupervised neural back end of the system is able to form a symbolic representation of any given frame within the micro-world of the game. In effect it has acquired the ontology of that micro-world, and this capability can be applied to any game within that micro-world irrespective of its specific rules. In the present case, no re-training of the back end was required when the system was applied to new variants of the game. However, this form of transfer learning operates at a superficial level. On a more abstract level, the architecture supports far more powerful forms of transfer learning. The key is for the system to understand when a new situation is analogous to one previously encountered or, more potently, to hypothesise that a new\nsituation contains elements of several previously encountered situations combined in a novel way. In the present system, this capability is barely exploited. But in future work we aim to explore the full potential for analogical reasoning made possible by symbolic representation.\nFinally, because the fundamental mode of operation of the front end of our system is to carry out inference with symbolic representations, there is a humanly-comprehensible chain of justifications for the decisions it makes. For our benchmark example, every action choice can be analysed in terms of the Q functions involved in the decision. Given that these Q functions describe what types of objects are involved in the interaction as well as their relations, we can track back the reasons that led to a certain decision. If the agent chooses to move upwards towards a positive object, for example, we will see that the Q function describing the interaction between this positive object and the agent assigns positive reward to the action of going up. This is a step towards DRL systems with greater transparency.\nThere are several avenues of further work to explore as we move to more complex games and richer environments with the aim of building a more complete implementation of the deep symbolic reinforcement learning architecture. As far as the neural back end is concerned, we intend to make far more extensive use of the state of the art in deep learning. In particular, a more sophisticated deep network capable of unsupervised learning of disentangled representations (having a compositional structure) will be required to handle more realistic images than occur in our benchmark games (eg: (Higgins et al., 2016)).\nSimilarly, the symbolic back end can draw far more heavily on achievements in classical AI than at present. Three potential elaborations to the architectural blueprint are particularly promising. First, the incorporation of inductive logic programming (Muggleton, 1991) would enable a more powerful form of generalisation to be applied to the Q function (D\u017eeroski et al., 1998). Recall the claim made in the introduction that general intelligence rests on the ability to determine that a newly encountered situation is similar to one or more situations encountered in the past. Much as a deep neural network is used as a function approximator to achieve this in conventional DRL, inductive logic programming can be used to do this in a symbolic context. Second, to further amplify the system\u2019s ability to determine similarity between current and past situations, formal techniques for analogical reasoning can be deployed, such as the structure mapping engine or one of its relatives (Gentner and Forbus, 2011). This technique is especially appropriate when the challenge is not so much to generalise from large numbers of past scenarios, but rather to see that the ongoing situation shares features with a single recorded past episode.\nA third way to elaborate the architecture is to build in a planning component that exploits the knowledge of the causal structure of the domain acquired during the learning process. In domains with sparse reward, it\u2019s often possible for an agent to discover a sequence of actions leading to a reward state through off-line search rather than on-line exploration. Contemporary logic-based planning\nmethods are capable of efficiently finding large plans in complex domains (eg:(Rintanen, 2012)), and it would be rash not to exploit the potential of these techniques.\nFinally, we are by no means ruling out the possibility that the symbolic components of our proposed architecture can themselves be implemented using neural networks. The success of deep learning has inspired a good deal of novel research, much of which involves the innovative use of neural networks with novel architectures. This research could well produce neurally-based implementations of the symbolic reasoning functions we have been advocating here, and there may be advantages to this approach. In the mean time, an architecture that combines deep neural networks with directly implemented symbolic reasoning seems like a promising research direction."}, {"heading": "Acknowledgments", "text": "We are grateful to Nvidia Corporation for the donation of a high-end GPU. Marta Garnelo is supported by an EPSRC doctoral training award.\nAuthor Contributions\nMG & MS conceptualised the problem and the technical framework. MG developed and tested the algorithms. MG & MS wrote the paper. MG & KA carried out comparison with DQN."}], "references": [{"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Minds and Machines,", "citeRegEx": "Legg and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Legg and Hutter.", "year": 2007}, {"title": "Data-efficient learning of feedback policies from image pixels using deep dynamical models", "author": ["John-Alexander M. Assael", "Niklas Wahlstr\u00f6m", "Thomas B. Sch\u00f6n", "Marc Peter Deisenroth"], "venue": null, "citeRegEx": "Assael et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Assael et al\\.", "year": 2015}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Shixiang Gu", "Timothy P. Lillicrap", "Ilya Sutskever", "Sergey Levine"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Emilio Parisotto", "Lei Jimmy Ba", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Parisotto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Classifying options for deep reinforcement learning", "author": ["Kai Arulkumaran", "Nat Dilokthanakul", "Murray Shanahan", "Anil Anthony Bharath"], "venue": null, "citeRegEx": "Arulkumaran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arulkumaran et al\\.", "year": 2016}, {"title": "Successor features for transfer in reinforcement learning", "author": ["Andr\u00e9 Barreto", "R\u00e9mi Munos", "Tom Schaul", "David Silver"], "venue": null, "citeRegEx": "Barreto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barreto et al\\.", "year": 2016}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["Xiaoxiao Guo", "Satinder Singh", "Honglak Lee", "Richard L. Lewis", "Xiaoshi Wang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Graying the black box: Understanding dqns", "author": ["Tom Zahavy", "Nir Ben-Zrihem", "Shie Mannor"], "venue": null, "citeRegEx": "Zahavy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zahavy et al\\.", "year": 2016}, {"title": "Generality in artificial intelligence", "author": ["John McCarthy"], "venue": "Communications of the ACM,", "citeRegEx": "McCarthy.,? \\Q1987\\E", "shortCiteRegEx": "McCarthy.", "year": 1987}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad.,? \\Q1990\\E", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Perception as abduction: Turning sensor data into meaningful representation", "author": ["Murray Shanahan"], "venue": "Cognitive science,", "citeRegEx": "Shanahan.,? \\Q2005\\E", "shortCiteRegEx": "Shanahan.", "year": 2005}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Tagger: Deep unsupervised perceptual grouping", "author": ["Klaus Greff", "Antti Rasmus", "Mathias Berglund", "Tele Hotloo Hao", "J\u00fcrgen Schmidhuber", "Harri Valpola"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2016}, {"title": "Early visual concept learning with unsupervised deep learning", "author": ["Irina Higgins", "Loic Matthey", "Xavier Glorot", "Arka Pal", "Benigno Uria", "Charles Blundell", "Shakir Mohamed", "Alexander Lerchner"], "venue": null, "citeRegEx": "Higgins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": null, "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Building machines that learn and think like people", "author": ["Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "An analysis of first-order logics of probability", "author": ["Joseph Y. Halpern"], "venue": "Artificial intelligence,", "citeRegEx": "Halpern.,? \\Q1990\\E", "shortCiteRegEx": "Halpern.", "year": 1990}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "The second naive physics manifesto", "author": ["Patrick J. Hayes"], "venue": null, "citeRegEx": "Hayes.,? \\Q1985\\E", "shortCiteRegEx": "Hayes.", "year": 1985}, {"title": "Representations of commonsense knowledge", "author": ["Ernest Davis"], "venue": null, "citeRegEx": "Davis.,? \\Q1990\\E", "shortCiteRegEx": "Davis.", "year": 1990}, {"title": "Commonsense reasoning: An event calculus based approach", "author": ["Erik T. Mueller"], "venue": null, "citeRegEx": "Mueller.,? \\Q2014\\E", "shortCiteRegEx": "Mueller.", "year": 2014}, {"title": "Default reasoning about spatial occupancy", "author": ["Murray Shanahan"], "venue": "Artificial Intelligence,", "citeRegEx": "Shanahan.,? \\Q1995\\E", "shortCiteRegEx": "Shanahan.", "year": 1995}, {"title": "Relief impression image detection: Unsupervised extracting objects directly from feature arrangements of deep cnn", "author": ["Guiying Li", "Junlong Liu", "Chunhui Jiang", "Ke Tang"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "The frame problem", "author": ["Murray Shanahan"], "venue": "The Stanford Encyclopedia of Philosophy,", "citeRegEx": "Shanahan.,? \\Q2016\\E", "shortCiteRegEx": "Shanahan.", "year": 2016}, {"title": "Inductive logic programming", "author": ["Stephen Muggleton"], "venue": "New generation computing,", "citeRegEx": "Muggleton.,? \\Q1991\\E", "shortCiteRegEx": "Muggleton.", "year": 1991}, {"title": "Relational reinforcement learning", "author": ["Sa\u0161o D\u017eeroski", "Luc De Raedt", "Hendrik Blockeel"], "venue": "In Inductive Logic Programming: 8th International Conference,", "citeRegEx": "D\u017eeroski et al\\.,? \\Q1998\\E", "shortCiteRegEx": "D\u017eeroski et al\\.", "year": 1998}, {"title": "Computational models of analogy", "author": ["Dedre Gentner", "Kenneth D. Forbus"], "venue": "Wiley interdisciplinary reviews: cognitive science,", "citeRegEx": "Gentner and Forbus.,? \\Q2011\\E", "shortCiteRegEx": "Gentner and Forbus.", "year": 2011}, {"title": "Planning as satisfiability: heuristics", "author": ["Jussi Rintanen"], "venue": "Artificial intelligence,", "citeRegEx": "Rintanen.,? \\Q2012\\E", "shortCiteRegEx": "Rintanen.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Deep reinforcement learning (DRL), wherein a deep neural network (LeCun et al., 2015; Schmidhuber, 2015) is used as a function approximator within a reinforcement learning system (Sutton and Barto, 1998), has recently been shown to be effective in a number of domains, including Atari video games (Mnih et al.", "startOffset": 65, "endOffset": 104}, {"referenceID": 1, "context": ", 2015; Schmidhuber, 2015) is used as a function approximator within a reinforcement learning system (Sutton and Barto, 1998), has recently been shown to be effective in a number of domains, including Atari video games (Mnih et al.", "startOffset": 101, "endOffset": 125}, {"referenceID": 2, "context": ", 2015; Schmidhuber, 2015) is used as a function approximator within a reinforcement learning system (Sutton and Barto, 1998), has recently been shown to be effective in a number of domains, including Atari video games (Mnih et al., 2015), robotics (Levine et al.", "startOffset": 219, "endOffset": 238}, {"referenceID": 3, "context": ", 2015), robotics (Levine et al., 2016), and the game of Go (Silver et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 4, "context": ", 2016), and the game of Go (Silver et al., 2016).", "startOffset": 28, "endOffset": 49}, {"referenceID": 5, "context": "DRL can be thought of as a step towards instantiating the formal characterisation of universal artificial intelligence presented by Hutter(Legg and Hutter, 2007), a theoretical framework for artificial general intelligence (AGI) founded on reinforcement learning.", "startOffset": 138, "endOffset": 161}, {"referenceID": 6, "context": "Each of these shortcomings is an active area of research in the DRL community, where data efficient learning (Assael et al., 2015; Gu et al., 2016), transfer learning (Parisotto et al.", "startOffset": 109, "endOffset": 147}, {"referenceID": 7, "context": "Each of these shortcomings is an active area of research in the DRL community, where data efficient learning (Assael et al., 2015; Gu et al., 2016), transfer learning (Parisotto et al.", "startOffset": 109, "endOffset": 147}, {"referenceID": 8, "context": ", 2016), transfer learning (Parisotto et al., 2015; Arulkumaran et al., 2016; Barreto et al., 2016; Rusu et al., 2016), planning (Guo et al.", "startOffset": 27, "endOffset": 118}, {"referenceID": 9, "context": ", 2016), transfer learning (Parisotto et al., 2015; Arulkumaran et al., 2016; Barreto et al., 2016; Rusu et al., 2016), planning (Guo et al.", "startOffset": 27, "endOffset": 118}, {"referenceID": 10, "context": ", 2016), transfer learning (Parisotto et al., 2015; Arulkumaran et al., 2016; Barreto et al., 2016; Rusu et al., 2016), planning (Guo et al.", "startOffset": 27, "endOffset": 118}, {"referenceID": 11, "context": ", 2016), planning (Guo et al., 2014; Vezhnevets et al., 2016), and transparency (Zahavy et al.", "startOffset": 18, "endOffset": 61}, {"referenceID": 12, "context": ", 2016), planning (Guo et al., 2014; Vezhnevets et al., 2016), and transparency (Zahavy et al.", "startOffset": 18, "endOffset": 61}, {"referenceID": 13, "context": ", 2016), and transparency (Zahavy et al., 2016) are all hot topics.", "startOffset": 26, "endOffset": 47}, {"referenceID": 14, "context": "Thanks to their compositional structure, such representations are amenable to endless extension and recombination, an essential feature for the acquisition and deployment of high-level abstract concepts, which are key to general intelligence (McCarthy, 1987).", "startOffset": 242, "endOffset": 258}, {"referenceID": 15, "context": "A major obstacle here is the symbol grounding problem (Harnad, 1990; Shanahan, 2005).", "startOffset": 54, "endOffset": 84}, {"referenceID": 16, "context": "A major obstacle here is the symbol grounding problem (Harnad, 1990; Shanahan, 2005).", "startOffset": 54, "endOffset": 84}, {"referenceID": 0, "context": "Deep neural networks in particular have proven to be remarkably effective for supervised learning from large datasets using backpropagation (LeCun et al., 2015; Schmidhuber, 2015).", "startOffset": 140, "endOffset": 179}, {"referenceID": 17, "context": "Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013).", "startOffset": 209, "endOffset": 321}, {"referenceID": 18, "context": "Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013).", "startOffset": 209, "endOffset": 321}, {"referenceID": 19, "context": "Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013).", "startOffset": 209, "endOffset": 321}, {"referenceID": 20, "context": "Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013).", "startOffset": 209, "endOffset": 321}, {"referenceID": 21, "context": "Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013).", "startOffset": 209, "endOffset": 321}, {"referenceID": 22, "context": "(For a related set of desiderata see (Lake et al., 2016).", "startOffset": 37, "endOffset": 56}, {"referenceID": 2, "context": "In a conventional DRL system, such as DQN (Mnih et al., 2015), this is achieved through the generalising capabilities of the neural network that approximates the Q function (or the value function or policy function, depending on the style of reinforcement learning in question).", "startOffset": 42, "endOffset": 61}, {"referenceID": 14, "context": "Classically, the theoretical foundation for such a representational medium is first-order logic, and the underlying language comprises predicates, quantifiers, constant symbols, function symbols, and boolean operators (McCarthy, 1987).", "startOffset": 218, "endOffset": 234}, {"referenceID": 23, "context": "To handle uncertainty, we propose probabilistic first-order logic for the semantic underpinnings of the low-dimensional conceptual state space representation into which the neural front end must map the system\u2019s high-dimensional raw input (Halpern, 1990).", "startOffset": 239, "endOffset": 254}, {"referenceID": 24, "context": "For example, in most DRL systems that take visual input, spatial priors, such as the likelihood that similar 2D patterns will appear in different locations in the visual field, are implicit in the convolutional structure of the network (Bengio et al., 2013).", "startOffset": 236, "endOffset": 257}, {"referenceID": 25, "context": "But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014).", "startOffset": 90, "endOffset": 171}, {"referenceID": 24, "context": "But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014).", "startOffset": 90, "endOffset": 171}, {"referenceID": 26, "context": "But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014).", "startOffset": 90, "endOffset": 171}, {"referenceID": 22, "context": "But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014).", "startOffset": 90, "endOffset": 171}, {"referenceID": 27, "context": "But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014).", "startOffset": 90, "endOffset": 171}, {"referenceID": 28, "context": "Consisting mostly of empty space, it contains a variety of objects that tend to persist over time and have various attributes such as shape, colour, and texture (Shanahan, 1995).", "startOffset": 161, "endOffset": 177}, {"referenceID": 29, "context": "As shown by Li et al (Li et al., 2016), salient areas in an image will result in higher activations throughout the layers of a convolutional network.", "startOffset": 21, "endOffset": 38}, {"referenceID": 30, "context": "So future implementations will have to handle this question of circumscribing relevance in a more nuanced way (Shanahan, 2016).", "startOffset": 110, "endOffset": 126}, {"referenceID": 20, "context": "In particular, a more sophisticated deep network capable of unsupervised learning of disentangled representations (having a compositional structure) will be required to handle more realistic images than occur in our benchmark games (eg: (Higgins et al., 2016)).", "startOffset": 237, "endOffset": 259}, {"referenceID": 31, "context": "First, the incorporation of inductive logic programming (Muggleton, 1991) would enable a more powerful form of generalisation to be applied to the Q function (D\u017eeroski et al.", "startOffset": 56, "endOffset": 73}, {"referenceID": 32, "context": "First, the incorporation of inductive logic programming (Muggleton, 1991) would enable a more powerful form of generalisation to be applied to the Q function (D\u017eeroski et al., 1998).", "startOffset": 158, "endOffset": 181}, {"referenceID": 33, "context": "Second, to further amplify the system\u2019s ability to determine similarity between current and past situations, formal techniques for analogical reasoning can be deployed, such as the structure mapping engine or one of its relatives (Gentner and Forbus, 2011).", "startOffset": 230, "endOffset": 256}, {"referenceID": 34, "context": "methods are capable of efficiently finding large plans in complex domains (eg:(Rintanen, 2012)), and it would be rash not to exploit the potential of these techniques.", "startOffset": 78, "endOffset": 94}], "year": 2016, "abstractText": "Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-ofconcept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system \u2013 though just a prototype \u2013 learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.", "creator": "LaTeX with hyperref package"}}}