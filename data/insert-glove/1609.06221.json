{"id": "1609.06221", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "An Efficient Method of Partitioning High Volumes of Multidimensional Data for Parallel Clustering Algorithms", "abstract": "sampled An 1.3965 optimal krestovsky data palaeopropithecus partitioning in verinag parallel & cochain amp; distributed implementation of tuscumbia clustering wiesler algorithms kulenovic is lyons a fould necessary computation as denistone it ensures independent task completion, mahavira fair devouring distribution, mickle less airbrakes number of i-5 affected mellencamp points and better & amp; faster novorossiisk merging. indologists Though interprovincial partitioning using Kd belneftekhim Tree is 28.7 being pranked conventionally used uninspected in wuterich academia, danell it infidel suffers from thumpamon performance drenches sufferance and bias (non equal 91-0 distribution) as perathoner dimensionality nistelrooy of data increases yankeenets and begetting hence is not gunzenhausen suitable for crg practical use 500-meter in industry 314th where german-controlled dimensionality can be tilla of order waunakee of nezam 100s to 1000s. perkiomen To 24-17 address mentewab these beik issues u2019 we propose ramamoorthy two new bides partitioning tsakiris techniques zveno using 200,000,000 existing nnn mathematical models & midrib amp; europos study district-free their 39.07 feasibility, wallabi performance (tasse bias and quandary partitioning speed) & amp; possible variants in sabatino choosing glosserman initial wphl seeds. day-to-day First shopowners method uses varejao an agreed-upon n captain-general dimensional koffler hashed grid snared based exposition approach ddt which is based on mapping the points in space karabila to voluble a set of monaco cubes dhruva which hashes mishaal the straggler points. Second method uses jijia a cottian tree of voronoi cambia planes tamakoshi where kamaishi each plane corresponds langenhagen to rutaremara a partition. mobilized We found bozulich that grid onnie based approach was muslih computationally impractical, while ecsc using cheron a partinico tree 95.0 of jacksoni voronoi sarason planes (using voyvoda scalable rodic K - kornak Means + + leprince initial seeds) drastically 51,600 outperformed underoath the azour Kd - tree schau tree rear-engined method ancheta as dimensionality tik-tok increased.", "histories": [["v1", "Tue, 20 Sep 2016 15:26:37 GMT  (656kb)", "http://arxiv.org/abs/1609.06221v1", "5 pages, 6 figures"]], "COMMENTS": "5 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["saraswati mishra", "avnish chandra suman"], "accepted": false, "id": "1609.06221"}, "pdf": {"name": "1609.06221.pdf", "metadata": {"source": "CRF", "title": "An Efficient Method of Partitioning High Volumes of Multidimensional Data for Parallel Clustering Algorithms", "authors": ["Saraswati Mishra", "Avnish Chandra Suman"], "emails": ["saraswatimishra18@gmail.com", "avnishchandrasuman@gmail.com"], "sections": [{"heading": null, "text": "www.ijera.com 67 | P a g e\nKeywords: Clustering, Data Partitioning, Parallel Processing, KD Tree, KD-Tree, Voronoi Diagrams\nI. INTRODUCTION While profiling a hybrid (parallel &\ndistributed) implementation of OPTICS (Goel et all., 2015) algorithm we had an observation that over 50% our threads were outperforming the rest by huge margins. Our method was to 1. partition existing data into x parts 2. treat each part as a separate input and run the parallel OPTICS thread on each 3. merge the resultant clusters. We used Kd \u2013 tree (Bentley, 1975) to make partitions. K-d tree is a variance of binary tree where each node represents a data point in n-dimensional space. Every leaf node of k-d tree represents a splitting of a (n-1) dimensional hyper-plane resulting in two half-planes which can be thought of as partitions. Following method was used. 1. Start with dimension having highest variance\nand divide data set based on value points in that dimension only. Result is two different partitions. 2. Repeat the process with next dimension of highest variance until desired numbers of partitions are made. A sample representation will look something like this\nWe observed various limitations. Assume x\npartitions, n data points, m partitions, d dimensions 1. Data scan was needed in every stage i.e. for\ngetting m partitions we needed O(m) scans over same points again and again. 2. Median finding was a costly operation of order O(n). However when executed for every nonleaf node , the overall cost was of order O(mn) 3. K-d tree doesn't guarantee considering every dimension. In-fact there it exhibits a bias towards a few dimensions of high variance, hence points in a partition can be relatively\nwww.ijera.com 68 | P a g e\nfar than points in different dimensions for a large d. This will lead to a poor clustering result. Also as the number of dimensions tend to increase, the performance of k-d tree with regards to bias decreases. 4. Partitions are half planes. They have a general tendency to result in more number of affected points in uniformly or near-uniformly distributed space. 5. No inherent merging structure. A merging strategy needs to be implemented\nThe most concerning of these at that time\nfor us was 1 & 2 as they were highly serial and slow part of our parallel implementation. We wanted to optimize the multiple passes over data to as few as possible. Also we wanted a better way of finding median or near median. In next section we will see how we tried solving 1 & 2 by splitting our ddimensional space into d-cubes such that in a single pass we determine the points in every cube & their cardinality and other computations which improves median finding performance significantly. However we soon realized the impractical aspects of our approach and it inherently lacking the solutions to 3, 4 & 5. We moved on to next approach where we partition our space using n-dimensional voronoi diagrams and found it to be extraordinarily outperforming kd-tree with proper initial seeds.\nII. N-CUBE BASED APPROACH The proposed partitioning approach works by initially dividing the n-dimensional space into nCubes by making y splits along each dimension so that we obtain (y+1)n cubes. The basic idea here is to create an overall index (a summary in space based on cubes) such that for any partitioning computation, we only need to use this summary and not the entire data. n-Cubes work as following"}, {"heading": "Algorithm: Construct Cubes", "text": "Input: Set of x points p in space (p1,p2...px) where each px is (x1,x2..xn). Number of partitions m k: a natural number 1<=k #higher values for k ensures better overall distribution but lower performance."}, {"heading": "Output: Set of Cubes (c1, c2 \u2026. cm )", "text": "Where m = (y+1)n Procedure: # Finding an optimal y & initializing cube boundaries"}, {"heading": "Let minxn be min (px) in n", "text": "th dimension Let maxxn be max (px) in n\nth dimension Let Y = ky. Let M = (Y+1)n for each ci in (c1..cM)\nBoundary(cMn) = {(i-1)*(maxn-minn)/M , i*(maxn-minn)/M} Binary sort c For each pi in (p1...px) Find pi\u2019s location in p. Add pi to cM cM.totalPonits++"}, {"heading": "Algorithm: Find Median", "text": "Input : Set of Cubes (c1...cM), Total Points Output : Median along a dimension n say mn Procedure : P : total points in space x=0; While x<P/2 Move to next cube , x = x+Cm.totalPoints #We stop at the cube that contains our median (or a close approximation if k is too low or too high) Find median of set of points in Cm.\nWe can see that cube creation is an O(n+logn) task while median creation is an O(n/M) task which looks very efficient . The approach should have worked in two passes over data plus a single pass on grid cells. However there were major design & implementation issues with this approach 1. The number of cubes is (y+1)n. Even if y is 2,\nfor a huge n, we get 2n cells. This grows closer to total number of data points. 2. Programmatically unfeasible in direct sense. Accessing n-dimensional arrays needs n loops, we don\u2019t know n beforehand. Solution is to convert n-d cells to 1-d (resulting in very long 1-d array). Unable to allocate memory on stack for the 1-d array. 3. Too many cells, sparse cells, data distribution across cells not uniform at all. 4. Most of the partitions will be empty, even when the number of data points N is large, leading to extreme waste of memory and CPU time. 5. Hence we conclude that the method was not Suitable for > 2d data 6. Didn\u2019t solve problem of bias but tends to worsen it with a higher cubes number\nIII. VORONOI DIAGRAM BASED PARTITIONING\nOur second approach involves voronoi diagrams (Aurenhammer,1991). Our goal is to construct a partitioning scheme that handles highdimensionality as well and not just provide good performance by ignoring a lot of dimensions. It is also necessary that partitions should not hold too many or too few points. Number of passes on data should be as less as possible preferably ~1. An efficient way to satisfy problem 1 & 2 of kd-tree is a tree structure that needs O(log n) number of comparisons on average to distribute a\nwww.ijera.com 69 | P a g e\npoint and to determine the affected partitions where n is the number of nodes in the tree. One way to satisfy problem of considering all dimensions all together is to use a Voronoi diagram which partitions the space into Voronoi cells, directed by a set of split points Q = q1, q2, . . . such that for each cell corresponding to split point qi, the points x in that cell are nearer to qi than to any other split point in Q. Hence, by constructing a tree of Voronoi diagrams, we can satisfy our two major concerns. Let\u2019s call such a structure as v_tree. The top or root node of a v_tree gives a brief summary of the whole data and is split to many Voronoi cells which are split as well and so on."}, {"heading": "Construction of v_tree", "text": "At each level, find k (2) centre 1. Points must be appropriately spaced and far 2. Use scalable kmeans++ (Bahmani et all., 2012)\nor gnat technique (Brin ,1995) 3. Assign all points to either centre based on\ndistance 4. All points that lie in current to current + eps\nboundary (Goel et all., 2015) are considered affected. 5. Delete extra points stored in parent node to remove redundancy Repeat for new sets until requisite number of partitions found."}, {"heading": "Data Structure", "text": "A minimal n-voronoi-tree implementation data structure in c will look like this. typedef struct member { int id; float *val; } member; struct v_node { int level; int core1,core2; member *mem1,*mem2,*mem_overlap; struct v_node *left,*right;\nint count_mem1,count_mem2, count_overlap, total_count; } struct v_tree { int levels; struct node *head; } Head node is the summary of Entire Data. Each parent node contains summary of points in child nodes. The exact points are stored until data is partitioned at level & deleted as soon as we move to next level. Note that v_tree might not necessarily be binary. More than two centres can be chosen. The leaf nodes are our final partitions, and going up the tree inherently makes a merging structure for resultant clusters. Load in each partition will depend upon the center chosen and distribution of data in n-dimensional space."}, {"heading": "How to choose initial seeds?", "text": "1. Select randomly: Choosing centre randomly\ndoesn\u2019t guarantee or breach anything and simply leaves thing to the centre chosen and distribution of data. There is equal probability of getting each load distribution. Hence the probability of getting a perfect load balance tends to zero. 2. GNAT Approach: Suppose we need n seeds. We start by selecting a point at random. The next point is chosen such that its distance from first point is maximum, the third point is chosen such that its distance from sum of previous two points is maximum. Similarly forth point should be the point farthest from sum of first three points and so on. This approach is good in terms of load balancing for a big value of n, but cannot guarantee good balance for small n (~2- 5). 3. Scalable K-Means++ Approach: Suppose k centre are needed, C be the set of initial seeds, then a. Sample a point uniformly at random from the data points. b. For each data point p, compute it\u2019s distance from nearest centre. c. Choose a m point p using Weighted probability distribution where a point p is chosen with probability proportional to D(p)2. Assume it be new centre d. If you have k centres, proceed with partitioning, else repeat 2 & 3 The centres that we get tend to be close to the centroids of clusters present in data, assuming there are k-clusters. Load distribution is entirely dependent on the data; however we can be optimistic about not getting very biased distribution with reallife datasets.\nwww.ijera.com 70 | P a g e\nSuch a partitioning hugely tends to reduce the time spent in merging the final clusters as there will be minimal affected points. 4. More than 2 centre: can be used in\nimplementation to satisfy various criteria. a. All the above approaches (or any random or\nprobability based approach) will tend to good balancing as we increase number of centre. This can be proved mathematically using induction. We receive a perfect balance when numbers of points equals number of centre i.e. each point is a partition. b. In a case when number of partitions is not in power of 2. c. Different number of centre at different level can be used for perfect guided partitioning. 5. How about Median: Points very close to median and on same axis as centre will produce exact partitions (similar to kd-tree), however complexity sumps up to O (kd-tree) +O (v_tree)."}, {"heading": "IV. EXPERIMENTATION & RESULTS", "text": "We used following environment to execute test & profile. Execution Environment: - Ubuntu 13.04, Intel Core i3 (3rd generation), 2.4GHz (2 cores hyper threaded), 4GB RAM, 3MB L2 Cache. Compilation Environment: - C, gcc, gdb, gprof, vampir, Geany IDE .Visualization of output was done using geogebra. Test Data-sets: (no. of points x no. of dimensions, double precision data) 1. 100x2, 2. 700x9, 3. 1500x1024, 4. 4000x1024, 5. 40000x1024\nWe noticed an increase in load bias as number of partitions increase. Distribution was almost uniform with uniform data. However we can see that Kd-tree (right) provides a better distribution with uniformly distributed data. However this will decrease with number of dimensions.\nwww.ijera.com 71 | P a g e\nPerformance (in approx ~10sec units) Data Set Approach 700*9 1500* 1024 4000*1024 40000*1024\nKd - Tree 0.02 9.88 46.67 512\nv_tree (Scalable Kmeans++)\n0.01 3.05 7.61 73.34\nv_tree (Median)\n0.02 9.96 54.43 542\nV. CONCLUSION We have seen that for higher\ndimensionality the approach proposed takes very less time as compared to kd-tree approach; however v_tree technique needs a load balanced variant to boast perfect results. Our future works will include better load balancing, comparison of merging time and distribution time, parallelizing the approach and implementing a grid based alternative."}, {"heading": "APPENDIX", "text": "Related Code, Data Sets & Results can be requested from https://drive.google.com/file/d/0Bxo9wQ432jhla1dQ NWFrbnM4bnc/view?usp=sharing"}], "references": [{"title": "Parallelizing OPTICS for Commodity Clusters", "author": ["Poonam Goyal", "Sonal Kumari", "Dhruv Kumar", "Sundar Balasubramaniam", "Navneet Goyal", "Saiyedul Islam", "Jagat Sesh Challa"], "venue": "In Proceedings of the 2015 International Conference on Distributed Computing and Networking (ICDCN '15)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Communications of the ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1975}, {"title": "Near neighbor search in large metric spaces", "author": ["S. Brin"], "venue": "in: Proceedings of the International Conference on Very Large Databases (VLDB),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}], "referenceMentions": [], "year": 2016, "abstractText": "An optimal data partitioning in parallel/distributed implementation of clustering algorithms is a necessary computation as it ensures independent task completion, fair distribution, less number of affected points and better & faster merging. Though partitioning using Kd-Tree is being conventionally used in academia, it suffers from performance drenches and bias (non equal distribution) as dimensionality of data increases and hence is not suitable for practical use in industry where dimensionality can be of order of 100\u2019s to 1000\u2019s. To address these issues we propose two new partitioning techniques using existing mathematical models & study their feasibility, performance (bias and partitioning speed) & possible variants in choosing initial seeds. First method uses an n-dimensional hashed grid based approach which is based on mapping the points in space to a set of cubes which hashes the points. Second method uses a tree of voronoi planes where each plane corresponds to a partition. We found that grid based approach was computationally impractical, while using a tree of voronoi planes (using scalable K-Means++ initial seeds) drastically outperformed the Kd-tree tree method as dimensionality increased.", "creator": "\u00fe\u00ff"}}}