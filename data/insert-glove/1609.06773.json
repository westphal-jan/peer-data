{"id": "1609.06773", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning", "abstract": "broh Recently, mendizabal there has clelia been an blout increasing tigrinya interest in end - fractals to - end bassila speech recognition doba that snedeker directly agle transcribes riffage speech esselte to hodgen text blak without 4music any vinick predefined 135-meter alignments. landore One approach is naranjo the attention - lewicki based encoder - lucite decoder abdelbaset framework that learns a declaim mapping warp between variable - tightwads length 79.55 input dams and 3,228 output havel sequences in rwisereka one step using 11.84 a eurocentric purely data - driven 1.4726 method. The 49b attention model has often fisseha been shown volksschule to ruck improve nozze the liaisons performance over another end - to - suryadi end potawatomis approach, ramakrishna the Connectionist burkinabe Temporal Classification (1.5375 CTC ), mainly because it explicitly uses the mohagher history harumi of the target crudely character asti without any mobygames conditional maruthi independence mayumi assumptions. nordlys However, 70.45 we observed irama that leduc the bnt attention 600,000-strong model agaist has shown poor ogres results milkmen especially subdirector in surfed noisy d-6 condition nso and nokia is hard 1969-1975 to arial be trained reynard in dudikoff the initial mareth training stage with popat long input ismailis sequences, gown as poussaint compared with makadmeh CTC. lipid-soluble This masondo is bertelsman because cd/lp the attention model breathed is accuweather too whitehawk flexible 2340 to vinschgau predict raghuvaran proper alignments foolishness in such hashlosha cases due to the lack of left - atiak to - wednsday right constraints as anthers used pillinger in CTC. This paper presents birectified a omigod novel goatherd method for end - radical-socialist to - 1-900-420-8002 end speech recognition to ,470 improve vac robustness and jadzia achieve fast convergence outer-space by 57.19 using a joint CTC - alternative-rock attention model 2/9th within the mckeith multi - manderlay task juh learning swivel framework, mazurkiewicz thereby cozy mitigating vivere the alignment demonte issue. hdax An edaran experiment wut on the divljan WSJ lamour and 73.01 CHiME - 4 tasks vermonter demonstrates its wignacourt advantages bagli over svrljig both oxa the CTC and attention - 40/40 based 3,023 encoder - decoder baselines, showing 6. cortex-a8 6 - vinas 10. tendencia 3% relative weber improvements bernoldi in corian Character Error Rate (mosquitoes CER ).", "histories": [["v1", "Wed, 21 Sep 2016 22:48:53 GMT  (5155kb,D)", "http://arxiv.org/abs/1609.06773v1", null], ["v2", "Tue, 31 Jan 2017 21:00:01 GMT  (5466kb,D)", "http://arxiv.org/abs/1609.06773v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["suyoun kim", "takaaki hori", "shinji watanabe"], "accepted": false, "id": "1609.06773"}, "pdf": {"name": "1609.06773.pdf", "metadata": {"source": "CRF", "title": "JOINT CTC-ATTENTION BASED END-TO-END SPEECH RECOGNITION USING MULTI-TASK LEARNING", "authors": ["Suyoun Kim", "Takaaki Hori", "Shinji Watanabe"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 end-to-end, speech recognition, connectionist temporal classification, attention, multi-task learning"}, {"heading": "1. INTRODUCTION", "text": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9]. The traditional hybrid approach, Deep Neural Networks - Hidden Markov Models (DNN-HMM), factorizes the system into several components trained separately (i.e. acoustic model, contextdependent phone model, pronunciation model, and language model) based on conditional independence assumptions (including Markov assumptions) and approximations [10, 11]. Unlike such hybrid approaches, the end-to-end model learns acoustic frames to character mappings in one step towards the final objective of interest, and attempts to rectify the suboptimal issues that arise from the disjoint training procedure.\nRecent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6]. Both methods address the problem of variable-length\nThe work is performed during Suyoun Kim is at MERL.\ninput and output sequences. The key idea of CTC is to use intermediate label representation allowing repetitions of labels and occurrences of blank labels to identify less informative frames. The CTC loss can be efficiently calculated by the forward-backward algorithm, but it still predicts targets for every frame, and assumes that the targets are conditionally independent of each other.\nAnother approach, the attention-based encoder-decoder directly learns a mapping from acoustic frame to character sequences. At each output time step, the model emits a character conditioned on the inputs and the history of the target character. Since the attention model does not use any conditional independence assumption, it has often shown to improve Character Error Rate (CER) than CTC when no external language model is used [7]. However, in realenvironment speech recognition tasks, the model shows poor results because the alignment estimated in the attention mechanism is easily corrupted due to the noise. Another issue is that the model is hard to be learned from scratch due to the misalignment on longer input sequences, and therefore a windowing technique is commonly used to limit the area explored by the attention mechanism [7], but several parameters for windowing need to be determined manually depending on the training data.\nTo overcome the above misalignment issues, this paper proposes a novel end-to-end speech recognition method to improve performance and accelerate learning by using a joint CTC-attention model within the multi-task learning framework. The key to our approach is that we use a shared-encoder representation trained by both CTC and attention model objectives simultaneously. We think that the weakness of the attention model is due to lack of left-to-right constraints as used in DNN-HMM and CTC, making it difficult to train the encoder network with proper alignments in the case of noisy data and/or long input sequences. Our proposed method improves the performance by rectifying the alignment problem using the CTC loss function based on the forward-backward algorithm. Along with improving performance, our framework significantly speeds up learning with fast convergence. We evaluate our model on the WSJ and CHiME-4 tasks, and show that our system outperforms both the CTC and attention models in CER and learning speed."}, {"heading": "2. JOINT CTC-ATTENTION MECHANISM", "text": "In this section, we review the CTC in Section 2.1 and the attentionbased encoder-decoder in Section 2.2, addressing the variable (T ) length input frames, x = (x1, \u00b7 \u00b7 \u00b7 , xT ), andU length output characters, y = (y1, \u00b7 \u00b7 \u00b7 , yU ), where yu \u2208 {1, \u00b7 \u00b7 \u00b7 ,K}. K is the number of distinct labels. Then, our joint CTC-attention based end-to-end framework will be described in Section 2.3.\nar X\niv :1\n60 9.\n06 77\n3v 1\n[ cs\n.C L\n] 2\n1 Se\np 20\n16"}, {"heading": "2.1. Connectionist temporal classification (CTC)", "text": "The key idea of CTC [12] is to use intermediate label representation \u03c0 = (\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0T ), allowing repetitions of labels and occurrences of a blank label (\u2212), which represents the special emission without labels, i.e., \u03c0t \u2208 {1, \u00b7 \u00b7 \u00b7 ,K} \u222a {\u2212}. CTC trains the model to maximize P (y|x), the probability distribution over all possible label sequences \u03a6(y\u2032):\nP (y|x) = \u2211\n\u03c0\u2208\u03a6(y\u2032)\nP (\u03c0|x), (1)\nwhere y\u2032 is a modified label sequence of y, which is made by inserting the blank symbols between labels for allowing blanks in the output, i.e., yu \u2208 {1, \u00b7 \u00b7 \u00b7 ,K} \u222a {\u2212}.\nCTC is applied on top of Recurrent Neural Networks (RNNs), which is interpreted as the label distribution including the blank. The probability of label sequence P (\u03c0|x) is approximately computed by the product of the probability of each label based on the conditional independence assumption:\nP (\u03c0|x) \u2248 T\u220f\nt=1\nP (\u03c0t|x) = T\u220f\nt=1\nqt(\u03c0t) (2)\nwhere qt(\u03c0t) denotes the softmax activation of \u03c0t label in RNN output layer q at time t.\nThe CTC loss to be minimized is defined as the negative log likelihood of the ground truth character sequence y\u2217, i.e.\nLCTC ,\u2212 lnP (y\u2217|x). (3)\nThe probability distribution P (y|x) can be computed efficiently using the forward-backward algorithm as\nP (y|x) = |y\u2032|\u2211 u=1 \u03b1t(u)\u03b2t(u) qt(y\u2032u) , (4)\nwhere \u03b1t(u) is the forward variable, representing the total probability of all possible prefixes (y\u20321:u) that end with the u-th label, and \u03b2t(u) is the backward variable of all possible suffixes (y\u2032u:U ) and vice versa. The network can then be trained with standard backpropagation by taking the derivative of the loss function with respect to qt(k) for any k label including the blank.\nSince CTC does not explicitly model inter-label dependencies based on the conditional independence assumption in Eq. (2), the model is limited to character-level language information. Therefore, lexicon or language models are commonly incorporated, like the hybrid framework [2, 3]."}, {"heading": "2.2. Attention-based encoder-decoder", "text": "Unlike the CTC approach, the attention model directly predicts each target without requiring intermediate representation or any assumptions, improving CER as compared to CTC when no external language model is used [7]. The model emits each label distribution at u conditioning on previous labels according to the following recursive equations:\nP (y|x) = \u220f u P (yu|x, y1:u\u22121) (5)\nh = Encoder(x) (6) yu \u223c AttentionDecoder(h, y1:u\u22121). (7)\nThe framework consists of two RNNs: Encoder and AttentionDecoder, so that it is able to learn two different lengths of sequences based on the cross-entropy criterion. Encoder transforms x, to highlevel representation h = (h1, \u00b7 \u00b7 \u00b7 , hL) in Eq. (6), then AttentionDecoder produces the probability distribution over characters, yu, conditioned on h and all the characters seen previously y1:u\u22121 in Eq. (7). L is the number of skipped input frames, and L < T . Here, a special start-of-sentence(sos)/end-of-sentence(eos) token is added to the target set, so that the decoder completes the generation of the hypothesis when (eos) is emitted. The loss function of the attention model is computed from Eq. (5) as:\nLAttention , \u2212 lnP (y\u2217|x) = \u2212 \u2211 u lnP (y\u2217u|x, y\u22171:u\u22121) (8)\nwhere y\u22171:u\u22121 is the ground truth of the previous characters. The attention mechanism aids in the decoding procedure by integrating all the inputs h into cu based on their attention weight vectors au \u2208 RL+ over input L identifying where to focus at output step u. The following equations represent how to compute au and cu:\neu,l =  content-based: wT tanh(Wsu\u22121 + V hl + b)\nlocation-based: fu = F \u2217 \u03b1u\u22121 wT tanh(Wsu\u22121 + V hl + Ufu,l + b)\n(9)\nau,l = exp(\u03b3eu,l)\u2211 l exp(\u03b3eu,l)\n(10)\ncu = \u2211 l au,lhl (11)\nwhere w,W, V, F, U, b are trainable parameters, su\u22121 is the decoder state, \u03b3 is the sharpening factor [5], and * denotes convolution.\nau can be computed by the softmax of energy eu,l from two types of attention mechanisms: content-based and location-based [5] in Eq. (9). Both depend on the decoder state, su\u22121, and the content of input, hl. The location-based attention mechanism additionally uses convolutional features fu,l from the previous attention au\u22121.\nWith cu, su\u22121, and yu\u22121, the decoder generates next label yu and updates the state as:\nyu \u223c Generate(cu, su\u22121) (12) su = Recurrency(su\u22121, cu, yu), (13)\nwhere the Generate and Recurrency functions indicate a feedforward network and a recurrent network, respectively.\nIn practice, the approach has two main issues. (1) The model is weak on noisy speech data. The attention model is easily affected by noises, and generates misalignments because the model does not have any constraint that guides the alignments be monotonic as in DNN-HMM and CTC. (2) Another issue is that it is hard to be trained from scratch on larger input sequences via purely data-driven methods. To make training faster, the author [5, 7] constrains the attention mechanism to only consider inputs within a narrow range. However, this modification may limit the model\u2019s capability to extract useful information from long character sequences."}, {"heading": "2.3. Proposed model: Joint CTC-attention (MTL)", "text": "The idea of our model is to use a CTC objective function as an auxiliary task to train the attention model encoder within the multitask learning (MTL) framework. Figure 1 illustrates the overall ar-\nchitecture of our framework, where the encoder network is shared with CTC and attention models. Unlike the attention model, the forward-backward algorithm of CTC can enforce monotonic alignment between speech and label sequences. We therefore expect that our framework is more robust in acquiring appropriate alignments in noisy conditions. Another advantage of using CTC as an auxiliary task is that the network is learned quickly. In our experiments, rather than solely depending on data-driven attention methods to estimate the desired alignments in long sequences, the forward-backward algorithm in CTC helps to speed up the process of estimating the desired alignment without the aid of rough estimates of the alignment which requires manual effort. The proposed objective is represented as follows by using both attention model in Eq. (8) and CTC in Eq. (3):\nLMTL = \u03bbLCTC + (1\u2212 \u03bb)LAttention, (14)\nwith a tunable parameter \u03bb : 0 \u2264 \u03bb \u2264 1."}, {"heading": "3. EXPERIMENTS", "text": ""}, {"heading": "3.1. Data", "text": "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16]. The CHiME-4 corpus was recorded using a tablet device in everyday environments - a cafe, a street junction, public transport, and a pedestrian area. As input features, we used 40 mel-scale filterbank coefficients, with their first and second order temporal derivatives to obtain a total of 120 feature values per frame. Evaluation was done on (1) \u201deval92\u201d for WSJ, and (2) \u201det05 real isolated 1ch track\u201d for CHiME-4. Hyperparameter selection was performed on the (1) \u201ddev93\u201d for WSJ, and (2) \u201ddt05 multi isolated 1ch track\u201d for CHiME-4. None of our experiments used any language model or lexicon information. For the attention model, we used only 32 distinct labels: 26 characters, apostrophe, period, dash, space, noise, and sos/eos tokens. The CTC\nmodel uses the blank instead of sos/eos, and our MTL model uses both sos/eos and the blank."}, {"heading": "3.2. Training and Decoding", "text": "Our model used four layers of 320 Bidirectional Long Short-Term Memory Networks (BLSTM) [17, 18] in the encoder, and one layer of 320 LSTM in the decoder. The top two layers read every second of hidden states in the network below, reducing the utterance length by the factor of 4, L = T/4. Ten centered convolution filters of width 100 were used in the location-based attention model to extract\nthe features from the previous step alignment. We use the sharpening factor \u03b3 = 2. Each linear projection layer is followed by the BLSTM layer. The AdaDelta algorithm [19] with gradient clipping [20] was used for optimization. All the weights are initialized with the range [-0.1, 0.1] of uniform distribution. For our MTL, we tested three different task weights, \u03bb: 0.2, 0.5, and 0.8.\nFor decoding, we used a beam search algorithm similar to [21] with the beam size 20 to reduce the computation cost. We adjusted the score by adding a length penalty, length(hyp) \u2217 0.3 for CHiME4 and length(hyp) \u2217 0.1 for WSJ experiments. Note that we do not use any lexicon or language models. Our framework is implemented with the Chainer library [22, 23]."}, {"heading": "3.3. Results", "text": "The results in Table 1 show that our proposed model MTL significantly outperformed both CTC and the attention model in CER on both the noisy CHiME-4 and clean WSJ tasks. Our model showed 7.0 - 9.5% and 6.6 - 10.3% relative improvements on validation and evaluation set, respectively. As we expected, the attention model showed relatively poor results on noisy corpus CHiME-4 compared to clean corpora WSJ. We observed that the benefit from our Joint CTC-attention increased in noisy condition, and when larger weights on CTC loss (i.e. \u03bb = 0.8) achieved the best performance in CHiME-4, while \u03bb = 0.5 showed the best performance in clean WSJ0.\nOne noticeable thing is that our framework significantly outperformed both the CTC and attention model even on clean corpora WSJ1 and WSJ0. It is possible that the CTC improved generalisation by less relying on the gold-standard transcription information,\nas it does not explicitly use character inter-dependencies. This point needs to be verified with additional experiments in future work.\nApart from the CER improvements, MTL can also be very helpful in accelerating the learning of the desired alignment. Figure 2 shows the learning curves of character accuracy on the validation sets of CHiME-4 over training epochs. Note that the accuracies of the attention and our MTL model were obtained with given gold standard history. As we use large \u03bb giving more weight to CTC loss, the network learns quickly and converges early. Figure 3 visualizes the attention alignments between characters and acoustic frames over training epoch. We observed that our MTL model learned the desired alignment in an early training stage, the 5th epoch, while the attention model could not learn the desired alignment even at the 9th epoch. This result indicates that the CTC loss guided the alignment to be monotonic in our MTL approach."}, {"heading": "4. CONCLUSIONS", "text": "We have introduced a novel, general method for end-to-end speech recognition based on the multi-task learning approach using the CTC and the attention encoder-decoder. Our method improves performance by training a shared encoder using an auxiliary CTC objective function. Moreover, it significantly speeds up the process of learning the desired alignment without requiring manual restriction of the range of inputs, even in longer sequences. Our method has outperformed both CTC and an attention model on a speech recognition task in real-world noisy conditions as well as in clean conditions. This work can potentially be applied to any sequence-to-sequence learning framework."}, {"heading": "5. REFERENCES", "text": "[1] Alex Graves and Navdeep Jaitly, \u201cTowards end-to-end speech recognition with recurrent neural networks,\u201d in Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764\u20131772.\n[2] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up endto-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014.\n[3] Yajie Miao, Mohammad Gowayyed, and Florian Metze, \u201cEesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,\u201d in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167\u2013174.\n[4] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, \u201cEnd-to-end continuous speech recognition using attention-based recurrent nn: First results,\u201d arXiv preprint arXiv:1412.1602, 2014.\n[5] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based models for speech recognition,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 577\u2013585.\n[6] William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals, \u201cListen, attend and spell,\u201d arXiv preprint arXiv:1508.01211, 2015.\n[7] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio, \u201cEnd-to-end attentionbased large vocabulary speech recognition,\u201d arXiv preprint arXiv:1508.04395, 2015.\n[8] Liang Lu, Xingxing Zhang, and Steve Renais, \u201cOn training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5060\u20135064.\n[9] William Chan and Ian Lane, \u201cOn online attention-based speech recognition and joint mandarin character-pinyin training,\u201d Interspeech 2016, pp. 3404\u20133408, 2016.\n[10] Abdel-rahman Mohamed, George E Dahl, and Geoffrey Hinton, \u201cAcoustic modeling using deep belief networks,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 14\u201322, 2012.\n[11] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.\n[12] Alex Graves, Santiago Ferna\u0301ndez, Faustino Gomez, and Ju\u0308rgen Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.\n[13] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014.\n[14] Linguistic Data Consortium, \u201cCsr-ii (wsj1) complete,\u201d Linguistic Data Consortium, Philadelphia, vol. LDC94S13A, 1994.\n[15] John Garofalo, David Graff, Doug Paul, and David Pallett, \u201cCsr-i (wsj0) complete,\u201d Linguistic Data Consortium, Philadelphia, vol. LDC93S6A, 2007.\n[16] Emmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha, Jon Barker, and Ricard Marxer, \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d in Computer Speech and Language, to appear.\n[17] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[18] Alan Graves, Navdeep Jaitly, and Abdel-rahman Mohamed, \u201cHybrid speech recognition with deep bidirectional lstm,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.\n[19] Matthew D Zeiler, \u201cAdadelta: an adaptive learning rate method,\u201d arXiv preprint arXiv:1212.5701, 2012.\n[20] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio, \u201cOn the difficulty of training recurrent neural networks,\u201d arXiv preprint arXiv:1211.5063, 2012.\n[21] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le, \u201cSequence to sequence learning with neural networks,\u201d in Advances in neural information processing systems, 2014, pp. 3104\u20133112.\n[22] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton, \u201cChainer: a next-generation open source framework for deep learning,\u201d in Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015.\n[23] Preferred Networks, \u201cChainer,\u201d in \u201dhttp://chainer.org/\u201d."}], "references": [{"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764\u20131772.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech: Scaling up endto-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167\u2013174.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.1602, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 577\u2013585.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end attentionbased large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1508.04395, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition", "author": ["Liang Lu", "Xingxing Zhang", "Steve Renais"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5060\u20135064.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "On online attention-based speech recognition and joint mandarin character-pinyin training", "author": ["William Chan", "Ian Lane"], "venue": "Interspeech 2016, pp. 3404\u20133408, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Acoustic modeling using deep belief networks", "author": ["Abdel-rahman Mohamed", "George E Dahl", "Geoffrey Hinton"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 14\u201322, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Csr-ii (wsj1) complete", "author": ["Linguistic Data Consortium"], "venue": "Linguistic Data Consortium, Philadelphia, vol. LDC94S13A, 1994.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "Csr-i (wsj0) complete", "author": ["John Garofalo", "David Graff", "Doug Paul", "David Pallett"], "venue": "Linguistic Data Consortium, Philadelphia, vol. LDC93S6A, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "An analysis of environment, microphone and data simulation mismatches in robust speech recognition", "author": ["Emmanuel Vincent", "Shinji Watanabe", "Aditya Arie Nugraha", "Jon Barker", "Ricard Marxer"], "venue": "Computer Speech and Language, to appear.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 0}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alan Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton"], "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Chainer", "author": ["Preferred Networks"], "venue": "\u201dhttp://chainer.org/\u201d.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 1, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 2, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 3, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 4, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 5, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 6, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 7, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 8, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 9, "context": "acoustic model, contextdependent phone model, pronunciation model, and language model) based on conditional independence assumptions (including Markov assumptions) and approximations [10, 11].", "startOffset": 183, "endOffset": 191}, {"referenceID": 10, "context": "acoustic model, contextdependent phone model, pronunciation model, and language model) based on conditional independence assumptions (including Markov assumptions) and approximations [10, 11].", "startOffset": 183, "endOffset": 191}, {"referenceID": 11, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 134, "endOffset": 147}, {"referenceID": 0, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 134, "endOffset": 147}, {"referenceID": 1, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 134, "endOffset": 147}, {"referenceID": 2, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 134, "endOffset": 147}, {"referenceID": 12, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 184, "endOffset": 197}, {"referenceID": 3, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 184, "endOffset": 197}, {"referenceID": 4, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 184, "endOffset": 197}, {"referenceID": 5, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 184, "endOffset": 197}, {"referenceID": 6, "context": "Since the attention model does not use any conditional independence assumption, it has often shown to improve Character Error Rate (CER) than CTC when no external language model is used [7].", "startOffset": 186, "endOffset": 189}, {"referenceID": 6, "context": "Another issue is that the model is hard to be learned from scratch due to the misalignment on longer input sequences, and therefore a windowing technique is commonly used to limit the area explored by the attention mechanism [7], but several parameters for windowing need to be determined manually depending on the training data.", "startOffset": 225, "endOffset": 228}, {"referenceID": 11, "context": "The key idea of CTC [12] is to use intermediate label representation \u03c0 = (\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0T ), allowing repetitions of labels and occurrences of a blank label (\u2212), which represents the special emission without labels, i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "Therefore, lexicon or language models are commonly incorporated, like the hybrid framework [2, 3].", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "Therefore, lexicon or language models are commonly incorporated, like the hybrid framework [2, 3].", "startOffset": 91, "endOffset": 97}, {"referenceID": 6, "context": "Unlike the CTC approach, the attention model directly predicts each target without requiring intermediate representation or any assumptions, improving CER as compared to CTC when no external language model is used [7].", "startOffset": 214, "endOffset": 217}, {"referenceID": 4, "context": "where w,W, V, F, U, b are trainable parameters, su\u22121 is the decoder state, \u03b3 is the sharpening factor [5], and * denotes convolution.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "au can be computed by the softmax of energy eu,l from two types of attention mechanisms: content-based and location-based [5] in Eq.", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "To make training faster, the author [5, 7] constrains the attention mechanism to only consider inputs within a narrow range.", "startOffset": 36, "endOffset": 42}, {"referenceID": 6, "context": "To make training faster, the author [5, 7] constrains the attention mechanism to only consider inputs within a narrow range.", "startOffset": 36, "endOffset": 42}, {"referenceID": 13, "context": "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16].", "startOffset": 105, "endOffset": 113}, {"referenceID": 14, "context": "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16].", "startOffset": 105, "endOffset": 113}, {"referenceID": 15, "context": "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "Our model used four layers of 320 Bidirectional Long Short-Term Memory Networks (BLSTM) [17, 18] in the encoder, and one layer of 320 LSTM in the decoder.", "startOffset": 88, "endOffset": 96}, {"referenceID": 17, "context": "Our model used four layers of 320 Bidirectional Long Short-Term Memory Networks (BLSTM) [17, 18] in the encoder, and one layer of 320 LSTM in the decoder.", "startOffset": 88, "endOffset": 96}, {"referenceID": 18, "context": "The AdaDelta algorithm [19] with gradient clipping [20] was used for optimization.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "The AdaDelta algorithm [19] with gradient clipping [20] was used for optimization.", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "For decoding, we used a beam search algorithm similar to [21] with the beam size 20 to reduce the computation cost.", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "Our framework is implemented with the Chainer library [22, 23].", "startOffset": 54, "endOffset": 62}, {"referenceID": 22, "context": "Our framework is implemented with the Chainer library [22, 23].", "startOffset": 54, "endOffset": 62}], "year": 2016, "abstractText": "Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoderdecoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the attention model has shown poor results especially in noisy condition and is hard to be trained in the initial training stage with long input sequences, as compared with CTC. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-toright constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 6.6-10.3% relative improvements in Character Error Rate (CER).", "creator": "LaTeX with hyperref package"}}}