{"id": "1609.07132", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "A Fully Convolutional Neural Network for Speech Enhancement", "abstract": "In hearing hayrides aids, robitussin the regulator presence of babble noise degrades hearing ncha intelligibility of heat-seeking human speech ahrweiler greatly. vice-chairperson However, otoliths removing gestas the beatitudes babble without koskei creating artifacts despotate in enclosures human rastorguyev speech 1.4875 is rotich a manimekalai challenging task in mackechnie a haircuts low SNR formalistic environment. Here, glinda we overweighting sought to solve ivf the 119-run problem km3 by sels finding a ` mapping ' 37.04 between noisy 89-68 speech shtick spectra indinavir and clean 4-star speech sartori spectra reuben via tollefsen supervised h\u014dj\u014d learning. 4-up Specifically, file-based we kirpan propose using 20.94 fully yakovenko Convolutional Neural capeman Networks, which consist of lesser kickstarts number quadros of kcrc parameters fenzi than skonto fully kidded connected networks. sokolove The phaltan proposed network, cryptogram Redundant abutbul Convolutional Encoder Decoder (capabilites R - br02 CED ), demonstrates .398 that commanding-in-chief a bergamot convolutional network 0.575 can reconstructive be 12 standard-issue times smaller grotesquely than a recurrent network lstreeter and yet achieves better polestar performance, prepayment which shows nostalgically its applicability amuzgo for an embedded system: the regierungsbezirke hearing aids.", "histories": [["v1", "Thu, 22 Sep 2016 19:57:08 GMT  (5020kb,D)", "http://arxiv.org/abs/1609.07132v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["se rim park", "jinwon lee"], "accepted": false, "id": "1609.07132"}, "pdf": {"name": "1609.07132.pdf", "metadata": {"source": "CRF", "title": "A FULLY CONVOLUTIONAL NEURAL NETWORK FOR SPEECH ENHANCEMENT", "authors": ["Se Rim Park", "Jin Won Lee"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Speech Enhancement, Speech Denoising, Babble Noise, Fully Convolutional Neural Network, Convolutional Encoder-Decoder Network, Redundant Convolutional EncoderDecoder Network"}, {"heading": "1. INTRODUCTION", "text": "Denoising speech signals has been a long standing problem. Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5]. Nonetheless, estimating the model for a babble noise, which is encountered when a crowd of people are talking, is still a challenging task.\nThe presence of babble noise, however, degrades hearing intelligibility of human speech greatly. When babble noise dominates over speech, aforementioned methods often times will fail to find the correct noise model [6]. If so, the noise-deduction will render distortion in speech, which creates discomforts to the users of hearing aids [7].\nHere, instead of explicitly modeling the babble noise, we focus on learning a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra, inspired by recent works on speech enhancement using neural networks [8, 9, 10, 11]. However, the model size of Neural Networks easily exceeds several hundreds of megabytes, limiting its applicability for an embedded system.\nOn the other hand, Convolutional Neural Networks (CNN) typically consist of lesser number of parameters than FNNs and RNNs due to its weight sharing property. CNNs already proved its efficacy on extracting features in speech recognition [12, 13] or on eliminating noises in images [14, 15]. But upon our knowledge, CNNs have not been tested in speech enhancement.\nIn this paper, we attempted to find a \u2018memory efficient\u2019 denoising algorithm for babble noise that creates minimal artifacts and that can be implemented in an embedded device: the hearing aid. Through experiments, we demonstrated that CNN can perform better than Feedforward Neural Networks (FNN) or Recurrent Neural Networks (RNN) with much smaller network size. A new network architecture, Redundant Convolutional Encoder Decoder (R-CED), is\nproposed, which extracts redundant representations of a noisy spectrum at the encoder and map it back to clean a spectrum at the decoder. This can be viewed as mapping the spectrum to higher dimensions (e.g. kernel method), and projecting the features back to lower dimensions.\nThe paper is organized as follows. In section 2, a formal definition of the problem is stated. In section 3, the fully convolutional network architectures are presented including the proposed R-CED network. In section 4, the experimental methods are provided. In section 5, description of the experiments and the corresponding network configurations are provided. In section 6, the results are discussed, and in section 7, we end with conclusion of this study."}, {"heading": "2. PROBLEM STATEMENT", "text": "Given a segment of noisy spectra {xt}Tt=1 and clean spectra {yt}Tt=1, our aim is to learn a mapping f which generates a segment of \u2018denoised\u2019 spectra {f(xt)}Tt=1 that approximate the clean spectra in the `2 norm, e.g.\nmin T\u2211 t=1 ||yt \u2212 f(xt)||22. (1)\nSpecifically, we formulate f using a Neural Network (see Fig.1). If f is a recurrent type network, the temporal behavior of input spectra is already addressed by the network, and hence objective (1) suffices. On the other hand, for a convolutional type network, the past nT noisy spectra {xi}ti=t\u2212nT+1 are considered to denoise the current spectra, e.g.\nT\u2211 t=1 ||yt \u2212 f(xt\u2212nT+1,\u00b7\u00b7\u00b7,xt)|| 2 2. (2)\nWe set nT = 8. Hence, input spectra to the network is equivalent to about 100ms of speech segment, whereas the output spectra of the network is of duration 32ms (see Fig.2, Fig.3).\nar X\niv :1\n60 9.\n07 13\n2v 1\n[ cs\n.L G\n] 2\n2 Se\np 20\n16"}, {"heading": "3. CONVOLUTIONAL NETWORK ARCHITECTURES", "text": ""}, {"heading": "3.1. Convolutional Encoder-Decoder Network (CED)", "text": "Convolutional Encoder-Decoder (CED) network proposed in [16] consists of symmetric encoding layers and decoding layers (see Fig.2, each block represents a feature). Encoder consists of repetitions of a convolution, batch-normalization [17], max-pooling, and an ReLU [18] activation layer. Decoder consists of repetitions of a convolution, batch-normalization, and an up-sampling layer. Typically, CED compresses the features along the encoder, and then reconstructs the features along the decoder. In our problem, the orignal Softmax layer at the last layer is modified to a convolution layer, to make CED fully convolutional network."}, {"heading": "3.2. Redundant CED Network (R-CED)", "text": "Here, we propose an alternative convolutional network architecture, namely Redundant Convolutional Encoder-Decoder (R-CED) network. R-CED consists of repetitions of a convolution, batchnormalization, and a ReLU activation layer (see Fig.3, each block represents a feature). No pooling layer is present, and thus no upsampling layer is required. As opposite to CED, R-CED encodes the features into higher dimension along the encoder and achieves compression along the decoder. The number of filters are kept symmetric: at the encoder, the number of filters are gradually increased, and at the decoder, the number of filters are gradually decreased. The last layer is a convolution layer, which makes R-CED a fully convolutional network.\nCascaded R-CED Network (CR-CED): Cascaded Redundant Convolutional Encoder-Decoder Network (CR-CED) is a variation of R-CED network. It consists of repetitions of R-CED Networks. Compared to the R-CED with the same network size (= the same number of parameters), CR-CED achieves better performance with less convergence time."}, {"heading": "3.3. Bypass Connections", "text": "For CED, R-CED, and CR-CED, bypass connections are added to the network to facilitate the optimization in the training phase and improve performance. Between two different bypass schemes \u2014 skip connections in [14] and residual connections in [15] \u2014 we chose to use skip connections in [14] which is more suitable for symmetric encoder-decoder design. Bypass connections are illus-\ntrated in Fig.2 and Fig.3 as an \u2018addition\u2019 operation symbol with an arrow. Bypass connections are added every other layer."}, {"heading": "3.4. 1-Dim Convolution Operation for Convolution Layers", "text": "At all convolution layers throughout the paper, convolution was performed only in 1 direction (see Fig.4). In Fig.4, the input (3 \u00d7 3 white matrix) and the filter (2\u00d7 3 blue matrix) has the same dimension in time axis, and convolution is performed in frequency axis. We found this more efficient than 2-dim convolution (see Fig.5) for our input spectra (129\u00d7 8)."}, {"heading": "4. EXPERIMENTAL METHODS", "text": ""}, {"heading": "4.1. Preprocessing", "text": "Dataset: The experiment was conducted on the TIMIT database [19] and 27 different types of noise clips were collected from freely available online resource [20]. The noise are mostly babble, but includes different types of noise like instrumental sounds. Both data in the training set (4620 utterances) and the testing set (200 utterances) were added with one of 27 noise clips at 0dB SNR. After all feature transformation steps were completed, 20% of the training features were assigned as the validation set.\nFeature Transformation: The audio signals were downsampled to 8kHz, and the silent frames were removed from the signal. The spectral vectors were computed using a 256-point Short Time Fourier Transform (32ms hamming window) with a window shift of 64-point (8ms). The frequency resolution was 31.25 Hz (=4kHz/128) per each frequency bin. 256-point STFT magnitude vectors were reduced to 129-point by removing the symmetric half. For FNN/RNN, the input feature consisted of a noisy STFT magnitude vector (size: 129\u00d71, duration: 32ms). For CNN, the input feature consisted of 8 consecutive noisy STFT magnitude vectors\n(size: 129 \u00d7 8, duration: 100ms). Both input features were standardized to have zero mean and and unit variance.\nPhase Aware Scaling: To avoid extreme differences (more than 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:\nsphase aware = sclean cos(\u03b8clean \u2212 \u03b8noisy).\nBesides, spectral phase was not used in the training phase. At reconstruction, noisy spectral phase was used instead to perform inverse STFT and recover human speech. However, because human ear is not susceptible to phase difference smaller than 45 degree, the distortion was negligible. Through \u2018phase aware scaling\u2019, the phase mismatch be smaller than 45 degree, and For all networks, the output feature consisted of a \u2018phase aware\u2019 magnitude vector (size: 129\u00d71, duration: 32ms), and were standardized to have zero mean and and unit variance."}, {"heading": "4.2. Optimization", "text": "Fully connected and convolution layer weights were initialized as in [22] and recurrent layer weights were initialized as in [23]. Fully connected and recurrent layer weights were pretrained from the networks of smaller depth with same number of nodes. Convolution layers were trained from scratch, with the aid of batch normalization layer [17] added after each convolution layer 1. All networks were trained using back propagation with gradient descent optimization using Adam [24] with a mini-batch size of 64. The learning rate started from lr = 0.0015 with \u03b21 = 0.9, \u03b22 = 0.999, and = 1.0e\u22128. When the validation loss didn\u2019t decrease for more than 4 epochs, learning rate was decreased to lr/2, lr/3, lr/4, subsequently. The training was repeated once more for FNN and RNN with `2 regularization (\u03bb = 10\u22125) which slightly improved the performance."}, {"heading": "4.3. Evaluation Metric", "text": "Signal to Distortion Ration (SDR) [25] was used to measure the amount of `2 error present between clean and denoised speech:\nSDR := 10 log10 ||y||2\n||f(x)\u2212 y||2 .\nSDR is inversely associated with the objective function presented in (1). In addition, Short time Objective Intelligibility (STOI) [26] and Perceptual Evaluation of Speech Distortion (PESQ) [27] \u2014 both assume that human perception has short term memory and hence the error is measured nonlinearly in time of interest\u2014 were used to measure the subjective quality of listening.\n1We note that adding BN layers on both FNN and RNN did not improve convergence nor performance for this experiment."}, {"heading": "5. EXPERIMENTAL SETUP", "text": ""}, {"heading": "5.1. Test 1: FNN vs. RNN vs. CNN", "text": "The first experiment compared CNN with FNN and RNN to demonstrate how feasible it is to use CNN for speech enhancement. Network configurations (e.g. number of nodes, number of layers) that yielded the best performance for each network are summarized in Table.2. The best FNN and RNN architectures have 4 fully connected (FC) layers whereas CNN has 16 convolutional layers."}, {"heading": "5.2. Test 2: CED vs. R-CED", "text": "In the second experiment, R-CED was compared to CED. For fair comparison, the total number of parameters were fixed to 33,000 (roughly 132MB of memory) while the depth of the network was fixed to 10 convolution layers. The filter width per each layer is determined accordingly such that i) the symmetric encoder-decoder structure is maintained, ii) the number of parameters are gradually increased and decreased, iii) the \u2018frequency coverage\u2019 is equal for both network. Here, \u2018frequency coverage\u2019 refers to how much nearby frequency bins at the input are used to reconstruct a single frequency bin at the output. We made sure that both networks utilize the same amount of frequency bins to reconstruct a single frequency bin. The configurations for Test 2 is summarized at top two rows of Table.1."}, {"heading": "5.3. Test 3: Finding the Best R-CED Performance", "text": "In the third experiment, we tested how far the performance can be improved using the R-CED network. The R-CED and CR-CED network with skip connections of various network size and depth are compared. The network size (the number of parameters) considered are 33K (132MB memory) and 100K (400MB memory). The depth of the network considered are 10, 16, and 20 convolution layers. Bottom 3 rows in Table.1 summarize the network configurations for Test 3."}, {"heading": "6. RESULTS", "text": ""}, {"heading": "6.1. Test 1: FNN vs. RNN vs. CNN", "text": "Fig.9 illustrates the denoising performance of FNN, RNN and CNN (left), and the corresponding network size (=the number of parameters, right). All networks exhibited similar performance based on both subjective (STOI, PESQ) and objective quality (SDR) measure. On the other hand, the model size of CNN was about 68 times smaller than that of FNN and about 12 times smaller than RNN. We note that FNN and RNN were optimized to have the smallest network architectures. This experiment validates that CNN requires far lesser number of parameters per layer due to its weight sharing property, and yet can achieve similar or better performance compared to FNN and RNN. 33,000 parameters for CNN are roughly 132MB of memory which can be implemented in an embedded system. Refer to Fig.6, Fig.7, Fig.8 for the example of noisy spectrogram, clean spectrogram, and denoised spectrogram from CNN respectively."}, {"heading": "6.2. Test 2: CED vs. R-CED", "text": "The denoising performance of CED and R-CED are shown in Fig.10 with first 4 bars. The R-CED with skip connections showed the best performance, whereas the CED without skip connections showed the worst performance. Regardless of the presence of skip connections, R-CED yielded better results than CED.\nThe effect of the skip connection was prominent in CED (5.96 to 7.92). This implies that the decoder itself could not reconstruct the \u2018lost\u2019 information compressed at the encoder, unless the \u2018lost\u2019 information was provided by the skip connections. In addition, the resulting speech from CED sounded artificial and mechanical. This confirms that the decoder could not reconstruct what is necessary for audios to sound like human speech.\nOn the other hand, the effect of the skip connection was not that notable in R-CED (8.07 to 8.19). This is because R-CED rather expands than compresses the input at the encoder, which can be viewed as mapping a spectrum to higher dimension (e.g. kernel method). By generating redundant representations of important features at the encoder, and removing unwanted features at the decoder, the speech quality was effectively enhanced."}, {"heading": "6.3. Test 3: Finding the Best R-CED Network", "text": "The denoising performance of R-CED of various network size and depth are presented in Fig.10 with the last five bars. A few interesting observations are i) the network size was the most dominant factor that was associated with the network performance, ii) the network depth was secondary, iii) CR-CED with skip connection yielded the best performance when other conditions were kept same (16 convolution layers, 33K parameters)."}, {"heading": "7. CONCLUSION", "text": "In this paper, we aimed to find a memory efficient denoising method that can be implemented in an embedded system. Inspired by past success of FNN and RNN, we hypothesized that CNN can effectively denoise speech with smaller network size according to its weight sharing property. We set up an experiment to denoise human speech from babble noise which is a major discomfort to the users of hearing aids. Through experiments, we demonstrated that CNN can yield similar or better performance with much lesser number of model parameters compared to FNN and RNN. Also, we proposed a new fully convolutional network architecture R-CED, and showed its efficacy\nin speech enhancement. We observed that the success of R-CED is associated with the increasing dimension of the feature space along the encoder and decreasing dimension along the decoder. We expect that R-CED can be also applied to other interesting domains. Our future work will include pruning the R-CED to minimize the operation count of convolution."}, {"heading": "8. REFERENCES", "text": "[1] Steven Boll, \u201cSuppression of acoustic noise in speech using spectral subtraction,\u201d IEEE Transactions on acoustics, speech, and signal processing, vol. 27, no. 2, pp. 113\u2013120, 1979.\n[2] Jae S Lim and Alan V Oppenheim, \u201cEnhancement and bandwidth compression of noisy speech,\u201d Proceedings of the IEEE, vol. 67, no. 12, pp. 1586\u20131604, 1979.\n[3] Yariv Ephraim and David Malah, \u201cSpeech enhancement using a minimum-mean square error short-time spectral amplitude estimator,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 6, pp. 1109\u20131121, 1984.\n[4] Pascal Scalart et al., \u201cSpeech enhancement based on a priori signal to noise estimation,\u201d in Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on. IEEE, 1996, vol. 2, pp. 629\u2013632.\n[5] Yariv Ephraim and Harry L Van Trees, \u201cA signal subspace approach for speech enhancement,\u201d IEEE Transactions on speech and audio processing, vol. 3, no. 4, pp. 251\u2013266, 1995.\n[6] Nitish Krishnamurthy and John HL Hansen, \u201cBabble noise: modeling, analysis, and applications,\u201d IEEE transactions on audio, speech, and language processing, vol. 17, no. 7, pp. 1394\u20131407, 2009.\n[7] Abby McCormack and Heather Fortnum, \u201cWhy do people fitted with hearing aids not wear them?,\u201d International Journal of Audiology, vol. 52, no. 5, pp. 360\u2013368, 2013.\n[8] Kun Han, Yuxuan Wang, and DeLiang Wang, \u201cLearning spectral mapping for speech dereverberation,\u201d in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 4628\u20134632.\n[9] Yong Xu, Jun Du, Li-Rong Dai, and Chin-Hui Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015.\n[10] Bingyin Xia and Changchun Bao, \u201cSpeech enhancement with weighted denoising auto-encoder.,\u201d in INTERSPEECH, 2013, pp. 3444\u20133448.\n[11] Keiichi Osako, Rita Singh, and Bhiksha Raj, \u201cComplex recurrent neural networks for denoising speech signals,\u201d in Applications of Signal Processing to Audio and Acoustics (WASPAA), 2015 IEEE Workshop on. IEEE, 2015, pp. 1\u20135.\n[12] Ossama Abdel-Hamid, Abdel-Rahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, and Dong Yu, \u201cConvolutional neural networks for speech recognition,\u201d IEEE/ACM Transactions on audio, speech, and language processing, vol. 22, no. 10, pp. 1533\u20131545, 2014.\n[13] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et al., \u201cDeep speech 2: End-to-end speech recognition in english and mandarin,\u201d arXiv preprint arXiv:1512.02595, 2015.\n[14] Xiao-Jiao Mao, Chunhua Shen, and Yu-Bin Yang, \u201cImage denoising using very deep fully convolutional encoder-decoder networks with symmetric skip connections,\u201d arXiv preprint arXiv:1603.09056, 2016.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cDeep residual learning for image recognition,\u201d arXiv preprint arXiv:1512.03385, 2015.\n[16] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol, \u201cStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,\u201d Journal of Machine Learning Research, vol. 11, no. Dec, pp. 3371\u20133408, 2010.\n[17] Sergey Ioffe and Christian Szegedy, \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift,\u201d arXiv preprint arXiv:1502.03167, 2015.\n[18] Vinod Nair and Geoffrey E Hinton, \u201cRectified linear units improve restricted boltzmann machines,\u201d in Proceedings of the 27th International Conference on Machine Learning (ICML10), 2010, pp. 807\u2013814.\n[19] John S Garofolo, Lori F Lamel, William M Fisher, Jonathon G Fiscus, and David S Pallett, \u201cDarpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1,\u201d NASA STI/Recon technical report n, vol. 93, 1993.\n[20] Vincent Akkermans, Frederic Font, Jordi Funollet, Bram De Jong, Gerard Roma, Stelios Togias, and Xavier Serra,\n\u201cFreesound 2: An improved platform for sharing audio clips,\u201d in Klapuri A, Leider C, editors. ISMIR 2011: Proceedings of the 12th International Society for Music Information Retrieval Conference; 2011 October 24-28; Miami, Florida (USA). Miami: University of Miami; 2011. International Society for Music Information Retrieval (ISMIR), 2011.\n[21] Pejman Mowlaee and Rahim Saeidi, \u201cIterative closed-loop phase-aware single-channel speech enhancement,\u201d IEEE Signal Processing Letters, vol. 20, no. 12, pp. 1235\u20131239, 2013.\n[22] Xavier Glorot and Yoshua Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks.,\u201d in Aistats, 2010, vol. 9, pp. 249\u2013256.\n[23] Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton, \u201cA simple way to initialize recurrent networks of rectified linear units,\u201d arXiv preprint arXiv:1504.00941, 2015.\n[24] Diederik P. Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization,\u201d CoRR, vol. abs/1412.6980, 2014.\n[25] Emmanuel Vincent, Re\u0301mi Gribonval, and Ce\u0301dric Fe\u0301votte, \u201cPerformance measurement in blind audio source separation,\u201d IEEE transactions on audio, speech, and language processing, vol. 14, no. 4, pp. 1462\u20131469, 2006.\n[26] Cees H Taal, Richard C Hendriks, Richard Heusdens, and Jesper Jensen, \u201cA short-time objective intelligibility measure for time-frequency weighted noisy speech.,\u201d in ICASSP, 2010, pp. 4214\u20134217.\n[27] Antony W Rix, John G Beerends, Michael P Hollier, and Andries P Hekstra, \u201cPerceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs,\u201d in Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP\u201901). 2001 IEEE International Conference on. IEEE, 2001, vol. 2, pp. 749\u2013752."}], "references": [{"title": "Suppression of acoustic noise in speech using spectral subtraction", "author": ["Steven Boll"], "venue": "IEEE Transactions on acoustics, speech, and signal processing, vol. 27, no. 2, pp. 113\u2013120, 1979.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1979}, {"title": "Enhancement and bandwidth compression of noisy speech", "author": ["Jae S Lim", "Alan V Oppenheim"], "venue": "Proceedings of the IEEE, vol. 67, no. 12, pp. 1586\u20131604, 1979.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1979}, {"title": "Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator", "author": ["Yariv Ephraim", "David Malah"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 6, pp. 1109\u20131121, 1984.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1984}, {"title": "Speech enhancement based on a priori signal to noise estimation", "author": ["Pascal Scalart"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on. IEEE, 1996, vol. 2, pp. 629\u2013632.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "A signal subspace approach for speech enhancement", "author": ["Yariv Ephraim", "Harry L Van Trees"], "venue": "IEEE Transactions on speech and audio processing, vol. 3, no. 4, pp. 251\u2013266, 1995.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Babble noise: modeling, analysis, and applications", "author": ["Nitish Krishnamurthy", "John HL Hansen"], "venue": "IEEE transactions on audio, speech, and language processing, vol. 17, no. 7, pp. 1394\u20131407, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Why do people fitted with hearing aids not wear them", "author": ["Abby McCormack", "Heather Fortnum"], "venue": "International Journal of Audiology, vol. 52, no. 5, pp. 360\u2013368, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning spectral mapping for speech dereverberation", "author": ["Kun Han", "Yuxuan Wang", "DeLiang Wang"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 4628\u20134632.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Yong Xu", "Jun Du", "Li-Rong Dai", "Chin-Hui Lee"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement with weighted denoising auto-encoder", "author": ["Bingyin Xia", "Changchun Bao"], "venue": "INTERSPEECH, 2013, pp. 3444\u20133448.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Complex recurrent neural networks for denoising speech signals", "author": ["Keiichi Osako", "Rita Singh", "Bhiksha Raj"], "venue": "Applications of Signal Processing to Audio and Acoustics (WASPAA), 2015 IEEE Workshop on. IEEE, 2015, pp. 1\u20135.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel-Rahman Mohamed", "Hui Jiang", "Li Deng", "Gerald Penn", "Dong Yu"], "venue": "IEEE/ACM Transactions on audio, speech, and language processing, vol. 22, no. 10, pp. 1533\u20131545, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Image denoising using very deep fully convolutional encoder-decoder networks with symmetric skip connections", "author": ["Xiao-Jiao Mao", "Chunhua Shen", "Yu-Bin Yang"], "venue": "arXiv preprint arXiv:1603.09056, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "Journal of Machine Learning Research, vol. 11, no. Dec, pp. 3371\u20133408, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML- 10), 2010, pp. 807\u2013814.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "author": ["John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathon G Fiscus", "David S Pallett"], "venue": "NASA STI/Recon technical report n, vol. 93, 1993.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1993}, {"title": "Iterative closed-loop phase-aware single-channel speech enhancement", "author": ["Pejman Mowlaee", "Rahim Saeidi"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 12, pp. 1235\u20131239, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Aistats, 2010, vol. 9, pp. 249\u2013256.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Performance measurement in blind audio source separation", "author": ["Emmanuel Vincent", "R\u00e9mi Gribonval", "C\u00e9dric F\u00e9votte"], "venue": "IEEE transactions on audio, speech, and language processing, vol. 14, no. 4, pp. 1462\u20131469, 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "A short-time objective intelligibility measure for time-frequency weighted noisy speech", "author": ["Cees H Taal", "Richard C Hendriks", "Richard Heusdens", "Jesper Jensen"], "venue": "ICASSP, 2010, pp. 4214\u20134217.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5].", "startOffset": 120, "endOffset": 135}, {"referenceID": 1, "context": "Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5].", "startOffset": 120, "endOffset": 135}, {"referenceID": 2, "context": "Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5].", "startOffset": 120, "endOffset": 135}, {"referenceID": 3, "context": "Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5].", "startOffset": 120, "endOffset": 135}, {"referenceID": 4, "context": "Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5].", "startOffset": 120, "endOffset": 135}, {"referenceID": 5, "context": "When babble noise dominates over speech, aforementioned methods often times will fail to find the correct noise model [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "If so, the noise-deduction will render distortion in speech, which creates discomforts to the users of hearing aids [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "Here, instead of explicitly modeling the babble noise, we focus on learning a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra, inspired by recent works on speech enhancement using neural networks [8, 9, 10, 11].", "startOffset": 212, "endOffset": 226}, {"referenceID": 8, "context": "Here, instead of explicitly modeling the babble noise, we focus on learning a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra, inspired by recent works on speech enhancement using neural networks [8, 9, 10, 11].", "startOffset": 212, "endOffset": 226}, {"referenceID": 9, "context": "Here, instead of explicitly modeling the babble noise, we focus on learning a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra, inspired by recent works on speech enhancement using neural networks [8, 9, 10, 11].", "startOffset": 212, "endOffset": 226}, {"referenceID": 10, "context": "Here, instead of explicitly modeling the babble noise, we focus on learning a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra, inspired by recent works on speech enhancement using neural networks [8, 9, 10, 11].", "startOffset": 212, "endOffset": 226}, {"referenceID": 11, "context": "CNNs already proved its efficacy on extracting features in speech recognition [12, 13] or on eliminating noises in images [14, 15].", "startOffset": 78, "endOffset": 86}, {"referenceID": 12, "context": "CNNs already proved its efficacy on extracting features in speech recognition [12, 13] or on eliminating noises in images [14, 15].", "startOffset": 78, "endOffset": 86}, {"referenceID": 13, "context": "CNNs already proved its efficacy on extracting features in speech recognition [12, 13] or on eliminating noises in images [14, 15].", "startOffset": 122, "endOffset": 130}, {"referenceID": 14, "context": "CNNs already proved its efficacy on extracting features in speech recognition [12, 13] or on eliminating noises in images [14, 15].", "startOffset": 122, "endOffset": 130}, {"referenceID": 15, "context": "Convolutional Encoder-Decoder (CED) network proposed in [16] consists of symmetric encoding layers and decoding layers (see Fig.", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "Encoder consists of repetitions of a convolution, batch-normalization [17], max-pooling, and an ReLU [18] activation layer.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "Encoder consists of repetitions of a convolution, batch-normalization [17], max-pooling, and an ReLU [18] activation layer.", "startOffset": 101, "endOffset": 105}, {"referenceID": 13, "context": "Between two different bypass schemes \u2014 skip connections in [14] and residual connections in [15] \u2014 we chose to use skip connections in [14] which is more suitable for symmetric encoder-decoder design.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "Between two different bypass schemes \u2014 skip connections in [14] and residual connections in [15] \u2014 we chose to use skip connections in [14] which is more suitable for symmetric encoder-decoder design.", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "Between two different bypass schemes \u2014 skip connections in [14] and residual connections in [15] \u2014 we chose to use skip connections in [14] which is more suitable for symmetric encoder-decoder design.", "startOffset": 135, "endOffset": 139}, {"referenceID": 18, "context": "Dataset: The experiment was conducted on the TIMIT database [19] and 27 different types of noise clips were collected from freely available online resource [20].", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Phase Aware Scaling: To avoid extreme differences (more than 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "Fully connected and convolution layer weights were initialized as in [22] and recurrent layer weights were initialized as in [23].", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "Fully connected and convolution layer weights were initialized as in [22] and recurrent layer weights were initialized as in [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "Convolution layers were trained from scratch, with the aid of batch normalization layer [17] added after each convolution layer .", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "All networks were trained using back propagation with gradient descent optimization using Adam [24] with a mini-batch size of 64.", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "Signal to Distortion Ration (SDR) [25] was used to measure the amount of `2 error present between clean and denoised speech:", "startOffset": 34, "endOffset": 38}, {"referenceID": 24, "context": "In addition, Short time Objective Intelligibility (STOI) [26] and Perceptual Evaluation of Speech Distortion (PESQ) [27] \u2014 both assume that human perception has short term memory and hence the error is measured nonlinearly in time of interest\u2014 were used to measure the subjective quality of listening.", "startOffset": 57, "endOffset": 61}], "year": 2016, "abstractText": "In hearing aids, the presence of babble noise degrades hearing intelligibility of human speech greatly. However, removing the babble without creating artifacts in human speech is a challenging task in a low SNR environment. Here, we sought to solve the problem by finding a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra via supervised learning. Specifically, we propose using fully Convolutional Neural Networks, which consist of lesser number of parameters than fully connected networks. The proposed network, Redundant Convolutional Encoder Decoder (R-CED), demonstrates that a convolutional network can be 12 times smaller than a recurrent network and yet achieves better performance, which shows its applicability for an embedded system: the hearing aids.", "creator": "LaTeX with hyperref package"}}}