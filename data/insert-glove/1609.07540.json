{"id": "1609.07540", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2016", "title": "Derivative Delay Embedding: Online Modeling of Streaming Time Series", "abstract": "unc-5 The nelder staggering amount l'authentique of flag-waving streaming mpofu time offeror series coming from the zicot real world calls b8 for newton more 2-19 efficient and effective online modeling solution. wilmeth For cuchillo time deliverables series sehwan modeling, wroclawski most existing works make some penang unrealistic assumptions such lixiong as the input biffle data sextuplet is of fixed repressed length sirico or qaboos well aligned, which requires voronezh extra effort khurmatu on 120.35 segmentation or normalization kusch of the raw bagh-e streaming data. eyeshot Although some literature claim their modern-style approaches to 45,750 be high-fructose invariant to ziggo data aminotransferase length and 53.7 misalignment, they are s1 too parseghian time - midways consuming to co-credited model a moraga streaming 42-page time series honiball in 3107 an online tunas manner. gaywood We diphthongization propose bc2 a novel and more marshwood practical online shindle modeling ragga and classification rockman scheme, DDE - MGM, wranglers which 600-square does waterlilies not skey make arriva any assumptions on gym the seraphin time introverted series lous while maintaining adrift high efficiency kuldevi and state - of - kamijo the - 83-59 art 17.51 performance. ka\u015f The mentalist derivative wordiness delay hf/df embedding (DDE) crematoriums is suisun developed farraj to incrementally angst-ridden transform time series to foxxx the gelly embedding olajire space, where the overcoming intrinsic characteristics eisteddfod of data spiraxidae is kilia preserved self-cultivation as recursive patterns regardless of luddites the stream zyberk length and misalignment. gubat Then, servilia a fd non - pentathletes parametric Markov hendrika geographic linkevicius model (tetrads MGM) fluker is ardila proposed 1,500-meter to carniolan both seedlings model inducements and classify divya the cissa pattern in nyassa an online ken manner. jakks Experimental 25.54 results part-timers demonstrate the coffee-table effectiveness and superior classification abbiss accuracy of the bochnia proposed DDE - suevi MGM in senders an sierpinski online armsbearer setting as erisa compared abtrust to automobile the leafbird state - of - 10:47 the - art.", "histories": [["v1", "Sat, 24 Sep 2016 00:03:49 GMT  (1839kb,D)", "http://arxiv.org/abs/1609.07540v1", "Accepted by The 25th ACM International Conference on Information and Knowledge Management (CIKM 2016)"]], "COMMENTS": "Accepted by The 25th ACM International Conference on Information and Knowledge Management (CIKM 2016)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhifei zhang", "yang song", "wei wang", "hairong qi"], "accepted": false, "id": "1609.07540"}, "pdf": {"name": "1609.07540.pdf", "metadata": {"source": "CRF", "title": "Derivative Delay Embedding: Online Modeling of Streaming Time Series", "authors": ["Zhifei Zhang", "Yang Song", "Wei Wang", "Hairong Qi"], "emails": ["zzhang61@vols.utk.edu", "ysong18@vols.utk.edu", "wwang@vols.utk.edu", "hqi@utk.edu"], "sections": [{"heading": null, "text": "Keywords Delay embedding; streaming time series; online modeling and classification; Markov geographical model\nSource code https://github.com/ZZUTK/Delay Embedding.git"}, {"heading": "1. INTRODUCTION", "text": "There has been an unprecedented surge of interest in streaming time series modeling and classification mainly due to the rapid deployment of smart devices. Traditionally, time series classification has been conducted using an offline training procedure coupled with an online/offline classification procedure. During the training process, some preprocessing steps are usually conducted including segmenting the time series into finite (usually fixed) length and aligning the segments perfectly to facilitate the subsequent feature extraction that normally yields discriminative patterns for classification purpose. However, with the smart device explosion and the related jump in data traffic, new challenges arise in time series data analysis. For example, time series data generally exhibit time-varying characteristics over an, in theory, infinite time span, therefore, manually truncating the time series into fixed-length, well-aligned segments would run the risk of missing some intrinsic characteristics of the data that degrades the performance of the classifier. On the other hand, many smart devices require real-time responses. Therefore, the computational complexity becomes the bottleneck for most classifiers.\nThese challenges call for a solution that is able to model a time series in an online manner without the need for any preprocessing such that time-varying characteristics of the time series can be well captured in real time and that the classification can be performed using the most updated model. Existing works either are time-consuming or have to make certain assumptions on the times series, e.g., fixed length (as opposed to random or infinite length) and well alignment (i.e., the time series are aligned to the same starting point or baseline), which have largely hindered the realization of online modeling or classification.\nTo the best of our knowledge, there has not been any related work that can achieve online processing in both modeling and classification stages without the assumptions of fixed length and well alignment. We develop the derivative delay embedding (DDE) method that transforms, in an online fashion, a time series to the embedding space, in which the patterns are preserved regardless of the assumptions mentioned above. We further propose the Markov geographic model (MGM) that enables both the modeling and classification of the transformed patterns in an online fashion. We refer to the proposed approach as DDE-MGM.\nar X\niv :1\n60 9.\n07 54\n0v 1\n[ cs\n.L G\n] 2\n4 Se\np 20"}, {"heading": "1.1 Motivation", "text": "The theory of delay embedding [24, 22] was first introduced to reconstruct a chaotic dynamical system from a sequence of observations of the system. The reconstruction preserves the coordinate and period changes of the dynamical system, but it is invariant to the change of phase. Therefore, a time series can be considered a sequence of observations from a latent dynamical system, and we can represent the time series by the reconstructed dynamical system, which is invariant to phase changes (i.e., misalignment). Another merit of delay embedding is its low computational complexity, approximately O(1). In addition, the reconstruction is performed in an incremental fashion, facilitating online processing, because only recent observations are considered when reconstructing the dynamical system from a streaming time series. The reconstructed dynamical system is usually represented in a higher dimensional space, in which the dynamics presents recursive patterns [8, 18, 9] regardless of the length of the original time series. Therefore, an infinite streaming time series can potentially be stored in a finite memory through the delay embedding because of the recursiveness of the reconstructed dynamical system.\nMotivated by the invariance properties of delay embedding, especially the invariance to the phase and length changes of the time series, we develop the online modeling and classification scheme, DDE-MGM, taking advantage of the invariance properties and high efficiency from the delay embedding technique."}, {"heading": "1.2 Related Work", "text": "The dynamic time warping (DTW) method [2] has achieved good performance in time series classification, especially the 1NN-DTW [29]. However, DTW-based methods normally suffer from the high computational complexity that is not suitable for many real-time applications. Several recent improvements [29, 19] have successfully reduced the computational complexity, however, they are still far from achieving online processing. Other methods such as HMMs [15], decision tree [21], SVM [28], and neural network [6], are also limited by their high computational complexity in the training stage and the necessity to make the two impractical assumptions, i.e., fixed length and well alignment of the time series.\nRecently, some works have been proposed attempting to relax these assumptions. For example, [7] removed the assumption of fixed length by learning a dictionary, but it needs a long time to learn an appropriate dictionary. [11, 23] removed the assumption of well alignment by sparse coding. However, they still have to learn a dictionary in an offline manner. [18, 30] exploited the delay embedding technique in time series analysis, which relaxed both assumptions of fixed length and well alignment, but neither can be performed in the \u201conline\u201d scenario.\nMany online learning methods were also proposed in resent years, e.g., [16] proposed the kernel based perceptron with budget, [27] improved the online passive-aggressive algorithm, and [14] extended online gradient descent. However, they require the data to be of the same length or well aligned. In addition, they are more suitable to operate in the feature space rather than on the raw time series. Therefore, we consider these methods as pseudo-online because they need to preprocess (i.e., truncating or aligning) the raw time series.\nIn this paper, we specifically consider the problem in a more practical \u201conline\u201d setting, where a time series streams in with random length.\nThe contribution of this paper is three-fold. First, the proposed DDE completely removes the common but unrealistic assumptions made on the time series, i.e., fixed length and well alignment, therefore, no preprocessing is needed on the time series, facilitating real-world problem solving. Second, the proposed MGM effectively and efficiently models the trajectory of the recursive patterns of different classes in the embedding space. Thus, both the modeling and classification using DDE-MGM are performed in an online and incremental fashion, while maintaining competitive classification accuracy as compared to the state-of-the-art. Third, the discretization of the embedding space enables an approximately constant memory footprint during the modeling regardless of the stream length."}, {"heading": "2. DERIVATIVE DELAY EMBEDDING", "text": "In this section, we first present background of delay embedding. Then, the proposed derivative delay embedding (DDE) is elaborated, as well as its invariant property to data length and misalignment. Finally, parameter selection for delay embedding and DDE is discussed to facilitate realworld applications of DDE."}, {"heading": "2.1 Delay Embedding", "text": "A time series [yt, yt+1, \u00b7 \u00b7 \u00b7 ] can be considered as an observable sequence from a latent deterministic dynamical system [10], which evolves in time\nxt+1 = \u03c6(xt), t = 0, 1, 2, \u00b7 \u00b7 \u00b7 , (1)\nwhere xt \u2208 X denotes the system state at time t, and \u03c6 : X \u2192 X is a deterministic function. Without loss of generality, we assume the time series is of high dimension, i.e., yt \u2208 Rn. Because we cannot directly observe those internal states xt of the system, the states are measured via an observation function \u03c8 : X \u2192 Rn. For each set of states [xt, xt+1, \u00b7 \u00b7 \u00b7 ], there is a corresponding time series\n[yt, yt+1, \u00b7 \u00b7 \u00b7 ] = [\u03c8(xt), \u03c8(xt+1), \u00b7 \u00b7 \u00b7 ] (2) = [\u03c8(xt), \u03c8(\u03c6(xt)), \u00b7 \u00b7 \u00b7 ] . (3)\nOur purpose is to estimate the deterministic function \u03c6 of the latent dynamical system by reconstructing the internal states [xt, xt+1, \u00b7 \u00b7 \u00b7 ] from the observations [yt, yt+1, \u00b7 \u00b7 \u00b7 ]. For classification purpose, the times series of the same class should share similar \u03c6.\nFrom Takens\u2019 embedding theory [24], a series of observations need to be considered to reconstruct a single state because a state of the deterministic dynamical system is associated with current and recent observations. Assuming X is a smooth manifold, \u03c6 \u2208 C2 is a diffeomorphism, and \u03c8 \u2208 C2, the mapping \u03a6\u03c8 : X \u2192 Rn \u00d7 Rd, defined by\n\u03a6(xt; s, d) = ( \u03c8(xt), \u03c8(xt+s), \u00b7 \u00b7 \u00b7 , \u03c8(xt+(d\u22121)s) ) (4)\n= ( yt, yt+s, \u00b7 \u00b7 \u00b7 , yt+(d\u22121)s ) , (5)\nis a delay embedding, where \u03a6(xt; s, d) is the reconstruction of the state xt in the Euclidean space, which is referred to as the embedding space. The parameter s is the delay step, and d denotes the embedding dimension. Based on the reconstructed states [\u03a6(xt),\u03a6(xt+1), \u00b7 \u00b7 \u00b7 ], the deterministic\nfunction \u03c6 can be estimated. For simplicity, we use \u03a6(xt) to denote \u03a6(xt; s, d) in the rest of this paper.\nA toy example of delay embedding is shown in Fig. 1, assuming a 1-D time series yt = f(t), t \u2208 Z+, d = 2, and s = 1. For each delay embedding, only the adjacent two observations are involved, and the time series is transformed to the embedding space, where the raw 1-D time series becomes a recursive 2-D time series. According to [18], the trajectory of the recursive 2-D time series forms a pattern corresponding to the intrinsic characteristics of data in the time domain. Fig. 2 illustrates that different patterns in the time series result in different trajectories in the embedding space. Intuitively, we can classify the time series through their trajectories, which is invariant to the phase and length changes of the time series."}, {"heading": "2.2 Derivative Delay Embedding", "text": "Although delay embedding is robust to the length and phase changes of the time series, as shown above, it is sensitive to the shift of baseline. For example, the zero-drift effect will make the sensor output drift away although the external environment has not changed at all; or different types of sensors monitoring the same variable may yield results in different baseline. Moreover, the embedding space\nis a continuous space, so recording the exact position of the states/trajectory would consume large memory.\nTo tackle these two problems, we develop the derivative delay embedding (DDE) method, letting the observation function \u03c8(xt) = y \u2032 t to offset the baseline shift, and then the embedding space is discretized into a grid to reduce the memory cost. The DDE of yt = f(t) at t \u2208 Z+ is\n\u03a6\u2032(xt) = G ( y\u2032t, y \u2032 t+s, \u00b7 \u00b7 \u00b7 , y\u2032t+(d\u22121)s ) , (6)\nwhere y\u2032t = (yt \u2212 yt\u2212\u03c4 ) /\u03c4 , \u03c4 \u2208 Z+. G(\u00b7) approximates a state to the nearest grid cell in the discretized embedding space. For simplicity, \u03c4 is set to 1, thus y\u2032t = yt \u2212 yt\u22121. An illustrative comparison between the delay embedding and DDE is shown in Fig. 3, assuming yt = f(t).\nBecause the derivative intrinsically has a zero baseline, the DDE gains the invariance to the shift of baseline, enabling the complete relaxation of those common but unrealistic assumptions, i.e., the same length and well alignment.\nOn the other hand, DDE generates recursive trajectories that occupy a limit region of the embedding space, the discretized embedding space further realizes an approximately constant footprint. In Fig. 1, for example, the raw time series consists of 15 points. After delay embedding, there are only 8 points in the embedding space because of recursiveness. In practice, however, the time series is normally corrupted by noise which will prevent the trajectory from presenting such perfect recursiveness as shown in Fig. 3. Therefore, the number of unrepeated states in the embedding space will be similar to the number of points in the raw time series, and then the memory consumption of storing all the states will not be less than that of recording the raw time series. In the discretized embedding space (Fig. 4), however, if the size of grid cell is chosen appropriately, the deviated states caused by noise will fall into the same grid cell, which drastically reduces the memory cost and achieves denoising effect at the same time. In addition, any recursive trajectory can be represented by a finite number of cells in the discretized embedding space. Therefore, the discretization drastically reduces the memory consumption and potentially preserves a constant memory footprint although handling an infinite time series. Intuitively, the memory cost is decided by the cell size \u2014 a smaller cell size requires larger memory\nand vice versa. Section 2.3 will discuss more details on the choice of parameters."}, {"heading": "2.3 Parameter Selection", "text": "There are two parameters in delay embedding, i.e., the delay step s and embedding dimension d. From empirical study, d and s both significantly affect the performance of delay embedding in the aspect of classification accuracy, and they are application orientated, varying with different datasets. In DDE, we have an extra parameter \u2014 the cell size of the discretized embedding space that decides the fidelity of representing the state trajectory. This section discusses the methods of selecting appropriate values for s, d, and the grid size.\n2.3.1 Delay Step (s) According to [17], an effective method of obtaining the\noptimal s is to minimize the mutual information [5] between yt and yt+s. The idea is to ensure a large enough s so that the information measured at t + s is significantly different from that at time t. However, it needs to manually divide the observation into equally sized bins in order to compute the mutual information. [18] provided a criterion to obtain the optimal s based on periodic time series,\n2\u03c0 \u00d7 d\u00d7 s\u00d7 f fs \u2261 0 mod \u03c0, (7)\nwhere f and fs denote the resonant and sampling frequency, respectively, of the time series. In practice, however, the time series is not necessarily periodic. Based on the ideas from [17, 18], we decide s based on the dominant frequency of the raw time series. Instead of assigning the resonant frequency to f as in Eq. 7, we adopt the dominant frequency \u2014 the frequency with the maximum magnitude not counting the DC component in the frequency domain. To obtain an appropriate s, let 2\u03c0 \u00d7 d \u00d7 s \u00d7 f/fs = \u03c0, which minimizes the information loss when transforming the time series to the embedding space because this is the case that yields the smallest s from Eq. 7. At the same time, the minimum s is bounded by fs/(2\u00d7d\u00d7f) to avoid large mutual information.\nPractically, a times series is a sequence of points of length N . Applying Fast Fourier Transform (FFT) [3], we can obtain the dominant frequency f = nfs/N , where n denotes the index of the maximum magnitude in the spectral space. Therefore, a more succinct formula of s is\ns = N\n2d\u00d7 n. (8)\nFig. 5 illustrates the selection of s. Fig. 5(a) shows a time series with the length of N = 151 points. The index of the dominant frequency from FFT is n = 3 as shown in Fig. 5(b). From Eq. 8, an appropriate step size for d = 2 is s = 151/(2 \u00d7 2 \u00d7 3) \u2248 12.58. Since s must be an integer, we set s = 12. Comparing Figs. 5(c), 5(d) and 5(e), the roundness of the trajectory is maximized when s = 12. Either smaller or larger s will result in more overlap (mutual information) which runs higher risk of misclassification.\n2.3.2 Embedding Dimension (d) To determine a proper embedding dimension d, we apply\nthe false nearest neighbor method developed in [8]. This method assumes that the states that are close in the embedding space have to stay sufficiently close during forward iteration. If a reconstructed state has a close neighbor that does not fulfill this criterion, it is marked as having a false nearest neighbor. The steps for finding the optimal d are:\n1. Given a state \u03a6(xi) in the d-dimensional embedding space, find a neighbor \u03a6(xj) so that \u2016\u03a6(xi)\u2212\u03a6(xj)\u20162 < \u03b5, where \u03b5 is a small constant usually not larger than 1/10 of the standard deviation of the time series.\n2. Based on the neighbors, compute the normalized distance Ri between the (m+1)th embedding coordinate of state \u03a6(xi) and \u03a6(xj):\nRi = \u2016yi+d\u00d7s \u2212 yj+d\u00d7s\u20162 \u2016\u03a6(xi)\u2212 \u03a6(xj)\u20162\n(9)\n3. If Ri is larger than a given threshold Rth, then \u03a6(xi) is marked as having a false nearest neighbor.\n4. Apply Eq. 9 for the whole time series and for various m = 1, 2, \u00b7 \u00b7 \u00b7 until the fraction of points for which Ri > Rth is negligible. According to [8], Rth = 10 has proven to be a good choice for most data sets.\nApplying the above method on the time series in Fig. 5(a), the process of finding the optimal d is shown in Fig. 6.\nIn DDE, the above methods of finding appropriate s and d can be applied iteratively on the derivative of the raw time series. In practice, we cannot ensure the optimal parameter setting for any time series in classification tasks because the optimal setting always varies with classes, and we have to use a uniform setting for all classes to achieve fair data representation. Therefore, we randomly choose some training examples from each class and use the mean of the optimal settings from each class as the final values. The setting of s only affects the classification accuracy, while d also affects computational complexity. A larger d does not necessarily improve the classification accuracy but certainly increases the burden on computation. To balance the accuracy and computational complexity, we prefer to select a smaller d.\n2.3.3 Size of Grid Cell The third parameter is the cell size of the discretized em-\nbedding space used in DDE. The cell size decides the fidelity of representing the trajectories, which in turn affects the classification accuracy. Generally, the accuracy increases as the cell size decreases, nevertheless, a too small cell size drastically increases the computational complexity and memory cost. Actually, when the cell size goes smaller and smaller, the overfitting problem starts to surface and the model ends up fitting the noisy data. From our experiment, an appropriate cell size is (y\u2032max\u2212y\u2032min)/50, where y\u2032max and y\u2032min denote the maximum and minimum of the derivative time series, respectively. In other words, the trajectories are represented on a grid with approximately 50 bins on each dimension."}, {"heading": "3. MARKOV GEOGRAPHIC MODEL", "text": "As discussed in DDE, the trajectory constructed from a time series preserves distinguishable and robust patterns. Therefore, we can model the trajectories of different classes during training. Then, the label of a testing time series could be decided by comparing with those learned trajectories. Many existing works related to delay embedding would model the trajectories by a group of differential functions, parametric models [9], or topological features [18], e.g., barcodes from persistent homology. However, they all perform in an offline manner, and it is difficult to find a parametric model that is suitable for all applications. We propose a non-parametric model MGM that could model the trajectories in an online manner.\nFrom Fig. 1, we can see that the trajectory and geographical location of the states both carry significant information that distinguish one pattern from another. The trajectory can be modeled by the Markov process \u2014 the arrows in the embedding space of Fig. 1 denote transition of the states. However, the Markov process is sensitive to the probability of the starting state, e.g., if the starting state is an outlier, the probability of starting state will be small, thus the final probability of the whole trajectory will be small although the transition probability is large. Therefore, the geographic distribution of the states is constructed instead which depicts the probability that a trajectory belongs to a class in a more global and robust manner. We refer to the proposed model as the Markov geographic model (MGM), which efficiently and effectively models both the geographic distribution of the states as well as their transition \u2014 the two pieces of information that non-parametrically identify the deterministic function \u03c6 in Eq. 1.\nSpecifically, the geographic distribution is represented by a probability map with the same size as the discretized embedding space. The state transitions are exhaustively recorded by an \u201cexpandable list\u201d. When a new transition appears, it is appended to the end of the list if it has not occurred in the past, otherwise, it is accumulated to the existing transition.\nAssuming a d-D discretized embedding space, and each grid cell is associated with an accumulator, counting the number of states falling into the cell during training. Then, the geographic distribution of the states can be obtained by\nP (xt) = log (|\u03a6\u2032(xt)|+ 1)\u2211 i log (|\u03a6\u2032(xi)|+ 1) , (10)\nwhere P (xt) is the probability of the state xt belonging to the training trajectories of a given MGM, |\u03a6\u2032(xt)| returns the number of states falling into the grid cell of coordinate \u03a6\u2032(xt) in the discretized embedding space. Because the derivative of a time series would result in many zerocrossing points, significantly increasing the number of states falling around the origin ([0]d) of the embedding space, we apply the logarithm to suppress these large counts.\nSimilarly, the transition probability from a state to another can be expressed as\nP (xt|xt\u22121) = |\u03a6\u2032(xt); \u03a6\u2032(xt\u22121)|\u2211 i |\u03a6\u2032(xi); \u03a6\u2032(xt\u22121)| , (11)\nwhere |\u03a6\u2032(xt); \u03a6\u2032(xt\u22121)| returns the number of transitions from xt\u22121 to xt during modeling, and xi denotes the ith possible state transiting from xt\u22121.\nBased on the Markov process and state distribution, the similarity between a testing trajectory and the MGM of a given class is defined by\nSMGM(X) = t\u2211 j=1 P (xj) t\u220f i=2 P (xi|xi\u22121)\n= SG(X)\u00d7 SM(X)\n, (12)\nwhere SMGM(\u00b7) is the similarity between the testing trajectory X = [x1, x2, \u00b7 \u00b7 \u00b7 , xt] and the MGM of the given class. SM(\u00b7) and SG(\u00b7) estimate the similarity in aspects of state transition and distribution, respectively. Compared to the original Markov process, we use the global probability of the whole trajectory as the starting probability instead of\nthe single state probability to make it more robust to noise and outliers."}, {"heading": "4. ONLINE MODELING", "text": "This section elaborates on how the proposed DDE-MGM models and classifies the time series both in an online manner. Fig. 7 shows the flow of the DDE-MGM scheme, assuming d = 2 and s = 1 for simplicity. During online modeling, a training time series streams through a buffer of size (d \u2212 1)s + 1 = 2. When a new data point arrives, the oldest one in the buffer will be removed, and only the data points in the buffer are applied to DDE to construct a state in the discretized embedding space. Location of the state and its transition from its previous state are updated in the corresponding MGM."}, {"heading": "4.1 Representing MGM", "text": "In a discretized embedding space of 50\u00d750 grid, for example, the geographic distribution require 502 bytes to record the counts (|\u03a6\u2032(xt)| in Eq. 10) of states falling into each cell. For the transition probability, a common way to record all possible transitions is to construct a (50\u00d750)2 matrix, which is huge and a waste of memory. Since such a matrix is normally sparse, we use a \u201clist\u201d to accumulate only those active transitions (|\u03a6\u2032(xt); \u03a6\u2032(xt\u22121)| in Eq. 11). Note that each class maintains a separate MGM. The modeling procedure transforms the time series and updates MGMs in real time without any preprocessing or making any assumptions on the time series. During the classification, a testing stream is also transformed to the embedding space, where Eq. 12 can be applied to incrementally calculate the similarity between the testing stream and each class:\nStG = S t\u22121 G + P (xt), (13) StM = S t\u22121 M \u00d7 P (xt|xt\u22121), (14) StMGM = S t G \u00d7 StM. (15)"}, {"heading": "4.2 Neighborhood Matching", "text": "In practice, a testing trajectory cannot perfectly match an MGM. Therefore, we further propose the neighborhood matching strategy to improve the robustness. The improved SM(X) based on neighborhood matching is defined as\nSM(X) = t\u220f i=2 \u2211 \u03b1\u2208Nr(\u03a6\u2032(xi)), \u03b2\u2208Nr(\u03a6\u2032(xi\u22121)) |\u03b1;\u03b2|\u2211 k \u2211 \u03b3\u2208Nr(\u03a6\u2032(xi\u22121)) |\u03a6 \u2032(xk); \u03b3| , (16)\nwhere Nr(\u03a6 \u2032(xi)) denotes the set of neighbors within radius r around \u03a6\u2032(xi), and k walks all possible states learned\nAlgorithm 1 DDE-MGM online modeling/classification\nInitialization delay step s, embedding dimension d, and cell size of discretized embedding space *** Online Modeling Thread *** Input a training stream f(t) for each time point t do\nobtain label index i \u03a6\u2032(xt) = G (f\n\u2032(t), f \u2032(t+ s), \u00b7 \u00b7 \u00b7 , f \u2032(t+ (d\u2212 1)s)) |\u03a6\u2032(xt)|i = |\u03a6\u2032(xt)|i + 1 |\u03a6\u2032(xt); \u03a6\u2032(xt \u2212 1)|i = |\u03a6\u2032(xt); \u03a6\u2032(xt \u2212 1)|i + 1 update |\u03a6\u2032(xt)|i and |\u03a6\u2032(xt); \u03a6\u2032(xt \u2212 1)|i to MGMi\nend for *** Online Classification Thread *** Input a testing stream g(t) Output label of g(t) for each time point t do\n\u03a6\u2032(xt) = G (g \u2032(t), g\u2032(t+ s), \u00b7 \u00b7 \u00b7 , g\u2032(t+ (d\u2212 1)s)) for each class i do query |\u03a6\u2032(xt)|i and |\u03a6\u2032(xt); \u03a6\u2032(xt\u2212 1)|i from MGMi compute P (xt) and P (xt|xt\u22121) by Eqs. 10 and 11 StGi = S t\u22121 Gi + P (xt)\nStMi = S t\u22121 Mi \u00d7 P (xt|xt\u22121) StMGMi = S t Gi \u00d7 StMi\nend for if output required then\nreturn arg max i {StMGMi}i=1,2,\u00b7\u00b7\u00b7\nend if end for\nby the MGM. Usually, radius r is set to be the cell size, which means the neighbors are searched from a 3 \u00d7 3 window. The improved SM(X) forms clusters centered at the testing states, becoming an estimate of cluster-wise transition probability, which effectively increases the robustness to noise and intra-class variation. Fig. 8 illustrates the neighbor matching.\nThe DDE-MGM scheme is summarized in Algorithm 1, where the accumulators, i.e., |\u03a6\u2032(xt)| and |\u03a6\u2032(xt); \u03a6\u2032(xt\u22121)|, are simultaneously updated and queried by the modeling and classification threads. After the initial training stage, the classification thread can be performed in parallel with\nthe modeling without having to wait until the end of the training stream."}, {"heading": "5. EXPERIMENTAL EVALUATION", "text": "The proposed DDE-MGM is evaluated on three datasets \u2014 UCI character trajectories [12], MSR Action3D [26], and PAMAP [20] outdoor activities. To illustrate the low computational complexity and superior classification performance, DDE-MGM is compared to HMM [15], SAX [13] and 1NNDTW [29], which are considered the best algorithms for time series classification. In addition, we also compare with some state-of-the-art online algorithms, RBP [4], Projectron [16], BPAS [27], BOGD [31], and NOGD [14], to verify the online performance of DDE-MGM. Besides classification accuracy, the run time is also concerned to evaluate computational complexity."}, {"heading": "5.1 Datasets", "text": "The UCI Character Trajectory dataset [12] consists of 2858 character samples of 20 classes. Three dimensions are kept \u2014 x, y, and pen tip force. The data was normalized and shifted so that their velocity profiles best match the mean of the set. This dataset serves as the baseline to compare different algorithms because its samples are well aligned, truncated to the similar length and normalized to the same baseline.\nThe MSR Action3D dataset [26] is the most popular dataset used by most action recognition related literature. It consists of 567 action samples of 20 classes performed by 10 subjects. This dataset is of random data length without careful alignment. Each action sample is presented by a sequence of skeletons with 20 joints in the 3-D spatial space. We consider such a sequence as a 60-D time series because each skeleton can be treated as a point of 20 (joints) \u00d7 3 (3-D space) dimensions. This dataset is to highlight the advantage of DDE-MGM in robustness to length variation and misalignment of the time series.\nThe PAMAP outdoor activities dataset [20] was collected from wearable sensors on subjects\u2019 hand, chest, and shoe when performing physical activities \u2014 walking very slow, normal walking, Nordic walking, running, cycling and rope jumping. The samples in this dataset last tens of minutes and do not have fixed length. Only the 3-D acceleration data on hand is used in the experiment, which is sufficient to warrant a competitive classification accuracy. Because the samples is long in time and repetitive in patterns, e.g., walking for tens of minutes, this dataset is adopted to mainly examine the efficiency of DDE-MGM in aspects of run time and memory cost."}, {"heading": "5.2 Experimental Setup", "text": "In DDE-MGM, there are four parameters in total\u2014s, d and grid size for DDE (Eq. 6), and r for the neighborhoodmatching-based similarity function (Eqs. 12 and 16). The parameters s and d can be obtained by applying Eqs. 8 and 9 (false nearest neighbor) on randomly selected training samples, and then choosing the averaged settings. Through extensive empirical studies, it is appropriate to divide each dimension of the embedding space into a roughly 50 bins. The size of one interval is set to be the cell size. The neighbor size r is set to be the cell size.\nIn the experiment, two groups of algorithms are cited to compare with the propose DDE-MGM: 1) offline algorithms\nthat can achieve the state-of-the-art classification accuracy but time-consuming or assuming the input data are of the same length and well aligned, and 2) online algorithms that learn models efficiently in an online fashion \u2014 the model is updated and applied to testing alternately for each sample in the dataset. When a sample arrives, for example, the model is first applied to classify this sample, and then the sample is used to update the corresponding model.\nAll algorithms are run with Matlab on a laptop with Intel i7 dual-core 2.4GHz CPU. Therefore, we can achieve a fair comparison on the run time. In the offline comparison, the classification accuracy is obtained by leave-50%-out cross validation. In the online comparison, training and testing are performed alternatively on each sample of the dataset."}, {"heading": "5.3 Classification Performance", "text": "In the comparison of classification performance, both accuracy and run time are considered. Parameter settings based on section 2.3 for each dataset is listed in Table 1, where the cell size and neighbor size r are not included because they can be considered as constant regardless of the different datasets. Compared to the MSR and PAMAP datasets, the dominant frequency of data samples in the UCI dataset is much smaller, so it is assigned a larger s and d to decrease the mutual information between the reconstructed states.\nThe UCI dataset: Table 2 compares the performance of DDE-MGM with both offline (upper block) and online (lower block) algorithms. The notation \u201cO/R\u201d is short for Online modeling/Random data length and alignment. \u201c+\u201d and \u201c-\u201d denote whether the algorithm is able to achieve O/R or not. The \u201cTime\u201d column shows the total run time of training and testing. From the aspect of run time, DDEMGM cannot beat most online methods. For accuracy, however, DDE-MGM is superior to the state-of-the-art in both off- and on-line categories. Note that the run time of DDE-MGM in the online testing is longer than that in the offline testing because the offline testing performs training and testing each on half of the dataset, while the online testing trains and tests alternatively on the whole dataset. Although DDE-MGM takes longer time on the whole dataset (2858 samples) in the online testing, it can still achieve realtime performance on any single sample. In addition, DDEMGM realizes O/R in both training and testing.\nIn the online experiment, although DDE-MGM is not the most efficient, it achieves the highest accuracy, even in the earlier stage (fewer samples) as illustrated in Fig. 9.\nThe MSR Action3D dataset: The most notable advantage of DDE-MGM over the other online algorithms is the robustness to random data length and misalignment, which is better demonstrated in the experiment on the MSR Action3D dataset as shown in Table 3. For this dataset, DDE-MGM significantly outperforms the other algorithms in both off- and on-line testing because the raw data is not well aligned and varies in length.\nThe highest accuracy of the other online algorithms is around 30% (BPAS) because they are sensitive to the alignment of the time series. Fig. 10 compares the online performance where it is obvious that DDE-MGM still preserves relatively good performance.\nBecause MSR Action3D is one of the most popular datasets used by the action recognition community, we follow the same experimental setup in most related works for a fair comparison with the state-of-the-art performance. The accuracy we obtain is about 93%, and the literature [25, 1] published in recent years achieved around 90%. Therefore, DDE-MGM is still competitive to those algorithms specifically designed for this dataset.\nThe PAMAP dataset: Actually, the previous two datasets are not infinite streaming time series because the time duration is only a few seconds for each sample. Therefore, all the cited algorithms are modeling on multiple examples (segments) rather than on a data stream. For a streaming time series, there is not specific start or end time, so that the online algorithms cited in this paper cannot work without employing extra segmentation methods. After incorporat-\ning certain segmentation algorithm, however, the algorithms may loose the online property or yield lower classification accuracy. To demonstrate the effectiveness of DDE-MGM on modeling streaming time series, the PAMAP dataset is adopted because its samples last tens of minutes (comprising tens of thousands of points) that could be considered a streaming time series. Assuming the data points arrive one-by-one, DDE-MGM incrementally models the stream without segmentation or any other preprocessing. Table 4 reports the results of DDE-MGM, as well as the offline methods. Leave-50%-out cross validation is applied, and the samples are truncated into the same length for SAX and HMM. The online methods are not involved in this comparison because they require extra segmentation algorithms.\nThe \u201cDictionary\u201d denotes the algorithm in [7], which relaxed the fixed-length assumption by learning a dictionary in an offline manner. It achieves the state-of-the-art accuracy of 84.8% that is a bit lower than DDE-MGM, but its run time is drastically longer than the proposed."}, {"heading": "5.4 How Fast is DDE-MGM Model?", "text": "We have claimed that DDE-MGM can achieve online modeling. However, no online method can handle a time series with infinite streaming rate. So, what is the limit of DDEMGM? To find the limitation, we use the PAMAP dataset again because its long time duration is suitable to examine the maximum modeling speed.\nIn the modeling stage of DDE-MGM, there are totally three parameters \u2014 delay step s, embedding dimension d and cell size of the discretized embedding space. The parameters d and s determine how many points and what interval they are extracted from the data stream to reconstruct a state in the embedding space. Variation of these parameters will not affect the computational complexity of DDE, whose run time is approximately constant. Therefore, we may ignore the effect of s and d on modeling speed. The only parameter remained is the cell size, which significantly affects the modeling speed in our experiment. A smaller cell\nsize, for example, will result in more cells in the discretized embedding space, so that the grid size (the number of bins on each dimension) will be larger, and more states and transitions need to be recorded. Note that the cell size is the size of a cell in the grid, and the grid size refers to the number of cells on each dimension of the grid.\nAs discussed in section 4, we use a \u201clist\u201d to represent the sparse transitions, therefore a larger grid size generates a longer list. Most run time of DDE-MGM during modeling is consumed on searching the list for accumulating new transitions to existing ones, so larger grid size results in longer run time as shown in Table 5. In addition, larger grid size does not necessarily increase the accuracy because when the grid size goes larger (i.e., the cell size goes smaller), the overfitting problem starts to surface and the model ends up fitting noisy data. The grid size of 50 is an appropriate setting based on extensive empirical studies.\nThe efficiency in the aspect of memory cost is uniquely determined by the grid size. Also shown in Table 5, the memory cost increases monotonously with the grid size. The raw time series from one class is over 10MB, the memory footprint of the learned model from one class is less than 7KB under the grid size of 50. Because the grid size is fixed, the number of memory units is fixed as well; then the memory cost approaches a constant regardless of the stream length.\nTo explicitly show how fast DDE-MGM can model a streaming time series, we investigate the maximally-allowed streaming rate \u2014 the maximum points from the time series that can be updated to the MGM model in one second, as shown in the last row of Table 5. The results are obtained by modeling several randomly truncated time series of 10,000 points from the PAMAP dataset. When the grid size is 50, for example, the averaged run time on each truncated time series is about 0.86 sec, thus the maximally-allowed streaming rate is 10, 000/0.86 \u2248 11, 600 Hz, which is sufficient for most real-world applications. Note that the maximally-allowed streaming rate only varies with the grid size."}, {"heading": "5.5 Effect of Parameter Setting", "text": "Although appropriate parameter settings for the delay step s and embedding dimension d can be obtained based on the methods in [18] and [8], respectively, it is still interesting to see the effect of the two parameters on classification accuracy. This section compares the accuracy using different s or d on the UCI character trajectories and MSR Action3D datasets. Fig. 11 demonstrates the effect of s on classification accuracy.\nFrom Fig. 11(a), the selected s in our experiment based on the method in [18] is not necessarily the optimal s because the accuracy is a little bit higher when s = 12 (92.21%). However, the accuracy of selected s is very close to that of\nthe optimal s. By the same token, the selected d is not necessarily the best in general as shown in Fig. 12.\nAbove all, both s and d significantly affect the classification accuracy. The extent of effect mainly depends on the dataset. If the dominant frequency of the samples is small, the changes of s and d will cause less variation on the accuracy, and vice versa. For example, the samples in the UCI dataset have lower dominant frequency than that in the MSR Action3D dataset, so the changing of parameters affects more on the latter. If the parameters vary around the selected values (deviating 1 or 2 from the selected value), the accuracy changes about two percent for the UCI dataset. In contrast, the accuracy changes approximately four percent on the MSR Action3D dataset."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we proposed a novel method, DDE-MGM, to model and classify time series in an online manner, where common but unrealistic assumptions like the same data length and well alignment are completely removed, facilitating the deployment of the method to real-world problem solving. The main objective of DDE-MGM is computational efficiency from the aspects of both computing time and memory consumption, while preserving superior classification accuracy as compared to the state-of-the-art methods. The experiments conducted on three real datasets had validated (1) the effectiveness of using the trajectory in the embedding space to distinguish the intrinsic patterns of different classes in the time-domain training steam, (2) the flexibility and feasibility of the novel online processing scheme for streaming data without making any assumptions, (3) the\ngreat potential for modeling and classification in real time, and (4) the small and constant memory footprint."}, {"heading": "7. REFERENCES", "text": "[1] R. Anirudh, P. Turaga, J. Su, and A. Srivastava.\nElastic functional coding of human actions: from vector-fields to latent variables. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3147\u20133155, 2015.\n[2] D. J. Berndt and J. Clifford. Using dynamic time warping to find patterns in time series. In KDD Workshop, volume 10, pages 359\u2013370. Seattle, WA, 1994.\n[3] E. O. Brigham. The Fast Fourier Transform and its applications. 1988.\n[4] G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Tracking the best hyperplane with a simple budget perceptron. Machine Learning, 69(2-3):143\u2013167, 2007.\n[5] A. M. Fraser and H. L. Swinney. Independent coordinates for strange attractors from mutual information. Physical Review A, 33(2):1134, 1986.\n[6] M. A. Hanson, H. Powell, A. T. Barth, J. Lach, and M. Brandt-Pearce. Neural network gait classification for on-body inertial sensors. In 6th International Workshop on Wearable and Implantable Body Sensor Networks, pages 181\u2013186. IEEE, 2009.\n[7] B. Hu, Y. Chen, and E. J. Keogh. Time series classification under more realistic assumptions. In SDM, pages 578\u2013586, 2013.\n[8] M. B. Kennel, R. Brown, and H. D. Abarbanel. Determining embedding dimension for phase-space reconstruction using a geometrical construction. Physical Review A, 45(6):3403, 1992.\n[9] C. Lainscsek and T. J. Sejnowski. Delay differential analysis of time series. Neural Computation, 2015.\n[10] S. Laur. Timeseries of determinisic dynamic systems. 2004.\n[11] X. Li, R. Guo, and C. Chen. Robust pedestrian tracking and recognition from flir video: A unified approach via sparse coding. Sensors, 14(6):11245\u201311259, 2014.\n[12] M. Lichman. UCI machine learning repository, 2013.\n[13] J. Lin, E. Keogh, S. Lonardi, and B. Chiu. A symbolic representation of time series, with implications for streaming algorithms. In Proceedings of the 8th ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery, pages 2\u201311. ACM, 2003.\n[14] J. Lu, S. C. Hoi, J. Wang, P. Zhao, and Z.-Y. Liu. Large scale online kernel learning. Journal of Machine Learning Research (JMLR), 2015.\n[15] F. Lv and R. Nevatia. Recognition and segmentation of 3-D human action using hmm and multi-class adaboost. In Computer Vision\u2013ECCV 2006, pages 359\u2013372. Springer, 2006.\n[16] F. Orabona, J. Keshet, and B. Caputo. The projectron: a bounded kernel-based perceptron. In Proceedings of the 25th International Conference on Machine Learning, pages 720\u2013727. ACM, 2008.\n[17] M. Perc. The dynamics of human gait. European Journal of Physics, 26(3):525, 2005.\n[18] J. A. Perea and J. Harer. Sliding windows and persistence: An application of topological methods to signal analysis. Foundations of Computational Mathematics, pages 1\u201340, 2013.\n[19] F. Petitjean, G. Forestier, G. Webb, A. Nicholson, Y. Chen, and E. Keogh. Dynamic time warping averaging of time series allows faster and more accurate classification. In IEEE International Conference on Data Mining, 2014.\n[20] A. Reiss and D. Stricker. Towards global aerobic activity monitoring. In Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments, page 12. ACM, 2011.\n[21] J. J. Rodr\u0301\u0131guez and C. J. Alonso. Interval and dynamic time warping-based decision trees. In Proceedings of the 2004 ACM Symposium on Applied Computing, pages 548\u2013552. ACM, 2004.\n[22] T. Sauer, J. A. Yorke, and M. Casdagli. Embedology. Journal of Statistical Physics, 65(3-4):579\u2013616, 1991.\n[23] Y. Song, W. Wang, Z. Zhang, H. Qi, and Y. Liu. Multiple event analysis for large-scale power systems through cluster-based sparse coding. In 2015 IEEE International Conference on Smart Grid Communications (SmartGridComm), pages 301\u2013306. IEEE, 2015.\n[24] F. Takens. Detecting strange attractors in turbulence. Springer, 1981.\n[25] R. Vemulapalli, F. Arrate, and R. Chellappa. Human action recognition by representing 3D skeletons as points in a lie group. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 588\u2013595, 2014.\n[26] J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet ensemble for action recognition with depth cameras. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 1290\u20131297. IEEE, 2012.\n[27] Z. Wang and S. Vucetic. Online passive-aggressive algorithms on a budget. In International Conference on Artificial Intelligence and Statistics, pages 908\u2013915, 2010.\n[28] Y. Wu and E. Y. Chang. Distance-function design and fusion for sequence data. In Proceedings of the 13th ACM International Conference on Information and Knowledge Management, pages 324\u2013333. ACM, 2004.\n[29] X. Xi, E. Keogh, C. Shelton, L. Wei, and C. A. Ratanamahatana. Fast time series classification using numerosity reduction. In Proceedings of the 23rd International Conference on Machine Learning, pages 1033\u20131040. ACM, 2006.\n[30] Z. Zhang, Y. Song, H. Cui, J. Wu, F. Schwartz, and H. Qi. Early mastitis diagnosis through topological analysis of biosignals from low-voltage alternate current electrokinetics. In 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2015.\n[31] P. Zhao, J. Wang, P. Wu, R. Jin, and S. C. Hoi. Fast bounded online gradient descent algorithms for scalable kernel-based online learning. arXiv preprint arXiv:1206.4633, 2012."}], "references": [{"title": "Elastic functional coding of human actions: from vector-fields to latent variables", "author": ["R. Anirudh", "P. Turaga", "J. Su", "A. Srivastava"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3147\u20133155,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Using dynamic time warping to find patterns in time series", "author": ["D.J. Berndt", "J. Clifford"], "venue": "KDD Workshop, volume 10, pages 359\u2013370. Seattle, WA,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "The Fast Fourier Transform and its applications", "author": ["E.O. Brigham"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "Tracking the best hyperplane with a simple budget perceptron", "author": ["G. Cavallanti", "N. Cesa-Bianchi", "C. Gentile"], "venue": "Machine Learning, 69(2-3):143\u2013167,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Independent coordinates for strange attractors from mutual information", "author": ["A.M. Fraser", "H.L. Swinney"], "venue": "Physical Review A, 33(2):1134,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1986}, {"title": "Neural network gait classification for on-body inertial sensors", "author": ["M.A. Hanson", "H. Powell", "A.T. Barth", "J. Lach", "M. Brandt-Pearce"], "venue": "6th International Workshop on Wearable and Implantable Body Sensor Networks, pages 181\u2013186. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Time series classification under more realistic assumptions", "author": ["B. Hu", "Y. Chen", "E.J. Keogh"], "venue": "SDM, pages 578\u2013586,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Determining embedding dimension for phase-space reconstruction using a geometrical construction", "author": ["M.B. Kennel", "R. Brown", "H.D. Abarbanel"], "venue": "Physical Review A, 45(6):3403,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Delay differential analysis of time series", "author": ["C. Lainscsek", "T.J. Sejnowski"], "venue": "Neural Computation,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Timeseries of determinisic dynamic systems", "author": ["S. Laur"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Robust pedestrian tracking and recognition from flir video: A unified approach via sparse coding", "author": ["X. Li", "R. Guo", "C. Chen"], "venue": "Sensors, 14(6):11245\u201311259,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A symbolic representation of time series, with implications for streaming algorithms", "author": ["J. Lin", "E. Keogh", "S. Lonardi", "B. Chiu"], "venue": "Proceedings of the 8th ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery, pages 2\u201311. ACM,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Large scale online kernel learning", "author": ["J. Lu", "S.C. Hoi", "J. Wang", "P. Zhao", "Z.-Y. Liu"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Recognition and segmentation of 3-D human action using hmm and multi-class adaboost", "author": ["F. Lv", "R. Nevatia"], "venue": "Computer Vision\u2013ECCV 2006, pages 359\u2013372. Springer,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "The projectron: a bounded kernel-based perceptron", "author": ["F. Orabona", "J. Keshet", "B. Caputo"], "venue": "Proceedings of the 25th International Conference on Machine Learning, pages 720\u2013727. ACM,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "The dynamics of human gait", "author": ["M. Perc"], "venue": "European Journal of Physics, 26(3):525,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Sliding windows and persistence: An application of topological methods to signal analysis", "author": ["J.A. Perea", "J. Harer"], "venue": "Foundations of Computational Mathematics, pages 1\u201340,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic time warping averaging of time series allows faster and more accurate classification", "author": ["F. Petitjean", "G. Forestier", "G. Webb", "A. Nicholson", "Y. Chen", "E. Keogh"], "venue": "IEEE International Conference on Data Mining,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards global aerobic activity monitoring", "author": ["A. Reiss", "D. Stricker"], "venue": "Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments, page 12. ACM,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Interval and dynamic time warping-based decision trees", "author": ["J.J. Rod\u0155\u0131guez", "C.J. Alonso"], "venue": "Proceedings of the 2004 ACM Symposium on Applied Computing, pages 548\u2013552. ACM,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Embedology", "author": ["T. Sauer", "J.A. Yorke", "M. Casdagli"], "venue": "Journal of Statistical Physics, 65(3-4):579\u2013616,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1991}, {"title": "Multiple event analysis for large-scale power systems through cluster-based sparse coding", "author": ["Y. Song", "W. Wang", "Z. Zhang", "H. Qi", "Y. Liu"], "venue": "2015 IEEE International Conference on Smart Grid Communications (SmartGridComm), pages 301\u2013306. IEEE,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Detecting strange attractors in turbulence", "author": ["F. Takens"], "venue": "Springer,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1981}, {"title": "Human action recognition by representing 3D skeletons as points in a lie group", "author": ["R. Vemulapalli", "F. Arrate", "R. Chellappa"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 588\u2013595,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining actionlet ensemble for action recognition with depth cameras", "author": ["J. Wang", "Z. Liu", "Y. Wu", "J. Yuan"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 1290\u20131297. IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Online passive-aggressive algorithms on a budget", "author": ["Z. Wang", "S. Vucetic"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 908\u2013915,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Distance-function design and fusion for sequence data", "author": ["Y. Wu", "E.Y. Chang"], "venue": "Proceedings of the 13th ACM International Conference on Information and Knowledge Management, pages 324\u2013333. ACM,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast time series classification using numerosity reduction", "author": ["X. Xi", "E. Keogh", "C. Shelton", "L. Wei", "C.A. Ratanamahatana"], "venue": "Proceedings of the 23rd International Conference on Machine Learning, pages 1033\u20131040. ACM,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Early mastitis diagnosis through topological analysis of biosignals from low-voltage alternate current electrokinetics", "author": ["Z. Zhang", "Y. Song", "H. Cui", "J. Wu", "F. Schwartz", "H. Qi"], "venue": "37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast bounded online gradient descent algorithms for scalable kernel-based online learning", "author": ["P. Zhao", "J. Wang", "P. Wu", "R. Jin", "S.C. Hoi"], "venue": "arXiv preprint arXiv:1206.4633,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 22, "context": "The theory of delay embedding [24, 22] was first introduced to reconstruct a chaotic dynamical system from a sequence of observations of the system.", "startOffset": 30, "endOffset": 38}, {"referenceID": 20, "context": "The theory of delay embedding [24, 22] was first introduced to reconstruct a chaotic dynamical system from a sequence of observations of the system.", "startOffset": 30, "endOffset": 38}, {"referenceID": 7, "context": "The reconstructed dynamical system is usually represented in a higher dimensional space, in which the dynamics presents recursive patterns [8, 18, 9] regardless of the length of the original time series.", "startOffset": 139, "endOffset": 149}, {"referenceID": 16, "context": "The reconstructed dynamical system is usually represented in a higher dimensional space, in which the dynamics presents recursive patterns [8, 18, 9] regardless of the length of the original time series.", "startOffset": 139, "endOffset": 149}, {"referenceID": 8, "context": "The reconstructed dynamical system is usually represented in a higher dimensional space, in which the dynamics presents recursive patterns [8, 18, 9] regardless of the length of the original time series.", "startOffset": 139, "endOffset": 149}, {"referenceID": 1, "context": "The dynamic time warping (DTW) method [2] has achieved good performance in time series classification, especially the 1NN-DTW [29].", "startOffset": 38, "endOffset": 41}, {"referenceID": 27, "context": "The dynamic time warping (DTW) method [2] has achieved good performance in time series classification, especially the 1NN-DTW [29].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "Several recent improvements [29, 19] have successfully reduced the computational complexity, however, they are still far from achieving online processing.", "startOffset": 28, "endOffset": 36}, {"referenceID": 17, "context": "Several recent improvements [29, 19] have successfully reduced the computational complexity, however, they are still far from achieving online processing.", "startOffset": 28, "endOffset": 36}, {"referenceID": 13, "context": "Other methods such as HMMs [15], decision tree [21], SVM [28], and neural network [6], are also limited by their high computational complexity in the training stage and the necessity to make the two impractical assumptions, i.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "Other methods such as HMMs [15], decision tree [21], SVM [28], and neural network [6], are also limited by their high computational complexity in the training stage and the necessity to make the two impractical assumptions, i.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "Other methods such as HMMs [15], decision tree [21], SVM [28], and neural network [6], are also limited by their high computational complexity in the training stage and the necessity to make the two impractical assumptions, i.", "startOffset": 57, "endOffset": 61}, {"referenceID": 5, "context": "Other methods such as HMMs [15], decision tree [21], SVM [28], and neural network [6], are also limited by their high computational complexity in the training stage and the necessity to make the two impractical assumptions, i.", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "For example, [7] removed the assumption of fixed length by learning a dictionary, but it needs a long time to learn an appropriate dictionary.", "startOffset": 13, "endOffset": 16}, {"referenceID": 10, "context": "[11, 23] removed the assumption of well alignment by sparse coding.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[11, 23] removed the assumption of well alignment by sparse coding.", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "[18, 30] exploited the delay embedding technique in time series analysis, which relaxed both assumptions of fixed length and well alignment, but neither can be performed in the \u201conline\u201d scenario.", "startOffset": 0, "endOffset": 8}, {"referenceID": 28, "context": "[18, 30] exploited the delay embedding technique in time series analysis, which relaxed both assumptions of fixed length and well alignment, but neither can be performed in the \u201conline\u201d scenario.", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": ", [16] proposed the kernel based perceptron with budget, [27] improved the online passive-aggressive algorithm, and [14] extended online gradient descent.", "startOffset": 2, "endOffset": 6}, {"referenceID": 25, "context": ", [16] proposed the kernel based perceptron with budget, [27] improved the online passive-aggressive algorithm, and [14] extended online gradient descent.", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": ", [16] proposed the kernel based perceptron with budget, [27] improved the online passive-aggressive algorithm, and [14] extended online gradient descent.", "startOffset": 116, "endOffset": 120}, {"referenceID": 9, "context": "1 Delay Embedding A time series [yt, yt+1, \u00b7 \u00b7 \u00b7 ] can be considered as an observable sequence from a latent deterministic dynamical system [10], which evolves in time", "startOffset": 140, "endOffset": 144}, {"referenceID": 22, "context": "From Takens\u2019 embedding theory [24], a series of observations need to be considered to reconstruct a single state because a state of the deterministic dynamical system is associated with current and recent observations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "According to [18], the trajectory of the recursive 2-D time series forms a pattern corresponding to the intrinsic characteristics of data in the time domain.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "According to [17], an effective method of obtaining the optimal s is to minimize the mutual information [5] between yt and yt+s.", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "According to [17], an effective method of obtaining the optimal s is to minimize the mutual information [5] between yt and yt+s.", "startOffset": 104, "endOffset": 107}, {"referenceID": 16, "context": "[18] provided a criterion to obtain the optimal s based on periodic time series,", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Based on the ideas from [17, 18], we decide s based on the dominant frequency of the raw time series.", "startOffset": 24, "endOffset": 32}, {"referenceID": 16, "context": "Based on the ideas from [17, 18], we decide s based on the dominant frequency of the raw time series.", "startOffset": 24, "endOffset": 32}, {"referenceID": 2, "context": "Applying Fast Fourier Transform (FFT) [3], we can obtain the dominant frequency f = nfs/N , where n denotes the index of the maximum magnitude in the spectral space.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "To determine a proper embedding dimension d, we apply the false nearest neighbor method developed in [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "According to [8], Rth = 10 has proven to be a good choice for most data sets.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "Many existing works related to delay embedding would model the trajectories by a group of differential functions, parametric models [9], or topological features [18], e.", "startOffset": 132, "endOffset": 135}, {"referenceID": 16, "context": "Many existing works related to delay embedding would model the trajectories by a group of differential functions, parametric models [9], or topological features [18], e.", "startOffset": 161, "endOffset": 165}, {"referenceID": 24, "context": "The proposed DDE-MGM is evaluated on three datasets \u2014 UCI character trajectories [12], MSR Action3D [26], and PAMAP [20] outdoor activities.", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "The proposed DDE-MGM is evaluated on three datasets \u2014 UCI character trajectories [12], MSR Action3D [26], and PAMAP [20] outdoor activities.", "startOffset": 116, "endOffset": 120}, {"referenceID": 13, "context": "To illustrate the low computational complexity and superior classification performance, DDE-MGM is compared to HMM [15], SAX [13] and 1NNDTW [29], which are considered the best algorithms for time series classification.", "startOffset": 115, "endOffset": 119}, {"referenceID": 11, "context": "To illustrate the low computational complexity and superior classification performance, DDE-MGM is compared to HMM [15], SAX [13] and 1NNDTW [29], which are considered the best algorithms for time series classification.", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "To illustrate the low computational complexity and superior classification performance, DDE-MGM is compared to HMM [15], SAX [13] and 1NNDTW [29], which are considered the best algorithms for time series classification.", "startOffset": 141, "endOffset": 145}, {"referenceID": 3, "context": "In addition, we also compare with some state-of-the-art online algorithms, RBP [4], Projectron [16], BPAS [27], BOGD [31], and NOGD [14], to verify the online performance of DDE-MGM.", "startOffset": 79, "endOffset": 82}, {"referenceID": 14, "context": "In addition, we also compare with some state-of-the-art online algorithms, RBP [4], Projectron [16], BPAS [27], BOGD [31], and NOGD [14], to verify the online performance of DDE-MGM.", "startOffset": 95, "endOffset": 99}, {"referenceID": 25, "context": "In addition, we also compare with some state-of-the-art online algorithms, RBP [4], Projectron [16], BPAS [27], BOGD [31], and NOGD [14], to verify the online performance of DDE-MGM.", "startOffset": 106, "endOffset": 110}, {"referenceID": 29, "context": "In addition, we also compare with some state-of-the-art online algorithms, RBP [4], Projectron [16], BPAS [27], BOGD [31], and NOGD [14], to verify the online performance of DDE-MGM.", "startOffset": 117, "endOffset": 121}, {"referenceID": 12, "context": "In addition, we also compare with some state-of-the-art online algorithms, RBP [4], Projectron [16], BPAS [27], BOGD [31], and NOGD [14], to verify the online performance of DDE-MGM.", "startOffset": 132, "endOffset": 136}, {"referenceID": 24, "context": "The MSR Action3D dataset [26] is the most popular dataset used by most action recognition related literature.", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": "The PAMAP outdoor activities dataset [20] was collected from wearable sensors on subjects\u2019 hand, chest, and shoe when performing physical activities \u2014 walking very slow, normal walking, Nordic walking, running, cycling and rope jumping.", "startOffset": 37, "endOffset": 41}, {"referenceID": 23, "context": "The accuracy we obtain is about 93%, and the literature [25, 1] published in recent years achieved around 90%.", "startOffset": 56, "endOffset": 63}, {"referenceID": 0, "context": "The accuracy we obtain is about 93%, and the literature [25, 1] published in recent years achieved around 90%.", "startOffset": 56, "endOffset": 63}, {"referenceID": 6, "context": "The \u201cDictionary\u201d denotes the algorithm in [7], which relaxed the fixed-length assumption by learning a dictionary in an offline manner.", "startOffset": 42, "endOffset": 45}, {"referenceID": 16, "context": "Although appropriate parameter settings for the delay step s and embedding dimension d can be obtained based on the methods in [18] and [8], respectively, it is still interesting to see the effect of the two parameters on classification accuracy.", "startOffset": 127, "endOffset": 131}, {"referenceID": 7, "context": "Although appropriate parameter settings for the delay step s and embedding dimension d can be obtained based on the methods in [18] and [8], respectively, it is still interesting to see the effect of the two parameters on classification accuracy.", "startOffset": 136, "endOffset": 139}, {"referenceID": 16, "context": "11(a), the selected s in our experiment based on the method in [18] is not necessarily the optimal s because the accuracy is a little bit higher when s = 12 (92.", "startOffset": 63, "endOffset": 67}], "year": 2016, "abstractText": "The staggering amount of streaming time series coming from the real world calls for more efficient and effective online modeling solution. For time series modeling, most existing works make some unrealistic assumptions such as the input data is of fixed length or well aligned, which requires extra effort on segmentation or normalization of the raw streaming data. Although some literature claim their approaches to be invariant to data length and misalignment, they are too time-consuming to model a streaming time series in an online manner. We propose a novel and more practical online modeling and classification scheme, DDE-MGM, which does not make any assumptions on the time series while maintaining high efficiency and state-of-the-art performance. The derivative delay embedding (DDE) is developed to incrementally transform time series to the embedding space, where the intrinsic characteristics of data is preserved as recursive patterns regardless of the stream length and misalignment. Then, a non-parametric Markov geographic model (MGM) is proposed to both model and classify the pattern in an online manner. Experimental results demonstrate the effectiveness and superior classification accuracy of the proposed DDE-MGM in an online setting as compared to the stateof-the-art.", "creator": "LaTeX with hyperref package"}}}