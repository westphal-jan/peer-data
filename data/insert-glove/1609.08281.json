{"id": "1609.08281", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "An Efficient Method for Robust Projection Matrix Design", "abstract": "goito The aim longkou of this brief 9.20 is sheraton to fellowship design a fingerings robust juemin projection overlord matrix 3,686 for larrabee the nacka Compressive Sensing (39-14 CS) giugno system when polymerizes the burgen signal is bylaw not televise exactly supremes sparse. spanish-french The bannister optimal efforts projection teen matrix design ghoni is aborigine obtained by anacin minimizing the pemako Frobenius norm demanded of the 8w difference between nitrogen-fixing the chatur identity bvsc matrix e.b. and the Gram matrix of guiseley the melioidosis equivalent dictionary $ \\ Phi \\ lavrentiy Psi $. mackoul A sinaloa novel jet-set penalty $ \\ | \\ 2.425 Phi \\ | _F $ is coorey added mengxun to make the angelos projection rampaged matrix robust quizon when gatchinsky the sparse representation dionisi error (setif SRE) bragged exists. Additionally, designing tamannaah the f.a. projection matrix 68-68 with rawal a bodu high dimensional dictionary levana improves 1281 the erlend signal lewit reconstruct accuracy hualon when system/360 the tillmans compression 8,210 rate gudalur is desha the suben same bowhunting as 104.78 in a vc-1 low agm-130 dimensional dictionary scenario. Simulation results demonstrate the 30-day effectiveness of gruy\u00e8re the proposed approach comparing with the state - of - the - 3.5-inch art microdistrict methods.", "histories": [["v1", "Tue, 27 Sep 2016 06:59:11 GMT  (75kb)", "https://arxiv.org/abs/1609.08281v1", "1 figure, 4 tables"], ["v2", "Thu, 19 Jan 2017 17:01:06 GMT  (177kb)", "http://arxiv.org/abs/1609.08281v2", "3 figures, 4 tables"], ["v3", "Wed, 6 Sep 2017 17:53:44 GMT  (6540kb,D)", "http://arxiv.org/abs/1609.08281v3", "8 figures, 4 tables"]], "COMMENTS": "1 figure, 4 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tao hong", "zhihui zhu"], "accepted": false, "id": "1609.08281"}, "pdf": {"name": "1609.08281.pdf", "metadata": {"source": "CRF", "title": "An Efficient Method for Robust Projection Matrix Design", "authors": ["Tao Honga", "Zhihui Zhub"], "emails": ["hongtao@cs.technion.ac.il", "zzhu@mines.edu"], "sections": [{"heading": null, "text": "Our objective is to efficiently design a robust projection matrix \u03a6 for the Compressive Sensing (CS) systems when applied to the signals that are not exactly sparse. The optimal projection matrix is obtained by mainly minimizing the average coherence of the equivalent dictionary. In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix. Without requiring of training data, we can efficiently design the robust projection matrix and apply it for most of CS systems, like a CS system for image processing with a conventional wavelet dictionary in which the SRE matrix is generally not available. Simulation results demonstrate the efficiency and effectiveness of the proposed approach compared with the state-of-the-art methods. In addition, we experimentally demonstrate with natural images that under similar compression rate, a CS system with a learned dictionary in high dimensions outperforms the one in low dimensions in terms of reconstruction accuracy. This together with the fact that our proposed method can efficiently work in high dimension suggests that a CS system can be potentially implemented beyond the small patches in sparsity-based image processing.\nKeywords: Robust projection matrix, sparse representation error (SRE), high dimensional dictionary, mutual coherence."}, {"heading": "1. Introduction", "text": "Since the beginning of this century, Compressive Sensing or Compressed Sensing (CS) has received a great deal of attention [1] - [6]. Generally speaking, CS is a mathematical framework that addresses accurate recovery of a signal vector x \u2208\u211cN from a set of linear measurements\ny = \u03a6x \u2208\u211cM (1)\nwhere M N and \u03a6 \u2208 \u211cM\u00d7N is referred to as the projection or sensing matrix. CS has found many applications in the areas such as image processing, machine learning, pattern recognition, signal detection/classification etc. We refer the reader to [5] [6] and the references therein to find the related topics mentioned above.\nSparsity and coherence are two important concepts in CS theory. We say a signal x of interest approximately sparse (in some basis or dictionary) if we can approximately express it as a linear combination of few columns (also called atoms) from a well-chosen dictionary:\nx = \u03a8\u03b8+ e (2)\nwhere \u03a8 \u2208 \u211cN\u00d7L is the given or determined dictionary, \u03b8 \u2208 \u211cL is a sparse coefficient vector with few non-zero elements, and e \u2208\u211cN stands for the sparse representation error (SRE). In particular, the vector x is called (purely or exactly) K-sparse in\nEmail addresses: hongtao@cs.technion.ac.il (Tao Hong), zzhu@mines.edu (Zhihui Zhu)\n\u03a8 if \u2016\u03b8\u20160 = K and e = 0 and approximately K-sparse in \u03a8 if \u2016\u03b8\u20160 = K and e has relatively small energy. Here, \u2016\u03b8\u20160 denotes the number of non-zero elements in \u03b8 and 0 represents a vector whose entries are equivalent to 0 . Through this paper, we say \u03b8 is K-sparse if \u2016\u03b8\u20160 = K regardless whether e = 0.\nSubstituting the sparse model (2) of x into (1) gives\ny = \u03a6\u03a8\u03b8+\u03a6e , D\u03b8+\u03a6e (3)\nwhere the matrix D =\u03a6\u03a8 is referred to as the equivalent dictionary of the CS system and \u03b5 , \u03a6e denotes the projection noise caused by SRE. The goal of a CS system is to retrieve \u03b8 (and hence x) from the measurements y. Due to the fact that M L, solving y \u2248D\u03b8 for \u03b8 is an undetermined problem which has an infinite number of solutions. By utilizing the priori knowledge that \u03b8 is sparse, a CS system typically attempts to recover \u03b8 by solving the following problem:\n\u03b8 = argmin \u03b8\u0303 \u2016\u03b8\u0303\u20160, s.t. \u2016y\u2212D\u03b8\u0303\u20162 \u2264 \u2016\u03b5\u20162 (4)\nwhich can be solved by many efficient numerical algorithms including basis pursuit (BP), orthogonal matching pursuit (OMP), least absolute shrinkage and selection operator (LASSO) etc. All of the methods can be found in [5] [7] and the references therein.\nTo ensure exact recovery of \u03b8 through (4), we need certain conditions on the equivalent dictionary D. One of such conditions is related to the concept of mutual coherence. The mutual coherence of a matrix D \u2208\u211cM\u00d7L is denoted by\n\u00b5(D), max 1\u2264i, j\u2264L |G\u0304(i, j)| (5)\nPreprint submitted to Signal Processing September 7, 2017\nar X\niv :1\n60 9.\n08 28\n1v 3\n[ cs\n.L G\n] 6\nS ep\n2 01\n7\nwhere G\u0304 = D\u0304T D\u0304 is called the Gram matrix of D\u0304 = DSsc with Ssc a diagonal scaling matrix such that each column of D\u0304 is of unit length. Here T represents the transpose operator. It is known that \u00b5(D) is lower bounded by the Welch bound \u00b5(D) = \u221a L\u2212M M(L\u22121) , i.e., \u00b5(D) \u2208 [\u221a L\u2212M M(L\u22121) ,1 ] . The mutual coherence \u00b5(D) measures the worst-case coherence between any two columns of D and is one of the fundamental quantities associated with the CS theory. As shown in [5], when there is no projection noise (i.e., \u03b5 = 0), any K-sparse signal \u03b8 can be exactly recovered by solving the linear system (4) as long as\nK < 1 2\n[ 1+\n1 \u00b5(D)\n] (6)\nwhich indicates that a smaller \u00b5(D) ensures a CS system to recover the signal with a larger K. Thus, [8] [9] proposed methods to design a dictionary with small mutual coherence. For a given dictionary \u03a8, the mutual coherence of the equivalent dictionary is actually determined or controlled by the projection matrix \u03a6. So it would be of great interest to design \u03a6 such that \u00b5(D) is minimized. Another similar indicator used to evaluate the average performance of a CS system is named average mutual coherence \u00b5av. The definition of \u00b5av is given as follows:\n\u00b5av(D), \u2211\u2200(i, j)\u2208Sav |G\u0304(i, j)|\nNav\nwhere Sav , {(i, j) : \u00b5\u0304 \u2264 |G\u0304(i, j)|} with 0 \u2264 \u00b5\u0304 < 1 as a prescribed parameter and Nav is the number of components in the index set Sav.\nThere has been much effort [10] - [14] devoted to designing an optimal \u03a6 that outperforms the widely used random matrix in terms of signal recovery accuracy (SRA). However, all these methods are based on the assumption that the signal is exactly sparse under a given dictionary, which is not true for practical applications. It is experimentally observed that the sensing matrix designed by [10] - [14] based on mutual coherence results in inferior performance for real images (which are generally approximately but not exactly sparse under a well-chosen dictionary). To address this issue, the recent work in [15] [16] proposed novel methods to design a robust projection matrix when the SRE exists.1 Through this paper, similar to what is used in [15] [16], a robust projection (or sensing) matrix means it is designed with consideration of possible SRE and hence the corresponding CS system yields superior performance when the SRE e in (2) is not nil. However, the approaches in [15] [16] need the explicit value of the SRE on the training dataset, making them inefficient in several aspects. First, many practical CS systems with predefined analytical dictionaries (e.g., the wavelet dictionary, and the modulated discrete prolate spheroidal sequences (DPSS) dictionary for sampled multiband signals [17]) actually do not involve any training dataset and hence no SRE available.\n1We note that the approaches considered in [15] [16] share the same framework. The difference is that in [16] the authors utilized an efficient iterative algorithm giving an approximate solution, while a closed form solution is derived in [15].\nIn order to design the robust projection matrix for these CS systems using the framework presented in [15] [16], one has to first construct plenty of extra representative dataset for the explicit SRE with the given dictionary, which limits the range of applications. Second, even for the CS system with a dictionary learned typically on a large-scale dataset, we need a lot of memories and computations to store and compute with the huge dataset as well its corresponding SRE for designing a robust sensing matrix. Moreover, if the CS system is applied to a dynamic dataset, e.g., video stream, it is practically impossible to store all the data and compute its corresponding SRE. Therefore, the requirement of the explicit value of SRE for the training dataset makes the methods in [15] [16] limited and inefficient for all the cases discussed above.\nIn this paper, to drop the requirement of the training dataset as well as its SRE, we propose a novel robust projection matrix framework only involving a predefined dictionary. With this new framework, we can efficiently design projection matrices for the CS systems mentioned above. We stress that by efficient method for robust projection matrix design (which is the title of this paper), we are not providing an efficient method for solving the problems in [15] [16]; instead we provide a new framework in which the training dataset and its corresponding SRE are not required any more. Experiments on synthetic data and real images demonstrate the proposed sensing matrix yields a comparable performance in terms of SRA compared with the ones obtained by [15] [16].\nBefore proceeding, we first briefly introduce some notation used throughout the paper. MATLAB notations are adopted in this paper. In this connection, for a vector, v(k) denotes the k-th component of v. For a matrix, Q(i, j) means the (i, j)-th element of matrix Q, while Q(k, :) and Q(:,k) indicate the k-th row and column vector of Q, respectively. We use I and IL to denote an identity matrix with arbitrary and L\u00d7L dimension, respectively. The k-th column of Q is also denoted by qk. trace(Q) denotes the calculation of the trace of Q. The Frobenius norm of\na given matrix Q is \u2016Q\u2016F = \u221a \u2211i, j \u2016Q(i, j)\u20162 = \u221a\ntrace(QT Q) where T represents the transpose operator. The definition of lp\nnorm for a vector v \u2208\u211cN is \u2016v\u2016p , ( N \u2211\nk=1 |v(k)|p\n) 1 p\n, p\u2265 1.\nThe remainder is arranged as follows. Some preliminaries are given in Section 2 to state the motivation of developing such a novel model. The proposed model which does not need the SRE is shown in Section 3 and the corresponding optimal sensing problem is solved in this section. The synthetic and real data experiments are carried out in Section 4 to demonstrate the efficiency and effectiveness of the proposed method. Some conclusions are given in Section 5 to end this paper."}, {"heading": "2. Preliminaries", "text": "A sparsifying dictionary \u03a8 for a given dataset {xk}Pk=1 is usually obtained by considering the following problem\n{\u03a8,\u03b8k}= argmin \u03a8\u0303,\u03b8\u0303k\nP\n\u2211 k=1 \u2016xk\u2212 \u03a8\u0303\u03b8\u0303k\u201622 s.t. \u2016\u03b8\u0303k\u20160 \u2264 K (7)\nwhich can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20]. As stated in the previous section, the SRE ek = xk\u2212\u03a8\u03b8k is generally not nil. We concatenate all the SRE {ek} into an N\u00d7P matrix:\nE , X \u2212\u03a8\u0398\nwhich is referred to as the SRE matrix corresponding to the training dataset {xk} and the learned dictionary \u03a8.\nThe recent work in [15] [16] attempted to design a robust projection matrix with consideration of the SRE matrix E and proposed to solve\n\u03a6 = argmin \u03a6\u0303 \u2016IL\u2212\u03a8T \u03a6\u0303 T \u03a6\u0303\u03a8\u20162F +\u03bb\u2016\u03a6\u0303E\u20162F (8)\nor \u03a6 = arg min\n\u03a6\u0303,G\u2208H\u03be \u2016G\u2212\u03a8T \u03a6\u0303T \u03a6\u0303\u03a8\u20162F +\u03bb\u2016\u03a6\u0303E\u20162F (9)\nwhere H\u03be is the set of relaxed equiangular tight frames (ETFs):\nH\u03be = {G|G = GT , G(i, i) = 1,\u2200 i,max i, j |G(i, j)| \u2264 \u03be}.\nCompared to (8) which requires the Gram matrix of the equivalent dictionary close to an identity matrix, (9) relaxes the requirement of coherence between the equivalent dictionary but is much harder to solve. See [15] [16] for more discussions on this issue.\nWe remark that to ensure the designed sensing matrix by (8) or (9) be robust to the SRE for all the signals of interest, the SRE matrix E should be well representative, i.e., we need sufficient number of training signals xk. As stated in [15] [16], these methods ((8) and (9)) can be applied naturally when the dictionary is learned by algorithms like K-SVD with plenty of training data {xk}, since the SRE E is available without any additional effort. However, this could be prohibitive when the CS system with an analytic dictionary is applied to some arbitrary signals (but still they are approximately sparse in this dictionary), since there are no sufficient number of data available to obtain the SRE matrix E . For example, one may only want to apply the CS system to an arbitrary image with the wavelet dictionary. Also, these methods are prohibitive for a dictionary trained on large datasets with millions of training samples and in a dynamic CS system for streaming signals. To train such a dictionary, we have to conduct online algorithms [21] - [23] which typically apply stochastic gradient method where in each iteration a randomly selected tiny part of the training signals called mini-batch instead of the whole data is utilized for computing the expected gradient. In these cases, additional efforts are needed to obtain the SRE matrix E and it is usually prohibitive to compute and store E for all the training dataset. All of these situations make the approach proposed in [15] [16] become limited.\nAiming to obtain a neural network well expressing the signals of interest, an empirical strategy widely used by deep learning community is to utilize a huge training dataset so that the\nnetwork can extract more important features and avoid overfitting. Similar to this phenomenon, a dictionary trained on a huge dataset is also expected to contain more features of the represented signal. Simulation results shown in Section 4 demonstrate that a CS system with such a dictionary and designing a projection matrix designed on this dictionary yields a higher reconstruction accuracy on natural images than the one with a dictionary obtained from a small dataset. The recent work in [26] [27] stated that a dictionary learned with larger patches (e.g., 64\u00d764) on a huge dataset can better capture features in natural images.2 In this paper, we also attempt to experimentally investigate the performance of designing a projection matrix on a high dimensional dictionary. These also motivate us to develop an efficient method for designing a robust sensing matrix without the requirement of the SRE matrix E as it is not easy to obtain for the above two situations.\nIn the next section, we provide a novel framework to efficiently design a robust sensing matrix, and more importantly, it can be applied to the situation when the SRE matrix E is not available."}, {"heading": "3. A Novel Approach to Projection Matrix Design", "text": "In this section, we provide an efficient robust sensing matrix design approach which drops the requirement of training signals and their corresponding SRE matrix E . Our proposed framework is actually inspired by (8) and (9) from the following two aspects."}, {"heading": "3.1. A Novel Framework for Robust Projection Matrix Design", "text": "First note that the energy of SRE \u2016E\u20162F is usually very small since the learned sparsifying dictionary is assumed to sparsely represent a signal well as in (2). Otherwise if \u2016E\u20162F is very large, it indicates that the dictionary is not well designed for this class of signals and it is possible that this class of signal can not be recovered from the compressive measurements no matter what projection matrix is utilized. It follows from the norm consistent property that\n\u2016\u03a6E\u2016F \u2264 \u2016\u03a6\u2016F\u2016E\u2016F (10)\nwhich implies informally that a smaller sensing matrix \u2016\u03a6\u2016F yields a smaller projected SRE \u2016\u03a6E\u2016F .\nAlso as illustrated before that the amount of training data should be sufficient so that they can represent the class of targeted signals, the energy in the corresponding SRE matrix E should spread out in every elements. In other words, one can view the expected SRE as an additive Guassian white noise. In this situation, we have the following result.\n2The dimension of a dictionary in such a case becomes high compared with the moderate dictionary size shown in [19]. In fact, the name of a high dimensional dictionary in this paper means the dictionary obtained by training on a larger size of represented signal.\nLemma 1. Suppose E (:,k)= ek,\u2200k = 1, \u00b7 \u00b7 \u00b7P are i.i.d Gaussian random vectors with each of zero mean and covariance \u03c32I . Then for any \u03a6 \u2208 RM\u00d7N , we have\nE [ \u2016\u03a6E\u20162F ] = P\u03c32\u2016\u03a6\u20162F . (11)\nwhere E denotes the expectation operator. Moreover, when the number of training samples P approaches to \u221e, we have \u2016\u03a6E\u2016 2 F\nP converges in probability and almost surely to \u03c32\u2016\u03a6\u20162F . In particular,\n\u221a p ( \u2016\u03a6E\u20162F\nP \u2212\u03c32\u2016\u03a6\u20162F\n) d\u2212\u2192N (0,2\u03c32\u2016\u03a6\u03a6T \u20162F) (12)\nwhere N (\u00b5,\u03c2) denotes the Gaussian distribution of mean \u00b5 and variance \u03c2, and d\u2212\u2192 means convergence in distribution.\nProof. For each k, we first define dk = \u03a6ek. Since ek \u223c N (0,\u03c32I), we have dk \u223c N (0,\u03c32\u03a6\u03a6T ). Let \u03a6\u03a6T = Q\u039bQT be an eigendecomposition of \u03a6\u03a6T , where \u039b is an M\u00d7M diagonal matrix with the non-negative eigenvalues \u03bb1, . . . ,\u03bbM along its diagonal. We have\n\u2016dk\u201622 = \u2016QT dk\u201622\nand QT dk \u223cN (0,\u03c32\u039b).\nFor convenience, we define new random variables c = QT dk and b1 = 1\u03bb1\u03c32 c 2(1),b2 = 1\u03bb1\u03c32 c 2(2), . . . ,bM = 1\u03bb1\u03c32 c\n2(M). It is clear that b1, b2, . . . , bM are independent random variables of \u03c721 distribution, the chi-squared distribution with 1 degree of freedom.\nNow we compute the mean of \u2016\u03a6ek\u20162F :\nE[\u2016\u03a6ek\u20162F ] = E[\u2016dk\u20162F ] = E[\u2016c\u20162F ]\n= M\n\u2211 i=1\n\u03bbi\u03c32E[bi] = M\n\u2211 i=1 \u03bbi\u03c32\n= trace(\u03c32\u03a6\u03a6T ) = \u03c32\u2016\u03a6\u20162F\n(13)\nwhere the second line we utilize E[\u03c721] = 1. The variance of \u2016\u03a6ek\u20162F is given by:\nVar[\u2016\u03a6ek\u20162F ] = Var[\u2016dk\u20162F ] = Var[\u2016c\u20162F ]\n= M\n\u2211 i=1 \u03bb2i \u03c3 4Var[bi] = 2\nM\n\u2211 i=1 \u03bb2i \u03c3 4\n= 2trace(\u03c34\u03a6\u03a6T \u03a6\u03a6T )\n= 2\u03c34\u2016\u03a6\u03a6T \u20162F\n(14)\nwhere the second line we utilize Var[\u03c721] = 2, and the third line follows because\n\u03a6\u03a6T \u03a6\u03a6T = Q\u039b2QT .\nThus, we obtain (11) by noting that\nE[\u2016\u03a6E\u20162F ] = E\n[ P\n\u2211 k=1 \u2016\u03a6ek\u201622\n] = P\u03c32\u2016\u03a6\u20162F\nIt follows from (13) and (14) that { \u2016\u03a6e1\u201622, . . . ,\u2016\u03a6eP\u201622 } is a sequence of independent and identically distributed random variable drawn from distributions of expected values given by \u03c32\u2016\u03a6\u20162F and variances given by 2\u03c34\u2016\u03a6\u03a6\nT \u20162F . Thus, by the law of large numbers [29], the average \u2016\u03a6E\u2016 2 F\nP converges in probability and almost surely to the expected value \u03c32\u2016\u03a6\u20162F as P\u2192 \u221e. Finally, the central limit theorem [29] establishes that as P approaches infinity, the random variables \u221a P( \u2016\u03a6E\u2016 2 F\nP \u2212\u03c3 2\u2016\u03a6\u20162F)\nconverges in distribution to a normal N (0,2\u03c32\u2016\u03a6\u03a6T \u20162F).\nIn words, Lemma 1 indicates that when the number of training samples approaches to infinity, \u2016\u03a6E\u20162F is proportional to \u2016\u03a6\u20162F . Inspired by (10)-(12), it is expected that without any training signals and their corresponding SRE matrix E , a robust projection matrix can be obtained by solving the following problem\n\u03a6 = argmin \u03a6\u0303\nf (\u03a6\u0303)\u2261 \u2016IL\u2212\u03a8T \u03a6\u0303 T \u03a6\u0303\u03a8\u20162F +\u03bb\u2016\u03a6\u0303\u20162F (15)\nor\n\u03a6 = arg min \u03a6\u0303,G\u2208H\u03be\nf (\u03a6\u0303,G)\u2261 \u2016G\u2212\u03a8T \u03a6\u0303T \u03a6\u0303\u03a8\u20162F +\u03bb\u2016\u03a6\u0303\u20162F (16)\nHere, with abuse of notation, we use both f to denote the objective function in (15) and (16). However, it should be clear from the context as we always use f (\u03a6\u0303) to represent the one in (15) and f (\u03a6\u0303,G) to represent the one in (16). Since the more training samples can better represent the signals of interest and the SRE, (12) indicates that the sensing matrices obtained by (15) and (16) are more robust to SRE than the ones obtained by (8) and (9). This is demonstrated by experiments in Section 4. The numerical algorithms is presented to solve (15) and (16) in the following section.\n3.2. Efficient Algorithms for Solving (15) and (16) Note that f (\u03a6\u0303) is a special case of f (\u03a6\u0303,G) with G = IL. Thus, we first consider solving\nmin \u03a6\u0303\nf (\u03a6\u0303,G) = \u2016G\u2212\u03a8T \u03a6\u0303T \u03a6\u0303\u03a8\u20162F +\u03bb\u2016\u03a6\u0303\u20162F (17)\nwith an arbitrary G. To that end, we introduce a low-rank minimization problem\nmin A\ng(A,G)\u2261 \u2016G\u2212\u03a8T A\u03a8\u20162F +\u03bbtrace(A)\ns.t. rank(A)\u2264M,A 0 (18)\nBy eigendecomposition of A, it is clear that (17) is equivalent to (18). The problem (17) is often referred to as the factor problem of (18). Also note that g(A,G) is a convex function of A for any fixed G, though the problem (18) is nonconvex because of the rank constraint. The recent work [24] has shown that a number of iterative algorithms (including gradient descent) can provably solve the factored problem (i.e., (17)) for a set of low-rank matrix optimizations (i.e., (18)). Thus, in this paper,\nthe Conjugate-Gradient (CG) [30] method is utilized to solve (17).3 The gradient of f (\u03a6\u0303,G) in terms of \u03a6\u0303 is given as follows:\n\u2207\u03a6\u0303 f (\u03a6\u0303,G) = 2\u03bb\u03a6\u0303\u22124\u03a6\u0303\u03a8G\u03a8 T +4\u03a6\u0303\u03a8\u03a8T \u03a6\u0303T \u03a6\u0303\u03a8\u03a8T (19)\nAfter obtaining the gradient of f (\u03a6\u0303,G), the toolbox minFunc4 [25] is utilized to solve (17) with CG method. We note that the gradient-based method only involves simple matrix multiplication in (19), without requiring performing SVD and matrix inversion. Hence it is also suitable for designing the projection matrix for a CS system working on high dimensional signals.\nWe now turn to solve (16) which has two variables \u03a6\u0303 and G \u2208 H\u03be. A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16]. The main idea behind alternating minimization for (16) is that we keep one variable constant (say \u03a6\u0303), and optimize over the other variable (say G). Once G is fixed, as we explained before, we utilize CG method to solve (17). On the other hand, the solution to minG f (\u03a6\u0303,G) can be simply obtained by projecting the Gram matrix of the equivalent dictionary onto the set H\u03be when we fix \u03a6\u0303. The main steps of the algorithm are outlined in Algorithm 1.\nAlgorithm 1\nInitialization: Set k = 1, \u03a60 as a random one and the number of iterations Iter.\nStep I: Set G\u0303k = \u03a8T \u03a6Tk\u22121\u03a6k\u22121\u03a8 and then project it onto the set H\u03be:\nGk(i, j) =  1, i = j, G\u0303k(i, j), i , j, |G\u0303k(i, j)| \u2264 \u03be, \u03be \u00b7 sign(G\u0303k(i, j)), i , j, |G\u0303k(i, j)|> \u03be\nwhere sign(\u00b7) is a sign function.\nStep II: Solve \u03a6k = argmin\u03a6\u0303 f (\u03a6\u0303,Gk) with CG. If k < Iter, set k = k+1 and go to Step I. Otherwise, terminate the algorithm and output \u03a6Iter.\nRemarks:\n\u2022 It is clear that this approach is independent of training data and can be utilized for most of CS systems as long as the sparsifying dictionary \u03a8 is given.\n\u2022 Even in the case where the SRE matrix E is available, it is much easier and more efficient to solve (15) than (8) since typically the number of columns in E is dramatically greater than the size of \u03a8 and \u03a6, i.e., P M,N,L.\n3We note that both of the methods shown in [15] [16] for solving (17) need to calculate the inversion of \u03a8\u03a8T . However, in practice, the learned dictionary sometimes is ill-conditioned, which may cause numerical instable problem if directly applying their methods. Thus, as global convergence of many local search algorithms for solving similar low-rank optimizations is guaranteed in [24], CG is chosen to solve (17). Obviously, if the aforementioned problem does not happen in practical cases, the method in [15] [16] can be used to address (15). Moreover, we will show that CG and the methods shown in [15] [16] yield a similar solution in the following experiments.\n4We note that minFunc is a stable toolbox that can be efficiently applied with millions of variables.\n\u2022 Simulation results with synthetic data and natural images (where the SRE matrix E is available) show that the proposed method also yields a comparable performance to or outperforms the methods in [15] [16] in terms of SRA. Moreover, the experiments on natural images show that designing a projection matrix on a given dictionary which is learned with large dataset or high-dimensional training signals can improve SRA significantly with the same compression rate MN . However, it requires a great deal of memories to store the SRE matrix E for either large dateset or high-dimensional training data."}, {"heading": "4. Simulation Results", "text": "In this section, we perform a set of experiments on synthetic data and natural images to demonstrate the performance of the CS system with projection matrix designed by the proposed methods. For convenience, the corresponding CS systems are denoted by CSMT with \u03a6\u0303 obtained via (15) and CSMT\u2212ET F with \u03a6\u0303 obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH\u2212ET F with the sensing matrix obtained via (9) [15], and CSDCS [28]. It was first proposed in [28] that simultaneously optimizing \u03a6 and \u03a8 for a CS system results in better performance in terms of SRA. In the sequel, we also examine this strategy in natural images and the corresponding CS system is denoted by CSS\u2212DCS.5 For simplicity, the parameter \u03be in H\u03be is set to Welch bound in the following experiments.\nThe SRA is evaluated in terms of the peak signal-to-noise ratio (PSNR) [5]\n\u03c1psnr , 10\u00d7 log10 [ (2r\u22121)2\n\u03c1mse\n] dB\nwith r = 8 bits per pixel. We also utilized the measure \u03c1mse:\n\u03c1mse , 1\nN\u00d7P\nP\n\u2211 k=1 \u2016x\u0303k\u2212 xk\u201622\nwhere xk is the original signal, x\u0303k = \u03a8\u03b8\u0303k stands for the reconstructed signal with \u03b8\u0303k the solution of (4), and P is the number of patches in an image or the testing data."}, {"heading": "A. Synthetic Data Experiments", "text": "An N\u00d7L dictionary \u03a8 is generated with normally distributed entries and then is normalized so that each column has unit l2 norm. We also generate a random M\u00d7N matrix \u03a60 (where each entry has Gaussian distribution of zero-mean and variance 1) as the initial condition for all of the aforementioned projection matrices. \u03a60 is also utilized as the sensing matrix in CSrandn.\n5In our experiment, the coupling factor utilized in CSS\u2212DCS is set to 0.5 which is the best value in our setting. Generally speaking, CSS\u2212DCS should have a best performance in terms of SRA because it optimizes projection matrix and dictionary simultaneously. Thus, the performance of CSS\u2212DCS serves as the indicator of the best performance can be achieved by other CS systems that only consider optimizing the projection matrix.\nThe synthetic data for training and testing is obtained as follows. A set of 2P K-sparse vectors {\u03b8k \u2208 \u211cL} is generated as the sparse coefficients where each non-zero elements of {\u03b8k} is randomly positioned with a Gaussian distribution of zero-mean and unit variance. The set of signal vectors {xk} is produced with xk = \u03a8sk + ek , x (0) k + ek, \u2200k, where \u03a8 is the given dictionary and ek is the random noise with Gaussian distribution of zero-mean and variance \u03c32e to yield different signal-to-noise ration (SNR) (in dB) of the signals. Clearly, x(0)k is exactly Ksparse in \u03a8, while xk is approximately K-sparse in \u03a8.\nDenote X = X (0)+\u2206 as the signal matrix of dimension N\u00d7 2P, where X (0)(:,k) = x(0)k and \u2206(:,k) = ek. We use the SRE matrix E = \u2206(:,1 : P) in (8) and (9) whose solutions are used for CSLH and CSLH\u2212ET F , respectively. The data X (:,P+ 1 : 2P) is utilized for testing the CS systems. The measurements {yk} are obtained by yk = \u03a6X (:,P+k), \u2200k \u2208 (0,P] where \u03a6 is the projection matrix of the CS systems. For simplicity, OMP is chosen to solve the sparse coding problem throughout the experiments.\nWith the synthetic data, we conduct three set of experiments to demonstrate the performance of our proposed framework for robust projection matrix design, i.e., (15) and (16). In these three set of experiments, we respectively show the convergence of CG method, the effect of \u03bb and the signal recovery accuracy of the proposed projection matrices CSMT and CSMT\u2212ET F versus different SNR of the signals.\n1) Convergence Analysis: Let M = 20, N = 60, L = 100 and K = 4.We utilize CG to solve (15). We note that a random dictionary with well-conditioned is chosen and thus we also compute the closed-form solution shown in [15] for (15). The objective value obtained by the closed-form solution is denoted by f \u2217 and is compared with the CG method. The evolution of f (\u03a6) for different \u03bb is shown in Figure 1. We note that different \u03bb results in different functions f (\u03a6) and hence different f \u2217. We observe global convergence of CG method for solving (15) with all the choices of \u03bb.\n2) The Choice of \u03bb: With M = 20, N = 60, L = 80, K = 4 and SNR = 15 dB, we check the effect of the trade-off parameter \u03bb in terms of \u03c1mse for CSMT and CSMT\u2212ET F . The \u03bb is chosen from 0 to 2 with step size 0.01. The evaluation of \u03c1mse versus different \u03bb is depicted in Figure 2. Remark 1:\n\u2022 As seen from Figure 2, different \u03bb yields different performance in terms of \u03c1mse for this practical situation where the SNR is 15dB. It is clear that a proper choice of \u03bb results in significantly better performance than other values, especially for CSMT\u2212ET F . Clearly, the advantage of the proposed method is shown by comparing the cases for \u03bb = 0 and other values of \u03bb as the former corresponds to the traditional approaches which do not take the SRE into account. In the sequel, we simplicity search the best \u03bb (with which the CS systems attain the minimal \u03c1mse for the test data) within (0,1] for each experiment setting.\n\u2022 According to this experiment, if \u03bb is well chosen, CSMT\u2212ET F has better performance than CSMT in terms of\n\u03c1mse. However, the performance of CSMT\u2212ET F is more sensitive than with \u03bb than CSMT . We will show in the next experiment that the performance of CSMT\u2212ET F outperforms CSMT in synthetic data when the SNR is not too small. However, for natural images which have relatively large SRE, CSMT always has better performance than CSMT\u2212ET F . This phenomenon is also observed for CSLH and CSLH\u2212ET F in [15] [16]. Thus, we only consider the performance of CSMT and CSLH for the natural images in next section.\n3) Signal Recovery Accuracy Evaluation: With M = 20, N =\n60, L = 80, K = 4 and P = 1000, we compare our CS systems CSMT and CSMT\u2212ET F with other CS systems for SNR varying from 5 to 45 dB. Figure 3 displays signal reconstruction error \u03c1mse versus SNR for all six CS systems.\nRemark 2:\n\u2022 It is clear that the sensing matrices obtained via (15) and (16) have at least similar performance to the ones obtained via (8) and (9) [15, 16], though our proposed framework does not utilize the SRE matrix E . We also observe that CSMT\u2212ET F outperforms CSLH\u2212ET F when SNR is larger than 15 dB. This demonstrates the effectiveness of our proposed framework in Section (3) and verifies our argument in Lemma 1 that the sparse representation error is not explicitly required.\n\u2022 As seen from Figure 3, CSMT has slightly better performance than CSMT\u2212ET F when the SNR is smaller than 15 dB. In other words, we recommend to utilize CSMT (with the sensing matrix obtained via (15)) when the sparse representation error is relatively large, e.g., natural images, which meets our claims in Remark 1."}, {"heading": "B. Natural Images Experiments", "text": "In this section, three set of experiments are conducted on natural images. Through these experiments, we verify the effectiveness of the proposed framework for robust sensing matrix design in Section 3 and demonstrate the reason for dropping the requirement on the SRE matrix E . As we explained before, since the SRE is relatively large for natural images, CSMT and CSLH are respectively superior to CSMT\u2212ET F and CSLH\u2212ET F . Thus, we only show the results for CSMT and CSLH .\nIn the first set of experiments, we compare the performance of CSMT and CSLH when a set of training signals and the corresponding SRE matrix E are available. In the second set of experiments, we design the projection matrix with a dictionary\nlearned on a much larger training dataset. The performance of CS systems with a higher dimensional dictionary is given in the Experiment C. We observe that a CS system with a higher dimensional dictionary and a projection matrix designed by our proposed algorithm yields better SRA under the same compression rate. Both training and testing datasets used in these three set of experiments are extracted as follows from the LabelMe database [31]. Note that Data I is extracted with small patches and Data II is obtained by sample larger patches for the third experiment.\nTraining Data I: A set of 8\u00d78 non-overlapping patches is obtained by randomly extracting 400 patches from each image in the whole LabelMe training dataset. We arrange each patch of 8\u00d78 as a vector of 64\u00d71. A set of 400\u00d72920 = 1.168\u00d7106 training samples is obtained to train the sparsifying dictionary.\nTesting Data I: A set of 8\u00d7 8 non-overlapping patches is obtained by randomly extracting 15 patches from 400 images in LableMe testing dataset as the testing data.\nTraining Data II: The training data contains a set of 16\u00d7 16 non-overlapping patches which are obtained by randomly extracting 400 patches from the whole images in the LabelMe training dataset. Each 16\u00d7 16 patch is then arranged as a length-256 vector. A set of 1.168\u00d7106 training samples is utilized.\nTesting Data II: The testing data is extracted in the same way for the training data but from the LabelMe testing dataset. We randomly extract 8000 testing samples from 400 images with each sample an 16\u00d716 non-overlapping patch.\nExperiment A: small dataset and low dimensional dictionary\nWe perform the same experiment as in [15] to demonstrate the effectiveness of the proposed CS system CSMT without using the SRE E . The training data is obtain by randomly chosen 6000 samples from Training Data I and the K-SVD algorithm is used to train the dictionary \u03a8.\nSimilar to [15], the parameters M, N, L and K are set to 20, 64, 100 and 4, respectively. The trade-off parameter \u03bb in CSLH is set to 0.1 to yield a highest \u03c1psnr for Testing Data I. We also set \u03bb = 0.1 for the proposed CS system CSMT .\nThe behavior of the five projection matrices in terms of mutual coherence and projection noise is examined and shown in Table 1. In order to illustrate the effectiveness of the proposed projection matrix, ten natural images are conducted to check its performance in terms of PSNR. The results are shown in Table 2.\nRemark 3:\n\u2022 As seen from Table 1 , the results are self-explanatory. It shows that CSMT has small \u2016\u03a6\u2016F and also small projection noise \u2016\u03a6E\u2016F . This supports the proposed idea of using \u2016\u03a6\u2016F as a surrogate of \u2016\u03a6E\u2016F to design the robust projection matrix.\n\u2022 As shown in Table 2, we observe that CSMT outperforms CSLH in terms of \u03c1psnr for most of the tested images.\nWe note that as long as an image can be approximately sparsely represented by the learned dictionary \u03a8, it is expected that the CS system CSMT yields reasonable performance for this image since the sensing matrix utilized in CSMT considers almost all the patterns of the SRE rather than a fixed one (as indicated by (12)) and thus is robust to SRE.\nWe also observe that CSS\u2212DCS has highest \u03c1psnr; this is because CSS\u2212DCS simultaneously optimizes the projection matrix and the sparsifying dictionary. It is of interest to note that CSS\u2212DCS also has small \u2016\u03a6\u2016F and \u2016\u03a6E\u2016F (as shown in Table 1). This again indicates that it is reasonable to minimize \u2016\u03a6\u2016F to get small projection noise \u2016\u03a6E\u2016F .\nExperiment B: large dataset and low dimensional dictionary In this set of experiments, we first learn a dictionary on largescale training samples, i.e., Training Data I, and then design the projection matrices with the learned dictionary. As discussed in the previous section, the large-scale training dataset makes it inefficient or even impossible to compute the SRE matrix E . Therefore, it is inefficient to utilize the methods in [15] [16] as they require the SRE matrix E . Similar reason holds for CSS\u2212DCS. Fortunately, the following results show that the proposed CS system CSMT performs comparably to CSLH .\nThe online dictionary learning algorithm in [22] [23] is chosen to train the sparsifying dictionary on the whole Training Data I. For a fair comparison, we calculate the SRE E off-line for CSLH in this experiment.6 The same M, N, L, K in Experiment A are used in this experiment. \u03bb = 0.9 and \u03bb = 1e\u2212 3 are selected for CSMT and CSLH , respectively. We note that the choice of \u03bb for CSLH is very sensitive to E . This is because the two terms \u2016IL\u2212\u03a8T \u03a6\u0303\nT \u03a6\u0303\u03a8\u20162F and \u2016\u03a6\u0303E\u20162F in (8) for CSLH have different physical meanings and more importantly, the second term \u2016\u03a6\u0303E\u20162F increases when we have more number of training data, while the first term is independent of the training data. Thus, we need to decrease \u03bb for CSLH when we increase the number of training data. Remark 4:\n\u2022 As shown in Table 3, benefiting from large-scale training samples, the performance of both CSLH and CSMT has been improved compared with the one in Table 2. Moreover, we also observe that CSMT performs similarly to CSLH . It is also of interest to note that the PSNR for CSMT in Table 3 is higher than the one for CSS\u2212DCS in Table 2 for most of the tested images. This suggests that if the dictionary and the projection matrix are simultaneously optimized by online algorithm with large dataset, the performance of the corresponding CS system can be further improved since joint optimization (CSS\u2212DCS) is expected to have better performance than only optimizing projection matrix with a given dictionary (CSMT ) under the same\n6In order to compare with CSLH , we still compute the SRE matrix E for the training data though it requires abundant of extra storage and computation resources.\nsettings. We note that the proposed framework for projection matrix design can be utilized for online simultaneous optimization of the dictionary and the projection matrix. Investigation along this direction is on-going.\n\u2022 We compare the computational complexity of our proposed method with the one in [15] [16]. The later mainly consists of two more steps: the calculations of the SRE matrix E and EE T . Calculating E involves the OMP algorithm [32] with computational complexity of O ( PKNL(KL logL+K3) ) , where we repeat that\nP, N, L and K denote the number of samples, the dimension of signal, the number of atoms in dictionary and the sparsity level, respectively. The complexity for calculating EE T is O ( PN2 ) . Thus, compared with CSMT , CSLH needs at least more computational time of O ( PNK2L2 logL+PNLK4 +PN2 ) . In the set of next experiments, we will show the advantage of designing the projection matrix on a high dimensional dictionary. With N and L increasing, the efficiency of the proposed method CSMT becomes more distinct.\nExperiment C: large dataset and high dimensional dictionary\nInspired by the work in [26], we attempt to design the projection matrix on a high dimensional dictionary in this set of experiments. The reason to utilize a high dimensional dictionary is as follows. The sparse representation of a natural image X can be written as,\nX = X\u0303 +E X\u0303 , \u03a8\u0398\nwhere E is the sparse representation error.7 We first recover \u0398 by solving a set of (3) and then take X\u0303 = \u03a8\u0398 as the recovered image. It is clear that no matter what projection matrix is utilized, the best we can obtain is X\u0303 instead of X . Thus, with a dictionary which can capture more information of the training dataset and better represent X with X\u0303 , the corresponding CS system is excepted to yield a higher SRA. As stated in [26], training the dictionary with larger patches results in smaller sparse representation errors for natural images. However, training dictionary on larger patches, we have to train on a large-scale dataset to better represent the signals of interest. This demonstrates the efficiency of the proposed method for designing a robust projection matrix on a high dimensional dictionary as this method drops the requirement of the SRE matrix E which is not only in high dimension, but also large-scale.\nThe parameters M, N, L, K and \u03bb are set to 80, 256, 800, 16 and 0.5, respectively. Due to the fact that CSMT has a similar performance with CSLH and the choice of \u03bb for CSLH is very sensitive to E , we omit the performance of CSLH in this experiment. The simulation results are presented in Table 4. In order to demonstrate the visual effect clearly, two images Lena\n7Since OMP is used to conduct the sparse coding mission in this paper, each column of \u0398 is exactly K-sparse.\nand Mandrill are shown in Figs. 4 and 5, respectively. For a clear comparison, We choose the projection matrices and corresponding dictionary which yields the highest average \u03c1psnr from Table 2 to Table 4 in Figs. 4 and 5. Remark 5:\n\u2022 We observe that for the CS systems CSrandn and CSDCS, designing the projection matrix on a high dimensional dictionary results in similar performance to what is shown in Experiments A and B with the same compression rate MN . Moreover, CSDCS has lower \u03c1psnr in Experiments B and C than Experiment A. However, the proposed CS system CSMT has increasing \u03c1psnr from Experiment A to Experiment C. This indicates the effectiveness of the proposed method for a CS system with a higher dimensional dictionary.\n\u2022 We also investigate the influence of the parameters M, L , K to the above mentioned CS systems. The simulation results on Testing Data II are given in Fig.s 6 to 8. As can\nbe observed, the proposed CS system CSMT has highest \u03c1psnr among the three CS systems.\n\u2022 The recent work in [33] states that it is possible to train the dictionary on millions of training signals whose dimension is also more than one million. The proposed method can be utilized to design a robust projection matrix on such high dimensional dictionaries since it gets rid of the requirement of the SRE matrix E . Note that in this case, more efforts for efficiently solving (15) are needed. A full investigation regarding this direction belongs to a future work.\nThree sets of experiments on natural images are conducted to illustrate the effectiveness and efficiency of the proposed framework in Section 3. A dictionary trained on a larger dataset can better represent the signal and the corresponding CS system yields better performance in terms of SRA. Additionally, a high dimensional dictionary has more freedom to represent the signals of interest. The CS system with a high dimensional dictionary and a projection matrix obtained by the proposed\nmethod results in higher \u03c1psnr. However, both cases need to train the dictionary on a large-scale training dataset, making it inefficient or even impossible for computing the SRE matrix E . One of the main contributions in this paper is proposing a new framework that is independent of the SRE matrix E ."}, {"heading": "5. Conclusion", "text": "This paper considers the problem of designing a robust projection matrix for the signals that are not exactly sparse. A novel cost function is proposed to decrease the influence of SRE for the measurements and at the same time is independent of training data and the corresponding SRE matrix (the independence of training data saves computations for practical designing). As shown in Lemma 1, we state that discarding the\nSRE matrix in designing procedure is reasonable as it is equivalent to the case when we have infinite number of training samples. We thus utilize \u2016\u03a6\u20162F as an surrogate to the projected SRE \u2016\u03a6E\u20162F to design the sensing matrices. The performance of designing projection matrices with dictionaries either learned on large-scale training dataset or of high dimension is experimentally examined. The simulation results on synthetic data and natural images demonstrate the effectiveness and efficiency of the proposed approach. It is of interest to note that the proposed method yields better performance when we increase the dimension of the dictionary, which surprisingly is not true for the other methods.\nOur proposed framework for designing robust sensing matrices\u2014which shares similar structure to that in [15] [16]\u2014 simultaneously minimizes the surrogate of sparse representation error (SRE) and the mutual coherence of the CS systems. Thus we need extract effort (like Figure 2) to find an optimal \u03bb that well balances these two terms. An ongoing research is to come up with a new framework without requiring balancing the tradeoff between minimizing the mutual coherence and decreasing the projected SRE."}, {"heading": "Acknowledgment", "text": "This research is supported in part by ERC Grant agreement no. 320649, and in part by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI). The code in this paper to represent the experiments can be downloaded through the link https://github.com/happyhongt/"}], "references": [{"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information,", "author": ["E.J. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Near optimal signal recovery from random projections: Universal encoding strategies,", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Compressed sensing,", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "An introduction to compressive samping,", "author": ["E.J. Cand\u00e8s", "M.B. Wakin"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Sparse and Redundant Representations: from theory to applications in signal and image processing", "author": ["M. Elad"], "venue": "Springer Science & Business Media", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "`1-`2 Optimization in Signal and Image Processing,", "author": ["M. Zibulevsky", "M. Elad"], "venue": "IEEE Signal Process. Mag", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Learning incoherent dictionaries for sparse approximation using iterative projections and rotations,", "author": ["D. Barchiesi", "M.D. Plumbley"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "and A", "author": ["G. Li", "Z. Zhu", "H. Bai"], "venue": "Yu, \u201cA new framework for designing incoherent sparsifying dictionaries,\u201d in IEEE Conf. Acous., Speech, Signal Process.(ICASSP), pp. 4416-4420", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Optimized projections for compressed sensing,", "author": ["M. Elad"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "A gradient-based alternating minimzation approach for optimization of the measurement matrix in compressive sensing,", "author": ["V. Abolghasemi", "S. Ferdowsi", "S. Sanei"], "venue": "Signal Process.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Design of projection matrix for compressive sensing by nonsmooth optimization,", "author": ["W.-S. Lu", "T. Hinamoto"], "venue": "IEEE International Symposium Circuits and Systems (ISCAS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Projection matrix optimization for block-sparse compressive sensing,", "author": ["S. Li", "Z. Zhu", "G. Li", "L. Chang", "Q. Li"], "venue": "IEEE Conf. Signal Process., Communicaton and Computation (ICSPCC),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "On projection matrix optimization for compressive sensing systems,", "author": ["G. Li", "Z.H. Zhu", "D.H. Yang", "L.P. Chang", "H. Bai"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Designint robust sensing matrix for image compression,", "author": ["G. Li", "X. Li", "S. Li", "H. Bai", "Q. Jiang", "X. He"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "An efficient algorithm for designing projection matrix in compressive sensing based on alternating optimization,", "author": ["T. Hong", "H. Bai", "S. Li", "Z. Zhu"], "venue": "Signal Process.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Approximating sampled sinusoids and multiband signals using multiband modulated DPSS dictionaries,", "author": ["Z. Zhu", "M.B. Wakin"], "venue": "J. Fourier Analysis Appl.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Dictionary Learning,", "author": ["I. Tosic", "P. Frossard"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation,", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Hakon-housoy, \u201cMethod of optimal direction for frame design,", "author": ["K. Engan", "J.H.S.O. Aase"], "venue": "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.(ICASSP),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Online algorithms and stochastic approximations,", "author": ["L. Botou"], "venue": "Online Learning and Neural Networks,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "J", "author": ["J. Mairal", "F. Bach"], "venue": "Ponce and G. Sapiro, \u201cOnline dictionary learning for sparse coding,\u201d Proceedings of the 26th annual international conference on machine learning ACM (ICML), pp. 689-696", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding,", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "and M", "author": ["Z. Zhu", "Q. Li", "G. Tang"], "venue": "B. Wakin, \u201cGlobal Optimality in Lowrank Matrix Optimization,\u201d arXiv preprint, arXiv:1702.07945", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "minFunc: unconstrained differentiale multivariate optimization in Matlab.", "author": ["M. Schmidt"], "venue": "https://www.cs.ubc.ca/ \u0303schmidtm/ Software/minFunc.html,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Trainlets: dictionary learning in high dimensions,", "author": ["J. Sulam", "B. Ophir", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Large inpainting of face images with trainlets,", "author": ["J. Sulam", "M. Elad"], "venue": "IEEE Signal Processing Letter,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Learning to sense sparse sig-  nals: simultaneous sensing matrix ans sparsifying dictionary optimization,", "author": ["J.M. Duarte-Carvajalino", "G. Sapiro"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Statistical Inference", "author": ["G. Casella", "L.B. Roger"], "venue": "Vol. 2. Pacific Grove, CA: Duxbury", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "LabelMe: A Database and Web-Based Tool for Image Annotation,", "author": ["B.C. Russell", "A. Torralba", "K.P. Murphy", "W.T. Freeman"], "venue": "International Journal of Computation Vision,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "M", "author": ["R. Rubinstein"], "venue": "Zibulevsky and M. Elad, \u201cEfficient implementation of the K-SVD algorithm and the Batch-OMP method,\u201d Department of Computer Science, Technion, Israel, Tech. Rep.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "B", "author": ["A. Mensch", "J. Mairal"], "venue": "Thirion and G. Varoquaux, \u201cDictionary learning for massive matrix factorization,\u201d Proceedings of the 33th annual international conference on machine learning ACM (ICML)", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix.", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix.", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Since the beginning of this century, Compressive Sensing or Compressed Sensing (CS) has received a great deal of attention [1] - [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "We refer the reader to [5] [6] and the references therein to find the related topics mentioned above.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "All of the methods can be found in [5] [7] and the references therein.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "All of the methods can be found in [5] [7] and the references therein.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "As shown in [5], when there is no projection noise (i.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "Thus, [8] [9] proposed methods to design a dictionary with small mutual coherence.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "Thus, [8] [9] proposed methods to design a dictionary with small mutual coherence.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "There has been much effort [10] - [14] devoted to designing an optimal \u03a6 that outperforms the widely used random matrix in terms of signal recovery accuracy (SRA).", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "There has been much effort [10] - [14] devoted to designing an optimal \u03a6 that outperforms the widely used random matrix in terms of signal recovery accuracy (SRA).", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "It is experimentally observed that the sensing matrix designed by [10] - [14] based on mutual coherence results", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "It is experimentally observed that the sensing matrix designed by [10] - [14] based on mutual coherence results", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "To address this issue, the recent work in [15] [16] proposed novel methods to design a robust projection matrix when the SRE exists.", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "To address this issue, the recent work in [15] [16] proposed novel methods to design a robust projection matrix when the SRE exists.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "1 Through this paper, similar to what is used in [15] [16], a robust projection (or sensing) matrix means it is designed with consideration of possible SRE and hence the corresponding CS system yields superior performance when the SRE e in (2) is not nil.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "1 Through this paper, similar to what is used in [15] [16], a robust projection (or sensing) matrix means it is designed with consideration of possible SRE and hence the corresponding CS system yields superior performance when the SRE e in (2) is not nil.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "However, the approaches in [15] [16] need the explicit value of the SRE on the training dataset, making them inefficient in several aspects.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "However, the approaches in [15] [16] need the explicit value of the SRE on the training dataset, making them inefficient in several aspects.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": ", the wavelet dictionary, and the modulated discrete prolate spheroidal sequences (DPSS) dictionary for sampled multiband signals [17]) actually do not involve any training dataset and hence no SRE available.", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "1We note that the approaches considered in [15] [16] share the same framework.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "1We note that the approaches considered in [15] [16] share the same framework.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "The difference is that in [16] the authors utilized an efficient iterative algorithm giving an approximate solution, while a closed form solution is derived in [15].", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "The difference is that in [16] the authors utilized an efficient iterative algorithm giving an approximate solution, while a closed form solution is derived in [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "In order to design the robust projection matrix for these CS systems using the framework presented in [15] [16], one has to first construct plenty of extra representative dataset for the explicit SRE with the given dictionary, which limits the range of applications.", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "In order to design the robust projection matrix for these CS systems using the framework presented in [15] [16], one has to first construct plenty of extra representative dataset for the explicit SRE with the given dictionary, which limits the range of applications.", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "Therefore, the requirement of the explicit value of SRE for the training dataset makes the methods in [15] [16] limited and inefficient for all the cases discussed above.", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "Therefore, the requirement of the explicit value of SRE for the training dataset makes the methods in [15] [16] limited and inefficient for all the cases discussed above.", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "We stress that by efficient method for robust projection matrix design (which is the title of this paper), we are not providing an efficient method for solving the problems in [15] [16]; instead we provide a new framework in which the training dataset and its corresponding SRE are not required any more.", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "We stress that by efficient method for robust projection matrix design (which is the title of this paper), we are not providing an efficient method for solving the problems in [15] [16]; instead we provide a new framework in which the training dataset and its corresponding SRE are not required any more.", "startOffset": 181, "endOffset": 185}, {"referenceID": 13, "context": "Experiments on synthetic data and real images demonstrate the proposed sensing matrix yields a comparable performance in terms of SRA compared with the ones obtained by [15] [16].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "Experiments on synthetic data and real images demonstrate the proposed sensing matrix yields a comparable performance in terms of SRA compared with the ones obtained by [15] [16].", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": "which can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20].", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "which can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "which can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20].", "startOffset": 197, "endOffset": 201}, {"referenceID": 13, "context": "The recent work in [15] [16] attempted to design a robust projection matrix with consideration of the SRE matrix E and proposed to solve", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "The recent work in [15] [16] attempted to design a robust projection matrix with consideration of the SRE matrix E and proposed to solve", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "See [15] [16] for more discussions on this issue.", "startOffset": 4, "endOffset": 8}, {"referenceID": 14, "context": "See [15] [16] for more discussions on this issue.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "As stated in [15] [16], these methods ((8) and (9)) can be applied naturally when the dictionary is learned by algorithms like K-SVD with plenty of training data {xk}, since the SRE E is available without any additional effort.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "As stated in [15] [16], these methods ((8) and (9)) can be applied naturally when the dictionary is learned by algorithms like K-SVD with plenty of training data {xk}, since the SRE E is available without any additional effort.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "To train such a dictionary, we have to conduct online algorithms [21] - [23] which typically apply stochastic gradient method where in each iteration a randomly selected tiny part of the training signals called mini-batch instead of the whole data is utilized for computing the expected gradient.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "To train such a dictionary, we have to conduct online algorithms [21] - [23] which typically apply stochastic gradient method where in each iteration a randomly selected tiny part of the training signals called mini-batch instead of the whole data is utilized for computing the expected gradient.", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "All of these situations make the approach proposed in [15] [16] become limited.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "All of these situations make the approach proposed in [15] [16] become limited.", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "The recent work in [26] [27] stated that a dictionary learned with larger patches (e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "The recent work in [26] [27] stated that a dictionary learned with larger patches (e.", "startOffset": 24, "endOffset": 28}, {"referenceID": 17, "context": "2The dimension of a dictionary in such a case becomes high compared with the moderate dictionary size shown in [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "Thus, by the law of large numbers [29], the average \u2016\u03a6E\u2016 2 F P converges in probability and almost surely to the expected value \u03c3\u2016\u03a6\u2016F as P\u2192 \u221e.", "startOffset": 34, "endOffset": 38}, {"referenceID": 27, "context": "Finally, the central limit theorem [29] establishes that as P ap-", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "The recent work [24] has shown that a number of iterative algorithms (including gradient descent) can provably solve the factored problem (i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "the Conjugate-Gradient (CG) [30] method is utilized to solve (17).", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "\u2207\u03a6\u0303 f (\u03a6\u0303,G) = 2\u03bb\u03a6\u0303\u22124\u03a6\u0303\u03a8G\u03a8 T +4\u03a6\u0303\u03a8\u03a8T \u03a6\u0303 \u03a6\u0303\u03a8\u03a8T (19) After obtaining the gradient of f (\u03a6\u0303,G), the toolbox minFunc4 [25] is utilized to solve (17) with CG method.", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "3We note that both of the methods shown in [15] [16] for solving (17) need to calculate the inversion of \u03a8\u03a8T .", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "3We note that both of the methods shown in [15] [16] for solving (17) need to calculate the inversion of \u03a8\u03a8T .", "startOffset": 48, "endOffset": 52}, {"referenceID": 22, "context": "Thus, as global convergence of many local search algorithms for solving similar low-rank optimizations is guaranteed in [24], CG is chosen to solve (17).", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "Obviously, if the aforementioned problem does not happen in practical cases, the method in [15] [16] can be used to address (15).", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "Obviously, if the aforementioned problem does not happen in practical cases, the method in [15] [16] can be used to address (15).", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "Moreover, we will show that CG and the methods shown in [15] [16] yield a similar solution in the following experiments.", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "Moreover, we will show that CG and the methods shown in [15] [16] yield a similar solution in the following experiments.", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "\u2022 Simulation results with synthetic data and natural images (where the SRE matrix E is available) show that the proposed method also yields a comparable performance to or outperforms the methods in [15] [16] in terms of SRA.", "startOffset": 198, "endOffset": 202}, {"referenceID": 14, "context": "\u2022 Simulation results with synthetic data and natural images (where the SRE matrix E is available) show that the proposed method also yields a comparable performance to or outperforms the methods in [15] [16] in terms of SRA.", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "For convenience, the corresponding CS systems are denoted by CSMT with \u03a6\u0303 obtained via (15) and CSMT\u2212ET F with \u03a6\u0303 obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH\u2212ET F with the sensing matrix obtained via (9) [15], and CSDCS [28].", "startOffset": 268, "endOffset": 272}, {"referenceID": 13, "context": "For convenience, the corresponding CS systems are denoted by CSMT with \u03a6\u0303 obtained via (15) and CSMT\u2212ET F with \u03a6\u0303 obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH\u2212ET F with the sensing matrix obtained via (9) [15], and CSDCS [28].", "startOffset": 325, "endOffset": 329}, {"referenceID": 26, "context": "For convenience, the corresponding CS systems are denoted by CSMT with \u03a6\u0303 obtained via (15) and CSMT\u2212ET F with \u03a6\u0303 obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH\u2212ET F with the sensing matrix obtained via (9) [15], and CSDCS [28].", "startOffset": 341, "endOffset": 345}, {"referenceID": 26, "context": "It was first proposed in [28] that simultaneously optimizing \u03a6 and \u03a8 for a CS system results in better performance in terms of SRA.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "The SRA is evaluated in terms of the peak signal-to-noise ratio (PSNR) [5]", "startOffset": 71, "endOffset": 74}, {"referenceID": 13, "context": "We note that a random dictionary with well-conditioned is chosen and thus we also compute the closed-form solution shown in [15] for (15).", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "This phenomenon is also observed for CSLH and CSLH\u2212ET F in [15] [16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "This phenomenon is also observed for CSLH and CSLH\u2212ET F in [15] [16].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "\u2022 It is clear that the sensing matrices obtained via (15) and (16) have at least similar performance to the ones obtained via (8) and (9) [15, 16], though our proposed framework does not utilize the SRE matrix E .", "startOffset": 138, "endOffset": 146}, {"referenceID": 14, "context": "\u2022 It is clear that the sensing matrices obtained via (15) and (16) have at least similar performance to the ones obtained via (8) and (9) [15, 16], though our proposed framework does not utilize the SRE matrix E .", "startOffset": 138, "endOffset": 146}, {"referenceID": 29, "context": "Both training and testing datasets used in these three set of experiments are extracted as follows from the LabelMe database [31].", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "We perform the same experiment as in [15] to demonstrate the effectiveness of the proposed CS system CSMT without using the SRE E .", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "Similar to [15], the parameters M, N, L and K are set to 20, 64, 100 and 4, respectively.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "Therefore, it is inefficient to utilize the methods in [15] [16] as they require the SRE matrix E .", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Therefore, it is inefficient to utilize the methods in [15] [16] as they require the SRE matrix E .", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "The online dictionary learning algorithm in [22] [23] is chosen to train the sparsifying dictionary on the whole Training Data I.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "The online dictionary learning algorithm in [22] [23] is chosen to train the sparsifying dictionary on the whole Training Data I.", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "\u2022 We compare the computational complexity of our proposed method with the one in [15] [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "\u2022 We compare the computational complexity of our proposed method with the one in [15] [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 30, "context": "Calculating E involves the OMP algorithm [32] with computational complexity of O ( PKNL(KL logL+K3) ) , where we repeat that P, N, L and K denote the number of samples, the dimension of signal, the number of atoms in dictionary and the sparsity level, respectively.", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "Inspired by the work in [26], we attempt to design the projection matrix on a high dimensional dictionary in this set of experiments.", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "As stated in [26], training the dictionary with larger patches results in smaller sparse representation errors for natural images.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "\u2022 The recent work in [33] states that it is possible to train the dictionary on millions of training signals whose dimension is also more than one million.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "Our proposed framework for designing robust sensing matrices\u2014which shares similar structure to that in [15] [16]\u2014 simultaneously minimizes the surrogate of sparse representation error (SRE) and the mutual coherence of the CS systems.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "Our proposed framework for designing robust sensing matrices\u2014which shares similar structure to that in [15] [16]\u2014 simultaneously minimizes the surrogate of sparse representation error (SRE) and the mutual coherence of the CS systems.", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "Our objective is to efficiently design a robust projection matrix \u03a6 for the Compressive Sensing (CS) systems when applied to the signals that are not exactly sparse. The optimal projection matrix is obtained by mainly minimizing the average coherence of the equivalent dictionary. In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix. Without requiring of training data, we can efficiently design the robust projection matrix and apply it for most of CS systems, like a CS system for image processing with a conventional wavelet dictionary in which the SRE matrix is generally not available. Simulation results demonstrate the efficiency and effectiveness of the proposed approach compared with the state-of-the-art methods. In addition, we experimentally demonstrate with natural images that under similar compression rate, a CS system with a learned dictionary in high dimensions outperforms the one in low dimensions in terms of reconstruction accuracy. This together with the fact that our proposed method can efficiently work in high dimension suggests that a CS system can be potentially implemented beyond the small patches in sparsity-based image processing.", "creator": "LaTeX with hyperref package"}}}