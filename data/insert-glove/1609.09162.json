{"id": "1609.09162", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Universum Learning for Multiclass SVM", "abstract": "dizengoff We confrontational introduce Universum prakasham learning for multiclass aondoakaa problems and propose mdgs a novel formulation kra for multiclass universum civitate SVM (MU - SVM ). weerasak We also slac propose a span mmr bound shibasaki for MU - meager SVM gifts that can box-office be used lactantius for rushall model burek selection abakan thereby avoiding faden resampling. 42-4 Empirical results nzabampema demonstrate snpl the kornilenko effectiveness commissaires of MU - kuleto SVM and statilius the gilaki proposed xehanort bound.", "histories": [["v1", "Thu, 29 Sep 2016 00:43:03 GMT  (728kb,D)", "http://arxiv.org/abs/1609.09162v1", "14 pages, 12 figures"]], "COMMENTS": "14 pages, 12 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sauptik dhar", "naveen ramakrishnan", "vladimir cherkassky", "mohak shah"], "accepted": false, "id": "1609.09162"}, "pdf": {"name": "1609.09162.pdf", "metadata": {"source": "CRF", "title": "Universum Learning for Multiclass SVM", "authors": ["Sauptik Dhar", "Naveen Ramakrishnan", "Vladimir Cherkassky", "Mohak Shah"], "emails": ["mohak.shah}@us.bosch.com", "cherk001@umn.edu"], "sections": [{"heading": "1 Introduction", "text": "Many applications of machine learning involve analysis of sparse high-dimensional data, where the number of input features is larger than the number of data samples. Such high-dimensional data sets present new challenges for most learning problems. Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314]. But, most such studies pertaining to classification problems are limited to binary (\u2018two\u2019- class) classification problems. On the other hand, many practical applications involve discrimination for more than two categories. Typical examples include, speech recognition, object recognition from images, prognostic health management etc [15,16]. In order to incorporate a priori knowledge (in the form of Universum data) for such applications, there is a need to extend Universum learning for multiclass problems.\nIn this paper we mainly focus on formulating the universum learning for multiclass SVM under balanced settings with equal misclassification costs. Support Vector Machines (SVM) have gained enormous popularity in machine learning, statistics and engineering over the last decades and are being used in many real-world applications. Researchers have proposed several methods to solve a multiclass SVM problem. Typically these methods follow two basic approaches (see [9, 17] for more details). The first approach follows an ensemble based setting, where several binary classifiers are combined to construct the multiclass classifier viz., one-vs-one, one-vs-all, directed acyclic graph SVM [18]. Previous works, such as [4, 19] which follow the ensemble based setting, focus on the binary universum learning paradigm and only provide some hints for their extensions to the multiclass problems. An alternative to the ensemble based setting is the direct approach, where the entire multiclass problem is solved through a single larger optimization formulation (see [1, 20, 21]). In this paper we develop and discuss MU-SVM, a direct approach for universum learning following the Crammer & Singer\u2019s (C&S) multiclass SVM formulation [20].\nThe main contributions of this work can be summarized as follows: 1). We propose a new (direct) formulation for universum learning for SVM under the multiclass setting, 2). we show that MU-SVM could be solved efficiently using any standard multiclass SVM solver and, 3). we derive a new leave-one-out bound for MU-SVM which provides a computationally efficient mechanism to perform model selection compared to the classical resampling based approach.\nThe paper is organized as follows. Section 2 describes the widely used multiclass SVM formulation in [20]. Section 3 formalizes the notion of Universum learning for multiclass problems and introduces the new MU-SVM formulation (in section 3.1). A discussion on the computational implementation\nar X\niv :1\n60 9.\n09 16\n2v 1\n[ cs\n.L G\n] 2\n9 Se\np 20\nof the MU-SVM is provided in section 3.2. We derive a new leave-one-out bound for the MU-SVM formulation in section 3.3 , and provide a simple two-step strategy for model selection. Section 4 provides the empirical results in support of the proposed strategy. Finally, conclusions are presented in section 5.\n2 Multiclass SVM\nThis section provides a brief description of the multiclass SVM formulation following Crammer & Singer (C&S) [20]. Given i.i.d training samples (xi, yi)ni=1, with x \u2208 <d and y \u2208 {1, . . . , L} ; where n = number of training samples, d = dimensionality of the input space and L = total number of classes. The task of a multiclass classifier is to estimate a vector valued function f = [f1, . . . , fL] for predicting the class labels for future unseen samples (x, y) using the decision rule y\u0302 = argmax\nl=1,...,L fl(x). The C&S multiclass\nSVM [20] is a widely used formulation which generalizes the concept of large margin classifier for multiclass problems. This multiclass SVM setting employs a special margin-based loss (similar to the hinge loss), L(y, f(x)) = [max\nl (fl(x) + 1 \u2212 \u03b4yl) \u2212 fy(x)]+ where [a]+ = max(0, a) and \u03b4yl = {\n1; y = l 0; y 6= l (see Fig 1). Here, for any sample (x, y = k), hav-\ning L(y, f(x)) = 0 ensures a margin-distance of \u2018+1\u2019 for the correct prediction i.e. fk(x)\u2212 fl(x) \u2265 1;\u2200l 6= k. The SVM multiclass formulation (for linear parameterization) is provided below:\nmin w1...wL,\u03be\n1\n2 \u2211 l \u2016wl\u201622 + C n\u2211 i=1 \u03bei (1)\ns.t. (wyi \u2212wl)>xi \u2265 eil \u2212 \u03bei; eil = 1\u2212 \u03b4il; i = 1 . . . n, l = 1 . . . L here, fl(x) = w>l x and \u03b4il = {\n1; yi = l 0; yi 6= l . Note that training samples falling inside the margin\nborder (\u2018+1\u2019) are linearly penalized using the slack variables \u03bei \u2265 0, i = 1 . . . n (as shown in Fig 1). These slack variables contribute to the empirical risk for the multiclass SVM formulation\nRemp(w) = n\u2211 i=1 \u03bei. The SVM formulation attempts to strike a balance between minimization of the empirical risk and the regularization term. This is controlled through the user-defined parameter C \u2265 0. For most SVM solvers eq. (1) is typically solved in it\u2019s dual form which provides a mechanism to extend the linear SVM to non-linear settings. This is accomplished by introducing a non-linear kernel function K(xi,xj) = \u3008\u03d5(xi) \u00b7 \u03d5(xj)\u3009 that implicitly captures the non-linear mapping of the data x\u2192 \u03d5(x) (see [20] for more details)."}, {"heading": "3 Multiclass Universum SVM", "text": "3.1 Multiclass U-SVM formulation\nThe idea of Universum learning was introduced by Vapnik [1, 2] to incorporate a priori knowledge about admissible data samples. The Universum learning was introduced for binary classification, where in addition to labeled training data we are also given a set of unlabeled examples from the Universum. The Universum contains data that belongs to the same application domain as the training data. However, these samples are known not to belong to either class. In fact, this idea can also be extended to multiclass problems. For multiclass problems in addition to the labeled training data we are also given a set\nof unlabeled examples from the Universum. However, now the Universum samples are known not to belong to any of the classes in the training data. For example, if the goal of learning is to discriminate\nbetween handwritten digits 0, 1, 2,...,9; one can introduce additional \u2018knowledge\u2019 in the form of handwritten letters A, B, C, ... ,Z. These examples from the Universum contain certain information about handwriting styles, but they cannot be assigned to any of the classes (1 to 9). Also note that, Universum samples do not have the same distribution as labeled training samples. These unlabeled Universum samples are introduced into the learning as contradictions and hence should lie close to the decision boundaries for all the classes f = [f1, . . . , fL]. This argument follows from [2, 22], where the universum samples lying close to the decision boundaries are more likely to falsify the classifier. To ensure this, we incorporate a \u2206 - insensitive loss function for the universum samples (shown in Fig 2). This \u2206 - insensitive loss forces the universum samples to lie close to the decision boundaries (\u20180\u2019 in Fig. 2). Note that, this idea of using a \u2206 - insensitive loss for Universum samples has been previously introduced in [22] for binary classification. However, different from [22], here the \u2206 - insensitive loss is introduced for the decision functions of all the classes i.e. f = [f1, . . . , fL]. This reasoning motivates the new multiclass Universum-SVM (MU-SVM) formulation where:\n\u2013 Standard hinge loss is used for the training samples (shown in Fig. 1). This loss forces the training samples to lie outside the \u2018+1\u2019 margin border.\n\u2013 The universum samples are penalized by a \u2206 - insensitive loss (see Fig. 2) for the decision functions of all the classes f = [f1, . . . , fL].\nThis leads to the following MU-SVM formulation. Given training samples T := (xi, yi)ni=1, where yi \u2208 {1, . . . , L} and additional unlabeled universum samples U := (x\u2217j )mj=1. Solve 1,\nmin w1...wL,\u03be,\u03b6\n1\n2 \u2211 l \u2016wl\u201622 + C n\u2211 i=1 \u03bei + C \u2217 m\u2211 j=1 \u03b6j (2)\ns.t. (wyi \u2212wl)>xi \u2265 eil \u2212 \u03bei; eil = 1\u2212 \u03b4il, i = 1 . . . n |(wk \u2212wl)>x\u2217j | \u2264 \u2206 + \u03b6j ; j = 1 . . .m, l, k = 1 . . . L\nHere, the universum samples that lie outside the \u2206 - insensitive zone are linearly penalized using the slack variables \u03b6j \u2265 0, j = 1 . . .m. The user-defined parameters C,C\u2217 \u2265 0 control the trade-off between the margin size, the error on training samples, and the contradictions (samples lying outside \u00b1\u2206 zone) on the universum samples. Note that for C\u2217 = 0 this formulation becomes equivalent to the multiclass SVM classifier."}, {"heading": "3.2 Computational Implementation of MU-SVM", "text": "In this section we discuss the current implementation of the MU-SVM formulation in (2). Following [22], for each universum sample (x\u2217) we create artificial samples belonging to all the classes, i.e. (x\u2217j , y \u2217 j = 1), . . . , (x \u2217 j , y \u2217 j = L). For simplicity we overload the variables as shown below:\nxi = { xi i = 1 . . . n (training samples) x\u2217j i = n+ 1 . . . n+mL; j = 1 . . .mL (universum samples)\nyi = { yi i = 1 . . . n y\u2217j i = n+ 1 . . . n+mL; j = 1 . . .mL\neil = { eil i = 1 . . . n; l = 1 . . . L \u2212\u2206(1\u2212 \u03b4jl) i = n+ 1 . . . n+mL; j = 1 . . .mL; l = 1 . . . L (3)\nCi = { C i = 1 . . . n C\u2217 i = n+ 1 . . . n+mL; j = 1 . . .mL\n\u03bei = { \u03bei i = 1 . . . n \u03b6j i = n+ 1 . . . n+mL; j = 1 . . .mL\nThen (2) can be re-written as,\nmin w1...wL,\u03be\n1\n2 \u2211 l \u2016wl\u201622 + n+mL\u2211 i=1 Ci \u03bei (4)\ns.t. (wyi \u2212wl)>xi \u2265 eil \u2212 \u03bei i = 1 . . . n+mL, l = 1 . . . L 1Throughout this paper, we use index i for training samples, j for universum samples and k, l for the class\nlabels.\nThe formulation (4) has the same form as (1) except that the former has additional mL constraints for the universum samples. Like most other SVM solvers, the MU-SVM formulation in (4) is also solved in its dual form (shown in Algorithm 1). Hence, the computational complexity is same as solving a multiclass SVM formulation (in (1)) with n+mL samples. Most off-the-shelf multiclass SVM solvers can be used for solving the proposed MU-SVM. For completeness, we show the steps for the proposed MU-SVM solver in Algorithm 1:\nAlgorithm 1: MU-SVM (dual form) 1. Given training (xi, yi)ni=1 and universum samples (x \u2217 j ) m j=1 perform the transformation in (3) ; 2. Solve (5) to obtain the MU-SVM solution,\nmax \u03b1\nW (\u03b1) = \u22121 2 \u2211 i,j \u2211 l \u03b1il\u03b1jlK(xi,xj) \u2212 \u2211 i,l \u03b1ileil (5)\ns.t. \u2211 l \u03b1il = 0; \u03b1i,l \u2264 Ci if l = yi ; \u03b1i,l \u2264 0 if l 6= yi\n3. Obtain the class label using the following decision rule: y\u0302 = argmax l \u2211 i \u03b1ilK(xi,x)"}, {"heading": "3.3 Model Selection", "text": "As presented in (5), the current MU-SVM algorithm has four tunable parameters: C,C\u2217, kernel parameter, and \u2206. So in practice, multiclass SVM may yield better results than MUSVM, simply because it has an inherently simpler model selection. A successful practical application of the proposed MU-SVM heavily depends on the optimal tuning of the model parameters. This paper proposes to adopt a simplified strategy (previously used in [5, 23]) for model selection which mainly involves two steps,\nStep a. First, perform optimal tuning of the C and kernel parameters for multiclass SVM classifier. This step equivalently performs model selection for the parameters specific only to the training samples in the MU-SVM formulation (2).\nStep b. Second, tune the parameter \u2206 while keeping C and kernel parameters fixed (as selected in Step a). Parameter C\u2217/C = nmL is kept fixed throughout this paper to have equal weightage on the loss due to training and universum samples.\nThe model parameters in Steps (a) & (b) are typically selected through resampling techniques or using a separate validation set. In this paper however, we provide a new analytic bound for the leave-one-out error (l.o.o) for MU-SVM formulation. Note that, by removing the universum samples, we obtain the l.o.o bound for the multiclass SVM formulation. Now, the model parameters in Steps (a) & (b) are selected to minimize this leave-one-out (l.o.o) error bound. A detailed discussion regarding this new l.o.o error bound is provided next.\nNote that, the l.o.o formulation with the tth training sample dropped is the same as in (5) with an additional constraint \u03b1tl = 0; \u2200l. Then, the l.o.o error is given as: Rl.o.o = 1n n\u2211 t=1 1[yt 6= y\u0302t], where y\u0302t = arg max l \u2211 i \u03b1tilK(xi,xt) is the predicted class label for the t th sample and \u03b1t = [\u03b1t11, . . . , \u03b1 t 1L\ufe38 \ufe37\ufe37 \ufe38\n\u03b1t1\n, . . . , \u03b1tt1 = 0, . . . , \u03b1 t tL = 0\ufe38 \ufe37\ufe37 \ufe38\n\u03b1tt=0\n, . . .] is the l.o.o solution. In this paper we follow a very\nsimilar strategy as used in [24], and derive the new l.o.o bound for the MU-SVM formulation in (5). The necessary prerequisites are presented next. Definition 1. (Support vector categories)\nType 1. A support vector obtained from eq. (5) is called a Type 1 support vector if 0 < \u03b1iyi < Ci. This is represented as, SV1 = { i |0 < \u03b1iyi < Ci}\nType 2. A support vector obtained from eq. (5) is called a Type 2 support vector if \u03b1iyi = Ci. This is represented as, SV2 = { i |\u03b1iyi = Ci}\nThe set of all support vectors are represented as, SV = SV1 \u222a SV2. Similarly, the set of support vectors for l.o.o solution is given as SV t. Under definition (1) we make the following assumptions.\nAssumption 1. The set of support vectors of the Type1 and Type2 categories remain the same during the leave-one-out procedure.\nThis is a well-established assumption which has been previously used to derive the l.o.o bound for binary SVM in [24]. The advantage of this assumption is that it reduces the computational complexity of the l.o.o bound (see Corollary (2)). However, in this paper we make an additional assumption as given below.\nAssumption 2. The dual variables of the Type1 support vectors have only two active elements i.e. \u2200\u03b1i s.t. {0 < \u03b1iyi < Ci} \u2203 k 6= yi s.t. \u03b1ik = \u2212\u03b1iyi .\nThis assumption provides the advantage of analyzing the bound for the multiclass problem in a one-vs-one (binary) fashion. We observe that, under high-dimensional low sample size settings this assumption holds true for almost all Type 1 support vectors. A more detailed analysis shall be provided in a longer version of this paper. Next we provide the main result used for the leave-one-out error bound.\nTheorem 1. Under Assumptions 1& 2 the following equality holds for the Type 1 support vectors \u2200\u03b1t s.t. {0 < \u03b1tyt < Ct},\nS2t = [\u03b1 > t \u2211 i\u2208SV \u2211 l \u03b1ilK(xi,xt)\u2212 \u03b1tytg>k \u2211 i\u2208SV t \u2211 l \u03b1tilK(xi,xt)] (6)\nwhere, S2t = {min \u03b2 \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj)| \u03b2t = \u03b1t; \u2211 l \u03b2il = 0 ; (i, j) \u2208 SV1} and\ngk = [0, . . . 1 lth=yt , . . . ,\u22121 kth , . . . , 0]. Proof See supplementary material. Note that, this equality is very similar to the result in [24] obtained for binary SVM. Same as [24] we refer to St as the (constrained) span of the Type 1 support vectors. However, for practical cases the computation of St can be simplified following the corollary (1).\nCorollary 1. The span S2t can be efficiently computed as S2t = \u03b1 > t [(H \u22121)tt] \u22121\u03b1t (7)\nhere, H := [ KSV1 \u2297 IL A>\nA 0\n] ; A := I|SV1| \u2297 (1L)\n>; 1L = [ 1 1 . . . 1\ufe38 \ufe37\ufe37 \ufe38 L elements ]\n(H\u22121)tt := sub-matrix of H\u22121 for index i = (t\u2212 1)L+ 1, . . . , tL KSV1 := Kernel matrix of Type 1 support vectors.\nwhere, \u2297 is the Kronecker product. Proof See supplementary material. Now rather than solving a quadratic program (as shown in Theorem (1)), the computation of St mainly involves computing the inverse of the H - matrix. This is an O(n + mL)3 operation, and provides a computational advantage over computing the l.o.o error, which is O(n+mL)4. Finally, we use the results in Theorem (1) and Corollary (1) to obtain the following,\nCorollary 2. Under the Assumptions 1 & 2 the leave-one-out error is upper bounded as:\nRl.o.o \u2264 1\nn [ Card{ t | \u03b1>t [(H\u22121)tt]\u22121\u03b1t \u2265 \u03b1>t \u2211 i\u2208SV \u2211 l \u03b1ilK(xi,xt) ; t \u2208 SV1 \u2229 T } (8)\n+ Card{ t | t \u2208 SV2 \u2229 T }]; where T := Training Set\nProof See supplementary material. For the rest of the paper we use the eq. (8) for model selection in Steps a & b and select the parameters which minimizes the right hand side of the bound in eq. (8).\n4 Empirical Results\n(a) Training samples\n(b) Universum samples\nFigure 3: GTSRB data.\nOur empirical results mainly use two real life datasets: German Traffic Sign Recognition Benchmark (GTSRB) dataset [25] : The goal here is to identify the traffic signs \u201830\u2019,\u201870\u2019 and \u201880\u2019 (shown in Fig.3a). Here, the sample images are represented by their histogram of gradient (HOG) features (following [5, 8]). Further, in addition to the training samples we are also provided with additional universum samples i.e. traffic signs for noentry\u2019 and \u2018roadworks\u2019(shown in Fig.3b). Note that these universum samples belong to the same application domain i.e. they are traffic sign images. However, they do not belong to any of the training classes. Analysis using the other types of Universum have been omitted due to space constraints.\nReal-life ABCDETC dataset [22]: This is a handwritten digit recognition dataset, where in addition to the digits \u20180-9\u2019 we are also provided with the images of the uppercase, lowercase handwritten letters and some additional special symbols. In this paper, the goal is to identify the handwritten digits \u20180\u2019 - \u20183\u2019 based on their pixel values. Further, we use the images of the handwritten \u2018letters a\u2019 and \u2018i\u2019 as universum samples for illustration.\n(a) Training samples\n(b) Universum samples\nFigure 4: ABCDETC dataset.\nThe experimental settings used for these datasets throughout the paper is provided in Table 1. For the GTSRB dataset we have performed number of experiments with varying universum set sizes and provide the optimal set size in Table 1. Further increase in the number of universum samples did not provide significant performance gains (see supplementary material for more details)."}, {"heading": "4.1 Comparison between Multiclass SVM vs. U-SVM", "text": "Our first set of experiment uses the GTSRB dataset. Initial experiments suggest that linear parameterization is optimal for this dataset; hence only linear kernel has been used. Here, the model selection is done over the range of parameters, C = [10\u22124, . . . , 103] , C\u2217/C = nmL = 0.2 and \u2206 = [0, 0.01, 0.05, 0.1] using stratified 5-Fold cross validation [26]. Performance comparisons between SVM and U-SVM for the different types of Universum: signs \u2018no-entry\u2019, and \u2018roadworks\u2019\nare shown in Table 2. The table shows the average Test Error = 1nT nT\u2211 i=1 1[ytesti 6= y\u0302testi ] over 10 random training/test partitioning of the data in similar proportions as shown in Table. 1. Here ytesti \u223c class label for ith test sample, y\u0302testi \u223c predicted label for ith test sample and nT = number of test samples.\nAs seen from Table 2, the MU-SVM models using both types of Universa provides better generalization than the multiclass SVM model. Here, for all the methods we have training error \u223c 0%. For better understanding of the MU-SVM modeling results we adopt the technique of \u2018histogram of projections\u2019 originally introduced for binary classification [23, 27]. However, different from binary classification, here we project a training sample (x, y = k) onto the decision space for that class i.e. w>k x\u2212max\nl 6=k w>l x = 0 and the universum samples onto the decision spaces of all the classes. Finally,\nwe generate the histograms of the projection values for our analysis. In addition to the histograms, we also generate the frequency plot of the predicted labels for the universum samples. Figs 5 and 6 shows the typical histograms and frequency plots for the SVM and MU-SVM models using the \u2018no-entry\u2019 sign (as universum). As seen from Fig. 5, the optimal SVM model has high separability for the training samples i.e., most of the training samples lie outside the margin borders with training\nerror \u223c 0. Infact, similar to binary SVM [27], we see data-piling effects for the training samples near the \u2018+1\u2019 - margin borders of the decision functions for all the classes. This is typically seen under high-dimensional low sample size settings. However, the universum samples (sign \u2018no-entry\u2019) are widely spread about the margin-borders. Moreover, for this case the universum samples are biased towards the positive side of the decision boundary of the sign \u201830\u2019 (see Fig 5(a)) and hence predominantly gets classified as sign \u201830\u2019(see Fig.5 (d)). As seen from Figs 6 (a)-(c), applying the MU-SVM model preserves the separability of the training samples and additionally reduces the spread of the universum samples. For such a model the uncertainity due to universum samples is uniform across all the classes i.e. signs \u201830\u2019,\u201870\u2019 and \u201880\u2019 (see Fig. 6(d)). The resulting MU-SVM model has higher contradiction on the universum samples and provides better generalization in comparison to SVM. The histograms for the multiclass SVM and MU-SVM models using the sign \u2018roadworks\u2019 as universa are provided in supplementary material.\nOur next experiment uses the ABCDETC dataset. For this dataset, using an RBF kernel of the form K(xi,xj) = exp(\u2212\u03b3\u2016xi \u2212 xj\u20162) with \u03b3 = 2\u22127 provided optimal results for SVM. The model selection is done over the range of parameters, C = [10\u22124, . . . , 103], C\u2217/C = 0.6 and \u2206 = [0, 0.01, 0.05, 0.1] using stratified 5-Fold cross validation. Performance comparisons between multiclass SVM and MU-SVM for the different types of Universum: letters \u2018a\u2019, and \u2018i\u2019 are shown in Table 2. In this case, MU-SVM using letter \u2018i\u2019 provides an improvement over the multiclass SVM solution. However, using letter \u2018a\u2019 as universum does not provide any improvement over the SVM solution. For better understanding we analyze the histogram of projections and the frequency plots for the multiclass SVM/MU-SVM models using the letter \u2018a\u2019 as universum in Figs. 7,8. As seen in Fig. 7 (a)-(d)) the SVM model already results in a narrow distribution of the universum samples and in turn provides near random prediction on the universum samples (Fig. 7(e)). Applying MU-SVM for this case provides no significant change to multiclass SVM solution and hence no additional improvement in generalization (see Table 2 and Fig. 8). Finally, the histograms for the multiclass SVM/MU-SVM models using letters \u2018i\u2019 as universum display similar properties as in Figs 5 & 6 (please refer to supplementary material).\nThe results in this section shows that MU-SVM provides better performance than multiclass SVM, typically for high-dimensional low sample size settings. Under such settings the training data exhibits large data-piling effects near the margin border (\u2018+1\u2019). For such ill-posed settings, introducing the Universum can provide improved generalization over the multiclass SVM solution. However, the\neffectiveness of the MU-SVM also depends on the properties of the universum data. Such statistical characteristics of the training and universum samples for the effectiveness of MU-SVM can be conveniently captured using the \u2018histogram-of-projections\u2019 method introduced in this paper.\n4.2 Effectiveness of the Model Selection using Bound in (8)\nNext we provide results showing the practical utility of the bound in (8) for model selection. Here, we provide the performance results of the MU-SVM model when the model parameters are selected using (8). That is, we select the model parameters which provides the smallest value for the bound (8). Table 3 shows the average test error over 10 random training/test partitioning of the data in similar proportions as shown in Table. 1. As seen from Table 3, the MU-SVM models selected using (8) provides comparable results to the standard 5-Fold resampling technique. This provides a practical alternative for model selection for the MU-SVM algorithm. 2 The proposed model selection strategy using (8) involves an O(n + mL)3 operation, and provides a computational edge over standard resampling techniques. For example, the average time complexity over 10 experiments (GTSRB with Universum:\u2019no-entry\u2019) shows, MUSVM(CV) \u223c 4413s vs. MUSVM(bound) \u223c 1583s. Detailed time complexity analysis and results on the LOO bound shall be provided in a longer version of this paper."}, {"heading": "5 Conclusions", "text": "We introduced universum learning for multiclass problems and provided a new universum-based formulation for multiclass SVM (MU-SVM). This formulation reduces to the classical multiclass SVM formulation in the absence of universum samples and can utilize standard SVM solvers. We also proposed a novel span bound for the MU-SVM that can be used to perform efficient model selection. We empirically demonstrated the effectiveness of the proposed formulation as well as the bound on real-world datasets. In addition, we also provided insights into the underlying behavior of universum learning and its dependence on the choice of universum samples using the proposed \u2018histogram-of-projections\u2019 method.\n2Modeling results using l.o.o strategy was prohibitively slow, and hence could not be reported in this paper."}, {"heading": "A Proofs", "text": "The references cited in this document follows the numbering used in the main paper."}, {"heading": "A.1 Proof of Theorem 1", "text": "The proof follows similar lines as in [21]. As previously discussed in Section 3.3, the leave-one-out formulation for U-SVM with the tth sample dropped is,\nmax \u03b1\nW (\u03b1) = \u22121 2 \u2211 i,j \u2211 l \u03b1il\u03b1jlK(xi,xj) \u2212 \u2211 i,l \u03b1ileil (9)\ns.t. \u2211 l \u03b1il = 0; \u03b1il \u2264 Ci if l = yi ; \u03b1il \u2264 0 if l 6= yi\n\u03b1tl = 0; \u2200l (additional constraint)\nThen, the leave-one-out (l.o.o) error is given as: Rl.o.o = 1n n\u2211 t=1 1[yt 6= y\u0302t] where, \u03b1t = [\u03b1t11, . . . , \u03b1 t 1L\ufe38 \ufe37\ufe37 \ufe38\n\u03b1t1\n, . . . , \u03b1tt1 = 0, . . . , \u03b1 t tL = 0\ufe38 \ufe37\ufe37 \ufe38\n\u03b1tt=0\n, . . .] is the solution for (9) and y\u0302t = arg max l \u2211 i \u03b1tilK(xi,xt) (es-\ntimated class label for the tth sample). The overall proof for the bound on the l.o.o error follows three major steps.\nFirst, we construct a feasible solution for (5) using the optimal leave-one-out solution \u03b1t. i.e., construct \u03b1t + \u03b3 as shown below,\n\u03b1til + \u03b3il \u2264 Ci; \u2200 (i, l) \u2208 {(i, l)| \u03b1til < Ci; l = yi} := At1 \u03b1til + \u03b3il \u2264 0; \u2200 (i, l) \u2208 {(i, l)| \u03b1til < 0; l 6= yi} := At2\u2211 l \u03b3il = 0;\nand,\n\u03b3il = 0 \u2200(i, l) /\u2208 SV t1 [with SV t1 = At1 \u222aAt2 = { i |0 < \u03b1tiyi < Ci}] (10) \u03b3tl = \u03b1tl \u2200l (underAssumption2)\nsuch that, it is a feasible solution for (5). Now,\nI1 =W (\u03b1 t + \u03b3)\u2212W (\u03b1t)\n= \u22121 2 \u2211 i,j \u2211 l (\u03b1til + \u03b3il)(\u03b1 t jl + \u03b3jl)K(xi,xj)\u2212 \u2211 i \u2211 l (\u03b1til + \u03b3il)eil\n+ 1\n2 \u2211 i,j \u2211 l \u03b1til\u03b1 t jlK(xi,xj) + \u2211 i \u2211 l \u03b1tileil\n= \u22121 2 \u2211 i,j ( \u2211 l \u03b3il\u03b3jl)K(xi,xj)\u2212 \u2211 i,j ( \u2211 l \u03b3il\u03b1 t jl)K(xi,xj)\u2212 \u2211 i \u2211 l \u03b3ileil\n= \u22121 2 \u2211 i,j ( \u2211 l \u03b3il\u03b3jl)K(xi,xj)\u2212 \u2211 i,l \u03b3il[ \u2211 j \u03b1tjlK(xi,xj) + eil]\n= \u22121 2 \u2211 i,j ( \u2211 l \u03b3il\u03b3jl)K(xi,xj)\u2212 \u2211 l \u03b1tl( \u2211 j \u03b1tjlK(xj ,xt)) + \u03b1tyt (11)\nThe last equality follows from assumption 2 and the construction of \u03b3 in (10).\nAs the second step, we construct a feasible solution for the leave-one-out formulation (9) using the optimal solution for (5). i.e., construct \u03b1\u2212 \u03b2 as shown below, \u03b1il \u2212 \u03b2il \u2264 Ci; \u2200 (i, l) \u2208 {(i, l)| \u03b1il < Ci; l = yi} := A1 \u03b1il \u2212 \u03b2il \u2264 0; \u2200 (i, l) \u2208 {(i, l)| \u03b1il < 0; l 6= yi} := A2\u2211 l \u03b2il = 0;\nand,\n\u03b2il = 0 \u2200(i, l) /\u2208 SV1 \u2212 {t} [with SV1 = A1 \u222aA2 = {i |0 < \u03b1iyi < Ci}] (12) \u03b2tl = \u03b1tl \u2200l (underAssumption2)\nsuch that, it is a feasible solution for (9). As before, define\nI2 =W (\u03b1)\u2212W (\u03b1\u2212 \u03b2)\n= \u22121 2 \u2211 i,j \u2211 k \u03b1il\u03b1jlK(xi,xj)\u2212 \u2211 i \u2211 l \u03b1ileil\n+ 1\n2 \u2211 i,j \u2211 l (\u03b1il \u2212 \u03b2il)(\u03b1jl \u2212 \u03b2jl)K(xi,xj) + \u2211 i \u2211 l (\u03b1il \u2212 \u03b2il)eil\n= 1\n2 \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj)\u2212 \u2211 i,j ( \u2211 l \u03b2il\u03b1jl)K(xi,xj)\u2212 \u2211 i \u2211 l \u03b2ileil\n= 1\n2 \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj)\u2212 \u2211 i,l \u03b2il[ \u2211 j \u03b1jlK(xi,xj) + eil]\n= 1\n2 \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj)\u2212 \u2211 l \u03b1tl( \u2211 j \u03b1jlK(xj ,xt)) + \u03b1tyt (13)\nThe last equality follows from assumption 2 and the construction of \u03b2 in (12). Moreover, from assumption 1, \u03b2 = \u03b1\u2212\u03b1t = \u03b3 satisfies the constraints in (9) and (11). Hence for such a \u03b2,\u03b3:\nI1 = I2 =W (\u03b1)\u2212W (\u03b1t) (14) \u21d2 \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj) = \u2211 l \u03b1tl( \u2211 j \u03b1jlK(xj ,xt))\u2212 \u2211 l \u03b1tl( \u2211 j \u03b1tjlK(xj ,xt))\n= \u2211 l \u03b1tl( \u2211 j \u03b1jlK(xj ,xt))\u2212 (\u03b1tlgk)>( \u2211 j \u03b1tjlK(xj ,xt))\nThe last equality follows from assumption 2, where gk = [0, . . . 1 lth , . . . ,\u22121 kth , . . . , 0] for \u03b1t = [0, . . . \u03b1tl lth=yt , . . . ,\u2212\u03b1tl kth , . . . , 0] (i.e. only two active elements for the support vector). As the third and final step define,\nS2t = min \u03b2 \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj) (15)\ns.t. \u03b1il \u2212 \u03b2il \u2264 Ci; (i, l) \u2208 A1 \u2212 {t} \u03b1il \u2212 \u03b2il \u2264 0; (i, l) \u2208 A2 \u2212 {t} \u03b2il = 0; \u2200(i, l) /\u2208 SV1 \u2212 {t} \u03b2tl = \u03b1tl; \u2200l\u2211 l \u03b2il = 0\nand let \u03b2\u2032 be the minimizer for (15). Then,\nW (\u03b1t) \u2265W (\u03b1\u2212 \u03b2\u2032) [From (9)] \u21d2W (\u03b1)\u2212W (\u03b1t) \u2264W (\u03b1)\u2212W (\u03b1\u2212 \u03b2\u2032)\n\u21d2 \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj) \u2264 S2t\nFrom Assumption 1, (\u03b1 \u2212 \u03b1t) is a feasible solution for (15) which gives : S2t \u2264 \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj).\nCombining the above inequality, S2t = \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj). Moreover, under Assumption 1 the inequality constraints in (15) are not activated. Hence, S2t = {min \u03b2 \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj)| \u03b2t = \u03b1t; \u2211 l \u03b2il = 0 ; (i, j) \u2208 SV1}. Proved."}, {"heading": "A.2 Proof of Corollary 1", "text": "The Span is defined as:\nS2t = min \u03b2 \u2211 i,j ( \u2211 l \u03b2il\u03b2jl)K(xi,xj) (16)\ns.t. \u03b2tl = \u03b1tl ; \u2200l = 1, . . . , L\u2211 l \u03b2il = 0 ; \u2200(i, j) \u2208 SV1\n= min \u03b2 \u2211 l (\u03b1tl\u03b1tl)K(xt,xt) + 2 \u2211 i 6=t \u2211 l \u03b1tl\u03b2ilK(xt,xi) + \u2211 (i,j)6=t ( \u2211 l \u03b2il\u03b2jl)K(xi,xj)\ns.t. (I|SV1\u2212{t}| \u2297 1L)\ufe38 \ufe37\ufe37 \ufe38 A \u03b2 = 0\n= min \u03b2 max \u00b5 \u03b1>t [K(xt,xt)\u2297 IL]\u03b1t + 2 \u2211 i6=t \u2211 l \u03b1tl\u03b2ilK(xt,xi) + \u2211 (i,j)6=t ( \u2211 l \u03b2il\u03b2jl)K(xi,xj)\n+ 2\u00b5>A\u03b2 (\u00b5 := Lagrange Multiplier)\n= \u03b1>t [K(xt,xt)\u2297 IL]\u03b1t + min \u03b2 max \u00b5 2\u03b1>t (H (\u2212t) t ) >\u03bb+ \u03bbH(\u2212t)\u03bb\ufe38 \ufe37\ufe37 \ufe38 L(\u03bb)\n(with \u03bb = [\u03b2;\u00b5])\nwhere, I|SV1\u2212{t}| := Identity Matrix of size |SV1 \u2212 {t}|, H(\u2212t) := (t\u2212 1)L+ 1, . . . , tL rows/columns of matrix H (in (7)) removed; and H\n(\u2212t) t := (t\u2212 1)L+ 1, . . . , tL columns of H.\nFurther, at saddle point : 5\u03bbL(\u03bb) = 0 \u21d2 \u03bb\u2217 = \u2212[H(\u2212t)]\u22121H(\u2212t)t \u03b1t. Hence,\nS2t = \u03b1 > t [(K(xt,xt)\u2297 IL)\u2212 (H (\u2212t) t ) >(H(\u2212t))\u22121H (\u2212t) t ]\u03b1t\n= \u03b1>t (H \u22121)tt\u03b1t (17)\nwhere, (H\u22121)tt := sub-matrix of H\u22121 for index i = (t\u2212 1)L+ 1, . . . , tL. Proved."}, {"heading": "A.3 Proof of Corollary 2", "text": "The proof has three steps and mainly depends on the contribution of a sample to the leave-one-out error.\n\u2013 First, for a sample (xt, yt) which is not a support vector, i.e. t /\u2208 SV and t \u2208 T (Training set); it lies outside margin borders. Dropping such a sample does not change the original solution (5). Hence, it does not contribute to an error.\n\u2013 Secondly, for a sample (xt, yt) with t \u2208 SV1 and t \u2208 T (Training set) Theorem 1 holds. For a leave-one-out error, (\u03b1tlgk)> \u2211 i\u2208SV t \u2211 l \u03b1tilK(xi,xt) \u2264 0 \u21d2 \u03b1>t [(H\u22121)tt]\u22121\u03b1t \u2265\n\u03b1>t \u2211 i\u2208SV \u2211 l \u03b1ilK(xi,xt). [From (14)]\n\u2013 Finally, for a sample (xt, yt) with t \u2208 SV2 and t \u2208 T (Training set) we add to the leave-one-out error.\nProved."}, {"heading": "B Additional Results", "text": ""}, {"heading": "B.1 Histogram of projections", "text": ""}, {"heading": "B.1.1 GTSRB dataset", "text": "Figs 9 and 10 provide the histograms and the frequency plots for SVM/MU-SVM models for GTSRB dataset using sign \u2018roadworks\u2019 (as universum). As seen from Fig. 9, the optimal SVM model has high separability for the training samples. Here, the universum samples are biased towards the positive side of the decision boundary\nof the sign \u201880\u2019 (see Fig 9(c)) and hence predominantly gets classified as sign \u201880\u2019(see Fig.9 (d)). As seen from Figs 10 (a)-(c), applying the MU-SVM model preserves the separability of the training samples and additionally reduces the spread of the universum samples. For such a model the uncertainity due to universum samples is uniform across all the classes i.e. signs \u201830\u2019,\u201870\u2019 and \u201880\u2019 (see Fig. 10(d)). The resulting MU-SVM model has higher contradiction on the universum samples and provides better generalization in comparison to SVM (see Table. 2)."}, {"heading": "B.1.2 ABCDETC dataset", "text": "Here we present the histograms and the frequency plots for SVM/MU-SVM models for ABCDETC dataset using letter \u2018i\u2019 (as universum). Here, the SVM model results in a wide distribution of the universum samples (see Fig. 11 (a)-(d)) and predicts majority of the universum samples as digit \u20191\u2019. Applying MU-SVM results in a narrower distribution of the universum samples (see Fig 12 (a)-(d)) and hence a more random prediction on the universum samples (see Fig 12). This results in a more generalizable model in comparison to SVM (see Table 2)\nB.2 Experiments with varying Universum size\nThis set of experiment demonstrates how the generalization performance of MU-SVM is affected by the number of Universum data samples for the GTSRB dataset. Here we use the same setting as provided in Table 1, except we vary the number of universum samples as shown in Table 4. Table 4 shows the performance comparison\nbetween multiclass SVM vs. MU-SVM, suggesting that, for both types of Universa, prediction performance of MU-SVM improves with the number universum samples. However,increasing the number of universum samples above certain value (\u223c 500) does not provide additional improvement."}], "references": [{"title": "Estimation of Dependences Based on Empirical Data (Information Science and Statistics)", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "An analysis of inference with the universum", "author": ["F. Sinz", "O. Chapelle", "A. Agarwal", "B. Sch\u00f6lkopf"], "venue": "Advances in neural information processing systems 20. NY, USA: Curran, Sep. 2008, pp. 1369\u20131376.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Selecting informative universum sample for semi-supervised learning.", "author": ["S. Chen", "C. Zhang"], "venue": "in IJCAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Development and evaluation of cost-sensitive universum-svm", "author": ["S. Dhar", "V. Cherkassky"], "venue": "Cybernetics, IEEE Transactions on, vol. 45, no. 4, pp. 806\u2013818, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Weighted twin support vector machine with universum", "author": ["S. Lu", "L. Tong"], "venue": "Advances in Computer Science: an International Journal, vol. 3, no. 2, pp. 17\u201323, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "A nonparallel support vector machine for a classification problem with universum learning", "author": ["Z. Qi", "Y. Tian", "Y. Shi"], "venue": "Journal of Computational and Applied Mathematics, vol. 263, pp. 288\u2013298, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Uboost: Boosting with the universum", "author": ["C. Shen", "P. Wang", "F. Shen", "H. Wang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 34, no. 4, pp. 825\u2013832, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-view learning with universum", "author": ["Z. Wang", "Y. Zhu", "W. Liu", "Z. Chen", "D. Gao"], "venue": "Knowledge-Based Systems, vol. 70, pp. 376\u2013391, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised classification with universum.", "author": ["D. Zhang", "J. Wang", "F. Wang", "C. Zhang"], "venue": "in SDM. SIAM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Least squares twin support vector machine with universum data for classification", "author": ["Y. Xu", "M. Chen", "G. Li"], "venue": "International Journal of Systems Science, pp. 1\u20139, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "\u03bd-twin support vector machine with universum data for classification", "author": ["Y. Xu", "M. Chen", "Z. Yang", "G. Li"], "venue": "Applied Intelligence, vol. 44, no. 4, pp. 956\u2013968, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved multi-kernel classification machine with nystr\u00f6m approximation technique and universum data", "author": ["C. Zhu"], "venue": "Neurocomputing, vol. 175, pp. 610\u2013634, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Double-fold localized multiple matrix learning machine with universum", "author": ["\u2014\u2014"], "venue": "Pattern Analysis and Applications, pp. 1\u201328.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 0}, {"title": "Learning from Data: Concepts, Theory, and Methods", "author": ["V. Cherkassky", "F.M. Mulier"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "The Elements of Statistical Learning, ser. Springer Series in Statistics", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "A comparison of methods for multiclass support vector machines", "author": ["C. Hsu", "C. Lin"], "venue": "Neural Networks, IEEE Transactions on, vol. 13, no. 2, pp. 415\u2013425, 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Large margin dags for multiclass classification.", "author": ["J.C. Platt", "N. Cristianini", "J. Shawe-Taylor"], "venue": "in NIPS, vol", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "A priori knowledge from non-examples", "author": ["F. Sinz"], "venue": "Ph.D. dissertation, Mar 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "On the learnability and design of output codes for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "Machine learning, vol. 47, no. 2-3, pp. 201\u2013233, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-class support vector machines", "author": ["J. Weston", "C. Watkins"], "venue": "Citeseer, Tech. Rep., 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Inference with the universum", "author": ["J. Weston", "R. Collobert", "F. Sinz", "L. Bottou", "V. Vapnik"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 1009\u20131016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Practical conditions for effectiveness of the universum learning", "author": ["V. Cherkassky", "S. Dhar", "W. Dai"], "venue": "Neural Networks, IEEE Transactions on, vol. 22, no. 8, pp. 1241\u20131255, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Bounds on error expectation for support vector machines", "author": ["V. Vapnik", "O. Chapelle"], "venue": "Neural computation, vol. 12, no. 9, pp. 2013\u20132036, 2000.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition", "author": ["J. Stallkamp", "M. Schlipsing", "J. Salmen", "C. Igel"], "venue": "Neural Networks, pp. \u2013, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluating learning algorithms: a classification perspective", "author": ["N. Japkowicz", "M. Shah"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Abstract We introduce Universum learning [1], [2] for multiclass problems and propose a novel formulation for multiclass universum SVM (MU-SVM).", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 2, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 3, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 4, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 5, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 6, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 7, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 8, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 9, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 10, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 11, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 12, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 13, "context": "Typical examples include, speech recognition, object recognition from images, prognostic health management etc [15,16].", "startOffset": 111, "endOffset": 118}, {"referenceID": 14, "context": "Typical examples include, speech recognition, object recognition from images, prognostic health management etc [15,16].", "startOffset": 111, "endOffset": 118}, {"referenceID": 7, "context": "Typically these methods follow two basic approaches (see [9, 17] for more details).", "startOffset": 57, "endOffset": 64}, {"referenceID": 15, "context": "Typically these methods follow two basic approaches (see [9, 17] for more details).", "startOffset": 57, "endOffset": 64}, {"referenceID": 16, "context": ", one-vs-one, one-vs-all, directed acyclic graph SVM [18].", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "Previous works, such as [4, 19] which follow the ensemble based setting, focus on the binary universum learning paradigm and only provide some hints for their extensions to the multiclass problems.", "startOffset": 24, "endOffset": 31}, {"referenceID": 17, "context": "Previous works, such as [4, 19] which follow the ensemble based setting, focus on the binary universum learning paradigm and only provide some hints for their extensions to the multiclass problems.", "startOffset": 24, "endOffset": 31}, {"referenceID": 18, "context": "An alternative to the ensemble based setting is the direct approach, where the entire multiclass problem is solved through a single larger optimization formulation (see [1, 20, 21]).", "startOffset": 169, "endOffset": 180}, {"referenceID": 19, "context": "An alternative to the ensemble based setting is the direct approach, where the entire multiclass problem is solved through a single larger optimization formulation (see [1, 20, 21]).", "startOffset": 169, "endOffset": 180}, {"referenceID": 18, "context": "In this paper we develop and discuss MU-SVM, a direct approach for universum learning following the Crammer & Singer\u2019s (C&S) multiclass SVM formulation [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 18, "context": "Section 2 describes the widely used multiclass SVM formulation in [20].", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "This section provides a brief description of the multiclass SVM formulation following Crammer & Singer (C&S) [20].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "The C&S multiclass SVM [20] is a widely used formulation which generalizes the concept of large margin classifier for multiclass problems.", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "This is accomplished by introducing a non-linear kernel function K(xi,xj) = \u3008\u03c6(xi) \u00b7 \u03c6(xj)\u3009 that implicitly captures the non-linear mapping of the data x\u2192 \u03c6(x) (see [20] for more details).", "startOffset": 165, "endOffset": 169}, {"referenceID": 0, "context": "The idea of Universum learning was introduced by Vapnik [1, 2] to incorporate a priori knowledge about admissible data samples.", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "This argument follows from [2, 22], where the universum samples lying close to the decision boundaries are more likely to falsify the classifier.", "startOffset": 27, "endOffset": 34}, {"referenceID": 20, "context": "This argument follows from [2, 22], where the universum samples lying close to the decision boundaries are more likely to falsify the classifier.", "startOffset": 27, "endOffset": 34}, {"referenceID": 20, "context": "Note that, this idea of using a \u2206 - insensitive loss for Universum samples has been previously introduced in [22] for binary classification.", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "However, different from [22], here the \u2206 - insensitive loss is introduced for the decision functions of all the classes i.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "Following [22], for each universum sample (x\u2217) we create artificial samples belonging to all the classes, i.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "This paper proposes to adopt a simplified strategy (previously used in [5, 23]) for model selection which mainly involves two steps, Step a.", "startOffset": 71, "endOffset": 78}, {"referenceID": 21, "context": "This paper proposes to adopt a simplified strategy (previously used in [5, 23]) for model selection which mainly involves two steps, Step a.", "startOffset": 71, "endOffset": 78}, {"referenceID": 22, "context": "In this paper we follow a very similar strategy as used in [24], and derive the new l.", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "o bound for binary SVM in [24].", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "Note that, this equality is very similar to the result in [24] obtained for binary SVM.", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "Same as [24] we refer to St as the (constrained) span of the Type 1 support vectors.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "Our empirical results mainly use two real life datasets: German Traffic Sign Recognition Benchmark (GTSRB) dataset [25] : The goal here is to identify the traffic signs \u201830\u2019,\u201870\u2019 and \u201880\u2019 (shown in Fig.", "startOffset": 115, "endOffset": 119}, {"referenceID": 3, "context": "Here, the sample images are represented by their histogram of gradient (HOG) features (following [5, 8]).", "startOffset": 97, "endOffset": 103}, {"referenceID": 6, "context": "Here, the sample images are represented by their histogram of gradient (HOG) features (following [5, 8]).", "startOffset": 97, "endOffset": 103}, {"referenceID": 20, "context": "Real-life ABCDETC dataset [22]: This is a handwritten digit recognition dataset, where in addition to the digits \u20180-9\u2019 we are also provided with the images of the uppercase, lowercase handwritten letters and some additional special symbols.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "1] using stratified 5-Fold cross validation [26].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "For better understanding of the MU-SVM modeling results we adopt the technique of \u2018histogram of projections\u2019 originally introduced for binary classification [23, 27].", "startOffset": 157, "endOffset": 165}, {"referenceID": 0, "context": "[2] V.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Z.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Z.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] \u2014\u2014, \u201cDouble-fold localized multiple matrix learning machine with universum,\u201d Pattern Analysis and Applications, pp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "1 Proof of Theorem 1 The proof follows similar lines as in [21].", "startOffset": 59, "endOffset": 63}], "year": 2016, "abstractText": "We introduce Universum learning [1], [2] for multiclass problems and propose a novel formulation for multiclass universum SVM (MU-SVM). We also propose a span bound for MU-SVM that can be used for model selection thereby avoiding resampling. Empirical results demonstrate the effectiveness of MU-SVM and the proposed bound.", "creator": "LaTeX with hyperref package"}}}