{"id": "1610.00388", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Learning to Translate in Real-time with Neural Machine Translation", "abstract": "juridical Translating spaso in real - time, therapsids a. 2.655 k. a. simultaneous moallim translation, nutrisystem outputs post-punk translation budoff words anlayst before the economou input v\u01b0\u01a1ng sentence ends, marble which .511 is mauzas/mahallas a metazoan challenging stabbing problem mydoom.a for fr\u00e6na conventional machine moldy translation 46.61 methods. 5-0-1 We slurm propose gabai a neural altmann machine raho translation (NMT) framework for simultaneous cryolife translation birkin in which armadas an 45-49 agent clambering learns to sitout make decisions on when revan to serm translate from the woomble interaction 1325 with falbo a towe pre - trained NMT givebacks environment. To trade loci off 74.1 quality v-twin and strassmann delay, d'elegance we french-bred extensively explore various targets for grihs delay 80-meter and design a 43-23 method for beam - communicants search 36.44 applicable in awcc the charal simultaneous 123-run MT setting. roslyn Experiments against frenzel state - r\u00edmac of - the - hlp art diamond-like baselines on 2,466 two foregone language hadep pairs strasshof demonstrate the shihz efficacy leoncio of the proposed framework both quantitatively and qualitatively.", "histories": [["v1", "Mon, 3 Oct 2016 02:11:03 GMT  (1464kb,D)", "http://arxiv.org/abs/1610.00388v1", "9 pages, 8 figures"], ["v2", "Thu, 6 Oct 2016 00:46:39 GMT  (1467kb,D)", "http://arxiv.org/abs/1610.00388v2", "10 pages, 8 figures (with additional related works)"], ["v3", "Tue, 10 Jan 2017 21:07:56 GMT  (2974kb,D)", "http://arxiv.org/abs/1610.00388v3", "10 pages, camera ready"]], "COMMENTS": "9 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jiatao gu", "graham neubig", "kyunghyun cho", "victor o k li"], "accepted": false, "id": "1610.00388"}, "pdf": {"name": "1610.00388.pdf", "metadata": {"source": "CRF", "title": "Learning to Translate in Real-time with Neural Machine Translation", "authors": ["Jiatao Gu", "Graham Neubig", "Kyunghyun Cho", "Victor O.K. Li"], "emails": ["vli}@eee.hku.hk", "gneubig@cs.cmu.edu", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Simultaneous translation, the task of translating content in real-time as it is produced, is an important tool for real-time understanding of spoken lectures or conversations (Fu\u0308gen et al., 2007; Bangalore et al., 2012). Different from the typical machine translation (MT) task, in which translation quality is paramount, simultaneous translation requires balancing the trade-off between translation quality and time delay to ensure that users receive translated content in an expeditious manner (Mieno et al., 2015). A number of methods have been proposed to solve this problem, mostly in the context of phrase-based machine translation. These methods are based on a segmenter, which receives the input one word at a time, then decides when to send it to a MT system that translates each\n1Code and data to replicate our experiments will be released upon the publication.\nLas t nig ht we ser ved Mr X a bee r , who die d dur ing the nig ht . < e os >\n< eos > .\nist storben\nge-Nacht der\nLaufe im\nder ,\nserviert Bier ein X\nHerrn wir\nhaben Abend\nGestern READ WRITE\nFigure 1: Example output from the proposed framework in DE \u2192 EN simultaneous translation. The heat-map represents the soft alignment between the incoming source sentence (left, up-to-down) and the emitted translation (top, leftto-right). The length of each column represents the number of source words being waited for before emitting the translation. Best viewed when zoomed digitally.\nsegment independently (Oda et al., 2014) or with a minimal amount of language model context (Bangalore et al., 2012).\nIndependently of simultaneous translation, accuracy of standard MT systems has greatly improved with the introduction of neural-networkbased MT systems (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014). Very recently, there have been a few efforts to apply NMT to simultaneous translation either through heuristic modifications to the decoding process (Cho and Esipova, 2016), or through the training of an independent segmentation network that chooses when to perform output using a standard NMT model (Satija and Pineau, 2016). However, the former model lacks a capability to learn the appropriate timing with which to perform translation, and the latter model uses a standard NMT model as-is, lacking a holistic design of the modeling and learning within the simultaneous MT context. In addition, neither model has demonstrated gains over previous segmentation-based baselines, leaving questions of their relative merit unresolved.\nIn this paper, we propose a unified design for\nar X\niv :1\n61 0.\n00 38\n8v 1\n[ cs\n.C L\n] 3\nO ct\n2 01\n6\nlearning to perform neural simultaneous machine translation. The proposed framework is based on formulating translation as an interleaved sequence of two actions: READ and WRITE. Based on this, we devise a model connecting the NMT system and these READ/WRITE decisions. An example of how translation is performed in this framework is shown in Fig. 1, and detailed definitions of the problem and proposed framework are described in \u00a72 and \u00a73. To learn which actions to take when, we propose a reinforcement-learning-based strategy with a reward function that considers both quality and delay (\u00a74). We also develop a beam-search method that performs search within the translation segments (\u00a75).\nWe evaluate the proposed method on EnglishRussian (EN-RU) and English-German (EN-DE) translation in both directions (\u00a77). The quantitative results show strong improvements compared to both the NMT-based algorithm and a conventional segmentation methods. We also extensively analyze the effectiveness of the learning algorithm and the influence of the trade-off in the optimization criterion, by varying a target delay. Finally, qualitative visualization is utilized to discuss the potential and limitations of the framework."}, {"heading": "2 Problem Definition", "text": "Suppose we have a buffer of input words X = {x1, ..., xTs} to be translated in real-time. We define the simultaneous translation task as sequentially making two interleaved decisions: READ or WRITE. More precisely, the translator READs a source word x\u03b7 from the input buffer in chronological order as translation context, or WRITEs a translated word y\u03c4 onto the output buffer, resulting in output sentence Y = {y1, ..., yTt}, and action sequence A = {a1, ..., aT } consists of Ts READs and Tt WRITEs, so T = Ts + Tt.\nSimilar to standard MT, we have a measure Q(Y ) to evaluate the translation quality, such as BLEU score (Papineni et al., 2002). For simultaneous translation we are also concerned with the fact that each action incurs a time delay D(A). D(A) will mainly be influenced by delay caused by READ, as this entails waiting for a human speaker to continue speaking, while WRITE consists of generating a few words from a machine translation system, which is possible on the order of milliseconds. Thus, our objective is finding an optimal policy that generates decision sequences with a good trade-off between higher\nquality Q(Y ) and lower delay D(A). We elaborate on exactly how to define this trade-off in \u00a74.2.\nIn the following sections, we first describe how to connect the READ/WRITE actions with the NMT system (\u00a73), and how to optimize the system to improve simultaneous MT results (\u00a74)."}, {"heading": "3 Simultaneous Translation with Neural Machine Translation", "text": "The proposed framework is shown in Fig. 2, and can be naturally decomposed into two parts: environment (\u00a73.1) and agent (\u00a73.2)."}, {"heading": "3.1 Environment", "text": "Encoder: READ The first element of the NMT system is the encoder, which converts input words X = {x1, ..., xTs} into context vectors H = {h1, ..., hTs}. Standard NMT uses bi-directional RNNs as encoders (Bahdanau et al., 2014), but this is not suitable for simultaneous processing as using a reverse-order encoder requires knowing the final word of the sentence before beginning processing. Thus, we utilize a simple left-to-right unidirectional RNN as our encoder:\nh\u03b7 = \u03c6UNI-ENC (h\u03b7\u22121, x\u03b7) (1)\nDecoder: WRITE Similar with standard MT, we use an attention-based decoder. In contrast, we only reference the words that have been read from the input when generating each target word:\nc\u03b7\u03c4 = \u03c6ATT (z\u03c4\u22121, y\u03c4\u22121, H \u03b7) z\u03b7\u03c4 = \u03c6DEC (z\u03c4\u22121, y\u03c4\u22121, c \u03b7 \u03c4 )\np (y|y<\u03c4 , H\u03b7) \u221d exp [\u03c6OUT (z\u03b7\u03c4 )] , (2)\nwhere for \u03c4 , z\u03c4\u22121 and y\u03c4\u22121 represent the previous decoder state and output word, respectively. H\u03b7 is used to represent the incomplete input states, where H\u03b7 is a prefix of H . As the WRITE action calculates the probability of the next word on the fly, we need greedy decoding for each step:\ny\u03b7\u03c4 = arg maxy p (y|y<\u03c4 , H\u03b7) (3)\nNote that y\u03b7\u03c4 , z \u03b7 \u03c4 corresponds to H\u03b7 and is the candidate for y\u03c4 , z\u03c4 . The agent described in the next section decides whether to take this candidate or wait for better predictions."}, {"heading": "3.2 Agent", "text": "A trainable agent is designed to make decisions A = {a1, .., aT }, at \u2208 A sequentially based on observations O = {o1, ..., oT }, ot \u2208 O, and then control the translation environment properly.\nObservation As shown in Fig 2, we concatenate the current context vector c\u03b7\u03c4 , the current decoder state z\u03b7\u03c4 and the embedding vector of the candidate word y\u03b7\u03c4 as the continuous observation, o\u03c4+\u03b7 = [c\u03b7\u03c4 ; z \u03b7 \u03c4 ;E(y \u03b7 \u03c4 )] to represent the current state of the environment.\nAction Similarly to prior work (Grissom II et al., 2014), we define the following set of actions:\n\u2022 READ: the agent rejects the candidate and waits to encode the next word from input buffer;\n\u2022 WRITE: the agent accepts the candidate and emits it as the prediction into output buffer;\nPolicy How the agent chooses the actions based on the observation defines the policy. In our setting, we utilize a stochastic policy \u03c0\u03b8 parameterized by a recurrent neural network, that is:\nst = f\u03b8 (st\u22121, ot) \u03c0\u03b8(at|a<t, o\u2264t) \u221d g\u03b8 (st) , (4)\nwhere st is the internal state of the agent, and is updated recurrently yielding the distribution of the action at. Based on the policy of our agent, the overall algorithm of greedy decoding is shown in Algorithm 1. The algorithm outputs the translation result and a sequence of observation-action pairs."}, {"heading": "4 Optimization", "text": "The proposed framework can be trained using reinforcement learning. More precisely, we use policy gradient algorithm together with variance reduction and regularization techniques.\nAlgorithm 1 Simultaneous Greedy Decoding Require: NMT system \u03c6, policy \u03c0\u03b8, \u03c4MAX, input\nbuffer X , output buffer Y , state buffer S. 1: Init x1 \u21d0 X,h1 \u2190 \u03c6ENC (x1) , H1 \u2190 {h1} 2: z0 \u2190 \u03c6INIT ( H1 ) , y0 \u2190 \u3008s\u3009 3: \u03c4 \u2190 0, \u03b7 \u2190 1 4: while \u03c4 < \u03c4MAX do 5: t\u2190 \u03c4 + \u03b7 6: y\u03b7\u03c4 , z \u03b7 \u03c4 , ot \u2190 \u03c6 (z\u03c4\u22121, y\u03c4\u22121, H\u03b7) 7: at \u223c \u03c0\u03b8 (at; a<t, o<t) , S \u21d0 (ot, at) 8: if at = READ and x\u03b7 6= \u3008/s\u3009 then 9: x\u03b7+1 \u21d0 X,h\u03b7+1 \u2190 \u03c6ENC (h\u03b7, x\u03b7+1)\n10: H\u03b7+1 \u2190 H\u03b7 \u222a {h\u03b7+1}, \u03b7 \u2190 \u03b7 + 1 11: if |Y | = 0 then z0 \u2190 \u03c6INIT (H\u03b7) 12: else if at = WRITE then 13: z\u03c4 \u2190 z\u03b7\u03c4 , y\u03c4 \u2190 y\u03b7\u03c4 14: Y \u21d0 y\u03c4 , \u03c4 \u2190 \u03c4 + 1 15: if y\u03c4 = \u3008/s\u3009 then break"}, {"heading": "4.1 Pre-training", "text": "We need an NMT environment for the agent to explore and use to generate translations. Here, we simply pre-train the NMT encoder-decoder on full sentence pairs with maximum likelihood, and assume the pre-trained model is still able to generate reasonable translations even on incomplete source sentences. Although this is likely sub-optimal, our NMT environment based on uni-directional RNNs can treat incomplete source sentences in a manner similar to shorter source sentences and has the potential to translate them more-or-less correctly."}, {"heading": "4.2 Reward Function", "text": "The policy is learned in order to increase a reward for the translation. At each step the agent will receive a reward signal rt based on (ot, at). To evaluate a good simultaneous machine translation, a reward must consider both quality and delay.\nQuality We evaluate the translation quality using automated metrics such as BLEU (Papineni et al., 2002). The BLEU score is defined as the weighted geometric average of the modified ngram precision BLEU0, multiplied by the brevity penalty BP to punish a short translation.\nBLEU(Y, Y \u2217) = BP \u00b7 BLEU0(Y, Y \u2217), (5) where Y \u2217 is the reference and Y is the output. We decompose BLEU and use the difference of partial BLEU scores as the reward, that is:\nrQt =\n{ \u2206BLEU0(Y, Y \u2217, t) t < T\nBLEU(Y, Y \u2217) t = T (6)\nwhere Y t is the cumulative output at t (Y 0 = \u2205), and \u2206BLEU0(Y, Y \u2217, t) = BLEU0(Y t, Y \u2217) \u2212 BLEU0(Y t\u22121, Y \u2217). Obviously, if at = READ, no new words are written into Y , yielding rQt = 0. Note that we do not multiply BP until the end of the sentence, as it would heavily penalize partial translation results.\nDelay As another critical feature, delay judges how much time is wasted waiting for the translation. Ideally we would directly measure the actual time delay incurred by waiting for the next word. For simplicity, however, we suppose it consumes the same amount of time listening for one more word. We define two measurements, global and local, respectively:\n\u2022 Average Proportion (AP): following the definition in (Cho and Esipova, 2016), we use s(\u03c4) to denote the number of source words that has been waited for when decoding word y\u03c4 ,\n0 < d (X,Y ) = 1 |X||Y | \u2211 \u03c4 s(\u03c4) \u2264 1\ndt = { 0 t < T d(X,Y ) t = T\n(7)\nd is a global delay metric, which defines the average waiting proportion of the source sentence when translating each word.\n\u2022 Consecutive Wait length (CW): in speech translation, listeners are also concerned with long silences during which no translation occurs. To capture this, we also consider on how many words were waited for (READ) consecutively between translating two words. For each action, where we initially define c0 = 0,\nct = { ct\u22121 + 1 at = READ 0 at = WRITE\n(8)\n\u2022 Target Delay: We further define \u201ctarget delay\u201d for both d and c as d\u2217 and c\u2217, respectively, as different simultaneous translation applications may have different requirements on delay. In our implementation, the reward function for delay is written as:\nrDt = \u03b1\u00b7[sgn(ct \u2212 c\u2217) + 1]+\u03b2 \u00b7bdt\u2212d\u2217c+ (9)\nwhere \u03b1 \u2264 0, \u03b2 \u2264 0.\nTrade-off between quality and delay A good simultaneous translation system requires balancing the trade-off of translation quality and time delay. Obviously, achieving the best translation quality and the shortest translation delays are in a sense contradictory. In this paper, the trade-off is achieved by balancing the rewards rt = r Q t +r D t provided to the system, that is, by adjusting the coefficients \u03b1, \u03b2 and the target delay d\u2217, c\u2217 in Eq. 9."}, {"heading": "4.3 Reinforcement Learning", "text": "Policy Gradient We freeze the pre-trained parameters of an NMT model, and train the agent using the policy gradient (Williams, 1992). The policy gradient maximizes the following expected cumulative future rewards,\nJ = E\u03c0\u03b8 [ T\u2211 t=1 Rt ] . (10)\nIts gradient is then,\n\u2207\u03b8J = E\u03c0\u03b8 [ T\u2211 t\u2032=1 \u2207\u03b8 log \u03c0\u03b8(at\u2032 |\u00b7) T\u2211 t=t\u2032 Rt ] (11)\nRt = \u2211T\nk=t [ rQk + r D k ] is the cumulative future\nrewards for current observation and action. In practice, Eq. 11 is estimated by sampling multiple action trajectories from the current policy \u03c0\u03b8, collecting the corresponding rewards.\nVariance Reduction Directly using the policy gradient suffers from high variance, which makes learning unstable and inefficient. We thus employ the variance reduction techniques suggested by Mnih and Gregor (2014). We subtract from Rt the output of a baseline network b\u03d5 to obtain R\u0302t = Rt \u2212 b\u03d5 (ot), and centered re-scale the reward as R\u0303t = R\u0302t\u2212b\u221a\u03c32+ with a running average b and standard deviation \u03c3. The baseline network is trained to minimize the squared loss as follows:\nL\u03c6 = E\u03c0\u03b8 [ T\u2211 t=1 \u2016Rt \u2212 b\u03d5 (ot) \u20162 ]\n(12)\nRegularization We also regularize the negative entropy of the policy distribution H (\u03c0\u03b8) to help the exploration.\nThe overall learning algorithm is summarized in Algorithm 2. For efficiency, instead of updating with stochastic gradient descent (SGD) on a single sentence, both the agent and the baseline are optimized using the Adam (Kingma and Ba, 2014) on a minibatch of multiple sentences.\nAlgorithm 2 Learning with Policy Gradient Require: NMT system \u03c6, agent \u03b8, baseline \u03d5\n1: Pretrain the NMT system \u03c6 using MLE; 2: Initialize the agent \u03b8; 3: while stopping criterion fails do 4: Obtain a translation pairs: {(X,Y \u2217)}; 5: for (Y, S) \u223c Simultaneous Decoding do 6: for (ot, at) in S do 7: Compute the quality: rQt ; 8: Compute the delay: rDt ; 9: Compute the baseline: b\u03d5 (ot);\n10: Collect the future rewards: {Rt}; 11: Perform variance reduction: {R\u0303t}; 12: Update: \u03b8 \u2190 \u03b8 + \u03bb1\u2207\u03b8 [J \u2212 \u03baH(\u03c0\u03b8)] 13: Update: \u03d5\u2190 \u03d5\u2212 \u03bb2\u2207\u03d5L"}, {"heading": "5 Simultaneous Beam Search", "text": "In previous sections we described a simultaneous greedy decoding algorithm. In standard NMT it has been shown that beam search, where the decoder keeps a beam of k translation trajectories, greatly improves translation quality (Sutskever et al., 2014). As shown in Fig. 3 (A), the final output is determined by choosing the best translation from the whole beams.\nIt is not trivial to directly apply beam-search into simultaneous machine translation, as beam search waits until the last source word to write down translation. We can perform a simultaneous beam-search when the agent chooses to consecutively WRITE: keep multiple beams of translation trajectories in temporary buffer and output the best path when the agent switches to READ. As shown in Fig. 3 (B) & (C), it tries to search for a relatively better path while keeping the delay unchanged.\nNote that we do not re-train the agent for simultaneous beam-search. At each step we simply in-\nput the observation of the current best trajectory into the agent for making next decision."}, {"heading": "6 Related Work", "text": "Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (Fu\u0308gen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013). In this approach, the incoming speech stream required to be translated are first recognized and segmented based on an automatic speech recognition (ASR) system. The translation model then works independently based on each of these segments, potentially limiting the quality of translation. To avoid using a fixed segmentation algorithm, Oda et al. (2014) introduced a trainable segmentation component into their system, so that the segmentation leads to better translation quality. Grissom II et al. (2014) proposed a similar framework, however, based on reinforcement learning. All these methods still rely on translating each segment independently, without considering previous conte.\nRecently, two research groups have tried to apply the NMT framework to the simultaneous translation task. Cho and Esipova (2016) proposed a similar waiting process in NMT. However, their waiting criterion is manually defined without learning. Satija and Pineau (2016) proposed a method similar to ours in overall concept, but it significantly differs from our proposed method in many details. The biggest difference is that they proposed to use an agent that passively reads a new word at each step. Because of this, it cannot consecutively decode multiple steps, rendering beam search difficult. In addition, they lack the comparison to any existing approaches. On the other hand, we perform an extensive experimental evaluation against state-of-the-art baselines, demonstrating the relative utility both quantitatively and qualitatively."}, {"heading": "7 Experiments", "text": ""}, {"heading": "7.1 Preliminary Work", "text": "Dataset To extensively study the proposed simultaneous translation model, we train and evaluate it on two different language pairs: \u201cEnglishGerman (EN-DE)\u201d and \u201cEnglish-Russian (ENRU)\u201d in both directions per pair. We use the par-\nallel corpora available from WMT\u2019152 for both pre-training the NMT environment and learning the policy. We utilize newstest-2013 as the validation set to evaluate the proposed algorithm. Both the training set and the validation set are tokenized and segmented into sub-word units with byte-pair encoding (BPE) (Sennrich et al., 2015). We only use sentence pairs where both sides are less than 50 BPE subword symbols long for training.\nEnvironment & Agent Settings We pre-trained the NMT environments for both language pairs and both directions following the same setting from (Cho and Esipova, 2016). We further built our agents, using a recurrent policy with 512 GRUs and a softmax function to produce the action distribution. All our agents are trained using policy gradient using Adam (Kingma and Ba, 2014) optimizer, with a mini-batch size of 10. For each sentence pair in a batch, 5 trajectories are sampled. For testing, instead of sampling we pick the action with higher probability each step.\nBaselines We compare the proposed methods against previously proposed baselines. For fair comparison, we use the same NMT environment:\n\u2022 Wait-Until-End (WUE): an agent that starts to WRITE only when the last source word is seen. In general, we expect this to achieve the best quality of translation. We perform both greedy decoding and beam-search with this method.\n\u2022 Wait-One-Step (WOS): an agent that WRITEs after each READs. Such a policy is problematic when the source and target language pairs have different word orders or lengths (e.g. EN-DE).\n\u2022 Wait-If-Worse/Wait-If-Diff (WIW/WID): as proposed by Cho and Esipova (2016), the algorithm first pre-READs the next source word, and accepts this READ when the probability of 2http://www.statmt.org/wmt15/\nthe most likely target word decreases (WIW), or the most likely target word changes (WID).\n\u2022 Segmentation-based (SEG) (Oda et al., 2014): a state-of-the-art segmentation-based algorithm based on optimizing segmentation to achieve the highest quality score. In this paper, we tried the simple greedy method (SEG1) and the greedy method with POS Constraint (SEG2)."}, {"heading": "7.2 Quantitative Analysis", "text": "In order to evaluate the effectiveness of our reinforcement learning algorithms with different reward functions, we vary the target delay d\u2217 \u2208 {0.3, 0.5, 0.7} and c\u2217 \u2208 {2, 5, 8} for Eq. 9 separately, and trained agents with \u03b1 and \u03b2 adjusted to values that provided stable learning for each language pair according to the validation set.\nLearning Curves As shown in Fig. 4, we plot learning progress for EN-RU translation with different target settings. It clearly shows that our algorithm effectively increases translation quality for all the models, while pushing the delay close, if not all of the way, to the target value. It can also be noted from Fig. 4 (a) and (b) that there exists strong correlation between the two delay measures, implying the agent can learn to decrease both AP and CW simultaneously. Quality v.s. Delay As shown in Fig. 5, it is clear that the trade-off between translation quality and delay has very similar behaviors across both language pairs and directions. The smaller delay (AP or CW) the learning algorithm is targeting, the lower quality (BLEU score) the output translation. It is also interesting to observe that, it is more difficult for \u201c\u2192EN\u201d translation to achieve a lower AP target while maintaining good quality, compared to \u201cEN\u2192\u201d. In addition, the models that are optimized on AP tend to perform better than those optimized on CW, especially in \u201c\u2192EN\u201d translation. German and Russian sentences tend to be\nlonger than English, hence require more consecutive waits before being able to emit the next English symbol.\nv.s. Baselines In Fig. 5 and 6, the points closer to the upper left corner achieve better trade-off performance. Compared to WUE and WOS which can ideally achieve the best quality (but the worst delay) and the best delay (but poor quality) respectively, all of our proposed models find a good balance between quality and delay.\nCompared to the method of Cho and Esipova (2016) based on two hand-crafted rules (WID, WIW), in most cases our proposed models consistently find better trade-off points, while there are a few exceptions. We also observe that the baseline models have trouble controlling the delay in a reasonable area. In contrast, by optimizing towards a given target delay, our proposed model is stable while maintaining good translation quality.\nWe also compared against Oda et al. (2014)\u2019s state-of-the-art segmentation algorithm (SEG). As shown in Fig 6, it is clear that although SEG can work with variant segmentation lengths (CW), the proposed model outputs high quality translations\nat a much smaller CW. We conjecture that this is due to the independence assumption in SEG, while the RNNs and attention mechanism in our model makes it possible to look at the whole history to decide each translated word.\nw/o Beam-Search We also plot the results of simultaneous beam-search instead of using greedy decoding. It is clear from Fig. 5 and 6 that most of the proposed models can achieve an visible increase on quality together with a slight increase on delay. It is because beam-search can help to avoid bad local minima. In addition, we also observe that the simultaneous beam-search cannot bring as much improvement as it did in the standard NMT setting. In most cases, the smaller delay the model achieves, the less beam search can help as it requires longer consecutive WRITE segments to search for a relatively higher translation quality. One possible solution is to consider the beam uncertainty in the agent\u2019s READ/WRITE decisions. We leave this to future work."}, {"heading": "7.3 Qualitative Analysis", "text": "In this section, we perform a more in-depth analysis using examples from both EN-RU and EN-DE pairs, in order to have a deeper understanding of the proposed algorithm and its remaining limitations. We only perform greedy decoding to simplify visualization.\nEN\u2192RU Results are shown in Fig 8. Since English and Russian are the both Subject-VerbObject (SVO) languages, the corresponding words may share the same order in both languages, which makes simultaneous translation easier. It is clear that the larger the target delay (AP or CW) is set, the more words are read before translating the corresponding words, which in turn results in better\nThe cos t of thecam pai\ngn\nisbas ica\nlly bei ng pai d by mysal ary as a sen -- ato r . < e os >\n< eos > .\ndeckt ge-ator\nSen-als alt\nGeh-mein\ndurch genommen Grunde\nim werden\nKampagne die fu\u0308r\nKosten Die READ\nWRITE\nThe cos t of thecam pai\ngn\nisbas ica\nlly cov ere d by my sal ary as a sen -- ato r . < e os >\n< eos > .\ndeckt ge-ator Sen--\nals alt\nGeh-mein durch\ngenommen Grunde\nim werden\nKampagne die fu\u0308r\nKosten Die READ\nWRITE\ntranslation quality. We also note that very early WRITE commonly causes bad translation. For example, for AP=0.3 & CW=2, both the models choose to WRITE in the very beginning the word \u201cThe\u201d, which is unreasonable since Russian has no articles, and there is no word corresponding to it. One good feature of using NMT is that the more words the decoder READs, the longer history is saved, rendering simultaneous translation easier.\nDE\u2192EN As shown in Fig 1 and 7 (a), where we visualize the attention weights as soft alignment between the progressive input and output sentences, the highest weights are basically along the diagonal line. This indicates that our simul-\ntaneous translator works by waiting for enough source words with high alignment weights and then switching to write them.\nDE-EN simultaneous translation is likely more difficult as German often uses Subject-ObjectVerb (SOV) constructions. As shown in Fig 1, when a sentence (or a clause) starts the agent has learned such policy to READ multiple steps to approach the verb (e.g. serviert and gestorben in Fig 1). Such policy is still limited when the verb is very far from the subject. For instance in Fig. 7, the simultaneous translator achieves almost the same translation with standard NMT except for the verb \u201cgedeckt\u201d which corresponds to \u201ccovered\u201d in NMT output. Since there are too many words between the verb \u201cgedeckt\u201d and the subject \u201cKosten fu\u0308r die Kampagne werden\u201d, the agent gives up reading (otherwise it will cause a large delay and a penalty) and WRITEs \u201cbeing paid\u201d based on the decoder\u2019s hypothesis. This is one of the limitations of the proposed framework, as the NMT environment is trained on complete source sentence and it may be difficult to predict the verb that has not been seen in the source sentence. One possible way is to fine-tune the NMT model on incomplete sentences to boost its prediction ability. We will leave this as future work."}, {"heading": "8 Conclusion", "text": "We propose a unified framework to do neural simultaneous machine translation. To trade off quality and delay, we extensively explore various targets for delay and design a method for beamsearch applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrates the efficacy both quantitatively and qualitatively."}, {"heading": "Acknowledgments", "text": "KC acknowledges the support by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016). GN acknowledges the support of the Microsoft CORE program. This work was also partly supported by Samsung Advanced Institute of Technology (Neural Machine Translation) and Samsung Electronics (Project: \u201dDevelopment and Application of Larger-Context Neural Machine Translation\u201d)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Real-time incremental speech-to-speech translation of dialogs", "author": ["Vivek Kumar Rangarajan Sridhar", "Prakash Kolan", "Ladan Golipour", "Aura Jimenez"], "venue": "In Proceedings of the 2012 Conference of the North", "citeRegEx": "Bangalore et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bangalore et al\\.", "year": 2012}, {"title": "Can neural machine translation do simultaneous translation? arXiv preprint arXiv:1606.02012", "author": ["Cho", "Esipova2016] Kyunghyun Cho", "Masha Esipova"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2016}, {"title": "Simultaneous translation of lectures and speeches", "author": ["Alex Waibel", "Muntsin Kolss"], "venue": "Machine Translation,", "citeRegEx": "F\u00fcgen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "F\u00fcgen et al\\.", "year": 2007}, {"title": "Simple, lexicalized choice of translation timing for simultaneous speech translation", "author": ["Fujita et al.2013] Tomoki Fujita", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura"], "venue": "In INTERSPEECH", "citeRegEx": "Fujita et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fujita et al\\.", "year": 2013}, {"title": "Dont until the final verb wait: Reinforcement learning for simultaneous machine translation", "author": ["He He", "Jordan Boyd-Graber", "John Morgan", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 2014 Conference", "citeRegEx": "II et al\\.,? \\Q2014\\E", "shortCiteRegEx": "II et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Speed or accuracy? a study in evaluation of simultaneous speech translation", "author": ["Mieno et al.2015] Takashi Mieno", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura"], "venue": "In INTERSPEECH", "citeRegEx": "Mieno et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mieno et al\\.", "year": 2015}, {"title": "Neural variational inference and learning in belief networks. arXiv preprint arXiv:1402.0030", "author": ["Mnih", "Gregor2014] Andriy Mnih", "Karol Gregor"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Optimizing segmentation strategies for simultaneous speech translation", "author": ["Oda et al.2014] Yusuke Oda", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Oda et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oda et al\\.", "year": 2014}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Segmentation strategies for streaming speech translation", "author": ["John Chen", "Srinivas Bangalore", "Andrej Ljolje", "Rathinavelu Chengalvarayan"], "venue": "In Proceedings of the 2013 Conference", "citeRegEx": "Sridhar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sridhar et al\\.", "year": 2013}, {"title": "Simultaneous machine translation using deep reinforcement learning. Abstraction in Reinforcement Learning Workshop, ICML2016", "author": ["Satija", "Pineau2016] Harsh Satija", "Joelle Pineau"], "venue": null, "citeRegEx": "Satija et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Satija et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Incremental segmentation and decoding strategies for simultaneous translation", "author": ["Vivek Kumar Rangarajan Sridhar", "Srinivas Bangalore", "Baskaran Sankaran"], "venue": "In IJCNLP,", "citeRegEx": "Yarmohammadi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yarmohammadi et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Simultaneous translation, the task of translating content in real-time as it is produced, is an important tool for real-time understanding of spoken lectures or conversations (F\u00fcgen et al., 2007; Bangalore et al., 2012).", "startOffset": 175, "endOffset": 219}, {"referenceID": 1, "context": "Simultaneous translation, the task of translating content in real-time as it is produced, is an important tool for real-time understanding of spoken lectures or conversations (F\u00fcgen et al., 2007; Bangalore et al., 2012).", "startOffset": 175, "endOffset": 219}, {"referenceID": 7, "context": "Different from the typical machine translation (MT) task, in which translation quality is paramount, simultaneous translation requires balancing the trade-off between translation quality and time delay to ensure that users receive translated content in an expeditious manner (Mieno et al., 2015).", "startOffset": 275, "endOffset": 295}, {"referenceID": 9, "context": "segment independently (Oda et al., 2014) or with a minimal amount of language model context (Bangalore et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 1, "context": ", 2014) or with a minimal amount of language model context (Bangalore et al., 2012).", "startOffset": 59, "endOffset": 83}, {"referenceID": 14, "context": "Independently of simultaneous translation, accuracy of standard MT systems has greatly improved with the introduction of neural-networkbased MT systems (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 158, "endOffset": 205}, {"referenceID": 0, "context": "Independently of simultaneous translation, accuracy of standard MT systems has greatly improved with the introduction of neural-networkbased MT systems (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 158, "endOffset": 205}, {"referenceID": 10, "context": "Similar to standard MT, we have a measure Q(Y ) to evaluate the translation quality, such as BLEU score (Papineni et al., 2002).", "startOffset": 104, "endOffset": 127}, {"referenceID": 0, "context": "Standard NMT uses bi-directional RNNs as encoders (Bahdanau et al., 2014), but this is not suitable for simultaneous processing as using a reverse-order encoder requires knowing the final word of the sentence before beginning processing.", "startOffset": 50, "endOffset": 73}, {"referenceID": 10, "context": "Quality We evaluate the translation quality using automated metrics such as BLEU (Papineni et al., 2002).", "startOffset": 81, "endOffset": 104}, {"referenceID": 15, "context": "Policy Gradient We freeze the pre-trained parameters of an NMT model, and train the agent using the policy gradient (Williams, 1992).", "startOffset": 116, "endOffset": 132}, {"referenceID": 14, "context": "In standard NMT it has been shown that beam search, where the decoder keeps a beam of k translation trajectories, greatly improves translation quality (Sutskever et al., 2014).", "startOffset": 151, "endOffset": 175}, {"referenceID": 3, "context": "Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (F\u00fcgen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 129, "endOffset": 254}, {"referenceID": 1, "context": "Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (F\u00fcgen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 129, "endOffset": 254}, {"referenceID": 4, "context": "Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (F\u00fcgen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 129, "endOffset": 254}, {"referenceID": 16, "context": "Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (F\u00fcgen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 129, "endOffset": 254}, {"referenceID": 1, "context": ", 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013). In this approach, the incoming speech stream required to be translated are first recognized and segmented based on an automatic speech recognition (ASR) system. The translation model then works independently based on each of these segments, potentially limiting the quality of translation. To avoid using a fixed segmentation algorithm, Oda et al. (2014) introduced a trainable segmentation component into their system, so that the segmentation leads to better translation quality.", "startOffset": 8, "endOffset": 469}, {"referenceID": 1, "context": ", 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013). In this approach, the incoming speech stream required to be translated are first recognized and segmented based on an automatic speech recognition (ASR) system. The translation model then works independently based on each of these segments, potentially limiting the quality of translation. To avoid using a fixed segmentation algorithm, Oda et al. (2014) introduced a trainable segmentation component into their system, so that the segmentation leads to better translation quality. Grissom II et al. (2014) proposed a similar framework, however, based on reinforcement learning.", "startOffset": 8, "endOffset": 621}, {"referenceID": 13, "context": "Both the training set and the validation set are tokenized and segmented into sub-word units with byte-pair encoding (BPE) (Sennrich et al., 2015).", "startOffset": 123, "endOffset": 146}, {"referenceID": 9, "context": "\u2022 Segmentation-based (SEG) (Oda et al., 2014): a state-of-the-art segmentation-based algorithm based on optimizing segmentation to achieve the highest quality score.", "startOffset": 27, "endOffset": 45}, {"referenceID": 9, "context": "We also compared against Oda et al. (2014)\u2019s state-of-the-art segmentation algorithm (SEG).", "startOffset": 25, "endOffset": 43}], "year": 2016, "abstractText": "Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. 1", "creator": "LaTeX with hyperref package"}}}