{"id": "1610.01076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Tutorial on Answering Questions about Images with Deep Learning", "abstract": "laevis Together with the development kruemmel of hollywoodland more kaitov accurate elaz\u0131\u011f methods brookula in Computer Vision and cassavetes Natural Language Understanding, holistic formicidae architectures that answer stover on pressured questions \u00f6zil about mordellistena the content of real - world distinctiveness images calvez have kr\u00f8yer emerged. In this hoki tutorial, we build a mettupalayam neural - based jeng approach deeded to 2503 answer bajaga questions about spurway images. We ipac base sleigh our tutorial on rigvedic two datasets: (jeriome mostly disjoint on) http://www.chrysler.com DAQUAR, democratise and (duncanson a non-roster bit regionally on) owais VQA. .328 With small dhar tweaks pajama the leitz models ettumanoor that we kopylov present gosse here agonize can bobov achieve rodenstock a competitive performance on both mongkok datasets, in fmln fact, they are classical-style among the upped best cearns methods countersigned that use a goncharov combination of regulars LSTM with a global, esq full frame CNN antasari representation of 693 an bissett image. hollyfield We charlemagne hope surin that frumkin after reading this whaleback tutorial, the stepanek reader lourenco will be able montclair to use crocodyliforms Deep navigational Learning mamoru frameworks, such jinggoy as Keras and acea introduced ost Kraino, 3,897 to marcey build ragona various quantcast architectures hands-on that will anglos lead to location-based a hopeman further cheerio performance a-20 improvement linc on this challenging task.", "histories": [["v1", "Tue, 4 Oct 2016 16:29:28 GMT  (1299kb,D)", "http://arxiv.org/abs/1610.01076v1", "The tutorial was presented at '2nd Summer School on Integrating Vision and Language: Deep Learning' in Malta, 2016"]], "COMMENTS": "The tutorial was presented at '2nd Summer School on Integrating Vision and Language: Deep Learning' in Malta, 2016", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG cs.NE", "authors": ["mateusz malinowski", "mario fritz"], "accepted": false, "id": "1610.01076"}, "pdf": {"name": "1610.01076.pdf", "metadata": {"source": "CRF", "title": "Tutorial on Answering Questions about Images with Deep Learning", "authors": ["Mateusz Malinowski", "Mario Fritz"], "emails": ["mmalinow@mpi-inf.mpg.de", "mfritz@mpi-inf.mpg.de"], "sections": [{"heading": "1 Preface", "text": "In this tutorial1 we build a few architectures that can answer questions about images. The architectures are based on our two papers on this topic: Malinowski et al. [2015] and Malinowski et al. [2016]; and more broadly, on our project towards a Visual Turing Test2. In particular, an encoder-decoder perspective of Malinowski et al. [2016] allows us to effectively experiment with various design choices. For the sake of simplicity, we only consider a classification-based approach to answer questions about images, although an approach that generate answers word-by-word is also studied in the community [Malinowski et al., 2015]. In the tutorial, we mainly focus on the DAQUAR dataset [Malinowski and Fritz, 2014a], but a few possible directions to apply learnt techniques to VQA [Antol et al., 2015] are also pointed. First, we will get familiar with the task of answering questions about images, and a dataset that implements the task (due to a small size, we mainly use DAQUAR as it better serves an educational purpose that we aim at this tutorial). Next, we build a few blind models that answer questions about images without actually seeing such images. Such models already exhibit a reasonable performance as they can effectively learn various biases that exist in a dataset, which we also interpret as learning a common sense knowledge [Malinowski et al., 2015, 2016]. Subsequently, we build a few language+vision models that answer questions based on both a textual and a visual inputs. Finally, we leave the tutorial with a few possible research directions.\nTechnical aspects The tutorial is originally written using Python Notebook, which the reader is welcome to download3 and use through the tutorial. Instructions necessary to run the Notebook version of this tutorial are provided in the following: https://github.com/mateuszmalinowski/visual_turing_test-tutorial. In this tutorial, we heavily use a Python code, and therefore it is expected the reader either already knows this language, or can quickly learn it. However, we made an effort to make this tutorial approachable to a wider audience. We use Kraino3 that is a framework prepared for this tutorial in order to simplify the development of the question answering architectures. Under the hood, it uses Theano4 [Bastien et al., 2012] and Keras5 [Chollet, 2015] \u2013 two frameworks to build Deep Learning models. We also use various CNNs representations extracted from images that can be downloaded as explained at the beginning of our Notebook tutorial3. We also highlight exercises that a curious reader may attempt to solve.\n1This tutorial was presented for the first time during the 2nd Summer School on Integrating Vision and Language: Deep Learning. 2http://mpii.de/visual_turing_test 3https://github.com/mateuszmalinowski/visual_turing_test-tutorial/blob/master/visual_\nturing_test.ipynb 4http://deeplearning.net/software/theano/ 5https://keras.io\nar X\niv :1\n61 0.\n01 07\n6v 1\n[ cs\n.C V\n] 4\nO ct\n2 01\n2 Dataset\nThis section introduces the DAQUAR dataset [Malinowski and Fritz, 2014a] from a programming perspective. Let us first list a few DAQUAR entries to become familiar with the format.\nIn [1]: ! head -15 data/daquar/qa.894.raw.train.format_triple\nwhat is on the right side of the black telephone and on the left side of the red chair ? desk image3 what is in front of the white door on the left side of the desk ? telephone image3 what is on the desk ? book, scissor, papers, tape dispenser image3 what is the largest brown objects ? carton image3 what color is the chair in front of the white wall ? red image3\nNote that the format is: question, answer (could be many answer words), and the image. Let us have a look at Figure 1. The figure lists images with associated question-answer pairs. It also comments on challenges associated with question-answerimage triplets. We see that to answer properly on the wide range of questions, an answerer not only needs to understand the scene visually or to just understand the question, but also, arguably, has to resort to the common sense knowledge, or even\nknow the preferences of the person asking the question, e.g. what \u2018behind\u2019 exactly means in \u2018What is behind the table?\u2019. Hence, architectures that answer questions about images have to face many challenges. Ambiguities make it also difficult to judge the provided answers. We revisit this issue in a later section. Meantime, a curious reader may try to answer the following question.\nCan you spot ambiguities that are present in the first column of the figure? Think of a spatial relationship between an observer, object of interest, and the world.\nThe following code returns a dictionary of three views on the DAQUAR dataset. For now, we look only into the \u2018text\u2019 view. dp[\u2018text\u2019] returns a function from a dataset split into the dataset\u2019s textual view. Executing the following code makes it more clear."}, {"heading": "In [ ]: #TODO: Execute the following procedure (Shift+Enter in the Notebook)", "text": "from kraino.utils import data_provider\ndp = data_provider.select[\u2019daquar-triples\u2019] train_text_representation = dp[\u2019text\u2019](train_or_test=\u2019train\u2019)\nThis view specifies how questions are ended (\u2018?\u2019), answers are ended (\u2018.\u2019), answer words are delimited (DAQUAR sometimes has a set of answer words as an answer, for instance \u2018knife, fork\u2019 may be an answer answer), but most important, it has questions (key \u2018x\u2019), answers (key \u2018y\u2019), and names of the corresponding images (key \u2018img name\u2019).\nIn [ ]: # let us check some entries of the text\u2019s representation n_elements = 10 print(\u2019== Questions:\u2019) print_list(train_text_representation[\u2019x\u2019][:n_elements]) print print(\u2019== Answers:\u2019) print_list(train_text_representation[\u2019y\u2019][:n_elements]) print print(\u2019== Image Names:\u2019) print_list(train_text_representation[\u2019img_name\u2019][:n_elements])\nSummary DAQUAR consists of question-answer-image triplets. Question-answer pairs for different folds are accessible from executing the following code.\ndata_provider.select[\u2019text\u2019]\nFinally, as we see in Figure 1, DAQUAR poses many challenges in designing good architectures, or evaluation metrics."}, {"heading": "3 Textual Features", "text": "We have an access to a textual representation of questions. This is however not very helpful since neural networks expect a numerical input, and hence we cannot really work with the raw text. We need to transform the textual input into some numerical value or a vector of values. One particularly successful representation is called one-hot vector and it is a binary vector with exactly one non-zero entry. This entry points to the corresponding word in the vocabulary. See the illustration shown in Figure 2.\nThe reader can pause here a bit to answer the following questions.\nCan you sum up the one-hot vectors for the \u2018What table is behind the table?\u2019. How would you interpret the resulting vector? Why is it a good idea to work with one-hot vector represetantions of the text?\nAs we see from the illustrative example above, we first need to build a suitable vocabulary from our raw textual training data, and next transform them into one-hot representations. The following code can do this.\nIn [ ]: from toolz import frequencies train_raw_x = train_text_representation[\u2019x\u2019]\n# we start from building the frequencies table wordcount_x = frequencies(\u2019 \u2019.join(train_raw_x).split(\u2019 \u2019)) # print the most and least frequent words n_show = 5 print(sorted(wordcount_x.items(), key=lambda x: x[1], reverse=True)[:n_show]) print(sorted(wordcount_x.items(), key=lambda x: x[1])[:n_show])\nIn many parts of this tutorial, we use Kraino, which was developed for the purpose of this tutorial to simplify the development of various \u2018question answering\u2019 models through prototyping.\nfrom kraino.utils.input_output_space import build_vocabulary\n# This function takes wordcounts, # and returns word2index - mapping from words into indices, # and index2word - mapping from indices to words. word2index_x, index2word_x = build_vocabulary(\nthis_wordcount=wordcount_x, truncate_to_most_frequent=0)\nword2index_x\nIn addition, we use a few special, extra symbols that do not occur in the training dataset. Most important are < pad > and < unk >. We use the former to pad sequences in order to have the same number of temporal elements; we use the latter for words (at test time) that do not exist in the training set. Armed with the vocabulary, we can build one-hot representations of the training data. However, this is not necessary and may even be wasteful. Our one-hot representation of the input text does not explicitly build long and sparse vectors, but instead it operates on indices. The example from Figure 2 would be encoded as [0,1,4,2,7,3].\nDue to the sparsity existing in the one-hot representation, we can more efficiently operate on indices instead of performing full linear transformations by matrix-vector multiplications. This is reflected in the following claim.\nClaim: Let x be a binary vector with exactly one value 1 at the position index, that is x[index] = 1. Then W [:, index] =Wx\nwhere W [:, b] denotes a vector built from a column b of W . This shows that matrix-vector multiplication can be replaced by retrieving a right vector of parameters according to the index.\nCan you show that the claim is valid?\nWe can encode textual questions into one-hot vector representations by executing the following code.\nIn [ ]: from kraino.utils.input_output_space import encode_questions_index one_hot_x = encode_questions_index(train_raw_x, word2index_x) print(train_raw_x[:3]) print(one_hot_x[:3])\nAs we can see, the sequences have different number of elements. We can pad the sequences to have the same length by setting up MAXLEN .\nfrom keras.preprocessing import sequence MAXLEN=30 train_x = sequence.pad_sequences(one_hot_x, maxlen=MAXLEN) train_x[:3]\nWe do the same with the answers.\nIn [ ]: # for simplicity, we consider only first answer words; In [ ]: # that is, if answer is \u2019knife,fork\u2019 we encode only \u2019knife\u2019\nMAX_ANSWER_TIME_STEPS=1\nfrom kraino.utils.input_output_space import encode_answers_one_hot train_raw_y = train_text_representation[\u2019y\u2019] wordcount_y = frequencies(\u2019 \u2019.join(train_raw_y).split(\u2019 \u2019)) word2index_y, index2word_y = build_vocabulary(this_wordcount=wordcount_y) train_y, _ = encode_answers_one_hot(\ntrain_raw_y, word2index_y, answer_words_delimiter=train_text_representation[\u2019answer_words_delimiter\u2019], is_only_first_answer_word=True, max_answer_time_steps=MAX_ANSWER_TIME_STEPS)\nprint(train_x.shape) print(train_y.shape)\nAt the last step, we encode test questions. We need them later to see how well our models generalize to new question-answerimage triplets. Remember, however, that we should use the vocabulary we generated from the training samples.\nWhy should we use the training vocabulary to encode test questions?\nIn [ ]: test_text_representation = dp[\u2019text\u2019](train_or_test=\u2019test\u2019) test_raw_x = test_text_representation[\u2019x\u2019] test_one_hot_x = encode_questions_index(test_raw_x, word2index_x) test_x = sequence.pad_sequences(test_one_hot_x, maxlen=MAXLEN) print_list(test_raw_x[:3]) test_x[:3]\nWith the encoded question-answer pairs we finish this section. However, before delving into details of building and training new models, let us have a look at the summary to see bigger picture.\nSummary We started from raw questions from the training set. We use them to build a vocabulary. Next, we encode questions into sequences of one-hot vectors based on the vocabulary. Finally, we use the same vocabulary to encode questions from the test set. If a word is absent, we use an extra token < unk > to denote this fact, so that we encode the < unk > token, not the word."}, {"heading": "4 Language Only Models", "text": "4.0.1 Training\nAs you may already know, we train models by weights updates. Let x and y be training samples (an input, and an output), and `(x, y) be an objective function. The formula for weights updates is:\nw := w \u2212 \u03b1\u2207`(x, y;w)\nwith \u03b1 that we call the learning rate, and \u2207 that is a gradient wrt. the weights w. The learning rate is a hyper-parameter that must be set in advance. The rule shown above is called the SGD update, but other variants are also possible. In fact, we use its variant called ADAM [Kingma and Ba, 2014].\nWe cast the question answering problem into a classification framework, so that we classify an input x into some class that represents an answer word. Therefore, we use, commonly used in the classification, logistic regression as the objective:\n`(x, y;w) := \u2211 y\u2032\u2208C 1{y\u2032 = y} log p(y\u2032 | x,w)\nwhere C is a set of all classes, and p(y | x,w) is the softmax: ewy\u03c6(x)/ \u2211 z e\nwz\u03c6(x). Here \u03c6(x) denotes an output of a model (more precisely, it is often a response of a neural network to the input, just before softmax of the neural network is applied). Note, however, that another variant of providing answers, called the answer generation, is also possible [Malinowski et al., 2015]. For training, we need to execute the following code.\ntraining(gradient_of_the_model, optimizer=\u2019Adam\u2019)\nSummary Given a model, and an optimization procedure (SGD, Adam, etc.) all we need is to compute gradient of the model\u2207`(x, y;w) wrt. to its parameters w, and next plug it to the optimization procedure.\n4.0.2 Theano\nSince computing gradients\u2207`(x, y;w) may quickly become tedious, especially for more complex models, we search for tools that could automatize this process. Imagine that you build a model M and you get its gradient\u2207M by just executing the tool, something like the following piece of code.\nnabla_M = compute_gradient_symbolically(M,x,y)\nThis would definitely speed up prototyping. Theano [Bastien et al., 2012] is such a tool that is specifically tailored to work with deep learning models. For a broader understanding of Theano, you can check a suitable tutorial6.\nThe following coding example defines ReLU, a popular activation function defined as ReLU(x) = max(x, 0), as well as derive its derivative using Theano. Note however that, with this example, we obviously only scratch the surface."}, {"heading": "In [ ]: import theano", "text": "import theano.tensor as T\n# Theano uses symbolic calculations, # so we need to first create symbolic variables theano_x = T.scalar() # we define a relationship between a symbolic input and a symbolic output theano_y = T.maximum(0,theano_x) # now it\u2019s time for a symbolic gradient wrt. to symbolic variable x theano_nabla_y = T.grad(theano_y, theano_x)\n# we can see that both variables are symbolic, they don\u2019t have numerical values print(theano_x) print(theano_y) print(theano_nabla_y)\n# theano.function compiles the symbolic representation of the network theano_f_x = theano.function([theano_x], theano_y) print(theano_f_x(3)) print(theano_f_x(-3)) # and now for gradients\nnabla_f_x = theano.function([theano_x], theano_nabla_y) print(nabla_f_x(3)) print(nabla_f_x(-3))\nCan you derive a derivative of ReLU on your own? Consider two cases.\nIt should also be mentioned that ReLU is a non-differentiable function at the point 0, and therefore, technically, we compute its sub-gradient \u2013 this is however still fine for Theano.\n6For instance, http://deeplearning.net/tutorial/.\nSummary To compute gradient symbolically, we can use Theano. This speeds up prototyping, and hence developing new question answering models.\n4.0.3 Keras\nKeras [Chollet, 2015] builds upon Theano, and significantly simplifies creating new deep learning models as well as training such models, effectively speeding up the prototyping even further. Keras also abstracts away from some technical burden such as a symbolic variable creation. Many examples of using Keras can be found by following the links: https://keras.io/getting-started/sequential-model-guide/, and https://keras.io/ getting-started/functional-api-guide/. Note that, in the tutorial we use an older sequential model. Please also pay attention to the version of the Keras, since not all versions are compatible with this tutorial.\n4.0.4 Models\nFor the purpose of the Visual Turing Test, and this tutorial, we have compiled a light framework that builds on top of Keras, and simplify building and training \u2018question answering\u2019 machines. With the tradition of using Greek names, we call it Kraino. Note that some parts of the Kraino, such as a data provider, were already covered in this tutorial.\nIn the following, we will go through BOW and LSTM approaches to answer questions about images, but, surprisingly, without the images. It turns out that a substantial fraction of questions can be answered without an access to an image, but rather by resorting to a common sense (or statistics of the dataset). For instance, \u2018what can be placed at the table?\u2019, or \u2018How many eyes this human have?\u2019. Answers like \u2018chair\u2019 and \u20182\u2019 are quite likely to be good answers.\n4.0.5 BOW\nFigure 3 illustrates the BOW (Bag Of Words) method. As we have already seen before, we first encode the input sentence into one-hot vector representations. Such a (very) sparse representation is next embedded into a denser space by a matrix We. Next, the denser representations are summed up and classified via \u2018Softmax\u2019. Notice that, if We were an identity matrix, we would obtain a histogram of the word\u2019s occurrences.\nWhat is your biggest complain about such a BOW representation? What happens if instead of \u2019What is behind the table\u2019 we would have \u2019is What the behind table\u2019? How does the BOW representation change?\nLet us now define a BOW model using our tools."}, {"heading": "In [ ]: #== Model definition", "text": "# First we define a model using keras/kraino from keras.layers.core import Activation from keras.layers.core import Dense from keras.layers.core import Dropout from keras.layers.core import TimeDistributedMerge from keras.layers.embeddings import Embedding\nfrom kraino.core.model_zoo import AbstractSequentialModel from kraino.core.model_zoo import AbstractSingleAnswer from kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer from kraino.core.model_zoo import Config from kraino.core.keras_extensions import DropMask from kraino.core.keras_extensions import LambdaWithMask from kraino.core.keras_extensions import time_distributed_masked_ave\n# This model inherits from AbstractSingleAnswer, # and so it produces single answer words # To use multiple answer words, # you need to inherit from AbstractSequentialMultiplewordAnswer class BlindBOW(AbstractSequentialModel, AbstractSingleAnswer):\n\"\"\" BOW Language only model that produces single word answers.\n\"\"\" def create(self):\nself.add(Embedding( self._config.input_dim, self._config.textual_embedding_dim, mask_zero=True)) self.add(LambdaWithMask (time_distributed_masked_ave, output_shape=[self.output_shape[2]])) self.add(DropMask()) self.add(Dropout(0.5)) self.add(Dense(self._config.output_dim)) self.add(Activation(\u2019softmax\u2019))\nIn [ ]: model_config = Config( textual_embedding_dim=500, input_dim=len(word2index_x.keys()), output_dim=len(word2index_y.keys()))\nmodel = BlindBOW(model_config) model.create()\nmodel.compile( loss=\u2019categorical_crossentropy\u2019, optimizer=\u2019adam\u2019) text_bow_model = model\nIn [ ]: #== Model training text_bow_model.fit(\ntrain_x,\ntrain_y, batch_size=512, nb_epoch=40, validation_split=0.1, show_accuracy=True)\n4.0.6 Recurrent Neural Network\nAlthough BOW is working pretty well, there is still something very disturbing about this approach. Consider the following question: \u2018what is on the right side of the black telephone and on the left side of the red chair ?\u2019 If we swap \u2018chair\u2019 with \u2018telephone\u2019 in the question, we would get a different meaning. Recurrent Neural Networks (RNNs) have been developed to mitigate this issue by directly processing time series. As Figure 4 illustrates, the (temporarily) first word embedding is given to an RNN unit. The RNN unit next processes such an embedding and outputs to the second RNN unit. This unit takes both the output of the first RNN unit and the 2nd word embedding as inputs, and outputs some algebraic combination of both inputs. And so on. The last recurrent unit builds the representation of the whole sequence. Its output is next given to Softmax for the classification. One among the challenges that such approaches have to deal with is maintaining long-term dependencies. Roughly speaking, as new inputs are coming in the following steps it is getting easier to \u2018forget\u2019 information from the beginning (the first temporal step). LSTM [Hochreiter and Schmidhuber, 1997] and GRU [Cho et al., 2014] are two particularly popular Recurrent Neural Networks that can preserve such longer dependencies to some extent7.\nLet us create a Recurrent Neural Network in the following."}, {"heading": "In [ ]: #== Model definition", "text": "# First we define a model using keras/kraino from keras.layers.core import Activation from keras.layers.core import Dense from keras.layers.core import Dropout from keras.layers.embeddings import Embedding from keras.layers.recurrent import GRU from keras.layers.recurrent import LSTM\nfrom kraino.core.model_zoo import AbstractSequentialModel from kraino.core.model_zoo import AbstractSingleAnswer\n7http://karpathy.github.io/2015/05/21/rnn-effectiveness/\nfrom kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer from kraino.core.model_zoo import Config from kraino.core.keras_extensions import DropMask from kraino.core.keras_extensions import LambdaWithMask from kraino.core.keras_extensions import time_distributed_masked_ave\n# This model inherits from AbstractSingleAnswer, # and so it produces single answer words # To use multiple answer words, # you need to inherit from AbstractSequentialMultiplewordAnswer class BlindRNN(AbstractSequentialModel, AbstractSingleAnswer):\n\"\"\" RNN Language only model that produces single word answers. \"\"\" def create(self):\nself.add(Embedding( self._config.input_dim, self._config.textual_embedding_dim, mask_zero=True)) #TODO: Replace averaging with RNN (you can choose between LSTM and GRU) self.add(GRU(self._config.hidden_state_dim, return_sequences=False)) self.add(Dropout(0.5)) self.add(Dense(self._config.output_dim)) self.add(Activation(\u2019softmax\u2019))\nIn [ ]: model_config = Config( textual_embedding_dim=500, hidden_state_dim=500, input_dim=len(word2index_x.keys()), output_dim=len(word2index_y.keys()))\nmodel = BlindRNN(model_config) model.create() model.compile(\nloss=\u2019categorical_crossentropy\u2019, optimizer=\u2019adam\u2019)\ntext_rnn_model = model\nIn [ ]: #== Model training text_rnn_model.fit(\ntrain_x, train_y, batch_size=512, nb_epoch=40, validation_split=0.1, show_accuracy=True)\nThe curious reader is encouraged to experiment with the language-only models. For instance, to see the influence of particular modules to the overall performance, the reader can do the following exercise.\nChange the number of hidden states. Change the number of epochs used to train a model. Modify models by using more RNN layers, or deeper classifiers.\nSummary RNN models, as opposite to BOW, consider order of the words in the question. Moreover, a substantial number of questions can be answered without any access to images. This can be explained as models learn some specific dataset statistics, some of them can be interpreted as common sense knowledge."}, {"heading": "5 Evaluation Measures", "text": "To be able to monitor a progress on a task, we need to find ways to evaluate architectures on the task. Otherwise, we would not know how to judge architectures, or even worse, we would not even know what the goal is. Moreover, we should also aim at automatic evaluation measures, otherwise reproducibility is questionable, and the evaluation costs are high.\n5.0.7 Ambiguities\nAlthough an early work on the Visual Turing Test argues for keeping the answer words from a fixed vocabulary in order to keep an evaluation simpler [Malinowski and Fritz, 2014a,b, 2015], it is still difficult to automatically evaluate architectures due to ambiguities that occur in the answers. We have ambiguities in naming objects, sometimes due to synonyms, but sometimes due to fuzziness. For instance, is \u2018chair\u2019 == \u2018armchair\u2019 or \u2018chair\u2019 != \u2018armchair\u2019 or something in between? Such semantic boundaries become even more fuzzy when we increase the number of categories. We could easily find a mutually exclusive set of 10 different categories, but what if there are 1000 categories, or 10000 categories? Arguably, we cannot think in terms of an equivalence class anymore, but rather in terms of similarities. That is \u2018chair\u2019 is semantically more similar to \u2018armchair\u2019, than to \u2018horse\u2019. This simple example shows the main drawback of a traditional, binary evaluation measure which is Accuracy. This metric scores 1 if the names are the same and 0 otherwise. So that Acc(\u2018chair\u2019, \u2018armchair\u2019) == Acc(\u2018chair\u2019, \u2018horse\u2019). We call these ambiguities, word-level ambiguities, but there are other ambiguities that are arguably more difficult to handle. For instance, the same question can be phrased in multiple other ways. The language of spatial relations is also ambiguous. Language tends to be also rather vague - we sometimes skip details and resort to common sense. Some ambiguities are rooted in a culture. To address world-level ambiguities, Malinowski and Fritz [2014a] propose WUPS. To address ambiguities caused by various interpretations of an image or a question, Malinowski et al. [2015] propose Consensus measures. For the sake of simplicity, in this tutorial, we only use WUPS. On the other hand, arguably, it is easier to evaluate architectures on DAQUAR than on Image Captioning datasets. The former restricts the output space to N categories, while it still requires a holistic comprehension. Let us remind that Figure 1 shows a few ambiguities that exists in DAQUAR.\n5.0.8 Wu-Palmer Similarity\nGiven an ontology a Wu-Palmer Similarity between two words (or broader concepts) is a soft measure defined as\nWuP (a, b) := lca(a, b)\ndepth(a) + depth(b)\nwhere lca(a, b) is the least common ancestor of a and b, and depth(a) is depth of a in the ontology. Figure 5 shows a toy-sized ontology. The curious reader can, based on Figure 5, address the following questions.\nWhat is WuP(Dog, Horse) and WuP(Dog, Dalmatian) according to the toy-sized ontology? Can you calculate Acc(Dog, Horse) and Acc(Dog, Dalmatian)?\n5.0.9 WUPS\nWu-Palmer Similarity depends on the choice of ontology. One popular, large ontology is WordNet [Miller, 1995, Fellbaum, 1999]. Although Wu-Palmer Similarity may work on shallow ontologies, we are rather interested in ontologies with hundreds or even thousands of categories. In indoor scenarios, it turns out that many indoor \u2018things\u2019 share similar levels in the ontology, and hence Wu-Palmer Similarities are very small between two entities. The following code exemplifies the issue.\nIn [ ]: from nltk.corpus import wordnet as wn armchair_synset = wn.synset(\u2019armchair.n.01\u2019) chair_synset = wn.synset(\u2019chair.n.01\u2019) wardrobe_synset = wn.synset(\u2019wardrobe.n.01\u2019)\nprint(armchair_synset.wup_similarity(armchair_synset)) print(armchair_synset.wup_similarity(chair_synset)) print(armchair_synset.wup_similarity(wardrobe_synset)) wn.synset(\u2019chair.n.01\u2019).wup_similarity(wn.synset(\u2019person.n.01\u2019))\nAs we can see that \u2018armchair\u2019 and \u2018wardrobe\u2019 are surprisingly close to each other. It is because, for large ontologies, all the indoor \u2018things\u2019 are semantically \u2018indoor things\u2019. This issue has motivated us to define thresholded Wu-Palmer Similarity Score, defined as follows\nWuP (a, b) if WuP (a, b) \u2265 \u03c4 0.1 \u00b7WuP (a, b) otherwise\nwhere \u03c4 is a hand-chosen threshold. Empirically, we found that \u03c4 = 0.9 works fine on DAQUAR [Malinowski and Fritz, 2014a]. Moreover, since DAQUAR has answers as sets of answer words, so that \u2018knife,fork\u2019 == \u2018fork,knife\u2019, we have extended the above measure to work with the sets. We call it Wu-Palmer Set score, or shortly WUPS.\nA detailed exposition of WUPS is beyond this tutorial, but a curious reader is encouraged to read the \u2018Performance Measure\u2019 paragraph in Malinowski and Fritz [2014a]. Note that the measure in Malinowski and Fritz [2014a] is defined broader, and essentially it abstracts away from any particular similarities such as Wu-Palmer Similarity, or an ontology. WUPS at 0.9 is WUPS with threshold \u03c4 = 0.9. It is worth noting, that a practical implementation of WUPS needs to deal with synsets. Thus it is recommended to download the script from http://datasets.d2.mpi-inf.mpg.de/ mateusz14visual-turing/calculate_wups.py or re-implement it with caution.\n5.0.10 Consensus\nThe consensus measure handles ambiguities that are caused by various interpretations of a question or an image. In this tutorial, we do not cover this measure. A curious reader is encouraged to read the \u2018Human Consensus\u2019 in Malinowski et al. [2015].\n5.0.11 A few caveats\nWe present a few caveats when using WUPS. These can be especially useful if one wants to adapt WUPS to other datasets.\nLack of coverage Since WUPS is based on an ontology, not always it recognizes words. For instance \u2018garbage bin\u2019 is missing, but \u2018garbage can\u2019 is perfectly fine. You can check it by yourself, either with the source code provided above, or by using an online script8.\nSynsets The execution of the following code\nwn.synsets(\u2019chair\u2019)\nproduces a list with many elements. These elements are semantically equivalent9.\nFor instance the following definition of \u2018chair\u2019\nwn.synset(\u2019chair.n.03\u2019).definition()\nindicates a person (e.g. a chairman). Indeed, the following gives quite high value 8http://wordnetweb.princeton.edu/perl/webwn 9https://en.wikipedia.org/wiki/Synonym_ring\nwn.synset(\u2019chair.n.03\u2019).wup_similarity(wn.synset(\u2019person.n.01\u2019))\nhowever the following one has a more preferred, much lower value\nwn.synset(\u2019chair.n.01\u2019).wup_similarity(wn.synset(\u2019person.n.01\u2019))\nHow to deal with such a problem? In DAQUAR we take an optimistic perspective and always consider the highest similarity score. This works with WUPS 0.9 and a restricted indoor domain with a vocabulary based only on the training set. To sum up, this issue should be taken with a caution whenever WUPS is adapted to other domains.\nOntology Since WUPS is based on an ontology, specifically on WordNet, it may give different scores on different ontologies, or even on different versions of the same ontology.\nThreshold A good threshold \u03c4 is dataset dependent. In our case \u03c4 = 0.9 seems to work well, while \u03c4 = 0.0 is too forgivable and is rather reported due to the \u2018historical\u2019 reasons. However, following our papers, you should still consider to report plain set-based accuracy scores (so that Acc(\u2018knife,\u2019fork\u2019,\u2018fork,knife\u2019)==1; it can be computed by our script10 using the argument -1 to WUPS.\n5.0.12 Summary\nWUPS is an evaluation measure that works with sets and word-level ambiguities. Arguably, WUPS at 0.9 is the most practical measure."}, {"heading": "6 New Predictions", "text": "Once the training of our models is over, we can evaluate their performance on a previously unknown test set. In the following, we show how to make predictions using the already discussed blind models.\n6.0.13 Predictions - BOW\nWe start from encoding textual input into one-hot vector representations.\nIn [ ]: test_text_representation = dp[\u2019text\u2019](train_or_test=\u2019test\u2019) test_raw_x = test_text_representation[\u2019x\u2019] test_one_hot_x = encode_questions_index(test_raw_x, word2index_x) test_x = sequence.pad_sequences(test_one_hot_x, maxlen=MAXLEN)\nGiven encoded test questions, we use the maximum likelihood principle to withdraw answers.\nIn [ ]: from numpy import argmax # predict the probabilities for every word predictions_scores = text_bow_model.predict([test_x]) print(predictions_scores.shape) # follow the maximum likelihood principle, and get the best indices to vocabulary predictions_best = argmax(predictions_scores, axis=-1) print(predictions_best.shape) # decode the predicted indices into word answers predictions_answers = [index2word_y[x] for x in predictions_best] print(len(predictions_answers))\nNow, we evaluate the answers using WUPS.\nIn [ ]: from kraino.utils import print_metrics test_raw_y = test_text_representation[\u2019y\u2019] _ = print_metrics.select[\u2019wups\u2019](\ngt_list=test_raw_y, pred_list=predictions_answers,\n10http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py\nverbose=1, extra_vars=None)\nLet us see the predictions.\nIn [ ]: from numpy import random test_image_name_list = test_text_representation[\u2019img_name\u2019] indices_to_see = random.randint(low=0, high=len(test_image_name_list), size=5) for index_now in indices_to_see:\nprint(test_raw_x[index_now], predictions_answers[index_now])\nWithout looking at images, a curious reader may attempt to answer the following questions.\nDo you agree with the answers given above? What are your guesses? Of course, neither you nor the model have seen any images so far.\nHowever, what happens if the reader can actually see the images?\nExecute the code below. Do your answers change after seeing the images?\nIn [1]: from matplotlib.pyplot import axis from matplotlib.pyplot import figure from matplotlib.pyplot import imshow\nimport numpy as np from PIL import Image\n%matplotlib inline for index_now in indices_to_see:\nimage_name_now = test_image_name_list[index_now] pil_im = Image.open(\u2019data/daquar/images/{0}.png\u2019.format(image_name_now), \u2019r\u2019) fig = figure() fig.text(.2,.05,test_raw_x[index_now], fontsize=14) axis(\u2019off\u2019) imshow(np.asarray(pil_im))\nFinally, let us also see the ground truth answers by executing the following code.\nIn [ ]: print(\u2019question, prediction, ground truth answer\u2019) for index_now in indices_to_see:\nprint(test_raw_x[index_now], predictions_answers[index_now], test_raw_y[index_now])\nIn the code above, we have randomly taken questions, and hence different executions of the code may lead to different answers.\n6.0.14 Predictions - RNN\nLet us do similar predictions with a Recurrent Neural Network. This time, we use Kraino, to make the code shorter.\nIn [ ]: from kraino.core.model_zoo import word_generator # we first need to add word_generator to _config # (we could have done this before, in the Config constructor) # we use maximum likelihood as a word generator text_rnn_model._config.word_generator = word_generator[\u2019max_likelihood\u2019] predictions_answers = text_rnn_model.decode_predictions(\nX=test_x, temperature=None, index2word=index2word_y, verbose=0)\nIn [ ]: _ = print_metrics.select[\u2019wups\u2019]( gt_list=test_raw_y, pred_list=predictions_answers, verbose=1, extra_vars=None)\nA curious reader is encouraged to try the following exercise.\nVisualise question, predicted answers, ground truth answers as before. Check also images."}, {"heading": "7 Visual Features", "text": "All the considered so far architectures predict answers based only on questions, even though the questions concern images. Therefore, in this section, we also build visual features. As shown in Figure 6, a quite common practice is to:\n1. Take an already pre-trained CNN; often pre-training is done in some large-scale classification task such as ImageNet [Russakovsky et al., 2014].\n2. \u2018Chop off\u2019 a CNN representation after some layer. We use responses of that layer as visual features.\nIn this tutorial, we use features extracted from the second last 4096 dimensional layer of the VGG Net [Simonyan and Zisserman, 2014]. We have already extracted features in advance using Caffe [Jia et al., 2014] - another excellent framework for deep learning, particularly good for CNNs.\nThe following code gives visual features aligned with textual features."}, {"heading": "In [ ]: # this contains a list of the image names of our interest;", "text": "# it also makes sure that visual and textual features are aligned correspondingly train_image_names = train_text_representation[\u2019img_name\u2019] # the name for visual features that we use # CNN_NAME=\u2019vgg_net\u2019 # CNN_NAME=\u2019googlenet\u2019 CNN_NAME=\u2019fb_resnet\u2019 # the layer in CNN that is used to extract features # PERCEPTION_LAYER=\u2019fc7\u2019 # PERCEPTION_LAYER=\u2019pool5-7x7_s1\u2019 # PERCEPTION_LAYER=\u2019res5c-152\u2019 # l2 prefix since there are l2-normalized visual features PERCEPTION_LAYER=\u2019l2_res5c-152\u2019\ntrain_visual_features = dp[\u2019perception\u2019]( train_or_test=\u2019train\u2019,\nnames_list=train_image_names, parts_extractor=None, max_parts=None, perception=CNN_NAME, layer=PERCEPTION_LAYER, second_layer=None )\ntrain_visual_features.shape"}, {"heading": "8 Vision+Language", "text": "Given visual features, we can now build a full model that answer questions about images. As we can see in Figure 1, it is hard to answer correctly on questions without seeing images.\nLet us create an input as a pair of textual and visual features using the following code.\nIn [ ]: train_input = [train_x, train_visual_features]\nIn the following, we investigate two approaches to question answering: an orderless BOW, and an RNN.\n8.0.15 BOW + Vision\nSimilarly to our blind model, we start with a BOW encoding of a question. Here, we explore two ways of combining both modalities (circle with \u2018C\u2019 in Figure 7): concatenation, and piece-wise multiplication. For the sake of simplicity, we do not fine-tune the visual representation (dotted line symbolizes the barrier that blocks back-propagation in Figure 7)."}, {"heading": "In [ ]: #== Model definition", "text": "# First we define a model using keras/kraino\nfrom keras.models import Sequential from keras.layers.core import Activation from keras.layers.core import Dense from keras.layers.core import Dropout from keras.layers.core import Layer from keras.layers.core import Merge from keras.layers.core import TimeDistributedMerge from keras.layers.embeddings import Embedding\nfrom kraino.core.model_zoo import AbstractSequentialModel from kraino.core.model_zoo import AbstractSingleAnswer from kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer from kraino.core.model_zoo import Config from kraino.core.keras_extensions import DropMask from kraino.core.keras_extensions import LambdaWithMask from kraino.core.keras_extensions import time_distributed_masked_ave\n# This model inherits from AbstractSingleAnswer, # and so it produces single answer words # To use multiple answer words, # you need to inherit from AbstractSequentialMultiplewordAnswer class VisionLanguageBOW(AbstractSequentialModel, AbstractSingleAnswer):\n\"\"\" BOW Language only model that produces single word answers. \"\"\" def create(self):\nlanguage_model = Sequential() language_model.add(Embedding(\nself._config.input_dim, self._config.textual_embedding_dim, mask_zero=True))\nlanguage_model.add(LambdaWithMask( time_distributed_masked_ave, output_shape=[language_model.output_shape[2]])) language_model.add(DropMask()) visual_model = Sequential() if self._config.visual_embedding_dim > 0:\nvisual_model.add(Dense( self._config.visual_embedding_dim, input_shape=(self._config.visual_dim,)))\nelse: visual_model.add(Layer(input_shape=(self._config.visual_dim,))) self.add(Merge([language_model, visual_model], mode=self._config.multimodal_merge_mode)) self.add(Dropout(0.5)) self.add(Dense(self._config.output_dim)) self.add(Activation(\u2019softmax\u2019))"}, {"heading": "In [ ]: # dimensionality of embeddings", "text": "EMBEDDING_DIM = 500 # kind of multimodal fusion (ave, concat, mul, sum) MULTIMODAL_MERGE_MODE = \u2019concat\u2019\nmodel_config = Config( textual_embedding_dim=EMBEDDING_DIM, visual_embedding_dim=0, multimodal_merge_mode=MULTIMODAL_MERGE_MODE, input_dim=len(word2index_x.keys()),\noutput_dim=len(word2index_y.keys()), visual_dim=train_visual_features.shape[1])\nmodel = VisionLanguageBOW(model_config) model.create() model.compile(\nloss=\u2019categorical_crossentropy\u2019, optimizer=\u2019adam\u2019)\nIn [ ]: #== Model training model.fit(\ntrain_input, train_y, batch_size=512, nb_epoch=40, validation_split=0.1, show_accuracy=True)\nTo achieve better results, we can use another operator that combines both modalities together. For instance, we can use a piece-wise multiplication."}, {"heading": "In [ ]: #== Model definition", "text": "# First we define a model using keras/kraino from keras.models import Sequential from keras.layers.core import Activation from keras.layers.core import Dense from keras.layers.core import Dropout from keras.layers.core import Layer from keras.layers.core import Merge from keras.layers.core import TimeDistributedMerge from keras.layers.embeddings import Embedding\nfrom kraino.core.model_zoo import AbstractSequentialModel from kraino.core.model_zoo import AbstractSingleAnswer from kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer from kraino.core.model_zoo import Config from kraino.core.keras_extensions import DropMask from kraino.core.keras_extensions import LambdaWithMask from kraino.core.keras_extensions import time_distributed_masked_ave\n# This model inherits from AbstractSingleAnswer, # and so it produces single answer words # To use multiple answer words, # you need to inherit from AbstractSequentialMultiplewordAnswer class VisionLanguageBOW(AbstractSequentialModel, AbstractSingleAnswer):\n\"\"\" BOW Language only model that produces single word answers. \"\"\" def create(self):\nlanguage_model = Sequential() language_model.add(Embedding(\nself._config.input_dim, self._config.textual_embedding_dim, mask_zero=True))\nlanguage_model.add(LambdaWithMask( time_distributed_masked_ave, output_shape=[language_model.output_shape[2]])) language_model.add(DropMask()) visual_model = Sequential()\nif self._config.visual_embedding_dim > 0: visual_model.add(Dense(\nself._config.visual_embedding_dim, input_shape=(self._config.visual_dim,)))\nelse: visual_model.add(Layer(input_shape=(self._config.visual_dim,))) self.add(Merge([language_model, visual_model], mode=self._config.multimodal_merge_mode)) self.add(Dropout(0.5)) self.add(Dense(self._config.output_dim)) self.add(Activation(\u2019softmax\u2019))"}, {"heading": "In [ ]: # dimensionality of embeddings", "text": "EMBEDDING_DIM = 500 # kind of multimodal fusion (ave, concat, mul, sum) MULTIMODAL_MERGE_MODE = \u2019mul\u2019\nmodel_config = Config( textual_embedding_dim=EMBEDDING_DIM, visual_embedding_dim=EMBEDDING_DIM, multimodal_merge_mode=MULTIMODAL_MERGE_MODE, input_dim=len(word2index_x.keys()), output_dim=len(word2index_y.keys()), visual_dim=train_visual_features.shape[1]) model = VisionLanguageBOW(model_config) model.create() model.compile(\nloss=\u2019categorical_crossentropy\u2019, optimizer=\u2019adam\u2019)\ntext_image_bow_model = model\nIn [ ]: #== Model training text_image_bow_model.fit(\ntrain_input, train_y, batch_size=512, nb_epoch=40, validation_split=0.1, show_accuracy=True)\nAt the end of this section, a curious reader can try to answer the following questions.\nIf we merge language and visual features with \u2019mul\u2019, do we need to set both embeddings to have the same number of dimensions? That is, do we require to have textual_embedding_dim == visual_embedding_dim?\n8.0.16 RNN + Vision\nNow, we repeat the BOW experiments but with RNN. Figure 8 depicts the architecture."}, {"heading": "In [ ]: #== Model definition", "text": "# First we define a model using keras/kraino from keras.models import Sequential from keras.layers.core import Activation from keras.layers.core import Dense from keras.layers.core import Dropout from keras.layers.core import Layer\nfrom keras.layers.core import Merge from keras.layers.core import TimeDistributedMerge from keras.layers.embeddings import Embedding from keras.layers.recurrent import GRU from keras.layers.recurrent import LSTM\nfrom kraino.core.model_zoo import AbstractSequentialModel from kraino.core.model_zoo import AbstractSingleAnswer from kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer from kraino.core.model_zoo import Config from kraino.core.keras_extensions import DropMask from kraino.core.keras_extensions import LambdaWithMask from kraino.core.keras_extensions import time_distributed_masked_ave\n# This model inherits from AbstractSingleAnswer, # and so it produces single answer words # To use multiple answer words, # you need to inherit from AbstractSequentialMultiplewordAnswer class VisionLanguageLSTM(AbstractSequentialModel, AbstractSingleAnswer):\n\"\"\" BOW Language only model that produces single word answers. \"\"\" def create(self):\nlanguage_model = Sequential() language_model.add(Embedding(\nself._config.input_dim, self._config.textual_embedding_dim, mask_zero=True))\nlanguage_model.add(LSTM(self._config.hidden_state_dim, return_sequences=False))\nvisual_model = Sequential() if self._config.visual_embedding_dim > 0:\nvisual_model.add(Dense( self._config.visual_embedding_dim, input_shape=(self._config.visual_dim,)))\nelse: visual_model.add(Layer(input_shape=(self._config.visual_dim,))) self.add(Merge([language_model, visual_model], mode=self._config.multimodal_merge_mode)) self.add(Dropout(0.5)) self.add(Dense(self._config.output_dim)) self.add(Activation(\u2019softmax\u2019))\n# dimensionality of embeddings EMBEDDING_DIM = 500 # kind of multimodal fusion (ave, concat, mul, sum) MULTIMODAL_MERGE_MODE = \u2019sum\u2019\nmodel_config = Config( textual_embedding_dim=EMBEDDING_DIM, visual_embedding_dim=EMBEDDING_DIM, hidden_state_dim=EMBEDDING_DIM, multimodal_merge_mode=MULTIMODAL_MERGE_MODE, input_dim=len(word2index_x.keys()), output_dim=len(word2index_y.keys()), visual_dim=train_visual_features.shape[1]) model = VisionLanguageLSTM(model_config) model.create() model.compile(\nloss=\u2019categorical_crossentropy\u2019, optimizer=\u2019adam\u2019)\ntext_image_rnn_model = model\n8.0.17 Batch Size\nWe can do training with batch size set to 512. If an error occurs due to a memory consumption, lowering the batch size should help.\nIn [ ]: #== Model training text_image_rnn_model.fit(\ntrain_input, train_y, batch_size=512, nb_epoch=40, validation_split=0.1, show_accuracy=True)\nA curious reader may experiment with a few different batch sizes, and answer the following questions.\nCan you experiment with batch-size=1, and next with batch-size=5000? Can you explain both issues regarding the batch size? When do you get the best performance, with multiplication, concatenation, or summation?\nSummary As previously, using RNN makes the sequence processing order-aware. This time, however, we combine two modalities so that the whole model \u2018sees\u2019 images. Finally, it is also important how both modalities are combined."}, {"heading": "9 New Predictions with Vision+Language", "text": "9.0.18 Predictions (Features)\nIn [ ]: test_image_names = test_text_representation[\u2019img_name\u2019] test_visual_features = dp[\u2019perception\u2019](\ntrain_or_test=\u2019test\u2019, names_list=test_image_names, parts_extractor=None, max_parts=None, perception=CNN_NAME, layer=PERCEPTION_LAYER, second_layer=None )\ntest_visual_features.shape\nIn [ ]: test_input = [test_x, test_visual_features]\n9.0.19 Predictions (Bow with Vision)\nIn [ ]: from kraino.core.model_zoo import word_generator # we first need to add word_generator to _config # (we could have done this before, in the Config constructor) # we use maximum likelihood as a word generator text_image_bow_model._config.word_generator = word_generator[\u2019max_likelihood\u2019] predictions_answers = text_image_bow_model.decode_predictions(\nX=test_input, temperature=None, index2word=index2word_y, verbose=0)\nIn [ ]: _ = print_metrics.select[\u2019wups\u2019]( gt_list=test_raw_y, pred_list=predictions_answers, verbose=1, extra_vars=None)\n9.0.20 Predictions (RNN with Vision)\nIn [ ]: from kraino.core.model_zoo import word_generator # we first need to add word_generator to _config # (we could have done this before, in the Config constructor) # we use maximum likelihood as a word generator text_image_rnn_model._config.word_generator = word_generator[\u2019max_likelihood\u2019] predictions_answers = text_image_rnn_model.decode_predictions(\nX=test_input, temperature=None, index2word=index2word_y, verbose=0)\nIn [ ]: _ = print_metrics.select[\u2019wups\u2019]( gt_list=test_raw_y, pred_list=predictions_answers, verbose=1, extra_vars=None)"}, {"heading": "10 VQA", "text": "The models that we have built so far can be transferred to other dataset. Let us consider a recently introduced large-scale dataset, which is named VQA [Antol et al., 2015]. In this section, we train and evaluate VQA models. Since the reader should already be familiar with all the pieces, we just quickly jump into coding. For the sake of simplicity, we use only BOW architectures. Since VQA hides the test data for the purpose of challenge, we use the publicly available validation set to evaluate the architectures.\n10.0.21 VQA Language Features"}, {"heading": "In [ ]: #TODO: Execute the following procedure (Shift+Enter)", "text": "from kraino.utils import data_provider\nvqa_dp = data_provider.select[\u2019vqa-real_images-open_ended\u2019] # VQA has a few answers associated with one question. # We take the most frequently occuring answers (single_frequent). # Formal argument \u2019keep_top_qa_pairs\u2019 allows to filter out # rare answers with the associated questions. # We use 0 as we want to keep all question answer pairs, # but you can change into 1000 and see how the results differ vqa_train_text_representation = vqa_dp[\u2019text\u2019](\ntrain_or_test=\u2019train\u2019, answer_mode=\u2019single_frequent\u2019, keep_top_qa_pairs=1000)\nvqa_val_text_representation = vqa_dp[\u2019text\u2019]( train_or_test=\u2019val\u2019, answer_mode=\u2019single_frequent\u2019)\nIn [ ]: from toolz import frequencies vqa_train_raw_x = vqa_train_text_representation[\u2019x\u2019] vqa_train_raw_y = vqa_train_text_representation[\u2019y\u2019] vqa_val_raw_x = vqa_val_text_representation[\u2019x\u2019] vqa_val_raw_y = vqa_val_text_representation[\u2019y\u2019] # we start from building the frequencies table vqa_wordcount_x = frequencies(\u2019 \u2019.join(vqa_train_raw_x).split(\u2019 \u2019)) # we can keep all answer words in the answer as a class # therefore we use an artificial split symbol \u2019{\u2019 # to not split the answer into words # you can see the difference if you replace \u2019{\u2019 # with \u2019 \u2019 and print vqa_wordcount_y vqa_wordcount_y = frequencies(\u2019{\u2019.join(vqa_train_raw_y).split(\u2019{\u2019)) vqa_wordcount_y\n10.0.22 Language-Only\nIn [ ]: from keras.preprocessing import sequence from kraino.utils.input_output_space import build_vocabulary from kraino.utils.input_output_space import encode_questions_index from kraino.utils.input_output_space import encode_answers_one_hot MAXLEN=30 vqa_word2index_x, vqa_index2word_x = build_vocabulary\n(this_wordcount = vqa_wordcount_x) vqa_word2index_y, vqa_index2word_y = build_vocabulary (this_wordcount = vqa_wordcount_y) vqa_train_x = sequence.pad_sequences(encode_questions_index (vqa_train_raw_x, vqa_word2index_x), maxlen=MAXLEN) vqa_val_x = sequence.pad_sequences(encode_questions_index (vqa_val_raw_x, vqa_word2index_x), maxlen=MAXLEN) vqa_train_y, _ = encode_answers_one_hot(\nvqa_train_raw_y, vqa_word2index_y, answer_words_delimiter=vqa_train_text_representation[\u2019answer_words_delimiter\u2019] is_only_first_answer_word=True, max_answer_time_steps=1)\nvqa_val_y, _ = encode_answers_one_hot( vqa_val_raw_y, vqa_word2index_y, answer_words_delimiter=vqa_train_text_representation[\u2019answer_words_delimiter\u2019] is_only_first_answer_word=True, max_answer_time_steps=1)\nIn [ ]: from kraino.core.model_zoo import Config from kraino.core.model_zoo import word_generator # We are re-using the BlindBOW mode # Please make sure you have run the cell with the class definition # VQA is larger, so we can increase the dimensionality of the embedding vqa_model_config = Config(\ntextual_embedding_dim=1000, input_dim=len(vqa_word2index_x.keys()), output_dim=len(vqa_word2index_y.keys()), word_generator = word_generator[\u2019max_likelihood\u2019])\nvqa_text_bow_model = BlindBOW(vqa_model_config) vqa_text_bow_model.create() vqa_text_bow_model.compile(\nloss=\u2019categorical_crossentropy\u2019, optimizer=\u2019adam\u2019)\nIn [ ]: vqa_text_bow_model.fit( vqa_train_x, vqa_train_y, batch_size=512, nb_epoch=10, validation_split=0.1, show_accuracy=True)\nvqa_predictions_answers = vqa_text_bow_model.decode_predictions( X=vqa_val_x, temperature=None, index2word=vqa_index2word_y, verbose=0)\nvqa_vars = { \u2019question_id\u2019:vqa_val_text_representation[\u2019question_id\u2019], \u2019vqa_object\u2019:vqa_val_text_representation[\u2019vqa_object\u2019], \u2019resfun\u2019:\nlambda x: \\ vqa_val_text_representation[\u2019vqa_object\u2019].loadRes(\nx, vqa_val_text_representation[\u2019questions_path\u2019]) }\nIn [ ]: from kraino.utils import print_metrics\n_ = print_metrics.select[\u2019vqa\u2019]( gt_list=vqa_val_raw_y, pred_list=vqa_predictions_answers, verbose=1, extra_vars=vqa_vars)\n10.0.23 VQA Language+Vision\nIn [ ]: # the name for visual features that we use VQA_CNN_NAME=\u2019vgg_net\u2019 # VQA_CNN_NAME=\u2019googlenet\u2019 # the layer in CNN that is used to extract features VQA_PERCEPTION_LAYER=\u2019fc7\u2019 # PERCEPTION_LAYER=\u2019pool5-7x7_s1\u2019\nvqa_train_visual_features = vqa_dp[\u2019perception\u2019]( train_or_test=\u2019train\u2019, names_list=vqa_train_text_representation[\u2019img_name\u2019], parts_extractor=None, max_parts=None, perception=VQA_CNN_NAME, layer=VQA_PERCEPTION_LAYER, second_layer=None ) vqa_train_visual_features.shape\nIn [ ]: vqa_val_visual_features = vqa_dp[\u2019perception\u2019]( train_or_test=\u2019val\u2019, names_list=vqa_val_text_representation[\u2019img_name\u2019], parts_extractor=None, max_parts=None, perception=VQA_CNN_NAME, layer=VQA_PERCEPTION_LAYER, second_layer=None )\nvqa_val_visual_features.shape\nIn [ ]: from kraino.core.model_zoo import Config from kraino.core.model_zoo import word_generator\n# dimensionality of embeddings VQA_EMBEDDING_DIM = 1000 # kind of multimodal fusion (ave, concat, mul, sum) VQA_MULTIMODAL_MERGE_MODE = \u2019mul\u2019\nvqa_model_config = Config( textual_embedding_dim=VQA_EMBEDDING_DIM, visual_embedding_dim=VQA_EMBEDDING_DIM, multimodal_merge_mode=VQA_MULTIMODAL_MERGE_MODE, input_dim=len(vqa_word2index_x.keys()), output_dim=len(vqa_word2index_y.keys()), visual_dim=vqa_train_visual_features.shape[1], word_generator=word_generator[\u2019max_likelihood\u2019]) vqa_text_image_bow_model = VisionLanguageBOW(vqa_model_config) vqa_text_image_bow_model.create() vqa_text_image_bow_model.compile(\nloss=\u2019categorical_crossentropy\u2019, optimizer=\u2019adam\u2019)\nIn [ ]: vqa_train_input = [vqa_train_x, vqa_train_visual_features] vqa_val_input = [vqa_val_x, vqa_val_visual_features]\nIn [ ]: #== Model training vqa_text_image_bow_model.fit(\nvqa_train_input, vqa_train_y,\nbatch_size=512, nb_epoch=10, validation_split=0.1, show_accuracy=True)\n# we use maximum likelihood as a word generator vqa_predictions_answers = vqa_text_image_bow_model.decode_predictions(\nX=vqa_val_input, temperature=None, index2word=vqa_index2word_y, verbose=0)\nvqa_vars = { \u2019question_id\u2019:vqa_val_text_representation[\u2019question_id\u2019], \u2019vqa_object\u2019:vqa_val_text_representation[\u2019vqa_object\u2019], \u2019resfun\u2019:\nlambda x: \\ vqa_val_text_representation[\u2019vqa_object\u2019].loadRes(\nx, vqa_val_text_representation[\u2019questions_path\u2019]) }\nIn [ ]: from kraino.utils import print_metrics\n_ = print_metrics.select[\u2019vqa\u2019]( gt_list=vqa_val_raw_y, pred_list=vqa_predictions_answers, verbose=1, extra_vars=vqa_vars)"}, {"heading": "11 New Research Opportunities", "text": "The task that tests machines via questions about the content of images is a quite new research direction that recently has gained popularity. Therefore, many opportunities are available. We end the tutorial by enlisting a few possible directions.\n\u2022 Global Representation In this tutorial, we use a global, full-frame representation of the images. Such a representation may destroy too much information. Therefore, it seems a fine-grained alternatives should be valid options. Maybe we should use detections, or object proposals (e.g. Ilievski et al. [2016] use a question dependent detections, and Mokarian Forooshani et al. [2016] use object proposals to enrich a visual representation). We could also use attention models, which become quite successful in answering questions about images [Lu et al., 2016]. However, there is still a hope for global representations if they are trained end-to-end for the task, and question dependent. In the end, our global representation is extracted from CNNs trained on a different dataset (ImageNet), and for different task (object classification). \u2022 3D Scene Representation Most of current approaches, and all neural-based approaches, are trained on 2D images. However, some spatial relations such as \u2018behind\u2019 may need a 3d representation of the scene (in fact Malinowski and Fritz [2014a] design spatial rules using a 3d coordinate system). DAQUAR is built on top of Silberman et al. [2012] that provides both modes (2D images, and 3D depth), however, such a richer visual information is currently not fully exploited. \u2022 Recurrent Neural Networks There is disturbingly small gap between BOW and RNN models. As we have seen in the tutorial, some questions clearly require an order, but such questions at the same time become longer, semantically more difficult, and require better a visual understanding of the world. To handle them we may need other RNNs architectures, or better ways of fusing two modalities, or better Global Representation. \u2022 Logical Reasoning There are few questions that require a bit more sophisticated logical reasoning such as negation. Can Recurrent Neural Networks learn such logical operators? What about compositionality of the language? Perhaps, we should aim at mixed approaches, similar to the work of Andreas et al. [2016]. \u2022 Language + Vision There is still a small gap between Language Only and Vision + Language models. But clearly, we need pictures to answer questions about images. So what is missing here? Is it due to Global Representation, 3D Scene Representation or there is something missing in fusing two modalities? The latter is studied, with encouraging results, in Fukui et al. [2016].\n\u2022 Learning from Few Examples In the Visual Turing Test, many questions are quite unique. But then how the models can generalize to new questions? What if a question is completely new, but its parts have been already observed (compositionality)? Can models guess the meaning of a new word from its context? \u2022 Ambiguities How to deal with ambiguities? They are all inherent in the task, so cannot be just ignored, and should be incorporated into question answering methods as well as evaluation metrics. \u2022 Evaluation Measures Although we have WUPS and Consensus, both are far from being perfect. Consensus has higher annotation cost for ambiguous tasks, and is unclear how to formally define good consensus measure. WUPS is an ontology dependent, which may be quite costly to build for all interesting domains? Finally, the current evaluation metrics ignore the tail of the answer distribution encouraging models to focus only on a few most frequent answers."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task. 1 Preface In this tutorial1 we build a few architectures that can answer questions about images. The architectures are based on our two papers on this topic: Malinowski et al. [2015] and Malinowski et al. [2016]; and more broadly, on our project towards a Visual Turing Test2. In particular, an encoder-decoder perspective of Malinowski et al. [2016] allows us to effectively experiment with various design choices. For the sake of simplicity, we only consider a classification-based approach to answer questions about images, although an approach that generate answers word-by-word is also studied in the community [Malinowski et al., 2015]. In the tutorial, we mainly focus on the DAQUAR dataset [Malinowski and Fritz, 2014a], but a few possible directions to apply learnt techniques to VQA [Antol et al., 2015] are also pointed. First, we will get familiar with the task of answering questions about images, and a dataset that implements the task (due to a small size, we mainly use DAQUAR as it better serves an educational purpose that we aim at this tutorial). Next, we build a few blind models that answer questions about images without actually seeing such images. Such models already exhibit a reasonable performance as they can effectively learn various biases that exist in a dataset, which we also interpret as learning a common sense knowledge [Malinowski et al., 2015, 2016]. Subsequently, we build a few language+vision models that answer questions based on both a textual and a visual inputs. Finally, we leave the tutorial with a few possible research directions. Technical aspects The tutorial is originally written using Python Notebook, which the reader is welcome to download and use through the tutorial. Instructions necessary to run the Notebook version of this tutorial are provided in the following: https://github.com/mateuszmalinowski/visual_turing_test-tutorial. In this tutorial, we heavily use a Python code, and therefore it is expected the reader either already knows this language, or can quickly learn it. However, we made an effort to make this tutorial approachable to a wider audience. We use Kraino that is a framework prepared for this tutorial in order to simplify the development of the question answering architectures. Under the hood, it uses Theano4 [Bastien et al., 2012] and Keras5 [Chollet, 2015] \u2013 two frameworks to build Deep Learning models. We also use various CNNs representations extracted from images that can be downloaded as explained at the beginning of our Notebook tutorial. We also highlight exercises that a curious reader may attempt to solve. This tutorial was presented for the first time during the 2nd Summer School on Integrating Vision and Language: Deep Learning. http://mpii.de/visual_turing_test https://github.com/mateuszmalinowski/visual_turing_test-tutorial/blob/master/visual_ turing_test.ipynb http://deeplearning.net/software/theano/ https://keras.io 1 ar X iv :1 61 0. 01 07 6v 1 [ cs .C V ] 4 O ct 2 01 6", "creator": "LaTeX with hyperref package"}}}