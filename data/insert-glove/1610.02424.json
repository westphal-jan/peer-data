{"id": "1610.02424", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models", "abstract": "blalock Neural sequence models are widely burgau used screnar to singer-songwriters model re-aligned time - underneath series data morrisson in cockbain many l\u2019art fields. Equally ubiquitous is term-limited the dowagiac usage rehobeth of beam search (estenoz BS) pravachol as an vesconte approximate inference revava algorithm to decode 640480 output sequences adlam from these christadelphian models. rowes BS explores qsa the eglantine search madou space augustulus in skoki a greedy mccormack left - right fashion retaining sorento only ulna the beardslee top - $ convoked B $ candidates - - croatians resulting kopf in sequences integrado that differ cavitt only slightly from superheavy each clemer other. kessai Producing lists tooms of floodplain nearly bastesen identical accusation sequences is not tunick only 4zzz computationally 4th-century wasteful but tiepolo also typically gurdy fails diplomacy to ronconi capture the gifs inherent ballgames ambiguity universitaria of meduna complex ebersberg AI tasks. To overcome this eve.com problem, we streambeds propose \\ emph {deathstars Diverse Beam meegeren Search} (DBS ), horwitz an alternative to BS 120-150 that decodes a minani list nebesar of mediterrean diverse outputs luchas by optimizing for agv a diversity - 123.03 augmented guder objective. We ngd observe that sorg our method finds better mattes top - 1 redrow solutions lepidus by hark controlling silvertown for the exploration houtkin and exploitation aldrington of the search space - - gilliard implying flappers that DBS is 3,120 a \\ emph {better 2,000-strong search hokkaido algorithm }. verlag Moreover, these gains crassipes are skrillex achieved nordwall with neudorf minimal coua computational half-acre or memory garling overhead veroli as compared .384 to beam knaphill search. pettigrew To zambonis demonstrate the m\u1ef9 broad gpw applicability of our method, 546 we present blumlein results pasatiempo on readmitting image finnessey captioning, accountancy machine collotta translation tic and visual abbottabad question vinukonda generation using non-terminal both standard besser quantitative l-carnitine metrics and qualitative pellston human mashingaidze studies. Our babycenter.com method consistently outperforms 116.57 BS nonnie and previously ordinal proposed deese techniques for jihads diverse decoding masetti from leps neural 1834 sequence orofacial models.", "histories": [["v1", "Fri, 7 Oct 2016 20:56:47 GMT  (7887kb,D)", "http://arxiv.org/abs/1610.02424v1", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.CV", "authors": ["ashwin k vijayakumar", "michael cogswell", "ramprasath r selvaraju", "qing sun", "stefan lee", "david crandall", "dhruv batra"], "accepted": false, "id": "1610.02424"}, "pdf": {"name": "1610.02424.pdf", "metadata": {"source": "CRF", "title": "DIVERSE BEAM SEARCH: DECODING DIVERSE SOLUTIONS FROM NEURAL SEQUENCE MODELS", "authors": ["Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasath R. Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra"], "emails": ["ashwinkv@vt.edu", "cogswell@vt.edu", "ram21@vt.edu", "sunqing@vt.edu", "steflee@vt.edu", "djcran@indiana.edu,", "dbatra@vt.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In the last few years, Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs) or more generally, neural sequence models have become the standard choice for modeling time-series data for a wide range of applications such as speech recognition (Graves et al., 2013), machine translation (Bahdanau et al., 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al., 2015; Venugopalan et al., 2015), and visual question answering (Antol et al., 2015). RNN based sequence generation architectures model the conditional probability, Pr(y|x) of an output sequence y = (y1, . . . , yT ) given an input x (possibly also a sequence); where the output tokens yt are from a finite vocabulary, V . Inference in RNNs. Maximum a Posteriori (MAP) inference for RNNs is te task of finding the most likely output sequence given the input. Since the number of possible sequences grows as |V|T , exact inference is NP-hard so approximate inference algorithms like Beam Search (BS) are commonly employed. BS is a heuristic graph-search algorithm that maintains the B top-scoring partial sequences expanded in a greedy left-to-right fashion. Fig. 1 shows a sample BS search tree.\nLack of Diversity in BS. Despite the widespread usage of BS, it has long been understood that solutions decoded by BS are generic and lacking in diversity (Finkel et al., 2006; Gimpel et al., 2013;\nar X\niv :1\n61 0.\n02 42\n4v 1\n[ cs\n.A I]\n7 O\na train\nsteam black locomotive\nis traveling\non\nengine train train\ncoming down a\nthe\ntrain engine\ndown track train tracks traveling is the with near track down\nthrough\ntracks\na with train tracks a\nin\na tracks\nforest lush\na\nan\ntrain steam\nan the\nis engine\nold train\na an\ncoming train\ntrain steam\ntrain black\ntraveling is\nengine locomotive\ntrain and\ndown through\ntrain is\nis white\ntrain a\nis traveling\ncoming on\ntracks\nforest\ndown through\nthe a\nBeam Search\nDiverse Beam Search\nA steam engine train travelling down train tracks. A steam engine train travelling down tracks. A steam engine train travelling through a forest. A steam engine train travelling through a lush green forest. A steam engine train travelling through a lush green countryside A train on a train track with a sky background.\nA steam engine travelling down train tracks. A steam engine train travelling through a forest. An old steam engine train travelling down train tracks. An old steam engine train travelling through a forest. A black train is on the tracks in a wooded area. A black train is on the tracks in a rural area.\nSingle engine train rolling down the tracks. A steam locomotive is blowing steam. A locomotive drives along the tracks amongst trees and bushes. An old fashion train with steam coming out of its pipe. A black and red train moving down a train track.\nAn engine is coming down the train track. Ground Truth Captions\nFigure 1: Comparing image captioning outputs decoded by BS and our method, Diverse Beam Search (DBS) \u2013 we notice that BS captions are near-duplicates with similar shared paths in the search tree and minor variations in the end. In contrast, DBS captions are significantly diverse and similar to the inter-human variability in describing images.\nLi et al., 2015; Li & Jurafsky, 2016). To illustrate this, a comparison of captions provided by humans (bottom) and BS (topmost) are shown in Fig. 1. While this behavior of BS is disadvantageous for many reasons, we highlight the three most crucial ones here:\ni) The production of near-identical beams make BS a computationally wasteful algorithm, with essentially the same computation being repeated for no significant gain in performance.\nii) Due to loss-evaluation mismatch i.e. improvements in posterior-probabilities not necessarily corresponding to improvements in task-specific metrics, it is common practice (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Ferraro et al., 2016) to deliberately throttle BS to become a poorer optimization algorithm by using reduced beam widths. This treatment of an optimization algorithm as a hyper-parameter is not only intellectually dissatisfying but also has a significant practical side-effect \u2013 it leads to the decoding of largely bland, generic, and \u201csafe\u201d outputs, e.g. always saying \u201cI don\u2019t know\u201d in conversation models (Corrado, 2015).\niii) Most importantly, lack of diversity in the decoded solutions is fundamentally crippling in AI problems with significant ambiguity \u2013 e.g. there are multiple ways of describing an image or responding in a conversation that are \u201ccorrect\u201d and it is important to capture this ambiguity by finding several diverse plausible hypotheses.\nOverview and Contributions. To address these shortcomings, we propose Diverse Beam Search (DBS) \u2013 a general framework to decode a list of diverse sequences that can be used as an alternative to BS. At a high level, DBS decodes diverse lists by dividing the beam budget into groups and enforcing diversity between groups of beams. Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms \u2013 the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ. This diversity-augmented model score is optimized in a doubly greedy manner \u2013 greedily optimizing along both time (like BS) and groups (like DivMBest).\nTo summarize, our primary technical contribution is Diverse Beam Search, a doubly greedy approximate inference algorithm for decoding diverse sequences. Our method consistently outperforms BS while being comparable in terms of both run-time and memory requirements. We report results on image captioning, machine translation and visual question generation to demonstrate the broad applicability of DBS. We find that DBS results in improvements on both oracle task-specific and diversity-related metrics against baselines. We conduct human studies to evaluate the role of diversity in human preferences between BS and DBS for image captions. We also analyze the parameters of DBS and show they are robust over a wide range of values. Finally, we also show that our method is general enough to incorporate various forms for the dissimilarity term.\nOverall, our algorithm is simple to implement and consistently outperforms BS in a wide range of domains without sacrificing efficiency. Our implementation is available at https://github.\ncom/ashwinkalyan/dbs. Also, a demo of DBS on image-captioning is available at dbs. cloudcv.org."}, {"heading": "2 PRELIMINARIES: DECODING RNNS WITH BEAM SEARCH", "text": "We begin with a refresher on BS, before describing our generalization, Diverse Beam Search. For notational convenience, let [n] denote the set of natural numbers from 1 to n and let v[n] index the first n elements of a vector v \u2208 Rm,. The Decoding Problem. RNNs are trained to estimate the likelihood of sequences of tokens from a finite dictionary V given an input x. The RNN updates its internal state and estimates the conditional probability distribution over the next output given the input and all previous output tokens. We denote the logarithm of this conditional probability distribution over all tokens at time t as \u03b8(yt) = log Pr(yt|yt\u22121, . . . , y1,x). To simplify notation, we index \u03b8(\u00b7) with a single variable yt; but it should be clear that it depends on the previous outputs, y[t\u22121] from the context. The log-probability of a partial solution (i.e. the sum of log-probabilities of all previous tokens decoded) can now be written as \u0398(y[t]) = \u2211 \u03c4\u2208[t] \u03b8(y\u03c4 ). The decoding problem is then the task of finding a sequence y that maximizes \u0398(y).\nAs each output is conditioned on all the previous outputs, decoding the optimal length-T sequence in this setting can be viewed as MAP inference on T -order Markov chain with the T nodes corresponding to output tokens. Not only does the size of the largest factor in such a graph grow as |V|T , but also requires wastefully forwarding of the RNN repeatedly to compute entries in the factors. Thus, approximate algorithms are employed.\nBeam Search. The most prevalent method for approximate decoding is BS, which stores the top-B highly scoring candidates at each time step; where B is known as the beam width. Let us denote the set of B solutions held by BS at the start of time t as Y[t\u22121] = {y1,[t\u22121], . . . ,yB,[t\u22121]}. At each time step, BS considers all possible single token extensions of these beams given by the set Yt = Y[t\u22121] \u00d7 V and selects the B most likely extensions. More formally, at each step,\nY[t] = argmax y1,[t],...,yB,[t]\u2208Yt \u2211 b\u2208[B] \u0398(yb,[t]) s.t. yi,[t] 6= yj,[t] (1)\nThe above objective can be trivially maximized by sorting all B \u00d7 |V| members of Yt by their log-probabilities and selecting the top-B. This process is repeated until time T and the most likely sequence is selected by ranking the B beams based on log-probabilities.\nWhile this method allows for multiple sequences to be explored in parallel, most completions tend to stem from a single highly valued beam \u2013 resulting in outputs that are typically only minor perturbations of a single sequence."}, {"heading": "3 DIVERSE BEAM SEARCH: FORMULATION AND ALGORITHM", "text": "To overcome this shortcoming, we consider augmenting the objective in Eq. 1 with a dissimilarity term \u2206(Y[t]) that measures the diversity between candidate sequences. Jointly optimizing for all B candidates at each time step is intractable as the number of possible solutions grows with |V|B (which can easily reach 1060 for typical language modeling settings). To avoid this joint optimization problem, we divide the beam budget B into G groups and greedily optimize each group using beam search while holding previous groups fixed. This doubly greedy approximation along both time and across groups turns \u2206(Y[t]) into a function of only the current group\u2019s possible extensions. We detail the specifics of our approach in this section.\nDiverse Beam Search. Let Y[t], the set of all B beams at time t be partitioned into G disjoint and non-empty subsets Y g[t], g\u2208[G]. Without loss of generality, consider an equal partition such that each group containsB\u2032 = B/G groups. Beam search can be applied to each group to produceB solutions; however, each group would produce identical outputs.\nFurther consider a modification to the objective in eq. 1 which adds a dissimilarity term \u2206(Y 1[t], . . . , Y g\u22121 [t] )[y] measuring the dissimilarity of group g against prior groups if token y is cho-\nsen to extend any of the beams in the group g. The exact form of \u2206(\u00b7) can vary and discussion of this choice is dealt in Section 5.1. As we optimize each group with the previous groups fixed, extending group g at time t amounts to a standard BS using dissimilarity augmented log-probabilities and can be written as:\nY g[t] = argmax yg 1,[t] ,...,yg B\u2032,[t]\u2208Y g t\n\u2211 b\u2208[B\u2032] \u0398(ygb,[t]) + \u03bbg\u2206 ( Y 1[t], . . . Y g\u22121 [t] ) [ygb,t] (2)\ns.t. ygi,[t] 6= y g j,[t], \u03bbg \u2265 0\nThis approach, which we call Diverse Beam Search (DBS) is detailed in Algorithm 1. An example run of DBS is shown in Figure 2 for decoding image-captions. In the example, B=6 and G=3 and so, each group performs a smaller, diversity-augmented BS of size 2. In the snapshot shown, group 3 is being stepped forward and the diversity augmented score of all words in the dictionary is computed conditioned on previous groups. The score of all words are adjusted by their similarity to previously chosen words \u2013 \u2018birds\u2019, \u2018the\u2019 and \u2018an\u2019 (Algorithm 1, Line 5). The optimal continuations are then found by standard BS (Algorithm 1, Line 6).\nAlgorithm 1: Diverse Beam Search 1 Perform a diverse beam search with G groups using a beam width of B 2 for t = 1, . . . T do\n// perform one step of beam search for first group without diversity 3 Y 1[t] \u2190 argmax(y11,[t],...,y1B\u2032,[t]) \u2211 b\u2208[B\u2032] \u0398(y 1 b,[t]) 4 for g = 2, . . . G do // augment log-probabilities with diversity penalty 5 \u0398(ygb,[t])\u2190 \u0398(y g b,[t]) + \u03bbg\u2206(Y 1 [t], . . . , Y g\u22121 [t] )[y g b,t] b \u2208 [B\u2032],y g b,[t] \u2208 Y\ng and \u03bbg > 0 // perform one step of beam search for the group\n6 Y g[t] \u2190 argmax(yg1,[t],...,ygB\u2032,[t]) \u2211 b\u2208[B\u2032] \u0398(y g b,[t])\n7 Return set of B solutions, Y[T ] = \u22c3G g=1 Y g [T ]\nIn summary, DBS works in a doubly greedy manner enabling us to incorporate diversity in beam search. Moreover, as the first group is not conditioned on other groups during optimization, our method is guaranteed to be at least as good as a beam search of size B/G."}, {"heading": "4 RELATED WORK", "text": "Diverse M-Best Lists. The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014). Batra et al. (2012) formalized this task for Markov Random Fields as the DivMBest problem and presented a greedy approach which solves for outputs iteratively, conditioning on previous solutions to induce diversity. Kirillov et al. (2015) show how these solutions can be found jointly for certain kinds of energy functions. The techniques developed by Kirillov are not directly applicable to decoding from RNNs, which do not satisfy the assumptions made.\nMost related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference.\nDiverse Decoding for RNNs. Some efforts have been made to produce diverse decodings from recurrent models for conversation modeling and machine translation.\nIn this context, our work is closely related to Li & Jurafsky (2016), who propose a BS diversification heuristic to overcome the shortcomings of Gimpel et al. (2013). This discourages sequences from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective as in DBS rather than a heuristic provides easier generalization to incorporate different notions of diversity and control for the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms this method.\nThrough a novel decoding objective that maximizes mutual information between inputs and predicted outputs, Li et al. (2015) penalize decoding generic, input independent sequences. This is achieved by training an additional target language model. Although this work and DBS share the same goals (producing diverse decodings), the techniques developed are disjoint and complementary \u2013 Li et al. (2015) develops a new model (RNN translation model with an RNN target language model), while DBS is a modified inference algorithm that can be applied to any model where BS is applicable. Combination of these complementary techniques is left as interesting future work."}, {"heading": "5 EXPERIMENTS", "text": "We first explain the baselines and evaluation metrics used in this paper. Next, we proceed to the analysis of the effects of DBS parameters. Further, we report results on image-captioning, machine translation and visual question generation. Although results are reported on these tasks, it should be noted that DBS is a task-agnostic algorithm that can replace BS to decode diverse solutions.\nBaselines. We compare with beam search and the following existing methods:\n- Li & Jurafsky (2016): This work modifies BS by introducing an intra-sibling rank. For each partial solution, the set of |V| continuations are sorted and assigned intra-sibling ranks k \u2208 [L] in order of decreasing log-probabilities, \u03b8t(yt). The log-probability of an extenstion is then reduced in proportion to its rank, and continuations are re-sorted under these modified log-probabilities to select the top-B diverse beam extensions.\n- Li et al. (2015): These models are decoded using a modified objective, P (y|x)\u2212 \u03bbU(y), where U(y) is an unconditioned target sequence model. This additional term penalizes generic input independent decoding.\nBoth works use secondary mechanisms such as re-rankers to pick a single solution from the generated lists. As we are interested in evaluating the quality of the generated lists and in isolating the gains due to diverse decoding, we do not implement any re-rankers. Instead, we simply sort the list based on log-probability. We compare to our own implementations of these methods as none are publicly available.\nEvaluation Metrics. We evaluate the performance of the generated lists using the following two metrics that quantify complementary details:\n- Oracle Accuracy: Oracle or top-k accuracy for some task-specific metric like BLEU is the maximum value of the metric over a list of k potential solutions. It is an upper bound on the\npotential impact diversity plays in finding relevant solutions.\n- Diversity Statistics: We count the number of distinct n-grams present in the list of generated outputs. Similar to Li et al. (2015), we divide these counts by the total number of words generated to bias against long sentences.\nSimultaneous improvements in both metrics indicate that output lists have increased diversity without sacrificing fluency and correctness with respect to target tasks. Human preference studies which compare image captions produced by DBS and BS also compare these methods. Finally, We discuss the role of diversity by relating it to intrinsic details contained in images."}, {"heading": "5.1 SENSITIVITY ANALYSIS AND EFFECT OF DIVERSITY FUNCTIONS", "text": "In this section, we study the impact of the number of groups, the strength of diversity penalty, and various forms of diversity functions for language models. Further discussion and experimental details are included in the supplementary materials.\nNumber of Groups (G). SettingG=B allows for the maximum exploration of the space, while setting G=1 reduces our method to BS, resulting in increased exploitation of the search-space around the 1-best decoding. Thus, increasing the number of groups enables us to explore various modes of the model. Empirically, we find that maximum exploration correlates with improved oracle accuracy and hence use G=B to report results unless mentioned otherwise.\nDiversity Strength (\u03bb). The diversity strength \u03bb specifies the trade-off between the joint logprobability and the diversity terms. As expected, we find that a higher value of \u03bb produces a more diverse list; however, excessively high values of \u03bb can overpower model probability and result in grammatically incorrect outputs. We set \u03bb by performing a grid search on the validation set for all experiments. We find a wide range of \u03bb values (0.2 to 0.8) work well for most tasks and datasets.\nChoice of Diversity Function (\u2206). In Section 3, we defined \u2206(\u00b7) as a function over a set of partial solutions that outputs a vector of similarity scores for potential beam completions. Assuming that each of the previous groups can influece the completion of the current group independently, we can simplify the dissimilarity term \u2206(Y 1[t], . . . , Y\ng\u22121 t] ) by summing over each group\u2019s contributions as\u2211g\u22121\nh=1 \u2206(Y h [t]). This factorized term can now take various forms ranging from simple hamming\ndiversity to neural embedding based penalties. We discuss some of these below:\n- Hamming Diversity. This form penalizes the selection of tokens used in previous groups proportional to the number of times it was selected before.\n- Cumulative Diversity. Once two sequences have diverged sufficiently, it seems unnecessary and perhaps harmful to restrict that they cannot use the same words at the same time. To encode this \u2018backing-off\u2019 of the diversity penalty we introduce cumulative diversity which keeps a count of identical words used at every time step, indicative of overall dissimilarity. Specifically, \u2206(Y h[t])[y g [t]] = exp{\u2212( \u2211 \u03c4\u2208t \u2211 b\u2208B\u2032 I[y h b,\u03c4 6=y g b,\u03c4 ])/\u0393} where \u0393 is a temperature parameter control-\nling the strength of the cumulative diversity term and I[\u00b7] is the indicator function. - n-gram Diversity. The current group is penalized for producing the same n-grams as previous\ngroups, regardless of alignment in time \u2013 similar to Gimpel et al. (2013). This is proportional to the number of times each n-gram in a candidate occurred in previous groups. Unlike hamming diversity, n-grams capture higher order structures in the sequences.\n- Neural-embedding Diversity. While all the previous diversity functions discussed above perform exact matches, neural embeddings such as word2vec (Mikolov et al., 2013) can penalize semantically similar words like synonyms. This is incorporated in each of the previous diversity functions by replacing the hamming similarity with a soft version obtained by computing the cosine similarity between word2vec representations. When using with n-gram diversity, the representation of the n-gram is obtained by summing the vectors of the constituent words.\nEach of these various forms encode different notions of diversity. Hamming diversity ensures different words are used at different times, but can be circumvented by small changes in sequence alignment. While n-gram diversity captures higher order statistics, it ignores sentence alignment. Neural-embedding based encodings can be seen as a semantic blurring of either the hamming or n-gram metrics, with word2vec representation similarity propagating diversity penalties not only\nto exact matches but also to close synonyms. We find that using any of the above functions help outperform BS in the tasks we examine; hamming diversity achieves the best oracle performance despite its simplicity. A comparison of the performance of these functions for image-captioning is provided in the supplementary."}, {"heading": "5.2 IMAGE CAPTIONING", "text": "Dataset and Models. We evaluate on two datasets \u2013 COCO (Lin et al., 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing save 200 validation images used to tune hyperparameters. We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository.\nResults. As it can be observed from Table 1, DBS outperforms both BS and Li & Jurafsky (2016) on both datasets. We observe that gains on PASCAL-50S are more pronounced (7.24% and 9.60% Oracle@20 improvements against BS and Li & Jurafsky (2016)) than COCO. This suggests diverse predictions are especially advantageous when there is a mismatch between training and testing sets making DBS a better inference strategy in real-world applications.\nTable 1 also shows the number of distinct n-grams produced by different techniques. Our method produces significantly more distinct n-grams (almost 300% increase in the number of 4-grams produced) as compared to BS. We also note that our method tends to produce slightly longer captions compared to beam search on average. Moreover, on the PASCAL-50S test split we observe that DBS finds more likely top-1 solutions on average \u2013 DBS obtains a maximum log-probability of -6.53 as against -6.91 got by BS of same beam width. While the performance of DBS is guaranteed to be better than a BS of size B/G, this experimental evidence suggests that using DBS as a replacement to BS leads to better or at least comparable performance.\nHuman Studies. To evaluate human preference between captions generated by DBS and BS, we perform a human study via Amazon Mechanical Turk using all 1000 images of PASCAL50S. For each image, both DBS and standard BS captions are shown to 5 different users. They are then asked \u2013 \u201cWhich of the two robots understands the image better displaying intelligent human-like behavior?\u201d In this forced-choice test, DBS captions were preferred over BS 60% of the time.\nIs diversity always needed? While these results show that diversity in outputs is important for systems that interact with consumers, is diversity always beneficial? While images with many objects (e.g., a park or a living room) can be described in multiple ways, the same is not true when there are few objects (e.g., a close up of a cat or a selfie). This notion is studied by Ionescu et al. (2016), which defines a \u201cdifficulty score\u201d: the human response time for solving a visual search task. On the PASCAL50S dataset, we observe a positive correlation (\u03c1 = 0.73) between difficulty scores and humans preferring DBS to BS. While DBS is generally preferred by humans for difficult images, there seems to be no such strong preference between the two in easier images. Details of both the human study and the correlation experiments are provided in the supplementary.\n1https://github.com/karpathy/neuraltalk2"}, {"heading": "5.3 MACHINE TRANSLATION", "text": "Dataset and Models. We use the English-French parallel data from the europarl corpus as the training set. We report results on news-test-2013 and news-test-2014 and use the newstest2012 to tune DBS parameters. We train a encoder-decoder architecture as proposed in Bahdanau et al. (2014) using the dl4mt-tutorial2 code repository. The encoder consists of a bi-directional recurrent network (Gated Recurrent Unit) with attention. We use sentence level BLEU scores (Papineni et al., 2002) to compute oracle metrics and report distinct n-grams similar to image-captioning. From Table 2, we see that DBS consistently outperforms standard baselines with respect to both metrics."}, {"heading": "5.4 VISUAL QUESTION GENERATION", "text": "We also report results on another novel task \u2013 Visual Question Generation (Mostafazadeh et al., 2016). We use the VQA dataset (Antol et al., 2015) to train a model similar to image captioning. Instead of captions, the training set now consists of 3 questions per image. VQA requires the model to reason about multiple problems that are central to vision \u2013 like the position and color of an object, relationships between objects and natural language. Similarly, learning to ask the \u201cright\u201d questions pertinent to the image also requires the model to reason about these finer aspects making question generation an interesting task.\nWhile using beam search to sample outputs results in similarly worded questions (see Fig. 3), DBS brings out the details captured by the model in other modes. This promotes diverse questions of different types as defined by Antol et al. (2015) (see Fig. 3). We observe that the number of question types generated per image increases from 2.3 to 3.7 by employing DBS (at B = 6)."}, {"heading": "6 CONCLUSION", "text": "Beam search is the most commonly used approximate inference algorithm to decode sequences from RNNs; however, it suffers from a lack of diversity. Producing multiple highly similar and generic outputs is not only wasteful in terms of computation but also detrimental for tasks with inherent ambiguity like image captioning. In this work, we presented Diverse Beam Search, which describes\n2https://github.com/nyu-dl/dl4mt-tutorial\nbeam search as an optimization problem and augments the objective with a diversity term. The result is a \u2018doubly greedy\u2019 approximate algorithm that produces diverse decodings while using about the same time and resources as beam search. Our method consistently outperforms beam search and other baselines across all our experiments without extra computation or task-specific overhead. DBS is task-agnostic and can be applied to any case where BS is used \u2013 making it applicable in multiple domains. Our implementation will be made publicly available."}], "references": [{"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Diverse M-Best Solutions in Markov Random Fields", "author": ["Dhruv Batra", "Payman Yadollahpour", "Abner Guzman-Rivera", "Gregory Shakhnarovich"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "Batra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Batra et al\\.", "year": 2012}, {"title": "Computer, respond to this email", "author": ["Greg Corrado"], "venue": "Google Research Blog,", "citeRegEx": "Corrado.,? \\Q2015\\E", "shortCiteRegEx": "Corrado.", "year": 2015}, {"title": "Visual storytelling", "author": ["Francis Ferraro", "Ishan Mostafazadeh", "Nasrinand Misra", "Aishwarya Agrawal", "Jacob Devlin", "Ross Girshick", "Xiadong He", "Pushmeet Kohli", "Dhruv Batra", "C Lawrence Zitnick"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language Technologies (NAACL HLT),", "citeRegEx": "Ferraro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ferraro et al\\.", "year": 2016}, {"title": "Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines", "author": ["Jenny Rose Finkel", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Finkel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2006}, {"title": "A systematic exploration of diversity in machine translation", "author": ["K. Gimpel", "D. Batra", "C. Dyer", "G. Shakhnarovich"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Gimpel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "How hard can it be? Estimating the difficulty of visual search in an image", "author": ["Radu Tudor Ionescu", "Bogdan Alexe", "Marius Leordeanu", "Marius Popescu", "Dim Papadopoulos", "Vittorio Ferrari"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Ionescu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ionescu et al\\.", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Inferring m-best diverse labelings in a single one", "author": ["Alexander Kirillov", "Bogdan Savchynskyy", "Dmitrij Schlesinger", "Dmitry Vetrov", "Carsten Rother"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kirillov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kirillov et al\\.", "year": 2015}, {"title": "Mutual information and diverse decoding improve neural machine translation", "author": ["Jiwei Li", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1601.00372,", "citeRegEx": "Li and Jurafsky.,? \\Q2016\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language Technologies (NAACL HLT),", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Generating natural questions about an image", "author": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende"], "venue": "Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL),", "citeRegEx": "Mostafazadeh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL),", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "N-best maximal decoders for part models", "author": ["Dennis Park", "Deva Ramanan"], "venue": "In Proceedings of IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Park and Ramanan.,? \\Q2011\\E", "shortCiteRegEx": "Park and Ramanan.", "year": 2011}, {"title": "Submodular meets structured: Finding diverse subsets in exponentially-large structured item sets", "author": ["Adarsh Prasad", "Stefanie Jegelka", "Dhruv Batra"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Prasad et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2014}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "In the last few years, Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs) or more generally, neural sequence models have become the standard choice for modeling time-series data for a wide range of applications such as speech recognition (Graves et al., 2013), machine translation (Bahdanau et al.", "startOffset": 261, "endOffset": 282}, {"referenceID": 1, "context": ", 2013), machine translation (Bahdanau et al., 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 21, "context": ", 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al., 2015; Venugopalan et al., 2015), and visual question answering (Antol et al.", "startOffset": 80, "endOffset": 128}, {"referenceID": 19, "context": ", 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al., 2015; Venugopalan et al., 2015), and visual question answering (Antol et al.", "startOffset": 80, "endOffset": 128}, {"referenceID": 0, "context": ", 2015), and visual question answering (Antol et al., 2015).", "startOffset": 39, "endOffset": 59}, {"referenceID": 21, "context": "improvements in posterior-probabilities not necessarily corresponding to improvements in task-specific metrics, it is common practice (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Ferraro et al., 2016) to deliberately throttle BS to become a poorer optimization algorithm by using reduced beam widths.", "startOffset": 134, "endOffset": 204}, {"referenceID": 4, "context": "improvements in posterior-probabilities not necessarily corresponding to improvements in task-specific metrics, it is common practice (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Ferraro et al., 2016) to deliberately throttle BS to become a poorer optimization algorithm by using reduced beam widths.", "startOffset": 134, "endOffset": 204}, {"referenceID": 3, "context": "always saying \u201cI don\u2019t know\u201d in conversation models (Corrado, 2015).", "startOffset": 52, "endOffset": 67}, {"referenceID": 2, "context": "Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms \u2013 the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ.", "startOffset": 117, "endOffset": 181}, {"referenceID": 17, "context": "Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms \u2013 the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ.", "startOffset": 117, "endOffset": 181}, {"referenceID": 10, "context": "Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms \u2013 the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ.", "startOffset": 117, "endOffset": 181}, {"referenceID": 2, "context": "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014).", "startOffset": 105, "endOffset": 191}, {"referenceID": 10, "context": "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014).", "startOffset": 105, "endOffset": 191}, {"referenceID": 17, "context": "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014).", "startOffset": 105, "endOffset": 191}, {"referenceID": 2, "context": "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014). Batra et al. (2012) formalized this task for Markov Random Fields as the DivMBest problem and presented a greedy approach which solves for outputs iteratively, conditioning on previous solutions to induce diversity.", "startOffset": 128, "endOffset": 213}, {"referenceID": 2, "context": "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014). Batra et al. (2012) formalized this task for Markov Random Fields as the DivMBest problem and presented a greedy approach which solves for outputs iteratively, conditioning on previous solutions to induce diversity. Kirillov et al. (2015) show how these solutions can be found jointly for certain kinds of energy functions.", "startOffset": 128, "endOffset": 432}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm.", "startOffset": 49, "endOffset": 70}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference.", "startOffset": 49, "endOffset": 1046}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference. Diverse Decoding for RNNs. Some efforts have been made to produce diverse decodings from recurrent models for conversation modeling and machine translation. In this context, our work is closely related to Li & Jurafsky (2016), who propose a BS diversification heuristic to overcome the shortcomings of Gimpel et al.", "startOffset": 49, "endOffset": 1442}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference. Diverse Decoding for RNNs. Some efforts have been made to produce diverse decodings from recurrent models for conversation modeling and machine translation. In this context, our work is closely related to Li & Jurafsky (2016), who propose a BS diversification heuristic to overcome the shortcomings of Gimpel et al. (2013). This discourages sequences from sharing common roots, implicitly resulting in diverse lists.", "startOffset": 49, "endOffset": 1539}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference. Diverse Decoding for RNNs. Some efforts have been made to produce diverse decodings from recurrent models for conversation modeling and machine translation. In this context, our work is closely related to Li & Jurafsky (2016), who propose a BS diversification heuristic to overcome the shortcomings of Gimpel et al. (2013). This discourages sequences from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective as in DBS rather than a heuristic provides easier generalization to incorporate different notions of diversity and control for the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms this method. Through a novel decoding objective that maximizes mutual information between inputs and predicted outputs, Li et al. (2015) penalize decoding generic, input independent sequences.", "startOffset": 49, "endOffset": 2057}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference. Diverse Decoding for RNNs. Some efforts have been made to produce diverse decodings from recurrent models for conversation modeling and machine translation. In this context, our work is closely related to Li & Jurafsky (2016), who propose a BS diversification heuristic to overcome the shortcomings of Gimpel et al. (2013). This discourages sequences from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective as in DBS rather than a heuristic provides easier generalization to incorporate different notions of diversity and control for the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms this method. Through a novel decoding objective that maximizes mutual information between inputs and predicted outputs, Li et al. (2015) penalize decoding generic, input independent sequences. This is achieved by training an additional target language model. Although this work and DBS share the same goals (producing diverse decodings), the techniques developed are disjoint and complementary \u2013 Li et al. (2015) develops a new model (RNN translation model with an RNN target language model), while DBS is a modified inference algorithm that can be applied to any model where BS is applicable.", "startOffset": 49, "endOffset": 2333}, {"referenceID": 12, "context": "- Li et al. (2015): These models are decoded using a modified objective, P (y|x)\u2212 \u03bbU(y), where U(y) is an unconditioned target sequence model.", "startOffset": 2, "endOffset": 19}, {"referenceID": 12, "context": "Similar to Li et al. (2015), we divide these counts by the total number of words generated to bias against long sentences.", "startOffset": 11, "endOffset": 28}, {"referenceID": 13, "context": "While all the previous diversity functions discussed above perform exact matches, neural embeddings such as word2vec (Mikolov et al., 2013) can penalize semantically similar words like synonyms.", "startOffset": 117, "endOffset": 139}, {"referenceID": 6, "context": "The current group is penalized for producing the same n-grams as previous groups, regardless of alignment in time \u2013 similar to Gimpel et al. (2013). This is proportional to the number of times each n-gram in a candidate occurred in previous groups.", "startOffset": 127, "endOffset": 148}, {"referenceID": 18, "context": ", 2014) and PASCAL-50S (Vedantam et al., 2015).", "startOffset": 23, "endOffset": 46}, {"referenceID": 21, "context": "We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository.", "startOffset": 28, "endOffset": 50}, {"referenceID": 17, "context": ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO.", "startOffset": 24, "endOffset": 105}, {"referenceID": 17, "context": ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing save 200 validation images used to tune hyperparameters. We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. As it can be observed from Table 1, DBS outperforms both BS and Li & Jurafsky (2016) on both datasets.", "startOffset": 24, "endOffset": 393}, {"referenceID": 17, "context": ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing save 200 validation images used to tune hyperparameters. We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. As it can be observed from Table 1, DBS outperforms both BS and Li & Jurafsky (2016) on both datasets. We observe that gains on PASCAL-50S are more pronounced (7.24% and 9.60% Oracle@20 improvements against BS and Li & Jurafsky (2016)) than COCO.", "startOffset": 24, "endOffset": 543}, {"referenceID": 17, "context": ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing save 200 validation images used to tune hyperparameters. We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. As it can be observed from Table 1, DBS outperforms both BS and Li & Jurafsky (2016) on both datasets. We observe that gains on PASCAL-50S are more pronounced (7.24% and 9.60% Oracle@20 improvements against BS and Li & Jurafsky (2016)) than COCO. This suggests diverse predictions are especially advantageous when there is a mismatch between training and testing sets making DBS a better inference strategy in real-world applications. Table 1 also shows the number of distinct n-grams produced by different techniques. Our method produces significantly more distinct n-grams (almost 300% increase in the number of 4-grams produced) as compared to BS. We also note that our method tends to produce slightly longer captions compared to beam search on average. Moreover, on the PASCAL-50S test split we observe that DBS finds more likely top-1 solutions on average \u2013 DBS obtains a maximum log-probability of -6.53 as against -6.91 got by BS of same beam width. While the performance of DBS is guaranteed to be better than a BS of size B/G, this experimental evidence suggests that using DBS as a replacement to BS leads to better or at least comparable performance. Table 1: Oracle accuracy and distinct n-grams on COCO and PASCAL-50S datasets for image captioning at B = 20. Although we report CIDEr, we observe similar trends in other standard metrics. Dataset Method Oracle Accuracy (CIDEr) Diversity Statistics @1 @5 @10 @20 distinct-1 distinct-2 distinct-3 distinct-4 Beam Search 0.5379 0.8394 0.9670 1.0763 0.40 1.51 3.25 5.67 Li & Jurafsky (2016) 0.", "startOffset": 24, "endOffset": 1859}, {"referenceID": 12, "context": "44 Li et al. (2015) 0.", "startOffset": 3, "endOffset": 20}, {"referenceID": 12, "context": "44 Li et al. (2015) 0.4980 0.8135 0.9687 1.0737 0.42 1.37 3.46 6.10 Beam Search 0.8727 1.2174 1.3346 1.4098 0.12 0.57 1.35 2.50 Li & Jurafsky (2016) 0.", "startOffset": 3, "endOffset": 149}, {"referenceID": 12, "context": "44 Li et al. (2015) 0.4980 0.8135 0.9687 1.0737 0.42 1.37 3.46 6.10 Beam Search 0.8727 1.2174 1.3346 1.4098 0.12 0.57 1.35 2.50 Li & Jurafsky (2016) 0.9142 1.1133 1.1694 1.1914 0.15 0.97 2.43 5.31 COCO DBS 0.8688 1.2338 1.3568 1.4288 0.18 1.26 3.67 7.33 Li et al. (2015) 0.", "startOffset": 3, "endOffset": 271}, {"referenceID": 8, "context": "This notion is studied by Ionescu et al. (2016), which defines a \u201cdifficulty score\u201d: the human response time for solving a visual search task.", "startOffset": 26, "endOffset": 48}, {"referenceID": 15, "context": "We use sentence level BLEU scores (Papineni et al., 2002) to compute oracle metrics and report distinct n-grams similar to image-captioning.", "startOffset": 34, "endOffset": 57}, {"referenceID": 1, "context": "We train a encoder-decoder architecture as proposed in Bahdanau et al. (2014) using the dl4mt-tutorial2 code repository.", "startOffset": 55, "endOffset": 78}, {"referenceID": 1, "context": "We train a encoder-decoder architecture as proposed in Bahdanau et al. (2014) using the dl4mt-tutorial2 code repository. The encoder consists of a bi-directional recurrent network (Gated Recurrent Unit) with attention. We use sentence level BLEU scores (Papineni et al., 2002) to compute oracle metrics and report distinct n-grams similar to image-captioning. From Table 2, we see that DBS consistently outperforms standard baselines with respect to both metrics. Table 2: Quantitative results on En-Fr machine translation on the newstest-2013 dataset (atB = 20). Although we report BLEU-4 values, we find similar trends hold for lower BLEU metrics as well. Method Oracle Accuracy (BLEU-4) Diversity Statistics @1 @5 @10 @20 distinct-1 distinct-2 distinct-3 distinct-4 Beam Search 13.52 16.67 17.63 18.44 0.04 0.75 2.10 3.23 Li & Jurafsky (2016) 13.", "startOffset": 55, "endOffset": 846}, {"referenceID": 1, "context": "We train a encoder-decoder architecture as proposed in Bahdanau et al. (2014) using the dl4mt-tutorial2 code repository. The encoder consists of a bi-directional recurrent network (Gated Recurrent Unit) with attention. We use sentence level BLEU scores (Papineni et al., 2002) to compute oracle metrics and report distinct n-grams similar to image-captioning. From Table 2, we see that DBS consistently outperforms standard baselines with respect to both metrics. Table 2: Quantitative results on En-Fr machine translation on the newstest-2013 dataset (atB = 20). Although we report BLEU-4 values, we find similar trends hold for lower BLEU metrics as well. Method Oracle Accuracy (BLEU-4) Diversity Statistics @1 @5 @10 @20 distinct-1 distinct-2 distinct-3 distinct-4 Beam Search 13.52 16.67 17.63 18.44 0.04 0.75 2.10 3.23 Li & Jurafsky (2016) 13.63 17.11 17.50 18.34 0.04 0.81 2.92 4.61 DBS 13.69 17.51 17.80 18.77 0.06 0.95 3.67 5.54 Li et al. (2015) 13.", "startOffset": 55, "endOffset": 955}, {"referenceID": 14, "context": "We also report results on another novel task \u2013 Visual Question Generation (Mostafazadeh et al., 2016).", "startOffset": 74, "endOffset": 101}, {"referenceID": 0, "context": "We use the VQA dataset (Antol et al., 2015) to train a model similar to image captioning.", "startOffset": 23, "endOffset": 43}, {"referenceID": 0, "context": "We use the VQA dataset (Antol et al., 2015) to train a model similar to image captioning. Instead of captions, the training set now consists of 3 questions per image. VQA requires the model to reason about multiple problems that are central to vision \u2013 like the position and color of an object, relationships between objects and natural language. Similarly, learning to ask the \u201cright\u201d questions pertinent to the image also requires the model to reason about these finer aspects making question generation an interesting task. While using beam search to sample outputs results in similarly worded questions (see Fig. 3), DBS brings out the details captured by the model in other modes. This promotes diverse questions of different types as defined by Antol et al. (2015) (see Fig.", "startOffset": 24, "endOffset": 771}], "year": 2016, "abstractText": "Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-B candidates \u2013 resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space \u2013 implying that DBS is a better search algorithm. Moreover, these gains are achieved with minimal computational or memory overhead as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.", "creator": "LaTeX with hyperref package"}}}