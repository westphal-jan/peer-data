{"id": "1610.02544", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2016", "title": "Computational linking theory", "abstract": "indovina A linking theory explains vt100 how cocalico verbs ' dhalla semantic 90.16 arguments are l\u00e4n mapped to their 2,487 syntactic arguments - - - wrn the henan inverse of usgs the classis Semantic ugta Role Labeling 6-game task mammadov from songhai the shingo shallow semantic falaschi parsing literature. In leebaw this zynga paper, carmeli we develop kanka the winshape Computational Linking Theory jolbert framework vaals as bruttium a velits method for implementing 11.3 and testing linking malekula theories bathypelagic proposed in the capitalists theoretical literature. rolando We husar deploy this framework liviu to cecily assess 21,900 two cross - nazeri cutting 9.525 types cipollina of edgley linking theory: local chaulnes v. farad global models kholmogory and categorical beilstein v. thebaid featural models. preassembled To mansoura further nonmembers investigate tetri the bograd behavior dumesnil of cingulate these catch-all models, we 26.86 develop 57mm a abadi measurement model word-final in the spirit liddell of subcabinet previous clewiston work 116.69 in cordovez semantic 84.43 role induction: the Semantic Proto - kq Role Linking Model. fifty-eight We use this convorbiri model, which bayan implements a generalization kinnersley of Dowty ' 69.12 s seminal 2.33 Proto - standard-sized Role inverlochy Theory, to induce semantic okrika proto - roles, adcock which we bosnich compare 46.63 to upgrading those Dowty proposes.", "histories": [["v1", "Sat, 8 Oct 2016 15:24:50 GMT  (88kb,D)", "http://arxiv.org/abs/1610.02544v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aaron steven white", "drew reisinger", "rachel rudinger", "kyle rawlins", "benjamin van durme"], "accepted": false, "id": "1610.02544"}, "pdf": {"name": "1610.02544.pdf", "metadata": {"source": "CRF", "title": "Computational linking theory", "authors": ["Aaron Steven White", "Drew Reisinger", "Rachel Rudinger", "Kyle Rawlins", "Benjamin Van Durme"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "A linking theory explains how verbs\u2019 semantic arguments are mapped to their syntactic arguments (Fillmore, 1970; Zwicky, 1971; Jackendoff, 1972; Carter, 1976; Pinker, 1984, 1989; Grimshaw, 1990; Levin, 1993). For example, the verb hit has three semantic arguments\u2014one for the HITTER, one for the HITTEE, and one for the hitting INSTRUMENT\u2014and for each token of hit, a subset of those semantic arguments are mapped to its syntactic arguments\u2014e.g. subject, direct object, or object of a preposition.\n(1) a. [John]HITTER hit [the fence]HITTEE. b. [The stick]INST hit [the fence]HITTEE.\n(2) a. #[The fence]HITTEE hit [John]HITTER. b. #[The fence]HITTEE hit [the stick]INST.\nThe main desideratum for selecting a linking theory is how well it explains linking regularities: which mappings do and do not occur. One example of a linking regularity is that HITTEE arguments cannot be mapped to subject, suggested by the fact that (1) and (2) cannot mean the same thing.\nThe task of constructing a linking theory that covers the entire lexicon is no small feat. One classic (though not the only) example of this difficulty concerns psych verbs, like fear and frighten (Lakoff, 1970; Postal, 1974; Perlmutter and Postal, 1984; Baker, 1988; Dowty, 1991; Pesetsky, 1995).\n(3) a. [Mary]FEARER feared [John]FEAREE. b. #[John]FEAREE feared [Mary]FEARER.\n(4) a. #[Mary]FEARER frightened [John]FEAREE. b. [John]FEAREE frightened [Mary]FEARER.\nPsych verbs raise issues for theories that disallow mapping FEARER to subject, since fear does that, as well as those that disallow mapping FEAREE to subject, since frighten does that.1\nLinking theory is intimately related to semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Marquez, 2004; Marquez et al., 2008), which is a form of shallow semantic parsing. Where a linking theory maps from semantic arguments to syntactic arguments, an SRL system maps from syntactic arguments to semantic\n1See Hartshorne et al. 2015 for recent work on how children learn psych verbs\u2019 linking regularities.\nar X\niv :1\n61 0.\n02 54\n4v 1\n[ cs\n.C L\n] 8\nO ct\n2 01\narguments. Thus, SRL systems can be thought of as interpreting language, and linking theory implementations can be thought of as generating language.2 But while much work has focused on building widecoverage SRL systems, linking theory has not commanded similar attention.\nIn this paper, we introduce a framework for implementing linking theories\u2014computational linking theory (CLT)\u2014which substantially generalizes an idea first introduced by Grenager and Manning (2006). In CLT, the traditional linking theoretic notion of a mapping from the space of semantic arguments Sem to the space of syntactic arguments Syn is implemented as a classifier.\nThis classifier can take several forms based on the structure of Syn. For instance, following Lang and Lapata (2010), who build on Grenager and Manning 2006, Synmight be a set of syntactic positions, such as {subject, object, . . . }, in which case Sem might be a set of thematic roles, such as {AGENT, PATIENT, . . . }. Another possibility is that Syn is a set of syntactic position sequences such as {(subject, object), (subject, oblique), . . . }, in which case Sem might similarly be a set of thematic role sequences {(AGENT, PATIENT), (AGENT, INST), . . . }, and the classifier would involve structured prediction.\nIn the first part of this paper, we deploy CLT in conjunction with PropBank (Palmer et al., 2005), VerbNet (Kipper-Schuler, 2005), SemLink (Loper et al., 2007), and Reisinger et al.\u2019s (2015) recently released Semantic Proto-Roles version 1 (SPR1) dataset to evaluate the efficacy of various linking theories proposed in the theoretical literature. In the second part, we show that CLT can be useful not only for evaluation, but also for exploratory analysis, by developing a measurement model\u2014the Semantic Proto-Role Linking Model (SPROLIM)\u2014for analyzing computational linking theories. And though our main aim is to compare and explore theoretical proposals using computational tools, we believe that those interested in semantic role induction (SRI) will find this measurement model useful for incorporating independent semantic annotations into SRI.\nWe focus on two cross-cutting types of linking theories that have been proposed in the theoretical\n2See Flanigan et al. 2016 for recent work on semanticsbased language generation with a looser coupling to the syntax.\nliterature: local v. global models and categorical v. featural models. The distinction between local and global models\u2014which, as we discuss in \u00a72, is analogous to the distinction between local and global SRL systems (cf. Toutanova et al., 2005, 2008a)\u2014 contacts a long-standing theoretical debate regarding whether semantic arguments are mapped to syntactic positions independently of other arguments (Baker, 1988) or whether there are dependencies among semantic arguments (Dowty, 1991). There is general consensus among theoreticians that this debate has been won in favor of localist theories\u2014a consensus that we hope to break here.\nThe distinction between categorical and featural models contacts an independent debate as to whether semantic arguments, such as HITTER and HITTEE, fall into discrete semantic role categories, such as AGENT or PATIENT, or whether they are associated to a greater or lesser extent with fuzzy semantic role prototypes, such as PROTOAGENT and PROTOPATIENT (Dowty, 1991). Because the featural models have been far less developed in the theoretical literature\u2014largely due to lack of good methodologies for understanding their behavior\u2014our goal here will be to further develop measurement models for exploring featural theories.\nOur main findings are:\n1. Global models outperform local models (\u00a74) 2. Categorical models outperform featural mod-\nels, particularly for oblique arguments (\u00a74) 3. Dowty\u2019s PROTOAGENT prototype is robustly\ndiscovered by our measurement model, but his PROTOPATIENT prototype appears to be a collection of multiple other prototype roles (\u00a75)\nWe begin with a discussion of related work in the statistical machine translation and shallow semantic parsing literatures, and we give a brief introduction to linking theory (\u00a72). We then describe the three datasets (PropBank, VerbNet, and SPR1) we build on to implement linking models (\u00a73). Based on these data, we implement and test four linking models built from crossing the categorical-featural distinction with the local-global distinction, and we establish the unequivocal superiority of the global models (\u00a74). These experiments reveal challenges faced by the featural model, which we investigate using our Semantic Proto-Role Linking Model (\u00a75)."}, {"heading": "2 Related work", "text": "A linking theory explains how verbs\u2019 semantic arguments are mapped to their syntactic arguments. Various types of theories have been proposed, differing mostly on how they define semantic roles. All of them share the feature that they predict syntactic position based on some aspect of the verb\u2019s semantics."}, {"heading": "2.1 Predicting syntactic position", "text": "The task of predicting an argument\u2019s syntactic position based on some set of linguistic features is not a new one in computational linguistics and natural language processing (cf. Hajic et al., 2004). This problem has been particularly important in the area of statistical machine translation (SMT), where one needs to translate from morphologically poor languages like English to morphologically richer languages like Japanese and German (Koehn, 2005).\nSMT researchers have focused for the most part on using morphological and syntactic predictors. Suzuki and Toutanova (2006, 2007) construct models for predicting Japanese morphological case (which marks syntactic position in languages that have such cases) using intralanguage positional and alignment-based features, and Jeong et al. (2010) extend this line of work to Bulgarian, Czech, and Korean. Koehn and Hoang (2007), Avramidis and Koehn (2008), and Toutanova et al. (2008b) use richer phrase-based features to do the same task.\nOther approaches have incorporated semantic roles into SMT reranking components (Wu and Fung, 2009), similar to the reranking conducted in many SRL systems (cf. Gildea and Jurafsky, 2002; Pradhan et al., 2004, 2005b,a; Toutanova et al., 2005, 2008a, among others), but directly predicting syntactic position has not been explored in SMT (though see Minkov et al. 2007, who suggest using semantic role information in future work)."}, {"heading": "2.2 Semantic role labeling", "text": "A semantic role labeling (SRL) system implements the inverse of a linking theory. Where a linking theory aims to map a verb\u2019s semantic arguments to it syntactic arguments, an SRL system aims to map a verb\u2019s syntactic arguments to its semantic arguments (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Marquez, 2004; Marquez et al., 2008).\nContinuing with examples (1) and (2) from Section 1, a linking theory would need to explain why (and when) HITTERs and INSTRUMENTs, but not HITTEEs, are mapped to subject position; in contrast, an SRL system would need to label the subject position with HITTER or INSTRUMENT (or some abstraction of those roles like A0 or A2) and the object with HITTEE (or some abstraction like A1).\nLocal v. global models Toutanova et al. (2005, 2008a) introduce a distinction between local and global (joint) SRL models. In a local SRL model, a labeling decision is made based on only the features of the argument being labeled, while in a global system, features of the other arguments can be taken into account. The analogous distinction for a linking theory is between local linking models, which predict an argument\u2019s syntactic position based only on that argument\u2019s semantic role, and global linking models, which predict an argument\u2019s syntactic position based on its semantic role along with others\u2019. In \u00a74, we implement both local and global linking models for each representation of Sem we consider.\nSemantic role induction Semantic role annotation is expensive, time-consuming, and hard to scale. This has led to the development of unsupervised SRL systems for semantic role induction (SRI). Work in SRI has tended to focus on using syntactic features to cluster arguments into semantic roles. Swier and Stevenson (2004) introduce the first such system, which uses a bootstrapping procedure to first associate verb tokens with frames containing typed slots (drawn from VerbNet), then iteratively compute probabilities based on cooccurrence counts and fill unfilled slots based on these probabilities.\nGrenager and Manning (2006) introduce the idea of predicting syntactic position based on a latent semantic role representation learned from syntactic and selectional features. Lang and Lapata (2010) expand on Grenager and Manning 2006 by introducing the notion of a canonicalized linking. We discuss these ideas further in \u00a75, incorporating both into our Semantic Proto-Role Linking Model (SPROLIM).\nSyntax-based clustering approaches which do not explicitly attempt to predict syntactic position have also been popular. Lang and Lapata (2011a, 2014) use graph clustering methods and Lang and Lapata (2011b) use a split-merge algorithm to cluster\narguments based on syntactic context. Titov and Klementiev (2011) use a non-parametric clustering method based on the Pitman-Yor Process, and Titov and Klementiev (2012) propose two nonparametric clustering models based on the Chinese Restaurant Process (CRP) and distance dependent CRP."}, {"heading": "2.3 Abstracting semantic roles", "text": "To predict syntactic position, linking theories aim to take advantage of linking regularities. One way theories take advantage of linking regularities is to abstract over semantic arguments in such a way that the abstractions correlate with syntactic position. Two main types of abstraction have been proposed. On the one hand are categorical theories, which group semantic arguments into a finite set of semantic roles\u2014e.g., HITTERs are AGENTs, HITTEEs are PATIENTs, etc.\u2014and then (deterministically) map these categories onto syntactic positions\u2014e.g., subject, direct object, etc. (Fillmore, 1970; Zwicky, 1971; Jackendoff, 1972; Carter, 1976, inter alia; see Levin and Rappaport Hovav 2005; Williams 2015 for a review). In a categorical theory, Sem is thus some set of discrete indices, such as the core NP argument roles assumed in PropBank\u2014i.e., {A0,A1, . . .} (Palmer et al., 2005)\u2014or VerbNet\u2014i.e., {AGENT, PATIENT, . . .} (Kipper-Schuler, 2005).\nOn the other hand are featural theories, which assign each semantic argument a set of feature values based on predicate entailments imposed on that argument. For instance, HITTERs are instigators and are thus assigned [+INSTIGATES]; they need not be volitional and are thus assigned [-VOLITIONAL]; and they are not affected by the event and are thus assigned [-AFFECTED]; in contrast, HITTEEs are [- INSTIGATES], [-VOLITIONAL], and [+AFFECTED]. A featural theory maps from (vectors of) those feature values onto syntactic positions. Thus, in a featural theory, Sem is (or is related to) some set of vectors representing some priveleged set of P entailments\u2014e.g., {0, 1}P , RP , etc.. A dataset that provides such a representation for verb-argument pairs in the Penn Treebank, which is also annotated for PropBank and (partially) for VerbNet roles, was recently made available by Reisinger et al. (2015).3\n3http://decomp.net"}, {"heading": "2.4 Role fragmentation", "text": "Featural theories were initially proposed to remedy certain failings of the categorical theories. In particular, reasonably wide-coverage categorical theories require an ever-growing number of roles to capture linking regularities\u2014a phenomenon that Dowty (1991) refers to as role fragmentation.\nIn the face of role fragmentation, categorical theories have two choices: (i) use a large number of roles or (ii) force the fragmented roles into classes that predict the syntax well but map onto many distinct (possibly non-cohesive) semantic notions. These same choices must be made when developing a resource. For instance, FrameNet (Baker et al., 1998) makes the first choice; PropBank (Palmer et al., 2005) makes the second; and VerbNet (KipperSchuler, 2005) splits the difference.\nFeatural theories remedy the role fragmentation found in categorical theories by positing a small number of properties that a verb might entail about each of its arguments. The properties that hold of a particular argument determine which position it gets mapped to. Different configurations of these properties correspond to a large space of roles. For example, P binary properties generate 2P potential roles. Dowty (1991) proposes the first (and, perhaps, most comprehensive) list of such properties as a part of specifying his proto-role linking theory (PRLT).\nIn PRLT, properties are grouped into two clusters: PROTOAGENT properties and PROTOPATIENT properties. These groupings are named this way for the fact that AGENTs in categorical theories, which tend to get mapped to subject position, tend to have PROTOAGENT properties and PATIENTs, which tend to get mapped to a non-subject position, tend to have PROTOPATIENT properties. With this idea in mind, Dowty proposed that semantic arguments\nwere linked to syntactic arguments by (i) counting the number of PROTOAGENT and PROTOPATIENT properties each argument has and (iia) mapping the argument with the most PROTOAGENT properties to subject or, (iib) if there is a tie for PROTOAGENT properties, mapping the argument with the least PROTOPATIENT properties to subject.\nAn important thing to note before moving on is that, unlike other linking theories, which are concerned with determining more general linking phenomena Dowty concerns himself only with determining which semantic argument is mapped to subject, though he does briefly suggest that his system might extend to selecting which argument is mapped to, e.g., object v. oblique position. We assess such an extension in the experiments described below."}, {"heading": "3 Data", "text": "For all experiments, datasets are based on the thematic role annotations of the Penn Treebank (Marcus et al., 1993) found in PropBank (Palmer et al., 2005) and VerbNet (Kipper-Schuler, 2005), mapped via SemLink (Loper et al., 2007), as well as the Semantic Proto-Roles version 1.0 (SPR1) dataset of crowd-sourced proto-role property annotations (Reisinger et al., 2015).\nThe SPR1 annotations consist of answers to simple questions about how likely, on a five-point scale, it is that particular relational properties hold of arguments of PropBank-annotated verbs (cp. Kako, 2006; Greene and Resnik, 2009; Hartshorne et al., 2013). Reisinger et al. constructed the questions to correspond to each of Dowty\u2019s proto-role properties. Figure 1 shows an example of the protocol.\nWe extracted syntactic position information about each SPR1 annotated argument by mapping Penn Treebank annotations to Universal Dependencies (De Marneffe et al., 2014; Nivre et al., 2015) using the Stanford Dependencies to Universal Dependencies converter available in PyStanfordDependencies.4"}, {"heading": "3.1 Argument and clause filtering", "text": "SPR1 contains only a subset of PTB sentences produced by applying various automated filters (see\n4https://pypi.python.org/pypi/ PyStanfordDependencies\nReisinger et al. 2015 for details). All of our models, including the PropBank- and VerbNet-based models are trained on this subset\u2014or in the case of the VerbNet-based models, a subset thereof (see \u00a73.2). The most important of these filters for current purposes is one that retains only NP core arguments. This is useful here, since linking theories tend to only treat semantic arguments that surface as NPs. In spite of these filters, some non-NPs occur in SPR1. To remedy this, we filter all dependents that do not have a UD dependency label matching nsubj, dobj, iobj, or nmod."}, {"heading": "3.2 VerbNet subset", "text": "VerbNet contains role annotations for only a subset of the SPR1 data. This has to do with the fact that only a subset of the verbs in the PTB that are annotated with PropBank rolesets are also annotated with VerbNet verb classes. A further subset of these verbs also have their arguments (whose spans are defined by PropBank) annotated with VerbNet semantic roles. Thus, there are three kinds of verbs in the corpus: those with no VerbNet annotation, those annotated only with verb class, and those with both verb class and semantic role annotations. We apply our models both to the full set of SPR1 annotations as well as the VerbNet-annotated subset."}, {"heading": "3.3 Final datasets", "text": "Table 1 gives the counts for each syntactic position after the preprocessing steps listed above, and Ta-\nble 2 gives the analogous counts for each clause type. The argument type counts show only subjects (nsubj), direct objects (dobj), indirect objects (iobj), and obliques (nmod) because all other arguments are filtered out as non-core arguments.\nThere are few indirect objects\u2014i.e., first direct objects in a double object construction\u2014in this dataset. This is problematic for the cross-validation we employ in \u00a74. To remedy this, we collapse the indirect object label to the direct object label. This is justified linguistically, since indirect objects in the sense employed in Universal Dependencies (and most other dependency parse standards) is really just a subtype of direct object."}, {"heading": "4 Evaluating linking models", "text": "In this section, we implement categorical and featural linking models by constructing classifiers that predict the syntactic position (subject, direct object, oblique) of an argument based either on that argument\u2019s thematic role (e.g. AGENT, PATIENT, etc.) or on the entailments that the verb requires of that argument (e.g. INSTIGATION, VOLITION, etc.).\nFor each type of predictor, two linear classifiers are constructed to instantiate (i) a local linking model (Experiment 1), which predicts syntactic position irrespective of other arguments, and (ii) a global linking model (Experiment 2), which predicts syntactic position relative to other arguments. In both experiments, the PropBank- and SPR1-based models are fit to the full SPR1 dataset, and all three models are fit to the VerbNet-annotated subset.\nThe featural models\u2014in particular, the global featural model\u2014can be seen as a generalization of Dowty\u2019s proto-role model. Like Dowty\u2019s model, it groups properties based on how predictive they are of particular syntactic positions\u2014e.g., PROTOAGENT properties are predictive of subject position. They are a generalization in two senses: (i) for Dowty, each role is weighted equally\u2014one simply counts how many of each kind of property hold\u2014 while here, these properties can receive distinct weights (and are ordinal- rather than binary-valued); and (ii) instead of predicting only subject v. nonsubject; we attempt to also differentiate among nonsubjects\u2014i.e., direct object v. obliques. another way of thinking about the featural models is that they,\nlike Dowty\u2019s model, admit of role prototypes with piecewise linear boundaries in the property space.\nThe main findings in this section are that (i) the global models substantially improve upon the local models for both categorical and featural predictors and (ii) the featural models perform worse overall than the categorical models. The first finding argues against local linking theories like that proposed by Baker (1988). We hypothesize that the second finding has two sources: (i) the set of properties in SPR1, which are essentially just Dowty\u2019s properties, is insufficient for capturing distinctions among nonsubject positions like direct object and oblique\u2014 likely because Dowty engineered his properties only to distinguish subjects from non-subjects; and (ii) because the models we use don\u2019t capture multimodality in the kinds of property configurations that exist. We explore this second possibility in \u00a75."}, {"heading": "4.1 Classifiers", "text": "L2-regularized maximum entropy models were used for classification in both Experiments 1 and 2. Experiment 1 uses simple logistic regression, and Experiment 2 uses a conditional random field (CRF) analogous to the logistic regression used for Experiment 1, but containing factors for each pair of arguments (see Figure 2 for a three argument example).\nExperiment 1 Syntactic position si \u2208 {subject, object, oblique} was used as the dependent variable and either thematic role or property configuration as predictors. Entailment judgments from SPR1 were represented as a vector li of likelihood ratings\nlij \u2208 {1, 2, 3, 4, 5} for each potential entailment j. Ratings lij in SPR1 are furthermore associated with values aij \u2208 {0, 1} corresponding to the applicability of a particular entailment question. If a question iwas annotated as not applicable (aij = 0), the combined rating rij was set to aijlij = 0. Because lij is strictly positive, by setting these ratings to 0, the classifier is effectively forced to project a probability from only the feature subspace corresponding to the applicable questions.\nExperiment 2 The sequence of syntactic positions in each clause was used as the dependent variable. For instance, (5) would be labeled {subj, obj, obl}.\n(5) [The bill]subj also imposes [the California auto-emissions standards]object on [all cars nationwide]oblique.\nThese sequences were predicted using the CRF corresponding to the factor graph in Figure 2. Because the maximum number of core NP arguments and syntactic position types for any verb token is relatively small, exact inference for s is possible in our case by enumerating all configurations in the relevant cartesian product of syntactic positions S\u2014S2 for a two-argument verb, S3 for a three-argument verb, etc.\u2014and computing their probabilities explicitly.5\n5We suspect that this strategy will generally be possible since the vast majority of verbs have only one or two core arguments (see Table 2). Furthermore, even if the number of"}, {"heading": "4.2 Cross-validation", "text": "Nested stratified cross-validation with 10-folds at both levels of nesting was used to validate and test the models in both experiments. For each of the 10 folds in the outer CV, the L2 regularization parameter \u03b1 was set using grid search over \u03b1 \u2208 {0.01, 0.1, 1, 2, 5, 10} on the 10 folds of the inner CV with highest average F1 as the selection criterion. For each of the outer folds, the model with this optimal \u03b1 was refit to the full training set on that fold and tested on that fold\u2019s held-out data. In Experiment 2, a further constraint was imposed that folds not separate arguments in the same sentence.\nAll reported F1, precision, and recall values are computed from testing on the outer held-out sets, and all error analyses are conducted on errors when an item was found in an outer held-out set."}, {"heading": "4.3 Results", "text": "Table 3 gives the mean F1, precision, and recall on the outer cross-validation test folds on both full SPR1 dataset and the VerbNet subset. There are two relevant patterns. First, across all predictor sets, the three measures for the global models improve substantially compared to the local models. This suggests that, regardless of one\u2019s representation of argument semantics, global models are to be preferred\narguments were high on average, most configurations are not possible\u2014e.g., one never finds two subjects. These sorts of syntax-aware constraints can significantly cut down the space of configurations that need to be considered.\nover local models. It furthermore suggests that the current trend in theoretical linguistics to prefer local models likely needs to be reassessed.\nSecond, we find that the featural models do worse than the categorical models, particularly comparing the respective local models, but that this gap is closed to some extent when considering the respective global models. This change is particularly apparent for obliques, for which the local featural model\u2019s performance is abysmal and the global featural model\u2019s performance is middling, at least when validated on the full dataset.6 As such, in the remainder of this section, we focus in on understanding the behavior of the global models.\nBecause these models are fairly simple, it is straightforward to analyze how they make their predictions by looking at their parameters. Figure 3 shows a heatmap of the mean coefficients for each global model across the 10 outer CV folds.\nTurning first to the coefficients relating the syntactic positions to each other, we see that, across all models, each syntactic position disprefers occurring with a syntactic position of the same kind, and this dispreference is particularly strong for subjects. This makes sense in that we never find a sentence with two subjects, and duplicates of the other syntactic positions are relatively rare, only occurring in double object constructions or sentences with multiple prepositions. On the other hand, unlike syntactic positions attract, with strongest preference for sentences containing a non-subject to have a subject.7\nTurning next to the categorical roles, we see that the majority of roles either prefer subject position or are agnostic, but none disprefer it. In contrast, all disprefer object to some extent. This likely arises because each of these roles can occur in intransitives, which always place their single argument in subject position.8 This pattern also gives rise to\n6There is a substantial decrement in F1 for the obliques when comparing the global featural model on the full dataset and VerbNet subset. This seems to arise from higher confusion with subject. We address this in the error analysis.\n7This is likely due to the fact that English requires subjects; in a language that has no such requirement, we might expect a different pattern.\n8There are languages that show, e.g., distinct case-marking behavior for different intransitives (Perlmutter, 1978; Burzio, 1986; Levin and Hovav, 1995; Hale and Keyser, 2002, see also Stevenson and Merlo 1999a,b), and so we might again expect a\nan ordering on the roles with respect to which role will be mapped to subject position when others are present\u2014e.g., A0 and AGENT will be mapped to subject before any other role. This is reminiscent of popular proposals from the theoretical literature regarding role hierarchies (see Levin and Rappaport Hovav 2005 for a review), and errors arise when that ordering is violated. For instance, because A2 does not disprefer objects as much as A1, (A1, A2) is mapped to (object, subject) by the model, when they should be mapped to (subject, oblique).\n(6) [Approximately 85% of the total]A1 consisted [of nonperforming commercial real estate assets]A2.\nThe featural model coefficients are slightly harder to interpret. We see that being likely to INSTIGATE,\ndifferent pattern in those languages.\none of Dowty\u2019s PROTOAGENT properties, matters a lot for being mapped to subject position and not being mapped to object, but the rest of the relationships are quite weak and their relationship with subject position doesn\u2019t match well with the predictions of a featural theory such as Dowty\u2019s. For instance, CHANGE OF STATE is supposed to be a PROTOPATIENT property under his theory, but here it has its highest weight on subject. This appears to produce problems for many of the same sentences that the categorical models fail on\u2014the featural model fails on 75% of the sentences the PropBank model fails on and 64% of the sentences the VerbNet model fails on\u2014but it also produces problems for psych verbs and their kin\u2014e.g., (7)\u2014whose subjects are traditionally referred to as EXPERIENCERS.\n(7) The real-estate market suffered even more severe setbacks.\nThe likely reason for this is that, as for the categorical models, the featural model must capture the fact that subjects of intransitives can be arguments that might occur in subject position in the presence of a \u2018better\u2019 subject. This might be further worsened by the existence of distinct clusters of properties that the model has no way of capturing. But because this representation doesn\u2019t provide a sense for which of the 618 different combinations are extant, it is hard to tell what kinds of role combinations there are and, thus, which fall into each category. We could obtain a rough estimate of this by taking some statistic over the arguments that are classified as subject, object, and oblique, but this would fail to capture categories with multimodality in the property space. If fully spelled out categorical theories are even a good approximation, one would expect such multimodality."}, {"heading": "4.4 Discussion", "text": "In this section, we established (i) that global models substantially improve upon the local models for both categorical and featural predictors and (ii) that featural models perform worse overall than the categorical models. In the next section, we demonstrate CLT\u2019s use as a framework for exploring linking theories\u2014particularly, featural linking theories\u2014by developing a measurement model that addresses the multimodality issue raised in this section."}, {"heading": "5 Exploring linking models", "text": "In this section, we present the Semantic Proto-Role Linking Model (SPROLIM), which is a multi-view mixture model for inducing semantic proto-roles for each argument of a predicate from the property judgments employed in the last section. This model can be seen as a further generalization of Dowty\u2019s protorole theory that incorporates the idea that semantic roles have a prototype structure.\nIn Experiment 3, we apply SPROLIM to the SPR1 data with the aim of discovering semantic protoroles. We investigate how the structure of the semantic protoroles changes as we allow for more distinct types of protoroles, finding that the one constant prototype is a PROTOAGENT role with exactly the structure proposed by Dowty. In contrast, Dowty\u2019s PROTOPATIENT role appears to rather be a collection multiple other protoroles."}, {"heading": "5.1 Semantic Proto-Role Linking Model", "text": "There are four main components of SPROLIM. The first component is a representation of the relationship between a predicate\u2019s l(exical)-thematic roles (Dowty, 1989)\u2014e.g., for the verb hit, the HITTER, HITTEE, and HITTING INSTRUMENT roles\u2014 and generalized semantic proto-roles\u2014e.g., PROTOAGENT and PROTOPATIENT. To make clear that l-thematic roles are abstract, we refer to them by the more transparent name argument types and denote the set of argument types for a verb v with Av.\nThe second component of SPROLIM is a representation of the relationship between the semantic proto-role that an argument has and (i) the likelihood that a property is applicable to that argument and, (ii) if applicable, how likely it is that the property holds of the argument. We call this second component the property model. The property model represents this relationship probabilistically\u2014i.e., each semantic proto-role is associated with a distribution over property applicability and likelihood. Thus, the property model is comparable to other mixture models implementing a prototype theory, and it is why we call the generalized roles semantic proto-roles.\nThe third component is a representation of the relationship between (a) the semantic proto-role that an argument (type) has and the syntactic positions an instantiation of that argument occupies in a par-\nticular sentence\u2014in our case, subject, object, and oblique\u2014as well as (b) the other syntactic positions in that sentence (cf. the global model from \u00a74). We call these instantiations argument tokens. Because this component determines how argument tokens are linked to syntactic positions, we refer to it as the linking model.9\nThe final component is a representation of the relationship between a predicate\u2019s argument tokens (in a given sentence) and that predicate\u2019s argument types. As noted by Lang and Lapata (2010), such a component is necessary to handle argument alternations like passivization (8), the double object alternation (9), and the causative-inchoative alternation (10). If we relied purely on, e.g., relative position to associate argument tokens with argument types, we would systematically make mistakes on such cases.\n(8) a. Eight Oakland players hit homers b. Homers were hit by eight Oakland players.\n(9) a. Some 46% give foreign cars higher quality ratings.\nb. Some 46% give higher quality ratings to foreign cars.\n(10) a. The earthquake shattered windows at SFO\u2019s air-traffic control tower.\n9Unless specifically noted, this is what we mean by linking model. When referring to the entire model, we use SPROLIM.\nAlgorithm 1 Semantic Proto-Role Linking Model 1: for verb type v \u2208 V do 2: for argument type i \u2208 Av do 3: draw semantic protorole zvi \u223c Cat(\u03b8vi) 4: for verb token j \u2208 Cv do 5: draw canonicalization k \u223c Cat(\u03c6v|Tvj |) 6: cvj \u2190 element of symmetric group S|Tvj |,k 7: let r : |Tvj |-length tuple 8: for argument token t \u2208 Tvj do 9: rt \u2190 semantic protorole zvcvjt 10: for property p \u2208 P do 11: draw avjt \u223c Bern(\u03b7rvjtp) 12: if avjt = 1 then 13: draw lvjt \u223c Cat(Ord\u03ba(\u00b5rtp)) 14: let \u03c1 : |S |Tvj ||-length vector 15: for linking s\u2032 \u2208 S |Tvj | do 16: \u03c1s\u2032 \u2190 \u220f t softmax ( \u03c8rt + \u2211 o 6=t \u03b4s\u2032ts\u2032o\n) 17: draw linking k \u223c Cat(\u03c1) 18: svj \u2190 S |Tvj | k\nb. Windows at SFO\u2019s air-traffic control tower shattered.\nFollowing Lang and Lapata, we refer to this component as the canonicalizer, and we refer to these final two components together as the mapping model, since they define how one maps from argument tokens to argument types (labeled with semantic proto-roles), and from semantic proto-roles to syntactic positions.\nIn the remainder of this section, we define the property and mapping models more formally then fit SPROLIM to our data. To guide the description, Algorithm 1 gives SPROLIM\u2019s generative story, and Figure 4 shows the corresponding plate diagram.\nProperty model The property model relates each semantic role to (i) the likelihood that a property is applicable to an argument that has that role and, (ii) if applicable, how likely it is that the property holds of that argument.\nWe implement this model using a cumulative link logit hurdle model (see Agresti, 2014).10 In this model, each semantic role r \u2208 R is associated with two |P|-length real-valued vectors: a real-valued vector \u00b5r, which corresponds to the likelihood of\n10Cumulative link logit models are standard in the analysis of ordinal judgments like those contained in SPR1.\neach property p \u2208 P when an argument has role r, and \u03b7k, which gives the probability that each property p is applicable to an argument that has role r. We first describe the cumulative link logit part of this model, which determines the probability of each likelihood rating, and then the hurdle part, which determines the probability that a particular property is applicable to a particular argument.\nIn the cumulative link logit portion of the model, a categorical probability mass function with support on the property likelihood ratings l \u2208 {1, . . . , 5} is determined by the latent value \u00b5 and a nondecreasing real-valued cutpoint vector \u03ba.\nP(l = j | \u00b5,\u03ba) = { 1\u2212 qj\u22121 if j = 5 qj \u2212 qj\u22121 otherwise\nwhere qj \u2261 logit\u22121(\u03baj+1 \u2212 \u00b5) and q0 \u2261 0. This model is known as a cumulative link logit model, since q is a valid cumulative distribution function for a categorical random variable with support on {1, . . . , 5}. In Algorithm 1, we denote the parameters of this distribution with Ord\u03ba(\u00b5).\nIn the hurdle portion of the model, a Bernoulli probability mass function for applicability a \u2208 {0, 1} is given by P(a | \u03b7) = \u03b7a(1 \u2212 \u03b7)1\u2212a. What makes this model a hurdle model is that the rating probability only kicks in if the rating crosses the applicability \u201churdle.\u201d The procedural way of thinking about this is that, first, a rater decides whether a property is applicable; if it is not, they stop; if it is, they generate a rating. The joint probability of l and a is then defined as\nP(l, a | \u00b5, \u03b7,\u03ba) = P(a | \u03b7)P(l | a, \u00b5,\u03ba) \u221d P(a | \u03b7)P(l | \u00b5,\u03ba)a\nThis has the effect that the value of \u00b5 is estimated from only (l, a)-pairs where a=1\u2014i.e., where the property was applicable to the argument.\nMapping model The mapping model defines how to map from argument tokens to argument types (labeled with semantic proto-roles), and from semantic proto-roles to syntactic positions. There are two components of this model: (i) the canonicalizer, which maps from argument tokens to argument types, and (ii) the linking model, which maps from\nargument types (labeled with semantic proto-roles) to syntactic positions.\nWe implement the canonicalizer by assuming that, for each predicate (verb) v, there is some canonical ordering of its argument types and that for each sentence (clause) j \u2208 Cv that v occurs in, there is some permutation of v\u2019s argument tokens in that sentence that aligns them with their argument type in the canonical order. Denoting the set of argument tokens in sentence j with Tvj , the set of possible mappings is the symmetric group S|Tvj |. We place a categorical distribution with parameter \u03c6v on the elements of this group.\nWe implement the linking model using the same CRF described in \u00a74, but replacing the factors for each property with a factor the the arguments semantic proto-role. Thus, in Figure 2, which gives the factor graph for this CRF, the factors linking the responses r = (a, l) directly to the syntactic position nodes s are replaced with factors linking the semantic roles z to those syntactic positions."}, {"heading": "5.2 Experiment 3", "text": "In this experiment, we fit SPROLIM to the SPR1 data and investigate the semantic protoroles it learns.\nModel fitting We use projected gradient descent with AdaGrad (Duchi et al., 2011) to find an approximation to the maximum likelihood estimates (MLE) for \u0398, \u03a6, M, E, \u03a8, \u2206, and \u03ba, with the variables Z and C integrated out of the likelihood.\nDetermining a number of protoroles The one free parameter that we must set prior to fitting SPROLIM is the number of semantic protoroles |R|. We are interested in the model\u2019s behavior as |R| increases but we cannot investigate the results for all possible values here. To cut down on this set, we use a stopping criterion based on the Akaike Information Criterion (AIC). We fit SPROLIM with increasing values of |R|, stopping when AIC is minimized. We find that the |R| that maximizes AIC is 6.\nResults Figure 5 shows the estimates of the property likelihood centroids L (top) and the role-syntax coefficients \u03a8 (bottom) for each value of |R| fit.11 Columns give the values for a single protorole.\n11The syntax-syntax coefficients show the same pattern seen among the corresponding coefficients represented in Figure 3.\nPerhaps the most striking aspect of this figure is that, at each value of |R|, we see a nearly identical protorole with strong positive values on exactly Dowty\u2019s PROTOAGENT properties and negatively (or near zero) on his PROTOPATIENT properties. As such, we refer to this role, which is always the first column, as the PROTOAGENT role.\nThe rest of the roles are more varied. For |R| \u2208 {2, 3}, the non-PROTOAGENT role loads negatively (or near zero) on all PROTOAGENT properties, and really, all other properties besides MANIPULATED BY ANOTHER. Comparing |R| = 2 and |R| = 3, it appears that, the non-PROTOAGENT role in |R| = 2 is split in two based on INSTIGATION and EXISTS AS PHYSICAL, where the protorole that disprefers INSTIGATION is more likely to be an object.\nMoving to |R| = 4, we see the addition of what looks to be a second PROTOAGENT role with fewer PROTOAGENT properties. Upon investigation of the protorole mixtures \u0398 for each argument, this appears to capture cases of nonsentient or abstract\u2014 but still relatively agentive\u2014subjects, as in (11).\n(11) The antibody then kills the cell.\nThis same protorole appears in |R| = 5 with nearly all the PROTOAGENT properties, but dispreferring MAKES PHYSICAL CONTACT. It also appears in |R| = 6 without SENTIENT or EXISTS AS PHYSICAL, bolstering its status as capturing abstract entities\u2014e.g., corporations."}, {"heading": "6 Conclusion", "text": "In this paper, we introduced a framework for computational linking theory (CLT) and deployed this framework for two distinct purposes: evaluation (\u00a74) and exploration (\u00a75). In \u00a74, we evaluated four linking models based in theoretical proposals: local v. global linking models and categorical v. featural linking models. We found that global models outperform local models and categorical models outperform featural models. In \u00a75, we developed the Semantic Proto-Role Linking Model in order to better understand how the property space employed in the featural models relates to the syntax. In investigating this model\u2019s behavior, we noted that it finds a protorole strikingly similar to the one proposed by Dowty (1991), but that others of Dowty\u2019s protoroles fall into multiple distinct prototypes."}], "references": [{"title": "Categorical data analysis", "author": ["Alan Agresti"], "venue": null, "citeRegEx": "Agresti.,? \\Q2014\\E", "shortCiteRegEx": "Agresti.", "year": 2014}, {"title": "Enriching Morphologically Poor Languages for Statistical Machine Translation", "author": ["Eleftherios Avramidis", "Philipp Koehn"], "venue": "In Proceedings of ACL08: HLT,", "citeRegEx": "Avramidis and Koehn.,? \\Q2008\\E", "shortCiteRegEx": "Avramidis and Koehn.", "year": 2008}, {"title": "Incorporation: A theory of grammatical function changing", "author": ["Mark C. Baker"], "venue": "University of Chicago Press Chicago,", "citeRegEx": "Baker.,? \\Q1988\\E", "shortCiteRegEx": "Baker.", "year": 1988}, {"title": "Italian syntax: A government-binding approach, volume 1", "author": ["Luigi Burzio"], "venue": "Springer Science & Business Media,", "citeRegEx": "Burzio.,? \\Q1986\\E", "shortCiteRegEx": "Burzio.", "year": 1986}, {"title": "Introduction to the CoNLL-2004 shared task: Semantic role labeling", "author": ["Xavier Carreras", "Llus Marquez"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning,", "citeRegEx": "Carreras and Marquez.,? \\Q2004\\E", "shortCiteRegEx": "Carreras and Marquez.", "year": 2004}, {"title": "Some linking regularities", "author": ["Richard Carter"], "venue": "Working Papers (Vol. 25). MIT Center for Cognitive Science,", "citeRegEx": "Carter.,? \\Q1976\\E", "shortCiteRegEx": "Carter.", "year": 1976}, {"title": "Thematic proto-roles and argument", "author": ["David Dowty"], "venue": "selection. Language,", "citeRegEx": "Dowty.,? \\Q1991\\E", "shortCiteRegEx": "Dowty.", "year": 1991}, {"title": "On the semantic content of the notion of thematic role", "author": ["David R. Dowty"], "venue": "In Properties, types and meaning,", "citeRegEx": "Dowty.,? \\Q1989\\E", "shortCiteRegEx": "Dowty.", "year": 1989}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "The grammar of hitting and breaking", "author": ["Charles John Fillmore"], "venue": "Readings in English Transformational Grammar,", "citeRegEx": "Fillmore.,? \\Q1970\\E", "shortCiteRegEx": "Fillmore.", "year": 1970}, {"title": "Generation from Abstract Meaning Representation using Tree Transducers", "author": ["Jeffrey Flanigan", "Chris Dyer", "Noah A. Smith", "Jaime Carbonell"], "venue": "In Proceedings of NAACL", "citeRegEx": "Flanigan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Flanigan et al\\.", "year": 2016}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky"], "venue": "Computational Linguistics,", "citeRegEx": "Gildea and Jurafsky.,? \\Q2002\\E", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "More than words: Syntactic packaging and implicit sentiment", "author": ["Stephan Greene", "Philip Resnik"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Greene and Resnik.,? \\Q2009\\E", "shortCiteRegEx": "Greene and Resnik.", "year": 2009}, {"title": "Unsupervised discovery of a statistical verb lexicon", "author": ["Trond Grenager", "Christopher D. Manning"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Grenager and Manning.,? \\Q2006\\E", "shortCiteRegEx": "Grenager and Manning.", "year": 2006}, {"title": "Argument structure", "author": ["Jane Grimshaw"], "venue": null, "citeRegEx": "Grimshaw.,? \\Q1990\\E", "shortCiteRegEx": "Grimshaw.", "year": 1990}, {"title": "Natural Language Generation in the Context of Machine Translation", "author": ["Jan Hajic", "Yuan Ding", "Dan Gildea", "Gerald Penn", "Dragomir Radevlo"], "venue": null, "citeRegEx": "Hajic et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hajic et al\\.", "year": 2004}, {"title": "Prolegomena to a Theory of Argument Structure", "author": ["Ken Hale", "Samuel Jay Keyser"], "venue": null, "citeRegEx": "Hale and Keyser.,? \\Q2002\\E", "shortCiteRegEx": "Hale and Keyser.", "year": 2002}, {"title": "The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs", "author": ["Joshua K. Hartshorne", "Claire Bonial", "Martha Palmer"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Hartshorne et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hartshorne et al\\.", "year": 2013}, {"title": "Love is hard to understand: the relationship between transitivity and caused events in the acquisition of emotion verbs", "author": ["Joshua K. Hartshorne", "Amanda Pogue", "Jesse Snedeker"], "venue": "Journal of Child Language,", "citeRegEx": "Hartshorne et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hartshorne et al\\.", "year": 2015}, {"title": "Semantic Interpretation in Generative Grammar", "author": ["Ray Jackendoff"], "venue": null, "citeRegEx": "Jackendoff.,? \\Q1972\\E", "shortCiteRegEx": "Jackendoff.", "year": 1972}, {"title": "A discriminative lexicon model for complex morphology", "author": ["Minwoo Jeong", "Kristina Toutanova", "Hisami Suzuki", "Chris Quirk"], "venue": "In Proceedings of the Ninth Conference of the Association for Machine Translation in the Americas,", "citeRegEx": "Jeong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jeong et al\\.", "year": 2010}, {"title": "Thematic role properties of subjects", "author": ["Edward Kako"], "venue": "and objects. Cognition,", "citeRegEx": "Kako.,? \\Q2006\\E", "shortCiteRegEx": "Kako.", "year": 2006}, {"title": "VerbNet: A broad-coverage, comprehensive verb lexicon", "author": ["Karin Kipper-Schuler"], "venue": "PhD thesis, University of Pennsylvania,", "citeRegEx": "Kipper.Schuler.,? \\Q2005\\E", "shortCiteRegEx": "Kipper.Schuler.", "year": 2005}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT Summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Factored Translation Models", "author": ["Philipp Koehn", "Hieu Hoang"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Koehn and Hoang.,? \\Q2007\\E", "shortCiteRegEx": "Koehn and Hoang.", "year": 2007}, {"title": "Irregularity in syntax", "author": ["George Lakoff"], "venue": null, "citeRegEx": "Lakoff.,? \\Q1970\\E", "shortCiteRegEx": "Lakoff.", "year": 1970}, {"title": "Unsupervised induction of semantic roles", "author": ["Joel Lang", "Mirella Lapata"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Lang and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Lang and Lapata.", "year": 2010}, {"title": "English verb classes and alternations: A preliminary investigation", "author": ["Beth Levin"], "venue": null, "citeRegEx": "Levin.,? \\Q1993\\E", "shortCiteRegEx": "Levin.", "year": 1993}, {"title": "Unaccusativity: At the syntax-lexical semantics interface, volume 26", "author": ["Beth Levin", "Malka Rappaport Hovav"], "venue": "MIT press,", "citeRegEx": "Levin and Hovav.,? \\Q1995\\E", "shortCiteRegEx": "Levin and Hovav.", "year": 1995}, {"title": "Argument realization", "author": ["Beth Levin", "Malka Rappaport Hovav"], "venue": null, "citeRegEx": "Levin and Hovav.,? \\Q2005\\E", "shortCiteRegEx": "Levin and Hovav.", "year": 2005}, {"title": "Automatic labeling of semantic roles", "author": ["Ken Litkowski"], "venue": "Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,", "citeRegEx": "Litkowski.,? \\Q2004\\E", "shortCiteRegEx": "Litkowski.", "year": 2004}, {"title": "Combining lexical resources: mapping between propbank and verbnet", "author": ["Edward Loper", "Szu-Ting Yi", "Martha Palmer"], "venue": "In Proceedings of the 7th International Workshop on Computational Linguistics, Tilburg, the Netherlands,", "citeRegEx": "Loper et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loper et al\\.", "year": 2007}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Semantic role labeling: an introduction to the special issue", "author": ["Lluis Marquez", "Xavier Carreras", "Kenneth C. Litkowski", "Suzanne Stevenson"], "venue": "Computational linguistics,", "citeRegEx": "Marquez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marquez et al\\.", "year": 2008}, {"title": "Generating complex morphology for machine translation", "author": ["Einat Minkov", "Kristina Toutanova", "Hisami Suzuki"], "venue": "In ACL,", "citeRegEx": "Minkov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Minkov et al\\.", "year": 2007}, {"title": "On raising: one rule of English grammar and its theoretical implications", "author": ["Paul Martin Postal"], "venue": "Current Studies in Linguistics", "citeRegEx": "Postal.,? \\Q1974\\E", "shortCiteRegEx": "Postal.", "year": 1974}, {"title": "Support vector learning for semantic argument classification", "author": ["Sameer Pradhan", "Kadri Hacioglu", "Valerie Krugler", "Wayne Ward", "James H. Martin", "Daniel Jurafsky"], "venue": "Machine Learning,", "citeRegEx": "Pradhan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2005}, {"title": "Shallow Semantic Parsing using Support Vector Machines", "author": ["Sameer S. Pradhan", "Wayne Ward", "Kadri Hacioglu", "James H. Martin", "Daniel Jurafsky"], "venue": "In HLT-NAACL,", "citeRegEx": "Pradhan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2004}, {"title": "Generating case markers in machine translation", "author": ["Kristina Toutanova Hisami Suzuki", "K. Toutanova"], "venue": "In Proceedings of NAACL HLT,", "citeRegEx": "Suzuki and Toutanova.,? \\Q2007\\E", "shortCiteRegEx": "Suzuki and Toutanova.", "year": 2007}, {"title": "Unsupervised semantic role labelling", "author": ["Robert S. Swier", "Suzanne Stevenson"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Swier and Stevenson.,? \\Q2004\\E", "shortCiteRegEx": "Swier and Stevenson.", "year": 2004}, {"title": "A global joint model for semantic role labeling", "author": ["Kristina Toutanova", "Aria Haghighi", "Christopher D. Manning"], "venue": "Computational Linguistics,", "citeRegEx": "Toutanova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}, {"title": "Applying Morphology Generation Models to Machine Translation", "author": ["Kristina Toutanova", "Hisami Suzuki", "Achim Ruopp"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Toutanova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}, {"title": "Arguments in Syntax and Semantics", "author": ["Alexander Williams"], "venue": null, "citeRegEx": "Williams.,? \\Q2015\\E", "shortCiteRegEx": "Williams.", "year": 2015}, {"title": "Semantic roles for smt: a hybrid two-pass model", "author": ["Dekai Wu", "Pascale Fung"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Wu and Fung.,? \\Q2009\\E", "shortCiteRegEx": "Wu and Fung.", "year": 2009}, {"title": "In a manner of speaking", "author": ["Arnold M. Zwicky"], "venue": "Linguistic Inquiry,", "citeRegEx": "Zwicky.,? \\Q1971\\E", "shortCiteRegEx": "Zwicky.", "year": 1971}], "referenceMentions": [{"referenceID": 9, "context": "A linking theory explains how verbs\u2019 semantic arguments are mapped to their syntactic arguments (Fillmore, 1970; Zwicky, 1971; Jackendoff, 1972; Carter, 1976; Pinker, 1984, 1989; Grimshaw, 1990; Levin, 1993).", "startOffset": 96, "endOffset": 207}, {"referenceID": 44, "context": "A linking theory explains how verbs\u2019 semantic arguments are mapped to their syntactic arguments (Fillmore, 1970; Zwicky, 1971; Jackendoff, 1972; Carter, 1976; Pinker, 1984, 1989; Grimshaw, 1990; Levin, 1993).", "startOffset": 96, "endOffset": 207}, {"referenceID": 19, "context": "A linking theory explains how verbs\u2019 semantic arguments are mapped to their syntactic arguments (Fillmore, 1970; Zwicky, 1971; Jackendoff, 1972; Carter, 1976; Pinker, 1984, 1989; Grimshaw, 1990; Levin, 1993).", "startOffset": 96, "endOffset": 207}, {"referenceID": 5, "context": "A linking theory explains how verbs\u2019 semantic arguments are mapped to their syntactic arguments (Fillmore, 1970; Zwicky, 1971; Jackendoff, 1972; Carter, 1976; Pinker, 1984, 1989; Grimshaw, 1990; Levin, 1993).", "startOffset": 96, "endOffset": 207}, {"referenceID": 14, "context": "A linking theory explains how verbs\u2019 semantic arguments are mapped to their syntactic arguments (Fillmore, 1970; Zwicky, 1971; Jackendoff, 1972; Carter, 1976; Pinker, 1984, 1989; Grimshaw, 1990; Levin, 1993).", "startOffset": 96, "endOffset": 207}, {"referenceID": 27, "context": "A linking theory explains how verbs\u2019 semantic arguments are mapped to their syntactic arguments (Fillmore, 1970; Zwicky, 1971; Jackendoff, 1972; Carter, 1976; Pinker, 1984, 1989; Grimshaw, 1990; Levin, 1993).", "startOffset": 96, "endOffset": 207}, {"referenceID": 25, "context": "One classic (though not the only) example of this difficulty concerns psych verbs, like fear and frighten (Lakoff, 1970; Postal, 1974; Perlmutter and Postal, 1984; Baker, 1988; Dowty, 1991; Pesetsky, 1995).", "startOffset": 106, "endOffset": 205}, {"referenceID": 35, "context": "One classic (though not the only) example of this difficulty concerns psych verbs, like fear and frighten (Lakoff, 1970; Postal, 1974; Perlmutter and Postal, 1984; Baker, 1988; Dowty, 1991; Pesetsky, 1995).", "startOffset": 106, "endOffset": 205}, {"referenceID": 2, "context": "One classic (though not the only) example of this difficulty concerns psych verbs, like fear and frighten (Lakoff, 1970; Postal, 1974; Perlmutter and Postal, 1984; Baker, 1988; Dowty, 1991; Pesetsky, 1995).", "startOffset": 106, "endOffset": 205}, {"referenceID": 6, "context": "One classic (though not the only) example of this difficulty concerns psych verbs, like fear and frighten (Lakoff, 1970; Postal, 1974; Perlmutter and Postal, 1984; Baker, 1988; Dowty, 1991; Pesetsky, 1995).", "startOffset": 106, "endOffset": 205}, {"referenceID": 11, "context": "Linking theory is intimately related to semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Marquez, 2004; Marquez et al., 2008), which is a form of shallow semantic parsing.", "startOffset": 69, "endOffset": 163}, {"referenceID": 30, "context": "Linking theory is intimately related to semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Marquez, 2004; Marquez et al., 2008), which is a form of shallow semantic parsing.", "startOffset": 69, "endOffset": 163}, {"referenceID": 4, "context": "Linking theory is intimately related to semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Marquez, 2004; Marquez et al., 2008), which is a form of shallow semantic parsing.", "startOffset": 69, "endOffset": 163}, {"referenceID": 33, "context": "Linking theory is intimately related to semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Marquez, 2004; Marquez et al., 2008), which is a form of shallow semantic parsing.", "startOffset": 69, "endOffset": 163}, {"referenceID": 13, "context": "In this paper, we introduce a framework for implementing linking theories\u2014computational linking theory (CLT)\u2014which substantially generalizes an idea first introduced by Grenager and Manning (2006). In CLT, the traditional linking theoretic notion of a mapping from the space of semantic arguments Sem to the space of syntactic arguments Syn is implemented as a classifier.", "startOffset": 169, "endOffset": 197}, {"referenceID": 25, "context": "For instance, following Lang and Lapata (2010), who build on Grenager and Manning 2006, Synmight be a set of syntactic positions, such as {subject, object, .", "startOffset": 24, "endOffset": 47}, {"referenceID": 22, "context": ", 2005), VerbNet (Kipper-Schuler, 2005), SemLink (Loper et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 31, "context": ", 2005), VerbNet (Kipper-Schuler, 2005), SemLink (Loper et al., 2007), and Reisinger et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 22, "context": ", 2005), VerbNet (Kipper-Schuler, 2005), SemLink (Loper et al., 2007), and Reisinger et al.\u2019s (2015) recently released Semantic Proto-Roles version 1 (SPR1) dataset to evaluate the efficacy of various linking theories proposed in the theoretical literature.", "startOffset": 18, "endOffset": 101}, {"referenceID": 2, "context": ", 2005, 2008a)\u2014 contacts a long-standing theoretical debate regarding whether semantic arguments are mapped to syntactic positions independently of other arguments (Baker, 1988) or whether there are dependencies among semantic arguments (Dowty, 1991).", "startOffset": 164, "endOffset": 177}, {"referenceID": 6, "context": ", 2005, 2008a)\u2014 contacts a long-standing theoretical debate regarding whether semantic arguments are mapped to syntactic positions independently of other arguments (Baker, 1988) or whether there are dependencies among semantic arguments (Dowty, 1991).", "startOffset": 237, "endOffset": 250}, {"referenceID": 6, "context": "The distinction between categorical and featural models contacts an independent debate as to whether semantic arguments, such as HITTER and HITTEE, fall into discrete semantic role categories, such as AGENT or PATIENT, or whether they are associated to a greater or lesser extent with fuzzy semantic role prototypes, such as PROTOAGENT and PROTOPATIENT (Dowty, 1991).", "startOffset": 353, "endOffset": 366}, {"referenceID": 23, "context": "This problem has been particularly important in the area of statistical machine translation (SMT), where one needs to translate from morphologically poor languages like English to morphologically richer languages like Japanese and German (Koehn, 2005).", "startOffset": 238, "endOffset": 251}, {"referenceID": 19, "context": "Suzuki and Toutanova (2006, 2007) construct models for predicting Japanese morphological case (which marks syntactic position in languages that have such cases) using intralanguage positional and alignment-based features, and Jeong et al. (2010) extend this line of work to Bulgarian, Czech, and Korean.", "startOffset": 226, "endOffset": 246}, {"referenceID": 19, "context": "Suzuki and Toutanova (2006, 2007) construct models for predicting Japanese morphological case (which marks syntactic position in languages that have such cases) using intralanguage positional and alignment-based features, and Jeong et al. (2010) extend this line of work to Bulgarian, Czech, and Korean. Koehn and Hoang (2007), Avramidis and Koehn (2008), and Toutanova et al.", "startOffset": 226, "endOffset": 327}, {"referenceID": 1, "context": "Koehn and Hoang (2007), Avramidis and Koehn (2008), and Toutanova et al.", "startOffset": 24, "endOffset": 51}, {"referenceID": 1, "context": "Koehn and Hoang (2007), Avramidis and Koehn (2008), and Toutanova et al. (2008b) use richer phrase-based features to do the same task.", "startOffset": 24, "endOffset": 81}, {"referenceID": 43, "context": "Other approaches have incorporated semantic roles into SMT reranking components (Wu and Fung, 2009), similar to the reranking conducted in many SRL systems (cf.", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": "Where a linking theory aims to map a verb\u2019s semantic arguments to it syntactic arguments, an SRL system aims to map a verb\u2019s syntactic arguments to its semantic arguments (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Marquez, 2004; Marquez et al., 2008).", "startOffset": 171, "endOffset": 265}, {"referenceID": 30, "context": "Where a linking theory aims to map a verb\u2019s semantic arguments to it syntactic arguments, an SRL system aims to map a verb\u2019s syntactic arguments to its semantic arguments (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Marquez, 2004; Marquez et al., 2008).", "startOffset": 171, "endOffset": 265}, {"referenceID": 4, "context": "Where a linking theory aims to map a verb\u2019s semantic arguments to it syntactic arguments, an SRL system aims to map a verb\u2019s syntactic arguments to its semantic arguments (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Marquez, 2004; Marquez et al., 2008).", "startOffset": 171, "endOffset": 265}, {"referenceID": 33, "context": "Where a linking theory aims to map a verb\u2019s semantic arguments to it syntactic arguments, an SRL system aims to map a verb\u2019s syntactic arguments to its semantic arguments (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Marquez, 2004; Marquez et al., 2008).", "startOffset": 171, "endOffset": 265}, {"referenceID": 37, "context": "Swier and Stevenson (2004) introduce the first such system, which uses a bootstrapping procedure to first associate verb tokens with frames containing typed slots (drawn from VerbNet), then iteratively compute probabilities based on cooccurrence counts and fill unfilled slots based on these probabilities.", "startOffset": 0, "endOffset": 27}, {"referenceID": 13, "context": "Grenager and Manning (2006) introduce the idea of predicting syntactic position based on a latent semantic role representation learned from syntactic and selectional features.", "startOffset": 0, "endOffset": 28}, {"referenceID": 13, "context": "Grenager and Manning (2006) introduce the idea of predicting syntactic position based on a latent semantic role representation learned from syntactic and selectional features. Lang and Lapata (2010) expand on Grenager and Manning 2006 by introducing the notion of a canonicalized linking.", "startOffset": 0, "endOffset": 199}, {"referenceID": 13, "context": "Grenager and Manning (2006) introduce the idea of predicting syntactic position based on a latent semantic role representation learned from syntactic and selectional features. Lang and Lapata (2010) expand on Grenager and Manning 2006 by introducing the notion of a canonicalized linking. We discuss these ideas further in \u00a75, incorporating both into our Semantic Proto-Role Linking Model (SPROLIM). Syntax-based clustering approaches which do not explicitly attempt to predict syntactic position have also been popular. Lang and Lapata (2011a, 2014) use graph clustering methods and Lang and Lapata (2011b) use a split-merge algorithm to cluster", "startOffset": 0, "endOffset": 608}, {"referenceID": 22, "context": "} (Kipper-Schuler, 2005).", "startOffset": 2, "endOffset": 24}, {"referenceID": 6, "context": "In particular, reasonably wide-coverage categorical theories require an ever-growing number of roles to capture linking regularities\u2014a phenomenon that Dowty (1991) refers to as role fragmentation.", "startOffset": 151, "endOffset": 164}, {"referenceID": 6, "context": "Dowty (1991) proposes the first (and, perhaps, most comprehensive) list of such properties as a part of specifying his proto-role linking theory (PRLT).", "startOffset": 0, "endOffset": 13}, {"referenceID": 32, "context": "For all experiments, datasets are based on the thematic role annotations of the Penn Treebank (Marcus et al., 1993) found in PropBank (Palmer et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 22, "context": ", 2005) and VerbNet (Kipper-Schuler, 2005), mapped via SemLink (Loper et al.", "startOffset": 20, "endOffset": 42}, {"referenceID": 31, "context": ", 2005) and VerbNet (Kipper-Schuler, 2005), mapped via SemLink (Loper et al., 2007), as well as the Semantic Proto-Roles version 1.", "startOffset": 63, "endOffset": 83}, {"referenceID": 12, "context": "The SPR1 annotations consist of answers to simple questions about how likely, on a five-point scale, it is that particular relational properties hold of arguments of PropBank-annotated verbs (cp. Kako, 2006; Greene and Resnik, 2009; Hartshorne et al., 2013).", "startOffset": 191, "endOffset": 257}, {"referenceID": 17, "context": "The SPR1 annotations consist of answers to simple questions about how likely, on a five-point scale, it is that particular relational properties hold of arguments of PropBank-annotated verbs (cp. Kako, 2006; Greene and Resnik, 2009; Hartshorne et al., 2013).", "startOffset": 191, "endOffset": 257}, {"referenceID": 2, "context": "The first finding argues against local linking theories like that proposed by Baker (1988). We hypothesize that the second finding has two sources: (i) the set of properties in SPR1, which are essentially just Dowty\u2019s properties, is insufficient for capturing distinctions among nonsubject positions like direct object and oblique\u2014 likely because Dowty engineered his properties only to distinguish subjects from non-subjects; and (ii) because the models we use don\u2019t capture multimodality in the kinds of property configurations that exist.", "startOffset": 78, "endOffset": 91}, {"referenceID": 7, "context": "The first component is a representation of the relationship between a predicate\u2019s l(exical)-thematic roles (Dowty, 1989)\u2014e.", "startOffset": 107, "endOffset": 120}, {"referenceID": 26, "context": "As noted by Lang and Lapata (2010), such a component is necessary to handle argument alternations like passivization (8), the double object alternation (9), and the causative-inchoative alternation (10).", "startOffset": 12, "endOffset": 35}, {"referenceID": 8, "context": "Model fitting We use projected gradient descent with AdaGrad (Duchi et al., 2011) to find an approximation to the maximum likelihood estimates (MLE) for \u0398, \u03a6, M, E, \u03a8, \u2206, and \u03ba, with the variables Z and C integrated out of the likelihood.", "startOffset": 61, "endOffset": 81}, {"referenceID": 6, "context": "In investigating this model\u2019s behavior, we noted that it finds a protorole strikingly similar to the one proposed by Dowty (1991), but that others of Dowty\u2019s protoroles fall into multiple distinct prototypes.", "startOffset": 117, "endOffset": 130}], "year": 2016, "abstractText": "A linking theory explains how verbs\u2019 semantic arguments are mapped to their syntactic arguments\u2014the inverse of the semantic role labeling task from the shallow semantic parsing literature. In this paper, we develop the computational linking theory framework as a method for implementing and testing linking theories proposed in the theoretical literature. We deploy this framework to assess two crosscutting types of linking theory: local v. global models and categorical v. featural models. To further investigate the behavior of these models, we develop a measurement model in the spirit of previous work in semantic role induction: the semantic proto-role linking model. We use this model, which implements a generalization of Dowty\u2019s seminal proto-role theory, to induce semantic proto-roles, which we compare to those Dowty proposes.", "creator": "LaTeX with hyperref package"}}}