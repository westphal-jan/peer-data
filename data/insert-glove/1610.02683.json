{"id": "1610.02683", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2016", "title": "Interpreting Neural Networks to Improve Politeness Comprehension", "abstract": "We yisrael present an flowing interpretable icfai neural network zimtrade approach to jaen predicting miniblinds and understanding brim politeness fxe5 in attachments natural blackville language buckcherry requests. Our models saucers are 1,096 based on simple co-writer convolutional 3,312 neural dolars networks directly super-villain on zimmers raw text, avoiding juneja any 39.05 manual vigour identification 166.75 of tiswas complex dunovant sentiment scribed or von syntactic features, while dakotah performing better than recharged such feature - based models 610,000 from previous work. More f.b.i. importantly, we centre-back use the ricotti challenging task blackmar of politeness antioxidants prediction as a booklet testbed to next resurveyed present encroachments a darweesh much - needed understanding tigerdirect of what 1003 these 45.38 successful 124.06 networks hauliers are battens actually learning. For honnold this, we present several network graffito visualizations cunnington based pre-historic on barkow activation christijan clusters, mydin first ntr derivative gypsies saliency, ssk and dyea embedding boodle space mopane transformations, helping murthy us automatically octa identify guitar-driven several subtle iberians linguistics markers codenamed of 1111 politeness theories. sa-7s Further, chaetophoraceae this analysis reveals tewaaraton multiple novel, teleporting high - presley scoring standiford politeness televisora strategies which, when added back flashbacks as 1.114 new features, birthers reduce taikang the accuracy gloomiest gap between oloroso the original featurized cronies system far-off and the maetel neural model, nehn thus articular providing a clear quantitative interpretation toste of the success of krekorian these fronton neural al\u012b networks.", "histories": [["v1", "Sun, 9 Oct 2016 14:42:58 GMT  (1181kb,D)", "http://arxiv.org/abs/1610.02683v1", "To appear at EMNLP 2016"]], "COMMENTS": "To appear at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["malika aubakirova", "mohit bansal"], "accepted": true, "id": "1610.02683"}, "pdf": {"name": "1610.02683.pdf", "metadata": {"source": "CRF", "title": "Interpreting Neural Networks to Improve Politeness Comprehension", "authors": ["Malika Aubakirova", "Mohit Bansal"], "emails": ["aubakirova@uchicago.edu", "mbansal@cs.unc.edu"], "sections": [{"heading": "1 Introduction", "text": "Politeness theories (Brown and Levinson, 1987; Gu, 1990; Bargiela-Chiappini, 2003) include key components such as modality, indirection, deference, and impersonalization. Positive politeness strategies focus on making the hearer feel good through offers, promises, and jokes. Negative politeness examples include favor seeking, orders, and requests. Differentiating among politeness types is a highly nontrivial task, because it depends on factors such as a context, relative power, and culture.\nDanescu-Niculescu-Mizil et al. (2013) proposed a useful computational framework for predicting politeness in natural language requests by designing various lexical and syntactic features about key politeness theories, e.g., first or second person start vs. plural. However, manually identifying such politeness features is very challenging, because there exist several complex theories and politeness in natural language is often realized via subtle markers and non-literal cues.\nNeural networks have been achieving high performance in sentiment analysis tasks, via their ability to automatically learn short and long range spatial relations. However, it is hard to interpret and explain what they have learned. In this paper, we first propose to address politeness prediction via simple CNNs working directly on the raw text. This helps us avoid the need for any complex, manuallydefined linguistic features, while still performing better than such featurized systems. More importantly, we next present an intuitive interpretation of what these successful neural networks are learning, using the challenging politeness task as a testbed.\nTo this end, we present several visualization strategies: activation clustering, first derivative saliency, and embedding space transformations, some of which are inspired by similar strategies in computer vision (Erhan et al., 2009; Simonyan et al., 2014; Girshick et al., 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al., 2016; Ka\u0301da\u0301r et al., 2016). The neuron activation clustering method not only rediscovers and extends several manually defined features from politeness theories, but also uncovers multiple novel strategies, whose importance we measure quantitatively. The first derivative saliency technique allows us to identify the impact of each phrase\nar X\niv :1\n61 0.\n02 68\n3v 1\n[ cs\n.C L\n] 9\nO ct\n2 01\n6\non the final politeness prediction score via heatmaps, revealing useful politeness markers and cues. Finally, we also plot lexical embeddings before and after training, showing how specific politeness markers move and cluster based on their polarity. Such visualization strategies should also be useful for understanding similar state-of-the-art neural network models on various other NLP tasks.\nImportantly, our activation clusters reveal two novel politeness strategies, namely indefinite pronouns and punctuation. Both strategies display high politeness and top-quartile scores (as defined by Danescu-Niculescu-Mizil et al. (2013)). Also, when added back as new features to the original featurized system, they improve its performance and reduce the accuracy gap between the featurized system and the neural model, thus providing a clear, quantitative interpretation of the success of these neural networks in automatically learning useful features."}, {"heading": "2 Related Work", "text": "Danescu-Niculescu-Mizil et al. (2013) presented one of the first useful datasets and computational approaches to politeness theories (Brown and Levinson, 1987; Goldsmith, 2007; Ka\u0301da\u0301r and Haugh, 2013; Locher and Watts, 2005), using manually defined lexical and syntactic features. Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013). Recent work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014). However, none of the above methods focused on visualizing and understanding the inner workings of these successful neural networks.\nThere have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015). Recently in NLP, Li et al. (2016) successfully adopt computer vision techniques, namely first-order saliency, and present representation plotting for sentiment compositionality across RNN variants. Similarly, Ka\u0301da\u0301r et al. (2016)\nanalyze the omission scores and top-k contexts of hidden units of a multimodal RNN. Karpathy et al. (2016) visualize character-level language models. We instead adopt visualization techniques for CNN style models for NLP1 and apply these to the challenging task of politeness prediction, which often involves identifying subtle and non-literal sociolinguistic cues. We also present a quantitative interpretation of the success of these CNNs on the politeness prediction task, based on closing the performance gap between the featurized and neural models."}, {"heading": "3 Approach", "text": ""}, {"heading": "3.1 Convolutional Neural Networks", "text": "We use one convolutional layer followed by a pooling layer. For a sentence v1:n (where each word vi is a d-dim vector), a filter m applied on a window of t words, produces a convolution feature ci = f(m \u2217 vi:i+t\u22121 + b), where f is a non-linear function, and b is a bias term. A feature map c \u2208 Rn\u2212t+1 is applied on each possible window of words so that c = [c1, ..., cn\u2212t+1]. This convolutional layer is then followed by a max-over-pooling operation (Collobert et al., 2011) that gives C = max{c} of the particular filter. To obtain multiple features, we use multiple filters of varying window sizes. The result is then passed to a fully-connected softmax layer that outputs probabilities over labels."}, {"heading": "4 Experimental Setup", "text": ""}, {"heading": "4.1 Datasets", "text": "We used the two datasets released by DanescuNiculescu-Mizil et al. (2013): Wikipedia (Wiki) and Stack Exchange (SE), containing community requests with politeness labels. Their \u2018feature development\u2019 was done on the Wiki dataset, and SE was used as the \u2018feature transfer\u2019 domain. We use a simpler train-validation-test split based setup for these datasets instead of the original leave-one-out crossvalidation setup, which makes training extremely slow for any neural network or sizable classifier.2\n1The same techniques can also be applied to RNN models. 2The result trends and visualizations using cross-validation were similar to our current results, in preliminary experiments. We will release our exact dataset split details."}, {"heading": "4.2 Training Details", "text": "Our tuned hyperparameters values (on the dev set of Wiki) are a mini-batch size of 32, a learning rate of 0.001 for the Adam (Kingma and Ba, 2015) optimizer, a dropout rate of 0.5, CNN filter windows of 3, 4, and 5 with 75 feature maps each, and ReLU as the non-linear function (Nair and Hinton, 2010). For convolution layers, we use valid padding and strides of all ones. We followed Danescu-Niculescu-Mizil et al. (2013) in using SE only as a transfer domain, i.e., we do not re-tune any hyperparameters or features on this domain and simply use the chosen values from the Wiki setting. The split and other training details are provided in the supplement."}, {"heading": "5 Results", "text": "Table 1 first presents our reproduced classification accuracy test results (two labels: positive or negative politeness) for the bag-of-words and linguistic features based models of Danescu-Niculescu-Mizil et al. (2013) (for our dataset splits) as well as the performance of our CNN model. As seen, without using any manually defined, theory-inspired linguistic features, the simple CNN model performs better than the feature-based methods.3\nNext, we also show how the linguistic features baseline improves on adding our novelly discovered features (plus correcting some exising features), revealed via the analysis in Sec. 6. Thus, this reduces the gap in performance between the linguistic features baseline and the CNN, and in turn provides a quantitative reasoning for the success of the CNN model. More details in Sec. 6."}, {"heading": "6 Analysis and Visualization", "text": "We present the primary interest and contribution of this work: performing an important qualitative and quantitative analysis of what is being learned by our neural networks w.r.t. politeness strategies.4"}, {"heading": "6.1 Activation Clusters", "text": "Activation clustering is a non-parametric approach (adopted from Girshick et al. (2014)) of computing\n3For reference, human performance on the original task setup of Danescu-Niculescu-Mizil et al. (2013) was 86.72% and 80.89% on the Wiki and SE datasets, respectively.\n4We only use the Wiki train/dev sets for all analysis.\neach CNN unit\u2019s activations on a dataset and then analyzing the top-scoring samples in each cluster. We keep track of which neurons get maximally activated for which Wikipedia requests and analyze the most frequent requests in each neuron\u2019s cluster, to understand what each neuron reacts to."}, {"heading": "6.1.1 Rediscovering Existing Strategies", "text": "We find that the different activation clusters of our neural network automatically rediscover a number of strategies from politeness theories considered in Danescu-Niculescu-Mizil et al. (2013) (see Table 3 in their paper). We present a few such strategies here with their supporting examples, and the rest (e.g., Gratitude, Greeting, Positive Lexicon, and Counterfactual Modal) are presented in the supplement. The majority politeness label of each category is indicated by (+) and (-). Deference (+) A way of sharing the burden of a request placed on the addressee. Activation cluster examples: {\u201cnice work so far on your rewrite...\u201d; \u201chey, good work on the new pages...\u201d} Direct Question (-) Questions imposed on the converser in a direct manner with a demand of a factual answer. Activation cluster examples: {\u201cwhat\u2019s with the radio , and fist in the air?\u201d; \u201cwhat level warning is appropriate?\u201d}"}, {"heading": "6.1.2 Extending Existing Strategies", "text": "We also found that certain activation clusters depicted interesting extensions of the politeness strategies given in previous work. Gratitude (+) Our CNN learns a special shade of gratitude, namely it distinguishes a cluster consisting of the bigram thanks for. Activation cluster examples: {\u201cthanks for the good advice.\u201d; \u201cthanks for letting me know.\u201d} Counterfactual Modal (+) Sentences with Would you/Could you get grouped together as expected; but in addition, the cluster contains requests with Do you mind as well as gapped 3-grams like Can you ... please?, which presumably implies that the combi-\nnation of a later please with future-oriented variants can/will in the request gives a similar effect as the conditional-oriented variants would/could. Activation cluster examples: {can this be reported ... grid, please?\u201d; do you mind having another look?\u201d}"}, {"heading": "6.1.3 Discovering Novel Strategies", "text": "In addition to rediscovering and extending politeness strategies mentioned in previous work, our network also automatically discovers some novel activation clusters, potentially corresponding to new politeness strategies. Indefinite Pronouns (-) Danescu-NiculescuMizil et al. (2013) distinguishes requests with first and second person (plural, starting position, etc.). However, we find activations that also react to indefinite pronouns such as something/somebody. Activation cluster examples: {\u201cam i missing something here?\u201d; \u201cwait for anyone to discuss it.\u201d} Punctuation (-) Though non-characteristic in direct speech, punctuation appears to be an important special marker in online communities, which in some sense captures verbal emotion in text. E.g., one of our neuron clusters gets activated on question marks \u201c???\u201d and one on ellipsis \u201c...\u201d. Activation cluster examples: {\u201cnow???\u201d; \u201coriginal article????\u201d; \u201chelllo?????\u201d}5\nIn the next section, via saliency heatmaps, we will further study the impact of indefinite pronouns in the final-decision making of the classifier. Finally, in Sec. 6.4, we will quantitatively show how our newly discovered strategies help directly improve the accuracy performance of the linguistic features baseline and achieve high politeness and top-quartile scores as per Danescu-Niculescu-Mizil et al. (2013).\n5More examples are given in the supplement."}, {"heading": "6.2 First Derivative Saliency", "text": "Inspired from neural network visualization in computer vision (Simonyan et al., 2014), the first derivative saliency method indicates how much each input unit contributes to the final decision of the classifier. If E is the input embedding, y is the true label, and Sy(E) is the neural network output, then we consider gradients \u2202Sy(E)\u2202e . Each image in Fig. 1 is a heatmap of the magnitudes of the derivative in absolute value with respect to each dimension.\nThe first heatmap gets signals from please (Please strategy) and could you (Counterfactual Modal strategy), but effectively puts much more mass on help. This is presumably due to the nature of Wikipedia requests such that the meaning boils down to asking for some help that reduces the social distance. In the second figure, the highest emphasis is put on why would you, conceivably used by Wikipedia administrators as an indicator of questioning. Also, the indefinite pronoun somebody makes a relatively high impact on the decision. This relates back to the activation clustering mentioned in the previous section, where indefinite pronouns had their own cluster. In the third heatmap, the neural network does not put much weight on the greeting-based start hey, because it instead focuses on the higher polarity6 gratitude part after the greeting, i.e., on the words thanks for. This will be further connected in Sec. 6.3."}, {"heading": "6.3 Embedding Space Transformations", "text": "We selected key words from Danescu-NiculescuMizil et al. (2013) and from our new activation clusters ( Sec. 6.1) and plotted (via PCA) their embed-\n6See Table 3 of Danescu-Niculescu-Mizil et al. (2013) for polarity scores of the various strategies.\nding space positions before and after training, to help us gain insights into specific sentiment transformations. Fig. 2 shows that the most positive keys such as hi, appreciate, and great get clustered even more tightly after training. The key thanks gets a notably separated position on a positive spectrum, signifying its importance in the NN\u2019s decision-making (also depicted via the saliency heatmaps in Sec. 6.2).\nThe indefinite pronoun something is located near direct question politeness strategy keys why and what. Please, as was shown by Danescu-NiculescuMizil et al. (2013), is not always a positive word because its sentiment depends on its sentence position, and it moves further away from a positive key group. Counterfactual Modal keys could and would as well as can of indicative modal get far more separated from positive keys. Moreover, after the training, the distance between could and would increases but it gets preserved between can and would, which might suggest that could has a far stronger sentiment."}, {"heading": "6.4 Quantitative Analysis", "text": "In this section, we present quantitative measures of the importance and polarity of the novelly discovered politeness strategies in the above sections, as well how they explain some of the improved performance of the neural model.\nIn Table 3 of Danescu-Niculescu-Mizil et al. (2013), the pronoun politeness strategy with the highest percentage in top quartile is 2nd Person (30%). Our extension Table 2 shows that our novelly discovered Indefinite Pronouns strategy represents a higher percentage (39%), with a politeness score of -0.13. Moreover, our Punctuation strategy also turns out to be a top scoring negative politeness strategy and in the top three among all strategies (after Gratitude and Deference). It has a score of -0.71, whereas the second top negative politeness strategy (Direct Start) has a much lower score of -0.43.\nFinally, in terms of accuracies, our newly discovered features of Indefinite Pronouns and Punctuation improved the featurized system of DanescuNiculescu-Mizil et al. (2013) (see Table 1).7 This reduction of performance gap w.r.t. the CNN partially explains the success of these neural models in automatically learning useful linguistic features."}, {"heading": "7 Conclusion", "text": "We presented an interpretable neural network approach to politeness prediction. Our simple CNN model improves over previous work with manuallydefined features. More importantly, we then understand the reasons for these improvements via three visualization techniques and discover some novel high-scoring politeness strategies which, in turn, quantitatively explain part of the performance gap between the featurized and neural models."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their helpful comments. This work was supported by an IBM Faculty Award, a Bloomberg Research Grant, and an NVIDIA GPU donation to MB.\n7Our NN visualizations also led to an interesting feature correction. In the \u2019With Discovered Features\u2019 result in Table 1, we also removed the existing pronoun features (#14-18) based on the observation that those had weaker activation and saliency contributions (and lower top-quartile %) than the new indefinite pronoun feature. This correction and adding the two new features contributed \u223c50-50 to the total accuracy improvement.\nAppendix This is supplementary material for the main paper, where we present more detailed analysis and visualization examples, and our dataset and training details."}, {"heading": "A Activation Clusters", "text": "A.1 Rediscovering Existing Strategies\nGratitude (+) Respect and appreciation paid to the listener. Activation cluster examples: \u201c{thanks for the heads up\u201d; \u201cthank you very much for this kind gesture\u201d; \u201cthanks for help!\u201d}\nGreeting (+) A welcoming message for the converser. Activation cluster examples: {\u201chey, long time no seeing! how\u2019s stuff?\u201d; \u201cgreetings, sorry to bother you here... \u201d}\nPositive Lexicon (+) Expressions that build a positive relationship in the conversation and contain positive words from the sentiment lexicon, e.g., great, nice, good. Activation cluster examples: {\u201cyour new map is a great\u201d; \u201cvery nice article\u201d; \u201cyes, this is a nice illustration. i \u2019d love to...\u201d}\nCounterfactual Modal (+) Indirect strategies that imply a burden on the addressee and yet provide a face-saving opportunity of denying the request, usually containing hedges such as Would it be.../Could you please. Activation cluster examples: {\u201cwould you be interested in creating an infobox for windmills...?; \u201cwould you mind retriveing the bibliographic data?\u201d}\nDeference (+) A way of sharing the burden of a request placed on the addressee. Activation cluster examples: {\u201cnice work so far on your rewrite...\u201d, \u201chey, good work on the new pages...\u201d, \u201cgood point for the text...\u201d, \u201cyou make some good points...\u201d}\nDirect Question (-) Questions imposed on the converser in a direct manner with a demand of a factual answer. Activation cluster examples: {\u201cwhy would one want to re-create gnaa?\u201d; \u201cwhat\u2019s with the radio , and fist in the air?\u201d; \u201cwhat level warning is appropriate?\u201d}\nA.2 Extending Existing Strategies Counterfactual Modal (+) Sentences with Would you/Could you get grouped together as expected; but in addition, the cluster contains requests with Do you mind. Activation cluster examples: \u201c{do you mind having another look?\u201d; \u201cdo you mind if i migrate these to your userspace for you?\u201d}\nGratitude (+) Our CNN learns a special shade of gratitude, namely it distinguishes a cluster consisting of the bigram thanks for. Activation cluster examples: \u201cthanks for the good advice.\u201d; \u201cthanks for letting me know.\u201d; \u201cfair enough, thanks for assuming good faith\u201d}\nIndicative Modal (+) The same neuron as for counterfactual modal cluster above also gets activated on gapped 3-grams like Can you ... please?, which presumably implies that the combination of a later please with future-oriented variants can/will in the request gives a similar effect as the conditionaloriented variants would/could. Activation cluster examples: \u201ccan this be reported to london grid, please?\u201d; \u201ccan you delete it again, please?\u201d; \u201cgood start . can you add more, please?\u201d}\nA.3 Discovering Novel Strategies Indefinite Pronouns (-) Danescu-NiculescuMizil et al. (2013) distinguishes requests with first and second person (plural, starting position, etc.). However, we find activations that also react to indefinite pronouns such as something/somebody. Activation cluster examples: {\u201cam i missing something here?\u201d; \u201che \u2019s gone. was it something i said?\u201d; \u201cyou added the tag and then mentioned it on talkyou did not gain consensus first or even wait for anyone to discuss it. \u201d; \u201cbut how can something be both correct and incorrect\u201d }\nPunctuation (-) Though non-characteristic in direct speech, punctuation appears to be an important special marker in online communities, which in some sense captures verbal emotion in text. One of our neuron clusters gets activated on question marks \u201c???\u201d and one on ellipsis \u201c...\u201d. Activation cluster examples of question marks: {\u201cnow???\u201d; \u201coriginal article????\u201d; \u201chelllo?????\u201d} Activation cluster examples of ellipsis: {\u201cummm , it \u2019s a soft redirect. a placeholder for a future page ... is there a problem\n?\u201d; \u201cIndeed ... the trolls just keep coming.\u201d; \u201cI can\u2019t remember if i asked/checked to see if it got to you? so ... did it ?\u201d}"}, {"heading": "B First Derivative Saliency", "text": "In Fig. 1, we show some additional examples of saliency heatmaps. In the first heatmap, we see a clear example of the Positive Lexicon politeness strategy. The key great captures most of the weight for the final decision making. Note that, in particular, the question mark in this case provides no influence. Contrast that to the second figure, which echos back the proposed negative politeness strategy on punctuation from Section 6.1.3. Initial question marks give a high influence in magnitude for the negative predicted label. In the third example, we see that these punctuation markers still provide a lot of emphasis. For instance, other words such as really, successful and a personal pronoun I have very little impact. Overall, this exemplifies Direct Question strategy since most of the focus is on why.\nAs was noted in the embedding space transfor-\nmations discussion, the Gratitude key thanks with a preposition for has a much stronger polarity than other positive politeness keys in the fourth heatmap. Indeed, can you please does not nearly provide as much value. In the fifth heatmap, the sensitivity of the final score comes more from the greeting, namely hi, as compared to the phrase can you please tell me or positive lexicon very nice. These results match the politeness score results in Table 3 of Danescu-Niculescu-Mizil et al. (2013), where the Greeting strategy has a score of 0.87 compared to 0.49 for Please strategy and 0.12 for Positive Lexicon strategy. The sixth and last heatmap demonstrates the contribution of indefinite pronouns. In this case phrase am I missing something with the focus on the latter two words decides the final label prediction."}, {"heading": "C Dataset and Training Details", "text": "We split the Wikipedia and Stack Exchange datasets of Danescu-Niculescu-Mizil et al. (2013) into training, validation and test sets with 70%, 10%, and\n20% of the data respectively (after random shuffling). Therefore, the final split for Wikipedia is 1523, 218 and 436; and for Stack Exchange it is 2298, 328, and 657, respectively. We will make the dataset split indices publicly available.\nWe use 300-dim pre-trained word2vec embeddings as input to the CNN Mikolov et al. (2014), and then allow fine-tuning of the embeddings during training. All sentence tokenization is done using NLTK (Bird, 2006). For words not present in the pre-trained set, we use uniform unit scaling initialization.\nWe implement our model using a python version of TensorFlow (Abadi et al., 2015). Hyperparameters, e.g., the mini-batch size, learning rate, optimizer type, and dropout rate were tuned using the validation set of Wikipedia via grid search.8 The final chosen values were a mini-batch size of 32, a learning rate of 0.001 for the Adam Optimizer, a dropout rate of 0.5, filter windows are 3, 4, and 5 with 75 feature maps each, and ReLU as non-linear transformation function (Nair and Hinton, 2010). For convolution layers, we use valid padding and strides of all ones."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["sudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "sudevan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "sudevan et al\\.", "year": 2015}, {"title": "Face and politeness: new (insights) for old (concepts)", "author": ["Francesca Bargiela-Chiappini"], "venue": "Journal of pragmatics,", "citeRegEx": "Bargiela.Chiappini.,? \\Q2003\\E", "shortCiteRegEx": "Bargiela.Chiappini.", "year": 2003}, {"title": "Nltk: the natural language toolkit", "author": ["Steven Bird"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "Bird.,? \\Q2006\\E", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "Politeness: Some universals in language usage, volume 4", "author": ["Brown", "Levinson1987] Penelope Brown", "Stephen C Levinson"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1987}, {"title": "A neural network based approach for sentiment classification in the blogosphere", "author": ["Chen et al.2011] Long-Sheng Chen", "Cheng-Hsiang Liu", "Hui-Ju Chiu"], "venue": "Journal of Informetrics,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A computational approach to politeness with application to social factors", "author": ["Moritz Sudhof", "Dan Jurafsky", "Jure Leskovec", "Christopher Potts"], "venue": "Proceedings of ACL", "citeRegEx": "DanescuNiculescu.Mizil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "DanescuNiculescu.Mizil et al\\.", "year": 2013}, {"title": "Adaptive recursive neural network for target-dependent twitter sentiment classification", "author": ["Dong et al.2014] Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["dos Santos", "Maira Gatti"], "venue": "In Proceedings of COLING,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Erhan et al.2009] Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Twitter brand sentiment analysis: A hybrid system using n-gram analysis and dynamic artificial neural network", "author": ["Ghiassi et al.2013] M Ghiassi", "J Skinner", "D Zimbra"], "venue": "Expert Systems with applications,", "citeRegEx": "Ghiassi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ghiassi et al\\.", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Go et al.2009] Alec Go", "Richa Bhayani", "Lei Huang"], "venue": "CS224N Project", "citeRegEx": "Go et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Politeness phenomena in modern chinese", "author": ["Yueguo Gu"], "venue": "Journal of pragmatics,", "citeRegEx": "Gu.,? \\Q1990\\E", "shortCiteRegEx": "Gu.", "year": 1990}, {"title": "Understanding politeness", "author": ["K\u00e1d\u00e1r", "Haugh2013] D\u00e1niel Z K\u00e1d\u00e1r", "Michael Haugh"], "venue": null, "citeRegEx": "K\u00e1d\u00e1r et al\\.,? \\Q2013\\E", "shortCiteRegEx": "K\u00e1d\u00e1r et al\\.", "year": 2013}, {"title": "Representation of linguistic form and function in recurrent neural networks. arXiv preprint arXiv:1602.08952", "author": ["K\u00e1d\u00e1r et al.2016] Akos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi"], "venue": null, "citeRegEx": "K\u00e1d\u00e1r et al\\.,? \\Q2016\\E", "shortCiteRegEx": "K\u00e1d\u00e1r et al\\.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of ACL", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Visualizing and understanding recurrent networks", "author": ["Justin Johnson", "Fei-Fei Li"], "venue": "In Proceedings of ICLR Workshop", "citeRegEx": "Karpathy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2016}, {"title": "Sentiment classification of movie reviews using contextual valence shifters", "author": ["Kennedy", "Inkpen2006] Alistair Kennedy", "Diana Inkpen"], "venue": "Computational intelligence,", "citeRegEx": "Kennedy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kennedy et al\\.", "year": 2006}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "In Proceedings of ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Visualizing and understanding neural models in NLP", "author": ["Li et al.2016] Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": "In Proceedings of NAACL", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Politeness theory and relational work", "author": ["Locher", "Watts2005] Miriam A Locher", "Richard J Watts"], "venue": "Journal of Politeness Research. Language, Behaviour,", "citeRegEx": "Locher et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Locher et al\\.", "year": 2005}, {"title": "Understanding deep image representations by inverting them", "author": ["Mahendran", "Vedaldi2015] Aravindh Mahendran", "Andrea Vedaldi"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Mahendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran et al\\.", "year": 2015}, {"title": "Documentlevel sentiment classification: An empirical comparison between svm and ann", "author": ["Joao Francisco Valiati", "Wilson P Gavi\u00e3O Neto"], "venue": "Expert Systems with Applications,", "citeRegEx": "Moraes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Moraes et al\\.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of ICML,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Lee2004] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of ACL,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Pang et al.2002] Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Evaluating the visualization of what a deep neural network has learned", "author": ["Samek et al.2016] Wojciech Samek", "Alexander Binder", "Gr\u00e9goire Montavon", "Sebastian Bach", "KlausRobert M\u00fcller"], "venue": "IEEE Transactions on Neural Networks and Learning Systems", "citeRegEx": "Samek et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Samek et al\\.", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": "In Proceedings of ICLR Workshop", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "Proceedings of EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Fergus2014] Matthew D Zeiler", "Rob Fergus"], "venue": "In Proceedings of ECCV,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Politeness theories (Brown and Levinson, 1987; Gu, 1990; Bargiela-Chiappini, 2003) include key components such as modality, indirection, deference, and impersonalization.", "startOffset": 20, "endOffset": 82}, {"referenceID": 1, "context": "Politeness theories (Brown and Levinson, 1987; Gu, 1990; Bargiela-Chiappini, 2003) include key components such as modality, indirection, deference, and impersonalization.", "startOffset": 20, "endOffset": 82}, {"referenceID": 1, "context": "Politeness theories (Brown and Levinson, 1987; Gu, 1990; Bargiela-Chiappini, 2003) include key components such as modality, indirection, deference, and impersonalization. Positive politeness strategies focus on making the hearer feel good through offers, promises, and jokes. Negative politeness examples include favor seeking, orders, and requests. Differentiating among politeness types is a highly nontrivial task, because it depends on factors such as a context, relative power, and culture. Danescu-Niculescu-Mizil et al. (2013) proposed a useful computational framework for predicting politeness in natural language requests by designing various lexical and syntactic features about key politeness theories, e.", "startOffset": 57, "endOffset": 534}, {"referenceID": 9, "context": "To this end, we present several visualization strategies: activation clustering, first derivative saliency, and embedding space transformations, some of which are inspired by similar strategies in computer vision (Erhan et al., 2009; Simonyan et al., 2014; Girshick et al., 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al.", "startOffset": 213, "endOffset": 279}, {"referenceID": 29, "context": "To this end, we present several visualization strategies: activation clustering, first derivative saliency, and embedding space transformations, some of which are inspired by similar strategies in computer vision (Erhan et al., 2009; Simonyan et al., 2014; Girshick et al., 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al.", "startOffset": 213, "endOffset": 279}, {"referenceID": 11, "context": "To this end, we present several visualization strategies: activation clustering, first derivative saliency, and embedding space transformations, some of which are inspired by similar strategies in computer vision (Erhan et al., 2009; Simonyan et al., 2014; Girshick et al., 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al.", "startOffset": 213, "endOffset": 279}, {"referenceID": 21, "context": ", 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al., 2016; K\u00e1d\u00e1r et al., 2016).", "startOffset": 82, "endOffset": 119}, {"referenceID": 15, "context": ", 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al., 2016; K\u00e1d\u00e1r et al., 2016).", "startOffset": 82, "endOffset": 119}, {"referenceID": 27, "context": "Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013).", "startOffset": 104, "endOffset": 208}, {"referenceID": 12, "context": "Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013).", "startOffset": 104, "endOffset": 208}, {"referenceID": 10, "context": "Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013).", "startOffset": 104, "endOffset": 208}, {"referenceID": 4, "context": "work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014).", "startOffset": 78, "endOffset": 213}, {"referenceID": 30, "context": "work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014).", "startOffset": 78, "endOffset": 213}, {"referenceID": 24, "context": "work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014).", "startOffset": 78, "endOffset": 213}, {"referenceID": 7, "context": "work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014).", "startOffset": 78, "endOffset": 213}, {"referenceID": 16, "context": "work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014).", "startOffset": 78, "endOffset": 213}, {"referenceID": 20, "context": "There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015).", "startOffset": 101, "endOffset": 223}, {"referenceID": 29, "context": "There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015).", "startOffset": 101, "endOffset": 223}, {"referenceID": 28, "context": "There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015).", "startOffset": 101, "endOffset": 223}, {"referenceID": 17, "context": "There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015). Recently in NLP, Li et al. (2016) successfully adopt computer vision techniques, namely first-order saliency, and present representation plotting for sentiment compositionality across RNN variants.", "startOffset": 102, "endOffset": 259}, {"referenceID": 14, "context": "Similarly, K\u00e1d\u00e1r et al. (2016) analyze the omission scores and top-k contexts of hidden units of a multimodal RNN.", "startOffset": 11, "endOffset": 31}, {"referenceID": 14, "context": "Similarly, K\u00e1d\u00e1r et al. (2016) analyze the omission scores and top-k contexts of hidden units of a multimodal RNN. Karpathy et al. (2016) visualize character-level language models.", "startOffset": 11, "endOffset": 138}, {"referenceID": 5, "context": "This convolutional layer is then followed by a max-over-pooling operation (Collobert et al., 2011) that gives C = max{c} of the particular filter.", "startOffset": 74, "endOffset": 98}, {"referenceID": 6, "context": "We used the two datasets released by DanescuNiculescu-Mizil et al. (2013): Wikipedia (Wiki) and Stack Exchange (SE), containing community requests with politeness labels.", "startOffset": 37, "endOffset": 74}, {"referenceID": 11, "context": "Activation clustering is a non-parametric approach (adopted from Girshick et al. (2014)) of computing", "startOffset": 65, "endOffset": 88}, {"referenceID": 29, "context": "Inspired from neural network visualization in computer vision (Simonyan et al., 2014), the first derivative saliency method indicates how much each input", "startOffset": 62, "endOffset": 85}, {"referenceID": 6, "context": "Finally, in terms of accuracies, our newly discovered features of Indefinite Pronouns and Punctuation improved the featurized system of DanescuNiculescu-Mizil et al. (2013) (see Table 1).", "startOffset": 136, "endOffset": 173}, {"referenceID": 2, "context": "All sentence tokenization is done using NLTK (Bird, 2006).", "startOffset": 45, "endOffset": 57}], "year": 2016, "abstractText": "We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks.", "creator": "LaTeX with hyperref package"}}}