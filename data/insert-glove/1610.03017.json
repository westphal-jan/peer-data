{"id": "1610.03017", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation", "abstract": "gosar Most xlii existing machine mlanghenry translation systems khaba operate tarhunah at the synovus level of 600-page words, antil relying mesilla on faroes explicit segmentation penmanship to coddling extract acocks tokens. f-4b We introduce 90-84 a drove neural machine translation (NMT) model half-court that americanos maps water-borne a source character paikin sequence to 2,025 a boyac\u00e1 target character sequence without any segmentation. zenawi We employ \u00e9vry a character - level convolutional coveleski network sander with max - pooling at the 2000-5 encoder boire to reduce the dollmaker length veedu of source mckeen representation, farian allowing sabato the model leonore to oversea be frederiksen trained kronen at latkes a speed comparable 119-110 to subword - ace level rainham models while atelectasis capturing pericardium local 19-foot regularities. qera Our sainty character - to - loay character biochar model outperforms admirers a danger recently tallien proposed moerman baseline 3,507 with a liddon subword - educating level niagara encoder toraja on WMT ' marshaling 15 DE - mycophenolate EN and CS - EN, 1,422 and orosz gives comparable performance mirinda on 15utl FI - EN hsichih and raps RU - gavard EN. sundridge We then demonstrate that xi'an it is possible jmalone@coxnews.com to pistols share a single edmonson character - level 7-of-13 encoder privatization across contactor multiple languages epiphyllum by karanjia training mgk a minetti model on columbiformes a many - jadran to - rantoul one kishori translation task. grubel In nyakairima this lyder multilingual ice-cold setting, dimier the \u00e1kos character - counteraction level encoder husni significantly outperforms komatsu the subword - level encoder propounding on cdc42 all carolers the language bropleh pairs. 1941-45 We liftback also observe that protohumans the garze quality of the multilingual character - level translation even indian surpasses jettison the models chappie trained and four-times tuned on dore one coppo language 102.92 pair, namely radnik on havilland CS - paray EN, washerwomen FI - EN xilie and 863 RU - EN.", "histories": [["v1", "Mon, 10 Oct 2016 18:19:34 GMT  (380kb,D)", "http://arxiv.org/abs/1610.03017v1", "14 pages, 2 figures"], ["v2", "Tue, 1 Nov 2016 17:51:32 GMT  (415kb,D)", "http://arxiv.org/abs/1610.03017v2", "15 pages, 2 figures"], ["v3", "Tue, 13 Jun 2017 03:32:34 GMT  (326kb,D)", "http://arxiv.org/abs/1610.03017v3", "Transactions of the Association for Computational Linguistics (TACL), 2017"]], "COMMENTS": "14 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jason lee", "kyunghyun cho", "thomas hofmann"], "accepted": true, "id": "1610.03017"}, "pdf": {"name": "1610.03017.pdf", "metadata": {"source": "CRF", "title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation", "authors": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann"], "emails": ["jasonlee@inf.ethz.ch", "kyunghyun.cho@nyu.edu", "thomas.hofmann@inf.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "Nearly all previous work in machine translation has been at the level of words. Aside from our intuitive understanding of word as a basic unit of mean-\n\u2217The majority of this work was completed while the author was visiting New York University.\ning (Jackendoff, 1992), one reason behind this is that sequences are significantly longer when represented in characters, compounding the problem of data sparsity and modeling long-range dependencies. This has driven NMT research to be almost exclusively word-level (Bahdanau et al., 2015; Sutskever et al., 2015).\nDespite their remarkable success, word-level NMT models suffer from several major weaknesses. For one, they are unable to model rare, out-ofvocabulary words, making them limited in translating languages with rich morphology such as Czech, Finnish and Turkish. If one uses a large vocabulary to combat this (Jean et al., 2015), the complexity of training and decoding grows linearly with respect to the target vocabulary size, leading to a vicious cycle.\nTo address this, we present a fully character-level NMT model that maps a character sequence in a source language to a character sequence in a target language. We show that our model outperforms a baseline with a subword-level encoder on DE-EN and CS-EN, and achieves a comparable result on FIEN and RU-EN. We were able to train this model at a reasonable speed by drastically reducing the length of source sentence representation using a stack of convolutional and highway layers.\nOne advantage of character-level models is that they are better suited for multilingual translation than their word-level counterparts which require a separate word vocabulary for each language. We verify this by training a single model to translate four languages (German, Czech, Finnish and Russian) to English. Our multilingual character-level model exceeds the subword-level baseline by a con-\nar X\niv :1\n61 0.\n03 01\n7v 1\n[ cs\n.C L\n] 1\n0 O\nct 2\nsiderable margin in all four language pairs, strongly indicating that a character-level model is more flexible in assigning its capacity to different language pairs. Furthermore, we also observe our multilingual character-level translation even exceeds the quality of bilingual translation in three out of four language pairs. This demonstrates excellent parameter efficiency of character-level translation in a multilingual setting. We also showcase our model\u2019s ability to handle intra-sentence code-switching, while performing language identification on the fly.\nThe contributions of this work are twofold: we empirically show that (1) we can train character-tocharacter NMT model without any explicit segmentation; and (2) we can share a single character-level encoder across multiple languages to build a multilingual translation system without increasing the model size."}, {"heading": "2 Background: Attentional Neural Machine Translation", "text": "Neural machine translation (NMT) is a recently proposed approach to machine translation that builds a single neural network which takes as an input a source sentence X = (x1, . . . , xTx) and generates its translation Y = (y1, . . . , yTy), where xt and yt\u2032 are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Cho et al., 2014a). Attentional NMT models have three components: an encoder, a decoder and an attention mechanism.\nEncoder Given a source sentence X , the encoder constructs a continuous representation that summarizes its meaning with a recurrent neural network (RNN). A bidirectional RNN is often implemented as proposed in (Bahdanau et al., 2015). A forward encoder reads the input sentence from left to right: \u2212\u2192 h t = \u2212\u2192 fenc ( Ex(xt), \u2212\u2192 h t\u22121 ) . Similarly, a backward encoder reads it from right to left: \u2190\u2212 h t = \u2190\u2212 fenc ( Ex(xt), \u2190\u2212 h t+1 ) , where Ex is a source embedding lookup table, and \u2212\u2192 fenc and\u2190\u2212 fenc are recurrent activation functions such as long short-term memory units (LSTMs, (Hochreiter and Schmidhuber, 1997)) or gated recurrent units (GRUs, (Cho et al., 2014b)). The encoder constructs a set of continuous source sentence representations\nC by concatenating the forward and backward hidden states at each timestep: C = { h1, . . . ,hTx } ,\nwhere ht = [\u2212\u2192 h t; \u2190\u2212 h t ] .\nAttention First introduced in (Bahdanau et al., 2015), the attention mechanism lets the decoder attend more to different source symbols for each target symbol. More concretely, it computes the context vector ct\u2032 at each decoding time step t\u2032 as a weighted sum of the source hidden states: ct\u2032 = \u2211Tx t=1 \u03b1t\u2032tht. Each attentional weight \u03b1t\u2032t represents how relevant the t-th source token xt is to the t\u2032-th target token yt\u2032 , and is computed as:\n\u03b1t\u2032t = 1 Z exp ( score ( Ey(yt\u2032\u22121), st\u2032\u22121,ht )) , (1)\nwhere Z = \u2211Tx k=1 exp ( score(Ey(yt\u2032\u22121), st\u2032\u22121,hk) ) is the normalization constant. score() is a feedforward neural network with a single hidden layer that scores how well the source symbol xt and the target symbol yt\u2032 match. Ey is the target embedding lookup table and st\u2032 is the target hidden state at time t\u2032.\nDecoder Given a source context vector ct\u2032 , the decoder computes its hidden state at time t\u2032 as: st\u2032 = fdec ( Ey(yt\u2032\u22121), st\u2032\u22121, ct\u2032 ) . Then, a parametric function outk() returns the conditional probability of the next target symbol being k:\np(yt\u2032 =k|y<t\u2032 , X) = 1 Z exp ( outk ( Ey(yt\u2032\u22121), st\u2032 , ct\u2032 )) (2) where Z is again the normalization constant:\nZ = \u2211 j exp ( outj(Ey(yt\u2032\u22121), st\u2032 , ct\u2032) ) .\nTraining The entire model can be trained end-toend by minimizing the negative conditional loglikelihood, which is defined as:\nL(\u03b8) = \u2212 1 N N\u2211 n=1 Ty\u2211 t=1 log p(yt = y (n) t |y (n) <t , X (n)),\nwhere N is the number of sentence pairs, and X(n) and y(n)t are the source sentence and the t-th target symbol in the n-th pair, respectively."}, {"heading": "3 Fully Character-Level Translation", "text": ""}, {"heading": "3.1 Why Character-Level?", "text": "The benefits of character-level translation over word-level translation are already well known. Chung et al. (2016) present three main arguments: character level models (1) do not suffer from outof-vocabulary issues, (2) are able to model different, rare morphological variants of a word, and (3) do not require segmentation. Particularly, text segmentation is highly non-trivial for many languages and problematic even for English as word tokenizers are either manually designed or trained on a corpus using an objective function that is unrelated to the translation task at hand, which makes the overall system sub-optimal.\nHere we present two additional arguments for character-level translation. First, a character-level translation system can easily be applied to a multilingual translation setting. Between European languages where the majority of alphabets overlaps, for instance, a character-level model may easily identify morphemes that are shared across different languages. A word-level model, however, will need a separate word vocabulary for each language, allowing no cross-lingual parameter sharing.\nAlso, by not segmenting source sentences into words, we no longer inject our knowledge of words and word boundaries into the system; instead, we encourage the model to find an internal structure of a sentence by itself and learn how a sequence of symbols can be mapped to a continuous meaning representation."}, {"heading": "3.2 Related Work", "text": "To address these limitations associated with wordlevel translation, a recent line of research has investigated using sub-word information.\nCosta-jussa\u0300 and Fonollosa (2016) replaced the word-lookup table with convolutional and highway layers on top of character embeddings, while still segmenting source sentences into words. Target sentences were also segmented into words, and prediction was made at word-level.\nSimilarly, Ling et al. (2015) employed a bidirectional LSTM to compose character embeddings into word embeddings. At the target side, another LSTM takes the hidden state of the decoder and\ngenerates the target word, character by character. While this system is completely open-vocabulary, it also requires offline segmentation. Also, characterto-word and word-to-character LSTMs significantly slow down training.\nMost recently, Luong and Manning (2016) proposed a hybrid scheme that consults character-level information whenever the model encounters an outof-vocabulary word. As a baseline, they also implemented a purely character-level NMT model with 4 layers of unidirectional LSTMs with 512 cells, with attention over each character. Despite being extremely slow (approximately 3 months to train), the character-level model gave comparable performance to the word-level baseline. This shows the possibility of fully character-level translation.\nHaving a word-level decoder restricts the model to only being able to generate previously seen words. Sennrich et al. (2015) introduced a subword-level NMT model that is capable of open-vocabulary translation using subword-level segmentation based on the byte pair encoding (BPE) algorithm. Starting from a character vocabulary, the algorithm identifies frequent character n-grams in the training data and iteratively adds them to the vocabulary, ultimately giving a subword vocabulary which consists of words, subwords and characters. Once the segmentation rules have been learned, their model performs subword-to-subword translation (bpe2bpe) in the same way as word-to-word translation.\nPerhaps the work that is closest to our end goal is (Chung et al., 2016), which used a subword-level encoder from (Sennrich et al., 2015) and a fully character-level decoder (bpe2char). Their results show that character-level decoding performs better than subword-level decoding. Motivated by this work, we aim for fully character-level translation at both sides (char2char).\nOutside NMT, our work is based on a few existing approaches that applied convolutional networks to text, most notably in text classification (Zhang et al., 2015; Xiao and Cho, 2016). Also, we drew inspiration for our multilingual models from previous work that showed the possibility of training a single recurrent model for multiple languages in domains other than translation (Tsvetkov et al., 2016; Gillick et al., 2015)."}, {"heading": "3.3 Challenges", "text": "Sentences are on average 6 (DE, CS and RU) to 8 (FI) times longer when represented in characters. This poses three major challenges to achieving fully character-level translation.\n(1) Training/decoding latency For the decoder, although the sequence to be generated is much longer, each character-level softmax operation costs considerably less compared to a word- or subword-level softmax. Chung et al. (2016) report that characterlevel decoding is only 14% slower than subwordlevel decoding.\nOn the other hand, computational complexity of the attention mechanism grows quadratically with respect to the sentence length, as it needs to attend to every source token for every target token. This makes a naive character-level approach, such as in (Luong and Manning, 2016), computationally prohibitive. Consequently, reducing the length of the source sequence is key to ensuring reasonable speed in both training and decoding.\n(2) Mapping character sequence to continuous representation The arbitrary relationship between the orthography of a word and its meaning is a wellknown problem in linguistics (de Saussure, 1916). Building a character-level encoder is arguably a more difficult problem, as the encoder needs to learn a highly non-linear function from a long sequence\nof character symbols to a meaning representation.\n(3) Long range dependencies in characters A character-level encoder needs to model dependencies over longer timespans than a word-level encoder does."}, {"heading": "4 Fully Character-Level NMT", "text": ""}, {"heading": "4.1 Encoder", "text": "We design an encoder that addresses all the challenges discussed above by using convolutional and pooling layers aggressively to both (1) drastically shorten the input sentence and (2) efficiently capture local regularities. Inspired by the character-level language model from (Kim et al., 2015), our encoder first reduces the source sentence length with a series of convolutional, pooling and highway layers. The shorter representation, instead of the full character sequence, is passed through a bidirectional GRU to (3) help it resolve long term dependencies. We illustrate the proposed encoder in Figure 1 and discuss each layer in detail below.\nEmbedding We map the source sentence (x1, . . . , xTx) \u2208 R1\u00d7Tx to a sequence of character embeddings X = (C(x1), . . . ,C(xTx)) \u2208 Rdc\u00d7Tx where C is the character embedding lookup table: C \u2208 Rdc\u00d7|C|.\nConvolution One-dimensional convolution opera-\ntion is then used along consecutive character embeddings. Assuming we have a single filter f \u2208 Rdc\u00d7w of width w, we first apply padding to the beginning and the end of X , such that the padded sentence X \u2032 \u2208 Rdc\u00d7(Tx+w\u22121) is w \u2212 1 symbols longer. We then apply narrow convolution between X \u2032 and f such that the k-th element of the output Yk is given as:\nYk = (X \u2032 \u2217 f)k = \u2211 i,j (X \u2032[:,k\u2212w+1:k] \u2297 f)ij , (3)\nwhere \u2297 denotes elementwise matrix multiplication and \u2217 is the convolution operation. X \u2032[:,k\u2212w+1:k] is the sliced subset of X \u2032 that contains all the rows but only w adjacent columns. The padding scheme employed above, commonly known as half convolution, ensures the length of the output is identical to the input\u2019s: Y \u2208 R1\u00d7Tx .\nWe just illustrated the case of a single convolutional filter of fixed width above. In order to extract informative character patterns of different lengths, we employ a set of filters of varying widths. More concretely, we use a filter bank F = {f1, . . . , fm} where fi = Rdc\u00d7i\u00d7ni is a collection of ni filters of width i. Our model uses m = 8, hence extracts character n-grams up to 8 characters long. Outputs from all the filters are stacked upon each other, giving a single representation Y \u2208 RN\u00d7Tx , where the dimensionality of each column is given by the total number of filters N = \u2211m i ni. Finally, rectified linear activation (ReLU) is applied elementwise to this representation.\nMax pooling with stride The output from the convolutional layer is first split into segments of width s, and max-pooling over time is applied to each segment. This procedure selects the most salient features to give a segment embedding. Each segment embedding is a summary of a particular (overlapping) segment in the source sentence; this acts as our internal linguistic unit from this layer and above: the attention mechanism, for instance, attends to each source segment instead of source character.\nThis shortens the source representation s-fold: Y \u2032 \u2208 RN\u00d7(Tx/s). Empirically, we found using smaller s leads to better performance at increased training time. We chose s = 5 in our experiments.\nHighway network A sequence of segment embeddings from the max pooling layer is fed into a highway network (Srivastava et al., 2015). A highway network transforms input x with a gating mechanism that adaptively regulates information flow:\ny = g ReLU(Wx+ b) + (1\u2212 g) x,\nwhere g = \u03c3((Wtx + bt)). We apply this to each segment embedding individually.\nRecurrent layer Finally, the output from the highway layer is given to a bidirectional GRU from \u00a72, using each segment embedding as input."}, {"heading": "4.2 Attention and Decoder", "text": "Similarly to the attention model in (Bahdanau et al., 2015), a single-layer feedforward network computes the attention score of next target character to be generated with every source segment representation. A two-layer character-level decoder then takes the source context vector from the attention mechanism and predicts each target character."}, {"heading": "5 Experiment Settings", "text": ""}, {"heading": "5.1 Task and Models", "text": "We evaluate the proposed character-to-character (char2char) translation model against subwordlevel baselines (bpe2bpe and bpe2char) on the WMT\u201915 DE\u2192EN, CS\u2192EN, FI\u2192EN and\nRU\u2192EN translation tasks.1 We compare them in two different scenarios: 1) a bilingual setting where we train a model on data from a single language pair; and 2) a multilingual setting where the task is manyto-one translation: we train a single model on data from all four language pairs. Hence, our baselines and models are:\n(a) bilingual bpe2bpe: from (Firat et al., 2016a). (b) bilingual bpe2char: from (Chung et al., 2016). (c) bilingual char2char (d) multilingual bpe2char (e) multilingual char2char\nWe train all the models ourselves other than (a), for which we report the results from (Firat et al., 2016a). We detail the configuration of our models in Table 1 and Table 2."}, {"heading": "5.2 Datasets and Preprocessing", "text": "We use all available parallel data on the four language pairs from WMT\u201915: DE-EN, CS-EN, FI-EN and RU-EN.\nFor the bpe2char baselines, we only use sentence pairs where the source is no longer than 50 subword symbols. For our char2char models, we only use pairs where the source sentence is no longer than 450 characters. For all the language pairs apart from FI-EN, we use newstest-2013 as a development set and newstest-2014 and newstest-2015 as test sets. For FI-EN, we use newsdev-2015 and newstest-2015 as development and test sets respectively. We tokenize each corpus using a script from Moses.2\nWhen training bilingual bpe2char models, we extract 20,000 BPE operations from each of the source and target corpus using a script from (Sennrich et al., 2015). This gives a source BPE vocabulary of size 20k\u221224k for each language."}, {"heading": "5.3 Training Details", "text": "Each model is trained using stochastic gradient descent and Adam (Kingma and Ba, 2014) with learning rate 0.0001 and minibatch size 64. Training continues until the BLEU score on the validation set\n1http://www.statmt.org/wmt15/translatio n-task.html\n2This is unnecessary for char2char models, yet was carried out for comparison.\nstops improving. The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2013). All weights are initialized from a uniform distribution [\u22120.01, 0.01].\nEach model is trained on a single pre-2016 GTX Titan X GPU with 12GB RAM."}, {"heading": "5.4 Decoding Details", "text": "As from (Chung et al., 2016), a two-layer characterlevel attentional decoder with 1024 GRU units is used for all our experiments. For decoding, we use beam search with length-normalization to penalize shorter hypotheses. The width of our beam is 20 for char2char models and 5 for bpe2char models."}, {"heading": "5.5 Training Multilingual Models", "text": "Task description We train a model on a many-toone translation task to translate a sentence in any of the four languages (German, Czech, Finnish and Russian) to English. We do not provide a language identifier to the encoder, but merely the sentence itself, encouraging the model to perform language identification on the fly. In addition, by not providing the language identifier, we expect the model to handle intra-sentence code-switching seamlessly.\nModel architecture The multilingual char2char model uses slightly more convolutional filters than the bilingual char2char model, namely (200-200- 250-250-300-300-400-400). Otherwise, the architecture remains the same as shown in Table 1. By not changing the size of the encoder and the decoder, we fix the capacity of the core translation module, and only allow the multilingual model to detect more character patterns.\nSimilarly, the multilingual bpe2char model has the same encoder and decoder as the bilingual bpe2char model, but a larger vocabulary. We learn 50,000 multilingual BPE operations on the multilingual corpus, resulting in 54,544 subwords. See Table 2 for the exact configuration of our multilingual models.\nData scheduling For the multilingual models, an appropriate scheduling of data from different languages is crucial to avoid overfitting to one language too soon. Following (Firat et al., 2016a; Firat et al., 2016b), each minibatch is balanced, in that the proportion of each language pair in a single minibatch corresponds to that of the full corpus. With this minibatch scheme, roughly the same number of updates is required to make one full pass over the entire training corpus of each language pair. Minibatches from all language pairs are combined and presented to the model as a single minibatch. See Table 3 for the minibatch size for each language pair.\nTreatment of Cyrillic To facilitate cross-lingual parameter sharing, we convert every Cyrillic character in the Russian source corpus to Latin alphabet according to ISO-9. Table 4 shows an example of how this conversion may help the multilingual models identify lexemes that are shared across multiple languages.\nMultilingual BPE For the multilingual bpe2char model, multilingual BPE segmentation rules are extracted from a large dataset containing training source corpora of all the language pairs. To ensure\nthe BPE rules are not biased towards one language, larger datasets such as Czech and German corpora are trimmed such that every corpus contains an approximately equal number of characters."}, {"heading": "6 Quantitative Analysis", "text": "In this section, we first establish our main hypotheses for introducing character-level and multilingual models, and investigate whether our observations support or disagree with our hypotheses. From our empirical results, we want to verify: (1) if fully character-level translation outperforms subwordlevel translation, (2) in which setting and to what extent is multilingual translation beneficial and (3) if multilingual, character-level translation achieves superior performance to other models. We discuss each hypothesis in detail below.\n(1) Character- vs. subword-level In a bilingual setting, the char2char model clearly outperforms both subword-level baselines on DE-EN (Table 5 (ac)) and CS-EN (Table 5 (f-h)). On the other two language pairs, it exceeds the bpe2bpe model and achieves similar performance with the bpe2char baseline (Table 5 (k-m) and (p-r)). Overall, we observe that character-level translation on a single language pair performs as well as, or better than, subword-level translation.\nMeanwhile, in a multilingual setting, the character-level encoder significantly surpasses the subword-level encoder consistently in all the language pairs (Table 5 (d-e), (i-j), (n-o) and (s-t)). From this, we conclude that translating at the level of characters allows the model to discover shared constructs between languages more effectively. This also demonstrates that the character-level model is more flexible in assigning model capacity to different language pairs.\n(2) Multilingual vs. bilingual At the level of characters, we note that multilingual translation is indeed strongly beneficial. On the test sets, the multilingual character-level model outperforms the singlepair character-level model by 2.64 BLEU in FI-EN (Table 5 (m, o)) and 0.93 BLEU in CS-EN (Table 5 (h, j)), while achieving comparable results on DE-EN and RU-EN.\nAt the level of subwords, on the other hand, we do not observe the same degree of performance benefit from multilingual translation. Also, the multilingual bpe2char model requires much more updates to reach the performance of the bilingual bpe2char model (see Figure 2). This suggests that learning useful subword segmentation across languages is difficult.\n(3) Multilingual char2char vs. others The multilingual char2char model is the best performer in CS-EN, FI-EN and RU-EN (Table 5 (j, o, t)), and is the runner-up in DE-EN (Table 5 (e)). The fact that the multilingual char2char model outperforms the single-pair models goes to show the parameter efficiency of character-level translation: instead of training N separate models for N language pairs, it is possible to get better performance with one multilingual character-level model."}, {"heading": "7 Qualitative Analysis", "text": "In Table 6, we demonstrate our character-level model\u2019s robustness in four translation scenarios that conventional NMT systems are known to suffer in.\nWe also showcase our model\u2019s ability to seamlessly handle intra-sentence code-switching, or mixed utterances from two or more languages. We compare sample translations from the character-level model with those from the subword-level model, which already sidesteps some of the issues associated with word-level translation.\nWith real-world text containing typos and spelling mistakes, the quality of word-based translation would severely drop, as every non-canonical form of a word cannot be represented. On the other hand, a character-level model has a much better chance recovering the original word or sentence. Indeed, our char2char model is robust against a few spelling mistakes (Table 6 (a)).\nGiven a long, rare word such as \u201cSiebentausendzweihundertvierundfu\u0308nfzig\u201d (seven thousand two hundred fifty four) in Table 6 (b), the subword-level model incorrectly segments \u201cSiebentausend\u201d into (Sieb, ent, aus, end), instead of (Sieben, tausend); this results in an inaccurate translation. The character-level model performs better on these long, concatenative words with ambiguous segmentation.\nHandling morphological inflections should also be relatively easy for a character-level model. We observe that this is indeed the case, as our char2char model correctly understands \u201cgesperrt\u201d, a past participle form of \u201csperren\u201d (to block) (Table 6 (c)).\nNonce words are terms coined for a single use. They are not actual words but are constructed in a way that humans can intuitively guess what they mean, such as workoliday and friyay. We construct a few DE-EN sentence pairs that contain German\nnonce words (one example shown in Table 6 (d)), and observe that the character-level model can indeed detect salient character patterns and arrive at a correct translation.\nFinally, we evaluate our multilingual models\u2019 capacity to perform intra-sentence code-switching, by giving them as input mixed sentences from multiple languages. We discover that when given sentences with high degree of language intermixing, as in Table 6 (e) or Table 12 (k) in the Appendix,\nthe multilingual bpe2char model fails to seamlessly handle alternation of languages. Overall, however, both multilingual models generate reasonable translations. This is possible because we did not provide a language identifier when training our multilingual models; as a result, they learned to understand a multilingual sentence and translate it into a coherent English sentence.\nWe show more examples for each category in the Appendix.\nTraining and decoding speed On a single Titan X GPU, we observe that our char2char models are approximately 50% slower to train than our bpe2char baselines when the same batch size was used. Our bilingual character-level models can be trained in roughly two weeks.\nWe further note that the bilingual bpe2char model can translate 3,000 sentences in 66.63 minutes while the bilingual char2char model requires 71.71 minutes (online, not in batch). See Table 7 for the exact details.\nFurther observations We also note that the multilingual models are less prone to overfitting than the bilingual models. This is particularly visible for low-resource language pairs such as FI-EN. Figure 2 shows the evolution of the FI-EN validation BLEU scores where the bilingual models overfit rapidly but the multilingual models seem to regularize learning by training simultaneously on other language pairs."}, {"heading": "8 Conclusion", "text": "We propose a fully character-level NMT model that accepts a sequence of characters in the source language and outputs a sequence of characters in the\ntarget language. What is remarkable about this model is the absence of explicitly hard-coded knowledge of words and their boundaries, and that the model learns these concepts from a translation task alone.\nOur empirical results show that the fully character-level model performs as well as, or better than, subword-level translation models. The performance gap is distinctly pronounced in the multilingual many-to-one translation task, where results show that character-level model can assign model capacities to different languages more efficiently than the subword-level models. We observe a particularly large improvement in FI-EN translation when the model was trained to translate multiple languages, indicating positive cross-lingual transfer to a low-resource language pair.\nWe discover two main benefits of the multilingual character-level model: (1) it is much more parameter efficient than the bilingual models and (2) it can naturally handle intra-sentence code-switching as a result of the many-to-one translation task.\nThis work presents a case for fully character-level translation: that translation at the level of character is strongly beneficial and should be encouraged more.\nIn the next stage of this research, we will investigate extending our multilingual many-to-one translation models to perform many-to-many translation, which will allow the decoder, similarly with the encoder, to learn from multiple target languages. Furthermore, a more thorough investigation into model architectures and hyperparameters is needed."}, {"heading": "Acknowledgements", "text": "KC thanks the support by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016). This work was partly supported by Samsung Advanced Institute of Technology (Neural Machine Translation). JL was supported by Qualcomm Innovation Fellowship, and thanks David Yenicelik and Kevin Wallimann for their contribution in designing the qualitative analysis. The authors would also like to thank Prof. Zheng Zhang (NYU Shanghai) for fruitful discussion and comments."}, {"heading": "A Supplementary Examples", "text": "We show additional sample translations in five scenarios: spelling mistakes (Table 8), rare and long words (Table 9), nonce words (Table 10), morphological inflections (Table 11) and intra-sentence code-switching (Table 12)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of the 8th Workshop on Syntax, Semantics, and Structure in Statistical Trans-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the Empiricial Methods in Nat-", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-Juss\u00e1", "Jos\u00e8 A.R. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, page 357.", "citeRegEx": "Costa.Juss\u00e1 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.Juss\u00e1 and Fonollosa.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "arXiv preprint arXiv:1512.00103.", "citeRegEx": "Gillick et al\\.,? 2015", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Semantic Structures, volume 18", "author": ["Ray S. Jackendoff."], "venue": "MIT press.", "citeRegEx": "Jackendoff.,? 1992", "shortCiteRegEx": "Jackendoff.", "year": 1992}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the 3rd International Conference for Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML).", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Advances in Neural Information Processing Systems (NIPS 2015), volume 28.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems (NIPS 2015), volume 28.", "citeRegEx": "Sutskever et al\\.,? 2015", "shortCiteRegEx": "Sutskever et al\\.", "year": 2015}, {"title": "Polyglot neural language models: A case study in cross-lingual phonetic representation learning", "author": ["Yulia Tsvetkov", "Sunayana Sitaram", "Manaal Faruqui", "Guillaume Lample", "Patrick Littell", "David Mortensen", "Alan W Black", "Lori Levin", "Chris Dyer."], "venue": "Pro-", "citeRegEx": "Tsvetkov et al\\.,? 2016", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2016}, {"title": "Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers", "author": ["Yijun Xiao", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1602.00367.", "citeRegEx": "Xiao and Cho.,? 2016", "shortCiteRegEx": "Xiao and Cho.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems (NIPS 2015), volume 28.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "ing (Jackendoff, 1992), one reason behind this is that sequences are significantly longer when represented in characters, compounding the problem of data sparsity and modeling long-range dependencies.", "startOffset": 4, "endOffset": 22}, {"referenceID": 0, "context": "This has driven NMT research to be almost exclusively word-level (Bahdanau et al., 2015; Sutskever et al., 2015).", "startOffset": 65, "endOffset": 112}, {"referenceID": 18, "context": "This has driven NMT research to be almost exclusively word-level (Bahdanau et al., 2015; Sutskever et al., 2015).", "startOffset": 65, "endOffset": 112}, {"referenceID": 9, "context": "If one uses a large vocabulary to combat this (Jean et al., 2015), the complexity of training and decoding grows linearly with respect to the target vocabulary size, leading to a vicious cycle.", "startOffset": 46, "endOffset": 65}, {"referenceID": 0, "context": ", yTy), where xt and yt\u2032 are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Cho et al., 2014a).", "startOffset": 55, "endOffset": 141}, {"referenceID": 18, "context": ", yTy), where xt and yt\u2032 are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Cho et al., 2014a).", "startOffset": 55, "endOffset": 141}, {"referenceID": 14, "context": ", yTy), where xt and yt\u2032 are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Cho et al., 2014a).", "startOffset": 55, "endOffset": 141}, {"referenceID": 1, "context": ", yTy), where xt and yt\u2032 are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Cho et al., 2014a).", "startOffset": 55, "endOffset": 141}, {"referenceID": 0, "context": "A bidirectional RNN is often implemented as proposed in (Bahdanau et al., 2015).", "startOffset": 56, "endOffset": 79}, {"referenceID": 7, "context": "a source embedding lookup table, and \u2212\u2192 fenc and \u2190\u2212 fenc are recurrent activation functions such as long short-term memory units (LSTMs, (Hochreiter and Schmidhuber, 1997)) or gated recurrent units (GRUs, (Cho et al.", "startOffset": 137, "endOffset": 171}, {"referenceID": 2, "context": "a source embedding lookup table, and \u2212\u2192 fenc and \u2190\u2212 fenc are recurrent activation functions such as long short-term memory units (LSTMs, (Hochreiter and Schmidhuber, 1997)) or gated recurrent units (GRUs, (Cho et al., 2014b)).", "startOffset": 205, "endOffset": 224}, {"referenceID": 0, "context": "Attention First introduced in (Bahdanau et al., 2015), the attention mechanism lets the decoder attend more to different source symbols for each target symbol.", "startOffset": 30, "endOffset": 53}, {"referenceID": 3, "context": "Chung et al. (2016) present three main arguments: character level models (1) do not suffer from outof-vocabulary issues, (2) are able to model different, rare morphological variants of a word, and (3) do not require segmentation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Similarly, Ling et al. (2015) employed a bidirectional LSTM to compose character embeddings into word embeddings.", "startOffset": 11, "endOffset": 30}, {"referenceID": 13, "context": "Most recently, Luong and Manning (2016) proposed a hybrid scheme that consults character-level information whenever the model encounters an outof-vocabulary word.", "startOffset": 15, "endOffset": 40}, {"referenceID": 16, "context": "Sennrich et al. (2015) introduced a subword-level NMT model that is capable of open-vocabulary translation using subword-level segmentation based on the byte pair encoding (BPE) algorithm.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Perhaps the work that is closest to our end goal is (Chung et al., 2016), which used a subword-level encoder from (Sennrich et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 16, "context": ", 2016), which used a subword-level encoder from (Sennrich et al., 2015) and a fully character-level decoder (bpe2char).", "startOffset": 49, "endOffset": 72}, {"referenceID": 21, "context": "Outside NMT, our work is based on a few existing approaches that applied convolutional networks to text, most notably in text classification (Zhang et al., 2015; Xiao and Cho, 2016).", "startOffset": 141, "endOffset": 181}, {"referenceID": 20, "context": "Outside NMT, our work is based on a few existing approaches that applied convolutional networks to text, most notably in text classification (Zhang et al., 2015; Xiao and Cho, 2016).", "startOffset": 141, "endOffset": 181}, {"referenceID": 19, "context": "Also, we drew inspiration for our multilingual models from previous work that showed the possibility of training a single recurrent model for multiple languages in domains other than translation (Tsvetkov et al., 2016; Gillick et al., 2015).", "startOffset": 195, "endOffset": 240}, {"referenceID": 6, "context": "Also, we drew inspiration for our multilingual models from previous work that showed the possibility of training a single recurrent model for multiple languages in domains other than translation (Tsvetkov et al., 2016; Gillick et al., 2015).", "startOffset": 195, "endOffset": 240}, {"referenceID": 3, "context": "Chung et al. (2016) report that characterlevel decoding is only 14% slower than subwordlevel decoding.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "This makes a naive character-level approach, such as in (Luong and Manning, 2016), computationally prohibitive.", "startOffset": 56, "endOffset": 81}, {"referenceID": 10, "context": "Inspired by the character-level language model from (Kim et al., 2015), our encoder first reduces the source sentence length with a series of convolutional, pooling and highway layers.", "startOffset": 52, "endOffset": 70}, {"referenceID": 17, "context": "Highway network A sequence of segment embeddings from the max pooling layer is fed into a highway network (Srivastava et al., 2015).", "startOffset": 106, "endOffset": 131}, {"referenceID": 0, "context": "Similarly to the attention model in (Bahdanau et al., 2015), a single-layer feedforward network computes the attention score of next target character to be generated with every source segment representation.", "startOffset": 36, "endOffset": 59}, {"referenceID": 5, "context": "(a) bilingual bpe2bpe: from (Firat et al., 2016a).", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "(b) bilingual bpe2char: from (Chung et al., 2016).", "startOffset": 29, "endOffset": 49}, {"referenceID": 5, "context": "We train all the models ourselves other than (a), for which we report the results from (Firat et al., 2016a).", "startOffset": 87, "endOffset": 108}, {"referenceID": 16, "context": "2 When training bilingual bpe2char models, we extract 20,000 BPE operations from each of the source and target corpus using a script from (Sennrich et al., 2015).", "startOffset": 138, "endOffset": 161}, {"referenceID": 11, "context": "Each model is trained using stochastic gradient descent and Adam (Kingma and Ba, 2014) with learning rate 0.", "startOffset": 65, "endOffset": 86}, {"referenceID": 15, "context": "The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2013).", "startOffset": 58, "endOffset": 80}, {"referenceID": 3, "context": "As from (Chung et al., 2016), a two-layer characterlevel attentional decoder with 1024 GRU units is used for all our experiments.", "startOffset": 8, "endOffset": 28}, {"referenceID": 5, "context": "Following (Firat et al., 2016a; Firat et al., 2016b), each minibatch is balanced, in that the proportion of each language pair in a single minibatch corresponds to that of the full corpus.", "startOffset": 10, "endOffset": 52}, {"referenceID": 5, "context": "(\u2217) results are taken from (Firat et al., 2016a).", "startOffset": 27, "endOffset": 48}], "year": 2016, "abstractText": "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subwordlevel encoder on WMT\u201915 DE-EN and CSEN, and gives comparable performance on FIEN and RU-EN. We then demonstrate that it is possible to share a single characterlevel encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We also observe that the quality of the multilingual character-level translation even surpasses the models trained and tuned on one language pair, namely on CSEN, FI-EN and RU-EN.", "creator": "LaTeX with hyperref package"}}}