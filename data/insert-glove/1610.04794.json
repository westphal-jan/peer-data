{"id": "1610.04794", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2016", "title": "Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering", "abstract": "semenya Most learning kunashiri approaches treat dimensionality mahfood reduction (DR) and clustering parnon separately (2.575 i. burnard e. , sequentially ), but hounded recent sulur research charlottetown has 888,000 shown tuleyev that harbinson optimizing channelization the two enjoining tasks parganas jointly can pharmacie substantially improve 200.4 the ranchi performance zinfandels of 5-shot both. monicas The kanakas premise behind mudlark the latter genre amirian is franky that 8.71 the natiq data samples are obtained 33-0 via babalon linear ferdous transformation of outsmarted latent representations ryckman that are easy to varvarin cluster; but w\u00f6rth in ss7 practice, the epiclesis transformation otley from physicality the latent jawed space bonfim to the bergavenny data al-nabawi can asla be more complicated. In this linesmen work, shijing we swanson assume sharpstown that formats this transformation jordaan is an md5 unknown benrubi and possibly nonlinear function. To recover the ` clustering - friendly ' latent 996-0089 representations and titid to better non-domestic cluster shenley the newsworthy data, we propose 1.5385 a joint DR spoofy and K - derr means clustering hausler approach euskara in rimini which DR is accomplished technocratic via learning information-theoretic a nikolsburg deep netherwood neural r-type network (equaled DNN ). \u03ba\u03cd\u03c1\u03b9\u03bf\u03c2 The souss-massa-dr\u00e2a motivation is combes to keep the 100c advantages of family-like jointly readout optimizing tudeh the kaepernick two tasks, while ouali exploiting montan the karter deep byg neural network ' joksimovi\u0107 s ability to pni approximate varmint any chraidi nonlinear pillowy function. This fotsis way, 14.625 the shover proposed landbridge approach railworkers can work well ballhandler for upcott a benchers broad class 5-million of generative models. dreamlands Towards megat this capria end, wahidullah we beauharnais carefully design the DNN structure and compassionate the associated joint neumarkt optimization abriendo criterion, amiloride and tabriz propose an bawi effective men and 145.2 scalable 23:50 algorithm to carven handle the corrals formulated optimization problem. Experiments using five different retailers real 84-percent datasets are employed kumul to showcase fidgeting the imo effectiveness of zhisheng the latifolium proposed descrambler approach.", "histories": [["v1", "Sat, 15 Oct 2016 22:51:06 GMT  (5826kb,D)", "http://arxiv.org/abs/1610.04794v1", "Main paper: 9 pages; Supplementary material: 3 pages"], ["v2", "Tue, 13 Jun 2017 22:40:26 GMT  (646kb,D)", "http://arxiv.org/abs/1610.04794v2", "Final ICML2017 version. Main paper: 10 pages; Supplementary material: 4 pages"]], "COMMENTS": "Main paper: 9 pages; Supplementary material: 3 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bo yang", "xiao fu", "nicholas d sidiropoulos", "mingyi hong"], "accepted": true, "id": "1610.04794"}, "pdf": {"name": "1610.04794.pdf", "metadata": {"source": "CRF", "title": "Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering", "authors": ["Bo Yang", "Xiao Fu", "Nicholas D. Sidiropoulos", "Mingyi Hong"], "emails": ["yang4173@umn.edu", "xfu@umn.edu", "nikos@ece.um.edu", "mingyi@iastate.edu"], "sections": null, "references": [{"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "For example, the stacked autoencoder (SAE) [27], deep CCA (DCCA) [1], and sparse autoencoder [18] take insights from PCA, CCA, and sparse coding, respectively, and make use of DNNs to learn nonlinear mappings from the data domain to low-dimensional latent spaces.", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "In addition to the above classic DR methods that essentially learn a linear generative model from the latent space to the data domain, nonlinear DR approaches such as those used in spectral clustering and DNN-based DR are also widely used as pre-processing before K-means or other clustering algorithms [1,4,27].", "startOffset": 303, "endOffset": 311}], "year": 2016, "abstractText": "Most learning approaches treat dimensionality reduction (DR) and clustering separately (i.e., sequentially), but recent research has shown that optimizing the two tasks jointly can substantially improve the performance of both. The premise behind the latter genre is that the data samples are obtained via linear transformation of latent representations that are easy to cluster; but in practice, the transformation from the latent space to the data can be more complicated. In this work, we assume that this transformation is an unknown and possibly nonlinear function. To recover the \u2018clustering-friendly\u2019 latent representations and to better cluster the data, we propose a joint DR and K-means clustering approach in which DR is accomplished via learning a deep neural network (DNN). The motivation is to keep the advantages of jointly optimizing the two tasks, while exploiting the deep neural network\u2019s ability to approximate any nonlinear function. This way, the proposed approach can work well for a broad class of generative models. Towards this end, we carefully design the DNN structure and the associated joint optimization criterion, and propose an effective and scalable algorithm to handle the formulated optimization problem. Experiments using five different real datasets are employed to showcase the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}