{"id": "1610.04850", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2016", "title": "Efficient Rectangular Maximal-Volume Algorithm for Rating Elicitation in Collaborative Filtering", "abstract": "Cold start problem gete in pala Collaborative impoverishment Filtering gna can be baoding solved m\u00e4r by asking new rft users church-related to stanimir rate edler a small areco seed set of representative items mortage or by asking cadoxton representative users handlers to psoe rate hutapea a new laudan item. adj The question is sheresky how to build zwane a seed set that can emhoff give deoxyribose enough tuburan preference information stolman for v16 making good artaphernes recommendations. saint-roch One of the most menards successful approaches, 301-493-9455 called Representative Based Matrix 123.72 Factorization, is based lawndale on kazemabad Maxvol algorithm. johansen Unfortunately, rusafi this tynagh approach shutoff has one important limitation - - - \u00eatre a seed set platero of a synthetic particular thieu size requires a rating authenticator matrix isabel factorization 23.25 of 724,000 fixed philippines rank that scots should torak coincide with 42-34 that dadon size. bygrave This gavage is campeonato not multimillions necessarily optimal bia\u0142a in m\u00e4rkisch-oderland the kieninger general eller case. In the heywood current genesco paper, nokiacorp we +.44 introduce sandbag a aud fast algorithm campton for www.chron.com an froilan analytical padel generalization of santaquin this approach misquotation that we call Rectangular dermatopathology Maxvol. It claidi allows blainey the cascini rank of factorization to tinissa be petkovski lower weinberger than the required size of nyako the seed qingzhi set. Moreover, 1,024-by-768 the paper includes lpo the contractors theoretical analysis prolific of gustavus the method ' s bdt error, nosair the 57-56 complexity analysis of yvars the glassboro existing llanwrtyd methods and the 6sec comparison 28,100 to hydrobromic the mcdaid state - fasnacht of - linpack the - art schistosomes approaches.", "histories": [["v1", "Sun, 16 Oct 2016 12:50:37 GMT  (98kb,D)", "http://arxiv.org/abs/1610.04850v1", "IEEE International Conference on Data Mining (ICDM) 2016"]], "COMMENTS": "IEEE International Conference on Data Mining (ICDM) 2016", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["alexander fonarev", "alexander mikhalev", "pavel serdyukov", "gleb gusev", "ivan oseledets"], "accepted": false, "id": "1610.04850"}, "pdf": {"name": "1610.04850.pdf", "metadata": {"source": "CRF", "title": "Efficient Rectangular Maximal-Volume Algorithm for Rating Elicitation in Collaborative Filtering", "authors": ["Alexander Fonarev", "Alexander Mikhalev", "Pavel Serdyukov", "Gleb Gusev", "Ivan Oseledets"], "emails": ["newo@newo.su,", "aleksandr.mikhalev@kaust.edu.sa,", "gleb57}@yandex-team.ru,", "i.oseledets@skoltech.ru"], "sections": [{"heading": null, "text": "I. INTRODUCTION Collaborative Filtering (CF) [1] is one of the most widely used approaches to recommender systems. It is based on the analysis of users\u2019 previous activity (likes, watches, skips, etc. of items) and discovering hidden relations between users and items. Among CF methods, matrix factorization techniques [2], [3] offer the most competitive performance [4]. These models map users and items into a latent factor space which contains information about preferences of users w.r.t. items. Due to the fact that CF approaches use only user behavioural data for predictions, but not any domain-specific context of users/items, they cannot generate recommendations for new cold users or cold items which have no ratings so far.\nA very common approach to solve this cold-start problem [5], called rating elicitation, is to explicitly ask cold users to rate a small representative seed set of items or to ask a representative seed set of users to rate a cold item [6], [7], [8]. One of the most successful approaches [8] to rating elicitation is based on the maximal-volume concept [9]. Its general intuition is that the most representative seed set should consist of the most representative and diverse latent vectors, i.e. they should have the largest length yet be as orthogonal as possible to each other. Formally, the degree to which these\ntwo requirements are met is measured by the volume of the parallelepiped spanned by these latent vectors. In matrix terms, the algorithm, called Maxvol [10], searches very efficiently for a submatrix of a factor matrix with the locally maximal determinant. Unfortunately, the determinant is defined only for square matrices, what means that a given fixed size of a seed set requires the same rank of the matrix factorization that may be not optimal. For example, the search for a sufficiently large seed set requires a relatively high rank of factorization, and hence a higher rank implies a larger number of the model parameters and a higher risk of overfitting, which, in turn, decreases the quality of recommendations.\nTo overcome the intrinsic \u201csquareness\u201d of the ordinary Maxvol, which is entirely based on the determinant, we use the notion of rectangular matrix volume, a generalization of the usual determinant. Searching a submatrix with high rectangular volume allows to use ranks of the factorization that are lower than the size of a seed set. However, the problem of searching for the globally optimal rectangular submatrix is NP-hard in the general case. In this paper, we propose a novel efficient algorithm, called Rectangular Maxvol, which generalizes original Maxvol.\nIt works in a greedy fashion and adds representative objects into a seed set one by one. This incremental update has low computational complexity that results in high algorithm efficiency. In this paper, we provide a detailed complexity analysis of the algorithm and its competitors and present a theoretical analysis of its error bounds. Moreover, as demonstrated by our experiments, the rectangular volume notion leads to a noticeable quality improvement of recommendations on popular recommender datasets.\nLet us briefly describe the organisation of the paper. Section II describes the background on the existing methods for searching representatives that is required for further understanding. Sections III-A, III-B present our novel approach based on the notion of rectangular matrix volume and the fast algorithm to search for submatrices with submaximal volume. In Sections III-C and III-D, we provide a theoretical analysis\nar X\niv :1\n61 0.\n04 85\n0v 1\n[ cs\n.I R\n] 1\n6 O\nct 2\nof the proposed method. Section IV reports the results of our experiments conducted on several large-scale real world datasets. Section V overviews the existing literature related to CF, the cold start problem and the basic maximal-volume concept papers."}, {"heading": "II. BACKGROUND AND FRAMEWORK", "text": ""}, {"heading": "A. Rating elicitation scheme", "text": "The rating elicitation methods, such as [6], [8], [11], [12], are based on the same common scheme, which is introduced in this section. Suppose we have a system that contains a history of users\u2019 ratings for items, where only a few items may be rated by a particular user. Denote the rating matrix by R \u2208 Rn\u00d7m, where n is the number of users and m is the number of items, and the value of its entry rui describes the feedback of user u on item i. If the rating for pair (u, i) is unknown, then rui is set to 0. Without loss of generality and due to the space limit, the following description of the methods is provided only for the user cold start problem. Without any modifications, these methods for the user cold start problem can be used to solve the item cold start problem after the transposition of matrix R.\nAlgorithm 1 presents the general scheme of a rating elicitation method. Such procedures ask a cold user to rate a seed set of representative items with indices k \u2208 NL0 for modeling his preference characteristics, where L0, called budget, is a parameter of the rating elicitation system.\nAlgorithm 1 Rating elicitation for user cold start problem Require: Warm rating matrix R \u2208 Rn\u00d7m, cold user, budget\nL0 Ensure: Predicted ratings of the cold user for all items\n1: Compute indices k \u2208 NL0 of representative items that form a seed set 2: Elicit ratings z\u2032 \u2208 R1\u00d7L0 of the cold user on items with indices k 3: Predict ratings of the cold user for all items z \u2208 R1\u00d7m using z\u2032 4: return z\nThe performance of a rating elicitation procedure should be measured using a quality of predictions z. For this purpose, we use ranking measures (such as Precision@k), which are well suitable for CF task (see Section IV for details).\nThe major contribution of this paper is a novel method of performing Step 1, described in Section III. It is based on PureSVD [4] Collaborative Filtering technique, that is described in Section II-B. In Section II-C, we discuss how to effectively perform Step 3 using the similar factorization based approach. And in Section II-D, we talk about the baseline method for seeking a seed set (Step 1), which is based on the maximal-volume concept."}, {"heading": "B. PureSVD", "text": "Let us briefly describe the general idea of PureSVD, which is a very effective CF method in terms of ranking measures [4]\nand therefore used as a basis of our rating elicitation approach. PureSVD provides a solution of the following optimization problem:\n\u2016R\u2212 P>Q\u2016F \u2192 min P\u2208Rf\u00d7n Q\u2208Rf\u00d7m , (1)\nwhere || \u00b7 ||F is the Frobenius norm and f is a parameter of PureSVD called rank. According to Eckart-Young theorem [13], the optimal solution can be found by computing the truncated sparse Singular Value Decomposition of the sparse rating matrix R.\nThis factorization can be interpreted as follows. Every user u has a low dimensional embedding pu \u2208 Rf , a row in the matrix P , and every item has an embedding qi \u2208 Rf , a column of the matrix Q. These embeddings are called latent vectors [2]. The PureSVD method provides an approximation r\u0303ui of the unknown rating for a pair (u, i), which is computed as the scalar product of the latent vectors:\nr\u0303ui = p > u qi.\nLow-rank factors P and Q are used in the rating elicitation procedures that are described further."}, {"heading": "C. Predicting Ratings with a Seed Set", "text": "Let us assume that some algorithm has selected a seed set with L0 representative items with indices k \u2208 NL0 , and assume a cold user has been asked to rate only items k, according to Steps 1-2 of the rating elicitation scheme described by Algorithm 1. In this section, we explain how to perform Step 3, i.e. how to predict ratings z for all items using only the ratings of the seed set.\nAs shown in [11], the most accurate way to do it is to find a coefficient matrix C \u2208 RL0\u00d7m that allows to linearly approximate each item rating via ratings z\u2032 of items from the seed set. Each column of C contains the coefficients of the representation of an item rating via the ratings of the items from the seed set. Shortly, this approximation can be written in the following way:\nz \u2190 z\u2032C.\nWe highlight two different approaches to compute matrix C. 1) Computing coefficients via the rating matrix: First approach is called Representative Based Matrix Factorization (RBMF) [8]. It aims to solve the following optimization task:\n\u2016R\u2212R(:, k)C\u2016F \u2192 min C . (2)\nIn our paper, we use the Matlab indexing notation1: R(:, k) is the matrix whose column j coincides with the column kj of R, where kj is the jth component of vector k. Note that z\u2032 is not a part of R(:, k), because there is still no information about a cold user ratings. This optimization task corresponds to the following approximation:\nR \u2248 R(:, k)C. (3) 1http://www.mathworks.com/company/newsletters/articles/\nmatrix-indexing-in-matlab.html\nThe solution of (2) is:\nCR = (R(:, k) >R(:, k))\u22121R(:, k)>R. (4)\nSince L0 n, the matrix R(:, k) is often well-conditioned. Therefore, the regularization term used in [8] is unnecessary and does not give a quality gain.\n2) Computing coefficients via a low-rank factor: In this paper, we propose a more efficient second approach that considers the rank-f factorization given by Equation (1), f \u2264 L0. Let Q(:, k) \u2208 Rf\u00d7L0 be the matrix formed by L0 columns of Q that correspond to the items of the seed set. Let us try to linearly recover all item latent vectors via the latent vectors from the seed set:\n\u2016Q\u2212Q(:, k)C\u2016F \u2192 min C . (5)\nIt is a low-rank version of the problem given by (2) and, therefore, is computationally easier. Solution C of this optimization problem can be also used for recovering all ratings using (3).\nUnlike (2), the optimization problem given by (5) does not have a unique solution C in general case, because there are infinitely many ways to linearly represent an f -dimensional vector via more than f other vectors. Therefore, we should find a solution of the underdetermined system of linear equations:\nQ = SC, (6)\nwhere we denote S = Q(:, k). Since the seed set latent vectors surely contain some noise and coefficients in C show how all item latent vectors depend on the seed set latent vectors, it is natural to find \u201csmall\u201d C, because larger coefficients produce larger noise in predictions. We use the least-norm solution C in our research, what is additionally theoretically motivated in Section III-D. The least-norm solution of (6) should be computed as follows:\nC = S\u2020Q, (7)\nwhere S\u2020 = S>(SS>)\u22121 is the right pseudo-inverse of S. Actually, such linear approach to rating recovering results in the following factorization model. Taking the latent vectors of the representative items S as a new basis of the decomposition given by Equation (1), we have\nR \u2248 P>Q = P>SC = ( P>Q(:, k) ) C \u2248\n\u2248 R(:, k)C = F>C,\nwhere F> = R(:, k). In this way, we approximate an unknown rating rui by the corresponding entry of matrix F>C, where factor F consists of the known ratings for the seed set items. This scheme is illustrated on Fig. 1."}, {"heading": "D. Square Maxvol", "text": "This section introduces the general idea of the maximalvolume concept and Maxvol algorithm [8] for selecting a good seed set, what corresponds to Step 1 in the rating elicitation scheme (Algorithm 1).\nSuppose we want to select L0 representative items with indices k \u2208 NL0 . First of all, Maxvol algorithm requires to\ncompute the rank-L0 SVD factorization of R given by Equation (1). After this, searching for an item seed set is equivalent to searching for a square submatrix S = Q(:, k) \u2208 RL0\u00d7L0 in the factor matrix Q. Note that every column of S or Q is a latent vector corresponding to an item from the seed set.\nAn algorithm of seeking for a set of representative items may rely on the following intuitions. First, it should not select items, if they are not popular and thus cover preferences of only a small non-representative group of users. That means that the latent vectors from the seed set should have large norms. Second, the algorithm has to select diverse items that are relevant to different users with different tastes. This can be formalized as selecting latent vectors that are far from being collinear. The requirements can be met by searching for a subset of columns of Q that maximizes the volume of the parallelepiped spanned by them. This intuition is demonstrated in Fig. 2, which captures a two-dimensional latent space and three seed sets. The volume of each seed set is proportional to the area of the triangle built on the corresponding latent vectors. The dark grey triangles have small volumes (because they contain not diverse vectors or vectors with small length) and hence correspond to bad seed sets. Contrariwise, the light gray triangle has a large volume and represents a better seed set.\nOverall, we have the following optimization task:\nk \u2190 argmax k Vol S = argmax k |detS|, S = Q(:, k). (8)\nThe problem is NP-hard in the general case [14] and, therefore, suboptimal greedy procedures are usually applied. One of the most popular procedures is called Maxvol algorithm [10] and is based on searching for a dominant submatrix S \u2208 RL0\u00d7L0 of Q. The dominant property of S means that all columns qi \u2208 RL0 of Q can be represented via a linear combination of columns from S with the coefficients not greater than 1 in modulus. Although, this property does not imply that S has the maximal volume, it guarantees that S is locally optimal, what means that replacing any column of S with a column of Q, does not increase the volume [10].\nAt the initialization step, Maxvol takes L0 linearly independent latent vectors that are the pivots from LUdecomposition [13] of matrix Q. Practice shows that this initialization usually provides a good initial approximation S to maximal volume matrix [10]. After this, the algorithm iteratively swaps a \u201cbad\u201d latent vector inside the seed set with a \u201cgood\u201d one out of it. The procedure repeats until convergence. See [10] for more rigorous explanation of Maxvol algorithm. In our paper, we also call this algorithm Square Maxvol, because it seeks for a square submatrix (since determinant is defined only for square S). Furthermore, it is important to note that the original algorithm presented in [10] has crucial speed optimizations for avoiding the expensive matrix multiplications and inversions, which are not presented in our paper due to the lack of space.\nLet us analyse the complexity of Maxvol. The LUdecomposition with pivoting takes O(mL20) operations. The\niterative updates take O(\u03b1mL0) operations, where \u03b1 is the number of iterations. Typically, \u03b1 \u2264 L0 iterations are needed. The overall complexity of Square Maxvol can be estimated as O(mL20). A more detailed complexity analysis of Square Maxvol is given in [10].\nThe obvious disadvantage of this approach to rating elicitation is the fixed size of the decomposition rank f = L0, because the matrix determinant is defined only for square matrices. That makes it impossible to build a seed set with fixed size L0 using an arbitrary rank of decomposition. However, as we further demonstrate in Section IV with experiments, using our Rectangular Maxvol generalization with a decomposition of rank f smaller than the size L0 of the seed set could result in better accuracy of recommendations for cold users."}, {"heading": "III. PROPOSED METHOD", "text": "A. Volume of Rectangular Matrices\nThis section introduces a generalization of the maximalvolume concept to rectangular submatrices, which allows to overcome the intrinsic \u201csquareness\u201d of the ordinary maximalvolume concept, which is entirely based on the determinant of a square matrix.\nConsider S \u2208 Rf\u00d7L0 , f \u2264 L0. It is easy to see that the volume of a square matrix is equal to the product of its singular values. In the case of a rectangular matrix S, its volume [15] can be defined in a similar way:\nRectvol(S) := L0\u220f s=1 \u03c3s = \u221a det(SS>).\nWe call it rectangular volume. The simple intuition behind this definition is that it is the volume of the ellipsoid defined as the image of a unit sphere under the linear transformation defined by S:\nRectvol(S) = Vol {v \u2208 Rf : \u2203c \u2208 RL0 , \u2016c\u20162 \u2264 1 | v = Sc}.\nThis can be verified using the singular value decomposition of S and the unitary invariance of the spectral norm. Moreover,\nin the case of a square matrix S, the rectangular volume is equal to the ordinary square volume:\nRectvol(S) = \u221a det(SS>) = |det(S)| = Vol(S).\nNote that, if f > L0, then detSS> = 0. Overall, searching for a seed set transforms to the following optimization task that is a generalization of Problem (8): k \u2190 argmaxk Rectvol(S), where S = Q(:, k). It is important to note that this maximization problem does not depend on the basis of the latent vectors from S.\nThe simplest method to find a suboptimal solution is to use a greedy algorithm that iteratively adds columns of Q to the seed set. Unfortunately, the straightforward greedy optimization (trying to add each item to the current seed set and computing its rectangular volume) costs O(mL20f\n2), that often is too expensive considering typical sizes of modern recommender datasets and number of model hyperparameters. Therefore, we developed a fast algorithm with complexity O(mL20) that is described in the following section."}, {"heading": "B. Algorithm", "text": "In this section, we introduce an algorithm for the selection of L0 representative items using the notion of rectangular volume. At the first step, the algorithm computes the best rankf approximation of the rating matrix R, PureSVD (see Section II-B for details), and selects f representative items with the pivot indices from LU-decomposition of Q or with Maxvol algorithm. This seed set is further expanded by Algorithm 2 in a greedy fashion: by adding new representative items one by one maximizing rectangular volume of the seed set. Further, we show that new representative item should have the maximal norm of the coefficients that represent its latent vector by the latent vectors of the current seed set. The procedure of such norm maximization is faster than the straightforward approach. At the end of this section we describe the algorithm for even faster rank-1 updating norms of coefficients.\n1) Maximization of coefficients norm: Suppose, at some step, we have already selected L < L0 representative items with the indices k \u2208 NL. Let S \u2208 Rf\u00d7L be the corresponding submatrix of Q \u2208 Rf\u00d7m. On the next step, the algorithm selects a column qi \u2282 Q, i /\u2208 k and adds it to the seed set: S \u2190 [S, qi] , where [A,B] is an operation of horizontal concatenation of two matrices A and B. This column should maximize the following volume:\nqi = argmax i/\u2208k Rectvol ([S, qi]) . (9)\nSuppose C \u2208 RL\u00d7m is the current matrix of coefficients from Equation (6), and let ci \u2208 RL be an i-th column of matrix C. Then the updated seed set from (9) can be written as following:\n[S, qi] = [S, Sci] = S[IL, ci]. (10)\nThen the volume of the seed set can be written in the following way:\nRectvol ([S, qi]) = \u221a det ([S, qi][S, qi]>) =\n= \u221a det ( SS> + Scic>i S > ) .\n(11)\nTaking into account the identity\ndet(X +AB) = det(X) det(I +BX\u22121A),\nthe volume (11) can be written as following:\nRectvol ([S, qi]) = Rectvol(S) \u221a 1 + wi, (12)\nwhere wi = \u2016ci\u201622. Thus, the maximization of rectangular volume is equivalent to the maximization of the l2-norm of the coefficients vector ci, which we know only after recomputing (7). Total recomputing of coefficient matrix C on each iteration is faster than the straightforward approach described in Section III-A and costs O(mL20fm). However, in the next section, we describe even faster algorithm with an efficient recomputation of the coefficients.\n2) Fast Computation of Coefficients: Since the matrix of coefficients C is the least-norm solution (7), after adding column qi to the seed set, C should be computed using Equation (10):\nC \u2190 [S, qi]\u2020Q = [IL, ci]\u2020S\u2020Q = [IL, ci]\u2020C. (13)\nThe pseudoinverse from (13) can be obtained in this way:\n[IL, ci] \u2020 = [IL, ci] > ([IL, ci][IL, ci]>)\u22121 = =\n[ IL c>i ] ( IL + cic > i )\u22121 ,\nwhere [ A B ] is an operation of vectical concatenation of A and B. The inversion in this formula can be computed by the Sherman-Morrison formula:(\nIL + cic > i )\u22121 = IL \u2212 cic > i\n1 + c>i ci .\nPutting it into (13), we finally get the main update formula for C:\nC \u2190 IL \u2212 cic > i 1+c>i ci\nc>i \u2212 c>i cic > i\n1+c>i ci\n \u00b7 C = C \u2212 cic > i C 1+c>i ci\nc>i C\n1+c>i ci  . (14) Recall that we should efficiently recompute norms of coefficients wi. Using Equation (14), we arrive at the following formula for the update of all norms wj :\nwj \u2190 wj \u2212 (c>i cj) 2\n1 + c>i ci . (15)\nIt is natural to see that coefficients norms are decreasing, because adding each new latent vector to the seed set gives more flexibility of representing all latent vectors via representative ones.\nEquations (14) and (15) allow to recompute C and W using the simple rank-1 update. Thus, the complexity of adding a new column into the seed set is low, what is shown in Section III-C. The pseudocode of the algorithm is provided in Algorithm 2.\nAlgorithm 2 Searching representative items using Rectangular Maxvol Require: Rating matrix R \u2208 Rn\u00d7m, number of representative\nitems L0, rank of decomposition f \u2264 L0 Ensure: Indices k \u2208 NL0 of L0 representative items\n1: Compute rank-f PureSVD of the matrix R \u2248 P>Q 2: Get the initial square seed set: k \u2190 L0 pivot indices from\nLU-decomposition of Q 3: S \u2190 Q(:, k) 4: C = S\u22121Q 5: \u2200i : wi \u2190 \u2016ci\u201622, where ci is the i-th column of C 6: while |k| < L0 do 7: i\u2190 argmaxi/\u2208k(wi) 8: k \u2190 [k, i] 9: S \u2190 [S, qi]\n10: C \u2190 C \u2212 cic > i C 1+c>i ci\nc>i C\n1+c>i ci  11: \u2200j : wj \u2190 wj \u2212 (c > i cj) 2 1+c>i ci 12: end while 13: return k\nThe seed sets provided by the algorithm can be used for rating elicitation and further prediction of ratings for the rest of the items, as demonstrated in Section II-C. Moreover, if the size of the seed set L0 is not limited by a fixed budget, alternative stopping criteria is proposed in Section III-D."}, {"heading": "C. Compelexity analysis", "text": "The proposed algorithm has two general steps: the initialization (Steps 1\u20135) and the iterative addition of columns or rows into the seed set (Steps 6\u201312). The initialization step corresponds to the LU-decomposition or Square Maxvol,\nwhich have O(mf2) complexity. Addition of one element into the seed set (Steps 7\u201311) requires the recomputation of the coefficients C (Step 10) and lengths of coefficient vectors (Step 11). The recomputation (Step 10) requires a rank-1 update of the coefficients matrix C \u2208 RL\u00d7m and the multiplication c>i C, where ci \u2208 RL is a column of C. The complexity of each of the two operations is O(Lm), so the total complexity of one iteration (Steps 7\u201311) is O(Lm). Since this procedure is iterated over L \u2208 {f, ..., L0}, the complexity of the loop (Step 6) is equal to O(m(L20 \u2212 f2)). So, in total, the complexity of Algorithm 2 is O(mL20)."}, {"heading": "D. Error Estimate", "text": "1) Analysis of Error: In this section, we theoretically analyse the estimation error of our method proposed in Section III-B. According to Section II-B we have a low-rank approximation of the rating matrix\nR = P>Q+ E ,\nwhere E \u2208 Rn\u00d7m is a random error matrix. On the other hand, we have RBMF approximation (3). Let us represent its error via E .\nFirst of all, we have\nR(:, k) = P>Q(:, k) + E(:, k) = P>S + E(:, k).\nSince C = S\u2020Q (see Section II-C for details), the RBMF approximation of R can be written in the following form:\nR(:, k)C = P>SS\u2020Q+ E(:, k)C = R\u2212 E + E(:, k)C,\nwhat means\nR = R(:, k)C + E \u2212 E(:, k)C.\nThe smaller in modulus the noise terms are, the better approximation of R we have. It means that we are interested in the small values of the matrix C, such as the least-norm solution of (5). Further, we prove a theorem providing an approximated bound for the maximal length of ci.\n2) Upper Bound of Coefficients Norm: Similarly to Square Maxvol algorithm, a rectangular submatrix is called dominant, if its rectangular volume does not increase by replacing one row with another one from the source matrix.\nTheorem 1. Let Q \u2208 Rf\u00d7m be a matrix of rank f . Assume k \u2208 NL0 is a vector of seed set element indices that produces rank-f dominant submatrix of S = Q(:, k), where S \u2208 Rf\u00d7L0 and m \u2265 L0 \u2265 f . Let C be a matrix of least-norm coefficients C \u2208 RL0\u00d7m, such that Q = SC. Then l2-norm of a column ci of C for i not from the seed set is bounded as:\n\u2016ci\u20162 \u2264\n\u221a f\nL0 + 1\u2212 f , i /\u2208 k.\nProof: Since S is a dominant submatrix of the matrix Q, it has the maximal rectangular volume among all possible\nsubmatrices of [S, qi] with the shape f \u00d7 L0. Therefore, applying Lemma 1 to the matrix [S, qi], we get\ndet ( [S, qi][S, qi] >) \u2264 L0 + 1 L0 + 1\u2212 f det(SS>).\nUsing Equation (12), we get: \u2016ci\u201622 = det ( [S, qi][S, qi] >) det(SS>) \u2212 1 \u2264 f L0 + 1\u2212 f ,\nwhat finishes the proof. The similar theoretical result was obtained in [16], However our proof seems to be much closely related to the notation used in our paper and in the proposed algorithm.\nTheorem 1 demonstrates that if we have an existing decomposition with the fixed rank f and the size of the seed set L0 is not limited by a fixed budget it is enough to take L0 = 2f items to the seed set for getting all coefficients norm less than 1. This condition of representativeness has a very natural geometric meaning: all item latent vectors are inside the ellipsoid spanned by the latent vectors from the seed set. The numerical experiments with randomly generated f \u00d7 m matrices have shown, that Algorithm 2 requires only L0 \u2248 1.2f rows to reach upper bound 2 for the length of each row of C and only L0 \u2248 2f to reach the upper bound 1 for the length of each row of C. So, although, our algorithm does not guarantee that the seed set submatrix is dominant, the experiment results are fully consistent with the theory.\nFurther, we prove the supporting lemma.\nLemma 1. Let A \u2208 RN\u00d7M and B \u2208 RM\u00d7N ,M > N . Let A\u2212i be N \u00d7 (M \u2212 1) submatrix of A without i-th column and B\u2212i be (M \u2212 1) \u00d7 N submatrix of B without i-th row. Then,\ndet(AB) \u2264 M M \u2212N max i (det(A\u2212iB\u2212i))\nProof: From the Cauchy-Binet formula we get det(AB) = \u2211 k detA(:, k) \u00b7 detB(k, :),\nwhere k \u2208 NN is a vector of N different indices. Since A\u2212i contains all columns of A except i-th column, then A(:, k) is a submatrix of A\u2212i for any i /\u2208 k. Since k consists of N different numbers, we have M\u2212N different i, such that A(:, k) is a submatrix of A\u2212i. The same is true for the matrix B. So get\nM\u2211 i=1 det(A\u2212iB\u2212i) = (M \u2212N) det(AB)\napplying Cauchy-Binet formula to each summand. Therefore,\ndet(AB) = 1\nM \u2212N M\u2211 i=1 det(A\u2212iB\u2212i),\nwhat finishes the proof."}, {"heading": "IV. EXPERIMENTS", "text": "The proposed experiments2 compare two algorithms: Square Maxvol based (our primary baseline) and Rectangular Maxvol based (Section III). Other competitors have either an infeasible computational complexity (see Section V for details) or have a lower quality than our baseline, as it is shown in [8] (we reproduced the conclusions from [8] but they are not demonstrated here due to the lack of space). Moreover, it is important to note that the experiments in [8] used smaller versions of the datasets. Therefore, the performance of Square Maxvol on the extended datasets is different from that reported in [8]."}, {"heading": "A. Datasets.", "text": "We used two popular publicly available datasets in our experiments. T first one is the Movielens dataset3 which contains 20,000,263 ratings of 26,744 movies from 138,493 users. The analysis of the older and smaller version of this dataset is provided in [17]. The second one is the Netflix dataset4. It contains 100,480,507 ratings of 17,770 movies from 480,189 users. The description of the dataset and the competition can be found in [18]. The rating matrix R was formed in the same way as in [8]."}, {"heading": "B. Evaluation Protocol.", "text": "Our evaluation pipeline for the comparison of the rating elicitation algorithms is similar to the one introduced in [8]. All our experiments are provided for both the user and the item cold start problems. However, without loss of generality, this section describes the evaluation protocol for the user cold start problem only. The item cold start problem can be evaluated in the same way after the transposition of the rating matrix.\nWe evaluate the algorithms for selecting representatives by the assessing the quality of the recommendations recovered after the acquisition of the actual ratings of the representatives, what can be done as shown in Section II-C. Note that users may not have ratings for the items from the seed set: if user u was asked to rate item i with unknown rating, then, according to PureSVD model, rui is set to 0. In case of the user cold start problem, all users are randomly divided into 5 folds of equal size, and the experiments are repeated 5 times, assuming that one part is a test set with cold users and the other four parts form the train set and the validation set contain warm users. Analogically, in case of the item cold start, all items were divided into 5 folds.\nPointwise quality measures are easy to be optimized directly, but they are not very suitable for recommendation quality evaluation, because the goal of a recommender system is not to predict particular rating values, but to predict the most relevant recommendations that should be shown to the user. That is why, we use ranking measures to evaluate all methods [19]. For evaluation, we divided all items for every\n2The source code is available here: https://bitbucket.org/muxas/rectmaxvol recommender\n3http://grouplens.org/datasets/movielens/ 4http://www.netflixprize.com/\nuser into relevant and irrelevant ones, as it was done in the baseline paper [8].\nOne of the most popular and interpretable ranking measures for the recommender systems evaluation are Precision@k and Recall@k [4] that measure the quality of top-k recommendations in terms of their relevance. More formally, Precision@k is the fraction of relevant items among the top-k recommendations. Recall@k is the fraction of relevant items from the top k among all relevant items. Our final evaluation measures were computed by averaging Precision@k and Recall@k over all users in the test set. Note that in the case of the item cold start problem, Precision@k and Recall@k are computed on the transposed rating matrix R. Moreover, following the methodology from [8], we compare algorithms in terms of coverage and diversity."}, {"heading": "C. Results of Experiments.", "text": "As we mentioned in Section II-C, there are two different ways to compute the coefficients for representing the hidden ratings via the ratings from a seed set. The first one is to compute them via the low-rank factors, as shown in Equation (7). The second one is to compute them via the source rating matrix R, as shown in Equation (4). Our experiments show that the second approach demonstrates the significantly better quality. Therefore, we use this method in all our experiments.\nWe processed experiments for the seed set sizes from 5 to 100 with a step of 5. These computations become possible for such dense grid of parameters, because of the high computational efficiency of our algorithm (see Section V). The average computational time of Rectangular Maxvol on the datasets is 1.12 seconds (Intel Xeon CPU 2.00GHz, 256Gb RAM). The average computational time of Square Maxvol is almost the same what confirms the theoretical complexity analysis.\nIn the case of Rectangular Maxvol, for every size of the seed set, we used the rank that gives the best performance on a separate evaluation set. Fig. 3 demonstrates the superiority of our approach over the ordinal Square Maxvol for all cold start problems types (user and item) and for both datasets. Moreover, it can be seen from the magnitudes of the differences that Rectangular Maxvol gives much more stable results that the square one. The same conclusions can be made for any combination of Precision/Recall, k and seed set sizes, but they are not demonstrated here due to the lack of space.\nAs mentioned above, Rectangular Maxvol used the optimal rank value in our experiments. Fig. 4 demonstrates the averaged optimal rank over all experiments for all datasets and for all cold start problem types. It is easy to see that, in each case, the required optimal rank is significantly smaller than the corresponding size of the seed set. This unequivocally confirms that the rectangular generalization of the square maximal-volume concept makes a great sense. Moreover, since Rectangular Maxvol requires a smaller rank of the rating matrix factorization, it is more computationally and memory efficient.\n0 20 40 60 80 100 Seed set size\n0\n20\n40\n60\n80\n100\nO pt\nim al\nra nk\nNetlix, repr. users Movielens, repr. users Netflix, repr. items Movielens, repr. items Square Maxvol\nFig. 4. Optimal rank dependence on the size of the seed set. 0 20 40 60 80 100 Number of represenatative items\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nC ov\ner ag\ne or\ndi ve\nrs ity\nRect. Maxvol diversity Square Maxvol diversity Rect. Maxvol coverage Square Maxvol coverage\nFig. 5. Coverage and diversity of the Netflix seed set items.\nOn Fig. 5, we can see that the coverage and diversity measures [8] of the representative Netflix items selected by Rectangular Maxvol are higher than the measures of Square Maxvol. The cases of representative users and Movielens dataset lead to the same results, but the corresponding figures are not demonstrated here due to the lack of space.\nIn the end, it is interesting to analyse the behaviour of the automatic stopping criterion that adds objects into the seed set until all latent vectors are covered by the ellipsoid spanned by the latent vectors of the representatives. The experiments show that increasing the rank results in a quality fall in the case of representative users and the ranks higher than 50, what means an overfitting of PureSVD. In case of the representative items, the quality becomes almost constant starting from the same ranks."}, {"heading": "V. RELATED WORK", "text": ""}, {"heading": "A. Collaborative Filtering.", "text": "Conventional CF methods do not analyse any domainspecific context of users/items [20], such as explicit user and item profiles, items\u2019 text descriptions or social relations between users. Therefore, they are domain- and data-independent and can be applied to a wide range of tasks, what is their major advantage. As shown in [4], CF approaches based on a factorization have high accuracy for the majority of datasets. While a particular choice of a factorization algorithm is not essential for our approach to the cold start problem, our methodology is based on the PureSVD, which performs better than other popular methods such as SVD++ [4]."}, {"heading": "B. Scoring Rating Elicitation Methods.", "text": "The simplest methods for the seed set selection rank users or items by some ad-hoc score which shows how representative they are and take the top-k ranked entities as a seed set [12],\n[21], [22], [23]. An obvious drawback of such methods that is avoided in our approach is that these elements are taken from the seed set independently and diversity of the selected elements is limited [6]. Further in this section, we overview the methods that aim on a selection of a diverse seed set and that have better performance. This is why we do not use the scoring methods in our experiments."}, {"heading": "C. GreedyExtend.", "text": "Among them, the most straightforward method is the GreedyExtend approach [6]. Unfortunately, the brute force manner of GreedyExtend implies very high computational costs. Hence, it is hardly scalable, in contrast to the approaches that are empirically compared in this paper. This method greedily adds the item i to the current seed set of indices k \u2208 NL that maximizes the target quality measure. The search of the best i is computed in a brute force manner, i.e. the algorithm iteratively adds the best item into the seed set: k \u2190 [k, i], where i = argmini\u2032 /\u2208k F([k, i\u2032]) and F([k, i\u2032]) is the quality measure of recommendations generated using the seed set indices [k, i\u2032]. The authors of this method reported the results only for an approach that uses similarities of items to predict the ratings via the seed set ratings. More effective [11] linear approach described in Section II-C costs O(Lnm), where L = |k|. At each step, the least squares solution is computed for almost all items, i.e. O(m) times. Since the algorithm has L0 such steps, the total complexity is O(L20nm\n2) (more than 1016 operations for the Netflix dataset and the seed set size L0 = 10). Therefore, we do not use this method in our experiments."}, {"heading": "D. Backward Greedy Selection.", "text": "Another class of methods of searching for diverse representatives is based on the factorization of the rating matrix. Since the selection of user or item representatives is equivalent to selecting a submatrix of the corresponding factor, these algorithms seek for the submatrix that maximizes some criterion. One such approach, called Backward Greedy Selection [11], solves only the item cold start problem, but not the user one. This method is based on the techniques for transductive experimental design introduced in [24]. To get the seed set, it greedily removes users from a source user set in order to get a good seed set minimizing the value Trace ( (SS>)\u22121 ) , where S \u2208 Rf\u00d7L is a submatrix in the items\u2019 factor Q \u2208 Rf\u00d7m of a rank-f decomposition. Each deletion of an item requires iterative look up of all the items in the data, where each iteration costs O(f2L). So, one deletion takes O(f2Lm) operations. Assuming that L0 m, the whole procedure takes O(f2m3) operations, which is too expensive to be computed on real world datasets (the authors have selected a small subset of users to perform their evaluation). Therefore, we do not use this method in our experiments."}, {"heading": "E. Representative Based Matrix Factorization.", "text": "The method presented in [8], called Representative Based Matrix Factorization (RBMF), takes the diversity into account\nas well. It uses maximal-volume concept and the Maxvol algorithm [10] for searching the most representative rows or columns in the factors of a CF factorization. This approach is highly efficient and more accurate than all ad-hoc competitors, but it also has one important limitation. It must use the same rank of factorization as the desired number of representative users or items for the seed set. The algorithm proposed in our paper is a generalization of Maxvol that allows to use different rank values. It often leads to a better recommendation accuracy, as shown in Section IV."}, {"heading": "F. Complexity analysis.", "text": "Let us overview the computational complexity of the proposed Rectangular Maxvol and its competitors. Some of these methods use low-rank factorizations of the matrix, whose detailed complexity analysis is provided in [13]. However, as this is not a key point of our work, we neglect the computational cost of factorizations in the further analysis, because it is same for all rating elicitation algorithms and usually is previously computed for the warm CF method. The summary of the complexity analysis is shown in Table I. The detailed complexity analysis of Square Maxvol and Rectangular Maxvol is provided in Sections II-D and III-C respectively."}, {"heading": "G. Cold Start Problem", "text": "Apart from rating elicitation methods, there were also different approaches to cold start problem proposed in the literature. Additional context information (e.g., category labels [25] or all available metadata [26]) may be used. Moreover, there is a class of methods that use adaptive tree-based questionnaires to acquire the initial information about new users [7], [27], [28], [29], [30], [31]. Moreover, the cold start problem can be viewed from the exploration-exploitation trade-off point of view [32], [33]. The methods from [34], [35] analyse the performance of CF methods w.r.t. the number of known ratings for a user."}, {"heading": "H. Maximal-Volume Concept", "text": "The maximal-volume concept, originally described in the field of low-rank approximation of matrices [9], provides an approach for a matrix approximation in a pseudo-skeleton form, which is a product of matrices formed by columns or rows of the source matrix. The algorithm, called Maxvol [10], allows to efficiently find a well-conditioned submatrix with a high enough volume for building such an approximation. Maximal volume submatrices are useful not only for low-rank approximations, but also in wireless communications [36],\npreconditioning of overdetermined systems [37], tensor decompositions [38], and recommender systems [8]. Our generalization of the maximal-volume concept to rectangular case offers additional degrees of freedom, what is potentially useful in any of these areas."}, {"heading": "VI. CONCLUSIONS", "text": "In our paper, we overviewed the existing approaches for the rating elicitation and introduced the efficient algorithm based on the definition of rectangular matrix volume. Moreover, in order to demonstrate the superiority of the proposed method, we provided the analytical and experimental comparison to the existing approaches. It seems to be an interesting direction of future work to apply the proposed framework to building treebased cold-start questionnaires in recommender systems.\nAnother interesting direction for future work is to join approaches from two classes: based on the maximal-volume concept and based on optimal design criteria. They historically came from absolutely different fields: from computational lineal algebra and from statistical experimental analysis respectively. Although all these methods are very similar from the mathematical point of view, it seems quite interesting to explore their similarities and differences."}, {"heading": "VII. ACKNOWLEDGMENTS", "text": "Work on problem setting and numerical examples was supported by Russian Science Foundation grant 14-11-00659. Work on theoretical estimations of approximation error and practical algorithm was supported by Russian Foundation for Basic Research 16-31-00351 mol a. Also we thank Evgeny Frolov for helpful discussions."}], "references": [{"title": "Grouplens: an open architecture for collaborative filtering of netnews", "author": ["P. Resnick", "N. Iacovou", "M. Suchak", "P. Bergstrom", "J. Riedl"], "venue": "Proceedings of the 1994 ACM conference on Computer supported cooperative work. ACM, 1994, pp. 175\u2013186.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Y. Koren"], "venue": "KDD\u201908, 2008, pp. 426\u2013434.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, no. 8, pp. 30\u201337, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Performance of recommender algorithms on top-n recommendation tasks", "author": ["P. Cremonesi", "Y. Koren", "R. Turrin"], "venue": "Recsys\u201910. ACM, 2010, pp. 39\u201346.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Methods and metrics for cold-start recommendations", "author": ["A.I. Schein", "A. Popescul", "L.H. Ungar", "D.M. Pennock"], "venue": "SIGIR\u201902. ACM, 2002, pp. 253\u2013260.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "On bootstrapping recommender systems", "author": ["N. Golbandi", "Y. Koren", "R. Lempel"], "venue": "CIKM\u201910, 2010, pp. 1805\u20131808.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive bootstrapping of recommender systems using decision trees", "author": ["\u2014\u2014"], "venue": "WSDM\u201911, 2011, pp. 595\u2013604.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Wisdom of the better few: cold start recommendation via representative based rating elicitation", "author": ["N.N. Liu", "X. Meng", "C. Liu", "Q. Yang"], "venue": "Recsys\u201911, 2011, pp. 37\u201344.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "The maximal-volume concept in approximation by low-rank matrices", "author": ["S.A. Goreinov", "E.E. Tyrtyshnikov"], "venue": "Contemporary Mathematics, vol. 280, pp. 47\u201352, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "How to find a good submatrix", "author": ["S. Goreinov", "I. Oseledets", "D. Savostyanov", "E. Tyrtyshnikov", "N. Zamarashkin"], "venue": "Matrix methods: theory, algorithms and applications, p. 247, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Budget-constrained item cold-start handling in collaborative filtering recommenders via optimal design", "author": ["O. Anava", "S. Golan", "N. Golbandi", "Z. Karnin", "R. Lempel", "O. Rokhlenko", "O. Somekh"], "venue": "WWW\u201915, 2015, pp. 45\u201354.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Getting to know you: learning new user preferences in recommender systems", "author": ["A.M. Rashid", "I. Albert", "D. Cosley", "S.K. Lam", "S.M. McNee", "J.A. Konstan", "J. Riedl"], "venue": "IUI\u201902, 2002, pp. 127\u2013134.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Finding maximum volume submatrices of a matrix", "author": ["A. Civril", "M. Magdon-Ismail"], "venue": "RPI Comp Sci Dept TR, pp. 07\u201308, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Linear algebra and its applications (gilbert strang)", "author": ["N.J. Rose"], "venue": "SIAM Review, vol. 24, no. 4, pp. 499\u2013501, 1982.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1982}, {"title": "Subset selection for matrices", "author": ["F. De Hoog", "R. Mattheij"], "venue": "Linear Algebra and its Applications, vol. 422, no. 2, pp. 349\u2013359, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Movielens unplugged: experiences with an occasionally connected recommender system", "author": ["B.N. Miller", "I. Albert", "S.K. Lam", "J.A. Konstan", "J. Riedl"], "venue": "IUI\u201903. ACM, 2003, pp. 263\u2013266.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "The netflix prize", "author": ["J. Bennett", "S. Lanning"], "venue": "Proceedings of KDD cup and workshop, vol. 2007, 2007, p. 35.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of accuracy evaluation metrics of recommendation tasks", "author": ["A. Gunawardana", "G. Shani"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 2935\u20132962, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative filtering recommender systems", "author": ["J.B. Schafer", "D. Frankowski", "J. Herlocker", "S. Sen"], "venue": "The adaptive web. Springer, 2007, pp. 291\u2013324.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning preferences of new users in recommender systems: an information theoretic approach", "author": ["A.M. Rashid", "G. Karypis", "J. Riedl"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 10, no. 2, pp. 90\u2013100, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Less is more: Sparse representative based preference elicitation for cold start recommendation", "author": ["X. Zhang", "J. Cheng", "H. Lu"], "venue": "IMCS\u201914, 2014, p. 117.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised discriminative preference elicitation for cold-start recommendation", "author": ["X. Zhang", "J. Cheng", "T. Yuan", "B. Niu", "H. Lu"], "venue": "CIKM\u201913. ACM, 2013, pp. 1813\u20131816.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning via transductive experimental design", "author": ["K. Yu", "J. Bi", "V. Tresp"], "venue": "ICML\u201906. ACM, 2006, pp. 1081\u20131088.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Dualds: A dual discriminative rating elicitation framework for cold start recommendation", "author": ["X. Zhang", "J. Cheng", "S. Qiu", "G. Zhu", "H. Lu"], "venue": "Knowledge-Based Systems, vol. 73, pp. 161\u2013172, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Cold-start item and user recommendation with decoupled completion and transduction", "author": ["I. Barjasteh", "R. Forsati", "F. Masrour", "A.-H. Esfahanian", "H. Radha"], "venue": "Recsys\u201915, 2015, pp. 91\u201398.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving the user experience during cold start through choice-based preference elicitation", "author": ["M.P. Graus", "M.C. Willemsen"], "venue": "Recsys\u201915, 2015, pp. 273\u2013276.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved questionnaire trees for active learning in recommender systems", "author": ["R. Karimi", "A. Nanopoulos", "L. Schmidt-Thieme"], "venue": "Proceedings of the 16th LWA Workshops: KDML, IR and FGWM, 2014, pp. 34\u201344.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Factorized decision trees for active learning in recommender systems", "author": ["R. Karimi", "M. Wistuba", "A. Nanopoulos", "L. Schmidt-Thieme"], "venue": "ICTAI\u201913. IEEE, 2013, pp. 404\u2013411.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multiple-question decision trees for cold-start recommendation", "author": ["M. Sun", "F. Li", "J. Lee", "K. Zhou", "G. Lebanon", "H. Zha"], "venue": "WSDM\u201913. ACM, 2013, pp. 445\u2013454.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Functional matrix factorizations for cold-start recommendation", "author": ["K. Zhou", "S.-H. Yang", "H. Zha"], "venue": "SIGIR\u201911, 2011, pp. 315\u2013324.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Excuseme: Asking users to help in item cold-start recommendations", "author": ["M. Aharon", "O. Anava", "N. Avigdor-Elgrabli", "D. Drachsler-Cohen", "S. Golan", "O. Somekh"], "venue": "Recsys\u201915, 2015, pp. 83\u201390.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive collaborative filtering", "author": ["X. Zhao", "W. Zhang", "J. Wang"], "venue": "CIKM\u201913, 2013, pp. 1411\u20131420.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "User effort vs. accuracy in rating-based elicitation", "author": ["P. Cremonesi", "F. Garzottto", "R. Turrin"], "venue": "Recsys\u201912, 2012, pp. 27\u201334.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluating recommender behavior for new users", "author": ["D. Kluver", "J.A. Konstan"], "venue": "Recsys\u201914, 2014, pp. 121\u2013128.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Global and fast receiver antenna selection for mimo systems", "author": ["B.H. Wang", "H.T. Hui", "M.S. Leong"], "venue": "Communications, IEEE Transactions on, vol. 58, no. 9, pp. 2505\u20132510, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Preconditioning of linear least-squares problems by identifying basic variables", "author": ["M. Arioli", "I.S. Duff"], "venue": "Preprint RAL-P-2014-007, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Tt-cross approximation for multidimensional arrays", "author": ["I. Oseledets", "E. Tyrtyshnikov"], "venue": "Linear Algebra and its Applications, vol. 432, no. 1, pp. 70\u201388, 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Collaborative Filtering (CF) [1] is one of the most widely used approaches to recommender systems.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "Among CF methods, matrix factorization techniques [2], [3] offer the most competitive performance [4].", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "Among CF methods, matrix factorization techniques [2], [3] offer the most competitive performance [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Among CF methods, matrix factorization techniques [2], [3] offer the most competitive performance [4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "A very common approach to solve this cold-start problem [5], called rating elicitation, is to explicitly ask cold users to rate a small representative seed set of items or to ask a representative seed set of users to rate a cold item [6], [7], [8].", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "A very common approach to solve this cold-start problem [5], called rating elicitation, is to explicitly ask cold users to rate a small representative seed set of items or to ask a representative seed set of users to rate a cold item [6], [7], [8].", "startOffset": 234, "endOffset": 237}, {"referenceID": 6, "context": "A very common approach to solve this cold-start problem [5], called rating elicitation, is to explicitly ask cold users to rate a small representative seed set of items or to ask a representative seed set of users to rate a cold item [6], [7], [8].", "startOffset": 239, "endOffset": 242}, {"referenceID": 7, "context": "A very common approach to solve this cold-start problem [5], called rating elicitation, is to explicitly ask cold users to rate a small representative seed set of items or to ask a representative seed set of users to rate a cold item [6], [7], [8].", "startOffset": 244, "endOffset": 247}, {"referenceID": 7, "context": "One of the most successful approaches [8] to rating elicitation is based on the maximal-volume concept [9].", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "One of the most successful approaches [8] to rating elicitation is based on the maximal-volume concept [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "In matrix terms, the algorithm, called Maxvol [10], searches very efficiently for a submatrix of a factor matrix with the locally maximal determinant.", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "The rating elicitation methods, such as [6], [8], [11], [12], are based on the same common scheme, which is introduced in this section.", "startOffset": 40, "endOffset": 43}, {"referenceID": 7, "context": "The rating elicitation methods, such as [6], [8], [11], [12], are based on the same common scheme, which is introduced in this section.", "startOffset": 45, "endOffset": 48}, {"referenceID": 10, "context": "The rating elicitation methods, such as [6], [8], [11], [12], are based on the same common scheme, which is introduced in this section.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "The rating elicitation methods, such as [6], [8], [11], [12], are based on the same common scheme, which is introduced in this section.", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "It is based on PureSVD [4] Collaborative Filtering technique, that is described in Section II-B.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "Let us briefly describe the general idea of PureSVD, which is a very effective CF method in terms of ranking measures [4] and therefore used as a basis of our rating elicitation approach.", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "These embeddings are called latent vectors [2].", "startOffset": 43, "endOffset": 46}, {"referenceID": 10, "context": "As shown in [11], the most accurate way to do it is to find a coefficient matrix C \u2208 RL0\u00d7m that allows to linearly approximate each item rating via ratings z\u2032 of items from the seed set.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "1) Computing coefficients via the rating matrix: First approach is called Representative Based Matrix Factorization (RBMF) [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "Therefore, the regularization term used in [8] is unnecessary and does not give a quality gain.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "This section introduces the general idea of the maximalvolume concept and Maxvol algorithm [8] for selecting a good seed set, what corresponds to Step 1 in the rating elicitation scheme (Algorithm 1).", "startOffset": 91, "endOffset": 94}, {"referenceID": 12, "context": "The problem is NP-hard in the general case [14] and, therefore, suboptimal greedy procedures are usually applied.", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "One of the most popular procedures is called Maxvol algorithm [10] and is based on searching for a dominant submatrix S \u2208 RL0\u00d7L0 of Q.", "startOffset": 62, "endOffset": 66}, {"referenceID": 9, "context": "Although, this property does not imply that S has the maximal volume, it guarantees that S is locally optimal, what means that replacing any column of S with a column of Q, does not increase the volume [10].", "startOffset": 202, "endOffset": 206}, {"referenceID": 9, "context": "Practice shows that this initialization usually provides a good initial approximation S to maximal volume matrix [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "See [10] for more rigorous explanation of Maxvol algorithm.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "Furthermore, it is important to note that the original algorithm presented in [10] has crucial speed optimizations for avoiding the expensive matrix multiplications and inversions, which are not presented in our paper due to the lack of space.", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "A more detailed complexity analysis of Square Maxvol is given in [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "In the case of a rectangular matrix S, its volume [15] can be defined in a similar way:", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "The similar theoretical result was obtained in [16], However our proof seems to be much closely related to the notation used in our paper and in the proposed algorithm.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "Other competitors have either an infeasible computational complexity (see Section V for details) or have a lower quality than our baseline, as it is shown in [8] (we reproduced the conclusions from [8] but they are not demonstrated here due to the lack of space).", "startOffset": 158, "endOffset": 161}, {"referenceID": 7, "context": "Other competitors have either an infeasible computational complexity (see Section V for details) or have a lower quality than our baseline, as it is shown in [8] (we reproduced the conclusions from [8] but they are not demonstrated here due to the lack of space).", "startOffset": 198, "endOffset": 201}, {"referenceID": 7, "context": "Moreover, it is important to note that the experiments in [8] used smaller versions of the datasets.", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "Therefore, the performance of Square Maxvol on the extended datasets is different from that reported in [8].", "startOffset": 104, "endOffset": 107}, {"referenceID": 15, "context": "The analysis of the older and smaller version of this dataset is provided in [17].", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "The description of the dataset and the competition can be found in [18].", "startOffset": 67, "endOffset": 71}, {"referenceID": 7, "context": "The rating matrix R was formed in the same way as in [8].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "Our evaluation pipeline for the comparison of the rating elicitation algorithms is similar to the one introduced in [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 17, "context": "That is why, we use ranking measures to evaluate all methods [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "com/ user into relevant and irrelevant ones, as it was done in the baseline paper [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "One of the most popular and interpretable ranking measures for the recommender systems evaluation are Precision@k and Recall@k [4] that measure the quality of top-k recommendations in terms of their relevance.", "startOffset": 127, "endOffset": 130}, {"referenceID": 7, "context": "Moreover, following the methodology from [8], we compare algorithms in terms of coverage and diversity.", "startOffset": 41, "endOffset": 44}, {"referenceID": 7, "context": "5, we can see that the coverage and diversity measures [8] of the representative Netflix items selected by Rectangular Maxvol are higher than the measures of Square Maxvol.", "startOffset": 55, "endOffset": 58}, {"referenceID": 18, "context": "Conventional CF methods do not analyse any domainspecific context of users/items [20], such as explicit user and item profiles, items\u2019 text descriptions or social relations between users.", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "As shown in [4], CF approaches based on a factorization have high accuracy for the majority of datasets.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "While a particular choice of a factorization algorithm is not essential for our approach to the cold start problem, our methodology is based on the PureSVD, which performs better than other popular methods such as SVD++ [4].", "startOffset": 220, "endOffset": 223}, {"referenceID": 11, "context": "The simplest methods for the seed set selection rank users or items by some ad-hoc score which shows how representative they are and take the top-k ranked entities as a seed set [12],", "startOffset": 178, "endOffset": 182}, {"referenceID": 19, "context": "[21], [22], [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21], [22], [23].", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "[21], [22], [23].", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "An obvious drawback of such methods that is avoided in our approach is that these elements are taken from the seed set independently and diversity of the selected elements is limited [6].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "Among them, the most straightforward method is the GreedyExtend approach [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 10, "context": "More effective [11] linear approach described in Section II-C costs O(Lnm), where L = |k|.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "One such approach, called Backward Greedy Selection [11], solves only the item cold start problem, but not the user one.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "This method is based on the techniques for transductive experimental design introduced in [24].", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "The method presented in [8], called Representative Based Matrix Factorization (RBMF), takes the diversity into account Algorithm Complexity", "startOffset": 24, "endOffset": 27}, {"referenceID": 9, "context": "It uses maximal-volume concept and the Maxvol algorithm [10] for searching the most representative rows or columns in the factors of a CF factorization.", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": ", category labels [25] or all available metadata [26]) may be used.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": ", category labels [25] or all available metadata [26]) may be used.", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "Moreover, there is a class of methods that use adaptive tree-based questionnaires to acquire the initial information about new users [7], [27], [28], [29], [30], [31].", "startOffset": 133, "endOffset": 136}, {"referenceID": 25, "context": "Moreover, there is a class of methods that use adaptive tree-based questionnaires to acquire the initial information about new users [7], [27], [28], [29], [30], [31].", "startOffset": 138, "endOffset": 142}, {"referenceID": 26, "context": "Moreover, there is a class of methods that use adaptive tree-based questionnaires to acquire the initial information about new users [7], [27], [28], [29], [30], [31].", "startOffset": 144, "endOffset": 148}, {"referenceID": 27, "context": "Moreover, there is a class of methods that use adaptive tree-based questionnaires to acquire the initial information about new users [7], [27], [28], [29], [30], [31].", "startOffset": 150, "endOffset": 154}, {"referenceID": 28, "context": "Moreover, there is a class of methods that use adaptive tree-based questionnaires to acquire the initial information about new users [7], [27], [28], [29], [30], [31].", "startOffset": 156, "endOffset": 160}, {"referenceID": 29, "context": "Moreover, there is a class of methods that use adaptive tree-based questionnaires to acquire the initial information about new users [7], [27], [28], [29], [30], [31].", "startOffset": 162, "endOffset": 166}, {"referenceID": 30, "context": "Moreover, the cold start problem can be viewed from the exploration-exploitation trade-off point of view [32], [33].", "startOffset": 105, "endOffset": 109}, {"referenceID": 31, "context": "Moreover, the cold start problem can be viewed from the exploration-exploitation trade-off point of view [32], [33].", "startOffset": 111, "endOffset": 115}, {"referenceID": 32, "context": "The methods from [34], [35] analyse the performance of CF methods w.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "The methods from [34], [35] analyse the performance of CF methods w.", "startOffset": 23, "endOffset": 27}, {"referenceID": 8, "context": "The maximal-volume concept, originally described in the field of low-rank approximation of matrices [9], provides an approach for a matrix approximation in a pseudo-skeleton form, which is a product of matrices formed by columns or rows of the source matrix.", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "The algorithm, called Maxvol [10], allows to efficiently find a well-conditioned submatrix with a high enough volume for building such an approximation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 34, "context": "Maximal volume submatrices are useful not only for low-rank approximations, but also in wireless communications [36],", "startOffset": 112, "endOffset": 116}, {"referenceID": 35, "context": "preconditioning of overdetermined systems [37], tensor decompositions [38], and recommender systems [8].", "startOffset": 42, "endOffset": 46}, {"referenceID": 36, "context": "preconditioning of overdetermined systems [37], tensor decompositions [38], and recommender systems [8].", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "preconditioning of overdetermined systems [37], tensor decompositions [38], and recommender systems [8].", "startOffset": 100, "endOffset": 103}], "year": 2016, "abstractText": "Cold start problem in Collaborative Filtering can be solved by asking new users to rate a small seed set of representative items or by asking representative users to rate a new item. The question is how to build a seed set that can give enough preference information for making good recommendations. One of the most successful approaches, called Representative Based Matrix Factorization, is based on Maxvol algorithm. Unfortunately, this approach has one important limitation \u2014 a seed set of a particular size requires a rating matrix factorization of fixed rank that should coincide with that size. This is not necessarily optimal in the general case. In the current paper, we introduce a fast algorithm for an analytical generalization of this approach that we call Rectangular Maxvol. It allows the rank of factorization to be lower than the required size of the seed set. Moreover, the paper includes the theoretical analysis of the method\u2019s error, the complexity analysis of the existing methods and the comparison to the state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}