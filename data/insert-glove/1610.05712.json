{"id": "1610.05712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Fast L1-NMF for Multiple Parametric Model Estimation", "abstract": "flacco In armanen this work we bathwater introduce a comprehensive algorithmic pipeline visalia for fengwu multiple 2ndlead parametric pithily model 83.0 estimation. The proposed approach cecropia analyzes 3,414 the information lassell produced by uyuni a random sampling interaural algorithm (e. g. , RANSAC) from biller a machine asad learning / zaini optimization perspective, using jeena a \\ ific textit {reformat parameterless} biclustering algorithm based on publication L1 socionics nonnegative matrix factorization (L1 - transpersonal NMF ). anupam The ketteler proposed paysage framework edek exploits autocross consistent patterns phae that naturally arise amarilis during the k7 RANSAC execution, 189 while brzezinski explicitly thorndyke avoiding arguably spurious scuttlebutt inconsistencies. \u0645\u062d\u0645\u062f Contrarily powderhorn to mohani the main trends wallbridge in akhal the jenwu literature, sargant the melbourn proposed besta technique kaiso does envoys not skinheads impose collectively non - intersecting p-51 parametric 53-member models. hogen A housebroken new byeon accelerated algorithm thang to compute ravenpaw L1 - NMFs 4/7 allows jerboa to handle scarp medium - innis sized problems xpressmusic faster sanghi while b\u00f6hler also extending dadabhai the usability of nosediving the algorithm hintsa to much larger datasets. moos This accelerated todas algorithm has oxenhorn applications vezzosi in ids any other context camp-dependent where naivety an 14.66 L1 - NMF is 106.62 needed, beyond the bachoco biclustering approach andrzejewski to cloudier parameter estimation here marzook addressed. We ung accompany the tarcisio algorithmic 30-index presentation verdecia with 52.14 theoretical farnellbnytimes.com foundations happend and wince numerous yosano and mukai diverse examples.", "histories": [["v1", "Tue, 18 Oct 2016 17:20:38 GMT  (8404kb,D)", "https://arxiv.org/abs/1610.05712v1", null], ["v2", "Fri, 11 Nov 2016 15:54:14 GMT  (8404kb,D)", "http://arxiv.org/abs/1610.05712v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["mariano tepper", "guillermo sapiro"], "accepted": false, "id": "1610.05712"}, "pdf": {"name": "1610.05712.pdf", "metadata": {"source": "CRF", "title": "Fast L1-NMF for Multiple Parametric Model Estimation\u2217", "authors": ["Mariano Tepper", "Guillermo Sapiro"], "emails": ["mariano.tepper@duke.edu", "guillermo.sapiro@duke.edu"], "sections": [{"heading": null, "text": "Keywords: Multiple parametric model estimation, robust fitting, RANSAC, nonnegative matrix factorization, biclustering, L1 optimization"}, {"heading": "1. Introduction", "text": "This paper addresses the problem of fitting multiple instances of a given parametric model to data corrupted by noise and outliers. This is a prevalent problem for example in computer vision, found in a wide range of applications such as finding lines/circles/ellipses in images, homography estimation in stereo vision, motion estimation and segmentation, and the geometric analysis of 3D point clouds. Finding a single instance of a parametric model in a dataset is a robust fitting problem that is hard on its own; the difficulties are exacerbated when multiple instances might be present in the dataset due to the unavoidable emergence of pseudo-outliers (data points that belong to one structure and are usually outliers to any other structure). Thus, we face the problem of simultaneous robust estimation of model parameters and attribution of data points to the estimated models. These two problems are intrinsically intertwined. Furthermore, the correct number of model instances is not known in advance.\nFormally, the data is a set X = {xi}mi=1 of m objects, described by\nX = (\u22c3 k Xk ) \u222a O with (\u2200k) Xk \u2229 O = \u2205. (1)\n\u2217. This work was partially supported by Google, NSF, ONR, NGA, and ARO.\nar X\niv :1\n61 0.\n05 71\n2v 2\n[ cs\n.C V\n] 1\n1 N\nov 2\n01 6\nThe objects in each subset Xk, which might intersect, are (noisy) measurements that can be described with a parametric model \u00b5(\u03b8k), with parameter vector \u03b8k. In the following, we say that the objects in Xk are inliers to the model \u00b5(\u03b8k). We also generally refer to a set of objects as inliers, in the sense that there is a statistically meaningful instance of a parametric model that describes them. The objects in O cannot be described with such a model and we refer to them as outliers.\nLet us define more formally these intuitive concepts. A model \u00b5 is defined as the zero level set of a smooth parametric function f\u00b5(x; \u03b8),\n\u00b5(\u03b8) = {x \u2208 Rd, f\u00b5(x; \u03b8) = 0}, (2)\nwhere \u03b8 is a parameter vector. We define the error associated with the datum x \u2208 X with respect to the model \u00b5(\u03b8) as\ne\u00b5(x, \u03b8) = min x\u2032\u2208\u00b5(\u03b8)\ndist(x,x\u2032), (3)\nwhere dist is an appropriate distance function. Using this error metric, we define the Consensus Set (CS) of a model as\nC\u00b5(X , \u03b8, \u03b4) = {x \u2208 X , e\u00b5(x, \u03b8) \u2264 \u03b4}, (4)\nwhere \u03b4 is a threshold that accounts for the measurement noise and/or model mismatch.\nMultiple Parametric Model Estimation (MPME) seeks to find the set of inliers-model pairs (Xk, \u03b8k) from the observed X such that Xk = C\u00b5 (X , \u03b8k, \u03b4). This problem is, by itself, ill-posed. It is standard in the literature to implicitly or explicitly impose a penalty on the number of recovered pairs to render it tractable. We also adopt such a design choice. Notice that once Xk is found, the corresponding \u03b8k can be estimated for example by simple least-squares regression, i.e.,\n\u03b8\u0302k = argmin \u03b8 \u2211 x\u2208Xk [ e\u00b5(x, \u03b8) ]2 . (5)\nMPME is an important but difficult problem, as standard robust estimators, like RANSAC (RANdom SAmple Consensus) [8, 13], are designed to extract a single model. Let us then begin by formally explaining how does the RANSAC machinery work, to further illustrate the value and perspective of our proposed MPME formulation.\nLet us denote by b the minimum number of elements necessary to uniquely characterize a given parametric model, e.g., b = 2 for lines and b = 3 for circles. For example, if we want to discover alignments in a 2D point cloud, the elements are 2D points, models \u00b5 are lines, and b = 2, since a line is defined by two points. A set of b objects is called a minimal sample set (MSS). Generically, the random-sample-and-model-generation framework can be described by Algorithm 1. We first randomly sample n MSSs, each generating one model hypothesis. The number n is an overestimation of the number of trials needed to obtain a certain number of \u201cgood\u201d models [13, 31, 37]. Then, we compute the CS of each model hypothesis using Equation (4). Let U be the set of all these consensus sets. From this point onwards, different algorithms perform different operations on U . Specifically, RANSAC, the\nAlgorithm 1: Random sampling algorithm\ninput : set of objects X , parametric function f\u00b5, inliers threshold \u03b4. output: pool U of consensus sets.\n1 b\u2190 minimum number of elements necessary to uniquely characterize model \u00b5, see Equation (2); 2 foreach j \u2208 {1 . . . n} do 3 Select at random a minimal sample set (MSS) Xmms(j) of size b from X ; 4 Estimate \u03b8j from Xmms(j) by solving (\u2200x \u2208 Xmms(j)) f\u00b5(x; \u03b8) = 0; 5 U \u2190 {C\u00b5 (X , \u03b8j , \u03b4)}nj=1, see Equation (4);\nalgorithm that introduced this framework, detects a single model by taking the CS from U with the largest size, and uses it to estimate \u03b8 from C as in Equation (5).\nApplying RANSAC sequentially, removing the inliers from the dataset as each model instance is detected, has been proposed as a solution for multi-model estimation, e.g., [24]. However, this approach is known to be suboptimal [37]. The multiRANSAC algorithm [37] provides a more effective alternative, although the number of models must be known a priori, imposing a very limiting constraint in many applications. An alternative approach consists of finding density modes in a parameter space. The overall idea is that we can map the data into the parameter space through random sampling, and then seek the modes of the distribution by discretizing the distribution, i.e., using the Randomized Hough Transform [35], or by using non-parametric density estimation techniques, like the mean-shift clustering algorithm [27]. These, however, are not intrinsically robust techniques, even if they can be robustified with outliers rejection heuristics [31]. Moreover, the choice of the parametrization and its discretization are critical, among other important shortcomings [31]. The computational cost of these techniques can be very high as well. From a different perspective, J-linkage [31], T-linkage [20], and RPA [21] address the problem by clustering the dataset. We will provide more details about these methods in the next section.\nContributions. We have previously introduced a novel framework and perspective to reach consensus in grouping problems by re-framing them as biclustering problems [30]. In particular, the task of finding/fitting multiple parametric models to a dataset was, for the first time, formally posed as a consensus/biclustering problem. In this work, we build upon this framework, introducing a complete and comprehensive algorithmic pipeline for multiple parametric model estimation. The proposed approach preserves and considers all the information produced by Algorithm 1 (i.e., no averaging/pooling is performed). This information is then analyzed from a machine learning/optimization perspective, using a parameterless biclustering algorithm based on nonnegative matrix factorization (NMF).\nContrarily to the main trends in the literature, the proposed modeling does not impose non-intersecting subsets Xi. Secondly, it exploits consistencies that naturally arise during the RANSAC execution, while explicitly avoiding spurious inconsistencies. This new formulation conceptually changes the way that the data produced by the popular RANSAC, and related model-candidate generation techniques, is analyzed.\nWith respect to our previous work [30, Section 5], the present work includes the following key differences and improvements:\n\u2022 We have streamlined the pre- and post-processing steps of the biclustering algorithm, simplifying algorithmic choices and unifying the pipeline for different types of parametric models; \u2022 No user intervention (i.e., no parameter tuning) is required to find the number of\nparametric models. During the biclustering process, this number is automatically determined using model selection techniques (i.e., minimum description length); \u2022 We accelerate the biclustering process introducing an accelerated algorithm to solve\nL1-based nonnegative matrix factorization (L1-NMF) problems. This allows to solve medium-sized problems faster while also extending the usability of the algorithm to much larger datasets. This contribution exceeds the context of this work, as this algorithm has potential applications in any other context where an L1-NMF is needed (e.g., traffic analysis [29], eldercare [29], shadow removal for face detection [36], video surveillance [36]).\nWe provide the complete source code of the proposed approach for generating all the examples presented in this work.1\nOrganization. The remainder of this paper is organized as follows. In Section 2 we present the proposed biclustering approach for multiple parametric model estimation. In Section 3 we describe the fast version of the biclustering algorithm. We present diverse and extensive experimental results in Section 4 and finally provide some closing remarks in Section 5."}, {"heading": "2. Random Sample Ensemble", "text": "Let X be a matrix. In the following, (X)ij , (X):j , (X)i: denote the (i, j)-th entry of X, the j-th column of X, and the i-th row of X, respectively.\nThe input of the algorithm is a pool U = {C\u00b5 (X , \u03b8j , \u03b4)}nj=1 of consensus sets (the output of Algorithm 1).\nDefinition 1 (Preference matrix) We define the m \u00d7 n preference matrix A, whose rows and columns represent respectively the m = |X | data elements {xi}mi=1 and the n = |U| consensus sets, as\n(A)ij = { 1 if xi \u2208 C\u00b5 (X , \u03b8j , \u03b4) ; 0 otherwise.\n(6)\nAn example of a preference matrix for a synthetic dataset can be seen in Figure 1. Traditionally, in algorithms like RANSAC, the preference matrix is (often implicitly) analyzed column-by-column or row-by-row. The preference matrix was explicitly introduced in the context of MPME in [31]. In the original formulation, the objects in X were clustered using the rows of A as feature vectors, obtaining a powerful state-of-the-art technique called J-linkage. An extension to work with a non-binary (using soft versus hard element-model membership) version of A was proposed in [20].\n1. https://github.com/marianotepper/arse\nAn alternative approach involves creating an m\u00d7m co-occurrence matrix, defined as\nB = 1n n\u2211 k=1 Bk, where (Bk)ij = { 1 if xi, xj \u2208 C\u00b5 (X , \u03b8k, \u03b4) ; 0 otherwise.\n(7)\nThe matrix B is widely used in the context of ensemble clustering where it is built from clusters obtained from multiple clustering algorithms, instead of consensus sets. There are, within this research area, many algorithms to analyze B: from simple techniques such as applying a clustering algorithm to it (e.g., k-means, hierarchical or spectral clustering), to more complex matrix factorization techniques [e.g., 19]. It is important to point out that A contains more information than B. Noticing that\nB = 1n n\u2211 j=1 (A):j(A) T :j , (8)\nit becomes clear that the averaging operation implies the loss of critical information. In the vast majority of matrix factorization/clustering methods, the number of factors/clusters is a critical and hard-to-tune parameter. But this is in fact one of the parameters we are interested in discovering! In the context of MPME, matrix factorization has been recently applied [21] to a normalized version of B (using soft membership). Provided that the correct number of factors is selected, this method delivers state-of-the-art results.\nAn additional constraint common to all the aforementioned clustering/factorization approaches is that they provide a segmentation of the dataset elements. This means that these models do not correctly handle intersecting/overlapping models. As we will see next, this limitation is not present in the proposed formulation."}, {"heading": "2.1 Analyzing the preference matrix", "text": "Our approach follows a different conceptual path, simultaneously analyzing the rows and columns of the preference matrix A. To explain the rationale behind our formulation, let us rewrite Equation (2) for clarity purposes:\nX =\n( r\u22c3\nk=1\nX\u0302k ) \u222a O with (\u2200k) X\u0302k \u2229 O = \u2205.\nAlgorithm 2: Random Sample Ensemble\ninput : set of objects X , parametric function f\u00b5, inliers threshold \u03b4. output : collection of inliers-model pairs {(Ct, \u03b8t)}Tt=1.\n1 Execute Algorithm 1 to obtain U ; 2 Form the preference matrix A from U (Definition 1); 3 Obtain T biclusters {(ut,vt)}Tt=1 from A (see Section 2.1 for details, note that T is\nautomatically estimated); 4 foreach ut do 5 Ct \u2190 {xi \u2208 X | (ut)i > 0}; 6 Estimate \u03b8t from Ct using least-squares, see Equation (5); 7 Re-compute Ct as C\u00b5 (X , \u03b8t, \u03b4), see Equation (4);\nEach X\u0302k represents one of the ground truth groups that we are seeking to recover. We assume (\u2200k) |X\u0302k| \u2265 b, where b is the minimum number of elements necessary to uniquely characterize a given parametric model.\nLet us assume that during the execution of Algorithm 1 we obtained several pure MSSs for some X\u0302k, i.e., Jk = {j | Xmms(j) \u2286 X\u0302k} (the index j corresponds to the iterations of Algorithm 1). Then, for almost all j, j\u2032 \u2208 Jk their respective models will be very similar, i.e., \u03b8j \u2248 \u03b8j\u2032 ; these models should also be similar to the model estimated from X\u0302k using least squares. Therefore their consensus sets should be similar, i.e., C\u00b5 (X , \u03b8j , \u03b4) \u2248 C\u00b5 ( X , \u03b8j\u2032 , \u03b4 ) \u2248 X\u0302k. From the definition of A, we can then write that\n(A):Jk = u\u0302k1 T + Ok, (9)\nwhere u\u0302k is a binary vector such that (\u2200j \u2208 Jk) (A):j \u2248 u\u0302k, 1 is the \u201call ones\u201d vector of size |Jk|, and Ok accounts for all the errors in this approximation. Finally, as we look to represent all columns of A, we can rewrite the above equation, adding the corresponding zeros, as A = u\u0302kv\u0302 T k +Ok, where v\u0302k is a binary vector of size n such that (\u2200j \u2208 Jk) (v\u0302j)i = 1 and Ok is now of size m\u00d7 n. We can then extend this representation to all ground truth groups X\u0302k, by writing\nA = r\u2211\nk=1\nu\u0302kv\u0302 T k + O, (10)\nwhere u\u0302k \u2208 {0, 1}m, v\u0302k \u2208 {0, 1}n and again O accounts for all errors in the approximation. Notice that Equation (10) allows to cast the problem of analyzing the output of Algorithm 1 in terms of a biclustering problem.\nOnce the biclusters of A have been identified, we can obtain the consensus sets and, from them, estimate the final output models. See Algorithm 2 for a full description of this process. The next question now is: how do we find each bicluster?"}, {"heading": "2.2 L1-NMF", "text": "As usual in the optimization literature, the problem of finding u\u0302 and v\u0302 in Equation (10) is easier to solve if we soften the binary constraints, imposing nonnegativity instead. The only missing component to formalize the problem is an appropriate prior for O. Since the\nerrors are also binary (and thus spurious), a reasonable choice would be to penalize its L1 norm. For convenience, we also drop the binary constraint on O. We can thus write the single-bicluster estimation problem as\nmin u,v \u2225\u2225A\u2212 uvT \u2212O\u2225\u22252 F s.t. u \u2265 0,v \u2265 0, \u2016O\u20161 \u2264 \u03c3, (11)\nwhich, for some \u03c3, is equivalent to our proposed formulation\nmin u,v \u2225\u2225A\u2212 uvT\u2225\u2225 1 s.t. u \u2265 0,v \u2265 0. (L1-NMF)\nWe use the L1 norm to cope with the impulsive nature of the errors in A. This formulation computes a robust median approximation to the preference matrix A, which carries all the needed information. Any standard NMF algorithm can be adapted to use the L1 norm and solve (L1-NMF). The algorithm in [30, App. A] delivers high-quality solutions at the expense of a higher computational cost. In this work, for the sake of speed, we use the following procedure:\n1. Find initializations for u and v using the iterative re-weighting scheme in [15]. In our experiments, this method proved to be rather fast for small-scale problems, but more inaccurate than other alternatives. This makes it a good choice for initialization.\n2. Given the initialization u, solve the convex problem minv\u22650 \u2225\u2225Du (A\u2212 uvT)\u2225\u22251, using\nthe ADMM technique in [30, App. A] (Dx is a diagonal matrix with the indicator vector 1[x>0] in its diagonal). 3. Given the new value of v, solve the convex problem minu\u22650 \u2225\u2225(A\u2212 uvT)Dv\u2225\u22251, using\nthe same technique as before.\nThe method in [15] is not particularly well suited for large-scale problems, as it deals with dense weighting matrices with the size of A. As this size increases, handling these matrices becomes computationally prohibitive. Section 3 is devoted to present a new technique to accelerate the above algorithm, rendering it capable of handling large-scale instances and producing a novel efficient L1-NMF solver."}, {"heading": "2.3 Dealing with multiple biclusters", "text": "A challenge with biclustering (as with classical clustering) is that the number of biclusters is not an easy parameter to set or to estimate in advance. Following a standard approach in the literature [22, 32] we sieve the information in a sequential way. Generically, we iterate two steps until some stopping criterion is met: (1) find one bicluster (u,v), and (2) subtract the information encoded by (u,v) from A.\nAlgorithm 3 summarizes the proposed biclustering approach. In Line 6 we set to zero the columns corresponding to the active set of vt. This enforces disjoint active sets between the successive vt, and hence orthogonality. This also ensures that non-negativity is maintained throughout the iterations. The proposed algorithm is very efficient, simple to code, and demonstrated to work well in the experimental results that we will present later.\nThe iterations should stop (1) when A is empty (Line 3), or (2) when A only contains noise (no structured patterns). We deal with the second case in Line 7, considering that any bicluster formed by a single model instance is spurious.\nAlgorithm 3: Sequential rank-one biclustering algorithm.\ninput : matrix A \u2208 Rm\u00d7n output : set of biclusters {(ut,vt)}Tt=1\n1 A1 \u2190 A; 2 foreach t \u2208 {1 . . . n} do 3 if At = 0 then 4 break;\n5 Find one bicluster (ut,vt) of At, where ut \u2208 Rm+ and vt \u2208 Rn+;\n6 (\u2200i, j) (At+1)ij \u2190\n{ 0 if (vt)j > 0,\n(At)ij otherwise;\n7 if \u2016vt\u20160 \u2264 1 then 8 break;\n9 if t < n then // if early stop, t\u2212 1 is the last good bicluster 10 T \u2190 t\u2212 1; 11 Prune {(ut,vt)}Tt=1 using a model selection strategy;"}, {"heading": "2.4 Minimum description length as a stopping criterion", "text": "The core of Algorithm 3 (lines 1 to 8) does not provide a reliable mean to determine the number of biclusters to be extracted from the preference matrix. Indeed, lines 3 and 7 only provide a rough stopping criterion when At+1 is empty or when the bicluster is not the product of a consensus between at least two models. This criterion will output more models than needed, and this section is devoted to prune the collection {(ut,vt)}Tt=1.\nFor each bicluster, we are only interested in the support of its vectors ut and vt (which are sparse by design). We can then binarize them to avoid being mislead by small numerical inaccuracies that might have occurred during the optimization procedure, i.e., ut = butc\u03b3 and vt = bvtc\u03b3 , where\n(bxc\u03b3)i = { 1 if (x)i > \u03b3 \u00b7 \u2016x\u2016\u221e ; 0 otherwise.\n(12)\nIn practice, we set \u03b3 = 10\u22124 once and for all the tests in this paper.\nWe now pose the following model selection problem: given the biclusters {(ut,vt)}Tt=1 and the remainders {At+1}Tt=1 (which are binary by definition), find the value K in [1, T ] such that the preference matrix A is optimally described by A \u2248 \u2211K t=1 utv T t + AK+1. This can be considered as an hypothetical compression problem where the task is to encode A using these elements. The model selection procedure then keeps the value K that minimizes the combined codelenghts L (in bits) of its components,\nargmin 1\u2264K<T K\u2211 t=1 L(ut) + K\u2211 t=1 L(vt) + L(AK+1). (13)\nUnder the usual assumption that ut, vt, and AK+1 are individually decorrelated, we can describe each one as a (one dimensional) i.i.d. Bernoulli sequence of values. For a p-\ndimensional vector p, this can be efficiently described using an enumerative code [10],\nL(p) = log2\n( p\n\u2016p\u20160\n) + log2 p, (14)\nwhere \u2016\u00b7 \u20160 denotes number of non-zero elements of the argument and ( a b ) is the binomial coefficient. With a slight abuse of notation, for a matrix M we write L(M) = L(vec(M)), where vec(\u00b7 ) is a vectorization operator."}, {"heading": "2.5 Statistical validation for pre-processing", "text": "The standard random sampling approach (Algorithm 1) to multiple model estimation generates many good model instances (composed of inliers), but also generates many bad models (composed mostly of outliers). In general, the number of bad models exceeds by far the number of good ones. It is not worth devoting computational effort to the analysis of these columns of A. Any pattern-discovery technique, such as the biclustering approach presented in the previous section, would benefit from having a simple, efficient, and statistically meaningful method for discarding bad models. These models will typically contain only a handful of objects. The question is how do we determine the minimum size for a good consensus set? This important computational contribution, based on the a contrario testing mechanism presented in depth in [12], is addressed next.\nLet us assume that we have a set X of random elements. Let \u00b5(\u03b8) be a model with an associated consensus set C\u00b5(X , \u03b8, \u03b4), built with tolerance \u03b4 (Equation (4), Page 2). We will assume, under the background model, that all objects are i.i.d. and that the error in Equation (3) locally follows an uniform distribution; this type of simple approximations has proven successful for outlier rejection [12]. Let \u03ba be a locality parameter. If there are |C\u00b5 (X , \u03b8, \u03b4)| elements at a distance \u03b4 from \u00b5(\u03b8), we expect to have in average \u03ba times more elements at a distance \u03ba\u03b4. We are interested in computing the probability that C\u00b5(X , \u03b8, \u03b4) has at least m\u03b4 elements given that C\u00b5(X , \u03b8, \u03ba\u03b4) contains m\u03ba\u03b4 elements. The probability of such an event is B (m\u03ba\u03b4 \u2212 b,m\u03b4 \u2212 b; p), where B is the binomial tail and p = \u03b4/\u03ba\u03b4 = \u03ba\u22121 is the probability that a random object belongs to the consensus set C\u00b5(X , \u03b8, \u03b4) given that it belongs to C\u00b5(X , \u03b8, \u03ba\u03b4). Recall that b is the minimum number of elements necessary to uniquely characterize a given parametric model. As such, b elements in C\u00b5(X , \u03b8, \u03ba\u03b4) are necessarily non-random, since they were used to estimate \u00b5(\u03b8). This is the reason behind subtracting b elements from m\u03ba\u03b4 and m\u03b4.\nDefinition 2 Let \u00b5(\u03b8) be a model instance, C\u00b5(X , \u03b8, \u03b4) be its associated consensus set, obtained with precision parameter \u03b4, and \u03ba > 1 be a locality parameter. We define the number of false Alarms (NFA) of model instance \u00b5(\u03b8) as\nNFA\u00b5,\u03b4,\u03ba(X , \u03b8) = Ntests \u00b7 B ( |C\u00b5 (X , \u03b8, \u03ba\u03b4)| \u2212 b, |C\u00b5 (X , \u03b8, \u03b4)| \u2212 b;\u03ba\u22121 ) , (15)\nwhere Ntests = ( m b ) represents the total number of possible models. We say that the model \u00b5(\u03b8) is said to be \u03b5-meaningful if\nNFA\u00b5,\u03b4,\u03ba(X , \u03b8) < \u03b5. (16)\nIt is easy to prove, by the linearity of expectation, that the expected number of \u03b5-meaningful models in a finite set of random models is smaller than \u03b5. Alternatively, Ntests can be empirically set by analyzing a training dataset [6], providing a tighter bound for the expectation.\nDefinition 2 provides a formal probabilistic method for testing if a model is likely to happen at random or not. From a statistical viewpoint, the method goes back to multiple hypothesis testing. Following an a contrario reasoning [12], we decide whether the event of interest has occurred if it has a very low probability of occurring by chance in the above defined random (background) model. In other words, a model instance \u00b5(\u03b8) is \u03b5-meaningful if |C\u00b5(X , \u03b8, \u03b4)| is sufficiently large to have NFA\u00b5,\u03b4,\u03ba(X , \u03b8) < \u03b5. We only keep columns of A corresponding \u03b5-meaningful model instances. We set \u03b5 = 1 for all the experiments.\nAs a result of this statistical validation procedure, the preference matrix A is considerably shrunk (the actual size reduction will depend on the inliers-outliers ratio and the number of model instances in the dataset). This shrunk preference matrix is fed to the biclustering algorithm (Algorithm 3), gaining in stability of the results as well as in speed.\nNote 1 There are techniques in the literature [e.g., 7], which follow a different route: instead of sampling first and prune later, they try to dynamically sample good models from the start. These techniques have proven successful in reducing the number of required samples but operate at a much slower rate per sample. Their dynamic nature makes parallelization more difficult and limited. In our view, a detailed comparison between these paradigms is needed, with special consideration given to the use of parallelization (e.g., GPUs)."}, {"heading": "2.6 Statistical validation for post-processing", "text": "Once the biclustering algorithm has returned a collection of inliers-model pairs, we need to verify that these models are statistically meaningful from a geometric point of view. For this, we use the test in Definition 2 once again.\nAlgorithm 4: Exclusion principle\ninput : collection of inliers-model pairs {(Ct, \u03b8t)}Tt=1. output : pruned collection of inliers-model pairs {(Ct, \u03b8t)}Tt=1.\n1 K \u2190 \u2205; 2 S \u2190 X ; 3 foreach (Ct, \u03b8t) do 4 if NFA\u00b5(S, \u03b8, \u03b4) is meaningful then 5 K \u2190 K \u222a {(Ct, \u03b8t)}; 6 S \u2190 X \\ Ct;\nAdditionally, models are allowed to overlap and can thus share elements, which makes situations like the one described in Figure 2 commonplace. In short, a good model should separate itself from the background noise regardless of its intersection with other models. The exclusion principle described in [12, Chapter 6] performs this check. It states that, given two groups obtained by the same detector, no point x is allowed to belong to both groups. In our case, since we are explicitly modeling overlaps between groups, we use a milder version of this principle: we simply ask that any point can only contribute to the NFA of at most one group. This is explained in Algorithm 4. Notice that the order in which the inliers-models pairs are explored affects the result; we sort the biclusters according to their total size \u2016ut\u20160 \u00b7 \u2016vt\u20160 (Equation (12)).\nOptionally and if required by the application, as the last step in our post-processing chain, we can force the models to have an empty intersection. There are many alternative ways to address this assignment problem. In this work, we simply assign elements in the intersection of several models to the closest model in distance (see Equation (3))."}, {"heading": "3. Accelerated Random Sample Ensemble", "text": "The problem of multiple parametric model estimation does not escape the current trend of growth in datasets size, which exposes the need of fast techniques to cope with these massive datasets. Thus, after having described the proposed Random Sample Ensemble algorithmic pipeline in Section 2, we now turn our attention to its acceleration. There are two computational bottlenecks in our approach.\nThe first one is shared by virtually all the algorithms in the field: running Algorithm 1 (Page 3). Fortunately, all the random samples can be computed in parallel, reducing the problem to clever software engineering. Alternatively, there are recent techniques to reduce the number of needed samples [7], at the expense of a less parallelizable algorithm.\nSecond, the main component of the proposed technique is the biclustering algorithm, and this section is thus devoted to describe how to efficiently solve this optimization problem. Let us remind the reader that we are seeking to solve Problem (L1-NMF) (Page 7), a challenge with applications beyond the problem at hand. Before moving forward with the exposition, we need to lay the ground by providing a few definitions and key concepts.\nThe fast Cauchy transform. The fast Johnson-Lindenstrauss transform [1, 2] provides a way to build a low dimensional embedding in the `2 case and has been widely used in many practical settings. Its `1-based analog is the fast Cauchy transform (FCT) [9], which\ndefines an h\u00d7m embedding matrix \u03a0 (h m) as\n\u03a0 = 4BCH\u0303. (17)\nThe matrices B, C, and H\u0303 are built such that\n\u2022 each column of B \u2208 Rh\u00d72m is chosen at random from the h standard basis vectors for Rh;\n\u2022 C \u2208 R2m\u00d72m is a diagonal matrix with entries sampled from a Cauchy distribution; and\n\u2022 H\u0303 \u2208 R2m\u00d7m is a block-diagonal matrix comprised of m/s equal blocks along the diagonal (we set s = h6 and we assume it to be a power of two and m/s is an integer), i.e.,\nH\u0303 def =\nGs Gs\n. . .\nGs\nwhere Gs def =\n[ s\u22121/2 \u00b7Hs\nIs\n] , (18)\nIs is the s\u00d7s identity matrix, and Hs is the s\u00d7s Hadamard matrix, defined recursively as\nH2 = [ +1 +1 +1 \u22121 ] , Hs = [ Hs/2 Hs/2 Hs/2 \u2212Hs/2 ] . (19)\nThe following theorem provides some guarantees about the FCT.\nTheorem 3 ([9]) Let A \u2208 Rm\u00d7n be a matrix of rank r (r n). There is a distribution (given by the above FCT construction) over matrices \u03a0 \u2208 Rh\u00d7m with h = O(r log r+r log 1\u03b7 ) such that, for all x \u2208 Rn, the inequalities\n\u2016Ax\u20161 \u2264 \u2016\u03a0Ax\u20161 \u2264 \u03c4 \u2016Ax\u20161 (20)\nhold with probability 1\u2212 \u03b7, where\n\u03c4 = O\n( r \u221a s\n\u03b7 log(hr)\n) . (21)\nConsidering \u03b7 as a (small) constant and recalling that s = h6, we have h = O(r log r) and finally \u03ba = O ( r4 log4 r ) . However, if we set s h6, the performance in practice does not seem to be negatively affected, even if this setting does not follow the above theorem [9, Section 6.1]. In our experiments, we use s = h."}, {"heading": "3.1 Fast `1 regression", "text": "The FCT can be used as a building block for the construction of fast solvers for `1 regression problems. We describe this first before presenting the L1-NMF proposed algorithm.\nDefinition 4 ([26]) Let A \u2208 Rm\u00d7n be a matrix of rank r. A basis B \u2208 Rn\u00d7r for the range of A is (\u03b1, \u03b2)-conditioned if \u2016B\u20161 \u2264 \u03b1 and (\u2200x \u2208 Rr) \u2016x\u2016\u221e \u2264 \u2016Bx\u20161. We say that B is well conditioned if \u03b1 and \u03b2 are low degree polynomials in r, independent of m and n.\nDefinition 5 ([26]) Given a well-conditioned basis B for the range of A \u2208 Rm\u00d7n, we define the `1 leverage scores of A as the m-dimensional vector \u03bb, with elements (\u03bb)i = \u2016(B):i\u20161.\nThe leverage scores of A can be found with the following procedure [9, 26]: 1. build an FCT matrix \u03a0 \u2208 Rr\u00d7m; 2. find a matrix R such that \u03a0AR is orthonormal; 3. using Definition 5, compute the leverage scores \u03bb of B = AR\u2020 (R\u2020 denotes the pseudoinverse of R); The leverage scores are used in [9, 26] to speed up the algorithmic solution of x\u2217 = argminx \u2016Ax\u2212 b\u20161. For this, the authors build a diagonal matrix E, i.e., a compression matrix, by sampling its diagonal entries independently according to a set of probabilities pi that are proportional to the `1 leverage scores of A. Then, given x\u0302 = argminx \u2016E(Ax\u0302\u2212 b)\u20161 we have that \u2016Ax\u0302\u2212 b\u20161 \u2264 (1 + ) \u2016Ax\u2217 \u2212 b\u20161 for some small . Key to the speedup is the size reduction of the regression problem, since only a few rows of A are kept in EA and considered to find x\u0302."}, {"heading": "3.2 Fast `1 NMF", "text": "We now have all the elements that we need to build a fast algorithm for solving Problem (L1NMF). Considering that we are dealing with a biconvex problem, it is a standard practice to find a solution to it by alternating two `1 least squares problems,\nu\u0302 = arg min u \u2225\u2225A\u2212 uv\u0302T\u2225\u2225 1 s.t. u \u2265 0, (22a)\nv\u0302 = arg min v \u2225\u2225A\u2212 u\u0302vT\u2225\u2225 1 s.t. v \u2265 0. (22b)\nEach sub-problem can be sped up by respectively compressing the rows and the columns of A. Similarly as in the regression case, row (resp. column) compression is achieved through the computation of the leverage scores of A (resp. AT). Let C and R be the matrices that perform row and column compression, respectively. We can then iterate the compressed least squares problems\nu\u0302 = arg min u \u2225\u2225(A\u2212 uv\u0302T)C\u2225\u2225 1 s.t. u \u2265 0, (23a)\nv\u0302 = arg min v \u2225\u2225R (A\u2212 u\u0302vT)\u2225\u2225 1 s.t. v \u2265 0. (23b)\nThis would already give a very efficient algorithm for solving the problem of interest. In our experiments, we found that augmenting the procedure described in Page 7 with the above described compression techniques was faster and produced good results. We thus obtain the following accelerated procedure for solving Problem (L1-NMF):\n1. Given a column compression matrix C for A, find an initialization for u by solving the compressed problem\nu\u0302, v\u0302 = argmin u,v\u0303\n\u2225\u2225AC\u2212 uv\u0303T\u2225\u2225 1 s.t. u \u2265 0, v\u0303 \u2265 0, (24)\nwith the iterative re-weighting scheme in [15]. The compressed vector v\u0302 has no further use in our algorithm. 2. Given the initialization u\u0302 and a row compression matrix Ru\u0302 for Du\u0302A (Dx is a diagonal matrix with the vector 1[x>0] in its diagonal.), solve the compressed convex problem\nminv\u22650 \u2225\u2225Ru\u0302Du\u0302 (A\u2212 u\u0302vT)\u2225\u22251, using the ADMM technique in [30, App. A].\n3. Given the new value of v and a column compression matrix Cv for ADv, solve the compressed convex problem minu\u22650 \u2225\u2225(A\u2212 uvT)DvCv\u2225\u22251, using the same technique as before. All compression matrices are computed with the same compression level r, used to build the FCT matrix. We also found in our experimental results that instead of using random sampling for building the row and column compression matrices, as in [9], good results were obtained by simply selecting the r largest leverage scores."}, {"heading": "4. Experimental results", "text": "In the following, we refer to the proposed Random Sample Ensemble as RSE and to its compressed version, Accelerated Random Sample Ensemble, as ARSE. In our experiments, we compared the quantitative results of both methods, but only present qualitative results for ARSE. All the parameters for RSE and ARSE are exactly the same and in each figure/table we specify \u03b4 (Equation (4)); unless specified, \u03ba = 3 (Definition 2). The only exception is the compression rate in ARSE, which will be set to h = 32 in all experiments, see Equation (17).\nEvaluation. To measure performance, we use the standard precision and recall. In order to compute these measures, we compute an optimal assignment between the ground truth and the tested groups, using the Hungarian algorithm. Once the two sets of groups are appropriately matched, we can then compute the precision and the recall of the tested groups in a standard fashion.\nIf models are not allowed to share elements, it is not unusual in the literature to consider the outliers as an additional ground truth group to recover; in this case, the misclassification error is often reported.\nIn the general case where models overlap, we also use, as an additional measure, the generalized normalized mutual information (GNMI) [16], which extends the normalized mutual information (also called symmetric uncertainty) [33, p. 310] to the case of groups with overlaps.\n2D lines and circles. We start our experimental evaluation with a few small synthetic datasets [31] where 2D points are arranged forming lines and circles. The results are shown in figures 3 and 4, where ARSE clearly detects the correct models in each case. In Table 1 we compare the performance of RSE and ARSE in all these datasets. As expected, when the dataset (and the preference matrix) are rather small, ARSE is only slightly faster than\nRSE; however, as the dataset gets larger (e.g., Figure 3(h)), ARSE becomes significantly faster than RSE.\nRegarding the accuracy of both methods, RSE and ARSE are virtually equal for the examples in Figure 3, except in figures 3(i) and 3(j), where RSE performs slightly better. We also observe in figures 3 and 4 that the proposed approach can correctly recover overlapping models. This is an intrinsic limitation of previous state-of-the-art competitors such as Jlinkage [31], T-linkage [20], RPA [21] and most multiple model estimation techniques, which are generally based on partitioning (clustering) the dataset.\nVanishing points. Under the assumption of perspective projection (e.g., with a pinhole camera), sets of parallel lines in 3D space are projected into a 2D image as a set of concurrent lines. The point in the image plane at which these lines meet is called a vanishing point. Vanishing points provide important information to make inferences about the 3D structures of a scene from its 2D (projected) image. Since these projections are non-invertible, concurrence in the image plane does not necessarily imply parallelism in 3D. This said, the counterexamples for this implication are rare in real images, and the problem of finding parallel lines in 3D is reduced in practice to finding vanishing points in the image plane [e.g., 3, 4, 11, 18, 28]. We are here interested in finding vanishing points when no a priori information is known about the image or the camera with which it was taken.\nThe dataset elements in this case are line segments extracted with a suitable algorithm (e.g., [14]). As the distance between a vanishing point and a line segment, we consider the angular difference between the line segment and the line joining the vanishing point and the segment\u2019s midpoint.\nTo evaluate the proposed framework for the task of detecting vanishing points, we use the York Urban database [11], which comes with ground truth assignments of image segments to three orthogonal directions in 3D. Of course, since the dataset images represent urban scenes, performance can be readily boosted by adding problem-specific constraints to the proposed algorithm as an extra postprocessing step; common simplifications include knowledge about the camera calibration and the Manhattan world assumption [e.g., 18].\nThe results of the proposed method are presented in Table 2. There we can observe that RSE and ARSE perform well in terms of precision and recall. ARSE is in average twice as fast as RSE. In Figure 5 we can observe a few characteristic examples of ARSE\u2019s results. Notice that in the two rightmost examples, the method correctly finds more than three vanishing points; these additional points are deemed incorrect in the results of Table 2, as the ground truth only contemplates three points.\nFundamental matrices and homographies. For these examples, where we are trying to estimate geometric transforms between two images, the base elements are keypoint matches. Any modern method to find those matches would work relatively well for our purposes; we use BRISK [17]. We show in Figure 6 two examples of fundamental matrix estimation on a stereo pair of images. The only motion in the scene corresponds to a change in camera perspective, and it can be described with a single fundamental matrix. Both\nmethods achieve extremely similar results and in both cases (left and right pairs) they find a single model. The example on the left contains 2518 matches. ARSE detects 1377 matches in 7.99 seconds, while RSE detects 1363 matches in 98.32 seconds. ARSE\u2019s precision and recall, considering the result of RSE as ground truth, are 0.959 and 0.968, respectively. The example on the right contains 8999 matches. ARSE detects 5533 matches in 30.40 seconds, whereas RSE detects 5537 matches in 1033.45 seconds. In this case, the precision is 0.959 and recall is 0.968. However, there is a huge difference in speed between both methods, which is accentuated with the size of the dataset. On the left example, ARSE is about 12 times faster, whereas on the right example it is about 34 times faster.\nIt is important to emphasize that other random sampling techniques can be used beyond the baseline in Algorithm 1. To show this, we have included experiments using the state-ofthe-art MultiGS algorithm [7]. It is worth pointing out that a recent technique [5] has been able to reduce the size of the minimal sample set for fundamental matrices to b = 2; its use would significantly reduce the computational complexity of the sampling step, imposing an upper bound of O(m2) to the total number of possible combinations.\nWe estimate multiple fundamental matrices (moving camera and moving objects) and multiple homographies (moving planar objects) on the images in the AdelaideRMF dataset [34]. Although we use this dataset because it is standard in the literature [e.g., 20, 21, 23, 34], we would like to point out that the ground truth of this dataset contains a nonnegligible quantity of severe errors. In Figure 7 we show two examples where these errors are easily identified upon visual inspection. The dataset\u2019s ground truth would need a thorough revision to be truly useful as a scientific benchmark. We include these results nonetheless for the sake of completeness.\nThe results on the full dataset are presented in Figure 8. We include comparisons with T-linkage [20], RCMSA [23], and RPA [21]. Among these three methods, RPA is the one that achieves the best performances; however, to yield these performances RPA uses as an input the ground truth number of models to estimate. The other methods, as the proposed method, automatically estimate this number.\nIn Figure 8(a), RSE and ARSE perform on average similarly as T-linkage. However, notice that the median performance of ARSE is similar to the one of RPA, with the added benefit of not having to tune a critical and fundamental parameter. In Figure 8(b), both RSE and ARSE clearly outperform all other methods. The complete numerical results are included in tables 3 and 4.\n3D planes estimation. In recent years, people have started \u201cscanning\u201d objects with range imaging hardware (e.g., Kinect or RealSense cameras) to obtain 3D point clouds. The geometric nature of man-made objects (composed of planes, cylinders, spheres, etc.) is very well adapted to the analysis of their corresponding point clouds with MPME.\nAs an example, we show the capabilities of our method in this setting with the detection of 3D planes on the Pozzovegianni dataset.2 In this case, the 3D points are obtained from different images of a building (Figure 9(a)) with a sparse multi-view 3D reconstruction algorithm. Our method recovers the 3D planes in the scene and properly reconstructs the building structure. We evaluate our results on four different versions of the dataset, using 10%, 20%, 50%, and 100% the points. In Figure 10(a), we show the estimated planes on the first (10%) and last (100%) versions. Both results are very similar except for the bell tower, which is correctly recovered in the latter case but not in the former (mainly because there are not enough points for a proper estimation).\n2. http://www.diegm.uniud.it/fusiello/demo/samantha/\nThe experimental setup used in the above paragraph provides a good opportunity to measure the speed difference between RSE and ARSE, see Figure 11(a). ARSE is faster than RSE for small datasets, as in all the previous examples; however, the speed difference is not as striking (2.4 times faster when using 10% of the points). When working with increasingly bigger datasets, this difference becomes more and more pronounced (60 times faster when using 100% of the points), see the left plot in Figure 11(a). The other appealing quality of ARSE is that its speed scales very gracefully with the preference matrix size, see the right plot in Figure 11(a).\nWe ran the same experiment on the Piazza Bra dataset,3 see Figure 12. We only ran RSE on the dataset versions that include 10% and 20% of the points, because of RAM memory issues. Notice that ARSE with a 82831 \u00d7 24841 preference matrix is faster than RSE with a 16567\u00d72040 preference matrix (the former matrix being about 61 times bigger than the latter)."}, {"heading": "5. Conclusions", "text": "In this work we introduced a complete and comprehensive algorithmic pipeline for multiple parametric model estimation. The proposed approach takes the information produced by a random sampling algorithm (e.g., RANSAC) and analyzes it from a machine learning/optimization perspective, using a parameterless biclustering algorithm based on L1 nonnegative matrix factorization (L1-NMF).\nThis new formulation conceptually changes the way that the data produced by the popular RANSAC, or related model-candidate generation techniques, is analyzed. It exploits\n3. See Footnote 2\nconsistencies that naturally arise during the RANSAC execution, while explicitly avoiding spurious inconsistencies. Additionally and contrarily to the main trends in the literature, the proposed modeling does not impose non-intersecting parametric models.\nWe also presented an accelerated version of the biclustering process, introducing an accelerated algorithm to solve L1-NMF problems. This allows to solve medium-sized problems faster while also extending the usability of the algorithm to much larger datasets. We emphasize that this contribution exceeds the context of this work, as this accelerated algorithm has potential applications in any other context where an L1-NMF is needed.\nAs a future line of research, we are currently investigating whether using a hard thresholding scheme is actually necessary. Instead of working with binary data, we could work with a real-valued object-model distance matrix, eliminating a critical parameter that has been haunting the RANSAC framework for years."}], "references": [{"title": "Approximate nearest neighbors and the fast Johnson- Lindenstrauss transform", "author": ["N. Ailon", "B. Chazelle"], "venue": "STOC,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "The fast Johnson-Lindenstrauss transform and approximate nearest neighbors", "author": ["N. Ailon", "B. Chazelle"], "venue": "SIAM J. Comput., 39(1):302\u2013322,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Vanishing point detection without any a priori information", "author": ["A. Almansa", "A. Desolneux", "S. Vamech"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 25(4):502\u2013507,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Vanishing point detection by segment clustering on the projective space", "author": ["F. Andal\u00f3", "G. Taubin", "S. Goldenstein"], "venue": "ECCV Workshops,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Two points fundamental matrix", "author": ["G. Ben-Artzi", "T. Halperin", "M. Werman", "S. Peleg"], "venue": "Technical report,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Image segmentation by a contrario simulation", "author": ["N. Burrus", "T.M. Bernard", "J.M. Jolion"], "venue": "Pattern Recognit., 42(7):1520\u20131532,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerated hypothesis generation for multistructure data via preference analysis", "author": ["T.J. Chin", "J. Yu", "D. Suter"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 34(4):625\u2013638,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Performance evaluation of RANSAC family", "author": ["S. Choi", "T. Kim", "W. Yu"], "venue": "BMVC,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "The fast Cauchy transform and faster robust linear regression", "author": ["K.L. Clarkson", "P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "X. Meng", "D.P. Woodruff"], "venue": "SIAM J. Comput., 45(3):763\u2013810,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Enumerative source encoding", "author": ["T.M. Cover"], "venue": "IEEE Trans. Inf. Theory, 19(1):73\u201377,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1973}, {"title": "Efficient edge-based methods for estimating manhattan frames in urban imagery", "author": ["P. Denis", "J.H. Elder", "F. Estrada"], "venue": "ECCV,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "From Gestalt Theory to Image Analysis, volume 34", "author": ["A. Desolneux", "L. Moisan", "J.M. Morel"], "venue": "Springer-Verlag,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography", "author": ["M. Fischler", "R. Bolles"], "venue": "Commun. ACM, 24 (6):381\u2013395,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1981}, {"title": "LSD: A fast line segment detector with a false detection control", "author": ["R. Grompone von Gioi", "J. Jakubowicz", "J.M. Morel", "G. Randall"], "venue": "IEEE Trans Pattern Anal Mach Intell,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Robust nonnegative matrix factorization using l21-norm", "author": ["D. Kong", "C. Ding", "H. Huang"], "venue": "CIKM,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Detecting the overlapping and hierarchical community structure in complex networks", "author": ["A. Lancichinetti", "S. Fortunato", "J. Kert\u00e9sz"], "venue": "New J. Phys., 11(3):33015,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "BRISK: Binary robust invariant scalable keypoints", "author": ["S. Leutenegger", "M. Chli", "R.Y. Siegwart"], "venue": "ICCV,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding vanishing points via point alignments in image primal and dual domains", "author": ["J. Lezama", "R. von Gioi", "G. Randall", "J.-M. Morel"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Solving consensus and semi-supervised clustering problems using nonnegative matrix factorization", "author": ["T. Li", "C. Ding", "M.I. Jordan"], "venue": "ICDM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "T-linkage: A continuous relaxation of J-linkage for multimodel fitting", "author": ["L. Magri", "A. Fusiello"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust multiple model fitting with preference analysis and low-rank approximation", "author": ["L. Magri", "A. Fusiello"], "venue": "BMVC,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "From K-means to higher-way coclustering: multilinear decomposition with sparse latent factors", "author": ["E.E. Papalexakis", "N.D. Sidiropoulos", "R. Bro"], "venue": "IEEE Trans. Signal Process., 61(2):493\u2013506,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "The random cluster model for robust geometric fitting", "author": ["T.T. Pham", "T.J. Chin", "J. Yu", "D. Suter"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 36(8):1658\u20131671,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "MAC-RANSAC: A robust algorithm for the recognition of multiple objects", "author": ["J. Rabin", "J. Delon", "Y. Gousseau", "L. Moisan"], "venue": "3DPTV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "High-resolution stereo datasets with subpixel-accurate ground truth", "author": ["D. Scharstein", "H. Hirschm\u00fcller", "Y. Kitajima", "G. Krathwohl", "N. Ne\u0161i\u0107", "X. Wang", "P. Westling"], "venue": "GCPR,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Subspace embeddings for the L1-norm with applications", "author": ["C. Sohler", "D. Woodruff"], "venue": "STOC,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonlinear mean shift for clustering over analytic manifolds", "author": ["R. Subbarao", "P. Meer"], "venue": "CVPR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Non-iterative approach for fast and accurate vanishing point detection", "author": ["J.P. Tardif"], "venue": "ICCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-temporal foreground detection in videos", "author": ["M. Tepper", "A. Newson", "P. Sprechmann", "G. Sapiro"], "venue": "ICIP,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "A bi-clustering framework for consensus problems", "author": ["M. Tepper", "G. Sapiro"], "venue": "SIAM J. Imaging Sci., 7(4):2488\u20132525,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust multiple structures estimation with J-linkage", "author": ["R. Toldo", "A. Fusiello"], "venue": "ECCV,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D.M. Witten", "R. Tibshirani", "T. Hastie"], "venue": "Biostatistics, 10(3):515\u2013534,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["I.H. Witten", "E. Frank", "M. a. Hall"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Dynamic and hierarchical multi-structure geometric model fitting", "author": ["H.S. Wong", "T.J. Chin", "J. Yu", "D. Suter"], "venue": "ICCV,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "A new curve detection method: Randomized Hough transform (RHT)", "author": ["L. Xu", "E. Oja", "P. Kultanen"], "venue": "Pattern Recognit. Lett., 11(5):331\u2013338,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1990}, {"title": "Online Nonnegative Matrix Factorization with Outliers", "author": ["R. Zhao", "V.Y.F. Tan"], "venue": "Technical report,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "The multiRANSAC algorithm and its application to detect planar homographies", "author": ["M. Zuliani", "C.S. Kenney", "B.S. Manjunath"], "venue": "ICIP,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "MPME is an important but difficult problem, as standard robust estimators, like RANSAC (RANdom SAmple Consensus) [8, 13], are designed to extract a single model.", "startOffset": 113, "endOffset": 120}, {"referenceID": 12, "context": "MPME is an important but difficult problem, as standard robust estimators, like RANSAC (RANdom SAmple Consensus) [8, 13], are designed to extract a single model.", "startOffset": 113, "endOffset": 120}, {"referenceID": 12, "context": "The number n is an overestimation of the number of trials needed to obtain a certain number of \u201cgood\u201d models [13, 31, 37].", "startOffset": 109, "endOffset": 121}, {"referenceID": 30, "context": "The number n is an overestimation of the number of trials needed to obtain a certain number of \u201cgood\u201d models [13, 31, 37].", "startOffset": 109, "endOffset": 121}, {"referenceID": 36, "context": "The number n is an overestimation of the number of trials needed to obtain a certain number of \u201cgood\u201d models [13, 31, 37].", "startOffset": 109, "endOffset": 121}, {"referenceID": 23, "context": ", [24].", "startOffset": 2, "endOffset": 6}, {"referenceID": 36, "context": "However, this approach is known to be suboptimal [37].", "startOffset": 49, "endOffset": 53}, {"referenceID": 36, "context": "The multiRANSAC algorithm [37] provides a more effective alternative, although the number of models must be known a priori, imposing a very limiting constraint in many applications.", "startOffset": 26, "endOffset": 30}, {"referenceID": 34, "context": ", using the Randomized Hough Transform [35], or by using non-parametric density estimation techniques, like the mean-shift clustering algorithm [27].", "startOffset": 39, "endOffset": 43}, {"referenceID": 26, "context": ", using the Randomized Hough Transform [35], or by using non-parametric density estimation techniques, like the mean-shift clustering algorithm [27].", "startOffset": 144, "endOffset": 148}, {"referenceID": 30, "context": "These, however, are not intrinsically robust techniques, even if they can be robustified with outliers rejection heuristics [31].", "startOffset": 124, "endOffset": 128}, {"referenceID": 30, "context": "Moreover, the choice of the parametrization and its discretization are critical, among other important shortcomings [31].", "startOffset": 116, "endOffset": 120}, {"referenceID": 30, "context": "From a different perspective, J-linkage [31], T-linkage [20], and RPA [21] address the problem by clustering the dataset.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "From a different perspective, J-linkage [31], T-linkage [20], and RPA [21] address the problem by clustering the dataset.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "From a different perspective, J-linkage [31], T-linkage [20], and RPA [21] address the problem by clustering the dataset.", "startOffset": 70, "endOffset": 74}, {"referenceID": 29, "context": "We have previously introduced a novel framework and perspective to reach consensus in grouping problems by re-framing them as biclustering problems [30].", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": ", traffic analysis [29], eldercare [29], shadow removal for face detection [36], video surveillance [36]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": ", traffic analysis [29], eldercare [29], shadow removal for face detection [36], video surveillance [36]).", "startOffset": 35, "endOffset": 39}, {"referenceID": 35, "context": ", traffic analysis [29], eldercare [29], shadow removal for face detection [36], video surveillance [36]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 35, "context": ", traffic analysis [29], eldercare [29], shadow removal for face detection [36], video surveillance [36]).", "startOffset": 100, "endOffset": 104}, {"referenceID": 30, "context": "The preference matrix was explicitly introduced in the context of MPME in [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "An extension to work with a non-binary (using soft versus hard element-model membership) version of A was proposed in [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "But this is in fact one of the parameters we are interested in discovering! In the context of MPME, matrix factorization has been recently applied [21] to a normalized version of B (using soft membership).", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "Find initializations for u and v using the iterative re-weighting scheme in [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "The method in [15] is not particularly well suited for large-scale problems, as it deals with dense weighting matrices with the size of A.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "Following a standard approach in the literature [22, 32] we sieve the information in a sequential way.", "startOffset": 48, "endOffset": 56}, {"referenceID": 31, "context": "Following a standard approach in the literature [22, 32] we sieve the information in a sequential way.", "startOffset": 48, "endOffset": 56}, {"referenceID": 9, "context": "dimensional vector p, this can be efficiently described using an enumerative code [10],", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "The question is how do we determine the minimum size for a good consensus set? This important computational contribution, based on the a contrario testing mechanism presented in depth in [12], is addressed next.", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "and that the error in Equation (3) locally follows an uniform distribution; this type of simple approximations has proven successful for outlier rejection [12].", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "On the left, an arrangement of points built from uniformly sampling 100 points in [0, 1]2, 100 points in [0.", "startOffset": 82, "endOffset": 88}, {"referenceID": 0, "context": "31]\u00d7 [0, 1], 100 points in [0.", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": "71]\u00d7[0, 1], 50 in [0.", "startOffset": 4, "endOffset": 10}, {"referenceID": 5, "context": "Alternatively, Ntests can be empirically set by analyzing a training dataset [6], providing a tighter bound for the expectation.", "startOffset": 77, "endOffset": 80}, {"referenceID": 11, "context": "Following an a contrario reasoning [12], we decide whether the event of interest has occurred if it has a very low probability of occurring by chance in the above defined random (background) model.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "Alternatively, there are recent techniques to reduce the number of needed samples [7], at the expense of a less parallelizable algorithm.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "The fast Johnson-Lindenstrauss transform [1, 2] provides a way to build a low dimensional embedding in the `2 case and has been widely used in many practical settings.", "startOffset": 41, "endOffset": 47}, {"referenceID": 1, "context": "The fast Johnson-Lindenstrauss transform [1, 2] provides a way to build a low dimensional embedding in the `2 case and has been widely used in many practical settings.", "startOffset": 41, "endOffset": 47}, {"referenceID": 8, "context": "Its `1-based analog is the fast Cauchy transform (FCT) [9], which", "startOffset": 55, "endOffset": 58}, {"referenceID": 8, "context": "Theorem 3 ([9]) Let A \u2208 Rm\u00d7n be a matrix of rank r (r n).", "startOffset": 11, "endOffset": 14}, {"referenceID": 25, "context": "Definition 4 ([26]) Let A \u2208 Rm\u00d7n be a matrix of rank r.", "startOffset": 14, "endOffset": 18}, {"referenceID": 25, "context": "Definition 5 ([26]) Given a well-conditioned basis B for the range of A \u2208 Rm\u00d7n, we define the `1 leverage scores of A as the m-dimensional vector \u03bb, with elements (\u03bb)i = \u2016(B):i\u20161.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "The leverage scores of A can be found with the following procedure [9, 26]: 1.", "startOffset": 67, "endOffset": 74}, {"referenceID": 25, "context": "The leverage scores of A can be found with the following procedure [9, 26]: 1.", "startOffset": 67, "endOffset": 74}, {"referenceID": 8, "context": "using Definition 5, compute the leverage scores \u03bb of B = AR\u2020 (R\u2020 denotes the pseudoinverse of R); The leverage scores are used in [9, 26] to speed up the algorithmic solution of x\u2217 = argminx \u2016Ax\u2212 b\u20161.", "startOffset": 130, "endOffset": 137}, {"referenceID": 25, "context": "using Definition 5, compute the leverage scores \u03bb of B = AR\u2020 (R\u2020 denotes the pseudoinverse of R); The leverage scores are used in [9, 26] to speed up the algorithmic solution of x\u2217 = argminx \u2016Ax\u2212 b\u20161.", "startOffset": 130, "endOffset": 137}, {"referenceID": 14, "context": "with the iterative re-weighting scheme in [15].", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "We also found in our experimental results that instead of using random sampling for building the row and column compression matrices, as in [9], good results were obtained by simply selecting the r largest leverage scores.", "startOffset": 140, "endOffset": 143}, {"referenceID": 15, "context": "In the general case where models overlap, we also use, as an additional measure, the generalized normalized mutual information (GNMI) [16], which extends the normalized mutual information (also called symmetric uncertainty) [33, p.", "startOffset": 134, "endOffset": 138}, {"referenceID": 30, "context": "We start our experimental evaluation with a few small synthetic datasets [31] where 2D points are arranged forming lines and circles.", "startOffset": 73, "endOffset": 77}, {"referenceID": 30, "context": "Figure 4: ARSE results for 2D circle detection on a synthetic dataset [31].", "startOffset": 70, "endOffset": 74}, {"referenceID": 30, "context": "This is an intrinsic limitation of previous state-of-the-art competitors such as Jlinkage [31], T-linkage [20], RPA [21] and most multiple model estimation techniques, which are generally based on partitioning (clustering) the dataset.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "This is an intrinsic limitation of previous state-of-the-art competitors such as Jlinkage [31], T-linkage [20], RPA [21] and most multiple model estimation techniques, which are generally based on partitioning (clustering) the dataset.", "startOffset": 106, "endOffset": 110}, {"referenceID": 20, "context": "This is an intrinsic limitation of previous state-of-the-art competitors such as Jlinkage [31], T-linkage [20], RPA [21] and most multiple model estimation techniques, which are generally based on partitioning (clustering) the dataset.", "startOffset": 116, "endOffset": 120}, {"referenceID": 30, "context": "The datasets (figures 3 and 4) were created [31] by sampling points from different parametric models, adding a certain amount of noise to these points, and then further sampling outliers from a uniform distribution.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Table 2: Comparative performance of RSE and ARSE to detect vanishing points on the York Urban database [11].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": ", [14]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "To evaluate the proposed framework for the task of detecting vanishing points, we use the York Urban database [11], which comes with ground truth assignments of image segments to three orthogonal directions in 3D.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "Any modern method to find those matches would work relatively well for our purposes; we use BRISK [17].", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "Figure 5: Examples of vanishing points detection on the York Urban database [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "To show this, we have included experiments using the state-ofthe-art MultiGS algorithm [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "It is worth pointing out that a recent technique [5] has been able to reduce the size of the minimal sample set for fundamental matrices to b = 2; its use would significantly reduce the computational complexity of the sampling step, imposing an upper bound of O(m2) to the total number of possible combinations.", "startOffset": 49, "endOffset": 52}, {"referenceID": 33, "context": "We estimate multiple fundamental matrices (moving camera and moving objects) and multiple homographies (moving planar objects) on the images in the AdelaideRMF dataset [34].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "We include comparisons with T-linkage [20], RCMSA [23], and RPA [21].", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "We include comparisons with T-linkage [20], RCMSA [23], and RPA [21].", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "We include comparisons with T-linkage [20], RCMSA [23], and RPA [21].", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "Figure 6: Fundamental matrix estimation on two stereo pairs of the Middlebury Stereo dataset [25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "From top to bottom: original pair, matches obtained from BRISK keypoints and descriptors [17] (2518 on the left, 8999 on the right), ARSE inliers (1363/2518 \u2248 54% on the left, 5537/8999 \u2248 61% on the right), and ARSE outliers (the RSE inlier sets are very similar).", "startOffset": 89, "endOffset": 93}, {"referenceID": 33, "context": "Figure 7: The ground truth of the standard AdelaideRMF dataset [34] has non-negligible errors (detectable by simple ocular inspection).", "startOffset": 63, "endOffset": 67}, {"referenceID": 33, "context": "Figure 8: Results on the AdelaideRMF dataset [34] for the estimation of fundamental matrices and homographies.", "startOffset": 45, "endOffset": 49}], "year": 2016, "abstractText": "In this work we introduce a comprehensive algorithmic pipeline for multiple parametric model estimation. The proposed approach analyzes the information produced by a random sampling algorithm (e.g., RANSAC) from a machine learning/optimization perspective, using a parameterless biclustering algorithm based on L1 nonnegative matrix factorization (L1-NMF). The proposed framework exploits consistent patterns that naturally arise during the RANSAC execution, while explicitly avoiding spurious inconsistencies. Contrarily to the main trends in the literature, the proposed technique does not impose non-intersecting parametric models. A new accelerated algorithm to compute L1-NMFs allows to handle medium-sized problems faster while also extending the usability of the algorithm to much larger datasets. This accelerated algorithm has applications in any other context where an L1-NMF is needed, beyond the biclustering approach to parameter estimation here addressed. We accompany the algorithmic presentation with theoretical foundations and numerous and diverse examples.", "creator": "LaTeX with hyperref package"}}}