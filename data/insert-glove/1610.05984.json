{"id": "1610.05984", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "Particle Swarm Optimization for Generating Interpretable Fuzzy Reinforcement Learning Policies", "abstract": "Fuzzy shahawar controllers are corniel known m\u00fchlen to vucitrn serve megaliths as hanns efficient usa-1 and dreger interpretable irvan system controllers 4.40 for kondotty continuous state and unionidae action 30.99 spaces. To 1,524 date these electrophysiology controllers josh have been constructed by hand, or nondefense automatically sharif trained either rebroadcasters on gothenburg expert timbuktu generated ga.-based problem 6.80 specific cost kadhimiyah functions or by incorporating guffaw detailed europlace knowledge about the optimal control lecointe strategy. dutchess Both requirements featurette for hobbyists automatic gajdo\u0161ov\u00e1 training elbow-high processes 80-degree are not given 41.30 in the baucus majority 18.10 of real tapestry world mcilravy reinforcement learning (blacklick RL) raus problems. aleiter We introduce shiatsu a weiqiang new yarmila particle diorhabda swarm astrological reinforcement westheimer learning (klyuyev PSRL) scoldings approach which almirante is beakman capable two-leg of sanit constructing fuzzy ruggles-brise RL aall policies marula solely archabbey by kalfin training kasereka parameters on garraf world models govigama produced \u00e1guilas from backstick randomly generated samples 662-8738 of the real system. daftari This 1-of-11 approach relates self - heraclius organizing fuzzy controllers to model - dublon based whitetails RL for lilley the first time. PSRL can ngc be 3,624 used straightforward tondena on any unbeaten RL problem, which discomfort is 218.4 demonstrated on misapprehensions three standard RL benchmarks, vidmar mountain inappropriately car, cart pole balancing cruellest and cart pole elementa swing up. Our ambulances experiments yielded scana high performing prenatal and well interpretable fuzzy vasilis policies.", "histories": [["v1", "Wed, 19 Oct 2016 12:41:52 GMT  (612kb,D)", "http://arxiv.org/abs/1610.05984v1", null], ["v2", "Fri, 7 Apr 2017 07:22:21 GMT  (651kb,D)", "http://arxiv.org/abs/1610.05984v2", null], ["v3", "Fri, 5 May 2017 09:01:41 GMT  (651kb,D)", "http://arxiv.org/abs/1610.05984v3", null], ["v4", "Thu, 29 Jun 2017 07:13:09 GMT  (650kb,D)", "http://arxiv.org/abs/1610.05984v4", null], ["v5", "Tue, 15 Aug 2017 21:41:03 GMT  (687kb,D)", "http://arxiv.org/abs/1610.05984v5", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG cs.SY", "authors": ["daniel hein", "alexander hentschel", "thomas runkler", "steffen udluft"], "accepted": false, "id": "1610.05984"}, "pdf": {"name": "1610.05984.pdf", "metadata": {"source": "CRF", "title": "Particle Swarm Optimization for Generating Fuzzy Reinforcement Learning Policies", "authors": ["Daniel Hein", "Alexander Hentschel", "Thomas Runkler", "Steffen Udluft"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this paper we focus on reinforcement learning (RL) [30] problems in continuous state spaces. Self-organizing fuzzy controllers are known to serve as efficient and interpretable [4] system controllers in control theory since decades [35, 7, 33]. On the other side, the search for interpretable RL policies is of high academic and industrial interest [25]. In the past years, new optimization algorithms, like particle swarm optimization (PSO) [18, 2], brought self-organizing fuzzy controllers back into the focus of researchers and might extend the scope of usage [14].\nIn 1993 Jang introduced ANFIS, a fuzzy inference system implemented in the framework of adaptive networks [17]. This approach has been applied\nar X\niv :1\n61 0.\n05 98\n4v 1\n[ cs\n.N E\n] 1\n9 O\nfor developing fuzzy controllers multiple times. For instance, the successful application of ANFIS on the cart pole (CP) balancing has been published in [1], [36], and [20]. During the ANFIS training process it is essential to have training data that represents the desired controller behavior, which makes it a supervised machine learning approach. In most industry applications such optimal controller trajectories are unknown.\nFeng applied particle swarm optimization (PSO) to generate fuzzy systems for balancing the cart pole system and approximating a nonlinear function [14, 15]. Debnath et al. optimized parameters of Gaussian membership functions on nonlinear problems and showed that the parameter tuning with PSO is much easier than with conventional methods, since there is no need for derivative knowledge nor complex mathematical equations [31]. Kothandaraman et al. applied PSO to tune adaptive neuro fuzzy controllers for a vehicle suspension system [21]. But as with ANFIS, the PSO fitness functions in all of these contributions have either been dedicated expert formulas or mean-square error functions depending on correctly classified samples.\nIn classic control theory, stability is a central property of a closed-loop controller. Lyapunov stability theory, for instance, analyzes the stability of a solution near a point of equilibrium. It is widely used for designing fuzzy controllers for non-linear systems [24]. Moreover, fault detection and robustness is also of high interest for fuzzy systems [37, 38, 39]. In contrast, RL is concerned with the optimization of a policy for a system that can be modeled as a Markov decision process (MDP). A policy is a mapping from system states to actions on the system. By repeatedly applying an RL policy to the system, it generates a trajectory in the state-action space. The goal in RL is to find a policy that maximizes the trajectorys expected return, without explicit consideration of stability. The mathematical formalism of RL is introduced in Section 2.\nTo the best of our knowledge self-organizing fuzzy rules have never been combined with a model-based RL approach. Generating a world model from data of the real system beforehand, and training the fuzzy policy offline using this model has several advantages: (1) in many real world scenarios data describing the system dynamics are already available or easy to collect; (2) we dont rely on evaluating policies on the real system, thereby avoiding detrimental effects from executing a bad policy; (3) dedicated expert generated cost functions are not required.\nIn our particle swarm reinforcement learning (PSRL) approach different fuzzy policy parameterizations are evaluated by testing the policy on a world model generating action sequences of fixed length. The combined return\nvalue of an amount of action sequences is the fitness value that is iteratively maximized by the optimizer.\nIn Sections 2 to 4 the methods employed in our framework are reviewed. Specifically, the problem of finding policies via RL is formalized as an optimization task. We review Gaussian shaped membership functions and describe the way of our parameterization. Finally PSO, an optimization heuristic to search for optimal policy parameters, and its different extensions are presented.\nAn overview of how our PSRL approach is derived from the different methods is given in Section 5.\nIn Section 6, we describe two benchmark problems on which we conducted several experiments. The first benchmark is the well known mountain car problem and the second benchmark is the cart pole balancing task. The setup process of the world models is explained and the applied fuzzy plicies are introduced.\nThe results of the experiments are discussed in Section 7. It is shown that the proposed PSRL approach is both, able to solve the benchmark problems and is human readable and understandable at the same time. For benchmarking PSRL, we compare our results to the established RL technique neural fitted Q iteration (NFQ) [28, 29]. However, as performance strongly depends on the training data amount and exploration process of generating it, comparing numerical performance results are prone to misinterpretation. To ensure fair comparison, we re-implemented NFQ for both benchmarks and learned an NFQ policy using the identical data that were used for PSRL. Our benchmarks show that PSRL outperforms NFQ. Furthermore, for the cart pole benchmark, PSRL learned polices that were capable of swinging-up the pole and balancing it in upright position, while NFQ failed at swinging up the pole."}, {"heading": "2 Model-Based Reinforcement Learning", "text": "In biological learning, an animal interacts with its environment and tries to find action strategies to maximize its perceived accumulated reward. This notion is formalized in reinforcement learning (RL), an area of machine learning. RL is called unsupervised, because the acting entity, the agent, is not told which actions to take. Instead, the agent must learn the best action strategy from the observed environment responses to its actions. For the most common and also most challenging RL problems, an action does not only affect the next reward, but also the subsequent rewards [30]. Examples\ninclude the non-linear change in position when a force is applied to a body with mass or the delayed heating in a combustion engine.\nIn the RL formalism, the agent interacts with the target system in discrete time steps t = 0, 1, 2, . . .. At each time step, the agent observes the system\u2019s state st \u2208 S and applies an action at \u2208 A, for S the state space and A action space. Depending on st and at, the system transitions into a new state and the agent receives a real-valued reward rt+1 \u2208 R. Here, we focus on deterministic systems, where state transition g and reward r can be expressed as a functions g : S \u00d7 A \u2192 S with g(st,at) = st+1 and r : S \u00d7 A \u00d7 S \u2192 R with r(st,at, st+1) = rt+1, respectively. The desired solution to an RL problem is an action strategy, called policy, that maximizes the expected cumulative reward, called return R.\nIn our present setup, the goal is to find the best policy among a set of policies that is spanned by a parameter vector x \u2208 X . The policy corresponding to one particular setting of parameter values x is denoted as \u03c0[x]. For state st, the policy outputs action \u03c0[x](st) = at. The policy\u2019s performance when starting from st is measured by the return R: the accumulated future rewards obtained when following the policy. To incorporate increasing uncertainties when accumulating future rewards, the reward rt+k for k time steps into the future is weighted by \u03b3k, for \u03b3 \u2208 [0, 1] the discount factor. Furthermore, we follow the common approach to include only a finite number of T \u2265 1 future rewards into the return [30]\nR(st, \u03c0[x]) = T\u22121\u2211 k=0 \u03b3kr(st+k, \u03c0[x](st+k), st+k+1),\nwith st+k+1 = g(st+k,at+k).\n(1)\nThe overall, state-independent policy performance F(x) is obtained by averaging over all starting states st \u2208 S with their respective probabilities wst as weight factors. Thus, optimal solutions to the RL problem are \u03c0[x] with\nx\u0302 \u2208 arg max x\u2208X F(x); F(x) = \u2211 st\u2208S wstR(st, \u03c0[x]). (2)\nIn optimization terminology, the policy performance function F(x) is referred to as fitness function.\nFor many real world problems the cost of executing a potentially bad policy is too high. For example, pilots learn using a flight simulator. Similarly, in model-based RL [3], the real world state transition function g is approximated with a model g\u0303. The model g\u0303 can be a physical model or created from previously gathered data. By substituting g\u0303 for the real world\nstate transition function g in (1), we obtain a model-based approximation F\u0303(x) of the true fitness function (2). Here, we employ models based on neural networks. However, our method extends to other models as well, e.g. physical models or Gaussian process models [27].\nTo solve RL problems, many techniques for policy learning have been developed in the past, like dynamic programming (DP) [30], neural fitted Q iteration (NFQ) [28, 29] or policy gradient neural rewards regression (PGNRR) [32]. More detailed information about reinforcement learning in general and various policy learning techniques in particular can be found in the book by Sutton and Barto [30]."}, {"heading": "3 Fuzzy Rules", "text": "Fuzzy set theory has been introduced in 1965 by Zadeh [23]. Based on fuzzy set theory Mamdani and Assilian [6] introduced a so-called fuzzy controller specified by a set of linguistic if-then rules, whose membership functions can fire independently from each other and produce a combined output computed by a siutable defuzzification function.\nIn a D-inputs-single-output system with C rules, a fuzzy rule R(i) can be described by:\nR(i) : IF s is m(i) THEN o(i), with i = 1, . . . , C, (3)\nwhere s \u2208 RD denotes the input vector (in our setting the environment state), m(i) is the membership of a fuzzy set of the input vector in the premise part, and o(i) is a real number in the consequent part.\nIn this paper we apply Gaussian membership functions [22]. We define the membership function of each rule by\nm(i)(s) = m[c(i), \u03c3(i)](s) = D\u220f j=1 exp \u2212(c (i) j \u2212 sj)2\n2\u03c3 (i) j\n2  (4) where m(i) is the i-th parameterized Gaussian m(c, \u03c3) with its center at c(i) and width \u03c3(i). The parameter vector x \u2208 X , with X the set of valid Gaussian fuzzy parameterizations, is of size d = (2D + 1) \u00b7 C and contains\nx = (c (1) 1 , c (1) 2 , . . . , c (1) D , \u03c3 (1) 1 , \u03c3 (1) 2 , . . . , \u03c3 (1) D , o (1),\nc (2) 1 , c (2) 2 , . . . , c (2) D , \u03c3 (2) 1 , \u03c3 (2) 2 , . . . , \u03c3 (2) D , o (2), . . . , c (C) 1 , c (C) 2 , . . . , c (C) D , \u03c3 (C) 1 , \u03c3 (C) 2 , . . . , \u03c3 (C) D , o (C)).\n(5)\nThe output is determined by\n\u03c0[x](s) =\n\u2211C i=1m\n(i)(s) \u00b7 o(i)\u2211C i=1m (i)(s) . (6)\nThe shape of an example rule defined by this Gaussian membership function is depicted in Fig. 1."}, {"heading": "4 Particle Swarm Optimization", "text": "The particle swarm optimization (PSO) algorithm is a population based, non-convex, stochastic optimization heuristic. Generally, PSO can operate on any search space that is a bounded sub-space of a finite-dimensional vector space [19].\nThe position of each particle of the PSO swarm represents a potential solution of the problem to solve. The particles are iteratively flying through the multidimensional search space, called the fitness landscape. After every movement, each particle receives a fitness value for its new position, which is used to update its own velocity vector and the velocity vectors of all particles in a certain neighborhood.\nAt each iteration, particle i remembers its local best position yi that it has visited so far (including its current position). Furthermore, particle i\nalso knows the neighborhood best position\ny\u0302i(p+ 1) \u2208 arg max z\u2208{yj(p)|\u2208Ni} F(z), (7)\nfound so far by any one particle in its neighborhood Ni (including itself). The neighborhood relations between particles are detemined by the swarm\u2019s population topology and are generally fixed, irrespective of the particles\u2019 positions. In the experiments presented in Section 6 the ring topology [5] has been used.\nLet xi(p) denote the position of particle i at iteration p. The change in position for each iteration is done by adding the velocity vector vi(p) to the particles position vector\nxi(p+ 1) = xi(p) + vi(p+ 1), (8)\nwith xi(0) \u223c U(xmin,xmax) uniformly distributed. The velocity vector contains both, a cognitive component and a social component. It is calculated as\nvij(p+ 1) =wvij(p) + c1r1j(p)[yij(p)\u2212 xij(p)]\ufe38 \ufe37\ufe37 \ufe38 cognitive component\n+ c2r2j(p)[y\u0302ij(p)\u2212 xij(p)]\ufe38 \ufe37\ufe37 \ufe38 social component\n, (9)\nwhere w is the inertia weight factor, vij(p) and xij(p) are the velocity and the position of particle i in dimension j, c1 and c2 are positive acceleration constants used to scale the contribution of the cognitive and the social components yij(p) and y\u0302ij(p) respectively. The factors r1j(p), r2j(p) \u223c U(0, 1) are random values sampled from a uniform distribution to introduce a stochastic element to the algorithm.\nThe best position of a particle for a maximization problem at iteration p is calculated as\nyi(p) = { xi(p), if F(xi(p)) > F(yi(p\u2212 1)) yi(p\u2212 1), else,\n(10)\nwhere in our framework F is the fitness function given in Eq. (2) and the particle positions represent the policy\u2019s parameters x.\nThe complete PSO algorithm as applied for the experiments in Section 6 is given in pseudo code in Appendix B."}, {"heading": "5 Particle Swarm Reinforcement Learning", "text": "The basis for our particle swarm reinforcement learning (PSRL) approach is a data set D containing state transition samples gathered from the real system. These samples are represented by tuples (s,a, s\u2032), where s\u2032 is the state following state s performing action a. The data can be generated by using any (even a random) policy.\nIn the second step we generate world models g\u0303 with inputs (s,a) predicting s\u2032, from data set D. It might be convenient to learn the deltas of each state variable and to train one model per state variable, to yield a better approximative quality:\n\u2206s\u20321 = g\u0303s1(s1, s2, . . . , sm,a) \u2206s\u20322 = g\u0303s2(s1, s2, . . . , sm,a)\n. . .\n\u2206s\u2032m = g\u0303sm(s1, s2, . . . , sm,a).\nThe resulting state is than calculated by s\u2032 = (s1 + \u2206s \u2032 1, s2 + \u2206s \u2032 2, . . . , sm + \u2206s\u2032m). For the next PSRL step an assumption about the rule amount per policy is necessary. In our experiments, we started for each benchmark with a minimal rule set and calculated the respective performance. After the experiments we increased the amount of rules and compared the resulting performance to the policies with fewer rules. This process is repeated until a satisfying performance is reached.\nDuring the optimization each particle\u2019s position x of the PSO represents a parameterization of the fuzzy policy \u03c0[x]. The fitness F\u0303 of a particle is calculated by generating trajectories on the world model g\u0303 starting from a fixed set of initial benchmark states (see Section 2).\nWe assume that the reward function r(s,a, s\u2032) is explicitly given by domain experts, since in real world industry applications this is most commonly the case. Nonetheless, the reward can also be incorporated as a part of data set D in tuple (s,a, s\u2032, r). In this case, the reward function is approximated by r\u0303(s,a, s\u2032) = r."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Mountain Car", "text": "In the mountain car (MC) benchmark an underpowered car has to be driven up to the top of a hill (see Fig. 2). This has to be done by building up momentum by first driving to the opposite direction to gain enough potential energy.\nIn our implementation the hill landscape is computed as sin(3\u03c1). The task for the RL agent is to find a sequence of force actions at, at+1, at+2, . . . \u2208 [\u22121, 1], that drive the car up to the hill, which is achieved when reaching a position \u03c1 \u2265 \u03c06 .\nAt the start of each episode the car\u2019s position is initialized in the interval of [\u2212\u03c0/3;\u03c0/6]. The agent perceives a reward of\nr(s\u2032) = { 0, if \u03c1\u2032 \u2265 \u03c0/6, \u22121, otherwise,\n(11)\nsubsequent to every action-state update. When the car reaches the goal position, the car\u2019s position is fixed, and the agent perceives the maximum reward in every following time step, regardless of the applied actions."}, {"heading": "6.2 Cart Pole Balancing", "text": "The cart pole (CP) experiments described in the following sections have been conducted using a software system called CLS2 (\u2019clsquare\u2019)1 . This software is a freely available RL benchmark system applying Runge-Kutta fourth-order method to approximate the CP dynamics.\nThe objective of the CP balancing benchmark is to apply forces to a cart which is moving on a one-dimensional track to keep a pole hinged to the cart in an upright position (see Fig. 3). The four Markov state variables are the pole angle \u03b8, the pole angular velocity \u03b8\u0307, the cart position \u03c1, and the cart velocity \u03c1\u0307. These four variables describe the Markov state completely; no additional information about the system\u2019s past behavior is necessary. The task for the RL agent is to find a sequence of force actions at, at+1, at+2, . . . \u2208 (\u22121,+1) that prevent the pole from falling over [9].\nIn the CP balancing (CPB) task the angle of the pole and the cart\u2019s position are restricted to the intervals of [\u22120.7; 0.7] and [\u22122.4; 2.4] respectively. Once the cart has left the restricted area the episode is referred to as failed and the system remains in the fail state for the rest of the episode. Both, the angle and the position, are initialized randomly in the interval of\n1Freely available at ml.informatik.uni-freiburg.de/research/clsquare.\n[\u22120.5; 0.5]. The RL policy can apply the force actions of 10 N and \u221210 N in time intervals of 0.025 s on the cart.\nThe reward function for the balancing problem is given by:\nr(s\u2032) =  0.00, if \u03b8\u2032 < 0.05 and \u03b8\u2032 > \u22120.05 and \u03c1\u2032 < 0.05 and \u03c1\u2032 > \u22120.05, \u22121.00, if \u03b8\u2032 > 0.7 or \u03b8\u2032 < \u22120.7 or \u03c1\u2032 > 2.4 or \u03c1\u2032 < 2.4,\n\u22120.01, otherwise.\n(12)\nBased on this reward function, the primary goal for the policy is to avoid reaching the fail state. The secondary goal is to drive the system into the goal state region and keep it there for the rest of the episode.\nSince the CP problem is symmetric around s = (\u03b8, \u03b8\u0307, \u03c1, \u03c1\u0307) = (0, 0, 0, 0), an optimal action at for state (\u03b8, \u03b8\u0307, \u03c1, \u03c1\u0307) corresponds to an optimal action \u2212at for state (\u2212\u03b8,\u2212\u03b8\u0307,\u2212\u03c1,\u2212\u03c1\u0307). For this reason the parameter search process can be simplified. It is only necessary to search for optimal parameters for one half of the rules. The other half of the parameter sets can be constructed by negating the position parameters and the respective action values of the policy\u2019s components. Note that the membership function span width of the fuzzy rules is not negated, since the membership functions need to preserve their shapes."}, {"heading": "6.3 Cart Pole Swing Up", "text": "The CP swing up (CPSU) benchmark is based on the same system dynamics as the CPB benchmark. In contrast to the CPB benchmark neither the position of the cart nor the angle of the pole are restricted to a special region. Consequently, the pole can swing through, which is an important property of CPSU. Since the pole\u2019s angle is initialized in the full interval of [\u2212\u03c0;\u03c0] it is often necessary for the policy to swing the pole several times from one side to the other to gain enough energy to erect the pole and yield the highest reward.\nIn the CPSU setting the policy is able to apply the actions of \u221230 N and +30 N on the cart. The reward function for the problem is given by\nr(s\u2032) =  0, if \u03b8\u2032 < 0.5 and \u03b8\u2032 > \u22120.5 and \u03c1\u2032 < 0.5 and \u03c1\u2032 > \u22120.5, \u22121, otherwise,\n(13)\nwhich is similar to the CPS benchmark, but does not contain any penalty for fail states."}, {"heading": "6.4 Neural network world models", "text": "We conducted the policy trainings on neural network world models yielding approximative fitness functions f\u0303(x) (see Section 2). For our experiments we created one neural network for each of the state variables. The MC neural networks have been trained with a data set DMC containing 100, 000 samples (s, a, g(s, a)) with s = (\u03c1, \u03c1\u0307) sampled from (s, a)MC \u223c [\u2212\u03c0/3;\u03c0/6]\u00d7 [\u22121; 1] \u00d7 {\u22121, 1}. The following two neural networks have been trained to approximate the MC task:\n\u2206\u03c1t+1 = g\u0303\u03c1(\u03c1t, \u03c1\u0307t, at)\n\u2206\u03c1\u0307t+1 = g\u0303\u03c1\u0307(\u03c1t, \u03c1\u0307t, at).\nSimilarly, for the CPS dynamic model we created the following four networks:\n\u2206\u03b8t+1 = g\u0303\u03b8(\u03b8t, \u03b8\u0307t, \u03c1t, \u03c1\u0307t, at)\n\u2206\u03b8\u0307t+1 = g\u0303\u03b8\u0307(\u03b8t, \u03b8\u0307t, \u03c1t, \u03c1\u0307t, at)\n\u2206\u03c1t+1 = g\u0303\u03c1(\u03b8t, \u03b8\u0307t, \u03c1t, \u03c1\u0307t, at)\n\u2206\u03c1\u0307t+1 = g\u0303\u03c1\u0307(\u03b8t, \u03b8\u0307t, \u03c1t, \u03c1\u0307t, at).\nAn approximation of the next state is then given by\nst+1 = (\u03b8t + \u2206\u03b8t+1, \u03b8\u0307t + \u2206\u03b8\u0307t+1, \u03c1t + \u2206\u03c1t+1, \u03c1\u0307t + \u2206\u03c1\u0307t+1). (14)\nSince with the CPSU benchmark the pole can swing through an angular value jump from \u03b8 = \u03c0 to \u03b8 = \u2212\u03c0 and vice versa can occur. This jump is prone to make the modeling task more difficult, so we decided to switch to another angular representation consisting of sin(\u03b8) and cos(\u03b8). We state the arctangent function with two arguments as follows:\n\u03b8 = arctan(sin(\u03b8), cos(\u03b8)). (15)\nThis representation yields the following five networks for the CPSU bench-\nmark:\n\u2206 sin(\u03b8t+1) = g\u0303sin(\u03b8)(sin(\u03b8t), cos(\u03b8t), \u03b8\u0307t, \u03c1t, \u03c1\u0307t, at)\n\u2206 cos(\u03b8t+1) = g\u0303cos(\u03b8)(sin(\u03b8t), cos(\u03b8t), \u03b8\u0307t, \u03c1t, \u03c1\u0307t, at)\n\u2206\u03b8\u0307t+1 = g\u0303\u03b8\u0307(sin(\u03b8t), cos(\u03b8t), \u03b8\u0307t, \u03c1t, \u03c1\u0307t, at)\n\u2206\u03c1t+1 = g\u0303\u03c1(sin(\u03b8t), cos(\u03b8t), \u03b8\u0307t, \u03c1t, \u03c1\u0307t, at)\n\u2206\u03c1\u0307t+1 = g\u0303\u03c1\u0307(sin(\u03b8t), cos(\u03b8t), \u03b8\u0307t, \u03c1t, \u03c1\u0307t, at).\nThe size for the training sets of both CP benchmarks has been 100, 000 samples. The data points have been randomly generated by sampling (s, a)CPS \u223c [\u22120.7; 0.7]\u00d7[\u221210; 10]\u00d7[\u22122.4; 2.4]\u00d7[\u221210; 10]\u00d7{\u221210; 10} for CPS and (s, a)CPSU \u223c [\u2212\u03c0;\u03c0]\u00d7 [\u221210; 10]\u00d7 [\u221210; 10]\u00d7 [\u221210; 10]\u00d7 {\u221230; 30} for CPSU.\nAll networks g\u0303 consist of two hidden layers with 20 hidden neurons each with arctan activation functions. The network training has been conducted by applying the Vario-Eta algorithm [26] and splitting the data sets into 90, 000 training and 10, 000 validation patterns. The training of the networks can be done in parallel and takes only a couple of minutes."}, {"heading": "6.5 Fuzzy Policy", "text": "With our PSRL approach we search for the parameterization x for a fuzzy policy build from C fuzzy rules. The continuous defuzzified output of the policy o = \u03c0[x](s) is mapped to the available actions by calculating\naMC = { \u22121, if o < 0 1, else,\n(16)\naCPS = { \u221210, if o < 0 10, else,\n(17)\naCPSU = { \u221230, if o < 0 30, else,\n(18)\nfor the MC, CPS, and CPSU benchmarks respectively."}, {"heading": "7 Results", "text": ""}, {"heading": "7.1 Mountain Car", "text": "We conducted ten NFQ trainings for the MC benchmark using the setup described in Appendix A. After every NFQ iteration the latest policy has\nbeen tested on the world model to compute an approximation of the real performance F\u0303 . The policy yielding the best fitness value so far has been saved as a intermediate solution. To evaluate the true performance of the NFQ approach we computed the true fitness value F of this intermediate solution applying the mathematical MC dynamics g which has been used to generate the sample data.\nFor the MC benchmark and \u03b3 = 0.99 a policy yielding F > \u221250 can be considered a successful solution to the benchmark problem. The policy is able to drive the car up the hill from any initial state in less than 200 time steps. Fig. 4 shows the individual results for each NFQ run and the average performance of the technique. Observe that every run successfully produced a policy, which is able to move the car to the top of the hill. The average fitness value after 300 NFQ iterations is \u221249.9\u00b1 0.6.\nThe task for our PSRL approach was to find parameters for two fuzzy rules yielding a policy for the MC benchmark. We used 100 particles and the standard PSO settings to conduct this search. The training took place on sample state sets S 3 s \u223c [\u2212\u03c0/3;\u03c0/6] \u00d7 0 of size 1000. Similar to the NFQ algorithm we evaluated the performance of each fuzzy policy on the MC world model.\nThe true performance of each policy is depicted in Fig. 5. Note that in each of the ten experiments the PSO was able to find high performance\npolicy parameterizations in less than 50 iterations. After 300 iterations the average performance is \u221248.99\u00b1 0.14.\nFuzzy policies are easy to visualize and interpret in two ways. The first is to present the set of rules as linguistic terms. One of the fuzzy policies we received as output of our PSRL approach for the MC benchmark can be written as follows:\nR(1) : IF s is m[(\u22120.79, 0.044), (0.69, 0.03)] THEN o(1) = 1.95,\nR(2) : IF s is m[(\u22120.43,\u22120.077), (0.49,\u22120.14)] THEN o(2) = \u22120.73.\nThe policy\u2019s output is then computed by\no =\n\u22114 i=1m\n(i)(s) \u00b7 o(i)\u22114 i=1m (i)(s) . (19)\nNote that the parameters are given in a compact and interpretable way. A second way to visualize fuzzy policies is by plotting the respective membership functions and analyzing the produced output for sample states. A graphical representation for a policy for the MC benchmark is given in Fig. 6. Domain experts are able to evaluate the policy\u2019s outputs for every state in a very convenient and safe way."}, {"heading": "7.2 Cart Pole Balancing", "text": "In the same manner we conducted the experiments for the MC benchmark in the subsection before, we performed ten NFQ runs for the CPB benchmark. policies yielding a performance F > \u22121 can be considered successful. The results presented in Fig. 7 show that it is harder to find such a policy using NFQ. Nevertheless, nine out of ten NFQ runs produced policies yielding F > \u22121. The average performance after 300 iterations and \u03b3 = 0.99 is \u22120.93\u00b1 0.16.\nThe task for PSRL was to find a parameterization for four fuzzy rules. Again, we used an amount of 100 particles and an out of the box PSO setup. The training took place on sample state sets S 3 s \u223c [\u22120.5; 0.5]\u00d70\u00d7 [\u22120.5; 0.5]\u00d7 0 of size 1000. In Fig. 8 the results evaluated on the true CPB dynamics are plotted. In each of the ten PSO runs the swarm converged to high performing fuzzy parameters in less than 50 PSO iterations. After 300 iterations the average performance is \u22120.51\u00b1 0.09.\nA visual representation of one of the resulting fuzzy policies is given in Fig. 9."}, {"heading": "7.3 Cart Pole Swing Up", "text": "While NFQ performed only slightly worse in the MC and CPB benchmarks its performance degreased dramatically for the CPSU problem. For this\nbenchmark solutions with F > \u221250 on a set of benchmark states can be considered successful policies.\nNone of the ten NFQ runs produced a working policies. The average performance with \u03b3 = 0.99 stagnated after 300 iterations at \u221292.9\u00b11.2 (see Fig. 10). These results reflect the general issues occurring with model-free\nRL techniques in high dimensional, continuous state spaces with long time horizons. Reasons for this are manyfold, like the lack of training samples for the problem dimensionality (100, 000 samples for five dimensions in our CPSU experiments) and/or the approximative quality of the neural network in use. Tackling both issues would result in a much higher demand of processing resources (memory and computation time) and has to be considered as infeasible for many real world problems.\nOn the contrary, our PSRL approach is able to find a parameterization for policies with six fuzzy rules, by assessing their performance on world models. Here we used PSO with 1, 000 particles. For the sake of comparability, these world models have been trained with the exact same training samples the NFQ runs have been conducted on. The PSRL training took place on sample state sets S 3 s \u223c [\u2212\u03c0;\u03c0]\u00d7 0\u00d7 [\u22120.5; 0.5]\u00d7 0 of size 1000. The average performance of the resulting fuzzy policies is \u221238.5 \u00b1 6.0 (see Fig. 11).\nThe results for CPSU show, that given a fixed set of data it can be significantly easier to train a model approximating system dynamics and optimize fuzzy policies on that model, compared to conducting a model-free RL approach like NFQ."}, {"heading": "8 Conclusions", "text": "The traditional way of creating self-organizing fuzzy controllers either requires an expert designed fitness function on which the optimizer finds optimal controller parameters, or relies on the existence of detailed knowledge about the optimal controller policy. Both requirements are rather hard to\nsatisfy in industrial real world problems. Data gathered on the system to be controlled using some default policy in contrast, is available in many cases. The PSRL approach introduced in this paper is capable of using such data and producing high performing and interpretable fuzzy policies for RL problems.\nOn three standard RL benchmarks, we have shown that not only PSRL\u2019s performance is equal to that of the state of the art model-free RL approach NFQ, but in addition to it PSRL outperformed NFQ in the CPSU benchmark, which is of higher dimensionality and has a long time horizon.\nThe application of PSRL in industry settings might be of high interest, since in many cases data from systems is already available and interpretable fuzzy policies are favored over black box RL solutions, like Q function based model-free approaches."}, {"heading": "Acknowledgment", "text": "The project this report is based on was supported with funds from the German Federal Ministry of Education and Research under project number 01IB15001. The sole responsibility for the report\u2019s contents lies with the authors.\nThe authors would like to thank Dragan Obradovic and Clemens Otte for insightful discussions and helpful suggestions."}, {"heading": "A Neural Fitted Q Iteration", "text": "Neural fitted Q iteration (NFQ) is a neural network based RL learning approach published by Riedmiller [28, 29] and is a special realization of the \u2019fitted Q iteration\u2019 algorithm proposed by Ernst et al. [8]. NFQ belongs to the family of fitted value iteration algorithms [12].\nNFQ is a model-free RL approach which can either work with batches of previously collected transition samples or learn successively from real world interactions. Transition experiences are collected in quadruples of the form (s,a, s\u2032, r). Using sample l the problem specific Q function Q : S \u00d7A \u2192 R, computing the state-action value of the pair sl and al, is learned iteratively by first computing target values\ntlk+1 = r l + \u03b3max\na\u2032 Q[xk](s\n\u2032l,a\u2032) (20)\nand subsequently using supervised machine learning techniques to minimize\nxk+1 \u2208 arg min x \u2211 l (Q[x](s\u2032l,a\u2032)\u2212 tlk+1)2, (21)\nwhere xk are the optimized network weights in NFQ iteration k for the neural network function Q.\nAs the NFQ algorithm proceeds, the output of the Q function theoretically converges to the state-action value of applying action a in state s and following the optimal policy afterwards. Hence, an optimal policy is given after finishing the algorithm in iteration K by evaluating \u03c0[xK ](s) \u2208 arg maxaQ[xK ](s,a).\nIn practice function approximation in RL is not known to converge to a point [13] and is prone to overestimation of utility values [34]. Since neural networks are not averagers, additional problems are likely to occur. To cover these problems, some improvements on Q iteration algorithms with neural networks have been published in past years. In [11] an RL method that monitors the learning process is presented. Furthermore, Hans et al. [16] and Fau\u00dfer et al. [10] applied ensembles of neural networks to form a committee of multiple agents and showed that this committee benefits from the diversity on the state-action value estimations.\nIn our experiments the NFQ algorithm has been performed on the exact same data sets DMC, DCPS and DCPSU the world models have been trained on. The Q functions have been approximated by neural networks with two hidden layers with 20 neurons each and arctan activation functions.\nAfter each NFQ iteration k the performance of the resulting Q function Q[xk] has been evaluated by testing it on 10, 000 random states with the respective world model. Since the performance with NFQ is expected to degrade [11] after time, the current best policy is saved. At k = 1000 the algorithm is stopped and the best performing policy so far is denoted the final result of the NFQ run."}, {"heading": "B Algorithm", "text": "Data: \u2022 N randomly initialized d-dimensional particle positions with xi = yi (Eq.\n(5)) and velocities vi of particle i, with i = 1, . . . , N\n\u2022 Fitness function F (Eq. (2)) \u2022 Inertia weight factor w and acceleration constants c1 and c2 \u2022 Random number generator rand() \u2022 Search space boundaries xmin and xmax \u2022 Velocity boundaries vmin = \u22120.1 \u00b7 (xmax \u2212 xmin) and\nvmax = 0.1 \u00b7 (xmax \u2212 xmin) \u2022 Swarm topology graph defining neighborhood Ni\nResult:\n\u2022 Global best position y\u0302\nrepeat foreach Particle i do\nNeighborhood best position of particle i (Eq. (7)); y\u0302i \u2190 arg maxz\u2208{yj | j\u2208Ni} F(z);\nend Position updates; foreach Particle i do\nDetermine new velocity of particle i (Eq. (9)); for j = 1, . . . , d do\nvij \u2190 wvij + c1 \u00b7 rand() \u00b7 [yij \u2212 xij ] + c2 \u00b7 rand() \u00b7 [y\u0302ij \u2212 xij ]; end Truncate particle i\u2019s velocity; for j = 1, . . . , d do\nvij \u2190 min(vmaxj ,max(vminj , vij)) end Compute new position of particle i (Eq. (8)); xi \u2190 xi + vi; Truncate particle i\u2019s position; for j = 1, . . . , d do\nxij \u2190 min(xmaxj ,max(xminj , xij)) end Personal best positions (Eq. (10)); if F(xi) > F(yi) then\nSet new personal best position of particle i; yi \u2190 xi;\nend\nend\nuntil Stopping criterion is met ; Determine the global best position; y\u0302\u2190 arg maxz\u2208{y1,...,yN} F(z); return y\u0302\nAlgorithm 1: The PSO algorithm. Particle i is represented by position xi, personal best position yi, and neighborhood best position y\u0302i.\n22"}], "references": [{"title": "Takagi-Sugeno fuzzy controller design via Anfis architecture for inverted pendulum system", "author": ["A.A. Saifizul", "C.A. Azlan", "N.F. Mohd Nasir"], "venue": "Proceedings of International Conference on Man-Machine Systems,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Fundamentals of computational swarm intelligence", "author": ["A.P. Engelbrecht"], "venue": "Wiley,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Reinforcement Learning and Dynamic Programming Using Function Approximation", "author": ["L. Busoniu", "R. Babuska", "B. De Shutter", "D. Ernst"], "venue": "CRC Press,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Interpretability improvements to find the balance interpretability-accuracy in fuzzy modeling: an overview", "author": ["J. Casillas", "O. Cordon", "F. Herrera", "L. Magdalena"], "venue": "Interpretability issues in fuzzy modeling, pages 3\u201322. Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Computational intelligence PC tools", "author": ["R. Eberhart", "P. Simpson", "R. Dobbins"], "venue": "Academic Press Professional, Inc., San Diego, CA, USA,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "An experiment in linguistic synthesis with a fuzzy logic controller", "author": ["E.H. Mamdani", "S. Assilian"], "venue": "International Journal of Man-Machine Studies, 7(1):1\u201313,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1975}, {"title": "The application of a fuzzy controller to the control of a multi-degree-freedom robot arm", "author": ["E.M. Scharf", "N.J. Mandve"], "venue": "M. Sugeno, editor, Industrial Application of Fuzzy Control, pages 41\u201362. North-Holland,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1985}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel", "L. Littman"], "venue": "Journal of Machine Learning Research, 6:503\u2013556,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Non-linear control for underactuated mechanical systems", "author": ["I. Fantoni", "R. Lozano"], "venue": "Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural network ensembles in reinforcement learning", "author": ["S. Fau\u00dfer", "F. Schwenker"], "venue": "Neural Process. Lett., 41(1):55\u201369,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Reducing policy degradation in neurodynamic programming", "author": ["T. Gabel", "M. Riedmiller"], "venue": "ESANN 2006, 14th European Symposium on Artificial Neural Networks, Bruges, Belgium, April 26-28, 2006, Proceedings, pages 653\u2013658,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Stable function approximation in dynamic programming", "author": ["G.J. Gordon"], "venue": "In Machine Learning: Proceedings of the Twelfth International Conference. Morgan Kaufmann,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Reinforcement learning with function approximation converges to a region", "author": ["G.J. Gordon"], "venue": "Advances in Neural Information Processing Systems, pages 1040\u20131046. The MIT Press,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Particle swarm optimization learning fuzzy systems design", "author": ["H.-M. Feng"], "venue": "Third International Conference on Information Technology and Applications, 2005. ICITA 2005, volume 1, pages 363\u2013366. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Self-generation fuzzy modeling systems through hierarchical recursive-based particle swarm optimization", "author": ["H.-M. Feng"], "venue": "Cybernetics and Systems, 36(6):623\u2013639,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Ensembles of neural networks for robust reinforcement learning", "author": ["A. Hans", "S. Udluft"], "venue": "Machine Learning and Applications (ICMLA), 2010 Ninth International Conference on, pages 401\u2013406,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive-network-based fuzzy inference system", "author": ["J.S. Jang"], "venue": "IEEE Transactions on Systems, Man & Cybernetics, 23(3):665\u2013685,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "Particle swarm optimization", "author": ["J. Kennedy", "R.C. Eberhart"], "venue": "Proceedings of the IEEE International Joint Conference on Neural Networks, pages 1942\u20131948,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Particle swarm optimization", "author": ["J. Kennedy", "R.C. Eberhart"], "venue": "Proceedings of the IEEE International Joint Conference on Neural Networks, pages 1942\u20131948,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Stabilization of inverted pendulum using hybrid adaptive neuro fuzzy (anfis) controller", "author": ["A. Kharola", "P. Gupta"], "venue": "Engineering Science Letters, 4:1\u201320,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "PSO tuned adaptive neurofuzzy controller for vehicle suspension systems", "author": ["R. Kothandaraman", "L. Ponnusamy"], "venue": "Journal of Advances in Information Technology, 3(1),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy basis functions, universal approximation, and orthogonal least-squares learning", "author": ["L.-X. Wang", "J.M. Mendel"], "venue": "IEEE Transactions on Neural Networks, 3(5):807\u2013814,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "Fuzzy sets", "author": ["L.A. Zadeh"], "venue": "Information and Control, 8:338\u2013353,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1965}, {"title": "Dynamic output feedback H\u221e control of discretetime fuzzy systems: a fuzzy-basis-dependent lyapunov function approach", "author": ["J. Lam", "S. Zhou"], "venue": "International Journal of Systems Science, 38(1):25\u201337,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Policy search in a space of simple closed-form formulas: towards interpretability of reinforcement learning", "author": ["F. Maes", "R. Fonteneau", "L. Wehenkel", "D. Ernst"], "venue": "Discovery Science, pages 37\u201350,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "How to train neural networks", "author": ["R. Neuneier", "H.-G. Zimmermann"], "venue": "G. Montavon, G. Orr, and K.-R. M\u00fcller, editors, Neural Networks: Tricks of the Trade, Second Edition, pages 369\u2013418. Springer,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Gaussian processes for machine learning (adaptive computation and machine learning)", "author": ["E. Rasmussen", "C.K.I. Williams"], "venue": "MIT Press Ltd,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural fitted Q iteration \u2014 first experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "Machine Learning: ECML 2005, volume 3720, pages 317\u2013328. Springer,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural reinforcement learning to swing-up and balance a real pole", "author": ["M. Riedmiller"], "venue": "Systems, Man and Cybernetics, 2005 IEEE International Conference on, volume 4, pages 3191\u20133196,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "A Bradford book,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Particle swarm optimization based adaptive strategy for tuning of fuzzy logic controller", "author": ["S.B.C. Debnath", "P.C. Shill", "K. Murase"], "venue": "International Journal of Artificial Intelligence & Applications, 4(1):37\u201350,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving optimality of neural rewards regression for data-efficient batch near-optimal policy identification", "author": ["D. Schneega\u00df", "S. Udluft", "T. Martinetz"], "venue": "Proceedings of the International Conference on Artificial Neural Networks,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Fuzzy self-organizing controller and its application for dynamic processes", "author": ["S. Shao"], "venue": "Fuzzy Sets and Systems, 26:151\u2013164,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1988}, {"title": "Issues in using function approximation for reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "In Proceedings of the Fourth Connectionist Models Summer School. Erlbaum,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1993}, {"title": "A linguistic self-organizing process controller", "author": ["T.J. Procyk", "E.H. Mamdani"], "venue": "Automatica, 15:15\u201330,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1979}, {"title": "Design and validation of real time neuro fuzzy controller for stabilization of pendulum-cart system", "author": ["T.O.S Hanafy"], "venue": "Life Science Journal, 8(1):52\u201360,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Fault detection for uncertain fuzzy systems based on the delta operator approach", "author": ["H. Yang", "X. Li", "Z. Liu", "C. Hua"], "venue": "Circuits, Systems, and Signal Processing, 33(3):733\u2013759,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust fuzzy-scheduling control for nonlinear systems subject to actuator saturation via delta operator approach", "author": ["H. Yang", "X. Li", "Z. Liu", "L. Zhao"], "venue": "Information Sciences, 272:158 \u2013 172,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Fault-tolerant control for a class of T-S fuzzy systems via delta operator approach", "author": ["H. Yang", "P. Shi", "X. Li", "Z. Li"], "venue": "Signal Process., 98:166\u2013173,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "In this paper we focus on reinforcement learning (RL) [30] problems in continuous state spaces.", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "Self-organizing fuzzy controllers are known to serve as efficient and interpretable [4] system controllers in control theory since decades [35, 7, 33].", "startOffset": 84, "endOffset": 87}, {"referenceID": 34, "context": "Self-organizing fuzzy controllers are known to serve as efficient and interpretable [4] system controllers in control theory since decades [35, 7, 33].", "startOffset": 139, "endOffset": 150}, {"referenceID": 6, "context": "Self-organizing fuzzy controllers are known to serve as efficient and interpretable [4] system controllers in control theory since decades [35, 7, 33].", "startOffset": 139, "endOffset": 150}, {"referenceID": 32, "context": "Self-organizing fuzzy controllers are known to serve as efficient and interpretable [4] system controllers in control theory since decades [35, 7, 33].", "startOffset": 139, "endOffset": 150}, {"referenceID": 24, "context": "On the other side, the search for interpretable RL policies is of high academic and industrial interest [25].", "startOffset": 104, "endOffset": 108}, {"referenceID": 17, "context": "In the past years, new optimization algorithms, like particle swarm optimization (PSO) [18, 2], brought self-organizing fuzzy controllers back into the focus of researchers and might extend the scope of usage [14].", "startOffset": 87, "endOffset": 94}, {"referenceID": 1, "context": "In the past years, new optimization algorithms, like particle swarm optimization (PSO) [18, 2], brought self-organizing fuzzy controllers back into the focus of researchers and might extend the scope of usage [14].", "startOffset": 87, "endOffset": 94}, {"referenceID": 13, "context": "In the past years, new optimization algorithms, like particle swarm optimization (PSO) [18, 2], brought self-organizing fuzzy controllers back into the focus of researchers and might extend the scope of usage [14].", "startOffset": 209, "endOffset": 213}, {"referenceID": 16, "context": "In 1993 Jang introduced ANFIS, a fuzzy inference system implemented in the framework of adaptive networks [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "For instance, the successful application of ANFIS on the cart pole (CP) balancing has been published in [1], [36], and [20].", "startOffset": 104, "endOffset": 107}, {"referenceID": 35, "context": "For instance, the successful application of ANFIS on the cart pole (CP) balancing has been published in [1], [36], and [20].", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "For instance, the successful application of ANFIS on the cart pole (CP) balancing has been published in [1], [36], and [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "Feng applied particle swarm optimization (PSO) to generate fuzzy systems for balancing the cart pole system and approximating a nonlinear function [14, 15].", "startOffset": 147, "endOffset": 155}, {"referenceID": 14, "context": "Feng applied particle swarm optimization (PSO) to generate fuzzy systems for balancing the cart pole system and approximating a nonlinear function [14, 15].", "startOffset": 147, "endOffset": 155}, {"referenceID": 30, "context": "optimized parameters of Gaussian membership functions on nonlinear problems and showed that the parameter tuning with PSO is much easier than with conventional methods, since there is no need for derivative knowledge nor complex mathematical equations [31].", "startOffset": 252, "endOffset": 256}, {"referenceID": 20, "context": "applied PSO to tune adaptive neuro fuzzy controllers for a vehicle suspension system [21].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "It is widely used for designing fuzzy controllers for non-linear systems [24].", "startOffset": 73, "endOffset": 77}, {"referenceID": 36, "context": "Moreover, fault detection and robustness is also of high interest for fuzzy systems [37, 38, 39].", "startOffset": 84, "endOffset": 96}, {"referenceID": 37, "context": "Moreover, fault detection and robustness is also of high interest for fuzzy systems [37, 38, 39].", "startOffset": 84, "endOffset": 96}, {"referenceID": 38, "context": "Moreover, fault detection and robustness is also of high interest for fuzzy systems [37, 38, 39].", "startOffset": 84, "endOffset": 96}, {"referenceID": 27, "context": "For benchmarking PSRL, we compare our results to the established RL technique neural fitted Q iteration (NFQ) [28, 29].", "startOffset": 110, "endOffset": 118}, {"referenceID": 28, "context": "For benchmarking PSRL, we compare our results to the established RL technique neural fitted Q iteration (NFQ) [28, 29].", "startOffset": 110, "endOffset": 118}, {"referenceID": 29, "context": "For the most common and also most challenging RL problems, an action does not only affect the next reward, but also the subsequent rewards [30].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "To incorporate increasing uncertainties when accumulating future rewards, the reward rt+k for k time steps into the future is weighted by \u03b3k, for \u03b3 \u2208 [0, 1] the discount factor.", "startOffset": 150, "endOffset": 156}, {"referenceID": 29, "context": "Furthermore, we follow the common approach to include only a finite number of T \u2265 1 future rewards into the return [30]", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "Similarly, in model-based RL [3], the real world state transition function g is approximated with a model g\u0303.", "startOffset": 29, "endOffset": 32}, {"referenceID": 26, "context": "physical models or Gaussian process models [27].", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "To solve RL problems, many techniques for policy learning have been developed in the past, like dynamic programming (DP) [30], neural fitted Q iteration (NFQ) [28, 29] or policy gradient neural rewards regression (PGNRR) [32].", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "To solve RL problems, many techniques for policy learning have been developed in the past, like dynamic programming (DP) [30], neural fitted Q iteration (NFQ) [28, 29] or policy gradient neural rewards regression (PGNRR) [32].", "startOffset": 159, "endOffset": 167}, {"referenceID": 28, "context": "To solve RL problems, many techniques for policy learning have been developed in the past, like dynamic programming (DP) [30], neural fitted Q iteration (NFQ) [28, 29] or policy gradient neural rewards regression (PGNRR) [32].", "startOffset": 159, "endOffset": 167}, {"referenceID": 31, "context": "To solve RL problems, many techniques for policy learning have been developed in the past, like dynamic programming (DP) [30], neural fitted Q iteration (NFQ) [28, 29] or policy gradient neural rewards regression (PGNRR) [32].", "startOffset": 221, "endOffset": 225}, {"referenceID": 29, "context": "More detailed information about reinforcement learning in general and various policy learning techniques in particular can be found in the book by Sutton and Barto [30].", "startOffset": 164, "endOffset": 168}, {"referenceID": 22, "context": "Fuzzy set theory has been introduced in 1965 by Zadeh [23].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "Based on fuzzy set theory Mamdani and Assilian [6] introduced a so-called fuzzy controller specified by a set of linguistic if-then rules, whose membership functions can fire independently from each other and produce a combined output computed by a siutable defuzzification function.", "startOffset": 47, "endOffset": 50}, {"referenceID": 21, "context": "In this paper we apply Gaussian membership functions [22].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "Generally, PSO can operate on any search space that is a bounded sub-space of a finite-dimensional vector space [19].", "startOffset": 112, "endOffset": 116}, {"referenceID": 4, "context": "In the experiments presented in Section 6 the ring topology [5] has been used.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "\u2208 (\u22121,+1) that prevent the pole from falling over [9].", "startOffset": 50, "endOffset": 53}, {"referenceID": 25, "context": "The network training has been conducted by applying the Vario-Eta algorithm [26] and splitting the data sets into 90, 000 training and 10, 000 validation patterns.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "Neural fitted Q iteration (NFQ) is a neural network based RL learning approach published by Riedmiller [28, 29] and is a special realization of the \u2019fitted Q iteration\u2019 algorithm proposed by Ernst et al.", "startOffset": 103, "endOffset": 111}, {"referenceID": 28, "context": "Neural fitted Q iteration (NFQ) is a neural network based RL learning approach published by Riedmiller [28, 29] and is a special realization of the \u2019fitted Q iteration\u2019 algorithm proposed by Ernst et al.", "startOffset": 103, "endOffset": 111}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "NFQ belongs to the family of fitted value iteration algorithms [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "In practice function approximation in RL is not known to converge to a point [13] and is prone to overestimation of utility values [34].", "startOffset": 77, "endOffset": 81}, {"referenceID": 33, "context": "In practice function approximation in RL is not known to converge to a point [13] and is prone to overestimation of utility values [34].", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "In [11] an RL method that monitors the learning process is presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "[16] and Fau\u00dfer et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] applied ensembles of neural networks to form a committee of multiple agents and showed that this committee benefits from the diversity on the state-action value estimations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Since the performance with NFQ is expected to degrade [11] after time, the current best policy is saved.", "startOffset": 54, "endOffset": 58}], "year": 2017, "abstractText": "Fuzzy controllers are known to serve as efficient and interpretable system controllers for continuous state and action spaces. To date these controllers have been constructed by hand, or automatically trained either on expert generated problem specific cost functions or by incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not given in the majority of real world reinforcement learning (RL) problems. We introduce a new particle swarm reinforcement learning (PSRL) approach which is capable of constructing fuzzy RL policies solely by training parameters on world models produced from randomly generated samples of the real system. This approach relates self-organizing fuzzy controllers to model-based RL for the first time. PSRL can be used straightforward on any RL problem, which is demonstrated on three standard RL benchmarks, mountain car, cart pole balancing and cart pole swing up. Our experiments yielded high performing and well interpretable fuzzy policies.", "creator": "LaTeX with hyperref package"}}}