{"id": "1610.06048", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "K-Nearest Neighbor Classification Using Anatomized Data", "abstract": "This analyticity paper analyzes roxburgh k mbanefo nearest neighbor classification rohri with sephiroth training clean-cut data chauffer anonymized 213.2 using woollahra anatomy. Anatomy preserves all semolina data values, but introduces uncertainty mows in gang-raped the gabol mapping acclimating between mchattie identifying mccuaig and sensitive imts values. 22.67 We cellulitis first leibovici study the 498th theoretical sithi effect of cullingworth the anatomized training data unprinted on dynamiting the k nearest porthos neighbor error lowest-charting rate bounds, nearest porsche neighbor 5,249 convergence rate, stearate and ascended Bayesian error. kmaq We negishi then validate the derived bounds 58-run empirically. berghahn We show dym that tackiness 1) Learning from re-offending anatomized data schuettler approaches the limits lamentations of 118.30 learning lustgarten through the unprotected data (barbarities although goethe requiring larger training valdiserri data ), sinkerballer and siss 2) butzel nearest 9-11 neighbor nijmeh using winy anatomized i.s. data outperforms nearest helmet neighbor on lovgren generalization - 1978-80 based maymi anonymization.", "histories": [["v1", "Wed, 19 Oct 2016 15:00:59 GMT  (113kb)", "http://arxiv.org/abs/1610.06048v1", "Technical Report. arXiv admin note: text overlap with arXiv:1610.05815"]], "COMMENTS": "Technical Report. arXiv admin note: text overlap with arXiv:1610.05815", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.DB", "authors": ["koray mancuhan", "chris clifton"], "accepted": false, "id": "1610.06048"}, "pdf": {"name": "1610.06048.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Koray Mancuhan", "Chris Clifton"], "emails": ["kmancuha@purdue.edu", "clifton@cs.purdue.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n06 04\n8v 1\n[ cs\n.L G\n] 1\n9 O\nct 2\n01 6\nI. INTRODUCTION\nData publishing without revealing sensitive information is an important problem. Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]). Other alternatives include value swapping [29], distortion [2], randomization [14], and noise addition (e.g., differential privacy [13]). Generalization consists of replacing identifying attribute values with a less specific version [6]. Suppression can be viewed as the ultimate generalization, replacing the identifying value with an \u201cany\u201d value [6]. These approaches have the advantage of preserving truth, but a less specific truth that reduces the utility of the published data.\nXiao and Tao proposed anatomization as a method to enforce l-diversity while preserving specific data values [37]. Anatomization splits instances across two tables, one containing identifying information and the other containing private information. The more general approach of fragmentation [7] divides a given dataset\u2019s attributes into two sets of attributes (2 partitions) such that an encryption mechanism avoids associations between two different small partitions. Vimercati et al. extend fragmentation to multiple partitions [11], and Tamas et al. propose an extension that deals with multiple sensitive attributes [19]. The main advantage of anatomization/fragmentation is that it preserves the original values of data; the uncertainty is only in the mapping between individuals and sensitive values.\nWe show that this additional information has real value. First, we demonstrate that in theory, learning from anatomized data can be as good as learning from the raw data. We then\ndemonstrate empirically that learning from anatomized data beats learning from generalization-based anonymization.\nThis paper looks only at instance-based learning, specifically non-parametric k nearest neighbor classifier (k-NN). This focus was chosen because we have solid theoretical results on the limits of learning, allowing us to compare theoretical bounds on learning from anatomized data with learning from the underlying unprotected data. We demonstrate this for a simple approach of using the anatomized data; we simply consider all possible mappings of individuals to sensitive values as equally likely.\nThere is concern that anatomization is vulnerable to several attacks [20], [23], [26]. While this can be an issue, any method that provides meaningful utility fails to provide perfect privacy against a sufficiently strong adversary [13], [25]. Introducing uncertainty into the anonymization process reduces the risk of many attacks, e.g, minimality [8], [35]. Our theoretical analysis holds for any assignment of items to anatomy groups, including a random assignment, which provides a high degree of robustness against minimality and correlation-based attacks. This paper has the following key contributions:\n1) We define a classification task on anatomized data without violating the random worlds assumption. A violating classification task would be the prediction of sensitive attribute, a task that was found to be #P-complete by Kifer [23]. 2) To our best knowledge, this is the first paper in the privacy community that studies the theoretical effect of training the k-NN on anatomized data. We show the anatomization effect for the error rate bounds and the convergence rate when the test data is neither anonymized nor anatomized. Inan et al. already gives a practical applications of such a learning scenario [21]. 3) We show the Bayesian error estimation for any nonparametric classifier using the anatomized training data. 4) We compare the k-NN classifier trained on the anatomized data with the k-NN classifier trained on the unprotected data. In case of nearest neighbor classifier (1-NN), we also make an additional comparison to generalization based learning scheme [21]. 5) We last compare the theoretical estimation of convergence rate with the practical measurements when the\nconvergence rate is defined in function of l-diversity.\nWe next summarize the related work, and give a set of definitions and notations necessary for further discussion. Section IV shows error rate bounds of the non-parametric kNN classifier; Section V analyzes the effect of anatomization on the Bayesian error. Section VI formulates the 1-NN convergence rate under l-diversity. The experimental analysis is presented in Section VII."}, {"heading": "II. RELATED WORK", "text": "There have been studies in how to mine anonymized data. Nearest neighbor classification using generalized data was investigated by Martin. Nested generalization and non-nested hyperrectangles were used to generalize the data from which the nearest neighbor classifiers were trained [28]. Inan et al. proposed nearest neighbor and support vector machine classifiers using anonymized training data that satisfy k-anonymity. Taylor approximation was used to estimate the Euclidean distance from the anonymized training data [21]. Zhang et al. studied Na\u0131\u0308ve Bayes using partially specified training data [38], proposing a conditional likehoods computation algorithm exploring the instance space of attribute-value generalization taxonomies. Agrawal et al. proposed an iterative distribution reconstruction algorithm for the distorted training data from which a C4.5 decision tree classifier was trained [1]. Iyengar suggested using a classification metric so as to find the optimum generalization. Then, a C4.5 decision tree classifier was trained from the optimally generalized training data [22]. Fung et al. gave a top-down specialization method (TDS) for anonymization so that the anonymized data allows accurate decision trees. A new scoring function was proposed for the calculation of decision tree splits from the compressed training data [18]. Dowd et al. studied C4.5 decision tree learning from training data perturbed by random substitutions. A matrix based distribution reconstruction algorithm was applied on the perturbed training data from which an accurate C4.5 decision tree classifier was learned [12].\nNone of the earlier work has provided a method directly applicable to anatomized training data. A classifier using the anatomized training data requires specific theoretical and experimental analysis, because anatomized training data provides additional detail that has the potential to improve learning; but also additional uncertainty that must be dealt with. Furthermore, previous work didn\u2019t justify theoretically why the proposed heuristics work in empirically."}, {"heading": "III. DEFINITIONS AND NOTATIONS", "text": "In this section, the first four definitions will recall the standard definitions of unprotected data and attribute types.\nDefinition 1: A dataset D is called a person specific dataset for population P if each instance X \u2208 D belongs to a unique individual p \u2208 P .\nThe person specific data will be called the training data in this paper. Next, we will give the first type of attributes.\nDefinition 2: A set of attributes are called direct identifying attributes if they let an adversary associate an instance X \u2208 D\nto a unique individual p \u2208 P without any background knowledge.\nDefinition 3: A set of attributes are called quasi-identifying attributes if there is background knowledge available to the adversary that associates the quasi-identifying attributes with a unique individual p \u2208 P .\nWe include both direct and quasi-identifying attributes under the name identifying attribute. First name, last name and social security number (SSN) are common examples of direct identifying attributes. Some common examples of quasi-identifying attributes are age, postal code, and occupation. Next, we will give the second type of attribute.\nDefinition 4: An attribute of instance X \u2208 D is called a sensitive attribute if it must be protected against adversaries from correctly inferring the value for an individual.\nPatient disease and individual income are common examples of sensitive attributes. Unique individuals p \u2208 P typically don\u2019t want these sensitive information to be publicly known when a dataset D is released to public. Provided an instance X \u2208 D, the class label is denoted by X.C. We don\u2019t consider the case where C is sensitive, as this would make the purpose of classification to violate privacy. Typically C is neither sensitive nor identifying, although the analysis holds for C being an identifying attribute.\nGiven the former definitions, we will next define the anonymized training data following the definition of kanonymity [32].\nDefinition 5: A training data D that satisfies the following conditions is said to be anonymized training data Dk [32]:\n1) The training data Dk does not contain any unique identifying attributes. 2) Every instance X \u2208 Dk is indistinguishable from at least (k \u2212 1) other instances in Dk with respect to its quasi-identifying attributes.\nIn this paper, we assume that the anonymized training data Dk is created according to a generalization based data publishing method. We next define the comparison baseline classifiers.\nDefinition 6: A non-parametric k nearest neighbor (k-NN) classifier that is trained on the anonymized training data Dk is called the anonymized k-NN classifier.\nDefinition 7: A non-parametric k-NN classifier that is trained on the training data D is called the original k-NN classifier.\nThe anonymized k-NN classifier will just be the comparison baseline in the evaluation and its theoretical discussion will not be included. We go further, requiring that there must be multiple possible sensitive values that could be linked to an individual. This requires the definition of groups [27].\nDefinition 8: A group Gj is a subset of instances in training data D such that D = \u222amj=1Gj , and for any pair (Gj1 , Gj2 ) where 1 \u2264 j1 6= j2 \u2264 m, Gj1 \u2229Gj2 = \u2205.\nNext, we define the concept of l-diversity or l-diverse given the former group definition.\nDefinition 9: A set of groups is said to be l-diverse if and\nonly if for all groups Gj \u2200v \u2208 \u03a0As(Gj), freq(v,Gj) |Gj| \u2264 1l where As is the sensitive attribute in D, \u03a0As(\u2217) is the database As projection operation on training data \u2217 (or on data table in the database community), freq(v,Gj) is the frequency of v in Gj and |Gj | is the number of instances in Gj .\nWe extend the data publishing method anatomization from Xiao et al. that is originally based on l-diverse groups [37].\nDefinition 10: Given a training data D partitioned in m l-diverse groups according to Definition 9, anatomization produces an identifier table IT and a sensitive table ST as follows. IT has schema\n(C,A1, ..., Ad, GID)\nincluding the class attribute, the quasi-identifying attributes Ai \u2208 IT for 1 \u2264 i \u2264 d, and the group id GID of the group Gj . For each group Gj \u2208 D and each instance X \u2208 Gj , IT has an instance X of the form:\n(X.C,X.A1, ..., X.Ad, j)\nST has schema (GID,As)\nwhere As is the sensitive attribute in D and GID is the group id of the group Gj . For each group Gj \u2208 D and each instance X \u2208 Gj , ST has an instance of the form:\n(j,X.As)\nGiven the learning task of predicting class attribute C, definition 10 lets us observe the following about training data D published according to anatomization: every instance Xi \u2208 IT can be matched to l instances Xj \u2208 ST using the common attribute GID in both data table schemas. This observation yields the anatomized training data and the anatomized k-NN classifier.\nDefinition 11: Given two data tables IT and ST resulting from the anatomization on training data D, the anatomized training data DA is\nDA = \u03a0IT.A1,\u00b7\u00b7\u00b7IT.Ad,ST.As( IT \u2736 ST )\nwhere \u2736 is the database inner join operation with respect to the condition IT.GID = ST.GID and \u03a0(\u2217) is the database projection operation on training data (*) processed according to definition 10.\nDefinition 12: A non-parametric k-NN classifier that is trained on the anatomized training data DA is called the anatomized k-NN classifier.\nUsing the former definitions, we now give assumptions and notations used in discussing the anatomized k-NN classifier. In the theoretical analysis, we assume that all the training data has a smooth probability distribution. Although anatomization requires a discrete probability distribution for the sensitive attribute As, such smoothness violation is negligible since the original k-NN classifier is known to fit well on discrete training data [33]. The sensitive attribute As is assumed to be non-binary. The anatomized k-NN cases where k > 1 and k is even will be ignored, because such cases include the tie\nbetween k-nearest neighbors that makes the bounds ambiguous and complicated [15]. The total number of attributes are assumed to be d + 1 (d identifying attributes and 1 sensitive attribute) and all instances are assumed to be in a separable metric space M \u2282 Rd+1 as in [9], [10], [15]. D has N instances whereas DA has Nl instances from definition 11. All instances are i.i.d whether they are in training or test data. For the sake of simplicity, Aid will denote the identifying attributes A1 \u00b7 \u00b7 \u00b7Ad \u2208 IT . T stands for a test data which is not processed by any anatomization and generalization method. X will be an instance of the test data T . d(U, V ) is the quadratic distance metric for a pair of instances U and V in metric space M . X \u2032N (k) denotes the set of k number of nearest neighbors of X in D that the original k-NN classifier uses while X \u2032Nl(k) denotes the set of k number of nearest neighbors of X in DA that the anatomized k-NN classifier uses. Xi will interchangeably be an instance of D or DA and Xj will interchangeably be an instance of X \u2032N (k) or X \u2032Nl(k). In case of k = 1, we will use X \u2032 N and X \u2032 Nl for the nearest neighbors in D and DA. X is the random variable with probability distribution P (X) from which X and Xi are drawn. Training and test instances will be column vectors in format of (A1, ..., Ad, As)T . C is the class attribute in D and DA with binary labels 1 and 2. Given the training data D and the class label i, qi(X), Pi(X) and Pi stand for the posterior probability, the likelihood probability and the prior probability respectively. If the anatomized training data DA is used, qAi(X), PAi(X) and PAi are the symmetric definitions for the class label i. R(X \u2032N(k), X) is the error rate when X \u2208 T is classified using X \u2032N (k). If X \u2032 Nl(k) is used to classify X , RA(X \u2032Nl(k), X) will be the error rate. When Xj \u223c= X hold for all Xj \u2208 X \u2032N (k), we denote the error rate by R\nk(X) in Equation 1 [15].\nRk(X) =\nk+1/2\u2211\ni=1\n1\ni\n( 2i\u2212 2\ni\u2212 1\n) [q1(X)q2(X)] i\n+ 1\n2\n( k + 1\nk + 1/2\n) [q1(X)q2(X)] k+1/2\n(1)\nRkA(X) is the error rate when Xj \u223c= X hold for all Xj \u2208 X \u2032Nl(k). R k A(X) can trivially be derived from Eqn. 1 by substituting qi(X) with qAi(X). The Bayesian errors given X are denoted by R\u2217(X) and R\u2217A(X) when Xj \u223c= X holds for all Xj \u2208 X \u2032N (k) and Xj \u2208 X \u2032 Nl(k) respectively. Eqn. 2 computes R\u2217(X) [15].\nR\u2217(X) = min{q1(X), q2(X)}\n\u223c=\n\u221e\u2211\ni=1\n1\ni\n( 2i\u2212 2\ni\u2212 1\n) [q1(X)q2(X)] i (2)\nR\u2217A(X) can trivially be derived again from 2 by substituting qi(X) with qAi(X). R k and RkA, which are E{R k(X)} and E{RkA(X)} with respect to X, will stand for the error rate of original k-NN and anatomized k-NN classifiers respectively. R\u2217 and R\u2217A, which are E{R\n\u2217(X)} and E{R\u2217A(X)} with respect to X, will stand for the Bayesian errors of original\ntraining data and anatomized training data respectively. We will denote R1(X) and R1A(X) by R(X) and RA(X) for convenience. Similarly, R and RA will denote R1 and R1A. Further notations and definitions will be given in the paper if necessary.\nIV. ERROR BOUNDS OF ANATOMIZED k-NN\nIn this section, we will first show the error bounds for the anatomized 1-NN classifier. We will then discuss the extension to the anatomized k-NN classifier for all odd k > 1. We give only proof sketches due to space limitations.\nWe first give Corollary 1 which is critical for the error bounds of the anatomized 1-NN classifier.\nCorollary 1: Convergence of the nearest neighbor in the anatomized training data DA. Let X \u2208 T and X1, \u00b7 \u00b7 \u00b7 , XNl \u2208 DA be i.i.d instances taking values separable in any metric space M \u2282 Rd+1. Let X \u2032Nl be the nearest neighbor of X in DA. Then, lim\nN\u2192\u221e X \u2032Nl = X with probability one.\nWe can intuitively say that Corollary 1 should hold for the anatomized training data DA if it already holds for the training data D. For the nearest neighbor X \u2032N \u2208 D of X , there are l instances in the anatomized training data DA including X \u2032N itself. Assuming very large training data size (N \u2192 \u221e), X \u2032N must still be the closest instance to X in the anatomized training data DA. The l \u2212 1 incorrect instances are expected to remain far and X \u2032Nl = X \u2032 N should eventually hold.\nWe now give a sketch of the proof or Corollary 1. Let SX(r) = {X\u0304 \u2208 M : d(X, X\u0304) \u2264 r} be the sphere with radius r > 0 centered at X . Let\u2019s consider that X has a sphere SX(r) with non-zero probability. Therefore, for any radius \u03b4 > 0 and any fixed l \u2265 0;\nP{ min i=1,\u00b7\u00b7\u00b7 ,Nl\nd(Xi, X) \u2265 \u03b4} = [1\u2212 P (SX(\u03b4))] Nl\n\u223c= lim N\u2192\u221e [(1\u2212 P (SX(\u03b4))) l]N\n= 0\n(3)\nSince d(Xi, X) is monotonically decreasing in terms of i for all Xi \u2208 DA, we can conclude that lim\nN\u2192\u221e X \u2032Nl = X holds\nwith probability 1. The rest of proof follows the denseness of the set Q in the set R according to Cover et al. [9].\nNext, Theorem 1 shows the error bounds of the anatomized 1-NN classifier using Corollary 1.\nTheorem 1: Error Rate Bounds of the anatomized 1-NN classifier Let M \u2282 Rd+1 be a metric space. Let PA1(X) and PA2(X) be the likelihood probabilities of X such that PA(X) = PA1PA1(X) + PA2PA2(X) with class priors PA1 and PA2 . Last, let\u2019s assume that X is either a point of nonzero probability measure or a continuity point of PA1(X) or PA2(X). Then the nearest neighbor has the probability of error RA with the bounds\nR\u2217A \u2264 RA \u2264 2R \u2217 A (4)\nwhere R\u2217A denotes the Bayesian error when the anatomized training data DA is used.\nWe now give a sketch of proof for Theorem 1. Let RA(X \u2032 Nl, X) denote the probability of error for a pair of instances X \u2208 T and X \u2032Nl \u2208 DA. Since Corollary 1 shows that lim\nN\u2192\u221e X \u2032Nl = X always hold, 5 is derived from 1 by\nsubstituting k with 1 and qi(X) with qAi(X).\nlim N\u2192\u221e\nRA(X \u2032 Nl, X) = RA(X) = 2qA1(X)qA2(X) (5)\nThe rest of the derivation follows Cover et al. using 1, 2 [9]. Extending 4 from the anatomized 1-NN classifier to the anatomized k-NN classifier for all odd k > 1 follows the steps in Corollary 1 and Theorem 1. The key is to show that lim\nN\u2192\u221e Xj = X holds for all Xj \u2208 X \u2032Nl(k). The rest is to derive an expression of RkA(X) as in 5 for all odd k > 1 and show that RkA(X) is always less than 2R \u2217 A and R k\u22122 A (X). We exclude this derivation due to space limitations, but the derivation follows from the original k-NN classifier analysis in [15]. The anatomized k-NN classifier has the bound 6\nR\u2217A \u2264 \u00b7 \u00b7 \u00b7 \u2264 R 5 A \u2264 R 3 A \u2264 RA \u2264 2R \u2217 A (6)\nfor all odd k > 1. Note that the Bayesian errors R\u2217A and R\n\u2217 are not always same due to the l-diverse groups of the anatomization. The l-diverse groups cause new likelihood PAi(X) and eventually posterior probabilities qAi(X). R \u2217 A thus differ from 2, because 2 uses qi(X) instead of qAi(X). The next section formulates this change."}, {"heading": "V. BAYESIAN ERROR ON ANATOMIZED TRAINING DATA", "text": "Since it is impossible to know the exact Bayesian error, many Bayesian error estimation techniques were suggested [4], [10], [15]. In this section, the Bayesian error will be estimated for binary classification using Parzen density estimation. Although such estimation would be very interesting for multilabel classification, the theoretical analysis on unprotected data only covers binary classification [4]. The Parzen density estimation approach, which is easier to derive than the k nearest neighbor density estimation approach, will follow Fukunaga [15] and Fukunaga et al. [16]. Both approaches show the same behavior in terms of the Bayesian estimation that makes the discussion general enough for any non-parametric density based binary classification method [15]. We first give three axioms and a lemma.\nAxiom 1: Given the anatomized training data DA and the training data D; let Pi and PAi be the class priors for class labels i = {1, 2}. Then, Pi = PAi is always true.\nAxiom 2: Let P1P1(X.Aid) + P2P2(X.Aid) and PA1PA1(X.Aid) + PA2PA2(X.Aid) be P (X.Aid) and PA(X.Aid) respectively. Given the anatomized training data DA and the training data D; let P (X.Aid) and PA(X.Aid) be the smooth joint densities of identifying attributes Aid. Then, P (X.Aid) = PA(X.Aid) is always true.\nAxiom 3: Let P1P1(X.As) + P2P2(X.As) and PA1PA1(X.As) + PA2PA2(X.As) be P (X.As) and PA(X.As) respectively. Given the anatomized training data DA and the training data D; let P (X.As) and\nPA(X.As) be the smooth densities of sensitive attribute As. Then, P (X.As) = PA(X.As) is always true. Axioms 1, 2 and 3 are obvious due to the following: provided a sample of size N drawn from a probability distribution P , repeating every instance for fixed l > 0 times and obtaining a sample of size Nl does not change the probability distribution P . The estimated parameters \u00b5\u0302 and \u03c3\u03022 of distribution P remain same.\nLemma 1: Given the anatomized training data DA and the training data D, let identifying attributes Aid and the sensitive attribute As be independent. Then, PA(X) = P (X) is always true under the axioms 2 and 3. Using axioms 2 and 3, the proof of lemma 1 is straightforward. Lemma 1 and axioms 1-3 yield the Theorem 2. Using lemma 1, we will assume that R\u2217A = R\n\u2217 holds asymptotically for Bayesian errors.\nTheorem 2: Let M \u2282 Rd+1 be a metric space. Let PA1(X) and PA2(X) be the smooth probability density functions of X . Let PA1 and PA2 be the class priors such that PA(X) = PA1PA1(X) + PA2PA2(X). Similarly, let P1(X) and P2(X) be the smooth probability density functions of X such that P (X) = P1P1(X) + P2P2(X) with class priors P1 and P2. Let hA(X) = \u2212ln( PA1(X)\nPA2 (X) ) and h(X) = \u2212ln(P1(X)P2(X) ) be\nthe classifiers with biases \u2206hA(X) and \u2206h(X) respectively. Let t = ln(PA1PA2\n) = ln(P1P2 ) be the decision threshold with threshold bias \u2206t. Let \u01ebA > 0 be the small changes on P1(X) and P2(X) resulting in PA1(X) and PA2(X); and R\u0302 \u2217 A, R\u0302\u2217 be the Bayesian error estimations with respective biases \u2206R\u2217A, \u2206R\n\u2217. Let P\u0302Ai(X) and P\u0302i(X) be the Parzen density estimations; and K(\u2217) be the kernel function for D with shape matrix A and size/volume parameter r [15]. Last, let\u2019s assume that 1)Aid and As are independent in the training data D and the anatomized training data DA 2) R\u2217A = R\n\u2217 hold 3) \u2206t < 1. Therefore,\nR\u0302\u2217A \u223c= R\u2217 + a1r 2 + a2r 4 + a3\nr\u2212(d+1)\nN\n+ \u01ebAa4r 2 + \u01ebAa5r 4 \u2212 \u01ebAa6 r\u2212(d+1)\nN\n(7)\nwhere \u01ebAa6 r \u2212(d+1)\nN > 0 always holds. Due to lack of space, we provide a brief summary of the proof. In 7, the terms other than R\u2217 stand for the expected estimation error E[\u2206R\u2217A] in 8 [15].\nE[\u2206R\u2217A] \u223c=\n1\n2\u03c0\n\u222b \u222b E[\u2206hA(X) + (j\u03c9)\n2 \u2206h2A(X)]e j\u03c9hA(X)\n\u00d7 [PA1PA1(X)\u2212 PA2PA2(X)]d\u03c9dX\n(8)\nHence, the proof of this theorem requires the second order approximations of E{\u2206hA(X)} and E{\u2206h2A(X)}. From Fukunaga [15], we know that E{\u2206hA(X)} and E{\u2206h2A(X)} are expressed in function of the E{P\u0302Ai(X)} and V ar{P\u0302Ai(X)}. The key point of the proof is to formulate the anatomized training data effect in E{P\u0302Ai(X)} and V ar{P\u0302Ai(X)} and show its propagation to the E{\u2206hA(X)} and E{\u2206h2A(X)}.\nLet \u01ebA > 0 be the small change in the likelihood probabilities Pi(X) which results in PAi(X), t be ln(P1/P2) and t = tA be true due to axiom 1. Therefore, we have 9 and 10 as the likelihood densities in the anatomized training data DA using lemma 1.\nPA1(X) = P1(X) + \u01ebA (9) PA2(X) = P2(X)\u2212 e t\u01ebA (10)\nUsing 9 and 10 in the Taylor approximations of E{P\u0302Ai(X)} and V ar{P\u0302Ai(X)} results in the approximations of E{\u2206hA(X)} in 11\nE{\u2206hA(X)} \u223c= E{\u2206h(X)}\n+ \u01ebA r2 2 [ \u03b11(X) P1(X) + et \u03b12(X) P2(X) ] \u2212 \u01ebA r4\n4 [ \u03b121(X) P1(X) + et \u03b122(X) P2(X) ]\n\u2212 \u01ebA r\u2212(d+1)\n2N [ s1 P 21 (X) + et s2 P 22 (X) ]\n(11)\nand E{\u2206h2A(X)} in 12\nE{\u2206h2A(X)} \u223c= E{\u2206h2(X)}\n\u2212 \u01ebA\u2206t r 2[ \u03b11(X)\nP1(X) + et\n\u03b12(X) P2(X) ]\n+ \u01ebA r4 2 [ \u03b11(X)\u03b12(X) P1(X) \u2212 et \u03b11(X)\u03b12(X) P2(X) ] \u2212 \u01ebA r4\n2 [ \u03b121(X)(1\u2212\u2206t) P1(X) \u2212 et \u03b122(X)(1 + \u2206t) P2(X) ]\n\u2212 \u01ebA r\u2212(d+1) N [ (1\u2212\u2206t)s1 P 21 (X) + et (1 + \u2206t)s2 P 22 (X) ]\n(12)\nwhere wi = sir\u2212(d+1) is true. The former equality is the result of using Parzen density estimate [15]. 11 and 12 are derived using the Taylor approximations up to second order. Plugging 11 and 12 in 8 and rewriting 8 gives 7 where each ai stands for an integration term.\nEqn. 7 shows that the anatomized training data DA reduces the variance term of the decision functions that estimate the Bayesian error. However, it is hard to determine the effect of the anatomized training data DA on bias terms. All \u01ebAa4r2 > 0, \u01ebAa4r2 < 0, \u01ebAa5r4 > 0 and \u01ebAa5r4 < 0 are possible cases depending on hA(X) which might yield bias terms of R\u0302\u2217A bigger or smaller than R\u0302 \u2217\u2019s ones."}, {"heading": "VI. ANATOMIZED 1-NN CONVERGENCE", "text": "We now discuss the error rate of the anatomized 1-NN classifier when the anatomized training data DA has finite size Nl. We will then derive the convergence rate from the former error rate. The discussion here won\u2019t be generalized to the anatomized k-NN classifier since the finite size training data performance of k-NN classifiers are not generalized to k > 2 in the pattern recognition literature [10], [15]. Also, only binary classification will be considered due to space limitations.\nFrom Theorem 2, we intuitively expect a faster convergence rate than the original 1-NN classifier\u2019s one. For N number of instances in training data D, using the anatomized training data DA reduces the variance of any classifier\u2019s Bayesian error estimation. Therefore, there are fewer possible models to consider for a given sample size which eventually means a faster convergence to the asymptotic result. Theorem 3 extends the analysis of Fukunaga et al. [15], [17].\nTheorem 3: Let M \u2282 Rd+1 be a metric space. Let PA1(X) and PA2(X) be the smooth probability density functions of X . Let PA1 and PA2 be the class priors such that PA(X) = PA1PA1(X) + PA2PA2(X). Let qA1(X) and qA2(X) be the smooth posterior probability densities such that qA1(X) + qA2(X) = 1 and Nl \u2192 \u221e. Let qA1(X \u2032 Nl) and qA2(X \u2032 Nl) be the smooth posterior probability densities such that qA1(X \u2032 Nl) + qA2(X \u2032 Nl) = 1 and Nl 9 \u221e. Let \u03b4 > 0 be the difference between qAi(X) and qAi(X \u2032 Nl) for class labels i = {1, 2}. Let d(X \u2032Nl, X) be the quadratic distance with matrix A and \u03c1 be the calculated value of d(X \u2032Nl, X). Let RA be the error rate of the anatomized 1-NN classifier when Nl \u2192 \u221e. Last, let RAN be the error rate of the anatomized 1-NN classifier when Nl 9 \u221e. Then,\nRAN \u223c= RA + \u03b2\n1\n(Nl) 2 d+1\nEX{|A| \u2212 1 d+1 tr{AB(X)}} (13)\nwhere \u03b2 is\n\u03b2 = \u0393\n2 d+1 (d+32 )\u0393( 2 d+1 + 1)\n\u03c0(d+ 1) (14)\nand B(X) is\nB(X) = P \u2212 2 d+1\nA (X)[qA2(X)\u2212 qA1(X)]\n\u00d7 [ 1\n2 \u22072qA1(X) + P \u22121 A (X)\u2207PA(X)\u2207 T qA1(X)]\n(15)\nWe will give here a summary of proof. We first define qAi(X \u2032 Nl) in function of qAi(X) \u00b1 \u03b4 such that qA1(X \u2032 Nl) + qA2(X \u2032 Nl) = 1 holds. Then, RAN is written in function of RA and \u03b4. The result is\nRAN = RA + E[[qA2(X)\u2212 qA1(X)]\u03b4] (16)\nwhere E[[qA2(X)\u2212 qA1(X)]\u03b4] is\nE{(qA2(X)\u2212 qA1(X))\u03b4)} =\nEX{E\u03c1{EX\u2032 Nl {[qA2(X)\u2212 qA1(X)]\u03b4|\u03c1,X}|X}}\n(17)\na 3-step expectation in 17. The rest of the proof follows Fukunaga [15]. The key deviation of the anatomized training data DA from the training data D results from the step 2. In step 2, the nearest neighbor density estimation is done on Nl training instances instead of N training instances. Thus, the expectation with respect to \u03c1 gives 18.\nE{\u03c12} \u223c= \u0393\n2 d+1 (d+32 )\u0393( 2 d+1 + 1)\nP 2 d+1\nA (X)\u03c0|A| 1 d+1\n1\n(Nl) 2 d+1\n(18)\nUsing 18, expectation with respect to X in 17 (step 3) according to Fukunaga [15] results in 13. Table I gives a summary of theoretical analysis, including a comparison between the anatomized training data DA and the training data D."}, {"heading": "VII. EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "A. Preprocessing, Setup and Implementation", "text": "We evaluate the anatomized k-NN classifier using cross validation on the Adult, Bank Marketing, IPUMS datasets from UCI collection [5] and on the Fatality (fars) dataset from Keel repository [3].\nIn the adult dataset, we predicted the income attribute. The instances with missing values were removed and features selected using the Pearson correlation filter (CfsSubsetEval) of Weka [34]. After preprocessing, we had 45222 instances with 5 attributes education, marital status, capital gain, capital loss and hours per week and the class attribute income. The other datasets were used without feature selection. In IPUMS, we predicted whether a person is veteran or not. After removing the N/A and missing values for veteran information, there were 148585 instances with 59 attributes. In the Fatality dataset, we predicted whether a person is injured or not in a car accident based on 29 attributes. Since the class attribute was non-binary in the original data, the instances with class labels \u201cInjured Severity Unknown\u201d, \u201cDied Prior to Accident\u201d and \u201cUnknown\u201d were removed and the binary class values\n\u201cInjured\u201d vs \u201cNot Injured\u201d were created. The former removal resulted in 91085 instances. In the Bank Marketing dataset, we predicted whether a person replied positively or negatively to the bank\u2019s phone marketing campaign. The dataset is used with 41188 instances and 20 attributes.\nIn the Adult, Bank Marketing and IPUMS datasets, education (educrec in IPUMS) was deemed sensitive whereas the remaining attributes were quasi-identifying attributes. Education had many discrete values which lets all samples satisfy l-diversity when l = 2, 3. In the Fatality dataset, \u201cPOLICE REPORTED ALCOHOL INVOLVEMENT\u201d was the sensitive attribute whereas rest of the attributes were quasiidentifying attributes. This was the only discrete attribute in the dataset other than class attribute that is not a typical quasiidentifying attribute such as state, age, zipcode.\nWeka (same version of Inan et al. [21]) was used to implement the k-NN classifier [34]. The anatomization algorithm was implemented by us following Xiao et al. [37]. All the anatomized training data were created from identifier and sensitive tables using the merge function of R. The error rates were measured on each test fold according to the definition in Weka implementation. When we compared the anatomized 1-NN with anonymized 1-NN, we also used the same generalization hierarchies that Inan et al. used. The statistical tests following Kumar et al. are provided [33]"}, {"heading": "B. Anatomized 1-NN vs Anonymized 1-NN and Original 1-NN", "text": "First, we compare the anatomized 1-NN classifier with both anonymized and original 1-NN classifiers. We consider anonymized and anatomized training data with the quasiidentifying groups having similar number of instances (k = l). Figure 1 shows the plot of error rates on 10-fold cross validation without outlier values. We give results for two scenarios: 1) k = l = 2 vs original data 2) k = l = 3 vs original data. Although we measured the error rates to k = l = 7, we omit these results due to space limitations. The results are similar when k = l > 4 even though some instances are suppressed to maintain l-diversity.\nIn Figure 1, the general trend is that anatomized 1-NN has the smallest error rates and anonymized 1-NN has the largest error rates. The average error rates for anonymized 1-NN and anatomized 1-NN classifiers are 0.3132 and 0.204 for k = l = 2 and 0.3132 and 0.2324 for k = l = 3. Meanwhile, the original 1-NN has average error rate of 0.2456. When k = l = 2, the anatomized 1-NN has significantly lower error rates than the original 1-NN at the confidence intervals 0.99, 0.98, 0.95, 0.9 and 0.8. When k = l = 3, the anatomized 1-NN has significantly lower error rates than the original 1-NN at the confidence interval 0.99. This is a surprising and an interesting result showing the practical interpretation of Theorem 2 in Section V. Theorem 2 shows that the Bayesian error of the anatomized training data DA has smaller variance term than the Bayesian error of the training data D. Hence, a model which is overfitted on the training data D is likely to be left out in the search space if the model is trained from the anatomized training data DA.\nThe anatomized 1-NN has significantly lower error rate than the anonymized 1-NN at the confidence intervals 0.99 and 0.98 when k = l = 2, and at the confidence interval 0.99 when k = l = 3. The results aren\u2019t statistically significant for confidence intervals smaller than 0.95 or 0.99, as the anonymized 1-NN consistently doesn\u2019t fit one fold\u2019s training data. Its high error rate results in a significant increase in sample variance, reducing the statistical confidence. When we analyzed this training data, we noticed that the instance values were generalized to the root values of the generalization hierarchies which could eliminate the decision boundary in the original data. This observation emphasizes the anatomy\u2019s advantage for keeping the original attribute values despite diversifying the sensitive attribute values within a group.\nC. Anatomized k-NN vs. Original k-NN\nIn this section, we compare the anatomized k-NN classifier with the original k-NN classifier. The comparison doesn\u2019t include the anonymized k-NN classifier because Inan et al.\u2019s work considers only the anonymized 1-NN classifier [21]. Its extension to k > 1 cases is beyond the scope of this work. Although we ran the experiments for anatomized 3-NN, 5- NN, 7-NN and 9-NN classifiers on the Adult, Bank Marketing, Fatality and IPUMS datasets, we give the results on the larger Fatality and IPUMS datasets due to space limitations. We again include the cases of l = 2 and l = 3. Figure 2 plots the error rate distributions of 3-NN and 5-NN classifiers on Fatality dataset, and 7-NN and 9-NN classifiers on IPUMS data.\nIn the Fatality data, the anatomized 3-NN and 5-NN classifiers outperform the original 3-NN and 5-NN classifiers at the confidence intervals 0.99 and 0.98 when l = 2. The anatomized 5-NN classifier also outperforms the original 5- NN classifier at the confidence interval 0.95 when l = 2. In contrast, the original 3-NN and 5-NN classifiers outperform the anatomized 3-NN and 5-NN classifiers when l = 3, although not to a statistically significant level. For 3-NN classifiers, the average error rates are 0.0128, 0.0135 and\n0.0132 for DA with l = 2, DA with l = 3 and original data respectively. On the other hand, the average error rates of 5- NN classifier on DA with l = 2, DA with l = 3 and original data are 0.0119, 0.0122 and 0.0122 respectively.\nIn the IPUMS data, the original 7-NN classifier outperforms the anatomized 7-NN classifier at the confidence intervals 0.99, 0.98, 0.95, 0.9 when l = 2 and l = 3. On the other hand, the original 9-NN classifier outperforms the anatomized 9- NN classifiers at the confidence interval 0.99 when l = 2 and l = 3. For 7-NN classifiers, the average error rates are 0.1567, 0.1586 and 0.1549 for DA with l = 2, DA with l = 3 and original data respectively. The average error rates of 9-NN classifier on DA with l = 2, DA with l = 3 and original data are 0.1552, 0.1568 and 0.1542 respectively.\nIn conclusion, the anatomized and original k-NN classifiers have similar statistically significant error rates for multiple values of l. These results confirm the theoretical analysis that we made in the earlier sections."}, {"heading": "D. Convergence Behavior", "text": "We now compare the anatomized 1-NN classifier versus the original 1-NN classifier on convergence behavior. We create 5 partitions from the Adult (after preprocessing), Bank Marketing, Fatality and IPUMS datasets. Each partition is used as test data, and the remaining 4 partitions are used incrementally for training. Our objective is to show how the parameter l in anatomized training data change the error rates when the training data size is increased incrementally. Figure 3 plots the average error rates for the original training data, the anatomized training data with l = 2, the anatomized training data with l = 3; and the theoretical error rate in function of the training data size.\nWe can\u2019t know the asymptotical RA practically for theoretical error rates. We thus make the following estimation for the theoretical result. For each dataset, we set the RA to the minimum of the error rates in the specific dataset\u2019s results. We then calculate the rate 1\n(Nl) 2 d+1 from the N , d and l values\nthat we set in the experiments. Using the RA and 1 (Nl) 2 d+1 , we computed the respective bias and eventually the theoretical error rate according to the respective training data size and l.\nThe measured error rates in Figure 3 show a convergence that is similar to the one that theoretical error rates show. Given the largest training data size 4N5 ; 0.015, 0.004, 0.008 and 0.0085 are approximately the maximum deviations of measured error rates from the theoretical error rates for the Adult, Bank Marketing, Fatality and the IPUMS datasets respectively. We can also see that the convergence of error rate does not make much difference between the original data, anatomized data with l = 2 and the anatomized data with l = 3. In all types of training data, the convergence rate of 1-NN classifier is slow."}, {"heading": "VIII. CONCLUSION", "text": "This work demonstrates the feasibility of k-NN classification using training data protected by anatomization under ldiversity. We show that the asymptotic error bounds are the same for anatomized data as for the original data. Perhaps surprisingly, the proposed 1-NN classifier has a faster convergence to the asymptotical error rate than the convergence of 1-NN classifier using the training data without anatomization. In addition, the analysis suggests that the Bayesian error esti-\nmation for any non-parametric classifier using the anatomized training data reduces the variance term of the Bayesian error estimation, although it is hard to define the characteristic of the bias term.\nExperiments on multiple datasets confirm the theoretical convergence rates. These experiments also demonstrate that proposed k-NN on anatomized data approaches or even outperforms k-NN on original data. In particular, the experiments on well known Adult data show that 1-NN on anatomized data outperforms learning on data anonymized to the same anonymity levels using generalization."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported by the \u201cAnonymous\u201d. We thank \u201cAnonymous\u201d for sharing his/her implementation used for evaluating 1-NN on generalization-based anonymization. We also thank \u201cAnonymous\u201d for helpful comments throughout the theoretical analysis."}], "references": [{"title": "On the design and quantification of privacy preserving data mining algorithms", "author": ["D. Agrawal", "C.C. Aggarwal"], "venue": "Proceedings of the Twentieth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. Santa Barbara, California: ACM, May 21-23 2001, pp. 247\u2013255. [Online]. Available: http://doi.acm.org/10.1145/375551.375602", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Privacy-preserving data mining", "author": ["R. Agrawal", "R. Srikant"], "venue": "Proceedings of the 2000 ACM SIGMOD Conference on Management of Data. Dallas, TX: ACM, May 14-19 2000, pp. 439\u2013450. [Online]. Available: http://doi.acm.org/10.1145/342009.335438", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Keel data-mining software tool: Data set repository, integration of algorithms and experimental analysis framework", "author": ["J. Alcal\u00e1", "A. Fern\u00e1ndez", "J. Luengo", "J. Derrac", "S. Garc\u0131\u0301a", "L. S\u00e1nchez", "F. Herrera"], "venue": "Journal of Multiple-Valued Logic and Soft Computing, vol. 17, no. 255-287, p. 11, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Lower bounds for bayes error estimation", "author": ["A. Antos", "L. Devroye", "L. Gy\u00f6rfi"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 21, no. 7, pp. 643\u2013645, 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "2007. [Online]. Available: http://www.ics.uci.edu/\u223cmlearn/", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "kanonymous data mining: A survey", "author": ["V. Ciriani", "S.D.C. di Vimercati", "S. Foresti", "P. Samarati"], "venue": "Privacy-preserving data mining. Springer, 2008, pp. 105\u2013136.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Combining fragmentation and encryption to protect privacy in data storage", "author": ["V. Ciriani", "S.D.C.D. Vimercati", "S. Foresti", "S. Jajodia", "S. Paraboschi", "P. Samarati"], "venue": "ACM Trans. Inf. Syst. Secur., vol. 13, pp. 22:1\u201322:33, July 2010. [Online]. Available: http://doi.acm.org/10.1145/1805974.1805978", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimizing minimality and maximizing utility: Analyzing method-based attacks on anonymized data", "author": ["G. Cormode", "N. Li", "T. Li", "D. Srivastava"], "venue": "Proceedings of the VLDB Endowment, vol. 3, no. 1, 2010, pp. 1045\u20131056. [Online]. Available: http://dl.acm.org/citation.cfm?id=1920972", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "Information Theory, IEEE Transactions on, vol. 13, no. 1, pp. 21\u201327, 1967.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1967}, {"title": "A probabilistic theory of pattern recognition", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "Springer Science & Business Media,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Extending loose associations to multiple fragments", "author": ["S.D.C. di Vimercati", "S. Foresti", "S. Jajodia", "G. Livraga", "S. Paraboschi", "P. Samarati"], "venue": "DBSec\u201913, 2013, pp. 1\u201316.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Privacy-preserving decision tree mining based on random substitutions", "author": ["J. Dowd", "S. Xu", "W. Zhang"], "venue": "Emerging Trends in Information and Communication Security. Springer, 2006, pp. 145\u2013159.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Differential privacy", "author": ["C. Dwork"], "venue": "33rd International Colloquium on Automata, Languages and Programming (ICALP 2006), Venice, Italy, Jul. 9-16 2006, pp. 1\u201312. [Online]. Available: http://dx.doi.org/10.1007/11787006 1", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Limiting privacy breaches in privacy preserving data mining", "author": ["A. Evfimievski", "J. Gehrke", "R. Srikant"], "venue": "Proceedings of the 22nd ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS 2003), San Diego, CA, Jun. 9-12 2003, pp. 211\u2013222.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Introduction to statistical pattern recognition", "author": ["K. Fukunaga"], "venue": "Academic press,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Bayes error estimation using parzen and k-nn procedures", "author": ["K. Fukunaga", "D.M. Hummels"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, no. 5, pp. 634\u2013643, 1987.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1987}, {"title": "Bias of nearest neighbor error estimates", "author": ["K. Fukunaga", "D.M. Hummels"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, no. 1, pp. 103\u2013112, 1987.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1987}, {"title": "Top-down specialization for information and privacy preservation", "author": ["B.C.M. Fung", "K. Wang", "P.S. Yu"], "venue": "Proceedings of the 21st International Conference on Data Engineering, ser. ICDE \u201905. Washington, DC, USA: IEEE Computer Society, 2005, pp. 205\u2013216. [Online]. Available: http://dx.doi.org/10.1109/ICDE.2005.143", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "A privacy protection model for patient data with multiple sensitive attributes", "author": ["T. Gal", "Z. Chen", "A. Gangopadhyay"], "venue": "International Journal of Information Security and Privacy, IGI Global, Hershey, PA, vol. 2, no. 3, pp. 28\u201344, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Permutation anonymization: Improving anatomy for privacy preservation in data publication.", "author": ["X. He", "Y. Xiao", "Y. Li", "Q. Wang", "W. Wang", "B. Shi"], "venue": "PAKDD Workshops, ser. Lecture Notes in Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Using anonymized data for classification", "author": ["A. Inan", "M. Kantarcioglu", "E. Bertino"], "venue": "Proceedings of the 2009 IEEE International Conference on Data Engineering, ser. ICDE \u201909. Washington, DC, USA: IEEE Computer Society, 2009, pp. 429\u2013440. [Online]. Available: http://dx.doi.org/10.1109/ICDE.2009.19", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Transforming data to satisfy privacy constraints", "author": ["V.S. Iyengar"], "venue": "Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201902. New York, NY, USA: ACM, 2002, pp. 279\u2013288. [Online]. Available: http://doi.acm.org/10.1145/775047.775089", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Attacks on privacy and definetti\u2019s theorem", "author": ["D. Kifer"], "venue": "Proceedings of the 2009 ACM SIGMOD International Conference on Management of data. ACM, 2009, pp. 127\u2013138.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "t-closeness: Privacy beyond k-anonymity and l-diversity", "author": ["N. Li", "T. Li"], "venue": "Proceedings of the 23nd International Conference on Data Engineering (ICDE \u201907), Istanbul, Turkey, Apr. 16-20 2007. [Online]. Available: http://dx.doi.org/10.1109/ICDE.2007.367856", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "On the tradeoff between privacy and utility in data publishing", "author": ["T. Li", "N. Li"], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Paris, France, June 28 - July 1, 2009, 2009, pp. 517\u2013526. [Online]. Available: http://doi.acm.org/10.1145/1557019.1557079", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Slicing: A new approach for privacy preserving data publishing", "author": ["T. Li", "N. Li", "J. Zhang", "I. Molloy"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 24, no. 3, pp. 561\u2013574, 2012. [Online]. Available: http://doi.ieeecomputersociety.org/10.1109/TKDE.2010.236", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "l-diversity: Privacy beyond k-anonymity", "author": ["A. Machanavajjhala", "J. Gehrke", "D. Kifer", "M. Venkitasubramaniam"], "venue": "Proceedings of the 22nd IEEE International Conference on Data Engineering (ICDE 2006), Atlanta Georgia, Apr. 2006. [Online]. Available: http://dx.doi.org/10.1109/ICDE.2006.1", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Instance-based learning : Nearest neighbor with generalization", "author": ["B. Martin"], "venue": "Tech. Rep., 1995.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1995}, {"title": "Controlled data-swapping techniques for masking public use microdata sets", "author": ["R.A. Moore", "Jr."], "venue": "U.S. Bureau of the Census, Washington, DC., Statistical Research Division Report Series RR 96-04, 1996. [Online]. Available: http://www.census.gov/srd/papers/pdf/rr96-4.pdf", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "\u03b4-presence without complete world knowledge", "author": ["M.E. Nergiz", "C. Clifton"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 22, no. 6, pp. 868\u2013883, Jun. 2010. [Online]. Available: http://doi.ieeecomputersociety.org/10.1109/TKDE.2009.125", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Protecting respondent\u2019s privacy in microdata release", "author": ["P. Samarati"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 13, no. 6, pp. 1010\u20131027, Nov./Dec. 2001. [Online]. Available: http://dx.doi.org/10.1109/69.971193", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "k-anonymity: a model for protecting privacy", "author": ["L. Sweeney"], "venue": "International Journal on Uncertainty, Fuzziness and Knowledgebased Systems, no. 5, pp. 557\u2013570, 2002. [Online]. Available: http://dx.doi.org/10.1142/S0218488502001648", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Introduction to Data Mining, (First Edition)", "author": ["P.-N. Tan", "M. Steinbach", "V. Kumar"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations", "author": ["I.H. Witten", "E. Frank"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1999}, {"title": "Minimality attack in privacy preserving data publishing", "author": ["R.C.-W. Wong", "A.W.-C. Fu", "K. Wang", "J. Pei"], "venue": "VLDB, 2007, pp. 543\u2013554.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "\u03b1, k)-anonymity: An enhanced k-anonymity model for privacy preserving data publishing", "author": ["R.C.-W. Wong", "J. Li", "A.W.-C. Fu", "K. Wang"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201906. New York, NY, USA: ACM, 2006, pp. 754\u2013759. [Online]. Available: http://doi.acm.org/10.1145/1150402.1150499", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Anatomy: Simple and effective privacy preservation", "author": ["X. Xiao", "Y. Tao"], "venue": "Proceedings of 32nd International Conference on Very Large Data Bases (VLDB 2006), Seoul, Korea, Sep. 12-15 2006. [Online]. Available: http://www.vldb.org/conf/2006/p139-xiao.pdf", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning accurate and concise na\u0131\u0308ve bayes classifiers from attribute value taxonomies and data", "author": ["J. Zhang", "D.-K. Kang", "A. Silvescu", "V. Honavar"], "venue": "Knowledge and Information Systems, vol. 9, no. 2, pp. 157\u2013179, 2006.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 26, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 96, "endOffset": 100}, {"referenceID": 30, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 138, "endOffset": 142}, {"referenceID": 29, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 155, "endOffset": 159}, {"referenceID": 35, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 177, "endOffset": 181}, {"referenceID": 28, "context": "Other alternatives include value swapping [29], distortion [2], randomization [14], and noise addition (e.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "Other alternatives include value swapping [29], distortion [2], randomization [14], and noise addition (e.", "startOffset": 59, "endOffset": 62}, {"referenceID": 13, "context": "Other alternatives include value swapping [29], distortion [2], randomization [14], and noise addition (e.", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": ", differential privacy [13]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "Generalization consists of replacing identifying attribute values with a less specific version [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "Suppression can be viewed as the ultimate generalization, replacing the identifying value with an \u201cany\u201d value [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 36, "context": "Xiao and Tao proposed anatomization as a method to enforce l-diversity while preserving specific data values [37].", "startOffset": 109, "endOffset": 113}, {"referenceID": 6, "context": "The more general approach of fragmentation [7] divides a given dataset\u2019s attributes into two sets of attributes (2 partitions) such that an encryption mechanism avoids associations between two different small partitions.", "startOffset": 43, "endOffset": 46}, {"referenceID": 10, "context": "extend fragmentation to multiple partitions [11], and Tamas et al.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "propose an extension that deals with multiple sensitive attributes [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "There is concern that anatomization is vulnerable to several attacks [20], [23], [26].", "startOffset": 69, "endOffset": 73}, {"referenceID": 22, "context": "There is concern that anatomization is vulnerable to several attacks [20], [23], [26].", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "There is concern that anatomization is vulnerable to several attacks [20], [23], [26].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "While this can be an issue, any method that provides meaningful utility fails to provide perfect privacy against a sufficiently strong adversary [13], [25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "While this can be an issue, any method that provides meaningful utility fails to provide perfect privacy against a sufficiently strong adversary [13], [25].", "startOffset": 151, "endOffset": 155}, {"referenceID": 7, "context": "g, minimality [8], [35].", "startOffset": 14, "endOffset": 17}, {"referenceID": 34, "context": "g, minimality [8], [35].", "startOffset": 19, "endOffset": 23}, {"referenceID": 22, "context": "attribute, a task that was found to be #P-complete by Kifer [23].", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "already gives a practical applications of such a learning scenario [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "In case of nearest neighbor classifier (1-NN), we also make an additional comparison to generalization based learning scheme [21].", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "Nested generalization and non-nested hyperrectangles were used to generalize the data from which the nearest neighbor classifiers were trained [28].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "Taylor approximation was used to estimate the Euclidean distance from the anonymized training data [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 37, "context": "studied Na\u0131\u0308ve Bayes using partially specified training data [38], proposing a conditional likehoods computation algorithm exploring the instance space of attribute-value generalization taxonomies.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "5 decision tree classifier was trained [1].", "startOffset": 39, "endOffset": 42}, {"referenceID": 21, "context": "5 decision tree classifier was trained from the optimally generalized training data [22].", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "A new scoring function was proposed for the calculation of decision tree splits from the compressed training data [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 11, "context": "5 decision tree classifier was learned [12].", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "Given the former definitions, we will next define the anonymized training data following the definition of kanonymity [32].", "startOffset": 118, "endOffset": 122}, {"referenceID": 31, "context": "Definition 5: A training data D that satisfies the following conditions is said to be anonymized training data Dk [32]:", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "This requires the definition of groups [27].", "startOffset": 39, "endOffset": 43}, {"referenceID": 36, "context": "that is originally based on l-diverse groups [37].", "startOffset": 45, "endOffset": 49}, {"referenceID": 32, "context": "Although anatomization requires a discrete probability distribution for the sensitive attribute As, such smoothness violation is negligible since the original k-NN classifier is known to fit well on discrete training data [33].", "startOffset": 222, "endOffset": 226}, {"referenceID": 14, "context": "The anatomized k-NN cases where k > 1 and k is even will be ignored, because such cases include the tie between k-nearest neighbors that makes the bounds ambiguous and complicated [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 8, "context": "The total number of attributes are assumed to be d + 1 (d identifying attributes and 1 sensitive attribute) and all instances are assumed to be in a separable metric space M \u2282 R as in [9], [10], [15].", "startOffset": 184, "endOffset": 187}, {"referenceID": 9, "context": "The total number of attributes are assumed to be d + 1 (d identifying attributes and 1 sensitive attribute) and all instances are assumed to be in a separable metric space M \u2282 R as in [9], [10], [15].", "startOffset": 189, "endOffset": 193}, {"referenceID": 14, "context": "The total number of attributes are assumed to be d + 1 (d identifying attributes and 1 sensitive attribute) and all instances are assumed to be in a separable metric space M \u2282 R as in [9], [10], [15].", "startOffset": 195, "endOffset": 199}, {"referenceID": 14, "context": "When Xj \u223c= X hold for all Xj \u2208 X \u2032 N (k), we denote the error rate by R (X) in Equation 1 [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "2 computes R(X) [15].", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "using 1, 2 [9].", "startOffset": 11, "endOffset": 14}, {"referenceID": 14, "context": "We exclude this derivation due to space limitations, but the derivation follows from the original k-NN classifier analysis in [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 3, "context": "Since it is impossible to know the exact Bayesian error, many Bayesian error estimation techniques were suggested [4], [10], [15].", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "Since it is impossible to know the exact Bayesian error, many Bayesian error estimation techniques were suggested [4], [10], [15].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "Since it is impossible to know the exact Bayesian error, many Bayesian error estimation techniques were suggested [4], [10], [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 3, "context": "Although such estimation would be very interesting for multilabel classification, the theoretical analysis on unprotected data only covers binary classification [4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 14, "context": "The Parzen density estimation approach, which is easier to derive than the k nearest neighbor density estimation approach, will follow Fukunaga [15] and Fukunaga et al.", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Both approaches show the same behavior in terms of the Bayesian estimation that makes the discussion general enough for any non-parametric density based binary classification method [15].", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "Let P\u0302Ai(X) and P\u0302i(X) be the Parzen density estimations; and K(\u2217) be the kernel function for D with shape matrix A and size/volume parameter r [15].", "startOffset": 144, "endOffset": 148}, {"referenceID": 14, "context": "In 7, the terms other than R stand for the expected estimation error E[\u2206R A] in 8 [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "From Fukunaga [15], we know that E{\u2206hA(X)} and E{\u2206hA(X)} are expressed in function of the E{P\u0302Ai(X)} and V ar{P\u0302Ai(X)}.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "The former equality is the result of using Parzen density estimate [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "The discussion here won\u2019t be generalized to the anatomized k-NN classifier since the finite size training data performance of k-NN classifiers are not generalized to k > 2 in the pattern recognition literature [10], [15].", "startOffset": 210, "endOffset": 214}, {"referenceID": 14, "context": "The discussion here won\u2019t be generalized to the anatomized k-NN classifier since the finite size training data performance of k-NN classifiers are not generalized to k > 2 in the pattern recognition literature [10], [15].", "startOffset": 216, "endOffset": 220}, {"referenceID": 14, "context": "[15], [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[15], [17].", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "The rest of the proof follows Fukunaga [15].", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "Using 18, expectation with respect to X in 17 (step 3) according to Fukunaga [15] results in 13.", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "We evaluate the anatomized k-NN classifier using cross validation on the Adult, Bank Marketing, IPUMS datasets from UCI collection [5] and on the Fatality (fars) dataset from Keel repository [3].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "We evaluate the anatomized k-NN classifier using cross validation on the Adult, Bank Marketing, IPUMS datasets from UCI collection [5] and on the Fatality (fars) dataset from Keel repository [3].", "startOffset": 191, "endOffset": 194}, {"referenceID": 33, "context": "The instances with missing values were removed and features selected using the Pearson correlation filter (CfsSubsetEval) of Weka [34].", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "[21]) was used to implement the k-NN classifier [34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[21]) was used to implement the k-NN classifier [34].", "startOffset": 48, "endOffset": 52}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "are provided [33]", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "\u2019s work considers only the anonymized 1-NN classifier [21].", "startOffset": 54, "endOffset": 58}], "year": 2016, "abstractText": "This paper analyzes k nearest neighbor classification with training data anonymized using anatomy. Anatomy preserves all data values, but introduces uncertainty in the mapping between identifying and sensitive values. We first study the theoretical effect of the anatomized training data on the k nearest neighbor error rate bounds, nearest neighbor convergence rate, and Bayesian error. We then validate the derived bounds empirically. We show that 1) Learning from anatomized data approaches the limits of learning through the unprotected data (although requiring larger training data), and 2) nearest neighbor using anatomized data outperforms nearest neighbor on generalization-based anonymization.", "creator": "LaTeX with hyperref package"}}}