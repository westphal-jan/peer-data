{"id": "1610.06449", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features", "abstract": "This paper hacks presents grub a szepes novel fixation prediction and nemesis saliency 2.80 modeling framework based on racketeer inter - image similarities and ensemble euro332 of justina Extreme waisted Learning 56.89 Machines (markow ELM ). The maffeo proposed framework sriprakash is inspired rydell by two observations, 35.59 1) 32.12 the contextual registrants information monie of white-bellied a bhavana scene along sheltie with low - level visual verena cues malberg modulates nontotient attention, pittoresque 2) wanderlei the orderic influence flinn of scene craic memorability on mahendradatta eye marois movement patterns kalanaur caused by the losheng resemblance of valiante a scene khadzhikurbanov to luzira a nominalism former wrf visual experience. supra\u015bl Motivated bullwhip by prothonotary such observations, we develop a framework that estimates idaho the huffstutter saliency of arbon a smartmedia given weening image using j\u00e9sus an ensemble bpn of radicals extreme baranowski learners, torrez each lead-singer trained on tid an image similar to 123-year the input image. moguer That is, humar after rahf retrieving rizzle a wds set of breadth-first similar images for moretti a given image, a saliency predictor salpeter is commemmorate learnt granit from each 4,957 of the xiannian images in froedtert the retrieved prodigal image tecnologico set using pericardial an ELM, wadel resulting surefooted in ejido an jind\u0159ich\u016fv ensemble. The arcade-style saliency of nutshell the given nyingmapa image swagger is reichswehr then ski-orienteering measured anosov in osovnikar terms of the mean lettersnytimes.com of chandran predicted saliency chavira value tendo by the affligem ensemble ' s 308th members.", "histories": [["v1", "Thu, 20 Oct 2016 14:55:29 GMT  (8900kb,D)", "http://arxiv.org/abs/1610.06449v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hamed r -tavakoli", "ali borji", "jorma laaksonen", "esa rahtu"], "accepted": false, "id": "1610.06449"}, "pdf": {"name": "1610.06449.pdf", "metadata": {"source": "CRF", "title": "Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features", "authors": ["Hamed R.-Tavakolia", "Ali Borji", "Jorma Laaksonen", "Esa Rahtu"], "emails": ["hamed.r-tavakoli@aalto.fi"], "sections": [{"heading": null, "text": "This paper presents a novel fixation prediction and saliency modeling framework based on inter-image similarities and ensemble of Extreme Learning Machines (ELM). The proposed framework is inspired by two observations, 1) the contextual information of a scene along with low-level visual cues modulates attention, 2) the influence of scene memorability on eye movement patterns caused by the resemblance of a scene to a former visual experience. Motivated by such observations, we develop a framework that estimates the saliency of a given image using an ensemble of extreme learners, each trained on an image similar to the input image. That is, after retrieving a set of similar images for a given image, a saliency predictor is learnt from each of the images in the retrieved image set using an ELM, resulting in an ensemble. The saliency of the given image is then measured in terms of the mean of predicted saliency value by the ensemble\u2019s members.\nKeywords: Visual attention, saliency prediction, fixation prediction, inter-image similarity, extreme learning machines"}, {"heading": "1. Introduction", "text": "The fixation prediction, also known as saliency modeling, is associated with the estimation of a saliency map, the probability map of the locations an observer will be looking at for a long enough period of time meanwhile viewing a scene. It is part of the computational perspective of visual attention [1], the process of narrowing down the available visual information upon which to focus for enhanced processing.\n\u2217Corresponding author Email address: hamed.r-tavakoli@aalto.fi (Hamed R.-Tavakoli)\nar X\niv :1\n61 0.\n06 44\n9v 1\n[ cs\n.C V\n] 2\n0 O\nComputer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34]. In many of these applications, a saliency map can facilitate the selection of a subset of regions in a scene for elaborate analysis which reduces the computation complexity and improves energy efficiency [35].\nFrom a human centric point of view, the formation of a saliency map is not a pure bottom-up process and is influenced by several factors such as the assigned task, level of expertise of the observer, scene familiarity, and memory. It is shown that human relies on the prior knowledge about the scene and long-term memory as crucial components for construction and maintenance of scene representation [36]. In a similar vein, [37] suggests that an abstract visual representation can be retained in memory upon a short exposure to a scene and this representation influences eye movements later.\nThe study of the role of scene memory in guiding eye movements in a natural experience entailing prolonged immersion in three-dimensional environments [38] suggests that observers learn the location of objects over time and use a spatial-memory-guided search scheme to locate them. These findings have been the basis of research for measuring memorability of scenes from pure observer eye movements [39, 32], that is similar images have alike eye movement patterns and statistics. Inspired by the findings of [37, 36, 38] and scene memorability research, we incorporate the similarity of images as an influencing factor in fixation prediction.\nBesides the fact that similar images may induce similar eye movement patterns due to memory recall, it is well agreed that the interaction of low-level visual cues (e.g., edges, color, etc.) affect saliency formation [40] and contextual information of a scene can modulate the saliency map [41, 42]. Imagine that you are watching two pairs of images, a pair of street scene and a pair of nature beach images, meanwhile having your eye movements recorded. It is not surprising to find similar salient regions for the images of alike scenes because similar low-level cues and contextual data are mostly present in each pair. Figure 1 depicts examples of such a scenario. In the case of street scene, the observers tend to converge to the traffic sings, while they tend to spot low-level structural information in beach images. This further motivates us to exploit learning saliency from inter-image similarities.\nThis paper presents a novel fixation prediction algorithm based on interimage similarities and an ensemble of saliency learners using features from deep convolutional neural networks. To meet this end, we first investigate the benefits from inter-image similarities for fixation prediction. Then, we introduce 1) an image similarity metric using gist descriptor [41] and classemes [43], 2) a fixation prediction algorithm, using an ensemble of extreme learning machines, where for a given image, each member of the ensemble is trained with an image similar\nto the input image. We report the performance of the proposed framework on MIT saliency benchmarks [44], both MIT300 and CAT2000 databases1, along with evaluations on databases with publicly available ground-truth.\nIn the rest of this paper, we briefly review the related work. Afterwards, using a toy problem, we demonstrate the benefit from inter-image similarity. In section 4, we explain the proposed model. We then continue with the experiments to assess the performance of the model. The paper ends with discussion and conclusion remarks."}, {"heading": "2. Related work", "text": "The field of computer vision is replete with a numerous variety of saliency models. A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54]. There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc. Investigating the extent of saliency modeling approaches is beyond the scope of this article and readers are advised to consult relevant surveys [72, 73]. We, however, briefly review some of the most relevant techniques.\nLearning-based techniques are a large group of methods which are establishing a relation between a feature space and human fixations. For example, [74] uses a nonlinear transformation to associate image patches with human eye movement statistics. In [75], a linear SVM classifier is used to establish a relation between three channels of low- (intensity, color, etc), mid- (horizon line)\n1These databases have their ground-truth unavailable to public in order to provide a fair model evaluation. Thus, the scores are computed by MIT saliency research team using our submitted maps.\nand high-level (faces and people) features and human eye movements in order to produce a saliency map. In a similar vein, [76] employs multiple-instance learning. By learning a classifier, [77, 78] estimate the optimal weights for fusing several conspicuity maps from observers\u2019 eye movement data. These approaches often learn a probabilistic classifier to determine the probability of a feature being salient. Then, they employ the estimated saliency probability in order to build a saliency map.\nThe recent saliency modeling methods, akin to other computer vision techniques, are revolutionized and advanced significantly by applying deep Convolutional Neural Networks (CNN). There exists significant number of models that employ CNNs, of which many are relevant to the proposed model.\nEnsembles of Deep Networks (eDN) [79] adopts the neural filters learned during image classification task by deep neural networks and learns a classifier to perform fixation prediction. eDN can be considered an extension to [75] in which the features are obtained from layers of a deep neural network. For each layer of the deep neural network, eDN first learns the optimal blend of the neural responses of all the previous layers and the current layer by a guided hyperparameter search. Then, it concatenates the optimal blend of all the layers to form a feature vector for learning a linear SVM classifier.\nDeep Gaze I [80] utilizes CNNs for the fixation prediction task by treating saliency prediction as point processing. Despite this model is justified differently than [79] and [75], in practice, it boils down to the same framework. Nonetheless, the objective function to be minimized is slightly different due to the explicit incorporation of the center-bias factor and the imposed sparsity constraint in the framework. SalNet [81] is another technique that employs a CNN-based architecture, where the last layer is a deconvolution. The first convolution layers are initialized by the VGG16 [82] and the deconvolution is learnt by fine-tuning the architecture for fixation prediction.\nMultiresolution CNN (Mr-CNN) [83] designs a deep CNN-based technique to discriminate image patches centered on fixations from non-fixated image patches at multiple resolutions. It hence trains a convolutional neural network at each scale, which results in three parallel networks. The outputs of these networks are connected together through a common classification layer in order to learn the best resolution combination.\nSALICON [84] develops a model by fine-tuning the convolutional neural network, trained on ImageNet, using saliency evaluation metrics as objective functions. It feeds an image into a CNN architecture at two resolutions, coarse and fine. Then, the response of the last convolution layer is obtained for each scale. These responses are then concatenated together and are fed into a linear integration scheme, optimizing the Kullback-Leibler divergence between the network output and the ground-truth fixation maps in a regression setup. The error is back-propagated to the convolution layers for fine-tuning the network.\nThe proposed method can be considered a learning-based approach. While many of the learning-based techniques are essentially solving a classification problem, the proposed model has a regression ideology in mind. It is thus closer to the recent deep learning approaches that treat the problem as estimation of\na probability map in terms of a regression problem [81, 84, 85]. Nonetheless, it exploits an ensemble of extreme learning machines."}, {"heading": "3. Saliency benefits from inter-image similarity", "text": "The main motivation behind the proposed model is that people may have similar fixation patterns in exposure to alike images. In other words, inter-image saliency benefits saliency prediction. In order to investigate such an assertion, we build a toy problem to tell how well the saliency map of an image predicts saliency in a similar image.\nWe choose a common saliency database [75] and computed the gist [41] of the scene for each image. Afterwards, the most similar image pairs and the most dissimilar pairs were identified. For each image pair, we use the fixation density map of one as the predicted saliency map of the other. The assessment reveals that such a fixation prediction scheme produces significantly different (p \u2264 0.05) shuffled AUC scores [86] where the score of prediction using similar pairs is 0.54 and the score of prediction by dissimilar image pairs is 0.5. The results indicate that while there is a degree of prediction for similar pairs, the dissimilar pairs are not doing better than chance. We observe the same performance difference for other metrics such as correlation score (0.175 vs. 0.115) and normalized scanpath score (0.86 vs. 0.59). Given the above observation, we lay the foundation of our saliency model for fixation prediction."}, {"heading": "4. Saliency Model", "text": "A high-level conceptual schematic of our proposed model is depicted in Figure 2. The framework components include: 1) an image feature transform, 2) a similar image retrieval engine and a scene repository bank, and 3) an ensemble of neural saliency (fixation) predictors. The image feature transform performs the feature extraction and produces a pool of features used by the other units in the system. The similar image retrieval finds the top most similar images, stored in the scene bank, corresponding to a given image. It then retrieves the predictors trained using those images in order to facilitate the formation of the ensemble of saliency predictors. In the rest of this section, we explained the details of the mentioned components."}, {"heading": "4.1. Image feature transform", "text": "The image feature transform unit extracts several features from the image and feeds them forward to the other units. There has been a recent surge in the application of features learnt from image statistics and deep convolutional neural networks (CNNs) in a wide range of computer vision related applications. In this work, we adopt a filter-bank approach to the use of CNNs [87] for saliency prediction. We, thus, build an image pyramid and compute the CNNs\u2019 responses over each scale using the architecture of VGG16 [82] . To combine the convolution responses of each scale, we employ an upsampling procedure\nand concat the features from the last convolution layer of each scale in order to build a feature map.\nFurthermore, we compute the classemes [43] from deep pipeline, that is, the probability of each of the one thousand classes of ImageNet [88] is computed using the fully-connected layers of the VGG16. The classemes are complemented by the low-level scene representation to make the gist of the scene [9]. The classemes and low-level scene features of [41] build a spatial representation of the outside world that is rich enough to convey the meaning of a scene as envisioned in [89]. The feature vector obtained by concatenating classemes and gist features is used for the recognition and retrieval of similar images."}, {"heading": "4.2. Similar image retrieval & scene bank", "text": "The similar image retrieval unit fetches the information required for building an ensemble of neural predictors from the scene bank. The scene bank holds a set of images in terms of scene representation feature vector, consisting of classemes feature and the gist descriptor, and a neural fixation predictor unit for each image.\nGiven the scene representation vector of an input image, denoted as vq, the retrieval method fetches the most n similar images from the set of scene vectors, V = {v1, \u00b7 \u00b7 \u00b7 , vn\u2032}, using the Euclidean distance, that is, disti = \u2016vq \u2212 vi\u2016. It then fetches the neural fixation predictor units corresponding to the n\nimages with the smallest disti in order to form the ensemble of neural fixation predictors, to be discussed in Section 4.3.\nFigure 3 demonstrates the results of retrieval system. It visualizes a query image and its corresponding most similar retrieved image between two different databases with the observer gaze information overlaid. Interestingly, the retrieved images not only share similar objects and bottom-up structures, but can also have similar attention grabbing regions. It is worth noting that the closest scene is not necessarily of the same scene category, however, it often contains similar low-level and/or high-level perceptual elements."}, {"heading": "4.3. Saliency prediction", "text": "We define the saliency of an image in terms of features and locations, that is, Sal = p(y|x,m), where y corresponds to pixel level saliency, x represents image features and m is the location. Under the independence assumption, the saliency formulation boils down to the following:\nSal = p(y|x)p(y|m). (1) The p(y|x) corresponds to saliency prediction from image features and p(y|m)\nrepresents a spatial prior. We estimate p(y|x) using an ensemble of neural predictors and p(y|m) is learnt from human gaze information.\nFigure 4 depicts the ensemble of neural saliency predictors. The ensemble of neural predictors consists of several neural units with equal contributions. In training phase, we train one neural unit for each image in the training set and store them in the scene bank. In the test phase, the retrieval unit fetches several neural units, corresponding to the n images most similar to the input image. The ensemble, then, computes the responses of each of the units and aggregates them in order to produce an estimate of p(y|x), as follows:\np(y|x) = Q \u2211 j C(tanh(yj)) \u03b1 , (2)\nC(x) = { x x > 0 0 x \u2264 0 , (3)\nwhere Q(\u00b7) resizes image or salience data to the size of preference (the size of input image), \u03b1 is an attenuation factor to emphasize more salient areas, and yj is the output of the jth unit of the ensemble."}, {"heading": "4.3.1. Neural units", "text": "The neural saliency predictor utilizes randomly-weighted single-layer feedforward networks in order to establish a mapping from the feature space to the saliency space. The idea of randomly-weighted single-hidden-layer feedforward networks (SLFNs) can be traced back to the Gamba perceptron [91] followed by others like [92, 93]. In the neural saliency predictor, we adopt the recent implementation of Extreme Learning Machines (ELM) [94]. The theory of ELM facilitates the implementation of a neural network architecture such that the hidden layer weights can be chosen randomly meanwhile the output layer weights are determined analytically [95]. Motivated by better function approximation properties of ELMs [96, 97], we employ them as the primary entity of the neural saliency prediction.\nHaving a set of training samples {(xi,yi)}Ni=1 \u2282 Rk\u00d7 Rm, the image features xi and the corresponding fixation density value yi are associated using a SLFNs with L hidden nodes defined as\nyi = L\u2211 j=1 \u03b3 jf(\u03c9j \u00b7 xi + bj), (4)\nwhere f(\u00b7) is a nonlinear activation function, \u03b3 j \u2208 Rm is the output weight vector, \u03c9j \u2208 Rk is the input weight vector, and bj is the bias of the jth hidden node. The conventional solution to (4) is gradient-based, which is a slow iterative process that requires to tune all the parameters like \u03b3 j , \u03c9j and bj . The iterative scheme is prone to divergence, local minima, and overfitting. The\nELM tries to soften such problems and avoid them by random selection of the hidden layer parameters (\u03c9j and bj) and the estimation of output weights. To this end, (4) can be rewritten as\nY = H\u0393, (5)\nwhere Y = [y1 y2 . . . yN ] T \u2208 RN\u00d7m, \u0393 = [\u03b31 \u03b32 . . . \u03b3L]T \u2208 RL\u00d7m, and\nH =  f(\u03c91 \u00b7 x1 + b1) \u00b7 \u00b7 \u00b7 f(\u03c9L \u00b7 x1 + bL)... . . . ... f(\u03c91 \u00b7 xN + b1) \u00b7 \u00b7 \u00b7 f(\u03c9L \u00b7 xN + bL)  N\u00d7L , (6)\nwhich is the hidden layer matrix of the neural network. Once the matrix H is decided by random selection of input weights and biases, the solution of (5) can be approximated as \u0393 = H\u2020Y, where H\u2020 is the Moore-Penrose pseudoinverse of matrix H."}, {"heading": "4.3.2. Learning spatial prior", "text": "In order to learn the spatial prior, p(y|m), we fit a mixture of Gaussian over the eye fixation data. We learn the spatial prior using the gaze data of [90], where the number of kernels corresponds to the number of fixation points. The spatial prior puts more weight on the regions that are more agreed by observers. As demonstrated in many saliency research papers, the spatial prior introduces a center-bias effect [98]. The same phenomenon is observed in Figure 5, depicting the spatial prior. While there exist arguments on getting advantage of location priors, we address the issue by selecting proper evaluation metrics and benchmarks. It is also worth noting that we are not using summation prior integration, which generally boosts all the regions in the center of the image equally."}, {"heading": "5. Experiments", "text": "We conduct several experiments in order to evaluate the model. The test databases include MIT [75], MIT300 [99], and CAT2000 [100]. The MIT database consists of 1003 images of indoor and outdoor scenes with eye movements of 15 observers. MIT300 consists of 300 natural indoor and outdoor scenes and CAT2000 consists of 4000 images divided into two sets of train and test, with\n2000 images in each set. CAT2000 includes 20 categories of images, including, action, affective, art, black & white, cartoon, fractal, indoor, outdoor, inverted, jumbled, line drawings, low resolution, noisy, object, outdoor man made, outdoor natural, pattern, random, satellite, sketch, and social.\nMIT300 and CAT2000 (test set) do not allow the ground-truth access in order to provide a fair comparison. At the moment, they are the widely accepted benchmarks and the results presented are provided by the MIT saliency benchmark team using our submitted maps. The results of the proposed model are also accessible on the benchmark website2 under the acronym \u201ciSEEL\u201d.\nWe learn two ensembles, ensembleOSIE and ensembleCAT2k. The first is trained on the OSIE database [90] and the latter is trained using the training set of CAT2000. We employ ensembleCAT2k in predicting the CAT2000 test images. The system parameters are optimized for each ensemble.\nIn this section, we first explain the system parameters. We then evaluate the performance generalization of the proposed model in comparison with a baseline model using the MIT database. We continue with the Benchmark results on the MIT300 and the CAT2000 databases."}, {"heading": "5.1. System parameters", "text": "The system parameters are the number of neural units in each ensemble, denoted n, the number of hidden layers in each unit, L, and the attenuation factor,\u03b1. We furthermore learn a post processing smoothing Gaussian kernel, denoted as \u03c3, which is used to smooth the model\u2019s maps. All the parameters, except the number of hidden nodes are learnt. For each of the ensembles, the number of hidden nodes of each neural unit is fixed and equal to 20. The rest of the parameters of the system are optimized on Toronto database [56]. The tuning cost function minimizes the KL-divergence between the maps of the model and the ground-truth fixation density maps.\nFigure 6 depicts the effect of the number of neural units in conjunction with the value of the attenuation factor \u03b1 on the ensemble performance. Based on our observations, an ensemble of size 10 is required to obtain an acceptable result. The optimization of parameters, however, recommend the following parameters for each ensemble, ensembleOSIE : [n = 697, \u03b1 = 6, \u03c3 = 13] and ensembleCAT2k: [n = 1710, \u03b1 = 9, \u03c3 = 13], where L = 20 has been fixed."}, {"heading": "5.2. Performance generalization", "text": "To test the generalization of the model, we evaluate its performance using the MIT database [75]. We choose the ensemble of deep neural networks (eDN) [79] as a baseline model because of the use of deep features and SVM classifiers. The proposed model, however, utilizes an ensemble of ELM regression units. We also evaluate several models including, AIM [55], GBVS [69], AWS [101], Judd [75], and FES [51] for the sake of comparison with traditional models.\n2MIT saliency benchmark website:http://saliency.mit.edu/results_mit300.html\nIn order to ease the interpretation of evaluation, we choose a subset of scores that complement each other. We employ shuffled AUC (sAUC, an AUC metric that is robust towards center bias), similarity metric (SIM, a metric indicating how two distributions resemble each other [44]), and normalized scanpath saliency (NSS, a metric to measure consistency with human fixation locations). NSS and sAUC scores are utilized in [86], which we borrow part of the scores from, and complement them with the SIM score.\nFigure 7 reports the results. As depicted, the proposed model outperforms all other models on two metrics and outperforms the eDN on all the three metrics. The highest gain compared to the eDN is on the NSS score, indicating a high consistency with human fixation locations which explains the high SIM score as well. To summarize, the proposed model generalizes well and has the edge over traditional models. We later compare the proposed model with the recent state-of-the-art models on well-established benchmarks."}, {"heading": "5.3. Benchmark", "text": "Many of the recent deep saliency models have their codes and maps unavailable to public, making comparisons difficult. We, hence, rely on available benchmarks. We report the performance using all the metrics and published works, reported on the MIT benchmark. For brevity, the focus will be on recent top-performing models. The results also include the performance of \u201cInfinite Human\u201d and \u201cMean One Human\u201d to indicate how well a model performs in comparison with mean eye position of several human (upper-bound performance) and the on average performance of one human, respectively.\nResults on MIT300. Table 1 summarizes the performance comparison, where the proposed model is 4th among published works on this benchmark on the basis of NSS. MIT300 is the largest benchmark with over 60 models at the time of this writing. We, however, report the best performing models and the most recent state-of-the-art ones. The comparison indicates that the models are becoming powerful enough to capture fixation location. It is, hence, difficult to distinguish them from each other on many metrics. NSS, however, seems to be the most informative metric that determines the models\u2019 performance well, particularly for top-performing models that judging AUC-based metrics and Similarity-based metrics are difficult.\nResults on CAT2000. Table 2 contains the performance comparison on the CAT2000 database. 19 models, which are mostly traditional ones, are evaluated on this database. The proposed model, ensembleCAT2k, ranks similarly with BMS [103] at the top of the ranking. Both models produce the highest NSS score among models and on average have indistinguishable values for the AUC-based and the Similarity-based metrics.\nWe also evaluate ensembleOSIE along with ensembleCAT2k in order to further investigate the improvements caused by incorporating similar images in the training phase. Backing the hypothesis, the ensemble trained on CAT2000 outperforms the ensemble that is learnt from only indoor and outdoor images of OSIE in terms of the overall scores.\nWe look into the performance of the models in each of the twenty class categories of CAT2000 database. To be concise, we investigate ensembleCAT2k,\nensembleOSIE , and BMS, which are the top three best performing models, using the three metrics of shuffled AUC (sAUC), SIM, and NSS. The results are summarized in Figure 8. The proposed model, both ensembleCAT2k and ensembleOSIE , are outperforming the BMS on low resolution, noisy, outdoor, black & white, action, affective and social categories. The BMS seems performing better when there is no particular contextual information and more low-level feature interactions matter, e.g., fractal category, and pattern. The other categories are, however, more difficult to judge. Overall, it seems the three models can complement each other in the areas where one falls behind the others."}, {"heading": "6. Discussion & conclusion", "text": "We demonstrated the usefulness of scene similarity in predicting the saliency motivated by the effect of the familiarity of a scene on the observer\u2019s eye movements. The idea can, however, be easily extended to the utilization of observers\u2019 eye movements in task-specific models, where a model is trained for a specific task and experts\u2019 eye movements are incorporated. An expert approach for solving a specific task is different from that of a naive observer. Thus, we can consider the encoding of expert observers\u2019 eye movements as an implicit expert knowledge utilization, which can be handy in scenarios of scene analysis such as spotting object-specific anomalies from saliency maps in order to reduce the search time.\nWe introduced a saliency model with the motive of exploiting the effect of immediate scene recall on the human perception. The proposed model uses randomly-weighted neural networks as an ensemble architecture. It establishes a mapping from a feature space, consisting of deep features, to the saliency space. The saliency prediction relies only on the neural units corresponding to the images that are similar to the input image. The neural units are pretrained and stored in a scene bank from a handful of images. For each neural unit, the scene bank also stores a scene descriptor, consisting of classemes and gist descriptor. To find the similar images from scene bank, the proposed model employs the distance between the scene descriptor of the input image and neural units.\nThe proposed model was evaluated on several databases. The results were reported on two well-established benchmark databases by the MIT benchmark team, namely MIT300 and CAT2000. Among the published methods and on the basis of NSS, consistency with the locations of human fixation, the proposed method was ranked 4th and 1st (in conjunction with BMS) on MIT300 and CAT2000, respectively. The results indicate benefit from learning saliency from images similar to the input image. The code for the proposed model is available at: http://github.com/hrtavakoli/iseel."}, {"heading": "Acknowledgement", "text": "Hamed R.-Tavakoli and Jorma Laaksonen were supported by the Finnish Center of Excellence in Computational Inference Research (COIN). The authors would like to thank the MIT saliency benchmark team, particularly Zoya Bylinskii, for their quick response on benchmark request."}], "references": [{"title": "A Computational Perspective on Visual Attention", "author": ["J.K. Tsotsos"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Vocus: A visual attention system for object detection and goal-directed search, Ph.D", "author": ["S. Frintrop"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "A selective attention-based method for visual pattern recognition with application to handwritten digit recognition and face recognition", "author": ["A. Salah", "E. Alpaydin", "L. Akarun"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Rapid biologically-inspired scene classification using features shared with visual attention", "author": ["C. Siagian", "L. Itti"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition", "author": ["D. Gao", "S. Han", "N. Vasconcelos"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Robust classification of objects, faces, and flowers using natural image statistics", "author": ["C. Kanan", "G. Cottrell"], "venue": "in: CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "A trainable system for object detection", "author": ["C. Papageorgiou", "T. Poggio"], "venue": "Int. J. Comput. Vision", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Hierarchical part-based visual object categorization", "author": ["G. Bouchard", "B. Triggs"], "venue": "in: CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "Int. J. Comput. Vision", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Contextual priming for object detection", "author": ["A. Torralba"], "venue": "Int. J. Comput. Vision", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Modeling search for people", "author": ["K. Ehinger", "B. Hidalgo-Sotelo", "A. Torralba", "A. Oliva"], "venue": "scenes, Vis. Cogn", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Attentive object detection using an information theoretic saliency measure", "author": ["G. Fritz", "C. Seifert", "L. Paletta", "H. Bischof"], "venue": "in: WAPCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Second-generation image-coding techniques", "author": ["M. Kunt", "A. Ikonomopoulos", "M. Kocher"], "venue": "Proc. IEEE", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Saliency-based multifoveated mpeg compression", "author": ["N. Dhavale", "L. Itti"], "venue": "in: ISSPA,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression", "author": ["C. Guo", "L. Zhang"], "venue": "IEEE Trans. Img. Proc", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Saliency-based discriminant tracking", "author": ["V. Mahadevan", "N. Vasconcelos"], "venue": "in: CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "General object tracking with a component-based target descriptor", "author": ["S. Frintrop"], "venue": "in: ICRA,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Adaptive object tracking by learning background context", "author": ["A. Borji", "S. Frintrop", "D. Sihite", "L. Itti"], "venue": "in: CVPRW,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Active segmentation with fixation", "author": ["A. Mishra", "Y. Aloimonos", "C.L. Fah"], "venue": "in: CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Saliency cuts: An automatic approach to object segmentation", "author": ["Y. Fu", "J. Cheng", "Z. Li", "H. Lu"], "venue": "in: ICPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Salient object detection: From pixels to segments, Image and Vision Comput", "author": ["V. Yanulevskaya", "J. Uijlings", "J.-M. Geusebroek"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "A generic virtual content insertion system based on visual attention analysis", "author": ["H. Liu", "S. Jiang", "Q. Huang", "C. Xu"], "venue": "in: ACM MM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Do predictions of visual perception aid design", "author": ["R. Rosenholtz", "A. Dorai", "R. Freeman"], "venue": "ACM Trans. Appl. Percept", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Does where you gaze on an image affect your perception of quality? applying visual attention to image quality", "author": ["A. Ninassi", "O.L. Meur", "P.L. Callet", "D. Barba"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Saliency-based image quality assessment criterion", "author": ["Q. Ma", "L. Zhang"], "venue": "in: ICIC,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Spatiotemporal saliency in dynamic scenes", "author": ["V. Mahadevan", "N. Vasconcelos"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Heikkil, Temporal saliency for fast motion detection", "author": ["H.R.-Tavakoli", "J.E. Rahtu"], "venue": "in: ACCV workshops,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Memorability of natural scenes: The role of attention", "author": ["M. Mancas", "O.L. Meur"], "venue": "in: ICIP,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Optimal scanning for faster object detection", "author": ["N. Butko", "J.R. Movellan"], "venue": "in: CVPR,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Performance evaluation of neuromorphic-vision object recognition", "author": ["R. Kasturi", "D. Goldgof", "R. Ekambaram", "R. Sharma", "G. Pratt", "M. Anderson", "M. Peot", "M. Aguilar", "E. Krotkov", "D. Hackett", "D. Khosla", "Y. Chen", "K. Kim", "Y. Ran", "Q. Zheng", "L. Elazary", "R. Voorhies", "D. Parks", "L. Itti"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Accurate visual memory for previously attended objects in natural scenes", "author": ["A. Hollingworth", "J.M. Henderson"], "venue": "J. Exp. Psychol. Hum. Percept. Perform", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "Initial scene representations facilitate eye movement guidance in visual search", "author": ["M. Castelhano", "J. Henderson"], "venue": "J. Exp. Psychol. Hum. Percept. Perform", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Recognition of visual memory recall processes using eye movement analysis", "author": ["A. Bulling", "D. Roggen"], "venue": "in: UbiComp,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "Gelade, A feature-integration theory of attention, Cognitive Psychol", "author": ["G.A.M. Treisman"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1980}, {"title": "Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search", "author": ["A. Torralba", "A. Oliva", "M. Castelhano", "J. Henderson"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Efficient object category recognition using classemes", "author": ["L. Torresani", "M. Szummer", "A. Fitzgibbon"], "venue": "in: ECCV,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "A benchmark of computational models of saliency to predict human fixations", "author": ["T. Judd", "F. Durand", "A. Torralba"], "venue": "Massachusetts institute of technology", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "A model of saliency-based visual attention for rapid scene analysis", "author": ["L. Itti", "C. Koch", "E. Niebur"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1998}, {"title": "Biologically motivated vergence control system using human-like selective attention model, Neurocomputing", "author": ["S.-B. Choi", "B.-S. Jung", "S.-W. Ban", "H. Niitsuma", "M. Lee"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "A coherent computational approach to model bottom-up visual attention", "author": ["O. Le Meur", "P. Le Callet", "D. Barba", "D. Thoreau"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2006}, {"title": "Saliency estimation using a non-parametric low-level vision", "author": ["N. Murray", "M. Vanrell", "X. Otazu", "C. Parraga"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "The discriminant center-surround hypothesis for bottom-up saliency", "author": ["D. Gao", "V. Mahadevan", "N. Vasconcelos"], "venue": "in: NIPS,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2007}, {"title": "Nonparametric bottom-up saliency detection by self-resemblance", "author": ["H.J. Seo", "P. Milanfar"], "venue": "in: CVPR,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2009}, {"title": "Heikkil, Fast and efficient saliency detection using sparse sampling and kernel density estimation", "author": ["H.R.-Tavakoli", "J.E. Rahtu"], "venue": "in: SCIA,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}, {"title": "Visual saliency by selective contrast, IEEE Transactions on Circuits and Systems for Video Technology", "author": ["Q. Wang", "Y. Yuan", "P. Yan"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2013}, {"title": "Computational attention: Towards attentive computers", "author": ["M. Mancas"], "venue": "Ph.D. thesis, CIACO University", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2007}, {"title": "Dynamic visual attention: searching for coding length increments", "author": ["X. Hou", "L. Zhang"], "venue": "in: NIPS,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2008}, {"title": "Visual saliency based on conditional entropy", "author": ["Y. Li", "Y. Zhou", "J. Yan", "Z. Niu", "J. Yang"], "venue": "in: ACCV,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2010}, {"title": "Incremental sparse saliency detection", "author": ["Y. Li", "Y. Zhou", "L. Xu", "X. Yang", "J. Yang"], "venue": "in: ICIP,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2009}, {"title": "Saliency detection: A spectral residual approach", "author": ["X. Hou", "L. Zhang"], "venue": "in: CVPR,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2007}, {"title": "Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform", "author": ["C. Guo", "Q. Ma", "L. Zhang"], "venue": "in: CVPR,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2008}, {"title": "Biological plausibility of spectral domain approach for spatiotemporal visual saliency", "author": ["P. Bian", "L. Zhang"], "venue": "in: ICONIP,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2008}, {"title": "Visual saliency: a biologically plausible contourlet-like frequency domain approach, Cogn. Neurodyn", "author": ["P. Bian", "L. Zhang"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2010}, {"title": "Saliency detection based on frequency and spatial domain analyses", "author": ["J. Li", "M. Levine", "X. An", "H. He"], "venue": "in: BMVC,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2011}, {"title": "Visual saliency based on scalespace analysis in the frequency domain", "author": ["J. Li", "M. Levine", "X. An", "X. Xu", "H. He"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2013}, {"title": "Predicting human gaze using quaternion dct image signature saliency and face detection", "author": ["B. Schauerte", "R. Stiefelhagen"], "venue": "in: WACV,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2012}, {"title": "Graph-based visual saliency", "author": ["J. Harel", "C. Koch", "P. Perona"], "venue": "in: NIPS,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2007}, {"title": "Random walks on graphs to model saliency in images", "author": ["V. Gopalakrishnan", "Y. Hu", "D. Rajan"], "venue": "in: CVPR,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2009}, {"title": "Measuring visual saliency by site entropy rate", "author": ["W. Wang", "Y. Wang", "Q. Huang", "W. Gao"], "venue": "in: CVPR,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2010}, {"title": "Computational versus psychophysical bottom-up image saliency: A comparative evaluation study", "author": ["A. Toet"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2011}, {"title": "State-of-the-art in visual attention modeling", "author": ["A. Borji", "L. Itti"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2013}, {"title": "Learning to predict where humans look", "author": ["T. Judd", "K. Ehinger", "F. Durand", "A. Torralba"], "venue": "in: ICCV,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2009}, {"title": "Saliency detection by multiple-instance learning", "author": ["Q. Wang", "Y. Yuan", "P. Yan", "X. Li"], "venue": "IEEE Trans. Cybern", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2013}, {"title": "Large-scale optimization of hierarchical features for saliency prediction in natural images", "author": ["E. Vig", "M. Dorr", "D. Cox"], "venue": "in: CVPR,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2014}, {"title": "Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet", "author": ["M. K\u00fcmmerer", "L. Theis", "M. Bethge"], "venue": "in: ICLR Workshop,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2015}, {"title": "Shallow and deep convolutional networks for saliency prediction", "author": ["J. Pan", "K. McGuinness", "E. Sayrol", "N. O\u2019Connor", "X. Giro-i Nieto"], "venue": null, "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2016}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "in: ICLR,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2015}, {"title": "Predicting eye fixations using convolutional neural networks", "author": ["N. Liu", "J. Han", "D. Zhang", "S. Wen", "T. Liu"], "venue": "in: CVPR,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2015}, {"title": "Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks", "author": ["X. Huang", "C. Shen", "X. Boix", "Q. Zhao"], "venue": "in: ICCV,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2015}, {"title": "End-to-end saliency mapping via probability distribution prediction", "author": ["S. Jetley", "N. Murray", "E. Vig"], "venue": null, "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2016}, {"title": "Analysis of scores, datasets, and models in visual saliency prediction", "author": ["A. Borji", "H.R.-Tavakoli", "D.N. Sihite", "L. Itti"], "venue": "in: ICCV,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2013}, {"title": "Deep filter banks for texture recognition and segmentation", "author": ["M. Cimpoi", "S. Maji", "A. Vedaldi"], "venue": "in: CVPR,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: NIPS,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2012}, {"title": "Predicting human gaze beyond", "author": ["J. Xu", "M. Jiang", "S. Wang", "M.S. Kankanhalli", "Q. Zhao"], "venue": "pixels, J. Vis", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2014}, {"title": "Perceptrons: an intorduction to computational geometry", "author": ["M. Minsky", "S. Papert"], "venue": null, "citeRegEx": "91", "shortCiteRegEx": "91", "year": 1969}, {"title": "Feedforward neural networks with random weights", "author": ["W. Schmidt", "M. Kraaijveld", "R. Duin"], "venue": "in: ICPR,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 1992}, {"title": "Extreme learning machine: a new learning scheme of feedforward neural networks", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "in: IJCNN,", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2004}, {"title": "Extereme learning machine: Theory and applicatons, Neurocomput", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": null, "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2006}, {"title": "Real-time learning capability of neural netwroks", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "IEEE Trans. Neural Netw", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2006}, {"title": "The prominence of behavioural biases in eye", "author": ["B.W. Tatler", "B.T. Vincent"], "venue": "guidance, Vis. Cogn", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2009}, {"title": "Mit saliency benchmark (Jul 2016)", "author": ["Z. Bylinskii", "T. Judd", "A. Borji", "L. Itti", "F. Durand", "A. Oliva", "A. Torralba"], "venue": null, "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2016}, {"title": "Cat2000: A large scale fixation dataset for boosting saliency research", "author": ["A. Borji", "L. Itti"], "venue": "in: CVPR workshops,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2015}, {"title": "Decorrelation and distinctiveness provide with human-like saliency", "author": ["A. Garcia-Diaz", "X. Fdez-Vidal", "X. Pardo", "R. Dosil"], "venue": "in: ACIVS,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2009}, {"title": "A deep multi-level network for saliency prediction", "author": ["M. Cornia", "L. Baraldi", "G. Serra", "R. Cucchiara"], "venue": null, "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2016}, {"title": "Sclaroff, saliency detection: a Boolean map approach", "author": ["S.J. Zhang"], "venue": "in: ICCV,", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "It is part of the computational perspective of visual attention [1], the process of narrowing down the available visual information upon which to focus for enhanced processing.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 173, "endOffset": 188}, {"referenceID": 2, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 173, "endOffset": 188}, {"referenceID": 3, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 173, "endOffset": 188}, {"referenceID": 4, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 173, "endOffset": 188}, {"referenceID": 5, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 173, "endOffset": 188}, {"referenceID": 6, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 7, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 8, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 9, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 10, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 11, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 12, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 235, "endOffset": 251}, {"referenceID": 13, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 235, "endOffset": 251}, {"referenceID": 14, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 235, "endOffset": 251}, {"referenceID": 15, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 262, "endOffset": 278}, {"referenceID": 16, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 262, "endOffset": 278}, {"referenceID": 17, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 262, "endOffset": 278}, {"referenceID": 18, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 293, "endOffset": 305}, {"referenceID": 19, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 293, "endOffset": 305}, {"referenceID": 20, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 293, "endOffset": 305}, {"referenceID": 21, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 344, "endOffset": 348}, {"referenceID": 22, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 371, "endOffset": 375}, {"referenceID": 23, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 402, "endOffset": 410}, {"referenceID": 24, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 402, "endOffset": 410}, {"referenceID": 25, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 456, "endOffset": 468}, {"referenceID": 26, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 456, "endOffset": 468}, {"referenceID": 27, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 489, "endOffset": 493}, {"referenceID": 28, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 512, "endOffset": 520}, {"referenceID": 29, "context": "In many of these applications, a saliency map can facilitate the selection of a subset of regions in a scene for elaborate analysis which reduces the computation complexity and improves energy efficiency [35].", "startOffset": 204, "endOffset": 208}, {"referenceID": 30, "context": "It is shown that human relies on the prior knowledge about the scene and long-term memory as crucial components for construction and maintenance of scene representation [36].", "startOffset": 169, "endOffset": 173}, {"referenceID": 31, "context": "In a similar vein, [37] suggests that an abstract visual representation can be retained in memory upon a short exposure to a scene and this representation influences eye movements later.", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "These findings have been the basis of research for measuring memorability of scenes from pure observer eye movements [39, 32], that is similar images have alike eye movement patterns and statistics.", "startOffset": 117, "endOffset": 125}, {"referenceID": 27, "context": "These findings have been the basis of research for measuring memorability of scenes from pure observer eye movements [39, 32], that is similar images have alike eye movement patterns and statistics.", "startOffset": 117, "endOffset": 125}, {"referenceID": 31, "context": "Inspired by the findings of [37, 36, 38] and scene memorability research, we incorporate the similarity of images as an influencing factor in fixation prediction.", "startOffset": 28, "endOffset": 40}, {"referenceID": 30, "context": "Inspired by the findings of [37, 36, 38] and scene memorability research, we incorporate the similarity of images as an influencing factor in fixation prediction.", "startOffset": 28, "endOffset": 40}, {"referenceID": 33, "context": ") affect saliency formation [40] and contextual information of a scene can modulate the saliency map [41, 42].", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": ") affect saliency formation [40] and contextual information of a scene can modulate the saliency map [41, 42].", "startOffset": 101, "endOffset": 109}, {"referenceID": 34, "context": "Then, we introduce 1) an image similarity metric using gist descriptor [41] and classemes [43], 2) a fixation prediction algorithm, using an ensemble of extreme learning machines, where for a given image, each member of the ensemble is trained with an image similar", "startOffset": 71, "endOffset": 75}, {"referenceID": 35, "context": "Then, we introduce 1) an image similarity metric using gist descriptor [41] and classemes [43], 2) a fixation prediction algorithm, using an ensemble of extreme learning machines, where for a given image, each member of the ensemble is trained with an image similar", "startOffset": 90, "endOffset": 94}, {"referenceID": 36, "context": "We report the performance of the proposed framework on MIT saliency benchmarks [44], both MIT300 and CAT2000 databases, along with evaluations on databases with publicly available ground-truth.", "startOffset": 79, "endOffset": 83}, {"referenceID": 33, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 73, "endOffset": 77}, {"referenceID": 37, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 1, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 38, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 39, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 40, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 41, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 42, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 43, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 44, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 45, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 70, "endOffset": 94}, {"referenceID": 46, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 70, "endOffset": 94}, {"referenceID": 47, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 70, "endOffset": 94}, {"referenceID": 48, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 70, "endOffset": 94}, {"referenceID": 49, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 50, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 14, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 51, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 52, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 53, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 54, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 55, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 56, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 195, "endOffset": 207}, {"referenceID": 57, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 195, "endOffset": 207}, {"referenceID": 58, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 195, "endOffset": 207}, {"referenceID": 59, "context": "Investigating the extent of saliency modeling approaches is beyond the scope of this article and readers are advised to consult relevant surveys [72, 73].", "startOffset": 145, "endOffset": 153}, {"referenceID": 60, "context": "Investigating the extent of saliency modeling approaches is beyond the scope of this article and readers are advised to consult relevant surveys [72, 73].", "startOffset": 145, "endOffset": 153}, {"referenceID": 61, "context": "In [75], a linear SVM classifier is used to establish a relation between three channels of low- (intensity, color, etc), mid- (horizon line)", "startOffset": 3, "endOffset": 7}, {"referenceID": 62, "context": "In a similar vein, [76] employs multiple-instance learning.", "startOffset": 19, "endOffset": 23}, {"referenceID": 63, "context": "Ensembles of Deep Networks (eDN) [79] adopts the neural filters learned during image classification task by deep neural networks and learns a classifier to perform fixation prediction.", "startOffset": 33, "endOffset": 37}, {"referenceID": 61, "context": "eDN can be considered an extension to [75] in which the features are obtained from layers of a deep neural network.", "startOffset": 38, "endOffset": 42}, {"referenceID": 64, "context": "Deep Gaze I [80] utilizes CNNs for the fixation prediction task by treating saliency prediction as point processing.", "startOffset": 12, "endOffset": 16}, {"referenceID": 63, "context": "Despite this model is justified differently than [79] and [75], in practice, it boils down to the same framework.", "startOffset": 49, "endOffset": 53}, {"referenceID": 61, "context": "Despite this model is justified differently than [79] and [75], in practice, it boils down to the same framework.", "startOffset": 58, "endOffset": 62}, {"referenceID": 65, "context": "SalNet [81] is another technique that employs a CNN-based architecture, where the last layer is a deconvolution.", "startOffset": 7, "endOffset": 11}, {"referenceID": 66, "context": "The first convolution layers are initialized by the VGG16 [82] and the deconvolution is learnt by fine-tuning the architecture for fixation prediction.", "startOffset": 58, "endOffset": 62}, {"referenceID": 67, "context": "Multiresolution CNN (Mr-CNN) [83] designs a deep CNN-based technique to discriminate image patches centered on fixations from non-fixated image patches at multiple resolutions.", "startOffset": 29, "endOffset": 33}, {"referenceID": 68, "context": "SALICON [84] develops a model by fine-tuning the convolutional neural network, trained on ImageNet, using saliency evaluation metrics as objective functions.", "startOffset": 8, "endOffset": 12}, {"referenceID": 65, "context": "a probability map in terms of a regression problem [81, 84, 85].", "startOffset": 51, "endOffset": 63}, {"referenceID": 68, "context": "a probability map in terms of a regression problem [81, 84, 85].", "startOffset": 51, "endOffset": 63}, {"referenceID": 69, "context": "a probability map in terms of a regression problem [81, 84, 85].", "startOffset": 51, "endOffset": 63}, {"referenceID": 61, "context": "We choose a common saliency database [75] and computed the gist [41] of the scene for each image.", "startOffset": 37, "endOffset": 41}, {"referenceID": 34, "context": "We choose a common saliency database [75] and computed the gist [41] of the scene for each image.", "startOffset": 64, "endOffset": 68}, {"referenceID": 70, "context": "05) shuffled AUC scores [86] where the score of prediction using similar pairs is 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 71, "context": "In this work, we adopt a filter-bank approach to the use of CNNs [87] for saliency prediction.", "startOffset": 65, "endOffset": 69}, {"referenceID": 66, "context": "We, thus, build an image pyramid and compute the CNNs\u2019 responses over each scale using the architecture of VGG16 [82] .", "startOffset": 113, "endOffset": 117}, {"referenceID": 35, "context": "Furthermore, we compute the classemes [43] from deep pipeline, that is, the probability of each of the one thousand classes of ImageNet [88] is computed using the fully-connected layers of the VGG16.", "startOffset": 38, "endOffset": 42}, {"referenceID": 72, "context": "Furthermore, we compute the classemes [43] from deep pipeline, that is, the probability of each of the one thousand classes of ImageNet [88] is computed using the fully-connected layers of the VGG16.", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "The classemes are complemented by the low-level scene representation to make the gist of the scene [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 34, "context": "The classemes and low-level scene features of [41] build a spatial representation of the outside world that is rich enough to convey the meaning of a scene as envisioned in [89].", "startOffset": 46, "endOffset": 50}, {"referenceID": 61, "context": "The query images are from [75] and the closest match is from [90].", "startOffset": 26, "endOffset": 30}, {"referenceID": 73, "context": "The query images are from [75] and the closest match is from [90].", "startOffset": 61, "endOffset": 65}, {"referenceID": 74, "context": "The idea of randomly-weighted single-hidden-layer feedforward networks (SLFNs) can be traced back to the Gamba perceptron [91] followed by others like [92, 93].", "startOffset": 122, "endOffset": 126}, {"referenceID": 75, "context": "The idea of randomly-weighted single-hidden-layer feedforward networks (SLFNs) can be traced back to the Gamba perceptron [91] followed by others like [92, 93].", "startOffset": 151, "endOffset": 159}, {"referenceID": 76, "context": "In the neural saliency predictor, we adopt the recent implementation of Extreme Learning Machines (ELM) [94].", "startOffset": 104, "endOffset": 108}, {"referenceID": 77, "context": "The theory of ELM facilitates the implementation of a neural network architecture such that the hidden layer weights can be chosen randomly meanwhile the output layer weights are determined analytically [95].", "startOffset": 203, "endOffset": 207}, {"referenceID": 78, "context": "Motivated by better function approximation properties of ELMs [96, 97], we employ them as the primary entity of the neural saliency prediction.", "startOffset": 62, "endOffset": 70}, {"referenceID": 73, "context": "We learn the spatial prior using the gaze data of [90], where the number of kernels corresponds to the number of fixation points.", "startOffset": 50, "endOffset": 54}, {"referenceID": 79, "context": "As demonstrated in many saliency research papers, the spatial prior introduces a center-bias effect [98].", "startOffset": 100, "endOffset": 104}, {"referenceID": 73, "context": "Figure 5: Spatial prior learnt from [90].", "startOffset": 36, "endOffset": 40}, {"referenceID": 61, "context": "The test databases include MIT [75], MIT300 [99], and CAT2000 [100].", "startOffset": 31, "endOffset": 35}, {"referenceID": 80, "context": "The test databases include MIT [75], MIT300 [99], and CAT2000 [100].", "startOffset": 44, "endOffset": 48}, {"referenceID": 81, "context": "The test databases include MIT [75], MIT300 [99], and CAT2000 [100].", "startOffset": 62, "endOffset": 67}, {"referenceID": 73, "context": "The first is trained on the OSIE database [90] and the latter is trained using the training set of CAT2000.", "startOffset": 42, "endOffset": 46}, {"referenceID": 61, "context": "Performance generalization To test the generalization of the model, we evaluate its performance using the MIT database [75].", "startOffset": 119, "endOffset": 123}, {"referenceID": 63, "context": "We choose the ensemble of deep neural networks (eDN) [79] as a baseline model because of the use of deep features and SVM classifiers.", "startOffset": 53, "endOffset": 57}, {"referenceID": 56, "context": "We also evaluate several models including, AIM [55], GBVS [69], AWS [101], Judd [75], and FES [51] for the sake of comparison with traditional models.", "startOffset": 58, "endOffset": 62}, {"referenceID": 82, "context": "We also evaluate several models including, AIM [55], GBVS [69], AWS [101], Judd [75], and FES [51] for the sake of comparison with traditional models.", "startOffset": 68, "endOffset": 73}, {"referenceID": 61, "context": "We also evaluate several models including, AIM [55], GBVS [69], AWS [101], Judd [75], and FES [51] for the sake of comparison with traditional models.", "startOffset": 80, "endOffset": 84}, {"referenceID": 43, "context": "We also evaluate several models including, AIM [55], GBVS [69], AWS [101], Judd [75], and FES [51] for the sake of comparison with traditional models.", "startOffset": 94, "endOffset": 98}, {"referenceID": 36, "context": "We employ shuffled AUC (sAUC, an AUC metric that is robust towards center bias), similarity metric (SIM, a metric indicating how two distributions resemble each other [44]), and normalized scanpath saliency (NSS, a metric to measure consistency with human fixation locations).", "startOffset": 167, "endOffset": 171}, {"referenceID": 70, "context": "NSS and sAUC scores are utilized in [86], which we borrow part of the scores from, and complement them with the SIM score.", "startOffset": 36, "endOffset": 40}, {"referenceID": 63, "context": "Figure 7: Performance generalization: the performance of the proposed model compared to traditional models and eDN [79] as a baseline model.", "startOffset": 115, "endOffset": 119}, {"referenceID": 68, "context": "29 SALICON [84] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 69, "context": "12 PDP [85] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 83, "context": "05 ML-Net [102] 0.", "startOffset": 10, "endOffset": 15}, {"referenceID": 65, "context": "65 SalNet [81] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 84, "context": "51 BMS [103] 0.", "startOffset": 7, "endOffset": 12}, {"referenceID": 67, "context": "41 Mr-CNN [83] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 63, "context": "37 eDN [79] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 84, "context": "The proposed model, ensembleCAT2k, ranks similarly with BMS [103] at the top of the ranking.", "startOffset": 60, "endOffset": 65}, {"referenceID": 84, "context": "67 BMS [103] 0.", "startOffset": 7, "endOffset": 12}, {"referenceID": 84, "context": "62 FES [103] 0.", "startOffset": 7, "endOffset": 12}, {"referenceID": 61, "context": "54 Judd [75] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 63, "context": "30 eDN [79] 0.", "startOffset": 7, "endOffset": 11}], "year": 2016, "abstractText": "This paper presents a novel fixation prediction and saliency modeling framework based on inter-image similarities and ensemble of Extreme Learning Machines (ELM). The proposed framework is inspired by two observations, 1) the contextual information of a scene along with low-level visual cues modulates attention, 2) the influence of scene memorability on eye movement patterns caused by the resemblance of a scene to a former visual experience. Motivated by such observations, we develop a framework that estimates the saliency of a given image using an ensemble of extreme learners, each trained on an image similar to the input image. That is, after retrieving a set of similar images for a given image, a saliency predictor is learnt from each of the images in the retrieved image set using an ELM, resulting in an ensemble. The saliency of the given image is then measured in terms of the mean of predicted saliency value by the ensemble\u2019s members.", "creator": "LaTeX with hyperref package"}}}