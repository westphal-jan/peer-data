{"id": "1610.06488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "An Evolving Neuro-Fuzzy System with Online Learning/Self-learning", "abstract": "An architecture reuchlin of kruma a new magnesia neuro - befitting fuzzy mythimna system is proposed. The basic mangouras idea of this isaly approach is heyl to 45-footer tune slabs both synaptic weights 120-degree and membership unscrewed functions with henday the help of dolfin the supervised learning and historical self - 26-under learning cuan paradigms. ambulette The approach emic to belugas solving the problem has \u00e1rbenz to do gayda with lonoke evolving online guzel neuro - 284.5 fuzzy berjaya systems that can washingtonians process data klimesova under kruszka uncertainty solero conditions. The results eudicots prove the gerringong effectiveness razing of deputy the formula developed hbv architecture guest-hosted and brinsmead the mansoorain learning procedure.", "histories": [["v1", "Thu, 20 Oct 2016 16:29:40 GMT  (335kb)", "http://arxiv.org/abs/1610.06488v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["yevgeniy v bodyanskiy", "oleksii k tyshchenko", "anastasiia o deineko"], "accepted": false, "id": "1610.06488"}, "pdf": {"name": "1610.06488.pdf", "metadata": {"source": "CRF", "title": "An Evolving Neuro-Fuzzy System with Online Learning/Self-learning", "authors": ["Yevgeniy V. Bodyanskiy", "Anastasiia O. Deineko"], "emails": ["bodya@kture.kharkov.ua", "anastasiya.deineko}@gmail.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nNowadays artificial neural networks (ANNs) are widely used in Data Mining tasks, prediction tasks, identification and emulation tasks etc. under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10]. They are universal approximators and are able to learn using data which characterize the object under study. If data should be processed in a sequential online mode, a convergence rate of a learning process comes to the forefront, which significantly limits the ANNs\u2019 class suitable for work under these conditions. ANNs, which use kernel activation functions (radial basis, bell-shaped, potential), are very effective from the speed optimization point of view in the learning process. Radial-basis neural networks (RBFN) are widely used, their output signal depends linearly on synaptic weights. It allows to use adaptive identification algorithms like the recurrent least-squares method, the Kaczmarz (Widrow-Hoff) algorithm etc. for their learning. However, the RBFN is exposed to the so-called \u201ccurse of dimensionality\u201d which means that when the input space dimensionality increases, there\u2019s an exponential growth of the adjustable parameters\u2019 (weights\u2019) amount.\nNeuro-fuzzy systems (NFSs) have more potential compared to neural networks [11-16], which combine learning capabilities, universal approximating properties and linguistic transparency of the results. The most popular NFSs are ANFIS and TSK-systems, whose output signal depends linearly on synaptic weights, that allows to use optimal linear identification adaptive algorithms for their learning. At the same time, to avoid gaps in the input space generated by scatter partitioning [17] which is used in ANFIS and TSK-systems, the parameters\u2019 tuning of membership functions is performed in the NFS\u2019s first hidden layer. The backpropagation algorithm is used for this purpose which is implemented with the help of multi-epochs learning [18]. Online tuning doesn\u2019t work in this case.\nThe idea of evolving computational systems is very popular nowadays with Data Mining scientists [19-26]. Both the system\u2019s architecture and the amount of adjustable parameters are growing rapidly while processing data. To control the RBFN activation functions\u2019 parameters (centers and matrix receptive fields) in an online mode, it was proposed in [27-29] to use the self-organizing Kohonen map [30], which provides these parameters\u2019 tuning in the self-learning process in an online mode. So the basic idea of this approach is to tune both synaptic weights and membership functions with the help of the supervised learning and self-learning paradigms.\nThe approach to solving the problem has to do with evolving online neuro-fuzzy systems that can process data under uncertainty conditions. It seems appropriate to extend this approach to an adaptive parameter tuning of membership functions in neuro-fuzzy systems.\nThe proposed tuning procedure of activation functions\u2019 parameters and their quantity was used in the proposed evolving neuro-fuzzy architecture. The tuning procedure works in an online mode. The proposed computational system was tested in forecasting tasks. The error was rather low (for a synthetic dataset: a training error was 0.02%, a test error was 1.5%; for a real-world dataset: a training error was 4.4%, a test error was 5.4%).\nThe remainder of this paper is organized as follows: Section 2 gives a neuro-fuzzy system\u2019s architecture and a learning procedure of output layer parameters. Section 3 describes membership functions\u2019 self-learning in the first\nhidden layer. Section 4 presents time-series forecasting with the help of the proposed neuro-fuzzy system. Conclusions and future work are given in the final section.\nII. A NEURO-FUZZY SYSTEM\u2019S ARCHITECTURE AND A LEARNING PROCEDURE OF OUTPUT LAYER PARAMETERS The proposed system\u2019s architecture (shown in fig.1) consists of five sequentially connected layers. A  1n  dimensional vector of input signals 1 2( ) ( ( ), ( ),..., ( ))Tnx k x k x k x k (here 1,2,...k  is current discrete\ntime) is fed to the input (zero) layer of the neuro-fuzzy system to be processed. The first hidden layer contains nh ( h for each input) membership functions ( )il x , 1,2,..., ;i n 1, 2,...,l h and carries out the input space fuzzification. The second hidden layer provides the membership levels\u2019 aggregation calculated in the first layer and consists of h multiplication blocks. The third hidden layer is a layer of synaptic weights to be defined during a learning process. The fourth layer is formed by two adders and calculates sums of the output signals of the second and third layers. And, finally, normalization is fulfilled in the fifth (output) layer, which results in an output signal y\u0302 calculation.\nThus, if a vector signal  x k is fed to the NFS\u2019s input the first hidden layer elements calculate membership levels   0 1li x k  , thus traditional Gaussian functions are commonly used as membership functions\n      2\n2exp 2 i li\nli i i\nx k c x k\n         \n(1)\nwhere lic , i are centers\u2019 parameters and width parameters correspondingly. It should be noticed that the preliminary data normalization on a certain interval, for example,  1 1,ix k   simplifies calculations, since the width parameters i can be accepted equal for all the inputs, i.\u0435. i  .\nAggregated values    1\nn\nli i i x k   are calculated in the second hidden layer, thus the Gaussian functions with the\nsame width parameters  are\n        22\n2 2 1 1 exp exp 2 2\nn n li li\nli i i i\nx k cx k c x k\n  \n            \n      (2)\n(here  1 2, ,..., T\nl l l l nc c c c ), i.\u0435. signals at the outputs of the multiplication blocks of the second hidden layer are similar to the signals at the neurons\u2019 outputs of the RBFN\u2019s first hidden layer.\nFig.1. The neuro-fuzzy system\u2019s architecture.\nThe third hidden layer outputs are    1\nn\nl li i i w x k   (here ,lw 1, 2,...,l h are synaptic weights to be defined),\nthe fourth hidden layer outputs are    1 1\nnh\nl li i l i w x k     and    1 1\nnh\nli i l i x k    and, finally, the system\u2019s output (the\nfifth layer) is\n     \n  \n  \n        1 1 1 1 1\n1 11 1\n\u02c6\nn nh\nl li i li ih h Tl i i\nl l ln nh h l l\nli i li i l li i\nw x k x k y x k w w x k w x k\nx k x k\n   \n \n  \n \n  \n      \n    \n(3)\nwhere\n     \n   1\n1 1\n,\nn\nli i i\nl nh\nli i l i\nx k x k\nx k\n \n\n\n \n \n  1 2, ,...,\nT hw w w w\n            1 2, ,..., . T\nhx k x k x k x k    It\u2019s easy to notice that the proposed system implements a nonlinear mapping of the input space into a scalar output signal like the normalized RBFN [31]. The proposed system in its architecture matches the zero-order Takagi-Sugeno-Kang system, i.\u0435. the Wang \u2013 Mendel architecture [15].\nAs already mentioned, to tune the NFS\u2019s synaptic weights, one can use the well-known adaptive algorithms of identification/learning like the exponentially weighted recurrent least-squares method\n                                            \n             \n       \n1 1 \u02c61 1 1 ,\n1 1\n1 11 1 , 1\n0 1\nT\nT T\nT\nT\nP k y k w k y x k P k y k y k w k w k x k w k x k\nx k P k x k x k P k x k\nP k x k x k P k P k P k\nx k P k x k\n       \n     \n\n                                  \n(4)\n(here  y k is a reference learning signal,  is a forgetting parameter of outdated information), or the one-step gradient Kaczmarz-Widrow-Hoff algorithm (it\u2019s optimal in speed):\n          \n     2\n1 1 , Ty k w k x k w k w k x k\nx k\n \n\n     (5)\na learning algorithm which possesses both tracking and smoothing properties [32, 33]                         1 2 1 1 , 1 ,0 1 Tw k w k p k y k w k x k x k p k p k x k            \n    \n(6)\nand similar procedures, including the well-known linear identification procedures [31]. It\u2019s interesting to note that the procedure (6) is associated with the algorithm (4) by the ratio\n    ,p k Tr P k (7) when 0  it gets the form of the algorithm (5).\nIII. MEMBERSHIP FUNCTIONS\u2019 SELF-LEARNING IN THE FIRST HIDDEN LAYER The membership functions\u2019 tuning process in the first hidden layer can be illustrated by a two-dimensional input\nvector       1 2, T x k x k x k and five membership functions    , 1, 2,3,4,5; 1,2li ix k l i   at every input. In this case, the NFS contains 10nh  membership functions. Centers\u2019 initial positions  0lic are evenly distributed along axes 1x and 2x , a distance between them is defined according to this relation\n  max min 20 0,5 1 1 i ix x h h       \n(8)\nfor 1 1ix   . This situation is illustrated in fig.2. In the case of the multi-dimensional input vector   nx k R , centers  0lic are evenly distributed along the\nhypercube axes  1,1 n .\nThe first vector 1x is fed to the system input (in fig.2 \u2013       1 21 1 , 2 T x x x ). There are centres-\"winners\"\n * 0lic on each axis, and they are the closest ones to  1ix in the sense of a distance    1 0li i lid x c  , (9)\nLet\u2019s notice that this procedure is in fact an implementation of the competitive process according to \u0422. Kohonen [30] but there\u2019s a slight difference that the \u00abwinner\u00bb on each axis can belong to membership functions with different indexes l . These \u00abwinners\u00bb in fig.2 are  *41 0c and  *42 0c .\nThen these \u00abwinners\u00bb catch up to the input signal\u2019s components  1ix according to the Kohonen self-learning method \u00abThe winner takes all\u00bb (WTA), which can be written down for the situation in fig.2 in the form\n          \n \n* *0 1 1 0\n1 4, 0 , 1,2,3,5;\nli li i li\nli\nli\nc x c\nc for the winner l c otherwise l       \n(11)\nand in a common case:\n          \n \n* *1 1\n1,2,..., ; 1,2,..., n; 1 .\nli li i li\nli\nli\nc k k x k c k\nc k for the winner l h i c k otherwise          \n(12)\nAt the same time a value\n  1li li k k   (13)\ncan be accepted as a learning step parameter in the simplest case where lik is an amount of times when  lic k was a \u00abwinner\u00bb that corresponds to the popular \u041a-means clusterization method (stochastic approximation in the case of online processing).\nIn a common case, one can use an estimate which was proposed for the traditional Kohonen map [34]:       1 2 , 1 , 0 1. li li\nli li i\nk p\np k p k x\n\n \n  \n     (14)\nIt can be noticed that the proposed approach is a modification of the Kohonen self-learning method but the difference is a traditional self-learning procedure is implemented on the hypersphere   2 1x k  , in our case \u2013 on\nthe hypercube  1,1 n (the hypersphere   1,qx k q  ). The combined learning/self-learning architecture of the neuro-fuzzy system is shown in fig.3.\nFig.3. A combined learning/self-learning procedure.\nThe functioning process of the system is performed in the following way. When an input vector  x k is fed, the\ncorrection of membership functions   li ix k is carried out at first in the self-learning block, which means that centers  lic k are calculated. Then the neuro-fuzzy system\u2019s output layer synaptic weights  w k are calculated on the grounds of clarified membership functions and a previously calculated synaptic weights\u2019 vector  1w k  with the help of the supervised learning algorithms ((4), (5) or (6)).\nIV. TIME-SERIES FORECASTING WITH THE HELP OF THE PROPOSED NEURO-FUZZY SYSTEM\nIn our experiment we used a signal generated by the Mackey-Glass equation [35] which is a non-linear differential equation\n1 n xdx x dt x \n\n   \n(15)\nwhere , ,n  are some coefficients, x is a value of a variable x in the  t   th time moment. The equation produces a number of periodic and chaotic values depending on parameters. In this work, these values were calculated with the help of the 4-th order Runge-Kutta method.\nFig.4. The Mackey-Glass time-series.\n1600 values were generated with the help of the Mackey-Glass equation during a simulation procedure. These values were normalized and fed to the neuro-fuzzy system input. The sample was divided into a training set and a test set in the ratio 2:3. The learning results are shown in Fig.5:\nThe learning error was about 0,02%. This high results\u2019 accuracy is explained by stationary properties of the Mackey-Glass time-series. Then the system was transferred to a prediction mode. Prediction of the time-series values was fulfilled in an online mode, and elaboration of adjustable parameters in the network in a prediction mode was not carried out. The forecasting results are shown in Fig.6:\nFig.6. The prediction results of the Mackey-Glass time-series.\nThe error was 1.5% in a prediction mode. To implement the experiment on real data, a sample that contains 2611 observations which characterize electricity consumption in Ukraine (December 1st 2008 \u2013 May 25th 2014, monthly) is used. To build a predictor model based on the proposed neural network\u2019s architecture data preprocessing was carried out in the form shown in Tab.1 where\n 1x k is the amount of consumed electric power for the next month,  2x k is the amount of consumed electric power in the current month,  3x k is a seasonal component and the amount of consumed electric power 12 months ago.\nx3(k) 594405 568605 573412 598185\nFig.7. The graphical display of electricity consumption data.\nThe electricity consumption data were normalized and fed to the neuro-fuzzy network\u2019s input. The network was\nlaunched in a learning mode initially. We obtained the results which are shown in Fig.8:\nFig.8. The neuro-fuzzy system\u2019s results in a learning mode.\nThe result error on a training set was 4,43%. Then our system was launched in a prediction mode. It should be noticed that the prediction results\u2019 validation was performed in such a way: the system returned a result vector whose values were in the range [0,1] as well as the input vector\u2019s values, and then values of this output vector with the help of the quadratic error criterion were compared to actual values. Which means that the values predicted by the network itself were used as history. A number of forecasted points was limited to 14. The prediction results are shown in Fig.9:\nFig.9. The prediction results of electricity consumption.\nThe error is 5,37%. We should notice that the total error at various time intervals was from 3 to 7%. At the beginning of the experiment centers\u2019 recalculation wasn\u2019t performed after every new value had come to the system\u2019s input, and the error was up to 20%. After the centers\u2019 values were recalculated, a function was rapidly leaving the local extremum area. This led to a sharp decrease of the result error.\nVI. CONCLUSION\nThe approach, which combines training of both synaptic weights and membership functions\u2019 centers and which is based on both supervised learning and self-learning, is proposed in this paper. The main advantage of the proposed approach is that it can be used in an online mode, when a training set is fed to a system\u2019s input sequentially, and its volume is not fixed beforehand. The results can be used for solving a wide class of Dynamic Data Mining problems.\nACKNOWLEDGMENT\nThe authors would like to thank anonymous reviewers for their careful reading of this paper and for their helpful comments.\nREFERENCES [1] A. Cichocki, R. Unbehauen, Neural Networks for Optimization and Signal Processing. Stuttgart: Teubner, 1993. [2] S. Haykin, Neural Networks: A Comprehensive Foundation. Upper Saddle River, New Jersey: Prentice Hall, 1999. [3] R.J. Schalkoff, Artificial Neural Networks, New York: The McGraw-Hill Comp., 1997. [4] D. Graupe, Principles of Artificial Neural Networks (Advanced Series in Circuits and Systems). Singapore: World Scientific\nPublishing Co. Pte. Ltd., 2007. [5] K. Suzuki, Artificial Neural Networks: Architectures and Applications. NY: InTech, 2013. [6] G. Hanrahan, Artificial Neural Networks in Biological and Environmental Analysis. NW: CRC Press, 2011. [7] K.-L. Du and M.N.S. Swamy, Neural Networks and Statistical Learning. London: Springer-Verlag, 2014. [8] S.F. Lilhare and N.G. Bawane, \u201cArtificial Neural Network Based Control Strategies for Paddy Drying Process\u201d, in Int. J.\nInformation Technology and Computer Science, vol. 6, no. 11, 2014, pp.28-35, doi: 10.5815/ijitcs.2014.11.04. [9] M. Abo-Zahhad, S.M. Ahmed, and S.A. Abd-Elrahman, \u201cIntegrated Model of DNA Sequence Numerical Representation\nand Artificial Neural Network for Human Donor and Acceptor Sites Prediction\u201d, in Int. J. Information Technology and Computer Science, vol. 6, no. 8, 2014, pp.51-57, doi: 10.5815/ijitcs.2014.08.07. [10] M.L. Pai, K.V. Pramod, and A.N. Balchand, \u201cLong Range Forecast on South West Monsoon Rainfall using Artificial Neural Networks based on Clustering Approach\u201d, in Int. J. Information Technology and Computer Science, vol. 6, no. 7, 2014, pp.1-8, doi: 10.5815/ijitcs.2014.07.01. [11] L. Rutkowski, Computational Intelligence. Methods and Tehniques. Berlin-Heidelberg: Springer-Verlag, 2008. [12] J.-S. Jang, C.-T. Sun, and E. Mizutani, Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and\nMaching Intelligence. Upper Saddle River, N.J.: Prentice Hall, 1997. [13] C.L. Mumford and L.C. Jain, Computational Intelligence. Berlin: Springer-Verlag, 2009. [14] R. Kruse, C. Borgelt, F. Klawonn, C. Moewes, M. Steinbrecher, and P. Held, Computational Intelligence. A Methodological\nIntroduction. Berlin: Springer-Verlag, 2013. [15] L.-X. Wang and J.M. Mendel, \u201cFuzzy basis functions, universal approximation and orthogonal least squares learning\u201d, in\nIEEE Trans. on Neural Networks, vol. 3, 1993, pp. 807-814. [16] K.J. Cios and W. Pedrycz, Neuro-fuzzy algorithms. Oxford: IOP Publishing Ltd and Oxford University Press, 1997. [17] J. Friedman, T. Hastie, and R. Tibshirani, The Elements of Statistical Learning. Data Mining, Inference and Prediction.\nBerlin: Springer, 2003. [18] S. Osowski, Sieci neuronowe do przetwarzania informacji. Warszawa: Oficijna Wydawnicza Politechniki Warszawskiej,\n2006. [19] N. Kasabov, Evolving Connectionist Systems. London: Springer-Verlag, 2003. [20] E. Lughofer, Evolving Fuzzy Systems \u2013 Methodologies, Advanced Concepts and Applications. Berlin-Heidelberg: Springer-\nVerlag, 2011. [21] Ye. Bodyanskiy, P. Grimm and N. Teslenko, \u201cEvolving cascaded neural network based on multidimensional\nEpanechnikov\u2019s kernels and its learning algorithm,\u201d in Int. J. Information Technologies and Knowledge, vol. 5, no. 1, 2011, pp. 25-30. [22] P. Angelov, D. Filev, and N. Kasabov, Evolving Intelligent Systems: Methodology and Applications. New York: John Wiley and Sons, 2010. [23] N. Kasabov, Evolving Connectionist Systems: The Knowledge Engineering Approach, London: Springer-Verlag, 2007. [24] N.K. Kasabov, \u201cEvolving fuzzy neural networks for supervised/unsupervised online knowledge-based learning\u201d, in IEEE\nTransactions on Systems, Man and Cybernetics, Part B: Cybernetics, no. 31(6), 2001, pp. 902-918. [25] N.K. Kasabov and Q. Song, \u201cDENFIS: Dynamic evolving neural-fuzzy inference system and its application for time-series\nprediction\u201d, in IEEE Transactions on Fuzzy Systems, no. 10(2), 2002, pp. 144-154. [26] E. Lughofer, \u201cFLEXFIS: A robust incremental learning approach for evolving TS fuzzy models\u201d, in IEEE Transactions on\nFuzzy Systems, no. 16(6), 2008, pp. 1393-1410. [27] Ye.V. Bodyanskiy and A.A. Deineko, \u201cAdaptive learning of the RBFN architecture and parameters\u201d, in System\nTechnologies, vol.4, no. 87, 2013, pp. 166-173. (in Russian) [28] Ye.V. Bodyanskiy and A.A. Deineko, \u201cThe evolving radial-basis neural network and its learning with the help of the\nKohonen map\u201d, in Proc. Sci. Conf. \u00abInformation Technologies in Metallurgy and Mechanical Engineering\u00bb, 2013, pp. 75- 77. (in Russian) [29] J.A. Torres, S. Martinez, F.J. Martinez, and M. Peralta, \u201cThe problem of organizing and partition large data sets in learning algorithms for SOM-RBF mixed structure\u201d, in Proc. 5th Int. Joint Conf. on Computational Intelligence, 2013, pp. 497-501. [30] T. Kohonen, Self-Organizing Maps. Berlin: Springer-Verlag, 1995. [31] O. Nelles, Nonlinear System Identification. Berlin: Springer, 2001. [32] Ye. Bodyanskiy, V. Kolodyazhniy, and A. Stephan, \u201cAn adaptive learning algorithm for a neuro-fuzzy network\u201d, in\nComputational Intelligence. Theory and Applications, B. Reusch, Ed. Berlin \u2013 Heidelberg \u2013 New York: Springer, 2001, pp. 68-75. [33] P. Otto, Ye. Bodyanskiy, and V. Kolodyazhniy, \u201cA new learning algorithm for a forecasting neuro-fuzzy network\u201d, in Integrated Computer-Aided Engeneering, vol. 10, no. 4, 2003, pp. 399-409. [34] Ye.V. Bodyanskiy and \u041e.G. Rudenko, Artificial Neural Networks: Architectures, Learning, Applications. Kharkiv: \u0422\u0415L\u0415\u0422\u0415H, 2004. (in Russian) [35] M.C. Mackey and L. Glass, \u201cOscillation and chaos in physiological control systems\u201d, in Science, no. 197, 1977, pp. 238- 289.\n\u00a0"}], "references": [{"title": "Neural Networks for Optimization and Signal Processing", "author": ["A. Cichocki", "R. Unbehauen"], "venue": "Stuttgart: Teubner", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "Neural Networks: A Comprehensive Foundation", "author": ["S. Haykin"], "venue": "Upper Saddle River, New Jersey: Prentice Hall", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Artificial Neural Networks", "author": ["R.J. Schalkoff"], "venue": "New York: The McGraw-Hill Comp.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Principles of Artificial Neural Networks (Advanced Series in Circuits and Systems)", "author": ["D. Graupe"], "venue": "Singapore: World Scientific Publishing Co. Pte. Ltd.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Artificial Neural Networks: Architectures and Applications", "author": ["K. Suzuki"], "venue": "NY: InTech", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Artificial Neural Networks in Biological and Environmental Analysis", "author": ["G. Hanrahan"], "venue": "NW: CRC Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural Networks and Statistical Learning", "author": ["K.-L. Du", "M.N.S. Swamy"], "venue": "London: Springer-Verlag", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Artificial Neural Network Based Control Strategies for Paddy Drying Process", "author": ["S.F. Lilhare", "N.G. Bawane"], "venue": "Int. J. Information Technology and Computer Science, vol. 6, no. 11", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Integrated Model of DNA Sequence Numerical Representation and Artificial Neural Network for Human Donor and Acceptor Sites Prediction", "author": ["M. Abo-Zahhad", "S.M. Ahmed", "S.A. Abd-Elrahman"], "venue": "Int. J. Information Technology and Computer Science, vol. 6, no. 8", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Long Range Forecast on South West Monsoon Rainfall using Artificial Neural Networks based on Clustering Approach", "author": ["M.L. Pai", "K.V. Pramod", "A.N. Balchand"], "venue": "Int. J. Information Technology and Computer Science, vol. 6, no. 7", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Computational Intelligence", "author": ["L. Rutkowski"], "venue": "Methods and Tehniques. Berlin-Heidelberg: Springer-Verlag", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and Maching Intelligence", "author": ["J.-S. Jang", "C.-T. Sun", "E. Mizutani"], "venue": "Upper Saddle River, N.J.: Prentice Hall", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Computational Intelligence", "author": ["C.L. Mumford", "L.C. Jain"], "venue": "Berlin: Springer-Verlag", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Computational Intelligence", "author": ["R. Kruse", "C. Borgelt", "F. Klawonn", "C. Moewes", "M. Steinbrecher", "P. Held"], "venue": "A Methodological Introduction. Berlin: Springer-Verlag", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Fuzzy basis functions", "author": ["L.-X. Wang", "J.M. Mendel"], "venue": "universal approximation and orthogonal least squares learning\u201d, in IEEE Trans. on Neural Networks, vol. 3", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1993}, {"title": "Neuro-fuzzy algorithms", "author": ["K.J. Cios", "W. Pedrycz"], "venue": "Oxford: IOP Publishing Ltd and Oxford University Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "The Elements of Statistical Learning", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Data Mining, Inference and Prediction. Berlin: Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Sieci neuronowe do przetwarzania informacji", "author": ["S. Osowski"], "venue": "Warszawa: Oficijna Wydawnicza Politechniki Warszawskiej", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Evolving Connectionist Systems", "author": ["N. Kasabov"], "venue": "London: Springer-Verlag", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Evolving Fuzzy Systems \u2013 Methodologies", "author": ["E. Lughofer"], "venue": "Advanced Concepts and Applications. Berlin-Heidelberg: Springer- Verlag", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Evolving cascaded neural network based on multidimensional Epanechnikov\u2019s kernels and its learning algorithm,", "author": ["Ye. Bodyanskiy", "P. Grimm", "N. Teslenko"], "venue": "in Int. J. Information Technologies and Knowledge,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Evolving Intelligent Systems: Methodology and Applications", "author": ["P. Angelov", "D. Filev", "N. Kasabov"], "venue": "New York: John Wiley and Sons", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Evolving Connectionist Systems: The Knowledge Engineering Approach", "author": ["N. Kasabov"], "venue": "London: Springer-Verlag", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Evolving fuzzy neural networks for supervised/unsupervised online knowledge-based learning", "author": ["N.K. Kasabov"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, Part B: Cybernetics, no. 31(6)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "DENFIS: Dynamic evolving neural-fuzzy inference system and its application for time-series prediction", "author": ["N.K. Kasabov", "Q. Song"], "venue": "IEEE Transactions on Fuzzy Systems, no. 10(2)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "FLEXFIS: A robust incremental learning approach for evolving TS fuzzy models", "author": ["E. Lughofer"], "venue": "IEEE Transactions on Fuzzy Systems, no. 16(6)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Deineko, \u201cAdaptive learning of the RBFN architecture and parameters", "author": ["A.A. Ye.V. Bodyanskiy"], "venue": "in System Technologies,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "The evolving radial-basis neural network and its learning with the help of the Kohonen map", "author": ["Ye.V. Bodyanskiy", "A.A. Deineko"], "venue": "in Proc. Sci. Conf. \u00abInformation Technologies in Metallurgy and Mechanical Engineering\u00bb,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "The problem of organizing and partition large data sets in learning algorithms for SOM-RBF mixed structure", "author": ["J.A. Torres", "S. Martinez", "F.J. Martinez", "M. Peralta"], "venue": "Proc. 5 Int. Joint Conf. on Computational Intelligence", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-Organizing Maps", "author": ["T. Kohonen"], "venue": "Berlin: Springer-Verlag", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1995}, {"title": "Nonlinear System Identification", "author": ["O. Nelles"], "venue": "Berlin: Springer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "An adaptive learning algorithm for a neuro-fuzzy network", "author": ["Ye. Bodyanskiy", "V. Kolodyazhniy", "A. Stephan"], "venue": "Computational Intelligence. Theory and Applications,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Ye", "author": ["P. Otto"], "venue": "Bodyanskiy, and V. Kolodyazhniy, \u201cA new learning algorithm for a forecasting neuro-fuzzy network\u201d, in Integrated Computer-Aided Engeneering, vol. 10, no. 4", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Artificial Neural Networks: Architectures, Learning, Applications", "author": ["Ye.V. Bodyanskiy", "\u041e.G. Rudenko"], "venue": "Kharkiv: \u0422\u0415L\u0415\u0422\u0415H,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Oscillation and chaos in physiological control systems", "author": ["M.C. Mackey", "L. Glass"], "venue": "Science, no. 197", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1977}], "referenceMentions": [{"referenceID": 0, "context": "under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10].", "startOffset": 116, "endOffset": 122}, {"referenceID": 1, "context": "under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10].", "startOffset": 116, "endOffset": 122}, {"referenceID": 2, "context": "under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10].", "startOffset": 116, "endOffset": 122}, {"referenceID": 3, "context": "under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10].", "startOffset": 116, "endOffset": 122}, {"referenceID": 4, "context": "under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10].", "startOffset": 116, "endOffset": 122}, {"referenceID": 5, "context": "under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10].", "startOffset": 116, "endOffset": 122}, {"referenceID": 6, "context": "under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10].", "startOffset": 116, "endOffset": 122}, {"referenceID": 7, "context": "under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10].", "startOffset": 116, "endOffset": 122}, {"referenceID": 8, "context": "under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10].", "startOffset": 116, "endOffset": 122}, {"referenceID": 9, "context": "under conditions of uncertainty, nonlinearity, stochasticity and chaoticity, various kinds of disturbance and noise [1-10].", "startOffset": 116, "endOffset": 122}, {"referenceID": 10, "context": "Neuro-fuzzy systems (NFSs) have more potential compared to neural networks [11-16], which combine learning capabilities, universal approximating properties and linguistic transparency of the results.", "startOffset": 75, "endOffset": 82}, {"referenceID": 11, "context": "Neuro-fuzzy systems (NFSs) have more potential compared to neural networks [11-16], which combine learning capabilities, universal approximating properties and linguistic transparency of the results.", "startOffset": 75, "endOffset": 82}, {"referenceID": 12, "context": "Neuro-fuzzy systems (NFSs) have more potential compared to neural networks [11-16], which combine learning capabilities, universal approximating properties and linguistic transparency of the results.", "startOffset": 75, "endOffset": 82}, {"referenceID": 13, "context": "Neuro-fuzzy systems (NFSs) have more potential compared to neural networks [11-16], which combine learning capabilities, universal approximating properties and linguistic transparency of the results.", "startOffset": 75, "endOffset": 82}, {"referenceID": 14, "context": "Neuro-fuzzy systems (NFSs) have more potential compared to neural networks [11-16], which combine learning capabilities, universal approximating properties and linguistic transparency of the results.", "startOffset": 75, "endOffset": 82}, {"referenceID": 15, "context": "Neuro-fuzzy systems (NFSs) have more potential compared to neural networks [11-16], which combine learning capabilities, universal approximating properties and linguistic transparency of the results.", "startOffset": 75, "endOffset": 82}, {"referenceID": 16, "context": "At the same time, to avoid gaps in the input space generated by scatter partitioning [17] which is used in ANFIS and TSK-systems, the parameters\u2019 tuning of membership functions is performed in the NFS\u2019s first hidden layer.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "The backpropagation algorithm is used for this purpose which is implemented with the help of multi-epochs learning [18].", "startOffset": 115, "endOffset": 119}, {"referenceID": 18, "context": "The idea of evolving computational systems is very popular nowadays with Data Mining scientists [19-26].", "startOffset": 96, "endOffset": 103}, {"referenceID": 19, "context": "The idea of evolving computational systems is very popular nowadays with Data Mining scientists [19-26].", "startOffset": 96, "endOffset": 103}, {"referenceID": 20, "context": "The idea of evolving computational systems is very popular nowadays with Data Mining scientists [19-26].", "startOffset": 96, "endOffset": 103}, {"referenceID": 21, "context": "The idea of evolving computational systems is very popular nowadays with Data Mining scientists [19-26].", "startOffset": 96, "endOffset": 103}, {"referenceID": 22, "context": "The idea of evolving computational systems is very popular nowadays with Data Mining scientists [19-26].", "startOffset": 96, "endOffset": 103}, {"referenceID": 23, "context": "The idea of evolving computational systems is very popular nowadays with Data Mining scientists [19-26].", "startOffset": 96, "endOffset": 103}, {"referenceID": 24, "context": "The idea of evolving computational systems is very popular nowadays with Data Mining scientists [19-26].", "startOffset": 96, "endOffset": 103}, {"referenceID": 25, "context": "The idea of evolving computational systems is very popular nowadays with Data Mining scientists [19-26].", "startOffset": 96, "endOffset": 103}, {"referenceID": 26, "context": "To control the RBFN activation functions\u2019 parameters (centers and matrix receptive fields) in an online mode, it was proposed in [27-29] to use the self-organizing Kohonen map [30], which provides these parameters\u2019 tuning in the self-learning process in an online mode.", "startOffset": 129, "endOffset": 136}, {"referenceID": 27, "context": "To control the RBFN activation functions\u2019 parameters (centers and matrix receptive fields) in an online mode, it was proposed in [27-29] to use the self-organizing Kohonen map [30], which provides these parameters\u2019 tuning in the self-learning process in an online mode.", "startOffset": 129, "endOffset": 136}, {"referenceID": 28, "context": "To control the RBFN activation functions\u2019 parameters (centers and matrix receptive fields) in an online mode, it was proposed in [27-29] to use the self-organizing Kohonen map [30], which provides these parameters\u2019 tuning in the self-learning process in an online mode.", "startOffset": 129, "endOffset": 136}, {"referenceID": 29, "context": "To control the RBFN activation functions\u2019 parameters (centers and matrix receptive fields) in an online mode, it was proposed in [27-29] to use the self-organizing Kohonen map [30], which provides these parameters\u2019 tuning in the self-learning process in an online mode.", "startOffset": 176, "endOffset": 180}, {"referenceID": 30, "context": "T h x k x k x k x k \uf06a \uf06a \uf06a \uf06a \uf03d It\u2019s easy to notice that the proposed system implements a nonlinear mapping of the input space into a scalar output signal like the normalized RBFN [31].", "startOffset": 178, "endOffset": 182}, {"referenceID": 14, "context": "the Wang \u2013 Mendel architecture [15].", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "a learning algorithm which possesses both tracking and smoothing properties [32, 33] \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1", "startOffset": 76, "endOffset": 84}, {"referenceID": 32, "context": "a learning algorithm which possesses both tracking and smoothing properties [32, 33] \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1", "startOffset": 76, "endOffset": 84}, {"referenceID": 30, "context": "and similar procedures, including the well-known linear identification procedures [31].", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "Kohonen [30] but there\u2019s a slight difference that the \u00abwinner\u00bb on each axis can belong to membership functions with different indexes l .", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "In a common case, one can use an estimate which was proposed for the traditional Kohonen map [34]: \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1", "startOffset": 93, "endOffset": 97}, {"referenceID": 34, "context": "TIME-SERIES FORECASTING WITH THE HELP OF THE PROPOSED NEURO-FUZZY SYSTEM In our experiment we used a signal generated by the Mackey-Glass equation [35] which is a non-linear differential equation", "startOffset": 147, "endOffset": 151}, {"referenceID": 0, "context": "It should be noticed that the prediction results\u2019 validation was performed in such a way: the system returned a result vector whose values were in the range [0,1] as well as the input vector\u2019s values, and then values of this output vector with the help of the quadratic error criterion were compared to actual values.", "startOffset": 157, "endOffset": 162}], "year": 2016, "abstractText": "A new neuro-fuzzy system\u2019s architecture and a learning method that adjusts its weights as well as automatically determines a number of neurons, centers\u2019 location of membership functions and the receptive field\u2019s parameters in an online mode with high processing speed is proposed in this paper. The basic idea of this approach is to tune both synaptic weights and membership functions with the help of the supervised learning and self-learning paradigms. The approach to solving the problem has to do with evolving online neuro-fuzzy systems that can process data under uncertainty conditions. The results proves the effectiveness of the developed architecture and the learning procedure.", "creator": "PScript5.dll Version 5.2.2"}}}