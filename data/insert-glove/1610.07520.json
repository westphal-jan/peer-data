{"id": "1610.07520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Nonlinear Adaptive Algorithms on Rank-One Tensor Models", "abstract": "armorican This work sveshnikov proposes shinki a pvh low krstic complexity abatis nonlinearity verheiden model bullis and develops p\u0142o\u0144sk adaptive hongs algorithms over shokhin it. cagney The g/mol model partap is notepad based virgilio on inordinately the wichter decomposable - - - heaving or velum rank - yannos one, sheymov in tensor ubd language - - - Volterra haseney kernels. liesing It 19,000-square may glemp also vincristine be emollients described kasym as a product masco of FIR eskdale filters, which explains 5-simplex its low - aeolus complexity. columbia-presbyterian The 170.1 rank - frank one 4,525 model krock is jacques also interesting because futhark it comes from a maqdah well - posed problem in ahp approximation diuretics theory. glamo\u010d The baffin paper dashkova uses such bartolomeo model haislett in buckled an estimation chinnaswamy theory papandoniou context feversham to develop an exact gradient - type algorithm, pickles from which sishu adaptive algorithms such njai as dynamix the least cerana mean squares (whit LMS) filter and sybilla its data - reuse version - - - the bartlesville TRUE - reigniting LMS - - - are drupa derived. Stability receive and convergence issues 40-bed are romulan addressed. tilton The oncologist algorithms are then morningstar tested quemada in simulations, which m60a3 show rubai its represents good eimiller performance docent when nebuad compared to www.haplecrone.com other nonlinear yeutter processing narodno algorithms in halard-decugis the 297.5 literature.", "histories": [["v1", "Mon, 24 Oct 2016 18:12:18 GMT  (884kb,D)", "http://arxiv.org/abs/1610.07520v1", null]], "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["felipe c pinheiro", "cassio g lopes"], "accepted": false, "id": "1610.07520"}, "pdf": {"name": "1610.07520.pdf", "metadata": {"source": "CRF", "title": "Nonlinear Adaptive Algorithms on Rank-One Tensor Models", "authors": ["Felipe C. Pinheiro", "Cassio G. Lopes"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Adaptive filtering, nonlinear signal processing, tensors, estimation theory.\nI. INTRODUCTION\nNONLINEAR signal processing has its uses wheneverthe performance of linear techniques start to become inadequate. A few situations that may ask for their use are echo cancellation [1], equalization of communication systems [2], acoustics [3], or broadband noise canceling [4].\nWhile an important part of current signal processing research, an usual feature of nonlinear techniques is their high computational complexity, which sometimes render them unusable for some applications, such as those that use slower computers or require low power consumption. This complexity may be high even for modern computers, as is the case of the Volterra series [5], [6] and its exponentially increasing complexity.\nThe Volterra series is an important model because it is able to work with any continuous nonlinearity. It is also a very simple structure that can be described in a linear-in-theparameters model. Given these advantages, various structures try to work with this model. But many times some restriction is necessary to allow for fast computation. Examples of these approaches are the filters with truncated diagonals [7], [8], which works with the central coefficients of the model, which are usually the most significant ones. Another kind of model is based in interpolation techniques [9], [10], in a way that the algorithms work with only a few coefficients while interpolating the others.\nAnother aspect of nonlinear signal processing is the difficulty in their analyses, specially if the models of nonlinearity are nonlinear in the parameters. This is the case with cascade structures [11]. Some problem of this kind is the lack of global asymptotic stability, with one example shown in this paper. Other models that have yet to be fully analyzed. An example is ones based in tensor decomposition[12].\nTensors are objects that can be accessed via many number of indexes. Vectors are one-index tensors, while matrices are twoindex ones. Tensors are also a bridge between the multilinear [13] world\u2014of multilinear functions, or functions of many variables that are linear in each one of them\u2014and the linear world. As it will be shown in this paper, this property creates a conceptual link between the Volterra series and tensors, with these objects being the natural way of representing a Volterra kernel. Not surprisingly, tensors have indeed been used in nonlinear signal processing-related problem for quite some time [14], [15].\nThe tensor representation, and specially tensor rank decomposition [16] allows for a dramatic decrease in representational complexity of the model and, should this be exploited in the Volterra series, many low-complexity algorithms may be obtained. Of particular importance is the rank-one approximation [17]. This concept has seem some success in approximating linear responses [18], [19] with certain structures, and the possible gains to nonlinear signal processing are even more pronounced\u2014exponential reductions in complexity can be achieved [20]. The other theoretical advantage of rank-one approximations is their well-posedness, which may not be the case for general low-rank decompositions [21].\nThis paper focuses on the development of the rank-one approximation of the Volterra series and on the results adaptive algorithms one can derive from the model. For this objective, the paper shall be organized as follows:\nTherefore, the paper structure is as follows: 1) Introduction and notation. 2) A presentation on tensors and rank-one approximations. 3) The Volterra series and its relation to tensors. The\ndecomposable (rank-one) model as a product of FIR linear filters, with low computational complexity. 4) Estimation theory of theory of the decomposable model. 5) The steepest descent algorithm as an iterative solution\nto the estimation problem. 6) LMS and TRUE-LMS algorithms as instantaneous ap-\nproximations to the steepest descent. Stability issues and choice of parameters. 7) Simulations: testing performance, stability and applicability. 8) The presence of chaotic behavior. 9) Concluding remarks."}, {"heading": "A. Notation", "text": "The notation of the paper follows the one on [22], while introducing new notation when necessary. The conventions are summarized as\nar X\niv :1\n61 0.\n07 52\n0v 1\n[ cs\n.S Y\n] 2\n4 O\nct 2\n01 6\n2 \u2022 Scalars and vectors are represented by lowercase letters. (E.g. x and a.) \u2022 Time varying vectors are indexed as xi, while time varying scalars are presented as a(i). The time variable is always i or j. \u2022 Matrices and constants are represented as uppercase letters. (E.g. R and A.) \u2022 Tensors are represented as calligraphic letters. (E.g. W and X .) \u2022 Time-varying matrices and tensors follow the vector convention. \u2022 Tensors by nature have a matrix representation. When interpreted as matrices, the involved operations are to be interpreted as the matrix equivalents. (E.g. the tensor product \u2297 is to be interpreted as the Kronecker product [23].) \u2022 This paper uses the classical convention of lower and upper indexes. Column vectors are indexed with an upper index (e.g. (v)i), row-vectors with a lower one (e.g. (w)j) and matrices with both an upper one and a lower one (e.g. (A)ij). The convention for general tensors will be explained in through the paper."}, {"heading": "II. TENSORS", "text": "Just like vectors are objects parameterized by a single index and matrices by two, tensors are objects parameterized by an arbitrary number K of indexes\u2014such number being called the order of the tensor. For example, an order K tensor T may be indexed as (T )i1...iK . Such notions have been used by some developments in signal processing, usually dealing with multidimensional data. [16]\nJust like vectors can alternatively characterized by the concept of a vector space, tensors can be abstractly defined as objects of an algebraic tensor product [13]. Given K vector spaces V1, . . . , VK , their tensor product is a new and bigger vector space V1\u2297 \u00b7 \u00b7 \u00b7 \u2297 VK together with a K-linear function \u2297 : V1\u00d7\u00b7 \u00b7 \u00b7\u00d7VK \u2192 V1\u2297\u00b7 \u00b7 \u00b7\u2297VK\u2014called the tensor product of vectors\u2014with the following universal property: any Klinear function f : V1, \u00b7 \u00b7 \u00b7 , VK \u2192 W to any vector space W may be decomposed, uniquely, as f = f\u0304 \u25e6 \u2297, where f\u0304 : V1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 VK \u2192W is a linear function that represents f in tensor language. In other words, the tensor product makes it possible to represent K-linear functions, often unfamiliar, as linear functions, which are well understood. This can be summarized in the following commutative diagram.\nV1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 VK V1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 VK\nW\n\u2297\nf \u2203!f\u0304 (1)\nThe tensor product of the vectors v1 \u2208 V1, . . . , vk \u2208 VK is often represented as v1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 vk, which is an order K tensor. In computational terms, the tensor product can be implemented as a Kronecker product [23]. Any tensor that can be decomposed in a tensor product of vectors is called decomposable. It is a consequence of the definition of the\nalgebraic tensor product that any tensor T \u2208 V1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 VK can be written as a sum of decomposable tensors, as in (2).\nT = R\u2211 r=1 v1,r \u2297 \u00b7 \u00b7 \u00b7 \u2297 vK,r. (2)\nThere are many ways of writing a tensor this way, but the well-ordering principle implies that, for any given tensor, there exists a minimum number R for which this is possible. This is called the rank of the tensor and will be represented as rank T . The rank of the zero tensor is defined as zero, being the only tensor with such rank. Decomposable tensors have rank either one or zero. As an abuse of language, the paper shall refer to decomposable tensors also as rank-one tensors. This will not create confusion and is quite usual in tensor literature. When K = 2, the notion of tensor rank reduces to that of matrix rank.\nOther property of interest is the dimension of the algebraic tensor product:\ndimV1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 VK = (dimV1) \u00b7 \u00b7 \u00b7 (dimVK), (3)\nthat is, the dimension of the resulting vector space is the product of the dimensions of the individual vector spaces. This is consistent with the multi-index representation of tensors.\nIn classical tensor literature, lower and upper indexes in the coordinate representation are treated differently. This is to facilitate the introduction of operations similar to the product of matrices. Upper indexes are related to column vectors, in the way of (v)i, while row vectors are represented by a lower index: (w)j . A general tensor may be represented as (T )i1...iKj1...jL .\nIn this paper the following convention will be made: given tensors T1 and T2, indexed as (T1)i1...iKj1...jP and (T2) i1...iP j1...jL\n, the product T1T2 is defined by the coordinates\n(T1T2)i1...iKj1...jL = \u2211\nk1,...,kP\n(T1)i1...iKk1...kP (T2) k1...kP j1...jL . (4)\nNotice that this is defined only for when the number of lower indexes of T1 equals the number of upper indexes of T2. In particular, this convention implies, for the column vector (v)i and the row vector (w)j , that the products wv and vw are, respectively, the inner and outer products of v and w, which is consistent with matrix notation.\nAnother important operation is the tensor product of tensors, which takes the two tensors (T1)i1...iKj1...jP and (T2) k1...kQ `1...`L\nand result in a higher order tensor T1 \u2297 T2 given by\n(T1 \u2297 T2)i1...iKk1...kQj1...jP `1...`L = (T1) i1...iK j1...jP (T2)k1...kQ`1...`L , (5)\nthat it, given by juxtaposition of their coordinates via multiplication."}, {"heading": "A. Rank-one approximation", "text": "It is possible to naturally extend norms and inner products of vectors to corresponding norms of tensors. The details of how to compute them are presented on Appendix A. These notions allow for the proposition of approximation problems. For example, it is possible to pose the rank R approximation\n3 problem: given a tensor T , find a tensor X that solves the problem\nmin X \u2016T \u2212 X\u2016, s. t. rankX \u2264 R. (6)\nThis problem has the potential to decrease the number of parameters necessary to represent the tensor, but it is, in general, not well posed and may not have a solution [21]. One case when this always has a solution is with order 2 tensors\u2014that is, matrices. But, for general order K tensors, the case where it is always guaranteed to have a solution is when R = 1, the so called rank-one approximation. This fact follows from the next proposition.\nTheorem 1. The set D = {X : rankX \u2264 1} of decomposable tensors is closed.\nProof. Proof given by [21] in Proposition 4.2.\nIt is, therefore, advantageous to base signal processing algorithms over the problem of rank-one approximations, as these algorithms will most likely be well-behaved."}, {"heading": "III. THE VOLTERRA SERIES", "text": "Traditionally, the Volterra series is treated as an universal approximator for nonlinear systems. It can be described as a polynomial representation of the system, closely related to a Taylor series representations. Explicitly, given a nonlinear system with input signal u(i) and output y(i), its Volterra series representation is the sum\ny(i) = y0 + y1(i) + y2(i) + y3(i) + \u00b7 \u00b7 \u00b7 , (7) where y0 is a constant and each yk(i) is called the order K homogeneous component of the system and is given by\nyk(i) = \u2211\ni1,...,ik\nHk(i1, . . . , ik)u(i\u2212 i1) \u00b7 \u00b7 \u00b7u(i\u2212 ik), (8)\nin which Hk(i1, \u00b7 \u00b7 \u00b7 , ik) is a set of parameters called the order k Volterra kernel.\nThe truncated Volterra series is obtained by limiting this series both in order and in time. This results in\ny(i) = y0 + y1(i) + \u00b7 \u00b7 \u00b7+ yK(i) (9) and\nyk(i) = M\u2211 i1,...,ik=0 Hk(i1, . . . , ik)u(i\u2212 i1) \u00b7 \u00b7 \u00b7u(i\u2212 ik), (10)\nwhere K is the order of the representation and M its memory parameter. If follows from the Stone\u2013Weierstrass theorem that the truncated Volterra series is an universal approximator\u2014it can arbitrarily approximate any continuous nonlinearity, given that K and M are big enough.\nAlthough this is an interesting property, the representational complexity, the number of the parameters necessary to describe an order k Volterra kernel is O(Mk), or, by exploring symmetry, O (( M\u2212k+1\nk\n)) . This high complexity\ngets transfered to Volterra-based algorithms, something that has historically barred their use in applications.\nTo deal with this problem, a common approach in the literature is to assume some restriction in the series and/or kernels. For example, for second order kernels, one possible approach is to truncate some of the diagonals of H2(i, j) [7], [8], which allows the the computation to be written as\ny2(i) = D\u22121\u2211 d=0 M\u22121\u2211 i1 H2(i1, i1 + d)u(i\u2212 i1)u(i\u2212 i1 \u2212 d), (11)\nwhere the D is the number of diagonal utilized. When D = 1, this uses only the main diagonal, with M parameters, resulting in a model called the Power Filter [24].\nAnother approach is to start from tensor representations of the Volterra kernel. The multi-index structure of the Volterra kernel is quite obvious and it may as well have been represented as (Hk)i1...ik , making evident that it is a tensor. In a more conceptual way, one may realize that the k-th order component may as well be computed as a k-linear transformation. First, define the input vector as the row vector (ui)j = u(i\u2212 j + 1), or:\nui = [ u(i) u(i\u2212 1) . . . u(i\u2212M + 1) ] (12)\nThen, notice how (10) may be rewritten, using the conventions of (5), as yk(i) = u\u2297ki Hk, (13) where u\u2297ki , ui \u2297 \u00b7 \u00b7 \u00b7 \u2297 ui is the tensor power of ui\u2014the tensor product of ui with itself k times. Eq. (13) could be seen as a function f : CM \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 CM \u2192 C given by\nf(v1, . . . , vk) = (v1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 vk)Hk, (14) which is, by the multilinearity of the tensor product, a k-linear function. So, from the universal property (1), it is natural to expect the Volterra series to have a tensor representation.\nThis representation has been used, for symmetric tensors, in [12], to reduce the representational complexity of the series.\nIn this paper, the restricting hypothesis will be simply the one of decomposability of kernel."}, {"heading": "A. Decomposable Model", "text": "Consider the Volterra kernel tensor HK of order K and assume that it is a decomposable tensor, that it, that there exists vectors w1, . . . , wK \u2208 CM such that\nHK = w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK . (15) A kernel like this will be said to satisfy the decomposable model\u2014also called the Simple Multilinear Model (SML) [20]. This model allows for great gains in computational complexity, as it can be seen by carrying out the following computations:\nyK(i) = u\u2297ki Hk = (ui \u2297 \u00b7 \u00b7 \u00b7 \u2297 ui\ufe38 \ufe37\ufe37 \ufe38 K times )(w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK)\n= M\u2211 i1,...,iK=0 u(i\u2212 i1) \u00b7 \u00b7 \u00b7u(i\u2212 iK)w1(i1) \u00b7 \u00b7 \u00b7wK(iK)\n= M\u2211 i1=1 w1(i1)u(i\u2212 i1) \u00b7 \u00b7 \u00b7 M\u2211 iK=1 w1(i1)u(i\u2212 i1) = (uiw1) \u00b7 \u00b7 \u00b7 (uiwK). (16)\n4 u(i) w2 \u00d7 w1\n... wK\ny(i)\nFig. 1. Block diagram of the decomposable model.\nThis means that the output may be computed as a product of the outputs of K FIR linear systems. This is represented in Fig. 1. In terms of computational complexity, this result in something based on O(KM), an exponential reduction when compared to the original complexity of the Volterra series, which was of O(MK). This, together with the well-posedness of the estimation problem should make for a good basis onto which develop adaptive algorithms."}, {"heading": "IV. ESTIMATION THEORY OF THE DECOMPOSABLE MODEL", "text": "Suppose one wants to estimate the best rank-one approximation of the homogeneous component yK(i) of the Volterra series by estimating the approximation of its kernel HK . The problem can be posed in the following way: one is given a random signal d (the desired signal) and a random 1\u00d7M vector u (the regressor). One want to find the K order decomposable Volterra kernel W that best estimates d, in the mean-square sense. This can be put in terms of the constrained optimization problem below.\nmin W\nE \u2223\u2223d\u2212 u\u2297KW\u2223\u22232 , s. t. rankW \u2264 1 (17)\nThis problem has a cost function given by J(W) , E \u2223\u2223d\u2212 u\u2297KW\u2223\u22232. This cost function is called the Mean Square Error (MSE), while the signal e , d \u2212 u\u2297KW . The decomposability constraint can be included in this function by using W = w1\u2297\u00b7 \u00b7 \u00b7\u2297wK , for vectors w1, . . . , wK (M \u00d71). It also will be convenient to also define the vector w (KM \u00d71) built by vertically stacking the vectors w1, . . . , wK .\nIt is possible to explicitly develop the MSE as1\nJ(w) = E|e|2 = E [ [d\u2212 u\u2297KW]\u2217[d\u2212 u\u2297KW] ] = E|d|2 \u2212W\u2217E[du\u2297K\u2217]\u2212 E[d\u2217u\u2297K ]W\n+W\u2217E[u\u2297K\u2217u\u2297K ]W. = Rd \u2212W\u2217R\u2217uKd \u2212RuKdW +W\u2217RuKW, (18)\nwhere the correlation parameters were defined as\nRuK = E[u\u2297K\u2217u\u2297K ], (19) RuKd = E[u\u2297Kd\u2217] = R\u2217duK , (20)\nRd = E|d|2. (21)\n1The star operation (\u2217) on a tensor (T )i1...iKj1...jL consists of changing lower indexes for upper indexes and taking the complex conjugate of the entries: (T \u2217)i1...iLj1...jK = (T ) j1...jK i1...iL .\nThe minimum of this function is achieved at a point where the gradient, over w, is zero. This gradient has a block structure, each depending only on one ws:\n\u2207J(w) = [ \u2202J \u2202w1 \u2202J \u2202w2 \u00b7 \u00b7 \u00b7 \u2202J\u2202wK ] . (22)\nEach block may be computed as in the next proposition. The derivatives used are Wirtinger derivatives.\nProposition 1. The gradient of the MSE function with respect to ws can be computed as\n\u2202J\n\u2202ws = [\u2212RuKd +W\u2217RuK ]W(s), (23)\nwith W(s) = (w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 w\u0302s \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK), (24) where w\u0302s implies that ws has been substituted for the identity matrix IM of order M in the product.\nProof. The following tensor indexations shall be used: (RuK )i1,...,iKj1,...,jK = E [ (u\u2297K\u2217)i1,...,iK (u\u2297K)j1,...,jK ] (25)\n(RuKd)j1,...,jK = E [ (u\u2297K)j1,...,jKd ] . (26)\nThrough this, we can write\nRuKdW = \u2211\nj1,...,jK (RuKd)j1,...,jK K\u220f `=1 (w`) j` , (27)\nwhere (w`)j` is the j`-th coordinate of w`, and\nW\u2217RuKW = \u2211\ni1,...,iK j1,...,jK\nK\u220f p=1 (wp) ip\u2217(RuK )i1,...,iKj1,...,jK K\u220f `=1 (w`) j` .\n(28) The other terms from (18) involve only the conjugates of the entries of w, so their Wirtinger derivatives become zero [22].\nThe gradient over the vector ws is given by the derivatives over each of its components, which, by introducing the symbol \u03b4ij\u2014called the Kronecker delta\u2014that evaluates to 0 if i 6= j and to 1 if i = j, results in:\n\u2202(RuKdW) \u2202(ws)jq\n= \u2211\nj1,...,jK (RuKd)j1,...,jK \u220f 6\u0300=s (w`) j`\u03b4jsjq . (29)\nAs jq take values from 1 through M , (29) indeed renders the vector (23). This can be shown as follows: the Kronecker delta \u03b4ij indexes the identity matrix, that is, (IM ) i j = \u03b4 i j . In (29), the delta works as if occupying the positions of the coordinates (ws)jq\u2014that is, the coordinates of an identity IM are occupying the positions of the coordinates of the vector ws. This is valid for any index jq , thus, when reconstructing the tensor form implied by the coordinates computed in (29), the substitution with IM in (24) must be made: \u2202(RuKdW)\n\u2202ws = RuKd(w1\u2297\u00b7 \u00b7 \u00b7\u2297w\u0302s\u2297\u00b7 \u00b7 \u00b7\u2297wK) = RuKdW(s).\n(30) For W\u2217RuKW , we have, remembering we do not derivate\nthe conjugates,\n\u2202(w\u2217RuKw) \u2202(ws)jq\n= \u2211\ni1,...,iK j1,...,jK\nK\u220f p=1 (w\u2217p)ip(RuK )i1,...,iKj1,...,jK \u220f ` 6=s (w`) j`\u03b4jsjq .\n(31)\n5 Under the same argument,\n\u2202(WRuKW\u2217) \u2202ws =W\u2217RuKW(s). (32)\nTherefore, one combines those two terms to get\n\u2202J\n\u2202ws =\n\u2202\n\u2202ws (Rd \u2212W\u2217R\u2217uKd \u2212RuKdW +W\u2217RuKW)\n= \u2212\u2202(RuKdW) \u2202ws + \u2202(WRuKW\u2217)\n\u2202ws = [\u2212RuKd +W\u2217RuK ]W(s). (33)\nIn a critical point \u2207J(w) = 0 of this surface, w satisfies a series of polynomial equations in its entries reminiscent of the normal equations of linear estimation theory:\n[\u2212RuKd +W\u2217RuK ]W(s) = 0, 1 \u2264 s \u2264 K. (34) One can verify the possibility\u2014because this surface is multimodal\u2014of minimality of a certain w by testing it in this equation, but a solution via direct methods may be difficult. It should be more viable to use iterative algorithms."}, {"heading": "V. THE STEEPEST DESCENT ALGORITHM", "text": "The steepest descent algorithm can be used to find the minimum of (18). This algorithm is based on the recursion\nwi = wi\u22121 \u2212 \u00b5[\u2207J(wi\u22121)]\u2217. (35) In terms of the individual vectors, this may be rewritten as\nws,i = ws,i\u22121 + \u00b5W(s)\u2217[RduK \u2212RuKW]. (36) Due to the nonlinear nature of the algorithm, there are some issues with the initializations of the ws parameters. First, the should not be initialized all at zero, as (36) shows that this would make the parameters stay at zero through all of the iteration process. Another bad initialization is to set all of them at the same initial values. The recursion shows that this would lead to them being adapted exactly in the same way, never being able to diverge from one another. Initializing the parameters as pairs of opposite vectors leads to a similar problem with alternating iterations. Aside from random initializations, the previous discussion makes the heuristic argument to initialize the vectors in different scales. The following set of initializations has shown good results in our experiments.\nws,\u22121 = [ 2\u2212s+1 0 . . . 0 ] , for 1 \u2264 s \u2264 K \u2212 1 (37)\nwK,\u22121 = [ 0 0 . . . 0 ] . (38)\nThis recursion was used to find a solution to the estimation problem with d = u\u2297KH+ v, (39) where u is a 1\u00d710 independent Gaussian vector, v is a signal independent from u with variance \u03c32 = 10\u22123 and the plant H is decomposable.. The resulting MSE curve is on Figs. 2a (K = 2) and 2b (K = 3). It is possible to notice how the algorithm is able to reach the theoretical minimum MSE of \u03c32. Moreover, it was possible to verify that the solutions found satisfy the normal equations (34).\n0 1000 2000 3000 \u221230\n\u221220\n\u221210\n0\nIterations\ndB\nMean Square Error\n(a) K = 2.\n0 2000 4000 6000 \u221230\n\u221220\n\u221210\n0\nIterations\ndB\nMean Square Error\n(b) K = 3.\nFig. 2. MSE curves for the steepest descent algorithm.\nAlthough some good behavior of this algorithm can be attested by these simulations, in practice it is very hard for one to obtain the R parameters. So a recursion like 36 is rarely used\u2014the interest in it is a rather theoretical one. A solution to this problem is to compute real-time approximations to (19). This is called an adaptive solution. Not only is solves the problem of unavailability of the correlation parameters, it also gives the algorithm some tracking capabilities. When doing this to the linear estimation problem, the algorithm obtained is the classical LMS [22]. When done to the decomposable model, it should, therefore, lead to similar algorithms."}, {"heading": "VI. ADAPTIVE ALGORITHMS", "text": "As it is impractical to use the steepest descent algorithm, one finds an adaptive solution that implements a recursion that approximates (36). This is done by computing a real-time approximation to the parameters (19)."}, {"heading": "A. Rectangular Window Approximation", "text": "A possible approximation is to use a rectangular window to estimate the correlation parameters\u2014that is, the last L samples of the signals are used in a average to compute the approximations\nR\u0303uK = 1\nL i\u2211 j=i\u2212L+1 u\u2297K\u2217j u \u2297K j , (40)\nR\u0303uKd = 1\nL i\u2211 j=i\u2212L+1 u\u2297Kj d(j) \u2217 = R\u0303\u2217duK . (41)\nThis allows for the computation of an estimate of the blockgradient (23):\n\u2202\u0303J\n\u2202ws = [\u2212R\u0303uKd +W\u2217R\u0303uK ]W(s)\n= 1\nL i\u2211 j=i\u2212L+1 [ \u2212d(j)\u2217 +W\u2217u\u2297K\u2217j ] u\u2297Kj W(s). (42)\nEquation (42) may be conveniently rewritten as follows. First define ys(j) , (ujw1) \u0302\u00b7 \u00b7 \u00b7 (ujws) \u00b7 \u00b7 \u00b7(ujwK), where the hat implies a factor being omitted, which implies u\u2297Kj W(s) = ys(j)uj , then define three variables:\ndi ,  d(i)... d(i\u2212 L+ 1)  (L\u00d71), UKi ,  u \u2297K i ...\nu\u2297Ki\u2212L+1  (L\u00d7KM), yj , ( y1(j) \u00b7 \u00b7 \u00b7 yK(j) ) (1\u00d7K). (43)\n6 Eq. (42) represents a block-gradient. For each j, the terms of this sum will be multiplied only by ys(j)\u2014the s-th element of yj\u2014which shows that the total gradient has a Kronecker structure. This follows from the fact that u\u2297Kj W(s) = yws (j)uj . Therefore, write\n\u2207\u0303J = 1 L i\u2211 j=i\u2212L+1 [ \u2212d(j)\u2217 +W\u2217u\u2297K\u2217j ] (yj \u2297 uj) . (44)\nBy using the variable\nTi ,  yi \u2297 ui... yi\u2212L+1 \u2297 ui\u2212L+1  (L\u00d7KM), (45) a compact expression for the instantaneous gradient is obtained:\n\u2207\u0303J = \u2212 1 L\n[ di \u2212 UKi W ]\u2217 Ti = \u2212 1\nL e\u2217i Ti, (46)\nwhere ei , di \u2212 UKi W (L \u00d7 1). Finally, defining y(j) , u\u2297KW and\nyi , ( y(i) \u00b7 \u00b7 \u00b7 y(i\u2212 L+ 1) )T (L\u00d7 1), (47)\nresults in an expression for ei as\nei = di \u2212 yi. (48)"}, {"heading": "B. LMS Algorithm", "text": "When L = 1, Eq. (46) becomes\n\u2207\u0303J = \u2212e(i)\u2217(yi \u2297 ui), (49) where e(i) , d(i)\u2212 u\u2297i W . When separated for each ws, the LMS equation becomes\nws,i = ws,i\u22121 + \u00b5e(i)ys(i) \u2217u\u2217i . (50)\nThis is a Least Mean Squares (LMS) recursion."}, {"heading": "C. True-LMS Algorithm", "text": "When using a general value of L, the recursion of the TRUE-LMS (also sometimes called the Dat Reuse LMS) is derived:\nwi = wi\u22121 + \u00b5 L T \u2217i ei. (51)\nThis can, by separating each ws, be further simplified to\nws,i = ws,i\u22121 + \u00b5\nL i\u2211 j=i\u2212L+1 [d(j)\u2212 u\u2297Kj Wi\u22121]ys(j)\u2217u\u2217j\n= ws,i\u22121 + \u00b5 L (ys,i \u25e6 Ui)\u2217ei, (52)\nwhere\nys,i ,  ys(i)... ys(i\u2212 L+ 1)  (L\u00d7K), Ui ,  ui... ui\u2212L+1  (L\u00d7M) (53)\nand \u25e6 denotes row-wise scalar multiplication.\n0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000\n\u221230\n\u221220\n\u221210\n0\nIterations\ndB\nMean Square Error (estimated)\n(a) The arrows point to signs of instability.The Figure is an ensemble average of 1.000 realizations"}, {"heading": "D. Stabilization of the Algorithms", "text": "It has been known that nonlinearities in the update equations may lead to a nonzero probability of divergence for the algorithm, no matter how small is the step-size [25], whenever the probability distribution of the signals has infinite support. This is related to the fact that the algorithms are not globally asymptotically stable. An example of this phenomenon is illustrated in Fig. 3. The big \u201cjumps\u201d in the MSE curve 3a shows the algorithm entering momentarily an area of instability. If the parameters got far enough from the stability region, the algorithm would diverge to infinity, as is shown on Fig. 3b.\nFor effective use of these algorithms, some normalization may be necessary. In the case of the LMS, a normalization may be performed by conditionally changing the update equation as in (54):\nws,i = { ws,i\u22121 + \u00b5e(i)ys(i)\u2217u\u2217i if |ys(i)| \u2264 MAX ws,i\u22121 + \u00b5e(i)u\u2217i if |ys(i)| > MAX . (54)\nThis can be justified in a heuristic way. Whenever the algorithm starts diverging, |ys(i)| starts to increase. If it gets above a certain threshold, the update equation turns into the second line of (54). If the step-size is small enough, this equation is approximately the classical LMS, an algorithm that is known to be stable in various situations. This stops the divergent behavior of the algorithm. At this point, the w is not converging to the optimal value. This works as a gross convergence mode, that is guaranteed to be stable enough to allow |ys(i)| to decrease, entering again the the mode of fine convergence. Figure 4 shows a simulation of this modified algorithm, averaged through 1 million realizations, with no signs of instability.\n7 0 500 1000 1500 2000 2500 3000 \u221230 \u221220 \u221210 0\nIterations\ndB Mean Square Error (estimated)\nFig. 4. Simulation of recursion (54), with an ensemble average of 1 million realizations.\nAdditionally, a TRUE-LMS version of this modification can be done as\nws,i =\n{ ws,i\u22121 + \u00b5 L (ys,i \u25e6 Ui)\u2217ei if |ys(i)| \u2264 MAX\nws,i\u22121 + \u00b5 LU \u2217 i ei if |ys(i)| > MAX\n,\n(55)\nwhich works for the same reasons as before."}, {"heading": "E. Step Bounds", "text": "For an initial description of the algorithms to complete, there must exist a method to choose the parameters of the algorithms. In this section a heuristic bound is obtained, based on comparisons with the classical linear LMS.\nWhen computing the gradient for the classical algorithm, one obtains \u2207\u0303J = \u2212e(i)\u2217ui. This leads to an instantaneous bound for the algorithm of 0 < \u00b5 < 2/\u2016ui\u20162. The gradient obtained for the new algorithm is in (49). Given that the bound depends only on this gradient, one would expect that an equivalent inequality for the new algorithm would be\n0 < \u00b5 < 2\n\u2016yi \u2297 ui\u20162 =\n2\n\u2016yi\u20162\u2016ui\u20162 . (56)\nThis result is similar to the one obtained in [19]. Although important, instantaneous bounds may not be sufficient to choose the parameters of the algorithm. The classical LMS has a bound given by 0 < \u00b5 < 2/(3 trRu) that guarantees convergence in the MSE [26]. Given that Ru = E [u\u2217i ui], an argument by comparison shows that the new nonlinear algorithm would depend on the trace of\nE [(yi \u2297 ui)\u2217(yi \u2297 ui)] = E [(y\u2217i yi)\u2297 (u\u2217i ui)] (57)\nIn fact, simulations show that, for the exact gradient algorithm, this is exactly the parameter that governs convergence. If \u03bbmax is the biggest eigenvalue of limi\u2192\u221e E [(y\u2217i yi)\u2297 (u\u2217i ui)], then the steepest descent algorithm on (36) with initial conditions (37) converges to its global minimum whenever 0 < \u00b5 < 2/\u03bbmax. This is analogous to the linear steepest descent. [22]\nFor the adaptive algorithms, since yi is not a stationary signal, this is still not an universal bound. To make it so, one should consider the signal after convergence, since, at that point, the weight vectors w1, . . . , wK should be stationary,\nwhich makes yi an stationary signal, given that ui is also one. Under these conditions, the bound should be\n0 < \u00b5 < lim i\u2192\u221e\n2\n\u03b1 trE [(y\u2217i yi)\u2297 (u\u2217i ui)] , (58)\nwhere \u03b1 is a constant. Due to the complexity involved in determining analytically the value of \u03b1, which must take into account the modified algorithms (54) and (55), an experimental approach was used, which pointed to a possible value of \u03b1 = 3K , which depends on the order of nonlinearity K. This dependence can be explained by noting that higher K yields a noisier convergence, which may put the algorithms in a unstable region, if the step-size is too big. One should note that in the linear case (K = 1) this bound reduces to the classical LMS bound.\nIt is also possible to give an estimate to the MAX parameter:\nMAX = lim i\u2192\u221e (K + 1)\n\u221a E \u2223\u2223u\u2297Ki Wi\u2223\u22232. (59)\nThis value was also obtained mostly through experimental verification, but heuristic reasoning is also possible. The\u221a E \u2223\u2223u\u2297Ki Wi\u2223\u22232 factor appears because the parameter MAX gives a condition on |ys(i)|, so it should depend on the power of the output y(i). The factor K + 1 appears due to the experimental observation that the bigger the K, the noisier was the convergence, therefore a bigger value of MAX is necessary to allow this natural convergence behavior to be expressed, otherwise the algorithm would not converge to the lowest possible minimum.\nThese results should be valid for both the LMS and the TRUE-LMS, while, as it will be shown in simulations, they may be conservative choices for the latter, as it has better stability properties.In fact, It can be verified experimentally that for reasonably sized L the ideal value of MAX for the TRUE-LMS is roughly half of (59)."}, {"heading": "F. Computational Complexity", "text": "Table I shows an efficient implementation of the TRUELMS algorithm, together with the necessary number of operations in each step of the process. It uses the following auxiliary variables:\n8\nIn Table II and implementation of the LMS is shown. Some optimizations in relation to the TRUE-LMS are possible, which makes it more efficient than one would expect by using L = 1 in the previous table."}, {"heading": "VII. SIMULATIONS", "text": "Some experiments with the algorithm were run. In all of these, the input vector ui was chosen to have a delay-line structure as in\nui = [ u(i) u(i\u2212 1) . . . u(i\u2212M + 1) ] , (63)\nwith u(i) an i.i.d. signal sampled from the normal distribution N (0, 1).\nThe desired signal follows a \u201csystem identification\u201d model, with d(i) = u\u2297Ki Wo + v(i), (64) where Wo is the plant to be identified and v(i) is an i.i.d. signal sampled from the distribution N (0, \u03c32v) independent from u(i).\nAll the adaptive algorithms simulated here are the stabilized versions 54 and 55. They will be referred hereafter as SML."}, {"heading": "A. Decomposable plants", "text": "The simulations in this section use a decomposable plant, which represents the best case scenario for the SML algorithms. They were put to run against the Volterra-LMS and Wiener-LMS algorithms [27]. The Volterra algorithm is based on (10) and the Wiener one is a modified version in which the\n0 1000 2000 3000 4000 5000\n\u221240\n\u221230\n\u221220\n\u221210\n0\nIterations\ndB\nExcess Mean Square Error (estimated)\nVolterra\nWiener\nSML\n(a) K = 2 and \u03c32v = 10 \u22123.\n0 1000 2000 3000 4000 5000\n\u221240\n\u221230\n\u221220\n\u221210\n0\nIterations\ndB\nExcess Mean Square Error (estimated)\nSML\nVolterra\nWiener\n(b) K = 2 and \u03c32v = 10 \u22126, but with a different plant.\nresulting regressor is statistically orthonormal. They both are linear-in-the-parameters algorithms, in contrast with the SML ones, which are not.\nSimulations were done for K = 2 and K = 3 and they were repeated through 1.000 and 10.000 realizations, respectively. The plants were chosen randomly and normalized to result in unitary power. The memory parameter was chosen as M = 10 for all cases.\nThe algorithms were first simulated on an order K = 2 plant with a noise of \u03c32v = 10\n\u22123 and only the SML-LMS algorithm was tested. The Excess Mean Square Error2 (EMSE) curves are on Fig. 5a. The algorithms all have similar performances, but a difference starts to show after a few iterations, when the SML starts to converge faster and shows itself to be the first to reach convergence.\nFig. 5b shows a similar scenario, with \u03c32v = 10 \u22126; the only thing that was changed was the plant wo. It is still decomposable and normalized plant, just a different one. This figure shows how the SML performance may be dependent on the plant being identified.\nFig. 5c shows a simulation of the identification of an order K = 3 plant. Similar results were obtained. The special feature here is the really distinct trajectory of the SML algorithm, which is caused by it being nonlinear in the parameters.\nOn the first K = 2 plant, the LMS was simulated against the TRUE-LMS and the results are on Fig. 6. The TRUE-LMS is shown as marginally faster while converging to the same EMSE as the LMS. Although the differences in the figure are minimal, the TRUE-LMS appeared to be a more stable algorithm. This can be explained by noting that the statistical parameters in its formulation are an average of samples, which\n2Formally defined as E|(Wo \u2212Wi)u|2.\n9\nhelps remove outliers which otherwise would try to take the algorithm to a region of instability."}, {"heading": "B. Stability", "text": "As previously addressed in subsection VI-D, nonlinearities in the algorithm recursion may lead to non global stability. The following experiments use the same decomposable plant as the one used on Fig. 5a and are intended to showcase the stability of the algorithms with various step-sizes.\nThis experiment was run through a total 10.000 realizations and the number of diverging realizations was counted. Table III shows the results, for the LMS and the TRUE-LMS (L = 4 and L = 8), as a function of the step-size.\nIt is possible to notice the stability of the algorithms for the steps lower than \u00b50. Moreover, they become more stable the greater the value of L, which was expected. Additionally, for the TRUE-LMS, the step-bound \u00b50 may be too conservative, but this depends in a complex way on the value of L."}, {"heading": "C. Adaptive algorithms as an approximation of the steepest descent", "text": "Continuing with the plant on Fig. 5a, this section intends to show how the adaptive algorithms can be considered an approximation to the the steepest descent algorithm on (36), as long as the step-size is small enough. Here the curves of the TRUE-LMS are not shown, as they would be almost superimposed on the ones of the LMS, as it can be seen on Fig. 6. The curves of the adaptive algorithms form an ensemble average of 10.000 realizations and are present Fig. 8.\nWhen the step-size is small, as in Fig. 8a, the curves are almost superimposed. When it is increased, they start to take different trajectories. Another noticeable effect is the increase of the minimum MSE in the adaptive algorithm.\n10"}, {"heading": "D. A family of non-decomposable plants", "text": "In this section the hypothesis of decomposability is studied. A Volterra kernel given by a bivariate normal probability function was chosen as the family of test plants. The reason for this is that the the correlation coefficient \u03c1 turns out to be a good measure of the decomposability of the plant, with \u03c1 = being perfect decomposability and \u03c1 = 1 the worst-case scenario. In explicit terms, the plant is given by\nwo(i, j) = \u03b1 exp [ \u2212 (i\u2212 1)\n2 + (i\u2212 1)2 + 2\u03c1(i\u2212 1)(j \u2212 1) 18(1\u2212 \u03c12)\n] ,\n(66) where \u03b1 is a normalization parameter. A typical plant is shown on Fig. 7a.\nThe tests were made with \u03c32v = 10 \u22123 and by varying the \u03c1 parameter between 0 and 1 by increments of 0.1. The results of the tests are on Fig. 7b. As predicted, the best results were with low values of \u03c1 and the worst one were with high values. This can be explained by thinking about the SVD. When \u03c1 \u2248 0, the singular value distribution of wo is concentrated in a single one. It is to the matrix associated to this singular value that the algorithm converges. Since this matrix contains most of the energy of the system, the minimum MSE is low. Conversely, with \u03c1 \u2248 1 the distribution of singular values of this matrix is more uniform. Even if the algorithm converges to the term with most energy, there will be a lot of energy remaining in the system, which corresponds to a higher minimum MSE."}, {"heading": "E. SML versus algorithms in the literature", "text": "The algorithm was tested against various others from the literature, with K = 2. The ones tested were: \u2022 The Power Filter (PF) [24] on (11) with D = 1, which\nuses only the diagonal elements of the Volterra series. 22 coefficients. \u2022 The Simplified Volterra Filter (SV) [7], [8], the truncated diagonals model in (11), with D = 3.. 60 coefficients. \u2022 The Sparse Interpolated Volterra (IV) [9], [10], which estimates only a few entries of the Volterra kernel and interpolates the others. 66 coefficients. \u2022 The full Volterra model (V) on (10). 231 coefficients. \u2022 The SML-LMS algorithm on (50). 42 coefficients. The plant to be identified is a smooth one, as in [10]. It is represented in 9a. Here the memory parameter was M = 21. The results are on Fig. 9b. It shows the fast convergence and low minimum MSE of the SML filter, doing so with only 42 coefficients.\nFig. 9b shows a simulation against the Parallel Cascade Filter [11] (CF \u2013 65 coefficients), a model that can be represented as a product of exactly two Volterra filters, and the SML (20 coefficients). They try to identify an K = 3 random decomposable plant. This experiment is here to showcase an structurally similar algorithm to the SML. They are both formed by products and are nonlinear in the parameters, but in this case the SML fares better. What is important to notice from this figure is that both algorithms have similar convergence paths, something that didn\u2019t happen with any of the others."}, {"heading": "VIII. CHAOTIC BEHAVIOR", "text": "One last experiment concerns the emergence of chaotic behavior of the algorithm for certain values of the step-size. Assume a scalar model, with M = 1, and K = 2. In this simple case, the algorithm will be adapting two parameters. Moreover, assume the model\nd(i) = 100u(i)2, (67)\nwhere u(i) = 1 is a constant signal. The bifurcation diagram of the LMS algorithm, parametrized by \u00b5, is given by Fig. 10. it is possible to recognize a well-behaved convergence region in 0 < \u00b5 < 0.01, as predicted by step bounds. From there on, convergence starts to become oscillatory and starts to experience period doubling, until a point that is becomes fully chaotic. Starting from 0.016, the parameter starts jumping from positive to negative regions. A typical trajectory in this region is presented in Fig. 11.\nWhen for \u00b5 > 0.02, the algorithm starts to diverge, but until 0.023 this divergence seems chaotic. From there on it starts to appear monotonic, as expected.\nSuch behavior may be used in the way of improving convergence properties, such being part of a method to search for global minimums in multi-modal surfaces. This shall be a topic of investigation in further publications."}, {"heading": "IX. CONCLUSION", "text": "This paper presents a low-complexity nonlinear adaptive filter based of the rank one approximation of the Volterra\n11\nseries. This is motivated both by the exponential reduction in complexity and in the well-posedeness of the problem of rank-one approximations.\nThe rank-one model can be represented as a product of linear FIR filters, which results in algorithms with complexity O(KM), as opposed to the initial O(MK) of the full Volterra series.\nThe description of this model is followed by the posing of an estimation problem. Its solution is given in the form of a steepest descent algorithm, which, as in classical adaptive filter theory, can make for the ideal model of an adaptive filter\u2014they are approximations of the exact gradient algorithm. Indeed, simulations show that in the limit of small step sizes, the adaptive algorithms are expected to approximate really well the exact algorithm.\nThese adaptive filters derived as an rectangular window instantaneous approximation of the steepest descent. Depending on the number of samples on the window, two algorithms were derived: the LMS, with one sample, and the TRUE-LMS, which uses a general L number of samples. Their trajectories can be verified to be roughly the same, but their properties differ in terms of stability.\nDue to nonlinearities on the equations that implement the algorithms, it is expected that they do not achieve global asymptotic stability. An attempt at ameliorating this problem\nwas done via a heuristic limitation of the algorithm. Other heuristic considerations were done in the choice of parameters for the filter. These considerations were later verified, albeit non-extensively, in simulations.\nOther simulations involved testing of the algorithm in the identification of both rank-one and higher rank plants. It fares well in comparison with other algorithms when identifying many of these plants, specially those that can be considered approximately decomposable. The cases were the algorithms have poor performance were identified, for order K = 2 plants, as those whose Volterra kernels have evenly distributed singular values.\nThis paper finalizes with considerations on extensions. Some properties, like chaotic regions of the algorithm, may be studied further. Whether these properties either show to be detrimental to its performance or if reveal themselves to be of some application remains to be investigated.\nOther areas of extension is in the algorithms themselves. The solution to the estimation problem can be further studied by the used of Newton\u2019s method [20]. There is some work to be done in the ill-posedness of the general rank approximation and any development there could be applied in the development of general rank algorithms. Another extension is in the use of combinations of filters. For example, data-reuse-type algorithms, such as the TRUE-LMS, can have its performance\n12\nimproved by the use of incremental combinations. [28] The same should apply in the algorithms developed here."}, {"heading": "APPENDIX A NORMS AND INNER PRODUCTS OF TENSORS", "text": "Let (V1, \u3008\u00b7, \u00b7\u30091), (V2, \u3008\u00b7, \u00b7\u30092), . . . , (VK , \u3008\u00b7, \u00b7\u3009K) be complex inner product spaces. The natural way of inducing an inner product on the tensor product V1\u2297V2\u2297\u00b7 \u00b7 \u00b7\u2297VK is by defining the inner product on the decomposable tensors as\n\u3008v1 \u2297 v2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 vK , w1 \u2297 w2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK\u3009 , \u3008v1, w1\u30091\u3008v2, w2\u30092 \u00b7 \u00b7 \u00b7 \u3008vK , wK\u3009K (68)\nand extending through sesquilinearity for general tensors, which are linear combinations of decomposable ones.\nThis inner product induces an `2 norm in the usual way as \u2016T \u2016 = \u221a \u3008T , T \u3009. For decomposable tensors, this is expressed as\n\u2016v1 \u2297 v2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 vK\u2016 = \u2016v1\u20161\u2016v2\u20162 \u00b7 \u00b7 \u00b7 \u2016vK\u2016K , (69)\nwhere each \u2016\u00b7\u2016k is the respective induced `2 norm. For general normed spaces the induced tensor norms are not as simply (or uniquely) defined, but reasonable ones should satisfy the cross norm identity on the decomposable tensors, which is also represented by the previous equation. On Banach spaces, an example of tensor norm is the projective norm, given by\n\u2016T \u2016\u03c0 = inf {\u2211\ni\n\u2016v1,i\u20161\u2016v2,i\u20162 \u00b7 \u00b7 \u00b7 \u2016vK,i\u2016K :\n\u2211 i\nv1,i \u2297 v2,i \u2297 \u00b7 \u00b7 \u00b7 \u2297 vK,i = T } , (70)\nthat is, the smallest sum of norms of all the possible forms of writing T as a sum of decomposable terms. Another norm, called the injective norm, is defined in terms of linear functionals, but, regardless of the details, what is important is that these are all cross norms i.e. they separate on the decomposable tensors."}], "references": [{"title": "Limitations of handsfree acoustic echo cancellers due to nonlinear loudspeaker distortion and enclosure vibration effects", "author": ["A.N. Birkett", "R.A. Goubran"], "venue": "Applications of Signal Processing to Audio and Acoustics, 1995., IEEE ASSP Workshop on, Oct 1995, pp. 103\u2013106.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Modulation and detection for data transmission on the telephone channel", "author": ["R.W. Lucky"], "venue": "New Directions in Signal Processing in Communication and Control, 1975.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1975}, {"title": "Adaptive linearization of a loudspeaker", "author": ["F.X.Y. Gao", "W.M. Snelgrove"], "venue": "Acoustics, Speech, and Signal Processing, 1991. ICASSP- 91., 1991 International Conference on, Apr 1991, pp. 3589\u20133592 vol.5.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "New nonlinear adaptive fir digital filter for broadband noise cancellation", "author": ["H.K. Kwan", "Q.P. Li"], "venue": "IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, vol. 41, no. 5, pp. 355\u2013360, May 1994.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Adaptive polynomial filters", "author": ["V. Mathews"], "venue": "Signal Processing Magazine, IEEE, vol. 8, no. 3, pp. 10\u201326, July 1991.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "Low-complexity nonlinear adaptive filters for acoustic echo cancellation in gsm handset receivers", "author": ["A. Fermo", "A. Carini", "G.L. Sicuranza"], "venue": "European Transactions on Telecommunications, vol. 14, no. 2, pp. 161\u2013169, 2003. [Online]. Available: http://dx.doi.org/10.1002/ett.908", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Simplified Volterra filters for acoustic echo cancellation in gsm receivers", "author": ["\u2014\u2014"], "venue": "Signal Processing Conference, 2000 10th European, Sept 2000, pp. 1\u20134.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "A fully lms adaptive interpolated Volterra structure", "author": ["E. Batista", "O.J. Tobias", "R. Seara"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on, March 2008, pp. 3613\u20133616.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "A sparse-interpolated scheme for implementing adaptive Volterra filters", "author": ["E. Batista", "O. Tobias", "R. Seara"], "venue": "Signal Processing, IEEE Transactions on, vol. 58, no. 4, pp. 2022\u20132035, April 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2022}, {"title": "Adaptive parallelcascade truncated Volterra filters", "author": ["T.M. Panicker", "V.J. Mathews", "G.L. Sicuranza"], "venue": "IEEE Transactions on Signal Processing, vol. 46, no. 10, pp. 2664\u20132673, Oct 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Nonlinear system modeling and identification using volterra-parafac models", "author": ["G. Favier", "A.Y. Kibangou", "T. Bouilloc"], "venue": "International Journal of Adaptive Control and Signal Processing, vol. 26, no. 1, pp. 30\u201353, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Multilinear Algebra, ser. Universitext", "author": ["W. Greub"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Low rank estimation of higher order statistics", "author": ["T. Andre", "R. Nowak", "B. Van Veen"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on, vol. 5, May 1996, pp. 3026\u2013308a vol. 5.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Tensor product basis approximations for volterra filters", "author": ["R. Nowak", "B. Van Veen"], "venue": "Signal Processing, IEEE Transactions on, vol. 44, no. 1, pp. 36\u201350, Jan 1996.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Tensors: a brief introduction", "author": ["P. Comon"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 3, pp. 44\u201353, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A finite algorithm to compute rank-1 tensor approximations", "author": ["A.P. da Silva", "P. Comon", "A.L.F. de Almeida"], "venue": "IEEE Signal Processing Letters, vol. 23, no. 7, pp. 959\u2013963, July 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient-based approaches to learn tensor products", "author": ["M. Rupp", "S. Schwarz"], "venue": "Signal Processing Conference (EUSIPCO), 2015 23rd European, Aug 2015, pp. 2486\u20132490.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A tensor lms algorithm", "author": ["\u2014\u2014"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), April 2015, pp. 3347\u20133351.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Newton-like nonlinear adaptive filters via simple multilinear functionals", "author": ["F.C. Pinheiro", "C. Lopes"], "venue": "2016 24th European Signal Processing Conference (EUSIPCO) (EUSIPCO 2016), Budapest, Hungary, Aug. 2016, pp. 1588\u20131592.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensor rank and the ill-posedness of the best low-rank approximation problem", "author": ["V. de Silva", "L.-H. Lim"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 30, no. 3, pp. 1084\u20131127, 2008. [Online]. Available: http://dx.doi.org/10.1137/06066518X", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Adaptive Filters", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Kronecker products and matrix calculus in system theory", "author": ["J. Brewer"], "venue": "Circuits and Systems, IEEE Transactions on, vol. 25, no. 9, pp. 772\u2013781, Sep 1978.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1978}, {"title": "Nonlinear acoustic echo cancellation using adaptive orthogonalized power filters", "author": ["F. Kuech", "A. Mitnacht", "W. Kellermann"], "venue": "Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP \u201905). IEEE International Conference on, vol. 3, March 2005, pp. iii/105\u2013iii/108 Vol. 3.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Probability of divergence for the least-mean fourth algorithm", "author": ["V.H. Nascimento", "J.C.M. Bermudez"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 4, pp. 1376\u20131385, April 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Convergence analysis of lms filters with uncorrelated gaussian data", "author": ["A. Feuer", "E. Weinstein"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 33, no. 1, pp. 222\u2013230, Feb 1985.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1985}, {"title": "Adaptive Nonlinear System Indentification: The Volterra and Wiener Model Approaches", "author": ["T. Ogunfunmi"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "There\u2019s plenty of room at the bottom: Incremental combinations of sign-error lms filters", "author": ["L.F.O. Chamon", "C.G. Lopes"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2014, pp. 7248\u20137252.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "A few situations that may ask for their use are echo cancellation [1], equalization of communication systems [2], acoustics [3], or broadband noise canceling [4].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "A few situations that may ask for their use are echo cancellation [1], equalization of communication systems [2], acoustics [3], or broadband noise canceling [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "A few situations that may ask for their use are echo cancellation [1], equalization of communication systems [2], acoustics [3], or broadband noise canceling [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "A few situations that may ask for their use are echo cancellation [1], equalization of communication systems [2], acoustics [3], or broadband noise canceling [4].", "startOffset": 158, "endOffset": 161}, {"referenceID": 4, "context": "This complexity may be high even for modern computers, as is the case of the Volterra series [5], [6] and its exponentially increasing complexity.", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "Examples of these approaches are the filters with truncated diagonals [7], [8], which works with the central coefficients of the model, which are usually the most significant ones.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "Examples of these approaches are the filters with truncated diagonals [7], [8], which works with the central coefficients of the model, which are usually the most significant ones.", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "Another kind of model is based in interpolation techniques [9], [10], in a way that the algorithms work with only a few coefficients while interpolating the others.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "Another kind of model is based in interpolation techniques [9], [10], in a way that the algorithms work with only a few coefficients while interpolating the others.", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "This is the case with cascade structures [11].", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "An example is ones based in tensor decomposition[12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "Tensors are also a bridge between the multilinear [13] world\u2014of multilinear functions, or functions of many variables that are linear in each one of them\u2014and the linear world.", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "Not surprisingly, tensors have indeed been used in nonlinear signal processing-related problem for quite some time [14], [15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Not surprisingly, tensors have indeed been used in nonlinear signal processing-related problem for quite some time [14], [15].", "startOffset": 121, "endOffset": 125}, {"referenceID": 14, "context": "The tensor representation, and specially tensor rank decomposition [16] allows for a dramatic decrease in representational complexity of the model and, should this be exploited in the Volterra series, many low-complexity algorithms may be obtained.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Of particular importance is the rank-one approximation [17].", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "This concept has seem some success in approximating linear responses [18], [19] with certain structures, and the possible gains to nonlinear signal processing are even more pronounced\u2014exponential reductions in complexity can be achieved [20].", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "This concept has seem some success in approximating linear responses [18], [19] with certain structures, and the possible gains to nonlinear signal processing are even more pronounced\u2014exponential reductions in complexity can be achieved [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "This concept has seem some success in approximating linear responses [18], [19] with certain structures, and the possible gains to nonlinear signal processing are even more pronounced\u2014exponential reductions in complexity can be achieved [20].", "startOffset": 237, "endOffset": 241}, {"referenceID": 19, "context": "The other theoretical advantage of rank-one approximations is their well-posedness, which may not be the case for general low-rank decompositions [21].", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": "Notation The notation of the paper follows the one on [22], while introducing new notation when necessary.", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "the tensor product \u2297 is to be interpreted as the Kronecker product [23].", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "[16] Just like vectors can alternatively characterized by the concept of a vector space, tensors can be abstractly defined as objects of an algebraic tensor product [13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[16] Just like vectors can alternatively characterized by the concept of a vector space, tensors can be abstractly defined as objects of an algebraic tensor product [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "In computational terms, the tensor product can be implemented as a Kronecker product [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "This problem has the potential to decrease the number of parameters necessary to represent the tensor, but it is, in general, not well posed and may not have a solution [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 19, "context": "Proof given by [21] in Proposition 4.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "For example, for second order kernels, one possible approach is to truncate some of the diagonals of H2(i, j) [7], [8], which allows the the computation to be written as", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "For example, for second order kernels, one possible approach is to truncate some of the diagonals of H2(i, j) [7], [8], which allows the the computation to be written as", "startOffset": 115, "endOffset": 118}, {"referenceID": 22, "context": "When D = 1, this uses only the main diagonal, with M parameters, resulting in a model called the Power Filter [24].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "This representation has been used, for symmetric tensors, in [12], to reduce the representational complexity of the series.", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "A kernel like this will be said to satisfy the decomposable model\u2014also called the Simple Multilinear Model (SML) [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 20, "context": "(28) The other terms from (18) involve only the conjugates of the entries of w, so their Wirtinger derivatives become zero [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 20, "context": "When doing this to the linear estimation problem, the algorithm obtained is the classical LMS [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "Stabilization of the Algorithms It has been known that nonlinearities in the update equations may lead to a nonzero probability of divergence for the algorithm, no matter how small is the step-size [25], whenever the probability distribution of the signals has infinite support.", "startOffset": 198, "endOffset": 202}, {"referenceID": 17, "context": "This result is similar to the one obtained in [19].", "startOffset": 46, "endOffset": 50}, {"referenceID": 24, "context": "The classical LMS has a bound given by 0 < \u03bc < 2/(3 trRu) that guarantees convergence in the MSE [26].", "startOffset": 97, "endOffset": 101}, {"referenceID": 20, "context": "[22] For the adaptive algorithms, since yi is not a stationary signal, this is still not an universal bound.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "They were put to run against the Volterra-LMS and Wiener-LMS algorithms [27].", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "The ones tested were: \u2022 The Power Filter (PF) [24] on (11) with D = 1, which uses only the diagonal elements of the Volterra series.", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "\u2022 The Simplified Volterra Filter (SV) [7], [8], the truncated diagonals model in (11), with D = 3.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "\u2022 The Simplified Volterra Filter (SV) [7], [8], the truncated diagonals model in (11), with D = 3.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "\u2022 The Sparse Interpolated Volterra (IV) [9], [10], which estimates only a few entries of the Volterra kernel and interpolates the others.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "\u2022 The Sparse Interpolated Volterra (IV) [9], [10], which estimates only a few entries of the Volterra kernel and interpolates the others.", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "The plant to be identified is a smooth one, as in [10].", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "9b shows a simulation against the Parallel Cascade Filter [11] (CF \u2013 65 coefficients), a model that can be represented as a product of exactly two Volterra filters, and the SML (20 coefficients).", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "The solution to the estimation problem can be further studied by the used of Newton\u2019s method [20].", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "[28] The same should apply in the algorithms developed here.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "This work proposes a low complexity nonlinearity model and develops adaptive algorithms over it. The model is based on the decomposable\u2014or rank-one, in tensor language\u2014 Volterra kernels. It may also be described as a product of FIR filters, which explains its low-complexity. The rank-one model is also interesting because it comes from a well-posed problem in approximation theory. The paper uses such model in an estimation theory context to develop an exact gradienttype algorithm, from which adaptive algorithms such as the least mean squares (LMS) filter and its data-reuse version\u2014the TRUE-LMS\u2014are derived. Stability and convergence issues are addressed. The algorithms are then tested in simulations, which show its good performance when compared to other nonlinear processing algorithms in the literature.", "creator": "LaTeX with hyperref package"}}}