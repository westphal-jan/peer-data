{"id": "1610.07651", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation", "abstract": "This document donders briefly describes gangwon the steyr-daimler-puch systems catalyst submitted infogear by the anpac Center bradlees for Robust covert Speech Systems (kcm CRSS) from gosdin The 2,783 University of gara\u0161anin Texas huske at Dallas (500-meter UTD) bratkowski to the 2016 konrad National euro270 Institute modan of bahana Standards and load-carrying Technology (NIST) sicily Speaker 116.31 Recognition surfliner Evaluation (SRE ). We rudder developed affifuddin several UBM schnall and DNN zied i - 2400m Vector based shanon speaker recognition systems with different data raikes sets glop and d?cor feature udaya representations. mt1 Given that the lucanamarca emphasis effectuate of britpop the NIST SRE re-arranged 2016 83.13 is on metallized language 1763 mismatch willendorf between boccie training and lucyna enrollment / test sccs data, batuta so - ragunan called domain mismatch, gzip in our ckx system paperwork development ph\u01b0\u1edbc we brammertz focused on: (1) sionko using unlabeled in - kohlberg domain 20,444 data dolcenera for qareh centralizing cannabidiol data 598 to sabuni alleviate antley the domain 79.61 mismatch bostwick problem, (dryers 2) finding the palaeontologist best data exorcised set prazuck for training montanas LDA / PLDA, (9:20 3) hyoid using newly proposed kri\u017e dimension reduction technique extensionality incorporating gandolfi unlabeled inflation in - geran domain breweries data dep before loughnane PLDA \u0111uro training, (21.66 4) dwell unsupervised speaker 320,000 clustering frets of i-8 unlabeled data and using lightscribe them jinyan alone nover or with al-ansar previous SREs for amawalk PLDA beco training, (5) spoe score calibration ruggles-brise using only unlabeled zholtok data n.c.-based and almeria combination \u03bf\u03c5 of unlabeled lacock and development (Dev) data ii. as separate 169.1 experiments.", "histories": [["v1", "Mon, 24 Oct 2016 21:05:05 GMT  (19kb)", "http://arxiv.org/abs/1610.07651v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chunlei zhang", "fahimeh bahmaninezhad", "shivesh ranjan", "chengzhu yu", "navid shokouhi", "john h l hansen"], "accepted": false, "id": "1610.07651"}, "pdf": {"name": "1610.07651.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Chunlei Zhang", "Fahimeh Bahmaninezhad", "Shivesh Ranjan", "Chengzhu Yu"], "emails": ["chunlei.zhang@utdallas.edu", "fahimeh.bahmaninezhad@utdallas.edu", "john.hansen@utdallas.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n07 65\n1v 1\n[ cs\n.C L\n] 2\n4 O\nct 2\n01 6\nThis document briefly describes the systems submitted by the Center for Robust Speech Systems (CRSS) from The University of Texas at Dallas (UTD) to the 2016 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE). We developed several UBM and DNN i-Vector based speaker recognition systems with different data sets and feature representations. Given that the emphasis of the NIST SRE 2016 is on language mismatch between training and enrollment/test data, so-called domain mismatch, in our system development we focused on: (1) using unlabeled indomain data for centralizing data to alleviate the domain mismatch problem, (2) finding the best data set for training LDA/PLDA, (3) using newly proposed dimension reduction technique incorporating unlabeled in-domain data before PLDA training, (4) unsupervised speaker clustering of unlabeled data and using them alone or with previous SREs for PLDA training, (5) score calibration using only unlabeled data and combination of unlabeled and development (Dev) data as separate experiments."}, {"heading": "1. INTRODUCTION", "text": "The main task for NIST\u2019s speaker recognition evaluations is speaker detection, i.e., to determine whether a specified target speaker is speaking during a given segment of speech or not. Compared with previous SRE challenges, there are some differences: (1) target speaker data will not be distributed in advance like in SRE12, (2) fixed condition is introduced, (3) more duration variability is introduced in the test data, (4) language mismatch between training (mainly English) and enrollment/test (non-English) data. All these new traits make this SRE very challenging, especially with limited labeled data in the fixed condition [1].\nThis report introduces how CRSS systems address the problem. The whole report is organized as follows: Sec.2 describes several baseline systems focusing on the front-end level overview, including both data sets and feature representations, Sec.3 introduces several core techniques that we used in SRE16, including speaker clustering for unlabeled training data, discriminant analysis via support vectors (SVDA) to reduce dimension and compensate domain mismatch, unlabeled data PLDA, calibration and fusion strategies etc. Sec.4 and Sec.5 details the configuration of every CRSS sub-system and the formation of CRSS final Eval submissions to NIST. Sec.6 shows the CRSS sub-system performance on Dev data. We also briefly introduce the CPU/GPU hardware systems that we used for NIST SRE 2016 in Sec.7."}, {"heading": "2. CRSS BASELINES", "text": "We developed 4 baseline systems in this SRE, all of them are i-Vector based systems but with different acoustic modeling, i.e., UBM or different DNN models [2, 3]. For back-end, we mainly use LDA/SVDA to reduce the dimension of i-Vectors and PLDA to calculate likelihood scores.\nTable 1 summarized the number of speakers, speech segments used for training UBM, total variability matrix (TV), LDA/SVDA and PLDA models as well as the statistics for the Dev set provided by NIST for the system development purposes."}, {"heading": "2.1. CRSS1: UBM i-Vector", "text": "This system is mainly modified version of Kaldi (sre10/v1). 60 dimensional feature vectors for each frame is adopted here including 20 dimensional MFCC features appended with \u2206+\u2206\u2206. Unvoiced parts of the utterances are removed with energy based voice activity detection (VAD). For training 2048-mixture UBM and total variability (TV) matrix, SRE2004, 2005, 2006, 2008, telephone data of SRE 2010, Switchboard II phase 2,3 and Switchboard Cellular Part1 and Part2 (SWB) and Fisher English are used. Next, 600 dimensional i-Vectors are extracted and their dimensions are reduced to 580 with LDA. For training LDA/PLDA, only SRE 04-08 are used; in addition, speakers who have less than 4 utterances is filtered out. Also, unsupervised speaker clustering is performed (see Sec. 3.1 for the details of speaker clustering), 75 speaker clusters for unlabeled minor data and 300 for unlabeled major data are generated. These clustered in-domain data are used separately to train PLDA and also for calibration. Before PLDA scoring, mean subtraction is also applied. For SRE16 development (Dev) trials, the mean i-Vector is generated using only unlabeled minor data, while for SRE16 evaluation (Eval), the mean is from unlabeled major data."}, {"heading": "2.2. CRSS2: SWB DNN i-Vector", "text": "We developed a DNN i-Vector system based on Kaldi (swbd/s5 & sre10/v2). The DNN acoustic model is used to generate the soft alignments for i-Vector extraction. The DNN architecture has 6 fully connected hidden layers with 1024 nodes for each layer. Crossentropy objective function is employed to estimate posterior probabilities of 3178 senones. The ASR corpus which we used for training DNN acoustic model is Switchboard. 11-frame context of 39 dimensional (\u2206 +\u2206\u2206 ) MFCC feature is projected into 40 dimensional using fMLLR transform for each utterance, which relies on a GMM-HMM decoding alignment.\nThe reason we apply fMLLR feature here is that, by speaker normalization, we expect to acquire more accurate phonetic alignment in the following TV matrix training, see more details in [3]. After\ni-Vector extraction, we apply similar strategies for back-end such as LDA and PLDA, briefly described in the above section (as CRSS1)."}, {"heading": "2.3. CRSS3: UBM i-Vector", "text": "An alternative UBM i-Vector system also adopted from Kaldi (sre10/v1). In this system, feature vectors contain 20 MFCCs appended with (\u2206 + \u2206\u2206) coefficients. The window length and shift size are 25-ms and 10-ms, respectively. In addition, we did cepstral mean normalization using 3-sec sliding window. Next, non-speech frames are discarded using energy-based voice activity detection. 2048-mixture full covariance UBM and total variability matrix have been trained using data collected from SRE2004, 2005, 2006, 2008 and Switchboard II phase 2,3 and Switchboard Cellular Part1 and Part2. At the back-end, after extracting i-Vectors, the global mean calculated from minor and major unlabeled data is subtracted from all i-Vectors. Next, i-Vectors are length-normalized and their dimension are reduced from 600 to 400 using LDA/SVDA. In some developed systems based on CRSS3 configuration, we used both LDA and SVDA; more specifically, first SVDA reduces the dimension from 600 to 500 and then LDA is used to reduce the dimension to 400. Again, i-Vectors are length-normalized. Finally, trial-based mean subtraction is used (the participant i-Vectors in a trial are averaged and the value is subtracted from both i-Vectors) and scores are calculated using PLDA. The front end is trained with SWB and SRE04-08; however, the back-end only uses SRE04-08 and unlabeled training data. For back-end, mostly MSR [4] toolkit has been adopted."}, {"heading": "2.4. CRSS4: Fisher English DNN i-Vector", "text": "The last baseline is a DNN i-Vector system using Kaldi (sre10/v2) that is based on the multisplice time delay DNN (TDNN) [5]. TDNN is trained with only a small portion of Fisher English data (1239 utterances). The feature vectors contain 40 dimensional f-bank features. TDNN has six layers; the hidden layers have an input dimension of 350 and an output dimension 3500. The softmax output layer computes posteriors for 3859 triphone states. More details on the TDNN structure and training procedure are provided in [5]. After TDNN training, 20 MFCCs appended with (\u2206 + \u2206\u2206) coefficients (overall 60 MFCCs) are employed for training TV matrix. Next, 600-dim i-Vectors are extracted.\nAfter i-Vector extraction, we apply similar strategies for backend such as LDA/SVDA and PLDA, briefly described in the above section (as CRSS3)."}, {"heading": "3. CORE COMPONENTS IN SYSTEM DEVELOPMENT", "text": "In the fixed condition of SRE 2016, we have a huge amount of outof-domain data, i.e., previous SREs, SWB, Fisher English etc. Only a small in-domain data is available (without speaker labels), which\nmake existing techniques very difficult to work with this so-called domain mismatch. In SRE 2016, NIST provided unlabeled training data, which contains two subsets, i.e., unlabeled minor and unlabeled major. The unlabeled minor data set has 200 utterances, while the major set contains 2272 utterances. The minor set has two languages for the purpose of system development, while the major set contains two different languages corresponding to the final evaluation.\nIn order to address this problem, several techniques are proposed in this evaluation."}, {"heading": "3.1. Speaker clustering of unlabeled data", "text": "For compensating the domain mismatch, the use of unlabeled data becomes very important. There are several stages where we can use the unlabeled data, such as, LDA/PLDA training and calibration. First, it is very intuitive to do a speaker clustering of the unlabeled data, and then generate an \u201cestimated\u201d speaker label for each utterance, similar with the method that we used in 2015 NIST LRE iVector challenge[6]. With these labels, we incorporate the in-domain information from unlabeled data to train LDA and PLDA. In fact, in the experiment, this simple operation improved the LDA/PLDA baseline performance for development set.\nIn practice, we train a gender identification using previous SRE data before speaker clustering, and then apply a simple K-means algorithm over gender dependent subsets, finally, we pool these two subsets together. In the experiment, we found this can provide more accurate speaker clustering and more benefits to the following LDA and PLDA training."}, {"heading": "3.2. Discriminant analysis via support vectors (SVDA)", "text": "Discriminant analysis via support vectors (SVDA) is a variation of LDA that only uses support vectors to calculate the between and within class covariance matrices. In contrast to LDA, SVDA captures the boundary of classes, and performs well for small sample size problem (i.e. when the dimensionality is greater than sample size). The idea of using support vectors with discriminant analysis has been previously introduced in [7] which made significant improvement over LDA. In addition, the effectiveness of SVDA in iVector/PLDA speaker recognition for NIST SRE2010 is studied in [8] previously for both long and short duration test utterances.\nMore specifically, LDA defines speaker classes separation criterion in direction A as,\n\u03bb = ATSbA\nATSwA , (1)\nwhere Sb and Sw are between class and within class covariance matrices. In traditional LDA every sample of all classes participate in calculating these covariance matrices; however, for SVDA only support vectors are used. The between class covariance matrix in SVDA is defined as,\nSb = \u2211\n1\u2264c1\u2264c2\u2264C\nwc1c2w T c1c2 . (2)\nWhere wc1c2 is the optimal direction to separate two classes c1 and c2 by a linear SVM (for calculating wc1c2 only support vectors of the two classes c1 and c2 are participating). If we define X\u0302 = [x\u03021, x\u03022, ..., x\u0302N\u0302 ] to contain all support vectors and N\u0302 to be their total number; then, the within class covariance matrix for SVDA will be formulated as,\nSw = C\u2211\nc=1\n\u2211\ni\u2208I\u0302c\n(x\u0302i \u2212 \u00b5\u0302c)(x\u0302i \u2212 \u00b5\u0302c) T , (3)\nthe index for support vectors in class c and their mean are represented by I\u0302c and \u00b5\u0302c, respectively. Finally, similar to LDA, the optimum transformation A\u0302 will contain the k eigenvectors corresponding to the k largest eigenvalues of S\u22121w Sb. More details on the advantages and properties of SVDA are provided in [8].\nTwo strategies can be adopted here for training linear SVM in SVDA framework: 1) one-versus-one and 2) one-versus-rest. We used the second approach as it can use minor and major unlabeled data optimally. More specifically, for training the SVM classifier to separate one class against data from the rest classes, minor and major unlabeled data are added to the rest class. Therefore, the class labels are not needed here."}, {"heading": "3.3. Unlabeled data PLDA", "text": "To fully explore the information from the in-domain unlabeled data, we did an interesting experiment, which uses only the in-domain unlabeled data to train PLDA (however, SRE04-08 are used for training LDA). To do that, we use the \u201cestimated\u201d labels from speaker clustering. Surprisingly, PLDA with only 75 estimated speakers for 200 minor language i-Vectors achieved 20.5% EER on Dev experiment (using i-Vectors for CRSS1 baseline), which is not so bad. However, if we add more data (i.e., 75 + 300 estimated speakers from 2472 i-Vectors in minor and major languages) for PLDA training, the performance degraded from 20.5% to 26.3%. This observation suggests that out-of-domain language data is not helpful to train a discriminative classifier, because in the view of Dev enrollment/test data, the major language data is also out-of-domain. We argue that even for data-driven algorithm such as PLDA, choosing a proper data set to train the classifier is still essential.\nMotivated by this, we believe the use of unlabeled data in the SRE 2016 evaluation will be more beneficial. Compared with only 200 minor language utterances, major language set has 2272 utterances. Although the speaker label is not given, we say the estimated\nlabel is still useful, and could probably perform better than 20.5% EER in the Eval set."}, {"heading": "3.4. Calibration and fusion", "text": "The CRSS calibration and fusion system is mainly based on the BOSARIS toolkit [9]. The PAV algorithm is used to create calibration transformation matrix. We used two data sets for calibration. The first one is Dev data, we use all the Dev trials information that NIST provided for system development to train the calibration system. Again, because in this SRE, the Dev and evaluation set have totally different languages, it\u2019s not guaranteed that the calibration will work well for the final Eval set. For this consideration, we created a new trial list to calibrate evaluation scores, and we used unlabeled data with estimated speaker labels. We believe the score distribution of unlabeled data will be closer to that of evaluation.\nAfter calibration, we fused our sub-systems for final submission. For system fusion, we employed a simple linear fusion system using logistic regression."}, {"heading": "4. CRSS SUB-SYSTEMS", "text": "We developed 7 sub-systems from 4 CRSS baselines that used SREs to train SVDA, LDA and PLDA. Also, as described above, we developed 4 sub-systems with just unlabeled data PLDA idea. The details of each system about data and techniques they used are listed in Table 2 and Table 3. More specifically, sub-systems 8, 9, 10, 11 are respectively share the same configuration as sub-systems 3, 4, 6, 2; however, only unlabeled data are used to train PLDA."}, {"heading": "5. CRSS SUBMISSIONS", "text": "The final submissions of CRSS is the fusion of several sub-systems. In the final submission, we tried different system combinations as well as different calibration strategies. We submit a 1-7 sub-systems fusion with Dev+unlabeled data for calibration as our primary submission. To test our hypothesis that unlabeled PLDA idea will benefit for the Eval set, we submitted this as a contrastive submission. All these combinations make our final submissions to SRE 2016."}, {"heading": "6. PERFORMANCE OF CRSS SUB-SYSTEMS ON SRE 2016 DEV DATA", "text": "Tables 5, 6, and 7 shows the equal error rate (EER), minimum Cprimary(min-Cprimary) and actual Cprimary (act-Cprimary) costs for single systems and fusion systems using NIST scoring software. These results are evaluated on Dev set."}, {"heading": "7. COMPUTATIONAL RESOURCES", "text": ""}, {"heading": "7.1. CPU cluster", "text": "The speaker recognition system was implemented on our inhouse high-performance Dell computing cluster, running Rocks 6.0 (Mamba) Linux distribution. The cluster comprises of eight 6C Intel Xeon 2.67 GHz CPUs, four 10C Intel Xeon 2.40 GHz CPUs, and 18 quad-core Intel Xeon 2.33 GHz CPUs, yielding a total of 408 processors. The total amount of internal RAM on the cluster exceeds 1 TB. All our data including audio files, features, statistics, etc. are stored on a 30 TB Dell PowerVault MD1000 direct attached storage."}, {"heading": "7.2. GPU machines", "text": "For DNN training on SWB data, GeForce GTX TITAN Black graphic card is used, 6144 MB Ram. For DNN training on Fisher English, we used a 12 GB Tesla K40."}, {"heading": "7.3. CPU execution time", "text": "We tested the systems scoring process using one CPU of 2.67 GHz clock speed and 32 GB RAM. We selected a 3 minute utterance (exact duration of 181.45 seconds) and calculated the time required to perform feature extraction (20 dimensional MFCC), voice activity detection (Kaldi SRE10/v1 default), extraction of zero and first order statistics and the 600 dimensional i-Vector. The time required for this chain of processes is for the selected utterance is 37.58s. This is computed by averaging the elapsed time obtained from three independent runs. Scoring an utterance using our PLDA model takes less than 0.1 seconds on average. For training the models, it depends on how many enrollment utterances are provided. Since the UBM and TV matrices are trained off-line, speaker enrollment requires only to extract the corresponding i-Vectors, thus the time required will be a multiple of the number of enrollment utterances provided for a speaker."}, {"heading": "8. ACKNOWLEDGEMENT", "text": "We would like to thank Dr. Kong Aik Lee for providing the unlabeled trial list, organizing I4U meetings, and other I4U group members sharing many ideas and insights during I4U meetings. We would like to thank Qian Zhang, Abhinav Misra, Dr. Finnian Kelly and other CRSS colleagues for their many insights and helpful discussions in the development of the systems. We would like to thank Dr. Gang Liu (CRSS graduate) from Alibaba Group for providing the fusion system."}, {"heading": "9. REFERENCES", "text": "[1] \u201cNIST 2016 speaker recognition evaluation plan,\u201d https://www.nist.gov/sites/default/files/documents/itl/iad/mig/SRE16 Eval Plan V1-0.pdf, 2016.\n[2] Y. Lei, N. Scheffer, L. Ferrer, and M. McLaren, \u201cA novel scheme for speaker recognition using a phonetically-aware deep neural network,\u201d in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 1695\u2013 1699.\n[3] S. O. Sadjadi, S. Ganapathy, and J. Pelecanos, \u201cThe ibm 2016 speaker recognition system,\u201d arXiv preprint arXiv:1602.07291, 2016.\n[4] S. O. Sadjadi, M. Slaney, and L. Heck, \u201cMsr identity toolbox v1. 0: A matlab toolbox for speaker-recognition research,\u201d Speech and Language Processing Technical Committee Newsletter, vol. 1, no. 4, 2013.\n[5] D. Snyder, D. Garcia-Romero, and D. Povey, \u201cTime delay deep neural network-based universal background models for speaker recognition,\u201d in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 92\u2013 97.\n[6] C. Yu, C. Zhang, S. Ranjan, Q. Zhang, A. Misra, F. Kelly, and J. H.L. Hansen, \u201cUtd-crss system for the nist 2015 language recognition i-vector machine learning challenge,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5835\u20135839.\n[7] S. Gu, Y. Tan, and X. He, \u201cDiscriminant analysis via support vectors,\u201d Neurocomputing, vol. 73, no. 10, pp. 1669\u20131675, 2010.\n[8] F. Bahmaninezhad and J. H.L. Hanesn, \u201ci-vector/PLDA speaker recognition using support vectors with discriminant analysis,\u201d in (submitted to) IEEE ICASSP, 2017.\n[9] N. Bru\u0308mmer and E. de Villiers, \u201cThe bosaris toolkit: Theory, algorithms and code for surviving the new dcf,\u201d arXiv preprint arXiv:1304.2865, 2013."}], "references": [{"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 1695\u2013 1699.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "The ibm 2016 speaker recognition system", "author": ["S.O. Sadjadi", "S. Ganapathy", "J. Pelecanos"], "venue": "arXiv preprint arXiv:1602.07291, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Msr identity toolbox v1. 0: A matlab toolbox for speaker-recognition research", "author": ["S.O. Sadjadi", "M. Slaney", "L. Heck"], "venue": "Speech and Language Processing Technical Committee Newsletter, vol. 1, no. 4, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Time delay deep neural network-based universal background models for speaker recognition", "author": ["D. Snyder", "D. Garcia-Romero", "D. Povey"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 92\u2013 97.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Utd-crss system for the nist 2015 language recognition i-vector machine learning challenge", "author": ["C. Yu", "C. Zhang", "S. Ranjan", "Q. Zhang", "A. Misra", "F. Kelly", "J.H.L. Hansen"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5835\u20135839.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminant analysis via support vectors", "author": ["S. Gu", "Y. Tan", "X. He"], "venue": "Neurocomputing, vol. 73, no. 10, pp. 1669\u20131675, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "i-vector/PLDA speaker recognition using support vectors with discriminant analysis", "author": ["F. Bahmaninezhad", "J.H.L. Hanesn"], "venue": "(submitted to) IEEE ICASSP, 2017.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "The bosaris toolkit: Theory, algorithms and code for surviving the new dcf", "author": ["N. Br\u00fcmmer", "E. de Villiers"], "venue": "arXiv preprint arXiv:1304.2865, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": ", UBM or different DNN models [2, 3].", "startOffset": 30, "endOffset": 36}, {"referenceID": 1, "context": ", UBM or different DNN models [2, 3].", "startOffset": 30, "endOffset": 36}, {"referenceID": 1, "context": "The reason we apply fMLLR feature here is that, by speaker normalization, we expect to acquire more accurate phonetic alignment in the following TV matrix training, see more details in [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "For back-end, mostly MSR [4] toolkit has been adopted.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "The last baseline is a DNN i-Vector system using Kaldi (sre10/v2) that is based on the multisplice time delay DNN (TDNN) [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "More details on the TDNN structure and training procedure are provided in [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "First, it is very intuitive to do a speaker clustering of the unlabeled data, and then generate an \u201cestimated\u201d speaker label for each utterance, similar with the method that we used in 2015 NIST LRE iVector challenge[6].", "startOffset": 216, "endOffset": 219}, {"referenceID": 5, "context": "The idea of using support vectors with discriminant analysis has been previously introduced in [7] which made significant improvement over LDA.", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "In addition, the effectiveness of SVDA in iVector/PLDA speaker recognition for NIST SRE2010 is studied in [8] previously for both long and short duration test utterances.", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": "More details on the advantages and properties of SVDA are provided in [8].", "startOffset": 70, "endOffset": 73}, {"referenceID": 7, "context": "The CRSS calibration and fusion system is mainly based on the BOSARIS toolkit [9].", "startOffset": 78, "endOffset": 81}], "year": 2016, "abstractText": "This document briefly describes the systems submitted by the Center for Robust Speech Systems (CRSS) from The University of Texas at Dallas (UTD) to the 2016 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE). We developed several UBM and DNN i-Vector based speaker recognition systems with different data sets and feature representations. Given that the emphasis of the NIST SRE 2016 is on language mismatch between training and enrollment/test data, so-called domain mismatch, in our system development we focused on: (1) using unlabeled indomain data for centralizing data to alleviate the domain mismatch problem, (2) finding the best data set for training LDA/PLDA, (3) using newly proposed dimension reduction technique incorporating unlabeled in-domain data before PLDA training, (4) unsupervised speaker clustering of unlabeled data and using them alone or with previous SREs for PLDA training, (5) score calibration using only unlabeled data and combination of unlabeled and development (Dev) data as separate experiments.", "creator": "LaTeX with hyperref package"}}}