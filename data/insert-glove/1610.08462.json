{"id": "1610.08462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2016", "title": "Distraction-Based Neural Networks for Document Summarization", "abstract": "chateauroux Distributed representation karimova learned duetted with neural hominem networks iia has recently shown goislard to be effective 73.78 in alfas modeling lencioni natural languages at fine granularities closings such kanye as damjanovic words, oscarsson phrases, and even giulietta sentences. Whether clinic and how imparted such 1,031 an 912 approach can be dipeptides extended pyrrhus to undergoes help model larger spans of text, e. maecenas g. , brudzew documents, fekeiki is intriguing, fdi and trnovo further no-hit investigation four-hour would 7-point still 65.24 be single-decker desirable. steinburg This bankrolling paper \u00fcdtirol aims know-nothings to shomo enhance woolhouse neural 319.1 network juncus models liimatainen for such a chipps purpose. norem A altissimo typical problem of document - cucuteni-trypillian level modeling saepo is automatic tamoil summarization, rammelsbach which aims supranuclear to dumper model artland documents kilty in order hisses to basswood generate abbell summaries. powerlifting In this instated paper, rateb we propose anderstorp neural frequented models wjz to train 98.09 computers drat not sivs just to karakas pay attention to specific regions qaissi and dollarama content acteal of input 10-3 documents with attention models, but antao also distract them mater to traverse uwa between different lahr content of a aoun document 2,977 so as inter-state to better grasp the overall 2825 meaning reinvention for summarization. red-headed Without engineering bermuda any features, morsels we solihin train 55.84 the models on slowest two 1-story large alberthal datasets. The arlindo models nogometni achieve weizs\u00e4cker the baap state - of - transmute the - palar art fairhall performance, myhrvold and they nee significantly benefit from aiki the distraction photoshop modeling, particularly when input 101.58 documents are long.", "histories": [["v1", "Wed, 26 Oct 2016 18:57:00 GMT  (89kb,D)", "http://arxiv.org/abs/1610.08462v1", "Published in IJCAI-2016: the 25th International Joint Conference on Artificial Intelligence"]], "COMMENTS": "Published in IJCAI-2016: the 25th International Joint Conference on Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qian chen", "xiaodan zhu", "zhenhua ling", "si wei", "hui jiang"], "accepted": false, "id": "1610.08462"}, "pdf": {"name": "1610.08462.pdf", "metadata": {"source": "CRF", "title": "Distraction-Based Neural Networks for Document Summarization", "authors": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang"], "emails": ["cq1231@mail.ustc.edu.cn,", "zhu2048@gmail.com,", "zhling@ustc.edu.cn,", "siwei@iflytek.com,", "hj@cse.yorku.ca"], "sections": [{"heading": "1 Introduction", "text": "Modeling the meaning of text lies in center of natural language understanding. Distributed representation learned with neural networks has recently shown to be effective in modeling fine granularities of text, including words [Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015], phrases [Yin and Schu\u0308tze, 2014; Zhu et al., 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].\nWhether and how such an approach can be extended to help model larger spans of text, e.g., documents, is intriguing, and further investigation would still be desirable, although there has been interesting research conducted recently along this line [Li et al., 2015; Hu et al., 2015; Wang and Cho, 2015;\nPublished in IJCAI-2016: the 25th International Joint Conference on Artificial Intelligence.\nHermann et al., 2015]. A typical problem of documentlevel modeling is automatic summarization [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013], in which computers generate summaries for documents, based on their shallow or deep understanding of the documents.\nIf one regards the process of representing input documents, generating summaries, and the interaction between them to be a (complicated) function, fitting such a function could expect to have a large-scale annotated dataset to estimate a large set of parameters, while on the other hand, hardcoding summarization knowledge (in different forms) or limiting the number of model parameters (e.g., as in many extractive summarization models) are often adopted when there are no enough training data. This work explores the former direction and utilizes relatively large datasets [Hu et al., 2015; Hermann et al., 2015] to train neural summarization models. In general, neural networks, as universal approximators, can fit very complicated functions and have shown to be very effective on many problems recently.\nUnderstanding the input documents and generating summaries are both challenging. On the understanding side, much recent work seems to have suggested that distributed representations (often vectors) by themselves may not be adequate for representing sentences, let along with longer documents. Additional modeling such as soft or hard attention has been applied to retrospect subsequences or even words in input text to remedy the limits, which has shown to improve performances of different tasks such as those discussed in [Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015] among others. We regard this to be a mechanism that provides a connection between input document modeling (encoding) and summary generating (decoding), which could model a level of cognitive controls\u2014human summarizers themselves often move between the input documents and summaries when they summarize a document.\nWe consider this control layer to be important, and in this paper we focus on better designing this control layer for summarization. We propose neural models to train computers not just to pay attention to specific regions and content of input documents with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization.\nWithout engineering any features, we train the models with two large datasets. The models achieve the state-of-the-art\nar X\niv :1\n61 0.\n08 46\n2v 1\n[ cs\n.C L\n] 2\n6 O\nct 2\n01 6\nperformance and they significantly benefit from the distraction modeling, particularly when input documents are long. We also explore several technologies that have been applied to sentence-level tasks and extend them to document summarization, and we present in this paper the technologies that showed to help improve the summarization performance. Even when it is applied onto the models that have leveraged these technologies, the distraction models can further improve the performance significantly. In general, our models here aim to perform abstractive summarization."}, {"heading": "2 Related work", "text": "Distributed representation Distributed representation has shown to be effective in modeling fine granularities of text as discussed above. Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015]. This includes research that incorporates document-level information for language modeling [Wang and Cho, 2015; Lin et al., 2015] and that answers questions [Hermann et al., 2015] by comprehending input documents with attention-based models. More relevant to ours, the work of [Li et al., 2015] learned distributed representation for short documents with the averaged length of about a hundred word tokens, although the objective is not summarization. Summarization typically faces documents longer than those, and summarization may be more necessary when documents are long. In this paper, we propose neural models for summarizing typical news articles with up to thousands of word tokens. We find it is necessary to enable computers not just to pay attention to specific content of input documents with attention models, but also distract them to traverse between different content so as to better grasp the overall meaning for summarization, particularly when documents are long. Neural summarization models Automatic summarization has been intensively studied for both text [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013] and speech [Zhu and Penn, 2006; Zhu et al., 2010]. Most stateof-the-art summarization models have focused on extractive summarization, although some efforts have also been exerted on abstractive summarization. Recent neural summarization models include the recent efforts of [Rush et al., 2015; Lopyrev, 2015; Hu et al., 2015]. The research performed in [Rush et al., 2015] focuses on neural models for sentence compression and rewriting, but not full document summarization. The work of [Lopyrev, 2015] leverages neural networks to generate news headline, where input documents are limited to 50 word tokens, and the work of [Hu et al., 2015] also deals with short texts (up to dozens of word tokens), in which summarization problems such as content redundancy is less prominent and attention-based models seem to be sufficient. However, summarization typically faces documents longer than that and summarization is often more needed when documents are long. In this work we attempt to explore neural summarization technologies for news articles with up to thousands of word tokens, in which we find distraction-based summarization models help improve performance. Note that\nour improvement is achieved over the model that has already outperformed the attention-based model reported in [Hu et al., 2015] on short documents."}, {"heading": "3 Our approach", "text": ""}, {"heading": "3.1 Overview", "text": "We base our model on the general encoder-decoder framework [Sutskever et al., 2011; Sutskever et al., 2014; Cho et al., 2014] that has shown to be effective recently on different tasks. This is a general sequence-to-sequence modeling framework in which the encoding part can be devoted to model the input documents and the decoder to generate output.\nWe believe the control layer that helps navigate the input documents to optimize the generation objectives would be of importance, and we will focus on the control layer in this paper and enrich its expressiveness. Specifically for summarization, unlike much recent work that focuses more on attention in order to grasp local context or correspondence (e.g, in machine translation and sentence compression), we force our models to traverse between different content of a document to avoid focusing on a region or same content, to better grasp the overall meaning for the summarization objective.\nWe also explore several popular technologies that have been applied to sentence-level tasks and extend them to document summarization, and we present those that help improve the summarization performance."}, {"heading": "3.2 GRU-based encoding and decoding", "text": "Encoding The general document modeling and summarizing framework takes in an input document x = x1, \u00b7 \u00b7 \u00b7 , xTx and write the summary of the document as the output y = y1, \u00b7 \u00b7 \u00b7 , yTy . The summarization process is modeled as finding the output text y\u2217 that maximizes the conditional probability argmaxy p(y|x), given gold summary sequences. As discussed above, such a model has been found to be very effective in modeling sentences or sub-sentential text spans. We will address the challenges faced at the document level.\nOn encoding we do not restrict the encoders\u2019 architectures as if it is a recurrent neural network (RNN). The recent literature shows long-short term memory (LSTM) [Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014] and gated recurrent units (GRU) [Bahdanau et al., 2014] are both good architectures. In developing our systems we empirically found GRU achieved similar performance as LSTM but it is fast to train; we will therefore describe the GRU implement of our neural summarization models.\nIn the simplest uni-directional setting, when reading input symbols from left to right, a GRU learns the hidden annotations hi at time i with\nhi = GRU(hi\u22121, e(xi)) (1)\nwhere the hi \u2208 Rn encodes all content seen so far at time i which is computed from hi\u22121 and e(xi), where e(xi) \u2208 Rm is the m-dimensional embedding of the current word xi. The\nforward propagation of GRU is computed as follows.\nhi = (1\u2212 ui) hi\u22121 + ui h\u0303i (2) h\u0303i = tanh(We(xi) + U(ri hi\u22121)) (3) ri = sigmoid(Wre(xi) + Urhi\u22121) (4) ui = sigmoid(Wue(xi) + Uuhi\u22121) (5)\nwhere Wu, Wr, W \u2208 Rn\u00d7m and Uu, Ur, U \u2208 Rn\u00d7n are weight matrices, n is the number of hidden units, and is element-wise multiplication.\nIn our work, we actually applied bi-directional GRUs (biGRUs), which we found achieving better results than single directional GRUs consistently. As its name suggests, in a bi-GRU unit, the annotation vector ht encodes the sequence from two directions, modeling both the left and right context. The bottom part of Figure 1 shows the encoder intuitively, while for more details, readers can refer to [Bahdanau et al., 2014] for further discussion.\nGeneration When generating summaries, the decoder predicts the next word yt given all annotations obtained in encoding h = h1, \u00b7 \u00b7 \u00b7 , hTx , as well as all previously predicted words y1, \u00b7 \u00b7 \u00b7 , yt\u22121. The objective is a probability over the summary y with decomposition into the ordered conditionals:\ny\u2217 = argmax y Ty\u220f t=1 p(yt|y1, \u00b7 \u00b7 \u00b7 , yt\u22121,h) (6)\n= argmax y Ty\u220f t=1 g(yt\u22121, st, ct)|yt (7)\nwhere Equation (6) depicts a high level abstraction of generating a summary word yt over previous output as well as input annotations h = h1, \u00b7 \u00b7 \u00b7 , hTx , and yt is a legal output word at time t, while y\u2217 is the optimal summary found by the model.\nThe conditional probability is further rewritten as Equation (7) to factorize the model according to the structures of neural networks. The function g(yt\u22121, st, ct) is a nonlinear\nfunction that computes the probability vector for all legal output words at output time t, and g(yt\u22121, st, ct)|yt takes the element of the resulting vector corresponding to word yt, i.e., the predicated probability for word yt.\nThe vector st and ct are the control layers that connect output y and input h, which we will discuss in details in Section 3.3. For completeness, function g(.) is computed with:\ng(yt\u22121, st, ct) = \u03c3(Wo tanh(Voe(yt\u22121) + Uost + Coct)) (8)\nwhere \u03c3 is a softmax function; Wo \u2208 RK\u00d7n, Uo \u2208 Rn\u00d7n, Vo \u2208 Rn\u00d7m and Co \u2208 Rn\u00d72n are weight matrices; K is the vocabulary size; e(yt\u22121) \u2208 Rm is them-dimensional embedding of the previously predicted word yt\u22121."}, {"heading": "3.3 The control layers", "text": "The document modeling and summary generation are described above as two components: input document encoding and summary generation. A core problem is how these two components are associated. In sentence-level modeling such as machine translation and speech recognition, attention model is often applied to grasp local context and correspondence between input and output texts. For example, in translation attention is shown to be very useful for aligning the words of the target language (the language being translated to) to the corresponding source words and their context.\nAttention can be regarded as a type of cognitive controls. In modeling documents, we take a general viewpoint on this control layer and propose distraction modeling to enable the model to traverse over different content of a long document, and we will show it improves the summarization performance significantly. In general, the control layer allows a complicated examination over the input. In this section we describe the controls that consider both attention and distraction to navigate input documents and to generate summaries. Two-layer hidden output We first extended the recent twolevel hidden output model [Luong et al., 2015] to our summarization models. As presented later in the experiments, the two-level hidden output model consistently improves the summarization performance on different datasets. More specifically, the updating of st follows a two-layer GRU architecture shown in the top part of Figure 1.\nst = GRU1(s\u2032t, ct) (9) s\u2032t = GRU2(st\u22121, e(yt\u22121)) (10)\nThe forward propagation of GRU1 and GRU2 are computed similar to Equation (1) above. GRU1 and GRU2 use untied parameter matrices. The two-layer model allows for capturing a direct interaction between s\u2032t and ct, with the former encoding the current and previous output information and the latter encoding the current input content that is primed with distraction and attention. We will discuss how these vectors are computed below. Distraction in training We propose to enforce distraction from two perspectives: adding the distraction constraints in training as well as in decoding. We first discuss the distraction in training. Distraction over input content vectors In training we force the model not to pay attention to the same content or same\npart of the input documents too much. We accumulate the previously viewed content vector as a history content vector\u2211t\u22121 j=1 cj and incorporate it into the currently computed c \u2032 t. We refer to this model as M1.\nct = tanh(Wcc \u2032 t \u2212 Uc t\u22121\u2211 j=1 cj) (11)\nwhere Wc \u2208 R2n\u00d72n and Ua \u2208 R2n\u00d72n are diagonal matrices. And c\u2032t is input content vectors that have not been directly penalized with history yet; c\u2032t is directly computed with conventional equation as follows:\nc\u2032t = Tx\u2211 i=1 \u03b1t,ihi (12)\nwhere hi are annotation vectors that encode the current input word and its context with the input GRU described above in Equation (1). And \u03b1t,i is the attention weight put on hi at the current output time t. The distraction-based ct computed in Equation (11) can then be incorporated in Equation (8). Distraction over attention weight vectors We also propose to add distraction directly on the attention weight vectors. Similarly as above, we accumulate the past attention weights as a history attention weight \u2211t\u22121 j=1 \u03b1j,i and use it to prime the current attention weights. The model in [Tu et al., 2016] also uses history attention weights, but we use history here to force distraction in order to avoid redundancy, which is not a concern in the machine translation task. We refer to the model as M2.\n\u03b1\u2032t,i = v T a tanh(Was \u2032 t + Uahi \u2212 ba t\u22121\u2211 j=1 \u03b1j,i) (13)\nwhere Wa \u2208 Rl\u00d7n, Ua \u2208 Rl\u00d72n, ba \u2208 Rl, and va \u2208 Rl are the weight matrices, and l is the number of hidden units. Note that Was\u2032t + Uahi in the equation computes the conventional attention without penalizing attention history. \u03b1\u2032 is often normalized with a softmax to generate attention weights \u03b1t,i below, which is in turn used in Equation (12).\n\u03b1t,i = exp(\u03b1\u2032t,i)\u2211Tx j=1 exp(\u03b1 \u2032 t,j)\n(14)\nDistraction in decoding In the decoding process, we also enforced different types of distraction, one by computing the difference between the distribution of the current attention weight \u03b1t and that of all previous attention weights \u03b11, \u00b7 \u00b7 \u00b7 , \u03b1t\u22121. Since \u03b1 can be seen as a proper probabilistic distribution, normalized in Equation (14), we used KullbackLeibler (KL) divergence to measure their difference with Equation (15), which was found to be consistently better than several other distance metrics we tried on the held-out data.\nWe also enforced distraction in a similar way on the attention-primed input content vector ct, as well as on the hidden output vector st. Both ct and st are not normalized but are regular content vectors, where the cosine similarity\nwas found achieving a better performance than several alternatives (e.g., l1 and l2 distances) on the held-out data.\nd\u03b1,t = min i\u2208{1,\u00b7\u00b7\u00b7t\u22121} KL(\u03b1t, \u03b1i) (15)\ndc,t = max i\u2208{1,\u00b7\u00b7\u00b7t\u22121} cosine(ct, ci) (16)\nds,t = max i\u2208{1,\u00b7\u00b7\u00b7t\u22121} cosine(st, si) (17)\nThe distraction score d\u2217,t was then added into the output probability and the beam search in order to encourage the model to avoid redundant content.\nscoret = Ty\u2211 t=1 {log(pyt) + \u03bb1d\u03b1,t + \u03bb2dc,t + \u03bb3ds,t} (18)\nwhere scoret was used as follows in the beam search with distraction algorithm, and parameter \u03bb1, \u03bb2, and \u03bb3 were determined on the development set. We refer to this model as M3.\nAlgorithm 1 Beam search with distraction Require: Vocabulary size K, beam size B, max output\nlength N . Computed probabilities of all the words in vocabulary . Choose the B most likely words and initialize the B hypotheses for i = 1 : N do\n. For each hypothesis, compute the next conditional probabilities, then have B \u00d7K candidates with the corresponding probabilities\n. Use the distraction-primed value score to choose B most likely candidates end for\nUnknown word replacement for summarization We borrowed the unknown word replacement [Jean et al., 2015] from machine translation to our summarization models and found it improved the performance when summarizing long documents. Specifically, due to the time complexity in handling a larger vocabulary in the softmax layer in summary generation, infrequent words were removed from the vocabulary and were replaced with the symbol \u3008UNK\u3009. The threshold of vocabulary size is data-dependent and will be detailed later in the experiment set-up section.\nAfter the first-round summary generated for a document, a token labeled as \u3008UNK\u3009 will be replaced with a word in the input documents. More specifically, we obtained the replacement using Equation (14); i.e., we used the largest element in \u03b1t to find the source location for the current \u3008UNK\u3009."}, {"heading": "4 Experiment set-up", "text": ""}, {"heading": "4.1 Data", "text": "We experiment with our summarization models on two publicly available corpora with different document lengths and in different languages: a CNN news collection [Hermann et al., 2015] and a Chinese corpus made available more recently in [Hu et al., 2015]. Both are large datasets appropriate\nfor training neural models, which, as discussed above, employ a large number of parameters to fit the potentially complicated summarization process involving representing input documents, generating summaries, and interacting between them. CNN data The CNN data [Hermann et al., 2015] have a human-generated real-life summary for each news article. The dataset collected in was made available at GitHub.1 The data was preprocessed with the Stanford CoreNLP tools [Manning et al., 2014] for tokenization and sentenceboundary detection; all capital information is kept. To speed up training, we removed the documents that are too long (over 2,500 word tokens) from the training and validation set, but kept all documents in the test set, which does not change the difficulty of the task. LCSTS data The second corpus is LCSTS, which is a Chinese corpus made available more recently in [Hu et al., 2015]. The data is constructed from the Chinese microblogging website, Sina Weibo. We used the original training/testing split mentioned in [Hu et al., 2015], but additionally randomly sampled a small part of the training data as our validation set.\nTable 1 gives more details about the two datasets. We can see from the table that averaged document length of the CNN corpus is about seven time as long as the LCSTS corpus, and the summary is about 2-3 times longer."}, {"heading": "4.2 Training details", "text": "We used mini-batch stochastic gradient descent (SGD) to optimize log-likelihood, and Adadelta [Zeiler, 2012] to automatically adapt the learning rate of parameters ( = 10\u22126 and \u03c1 = 0.95).\nFor the CNN dataset, training was performed with shuffled mini-batches of size 64 after sorting by length. We limit our vocabulary to include the top 25,000 most frequent words. Other words were replaced with the token \u3008UNK\u3009, as discussed earlier in the paper. Based on the validation data, we set embedding dimension to be 120, the vector length in hidden layers to be 500 for uni-GRU and 600 for bi-GRU. An end-of-sentence token was inserted between every sentence, and an end-of-document token was added at the end. The beam size of decoder was set to be 5.\nFor the LCSTS data, a larger mini-batch size 256 was found to be better based the observation on the validation set.\n1https://github.com/deepmind/rc-data\nSame as in [Hu et al., 2015], we used characters rather than words as our tokens. The vocabulary size is 4000, embedding dimension is 500, and the vector size of the hidden-layer nodes is 500. Beam search size is 5, same as in the CNN dataset.\nWe make our code publicly available2. Our implementation uses python and is based on the Theano library [Bergstra et al., 2010]."}, {"heading": "5 Experimental results", "text": ""}, {"heading": "5.1 Results on the CNN dataset", "text": "Overall performance Our results on the CNN dataset are presented in Table 2. We used Rouge scores [Lin, 2004] to measure performance. Since the summary lengths are not preset to be the same, we report F1 Rouge. The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al., 2007], and KL-sum [Haghighi and Vanderwende, 2009]. These baseline results are implemented in the open-source tool SUMY3.\nThe results at the lower half of the table show that the bi-GRU encoder achieves a better performance than the uniGRU encoder. This is consistent with the results on the LCSTS dataset reported later in Table 4. We show that two-level output model we discussed in the method section is beneficial, which is also consistent with the results on the LCSTS dataset. In addition, the unknown replacement technique yields an additional improvement.\nOver the strong model that has used these technologies (the row marked as \u201d+UNK replace\u201d), the model in the last row that incorporates all distraction modeling (M1, M2 and M3) finally achieves a Rouge-1 score of 27.1, a Rouge-2 score of 8.2, and a Rouge-L score of 18.7, significantly improving the three Rouge scores by 5.8, 1.9, and 2.3, respectively. These are also the largest improvement presented in the table, compared with the other techniques listed. The table also lists the details of how the model M1, M2, and M3 improve the performance additively. Again, the neural models do not engineer any features and use only content but not any additional formality features such as locations of input sentences, which may bring additional improvement. Performance on different lengths of documents To observe the effectiveness of the distraction model over different document lengths, we further selected all short documents from the CNN training dataset into a subset (subset-1) with average length at 335 word tokens, and a subset of data (subset-2) that have the same number of documents as the subset-1, with averaged document length at 680 word tokens. As shown in Table 3, on the data subset-2, the distraction model improves the results more significantly. The relative improvement is 29.0%, 25.6%, and 10.8%, compared with 25.9%, 20.5%, and 8.1% on subset-1, respectively. In general the best performance on both dataset is lower than that using all training\n2Our code is available at https://github.com/lukecq1231/nats 3 https://pypi.python.org/pypi/sumy\ndata, suggesting using more training data can improve summarization performance."}, {"heading": "5.2 Results on the LCSTS dataset", "text": "We experiment with the proposed model on public LCSTS corpus. The baseline is the best result reported in [Hu et al., 2015]4. Our modified uni-GRU achieves a slight improvement over the reported results. The Bi-GRU attention-based model achieves a better performance, confirming the usefulness of bi-directional models for summarization as well as that our implementation is the state-of-the-art and serves as a very strong baseline in the CNN dataset discussed above. Note that since the input text length of LCSTS is far shorter than the CNN documents, each containing about 100 words and roughly 6-8 sentences, we show that distraction does not improve the performance, but in contrast, when documents are longer, its benefits are significant, achieving the biggest improvement as discussed earlier. This suggests the effectiveness of distraction modeling in helping summarize the more\n4We thank the authors of [Hu et al., 2015] for generously sharing us the latest output of their models, which achieves a better performance than the results reported in [Hu et al., 2015]. We reported here the updated scores higher performance as our baseline.\nchallenging longer documents, where summarization is often more necessary than for short texts.\nWe also compare our models with the simple baseline that selects the first N numbers of word tokens from the input documents, which reaches its maximal Rouge scores when the first 30 tokens were taken, and achieves Rouge-1, Rouge2, and Rouge-L at 25.5, 14.1 and 21.4. And our models are significantly better than that. For the CNN data set, choosing the first three sentences achieves the best results, which reach Rouge-1, Rouge-2, and Rouge-L at 26.1, 9.6 and 17.8, respectively. Since the CNN data is news data, the baseline of selecting first several sentences has known to be a very strong baseline. Again, the models we explore here are towards performing abstractive summarization."}, {"heading": "6 Conclusions and future work", "text": "We propose to train neural document summarization models not just to pay attention to specific regions of input documents with attention models, but also distract the models to different content in order to better grasp the overall meaning of input documents. Without engineering any features, we train the models on two large datasets. The models achieve the stateof-the-art performance and they significantly benefit from the distraction modeling, particularly when the input documents are long. We also explore several recent technologies for summarization and show that they help improve summarization performance as well. Even if applied onto the models that have already leveraged these technologies, the distraction models can further improve the performance significantly.\nFrom a more general viewpoint, enriching the expressiveness of the control layers that link the input encoding layer and the output decoding layer could be of importance to remedy the shortcomings of the current models. We plan to perform more work along this direction."}, {"heading": "Acknowledgments", "text": "The first and the third author of this paper were supported in part by the Science and Technology Development of Anhui Province, China (Grants No. 2014z02006), the Fundamental Research Funds for the Central Universities (Grant No. WK2350000001) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070006)."}], "references": [{"title": "CoRR", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio. Neural machine translation by jointly learning to align", "translate"], "venue": "abs/1409.0473,", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["J Bergstra", "O Breuleux", "F Bastien", "P Lamblin", "R Pascanu", "G Desjardins", "J Turian", "D Warde-Farley", "Y Bengio"], "venue": "SciPy, volume 4, page 3. Austin, TX", "citeRegEx": "Bergstra et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Revisiting word embedding for contrasting meaning", "author": ["Zhigang Chen", "Wei Lin", "Qian Chen", "Si Wei", "Hui Jiang", "Xiaodan Zhu"], "venue": "Proceedings of ACL,", "citeRegEx": "Chen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Enhancing and combining sequential and tree lstm for natural language inference", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang"], "venue": "arXiv:1609.06038v1,", "citeRegEx": "Chen et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP", "citeRegEx": "Cho et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR, 12:2493\u20132537", "citeRegEx": "Collobert et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey on automatic text summarization", "author": ["Das", "Martins", "2007] Dipanjan Das", "Andre Martins"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Das et al\\.", "year": 2007}, {"title": "JACM", "author": ["Harold P Edmundson. New methods in automatic extracting"], "venue": "16(2):264\u2013285,", "citeRegEx": "Edmundson. 1969", "shortCiteRegEx": null, "year": 1969}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G\u00fcnes Erkan", "Dragomir R Radev"], "venue": "JAIR, pages 457\u2013479,", "citeRegEx": "Erkan and Radev. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Exploring content models for multi-document summarization", "author": ["Aria Haghighi", "Lucy Vanderwende"], "venue": "NAACL,", "citeRegEx": "Haghighi and Vanderwende. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Teaching machines to read and comprehend", "author": ["K. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "NIPS", "citeRegEx": "Hermann et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Lcsts: A large scale chinese short text summarization dataset", "author": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu"], "venue": "EMNLP,", "citeRegEx": "Hu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie", "Ozan Irsoy", "Claire Cardie"], "venue": "In NIPS,", "citeRegEx": "Irsoy et al\\.,? \\Q2096\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2096}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "KyungHyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "ACL,", "citeRegEx": "Jean et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "ACL", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom. A convolutional neural network for modelling sentences"], "venue": "June", "citeRegEx": "Kalchbrenner et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "venue": "ACL,", "citeRegEx": "Li et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["R. Lin", "S. Liu", "M. Yang", "M. Li", "M. Zhou", "S. Li"], "venue": "EMNLP", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Konstantin Lopyrev. Generating news headlines with recurrent neural networks"], "venue": "abs/1512.01712,", "citeRegEx": "Lopyrev. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IBM Journal of research and development", "author": ["Hans Peter Luhn. The automatic creation of literature abstracts"], "venue": "2(2):159\u2013 165,", "citeRegEx": "Luhn. 1958", "shortCiteRegEx": null, "year": 1958}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "EMNLP,", "citeRegEx": "Luong et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Benjamins Pub", "author": ["J Inderjeet Mani. Automatic Summarization."], "venue": "Co., Amsterdam,", "citeRegEx": "Mani. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["C Manning", "M Surdeanu", "J Bauer", "J Finkel", "S Bethard", "D McClosky"], "venue": "ACL", "citeRegEx": "Manning et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Textrank: Bringing order into text", "author": ["Rada Mihalcea", "Paul Tarau"], "venue": "EMNLP,", "citeRegEx": "Mihalcea and Tarau. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey of text summarization techniques", "author": ["Ani Nenkova", "Kathleen McKeown"], "venue": "Springer,", "citeRegEx": "Nenkova and McKeown. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "EMNLP,", "citeRegEx": "Rush et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "A. Ng"], "venue": "EMNLP", "citeRegEx": "Socher et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Using latent semantic analysis in text summarization and summary evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "ISIM, pages 93\u2013100", "citeRegEx": "Steinberger and Jezek. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In ICML", "author": ["Ilya Sutskever", "James Martens", "Geoffrey Hinton. Generating text with recurrent neural networks"], "venue": "pages 1017\u20131024,", "citeRegEx": "Sutskever et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In NIPS", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le. Sequence to sequence learning with neural networks"], "venue": "pages 3104\u20133112,", "citeRegEx": "Sutskever et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved Semantic Representations From TreeStructured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher Manning"], "venue": "ACL,", "citeRegEx": "Tai et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li. Coverage-based neural machine translation"], "venue": "abs/1601.04811,", "citeRegEx": "Tu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion", "author": ["L. Vanderwende", "H. Suzuki", "C. Brockett", "A. Nenkova"], "venue": "IP&M, 43(6):1606\u20131618", "citeRegEx": "Vanderwende et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "CoRR", "author": ["Tian Wang", "Kyunghyun Cho. Largercontext language modelling"], "venue": "abs/1511.03729,", "citeRegEx": "Wang and Cho. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In ACL 2014 Student Research Workshop", "author": ["W. Yin", "H. Sch\u00fctze. An exploration of embeddings for generalized phrases"], "venue": "pages 41\u201347, June", "citeRegEx": "Yin and Sch\u00fctze. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "Zeiler. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "acoustic and spoken-language features on spontaneous conversation summarization", "author": ["Xiaodan Zhu", "Gerald Penn. Comparing the roles of textual"], "venue": "NAACL,", "citeRegEx": "Zhu and Penn. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Summarizing multiple spoken documents: Finding evidence from untranscribed audio", "author": ["Xiaodan Zhu", "Gerald Penn", "Frank Rudzicz"], "venue": "ACL,", "citeRegEx": "Zhu et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "An empirical study on the effect of negation words on sentiment", "author": ["Xiaodan Zhu", "Hongyu Guo", "Saif Mohammad", "Svetlana Kiritchenko"], "venue": "Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Zhu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of Joint Conference on Lexical and Computational Semantics", "author": ["Xiaodan Zhu", "Hongyu Guo", "Parinaz Sobhani. Neural networks for integrating compositional", "noncompositional sentiment in sentiment composition"], "venue": "June", "citeRegEx": "Zhu et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "Long Short-Term Memory over Recursive Structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "Proceedings of International Conference on Machine Learning,", "citeRegEx": "Zhu et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "Dag-structured long short-term memory for semantic compositionality", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "Proceedings of the Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Zhu et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Distributed representation learned with neural networks has recently shown to be effective in modeling fine granularities of text, including words [Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015], phrases [Yin and Sch\u00fctze, 2014; Zhu et al.", "startOffset": 147, "endOffset": 212}, {"referenceID": 24, "context": "Distributed representation learned with neural networks has recently shown to be effective in modeling fine granularities of text, including words [Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015], phrases [Yin and Sch\u00fctze, 2014; Zhu et al.", "startOffset": 147, "endOffset": 212}, {"referenceID": 2, "context": "Distributed representation learned with neural networks has recently shown to be effective in modeling fine granularities of text, including words [Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015], phrases [Yin and Sch\u00fctze, 2014; Zhu et al.", "startOffset": 147, "endOffset": 212}, {"referenceID": 35, "context": ", 2015], phrases [Yin and Sch\u00fctze, 2014; Zhu et al., 2014], and arguably sentences [Socher et al.", "startOffset": 17, "endOffset": 58}, {"referenceID": 39, "context": ", 2015], phrases [Yin and Sch\u00fctze, 2014; Zhu et al., 2014], and arguably sentences [Socher et al.", "startOffset": 17, "endOffset": 58}, {"referenceID": 27, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 15, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 31, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 41, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 40, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 3, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 42, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 21, "context": "A typical problem of documentlevel modeling is automatic summarization [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013], in which computers generate summaries for documents, based on their shallow or deep understanding of the documents.", "startOffset": 71, "endOffset": 133}, {"referenceID": 25, "context": "A typical problem of documentlevel modeling is automatic summarization [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013], in which computers generate summaries for documents, based on their shallow or deep understanding of the documents.", "startOffset": 71, "endOffset": 133}, {"referenceID": 12, "context": "This work explores the former direction and utilizes relatively large datasets [Hu et al., 2015; Hermann et al., 2015] to train neural summarization models.", "startOffset": 79, "endOffset": 118}, {"referenceID": 10, "context": "This work explores the former direction and utilizes relatively large datasets [Hu et al., 2015; Hermann et al., 2015] to train neural summarization models.", "startOffset": 79, "endOffset": 118}, {"referenceID": 0, "context": "Additional modeling such as soft or hard attention has been applied to retrospect subsequences or even words in input text to remedy the limits, which has shown to improve performances of different tasks such as those discussed in [Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015] among others.", "startOffset": 231, "endOffset": 293}, {"referenceID": 20, "context": "Additional modeling such as soft or hard attention has been applied to retrospect subsequences or even words in input text to remedy the limits, which has shown to improve performances of different tasks such as those discussed in [Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015] among others.", "startOffset": 231, "endOffset": 293}, {"referenceID": 26, "context": "Additional modeling such as soft or hard attention has been applied to retrospect subsequences or even words in input text to remedy the limits, which has shown to improve performances of different tasks such as those discussed in [Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015] among others.", "startOffset": 231, "endOffset": 293}, {"referenceID": 16, "context": "Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015].", "startOffset": 87, "endOffset": 181}, {"referenceID": 12, "context": "Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015].", "startOffset": 87, "endOffset": 181}, {"referenceID": 17, "context": "Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015].", "startOffset": 87, "endOffset": 181}, {"referenceID": 34, "context": "Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015].", "startOffset": 87, "endOffset": 181}, {"referenceID": 10, "context": "Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015].", "startOffset": 87, "endOffset": 181}, {"referenceID": 34, "context": "This includes research that incorporates document-level information for language modeling [Wang and Cho, 2015; Lin et al., 2015] and that answers questions [Hermann et al.", "startOffset": 90, "endOffset": 128}, {"referenceID": 17, "context": "This includes research that incorporates document-level information for language modeling [Wang and Cho, 2015; Lin et al., 2015] and that answers questions [Hermann et al.", "startOffset": 90, "endOffset": 128}, {"referenceID": 10, "context": ", 2015] and that answers questions [Hermann et al., 2015] by comprehending input documents with attention-based models.", "startOffset": 35, "endOffset": 57}, {"referenceID": 16, "context": "More relevant to ours, the work of [Li et al., 2015] learned distributed representation for short documents with the averaged length of about a hundred word tokens, although the objective is not summarization.", "startOffset": 35, "endOffset": 52}, {"referenceID": 21, "context": "Neural summarization models Automatic summarization has been intensively studied for both text [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013] and speech [Zhu and Penn, 2006; Zhu et al.", "startOffset": 95, "endOffset": 157}, {"referenceID": 25, "context": "Neural summarization models Automatic summarization has been intensively studied for both text [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013] and speech [Zhu and Penn, 2006; Zhu et al.", "startOffset": 95, "endOffset": 157}, {"referenceID": 37, "context": "Neural summarization models Automatic summarization has been intensively studied for both text [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013] and speech [Zhu and Penn, 2006; Zhu et al., 2010].", "startOffset": 169, "endOffset": 207}, {"referenceID": 38, "context": "Neural summarization models Automatic summarization has been intensively studied for both text [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013] and speech [Zhu and Penn, 2006; Zhu et al., 2010].", "startOffset": 169, "endOffset": 207}, {"referenceID": 26, "context": "Recent neural summarization models include the recent efforts of [Rush et al., 2015; Lopyrev, 2015; Hu et al., 2015].", "startOffset": 65, "endOffset": 116}, {"referenceID": 18, "context": "Recent neural summarization models include the recent efforts of [Rush et al., 2015; Lopyrev, 2015; Hu et al., 2015].", "startOffset": 65, "endOffset": 116}, {"referenceID": 12, "context": "Recent neural summarization models include the recent efforts of [Rush et al., 2015; Lopyrev, 2015; Hu et al., 2015].", "startOffset": 65, "endOffset": 116}, {"referenceID": 26, "context": "The research performed in [Rush et al., 2015] focuses on neural models for sentence compression and rewriting, but not full document summarization.", "startOffset": 26, "endOffset": 45}, {"referenceID": 18, "context": "The work of [Lopyrev, 2015] leverages neural networks to generate news headline, where input documents are limited to 50 word tokens, and the work of [Hu et al.", "startOffset": 12, "endOffset": 27}, {"referenceID": 12, "context": "The work of [Lopyrev, 2015] leverages neural networks to generate news headline, where input documents are limited to 50 word tokens, and the work of [Hu et al., 2015] also deals with short texts (up to dozens of word tokens), in which summarization problems such as content redundancy is less prominent and attention-based models seem to be sufficient.", "startOffset": 150, "endOffset": 167}, {"referenceID": 12, "context": "Note that our improvement is achieved over the model that has already outperformed the attention-based model reported in [Hu et al., 2015] on short documents.", "startOffset": 121, "endOffset": 138}, {"referenceID": 29, "context": "We base our model on the general encoder-decoder framework [Sutskever et al., 2011; Sutskever et al., 2014; Cho et al., 2014] that has shown to be effective recently on different tasks.", "startOffset": 59, "endOffset": 125}, {"referenceID": 30, "context": "We base our model on the general encoder-decoder framework [Sutskever et al., 2011; Sutskever et al., 2014; Cho et al., 2014] that has shown to be effective recently on different tasks.", "startOffset": 59, "endOffset": 125}, {"referenceID": 4, "context": "We base our model on the general encoder-decoder framework [Sutskever et al., 2011; Sutskever et al., 2014; Cho et al., 2014] that has shown to be effective recently on different tasks.", "startOffset": 59, "endOffset": 125}, {"referenceID": 11, "context": "The recent literature shows long-short term memory (LSTM) [Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014] and gated recurrent units (GRU) [Bahdanau et al.", "startOffset": 58, "endOffset": 116}, {"referenceID": 30, "context": "The recent literature shows long-short term memory (LSTM) [Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014] and gated recurrent units (GRU) [Bahdanau et al.", "startOffset": 58, "endOffset": 116}, {"referenceID": 0, "context": ", 2014] and gated recurrent units (GRU) [Bahdanau et al., 2014] are both good architectures.", "startOffset": 40, "endOffset": 63}, {"referenceID": 0, "context": "The bottom part of Figure 1 shows the encoder intuitively, while for more details, readers can refer to [Bahdanau et al., 2014] for further discussion.", "startOffset": 104, "endOffset": 127}, {"referenceID": 20, "context": "Two-layer hidden output We first extended the recent twolevel hidden output model [Luong et al., 2015] to our summarization models.", "startOffset": 82, "endOffset": 102}, {"referenceID": 32, "context": "The model in [Tu et al., 2016] also uses history attention weights, but we use history here to force distraction in order to avoid redundancy, which is not a concern in the machine translation task.", "startOffset": 13, "endOffset": 30}, {"referenceID": 14, "context": "Unknown word replacement for summarization We borrowed the unknown word replacement [Jean et al., 2015] from machine translation to our summarization models and found it improved the performance when summarizing long documents.", "startOffset": 84, "endOffset": 103}, {"referenceID": 10, "context": "We experiment with our summarization models on two publicly available corpora with different document lengths and in different languages: a CNN news collection [Hermann et al., 2015] and a Chinese corpus made available more recently in [Hu et al.", "startOffset": 160, "endOffset": 182}, {"referenceID": 12, "context": ", 2015] and a Chinese corpus made available more recently in [Hu et al., 2015].", "startOffset": 61, "endOffset": 78}, {"referenceID": 10, "context": "CNN data The CNN data [Hermann et al., 2015] have a human-generated real-life summary for each news article.", "startOffset": 22, "endOffset": 44}, {"referenceID": 22, "context": "1 The data was preprocessed with the Stanford CoreNLP tools [Manning et al., 2014] for tokenization and sentenceboundary detection; all capital information is kept.", "startOffset": 60, "endOffset": 82}, {"referenceID": 12, "context": "LCSTS data The second corpus is LCSTS, which is a Chinese corpus made available more recently in [Hu et al., 2015].", "startOffset": 97, "endOffset": 114}, {"referenceID": 12, "context": "We used the original training/testing split mentioned in [Hu et al., 2015], but additionally randomly sampled a small part of the training data as our validation set.", "startOffset": 57, "endOffset": 74}, {"referenceID": 36, "context": "We used mini-batch stochastic gradient descent (SGD) to optimize log-likelihood, and Adadelta [Zeiler, 2012] to automatically adapt the learning rate of parameters ( = 10\u22126 and \u03c1 = 0.", "startOffset": 94, "endOffset": 108}, {"referenceID": 12, "context": "com/deepmind/rc-data Same as in [Hu et al., 2015], we used characters rather than words as our tokens.", "startOffset": 32, "endOffset": 49}, {"referenceID": 1, "context": "Our implementation uses python and is based on the Theano library [Bergstra et al., 2010].", "startOffset": 66, "endOffset": 89}, {"referenceID": 19, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al.", "startOffset": 144, "endOffset": 156}, {"referenceID": 7, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al.", "startOffset": 168, "endOffset": 185}, {"referenceID": 28, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al.", "startOffset": 191, "endOffset": 220}, {"referenceID": 8, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al.", "startOffset": 231, "endOffset": 254}, {"referenceID": 23, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al.", "startOffset": 266, "endOffset": 292}, {"referenceID": 33, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al., 2007], and KL-sum [Haghighi and Vanderwende, 2009].", "startOffset": 303, "endOffset": 329}, {"referenceID": 9, "context": ", 2007], and KL-sum [Haghighi and Vanderwende, 2009].", "startOffset": 20, "endOffset": 52}, {"referenceID": 12, "context": "The baseline is the best result reported in [Hu et al., 2015]4.", "startOffset": 44, "endOffset": 61}, {"referenceID": 12, "context": "We thank the authors of [Hu et al., 2015] for generously sharing us the latest output of their models, which achieves a better performance than the results reported in [Hu et al.", "startOffset": 24, "endOffset": 41}, {"referenceID": 12, "context": ", 2015] for generously sharing us the latest output of their models, which achieves a better performance than the results reported in [Hu et al., 2015].", "startOffset": 134, "endOffset": 151}, {"referenceID": 12, "context": "[Hu et al., 2015] 29.", "startOffset": 0, "endOffset": 17}], "year": 2016, "abstractText": "Distributed representation learned with neural networks has recently shown to be effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e.g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization, which aims to model documents in order to generate summaries. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of input documents with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization. Without engineering any features, we train the models on two large datasets. The models achieve the state-of-the-art performance, and they significantly benefit from the distraction modeling, particularly when input documents are long.", "creator": "LaTeX with hyperref package"}}}