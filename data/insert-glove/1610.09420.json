{"id": "1610.09420", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Dynamic matrix recovery from incomplete observations under an exact low-rank constraint", "abstract": "Low - rank matrix factorizations tienes arise fourfold in a prostitution wide ruch variety of applications - - including milk-white recommendation systems, meals topic models, renacimiento and nielly source separation, hansol to sch\u00f6rner name just a few. In these suckled and cave-in many other 87.48 applications, allocation it forgets has summerland been medcom widely noted aethelred that by incorporating solf temporal information and allowing 108.0 for campagnolo the possibility rubinek of time - maes varying models, significant improvements are independently possible in pro-soviet practice. mahrt However, despite the nagapattinam reported garsdale superior empirical performance of pilli these treader dynamic aqib models 10.11 over their static heliamphora counterparts, yang there kinghorn is leiston limited surana theoretical justification frison for introducing loiola these more santals complex models. In this omeish paper dunluce we uludere aim to kuhnhenn address 24.3 this cronstedt gap bfn by studying ashlyn the cugnot problem plumbing of recovering slugfest a dynamically alpinist evolving raimondas low - quizzically rank matrix from fetu'u incomplete kunze observations. First, prewett we propose consett the locally eo weighted riecke matrix dharmavaram smoothing (lugu LOWEMS) doubtfully framework tugboats as korat one latinoamericanos possible wpm approach stora to gallaway dynamic askjeeves matrix shilpakala recovery. nine-tenths We then tittering establish error bounds for LOWEMS discos in highwaymen both the {\\ darville em monographs matrix a-division sensing} and {\\ em spits matrix huana completion} secretary-manager observation innerhofer models. moallim Our bethalto results fighers quantify ognjenovic the bewailed potential fred benefits of exploiting kassi dynamic constraints tallboy both jeremic in sleeker terms kirlian of recovery psychostimulants accuracy tesseractic and umist sample complexity. dnipropetrovsk To illustrate these cyclopentadienyl benefits raskulinecz we flatley provide both synthetic and llandysul real - world experimental yugra results.", "histories": [["v1", "Fri, 28 Oct 2016 22:44:29 GMT  (144kb)", "http://arxiv.org/abs/1610.09420v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["liangbei xu", "mark a davenport"], "accepted": true, "id": "1610.09420"}, "pdf": {"name": "1610.09420.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["lxu66@gatech.edu", "mdav@gatech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n09 42\n0v 1\n[ st\nat .M\nL ]\n2 8"}, {"heading": "1 Introduction", "text": "Suppose that X \u2208 Rn1\u00d7n2 is a rank-r matrix with r much smaller than n1 and n2. We observe X through a linear operator A : Rn1\u00d7n2 \u2192 Rm,\ny = A(X), y \u2208 Rm. In recent years there has been a significant amount of progress in our understanding of how to recover X from observations of this form even when the number of observations m is much less than the number of entries in X . (See [8] for an overview of this literature.) When A is a set of weighted linear combinations of the entries of X , this problem is often referred to as the matrix sensing problem. In the special case where A samples a subset of entries of X , it is known as the matrix completion problem. There are a number of ways to establish recovery guarantee in these settings. Perhaps the most popular approach for theoretical analysis in recent years has focused on the use of nuclear norm minimization as a convex surrogate for the (nonconvex) rank constraint [1, 3, 4, 5, 6, 7, 15, 19, 21, 22]. An alternative, however is to aim to directly solve the problem under an exact low-rank constraint. This leads a non-convex optimization problem, but has several computational advantages over most approaches to minimizing the nuclear norm and is widely used in large-scale applications (such as recommendation systems) [16]. In general, popular algorithms for solving the rank-constrained models \u2013 e.g., alternating minimization and alternating gradient descent \u2013 do not have as strong of convergence or recovery error guarantees due to the non-convexity of the rank constraint. However, there has been significant progress on this front in recent years [11, 10, 12, 13, 14, 23, 25], with many of these algorithms now having guarantees comparable to those for nuclear norm minimization.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nNearly all of this existing work assumes that the underlying low-rank matrix X remains fixed throughout the measurement process. In many practical applications, this is a tremendous limitation. For example, users\u2019 preferences for various items may change (sometimes quite dramatically) over time. Modelling such drift of user\u2019s preference has been proposed in the context of both music and movies as a way to achieve higher accuracy in recommendation systems [9, 17]. Another example in signal processing is dynamic non-negative matrix factorization for the blind signal separation problem [18]. In these and many other applications, explicitly modelling the dynamic structure in the data has led to superior empirical performance. However, our theoretical understanding of dynamic low-rank matrix recovery is still very limited.\nIn this paper we provide the first theoretical results on the dynamic low-rank matrix recovery problem. We determine the sense in which dynamic constraints can help to recover the underlying timevarying low-rank matrix in a particular dynamic model and quantify this impact through recovery error bounds. To describe our approach, we consider a simple example where we have two rank-r matrices X1 and X2. Suppose that we have a set of observations for each of X1 and X2, given by\nyi = Ai ( X i ) , i = 1, 2.\nThe na\u00efve approach is to use y1 to recover X1 and y2 to recover X2 separately. In this case the number of observations required to guarantee successful recovery is roughlymi \u2265 Cirmax(n1, n2) for i = 1, 2 respectively, where C1, C2 are fixed positive constants (see [4]). However, if we know that X2 is close to X1 in some sense (for example, if X2 is a small perturbation of X1), then the above approach is suboptimal both in terms of recovery accuracy and sample complexity, since in this setting y1 actually contains information about X2 (and similarly, y2 contains information about X1). There are a variety of possible approaches to incorporating this additional information. The approach we will take is inspired by the LOWESS (locally weighted scatterplot smoothing) approach from non-parametric regression. In the case of this simple example, if we look just at the problem of estimating X2, our approach reduces to solving a problem of the form\nmin X2\n\u2016A2(X2)\u2212 y2\u201622 + \u03bb\u2016A1(X2)\u2212 y1\u201622 s.t. rank ( X2 ) \u2264 r,\nwhere \u03bb is a parameter that determines how strictly we are enforcing the dynamic constraint (if X1 is very close to X2 we can set \u03bb to be larger, but if X1 is far from X2 we will set it to be comparatively small). This approach generalizes naturally to the locally weighted matrix smoothing (LOWEMS) program described in Section 2. Note that it has a (simple) convex objective function, but a nonconvex rank constraint. Our analysis in Section 3 shows that the proposed program outperforms the above na\u00efve recovery strategy both in terms of recovery accuracy and sample complexity.\nWe should emphasize that the proposed LOWEMS program is non-convex due to the exact lowrank constraint. Inspired by previous work on matrix factorization, we propose using an efficient alternating minimization algorithm (described in more detail in Section 4). We explicitly enforce the low-rank constraint by optimizing over a rank-r factorization and alternately minimize with respect to one of the factors while holding the other one fixed. This approach is popular in practice since it is typically less computationally complex than nuclear norm minimization based algorithms. In addition, thanks to recent work on global convergence guarantees for alternating minimization for low-rank matrix recovery [10, 13, 25], one can reasonably expect similar convergence guarantees to hold for alternating minimization in the context of LOWEMS, although we leave the pursuit of such guarantees for future work.\nTo empirically verify our analysis, we perform both synthetic and real world experiments, described in Section 5. The synthetic experimental results demonstrate that LOWEMS outperforms the na\u00efve approach in practice both in terms of recovery accuracy and sample complexity. We also demonstrate the effectiveness of LOWEMS in the context of recommendation systems.\nBefore proceeding, we briefly state some of the notation that we will use throughout. For a vector x \u2208 Rn, we let \u2016x\u2016p denote the standard \u2113p norm. Given a matrixX \u2208 Rn1\u00d7n2 , we use Xi: to denote the ith row of X and X:j to denote the j th column of X . We let \u2016X\u2016F denote the the Frobenius norm, \u2016X\u20162 the operator norm, \u2016X\u2016\u2217 the nuclear norm, and \u2016X\u2016\u221e = maxi,j |Xij | the elementwise infinity norm. Given a pair of matrices X,Y \u2208 Rn1\u00d7n2 , we let \u3008X,Y \u3009 = \u2211i,j XijYij = Tr ( Y TX )\ndenote the standard inner product. Finally, we let nmax and nmin denote max{n1, n2} and min{n1, n2} respectively."}, {"heading": "2 Problem formulation", "text": "The underlying assumption throughout this paper is that our low-rank matrix is changing over time during the measurement process. For simplicity we will model this through the following discrete dynamic process: at time t, we have a low-rank matrix Xt \u2208 Rn1\u00d7n2 with rank r, which we assume is related to the matrix at previous time-steps via\nXt = f(X1, . . . , Xt\u22121) + \u01ebt,\nwhere \u01ebt represents noise. We assume that we observe each Xt through a linear operator At : Rn1\u00d7n2 \u2192 Rmt , yt = At(Xt) + zt, yt, zt \u2208 Rmt , (1) where zt is measurement noise. In our problem we will suppose that we observe up to d time steps, and our goal is to recover {Xt}dt=1 jointly from {yt}dt=1. The above model is sufficiently flexible to incorporate a wide variety of dynamics, but we will make several simplifications. First, we note that we can impose the low-rank constraint explicitly by factorizing Xt as Xt = U t (V t)T , U t \u2208 Rn1\u00d7r, V t \u2208 Rn2\u00d7r. In general both U t and V t may be changing over time. However, in some applications, it is reasonable to assume that only one set of factors is changing. For example, in a recommendation system where our matrix represent user preferences, if the rows correspond to items and the columns correspond to users, then U t contains the latent properties of the items and V t models the latent preferences of the users. In this context it is reasonable to assume that only V t changes over time [9, 17], and that there is a fixed matrix U (which we may assume to be orthonormal) such that we can write Xt = UV t for all t. Similar arguments can be made in a variety of other applications, including personalized learning systems, blind signal separation, and more.\nSecond, we will assume a Markov property on f , so that Xt (or equivalently, V t) only depends on the previousXt\u22121 (orV t\u22121). Furthermore, although other dynamic models could be accommodated, for the sake of simplicity in our analysis we consider the simple model on V t where\nV t = V t\u22121 + \u01ebt, t = 2, . . . , d. (2)\nWe will also assume that both \u01ebt and the measurement noise zt are i.i.d. zero-mean Gaussian noise.\nTo simplify our discussion, we will assume that our goal is to recover the matrix at the most recent time-step, i.e., we wish to estimate Xd from {yt}dt=1. Our general approach can be stated as follows. Let C(r) = {X \u2208 Rn1\u00d7n2 : rank(X) \u2264 r}. The LOWEMS estimator is given by the following optimization program:\nX\u0302d = arg min X\u2208C(r) L (X) = arg min X\u2208C(r)\n1\n2\nd \u2211\nt=1\nwt \u2225 \u2225At (X)\u2212 yt \u2225 \u2225 2\n2 , (3)\nwhere {wt}dt=1 are non-negative weights, and we assume \u2211d\nt=1 wt = 1 to avoid ambiguity. In the following section we provide bounds on the performance of the LOWEMS estimator for two common choices of operators At."}, {"heading": "3 Recovery error bounds", "text": "Given the estimator X\u0302d from (3), we define the recovery error to be \u2206d := X\u0302d \u2212Xd. Our goal in this section will be to provide bounds on \u2016X\u0302d\u2212Xd\u2016F under two common observation models. Our analysis builds on the following (deterministic) inequality.\nProposition 3.1. Both the estimator X\u0302d by (3) and (9) satisfies\nd \u2211\nt=1\nwt \u2225 \u2225At ( \u2206d )\u2225 \u2225\n2 2 \u2264 2\n\u221a 2r\n\u2225 \u2225 \u2225 \u2225 \u2225 d \u2211\nt=1\nwtAt\u2217 ( ht \u2212 zt )\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n\u2225 \u2225\u2206d \u2225 \u2225\nF , (4)\nwhere ht = At ( Xd \u2212Xt ) and At\u2217 is the adjoint operator of At.\nThis is a deterministic result that holds for any set of {At}. The remaining work is to lower bound the LHS of (4), and upper bound the RHS of (4) for concrete choices of {At}. In the following sections we derive such bounds in the settings of both Gaussian matrix sensing and matrix completion. For simplicity and without loss of generality, we will assume m1 = . . . = md =: m0, so that the total number of observations is simply m = dm0."}, {"heading": "3.1 Matrix sensing setting", "text": "For the matrix sensing problem, we will consider the case where all operators At correspond to Gaussian measurement ensembles, defined as follows.\nDefinition 3.2. [4] A linear operator A : Rn1\u00d7n2 \u2192 Rm is a Gaussian measurement ensemble if we can express each entry of A (X) as [A (X)]i = \u3008Ai, X\u3009 for a matrix Ai whose entries are i.i.d. according to N (0, 1/m), and where the matrices A1, . . . , Am are independent from each other.\nAlso, we define the matrix restricted isometry property (RIP) for a linear map A. Definition 3.3. [4] For each integer r = 1, . . . , nmin, the isometry constant \u03b4r of A is the smallest quantity such that\n(1\u2212 \u03b4r) \u2016X\u20162F \u2264 \u2016A (X)\u2016 2 2 \u2264 (1 + \u03b4r) \u2016X\u2016 2 F\nholds for all matrices X of rank at most r.\nAn important result (that we use in the proof of Theorem 3.4) is that Gaussian measurement ensembles satisfy the matrix RIP with high probability provided m \u2265 Crnmax. See, for example, [4] for details.\nTo obtain an error bound in the matrix sensing case we lower bound the LHS of (4) using the matrix RIP and upper bound the stochastic error (the RHS of (4)) using a covering argument. The following is our main result in the context of matrix setting.\nTheorem 3.4. Suppose that we are given measurements as in (1) where all At\u2019s are Gaussian measurement ensembles. Assume that Xt evolves according to (2) and has rank r. Further assume that the measurement noise zt is i.i.d. N (\n0, \u03c321 ) for 1 \u2264 t \u2264 d and that the perturbation noise \u01ebt is i.i.d. N (\n0, \u03c322 ) for 2 \u2264 t \u2264 d. If\nm0 \u2265 D1 max { nmaxr d \u2211\nt=1\nw2t , nmax\n}\n, (5)\nwhere D1 is a fixed positive constant, then the estimator X\u0302d from (3) satisfies\n\u2225 \u2225\u2206d \u2225 \u2225\n2 F \u2264 C0\n(\nd \u2211\nt=1\nw2t \u03c3 2 1 +\nd\u22121 \u2211\nt=1\n(d\u2212 t)w2t \u03c322\n)\nnmaxr (6)\nwith probability at least P1 = 1\u2212 dC1 exp (\u2212c1n2), where C0, C1, c1 are positive constants.\nIf we choose the weights as wd = 1 and wt = 0 for 1 \u2264 t \u2264 d \u2212 1, the bound in Theorem 3.4 reduces to a bound matching classical (static) matrix recovery results (see, for example, [4] Theorem 2.4). Also note that in this case Theorem 3.4 implies exact recovery when the sample complexity is O(rn/d). In order to help interpret this result for other choices of the weights, we note that for a given set of parameters, we can determine the optimal weights that will minimize this bound. Towards this end, we define \u03ba := \u03c322/\u03c3 2 1 and set pt = (d\u2212 t), 1 \u2264 t \u2264 d. Then one can calculate the optimal weights by solving the following quadratic program:\n{w\u2217t }dt=1 = arg min\u2211 t wt=1; wt\u22650\nd \u2211\nt=1\nw2t + d\u22121 \u2211\nt=1\npt\u03baw 2 t . (7)\nUsing the method of Lagrange multipliers one can show that (7) has the analytical solution:\nw\u2217j = 1\n\u2211d i=1 1 1+pi\u03ba\n1\n1 + pj\u03ba , 1 \u2264 j \u2264 d. (8)\nA simple special case occurs when \u03c32 = 0. In this case all V t\u2019s are the same, and the optimal weights go to wt = 1d for all t. In contrast, when \u03c32 grows large the weights eventually converge to wd = 1 andwt = 0 for all t 6= d. This results in essentially using only yd to recoverXd and ignoring the rest of the measurements. Combining these, we note that when the \u03c32 is small, we can gain by a factor of approximately d over the na\u00efve strategy that ignores dynamics and tries to recover Xd using only yd. Notice also that the minimum sample complexity is proportional to \u2211d\nt=1 w 2 t when\nr/d is relatively large. Thus, when \u03c32 is small, the required number of measurements can be reduced by a factor of d compared to what would be required to recover Xd using only yd."}, {"heading": "3.2 Matrix completion setting", "text": "For the matrix completion problem, we consider the following simple uniform sampling ensemble: Definition 3.5. A linear operator A : Rn1\u00d7n2 \u2192 Rm is a uniform sampling ensemble (with replacement) if all sensing matrices Ai are i.i.d. uniformly distributed on the set\nX = { ej (n1) e T k (n2) , 1 \u2264 j \u2264 n1, 1 \u2264 k \u2264 n2 ) ,\nwhere ej (n) are the canonical basis vectors in Rn. We let p = m0/(n1n2) denote the fraction of sampled entries.\nFor this observation architecture, our analysis is complicated by the fact that it does not satisfy the matrix RIP. (A quick problematic example is a rank-1 matrix with only one non-zero entry.) To handle this we follow the typical approach and restrict our focus to matrices that satisfy certain incoherence properties. Definition 3.6. (Subspace incoherence [10]) Let U \u2208 Rn\u00d7r be the orthonormal basis for an rdimensional subspace U , then the incoherence of U is defined as \u00b5(U) := maxi\u2208[n] \u221a n\u221a r \u2225 \u2225eTi U \u2225 \u2225 2 , where ei denotes the ith standard basis vector. We also simply denote \u00b5(span(U)) as \u00b5(U). Definition 3.7. (Matrix incoherence [13]) A rank-r matrix X \u2208 Rn1\u00d7n2 with SVD X = U\u03a3V T is incoherent with parameter \u00b5 if\n\u2016U:i\u20162 \u2264 \u00b5 \u221a r\u221a\nn1 for any i \u2208 [n1] and \u2016V:j\u20162 \u2264\n\u00b5 \u221a r\u221a\nn2 for any j \u2208 [n2],\ni.e., the subspaces spanned by the columns of U and V are both \u00b5-incoherent.\nThe incoherence assumption guarantees that X is far from sparse, which make it possible to recover X from incomplete measurements since a measurement contains roughly the same amount of information for all dimensions.\nTo proceed we also assume that the matrix Xd has \u201cbounded spikiness\u201d in that the maximum entry of Xd is bounded by a, i.e., \u2225 \u2225Xd \u2225 \u2225\n\u221e \u2264 a. To exploit the spikiness constraint below we replace the optimization constraints C (r) in (3) with C (r, a) :== {X \u2208 Rn1\u00d7n2 : rank (X) \u2264 r, \u2016X\u2016\u221e \u2264 a}:\nX\u0302d = arg min X\u2208C(r,a) L (X) = arg min X\u2208C(r,a)\n1\n2\nd \u2211\nt=1\nwt \u2225 \u2225At (X)\u2212 yt \u2225 \u2225 2\n2 . (9)\nNote that Proposition 3.1 still holds for (9).\nTo obtain an error bound in the matrix completion case, we lower bound the LHS of 4 using a restricted convexity argument (see, for example, [20]) and upper bound the RHS using matrix Bernstein inequality. The result of this approach is the following theorem. Theorem 3.8. Suppose that we are given measurements as in (1) where all At\u2019s are uniform sampling ensembles. Assume that Xt evolves according to (2), has rank r, and is incoherent with parameter \u00b50 and \u2225 \u2225Xd \u2225 \u2225\n\u221e \u2264 a. Further assume that the perturbation noise and the measurement noise satisfy the same assumptions in Theorem 3.4. If m0 \u2265 D2nmin log2(n1 + n2)\u03c6\u2032(w), (10) where \u03c6\u2032(w) = maxt w 2 t ((d\u2212t)\u00b520r\u03c322/n1+\u03c321) \u2211\nd t=1 w 2 t ((d\u2212t)\u03c322+\u03c321) , then the estimator X\u0302d from (9) satisfies\n\u2225 \u2225\u2206d \u2225 \u2225\n2 F \u2264 max\n\n\n\nB1 := C2a 2n1n2\n\u221a\n\u2211d t=1 w 2 t log(n1 + n2)\nm0 , B2\n\n\n\n, (11)\nwith probability at least P1 = 1\u2212 5/(n1 + n2)\u2212 5dnmax exp(\u2212nmin), where\nB2 = C3rn\n2 1n 2 2 log(n1 + n2)\nnminm0\n((\nd \u2211\nt=1\nw2t \u03c3 2 1 +\nd\u22121 \u2211\nt=1\n(d\u2212 t)w2t \u03c322\n)\n+\nd \u2211\nt=1\nw2t a 2\n)\n, (12)\nand C2, C3, D2 are absolute positive constants.\nIf we choose the weights as wd = 1 and wt = 0 for 1 \u2264 t \u2264 d \u2212 1, the bound in Theorem 3.8 reduces to a result comparable to classical (static) matrix completion results (see, for example, [15] Theorem 7). Moreover, from the B2 term in (11), we obtain the same dependence on m as that of (6), i.e., 1/m. However, there are also a few key differences between Theorem 3.4 and our results for matrix completion. In general the bound is loose in several aspects compared to the matrix sensing bound. For example, when m0 is small, B1 actually dominates, in which case the dependence on m is actually 1/ \u221a m instead of 1/m. When m0 is sufficiently large, then B2 dominates, in which case we can consider two cases. The first case corresponds to when a is relatively large compared to \u03c31, \u03c32 \u2013 i.e., the low-rank matrix is spiky. In this case the term containing a2 in B2 dominates, and the optimal weights are equal weights of 1/d. This occurs because the term involving a dominates and there is little improvement to be obtained by exploiting temporal dynamics. In the second case, when a is relatively small compared to \u03c31, \u03c32 (which is usually the case in practice), the bound can be simplified to\n\u2016\u2206\u20162F \u2264 c3rn\n2 1n 2 2 log(n1 + n2)\nnminm0\n((\nd \u2211\nt=1\nw2t \u03c3 2 1 +\nd\u22121 \u2211\nt=1\n(d\u2212 t)w2t \u03c322\n))\n.\nThe above bound is much more similar to the bound in (6) from Theorem 3.4. In fact, we can also obtain the optimal weights by solving the same quadratic program as (7).\nWhen n1 \u2248 n2, the sample complexity is \u0398(nmin log2(n1 + n2)\u03c6\u2032(w)). In this case Theorem 3.8 also implies a similar sample complexity reduction as we observed in the matrix sensing setting. However, the precise relations between sample complexity and weights wt\u2019s are different in these two cases (deriving from the fact that the proof uses matrix Bernstein inequalities in the matrix completion setting rather than concentration inequalities of Chi-squared variables as in the matrix sensing setting)."}, {"heading": "4 An algorithm based on alternating minimization", "text": "As noted in Section 2, any rank-r matrix can be factorized as X = UV T where U is n1 \u00d7 r and V is n2 \u00d7 r, therefore the LOWEMS estimator in (3) can be reformulated as\nX\u0302d = arg min X\u2208C(r) L (X) = arg min X=UV T\nd \u2211\nt=1\n1 2 wt \u2225 \u2225At ( UV T ) \u2212 yt \u2225 \u2225 2 2 . (13)\nThe above program can be solved by alternating minimization (see [17]), which alternatively minimizes the objective function over U (or V ) while holding V (or U ) fixed until a stopping criterion is reached. Since the objective function is quadratic, each step in this procedure reduces to conventional weighted least squares, which can be solved via efficient numerical procedures. Theoretical guarantees for global convergence of alternating minimization for the static matrix sensing/completion problem have recently been established in [10, 13, 25] by treating the alternating minimization as a noisy version of the power method. Extending these results to establish convergence guarantees for (13) would involve analyzing a weighted power method. We leave this analysis for future work, but expect that similar convergence guarantees should be possible in this setting."}, {"heading": "5 Simulations and experiments", "text": ""}, {"heading": "5.1 Synthetic simulations", "text": "Our synthetic simulations consider both matrix sensing and matrix completion, but with an emphasis on matrix completion. We set n1 = 100, n2 = 50, d = 4 and r = 5. We consider two baselines:\nbaseline one is only using yd to recover Xd and simply ignoring y1, . . . yd\u22121; baseline two is using {yt}dt=1 with equal weights. Note that both of these can be viewed as special cases of LOWEMS with weights (0, . . . , 0, 1) and ( 1d , 1 d , . . . , 1 d) respectively. Recalling the formula for the optimal choice of weights in (8), it is easy to show that baseline one is equivalent to the case where \u03ba = (\u03c322)/(\u03c3 2 1) \u2192 \u221e and the baseline two equivalent to the case where \u03ba \u2192 0. This also makes intuitive sense since \u03ba \u2192 \u221e means the perturbation is arbitrarily large between time steps, while \u03ba \u2192 0 reduces to the static setting.\n1). Recovery error. In this simulation, we set m0 = 4000 and set the measurement noise level \u03c31 to 0.05. We vary the perturbation noise level \u03c32. For every pair of (\u03c31, \u03c32) we perform 10 trials, and show the average relative recovery error \u2225 \u2225\u2206d \u2225 \u2225 2\nF / \u2225 \u2225Xd \u2225 \u2225 2 F . Figure 1 illustrates how LOWEMS\nreduces the recovery error compared to our baselines. As one can see, when \u03c32 is small, the optimal \u03ba, i.e., \u03c322/\u03c3 2 1 , generates nearly equal weights (baseline two), reducing recovery error approximately by a factor of 4 over baseline one, which is roughly equal to d as expected. As \u03c32 grows, the recovery error of baseline two will increase dramatically due to the perturbation noise. However in this case the optimal \u03ba of LOWEMS grows with it, leading to a more uneven weighting and to somewhat diminished performance gains. We also note that, as expected, LOWEMS converges to baseline one when \u03c32 is large.\n2). Sample complexity. In the interest of conciseness we only provide results here for the matrix completion setting (matrix sensing yields broadly similar results). In this simulation we vary the fraction of observed entries p to empirically find the minimum sample complexity required to guarantee successful recovery (defined as a relative error \u2264 0.04). We compare the sample complexity of the proposed LOWEMS to baseline one and baseline two under different perturbation noise level \u03c32. For fixed \u03c32, the relative recovery error is the averaged over 10 trials. Figure 2 illustrates how LOWEMS reduces the sample complexity required to guarantee successful recovery. When the perturbation noise is weaker than the measurement noise, the sample complexity can be reduced approximately by a factor of d compared to baseline one. When the perturbation noise is much stronger than measurement noise, the recovery error of baseline two will increase due to the perturbation noise and hence the sample complexity increase rapidly. However in this case proposed LOWEMS still achieves relatively small sample complexity and its sample complexity converges to baseline one when \u03c32 is relatively large."}, {"heading": "5.2 Real world experiments", "text": "We next test the LOWEMS approach in the context of a recommendation system using the (truncated) Netflix dataset. We eliminate those movies with few ratings, and those users rating few movies, and generate a truncated dataset with 3199 users, 1042 movies, 2462840 ratings, and hence the fraction of visible entries in the rating matrix is \u2248 0.74. All the ratings are distributed over a period of 2191 days. For the sake of robustness, we additionally impose a Frobenius norm penalty on the factor matrices U and V in (13). We keep the latest (in time) 10% of the ratings as a testing set. The remaining ratings are split into a validation set and a training set for the purpose of cross valida-\ntion. We divide the remaining ratings into d \u2208 {1, 3, 6, 8} bins respectively with same time period according to their timestamps. We use 5-fold cross validation, and we keep 1/5 of the ratings from the dth bin as a validation set. The number of latent factors r is set to 10. The Frobenius norm regularization parameter \u03b3 is set to 1. We also note that in practice one likely has no prior information on \u03c31, \u03c32 and hence \u03ba. However, we use model selection techniques like cross validation to select the best \u03ba incorporating the unknown prior information on measurement/perturbation noise. We use root mean squared error (RMSE) to measure prediction accuracy. Since alternating minimization uses a random initialization, we generate 10 test RMSE\u2019s (using a boxplot) for the same testing set. Figure 3(a) shows that the proposed LOWEMS estimator improves the testing RMSE significantly with appropriate \u03ba. Additionally, the performance improvement increases as d gets larger.\nTo further investigate how the parameter \u03ba affects accuracy, we also show the validation RMSE compared to \u03ba in Figure 3(b). When \u03ba \u2248 1, LOWEMS achieves the best RMSE on the validation data. This further demonstrates that imposing an appropriate dynamic constraint should improve recovery accuracy in practice."}, {"heading": "6 Conclusion", "text": "In this paper we consider the low-rank matrix recovery problem in a novel setting, where one of the factor matrices changes over time. We propose the locally weighted matrix smoothing (LOWEMS) framework, and have established error bounds for LOWEMS in both the matrix sensing and matrix completion cases. Our analysis quantifies how the proposed estimator improves recovery accuracy and reduces sample complexity compared to static recovery methods. Finally, we provide both synthetic and real world experimental results to verify our analysis and demonstrate superior empirical performance when exploiting dynamic constraints in a recommendation system."}, {"heading": "A Proof of Proposition 3.1", "text": "Proof. Let x := vec (X) \u2208 Rn1n2 and L\u0303 (x) := L (X). Since the objective function is continuous in X and the set C (r) is compact, L (X) achieves a minimizer at some point X\u0302d \u2208 C (r). Since X\u0302d is a minimizer of the constrained problem, then for all matrices X \u2208 C (r) we have the following inequality\nL\u0303 ( x\u0302d ) \u2212 L\u0303 (x) \u2264 0. (14) By the second-order Taylor\u2019s theorem, we expand L\u0303 (x) around xd = vec ( Xd )\nL\u0303 (x) = L\u0303 ( xd ) + \u2329 \u2207L\u0303 ( xd ) , x\u2212 xd \u232a + 1\n2\n\u2329\n\u22072L\u0303 (x\u0304) ( x\u2212 xd ) , x\u2212 xd \u232a , (15)\nwhere x\u0304 = \u03b1xd + (1\u2212 \u03b1) x for some \u03b1 \u2208 [0, 1]. Plugging (15) with x = x\u0302d into (14) we obtain \u2329\n\u2207L\u0303 ( xd ) , x\u0302d \u2212 xd \u232a + 1\n2\n\u2329\n\u22072L\u0303 (x\u0304) ( x\u0302d \u2212 xd ) , x\u0302d \u2212 xd \u232a \u2264 0. (16)\nThrough some algebraic manipulation we have the following expression for the gradient of L\u0303 (x):\n\u2207L\u0303 (x) = vec ( d \u2211\nt=1\nwtAt\u2217 [ At (X)\u2212 yt ]\n)\n. (17)\nBased on the above gradient it follows that\n\u22072L\u0303 (x) b = vec ( d \u2211\nt=1\nwtAt\u2217 [ At (B) ]\n)\n, (18)\nwhere b = vec (B).\nNow based on (17) and (18), the absolute value of first term in (16) can be bounded as\n\u2223 \u2223 \u2223\u3008\u2207L\u0303 ( xd ) , x\u0302d \u2212 xd\u3009 \u2223 \u2223 \u2223 =\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2329 d \u2211\nt=1\nwtAt\u2217 [ At ( Xd ) \u2212 yt ] ,\u2206d\n\u232a\u2223\n\u2223 \u2223 \u2223 \u2223\n\u2264 \u2225 \u2225 \u2225\n\u2225 \u2225\nd \u2211\nt=1\nwtAt\u2217 [ At ( Xd ) \u2212 yt ]\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n\u2225 \u2225\u2206d \u2225 \u2225\n\u2217\n\u2264 \u2225 \u2225 \u2225\n\u2225 \u2225\nd \u2211\nt=1\nwtAt\u2217 ( ht \u2212 zt )\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n\u221a 2r \u2225 \u2225\u2206d \u2225 \u2225\nF\n(19)\nThe first inequality above used the trace dual norm inequality, while the second inequality follows from a basic inequality for rank-2r matrices. Similarly the second term in (16) is\n1\n2\n\u2329\n\u22072L\u0303 (x\u0304) ( x\u0302d \u2212 xd ) , x\u0302d \u2212 xd \u232a = 1\n2\n\u2329\nd \u2211\nt=1\nwtAt\u2217At ( \u2206d ) ,\u2206d\n\u232a\n= 1\n2\nd \u2211\nt=1\nwt \u2329 At ( \u2206d ) ,At ( \u2206d )\u232a .\n(20)\nThe result follows from combining (19) and (20). Note that the above proof holds if we replace C (r, ) with C (r, a), which completes our proof."}, {"heading": "B Proof of Theorem 3.4", "text": "Proof. The proof consists of lower bounding the LHS of (4) and upper bounding the RHS of (4).\nWe use the following lemma to lower bound \u2211d t=1 wt \u2225 \u2225At ( \u2206d )\u2225 \u2225 2 2 .\nLemma B.1. Suppose the linear operator At : Rn1\u00d7n2 \u2192 Rm0 is random Gaussian ensemble for all 1 \u2264 t \u2264 d. If m0 > Dnmaxr \u2211d t=1 w 2 t , the composite operator {\u221a wtAt }d t=1 satisfies the rank-2r matrix RIP with constant \u03b42r \u2264 \u03b4 with probability exceeding 1 \u2212 C exp (\u2212cm0), where D,C and c (which depends on \u03c3) are absolute positive constants.\nProof. See Appendix C.\nNext lemma gives us an upper bound for the stochastic error \u2225 \u2225\n\u2225 \u2211d t=1 wtAt\u2217 (ht \u2212 zt)\n\u2225 \u2225 \u2225\n2 .\nLemma B.2. Under the assumptions of Theorem 3.4, when m0 \u2265 Dnmax, we have \u2225\n\u2225 \u2225 \u2225 \u2225\nd \u2211\nt=1\nwtAt\u2217 ( ht \u2212 zt )\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n\u2264 C1\n\u221a \u221a \u221a \u221anmax(1 + \u03b41) ( d \u2211\nt=1\nw2t \u03c3 2 1 +\nd\u22121 \u2211\nt=1\n(d\u2212 t)w2t 2rn2 m0 \u03c322\n)\nwith probability exceeding 1 \u2212 dC exp(\u2212cn2), where D,C1, C, c are positive constants and \u03b41 is the rank-1 matrix RIP parameter for all At\u2019s.\nProof. See Appendix D.\nTheorem 3.4 follows by combining Lemma B.1, Lemma B.2 and Definition 3.3."}, {"heading": "C Proof of Lemma B.1", "text": "Proof. First we introduce the following theorem providing a double-sided tail bound on the sum of independent sub-exponential random variables.\nTheorem C.1. For independent Xi sub-exponential with parameters (\u03c3i, bi), with mean \u00b5i,\nP\n(\u2223\n\u2223 \u2223 \u2223 \u2223\nn \u2211\ni=1\n(Xi \u2212 \u00b5i) \u2223 \u2223 \u2223 \u2223\n\u2223\n\u2265 nt ) \u2264 2 exp ( \u2212 nt 2\n2 (\u03c32 + bt)\n)\n,\nwhere \u03c32 = \u2211 i \u03c3 2 i and b = maxi bi.\nWe now lower bound \u2211d t=1 wt \u2225 \u2225At ( \u2206d )\u2225 \u2225 2 2 . Since all At\u2019s are Gaussian random measurement ensembles, then a particular measurement \u2329\nAti,\u2206 d \u232a2 is distributed as m\u221210 \u2225 \u2225\u2206d \u2225 \u2225 2 F \u03c72 (1). Therefore\n\u2211d t=1 wt\n\u2225 \u2225At ( \u2206d )\u2225 \u2225 2\n2 =\n\u2211 t,i wt \u2329 Ati, ( \u2206d )\u232a2 is a weighted sum of i.i.d. \u03c72 (1) random variables. Since \u03c72 (1) is sub-exponential with parameters (4, 4), Theorem C.1 implies a double-sided tail bound for \u2211d\nt=1 wt \u2225 \u2225At ( \u2206d )\u2225 \u2225 2 2 : for any given \u2206d \u2208 Rn1\u00d7n2 and any fixed 0 < s < 1\nP\n(\u2223\n\u2223 \u2223 \u2223 \u2223\nd \u2211\nt=1\nwt \u2225 \u2225At ( \u2206d )\u2225 \u2225\n2 2 \u2212 \u2225 \u2225\u2206d \u2225 \u2225 2 F\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2264 s \u2225 \u2225\u2206d \u2225 \u2225 2 F ) \u2264 2 exp ( \u2212 m0s 2\n8 \u2211d t=1 w 2 t + 8wmaxs\n)\n,\nwhere wmax = max{w1, . . . , wd}. The probability can be further simplified if s is very small (\u2264 1/d). Rank of \u2206d is at most 2r since X\u0302d, Xd are rank-r matrices. By Theorem 2.3 in [4] (one may see the proof if necessary) if m0 > Dnmaxr \u2211d t=1 w 2 t , the composite operator {\u221a wtAt }d t=1 satisfies the rank-2r matrix RIP with constant \u03b42r \u2264 \u03b4 with probability exceeding 1\u2212C exp (\u2212cm0), where C and c (depends on \u03b4) are absolute positive constants."}, {"heading": "D Proof of Lemma B.2", "text": "Proof. Let W = \u2211d t=1 wtAt\u2217 (ht \u2212 zt) and n = nmax for short. Following the basic framework of the proof of Lemma 1.1 in [4], we use \u01eb-nets method to bound the stochastic error \u2016W\u20162. The operator norm of W is\n\u2016W\u20162 = sup \u2016u\u2016=\u2016v\u2016=1 \u3008u,Wv\u3009 ,\nConsider a 1/4-net N1/4 of the unite sphere Sn\u22121 with \u2223 \u2223N1/4 \u2223 \u2223 \u2264 12n (see (III.1) in [4]). For any v, u \u2208 Sn\u22121\n\u3008u,Wv\u3009 = \u3008u\u2212 u0,Wv\u3009+ \u3008u0,W (v \u2212 v0)\u3009+ \u3008u0,Wv0\u3009 \u2264 \u2016W\u20162 \u2016u\u2212 u0\u20162 + \u2016W\u20162 \u2016v \u2212 v0\u20162 + \u3008u0,Wv0\u3009 ,\nfor some v0, w0 \u2208 N1/4 obeying \u2016u\u2212 u0\u20162 \u2264 1/4 and \u2016v \u2212 v0\u2016 \u2264 1/4. So the operator norm of W is\n\u2016W\u20162 \u2264 2 sup u0,v0\u2208N1/4 \u3008u0,Wv0\u3009 .\nFor fixed u0, v0\n\u3008u0,Wv0\u3009 = Tr ( uT0 Wv0 ) = Tr ( v0u T 0 W ) = \u2329 u0v T 0 ,W \u232a =\nd \u2211\nt=1\nwt \u2329 At ( u0v T 0 ) , ht \u2212 zt \u232a .\nLet Z = \u2211d t=1 wt \u2329 At ( u0v T 0 ) , zt \u232a and H = \u2211d t=1 wt \u2329 At ( u0v T 0 ) , ht \u232a . Since for all 1 \u2264 t \u2264 d, entries of zt are i.i.d. N (\n0, \u03c321 ) , therefore Z \u223c N ( 0, \u03c32Z ) , where the variance \u03c32Z is\n\u03c32Z =\nd \u2211\nt=1\nw2t \u2225 \u2225At ( u0v T 0 )\u2225 \u2225\n2 2 \u03c321 \u2264\nd \u2211\nt=1\nw2t (1 + \u03b41) \u2225 \u2225u0v T 0 \u2225 \u2225\n2 F \u03c321 =\nd \u2211\nt=1\nw2t (1 + \u03b41)\u03c3 2 1 . (21)\nThe first inequality uses the matrix RIP for rank-1 matrices. For a fixed t, At satisfies the rank-1 matrix RIP with constant \u03b41, with probability at least 1\u2212C2 exp(\u2212c2m0) provided that m0 \u2265 D2n by Theorem 2.3 in [4], where C2, c2 and D2 are fixed positive constants. Then by a union bound, for all 1 \u2264 t \u2264 d, At satisfies the rank-1 matrix RIP property with parameter \u03c31, with probability at least 1\u2212 dC2 exp(\u2212c2m0) provided that m0 \u2265 D2n. We now simplify H as\nH = d \u2211\nt=1\nwt \u2329 At ( u0v T 0 ) , ht \u232a\n= d\u22121 \u2211\nt=1\nwt\n\u2329\nAt ( u0v T 0 )\n, d \u2211\ns=t+1\nAt [ U (\u01ebs)T ]\n\u232a\n=\nd \u2211\ns=2\ns\u22121 \u2211\nt=1\n\u2329\nwtAt ( u0v T 0 )\n,At [ U (\u01ebs) T ]\u232a\n=\nd \u2211\ns=2\ns\u22121 \u2211\nt=1\n\u2329\nwtAt\u2217At ( u0v T 0 ) , U (\u01ebs) T \u232a\n=\nd \u2211\ns=2\ns\u22121 \u2211\nt=1\nm0 \u2211\ni=1\n\u2329\nwt [ At ( u0v T 0 )] i Ati, U (\u01eb\ns) T \u232a\n=\nd \u2211\ns=2\n\u2329 s\u22121 \u2211\nt=1\nwt \u2225 \u2225At ( u0v T 0 )\u2225 \u2225 2 UTAt, (\u01ebs) T\n\u232a\n,\nwhere At \u2208 Rn1\u00d7n2 contains i.i.d. N (0, 1/m0) entries. The last equality uses the property that sum of independent Gaussian variables is also Gaussian, and the variance is the sum of individual variances. Since for all 2 \u2264 s \u2264 d, entries of \u01ebs are i.i.d. N (\n0, \u03c322 ) , therefore H \u223c N ( 0, \u03c32H ) ,\nwhere the variance \u03c32H is\n\u03c32H = d \u2211\ns=2\n\u2225 \u2225 \u2225 \u2225 \u2225 s\u22121 \u2211\nt=1\nwt \u2225 \u2225At ( u0v T 0 )\u2225 \u2225 2 UTAt\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\nF\n\u03c322 (\u03be1) \u2264 d \u2211\ns=2\n\u2225 \u2225 \u2225 \u2225 \u2225 s\u22121 \u2211\nt=1\nwt \u221a 1 + \u03b41U TAt\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\nF\n\u03c322\n(\u03be2) =\nd \u2211\ns=2\ns\u22121 \u2211\nt=1\nw2t (1 + \u03b41) \u2225 \u2225UTBs \u2225 \u2225\n2 F \u03c322\n= d \u2211\ns=2\ns\u22121 \u2211\nt=1\nw2t (1 + \u03b41) 1\nm0 \u03c72s (rn2)\u03c3 2 2\n(\u03be3) \u2264 d \u2211\ns=2\ns\u22121 \u2211\nt=1\nw2t (1 + \u03b41) 1\nm0 3m0\u03c3\n2 2\n=\nd\u22121 \u2211\nt=1\n(d\u2212 t)w2t (1 + \u03b41)\u03c322 .\n(22)\nInequality (\u03be1) holds with probability exceeding 1 \u2212 dC2 exp(\u2212c2m0) provided that m0 \u2265 Dn based on the matrix RIP for rank-1 matrices as used while bounding \u03c32Z . Equality (\u03be2) uses the property that sum of independent Gaussian variables is also Gaussian and entries of Bs are i.i.d. N (0, 1/m0). Inequality (\u03be3) holds with probability at least 1\u2212 dC3 exp(\u2212c3m0) by the concentration property of correlated Chi-squared variables.\nSince the measurement noise Z and dynamic perturbation H are independent, then \u3008u0,Wv0\u3009 \u223c N (\n0, \u03c32Z + \u03c3 2 H\n)\n. Then by a standard tail bound for Gaussian random variables we have\nP (|\u3008u0,Wv0\u3009| > \u03bb) \u2264 2 exp ( \u2212 \u03bb 2\n2 (\u03c32H + \u03c3 2 Z)\n)\n.\nTherefore by an standard union bound we bound the stochastic error\nP\n(\n\u2016W\u20162 \u2265 C0 \u221a n (\u03c32H + \u03c3 2 Z)\n)\n\u2264 2 \u2223 \u2223N1/4 \u2223 \u2223 2 exp\n(\n\u2212C 2 0n\n8\n)\n\u2264 2 exp (\u2212cn) , (23)\nwhere c = C 2 0 8 \u2212 2 log 12. To ensure c > 0, we require C0 > 4 \u221a log 12. Combining (21), (22), and (23), if m0 \u2265 Dn we have\n\u2016W\u20162 \u2264 C0\n\u221a \u221a \u221a \u221an ( (1 + \u03b41) d \u2211\nt=1\nw2t\n(\n\u03c321 + (d\u2212 t) 5rn2 m0 \u03c322\n)\n)\nwith probability exceeding 1 \u2212 [dC2 exp(\u2212c2m0) + dC3 exp(\u2212c3m0) + 2 exp(\u2212cn)] \u2265 1 \u2212 dC exp(\u2212cn2)."}, {"heading": "E Proof of Theorem 3.8", "text": "Proof. The proof follows the same framework of the proof of Theorem 7 in [15].\nBefore we lower bound \u2211d t=1 wt \u2225 \u2225At ( \u2206d )\u2225 \u2225 2 2 , we consider the following constraint set for a given 0 < r \u2264 n:\nE (r) =\n\n\n\nX \u2208 C(r) : \u2016X\u2016\u221e = 1, \u2016X\u2016 2 F \u2265 n1n2\n\u221a\n64 \u2211d t=1 w 2 t log(n1 + n2)\nlog(6/5)m0\n\n\n\n.\nDefine the following random matrix\n\u03a3R =\nd \u2211\nt=1\nm0 \u2211\ni=1\nwt\u03b3 t iA t i,\nwhere \u03b3ti is Rademacher variable. The following lemma bounds the restricted strong convexity (see [20]) of the operator {\u221a wtAt }d\nt=1 .\nLemma E.1. Suppose all At\u2019s are fixed uniform sampling ensembles. For all X \u2208 E (r) d\n\u2211\nt=1\nwt \u2225 \u2225At (X) \u2225 \u2225 2 2 \u2265 p 2 \u2016X\u20162F \u2212 44rn1n2 m0 (E(\u2016\u03a3R\u2016))2 (24)\nwith probability at least 1\u2212 2(n1+n2) .\nProof. See Appendix F.\nNote that \u2225 \u2225\u2206d \u2225 \u2225 \u221e \u2264 \u2225 \u2225 \u2225X\u0302d \u2225 \u2225 \u2225\n\u221e +\n\u2225 \u2225Xd \u2225 \u2225 \u221e \u2264 2 \u2225 \u2225Xd \u2225 \u2225\n\u221e. To proceed, we consider the following two cases.\nCase I. \u2206 d\n2\u2016Xd\u2016 \u221e\n/\u2208 E(2r).\nFollowing the definition of E(2r) we have\n\u2225 \u2225\u2206d \u2225 \u2225\n2 F \u2264 c2 \u2225 \u2225Xd \u2225 \u2225 2 \u221e n1n2\n\u221a\n\u2211d t=1 w 2 t log(n1 + n2)\nm0 ,\nwhere C2 = 4 \u221a 64 log(6/5) . This yields the first part of inequality (11) in Theorem 3.8.\nCase II. \u2206 d\n2\u2016Xd\u2016 \u221e\n\u2208 E(2r).\nSince \u2206 d\n2\u2016Xd\u2016 \u221e \u2208 E(2r), applying Lemma E.1 yields d\n\u2211\nt=1\nwt \u2225 \u2225At ( \u2206d )\u2225 \u2225\n2 2 \u2265 p\n2\n\u2225 \u2225\u2206d \u2225 \u2225\n2 F \u2212 362rn1n2\nm0 (E(\u2016\u03a3R\u2016))2\n\u2225 \u2225Xd \u2225 \u2225\n2 \u221e . (25)\nCombining (25) and (4) yields\np\n2\n\u2225 \u2225\u2206d \u2225 \u2225\n2 F \u2264 2\n\u221a 2r\n\u2225 \u2225 \u2225 \u2225 \u2225 d \u2211\nt=1\nwtAt\u2217 ( ht \u2212 zt )\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n\u2225 \u2225\u2206d \u2225 \u2225\nF + 362rn1n2 m0 (E(\u2016\u03a3R\u2016))2 \u2225 \u2225Xd \u2225 \u2225 2 \u221e\n\u2264 8r p\n\u2225 \u2225 \u2225 \u2225 \u2225 d \u2211\nt=1\nwtAt\u2217 ( ht \u2212 zt )\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n2\n+ p\n4\n\u2225 \u2225\u2206d \u2225 \u2225\n2 F + 362rn1n2 m0 (E(\u2016\u03a3R\u2016))2 \u2225 \u2225Xd \u2225 \u2225 2 \u221e .\nThe above inequality can be further simplified as\n\u2225 \u2225\u2206d \u2225 \u2225\n2 F \u2264 32rn\n2 1n 2 2\nm20\n\u2225 \u2225 \u2225 \u2225 \u2225 d \u2211\nt=1\nwtAt\u2217 ( ht \u2212 zt )\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n2\n+ 1448rn21n 2 2\nm20 (E(\u2016\u03a3R\u2016))2\n\u2225 \u2225Xd \u2225 \u2225\n2 \u221e . (26)\nNext we bound E(\u2016\u03a3R\u2016) in the following lemma. Lemma E.2. Suppose all At\u2019s are fixed uniform sampling ensembles. For m0 \u2265 Dnmin log (n1 + n2)\u03c6(w), where \u03c6(w) = w2 max\u2211d\nt=1 w 2 t\n, there exists an absolute positive constant C\nsuch that\nE(\u2016\u03a3R\u2016) \u2264 C\n\u221a\n2e log (n1 + n2) \u2211d t=1 w 2 tm0\nnmin . (27)\nThe proof is not provided since it is almost the same as that of Lemma 6 in [15] with some minor modifications. Note that our results are a bit stronger compared to Lemma 6 in [15], since we are dealing with bounded variables.\nNow we upper bound the stochastic error \u2016J\u201622 := \u2225 \u2225 \u2225 \u2211d t=1 wtAt\u2217 (ht \u2212 zt) \u2225 \u2225 \u2225 2\n2 . First, we rewrite J\nas\nJ =\nd \u2211\nt=1\nwtAt\u2217At  U ( d \u2211\ns=t+1\n\u01ebs\n)T\n+ Zt\n\n ,\nwhere each entry of the random matrix Zt \u2208 Rn1\u00d7n2 is i.i.d. Gaussian distributed with variance \u03c321 . Set Y t = U ( \u2211d s=t+1 \u01eb s )T and F t = Y t + Zt. Note that F t may be correlated for different 1 \u2264 t \u2264 d, though for a given t the entries of F t are independent. We now introduce an n1 \u00d7 n2 random matrix Gt that has exactly one non-zero entry:\nGt = wtn1n2F t ijEij , with probability\n1\nn1n2 ,\nwhereEij is the canonical basis of matrices with dimensionn1\u00d7n2. We also introduce the following random matrix Ht, which is the average of m0 independent copies of Gt:\nHt = 1\nm0\nm0 \u2211\ni=1\nGti where each G t i is an independent copy of G t.\nThen J can be decomposed as sum of independent random matrices: J = m0n1n2 \u2211d t=1 H t. It is immediate that\nEGt = EHt = wtF t, EJ = m0 n1n2\nd \u2211\nt=1\nwtF t.\nBefore we proceed we introduce a lemma describing the spectral norm deviation of a sum of uncentered random matrices from its mean value.\nLemma E.3. (Corollary 6.1.2 in [24]) Consider a finite sequence {Sk} of independent random matrices with common dimension n1\u00d7n2. Assume that each matrix has uniformly bounded deviation from its mean: \u2016Sk \u2212 ESk\u2016 \u2264 L for each index k. Consider the sum\nZ = \u2211\nk\nSk.\nLet \u03c1(Z) denotes the matrix variance statistic of the sum:\n\u03c1(Z) = max {\u2225 \u2225E[(Z \u2212 EZ)(Z \u2212 EZ)T ] \u2225 \u2225 , \u2225 \u2225E[(Z \u2212 EZ)T (Z \u2212 EZ)] \u2225 \u2225 }\n= max\n{\u2225\n\u2225 \u2225 \u2225 \u2225 \u2211\nk\nE[(Sk \u2212 ESk)(Sk \u2212 ESk)T ] \u2225 \u2225 \u2225 \u2225\n\u2225\n,\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nk\nE[(Sk \u2212 ESk)T (Sk \u2212 ESk)] \u2225 \u2225 \u2225 \u2225\n\u2225\n}\n.\nThen for all s \u2265 0,\nP (\u2016Z \u2212 EZ\u2016 \u2265 s) \u2264 (n1 + n2) exp ( \u2212s2/2 \u03c1(Z) + Ls/3 ) .\nWe are going to apply the above uncentered Bernstein inequality to the sum of dm0 independent random matrices\n\u2211d t=1 H t = 1m0 \u2211d t=1 \u2211m0 k=1 G t k. Before doing so, we note that for given t and k,\n\u2225 \u2225Gtk \u2212 EGtk \u2225 \u2225 \u2264 \u2225 \u2225Gtk \u2225 \u2225+ \u2225 \u2225EGtk \u2225 \u2225 \u2264 \u2225 \u2225Gtk \u2225 \u2225+ E \u2225 \u2225Gtk \u2225 \u2225 \u2264 2 \u2225 \u2225Gtk \u2225 \u2225 .\nThe first inequality uses the triangle inequality; the second is Jensen\u2019s inequality.\nTo control \u03c1( \u2211d t=1 H t), first note that\n0 \u2211\nt\n\u2211\nk\nE [ Gtk \u2212 EGtk)(Gtk \u2212 EGtk)T ] = \u2211\nt\n\u2211\nk\nE [ (Gtk(G t k) T ] \u2212 (EGtk)(EGtk)T\n\u2211\nt\n\u2211\nk\nE [ Gtk(G t k) T ]\n= m0 \u2211\nt\nE [ Gt(Gt)T ] .\nThe third relation holds because (EGtk)(EG t k) T is positive semidefinite; the last relation uses the fact that for a fixed t, Gtk are random matrices following identical distributions independently for all 1 \u2264 k \u2264 m0. Now we can control \u03c1( \u2211d t=1 H t) in the following\n\u03c1\n(\nd \u2211\nt=1\nHt\n)\n\u2264 1 m0 max\n{\u2225\n\u2225 \u2225 \u2225 \u2225 \u2211\nt\nE [ (Gt(Gt)T ]\n\u2225 \u2225 \u2225 \u2225 \u2225 , \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nt\nE [ (Gt)TGt ]\n\u2225 \u2225 \u2225 \u2225 \u2225 } .\nSet \u03c10 := max {\u2225 \u2225 \u2225 \u2211d t=1 E(G t(Gt)T ) \u2225 \u2225 \u2225 , \u2225 \u2225 \u2225 \u2211d t=1 E((G t)TGt) \u2225 \u2225 \u2225 } . Then the remaining work is to uniformly upper bound \u2016Gtk\u2016 for all 1 \u2264 t \u2264 d and 1 \u2264 k \u2264 m0 and upper bound \u03c10. First we turn to the uniform bound on the spectral norm of the random matrix Gtk for all 1 \u2264 t \u2264 d and 1 \u2264 k \u2264 m0. We have for all 1 \u2264 t \u2264 d and 1 \u2264 k \u2264 m0\n\u2225 \u2225Gtk \u2225 \u2225 \u2264 max i,j,t wt \u2225 \u2225n1n2F t ijEij \u2225 \u2225 = n1n2 max i,j,t wt|F tij |.\nSince \u00b5(U) \u2264 \u00b50, the variance of each entry of the random matrix F t can be bounded as Var(F tij) \u2264 \u00b52 0 r\nn1 \u03c322(d \u2212 t) + \u03c321 . Let \u03c32max = maxt w2t\n(\n\u00b52 0 r\nn1 \u03c322(d\u2212 t) + \u03c321\n)\n. Then by the tail probability\nof Gaussian random variables and the standard union bound (over i, j), for all 1 \u2264 t \u2264 d and 1 \u2264 k \u2264 m0 we have\nP\n(\n\u2225 \u2225Gtk \u2225 \u2225 \u2264 n1n2 \u221a 2 log(d(n1 + n2)n1n2)\u03c32max =: L ) \u2265 1\u2212 2/(n1 + n2).\nSecond we turn to the computation of E(Gt(Gt)T ). We have\nE(Gt(Gt)T ) = w2t n 2 1n 2 2\nn1 \u2211\ni=1\nn2 \u2211\nj=1\n(F tij) 2EijE T ij\n1\nn1n2 = w2t n1n2\nn1 \u2211\ni=1\nn2 \u2211\nj=1\n(F tij) 2Eii.\nSimilarly E((Gt)TGt) = w2t n1n2 \u2211n1 i=1 \u2211n2 j=1(F t ij) 2Ejj . Then\n\u03c1 = n1n2max\n\n\n\nmax i\nd \u2211\nt=1\nn2 \u2211\nj=1\nw2t (F t ij) 2,max j\nd \u2211\nt=1\nn1 \u2211\ni=1\nw2t (F t ij) 2\n\n\n\n.\nLet ai = \u2211d\nt=1 \u2211n2 j=1 w 2 t (F t ij) 2 and bj = \u2211d t=1 \u2211n1 i=1 w 2 t (F t ij) 2. We first bound maxi ai. Note that\nai = \u2211d t=1 w 2 t \u2211n2 j=1(Y t ij + Z t ij) 2 \u2264 2\u2211dt=1 w2t \u2211n2 j=1[(Y t ij) 2 + (Ztij) 2]. Note that for 1 \u2264 i \u2264 n1 and 1 \u2264 t \u2264 d, \u2211n2j=1(Ztij)2 \u223c \u03c321\u03c72(n2) and are independent. So by the tail bound of Chi-squared variable and the standard union bound (over i and t) we have\nP\n\nmax i\nd \u2211\nt=1\nw2t\nn2 \u2211\nj=1\n(Ztij) 2 \u2264 5n2\nd \u2211\nt=1\nw2t \u03c3 2 1\n\n \u2265 1\u2212 dn1 exp(\u2212n2). (28)\nSimilarly we have\nP\n(\nmax j\nd \u2211\nt=1\nw2t\nn2 \u2211\ni=1\n(Ztij) 2 \u2264 5n1\nd \u2211\nt=1\nw2t \u03c3 2 1\n)\n\u2265 1\u2212 dn2 exp(\u2212n1). (29)\nFor \u2211n2 j=1(Y t ij) 2, note that Y tij is Gaussian distributed and the variance is not greater than \u00b52 0 r n1 (d \u2212 t)\u03c322 for all i, j, t, since \u00b5(U) \u2264 \u00b50. For a fixed i, for all 1 \u2264 j \u2264 n2, Y tij are independent Gaussian random variables. So given i and t, applying the tail bound of Chi-squared random variables yields\nP\n\n\nn2 \u2211\nj=1\n(Y tij) 2 \u2264 5n2(d\u2212 t)\n\u00b520r\nn1 \u03c322\n\n \u2265 1\u2212 exp(\u2212n2).\nBy the standard union bound (over i and t) we have\nP\n\nmax i\nd \u2211\nt=1\nw2t\nn2 \u2211\nj=1\n(Y tij) 2 \u2264 5n2\n\u00b520r\nn1\nd \u2211\nt=1\n(d\u2212 t)w2t \u03c322\n\n \u2265 1\u2212 dn1 exp(\u2212n2). (30)\nNow we turn to \u2211n1 i=1(Y t ij) 2, which follows a Chi-squared distribution (d\u2212 t)\u03c322\u03c72(r), since n1 \u2211\ni=1\n(Y tij) 2 = (Y t:j) TY t:j = \u01eb\u0304 t j:U TU ( \u01eb\u0304tj: )T = \u01eb\u0304tj: ( \u01eb\u0304tj: )T\nwhere \u01eb\u0304t = \u2211d s=t+1 \u01eb s. The last equality uses the fact that U is orthonormal. Then by the tail bound of Chi-squared random variables and the standard union bound (over j and t) we have\nP\n(\nmax j\nd \u2211\nt=1\nw2t\nn1 \u2211\ni=1\n(Y tij) 2 \u2264 5n1\nd \u2211\nt=1\n(d\u2212 t)w2t \u03c322\n)\n\u2265 1\u2212 dn2 exp(\u2212n1). (31)\nCombining (28) and (30) yields\nP\n(\nmax i\nai \u2264 10n2 d \u2211\nt=1\nw2t\n(\n\u03c321 + \u00b520r\nn1 (d\u2212 t)\u03c322\n)\n)\n\u2265 1\u2212 2dn1 exp(\u2212n2). (32)\nSimilarly combining (29) and (31) yields\nP\n(\nmax j\nbj \u2264 10n1 d \u2211\nt=1\nw2t ( \u03c321 + (d\u2212 t)\u03c322 )\n)\n\u2265 1\u2212 2dn2 exp(\u2212n1). (33)\nNote that 1 \u2264 \u00b50 \u2264 \u221a n1/ \u221a r, so \u00b5 2 0 r\nn1 \u2264 1. Now we are ready to bound \u03c10 by combining (32) and\n(33):\nP\n( \u03c10 \u2264 10nmaxn1n2 ( d \u2211\nt=1\nw2t \u03c3 2 1 +\nd \u2211\nt=1\nw2t (d\u2212 t)\u03c322\n)\n=: \u03bd\n)\n\u2265 1\u22124dnmax exp(\u2212nmin). (34)\nNow by Lemma E.3, we have\nP\n(\u2225\n\u2225 \u2225 \u2225 \u2225\nd \u2211\nt=1\nHt \u2212 d \u2211\nt=1\nwtF t\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2265 s ) \u2264 (n1 + n2) exp ( \u2212m0s2/2 \u03bd + 2Ls/3 ) .\nIf we let s = \u221a\n8 log(n1+n2)\u03bd m0 and substitute this into the above matrix Bernstein inequality we obtain\nP\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 d \u2211\nt=1\nHt \u2212 d \u2211\nt=1\nwtF t\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2265 \u221a 8 log(n1 + n2)\u03bd\nm0\n\n \u2264 1/(n1 + n2).\nA hidden condition when the above inequality holds is that \u03bd dominates the denominator of the exponential term. The remaining work is to have sufficiently large m0 to guarantee that \u03bd dominates the denominator of the exponential, which follows\n\u03bd \u2265 2/3L \u221a 8 log(n1 + n2)\u03bd\nm0 .\nThe above inequality immediately implies that\nm0 \u2265 32\n45 nmin log(d(n1 + n2)n1n2) log(n1 + n2)\nmaxt w 2 t\n(\n(d\u2212 t)\u00b5 2 0 r\nn1 \u03c322 + \u03c3 2 1\n)\n\u2211d t=1 w 2 t ((d\u2212 t)\u03c322 + \u03c321)\n.\nNote that n1 + n2 > ni, i = 1, 2, and n1 + n2 > d, then the above sample complexity can be simplified as\nm0 \u2265 128\n45 nmin log\n2(n1 + n2) maxt w\n2 t\n(\n(d\u2212 t)\u00b5 2 0 r\nn1 \u03c322 + \u03c3 2 1\n)\n\u2211d t=1 w 2 t ((d\u2212 t)\u03c322 + \u03c321)\n. (35)\nThe remaining work is to bound \u2225 \u2225\n\u2225 \u2211d t=1 wtF\nt \u2225 \u2225\n\u2225 . First we note that each entry of F t is Gaussian and\nthe variance is not greater than \u03c321 +(d\u2212 t)\u03c322 . Then, according to results on bounds for the spectral norm of i.i.d. Gaussian ensemble, we have\nP\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 d \u2211\nt=1\nwtF t\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2264 2\n\u221a \u221a \u221a \u221a d \u2211\nt=1\nw2t (\u03c3 2 1 + (d\u2212 t)\u03c322) \u221a nmax\n\n \u2265 1\u2212 C1 exp(\u2212c2nmax), (36)\nwhere C1, c2 are absolute positive constants. Note that C1 exp(\u2212c2nmax) \u226a dnmax exp(\u2212nmin). Now we are ready to bound \u2016J\u201622. With probability at least 1 \u2212 3n1+n2 \u2212 5dnmax exp(\u2212nmin) we have\n\u2016J\u201622 \u2264 p2   \u2225 \u2225 \u2225 \u2225\n\u2225\nd \u2211\nt=1\nwtF t\n\u2225 \u2225 \u2225 \u2225 \u2225 +\n\u221a\n8 log(n1 + n2)\u03bd\nm0\n\n\n2\n\u2264 320p2max{n1n2 log(n1 + n2)/m0, 1}nmax d \u2211\nt=1\nw2t ((d\u2212 t)\u03c322 + \u03c321)\n= 320p2 d \u2211\nt=1\nw2t ((d\u2212 t)w22 + \u03c321)n1n2 log(n1 + n2)nmax/m0\n= 320m0 log(n1 + n2)\n\u2211d t=1 w 2 t ((d\u2212 t)\u03c322 + \u03c321) nmin .\n(37)\nThe first equality uses the fact that m0 < n1n2 log(n1 + n2).\nCombining (26),(27) and (37) yields the second part of inequality (11) in Theorem 3.8."}, {"heading": "F Proof of Lemma E.1", "text": "Proof. The proof is almost the same as the proof of Lemma 12 in [15] with some minor modifications.\nSet F = 44rn1n2m0 (E(\u2016\u03a3R\u2016)) 2. We will show that the probability of the following bad event is small:\nB = { \u2203X \u2208 E(r) such that \u2223 \u2223 \u2223\n\u2223 \u2223\nd \u2211\nt=1\nwt \u2225 \u2225At (X) \u2225 \u2225 2 2 \u2212 p \u2016X\u20162F\n\u2223 \u2223 \u2223 \u2223 \u2223 > p 2 \u2016X\u20162F + F } .\nNote that B contains the complement of the event in Lemma E.1.\nWe use a peeling argument to bound the probability of B. Let \u03bd = \u221a 64 \u2211d t=1 w 2 t log(n1+n2)\nlog(6/5)m0 and\n\u03b1 = 6/5. For l \u2208 N let\nSl =\n{\nX \u2208 E(r) : \u03bd\u03b1l\u22121 \u2264 1 n1n2 \u2016X\u20162F \u2264 \u03bd\u03b1l } .\nThen if event B holds for some X \u2208 E(r), it must be that X belongs to some Sl and \u2223\n\u2223 \u2223 \u2223 \u2223\nd \u2211\nt=1\nwt \u2225 \u2225At (X) \u2225 \u2225 2 2 \u2212 p \u2016X\u20162F\n\u2223 \u2223 \u2223 \u2223 \u2223 > p 2 \u2016X\u20162F + F > 5 12 \u03b1l\u03bdm0 + F . (38)\nFor T > \u03bd consider the set\nE(r, T ) = { X \u2208 E(r) : \u2016X\u20162F \u2264 n1n2T }\nand the event\nBl = { \u2203X \u2208 E(r, \u03b1l\u03bd) such that \u2223 \u2223 \u2223 \u2223\n\u2223\nd \u2211\nt=1\nwt \u2225 \u2225At (X) \u2225 \u2225 2 2 \u2212 p \u2016X\u20162F\n\u2223 \u2223 \u2223 \u2223 \u2223 > 5 12 \u03b1l\u03bdm0 + F } . (39)\nNote that X \u2208 Sl implies that X \u2208 E(r, \u03b1l\u03bd). Then (38) implies that Bl holds and B \u2282 \u222aBl. Thus, it is sufficient to bound the probability of the simpler event Bl and then apply the union bound. Such a bound is given by the following lemma. Its proof is given in Appendix G. Let\nHT = sup X\u2208E(r,T )\n\u2223 \u2223 \u2223 \u2223 \u2223 d \u2211\nt=1\nwt \u2225 \u2225At (X) \u2225 \u2225 2 2 \u2212 p \u2016X\u20162F\n\u2223 \u2223 \u2223 \u2223 \u2223 .\nLemma F.1. Suppose all At\u2019s are fixed uniform sampling ensembles. Then\nP\n(\nHT > 5\n12 \u03b1l\u03bdm0 + F\n) \u2264 exp ( \u2212c5m0T 2 \u2211d\nt=1 w 2 t\n)\n,\nwhere c5 = 1/128.\nThe above lemma implies that P(Bl) \u2264 exp(\u2212c5m0\u03b12l\u03bd2). By a union bound, we have\nP(B) \u2264 \u221e \u2211\nl=1\nP(Bl) \u2264 \u221e \u2211\nl=1\nexp\n(\n\u2212c5m0\u03b12l\u03bd2 \u2211d\nt=1 w 2 t\n) \u2264 \u221e \u2211\nl=1\nexp\n(\n\u2212(2c5m0\u03b1\u03bd2)l \u2211d\nt=1 w 2 t\n)\n,\nwhere the last inequality uses the bound ex \u2265 x. Then, substituting v = \u221a 64 \u2211d t=1 w 2 t log(n1+n2)\nlog(6/5)m0\ninto the above summation we obtain\nP(B) \u2264 2/(n1 + n2). This completes the proof."}, {"heading": "G Proof of Lemma F.1", "text": "Proof. The proof is almost the same as the proof of Lemma 14 in [15] with some minor modifications.\nBy Massart\u2019s concentration inequality (see, e.g., [2], Theorem 14.2), we have\nP\n(\nHT \u2265 E(HT ) + 1\n9\n5\n12 T\n) \u2264 exp ( \u2212c5m0T 2 \u2211d\nt=1 w 2 t\n)\n, (40)\nwhere c5 = 1/128. Next we bound the expectation E(HT ). Using a symmetrization argument we obtain\nE(HT ) \u2264 2E (\nsup X\u2208E(r,T )\n\u2223 \u2223 \u2223 \u2223 \u2223 d \u2211\nt=1\nwt\u03b3 t i\nm0 \u2211\ni=1\n\u2329 Ati, X \u232a2\n\u2223 \u2223 \u2223 \u2223 \u2223 ) ,\nwhere \u03b3ti is a Rademacher variable (independent on both i and t). The assumption \u2016X\u2016\u221e = 1 implies that |\u3008Ati, X\u3009| \u2264 1. Then the contraction inequality yields\nE(HT ) \u2264 8E (\nsup X\u2208E(r,T )\n\u2223 \u2223 \u2223 \u2223 \u2223 d \u2211\nt=1\nwt\u03b3 t i\nm0 \u2211\ni=1\n\u2329 Ati, X \u232a\n\u2223 \u2223 \u2223 \u2223 \u2223 ) = 8E ( sup X\u2208E(r,T ) |\u3008\u03a3R, X\u3009| ) ,\nwhere \u03a3R = \u2211d\nt=1 \u2211m0 i=1 wt\u03b3 t iA t i . Since X \u2208 E(r, T ), we have\n\u2016X\u2016\u2217 \u2264 \u221a r \u2016X\u2016F \u2264 \u221a rn1n2T .\nThen by the trace duality inequality, we obtain\nE(HT ) \u2264 8 \u221a rn1n2TE \u2016\u03a3R\u20162 . Finally using\n1\n9\n5\n12 T + 8\n\u221a rn1n2TE \u2016\u03a3R\u20162 \u2264 1\n9\n5\n12 T +\n8\n9\n5\n12 T + 44rn1n2 (E \u2016\u03a3R\u20162)\n2\ncombined with (40) we complete the proof."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Low-rank matrix factorizations arise in a wide variety of applications \u2013 including recommendation systems, topic models, and source separation, to name just a few. In these and many other applications, it has been widely noted that by incorporating temporal information and allowing for the possibility of time-varying models, significant improvements are possible in practice. However, despite the reported superior empirical performance of these dynamic models over their static counterparts, there is limited theoretical justification for introducing these more complex models. In this paper we aim to address this gap by studying the problem of recovering a dynamically evolving low-rank matrix from incomplete observations. First, we propose the locally weighted matrix smoothing (LOWEMS) framework as one possible approach to dynamic matrix recovery. We then establish error bounds for LOWEMS in both the matrix sensing and matrix completion observation models. Our results quantify the potential benefits of exploiting dynamic constraints both in terms of recovery accuracy and sample complexity. To illustrate these benefits we provide both synthetic and real-world experimental results.", "creator": "LaTeX with hyperref package"}}}