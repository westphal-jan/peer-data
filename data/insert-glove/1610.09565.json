{"id": "1610.09565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "Sequence-to-sequence neural network models for transliteration", "abstract": "Transliteration navier is a key component hooydonk of machine hiroyoshi translation settignano systems and software gnatcatchers internationalization. college This baked paper strudel demonstrates charitha that neural sequence - decembrist to - sukhoi-25 sequence lauitiiti models bellotti obtain refki state sintef of aronimink the art or close pajot to state of 87,000 the art dingbats results internationals on existing kukulkan datasets. In carnal an bisexuals effort dbcp to vuja\u010di\u0107 make machine hcr transliteration accessible, jurgis we open gainor source a new wajima Arabic to English transliteration shoora dataset and kamberi our trained marie-christine models.", "histories": [["v1", "Sat, 29 Oct 2016 19:21:19 GMT  (15kb,D)", "http://arxiv.org/abs/1610.09565v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mihaela rosca", "thomas breuel"], "accepted": false, "id": "1610.09565"}, "pdf": {"name": "1610.09565.pdf", "metadata": {"source": "CRF", "title": "Sequence-to-sequence neural network models for transliteration", "authors": ["Mihaela Rosca", "Thomas Breuel"], "emails": ["mihaelacr@google.com", "tmb@google.com"], "sections": [{"heading": "1 Introduction", "text": "Transliteration\u2013the conversion of proper nouns from one orthographic system to another\u2013is an important task in multilingual text processing, useful in applications like online mapping and as a component of machine translation systems. Transliteration is determined by collection of historical accidents, conventions, and statistical regularities: many language pairs have adopted different rules for transliteration over time and many transliterations depends on the origin of a word. These properties make it desirable to look for high-quality, automated machine learning solutions to the problem.\nA number of model-based methods for machine transliteration have been developed in the past. Such models assume that character sequences in the source orthography correspond to predictable character sequences in the target orthography, possibly depending on context. Some models additionally assume that such correspondences are influenced by phonetic information. Due to the statistical nature of the problem, components of such models frequently involve parameters and statistical modeling\nsuch as hidden Markov models, logistic regression, finite state transducers, and/or conditional random fields (CRFs). Typical examples of such models are (Ammar et al., 2012; Ganesh et al., 2008); transliteration in such a system is based on an alignment step, followed by a CRF model that performs local string rewriting.\nIn many areas, including machine translation, end-to-end deep learning models have become a good alternatives to more traditional statistical approaches. This is our motivation for taking a similar approach to transliteration. Unlike statistical models, such end-to-end systems simply take a character string in the source orthography and are trained directly to produce a character string in the target orthography. The closest approaches to the transliteration methods described in this paper are probably found in (Rao et al., 2015), using a bidirectional LSTM models together with input delays for grapheme to phoneme conversion (which can be viewed as a kind of \u201ctransliteration\u201d from English to IPA) and (Yao and Zweig, 2015), using attentionless sequence-sequence models for the same task.\nThis paper describes the application of two neural-network based sequence-to-sequence models to transliteration that take principled and generalpurpose approaches to alignment and one-to-many or many-to-one correspondences. The first model is based on epsilon insertions and CTC (Graves et al., 2006) alignment, the second model is an attentional sequence-to-sequence model commonly used in end-to-end machine translation (Bahdanau et al., 2014). We report and compare both character (CER) and word error rates (WER) on all problems.\nar X\niv :1\n61 0.\n09 56\n5v 1\n[ cs\n.C L\n] 2\n9 O\nct 2"}, {"heading": "2 Models and datasets", "text": ""}, {"heading": "2.1 Epsilon Insertion", "text": "Epsilon insertion (EI) (Azawi et al., 2013) is a simple technique for allowing sequence-to-sequence models to produce strings of different lengths from an input string. Epsilon insertion replaces the original problem of transliteration with a similar problem in which the source string has been modified by the insertion of epsilons (which we will represent as \u2018 \u2018). Transliteration is then performed by an LSTM (possibly bidirectional and deep), and the output is aligned using CTC.\n\u2022 source string: \u304d\u3087\u3046\u3068 \u2022 source string with epsilons: \u304d \u3087 \u3046 \u3068 \u2022 LSTM output (after training): ki yo u to \u2022 after CTC alignment: ky o to The implementation used for epsilon insertion models is CLSTM1, an open source C++ library. We also open source all our described trained EI.2."}, {"heading": "2.2 Attentional Sequence-to-Sequence Models", "text": "Attentional sequence-to-sequence models (Bahdanau et al., 2014) (Seq2Seq) work by using an encoder RNN to learn representations of the input sequence and a decoder RNN to produce the output sequence from the hidden representations the encoder created. The attention mechanism allows the decoder to focus on different parts of the input for each time step in the output sequence and can be seen as the analog of the alignment mechanism used in traditional statistical translation models. Sequenceto-sequence models do not have the implicit monotonicity assumption that unidirectional CTC models do, hence they are more flexible regarding inputoutput reordering. This is crucial for machine translation, but less important for transliteration where the sound order gets preserved from the source to the target.\nFor sequence-to-sequence models we experimented with GRU and LSTM cells and assessed the impact of using a bidirectional encoder. As recommended in (Sutskever et al., 2014), we feed the input sequence to the encoder in reverse order. For our experiments we used the implementation with an em-\n1https://github.com/tmbdev/clstm 2https://github.com/googlei18n/\ntransliteration\nbedding layer provided by TensorFlow (Abadi et al., 2016) ."}, {"heading": "2.3 Datasets", "text": "We asses the proposed models on Arabic to English (AR-EN), English to Japanese (EN-JA) transliteration and grapheme to phoneme conversion, specifically English to IPA (EN-IPA). The datasets we used are described in Table 2.3. For Arabic to English transliteration, we introduce a new corpus extracted from Wikipedia: firstly, we created a bilingual dataset of full names from titles of Arabic and English articles referring to the same person; secondly, we used it to learn alignments between name parts to create the final dataset. Since no direction specific information was used in data gathering, the data can be used both for English to Arabic and Arabic to English transliteration. Due to the extraction process, the dataset includes names of various origins (eg. Papadopoulos has Greek origin) and some English tokens contain characters specific to other languages such as: \u00df, \u00f8, \u0142.\nFor the transliteration datasets (EN-JA, AR-EN), the English tokens were lowercased and diacritics removed (e\u0300 becomes e, u\u0308 becomes u). The inputs and outputs of our models are unicode codepoints: the model reads one unicode codepoint at a time in the source string and produces unicode codepoints.\nIt should be observed that datasets usually used for transliteration differ substantially in their statistical properties from datasets used in other machine learning research. In particular, transliteration datasets usually represent transliterations only once, regardless of how common the word is in the source language. A second issue is that transliteration datasets used for training contain a large number of exceptional words, whose transliteration probably cannot be learned at all. Finally, there are frequently multiple acceptable transliterations for a source word, but these are not usually represented in the training data; that is, many transliterations counted as errors during training and evaluation may be acceptable. In order to remain comparable to prior work in the area, we did not attempt to address these issues in the existing datasets or the datasets we created for this paper; we will return to the question of how this influences performance in the Discussion.\nTarget"}, {"heading": "3 Experimental results", "text": ""}, {"heading": "3.1 Training and parameters", "text": "For all experiments we used 10% of data for testing, 10% of the remaining data for evaluation and the rest for training. All networks were trained using gradient descent with momentum. We used gradient clipping to avoid exploding RNN gradients (Pascanu et al., 2012). EI models use a batch size of 1, gradient clipping norm of 9 and 3 epsilons. When training EI models we randomly varied the learning rate (10\u22125 to 0.1), momentum rate (0.5 to 0.99) and number of hidden units (100 to 1000). For sequence-to-sequence models we varied the following hyperparameters: learning rate (10\u22125 to 10), momentum rate (0.5 to 0.99), batch size (1 to 50), gradient clipping norm (1 to 10) and number of hidden units (50 to 1000). For both models we trained 1000 networks with different hyperparameter values but a fixed number of layers and chose the one that performed best on the evaluation set and reported performance on the test set. We verified that for each parameter range, optimal performance was reached within the interior of the parameter interval explored."}, {"heading": "3.2 Results", "text": "Our results are described in Tables 2, 3 and 4. Table 3.2 compares our results against models trained on the same datasets. For completeness, we report other transliteration results despite not being directly comparable to our work since they used different datasets. Using statistical phonetic based machine translation (Finch and Sumita, 2008) reports a 31% CER for English to Japanese transliteration. (De-\n3http://www.speech.cs.cmu.edu/cgi-bin/ cmudict\n4https://github.com/eob/ english-japanese-transliteration/\n5https://github.com/googlei18n/ transliteration\nselaers et al., 2009) use deep neural networks for Arabic to English transliteration and report a 22.7% CER, while traditional approaches combined with a single layer perceptron achieved 11.1% on the same task (Freitag et al., 2007)."}, {"heading": "3.3 Error analysis", "text": "Table 6 shows a list of errors made by a Arabic to English transliteration model. The most common mistake both explored models make on this task is to confuse vowels in the output. This is expected, given that Arabic has less vowels than English and that often short vowels are not written. Confusing \u201cp\u201d with \u201cb\u201d is another common mistake, accounted by the lack of a corresponding sound for the English \u201cp\u201d in Arabic."}, {"heading": "4 Discussion", "text": "This paper has demonstrated that end-to-end recurrent neural networks achieve high performance on cross script transliteration on common transliteration tasks: EN-JA, EN-IPA and AR-EN.\nWe have compared epsilon insertion models and attentional sequence-to-sequence models on three benchmarks, and our results show attentional sequence-to-sequence models generally seem to perform better, but not uniformly. For grapheme to phoneme conversion our attention based sequenceto-sequence models perform better than the attentionless sequence-to-sequence models used in (Yao and Zweig, 2015). However, their bidirectional LSTM models which use alignment features outperform our attention based models. The reason can be two fold: the alignment features learned by an alignment specific model help more than the attention implicitly learned by our model, or the simplicity of the LSTM models is advantage against sequence-tosequence models.\nOur results can be extended and potentially improved in a number of ways. A way is by exploring other recurrent network architectures, such as adaptive computation time networks (Graves, 2016) or classifier combination via boosting or other methods (Rao et al., 2015). Another way is by combining the neural network cost with a target language model cost (Chan et al., 2015). Since transliteration involves a combination of orthographic and phonetic\nfeatures, it might be useful to run a separate pronunciation model on the input string and then provide both the grapheme and the phoneme string as input to the transliteration model, mirroring previous non-neural approaches to transliteration (Jansche and Sproat, 2009).\nPerhaps one of the most important areas of improvements is that of training data. Right now, transliteration research (including the described work) performs training and evaluation on plain correspondences between strings in two orthographic systems. Such an approach disregards word frequencies, and treats predictions involving alternative, valid transcriptions as errors. Improvements to both the training datasets and the mechanisms for handling multiple predictions will likely result in significant improvements in model performance and correlate more with human evaluations. In addition to improving datasets, our work also points out the need for understanding the relative importance of character and word error rates in evaluating transliterations, since they appear to vary independently.\nGiven that transliteration is often used as part of machine translation systems, and that such systems themselves are increasingly character based end-toend system, the question arises whether we need separate transliteration models at all. It appears likely that transliteration will remain a distinct submodule of such systems, since internal graphemic and phonetic representations inside transliteration modules are likely quite different from internal semantic representations required for translation. Experimental evidence from humans also supports the notion of separate and distinct processing of proper nouns and other nouns (Adorni et al., 2014).\nIn addition to demonstrating a simple and novel way of constructing efficient transliteration systems, the benchmarks presented in this paper should be a useful baseline for future work. To this end, we open sourced a new Arabic-English transliteration\ndataset."}, {"heading": "5 Acknowledgments", "text": "We would like to thank Andy Staudacher, Lara Scheidegger and Vincent Vanhoucke for their support throughout this work."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Abadi et al.2016] Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Electro-cortical manifestations of common vs. proper name processing during reading", "author": ["Mirella Manfredi", "Alice Mado Proverbio"], "venue": "Brain and language,", "citeRegEx": "Adorni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Adorni et al\\.", "year": 2014}, {"title": "Transliteration by sequence labeling with lattice encodings and reranking", "author": ["Ammar et al.2012] Waleed Ammar", "Chris Dyer", "Noah A Smith"], "venue": "In Proceedings of the 4th Named Entity Workshop,", "citeRegEx": "Ammar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2012}, {"title": "Normalizing historical orthography for ocr historical documents using lstm", "author": ["Azawi et al.2013] Mayce Al Azawi", "Muhammad Zeshan Afzal", "Thomas M Breuel"], "venue": "In Proceedings of the 2nd International Workshop on Historical Document Imaging and Process-", "citeRegEx": "Azawi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Azawi et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Listen, attend and spell", "author": ["Chan et al.2015] William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "A deep learning approach to machine transliteration", "author": ["Sa\u0161a Hasan", "Oliver Bender", "Hermann Ney"], "venue": "In Proceedings of the Fourth Workshop on Statistical Machine Translation,", "citeRegEx": "Deselaers et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deselaers et al\\.", "year": 2009}, {"title": "Phrase-based machine transliteration", "author": ["Finch", "Sumita2008] Andrew Finch", "Eiichiro Sumita"], "venue": "In Proceedings of the Workshop on Technologies and Corpora for Asia-Pacific Speech Translation (TCAST),", "citeRegEx": "Finch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finch et al\\.", "year": 2008}, {"title": "A sequence alignment model based on the averaged perceptron", "author": ["Shahram Khadivi"], "venue": null, "citeRegEx": "Freitag and Khadivi,? \\Q2007\\E", "shortCiteRegEx": "Freitag and Khadivi", "year": 2007}, {"title": "Statistical transliterationalledfor cross language information retrieval using hmm alignment model and crf", "author": ["Ganesh et al.2008] Surya Ganesh", "Sree Harsha", "Prasad Pingali", "Vasudeva Verma"], "venue": "In Proceedings of the 2nd Workshop on Cross Lingual Infor-", "citeRegEx": "Ganesh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganesh et al\\.", "year": 2008}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves et al.2006] Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 23rd international conference on", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2016\\E", "shortCiteRegEx": "Graves.", "year": 2016}, {"title": "Named entity transcription with pair ngram models", "author": ["Jansche", "Sproat2009] Martin Jansche", "Richard Sproat"], "venue": "In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration,", "citeRegEx": "Jansche et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jansche et al\\.", "year": 2009}, {"title": "On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Grapheme-tophoneme conversion using long short-term memory recurrent neural networks", "author": ["Rao et al.2015] Kanishka Rao", "Fuchun Peng", "Hasim Sak", "Fran\u00e7oise Beaufays"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Rao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rao et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Sequence-to-sequence neural net models for grapheme-to-phoneme conversion", "author": ["Yao", "Zweig2015] Kaisheng Yao", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1506.00196", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Typical examples of such models are (Ammar et al., 2012; Ganesh et al., 2008); transliteration in such a system is based on an alignment step, followed by a CRF model that performs local string rewriting.", "startOffset": 36, "endOffset": 77}, {"referenceID": 9, "context": "Typical examples of such models are (Ammar et al., 2012; Ganesh et al., 2008); transliteration in such a system is based on an alignment step, followed by a CRF model that performs local string rewriting.", "startOffset": 36, "endOffset": 77}, {"referenceID": 14, "context": "The closest approaches to the transliteration methods described in this paper are probably found in (Rao et al., 2015), using a bidirectional LSTM models together with input delays for grapheme to phoneme conversion (which can be viewed as a kind of \u201ctransliteration\u201d from English to IPA) and (Yao and Zweig, 2015), using attentionless sequence-sequence models for the same task.", "startOffset": 100, "endOffset": 118}, {"referenceID": 10, "context": "The first model is based on epsilon insertions and CTC (Graves et al., 2006) alignment, the second model is an attentional sequence-to-sequence model commonly used in end-to-end machine translation (Bahdanau et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 4, "context": ", 2006) alignment, the second model is an attentional sequence-to-sequence model commonly used in end-to-end machine translation (Bahdanau et al., 2014).", "startOffset": 129, "endOffset": 152}, {"referenceID": 3, "context": "Epsilon insertion (EI) (Azawi et al., 2013) is a simple technique for allowing sequence-to-sequence models to produce strings of different lengths from an input string.", "startOffset": 23, "endOffset": 43}, {"referenceID": 4, "context": "Attentional sequence-to-sequence models (Bahdanau et al., 2014) (Seq2Seq) work by using an encoder RNN to learn representations of the input sequence and a decoder RNN to produce the output sequence from the hidden representations the encoder created.", "startOffset": 40, "endOffset": 63}, {"referenceID": 15, "context": "As recommended in (Sutskever et al., 2014), we feed the input sequence to the encoder in reverse order.", "startOffset": 18, "endOffset": 42}, {"referenceID": 0, "context": "transliteration bedding layer provided by TensorFlow (Abadi et al., 2016) .", "startOffset": 53, "endOffset": 73}, {"referenceID": 13, "context": "We used gradient clipping to avoid exploding RNN gradients (Pascanu et al., 2012).", "startOffset": 59, "endOffset": 81}, {"referenceID": 11, "context": "A way is by exploring other recurrent network architectures, such as adaptive computation time networks (Graves, 2016) or classifier combination via boosting or other methods (Rao et al.", "startOffset": 104, "endOffset": 118}, {"referenceID": 14, "context": "A way is by exploring other recurrent network architectures, such as adaptive computation time networks (Graves, 2016) or classifier combination via boosting or other methods (Rao et al., 2015).", "startOffset": 175, "endOffset": 193}, {"referenceID": 5, "context": "Another way is by combining the neural network cost with a target language model cost (Chan et al., 2015).", "startOffset": 86, "endOffset": 105}, {"referenceID": 14, "context": "3 (Rao et al., 2015) EN-IPA 26.", "startOffset": 2, "endOffset": 20}, {"referenceID": 1, "context": "Experimental evidence from humans also supports the notion of separate and distinct processing of proper nouns and other nouns (Adorni et al., 2014).", "startOffset": 127, "endOffset": 148}], "year": 2016, "abstractText": "Transliteration is a key component of machine translation systems and software internationalization. This paper demonstrates that neural sequence-to-sequence models obtain state of the art or close to state of the art results on existing datasets. In an effort to make machine transliteration accessible, we open source a new Arabic to English transliteration dataset and our trained models.", "creator": "LaTeX with hyperref package"}}}