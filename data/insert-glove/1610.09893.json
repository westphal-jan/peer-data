{"id": "1610.09893", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks", "abstract": "yakutiya Recurrent zupanc neural adeleke networks (dubrovnik RNNs) have non-american achieved state - tejaji of - disulfides the - stroup art performances deadhorse in coming many natural language digamber processing adam-12 tasks, hattner such ibazeta as voice-over language 31.85 modeling and machine flexo translation. kagwa However, when the kv-1 vocabulary is dalbeattie large, 1890-91 the RNN apx model will become very aze big (e. nerves g. , possibly beyond the pheasant memory yamase capacity riverstone of 1,297 a GPU stumpage device) chuppah and 16,650 its training will become torreya very inefficient. In this 32-25 work, unhealthiest we propose estrogenic a renaudie novel sole technique n\u00e9e to 96.21 tackle stoltzman this challenge. bushs The litterbugs key eliashiv idea 6,039 is 7070 to use rsquo 2 - Component (2C) shared embedding tymnet for nixey word representations. dengler We adaminaby allocate every word in the sucher vocabulary into ranganathan a table, each gandini row of which auraria is associated 90,800 with a pittsylvania vector, and dismember each column tronics associated with wolves another banja vector. Depending on its puccinelli position hamister in bluestein the table, 2337 a word vimto is 23,400 jointly goggin represented men-at-arms by 2,064 two chilo components: a morgans row 3,838 vector 95.17 and strabismus a column badalona vector. bernstadt Since kelis the ajayi words in pyogenic the 34.12 same lhomme row lalganj share thordur the row vector ludo and the nikolaevich words henricks in consejos the same klabin column daryle share palestro the latics column microfiltration vector, we graters only need $ 14:09 2 \\ sqrt {| preventively V |} $ beachwood vectors to chowdhary represent a asokan vocabulary berty of $ | littlewood V | $ coffee unique words, which dendrobatidis are la. far less than shucking the $ | V | $ ninety-six vectors required qichen by xxxvii existing approaches. motton Based pug on poyntz the meares 2 - Component shared embedding, n'doye we design permitir a new betancourt RNN stutterer algorithm and broncs evaluate katarzyna it savina using fuat the language modeling tyibilika task 138.7 on 21.48 several benchmark x15 datasets. wtf The kettering results velopharyngeal show that our dinari algorithm significantly inheriting reduces persichini the biopesticide model sanhedria size collaborationists and batalov speeds gunsberg up plethora the training process, groclin without sacrifice of custom-made accuracy (subkoff it achieves similar, eus\u00e9bio if striding not better, vilfort perplexity as howeth compared spinothalamic to sarvis state - m-47 of - the - listing art aldeen language diana models ). o-bahn Remarkably, ystwyth on sejil the One - Billion - facebook.com Word benchmark worthing Dataset, co-sponsored our saidamir algorithm achieves fruin comparable hides perplexity to excl previous language models, whilst noli reducing galyon the model size zevenbergen by xinxing a dik\u00f6tter factor of stanimirovic 40 - mitofsky 100, and speeding staalsett up the rinuccini training process 14-over by kittie a mormeck factor spc of 2. We name our bedded proposed dividend algorithm \\ emph {pokhrel LightRNN} to innateness reflect officers its p-value very rehoused small model size hyperreals and edizione very weyer high cagiva training krasnyi speed.", "histories": [["v1", "Mon, 31 Oct 2016 12:24:13 GMT  (574kb,D)", "http://arxiv.org/abs/1610.09893v1", "NIPS 2016"]], "COMMENTS": "NIPS 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xiang li", "tao qin", "jian yang", "xiaolin hu", "tie-yan liu"], "accepted": true, "id": "1610.09893"}, "pdf": {"name": "1610.09893.pdf", "metadata": {"source": "CRF", "title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks", "authors": ["Xiang Li", "Tao Qin", "Jian Yang", "Tie-Yan Liu"], "emails": ["implusdream@gmail.com", "csjyang@njust.edu.cn", "tie-yan.liu}@microsoft.com"], "sections": [{"heading": null, "text": "\u221a |V | vectors to represent a vocabulary of |V | unique words,\nwhich are far less than the |V | vectors required by existing approaches. Based on the 2-Component shared embedding, we design a new RNN algorithm and evaluate it using the language modeling task on several benchmark datasets. The results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy (it achieves similar, if not better, perplexity as compared to state-of-the-art language models). Remarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. We name our proposed algorithm LightRNN to reflect its very small model size and very high training speed."}, {"heading": "1 Introduction", "text": "Recently recurrent neural networks (RNNs) have been used in many natural language processing (NLP) tasks, such as language modeling [14], machine translation [23], sentiment analysis [24], and question answering [26]. A popular RNN architecture is long short-term memory (LSTM) [8, 11, 22], which can model long-term dependence and resolve the gradient-vanishing problem by using memory cells and gating functions. With these elements, LSTM RNNs have achieved state-of-the-art performance in several NLP tasks, although almost learning from scratch.\nWhile RNNs are becoming increasingly popular, they have a known limitation: when applied to textual corpora with large vocabularies, the size of the model will become very big. For instance, when using RNNs for language modeling, a word is first mapped from a one-hot vector (whose dimension is equal to the size of the vocabulary) to an embedding vector by an input-embedding matrix. Then, to predict the probability of the next word, the top hidden layer is projected by an output-embedding matrix onto a probability distribution over all the words in the vocabulary. When\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n09 89\n3v 1\n[ cs\n.C L\n] 3\n1 O\nct 2\nthe vocabulary contains tens of millions of unique words, which is very common in Web corpora, the two embedding matrices will contain tens of billions of elements, making the RNN model too big to fit into the memory of GPU devices. Take the ClueWeb dataset [19] as an example, whose vocabulary contains over 10M words. If the embedding vectors are of 1024 dimensions and each dimension is represented by a 32-bit floating point, the size of the input-embedding matrix will be around 40GB. Further considering the output-embedding matrix and those weights between hidden layers, the RNN model will be larger than 80GB, which is far beyond the capability of the best GPU devices on the market [2]. Even if the memory constraint is not a problem, the computational complexity for training such a big model will also be too high to afford. In RNN language models, the most time-consuming operation is to calculate a probability distribution over all the words in the vocabulary, which requires the multiplication of the output-embedding matrix and the hidden state at each position of a sequence. According to simple calculations, we can get that it will take tens of years for the best single GPU today to finish the training of a language model on the ClueWeb dataset. Furthermore, in addition to the challenges during the training phase, even if we can successfully train such a big model, it is almost impossible to host it in mobile devices for efficient inferences.\nTo address the above challenges, in this work, we propose to use 2-Component (2C) shared embedding for word representations in RNNs. We allocate all the words in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector. Then we use two components to represent a word depending on its position in the table: the corresponding row vector and column vector. Since the words in the same row share the row vector and the words in the same column share the column vector, we only need 2 \u221a |V | vectors to represent a vocabulary with |V | unique words, and thus greatly reduce the model size as compared with the vanilla approach that needs |V | unique vectors. In the meanwhile, due to the reduced model size, the training of the RNN model can also significantly speed up. We therefore call our proposed new algorithm (LightRNN), to reflect its very small model size and very high training speed.\nA central technical challenge of our approach is how to appropriately allocate the words into the table. To this end, we propose a bootstrap framework: (1) We first randomly initialize the word allocation and then train the LightRNN model. (2) We fix the trained embedding vectors (corresponding to the row and column vectors in the table), and refine the allocation to minimize the training loss, which is a minimum weight perfect matching problem in graph theory and can be effectively solved. (3) We repeat the second step until certain stopping criterion is met.\nWe evaluate LightRNN using the language modeling task on several benchmark datasets. The experimental results show that LightRNN achieves comparable (if not better) accuracy to state-of-theart language models in terms of perplexity, while reducing the model size by a factor of up to 100 and speeding up the training process by a factor of 2.\nPlease note that it is desirable to have a highly compact model (without accuracy drop). First, it makes it possible to put the RNN model into a GPU or even a mobile device. Second, if the training data is large and one needs to perform distributed data-parallel training, the communication cost for aggregating the models from local workers will be low. In this way, our approach makes previously expensive RNN algorithms very economical and scalable, and therefore has its profound impact on deep learning for NLP tasks."}, {"heading": "2 Related work", "text": "In the literature of deep learning, there have been several works that try to resolve the problem caused by the large vocabulary of the text corpus.\nSome works focus on reducing the computational complexity of the softmax operation on the outputembedding matrix. In [16, 17], a binary tree is used to represent a hierarchical clustering of words in the vocabulary. Each leaf node of the tree is associated with a word, and every word has a unique path from the root to the leaf where it is in. In this way, when calculating the probability of the next word, one can replace the original |V |-way normalization with a sequence of log |V | binary normalizations. In [9, 15], the words in the vocabulary are organized into a tree with two layers: the root node has roughly \u221a |V | intermediate nodes, each of which also has roughly \u221a |V | leaf nodes. Each intermediate node represents a cluster of words, and each leaf node represents a word in the cluster. To calculate the probability of the next word, one first calculates the probability of the cluster of the word and then the conditional probability of the word given its cluster. Besides, methods based\non sampling-based approximations intend to select randomly or heuristically a small subset of the output layer and estimate the gradient only from those samples, such as importance sampling [3] and BlackOut [12]. Although these methods can speed up the training process by means of efficient softmax, they do not reduce the size of the model.\nSome other works focus on reducing the model size. Techniques [6, 21] like differentiated softmax and recurrent projection are employed to reduce the size of the output-embedding matrix. However, they only slightly compress the model, and the number of parameters is still in the same order of the vocabulary size. Character-level convolutional filters are used to shrink the size of the inputembedding matrix in [13]. However, it still suffers from the gigantic output-embedding matrix. Besides, these methods have not addressed the challenge of computational complexity caused by the time-consuming softmax operations.\nAs can be seen from the above introductions, no existing work has simultaneously achieved the significant reduction of both model size and computational complexity. This is exactly the problem that we will address in this paper."}, {"heading": "3 LightRNN", "text": "In this section, we introduce our proposed LightRNN algorithm."}, {"heading": "3.1 RNN Model with 2-Component Shared Embedding", "text": "A key technical innovation in the LightRNN algorithm is its 2-Component shared embedding for word representations. As shown in Figure 1, we allocate all the words in the vocabulary into a table. The i-th row of the table is associated with an embedding vector xri and the j-th column of the table is associated with an embedding vector xcj . Then a word in the i-th row and the j-th column is represented by two components: xri and x c j . By sharing the embedding vector among words in the\nsame row (and also in the same column), for a vocabulary with |V | words, we only need 2 \u221a |V | unique vectors for the input word embedding. It is the same case for the output word embedding.\nFigure 2: LightRNN (left) vs. Conventional RNN (right).\nWith the 2-Component shared embedding, we can construct the LightRNN model by doubling the basic units of a vanilla RNN model, as shown in Figure 2. Let n and m denote the dimension of a row/column input vector and that of a hidden state vector respectively. To compute the probability distribution of wt, we need to use the column vector xct\u22121 \u2208 Rn, the row vector xrt \u2208 Rn, and the hidden state vector hrt\u22121 \u2208 Rm.\nThe column and row vectors are from input-embedding matrices Xc, Xr \u2208 Rn\u00d7 \u221a |V | respectively. Next two hidden state vectors hct\u22121, h r t \u2208 Rm are produced by applying the following recursive\noperations: hct\u22121 = f (Wx c t\u22121 + Uh r t\u22121 + b) h r t = f (Wx r t + Uh c t\u22121 + b). (1)\nIn the above function, W \u2208 Rm\u00d7n, U \u2208 Rm\u00d7m, b \u2208 Rm are parameters of affine transformations, and f is a nonlinear activation function (e.g., the sigmoid function).\nThe probability P (wt) of a word w at position t is determined by its row probability Pr(wt) and column probability Pc(wt):\nPr(wt) = exp(hct\u22121 \u00b7 yrr(w))\u2211 i\u2208Sr exp(h c t\u22121 \u00b7 yri )\nPc(wt) = exp(hrt \u00b7 ycc(w))\u2211 i\u2208Sc exp(h r t \u00b7 yci ) , (2)\nP (wt) = Pr(wt) \u00b7 Pc(wt), (3) where r(w) is the row index of word w, c(w) is its column index, yri \u2208 Rm is the i-th vector of Y r \u2208 Rm\u00d7 \u221a |V |, yci \u2208 Rm is the i-th vector of Y c \u2208 Rm\u00d7 \u221a |V |, and Sr and Sc denote the set of rows and columns of the word table respectively. Note that we do not see the t-th word before predicting it. In Figure 2, given the input column vector xct\u22121 of the (t \u2212 1)-th word, we first infer the row probability Pr(wt) of the t-th word, and then choose the index of the row with the largest probability in Pr(wt) to look up the next input row vector xrt . Similarly, we can then infer the column probability Pc(wt) of the t-th word.\nWe can see that by using Eqn.(3), we effectively reduce the computation of the probability of the next word from a |V |-way normalization (in standard RNN models) to two \u221a |V |-way normalizations. To better understand the reduction of the model size, we compare the key components in a vanilla RNN model and in our proposed LightRNN model by considering an example with embedding dimension n = 1024, hidden unit dimension m = 1024 and vocabulary size |V | = 10M. Suppose we use 32-bit floating point representation for each dimension. The total size of the two embedding matrices X,Y is (m \u00d7 |V | + n \u00d7 |V |) \u00d7 4 = 80GB for the vanilla RNN model and that of the four embedding matrices Xr, Xc, Y r, Y c in LightRNN is 2\u00d7 (m\u00d7 \u221a |V |+n\u00d7 \u221a |V |)\u00d74 \u2248 50MB. It is clear that LightRNN shrinks the model size by a significant factor so that it can be easily fit into the memory of a GPU device or a mobile device.\nThe cell of hidden state h can be implemented by a LSTM [22] or a gated recurrent unit (GRU) [7], and our idea works with any kind of recurrent unit. Please note that in LightRNN, the input and output use different embedding matrices but they share the same word-allocation table."}, {"heading": "3.2 Bootstrap for Word Allocation", "text": "The LightRNN algorithm described in the previous subsection assumes that there exists a word allocation table. It remains as a problem how to appropriately generate this table, i.e., how to allocate the words into appropriate columns and rows. In this subsection, we will discuss on this issue.\nSpecifically, we propose a bootstrap procedure to iteratively refine word allocation based on the learned word embedding in the LightRNN model:\n(1) For cold start, randomly allocate the words into the table.\n(2) Train the input/output embedding vectors in LightRNN based on the given allocation until convergence. Exit if a stopping criterion (e.g., training time, or perplexity for language modeling) is met, otherwise go to the next step.\n(3) Fixing the embedding vectors learned in the previous step, refine the allocation in the table, to minimize the loss function over all the words. Go to Step (2).\nAs can be seen above, the refinement of the word allocation table according to the learned embedding vectors is a key step in the bootstrap procedure. We will provide more details about it, by taking language modeling as an example.\nThe target in language modeling is to minimize the negative log-likelihood of the next word in a sequence, which is equivalent to optimizing the cross-entropy between the target probability distribution and the prediction given by the LightRNN model. Given a context with T words, the\noverall negative log-likelihood can be expressed as follows:\nNLL = T\u2211 t=1 \u2212 logP (wt) = T\u2211 t=1 \u2212 logPr(wt)\u2212 logPc(wt). (4)\nNLL can be expanded with respect to words: NLL = \u2211|V |\nw=1 NLLw, where NLLw is the negative log-likelihood for a specific word w.\nFor ease of deduction, we rewrite NLLw as l(w, r(w), c(w)), where (r(w), c(w)) is the position of word w in the word allocation table. In addition, we use lr(w, r(w)) and lc(w, c(w)) to represent the row component and column component of l(w, r(w), c(w)) (which we call row loss and column loss of word w for ease of reference). The relationship between these quantities is\nNLLw = \u2211 t\u2208Sw \u2212 logP (wt) = l(w, r(w), c(w))\n= \u2211 t\u2208Sw \u2212 logPr(wt) + \u2211 t\u2208Sw \u2212 logPc(wt) = lr(w, r(w)) + lc(w, c(w)), (5)\nwhere Sw is the set of all the positions for the word w in the corpus.\nNow we consider adjusting the allocation table to minimize the loss function NLL. For word w, suppose we plan to move it from the original cell (r(w), c(w)) to another cell (i, j) in the table. Then we can calculate the row loss lr(w, i) if it is moved to row i while its column and the allocation of all the other words remain unchanged. We can also calculate the column loss lc(w, j) in a similar way. Next we define the total loss of this move as l(w, i, j) which is equal to lr(w, i) + lc(w, j) according to Eqn.(5). The total cost of calculating all l(w, i, j) is O(|V |2), by assuming l(w, i, j) = lr(w, i) + lc(w, j), since we only need to calculate the loss of each word allocated in every row and column separately. In fact, all lr(w, i) and lc(w, j) have already been calculated during the forward part of LightRNN training: to predict the next word we need to compute the scores (i.e., in Eqn.(2), hct\u22121 \u00b7 yri and hrt \u00b7 yci for all i) of all the words in the vocabulary for normalization and lr(w, i) is the sum of \u2212 log( exp(hct\u22121\u00b7y r i )\u2211\nk(exp(h c t\u22121\u00b7yrk))\n) over all the appearances of word w\nin the training data. After we calculate l(w, i, j) for all possible w, i, j, we can write the reallocation problem as the following optimization problem:\nmin a \u2211 (w,i,j) l(w, i, j)a(w, i, j) subject to\n\u2211 (i,j) a(w, i, j) = 1 \u2200w \u2208 V, \u2211 w a(w, i, j) = 1 \u2200i \u2208 Sr, j \u2208 Sc, (6)\na(w, i, j) \u2208 {0, 1}, \u2200w \u2208 V, i \u2208 Sr, j \u2208 Sc, where a(w, i, j) = 1 means allocating word w to position (i, j) of the table, and Sr and Sc denote the row set and column set of the table respectively.\nBy defining a weighted bipartite graph G = (V, E) with V = (V, Sr\u00d7Sc), in which the weight of the edge in E connecting a node w \u2208 V and node (i, j) \u2208 Sr \u00d7Sc is l(w, i, j), we will see that the above optimization problem is equivalent to a standard minimum weight perfect matching problem [18] on graph G. This problem has been well studied in the literature, and one of the best practical algorithms for the problem is the minimum cost maximum flow (MCMF) algorithm [1], whose basic idea is shown in Figure 3. In Figure 3(a), we assign each edge connecting a word node w and a position node (i, j) with flow capacity 1 and cost l(w, i, j). The remaining edges starting from source (src) or ending at destination (dst) are all with flow capacity 1 and cost 0. The thick solid lines in Figure 3(a) give an example of the optimal weighted matching solution, while Figure 3(b) illustrates how the allocation gets updated correspondingly. Since the computational complexity of MCMF is O(|V |3), which is still costly for a large vocabulary, we alternatively leverage a linear time (with respect to |E|) 1 2 -approximation algorithm [20] in our experiments whose computational complexity is O(|V |\n2). When the number of tokens in the dataset is far larger than the size of the vocabulary (which is the common case), this complexity can be ignored as compared with the overall complexity of LightRNN training (which is around O(|V |KT ), where K is the number of epochs in the training process and T is the total number of tokens in the training data)."}, {"heading": "4 Experiments", "text": "To test LightRNN, we conducted a set of experiments on the language modeling task."}, {"heading": "4.1 Settings", "text": "We use perplexity (PPL) as the measure to evaluate the performance of an algorithm for language modeling (the lower, the better), defined as PPL = exp(NLLT ), where T is the number of tokens in the test set. We used all the linguistic corpora from 2013 ACL Workshop Morphological Language Datasets (ACLW) [4] and the One-Billion-Word Benchmark Dataset (BillionW) [5] in our experiments. The detailed information of these public datasets is listed in Table 1.\nTable 1: Statistics of the datasets\nDataset #Token Vocabulary Size ACLW-Spanish 56M 152K ACLW-French 57M 137K ACLW-English 20M 60K ACLW-Czech 17M 206K ACLW-German 51M 339K ACLW-Russian 25M 497K BillionW 799M 793K\nFor the ACLW datasets, we kept all the training/validation/test sets exactly the same as those in [4, 13] by using their processed data 1. For the BillionW dataset, since the data2 are unprocessed, we processed the data according to the standard procedure as listed in [5]: We discarded all words with count below 3 and padded the sentence boundary markers <S>,<\\S>. Words outside the vocabulary were mapped to the <UNK> token. Meanwhile, the partition of training/validation/test sets on Bil-\nlionW was the same with public settings in [5] for fair comparisons.\nWe trained LSTM-based LightRNN using stochastic gradient descent with truncated backpropagation through time [10, 25]. The initial learning rate was 1.0 and then decreased by a ratio of 2 if the perplexity did not improve on the validation set after a certain number of mini batches. We clipped the gradients of the parameters such that their norms were bounded by 5.0. We further performed dropout with probability 0.5 [28]. All the training processes were conducted on one single GPU K20 with 5GB memory."}, {"heading": "4.2 Results and Discussions", "text": "For the ACLW datasets, we mainly compared LightRNN with two state-of-the-art LSTM RNN algorithms in [13]: one utilizes hierarchical softmax for word prediction (denoted as HSM), and the other one utilizes hierarchical softmax as well as character-level convolutional filters for input embedding (denoted as C-HSM). We explored several choices of dimensions of shared embedding for LightRNN: 200, 600, and 1000. Note that 200 is exactly the word embedding size of HSM and C-HSM models used in [13]. Since our algorithm significantly reduces the model size, it allows us to use larger dimensions of embedding vectors while still keeping our model size very small. Therefore, we also tried 600 and 1000 in LightRNN, and the results are showed in Table 2. We can see that with larger embedding sizes, LightRNN achieves bet-\n1https://www.dropbox.com/s/m83wwnlz3dw5zhk/large.zip?dl=0 2http://tiny.cc/1billionLM\nTable 3: Runtime comparisons in order to achieve the HSMs\u2019 baseline PPL\nACLW Method Runtime(hours) Reallocation/Training C-HSM[13] 168 \u2013 LightRNN 82 0.19%\nBillionW Method Runtime(hours) Reallocation/Training HSM[6] 168 \u2013 LightRNN 70 2.36%\nTable 4: Results on BillionW dataset\nMethod PPL #param KN[5] 68 2G HSM[6] 85 1.6G B-RNN[12] 68 4.1G LightRNN 66 41M KN + HSM[6] 56 \u2013 KN + B-RNN[12] 47 \u2013 KN + LightRNN 43 \u2013\nter accuracy in terms of perplexity. With 1000-dimensional embedding, it achieves the best result while the total model size is still quite small. Thus, we set 1000 as the shared embedding size while comparing with baselines on all the ACLW datasets in the following experiments.\nEmbedding size PPL #param 200 340 0.9M 600 208 7M\n1000 176 17M\nFor the BillionW dataset, we mainly compared with BlackOut for RNN [12] (B-RNN) which achieves the state-of-the-art result by interpolating with KN (Kneser-Ney) 5-gram. Since the best single model reported in the paper is a 1-layer RNN with 2048-dimenional word embedding, we also used this embedding size for LightRNN. In addition, we compared with the HSM result reported in [6], which used 1024 dimensions for word embedding, but still has 40x more parameters than our model. For further comparisons, we also ensembled LightRNN with the KN 5-gram model. We utilized the KenLM Language Model Toolkit 3 to get the probability distribution from the KN model with the same vocabulary setting.\nally give satisfactory results.\nTable 3 shows the training time of our algorithm in order to achieve the same perplexity as some baselines on the two datasets. As can be seen, LightRNN saves half of the runtime to achieve the same perplexity as C-HSM and HSM. This table also shows the time cost of word table refinement in the whole training process. Obviously, the word reallocation part accounts for very little fraction of the total training time.\n3http://kheafield.com/code/kenlm/\nFigure 5 shows a set of rows in the word allocation table on the BillionW dataset after several rounds of bootstrap. Surprisingly, our approach could automatically discover the semantic and syntactic relationship of words in natural languages. For example, the place names are allocated together in row 832; the expressions about the concept of time are allocated together in row 889; and URLs are allocated together in row 887. This automatically discovered semantic/syntactic relationship may explain why LightRNN, with such a small number of parameters, sometimes outperforms those baselines that assume all the words are independent of each other (i.e., embedding each word as an independent vector)."}, {"heading": "5 Conclusion and future work", "text": "In this work, we have proposed a novel algorithm, LightRNN, for natural language processing tasks. Through the 2-Component shared embedding for word representations, LightRNN achieves high efficiency in terms of both model size and running time, especially for text corpora with large vocabularies.\nThere are many directions to explore in the future. First, we plan to apply LightRNN on even larger corpora, such as the ClueWeb dataset, for which conventional RNN models cannot be fit into a modern GPU. Second, we will apply LightRNN to other NLP tasks such as machine translation and question answering. Third, we will explore k-Component shared embedding (k > 2) and study the role of k in the tradeoff between efficiency and effectiveness. Fourth, we are cleaning our codes and will release them soon through CNTK [27]."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers for their critical and constructive comments and suggestions. This work was partially supported by the National Science Fund of China under Grant Nos. 91420201, 61472187, 61502235, 61233011 and 61373063, the Key Project of Chinese Ministry of Education under Grant No. 313030, the 973 Program No. 2014CB349303, and Program for Changjiang Scholars and Innovative Research Team in University. We also would like to thank Professor Xiaolin Hu from Department of Computer Science and Technology, Tsinghua National Laboratory for Information Science and Technology (TNList) for giving a lot of wonderful advices."}], "references": [{"title": "Optimizing performance of recurrent neural networks on gpus", "author": ["Jeremy Appleyard", "Tomas Kocisky", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1604.01946,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Yoshua Bengio", "Jean-S\u00e9bastien Sen\u00e9cal"], "venue": "In AISTATS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A Botha", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1405.4273,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Strategies for training large vocabulary neural language models", "author": ["Welin Chen", "David Grangier", "Michael Auli"], "venue": "arXiv preprint arXiv:1512.04906,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Classes for fast maximum entropy training", "author": ["Joshua Goodman"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["Shihao Ji", "SVN Vishwanathan", "Nadathur Satish", "Michael J Anderson", "Pradeep Dubey"], "venue": "arXiv preprint arXiv:1511.06909,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Honza \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Combinatorial optimization: algorithms and complexity", "author": ["Christos H Papadimitriou", "Kenneth Steiglitz"], "venue": "Courier Corporation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1982}, {"title": "Building a 70 billion word corpus of english from clueweb", "author": ["Jan Pomik\u00e1lek", "Milos Jakub\u00edcek", "Pavel Rychl\u1ef3"], "venue": "In LREC,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Linear time 1/2-approximation algorithm for maximum weighted matching in general graphs", "author": ["Robert Preis"], "venue": "In STACS", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1402.1128,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In INTERSPEECH,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1990}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["Dong Yu", "Adam Eversole", "Mike Seltzer", "Kaisheng Yao", "Zhiheng Huang", "Brian Guenter", "Oleksii Kuchaiev", "Yu Zhang", "Frank Seide", "Huaming Wang"], "venue": "Technical report,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Recently recurrent neural networks (RNNs) have been used in many natural language processing (NLP) tasks, such as language modeling [14], machine translation [23], sentiment analysis [24], and question answering [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "Recently recurrent neural networks (RNNs) have been used in many natural language processing (NLP) tasks, such as language modeling [14], machine translation [23], sentiment analysis [24], and question answering [26].", "startOffset": 158, "endOffset": 162}, {"referenceID": 22, "context": "Recently recurrent neural networks (RNNs) have been used in many natural language processing (NLP) tasks, such as language modeling [14], machine translation [23], sentiment analysis [24], and question answering [26].", "startOffset": 183, "endOffset": 187}, {"referenceID": 6, "context": "A popular RNN architecture is long short-term memory (LSTM) [8, 11, 22], which can model long-term dependence and resolve the gradient-vanishing problem by using memory cells and gating functions.", "startOffset": 60, "endOffset": 71}, {"referenceID": 9, "context": "A popular RNN architecture is long short-term memory (LSTM) [8, 11, 22], which can model long-term dependence and resolve the gradient-vanishing problem by using memory cells and gating functions.", "startOffset": 60, "endOffset": 71}, {"referenceID": 20, "context": "A popular RNN architecture is long short-term memory (LSTM) [8, 11, 22], which can model long-term dependence and resolve the gradient-vanishing problem by using memory cells and gating functions.", "startOffset": 60, "endOffset": 71}, {"referenceID": 17, "context": "Take the ClueWeb dataset [19] as an example, whose vocabulary contains over 10M words.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "Further considering the output-embedding matrix and those weights between hidden layers, the RNN model will be larger than 80GB, which is far beyond the capability of the best GPU devices on the market [2].", "startOffset": 202, "endOffset": 205}, {"referenceID": 14, "context": "In [16, 17], a binary tree is used to represent a hierarchical clustering of words in the vocabulary.", "startOffset": 3, "endOffset": 11}, {"referenceID": 15, "context": "In [16, 17], a binary tree is used to represent a hierarchical clustering of words in the vocabulary.", "startOffset": 3, "endOffset": 11}, {"referenceID": 7, "context": "In [9, 15], the words in the vocabulary are organized into a tree with two layers: the root node has roughly \u221a |V | intermediate nodes, each of which also has roughly \u221a |V | leaf nodes.", "startOffset": 3, "endOffset": 10}, {"referenceID": 13, "context": "In [9, 15], the words in the vocabulary are organized into a tree with two layers: the root node has roughly \u221a |V | intermediate nodes, each of which also has roughly \u221a |V | leaf nodes.", "startOffset": 3, "endOffset": 10}, {"referenceID": 1, "context": "on sampling-based approximations intend to select randomly or heuristically a small subset of the output layer and estimate the gradient only from those samples, such as importance sampling [3] and BlackOut [12].", "startOffset": 190, "endOffset": 193}, {"referenceID": 10, "context": "on sampling-based approximations intend to select randomly or heuristically a small subset of the output layer and estimate the gradient only from those samples, such as importance sampling [3] and BlackOut [12].", "startOffset": 207, "endOffset": 211}, {"referenceID": 4, "context": "Techniques [6, 21] like differentiated softmax and recurrent projection are employed to reduce the size of the output-embedding matrix.", "startOffset": 11, "endOffset": 18}, {"referenceID": 19, "context": "Techniques [6, 21] like differentiated softmax and recurrent projection are employed to reduce the size of the output-embedding matrix.", "startOffset": 11, "endOffset": 18}, {"referenceID": 11, "context": "Character-level convolutional filters are used to shrink the size of the inputembedding matrix in [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "The cell of hidden state h can be implemented by a LSTM [22] or a gated recurrent unit (GRU) [7], and our idea works with any kind of recurrent unit.", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "The cell of hidden state h can be implemented by a LSTM [22] or a gated recurrent unit (GRU) [7], and our idea works with any kind of recurrent unit.", "startOffset": 93, "endOffset": 96}, {"referenceID": 16, "context": "By defining a weighted bipartite graph G = (V, E) with V = (V, Sr\u00d7Sc), in which the weight of the edge in E connecting a node w \u2208 V and node (i, j) \u2208 Sr \u00d7Sc is l(w, i, j), we will see that the above optimization problem is equivalent to a standard minimum weight perfect matching problem [18] on graph G.", "startOffset": 288, "endOffset": 292}, {"referenceID": 18, "context": "Since the computational complexity of MCMF is O(|V |), which is still costly for a large vocabulary, we alternatively leverage a linear time (with respect to |E|) 1 2 -approximation algorithm [20] in our experiments whose computational complexity is O(|V | ).", "startOffset": 192, "endOffset": 196}, {"referenceID": 2, "context": "We used all the linguistic corpora from 2013 ACL Workshop Morphological Language Datasets (ACLW) [4] and the One-Billion-Word Benchmark Dataset (BillionW) [5] in our experiments.", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "We used all the linguistic corpora from 2013 ACL Workshop Morphological Language Datasets (ACLW) [4] and the One-Billion-Word Benchmark Dataset (BillionW) [5] in our experiments.", "startOffset": 155, "endOffset": 158}, {"referenceID": 2, "context": "BillionW 799M 793K For the ACLW datasets, we kept all the training/validation/test sets exactly the same as those in [4, 13] by using their processed data 1.", "startOffset": 117, "endOffset": 124}, {"referenceID": 11, "context": "BillionW 799M 793K For the ACLW datasets, we kept all the training/validation/test sets exactly the same as those in [4, 13] by using their processed data 1.", "startOffset": 117, "endOffset": 124}, {"referenceID": 3, "context": "For the BillionW dataset, since the data2 are unprocessed, we processed the data according to the standard procedure as listed in [5]: We discarded all words with count below 3 and padded the sentence boundary markers <S>,<\\S>.", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "Meanwhile, the partition of training/validation/test sets on BillionW was the same with public settings in [5] for fair comparisons.", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "We trained LSTM-based LightRNN using stochastic gradient descent with truncated backpropagation through time [10, 25].", "startOffset": 109, "endOffset": 117}, {"referenceID": 23, "context": "We trained LSTM-based LightRNN using stochastic gradient descent with truncated backpropagation through time [10, 25].", "startOffset": 109, "endOffset": 117}, {"referenceID": 25, "context": "5 [28].", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "For the ACLW datasets, we mainly compared LightRNN with two state-of-the-art LSTM RNN algorithms in [13]: one utilizes hierarchical softmax for word prediction (denoted as HSM), and the other one utilizes hierarchical softmax as well as character-level convolutional filters for input embedding (denoted as C-HSM).", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "Note that 200 is exactly the word embedding size of HSM and C-HSM models used in [13].", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "ACLW Method Runtime(hours) Reallocation/Training C-HSM[13] 168 \u2013 LightRNN 82 0.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "19% BillionW Method Runtime(hours) Reallocation/Training HSM[6] 168 \u2013 LightRNN 70 2.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "Method PPL #param KN[5] 68 2G HSM[6] 85 1.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "Method PPL #param KN[5] 68 2G HSM[6] 85 1.", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "6G B-RNN[12] 68 4.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "1G LightRNN 66 41M KN + HSM[6] 56 \u2013 KN + B-RNN[12] 47 \u2013 KN + LightRNN 43 \u2013", "startOffset": 27, "endOffset": 30}, {"referenceID": 10, "context": "1G LightRNN 66 41M KN + HSM[6] 56 \u2013 KN + B-RNN[12] 47 \u2013 KN + LightRNN 43 \u2013", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "For the BillionW dataset, we mainly compared with BlackOut for RNN [12] (B-RNN) which achieves the state-of-the-art result by interpolating with KN (Kneser-Ney) 5-gram.", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "In addition, we compared with the HSM result reported in [6], which used 1024 dimensions for word embedding, but still has 40x more parameters than our model.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "PPL on ACLW test Method Spanish/#P French/#P English/#P Czech/#P German/#P Russian/#P KN[4] 219/\u2013 243/\u2013 291/\u2013 862/\u2013 463/\u2013 390/\u2013", "startOffset": 88, "endOffset": 91}, {"referenceID": 11, "context": "HSM[13] 186/61M 202/56M 236/25M 701/83M 347/137M 353/200M C-HSM[13] 169/48M 190/44M 216/20M 578/64M 305/104M 313/152M LightRNN 157/18M 176/17M 191/17M 558/18M 281/18M 288/19M", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "HSM[13] 186/61M 202/56M 236/25M 701/83M 347/137M 353/200M C-HSM[13] 169/48M 190/44M 216/20M 578/64M 305/104M 313/152M LightRNN 157/18M 176/17M 191/17M 558/18M 281/18M 288/19M", "startOffset": 63, "endOffset": 67}, {"referenceID": 24, "context": "Fourth, we are cleaning our codes and will release them soon through CNTK [27].", "startOffset": 74, "endOffset": 78}], "year": 2016, "abstractText": "Recurrent neural networks (RNNs) have achieved state-of-the-art performances in many natural language processing tasks, such as language modeling and machine translation. However, when the vocabulary is large, the RNN model will become very big (e.g., possibly beyond the memory capacity of a GPU device) and its training will become very inefficient. In this work, we propose a novel technique to tackle this challenge. The key idea is to use 2-Component (2C) shared embedding for word representations. We allocate every word in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector. Depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector. Since the words in the same row share the row vector and the words in the same column share the column vector, we only need 2 \u221a |V | vectors to represent a vocabulary of |V | unique words, which are far less than the |V | vectors required by existing approaches. Based on the 2-Component shared embedding, we design a new RNN algorithm and evaluate it using the language modeling task on several benchmark datasets. The results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy (it achieves similar, if not better, perplexity as compared to state-of-the-art language models). Remarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. We name our proposed algorithm LightRNN to reflect its very small model size and very high training speed.", "creator": "LaTeX with hyperref package"}}}