{"id": "1611.00050", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Exploiting Spatio-Temporal Structure with Recurrent Winner-Take-All Networks", "abstract": "We disparate propose a convolutional nitwits recurrent plumm neural robicheaux network, with Winner - -5000 Take - All nellas dropout for high dimensional unsupervised feature saulet learning sjafrie in multi - dimensional chimurenga time calcaterra series. desmoulins We apply the 5.90 proposedmethod extols for object soltan recognition with shawi temporal sentosa context apatite in kaministiquia videos and zuid-holland obtain tracys better results hiko than comparable methods 75-kilogram in the literature, including 1.2305 the indelibly Deep Predictive qabus Coding Networks otherwordly previously proposed 4x10-kilometer by abeysekera Chalasani and Principe. overlain Our contributions v\u00e4th can offficials be goerges summarized half-completed as crescenzi a frazioni scalable indwelling reinterpretation auslan of unflagging the kolubara Deep Predictive 177.2 Coding cheeseball Networks trained end - to - tomis end korfanty with backpropagation camp through time, atalla an spiky extension of the skinflint previously proposed senatore Winner - cimbri Take - All holway Autoencoders 5,464 to jannuzi sequences cbrne in 4100-meter time, and a new technique onehunga for unchain initializing estephan and salvino regularizing convolutional - frognal recurrent 15-year-olds neural networks.", "histories": [["v1", "Mon, 31 Oct 2016 21:16:46 GMT  (3299kb)", "https://arxiv.org/abs/1611.00050v1", "under review"], ["v2", "Wed, 15 Mar 2017 16:01:43 GMT  (3299kb)", "http://arxiv.org/abs/1611.00050v2", "under review"]], "COMMENTS": "under review", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["eder santana", "matthew emigh", "pablo zegers", "jose c principe"], "accepted": false, "id": "1611.00050"}, "pdf": {"name": "1611.00050.pdf", "metadata": {"source": "CRF", "title": "Exploiting Spatio-Temporal Structure with Recurrent Winner-Take-All Networks", "authors": ["Eder Santana", "Matthew Emigh", "Pablo Zegers", "Jose C Principe"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n00 05\n0v 2\n[ cs\n.L G\n] 1\n5 M\nar 2\n01 7\nExploiting Spatio-Temporal Structure with Recurrent\nWinner-Take-All Networks\nEder Santana1, Matthew Emigh1, Pablo Zegers2, Jose C Principe1, Fellow, IEEE 1Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611 USA\n2 Facultad de Ingenieria y Ciencias Aplicadas, Universidad de los Andes, Chile\nWe propose a convolutional recurrent neural network, with Winner-Take-All dropout for high dimensional unsupervised feature learning in multi-dimensional time series. We apply the proposedmethod for object recognition with temporal context in videos and obtain better results than comparable methods in the literature, including the Deep Predictive Coding Networks previously proposed by Chalasani and Principe.Our contributions can be summarized as a scalable reinterpretation of the Deep Predictive Coding Networks trained end-to-end with backpropagation through time, an extension of the previously proposed Winner-Take-All Autoencoders to sequences in time, and a new technique for initializing and regularizing convolutional-recurrent neural networks.\nIndex Terms\u2014deep learning, unsupervised learning, convolutional recurrent neural networks, winner-take-all, object recognition\nI. INTRODUCTION\nAn elusive problem for both the cognitive and machine learning communities is the precise algorithm by which the human sensory system interprets the continuous stream of sensory inputs as stable perceptions of recognized objects and actions. In engineering, object and action recognition are tackled with supervised learning; on the other hand we have no evidence that the brain uses hard-wired or genetically evolved \"supervisors\" for training recognition networks. We argue that even if our nervous system applies supervised learning, the object classes should emerge in a self organizing way from experience and can be used as supervising labels.\nIt has been suggested that temporal information could be a exploited as a possible source of supervision[1]. In this respect, suppose an observer is moving around and looking at an object in the middle of a room. Given that the input visual stream is continuous and that the object does not move, all the observed images of the object must belong to related perceptions and be represented in similar ways. This smooth, temporally coherent representation is biologically plausible and has been shown to self-organize V1-like Gabor filters when applied to learn image transition representations in video [2].\nA machine learning application of this paradigm is to use temporal coherence as a proxy for learning sensory representations without strong supervision or explicit labels [3][4][5]. One approach is a Bayesian formulation in which we assume the learning system builds an internal model of the world p\u0303(xt; \u03b8) for explaining input streams xt using a system parameterized by \u03b8. Predictive Coding proposes to adapt this model to reduce discrepancies between predictions p\u0303(xt|xt\u22121; \u03b8) and observations p(xt). Chalasani and Principe [6] developed a hierarchical, distributed, generative architecture called Deep Predictive Coding Networks (DPCN) that learns with free energy to build spatio-temporal shift-invariant representations of input video streams. They showed that DPCNs learned\nThis paper was partially funded by University of Florida Graduate Scholarship and ONR N00014-14-1-0542. Corresponding author: E. Santana (DM: https://twitter.com/edersantana).\nfeatures which can be used for classification, with competitive results albeit of being unsupervised. One of the difficulties of this approach is the required inference even in the testing state, which makes it rather slow.\nThis paper is inspired in DPCNs but substitutes the top down inference step in DPCN by a recurrent convolutional encoder-decoder that predicts the next frame of the video, can be trained with backpropagation, and does simples recall in test phase. The paper consists of three main contributions. First, we evolved the self-organizing object recognition in video work of Chalasani and Principe [6][7] by developing a scalable counterpart to the DPCN architecture and algorithms. Second, we build our contributions on top of recent findings in convolutional winner-take-all autoencoders [8] and convolutionalrecurrent neural networks [9][10]. Thus, we extend the results of winner-take-all autoencoders to the time domain. We show results in video predictions, object recognition and action recognition. Third, Luong et. al. [11] showed that RNNs benefit from unsupervised pre-training and multi-task learning. We show that our method can used as a pre-training technique for initializing convolutional RNNs.\nTo present the guiding principles for the proposed architecture, in the next section we overview DPCNs and point its desirable features we would like to preserve and the undesirable features will like to substitute."}, {"heading": "II. DEEP PREDICTIVE CODING NETWORKS", "text": "Assume a multi-dimensional time series (e.g., a video) xt. Chalasani and Principe [6][7] proposed a generative, dynamical, hierarchical model with sparse coefficients st:\nst = Ast\u22121 + vt (1)\nxt = Cst +wt, (2)\nThe sparsity of st is controlled by a nonlinear higher order statistical component But, where B are the component weights and ut is itself L1-constrained to be sparse. This model can be stacked by generatively explaining, at layer l, the ut from the layer below it: u l t = Cs l+1 t +w l+1 t . In practice,\nthis is accomplished by greedy layer-wise training. DPCNs are trained with Expectation-Maximization (EM) using the following energy function:\nEt = \u2016xt \u2212Cst\u20162 + \u03bb\u2016st \u2212Ast\u22121\u20161 + \u03b2\u2016ut\u20161\n+\nK \u2211\nk\n|zt(k) \u00b7 st(k)|, (3)\nzt = \u03b30 1 + exp(\u2212But)\n2 , (4)\nwhere the exponential function is applied to each element of the vector \u2212But, and k represents the vector indices. Thus, given parameters C, A and B, the DPCN algorithm searches for the st that best fits an input xt. The term \u2016st \u2212 Ast\u22121\u2016 is a constraint that forces the solution to be as close as possible to a linear update of the solution for the previous input frame xt\u22121. This is an L1-slowness constraint to force temporal smoothness in the representation. The constraint \u2211K\nk |zt(k)\u00b7xt(k)| forces the solution to be sparse, with sparsity level controlled by the higher level component ut. By doing so it also forces the network to learn time and space invariant features in the variable zt(k). DPCNs can be extended to handle large images by substituting the projection matrices A,B with convolutions and the latent codes st,ut with feature maps, similarly to convolutional sparse coding [12]. Chalasani and Principe [7] used two layer convolutional DPCNs to extract features ut that were then used to train SVMs for classification surpassing several other sparse auto-encoding [13] and deconvolutional [14] techniques in accuracy on Caltech 101, Honda/UCSD faces [15], Celebrity Faces, and Coil-100 datasets.\nUnfortunately, a few drawbacks exist that prevent DPCNs from scaling up and limit their application to other data domains. The DPCN uses expectation maximization to search for the codes st,ut which best maximize the likelihood of input xt. This search is one of the reasons for DPCN\u2019s good results, but it is also a drawback. Even during test time it is necessary to execute computationally expensive expectation steps in order to estimate multidimensional coefficients st,ut for each layer. Here instead we propose a parametrization using deep convolutional auto-encoders to compute the coefficients in a single operation for each layer.\nAlthough successful as a model of the early stages of the visual system, imposing the multiplicative constraint zt(k) limits the types of features learned in the higher layers of the architecture. In fact, previous experiments with DPCNs failed to learn better features with more than two layers, which is not usually the case for other deep learning architectures either supervised [16] or unsupervised [17].\nHere instead, we will focus on learning deep representations from data using stacked rectified linear unity (ReLU) layers. This allows for simpler model building blocks which are still powerful enough for vision [16] and can be possibly extended to other datasets, such as audio [18].\nDPCNs assume that given an input xt, the current latent code st can be estimated given only the previous state st\u22121 and ut. Meanwhile, no backpropagation through time is carried\nout to update the parameters A,B,C. Although this might be interesting for learning in real-time without storing previous states, it limits the model from learning long-term dependencies. Also, without end-to-end connection the model cannot be fully fine-tuned with supervised learning, which has been shown to improve results on auto-encoders [19].\nIn the next section, the model we propose explicitly decouples spatial and temporal representations by learning state transitions in the latent space with RNNs. We keep the smoothness in time by forcing the RNN transitions to stay in a fixed radius open ball around the previous states.\nGiven DPCN\u2019s strengths and weaknesses, we attempt to develop an alternative architecture that is scalable, flexible and differentiable through time, while keeping as much as possible DPCN\u2019s abilities to learn discriminative statistics from spatiotemporal context."}, {"heading": "III. RECURRENT WINNER-TAKE-ALL NETWORK", "text": "In this section we propose an end-to-end differentiable convolutional-recurrent neural network with Winner-Take-All dropout for feature extraction from video. In place of the linear state prediction matrix A, we use Convolutional Recurrent Neural Networks (ConvRNNs) to predict future states. RNNs are particularly appropriate for this framework as they have a long history [20] of successfully modeling dynamical systems. Furthermore, in place of using computationally expensive EM algorithms to compute the sparse states and causes, we use convolutional autoencoders with Winner-Take-All [8] regularization to encode the states in feedforward manner.\nConvolutional-recurrent neural networks [21][22] are RNNs where all the input-to-state and state-to-state transformations are implemented with convolutions. A vanilla-RNN state update can be represented in an equation as\nht = f(Wht\u22121 +Vxt), (5)\nwhere the dynamic state vector ht represents the history of the time series up to time t, and f is a nonlinearity such as the hyperbolic tangent or ReLU. In a convolutional RNN, the dynamic state is a 3-way tensor ht;f,r,c with f channels, r rows and c columns. The hidden-to-hidden transition operation is a convolutional kernel Wo,f,rw,cw with rw < r and cw < c, and o = f output channels. Similarly, we have a convolutional kernel for the input-to-hidden transition Vf,fx,rw,cw , where fx is the number of channels in the input xt,fx,r,c.\nht;f,r,c = f(Wf,f,rw,cw \u22c6ht\u22121;f,r,c +Vf,fx,rw,cw \u22c6 xt;fx,c,r), (6) where \u22c6 denotes the multi-channel convolution operator used in deep learning:\n(W \u22c6 h)fij = \u2211\na,b,c\nWf,a,b,cha,i\u2212b,j\u2212c. (7)\nA schematic representation of convolutional RNN is shown in Figure 1.\nRNNs can be trained in several ways for sequence prediction [20][23]; here we focus on training our ConvRNN to predict the next frame of the sequence. Overfitting in conventional RNNs is avoided using bottleneck layers, where the RNN\nhidden state dimensions are smaller than the input. On the other hand, as discussed in the DPCN section, for image analysis we want to expand the latent space dimensionality and avoid overfitting with sparseness constraints. The advantages of high dimensional representations are formalized by Cover\u2019s theorem.\nIn autoencoders, sparsity can be imposed as a constraint on the objective function [24]. Unfortunately, sparseness as measured by L1 or L0 norms is hard to optimize, requiring elaborate techniques such as proximal gradients (ex. FISTA [25]) or learned approaches (ex. LISTA [26]). Since we want sparseness in the network outputs and not network weights, those techniques would also require optimization during test time, as done in the original formulation of DPCNs. Recent research has shown that simple regularization techniques such as Dropout [27] combined with ReLU activations are enough to learn sparse representations without extra penalties in the cost functions. This represents a paradigm shift from cost function to architectural regularization that provides faster training and testing.\nMakhzani and Frey proposed Winner-Take-All (WTA) Autoencoders [8] which use aggressive Dropout, where all the elements but the strongest of a convolutional map are zeroed out. This forces sparseness in the latent codes and the convolutional decoder to learn robust features. Here we extend convolutional Winner-Take-All autoencoders through time using convolutional RNNs. WTA for a map xf,r,c in the output of convolutional layer can be expressed as in (8). The indices f, r, c represent respectively the number of rows, the number of columns, and the number of channels in the map.\nWTA(xf,r,c) =\n{\nxf,r,c, if xf,r,c = max r,c (xf,r,c) 0, otherwise . (8)\nThus, WTA(xf,r,c) has only one non-zero value for each channel f . To backpropagate through (8) we use \u2207WTA(xf,r,c) = WTA(\u2207xf,r,c). In the present paper, we apply (8) to the output of the convolutional maps of the ConvRNNs after they have been calculated. In other words, the full convolutional map hidden state is used inside the dynamics\nof the ConvRNN, WTA is applied only before they are fed as input to the convolutional decoder. We leave investigations of how WTA would affect the inner dynamics of ConvRNNs for future work.\nWe propose to learn smoothness in time with architectural constraints using a two-stream encoder as shown in Figure 1. This architecture was inspired by the dorsal and ventral streams hypothesis in the human visual cortex [28]. Roughly speaking, the dorsal stream models \"vision for action\" and movements and the ventral stream represents \"vision for perception\". In our proposed architecture one stream is a stateless convolutional encoder-decoder and the other stream has a convolutional RNN encoder, thus a dynamic state. Using Siamese decoders for both streams, we force the stateless encoder and the convolutional RNN to project into the same space\u2014one which can be reconstructed by the shared weights decoder. It is important to stress that from the point of view of spatio-temporal feature extraction with the ConvRNN, the stateless stream works as for regularization. As any other sort of regularization its usefulness can only be totally stated in practice and the practitioner might optionally not use it. Nevertheless, we opted for using the full architecture in all the experiments of this paper. In Appendix A, we show how this proposed architecture enforces spatio-temporal smoothness in the embedded space.\nGiven an input video stream xt, denoting the stateless encoder by E, the decoder D, and the convolutional RNN by R, the cost function for training our architecture is the sum of reconstruction and prediction errors:\nLt = E [ (xt\u22121 \u2212D(E(xt\u22121))) 2 + (xt \u2212D(R(xt\u22121))) 2 ] ,\n(9)\nwhere E denotes the expectation operator. Notice that as depicted in Figure 1, E and R have shared parameters. During training, we observe a few input frames t = [1, 2, ..., T ] and adapt all the parameters using backpropagation through time (BPTT) [29]. Notice that due to BPTT both streams of our architecture are adapted while considering temporal context. Thus, the stateless encoder E will learn richer features than it would if trained on individual frames.\nAs great power brings great responsibility, the main draw-\nback of our proposed architecture is the memory required by BPTT and convolutions. The gradients of convolutions require storing the multi-way tensor output of the convolutions, and BPTT requires storing all the outputs for all time steps. The combination of both methods in a single architecture requires powerful hardware. We limited the length of our input time series between 5 and 10 frames, which is also the length used by the methods with which we compare in the Experiments section. In the next section, we compare our proposed architecture with similar methods proposed in the literature beyond the already discussed DPCN."}, {"heading": "IV. RELATED WORK", "text": "This research is related to DPCNs and a larger family of deep unsupervised neural networks [24][30][17]. The aforementioned Winner-Take-All Autoencoders (WTA-AE) [8] consist of a deep convolutional encoder and a single layer convolutional decoder, which inspired our choice. WTA-AE drops out all the elements of a convolutional channel map but the largest, forcing the whole system to learn robust, sparse features for reconstruction. With the proposed convolutional RNN our method can be seen as a natural extension of WTAAE.\nUnsupervised learning with temporal context was also previously explored by Goroshin et. al. [3] Wang and Gupta [31]. Their approach was based on metric learning of related frames in video, but their approaches were not capable of learning long term dependencies since they assumed only a simple zeromean Gaussian innovation between frames. Also, neither of these approaches can be fine-tuned by BPTT to learn end-toend classifiers in time.\nConvolutional RNNs were proposed simultaneously by several authors [22][21] as an extension of Network-in-Networks [32] architectures where each convolutional layer of a CNN are themselves deep networks. Liang et. al [22] proposed to make each convolutional layer a fixed input RNN. They used that architecture for object recognition in static scenes without exploring temporal context. Xingjian et. al. [21] and Patraucean et. al. [33] on the other hand used temporal context in videos for weather and video forecasts. Their architectures are similar to predictive networks, but they did not address the problem of regularizing the latent space features, nor how\nto train deep architectures\u2014 their models consist only of a single convolutional RNN module for predicting future frames. Furthermore, they do not investigate how to extract interesting features without context, which is the problem addressed by our stateless encoder-decoder stream trained in parallel with the dynamic stream.\nIn parallel to this paper, another follow up on the DPCN approach was published by Bill Lotte et. al [34]. Differently from this paper, their method, called PredNet, focused on frame prediction for video and not sparse feature extraction. Nevertheless, both approaches are complementary and could be combined in future work."}, {"heading": "V. EXPERIMENTS", "text": "To illustrate the capabilities of our proposed architecture we applied it first to two artificial datasets generated by modifying the MNIST and Cifar10 datasets. We used MNIST and Cifar10 as development datasets to understand how hyperparameter choices affect our method; that is, to understand how many filters per layer are necessary, how much temporal context contributes to learning unsupervised features, and how long to take in the unsupervised phase. The full list of hyperparameters is shown on Table I. Note that we fixed the number of channels per convolutional layer be equal in layers to limit the number of hyperparameters. An exception is the number of channels in the decoder (the very last layer) since it has to match the number of channels in the input (i.e. 1 for black and white images and 3 for color images).\nFurthermore, for the modified MNIST dataset, we show our architecture learns, using temporal information, more discriminative features. For the modified Cifar10 dataset, we show the advantage of pre-training convolutional RNNs with our method.\nAfterwards, we applied our best performing architectures to the Coil100 and Honda/UCSD Faces Dataset for a direct comparison with DPCN and other unsupervised learning techniques.\nA. Rotated MNIST Dataset\nWe extended the MNIST dataset by generating videos by rotating each image counter-clockwise. Sample videos are\nshown in Fig. 2. We trained our two-stream convolutional RNN on videos generated with MNIST training dataset. The task was to learn to reconstruct and predict frames as described in 9. We trained the networks with batches of size 100 for 3 epochs (a total of 1800 updates) using the Adam learning rule [35]. We trained a linear Support Vector Machine (SVM) on the features computed by the convolutional RNN R. We collapsed the temporal features into one using addition: z = \u2211\nt R(xt), where the z\u2019s are the input to train the SVM. All the encoder convolutional kernels had f = 64 channels of size c = r = 3. The classification error probability on videos generated with the MNIST test set was 0.94%. An equivalent WTA-AE obtained only 1.02% accuracy.\nWe argue that the possible reasons for the better performance of the proposed method are due to data augmentation and the capacity of the proposed method to use that augmentation to compose a single, less ambiguous interpretation of the data. In Figure 2 we show the 64 filters of 11x11 pixels learned by the decoder D.\nB. Scanned Cifar10 dataset\nThe Cifar10 dataset consists of 50k RGB images of 32x32 pixels for training and an additional 10k images for testing. There is a total of 10 different classes. We converted this dataset to videos by scanning it with 16x16 windows that move 8 pixels at a time as shown in Fig. 2. The 16x16 windows were pre-processed with ZCA. In most of the videos no single 16x16 window completely captures the object to be classified. This forces a classifier to use \"temporal\" context to perform well.\nWe trained the proposed method on this dataset for 10 epochs. Each convolutional map of the encoders E and R had 256 filters of size 5x5 pixels. The decoder had filters of size 7x7. We then fine-tuned the convolutional RNN for classification using supervised learning and obtained a classification rate of 75.6%, while a similar convolutional RNN trained from scratch obtained only 74.1%. Both networks were equally initialized using the Glorot uniform method [36].\nWe did not have success using a single linear SVM on sum-collapsed features for this dataset. Nevertheless, this experiment suggests that even when the proposed pre-training technique in itself is not enough for learning relevant features, it can still be used as an initialization technique for complex recurrent systems.\nUsing what we learned with these two preliminary examples on the modified MNIST and Cifar10 datasets we decided to use the following general guidelines for the following experiments: 1) Color videos are preprocessed by ZCA. 2) Convolutional filters use either 128 or 256 channels of size 3x3 or 5x5 in the encoder and 7x7 in the decoder. 3) Training takes\u2248 1500 updates. For our dataset this was about 10 epochs long, with batch sizes of 16 or 32, depending on GPU memory. 4) Linear SVM classifiers were trained on encoded version of the last frame of a sequence R(xT ). In our experiments T = 5. For longer videos in test time, we classified each frame using by moving the T = 5 window one frame at a time and took the most voted class as the final guess.\nC. Coil-100 Dataset\nThe COIL-100 dataset [37] consists of 100 videos of different objects. Each video is 72 frames long and were generated by placing the object on a turn table and taking a picture every 5\u25e6. The pictures are 128x128 pixels RGB. For our experiments, we rescaled the images to 32x32 pixels and used ZCA pre-processing.\nThe classification protocol proposed in the COIL-100 [37] uses 4 frames per video as labeled samples, the frames corresponding to angles 0\u25e6, 90\u25e6, 180\u25e6 and 270\u25e6. Chalasani and Principe[7] and Mobahi et. al. [38] used the entire dataset for unsupervised pre-training. For this reason, we believe the results in this experiment should be understood with this in mind. Note that the compared methods enforce smoothness in the representation of adjacent frames, and since the test frames are observed in context for feature extraction, information is carried from labeled to unlabeled samples. In other words, this experiment is better described as semi-supervised metric learning than unsupervised learning. Here, we followed that same protocol, using 14 frames per video. Results are reported in Table II. We used encoders with 128 filters of 5x5 pixels and a decoder with 7x7 pixels. The decoder filters are shown in Fig. 3\nD. Honda/UCSD Dataset\nThe Honda/UCSD dataset consists of 59 videos of 20 different people moving their heads in various ways. The training set consists of 20 videos (one for each person), \u223c 300\u22121000 frames each. The test set consists of 39 videos (1- 4 per person), \u223c 300\u2212500 frames each. For each frame of all\nvideos, we detected and cropped the faces using Viola-Jones face detection. Each face was then converted to grayscale, resized to 20x20 pixels, and histogram equalized.\nDuring training, the entire training set was fed into the network, 9 frames at a time, with a batch size of 32. After training was complete, the training set was again fed into the network. For each input frame in the sequence, the feature maps from the convolutional RNN were extracted, and then (5,5) max-pooled with a stride of (3,3). In accordance with the test procedure of Chalasani and Principe [7], a linear SVM was trained using these features and labels indicating the identity of the face. Finally, each video of the test set was fed into the network, one frame at a time, and features were extracted from the RNN in the same way as described above. Each frame was then classified using the linear SVM. Each sequence was assigned a class based on the maximally polled predicted label across each frame in the sequence. Table III summarizes the results for 50 frames, 100 frames, and the full video, comparing with 3 other methods, including the original convolutional implementation of DPCN [7]. The results for the 3 other methods were taken from [7]. The results for our method were perfect for all the tested cases."}, {"heading": "VI. PRELIMINARY RESULTS AND FUTURE WORK", "text": "In all our experiments we investigate single scale feature extraction, i.e., we did not use pooling or strided convolutions. In experiments not discussed in this paper, we explored pooling and unpooling in the proposed architecture. However, this did not improve the results considerably. Nevertheless,\nMakhzani and Frey [8] showed that layer wise training at different scales improved their results. They first trained a convolutional WTA-AE on the raw data, downsampled the features with maxpooling and trained another layer on top the pooled features. Learning at a different scale could be done in two different ways with the proposed architecture. The first way would be by collapsing the temporal features into a single feature map (by addition or picking the last state), downsampling and learning a WTA-AE on top for the resulting features. The second approach would be to downsample every feature in time and train a second two-stream convolutional RNN. Which approach is best remains elusive, but we plan to investigate further in future work.\nIn our experiment sections, we showed how the proposed architecture compares favorably to other methods that leverage temporal context for object recognition. Future experiments should focus on problems such as action recognition, where the best performance cannot be achieved using single frame recognition. In preliminary experiments, we applied the same architecture used with the Coil-100 dataset to the UCF-101 action recognition dataset. A linear SVM applied to unsupervised features was used to obtain a recognition rate of 44% which is slightly better than the baseline of 43.90%. On the other hand, methods based on supervised learning with deep convolutional neural networks using hand-engineered spatial flow features can obtain a recognition rates > 80% [42][43]. Our follow up work should investigate how to learn unsupervised features with architectural constraints similar to those used for calculating spatial flow, e.g. local differences in the pixel space.\nWhen investigating the benefits of unsupervised training for discriminative convolutional RNNs, we only showed results for unsupervised initialization. However, Luong et. al. [11] showed that multitask learning can improve the overall results of the supervised learning task. In other words, while training a conventional RNN for classification, they also added an unsupervised term to the objective function. They argued that such multitask learning regularized the RNN and avoided overfitting. In convolutional architectures the number of latent features (outputs of convolutional layers) is much larger than the number of learned parameters, which already enforces some regularization. However, given that natural images are highly correlated in local neighborhoods, we believe that multitask learning will also benefit convolutional RNNs trained for supervised tasks."}, {"heading": "VII. CONCLUSIONS", "text": "This paper proposes RWTA, a deep convolutional recurrent neural network with Winner-Take-All dropoutfor extracting features from time series. Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10]. We showed that our method outperforms DPCNs and other similar methods\nin contextual object recognition tasks. We also showed that this method can be used as an initialization technique for supervised convolutional RNNs, obtaning better results than Glorot initialization [36]."}, {"heading": "APPENDIX A RWTA LEARNED INVARIANCES", "text": "Assume input stream xt, where t is a countable index, is fed into two modules, a static and a recurrent neural network respectively:\no E t = e(xt) (10) o R t = r(xt,ot\u22121), (11)\nwhere r is a recurrent neural network and the state ot\u22121 gives it context for prediction. These two outputs are fed into a siamese decoders that produces another two outputs\nf E t = d(o E t ) (12) f R t = d(o R t ) (13)\nTraining is done such that the following expression is minimized:\nE = \u2211\nt\n(xt\u22121 \u2212 f E t\u22121)\n2 + \u2211\nt\n(xt \u2212 f R t\u22121) 2 (14)\nThe main effect of the WTA algorithm [?] when applied to\no E t and o R t , is to partition the input space of the corresponding functions into volumes that produce the same output. Hence, if there is a sequence of inputs in the interval {t, . . . , t + k} that is contained inside one of these volumes, then:\no E = oEt = o E t+1 = \u00b7 \u00b7 \u00b7 = o E t+k = e(xt) =\ne(xt+1) = \u00b7 \u00b7 \u00b7 = e(xt+k) (15)\no R = oRt = o R t+1 = \u00b7 \u00b7 \u00b7 = o R t+k = r(xt) =\nr(xt+1) = \u00b7 \u00b7 \u00b7 = r(xt+k) (16)\nwhich implies\nf E = fEt = f E t+1 = . . . = f E t+k = d(o E) (17) f R = fRt = f R t+1 = . . . = f R t+k = d(o R) (18)\nHence, isolating the corresponding section of the objective function, and using the previous equalities, the following\nexpression needs to be minimized:\nEt,t+k = (xt \u2212 f E t ) 2 + (xt+1 \u2212 f R t ) 2\n+(xt+1 \u2212 f E t+1) 2 + (xt+2 \u2212 f R t+1) 2 \u00b7 \u00b7 \u00b7 +(xt+k \u2212 f E t+k) 2 + (xt+k+1 \u2212 f R t+k) 2 (19)\n= (xt \u2212 f E)2 + (xt+1 \u2212 f R)2\n+(xt+1 \u2212 f E)2 + (xt+2 \u2212 f R)2\n\u00b7 \u00b7 \u00b7 +(xt+k \u2212 f E)2 + (xt+k+1 \u2212 f R)2 (20)\n= (xt \u2212 d(o E))2 + (xt+1 \u2212 d(o R))2\n+(xt+1 \u2212 d(o E))2 + (xt+2 \u2212 d(o R))2\n\u00b7 \u00b7 \u00b7 +(xt+k \u2212 d(o E))2 + (xt+k+1 \u2212 d(o R))2(21)\nThus we can do the following considerations. Minimization of the last expression shows that d(oE) and d(oR) will try to be close to all the xt,xt+1, . . . ,xt+k in a square sense. Hence it will produce fE\u22c6 = d(o E) and fR\u22c6 = d(o\nR). Moreover, neglecting the two extreme terms of the last expression shows that it is composed of pairs of sums such as (xt+1 \u2212 d(o R))2 + (xt+1 \u2212 d(o\nE))2. Thus the minimization process is also trying to make d(oR) \u2248 d(oE) which implies o R \u2248 oE . This forces a balance between the stateless encoder and the recurrent neural network. Hence the system will look for solution that consider spatial and temporal invariances. Given that the capacity of the system to partition the space is limited, there is a limited number of oR and oE encodings that the system can produce. Thus the training procedure will focus on finding those volume partitions that can be used to explain the incoming stream in an efficient manner. The combined effect of all these components finally act like an invariance detector."}], "references": [{"title": "Learning to predict: Exposure to temporal sequences facilitates prediction of future events", "author": ["R. Baker", "M. Dexter", "T.E. Hardwicke", "A. Goldstone", "Z. Kourtzi"], "venue": "Vision Research, vol. 99, pp. 124 \u2013 133, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal coherence, natural image sequences, and the visual cortex", "author": ["J. Hurri", "A. Hyv\u00e4rinen"], "venue": "Advances in Neural Information Processing Systems, 2002, pp. 141\u2013148.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Unsupervised learning of spatiotemporally coherent metrics", "author": ["R. Goroshin", "J. Bruna", "J. Tompson", "D. Eigen", "Y. LeCun"], "venue": "The IEEE International Conference on Computer Vision (ICCV), December 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "CoRR, vol. abs/1505.00687, 2015. [Online]. Available: http://arxiv.org/abs/1505.00687", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "CoRR, vol. abs/1505.01596, 2015. [Online]. Available: http://arxiv.org/abs/1505.01596  UNDER REVIEW  8", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep predictive coding networks", "author": ["R. Chalasani", "J. Principe"], "venue": "Workshop at International Conference on Learning Representations (ICLR), 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Context dependent encoding using convolutional dynamic networks", "author": ["R. Chalasani", "J.C. Principe"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 26, no. 9, pp. 1992\u20132004, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Winner-take-all autoencoders", "author": ["A. Makhzani", "B.J. Frey"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2773\u20132781.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["M. Liang", "X. Hu"], "venue": "CVPR, June 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["X. Shi", "Z. Chen", "H. Wang", "D.-Y. Yeung", "W.-K. Wong", "W.-c. Woo"], "venue": "arXiv preprint arXiv:1506.04214, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["M. Luong", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser"], "venue": "CoRR, vol. abs/1511.06114, 2015. [Online]. Available: http://arxiv.org/abs/1511.06114", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised translation-invariant sparse coding", "author": ["J. Yang", "K. Yu", "T. Huang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 3517\u20133524.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.-L. Boureau", "K. Gregor", "M. Mathieu", "Y.L. Cun"], "venue": "Advances in neural information processing systems, 2010, pp. 1090\u20131098.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 2528\u20132535.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Video-based face recognition using probabilistic appearance manifolds", "author": ["K. Lee", "J. Ho", "M. Yang", "D. Kriegman"], "venue": "IEEE Conf. On Computer Vision and Pattern Recognition, vol. 1, pp. 313\u2013320, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 609\u2013616.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3517\u20133521.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["S. Xingjian", "Z. Chen", "H. Wang", "D.-Y. Yeung", "W.-k. Wong", "W.-c. WOO"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 802\u2013810.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["M. Liang", "X. Hu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3367\u20133375.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Training recurrent neural networks", "author": ["I. Sutskever"], "venue": "Ph.D. dissertation, University of Toronto, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen"], "venue": "Nature, vol. 381, no. 6583, pp. 607\u2013609, 1996.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 399\u2013406.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Separate visual pathways for perception and action", "author": ["M.A. Goodale", "A.D. Milner"], "venue": "Trends in neurosciences, vol. 15, no. 1, pp. 20\u2013 25, 1992.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1992}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1990}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "arXiv preprint arXiv:1603.09246, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "2015 IEEE International Conference on Computer Vision (ICCV), Dec 2015, pp. 2794\u20132802.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "CoRR, vol. abs/1312.4400, 2013. [Online]. Available: http://arxiv.org/abs/1312.4400", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Spatio-temporal video autoencoder with differentiable memory", "author": ["V. Patraucean", "A. Handa", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.06309, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep predictive coding networks for video prediction and unsupervised learning", "author": ["W. Lotter", "G. Kreiman", "D. Cox"], "venue": "arXiv preprint arXiv:1605.08104, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "technical report CUCS-005-96, Tech. Rep., 1996.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1996}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 737\u2013744.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "CVPR. IEEE, 2011, pp. 3361\u20133368.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Manifold discriminant analysis", "author": ["R. Wang", "X. Chen"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 429\u2013436.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse approximated nearest points for image set classification", "author": ["Y. Hu", "A.S. Mian", "R. Owens"], "venue": "Computer vision and pattern recognition (CVPR), 2011 IEEE conference on. IEEE, 2011, pp. 121\u2013128.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 568\u2013576.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "arXiv preprint arXiv:1412.0767, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It has been suggested that temporal information could be a exploited as a possible source of supervision[1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "This smooth, temporally coherent representation is biologically plausible and has been shown to self-organize V1-like Gabor filters when applied to learn image transition representations in video [2].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "A machine learning application of this paradigm is to use temporal coherence as a proxy for learning sensory representations without strong supervision or explicit labels [3][4][5].", "startOffset": 171, "endOffset": 174}, {"referenceID": 3, "context": "A machine learning application of this paradigm is to use temporal coherence as a proxy for learning sensory representations without strong supervision or explicit labels [3][4][5].", "startOffset": 174, "endOffset": 177}, {"referenceID": 4, "context": "A machine learning application of this paradigm is to use temporal coherence as a proxy for learning sensory representations without strong supervision or explicit labels [3][4][5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "Chalasani and Principe [6] developed a hierarchical, distributed, generative architecture called Deep Predictive Coding Networks (DPCN) that learns with free energy to build spatio-temporal shift-invariant representations of input video streams.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "First, we evolved the self-organizing object recognition in video work of Chalasani and Principe [6][7] by developing a scalable counterpart to the DPCN architecture and algorithms.", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "First, we evolved the self-organizing object recognition in video work of Chalasani and Principe [6][7] by developing a scalable counterpart to the DPCN architecture and algorithms.", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "Second, we build our contributions on top of recent findings in convolutional winner-take-all autoencoders [8] and convolutionalrecurrent neural networks [9][10].", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "Second, we build our contributions on top of recent findings in convolutional winner-take-all autoencoders [8] and convolutionalrecurrent neural networks [9][10].", "startOffset": 154, "endOffset": 157}, {"referenceID": 9, "context": "Second, we build our contributions on top of recent findings in convolutional winner-take-all autoencoders [8] and convolutionalrecurrent neural networks [9][10].", "startOffset": 157, "endOffset": 161}, {"referenceID": 10, "context": "[11] showed that RNNs benefit from unsupervised pre-training and multi-task learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Chalasani and Principe [6][7] proposed a generative, dynamical, hierarchical model with sparse coefficients st:", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "Chalasani and Principe [6][7] proposed a generative, dynamical, hierarchical model with sparse coefficients st:", "startOffset": 26, "endOffset": 29}, {"referenceID": 11, "context": "DPCNs can be extended to handle large images by substituting the projection matrices A,B with convolutions and the latent codes st,ut with feature maps, similarly to convolutional sparse coding [12].", "startOffset": 194, "endOffset": 198}, {"referenceID": 6, "context": "Chalasani and Principe [7] used two layer convolutional DPCNs to extract features ut that were then used to train SVMs for classification surpassing several other sparse auto-encoding [13] and deconvolutional [14] techniques in accuracy on Caltech 101, Honda/UCSD faces [15], Celebrity Faces, and Coil-100 datasets.", "startOffset": 23, "endOffset": 26}, {"referenceID": 12, "context": "Chalasani and Principe [7] used two layer convolutional DPCNs to extract features ut that were then used to train SVMs for classification surpassing several other sparse auto-encoding [13] and deconvolutional [14] techniques in accuracy on Caltech 101, Honda/UCSD faces [15], Celebrity Faces, and Coil-100 datasets.", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "Chalasani and Principe [7] used two layer convolutional DPCNs to extract features ut that were then used to train SVMs for classification surpassing several other sparse auto-encoding [13] and deconvolutional [14] techniques in accuracy on Caltech 101, Honda/UCSD faces [15], Celebrity Faces, and Coil-100 datasets.", "startOffset": 209, "endOffset": 213}, {"referenceID": 14, "context": "Chalasani and Principe [7] used two layer convolutional DPCNs to extract features ut that were then used to train SVMs for classification surpassing several other sparse auto-encoding [13] and deconvolutional [14] techniques in accuracy on Caltech 101, Honda/UCSD faces [15], Celebrity Faces, and Coil-100 datasets.", "startOffset": 270, "endOffset": 274}, {"referenceID": 15, "context": "In fact, previous experiments with DPCNs failed to learn better features with more than two layers, which is not usually the case for other deep learning architectures either supervised [16] or unsupervised [17].", "startOffset": 186, "endOffset": 190}, {"referenceID": 16, "context": "In fact, previous experiments with DPCNs failed to learn better features with more than two layers, which is not usually the case for other deep learning architectures either supervised [16] or unsupervised [17].", "startOffset": 207, "endOffset": 211}, {"referenceID": 15, "context": "This allows for simpler model building blocks which are still powerful enough for vision [16] and can be possibly extended to other datasets, such as audio [18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "This allows for simpler model building blocks which are still powerful enough for vision [16] and can be possibly extended to other datasets, such as audio [18].", "startOffset": 156, "endOffset": 160}, {"referenceID": 18, "context": "Also, without end-to-end connection the model cannot be fully fine-tuned with supervised learning, which has been shown to improve results on auto-encoders [19].", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": "RNNs are particularly appropriate for this framework as they have a long history [20] of successfully modeling dynamical systems.", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "Furthermore, in place of using computationally expensive EM algorithms to compute the sparse states and causes, we use convolutional autoencoders with Winner-Take-All [8] regularization to encode the states in feedforward manner.", "startOffset": 167, "endOffset": 170}, {"referenceID": 20, "context": "Convolutional-recurrent neural networks [21][22] are RNNs where all the input-to-state and state-to-state transformations are implemented with convolutions.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "Convolutional-recurrent neural networks [21][22] are RNNs where all the input-to-state and state-to-state transformations are implemented with convolutions.", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "RNNs can be trained in several ways for sequence prediction [20][23]; here we focus on training our ConvRNN to predict the next frame of the sequence.", "startOffset": 60, "endOffset": 64}, {"referenceID": 22, "context": "RNNs can be trained in several ways for sequence prediction [20][23]; here we focus on training our ConvRNN to predict the next frame of the sequence.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "In autoencoders, sparsity can be imposed as a constraint on the objective function [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "FISTA [25]) or learned approaches (ex.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "LISTA [26]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "Recent research has shown that simple regularization techniques such as Dropout [27] combined with ReLU activations are enough to learn sparse representations without extra penalties in the cost functions.", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "Makhzani and Frey proposed Winner-Take-All (WTA) Autoencoders [8] which use aggressive Dropout, where all the elements but the strongest of a convolutional map are zeroed out.", "startOffset": 62, "endOffset": 65}, {"referenceID": 27, "context": "This architecture was inspired by the dorsal and ventral streams hypothesis in the human visual cortex [28].", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": ", T ] and adapt all the parameters using backpropagation through time (BPTT) [29].", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "This research is related to DPCNs and a larger family of deep unsupervised neural networks [24][30][17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "This research is related to DPCNs and a larger family of deep unsupervised neural networks [24][30][17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "This research is related to DPCNs and a larger family of deep unsupervised neural networks [24][30][17].", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "The aforementioned Winner-Take-All Autoencoders (WTA-AE) [8] consist of a deep convolutional encoder and a single layer convolutional decoder, which inspired our choice.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "[3] Wang and Gupta [31].", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[3] Wang and Gupta [31].", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Convolutional RNNs were proposed simultaneously by several authors [22][21] as an extension of Network-in-Networks [32] architectures where each convolutional layer of a CNN are themselves deep networks.", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "Convolutional RNNs were proposed simultaneously by several authors [22][21] as an extension of Network-in-Networks [32] architectures where each convolutional layer of a CNN are themselves deep networks.", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "Convolutional RNNs were proposed simultaneously by several authors [22][21] as an extension of Network-in-Networks [32] architectures where each convolutional layer of a CNN are themselves deep networks.", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "al [22] proposed to make each convolutional layer a fixed input RNN.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "[21] and Patraucean et.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] on the other hand used temporal context in videos for weather and video forecasts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "al [34].", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "We trained the networks with batches of size 100 for 3 epochs (a total of 1800 updates) using the Adam learning rule [35].", "startOffset": 117, "endOffset": 121}, {"referenceID": 35, "context": "Both networks were equally initialized using the Glorot uniform method [36].", "startOffset": 71, "endOffset": 75}, {"referenceID": 36, "context": "The COIL-100 dataset [37] consists of 100 videos of different objects.", "startOffset": 21, "endOffset": 25}, {"referenceID": 36, "context": "The classification protocol proposed in the COIL-100 [37] uses 4 frames per video as labeled samples, the frames corresponding to angles 0\u25e6, 90\u25e6, 180\u25e6 and 270\u25e6.", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "Chalasani and Principe[7] and Mobahi et.", "startOffset": 22, "endOffset": 25}, {"referenceID": 37, "context": "[38] used the entire dataset for unsupervised pre-training.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "DPCN no context [7] 79.", "startOffset": 16, "endOffset": 19}, {"referenceID": 38, "context": "45 Stacked ISA + temporal [39] 87 ConvNets + Temporal [38] 92.", "startOffset": 26, "endOffset": 30}, {"referenceID": 37, "context": "45 Stacked ISA + temporal [39] 87 ConvNets + Temporal [38] 92.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "25 DPCN + temporal + top down [7] 98.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "In accordance with the test procedure of Chalasani and Principe [7], a linear SVM was trained using these features and labels indicating the identity of the face.", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "Table III summarizes the results for 50 frames, 100 frames, and the full video, comparing with 3 other methods, including the original convolutional implementation of DPCN [7].", "startOffset": 172, "endOffset": 175}, {"referenceID": 6, "context": "The results for the 3 other methods were taken from [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "Nevertheless, Makhzani and Frey [8] showed that layer wise training at different scales improved their results.", "startOffset": 32, "endOffset": 35}, {"referenceID": 41, "context": "On the other hand, methods based on supervised learning with deep convolutional neural networks using hand-engineered spatial flow features can obtain a recognition rates > 80% [42][43].", "startOffset": 177, "endOffset": 181}, {"referenceID": 42, "context": "On the other hand, methods based on supervised learning with deep convolutional neural networks using hand-engineered spatial flow features can obtain a recognition rates > 80% [42][43].", "startOffset": 181, "endOffset": 185}, {"referenceID": 10, "context": "[11] showed that multitask learning can improve the overall results of the supervised learning task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10].", "startOffset": 176, "endOffset": 179}, {"referenceID": 7, "context": "Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10].", "startOffset": 229, "endOffset": 232}, {"referenceID": 10, "context": "Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10].", "startOffset": 325, "endOffset": 329}, {"referenceID": 8, "context": "Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10].", "startOffset": 370, "endOffset": 373}, {"referenceID": 9, "context": "Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10].", "startOffset": 373, "endOffset": 377}, {"referenceID": 39, "context": "Sequences Lengths MDA [40] SANP [41] CDN [7] Proposed Method", "startOffset": 22, "endOffset": 26}, {"referenceID": 40, "context": "Sequences Lengths MDA [40] SANP [41] CDN [7] Proposed Method", "startOffset": 32, "endOffset": 36}, {"referenceID": 6, "context": "Sequences Lengths MDA [40] SANP [41] CDN [7] Proposed Method", "startOffset": 41, "endOffset": 44}, {"referenceID": 35, "context": "We also showed that this method can be used as an initialization technique for supervised convolutional RNNs, obtaning better results than Glorot initialization [36].", "startOffset": 161, "endOffset": 165}], "year": 2017, "abstractText": "We propose a convolutional recurrent neural network, with Winner-Take-All dropout for high dimensional unsupervised feature learning in multi-dimensional time series. We apply the proposedmethod for object recognition with temporal context in videos and obtain better results than comparable methods in the literature, including the Deep Predictive Coding Networks previously proposed by Chalasani and Principe.Our contributions can be summarized as a scalable reinterpretation of the Deep Predictive Coding Networks trained end-to-end with backpropagation through time, an extension of the previously proposed Winner-Take-All Autoencoders to sequences in time, and a new technique for initializing and regularizing convolutional-recurrent neural networks.", "creator": "LaTeX with hyperref package"}}}