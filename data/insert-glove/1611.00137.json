{"id": "1611.00137", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Embedding Deep Metric for Person Re-identication A Study Against Large Variations", "abstract": "Person flux re - sonika identification skvortsova is kieninger challenging due to the large variations poghosyan of nadezhda pose, illumination, occlusion and camera madelon view. Owing to these harache variations, flit the ansaar pedestrian data borochov is distributed guanling as ury highly - curved emarginate manifolds in blackburn the reser feature sheibani space, despite 108.97 the tapas current convolutional neural obuchowski networks (CNN) ' gel s capability protium of hermeticum feature 72.42 extraction. nuaimi However, the distribution gamsakhurdia is unknown, 541-856-3277 so it egan is shorthand difficult to athirson use tullamarine the russian-ukrainian geodesic esajas distance 44.46 when budlong comparing wiling two rockefellers samples. bergy In 57.1 practice, childe the current volvarina deep embedding iut methods use ruad the Euclidean distance pousseur for ground-control the 500-odd training devas and eagling test. On graet the ichkeria other hand, armi the bg5 manifold learning clearplay methods suggest to use hedwiges the Euclidean distance in rikshospitalet the local osteomyelitis range, rumpled combining with stamps the graphical relationship between samples, cammack for 17-city approximating paratyphoid the toms geodesic polyamorous distance. From 73.44 this hrytsenko point of view, upadhyay selecting suitable positive national/international i. dzhakishev e. intra - wenjiang class) training 3,567 samples rossier within riether a 55-53 local range broncomaniacs is rosellini critical for training the trying CNN embedding, especially daas when sapard the data bereshit has large annmarie intra - roundworms class lethal variations. In 54.2 this nsps paper, remapped we propose matthias a novel moderate erbakan positive sample dont\u00e9 mining method to train halprin robust 5,460 CNN grinstein for boreham person re - euro414 identification, salutation dealing with clunky the cyc problem microscopically of camino large variation. In scalas addition, misumi we 96.93 improve the waterside learning by a metric tartans weight constraint, tausend so beauharnois that castellany the prebendal learned olbrich metric has a kalian better gordeev generalization '99 ability. Experiments fendahl show that marousi these 25th two steinbrueck strategies boride are effective raducioiu in learning robust deep kujat metrics gomhuriya for petkovski person re - identification, hogged and accordingly burgenland our rake deep breakwater model laquidara significantly zambri outperforms the state - tomassoni of - parriott the - kracht art methods odiferous on lafever several cross-cousin benchmarks phyo of zalazar person t28 re - identification. equiano Therefore, thissen the study piia presented in this bele paper may be useful absolom in minwax inspiring tulk new designs kanungu of banti deep models auliya for siphon person sayce re - identification.", "histories": [["v1", "Tue, 1 Nov 2016 06:03:48 GMT  (2735kb,D)", "http://arxiv.org/abs/1611.00137v1", "Published in ECCV2016. arXiv admin note: substantial text overlap witharXiv:1511.07545"]], "COMMENTS": "Published in ECCV2016. arXiv admin note: substantial text overlap witharXiv:1511.07545", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hailin shi", "yang yang", "xiangyu zhu", "shengcai liao", "zhen lei", "weishi zheng", "stan z li"], "accepted": false, "id": "1611.00137"}, "pdf": {"name": "1611.00137.pdf", "metadata": {"source": "CRF", "title": "Embedding Deep Metric for Person Re-identification: A Study Against Large Variations", "authors": ["Hailin Shi", "Yang Yang", "Xiangyu Zhu", "Shengcai Liao", "Zhen Lei", "Weishi Zheng", "Stan Z. Li"], "emails": ["hailin.shi@nlpr.ia.ac.cn", "yang.yang@nlpr.ia.ac.cn", "xiangyu.zhu@nlpr.ia.ac.cn", "scliao@nlpr.ia.ac.cn", "zlei@nlpr.ia.ac.cn", "szli@nlpr.ia.ac.cn"], "sections": [{"heading": null, "text": "Keywords: person re-identification, deep learning, CNN"}, {"heading": "1 Introduction", "text": "Given a set of pedestrian images, person re-identification aims to identify the probe image that generally captured by different cameras. Nowadays, person\n? Corresponding Author.\nar X\niv :1\n61 1.\n00 13\n7v 1\nre-identification becomes increasingly important for surveillance and security system, e.g. replacing manual video screening and other heavy loads. Person reidentification is a challenging task due to large variations of body pose, lighting, view angles, scenarios across time and cameras.\nThe framework of existing methods usually consists of two parts: (1) extracting discriminative features from pedestrian images; (2) computing the distance of samples by feature comparison. There are many works focus on these two aspects. The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37]. The first aspect considers to find features that are robust to challenging factors (lighting, pose etc.) while preserving the identity information. The second aspect comes to the metric learning problem which generally minimizes the intra-class distance while maximizing the inter-class distance.\nMore recently, the deep learning methods gradually gain the popularity in person re-identification. The re-identification methods by deep learning [1,6,17,35] incorporate the two above-mentioned aspects (feature extraction and metric learning) of person re-identification into an integrated framework. The feature extraction and the metric learning are fulfilled respectively by two components in a deep neural network: the CNN part which extracts features from images, and the following metric learning part which compares the features with the metric. The FPNN [17] algorithm introduced a patch matching layer for the CNN part for the first time. Ahmed et al. [1] proposed an improved deep learning architecture (IDLA) with cross-input neighborhood differences and patch summary features. These two methods are both dedicated to improve the CNN architecture. Their purpose is to evaluate the pair similarity early in the CNN stage, so that it could make use of spatial correspondence of feature maps. As for the metric learning part, DML [35] adopted the cosine similarity and Binomial deviance. DeepFeature [6] adopted the Euclidean distance and triplet loss. Some others [1,17] used the logistic loss to directly form a binary classification problem of whether the input image pair belongs to the same identity.\nThe following are our contributions.\n\u2013 For training the CNN, the hard negative mining strategy has been used in [1,27,30]. Considering the large intra-class variations in pedestrian data, we argue that, in person re-identification, the positive training pairs should also be sampled carefully since the pedestrian data is distributed as the manifold that are highly curved in the feature space. As argued in some manifold learning methods [31,29,4], it is effective to use the local Euclidean distance, combining with the graphical relationship between samples, to approximate the geodesic distance. Thus, selecting the moderate positive pairs in the local range is critical for training the network. This is an important issue but has been seldom noticed. In this paper, we propose a new training strategy, named moderate positive mining1, to adaptively search the moderate positives for training. This novel training method significantly improves the identification accuracy.\n1 The source codes is available at http://www.cbsr.ia.ac.cn/users/hailinshi.\n\u2013 In addition, we improve the network by the weight constraint for the metric layers. The weight constraint regularizes the metric learning part and alleviates the over-fitting problem."}, {"heading": "2 Related Work", "text": "Positive Sample Mining. The hard negative mining strategy [30] has been used for face recognition. In person re-identification, IDLA [1] also adopted hard negative mining for the training. By forcing the model to focus on the hard negatives near the decision boundary, hard negative mining improves the training efficiency and the model performance. In this paper, we find that how to select moderate positive samples is also an essential issue for learning person reidentification model. The moderate positives are as critical as hard negatives for training the network, especially when the data has large intra-class variations. However, there are barely any previous attempt in this aspect for learning the deep embedding. In our approach, we propose the novel strategy of moderate positive mining to address the problem. We sample the moderate positives for training, and avoid using the hard ones from extreme intra-class variations of pedestrian data. We empirically find that this strategy effectively improves the identification accuracy (see Section 4.2).\nWeight Constraint for Metric Learning. A commonly used metric by deep learning methods is the Euclidean distance [6,30,27]. However, the Euclidean distance is sensitive to the scale, and is blind to the correlation across dimensions. In practice, we cannot guarantee the CNN-learned features have similar scales and the de-correlation across dimensions. Therefore, using the Mahalanobis distance is a better choice for multivariate metric [22]. In the area of face recognition, DDML [11] implemented the Mahalanobis metric in their network, but without any constraint. Our metric is learned in a similar way and improved by the proposed weight constraint which helps to gain a better generalization ability."}, {"heading": "3 Proposed Method", "text": "In this section, we firstly introduce the moderate positive mining method. Then, we revisit DDML and introduce the weight constraint."}, {"heading": "3.1 Moderate Positive Mining", "text": "Large Intra-class Variations There are many factors lead to the large intraclass variations in pedestrian data, such as illumination, background, misalignment, occlusion, co-occurrence of people, appearance changing, etc. Many of them are specific with pedestrian data. Fig. 1(a) shows some hard positive cases in the data set of CUHK03 [17]. Some of them are even difficult for human to recognize.\nAlthough CNN has a strong ability to extract features, pedestrian data follows the very irregular distribution in the feature space due to the large variations, such as the example of highly-curved manifold illustrated in Fig. 1(b). This is reflected by the fact the state-of-the-art performances on several person re-identification benchmarks are relatively poor comparing with the human face recognition task which is easier due to less intra-class variations.\nModerate Positive Mining Method Considering the distribution in Fig. 1(b) is unknown, it is difficult to apply the geodesic distance for comparing two samples. The usual way is to use the Mahalanobis distance (or the special case Euclidean) [6,30,11] which is a suitable metric in the ideal condition (Fig. 1(c)).\nOn the other hand, the manifold learning methods [31,29,4] suggest to use the Euclidean distance (or heat kernel) in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance. This is a feasible way to minimize the intra-class variance along the manifold for the supervised learning. However, when training the deep CNN with contrastive or triplet loss for embedding, the existing deep embedding methods use the Euclidean distance undiscriminatingly with all the positive samples.\nHere, we argue that selecting positive samples in the local range (pairing by the yellow line in Fig. 1(b)) is critical for training the network; training with the positive samples of large distance (the yellow line with cross) may distort the manifold and harm the manifold learning.\nThe basic idea is that we reduce the intra-class variance while preserving the intrinsic graphical structure of pedestrian data via mining the moderate positive pairs in the local range.\nWe introduce the moderate positive mining method as follows: we select the moderate positive pairs in the range of the same subject at one time. For example, suppose a subject having 6 images, of which 3 from a camera and 3 from another. We can totally match 9 positive pairs from this subject. If we use\nthe easiest positive pair of the nine, the convergence will be very slow; if we use the hardest, the learning will be damaged. Thus, we pick the moderate positive pairs that are between the two extreme cases.\nGiven two sets of pedestrian images I1 and I2 come from two disjoint cameras. Denote I1 \u2208 I1 and Ip2 \u2208 I2 as a positive pair (from the same identity), and I1 \u2208 I1 and In2 \u2208 I2 as a negative pair (from different identities). Denote \u03a8(\u00b7) as the CNN, d(\u00b7, \u00b7) is the Mahalanobis or Euclidean distance. The mining method is described as follows:\nAlgorithm 1: Moderate Positive Mining\nInput: randomly select an anchor sample I1, its positive samples {Ip12 , . . . , I pk 2 }\nand negative samples {In12 , . . . , I nk 2 } to form a mini-batch.\nStep 1 Input the images into the network for obtaining the features, and compute their distances {d(\u03a8(I1),\u03a8(Ip12 )), . . . , d(\u03a8(I1),\u03a8(I pk 2 ))} and {d(\u03a8(I1),\u03a8(In12 )), . . . , d(\u03a8(I1),\u03a8(I nk 2 ))}; Step 2 mine the hardest negative sample\nI\u0302n2 = argminj=1...k{d(\u03a8(I1),\u03a8(I nj 2 ))};\nStep 3 from the positive samples, choose those I\u0303pm2 satisfying\nd(\u03a8(I1),\u03a8(I\u0303 pm 2 )) \u2264 d(\u03a8(I1),\u03a8(I\u0302 n 2 ));\nStep 4 mine the hardest one among these chosen positives as our moderate positive sample\nI\u0302p2 = argmaxI\u0303pm2 {d(\u03a8(I1),\u03a8(I\u0303pm2 ))}.\nIf none of the positives satisfies the condition in Step 3, choose the positive with the smallest distance as the moderate positive sample. Output: The moderate positive sample I\u0302p2.\nFirstly, we randomly select an anchor sample and its positive samples and negative samples (with equal number) to form a mini-batch; then, we mine the hardest negative sample, and choose the positive samples that have smaller distances than the hardest negative; finally, we mine the hardest one among these chosen positives as our moderate positive sample. The reason to do so is that we define the \u201cmoderate positive\u201d adaptively within each subject while their hard negatives are also involved in case the positives are too easy or too hard to be mined.\nAn example is given in Fig. 2. In the experiments, this dynamic mining strategy improves the performance significantly, and shows good stability since all the positives are considered in each subject and the data is augmented by random translation."}, {"heading": "3.2 Weight Constraint for Deep Metric Learning", "text": "Once the CNN extract the features from a pair of images, the metric layers are performed subsequently to calculate the distance, as shown in Fig. 3. The metric learning layer is like the structure proposed in DDML [11], and its learning is improved via a weight constraint.\nRecalling the two sets of pedestrian images I1 and I2 mentioned above, denote X1 and X2 are the corresponding feature sets extracted by the CNN. x1 = \u03a8(I1), x p 2 = \u03a8(I p 2) and x n 2 = \u03a8(I n 2 ) are the corresponding features of the anchor, positive and negative samples.\nRevisiting DDML The Mahalanobis distance is formulated as d(x1,x2) = \u221a (x1 \u2212 x2)TM(x1 \u2212 x2), (1)\nwhere x2 \u2208 {xp2,xn2}, M is a symmetric positive semi-definite matrix. Learning M under the constraint of positive semi-definite is difficult. We make use of its decomposition M = WWT . Learning W is much easier, and WWT is always positive semi-definite. We develop the distance as follows\nd(x1,x2) = \u221a (x1 \u2212 x2)TWWT (x1 \u2212 x2)\n= \u221a (WT (x1 \u2212 x2))T (WT (x1 \u2212 x2))\n= \u2016WT (x1 \u2212 x2)\u20162. (2)\nThe inner product WT (x1\u2212x2) can be implemented by a linear fully-connected (FC) layer in which the weight matrix is defined by WT . The output of the FC layer is calculated by\ny = f(WTx + b), (3)\nwhere b is the bias term. The identity function is used as the activation f(\u00b7) for the linear FC layer. As shown in Fig. 3, the feature vectors x1 and x2 are fed into the subtraction layer. Then, the difference is transformed by the linear FC layer with the weight matrix WT . For the symmetry of the distance, we fix the bias term b to zero throughout the training and test. Finally, the L2 norm is computed as the output distance d(x1,x2). This structure remains equivalent when switching the position of the subtraction layer and the FC layer.\nWeight Constraint The objective is to minimize the intra-class distance and maximize the inter-class distance. The training loss is defined as\nL = d(\u03a8(I1),\u03a8(I p 2)) + [m\u2212 d(\u03a8(I1),\u03a8(In2 ))]+, (4)\nwhere I1, I p 2 and I n 2 are the input images corresponding to the features x1, x p 2 and xn2 , and m is the margin which is set to 2 in the implementation. In each time of the forward propagation, either the first term or the second term of Eq. 4 is computed. Then the loss is obtained by combining the two terms, and we compute the gradient.\nCompared with the Mahalanobis distance, the Euclidean distance has less discriminability but better generalization ability, because it does not take account of the scales and the correlation across dimensions [22]. Here, we impose a constraint that keep the matrix M having large values at the diagonal and small entries elsewhere, so we can achieve a balance between the unconstrained Mahalanobis distance and the Euclidean distance. The constraint is formulated as the Frobenius norm of the difference between WWT and identity matrix I,\nL = d(\u03a8(I1),\u03a8(I p 2)) + [m\u2212 d(\u03a8(I1),\u03a8(In2 ))]+\ns.t. \u2016WWT \u2212 I\u20162F \u2264 C, (5)\nwhere C is a constant. We further combine the constraint into the loss function as a regularization term:\nL\u0302 = L + \u03bb\n2 \u2016WWT \u2212 I\u20162F , (6)\nwhere \u03bb is the relative weight of regularization, L\u0302 is the new loss function. For updating the weight matrix W, the gradient w.r.t W is computed by\n\u2202L\u0302\n\u2202W =\n\u2202L\n\u2202W + \u03bb(WWT \u2212 I)W. (7)\nWhen \u03bb is small, the Mahalanobis distance takes into account the correlations across dimensions. However, it may overfit to the training set, since the metric\nmatrix (i.e. WWT ) is learnt from the training set which is usually small in person re-identification. On the other hand, when \u03bb is large, the matrix WWT becomes close to the identity matrix. In the extreme case, WWT equals to the identity matrix, and the distance reduces to the Euclidean distance. In this situation, the Euclidean distance does not consider the correlation, but may generalize robustly to unseen test sets. So, we incorporate the advantage of the Mahalanobis and Euclidean distances and balance the matching accuracy and generalization performance via the constraint."}, {"heading": "4 Experiments", "text": "Our method is implemented via remodifying the CUDA-Convnet [14] framework. We report the evaluation with the one-shot standard protocol on three common benchmarks of person re-identification, i.e. CUHK03 [17], CUHK01 [16] and VIPeR [9].\nWe begin with the description of CNN architecture we used for extracting features. Then we report the evaluation on the validation set of CUHK03 for analyzing the effects of the moderate positive mining (Section 4.2), the weight constraint (Section 4.3), and the CNN architecture (Section 4.4). Then, we compare our performance with the state-of-the-art methods on CUHK03 and CUHK01 (Section 4.5 and Section 4.6). Finally, we show the proposed method also performs well on the small data-set of VIPeR [9] and gains competitive results (Section 4.7)."}, {"heading": "4.1 CNN architecture", "text": "The CNN is built by 3 branches with the details shown in Fig. 4. The input image is normalized to 128\u00d7 64 RGB. Then, it is split into three 64\u00d7 64 overlapping color patches, each of which is charged by a branch. Each branch is constituted of 3 convolutional layers and 2 pooling layers. No parameter sharing is performed between branches. Then, the 3 branches are concluded by a FC layer with the ReLU activation. Finally, the output feature vector x is computed by another FC layer with linear activation. For the computational stability, the features are normalized before sending to the metric learning layers. The CNN and the metric layers are learned jointly via backward propagation.\nOur network has much lighter weights (0.84M parameters) compared with the previous best methods on CUHK03&01 (IDLA [1], 2.32M) and VIPeR (DeepFeature [6], 26M). The reason that we build the CNN architecture in branches is to learn specific features from the different human body parts of pedestrian image; meanwhile, the morphological information is preserved from each part of human body. DML [35] adopted a similar architecture but with tied weights between branches. In Section 4.4, the experiments show the advantage of our untied architecture."}, {"heading": "4.2 Analysis of Moderate Positive Mining", "text": "CUHK03 contains 1,369 subjects, each of which has around 10 images. The default protocol randomly selects 1,169 subjects for training, 100 for validation, and 100 for test. We pre-train the CNN with a softmax classification on the training set as the baseline. The outputs of softmax correspond to the identities.\nTo demonstrate the advantage of moderate positive mining, we compare the performances on the validation set with and without the moderate positive mining. The cumulative matching characteristic (CMC) curves and the rank-1 identification rates are shown in Fig. 5(a). We can find that the collaboration of moderate positive mining and hard negative mining achieves the best result (red line). The absence of moderate positive mining leads to a significant derogation of performance (blue). This reflects that the manifold is badly learned if all the positive pairs are used undiscriminatingly.\nIf both of the two mining methods are not used (magenta), the network gives very low identification rate at low ranks, even worse than the baseline (black). This indicates that moderate positive mining and hard negative mining are both crucial for training.\nThe CMC curves of the 3 trained networks tend to be saturated after the rank exceeds 20, whereas the baseline network remains at a relatively low identification rate. This indicates that the training with the metric layers is the basic contributor of the improvement.\nThe training of network converges well as the loss value descending with respect to the iterations (shown in Fig. 5(b)). Some positives, which are mined by moderate positive mining during training, are shown in Fig. 5(c). These positives are with moderate extent of difficulty compared with those hard ones in Fig. 1(a)."}, {"heading": "4.3 Analysis of Weight Constraint", "text": "We inspect the metric matrices learned with different relative weights (\u03bb) of the regularization. In Fig. 6(a), we show the spectrums of the matrix M. We also show the corresponding rank-1 identification rates in Fig. 6(b).\nWhen \u03bb = 102, the singular values are almost constant at 1, which means the metric layers almost give the Euclidean distance. This leads to the low variance and high bias. As \u03bb increases, the matrix has varying singular values across\ndimensions. This implies that the learned metric suits the training data well, but is more likely to have over-fitting. Therefore, a moderate value of \u03bb gives a trade-off between the variance and bias, which is an appropriate choice for good performance (Fig. 6(b))."}, {"heading": "4.4 Analysis of Untied Branches", "text": "We show the learned filters of untied branches in Fig. 7(a). The network has learned remarkable color representations, which is coherent with the results of IDLA [1]. Since we apply untied weights between branches, each branch learns\ndifferent filters from their own part. As shown in Fig. 7(a) where each row demonstrates a filter set from one branch, we can find each branch has its own emphasis in color. For example, the middle branch inclines to violet and blue, whereas the bottom branch has learned filters of obviously lighter colors than the other two. The reason is that pedestrian images have regular appearance of human body. Each part has its own color distribution. Therefore, the branches learn the part-specific filters, the morphological information is taken into account for the features.\nWe compare the performances with and without tied weights between branches in Fig. 7(b). We augment the filter number in the tied-branches network so to make roughly equal parameter number with the untied-branch. The untiedbranch network gains a better performance than that of tied branches. It reflects that, when the network has a certain complexity, the neural structure (i.e. tied vs untied) becomes very important. How to organize the network structure is a critical issue for good performance."}, {"heading": "4.5 Performance on CUHK03", "text": "We adopt a random translation for the training data augmentation. The images are randomly cropped (0-5 pixels) in horizon and vertical, and stretched to recover the size. According to the validation results (Section 4.3), we set the parameter \u03bb = 10\u22122 in all the following experiments. The moderate positive mining and hard negative mining are employed.\nCUHK03 has 2 versions, one has manually labeled images, and the other has detected images. We evaluate our method on the test set of both versions. We compare our performance with the traditional methods and deep learning\nmethods. The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40]. The deep learning methods include FPNN [17] and IDLA [1]. IDLA and LOMOXQDA gained the previously best performance on CUHK03. The CMC curves and the rank-1 identification rates are shown in Fig. 8. Our method achieves better performance than the previous state-of-the-art methods on not only the labeled version but also the detected version. This indicates that our method achieves good robustness to the misalignment of detection."}, {"heading": "4.6 Performance on CUHK01", "text": "The CUHK01 data set contains 971 subjects, each of which has 4 images under 2 camera views. According to the protocol in [16], the data set is divided into a training set of 871 subjects and a test set of 100. We train the network on CUHK03, and further fine-tune it on CUHK01, as the same setting with the state-of-the-art method IDLA [1]. We compare our approach with the previously mentioned methods. The CMC curves and rank-1 identification rates are shown in Fig. 9(a). Our approach gains the best result (the red line) with 69% rank-1 identification rate.\nBesides, to inspect the limitation of the data set CUHK01, we involve the recently released Market1501 [42] into the training. As the training data increases, our network gives a better performance (the red dash line marked as \u201cOurs *\u201d) with 87% rank-1 identification rate. We show certain failed cases in Fig. 10. In each block, we give the true gallery, probe and false positive image from left to right. We find that most failed cases come from the dark color images or the negative pairs with significant color correspondence. This phenomenon is in\nline with the fact [1] that the learned filters in network mainly focus on image colors (as shown in Fig. 7(a)). The re-identification problem becomes extremely difficult when the true positive pairs have inconsistent colors in view while the negative pairs have similar colors (due to the lighting, camera setting etc.)."}, {"heading": "4.7 Performance on VIPeR", "text": "The VIPeR [9] data set includes 632 subjects, each of which has 2 images from two different cameras. Although VIPeR is a small data set which is not suitable for training CNN, we are still interested in the performance on this challenging task. The data set is randomly split into two subsets, each has non-overlapping subjects of the same size. The two subsets are for either training or test. We fine-tune the network on the 316-person training set and test it on the test\nset. We also adopt a random translation for training data augmentation. The results are shown in Fig. 9(b). We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18]. Our approach achieves the identification rate of 40.91% at rank 1, which is the best result on VIPeR compared with the existing deep learning methods. Note that the highest rank-1 identification rate (43.39%) is obtained by a combination of two methods (mFilter+LADF) [18]. The identification rate by DeepFeature [6] is close to ours at rank 1, but much lower at higher ranks."}, {"heading": "5 Conclusion", "text": "The large variations of pedestrian data is a challenging point for the person reidentification methods. Although CNN has a strong ability to extract features, pedestrian data follows the very irregular distribution in the feature space due to the large variations. In order to cope with the problem and train the robust deep embedding, the positive training samples should be selected deliberately. In this paper, we propose a novel moderate positive mining method to embed robust deep metric for person re-identification. We find that mining the moderate positive samples is crucial for training deep networks, especially when it comes to the difficult data with large intra-class variations (e.g. pedestrian). The moderate positive mining method dynamically select the suitable positive pairs for learning robust embedding adaptive to the data manifold. Moreover, we propose the weight constraint for gaining the good robustness to the over-fitting problem in person re-identification.\nDue to these improvements, our method achieves state-of-the-art performances on CUHK03 and CUHK01, and competitive results on VIPeR. By mining the moderate positive samples for the training, we can reduce the intra-class variance while preserving the intrinsic graphical structure of pedestrian data; the metric weight constraint helps to improve the generalization ability of the network, especially when the most parameters are in the metric layers."}, {"heading": "6 Acknowledgement", "text": "This work was supported by the National Key Research and Development Plan (Grant No.2016YFC0801003), the Chinese National Natural Science Foundation Projects #61473291, #61572501, #61502491, #61572536, NVIDIA GPU donation program and AuthenMetric R&D Funds."}], "references": [{"title": "An improved deep learning architecture for person re-identification", "author": ["E. Ahmed", "M. Jones", "T.K. Marks"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. IEEE", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple-shot human reidentification by mean riemannian covariance grid", "author": ["S. Bak", "E. Corvee", "F. Bremond", "M. Thonnat"], "venue": "Advanced Video and SignalBased Surveillance (AVSS), 2011 8th IEEE International Conference on. pp. 179\u2013 184. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple-shot person reidentification by chromatic and epitomic analyses", "author": ["L. Bazzani", "M. Cristani", "A. Perina", "V. Murino"], "venue": "Pattern Recognition Letters 33(7), 898\u2013903", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural computation 15(6), 1373\u20131396", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proceedings of the 24th international conference on Machine learning. pp. 209\u2013216. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep feature learning with relative distance comparison for person re-identification", "author": ["S. Ding", "L. Lin", "G. Wang", "H. Chao"], "venue": "Pattern Recognition", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Person reidentification by symmetry-driven accumulation of local features", "author": ["M. Farenzena", "L. Bazzani", "A. Perina", "V. Murino", "M. Cristani"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. pp. 2360\u2013 2367. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Person reidentification using spatiotemporal appearance", "author": ["N. Gheissari", "T.B. Sebastian", "R. Hartley"], "venue": "Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on. vol. 2, pp. 1528\u20131535. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Evaluating appearance models for recognition, reacquisition, and tracking", "author": ["D. Gray", "S. Brennan", "H. Tao"], "venue": "Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS). vol. 3. Citeseer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. pp. 498\u2013505. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative deep metric learning for face verification in the wild", "author": ["J. Hu", "J. Lu", "Y.P. Tan"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1875\u20131882. IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint learning for attribute-consistent person re-identification", "author": ["S. Khamis", "C.H. Kuo", "V.K. Singh", "V.D. Shet", "L.S. Davis"], "venue": "Computer Vision-ECCV 2014 Workshops. pp. 134\u2013146. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Koestinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 2288\u20132295. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems. pp. 1097\u20131105", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Locally aligned feature transforms across views", "author": ["W. Li", "X. Wang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. pp. 3594\u2013 3601. IEEE", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Human reidentification with transferred metric learning", "author": ["W. Li", "R. Zhao", "X. Wang"], "venue": "ACCV (1). pp. 31\u201344", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Deepreid: Deep filter pairing neural network for person re-identification", "author": ["W. Li", "R. Zhao", "T. Xiao", "X. Wang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 152\u2013159. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning locallyadaptive decision functions for person verification", "author": ["Z. Li", "S. Chang", "F. Liang", "T.S. Huang", "L. Cao", "J.R. Smith"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. pp. 3610\u20133617. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Person re-identification by local maximal occurrence representation and metric learning", "author": ["S. Liao", "Y. Hu", "X. Zhu", "S.Z. Li"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2197\u20132206", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Person re-identification: What features are important? In: Computer Vision\u2013ECCV 2012", "author": ["C. Liu", "S. Gong", "C.C. Loy", "X. Lin"], "venue": "Workshops and Demonstrations. pp. 391\u2013401. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Bicov: a novel image representation for person reidentification and face verification", "author": ["B. Ma", "Y. Su", "F. Jurie"], "venue": "British Machive Vision Conference. pp. 11\u2013pages", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Multivariate statistical methods: a primer", "author": ["B.F. Manly"], "venue": "CRC Press", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Saliency weighted features for person reidentification", "author": ["N. Martinel", "C. Micheloni", "G.L. Foresti"], "venue": "Computer Vision-ECCV 2014 Workshops. pp. 191\u2013208. Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Metric learning to rank", "author": ["B. McFee", "G.R. Lanckriet"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10). pp. 775\u2013782", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Pcca: A new approach for distance learning from sparse pairwise constraints", "author": ["A. Mignon", "F. Jurie"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 2666\u20132672. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to rank in person reidentification with metric ensembles", "author": ["S. Paisitkriangkrai", "C. Shen", "A. van den Hengel"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1846\u20131855", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep face recognition", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "Proceedings of the British Machine Vision", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Local fisher discriminant analysis for pedestrian re-identification", "author": ["S. Pedagadi", "J. Orwell", "S. Velastin", "B. Boghossian"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. pp. 3318\u20133325. IEEE", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science 290(5500), 2323\u20132326", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": "science 290(5500), 2319\u20132323", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "Advances in neural information processing systems. pp. 1473\u20131480", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Person re-identification using kernelbased metric learning methods", "author": ["F. Xiong", "M. Gou", "O. Camps", "M. Sznaier"], "venue": "Computer Vision\u2013ECCV 2014, pp. 1\u201316. Springer", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Salient color names for person re-identification", "author": ["Y. Yang", "J. Yang", "J. Yan", "S. Liao", "D. Yi", "S.Z. Li"], "venue": "Computer Vision\u2013ECCV 2014, pp. 536\u2013551. Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep metric learning for practical person re-identification", "author": ["D. Yi", "Z. Lei", "S.Z. Li"], "venue": "arXiv preprint arXiv:1407.4979", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Prism: Person re-identification via structured matching", "author": ["Z. Zhang", "V. Saligrama"], "venue": "arxiv preprint. IEEE Transaction on Pattern Analysis and Machine Intelligence", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "A novel visual word co-occurrence model for person re-identification", "author": ["Z. Zhang", "Y. Chen", "V. Saligrama"], "venue": "Computer Vision-ECCV 2014 Workshops. pp. 122\u2013133. Springer", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Group membership prediction", "author": ["Z. Zhang", "Y. Chen", "V. Saligrama"], "venue": "Computer Vision (ICCV), 2015 IEEE International Conference on. IEEE", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Person re-identification by salience matching", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "Computer Vision (ICCV), 2013 IEEE International Conference on. pp. 2528\u20132535. IEEE", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised salience learning for person reidentification", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. pp. 3586\u20133593. IEEE", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning mid-level filters for person reidentification", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 144\u2013151. IEEE", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable person reidentification: A benchmark", "author": ["L. Zheng", "L. Shen", "L. Tian", "S. Wang", "J. Wang", "Q. Tian"], "venue": "Computer Vision, IEEE International Conference on", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Person re-identification by probabilistic relative distance comparison", "author": ["W.S. Zheng", "S. Gong", "T. Xiang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. pp. 649\u2013656. IEEE", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 33, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 73, "endOffset": 80}, {"referenceID": 40, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 73, "endOffset": 80}, {"referenceID": 25, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 12, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 14, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 17, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 22, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 38, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 35, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 37, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 11, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 155, "endOffset": 168}, {"referenceID": 18, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 155, "endOffset": 168}, {"referenceID": 32, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 155, "endOffset": 168}, {"referenceID": 36, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 155, "endOffset": 168}, {"referenceID": 0, "context": "The re-identification methods by deep learning [1,6,17,35] incorporate the two above-mentioned aspects (feature extraction and metric learning) of person re-identification into an integrated framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 5, "context": "The re-identification methods by deep learning [1,6,17,35] incorporate the two above-mentioned aspects (feature extraction and metric learning) of person re-identification into an integrated framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 16, "context": "The re-identification methods by deep learning [1,6,17,35] incorporate the two above-mentioned aspects (feature extraction and metric learning) of person re-identification into an integrated framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 34, "context": "The re-identification methods by deep learning [1,6,17,35] incorporate the two above-mentioned aspects (feature extraction and metric learning) of person re-identification into an integrated framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 16, "context": "The FPNN [17] algorithm introduced a patch matching layer for the CNN part for the first time.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "[1] proposed an improved deep learning architecture (IDLA) with cross-input neighborhood differences and patch summary features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "As for the metric learning part, DML [35] adopted the cosine similarity and Binomial deviance.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "DeepFeature [6] adopted the Euclidean distance and triplet loss.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "Some others [1,17] used the logistic loss to directly form a binary classification problem of whether the input image pair belongs to the same identity.", "startOffset": 12, "endOffset": 18}, {"referenceID": 16, "context": "Some others [1,17] used the logistic loss to directly form a binary classification problem of whether the input image pair belongs to the same identity.", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "\u2013 For training the CNN, the hard negative mining strategy has been used in [1,27,30].", "startOffset": 75, "endOffset": 84}, {"referenceID": 26, "context": "\u2013 For training the CNN, the hard negative mining strategy has been used in [1,27,30].", "startOffset": 75, "endOffset": 84}, {"referenceID": 29, "context": "\u2013 For training the CNN, the hard negative mining strategy has been used in [1,27,30].", "startOffset": 75, "endOffset": 84}, {"referenceID": 30, "context": "As argued in some manifold learning methods [31,29,4], it is effective to use the local Euclidean distance, combining with the graphical relationship between samples, to approximate the geodesic distance.", "startOffset": 44, "endOffset": 53}, {"referenceID": 28, "context": "As argued in some manifold learning methods [31,29,4], it is effective to use the local Euclidean distance, combining with the graphical relationship between samples, to approximate the geodesic distance.", "startOffset": 44, "endOffset": 53}, {"referenceID": 3, "context": "As argued in some manifold learning methods [31,29,4], it is effective to use the local Euclidean distance, combining with the graphical relationship between samples, to approximate the geodesic distance.", "startOffset": 44, "endOffset": 53}, {"referenceID": 29, "context": "The hard negative mining strategy [30] has been used for face recognition.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "In person re-identification, IDLA [1] also adopted hard negative mining for the training.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "A commonly used metric by deep learning methods is the Euclidean distance [6,30,27].", "startOffset": 74, "endOffset": 83}, {"referenceID": 29, "context": "A commonly used metric by deep learning methods is the Euclidean distance [6,30,27].", "startOffset": 74, "endOffset": 83}, {"referenceID": 26, "context": "A commonly used metric by deep learning methods is the Euclidean distance [6,30,27].", "startOffset": 74, "endOffset": 83}, {"referenceID": 21, "context": "Therefore, using the Mahalanobis distance is a better choice for multivariate metric [22].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "In the area of face recognition, DDML [11] implemented the Mahalanobis metric in their network, but without any constraint.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "1(a) shows some hard positive cases in the data set of CUHK03 [17].", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "The usual way is to use the Mahalanobis distance (or the special case Euclidean) [6,30,11] which is a suitable metric in the ideal condition (Fig.", "startOffset": 81, "endOffset": 90}, {"referenceID": 29, "context": "The usual way is to use the Mahalanobis distance (or the special case Euclidean) [6,30,11] which is a suitable metric in the ideal condition (Fig.", "startOffset": 81, "endOffset": 90}, {"referenceID": 10, "context": "The usual way is to use the Mahalanobis distance (or the special case Euclidean) [6,30,11] which is a suitable metric in the ideal condition (Fig.", "startOffset": 81, "endOffset": 90}, {"referenceID": 30, "context": "On the other hand, the manifold learning methods [31,29,4] suggest to use the Euclidean distance (or heat kernel) in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance.", "startOffset": 49, "endOffset": 58}, {"referenceID": 28, "context": "On the other hand, the manifold learning methods [31,29,4] suggest to use the Euclidean distance (or heat kernel) in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance.", "startOffset": 49, "endOffset": 58}, {"referenceID": 3, "context": "On the other hand, the manifold learning methods [31,29,4] suggest to use the Euclidean distance (or heat kernel) in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance.", "startOffset": 49, "endOffset": 58}, {"referenceID": 10, "context": "The metric learning layer is like the structure proposed in DDML [11], and its learning is improved via a weight constraint.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "Compared with the Mahalanobis distance, the Euclidean distance has less discriminability but better generalization ability, because it does not take account of the scales and the correlation across dimensions [22].", "startOffset": 209, "endOffset": 213}, {"referenceID": 13, "context": "Our method is implemented via remodifying the CUDA-Convnet [14] framework.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "CUHK03 [17], CUHK01 [16] and VIPeR [9].", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "CUHK03 [17], CUHK01 [16] and VIPeR [9].", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "CUHK03 [17], CUHK01 [16] and VIPeR [9].", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "Finally, we show the proposed method also performs well on the small data-set of VIPeR [9] and gains competitive results (Section 4.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "84M parameters) compared with the previous best methods on CUHK03&01 (IDLA [1], 2.", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "32M) and VIPeR (DeepFeature [6], 26M).", "startOffset": 28, "endOffset": 31}, {"referenceID": 34, "context": "DML [35] adopted a similar architecture but with tied weights between branches.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "The network has learned remarkable color representations, which is coherent with the results of IDLA [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 18, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 65, "endOffset": 69}, {"referenceID": 23, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 76, "endOffset": 80}, {"referenceID": 39, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 99, "endOffset": 102}, {"referenceID": 31, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 109, "endOffset": 113}, {"referenceID": 4, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 120, "endOffset": 123}, {"referenceID": 39, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "The deep learning methods include FPNN [17] and IDLA [1].", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "The deep learning methods include FPNN [17] and IDLA [1].", "startOffset": 53, "endOffset": 56}, {"referenceID": 15, "context": "According to the protocol in [16], the data set is divided into a training set of 871 subjects and a test set of 100.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "We train the network on CUHK03, and further fine-tune it on CUHK01, as the same setting with the state-of-the-art method IDLA [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 41, "context": "Besides, to inspect the limitation of the data set CUHK01, we involve the recently released Market1501 [42] into the training.", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "line with the fact [1] that the learned filters in network mainly focus on image colors (as shown in Fig.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "The VIPeR [9] data set includes 632 subjects, each of which has 2 images from two different cameras.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 48, "endOffset": 51}, {"referenceID": 36, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 75, "endOffset": 79}, {"referenceID": 38, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 137, "endOffset": 141}, {"referenceID": 7, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 147, "endOffset": 150}, {"referenceID": 2, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 169, "endOffset": 172}, {"referenceID": 20, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 181, "endOffset": 185}, {"referenceID": 27, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 227, "endOffset": 231}, {"referenceID": 42, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 238, "endOffset": 242}, {"referenceID": 19, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 250, "endOffset": 254}, {"referenceID": 24, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 261, "endOffset": 265}, {"referenceID": 40, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 295, "endOffset": 299}, {"referenceID": 17, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 335, "endOffset": 339}, {"referenceID": 17, "context": "39%) is obtained by a combination of two methods (mFilter+LADF) [18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "The identification rate by DeepFeature [6] is close to ours at rank 1, but much lower at higher ranks.", "startOffset": 39, "endOffset": 42}], "year": 2016, "abstractText": "Person re-identification is challenging due to the large variations of pose, illumination, occlusion and camera view. Owing to these variations, the pedestrian data is distributed as highly-curved manifolds in the feature space, despite the current convolutional neural networks (CNN)\u2019s capability of feature extraction. However, the distribution is unknown, so it is difficult to use the geodesic distance when comparing two samples. In practice, the current deep embedding methods use the Euclidean distance for the training and test. On the other hand, the manifold learning methods suggest to use the Euclidean distance in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance. From this point of view, selecting suitable positive (i.e. intra-class) training samples within a local range is critical for training the CNN embedding, especially when the data has large intra-class variations. In this paper, we propose a novel moderate positive sample mining method to train robust CNN for person re-identification, dealing with the problem of large variation. In addition, we improve the learning by a metric weight constraint, so that the learned metric has a better generalization ability. Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification, and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification. Therefore, the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification.", "creator": "LaTeX with hyperref package"}}}