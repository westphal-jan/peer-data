{"id": "1611.00175", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Robust Spectral Inference for Joint Stochastic Matrix Factorization", "abstract": "winnicott Spectral zuiderzee inference provides .467 fast morgano algorithms recite and provable luq optimality heagy for latent generalfeldmarschall topic reciente analysis. communitarianism But for real data hanan these kremin algorithms carisbrook require kanwar additional yavari ad - hoc heuristics, coordination and even then often produce munnar unusable gabinete results. marek We explain this enter poor maffay performance eberswalde by casting sanderling the mumias problem wftv of tsurunen topic farahani inference masking in goonies the framework vrbanja of landgraaf Joint Stochastic borjas Matrix licinio Factorization (JSMF) cramp and showing hoarsely that previous methods violate the repackages theoretical conditions intersectional necessary squealing for a good 73-77 solution to exist. We then propose aichatou a hambach novel rectification method that halva learns high quality 72.13 topics three-course and their interactions even 3.56 on radioiodine small, reactor noisy data. gooses This method achieves wrightwood results comparable 3,664 to phosphorous probabilistic techniques db9 in 17.63 several domains 18-27 while mastenbroek maintaining scalability emeap and moncreiffe provable optimality.", "histories": [["v1", "Tue, 1 Nov 2016 10:06:57 GMT  (507kb,D)", "http://arxiv.org/abs/1611.00175v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["moontae lee", "david bindel", "david m mimno"], "accepted": true, "id": "1611.00175"}, "pdf": {"name": "1611.00175.pdf", "metadata": {"source": "CRF", "title": "Robust Spectral Inference for Joint Stochastic Matrix Factorization", "authors": ["Moontae Lee", "David Bindel"], "emails": ["moontae@cs.cornell.edu", "bindel@cs.cornell.edu", "mimno@cornell.edu"], "sections": [{"heading": null, "text": "Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality."}, {"heading": "1 Introduction", "text": "Summarizing large data sets using pairwise co-occurrence frequencies is a powerful tool for data mining. Objects can often be better described by their relationships than their inherent characteristics. Communities can be discovered from friendships [1], song genres can be identified from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4]. Recent Anchor Word algorithms [5, 6] perform spectral inference on co-occurrence statistics for inferring topic models [7, 8]. Co-occurrence statistics can be calculated using a single parallel pass through a training corpus. While these algorithms are fast, deterministic, and provably guaranteed, they are sensitive to observation noise and small samples, often producing effectively useless results on real documents that present no problems for probabilistic algorithms.\nWe cast this general problem of learning overlapping latent clusters as Joint-Stochastic Matrix Factorization (JSMF), a subset of non-negative matrix factorization that contains topic modeling as a special case. We explore the conditions necessary for inference from cooccurrence statistics and show that the Anchor Words algorithms necessarily violate such\nconditions. Then we propose a rectified algorithm that matches the performance of probabilistic inference\u2014even on small and noisy datasets\u2014without losing efficiency and provable guarantees. Validating on both real and synthetic data, we demonstrate that our rectification not only produces better clusters, but also, unlike previous work, learns meaningful cluster interactions.\nar X\niv :1\n61 1.\n00 17\n5v 1\n[ cs\n.L G\n] 1\nN ov\nLet the matrix C represent the co-occurrence of pairs drawn from N objects: Cij is the joint probability p(X1 = i,X2 = j) for a pair of objects i and j. Our goal is to discover K latent clusters by approximately decomposing C \u2248 BABT . B is the object-cluster matrix, in which each column corresponds to a cluster and Bik = p(X = i|Z = k) is the probability of drawing an object i conditioned on the object belonging to the cluster k; and A is the cluster-cluster matrix, in which Akl = p(Z1 = k, Z2 = l) represents the joint probability of pairs of clusters. We call the matrices C and A joint-stochastic (i.e., C \u2208 JSN , A \u2208 JSK) due to their correspondence to joint distributions; B is column-stochastic. Example applications are shown in Table 1.\nAnchor Word algorithms [5, 6] solve JSMF problems using a separability assumption: each topic contains at least one \u201canchor\u201d word that has non-negligible probability exclusively in that topic. The algorithm uses the co-occurrence\npatterns of the anchor words as a summary basis for the co-occurrence patterns of all other words. The initial algorithm [5] is theoretically sound but unable to produce column-stochastic word-topic matrix B due to unstable matrix inversions. A subsequent algorithm [6] fixes negative entries in B, but still produces large negative entries in the estimated topic-topic matrix A. As shown in Figure 3, the proposed algorithm infers valid topic-topic interactions."}, {"heading": "2 Requirements for Factorization", "text": "In this section we review the probabilistic and statistical structures of JSMF and then define geometric structures of co-occurrence matrices required for successful factorization. C \u2208 RN\u00d7N is a joint-stochastic matrix constructed from M training examples, each of which contain some subset of N objects. We wish to find K N latent clusters by factorizing C into a column-stochastic matrix B \u2208 RN\u00d7K and a joint-stochastic matrix A \u2208 RK\u00d7K , satisfying C \u2248 BABT .\n1\nProbabilistic structure. Figure 2 shows the event space of our model. The distribution A over pairs of clusters is generated first from a stochastic process with a hyperparameter \u03b1. If the m-th training example contains a total of nm objects, our model views the example as consisting of all possible nm(nm \u2212 1) pairs of objects.1 For each of these pairs, cluster assignments are sampled from the selected distribution ((z1, z2) \u223c A). Then an actual object pair is drawn with respect to the corresponding cluster assignments (x1 \u223c Bz1 , x2 \u223c Bz2 ). Note that this process does not explain how each training example is generated from a model, but shows how our model understands the objects in the training examples.\nFollowing [5, 6], our model views B as a set of parameters rather than random variables.2 The primary learning task is to estimate B; we then estimate A to recover the hyperparameter \u03b1. Due to the conditional independence X1 \u22a5 X2 | (Z1 or Z2), the factorization C \u2248 BABT is equivalent to\np(X1, X2|A;B) = \u2211 z1 \u2211 z2 p(X1|Z1;B)p(Z1, Z2|A)p(X2|Z2;B).\nUnder the separability assumption, each cluster k has a basis object sk such that p(X = sk|Z = k) > 0 and p(X = sk|Z 6= k) = 0. In matrix terms, we assume the submatrix of B comprised of\n1Due to the bag-of-words assumption, every object can pair with any other object in that example, except itself. One implication of our work is better understanding the self-co-occurrences, the diagonal entries in the co-occurrence matrix.\n2In LDA, each column of B is generated from a known distribution Bk \u223c Dir(\u03b2).\nthe rows with indices S = {s1, . . . , sK} is diagonal. As these rows form a non-negative basis for the row space of B, the assumption implies rank+(B) = K = rank(B).3 Providing identifiability to the factorization, this assumption becomes crucial for inference of bothB andA. Note that JSMF factorization is unique up to column permutation, meaning that no specific ordering exists among the discovered clusters, equivalent to probabilistic topic models (see the Appendix).\nStatistical structure. Let f(\u03b1) be a (known) distribution of distributions from which a cluster distribution is sampled for each training example. Saying Wm \u223c f(\u03b1), we have M i.i.d samples {W1, . . . ,WM} which are not directly observable. Defining the posterior cluster-cluster matrix A\u2217M = 1 M \u2211M m=1WmW T m and the expectation A \u2217 = E[WmWTm], Lemma 2.2 in [5] showed that4\nA\u2217M \u2212\u2192 A\u2217 as M \u2212\u2192\u221e. (1) Denote the posterior co-occurrence for the m-th training example by C\u2217m and all examples by C\n\u2217. Then C\u2217m = BWmW T mB T , and C\u2217 = 1M \u2211M m=1 C \u2217 m. Thus\nC\u2217 = B\n( 1\nM M\u2211 m=1 WmW T m ) BT = BA\u2217MB T . (2)\nDenote the noisy observation for the m-th training example by Cm, and all examples by C. Let W = [W1|...|WM ] be a matrix of topics. We will construct Cm so that E[C|W ] is an unbiased estimator of C\u2217. Thus as M \u2192\u221e\nC \u2212\u2192 E[C] = C\u2217 = BA\u2217MBT \u2212\u2192 BA\u2217BT . (3)\nGeometric structure. Though the separability assumption allows us to identify B even from the noisy observation C, we need to throughly investigate the structure of cluster interactions. This is because it will eventually be related to how much useful information the co-occurrence between corresponding anchor bases contains, enabling us to best use our training data. Say DNNn is the set of n\u00d7n doubly non-negative matrices: entrywise non-negative and positive semidefinite (PSD). Claim A\u2217M , A\u2217 \u2208 DNNK and C\u2217 \u2208 DNNN Proof Take any vector y \u2208 RK . As A\u2217M is defined as a sum of outer-products,\nyTA\u2217My = 1\nM M\u2211 m=1 yTWmW T my = 1 M \u2211 (WTmy) T (WTmy) = \u2211 (non-negative) \u2265 0. (4)\nThus A\u2217M \u2208 PSDK . In addition, (A\u2217M )kl = p(Z1 = k, Z2 = l) \u2265 0 for all k, l. Proving A\u2217 \u2208 DNNK is analogous by the linearity of expectation. Relying on double non-negativity of A\u2217M , Equation (3) implies not only the low-rank structure of C\n\u2217, but also double non-negativity of C\u2217 by a similar proof (see the Appendix).\nThe Anchor Word algorithms in [5, 6] consider neither double non-negativity of cluster interactions nor its implication on co-occurrence statistics. Indeed, the empirical co-occurrence matrices collected from limited data are generally indefinite and full-rank, whereas the posterior co-occurrences must be positive semidefinite and low-rank. Our new approach will efficiently enforce double nonnegativity and low-rankness of the co-occurrence matrix C based on the geometric property of its posterior behavior. We will later clarify how this process substantially improves the quality of the clusters and their interactions by eliminating noises and restoring missing information."}, {"heading": "3 Rectified Anchor Words Algorithm", "text": "In this section, we describe how to estimate the co-occurrence matrix C from the training data, and how to rectify C so that it is low-rank and doubly non-negative. We then decompose the rectified C \u2032 in a way that preserves the doubly non-negative structure in the cluster interaction matrix.\n3rank+(B) means the non-negative rank of the matrix B, whereas rank(B) means the usual rank. 4This convergence is not trivial while 1\nM \u2211M m=1Wm \u2192 E[Wm] asM \u2192\u221e by the Central Limit Theorem.\nGenerating co-occurrence C. Let Hm be the vector of object counts for the m-th training example, and let pm = BWm whereWm is the document\u2019s latent topic distribution. ThenHm is assumed to be a sample from a multinomial distributionHm \u223c Multi(nm, pm) where nm = \u2211N i=1H (i) m , and\nrecall E[Hm] = nmpm = nmBWm and Cov(Hm) = nm ( diag(pm)\u2212 pmpTm ) . As in [6], we generate the co-occurrence for the m-th example by\nCm = HmH\nT m \u2212 diag(Hm)\nnm(nm \u2212 1) . (5)\nThe diagonal penalty in Eq. 5 cancels out the diagonal matrix term in the variance-covariance matrix, making the estimator unbiased. Putting dm = nm(nm \u2212 1), that is E[Cm|Wm] = 1dmE[HmH T m]\u2212\n1 dm diag(E[Hm]) = 1dm (E[Hm]E[Hm] T + Cov(Hm) \u2212 diag(E[Hm])) = B(WmWTm)BT \u2261 C\u2217m. Thus E[C|W ] = C\u2217 by the linearity of expectation.\nRectifying co-occurrence C. While C is an unbiased estimator for C\u2217 in our model, in reality the two matrices often differ due to a mismatch between our model assumptions and the data5 or due to error in estimation from limited data. The computed C is generally full-rank with many negative eigenvalues, causing a large approximation error. As the posterior co-occurrence C\u2217 must be lowrank, doubly non-negative, and joint-stochastic, we propose two rectification methods: Diagonal Completion (DC) and Alternating Projection (AP). DC modifies only diagonal entries so that C becomes low-rank, non-negative, and joint-stochastic; while AP enforces modifies every entry and enforces the same properties as well as positive semi-definiteness. As our empirical results strongly favor alternating projection, we defer the details of diagonal completion to the Appendix.\nBased on the desired property of the posterior co-occurrence C\u2217, we seek to project our estimator C onto the set of joint-stochastic, doubly non-negative, low rank matrices. Alternating projection methods like Dykstra\u2019s algorithm [9] allow us to project onto an intersection of finitely many convex sets using projections onto each individual set in turn. In our setting, we consider the intersection of three sets of symmetric N \u00d7 N matrices: the elementwise non-negative matrices NNN , the normalized matricesNORN whose entry sum is equal to 1, and the positive semi-definite matrices with rank K, PSDNK . We project onto these three sets as follows:\n\u03a0PSDNK (C) = U\u039b + KU T , \u03a0NORN (C) = C + 1\u2212\u2211i,j Cij\nN2 11T , \u03a0NNN (C) = max{C, 0}.\nwhere C = U\u039bUT is an eigendecomposition and \u039b+K is the matrix \u039b modified so that all negative eigenvalues and any but the K largest positive eigenvalues are set to zero. Truncated eigendecompositions can be computed efficiently, and the other projections are likewise efficient. While NNN and NORN are convex, PSDNK is not. However, [10] show that alternating projection with a non-convex set still works under certain conditions, guaranteeing a local convergence. Thus iterating three projections in turn until the convergence rectifies C to be in the desired space. We will show how to satisfy such conditions and the convergence behavior in Section 5.\nSelecting basis S. The first step of the factorization is to select the subset S of objects that satisfy the separability assumption. We want the K best rows of the row-normalized co-occurrence matrix C so that all other rows lie nearly in the convex hull of the selected rows. [6] use the GramSchmidt process to select anchors, which computes pivoted QR decomposition, but did not utilize the sparsity of C. To scale beyond small vocabularies, they use random projections that approximately preserve `2 distances between rows of C. For all experiments we use a new pivoted QR algorithm (see the Appendix) that exploits sparsity instead of using random projections, and thus preserves deterministic inference.6\nRecovering object-cluster B. After finding the set of basis objects S, we can infer each entry of B by Bayes\u2019 rule as in [6]. Let {p(Z1 = k|X1 = i)}Kk=1 be the coefficients that reconstruct the i-th row of C in terms of the basis rows corresponding to S. Since Bik = p(X1 = i|Z1 = k),\n5There is no reason to expect real data to be generated from topics, much less exactly K latent topics. 6To effectively use random projections, it is necessary to either find proper dimensions based on multiple\ntrials or perform low-dimensional random projection multiple times [11] and merge the resulting anchors.\nwe can use the corpus frequencies p(X1 = i) = \u2211 j Cij to estimate Bik \u221d p(Z1 = k|X1 = i)p(X1 = i). Thus the main task for this step is to solve simplex-constrained QPs to infer a set of such coefficients for each object. We use an exponentiated gradient algorithm to solve the problem similar to [6]. Note that this step can be efficiently done in parallel for each object.\nRecovering cluster-cluster A. [6] recovered A by minimizing \u2016C \u2212 BABT \u2016F ; but the inferred A generally has many negative entries, failing to model the probabilistic interaction between topics. While we can further project A onto the joint-stochastic matrices, this produces a large approximation error.\nWe consider an alternate recovery method that again leverages the\nseparability assumption. Let CSS be the submatrix whose rows and columns correspond to the selected objects S, and let D be the diagonal submatrix BS\u2217 of rows of B corresponding to S. Then\nCSS = DAD T = DAD =\u21d2 A = D\u22121CSSD\u22121. (6)\nThis approach efficiently recovers a cluster-cluster matrix A mostly based on the co-occrrurence information between corresponding anchor basis, and produces no negative entries due to the stability of diagonal matrix inversion. Note that the principle submatrices of a PSD matrix are also PSD; hence, if C \u2208 PSDN then CSS , A \u2208 PSDK . Thus, not only is the recovered A an unbiased estimator for A\u2217M , but also it is now doubly non-negative as A \u2217 M \u2208 DNNK after the rectification.7"}, {"heading": "4 Experimental Results", "text": "Our Rectified Anchor Words algorithm with alternating projection fixes many problems in the baseline Anchor Words algorithm [6] while matching the performance of Gibbs sampling [12] and maintaining spectral inference\u2019s determinism and independence from corpus size. We evaluate direct measurement of matrix quality as well as indicators of topic utility. We use two text datasets: NIPS full papers and New York Times news articles.8 We eliminate a minimal list of 347 English stop words and prune rare words based on tf-idf scores and remove documents with fewer than five tokens after vocabulary curation. We also prepare two non-textual item-selection datasets: users\u2019 movie reviews from the Movielens 10M Dataset,9 and music playlists from the complete Yes.com dataset.10 We perform similar vocabulary curation and document tailoring, with the exception of frequent stop-object elimination. Playlists often contain the same songs multiple times, but users are unlikely to review the same movies more than once, so we augment the movie dataset so that each review contains 2 \u00d7 (stars) number of movies based on the half-scaled rating information that varies from 0.5 stars to 5 stars. Statistics of our datasets are shown in Table 2.\nTable 2: Statistics of four datasets.\nDataset M N Avg. Len NIPS 1,348 5k 380.5\nNYTimes 269,325 15k 204.9 Movies 63,041 10k 142.8 Songs 14,653 10k 119.2\nWe run DC 30 times for each experiment, randomly permuting the order of objects and using the median results to minimize the effect of different orderings. We also run 150 iterations of AP alternating PSDNK , NORN , and NNN in turn. For probabilistic Gibbs sampling, we use the Mallet with the standard option doing 1,000 iterations. All metrics are evaluated against\nthe original C, not against the rectified C \u2032, whereas we use B and A inferred from the rectified C \u2032. 7We later realized that essentially same approach was previously tried in [5], but it was not able to generate a valid topic-topic matrix as shown in the middle panel of Figure 3. 8https://archive.ics.uci.edu/ml/datasets/Bag+of+Words 9http://grouplens.org/datasets/movielens\n10http://www.cs.cornell.edu/\u02dcshuochen/lme\nQualitative results. Although [6] report comparable results to probabilistic algorithms for LDA, the algorithm fails under many circumstances. The algorithm prefers rare and unusual anchor words that form a poor basis, so topic clusters consist of the same high-frequency terms repeatedly, as shown in the upper third of Table 3. In contrast, our algorithm with AP rectification successfully learns themes similar to the probabilistic algorithm. One can also verify that cluster interactions given in the third panel of Figure 3 explain how the five topics correlate with each other.\nSimilar to [13], we visualize the five anchor words in the cooccurrence space after 2D PCA of C. Each panel in Figure 1 shows a 2D embedding of the NIPS vocabulary as blue dots and five selected anchor words in red. The first plot shows standard anchor words and the original cooccurrence space. The second plot shows anchor words selected from the rectified space overlaid on the original co-occurrence space. The third plot shows the same anchor words as the second plot overlaid on the AP-rectified space. The rectified anchor words provide better coverage on both spaces, explaining why we are able to achieve reasonable topics even with K = 5.\nRectification also produces better clusters in the non-textual movie dataset. Each cluster is notably more genre-coherent and year-coherent than the clusters from the original algorithm. WhenK = 15, for example, we verify a cluster of Walt Disney 2D Animations mostly from the 1990s and a cluster of Fantasy movies represented by Lord of the Rings films, similar to clusters found by probabilistic Gibbs sampling. The Baseline algorithm [6] repeats Pulp Fiction and Silence of the Lambs 15 times.\nQuantitative results. We measure the intrinsic quality of inference and summarization with respect to the JSMF objectives as well as the extrinsic quality of resulting topics. Lines correspond to four methods: \u25e6 Baseline for the algorithm in the previous work [6] without any rectification,4 DC for Diagonal Completion, AP for Alternating Projection, and Gibbs for Gibbs sampling. Anchor objects should form a good basis for the remaining objects. We measure Recovery error(\n1 N \u2211N i \u2016Ci \u2212 \u2211K k p(Z1 = k|X1 = i)CSk\u20162 ) with respect to the original C matrix, not the rectified matrix. AP reduces error in almost all cases and is more effective than DC. Although we expect error to decrease as we increase the number of clusters K, reducing recovery error for a fixed K by choosing better anchors is extremely difficult: no other subset selection algorithm [14] decreased error by more than 0.001. A good matrix factorization should have small elementwise Approximation error ( \u2016C \u2212 BABT \u2016F ) . DC and AP preserve more of the information in the original matrix C than the Baseline method, especially when K is small.11 We expect nontrivial interactions between clusters, even when we do not explicitly model them as in [15]. Greater diagonal Dominancy ( 1 K \u2211K k p(Z2 = k|Z1 = k) ) indicates lower correlation between clusters.12 AP and Gibbs results are similar. We do not report held-out probability because we find that relative results are determined by user-defined smoothing parameters [13, 16].\nSpecificity (\n1 K \u2211K k KL (p(X|Z = k)\u2016p(X)) ) measures how much each cluster is distinct from\nthe corpus distribution. When anchors produce a poor basis, the conditional distribution of clus-\n11In the NYTimes corpus, 10\u22122 is a large error: each element is around 10\u22129 due to the number of normalized entries.\n12Dominancy in Songs corpus lacks any Baseline results at K > 10 because dominancy is undefined if an algorithm picks a song that occurs at most once in each playlist as a basis object. In this case, the original construction of CSS , and hence of A, has a zero diagonal element, making dominancy NaN.\nters given objects becomes uniform, making p(X|Z) similar to p(X). Inter-topic Dissimilarity counts the average number of objects in each cluster that do not occur in any other cluster\u2019s top 20 objects. Our experiments validate that AP and Gibbs yield comparably specific and distinct topics, while Baseline and DC simply repeat the corpus distribution as in Table 3. Coherence(\n1 K \u2211K k \u2211\u2208Topk x1 6=x2 log D2(x1,x2)+ D1(x2) ) penalizes topics that assign high probability (rank > 20) to words that do not occur together frequently. AP produces results close to Gibbs sampling, and far from the Baseline and DC. While this metric correlates with human evaluation of clusters [17] \u201cworse\u201d coherence can actually be better because the metric does not penalize repetition [13].\nIn semi-synthetic experiments [6] AP matches Gibbs sampling and outperforms the Baseline, but the discrepancies in topic quality metrics are smaller than in the real experiments (see Appendix). We speculate that semi-synthetic data is more \u201cwell-behaved\u201d than real data, explaining why issues were not recognized previously."}, {"heading": "5 Analysis of Algorithm", "text": "Why does AP work? Before rectification, diagonals of the empirical C matrix may be far from correct. Bursty objects yield diagonal entries that are too large; extremely rare objects that occur at most once per document yield zero diagonals. Rare objects are problematic in general: the corresponding rows in the C matrix are sparse and noisy, and these rows are likely to be selected by the pivoted QR. Because rare objects are likely to be anchors, the matrix CSS is likely to be highly diagonally dominant, and provides an uninformative picture of topic correlations. These problems are exacerbated when K is small relative to the effective rank of C, so that an early choice of a poor anchor precludes a better choice later on; and when the number of documents M is small, in which case the empirical C is relatively sparse and is strongly affected by noise. To mitigate this issue, [16] run exhaustive grid search to find document frequency cutoffs to get informative anchors. As\nmodel performance is inconsistent for different cutoffs and search requires cross-validation for each case, it is nearly impossible to find good heuristics for each dataset and number of topics.\nFortunately, a low-rank PSD matrix cannot have too many diagonally-dominant rows, since this violates the low rank property. Nor can it have diagonal entries that are small relative to off-diagonals, since this violates positive semi-definiteness. Because the anchor word assumption implies that non-negative rank and ordinary rank are the same, the AP algorithm ideally does not remove the information we wish to learn; rather, 1) the low-rank projection in AP suppresses the influence of small numbers of noisy rows associated with rare words which may not be well correlated with the others, and 2) the PSD projection in AP recovers missing information in diagonals. (As illustrated in the Dominancy panel of the Songs corpus in Figure 4, AP shows valid dominancies even after K > 10 in contrast to the Baseline algorithm.)\nWhy does AP converge? AP enjoys local linear convergence [10] if 1) the initial C is near the convergence point C \u2032, 2) PSDNK is super-regular at C \u2032, and 3) strong regularity holds at C \u2032. For the first condition, recall that we rectifiedC \u2032 by pushingC towardC\u2217, which is the ideal convergence point inside the intersection. Since C \u2192 C\u2217 as shown in (5), C is close to C \u2032 as desired.The proxregular sets13 are subsets of super-regular sets, so prox-regularity of PSDNK at C \u2032 is sufficient for the second condition. For permutation invariantM \u2282 RN , the spectral set of symmetric matrices is defined as \u03bb\u22121(M) = {X \u2208 SN : (\u03bb1(X), . . . , \u03bbN (X)) \u2208 M}, and \u03bb\u22121(M) is prox-regular if and only ifM is prox-regular [18, Th. 2.4]. LetM be {x \u2208 R+n : |supp(x)| = K}. Since each element inM has exactly K positive components and all others are zero, \u03bb\u22121(M) = PSDNK . By the definition ofM and K < N , PM is locally unique almost everywhere, satisfying the second condition almost surely. (As the intersection of the convex set PSDN and the smooth manifold of rank K matrices, PSDNK is a smooth manifold almost everywhere.) Checking the third condition a priori is challenging, but we expect noise in the empirical C to prevent an irregular solution, following the argument of Numerical Example 9 in [10]. We expect AP to converge locally linearly and we can verify local convergence of AP in practice. Empirically, the ratio of average distances between two iterations are always \u2264 0.9794 on the NYTimes dataset (see the Appendix), and other datasets were similar. Note again that our rectified C \u2032 is a result of pushing the empirical C toward the ideal C\u2217. Because approximation factors of [6] are all computed based on how far C and its co-occurrence shape could be distant from C\u2217\u2019s, all provable guarantees of [6] hold better with our rectified C \u2032."}, {"heading": "6 Related and Future Work", "text": "JSMF is a specific structure-preserving Non-negative Matrix Factorization (NMF) performing spectral inference. [19, 20] exploit a similar separable structure for NMF problmes. To tackle hyperspectral unmixing problems, [21, 22] assume pure pixels, a separability-equivalent in computer vision. In more general NMF without such structures, RESCAL [23] studies tensorial extension of similar factorization and SymNMF [24] infers BBT rather than BABT . For topic modeling, [25] performs spectral inference on third moment tensor assuming topics are uncorrelated.\nAs the core of our algorithm is to rectify the input co-occurrence matrix, it can be combined with several recent developments. [16] proposes two regularization methods for recovering better B. [13] nonlinearly projects co-occurrence to low-dimensional space via t-SNE and achieves better anchors by finding the exact anchors in that space. [11] performs multiple random projections to low-dimensional spaces and recovers approximate anchors efficiently by divide-and-conquer strategy. In addition, our work also opens several promising research directions. How exactly do anchors found in the rectified C \u2032 form better bases than ones found in the original space C? Since now the topic-topic matrix A is again doubly non-negative and joint-stochastic, can we learn super-topics in a multi-layered hierarchical model by recursively applying JSMF to topic-topic co-occurrence A?"}, {"heading": "Acknowledgments", "text": "This research is supported by NSF grant HCC:Large-0910664. We thank Adrian Lewis for valuable discussions on AP convergence.\n13A setM is prox-regular if PM is locally unique."}], "references": [{"title": "You are who you know: Inferring user profiles in Online Social Networks", "author": ["Alan Mislove", "Bimal Viswanath", "Krishna P. Gummadi", "Peter Druschel"], "venue": "In Proceedings of the 3rd ACM International Conference of Web Search and Data Mining (WSDM\u201910),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Playlist prediction via metric embedding", "author": ["Shuo Chen", "J. Moore", "D. Turnbull", "T. Joachims"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning topic models \u2013 going beyond SVD", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "In FOCS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["Sanjeev Arora", "Rong Ge", "Yonatan Halpern", "David Mimno", "Ankur Moitra", "David Sontag", "Yichen Wu", "Michael Zhu"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "In UAI, pages 289\u2013296,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research, pages 993\u20131022,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "A method for finding projections onto the intersection of convex sets in Hilbert spaces. In Advances in Order Restricted Statistical Inference, volume 37 of Lecture Notes in Statistics, pages 28\u201347", "author": ["JamesP. Boyle", "RichardL. Dykstra"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1986}, {"title": "Local linear convergence for alternating and averaged nonconvex projections", "author": ["Adrian S. Lewis", "D.R. Luke", "Jrme Malick"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Divide-and-conquer learning by anchoring a conical hull", "author": ["Tianyi Zhou", "Jeff A Bilmes", "Carlos Guestrin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Low-dimensional embeddings for interpretable anchor-based topic inference", "author": ["Moontae Lee", "David Mimno"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Subset selection algorithms: Randomized vs. deterministic", "author": ["Mary E Broadbent", "Martin Brown", "Kevin Penner", "I Ipsen", "R Rehman"], "venue": "SIAM Undergraduate Research Online,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "A correlated topic model of science", "author": ["D. Blei", "J. Lafferty"], "venue": "Annals of Applied Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Anchors regularized: Adding robustness and extensibility to scalable topic-modeling algorithms", "author": ["Thang Nguyen", "Yuening Hu", "Jordan Boyd-Graber"], "venue": "In Association for Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Optimizing semantic coherence in topic models", "author": ["David Mimno", "Hanna Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum"], "venue": "In EMNLP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Prox-regularity of spectral functions and spectral sets", "author": ["A. Daniilidis", "A.S. Lewis", "J. Malick", "H. Sendov"], "venue": "Journal of Convex Analysis,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Yes we can: simplex volume maximization for descriptive web-scale matrix factorization", "author": ["Christian Thurau", "Kristian Kersting", "Christian Bauckhage"], "venue": "In CIKM\u201910,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Fast conical hull algorithms for nearseparable non-negative matrix factorization", "author": ["Abhishek Kumar", "Vikas Sindhwani", "Prabhanjan Kambadur"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Vertex component analysis: A fast algorithm to unmix hyperspectral data", "author": ["Jos M.P. Nascimento", "Student Member", "Jos M. Bioucas Dias"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "N-FindR method versus independent component analysis for lithological identification in hyperspectral imagery", "author": ["C\u00e9cile Gomez", "H. Le Borgne", "Pascal Allemand", "Christophe Delacourt", "Patrick Ledru"], "venue": "International Journal of Remote Sensing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML- 11),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Symmetric nonnegative matrix factorization for graph clustering", "author": ["Da Kuang", "Haesun Park", "Chris H.Q. Ding"], "venue": "In SDM. SIAM / Omnipress,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "A spectral algorithm for latent Dirichlet allocation", "author": ["Anima Anandkumar", "Dean P. Foster", "Daniel Hsu", "Sham Kakade", "Yi-Kai Liu"], "venue": "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Communities can be discovered from friendships [1], song genres can be identified from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4].", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "Communities can be discovered from friendships [1], song genres can be identified from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "Communities can be discovered from friendships [1], song genres can be identified from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4].", "startOffset": 202, "endOffset": 208}, {"referenceID": 3, "context": "Communities can be discovered from friendships [1], song genres can be identified from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4].", "startOffset": 202, "endOffset": 208}, {"referenceID": 4, "context": "Recent Anchor Word algorithms [5, 6] perform spectral inference on co-occurrence statistics for inferring topic models [7, 8].", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "Recent Anchor Word algorithms [5, 6] perform spectral inference on co-occurrence statistics for inferring topic models [7, 8].", "startOffset": 30, "endOffset": 36}, {"referenceID": 6, "context": "Recent Anchor Word algorithms [5, 6] perform spectral inference on co-occurrence statistics for inferring topic models [7, 8].", "startOffset": 119, "endOffset": 125}, {"referenceID": 7, "context": "Recent Anchor Word algorithms [5, 6] perform spectral inference on co-occurrence statistics for inferring topic models [7, 8].", "startOffset": 119, "endOffset": 125}, {"referenceID": 5, "context": "Figure 1: 2D visualizations show the low-quality convex hull found by Anchor Words [6] (left) and a better convex hull (middle) found by discovering anchor words on a rectified space (right).", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "Domain Object Cluster Basis Document Word Topic Anchor Word Image Pixel Segment Pure Pixel Network User Community Representative Legislature Member Party/Group Partisan Playlist Song Genre Signature Song Anchor Word algorithms [5, 6] solve JSMF problems using a separability assumption: each topic contains at least one \u201canchor\u201d word that has non-negligible probability exclusively in that topic.", "startOffset": 227, "endOffset": 233}, {"referenceID": 5, "context": "Domain Object Cluster Basis Document Word Topic Anchor Word Image Pixel Segment Pure Pixel Network User Community Representative Legislature Member Party/Group Partisan Playlist Song Genre Signature Song Anchor Word algorithms [5, 6] solve JSMF problems using a separability assumption: each topic contains at least one \u201canchor\u201d word that has non-negligible probability exclusively in that topic.", "startOffset": 227, "endOffset": 233}, {"referenceID": 4, "context": "The initial algorithm [5] is theoretically sound but unable to produce column-stochastic word-topic matrix B due to unstable matrix inversions.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "A subsequent algorithm [6] fixes negative entries in B, but still produces large negative entries in the estimated topic-topic matrix A.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Following [5, 6], our model views B as a set of parameters rather than random variables.", "startOffset": 10, "endOffset": 16}, {"referenceID": 5, "context": "Following [5, 6], our model views B as a set of parameters rather than random variables.", "startOffset": 10, "endOffset": 16}, {"referenceID": 4, "context": "2 in [5] showed that4 AM \u2212\u2192 A\u2217 as M \u2212\u2192\u221e.", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "The Anchor Word algorithms in [5, 6] consider neither double non-negativity of cluster interactions nor its implication on co-occurrence statistics.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "The Anchor Word algorithms in [5, 6] consider neither double non-negativity of cluster interactions nor its implication on co-occurrence statistics.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "As in [6], we generate the co-occurrence for the m-th example by", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "Alternating projection methods like Dykstra\u2019s algorithm [9] allow us to project onto an intersection of finitely many convex sets using projections onto each individual set in turn.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "However, [10] show that alternating projection with a non-convex set still works under certain conditions, guaranteeing a local convergence.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "[6] use the GramSchmidt process to select anchors, which computes pivoted QR decomposition, but did not utilize the sparsity of C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "After finding the set of basis objects S, we can infer each entry of B by Bayes\u2019 rule as in [6].", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "To effectively use random projections, it is necessary to either find proper dimensions based on multiple trials or perform low-dimensional random projection multiple times [11] and merge the resulting anchors.", "startOffset": 173, "endOffset": 177}, {"referenceID": 5, "context": "We use an exponentiated gradient algorithm to solve the problem similar to [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "Figure 3: The algorithm of [6] (first panel) produces negative cluster co-occurrence probabilities.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "A probabilistic reconstruction alone (this paper & [5], second panel) removes negative entries but has no offdiagonals and does not sum to one.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "[6] recovered A by minimizing \u2016C \u2212 BAB \u2016F ; but the inferred A generally has many negative entries, failing to model the probabilistic interaction between topics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Our Rectified Anchor Words algorithm with alternating projection fixes many problems in the baseline Anchor Words algorithm [6] while matching the performance of Gibbs sampling [12] and maintaining spectral inference\u2019s determinism and independence from corpus size.", "startOffset": 124, "endOffset": 127}, {"referenceID": 11, "context": "Our Rectified Anchor Words algorithm with alternating projection fixes many problems in the baseline Anchor Words algorithm [6] while matching the performance of Gibbs sampling [12] and maintaining spectral inference\u2019s determinism and independence from corpus size.", "startOffset": 177, "endOffset": 181}, {"referenceID": 4, "context": "We later realized that essentially same approach was previously tried in [5], but it was not able to generate a valid topic-topic matrix as shown in the middle panel of Figure 3.", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "Although [6] report comparable results to probabilistic algorithms for LDA, the algorithm fails under many circumstances.", "startOffset": 9, "endOffset": 12}, {"referenceID": 12, "context": "2013 (Baseline) neuron layer hidden recognition signal cell noise neuron layer hidden cell signal representation noise neuron layer cell hidden signal noise dynamic neuron layer cell hidden control signal noise neuron layer hidden cell signal recognition noise This paper (AP) neuron circuit cell synaptic signal layer activity control action dynamic optimal policy controller reinforcement recognition layer hidden word speech image net cell field visual direction image motion object orientation gaussian noise hidden approximation matrix bound examples Probabilistic LDA (Gibbs) neuron cell visual signal response field activity control action policy optimal reinforcement dynamic robot recognition image object feature word speech features hidden net layer dynamic neuron recurrent noise gaussian approximation matrix bound component variables Similar to [13], we visualize the five anchor words in the cooccurrence space after 2D PCA of C.", "startOffset": 859, "endOffset": 863}, {"referenceID": 5, "context": "The Baseline algorithm [6] repeats Pulp Fiction and Silence of the Lambs 15 times.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "Lines correspond to four methods: \u25e6 Baseline for the algorithm in the previous work [6] without any rectification,4 DC for Diagonal Completion, AP for Alternating Projection, and Gibbs for Gibbs sampling.", "startOffset": 84, "endOffset": 87}, {"referenceID": 13, "context": "Although we expect error to decrease as we increase the number of clusters K, reducing recovery error for a fixed K by choosing better anchors is extremely difficult: no other subset selection algorithm [14] decreased error by more than 0.", "startOffset": 203, "endOffset": 207}, {"referenceID": 14, "context": "11 We expect nontrivial interactions between clusters, even when we do not explicitly model them as in [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "We do not report held-out probability because we find that relative results are determined by user-defined smoothing parameters [13, 16].", "startOffset": 128, "endOffset": 136}, {"referenceID": 15, "context": "We do not report held-out probability because we find that relative results are determined by user-defined smoothing parameters [13, 16].", "startOffset": 128, "endOffset": 136}, {"referenceID": 16, "context": "While this metric correlates with human evaluation of clusters [17] \u201cworse\u201d coherence can actually be better because the metric does not penalize repetition [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "While this metric correlates with human evaluation of clusters [17] \u201cworse\u201d coherence can actually be better because the metric does not penalize repetition [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 5, "context": "In semi-synthetic experiments [6] AP matches Gibbs sampling and outperforms the Baseline, but the discrepancies in topic quality metrics are smaller than in the real experiments (see Appendix).", "startOffset": 30, "endOffset": 33}, {"referenceID": 15, "context": "To mitigate this issue, [16] run exhaustive grid search to find document frequency cutoffs to get informative anchors.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": ") Why does AP converge? AP enjoys local linear convergence [10] if 1) the initial C is near the convergence point C \u2032, 2) PSDNK is super-regular at C \u2032, and 3) strong regularity holds at C \u2032.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": ") Checking the third condition a priori is challenging, but we expect noise in the empirical C to prevent an irregular solution, following the argument of Numerical Example 9 in [10].", "startOffset": 178, "endOffset": 182}, {"referenceID": 5, "context": "Because approximation factors of [6] are all computed based on how far C and its co-occurrence shape could be distant from C\u2217\u2019s, all provable guarantees of [6] hold better with our rectified C \u2032.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Because approximation factors of [6] are all computed based on how far C and its co-occurrence shape could be distant from C\u2217\u2019s, all provable guarantees of [6] hold better with our rectified C \u2032.", "startOffset": 156, "endOffset": 159}, {"referenceID": 18, "context": "[19, 20] exploit a similar separable structure for NMF problmes.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[19, 20] exploit a similar separable structure for NMF problmes.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "To tackle hyperspectral unmixing problems, [21, 22] assume pure pixels, a separability-equivalent in computer vision.", "startOffset": 43, "endOffset": 51}, {"referenceID": 21, "context": "To tackle hyperspectral unmixing problems, [21, 22] assume pure pixels, a separability-equivalent in computer vision.", "startOffset": 43, "endOffset": 51}, {"referenceID": 22, "context": "In more general NMF without such structures, RESCAL [23] studies tensorial extension of similar factorization and SymNMF [24] infers BB rather than BAB .", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "In more general NMF without such structures, RESCAL [23] studies tensorial extension of similar factorization and SymNMF [24] infers BB rather than BAB .", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "For topic modeling, [25] performs spectral inference on third moment tensor assuming topics are uncorrelated.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "[16] proposes two regularization methods for recovering better B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] nonlinearly projects co-occurrence to low-dimensional space via t-SNE and achieves better anchors by finding the exact anchors in that space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] performs multiple random projections to low-dimensional spaces and recovers approximate anchors efficiently by divide-and-conquer strategy.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.", "creator": "LaTeX with hyperref package"}}}