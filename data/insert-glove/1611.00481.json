{"id": "1611.00481", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Online Multi-view Clustering with Incomplete Views", "abstract": "In the busso era warbucks of kolodziejczyk big data, unforgiven it is common hunnam to have data with allen multiple conciliatory modalities klickitat or navigare coming fangman from multiple 77.86 sources, caracalla known 62-year as \" description multi - xianlin view steepled data \". kohlhase Multi - colis\u00e9e view iashvili clustering bauder provides a natural way befehlshaber to crtc generate binder clusters from bakeel such checkitout data. Since different views share asf some consistency and complementary information, saawariya previous adelt works on multi - messerschmitts view sequin clustering davorin mainly acquaint focus convinced on alesi how revolucionarias to combine dilgar various numbers krajewski of views 44.44 to improve clustering barce performance. However, in transferases reality, merited each sikkema view beauvau may a-terre be incomplete, 30-1 i. brick-red e. , instances \u03c3 missing in extraleague the view. haigerloch Furthermore, ramazanov the mikell@nytimes.com size 4-100 of alaams data could patre be jagannathpur extremely occhipinti huge. 108.83 It wimshurst is a340-300 unrealistic pentium to c\u00fcneyt apply tawdriness multi - marquita view harro clustering lionni in virgilius large hime real - world applications castronovo without considering the incompleteness heightened of views and schnapps the kcra memory biesbroeck requirement. None 757-200 of falkenberg previous trunkline works have addressed interlining all comparacion these alzery challenges simultaneously. In this paper, we dutronc propose hillsboro an antlers online multi - view clustering algorithm, OMVC, which vilanch deals with pisarcik large - scale incomplete views. 8,430 We model the multi - garrigue view clustering moravcsik problem aquarian as a joint weighted raki nonnegative elephant matrix enact factorization 290 problem acevedo and process pre-compiled the multi - anakkara view ballincollig data modyford chunk pelican by chunk to metahumans reduce the 72.6 memory requirement. negligeable OMVC imps learns the latent feature matrices mazz for runcinated all 68,000 the vendramin views yiannis and pushes bbdo them evenhandedness towards stittsville a 52.12 consensus. We further sunniness increase 254th the robustness of the learned latent 40.71 feature seville matrices maness in OMVC narnaul via 25-hectare lasso zhengding regularization. d\u00e9cada To toyokawa minimize the apple.com influence majest\u00e9 of fico incompleteness, 1,077 dynamic residentie weight setting is ehp introduced to tutut give koshis lower weights skerries to consortiums the incoming missing instances ioseliani in parkroyal different absence views. norum More rolande importantly, srinu to reduce tasr the eredivisie computational www.eelamweb.com time, we 400 incorporate a suzuka faster projected green-colored gradient phuong descent cnbc.com by darlene utilizing match-making the Hessian matrices in OMVC. hazor Extensive experiments conducted \u00e5rhus on four real data 26 demonstrate dokey the effectiveness of lisitsyn the proposed OMVC bbcs method.", "histories": [["v1", "Wed, 2 Nov 2016 06:29:46 GMT  (436kb,D)", "http://arxiv.org/abs/1611.00481v1", null], ["v2", "Sun, 6 Nov 2016 18:05:35 GMT  (436kb,D)", "http://arxiv.org/abs/1611.00481v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weixiang shao", "lifang he", "chun-ta lu", "philip s yu"], "accepted": false, "id": "1611.00481"}, "pdf": {"name": "1611.00481.pdf", "metadata": {"source": "META", "title": "Online Multi-view Clustering with Incomplete Views", "authors": ["Weixiang Shao", "Lifang He", "Chun-ta Lu", "Philip S. Yu"], "emails": ["psyu}@uic.edu", "lifanghescut@gmail.com"], "sections": [{"heading": null, "text": "Keywords-Multi-view clustering; Online algorithm; Incomplete views; Nonnegative matrix factorization\nI. INTRODUCTION\nWith the advance of technology, real data are often with multiple modalities or coming from multiple sources. Such data is called multi-view data. For example, in web image retrieval, the visual information of images and their textual tags can be regarded as two views; in web page clustering, a web page may be translated into multiple languages, each language can be seen as a view. Usually, multiple views provide consistent and complementary information for the semantically same data. By exploiting these characteristics between multi-view data, multi-view learning can obtain better performance of learning tasks than relying on just one single view [28].\nMulti-view clustering [4], as one of basic tasks of multiview learning, provides a natural way for generating clus-\nters from multi-view data. A number of approaches have been proposed for multi-view clustering. Existing multi-view clustering algorithms can be roughly categorized into four categories [28]. Methods in the first category are based on subspace [8, 15, 20, 25], which learn a latent space so that different views are comparable in that space. Methods in the second category are co-training based algorithms [4, 14], which obtain the clustering results in an iterative manner. The third category aims to learn a unified similarity matrix among multi-view data, which serves as affinity matrix for final clustering [7, 23]. The last category is called late fusion [5, 9, 21]. Methods in this category first cluster each view independently and then combine the individual clusterings to produce a final clustering result.\nMost of the above studies are based on the assumption that all of the views are complete, i.e., each instance appears in all the views. However, due to the nature of the data or the cost of data collection, some views may suffer from the incompleteness of data (i.e., instances within some views missing). In order to deal with this problem, different approaches have been explored [17, 25, 26, 29]. [29] is the first to deal with incomplete views by utilizing information from one complete view to refer to the kernel of incomplete views. [17, 26] are among the first attempts to solve multiview clustering with none of the views complete. [25, 17] are the first attempts to solve multiple incomplete views clustering based on nonnegative matrix factorization (NMF). All the previous works require that the multi-view data can be fitted into the memory.\nHowever, in reality, the size of data in multi-view may be extremely huge. For example, in Web scale data mining, one may encounter billions of Web pages and the dimension of the features may be as large as O(106). Data with such size and dimension clearly will not fit into the memory of a single machine. In fact, even a small corpus like Wikipedia has more than 3 \u00d7 107 pages in multiple languages. None of the existing multi-view clustering algorithms can handle data in such scale.\nThere are several challenges preventing us from applying multi-view clustering algorithms to large-scale data.\n1) With the memory limitation, how to combine various views in different feature spaces and explore the consistency and complementary properties of different\nar X\niv :1\n61 1.\n00 48\n1v 1\n[ cs\n.L G\n] 2\nN ov\n2 01\nviews to get better clustering solutions. 2) When the data are too large to fit into memory, how\nto deal with incomplete views, i.e., how to minimize the influence of incompleteness of views. 3) How to effectively and efficiently learn the clustering solution even if the multi-view data are extremely large.\nIn this paper, we propose OMVC (Online Multi-View Clustering) to solve the above three challenges. To the best of our knowledge, this is the first online approach to solve the large-scale multi-view clustering problem with incomplete views. Basically, OMVC aims to reduce memory requirements by solving the problem in an online fashion. Instead of holding all the data in memory, it processes the multi-view data chunk by chunk. OMVC models the problem of multi-view clustering with incomplete views as a joint weighted nonnegative matrix factorization problem. It learns the latent feature matrices for incoming data chunk in each view and pushes them towards a common consensus. By storing the information of previous chunks in an aggregated form, OMVC does not store any specific previous data chunks. Inspired by the idea of weighted NMF [13], we use a dynamic weight setting to give lower weights to the incoming missing instances in different views. Thus, the proposed OMVC minimizes the negative influence from the missing instances. As one of the most commonly used regularization, `1 (Lasso) regularization has been successfully applied in many algorithms [12, 17, 18]. By integrating weighted joint nonnegative matrix factorization and `1 norm, OMVC tries to learn a latent subspace where the features of the same instance from different views will be co-regularized to a common consensus, while increasing the robustness of the learned latent feature matrices. More importantly, to reduce the computational time, OMVC incorporates a faster projected gradient descent algorithm by utilizing the Hessian matrices.\nThe contributions of this paper can be summarized as follows:\n1) The proposed OMVC method is the first attempt to solve the problem of large-scale multi-view clustering with incomplete views in an online fashion. 2) We model the multi-view clustering as a joint nonnegative matrix factorization problem. The proposed method will capture the relation between different heterogeneous views and learn a consensus latent feature matrix across all the views. 3) We introduce lower weights for missing data in different views to reduce the influence of incomplete views. By using a dynamic weight setting, we can fill the incoming missing data with a quick estimation and give lower weights to the less informative estimations. 4) By utilizing lasso regularization, OMVC enforces the sparsity of latent feature matrices, and increase its\nThe rest of this paper is organized as follows. In Section II, problem description and backgrounds are given. The details of the OMVC method are presented in Sections III and IV. Extensive experimental results and analysis are shown in Section V. Related works are discussed in Section VI and followed by conclusion in Section VII."}, {"heading": "II. PRELIMINARIES", "text": "In this section, we will briefly describe the problem of online multi-view clustering with incomplete views. Then some background knowledge about clustering using nonnegative matrix factorization will be introduced. Table I summarizes the notations used throughout the paper."}, {"heading": "A. Problem Description", "text": "Assume that we are given N instances in nv incomplete views {X(v), v = 1, 2, ..., nv}, where X(v) \u2208 RDv\u00d7N represents the data in the v-th view and Dv is the dimensionality of features in the v-th view. Each view may be incomplete, i.e., each of the view may have instances missing. We define an instance-view indicator matrix M \u2208 RN\u00d7nv by\nmi,j = { 1 if the i-th instance is in the j-th view. 0 otherwise.\n(1)\nwhere each column of M represents the instance presence in one view. Thus, in the incomplete views scenario,\u2211N\ni=1mi,j < N for j = 1, 2, ..., nv . Our goal is to partition all the N instances into K clusters by integrating all the nv incomplete views in an online fashion."}, {"heading": "B. Backgrounds", "text": "Let X \u2208 RD\u00d7N+ denote the nonnegative data matrix where each column represents an instance and each row represents a feature. Nonnegative matrix factorization (NMF) aims to factorize the data matrix X into two nonnegative matrices. We denote the two nonnegative matrices as U \u2208 RD\u00d7R+ and V \u2208 RN\u00d7R+ . Here R is the desired reduced dimensionality. To facilitate discussions, we call U the basis matrix and V\nthe latent feature matrix. The objective function of NMF can be formulated as below:\nmin U,V\nL = \u2016X\u2212UVT \u20162F s.t. U \u2265 0,V \u2265 0, (2)\nwhere \u2016\u00b7\u2016F is the Frobenius norm of the matrix. In clustering problems, the latent feature matrix V is used to extract the clustering solution. One option is to apply standard Kmeans on V to get the clustering solution. Another option is to add constraints to further restrict the rows of V and get the clustering indicators directly from V. For example, we can constrain \u2211 j vi,j = 1 for every row i. Thus, vi,j will become the probability that the i-th instance belongs to cluster j.\nIn general, it is difficult to solve Eq. (2) as the objective function is not convex with U and V jointly. A common solution is to use an alternating way to update U and V [2]. One of the most well-known algorithms for implementing the alternating update rules is the multiplicative update approach in [16], which iteratively updates U and V by\nUi,j \u2190 Ui,j (XV)i,j (UVTV)i,j > 0 Vi,j \u2190 Vi,j (X TU)i,j (VUTU)i,j > 0\nAnother algorithm that solves this problem is Projected Gradient Descent (PGD) [19]. By fixing V, PGD updates U using the first-order gradient:\nU\u2190 P [U\u2212 \u03b3k\u2207UL(U,V)] (3)\nwhere \u2207UL(U,V) is the gradient of L in Eq. (2) with respect to U, k is the index of the projected gradient iterations, \u03b3k is the step size and P is defined as\nP [ui,j ] = { ui,j , if ui,j \u2265 0 0, otherwise.\nSimilarly, PGD can be applied to update V with U fixed. PGD iteratively updates U and V until convergence. Both multiplicative update and PGD is proved to converge. However, both of them only use the first-order information, and thus the convergence rate is slow. To further accelerate the solving process, we borrow the idea from Newton\u2019s method [3] by utilizing the second-order information (i.e., Hessian matrix) in our paper. Thus, the update equation for U in Eq. (3) becomes:\nU\u2190 P [ U\u2212 \u03b3kH\u22121 [L(U,V)]\u2207UL(U,V) ] , (4)\nwhere H\u22121 [L(U,V)] is the inverse of the Hessian matrix. Similarly, we can apply the second-order PGD to update V with U fixed. We iteratively updates U and V until convergence."}, {"heading": "III. ONLINE MULTI-VIEW CLUSTERING", "text": "The proposed online multi-view clustering algorithm processes the data in a streaming fashion with low computational and storage complexity. We will first describe how to derive the objective function."}, {"heading": "A. Objective of OMVC", "text": "Given a set of incomplete multi-view data {X(v) \u2208 RDv\u00d7N+ , v = 1, 2, ..., nv}, we aim to find the latent feature matrices for each of the view and a common consensus, which represents the integrated information of all the views. The objective function can be written as below:\nmin {U(v)},{V(v)},V\u2217\nL = nv\u2211\nv=1\n\u2016X(v) \u2212U(v)V(v)T \u20162F + nv\u2211\nv=1\n\u03b1v\u2016V(v) \u2212V\u2217\u20162F\ns.t. V\u2217 \u2265 0,U(v) \u2265 0,V(v) \u2265 0, v = 1, 2, .., nv. (5) where U(v) \u2208 RDv\u00d7K+ and V(v) \u2208 RN\u00d7K+ are the basis matrix and latent feature matrix for the v-th view, V\u2217 \u2208 RN\u00d7K+ is the consensus latent feature matrix across all the views, K is the number of clusters and \u03b1v is the tradeoff parameter between reconstruction error and disagreement between view v and the consensus.\nDue to the incompleteness of each view, we cannot directly optimize the above objective function. One simple solution is to fill the missing instances with average value of the features first, and then solve the above objective function. However, this approach depends on the quality of the filled instances. For small incomplete percentages, the quality of the information contained in the filled features may be sufficient, whereas when the number of missing instance increases, the quality is often poor or even misleading. Thus, simply filling the missing instances cannot solve the problem. To eliminate the influence of the incomplete data, we borrow the idea from weighted NMF. We introduce a diagonal weight matrix W(v) \u2208 RN\u00d7N , whose diagonal element w(v)i,i represents the weight of the i-th instance in the v-th view. We give weight 1 to the instances that appear in the view, and give lower weight to the missing instances (average filled instances) in the view. We will discuss how to dynamically adjust the weight later in this section. The objective function after adding the weight matrices is:\nL = nv\u2211\nv=1\n\u2016(X(v) \u2212U(v)V(v)T )W(v)\u20162F + nv\u2211\nv=1 \u03b1v\u2016W(v)(V(v) \u2212V\u2217)\u20162F (6)\nBy assigning different weights to instances in difference views, we can give larger weights to more informative estimations of the missing instances and lower weights to less informative or misleading estimations. Additionally, considering the nature of incomplete views, we adopt `1 norm to enforce the sparsity of the latent feature matrix, which is robust to noises and outliers and widely used in many algorithms [12, 17].\nL = nv\u2211\nv=1\n\u2016(X(v) \u2212U(v)V(v)T )W(v)\u20162F\n+\nnv\u2211\nv=1\n\u03b1v\u2016W(v)(V(v) \u2212V\u2217)\u20162F + nv\u2211\nv=1\n\u03b2v\u2016V(v)\u20161 (7)\nwhere \u2016 \u00b7\u20161 is the `1 norm and \u03b2v is the trade-off parameter\nbetween the sparsity and accuracy of reconstruction for the v-th view.\nIn real-world applications, the data matrices may be too large to fit into the memory. We propose to solve the above optimization problem in an online/streaming fashion with low computational and storage complexity. Note that the objective function L can be decomposed as:\nL = nv\u2211\nv=1\nN\u2211\ni=1\n\u2016w(v)i,i (x (v) i \u2212U(v)v (v) i )\u20162F\n+\nnv\u2211\nv=1\nN\u2211\ni=1\n\u03b1v\u2016w(v)i,i (v (v) i \u2212 v\u2217i )\u20162F +\nnv\u2211\nv=1\nN\u2211\ni=1\n\u03b2v\u2016v(v)i \u20161 (8)\nwhere x(v)i is the i-th column of X (v) and v(v)i \u2208 RK is the i-th column of V(v)T . Clearly, when all the basis matrices U(v) are fixed, the calculation of v(v)i and v \u2217 i is independent for different i. This property would allow us approximate the optimal solution by processing the data one by one (chunk by chunk). Let\u2019s split the input data into chunks and at time t, we process a chunk of data points in all the views {X(v)t \u2208 Rs\u00d7Dv}, where s is the size of the data chunk (number of instances). Eq. (8) can be written as:\nL = nv\u2211\nv=1\ndN/se\u2211\nt=1\n\u2016(X(v)t \u2212U(v)V(v)t T )W (v) t \u20162F\n+\nnv\u2211\nv=1\ndN/se\u2211\nt=1\n\u03b1v\u2016W(v)t (V(v)t \u2212V\u2217t )\u20162F + nv\u2211\nv=1\ndN/se\u2211\nt=1\n\u03b2v\u2016V(v)t \u20161\n(9)\nwhere X(v)t is the t-th data chunk in the v-th view, V (v) t \u2208 Rs\u00d7K is the latent feature matrix for the t-th data chunk, and W(v)t \u2208 Rs\u00d7s is the diagonal weight matrix for the t-th data chunk."}, {"heading": "B. Dynamic Weight Setting for Missing Instances", "text": "In the previous discussion, we mentioned that we will fill the missing instances with the average values of the features and assign different weights to them. We would like to assign lower weights to the less informative estimations (averaged instances) and higher weights to the more informative estimations. However, since the entire data cannot be held in memory, the data can only be read in a streaming fashion. Thus, the average values cannot be directly calculated. Instead of filling the missing instances with the global average values, we fill the missing instances with the dynamic (up-to-date) average when we read in a new data point/chunk:\nx (v) t =\n\u2211t i=1mi,vx\n(v) i\u2211t\ni=1mi,v , (10)\nwhich can be calculated efficiently for every incoming missing instances. The weight w(v)t,t is set dynamically to the up-to-date percentage of the available instances in view v:\nw (v) t,t =\n{ 1 if instance t appears in view v. \u2211t\ni=1 mi,v t\notherwise. (11)\nWe can observe that w(v)t,t is lower if the estimate of x (v) t is made under a higher percentage of missing instances. Thus, w\n(v) t,t represents the quality of the estimated average features. Next, we will describe how to optimize the objective function in an online fashion."}, {"heading": "IV. OPTIMIZATION ALGORITHMS", "text": "In this section, we first solve the objective function of OMVC derived in the previous section. Then, we will discuss the one-pass OMVC and the multi-pass OMVC algorithms. The convergence and complexity analysis will be given in the end of this section."}, {"heading": "A. Solution", "text": "From Eq. (9) we can see that at each time t, we need to optimize {U(v)}, {V(v)t } and V\u2217t . However, the objective function is not jointly convex, so we have to update {U(v)}, {V(v)t } and V\u2217t in an alternating way. Thus, there are three subproblems in OMVC described as follows.\n1) Optimize {U(v)} with {V(v)t } and V\u2217t Fixed: From Eq. (9) we can observe that the optimization of U(v) is independent for different v with {V(v)t } and V\u2217t fixed. To optimize U(v) for a specific view v at time t, we only need to minimize the following objective:\nJ (t)(U(v)) = t\u2211\ni=1\n\u2016(X(v)i \u2212U(v)V (v) i\nT )W\n(v) i \u20162F\ns.t. U(v) \u2265 0 (12)\nTaking the first-order derivative, the gradient of J (t) with respect to U(v) is\n\u2207J (t)(U(v)) =2U(v) t\u2211\ni=1\nV (v) i\nT W\u0303\n(v) t V (v) i \u2212 2\nt\u2211\ni=1\nX (v) i W\u0303 (v) t V (v) i\n(13) Here, W\u0303(v)t = W (v) t T W (v) t = W (v) t W (v) t T . For the sake of convenience, we introduce two terms A(v)t and B (v) t :\nA (v) t =\nt\u2211\ni=1\nV (v) i\nT W\u0303\n(v) t V (v) i B (v) t =\nt\u2211\ni=1\nX (v) i W\u0303 (v) t V (v) i\nwhich can be computed incrementally with low storage. Thus, Eq. (13) can be written as:\n\u2207J (t)(U(v)) =2U(v)A(v)t \u2212 2B(v)t (14)\nTherefore, the Hessian matrix of J (t)(U(v)) with respect to U(v) is\nH [ U(v) ] = 2A\n(v) t (15)\nUsing the second order PGD, the update equation for U(v) at time t is:\nU (v) k+1 \u2190 P\n[ U\n(v) k \u2212 \u03b3k\u2207J (t)(U (v) k )H\u22121[U (v) k ] ]\n(16)\nwhere k is the number of iterations, and \u03b3k is the step size.\nFor choosing an appropriate step size \u03b3k, we consider the simple and effective Armijo rule along the projection described in [2], that is, \u03b3k = \u03b7\u03d5k , and \u03d5k is the first nonnegative integer such that\nJ (t)(U(v)k+1)\u2212 J (t)(U (v) k ) \u2264 \u03c3\u3008\u2207J (t)(U (v) k ),U (v) k+1 \u2212U (v) k \u3009\n(17) where \u03c3 \u2208 (0, 1) and \u3008\u00b7, \u00b7\u3009 is the sum of the component-wise product of two matrices. The condition (17) ensures that the sufficient decrease of the function value per iteration. Bertsekas [3] has proved that by selecting the step sizes 1, \u03b2, \u03b22, \u00b7 \u00b7 \u00b7 , \u03b3k > 0 satisfying (17) always exists and every limit point of {U(v)k } is a stationary point of (12).\nFollowing [19], to reduce the computational cost, inequality (17) can be reformulated as:\n(1\u2212 \u03c3)\u3008\u2207J (t)(U(v)k ),U (v) k+1 \u2212U (v) k \u3009 + 1\n2 \u3008U(v)k+1 \u2212U (v) k ,H(U (v) k )(U (v) k+1 \u2212U (v) k )\u3009 \u2264 0\n(18)\n2) Optimize {V(v)t } with V\u2217t and {U(v)} Fixed: Given V\u2217t and {U(v)} fixed, the optimization of {V(v)t } is independent for different v. To optimize V(v)t for the v-th view, we only need to minimize the following objective:\nJ (V(v)t ) = \u2016(X(v)t \u2212U(v)V(v)t T )W (v) t \u20162F\n+ \u03b1v\u2016W(v)t (V(v)t \u2212V\u2217t )\u20162F + \u03b2v\u2016V(v)t \u20161 s.t. V(v)t \u2265 0 (19)\nTaking the first-order derivative, the gradient of J with respect to V(v)t is\n\u2207J (V(v)t ) =2W\u0303(v)t (V(v)t U(v)T \u2212X(v) T )U(v)\n+ 2\u03b1vW\u0303 (v) t (V (v) t \u2212V\u2217t ) + \u03b2v1\n(20)\nLet v(v)ti be the i-th column of V (v) t T and w(v)ti,ti be the ith diagonal element of W(v)t , then the Hessian matrix of J (v(v)ti ) with respect to v (v) ti is\nH [ v (v) ti ] = 2w (v) ti,ti 2 (U(v)TU(v) + \u03b1vIK) (21)\nUsing the second-order PGD, the update equation for v(v)ti is:\nv (v) ti \u2190 P [ v (v) ti \u2212 \u03b3H\u22121[v (v) ti ]\u2207J (v (v) ti ) ]\n(22)\nHere, \u03b3 is the step size. We use the similar search procedure as the previous subsection to find the step size that satisfies the Armijo rule.\n3) Optimize V\u2217t with {U(v)} and {V(v)t } Fixed: To optimize V\u2217t with {V(v)t } and {U(v)} fixed, we only need to minimize the following objective function:\nJ (V\u2217t ) = nv\u2211\nv=1\n\u03b1v\u2016W(v)t (V(v)i \u2212V\u2217i )\u20162F s.t. V\u2217t \u2265 0 (23)\nAlgorithm 1: One-pass OMVC with mini-batch mode. Input: Data matrices of all the incomplete views {X(v)}.\nThe number of clusters K, the batch size m. Parameters {\u03b1v} and {\u03b2v}.\n1 A (v) 0 = 0, B (v) 0 = 0 for each view v. for t = 1 : dN/se do 2 Draw X(v)t for all the views. 3 Fill in the missing instances and set the weights. repeat 4 for v = 1 : nv do 5 Update U(v) according to Eq. (16). 6 Update V(v)t according to Eq. (22). 7 Calculate V\u2217t according to Eq. (25). 8 until Convergence; 9 A\n(v) t = A (v) t\u22121 +V (v) t\nT W\u0303\n(v) t V (v) t\n10 B (v) t = B (v) t\u22121 +X (v) t W\u0303 (v) t V (v)T t 11 Extract clustering solution from V\u2217t\nHere, we assume that \u03b1v is positive. Taking the derivative of the objective J in Eq. (23) over v\u2217t and set it to 0:\n\u2202J \u2202V\u2217t =\nnv\u2211\nv=1\n2\u03b1vW\u0303 (v) t V \u2217 t \u2212\nnv\u2211\nv=1\n2\u03b1vW\u0303 (v) t V (v) t = 0 (24)\nSince W(v)t is a positive diagonal matrix and \u03b1v is positive,\u2211nv v=1 \u03b1vW\u0303 (v) t = \u2211nv v=1 \u03b1vW (v) t T W (v) t is also a positive diagonal matrix, whose inverse can be quickly calculated. Solving Eq. (24), we have an exact solution for V\u2217t :\nV\u2217t =\n( nv\u2211\nv=1\n\u03b1vW\u0303 (v) t\n)\u22121 nv\u2211\nv=1\n\u03b1vW\u0303 (v) t V (v) t \u2265 0 (25)"}, {"heading": "B. One-Pass OMVC", "text": "The complete one-pass algorithm procedure is shown in Algorithm 1. Several important points need to be noted. First, at each time t, we do not need to recompute new A\n(v) t and B (v) t . We only need to compute V (v) i\nT W\u0303\n(v) t V (v) i\nand X(v)i W\u0303 (v) t V (v)T i , and add them to old A (v) t\u22121 and B (v) t\u22121.\nA (v) t = A (v) t\u22121 +V (v) i\nT W\u0303\n(v) t V (v) i (26)\nB (v) t = B (v) t\u22121 +X (v) i W\u0303 (v) t V (v) i (27)\nSecond, in the algorithm, we need to calculate the inverse of Hessian matrix (with dimension K \u00d7K) in the iteration. The computation cost of inverse of a matrix will be high if the number of clusters K becomes large. However, in most of the cases, the number of clusters is limited to a small number. Even if K is very large in some cases, we can use other ways to approximate the inverse, such as the diagonal approximation [30]. Third, the proposed alternative update procedures for {U(v),V(v)t ,V\u2217t } converge. The proof of convergence is shown in Section IV-D."}, {"heading": "C. Multi-Pass OMVC", "text": "In data stream scenario, often only one pass over the data is available. In many other applications, it is feasible to do multiple passes. In the one-pass OMVC, the consensus latent feature matrix V\u2217, which represents the clustering assignment/possibility, is computed in a sequential greedy way. It is expected that for the data points that come first, the performance of clustering may not be satisfactory. However, multi-pass OMVC gives a chance to improve the performance for those earlier data points.\nIn the multi-pass OMVC, {V(v)} and V\u2217 can be updated using {U(v)} in the previous pass. {A(v)} and {B(v)} from previous pass can be used and updated. Also, the weights for missing instances will be more accurate after the first pass. Thus, the performance of clustering after multiple passes is expected to be better than that of one-pass OMVC."}, {"heading": "D. Convergence and Complexity", "text": "The convergence of the proposed algorithm OMVC can be illustrated by the following theorem.\nTHEOREM 1: Any limit point of the sequence {U(v)k ,V (v) tk , V \u2217 tk} generated by Algorithm 1 is a stationary point of Eq. (19). Proof: The Algorithm 1 can be viewed as the \u201cblock coordinate descent\u201d method in bound-constrained optimization [2], where sequentially one block of variables is minimized under corresponding constraints and the remaining two blocks are fixed. Regarding the convergence of \u201cblock coordinate descent\u201d methods, Grippo and Sciandrone [10] have shown that for the case of three blocks, the convergence can be ensured by requiring only the strict quasiconvexity of the objective function with respect to one component. Clearly, we can satisfy this condition over V\u2217(t). Therefore, the proof of Theorem 1 is an immediate consequence of Proposition 5 of [10].\nNext, we discuss the computational complexity of OMVC algorithm. There are three subproblems for OMVC algorithm: optimizing U(v), optimizing V(v)t and optimizing V \u2217 t .\nTo optimize U(v), we need to calculate the gradient \u2207J (U(v)) and the Hessian matrix H[U(v)]. Assume that K Dv and K < s. From Eq. (14) and Eq. (15), we can see that the complexity is O(KDvs) for the gradient and O(K2s) for the Hessian, where K is the number of clusters, Dv is the feature dimension in the v-th view and s is the size of data chunk. Also, the complexity to update U(v) using gradient and Hessian is O(K3 +DvK2), where K3 is the time complexity for the inversion of Hessian matrix. Since K is usually very small, the inverse will be done very quickly. So the complexity for updating U(v) once is O(KDvs). According to [19], the complexity for searching the step size satisfying Eq. (18) is tinK2Dv , where tin is the number of iterations to find the step size. So the overall complexity for updating U(v) is O(KDvs + tinK2Dv) < O(tinKDvs).\nSimilarly, we can find that the overall time complexity for updating V(v)t is O(KDvs+ tinK2s) < O(tinKDvs).\nAnother subproblem of OMVC is optimizing V\u2217t . From Eq. (25), we can see that it takes O(nvKs) to calculate V\u2217t , where nv is the number of views. Thus, the complexity for one data chunk in one pass will be O(touttinnvKDs + toutnvKs) = O(touttinnvKDs), where tout is the average number of iterations to update V(v)t and U\n(v) and D is the average feature dimension for multiple views. Thus, the overall time complexity of one pass OMVC is O(touttinnvKDN), where N is the total number of instances.\nThe complexity of some most recent NMF based off-line multi-view clustering methods such as MultiNMF [20] and MIC [25] are O(touttinnvKDN), which is in the same order as OMVC. However, all the off-line algorithms require O(nvDN) memory space. When the data are too big to fit in the memory, the off-line algorithms will not work. The proposed OMVC only requiresO(nvDs) memory space (s N ), which makes it work for really large data.\nWhen the data size is large, the IO cost can be a significant (sometimes dominating) portion of the total cost. Our experiment results will verify that we often do not need many passes to achieve very accurate results."}, {"heading": "V. EXPERIMENT", "text": ""}, {"heading": "A. Dataset", "text": "In this paper, two small datasets and two large datasets are used to evaluate the proposed method OMVC. The summary of the datasets is shown in Table II, and the details of the datasets are as follows:\n\u2022 Web Knowledge Base (WebKB)1: It is a subset of web documents from four universities [27]. The data contains 1,051 documents from two classes, course or non-course. Each document has two views: 1) the textual content of the web page and 2) the anchor text on links in other web pages pointing to the web page. \u2022 Handwritten Dutch Digit Recognition (Digit)2: This data contains 2,000 handwritten numerals (\u201c0\u201d-\u201c9\u201d) extracted from a collection of maps. Five views are used in our experiments: (1) 76 Fourier coefficients of the character shapes, (2) 216 profile correlations, (3) 64 Karhunen-Loeve coefficients, (4) 240 pixel averages in 2\u00d7 3 windows, (5) 47 Zernike moments. \u2022 Reuters Multilingual Text Data (Reuters)3: This data contains features of 111,740 documents originally written in five different languages (English, French, German, Spanish and Italian), and their translations, over a common set of 6 topic categories [1].\n1http://vikas.sindhwani.org/manifoldregularization.html 2http://archive.ics.uci.edu/ml/datasets/Multiple+Features 3http://archive.ics.uci.edu/ml/machine-learning-databases/00259/\nTable II: Summary of the datasets\nDataset # of instances views # of clusters WebKB 1,051 Content(3,000), Anchor text (1,840) 2\nDigit 2,000 Fourier (76), Profile (216), Karhunen-Loeve (64), Pixel (240), Zernike (47) 10 Reuters 111,740 English (21,531), French (24,893), German (34,279), Spanish (15,506), Italian (11,547) 6\nYouTube 92,457 Vision (512), Audio (2,000), Text (1,000) 31\nPass\n0 5 10 15 20 25\nN M\nI\n0\n0.1\n0.2\n0.3\n0.4\nOMVC MIC MultiNMF ONMF-DA ONMF-I\nFigure 1: NMI on WebKB Pass 0 5 10 15 20 25\nA C\n0.5\n0.6\n0.7\n0.8\n0.9\nOMVC MIC MultiNMF ONMF-DA ONMF-I\nFigure 2: AC on WebKB Pass 0 5 10 15 20 25\nN M\nI\n0.2\n0.3\n0.4\n0.5\n0.6\nOMVC MIC MultiNMF ONMF-DA ONMF-I\nFigure 3: NMI on Digit Pass 0 5 10 15 20 25\nA C\n0.2\n0.3\n0.4\n0.5\n0.6\nOMVC MIC MultiNMF ONMF-DA ONMF-I\nFigure 4: AC on Digit\nFigure 6: AC on Reuters Pass 0 5 10 15\nN M\nI\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\nOMVC ONMF-DA ONMF-I"}, {"heading": "B. Comparison Methods", "text": "We compare OMVC with several state-of-the-art methods. The differences between these comparison methods are summarized in Table III, and the details of comparison methods are as follows: \u2022 OMVC: OMVC is the proposed online multi-view\nclustering method in this paper. To facilitate comparison, we set \u03b1vs (\u03b2vs) to be the same for all the views. \u2022 MultiNMF: MultiNMF is the state-of-the-art off-line multi-view clustering method based on joint nonnegative matrix factorization [20] for complete views. \u2022 MIC: MIC is one of the most recent works that solve the off-line multi-view clustering problem with\n4https://archive.ics.uci.edu/ml/datasets/YouTube+Multiview +Video+Games+Dataset\nIt is worth mentioning that MIC and MultiNMF are off-line methods which take all data into consideration and can often achieve better performance than on-line methods."}, {"heading": "C. Experiment Setup", "text": "In our experiments, two widely used evaluation metrics, accuracy (AC) and normalized mutual information (NMI), are used to measure the clustering performance [31]. Note that all the four datasets are complete. In order to simulate situations with missing instances, we randomly delete instances (0% to 40%) from each view to make the views incomplete. Since all the methods except ONMF have several parameters, we do a grid search for all the parameters in the comparison methods and present the best results obtained. Furthermore, MultiNMF, ONMF-I and ONMF-DA cannot handle incomplete views. In order to apply these methods, we fill the missing instances with average features. In the evaluation, we use K-means to get the clustering solution from the consensus latent feature matrix. Since K-means depends on initialization, we repeat clustering 20 times with random initialization and report the average performance."}, {"heading": "D. Results", "text": "To show the performance of proposed mult-pass OMVC, we randomly deleted 40% of the instances in each view for all the datasets, and run the comparison methods. The chunk size s for online methods is set to 50 for small datasets and 2000 for large datasets. We report both NMI and AC for different passes. The results are shown in Figs. 1-8 and the run times are reported in Table IV. It is worth noting that both MIC and MultiNMF are off-line methods with one pass, so the NMI and AC are two horizontal lines in the figures.\nFrom Figs. 1-4 on WebKB and Digit, we can observe that for the three online methods (OMVC, ONMF-DA and ONMF-I), both NMI and AC increase as the number of passes increases. The performance of the online methods converges as the number of passes increases. Although the off-line method MIC achieves the best performance, the proposed OMVC gets close performance within a few passes and outperforms the other three comparison methods by a large margin. Even in the first pass, the proposed OMVC already outperforms the other two online methods and even the off-line MultiNMF. This shows that even onepass OMVC can achieve reasonable performance. From these figures, we can see that the comparison methods can be grouped into two groups by the performance. The first group, MIC and OMVC, achieves better performance than the other three methods. It is because that both MIC and OMVC utilize a weight matrix for each view to eliminate the influence of the incomplete data and enforce the sparsity of the latent features, while the other three methods do not consider the incompleteness and sparsity of the data.\nFigs. 5-8 demonstrate the performance on the two large data datasets, Reuters and YouTube. However, as the data is too large, the two off-line methods (MIC and MultiNMF) cannot be applied. We only report the NMI and AC for the three online methods. In the four figures, OMVC outperforms the other two online methods in all the passes.\nFrom Figs. 1-8, we can conclude that the proposed OMVC outperforms all the other online methods and perform on par with the best off-line method within a small number of passes. Another interesting observation we can get from the figures is that ONMF-I performs better than ONMF-DA on all the datasets except for YouTube, which indicates that the diagonal approximation sacrifices the accuracy for the computation efficiency in the three datasets.\nWe also reported the run time for the comparison methods on the four datasets in Table IV. From the table, we can see that all the three online methods are much faster than the\ntwo off-line methods. Although the ONMF-DA and ONMF-I run faster than the proposed OMVC, OMVC achieves much better performance than ONMF-DA and ONMF-I.\nIn OMVC, we need to set the size of data chunk. In order to study the performance of OMVC with different chunk sizes, we conducted another set of experiments. Moreover, to show how the instances incomplete rate affects the performance, We ran the comparison methods with different chunk sizes on WebKB and Digit with different incomplete rates and report the NMI after 10 passes in Table V and Table VI. From the tables, we can first observe that OMVC outperforms the other online methods in all the cases and is very close to the best off-line method, if not better. If we look at the performance for different incomplete rates, we can see that as the incomplete rate increases, the performance for all the methods decreases. It is because as the incomplete rate increases, the useful information contains in each view decreases, and all the methods suffer from the incompleteness of views. We can also observe that for each incomplete rate, when the chunk size is too small (e.g., s = 2), all the online methods show low performance. This is because the more data in one chunk, the more information we can use to improve the performance. When s is large enough (larger than K), the performance of online methods will improve. From the tables, we can see that when s is 50 or 250, the performance is already pretty close to the best off-line method. Also, using larger chunks means fewer iterations, which reduces the IO cost significantly comparing with using smaller chunks."}, {"heading": "E. Convergence Study", "text": "We use the average loss to measure the performance of OMVC after reading each data chunk t in each passes. The average loss is defined as follows:\nL(t) = 1 min{s\u00d7 t,N}\nnv\u2211\nv=1\nt\u2211\ni=1\n( P\n(v) i + \u03b1vQ (v) i + \u03b2v\u2016V (v) i \u20161\n)\n(28)\nwhere P (v)i = \u2016(X (v) i \u2212U(v)V (v) i\nT )W\n(v) i \u20162F is the recon-\nstruction error for the i-th data chunk in v-th view and Q\n(v) i = \u2016W (v) i (V (v) i \u2212V\u2217i )\u20162F is the distance between the latent features for the i-th data chunk in the v-th view and the common consensus.\nWe run OMVC on all the four datasets with 40% incomplete views and report the average loss in Figs. 9-12. From Fig. 9 for WebKB and Fig. 10 for YouTube, we can observe that for each pass, the average loss goes up first and then slowly drops to a certain value. If we compare the lines for different passes, we can see that, at the end of each pass, all the lines converges to one value and the average loss for later pass is lower than the previous pass. From Fig. 11 for Digit and Fig. 12 for Reuters, we can clearly observe that for each pass, as more chunks of data come, the average loss drops and converges. At the end of each pass, the average\nTable VI: NMI on Digit with different incomplete rates and chunk sizes\n0% 20% 40%\ns = 2 s = 10 s = 50 s = 250 s = 2 s = 10 s = 50 s = 250 s = 2 s = 10 s = 50 s = 250 OMVC 0.4631 0.7203 0.7303 0.7258 0.3240 0.6590 0.6614 0.6527 0.2594 0.4635 0.4885 0.4976 ONMF-DA 0.2762 0.2994 0.4218 0.4195 0.3752 0.2933 0.6118 0.3600 0.2205 0.1473 0.3103 0.3136 ONMF-I 0.2545 0.3142 0.6735 0.6688 0.1892 0.1792 0.3318 0.3102 0.2588 0.3146 0.3785 0.4138 MIC 0.7305 0.6569 0.4903 MultiNMF 0.7313 0.6412 0.4304\n# of Chunks 0 5 10 15 20\nA v e ra\ng e L\no s s\n\u00d710 -7\n1.5\n1.6\n1.7\n1.8\n1.9\n2\nOMVC-Pass 1 OMVC-Pass 2 OMVC-Pass 3 OMVC-Pass 4\nFigure 9: Average Loss vs # of chunks on WebKB # of Chunks 0 10 20 30 40 50\nA v e ra\ng e L\no s s\n\u00d710 -11\n1.5\n1.6\n1.7\n1.8\n1.9\n2\n2.1\n2.2\nOMVC-Pass 1 OMVC-Pass 2 OMVC-Pass 3 OMVC-Pass 4\nFigure 10: Average Loss vs # of chunks on YouTube # of Chunk 0 10 20 30 40\nA v e ra\ng e L\no s s\n\u00d710 -9\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\nOMVC-Pass 1 OMVC-Pass 2 OMVC-Pass 3 OMVC-Pass 4 OMVC-Pass 5\nFigure 11: Average Loss vs # of chunks on Digit # of Chunks 0 10 20 30 40 50 60\nA v e ra\ng e L\no s s\n\u00d710 -11\n1.05\n1.1\n1.15\n1.2\n1.25\n1.3 1.35 OMVC-Pass 1 OMVC-Pass 2 OMVC-Pass 3 OMVC-Pass 4\nFigure 12: Average Loss vs # of chunks on Reuters\nlosses for later passes are lower than previous passes and converge to a certain value."}, {"heading": "F. Sensitivity Analysis", "text": "There are two sets of parameters in the proposed methods: {\u03b1v} and {\u03b2v}. Here we explore the effects of the two parameter sets. As mentioned in the previous section, we set \u03b1v (\u03b2v) to be the same for all the views. For simplicity, assume that \u03b1v = \u03b1 and \u03b2v = \u03b2 for all the views. We run OMVC with different values for \u03b1 and \u03b2 on WebKB and Digit data. To save space, we only show the results w.r.t. NMI in Fig. 13 since we have similar observations in AC.\nFrom Fig. 13a, we can observe that OMVC achieves the best performance when \u03b1 is about 10\u22122 and \u03b2 is about 10\u22127. Parameter \u03b1 controls the importance of coregularization between views and the consensus. When it becomes too small, the consensus has little contribute to the learning of each view, and the performance will decline. When \u03b1 increases, the consensus has too much influence to each view and the performance will drop. Parameter\n\u03b2 controls the sparsity of the latent feature matrices. We can observe that when it is too small, we barely enforce the sparsity, and thus the performance decline. When it is too large, most of the entries in the latent feature matrices will be 0, and the performance will drop. We can have similar observation from Fig. 13b. These results show that an appropriate combination of these two parameters is crucial for OMVC to improve the performance."}, {"heading": "VI. RELATED WORK", "text": "Multi-view clustering [4] provides a natural way for generating clusters from multi-view data. In the introduction, we have discussed four categories of multi-view clustering algorithms. Here we particularly address several subspace based multi-view clustering methods [8, 20, 25, 29]. [8] proposed a CCA based multi-view clustering method to learn the subspace, in which the correlations among views are maximized. [20] approached the problem by learning a common consensus based on a co-regularized joint NMF framework. Recently, [29] proposed to solve multi-view clustering with at least one complete view based on CCA. Later, [25, 17] proposed two NMF based methods that can solve multi-view clustering even without any complete view. Although various methods have been proposed to integrate heterogeneous views, none of the previous methods can handle large-scale data that cannot fit into the memory.\nNonnegative matrix factorization [16], especially online NMF is the second area that is related to our work. Since traditional NMF cannot deal with really large data, online\nNMF was first proposed to handle really large data or streaming data [6]. Different variations were proposed in the last few years. For example, [11] proposed an efficient online NMF algorithm that takes one chunk of samples per step and updates the bases via robust stochastic approximation. [30] proposed an online NMF algorithm for document clustering. It utilizes the second-order Hessian matrix to optimize the objective incrementally. [24] added graph regularization to an online joint NMF framwork for multi-view feature selection. However, none of them can deal with multiple incomplete views. OMVC uses a weighted joint NMF model to handle the incompleteness of the views and enforces the sparseness of the learned latent feature matrices."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we present possibly the first attempt to solve the online multi-view clustering with incomplete views where each view may suffer from missing some instances. Based on NMF, the proposed OMVC learns the latent feature matrices for each individual incomplete view and pushes them towards a common consensus. To achieve the goal, a joint NMF algorihm is used to not only incorporate individual matrix factorizations but also minimize the disagreement between the latent feature matrices and the consensus. By giving missing instances lower weights dynamically, OMVC minimizes the negative influences of the missing data. OMVC also enforces the sparsity of the learned latent feature matrices by introducing lasso regularization, which makes the method robust to noises and outliers. Most important, OMVC does not require holding the entire data matrix into memory, which reduces the space complexity dramatically. It processes the data one by one (or chunk by chunk), learns the latent feature and updates the basis matrix simultaneously. Extensive experiments conducted on both small and large real data demonstrate the effectiveness of the proposed OMVC method comparing with other stateof-the-art methods."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported in part by NSF through grants IIS-1526499, and CNS-1626432. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research."}], "references": [{"title": "Learning from multiple partially observed views - an application to multilingual text categorization", "author": ["M.R. Amini", "N. Usunier", "C. Goutte"], "venue": "NIPS", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear Programming", "author": ["D. Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Projected newton methods for optimization problems with simple constraints", "author": ["D.P. Bertsekas"], "venue": "SIAM Journal on control and Optimization, 20(2):221\u2013246", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1982}, {"title": "Multi-View Clustering", "author": ["S. Bickel", "T. Scheffer"], "venue": "ICDM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Multiview Clustering: A Late Fusion Approach Using Latent Models", "author": ["E. Bruno", "S. Marchand-Maillet"], "venue": "SIGIR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Detect and Track Latent Factors with Online Nonnegative Matrix Factorization", "author": ["B. Cao", "D. Shen", "J.-T. Sun", "X. Wang", "Q. Yang", "Z. Chen"], "venue": "IJCAI", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Diversityinduced multi-view subspace clustering", "author": ["X. Cao", "C. Zhang", "H. Fu", "S. Liu", "H. Zhang"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view Clustering via Canonical Correlation Analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "A Matrix Factorization Approach for Integrating Multiple Data Views", "author": ["D. Greene", "P. Cunningham"], "venue": "ECML PKDD", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "On the convergence of the block nonlinear gauss\u2013seidel method under convex constraints", "author": ["L. Grippo", "M. Sciandrone"], "venue": "Operations Research Letters, 26(3):127\u2013136", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Online nonnegative matrix factorization with robust stochastic approximation", "author": ["N. Guan", "D. Tao", "Z. Luo", "B. Yuan"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, 23(7):1087\u20131099", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-negative sparse coding", "author": ["P. Hoyer"], "venue": "12th IEEE Workshop on Neural Networks for Signal Processing, pages 557\u2013565. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Weighted Nonnegative Matrix Factorization", "author": ["Y. Kim", "S. Choi"], "venue": "ICASSP", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "A Co-training Approach for Multiview Spectral Clustering", "author": ["A. Kumar", "H.D. III"], "venue": "ICML", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Co-regularized Multi-view Spectral Clustering", "author": ["A. Kumar", "P. Rai", "H.D. III"], "venue": "NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning the Parts of Objects by Nonnegative Matrix Factorization", "author": ["D. Lee", "S. Seung"], "venue": "Nature, 401:788\u2013791", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Partial Multi-View Clustering", "author": ["S. Li", "Y. Jiang", "Z. Zhou"], "venue": "AAAI", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Sound field reproduction using the lasso", "author": ["G. Lilis", "D. Angelosante", "G. Giannakis"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 18(8):1902\u20131912", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["C. Lin"], "venue": "Neural computation, 19(10):2756\u20132779", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-View Clustering via Joint Nonnegative Matrix Factorization", "author": ["J. Liu", "C. Wang", "J. Gao", "J. Han"], "venue": "SDM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A General Model for Multiple View Unsupervised Learning", "author": ["B. Long", "P.S. Yu", "Z. Zhang"], "venue": "SDM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "On using nearlyindependent feature families for high precision and confidence", "author": ["O. Madani", "M. Georg", "D.A. Ross"], "venue": "Machine Learning, 92:457\u2013477", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Edge weight regularization over multiple graphs for similarity learning", "author": ["P. Muthukrishnan", "D. Radev", "Q. Mei"], "venue": "ICDM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Online unsupervised multi-view feature selection", "author": ["W. Shao", "L. He", "C.-T. Lu", "X. Wei", "P.S. Yu"], "venue": "ICDM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "and P", "author": ["W. Shao", "L. He"], "venue": "S. Yu. Multiple Incomplete Views Clustering via Weighted Nonnegative Matrix Factorization with L2,1 Regularization. In ECML PKDD", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustering on multiple incomplete datasets via collective kernel learning", "author": ["W. Shao", "X. Shi", "P. Yu"], "venue": "ICDM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Beyond the point cloud: from transductive to semi-supervised learning", "author": ["V. Sindhwani", "P. Niyogi", "M. Belkin"], "venue": "ICML", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "A survey of multi-view machine learning", "author": ["S. Sun"], "venue": "Neural Computing and Applications, 23(7-8):2031\u20132038", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiview Clustering with Incomplete Views", "author": ["A. Trivedi", "P. Rai", "H. Daum\u00e9 III", "S. DuVall"], "venue": "NIPS 2010: Workshop on Machine Learning for Social Computing", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient document clustering via online nonnegative matrix factorizations", "author": ["F. Wang", "P. Li", "A. K\u00f6nig"], "venue": "SDM", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "SIGIR", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 27, "context": "By exploiting these characteristics between multi-view data, multi-view learning can obtain better performance of learning tasks than relying on just one single view [28].", "startOffset": 166, "endOffset": 170}, {"referenceID": 3, "context": "Multi-view clustering [4], as one of basic tasks of multiview learning, provides a natural way for generating clusters from multi-view data.", "startOffset": 22, "endOffset": 25}, {"referenceID": 27, "context": "Existing multi-view clustering algorithms can be roughly categorized into four categories [28].", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "Methods in the first category are based on subspace [8, 15, 20, 25], which learn a latent space so that different views are comparable in that space.", "startOffset": 52, "endOffset": 67}, {"referenceID": 14, "context": "Methods in the first category are based on subspace [8, 15, 20, 25], which learn a latent space so that different views are comparable in that space.", "startOffset": 52, "endOffset": 67}, {"referenceID": 19, "context": "Methods in the first category are based on subspace [8, 15, 20, 25], which learn a latent space so that different views are comparable in that space.", "startOffset": 52, "endOffset": 67}, {"referenceID": 24, "context": "Methods in the first category are based on subspace [8, 15, 20, 25], which learn a latent space so that different views are comparable in that space.", "startOffset": 52, "endOffset": 67}, {"referenceID": 3, "context": "Methods in the second category are co-training based algorithms [4, 14], which obtain the clustering results in an iterative manner.", "startOffset": 64, "endOffset": 71}, {"referenceID": 13, "context": "Methods in the second category are co-training based algorithms [4, 14], which obtain the clustering results in an iterative manner.", "startOffset": 64, "endOffset": 71}, {"referenceID": 6, "context": "The third category aims to learn a unified similarity matrix among multi-view data, which serves as affinity matrix for final clustering [7, 23].", "startOffset": 137, "endOffset": 144}, {"referenceID": 22, "context": "The third category aims to learn a unified similarity matrix among multi-view data, which serves as affinity matrix for final clustering [7, 23].", "startOffset": 137, "endOffset": 144}, {"referenceID": 4, "context": "The last category is called late fusion [5, 9, 21].", "startOffset": 40, "endOffset": 50}, {"referenceID": 8, "context": "The last category is called late fusion [5, 9, 21].", "startOffset": 40, "endOffset": 50}, {"referenceID": 20, "context": "The last category is called late fusion [5, 9, 21].", "startOffset": 40, "endOffset": 50}, {"referenceID": 16, "context": "In order to deal with this problem, different approaches have been explored [17, 25, 26, 29].", "startOffset": 76, "endOffset": 92}, {"referenceID": 24, "context": "In order to deal with this problem, different approaches have been explored [17, 25, 26, 29].", "startOffset": 76, "endOffset": 92}, {"referenceID": 25, "context": "In order to deal with this problem, different approaches have been explored [17, 25, 26, 29].", "startOffset": 76, "endOffset": 92}, {"referenceID": 28, "context": "In order to deal with this problem, different approaches have been explored [17, 25, 26, 29].", "startOffset": 76, "endOffset": 92}, {"referenceID": 28, "context": "[29] is the first to deal with incomplete views by utilizing information from one complete view to refer to the kernel of incomplete views.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17, 26] are among the first attempts to solve multiview clustering with none of the views complete.", "startOffset": 0, "endOffset": 8}, {"referenceID": 25, "context": "[17, 26] are among the first attempts to solve multiview clustering with none of the views complete.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[25, 17] are the first attempts to solve multiple incomplete views clustering based on nonnegative matrix factorization (NMF).", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "[25, 17] are the first attempts to solve multiple incomplete views clustering based on nonnegative matrix factorization (NMF).", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "Inspired by the idea of weighted NMF [13], we use a dynamic weight setting to give lower weights to the incoming missing instances in different views.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "As one of the most commonly used regularization, `1 (Lasso) regularization has been successfully applied in many algorithms [12, 17, 18].", "startOffset": 124, "endOffset": 136}, {"referenceID": 16, "context": "As one of the most commonly used regularization, `1 (Lasso) regularization has been successfully applied in many algorithms [12, 17, 18].", "startOffset": 124, "endOffset": 136}, {"referenceID": 17, "context": "As one of the most commonly used regularization, `1 (Lasso) regularization has been successfully applied in many algorithms [12, 17, 18].", "startOffset": 124, "endOffset": 136}, {"referenceID": 1, "context": "A common solution is to use an alternating way to update U and V [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 15, "context": "One of the most well-known algorithms for implementing the alternating update rules is the multiplicative update approach in [16], which iteratively updates U and V by", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "Another algorithm that solves this problem is Projected Gradient Descent (PGD) [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "To further accelerate the solving process, we borrow the idea from Newton\u2019s method [3] by utilizing the second-order information (i.", "startOffset": 83, "endOffset": 86}, {"referenceID": 11, "context": "Additionally, considering the nature of incomplete views, we adopt `1 norm to enforce the sparsity of the latent feature matrix, which is robust to noises and outliers and widely used in many algorithms [12, 17].", "startOffset": 203, "endOffset": 211}, {"referenceID": 16, "context": "Additionally, considering the nature of incomplete views, we adopt `1 norm to enforce the sparsity of the latent feature matrix, which is robust to noises and outliers and widely used in many algorithms [12, 17].", "startOffset": 203, "endOffset": 211}, {"referenceID": 1, "context": "For choosing an appropriate step size \u03b3k, we consider the simple and effective Armijo rule along the projection described in [2], that is, \u03b3k = \u03b7k , and \u03c6k is the first nonnegative integer such that", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "Bertsekas [3] has proved that by selecting the step sizes 1, \u03b2, \u03b2, \u00b7 \u00b7 \u00b7 , \u03b3k > 0 satisfying (17) always exists and every limit point of {U k } is a stationary point of (12).", "startOffset": 10, "endOffset": 13}, {"referenceID": 18, "context": "Following [19], to reduce the computational cost, inequality (17) can be reformulated as:", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "Even if K is very large in some cases, we can use other ways to approximate the inverse, such as the diagonal approximation [30].", "startOffset": 124, "endOffset": 128}, {"referenceID": 1, "context": "Proof: The Algorithm 1 can be viewed as the \u201cblock coordinate descent\u201d method in bound-constrained optimization [2], where sequentially one block of variables is minimized under corresponding constraints and the remaining two blocks are fixed.", "startOffset": 112, "endOffset": 115}, {"referenceID": 9, "context": "Regarding the convergence of \u201cblock coordinate descent\u201d methods, Grippo and Sciandrone [10] have shown that for the case of three blocks, the convergence can be ensured by requiring only the strict quasiconvexity of the objective function with respect to one component.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "Therefore, the proof of Theorem 1 is an immediate consequence of Proposition 5 of [10].", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": "According to [19], the complexity for searching the step size satisfying Eq.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "The complexity of some most recent NMF based off-line multi-view clustering methods such as MultiNMF [20] and MIC [25] are O(touttinnvKDN), which is in the same order as OMVC.", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "The complexity of some most recent NMF based off-line multi-view clustering methods such as MultiNMF [20] and MIC [25] are O(touttinnvKDN), which is in the same order as OMVC.", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "The summary of the datasets is shown in Table II, and the details of the datasets are as follows: \u2022 Web Knowledge Base (WebKB)1: It is a subset of web documents from four universities [27].", "startOffset": 184, "endOffset": 188}, {"referenceID": 0, "context": "\u2022 Reuters Multilingual Text Data (Reuters)3: This data contains features of 111,740 documents originally written in five different languages (English, French, German, Spanish and Italian), and their translations, over a common set of 6 topic categories [1].", "startOffset": 253, "endOffset": 256}, {"referenceID": 21, "context": "\u2022 YouTube Multiview Video Games (YouTube)4: This data contains about 120,000 videos from 31 classes corresponding to 30 popular video games and other games [22].", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": "\u2022 MultiNMF: MultiNMF is the state-of-the-art off-line multi-view clustering method based on joint nonnegative matrix factorization [20] for complete views.", "startOffset": 131, "endOffset": 135}, {"referenceID": 24, "context": "edu/ml/datasets/YouTube+Multiview +Video+Games+Dataset incomplete data via weighted joint NMF [25].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "\u2022 ONMF: ONMF is an online document clustering algorithm for single view using NMF [30].", "startOffset": 82, "endOffset": 86}, {"referenceID": 30, "context": "In our experiments, two widely used evaluation metrics, accuracy (AC) and normalized mutual information (NMI), are used to measure the clustering performance [31].", "startOffset": 158, "endOffset": 162}, {"referenceID": 3, "context": "Multi-view clustering [4] provides a natural way for generating clusters from multi-view data.", "startOffset": 22, "endOffset": 25}, {"referenceID": 7, "context": "Here we particularly address several subspace based multi-view clustering methods [8, 20, 25, 29].", "startOffset": 82, "endOffset": 97}, {"referenceID": 19, "context": "Here we particularly address several subspace based multi-view clustering methods [8, 20, 25, 29].", "startOffset": 82, "endOffset": 97}, {"referenceID": 24, "context": "Here we particularly address several subspace based multi-view clustering methods [8, 20, 25, 29].", "startOffset": 82, "endOffset": 97}, {"referenceID": 28, "context": "Here we particularly address several subspace based multi-view clustering methods [8, 20, 25, 29].", "startOffset": 82, "endOffset": 97}, {"referenceID": 7, "context": "[8] proposed a CCA based multi-view clustering method to learn the subspace, in which the correlations among views are maximized.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] approached the problem by learning a common consensus based on a co-regularized joint NMF framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Recently, [29] proposed to solve multi-view clustering with at least one complete view based on CCA.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "Later, [25, 17] proposed two NMF based methods that can solve multi-view clustering even without any complete view.", "startOffset": 7, "endOffset": 15}, {"referenceID": 16, "context": "Later, [25, 17] proposed two NMF based methods that can solve multi-view clustering even without any complete view.", "startOffset": 7, "endOffset": 15}, {"referenceID": 15, "context": "Nonnegative matrix factorization [16], especially online NMF is the second area that is related to our work.", "startOffset": 33, "endOffset": 37}, {"referenceID": 5, "context": "NMF was first proposed to handle really large data or streaming data [6].", "startOffset": 69, "endOffset": 72}, {"referenceID": 10, "context": "For example, [11] proposed an efficient online NMF algorithm that takes one chunk of samples per step and updates the bases via robust stochastic approximation.", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "[30] proposed an online NMF algorithm for document clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] added graph regularization to an online joint NMF framwork for multi-view feature selection.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "In the era of big data, it is common to have data with multiple modalities or coming from multiple sources, known as \u201cmulti-view data\u201d. Multi-view clustering provides a natural way to generate clusters from such data. Since different views share some consistency and complementary information, previous works on multi-view clustering mainly focus on how to combine various numbers of views to improve clustering performance. However, in reality, each view may be incomplete, i.e., instances missing in the view. Furthermore, the size of data could be extremely huge. It is unrealistic to apply multi-view clustering in large real-world applications without considering the incompleteness of views and the memory requirement. None of previous works have addressed all these challenges simultaneously. In this paper, we propose an online multiview clustering algorithm, OMVC, which deals with large-scale incomplete views. We model the multi-view clustering problem as a joint weighted nonnegative matrix factorization problem and process the multi-view data chunk by chunk to reduce the memory requirement. OMVC learns the latent feature matrices for all the views and pushes them towards a consensus. We further increase the robustness of the learned latent feature matrices in OMVC via lasso regularization. To minimize the influence of incompleteness, dynamic weight setting is introduced to give lower weights to the incoming missing instances in different views. More importantly, to reduce the computational time, we incorporate a faster projected gradient descent by utilizing the Hessian matrices in OMVC. Extensive experiments conducted on four real data demonstrate the effectiveness of the proposed OMVC method. Keywords-Multi-view clustering; Online algorithm; Incomplete views; Nonnegative matrix factorization", "creator": "LaTeX with hyperref package"}}}