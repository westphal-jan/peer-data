{"id": "1611.00674", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "We eguren figure pemex out gusarov a trap french-born that is fourth-oldest not carefully addressed bridges in sparrer the 6-digit previous outpouring works using -17 lexicons ruthenians or ghaggar-hakra ontologies graber to formula train or vergakis improve mirabad distributed word representations: manlow For stateliness polysemantic words kornwestheim and schmidbauer utterances 78.72 changing jamundi meaning alay in ambalangoda different naiman contexts, their paraphrases or 35.55 related entities dodoma in a karras lexicon or el-hage an glaza ontology meinhard are unreliable and sometimes deteriorate the learning of munnetra word representations. skrulls Thus, raub we propose an filgoal.com approach to address verba the problem that rubberneck considers klaas each koyuki paraphrase arisan of a poky word polypore in a lexicon not sathirathai fully wrixon a ranitidine paraphrase, gulfs but cunxu a fuzzy 296.6 member (asosa i. delbridge e. , 1,639 fuzzy paraphrase) seon in the mechtilde paraphrase stradford set hostas whose alyn degree of distended truth (87.7 i. e. , cleated membership) depends readjustments on nura the 4,800-pound contexts. Then jeweled we amg propose mobuto an bottega efficient method gona\u00efves to 15,100 use the fuzzy paraphrases to c\u00e1pac learn word embeddings. divers We mongooses approximately escobar estimate advertisers the grabsch local membership of paraphrases, and 5-foot-2 train krav word embeddings using niaga a gcu lexicon scanlon jointly kawata by replacing the words ghali in quezada the contexts with their paraphrases randomly subject ceccarelli to nathanial the membership of each on-deck paraphrase. leviev The experimental ramda results diastole show that palatial our coppock method is bellovin efficient, valgus overcomes the weakness of the 2-level previous related works bilabial in extracting semantic information penda and outperforms the apps previous acconci works rodden of scruples learning adeos word representations lisc using cruddas lexicons.", "histories": [["v1", "Wed, 2 Nov 2016 16:38:18 GMT  (211kb)", "http://arxiv.org/abs/1611.00674v1", "10 pages, 1 figures, 5 tables. Under review as a conference paper at ICLR 2017"], ["v2", "Thu, 3 Nov 2016 15:32:40 GMT  (211kb)", "http://arxiv.org/abs/1611.00674v2", "10 pages, 1 figures, 5 tables. Under review as a conference paper at ICLR 2017"], ["v3", "Fri, 4 Nov 2016 18:51:17 GMT  (211kb)", "http://arxiv.org/abs/1611.00674v3", "10 pages, 1 figures, 5 tables. Under review as a conference paper at ICLR 2017"], ["v4", "Mon, 28 Nov 2016 10:22:42 GMT  (211kb)", "http://arxiv.org/abs/1611.00674v4", "10 pages, 1 figures, 5 tables. Under review as a conference paper at ICLR 2017"], ["v5", "Sat, 14 Jan 2017 07:57:01 GMT  (611kb)", "http://arxiv.org/abs/1611.00674v5", "12 pages, 5 figures, 3 tables. Under review as a conference paper at ICLR 2017"], ["v6", "Thu, 19 Jan 2017 09:10:35 GMT  (682kb)", "http://arxiv.org/abs/1611.00674v6", "13 pages, 21 figures, 3 tables. Under review as a conference paper at ICLR 2017"], ["v7", "Tue, 7 Feb 2017 10:18:54 GMT  (285kb)", "http://arxiv.org/abs/1611.00674v7", "13 pages, 21 figures, 3 tables"], ["v8", "Wed, 9 Aug 2017 06:05:46 GMT  (0kb,I)", "http://arxiv.org/abs/1611.00674v8", "It lacks of study. The results are not reasonable. Our recent study shows that there is no significant difference when we remove some part of the lexicon"], ["v9", "Fri, 8 Sep 2017 11:46:56 GMT  (0kb,I)", "http://arxiv.org/abs/1611.00674v9", "Withdrawn. Our recent study shows that there is no significant difference when we remove some part of the lexicon with the method in the paper. We decided to withdraw this paper"]], "COMMENTS": "10 pages, 1 figures, 5 tables. Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuanzhi ke", "masafumi hagiwara"], "accepted": false, "id": "1611.00674"}, "pdf": {"name": "1611.00674.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["hagiwara}@soft.ics.keio.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n00 67\n4v 1\n[ cs\n.C L\n] 2\nN ov\n2 01"}, {"heading": "1 INTRODUCTION", "text": "There have been many works and models to estimate the distributed representations of words, i.e. the word embeddings for a corpus (Bengio et al., 2003; Mnih & Hinton, 2007; 2009; Collobert et al., 2011; Huang et al., 2012; Mikolov, 2012; Mikolov et al., 2013b;a;c; Pennington et al., 2014; Bojanowski et al., 2016). Benefiting from the works, high quality word embeddings can be estimated efficiently nowadays.\nWord embeddings are reported useful and improve the performance of the machine learning algorithms for many natural language processing tasks such as name entity recognition and chunking (Turian et al., 2010), text classification (Socher et al., 2012; Le & Mikolov, 2014; Kim, 2014; Joulin et al., 2016), topic extraction (Das et al., 2015; Li et al., 2016), and machine translation (Zaremba et al., 2014; Sutskever et al., 2014).\nNevertheless, there is still room for improvement. For example, for the fine-grained sentiment analysis tasks such as predicating the number of stars of a review, the reported accuracy is much lower than the other text classification tasks (Joulin et al., 2016). It indicates the needs of word representations that embed the semantic information more efficiently.\nBojanowski et al. (2016) attempt to improve word embeddings by involving character level information. There is a big improvement on syntactic questions in the word analogical reasoning task introduced by Mikolov et al. (2013a). However, the accuracy for the semantic part is not improved in the reported results.\nSome works (Yu & Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Bollegala et al., 2016) try to estimate better word embeddings by using a lexicon or an ontology. The idea is simple: because a lexicon or an ontology contains well-defined relations about words, word embeddings of high quality can be learned from it, or we can refine trained word embeddings using the lexicon or the ontology.\nHowever, a problem is not well addressed in the previous works using lexicons to learn word embeddings: For a polysemantic word or utterance, its paraphrase in a lexicon or an ontology is not always its paraphrase in different contexts. For example, we can replace the word \u201cEarth\u201d in the sentence \u201cEarth goes around the sun\u201d with its paraphrase \u201cTerra\u201d, however the same word \u201cEarth\u201d in the sentence \u201cI fill the hole with earth\u201d cannot be replaced with \u201cTerra\u201d. Henceforth, the lexicon or the ontology is sometimes unreliable and deteriorates the learning of word embeddings for the polysemantic words and utterances.\nIn this paper, we propose a method to learn word embeddings using both a corpus and a lexicon that is able to alleviate the bad effect of polysemants, by estimating the degree of truth of each paraphrase in the lexicon. Our method for estimating is simple, efficient and easy to be combined with the previous learning algorithms on the basis of co-occurrences of words. The experimental results show that our method is efficient and outperforms the previous works."}, {"heading": "2 RELATED WORKS", "text": ""}, {"heading": "2.1 WORKS ON LEARNING WORD EMBEDDINGS FOR A CORPUS", "text": "The first approaches learning word embeddings use n-gram model (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012) and recurrent neural networks (Mikolov, 2012). Recently, more efficient methods like continuous bag-of-words (CBOW) model and skip-gram (SG) model (Mikolov et al., 2013b;a), also called word2vec, provide a more efficient way to learn word embeddings based on the local co-occurrences of words in a corpus. Continuous bag-of-words tries to maximize the log probability of a word given its context, while skip-gram tries to maximize the log probability of the words in the context given a word. Negative sampling is an efficient algorithm to train them, that approximately maximizes the log probability for the targets by doing a logistic regression to discriminate the target word from noise randomly drawn from a noise distribution.\nBojanowski et al. (2016) extend word2vec using character-level information. They achieve a considerable improvement for rare words and the syntactic part of the word analogical reasoning task (Mikolov et al., 2013a). However, they fail to improve the performance for the semantic part of the task.\nOther works attempt to train word embeddings via global information (Huang et al., 2012; Pennington et al., 2014) and report improvement than the other works that use only local information. However the global information is still limited by the corpus. Some other works follow another approach that uses a lexicon or an ontology to improve the word embeddings."}, {"heading": "2.2 WORKS ON LEARNING WORD EMBEDDINGS USING LEXICONS", "text": "The models proposed by Yu & Dredze (2014), and Bollegala et al. (2016) jointly learn word embeddings from a corpus and a semantic lexicon. The method proposed by Yu & Dredze (2014) called jointRCM improves word representations by maximizing the similarity of the word representations of the paraphrase pairs in the lexicon jointly with word2vec. The works by Bollegala et al. (2016) also improve the word embeddings by minimizing the distance of the word representations of related words. But they use not only the synonyms, but also the global information in corpus and other relationships such as antonyms and hypernyms.\nXu et al. (2014) propose models called R-Net and C-Net. R-Net, for a triplet of words (head, relation, tail) in a knowledge graph, minimizes the distance of the embedding vector of the tail word, from the sum of the vectors of the head word and the relation. On the other hand, C-Net makes a word less similar than the other words share the same category if the size of the category is large. They jointly train the R-Net and C-Net with skip-gram.\nFaruqui et al. (2015) concentrate on refining pre-trained word embeddings using semantic lexicons. However, as also pointed out by Bollegala et al. (2016) as incompatibilities between the corpus and the lexicon, some features extracted from the corpus that are not contained in the corpus, such as those from idioms or new words not contained in the lexicon, are improperly removed."}, {"heading": "2.3 A NOT WELL ADDRESSED TRAP IN LEARNING WORD REPRESENTATIONS USING LEXICONS", "text": "In most of the previous works using lexicons to learn word representations, weight coefficients are used to control the input from the lexicon. These coefficients are manually optimized with a separated dataset. However, it cannot address the problem that the reliability of the paraphrases in a lexicon depends on different words and different contexts because a word or an utterance can have several meanings. For example, though \u201cTerra\u201d is the paraphrase of \u201cEarth\u201d in \u201cEarth goes around the sun\u201d, but obviously not its paraphrase in \u201cI fill the hole with earth\u201d."}, {"heading": "3 THE PROPOSED METHOD", "text": ""}, {"heading": "3.1 LEARNING WORD EMBEDDINGS WITH FUZZY PARAPHRASES", "text": "Our method is based on two ideas. The first, if the meanings of word j and word k are ideally the same, they can replace each other in a text without changing the meaning and all the other implicit features of the text. Further, when we train word embeddings by predicating a word i by the context containing word j like word2vec (Mikolov et al., 2013b;a), the probability of word i keeps unchanged in that case even though word j is replaced by word k. Henceforth, we can learn word embeddings using both a corpus and a lexicon by learning the original texts and those that some words are replaced with their paraphrases at the same time in the ideal case.\nThe second, as described in section 1 and 2, the paraphrases of a word in a lexicon are not always the paraphrases in a certain text but depend on the contexts because of polysemantic words and utterances. Henceforth, if we simply consider all the paraphrases of a word in the lexicon fully the paraphrases for the whole corpus, they deteriorate the word embeddings of some words and texts. To avoid that, we consider each paraphrase of a word in the lexicon not fully included in the paraphrase set, but a fuzzy member with a grade of membership (i.e. a degree of truth). Then we reject some paraphrases for some contexts subject to their membership.\nFor a text T , denote wi the ith word in T , c the context window, wj a word in the context window, Lwj the paraphrase set of wj in the lexicon L, wk the kth fuzzy paraphrase in Lwj , and xjk the membership of wk for wj , based on the CBOW model (Mikolov et al., 2013a) and the two ideas, we\npropose a model called continuous bag-of-fuzzy-paraphrases (CBOFP) to train word embeddings using both a corpus and a lexicon, by maximizing not only the probability of a word for a given context, but also the probability after some of the words in the context are replaced by their paraphrases randomly subject to a function of the membership of each paraphrase:\nT \u2211\nwi\u2208T\n\u2211\n(i\u2212c)\u2264j\u2264(i+c)\n\nlog p(wi|wj) +\nLwj \u2211\nwk\u2208Lwj\nf(xjk) log p(wi|wk)\n\n (1)\nThe function f(xjk) of the membership xjk returns 1 or 0 for different paraphrases of different contexts and reduces the probabilities of the bad replacements that deteriorate the word embeddings by returning 0 more for the paraphrases that have lower grades of membership. The model can be considered as a revised CBOW model with an additional layer whose output is weighted by f(xjk) as shown in Figure 1."}, {"heading": "3.2 MEMBERSHIP ESTIMATION", "text": "If we want the control function f(xjk) to reject bad replacements perfectly, f(xjk) or the membership xjk should consider all of the contexts because the similarity of the paraphrases depends on not only themselves but also the other contexts. However, it is not easy to train such a function.\nLooking for a control function that is easy to train, we notice that if two words are more often to be translated to the same word in another language, the replacement of them are less likely to change the meaning of the original sentence. Thus, we use a function of the bilingual similarity (denoted as Sjk) as the membership function without considering the other contexts:\nxjk = g(Sjk) (2)\nThere have been works about calculating the similarity of words using such bilingual information and a lexicon called the paraphrase database (PPDB) provides scores of the similarity of paraphrases (Ganitkevitch et al., 2013; Pavlick et al., 2015b;a) on the basis of bilingual features. We scale the similarity score of the paraphrase wk to [0, 1] in PPDB2.0 as the membership, and draw the values of f(xjk) from a Bernoulli distribution subject to the membership calculated in this way. Denote Sjk the similarity score of word wj and wk in PPDB2.0, the value of f(xjk) is drawn from the Bernoulli distribution:\nf(xjk) \u223c Bernoulli(xjk) (3)\nxjk = Sjk\nmax j\u2208T,k\u2208L\nSjk (4)\nWe find the control function defined above is efficient in the experiments as described later in section 4."}, {"heading": "3.3 TRAINING", "text": "Hence we do not need to train f(xjk) using the method described above. The model can be trained by negative sampling proposed by Mikolov et al. (2013b): For word wO and a word wI in its context, denote AI as the set of the paraphrases for wI accepted by f(xjk), we maximize log p(wO|wI) by distinguishing the noise words from a noise distribution Pn(w) from wO and its accepted paraphrases in AI by logistic regression:\nlog p(wO|wI) = log \u03c3(vwO \u2032vwI ) +\nn \u2211\ni=1\nEwi \u223c Pn(w)[log \u03c3(\u2212vwi \u2032vwI )], wi 6= wO, wi /\u2208 AI (5)\nHere, n is the number of total negative samples. \u03c3(x) is a sigmoid function, \u03c3(x) = 1/(1 + e\u2212x)."}, {"heading": "3.4 DIFFERENT TYPES OF PARAPHRASES AND THE PARAPHRASE SET FOR EACH WORD", "text": "In PPDB2.0, there are 6 relationships for paraphrases on the basis of the thesis of MacCartney (2009). For word X and word Y , the different relationships of them defined in PPDB2.0 are shown in Table 1. We see that they are far more than we need as some of them are not the conventional \u201cparaphrases\u201d that can replace each other. Only the paraphrases of equivalence, forward entailment and reverse entailment are used in our method. For each word in the vocabulary, the paraphrases equal to it or entailed by it are put into its paraphrase set for learning. For example, denote each paraphrase in PPDB2.0 as (headword, tailword, relationship), for word A, B, C, D, E, if there are paraphrases (A,B,Equivalence), (A,C, ForwardEntailment), (D,A,ReverseEntailment), (A,E, Independent), the paraphrase set for A is (B,C,D), and E is discarded."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "4.1 THE CORPUS, LEXICON AND PARAMETERS", "text": "In the experiments, we used enwiki91 as the corpus to train our model. It contains the first one billion bytes in the English Wikipedia. After removing the meta-data, tags, hyperlinks, references, URL encoded characters and converting uppercase letters, spaces and spell digits, the corpus contains 123,353,508 tokens. Among them, there are 218,317 different words.\nPPDB2.0 (Pavlick et al., 2015b;a) is used as the lexicon. It contains more than 100 million paraphrases and 26 thousand manually rated phrase pairs. Only the paraphrase pairs whose relationships are equivalence, forward entailment, or reverse entailment are used for our method in the experiments as described in 3.4.\n200-dimension word embeddings are trained using our method for the experiments. The context window is set to 8, the number of negative samples is set to 25, the total number of iterations is set to 15 for training. We made an implementation of the proposed method learning enwiki9 and using PPDB2.0 as the lexicon available online2."}, {"heading": "4.2 BASELINES", "text": "As baselines to compare with our proposed method, we use word2vec (Mikolov et al., 2013b;a) (Marked as CBOW and SG, for continuous bag-of-word and skip-gram, respectively), word2vec enriched with subword information (Bojanowski et al., 2016) (Marked as Enriched CBOW and Enriched SG), GloVe (Pennington et al., 2014), which are widely used to extract word embeddings from a corpus. We also compare our method with the other works using a lexicon to improve word embeddings, which are jointRCM (Yu & Dredze, 2014), jointReps (Bollegala et al., 2016), RC-Net (Xu et al., 2014), and the method to retrofit pre-trained word embeddings using a lexicon (Marked as Retro) (Faruqui et al., 2015).\n1http://mattmahoney.net/dc/enwiki9.zip 2https://github.com/huajianjiu/Bernoulli-CBOFP\nWe used the public online available source code of word2vec3, word2vec enriched with subword information4, GloVe5, jointRCM6, jointReps7,and Retro8 to build them for the experiments. But for jointRCM and jointReps, the results in our experiments are one percent the number of the reported results in the papers . It is unreasonably low, even though the corpus in our experiments is smaller than those used in their experiments. Thus we use the reported results in the papers to compare with our method. The reported results of jointRCM are achieved using the New York Time 1994- 97 subset from Gigaword v5.0 (Parker et al., 2011) containing 518,103,942 tokens. The reported results of jointReps are achieved with ukWaC9 containing 2 billion tokens. For RC-Net, there are no publicly available implementations unfortunately. We report the published results in their paper that are also achieved with enwiki9 to compare with ours. For the other baselines, 200-dimension word embeddings are trained using enwiki9. The context window is set to 8. For word2vec and that enriched with subword information, the number of negative samples is set to 25. The word embeddings trained by CBOW and SG are both used for Retro respectively, and marked as Retro (CBOW) and Retro (SG)."}, {"heading": "4.3 WORD ANALOGICAL REASONING TASK", "text": "The word analogical reasoning task is introduced by Mikolov et al. (2013a). Given a quaternion of words (wA, wB , wC , wD) that wA and wB have the similar relationship with that of wC and wD, the objective is to predict wD on the basis of wA, wB and wC . Given the word embedding vA, vB and vC for wA, wB and wC , it can be solved by finding the word whose word embedding is the closest to vB \u2212 vA + vC . The dataset is separated into two parts: the semantic part and the syntactic part. The semantic part is about analogical reasoning via semantic relationships, such as predicting the capital of a country. The syntactic part is about syntactic relationships, such as predicting the adverb form for an adjective.\nIn Table 2, we compare our method with the works using only corpus to train word embeddings. Our method gets the best overall accuracy and the best for the semantic part. For the syntactic part, our method fails to outperform the word2vec enriched with character-level subword information that is reported powerful for the syntactic part.\nIn Table 3, we compare our method with the previous works using the lexicon to improve the word embeddings. The numbers in the brackets are the differences from the accuracies achieved by the models they base on. For our method and jointRCM, it is CBOW. For jointReps, it is GloVe. For RC-Net, it is SG. For Retro, we report the results retrofitting the word embeddings trained by CBOW and SG, respectively.\n3https://code.google.com/archive/p/word2vec/ 4https://github.com/facebookresearch/fastText 5https://github.com/stanfordnlp/GloVe 6https://github.com/Gorov/JointRCM 7https://github.com/Bollegala/jointreps 8https://github.com/mfaruqui/retrofitting 9http://wacky.sslmit.unibo.it\nWe see that while the other works perform worse in the semantic part than the model they base on, ours outperform CBOW and outperform the other works in the semantic part. Benefited from alleviating the bad influence of polysemantic words and utterances, our method successfully improves the word embeddings in representing semantic information using a lexicon while the other works fail to achieve it. Our method also achieves the best overall accuracy. But for syntactic part, the result of our method using enwiki9 is not as good as the reported result of jointReps using ukWaC."}, {"heading": "4.4 EFFECTS OF THE SIZE OF THE CORPUS", "text": "To see how the size of the corpus affects the performance of our method, we also used a smaller corpus called Text810 to learn word embeddings using our methods and then run the word analogical reasoning task using the trained word embeddings. Text8 contains the words in the first 100 million bytes of English Wikipedia. There are 16,718,843 tokens in it and 71,291 different words among them.\nWe compare the difference of the results by our method and CBOW using the different corpora in Table 4. We see that our method does not achieve obvious improvement over CBOW for text8, but outperforms CBOW for enwiki9. It indicates that our method is weak at small corpus. It is because we use a probabilistic method that requires plenty of samples. By increasing the size of the corpus, our method achieves more improvement."}, {"heading": "4.5 THE LEARNING SPEED", "text": "We compare the learning speed on our machine of our method training 200-dimension word embeddings on text8 against CBOW and the other related works on the basis of CBOW in Table 5. 20 threads were used to train the word embeddings for every model. Unfortunately, the public implement of jointRCM by the original authors11 fails to run correctly on our machine, and there is no reported learning speed. We see that there is almost no loss for our method in learning speed in comparison with CBOW while the word2vec enriched with subword information is obviously slower."}, {"heading": "5 CONCLUSION", "text": "We figure out a problem that is not paid enough attention to in the previous works using lexicons to improve word embeddings: Because some words and utterances have multiple meanings, a paraphrase of a word in a lexicon may not be a paraphrase actually in a certain context. Then we propose a method to avoid the trap: We treat the lexicon as a fuzzy set, approximately estimate the membership of the paraphrases, and learn word embeddings using both a corpus and a lexicon by replacing the words in the context with the paraphrases randomly subject to their grades of membership.\nBy comparison with the previous works in the word analogical reasoning task, it has been shown that our method overcomes the weakness of the previous related works in extracting the semantic features, outperforms the previous works and keeps fast.\nThe results using corpora in different sizes show that the proposed method works better with a larger corpus but less effectively with a small corpus. We are looking for another robust method to control the replacements of the paraphrases that keeps efficient for small corpora."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "journal of machine learning research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1607.04606,", "citeRegEx": "Bojanowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Joint word representation learning using a corpus and a semantic lexicon", "author": ["Danushka Bollegala", "Alsuhaibani Mohammed", "Takanori Maehara", "Ken-Ichi Kawarabayashi"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "Bollegala et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bollegala et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Gaussian lda for topic models with word embeddings. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "author": ["Rajarshi Das", "Manzil Zaheer", "Chris Dyer"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Das et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Das et al\\.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Bag of tricks for efficient text classification", "author": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1607.01759,", "citeRegEx": "Joulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In the 31st International Conference on Machine Learning (ICML 2014),", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Generative topic embedding: a continuous representation of documents. In the 54th annual meeting of the Association for Computational Linguistics (ACL 2016)", "author": ["Shaohua Li", "Tat-Seng Chua", "Jun Zhu", "Chunyan Miao"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "NATURAL LANGUAGE INFERENCE", "author": ["Bill MacCartney"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "MacCartney.,? \\Q2009\\E", "shortCiteRegEx": "MacCartney.", "year": 2009}, {"title": "Statistical Language Models Based on Neural Networks", "author": ["Tomas Mikolov"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In ICLR Workshop,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL HLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Mnih and Hinton.,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "English gigaword fifth edition", "author": ["Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda"], "venue": "Technical report, Linguistic Data Consortium,", "citeRegEx": "Parker et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parker et al\\.", "year": 2011}, {"title": "Adding semantics to data-driven paraphrasing. In Association for Computational Linguistics, Beijing, China, July 2015a", "author": ["Ellie Pavlick", "Johan Bos", "Malvina Nissim", "Charley Beller", "Benjamin Van Durme", "Chris CallisonBurch"], "venue": null, "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Ppdb 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevich", "Benjamin Van Durme", "Chris CallisonBurch"], "venue": "In Association for Computational Linguistics,", "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics (ACL", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze"], "venue": "In the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yu and Dredze.,? \\Q2014\\E", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "CoRR, abs/1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "the word embeddings for a corpus (Bengio et al., 2003; Mnih & Hinton, 2007; 2009; Collobert et al., 2011; Huang et al., 2012; Mikolov, 2012; Mikolov et al., 2013b;a;c; Pennington et al., 2014; Bojanowski et al., 2016).", "startOffset": 33, "endOffset": 217}, {"referenceID": 3, "context": "the word embeddings for a corpus (Bengio et al., 2003; Mnih & Hinton, 2007; 2009; Collobert et al., 2011; Huang et al., 2012; Mikolov, 2012; Mikolov et al., 2013b;a;c; Pennington et al., 2014; Bojanowski et al., 2016).", "startOffset": 33, "endOffset": 217}, {"referenceID": 7, "context": "the word embeddings for a corpus (Bengio et al., 2003; Mnih & Hinton, 2007; 2009; Collobert et al., 2011; Huang et al., 2012; Mikolov, 2012; Mikolov et al., 2013b;a;c; Pennington et al., 2014; Bojanowski et al., 2016).", "startOffset": 33, "endOffset": 217}, {"referenceID": 13, "context": "the word embeddings for a corpus (Bengio et al., 2003; Mnih & Hinton, 2007; 2009; Collobert et al., 2011; Huang et al., 2012; Mikolov, 2012; Mikolov et al., 2013b;a;c; Pennington et al., 2014; Bojanowski et al., 2016).", "startOffset": 33, "endOffset": 217}, {"referenceID": 22, "context": "the word embeddings for a corpus (Bengio et al., 2003; Mnih & Hinton, 2007; 2009; Collobert et al., 2011; Huang et al., 2012; Mikolov, 2012; Mikolov et al., 2013b;a;c; Pennington et al., 2014; Bojanowski et al., 2016).", "startOffset": 33, "endOffset": 217}, {"referenceID": 1, "context": "the word embeddings for a corpus (Bengio et al., 2003; Mnih & Hinton, 2007; 2009; Collobert et al., 2011; Huang et al., 2012; Mikolov, 2012; Mikolov et al., 2013b;a;c; Pennington et al., 2014; Bojanowski et al., 2016).", "startOffset": 33, "endOffset": 217}, {"referenceID": 25, "context": "Word embeddings are reported useful and improve the performance of the machine learning algorithms for many natural language processing tasks such as name entity recognition and chunking (Turian et al., 2010), text classification (Socher et al.", "startOffset": 187, "endOffset": 208}, {"referenceID": 23, "context": ", 2010), text classification (Socher et al., 2012; Le & Mikolov, 2014; Kim, 2014; Joulin et al., 2016), topic extraction (Das et al.", "startOffset": 29, "endOffset": 102}, {"referenceID": 9, "context": ", 2010), text classification (Socher et al., 2012; Le & Mikolov, 2014; Kim, 2014; Joulin et al., 2016), topic extraction (Das et al.", "startOffset": 29, "endOffset": 102}, {"referenceID": 8, "context": ", 2010), text classification (Socher et al., 2012; Le & Mikolov, 2014; Kim, 2014; Joulin et al., 2016), topic extraction (Das et al.", "startOffset": 29, "endOffset": 102}, {"referenceID": 4, "context": ", 2016), topic extraction (Das et al., 2015; Li et al., 2016), and machine translation (Zaremba et al.", "startOffset": 26, "endOffset": 61}, {"referenceID": 11, "context": ", 2016), topic extraction (Das et al., 2015; Li et al., 2016), and machine translation (Zaremba et al.", "startOffset": 26, "endOffset": 61}, {"referenceID": 28, "context": ", 2016), and machine translation (Zaremba et al., 2014; Sutskever et al., 2014).", "startOffset": 33, "endOffset": 79}, {"referenceID": 24, "context": ", 2016), and machine translation (Zaremba et al., 2014; Sutskever et al., 2014).", "startOffset": 33, "endOffset": 79}, {"referenceID": 8, "context": "For example, for the fine-grained sentiment analysis tasks such as predicating the number of stars of a review, the reported accuracy is much lower than the other text classification tasks (Joulin et al., 2016).", "startOffset": 189, "endOffset": 210}, {"referenceID": 26, "context": "Some works (Yu & Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Bollegala et al., 2016) try to estimate better word embeddings by using a lexicon or an ontology.", "startOffset": 11, "endOffset": 93}, {"referenceID": 5, "context": "Some works (Yu & Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Bollegala et al., 2016) try to estimate better word embeddings by using a lexicon or an ontology.", "startOffset": 11, "endOffset": 93}, {"referenceID": 2, "context": "Some works (Yu & Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Bollegala et al., 2016) try to estimate better word embeddings by using a lexicon or an ontology.", "startOffset": 11, "endOffset": 93}, {"referenceID": 0, "context": "the word embeddings for a corpus (Bengio et al., 2003; Mnih & Hinton, 2007; 2009; Collobert et al., 2011; Huang et al., 2012; Mikolov, 2012; Mikolov et al., 2013b;a;c; Pennington et al., 2014; Bojanowski et al., 2016). Benefiting from the works, high quality word embeddings can be estimated efficiently nowadays. Word embeddings are reported useful and improve the performance of the machine learning algorithms for many natural language processing tasks such as name entity recognition and chunking (Turian et al., 2010), text classification (Socher et al., 2012; Le & Mikolov, 2014; Kim, 2014; Joulin et al., 2016), topic extraction (Das et al., 2015; Li et al., 2016), and machine translation (Zaremba et al., 2014; Sutskever et al., 2014). Nevertheless, there is still room for improvement. For example, for the fine-grained sentiment analysis tasks such as predicating the number of stars of a review, the reported accuracy is much lower than the other text classification tasks (Joulin et al., 2016). It indicates the needs of word representations that embed the semantic information more efficiently. Bojanowski et al. (2016) attempt to improve word embeddings by involving character level information.", "startOffset": 34, "endOffset": 1134}, {"referenceID": 0, "context": "the word embeddings for a corpus (Bengio et al., 2003; Mnih & Hinton, 2007; 2009; Collobert et al., 2011; Huang et al., 2012; Mikolov, 2012; Mikolov et al., 2013b;a;c; Pennington et al., 2014; Bojanowski et al., 2016). Benefiting from the works, high quality word embeddings can be estimated efficiently nowadays. Word embeddings are reported useful and improve the performance of the machine learning algorithms for many natural language processing tasks such as name entity recognition and chunking (Turian et al., 2010), text classification (Socher et al., 2012; Le & Mikolov, 2014; Kim, 2014; Joulin et al., 2016), topic extraction (Das et al., 2015; Li et al., 2016), and machine translation (Zaremba et al., 2014; Sutskever et al., 2014). Nevertheless, there is still room for improvement. For example, for the fine-grained sentiment analysis tasks such as predicating the number of stars of a review, the reported accuracy is much lower than the other text classification tasks (Joulin et al., 2016). It indicates the needs of word representations that embed the semantic information more efficiently. Bojanowski et al. (2016) attempt to improve word embeddings by involving character level information. There is a big improvement on syntactic questions in the word analogical reasoning task introduced by Mikolov et al. (2013a). However, the accuracy for the semantic part is not improved in the reported results.", "startOffset": 34, "endOffset": 1336}, {"referenceID": 0, "context": "The first approaches learning word embeddings use n-gram model (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012) and recurrent neural networks (Mikolov, 2012).", "startOffset": 63, "endOffset": 128}, {"referenceID": 3, "context": "The first approaches learning word embeddings use n-gram model (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012) and recurrent neural networks (Mikolov, 2012).", "startOffset": 63, "endOffset": 128}, {"referenceID": 7, "context": "The first approaches learning word embeddings use n-gram model (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012) and recurrent neural networks (Mikolov, 2012).", "startOffset": 63, "endOffset": 128}, {"referenceID": 13, "context": ", 2012) and recurrent neural networks (Mikolov, 2012).", "startOffset": 38, "endOffset": 53}, {"referenceID": 7, "context": "Other works attempt to train word embeddings via global information (Huang et al., 2012; Pennington et al., 2014) and report improvement than the other works that use only local information.", "startOffset": 68, "endOffset": 113}, {"referenceID": 22, "context": "Other works attempt to train word embeddings via global information (Huang et al., 2012; Pennington et al., 2014) and report improvement than the other works that use only local information.", "startOffset": 68, "endOffset": 113}, {"referenceID": 0, "context": "The first approaches learning word embeddings use n-gram model (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012) and recurrent neural networks (Mikolov, 2012). Recently, more efficient methods like continuous bag-of-words (CBOW) model and skip-gram (SG) model (Mikolov et al., 2013b;a), also called word2vec, provide a more efficient way to learn word embeddings based on the local co-occurrences of words in a corpus. Continuous bag-of-words tries to maximize the log probability of a word given its context, while skip-gram tries to maximize the log probability of the words in the context given a word. Negative sampling is an efficient algorithm to train them, that approximately maximizes the log probability for the targets by doing a logistic regression to discriminate the target word from noise randomly drawn from a noise distribution. Bojanowski et al. (2016) extend word2vec using character-level information.", "startOffset": 64, "endOffset": 887}, {"referenceID": 2, "context": "The models proposed by Yu & Dredze (2014), and Bollegala et al. (2016) jointly learn word embeddings from a corpus and a semantic lexicon.", "startOffset": 47, "endOffset": 71}, {"referenceID": 2, "context": "The models proposed by Yu & Dredze (2014), and Bollegala et al. (2016) jointly learn word embeddings from a corpus and a semantic lexicon. The method proposed by Yu & Dredze (2014) called jointRCM improves word representations by maximizing the similarity of the word representations of the paraphrase pairs in the lexicon jointly with word2vec.", "startOffset": 47, "endOffset": 181}, {"referenceID": 2, "context": "The models proposed by Yu & Dredze (2014), and Bollegala et al. (2016) jointly learn word embeddings from a corpus and a semantic lexicon. The method proposed by Yu & Dredze (2014) called jointRCM improves word representations by maximizing the similarity of the word representations of the paraphrase pairs in the lexicon jointly with word2vec. The works by Bollegala et al. (2016) also improve the word embeddings by minimizing the distance of the word representations of related words.", "startOffset": 47, "endOffset": 383}, {"referenceID": 2, "context": "The models proposed by Yu & Dredze (2014), and Bollegala et al. (2016) jointly learn word embeddings from a corpus and a semantic lexicon. The method proposed by Yu & Dredze (2014) called jointRCM improves word representations by maximizing the similarity of the word representations of the paraphrase pairs in the lexicon jointly with word2vec. The works by Bollegala et al. (2016) also improve the word embeddings by minimizing the distance of the word representations of related words. But they use not only the synonyms, but also the global information in corpus and other relationships such as antonyms and hypernyms. Xu et al. (2014) propose models called R-Net and C-Net.", "startOffset": 47, "endOffset": 640}, {"referenceID": 2, "context": "The models proposed by Yu & Dredze (2014), and Bollegala et al. (2016) jointly learn word embeddings from a corpus and a semantic lexicon. The method proposed by Yu & Dredze (2014) called jointRCM improves word representations by maximizing the similarity of the word representations of the paraphrase pairs in the lexicon jointly with word2vec. The works by Bollegala et al. (2016) also improve the word embeddings by minimizing the distance of the word representations of related words. But they use not only the synonyms, but also the global information in corpus and other relationships such as antonyms and hypernyms. Xu et al. (2014) propose models called R-Net and C-Net. R-Net, for a triplet of words (head, relation, tail) in a knowledge graph, minimizes the distance of the embedding vector of the tail word, from the sum of the vectors of the head word and the relation. On the other hand, C-Net makes a word less similar than the other words share the same category if the size of the category is large. They jointly train the R-Net and C-Net with skip-gram. Faruqui et al. (2015) concentrate on refining pre-trained word embeddings using semantic lexicons.", "startOffset": 47, "endOffset": 1093}, {"referenceID": 2, "context": "The models proposed by Yu & Dredze (2014), and Bollegala et al. (2016) jointly learn word embeddings from a corpus and a semantic lexicon. The method proposed by Yu & Dredze (2014) called jointRCM improves word representations by maximizing the similarity of the word representations of the paraphrase pairs in the lexicon jointly with word2vec. The works by Bollegala et al. (2016) also improve the word embeddings by minimizing the distance of the word representations of related words. But they use not only the synonyms, but also the global information in corpus and other relationships such as antonyms and hypernyms. Xu et al. (2014) propose models called R-Net and C-Net. R-Net, for a triplet of words (head, relation, tail) in a knowledge graph, minimizes the distance of the embedding vector of the tail word, from the sum of the vectors of the head word and the relation. On the other hand, C-Net makes a word less similar than the other words share the same category if the size of the category is large. They jointly train the R-Net and C-Net with skip-gram. Faruqui et al. (2015) concentrate on refining pre-trained word embeddings using semantic lexicons. However, as also pointed out by Bollegala et al. (2016) as incompatibilities between the corpus and the lexicon, some features extracted from the corpus that are not contained in the corpus, such as those from idioms or new words not contained in the lexicon, are improperly removed.", "startOffset": 47, "endOffset": 1226}, {"referenceID": 13, "context": "The model can be trained by negative sampling proposed by Mikolov et al. (2013b): For word wO and a word wI in its context, denote AI as the set of the paraphrases for wI accepted by f(xjk), we maximize log p(wO|wI) by distinguishing the noise words from a noise distribution Pn(w) from wO and its accepted paraphrases in AI by logistic regression:", "startOffset": 58, "endOffset": 81}, {"referenceID": 12, "context": "0, there are 6 relationships for paraphrases on the basis of the thesis of MacCartney (2009). For word X and word Y , the different relationships of them defined in PPDB2.", "startOffset": 75, "endOffset": 93}, {"referenceID": 1, "context": ", 2013b;a) (Marked as CBOW and SG, for continuous bag-of-word and skip-gram, respectively), word2vec enriched with subword information (Bojanowski et al., 2016) (Marked as Enriched CBOW and Enriched SG), GloVe (Pennington et al.", "startOffset": 135, "endOffset": 160}, {"referenceID": 22, "context": ", 2016) (Marked as Enriched CBOW and Enriched SG), GloVe (Pennington et al., 2014), which are widely used to extract word embeddings from a corpus.", "startOffset": 57, "endOffset": 82}, {"referenceID": 2, "context": "We also compare our method with the other works using a lexicon to improve word embeddings, which are jointRCM (Yu & Dredze, 2014), jointReps (Bollegala et al., 2016), RC-Net (Xu et al.", "startOffset": 142, "endOffset": 166}, {"referenceID": 26, "context": ", 2016), RC-Net (Xu et al., 2014), and the method to retrofit pre-trained word embeddings using a lexicon (Marked as Retro) (Faruqui et al.", "startOffset": 16, "endOffset": 33}, {"referenceID": 5, "context": ", 2014), and the method to retrofit pre-trained word embeddings using a lexicon (Marked as Retro) (Faruqui et al., 2015).", "startOffset": 98, "endOffset": 120}, {"referenceID": 19, "context": "0 (Parker et al., 2011) containing 518,103,942 tokens.", "startOffset": 2, "endOffset": 23}, {"referenceID": 13, "context": "The word analogical reasoning task is introduced by Mikolov et al. (2013a). Given a quaternion of words (wA, wB , wC , wD) that wA and wB have the similar relationship with that of wC and wD, the objective is to predict wD on the basis of wA, wB and wC .", "startOffset": 52, "endOffset": 75}], "year": 2016, "abstractText": "We figure out a trap that is not carefully addressed in the previous works using lexicons or ontologies to train or improve distributed word representations: For polysemantic words and utterances changing meaning in different contexts, their paraphrases or related entities in a lexicon or an ontology are unreliable and sometimes deteriorate the learning of word representations. Thus, we propose an approach to address the problem that considers each paraphrase of a word in a lexicon not fully a paraphrase, but a fuzzy member (i.e., fuzzy paraphrase) in the paraphrase set whose degree of truth (i.e., membership) depends on the contexts. Then we propose an efficient method to use the fuzzy paraphrases to learn word embeddings. We approximately estimate the local membership of paraphrases, and train word embeddings using a lexicon jointly by replacing the words in the contexts with their paraphrases randomly subject to the membership of each paraphrase. The experimental results show that our method is efficient, overcomes the weakness of the previous related works in extracting semantic information and outperforms the previous works of learning word representations using lexicons.", "creator": "LaTeX with hyperref package"}}}