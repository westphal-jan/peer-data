{"id": "1611.01116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Binary Paragraph Vectors", "abstract": "Recently weyhrauch Le & amp; 194.1 Mikolov bieszczady described light-brown two log - 279th linear models, obrad called kuning Paragraph Vector, vastic that can be used to 271-8978 learn viljo state - of - ambiguous the - scullers art multimers distributed representations of documents. Inspired magainin by shaya this lambar\u00e9n\u00e9 work gassings we tilak present kingman Binary maramon Paragraph 18.98 Vectors, 1.5175 simple neural networks that wesbury learn azzoni short binary codes for fast chirping information kavalier retrieval. 107-171 We show that iheartradio binary one-touch paragraph vectors distilleries outperform geragos autoencoder - d/l based binary detained codes, scahill despite using fewer bits. ispl We sediments also evaluate kulmiye their precision defilement in transfer kamark learning lassen settings, villaecija where binary codes are environs inferred lobanovs for d'argento documents unrelated to liel the training pochentong corpus. zoetemelk Results warnecke from these turnverein experiments eigenvalue indicate that 69-29 Binary Paragraph wakka Vectors kasai can capture 64.31 semantics flury relevant nuraghe for various vetter domain - albertz specific documents. Finally, we quarshie present klehr a turker model that welioya simultaneously damnatio learns pteropodidae short binary codes and jcw longer, commenting real - valued salafists representations. tonganoxie This agrichemical model can mccourt be used to rapidly aponte retrieve bogado a short matric list mini-tours of fatsis highly balzaretti relevant 814,000 documents lewin from jubera a large khordadian document collection.", "histories": [["v1", "Thu, 3 Nov 2016 18:10:35 GMT  (285kb,D)", "https://arxiv.org/abs/1611.01116v1", "Under review as a conference paper at ICLR 2017"], ["v2", "Mon, 14 Nov 2016 17:29:34 GMT  (285kb,D)", "http://arxiv.org/abs/1611.01116v2", "Under review as a conference paper at ICLR 2017"], ["v3", "Fri, 9 Jun 2017 14:33:06 GMT  (354kb,D)", "http://arxiv.org/abs/1611.01116v3", "Accepted to appear as a regular paper at the 2nd Workshop on Representation Learning for NLP at ACL 2017"]], "COMMENTS": "Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karol grzegorczyk", "marcin kurdziel"], "accepted": false, "id": "1611.01116"}, "pdf": {"name": "1611.01116.pdf", "metadata": {"source": "CRF", "title": "Binary Paragraph Vectors", "authors": ["Karol Grzegorczyk", "Marcin Kurdziel"], "emails": ["kgr@agh.edu.pl", "kurdziel@agh.edu.pl"], "sections": [{"heading": "1 Introduction", "text": "One of the significant challenges in contemporary information processing is the sheer volume of available data. Gantz and Reinsel (2012), for example, claim that the amount of digital data in the world doubles every two years. This trend underpins efforts to develop algorithms that can efficiently search for relevant information in huge datasets. One class of such algorithms, represented by, e.g., Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al., 2014). The codes can then be used to group\nthe data into buckets, thereby enabling sublinear search for relevant information, or for fast comparison of data items. Most of the algorithms from this family are data-oblivious, i.e. can generate hashes for any type of data. Nevertheless, some methods target specific kind of input data, like text or image.\nIn this work we focus on learning binary codes for text documents. An important work in this direction has been presented by Salakhutdinov and Hinton (2009). Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation. Salakhutdinov & Hinton report that binary codes allow for up to 20-fold improvement in document ranking speed, compared to real-valued representation of the same dimensionality. Moreover, they demonstrate that semantic hashing codes used as an initial document filter can improve precision of TFIDF-based retrieval. Learning binary representation from BOW, however, has its disadvantages. First, word-count representation, and in turn the learned codes, are not in itself stronger than TFIDF. Second, BOW is an inefficient representation: even for moderate-size vocabularies BOW vectors can have thousands of dimensions. Learning fully-connected autoencoders for such highdimensional vectors is impractical. Salakhutdinov & Hinton restricted the BOW vocabulary in their experiments to 2000 most frequent words.\nBinary codes have also been applied to crossmodal retrieval where text is one of the modalities. Specifically, Wang et al. (2013) incorporated tag information that often accompany text documents, while Masci et al. (2014) employed siamese neural networks to learn single binary representation for text and image data.\nRecently several works explored simple neural models for unsupervised learning of distributed\nar X\niv :1\n61 1.\n01 11\n6v 3\n[ cs\n.C L\n] 9\nJ un\n2 01\n7\nrepresentations of words, sentences and documents. Mikolov et al. (2013) proposed loglinear models that learn distributed representations of words by predicting a central word from its context (CBOW model) or by predicting context words given the central word (Skip-gram model). The CBOW model was then extended by Le and Mikolov (2014) to learn distributed representations of documents. Specifically, they proposed Paragraph Vector Distributed Memory (PV-DM) model, in which the central word is predicted given the context words and the document vector. During training, PV-DM learns the word embeddings and the parameters of the softmax that models the conditional probability distribution for the central words. During inference, word embeddings and softmax weights are fixed, but the gradients are backpropagated to the inferred document vector. In addition to PV-DM, Le & Mikolov studied also a simpler model, namely Paragraph Vector Distributed Bag of Words (PV-DBOW). This model predicts words in the document given only the document vector. It therefore disregards context surrounding the predicted word and does not learn word embeddings. Le & Mikolov demonstrated that paragraph vectors outperform BOW and bag-of-bigrams in information retrieval task, while using only few hundreds of dimensions. These models are also amendable to learning and inference over large vocabularies. Original CBOW network used hierarchical softmax to model the probability distribution for the central word. One can also use noise-contrastive estimation (Gutmann and Hyva\u0308rinen, 2010) or importance sampling (Cho et al., 2015) to approximate the gradients with respect to the softmax logits.\nAn alternative approach to learning representation of pieces of text has been recently described by Kiros et al. (2015). Networks proposed therein, inspired by the Skip-gram model, learn to predict surrounding sentences given the center sentence. To this end, the center sentence is encoded by an encoder network and the surrounding sentences are predicted by a decoder network conditioned on the center sentence code. Once trained, these models can encode sentences without resorting to backpropagation inference. However, they learn representations at the sentence level but not at the document level.\nIn this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-\nDM that learn short binary codes for text documents. One inspiration for binary paragraph vectors comes from a recent work by Lin et al. (2015) on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While Lin et al. (2015) employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents."}, {"heading": "2 Binary paragraph vector models", "text": "The basic idea in binary paragraph vector models is to introduce a sigmoid nonlinearity before the softmax that models the conditional probability of words given the context. If we then enforce binary or near-binary activations in this nonlinearity, the probability distribution over words will be conditioned on a bit vector context, rather than realvalued representation. The inference in the model proceeds like in Paragraph Vector, except the document code is constructed from the sigmoid activations. After rounding, this code can be seen as a distributed binary representation of the document.\nIn the simplest Binary PV-DBOW model (Figure 1) the dimensionality of the real-valued document embeddings is equal to the length of the binary codes. Despite this low dimensional representation \u2013 a useful binary hash will typically have 128 or fewer bits \u2013 this model performed surprisingly well in our experiments. Note that we cannot simply increase the embedding dimension-\nality in Binary PV-DBOW in order to learn better codes: binary vectors learned in this way would be too long to be useful in document hashing. The retrieval performance can, however, be improved by using binary codes for initial filtering of documents, and then using a representation with higher capacity to rank the remaining documents by their similarity to the query. Salakhutdinov and Hinton (2009), for example, used semantic hashing codes for initial filtering and TF-IDF for ranking. A similar document retrieval strategy can be realized with binary paragraph vectors. Furthermore, we can extend the Binary PV-DBOW model to simultaneously learn short binary codes and higherdimensional real-valued representations. Specifically, in the Real-Binary PV-DBOW model (Figure 2) we introduce a linear projection between the document embedding matrix and the sigmoid nonlinearity. During training, we learn the softmax parameters and the projection matrix. During inference, softmax weights and the projection matrix are fixed. This way, we simultaneously obtain a high-capacity representation of a document in the embedding matrix, e.g. 300-dimensional realvalued vector, and a short binary representation from the sigmoid activations. One advantage of\nusing the Real-Binary PV-DBOW model over two separate networks is that we need to store only one set of softmax parameters (and a small projection matrix) in the memory, instead of two large weight matrices. Additionally, only one model needs to be trained, rather than two distinct networks.\nBinary document codes can also be learned by extending distributed memory models. Le and Mikolov (2014) suggest that in PV-DM, a context of the central word can be constructed by either concatenating or averaging the document vector and the embeddings of the surrounding words. However, in Binary PV-DM (Figure 3) we always construct the context by concatenating the relevant vectors before applying the sigmoid nonlinearity. This way, the length of binary codes is not tied to\nthe dimensionality of word embeddings. Softmax layers in the models described above should be trained to predict words in documents given binary context vectors. Training should therefore encourage binary activations in the preceding sigmoid layers. This can be done in several ways. In semantic hashing autoencoders Salakhutdinov and Hinton (2009) added noise to the sigmoid coding layer. Error backpropagation then countered the noise, by forcing the activations to be close to 0 or 1. Another approach was used by Krizhevsky and Hinton (2011) in autoencoders that learned binary codes for small images. During the forward pass, activations in the coding layer were rounded to 0 or 1. Original (i.e. not rounded) activations were used when backpropagating errors. Alternatively, one could model the document codes with stochastic binary neurons. Learning in this case can still proceed with error backpropagation, provided that a suitable gradient estimator is used alongside stochastic activations. We experimented with the methods used in semantic hashing and Krizhevsky\u2019s autoencoders, as well as with the two biased gradient estimators for stochastic binary neurons discussed by Bengio et al. (2013). We also investigated the slope annealing trick (Chung et al., 2016) when training networks with stochastic binary activations. From our experience, binary paragraph vector models with rounded activations are easy to train and learn better codes than models with noise-based binarization or stochastic neurons. We therefore use Krizhevsky\u2019s binarization in our models."}, {"heading": "3 Experiments", "text": "To assess the performance of binary paragraph vectors, we carried out experiments on three\ndatasets: 20 Newsgroups1, a cleansed version (also called v2) of Reuters Corpus Volume 12 (RCV1) and English Wikipedia3. As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text. However, we removed stop words as well as words shorter than two characters and longer than 15 characters. Results reported by (Li et al., 2015) indicate that performance of PV-DBOW can be improved by including n-grams in the model. We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams. Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents. This amounts to a vocabulary with slightly over one million elements. For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements. In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.\nThe 20 Newsgroups dataset comes with reference train/test sets. In case of RCV1 we used half of the documents for training and the other half for evaluation. In case of English Wikipedia we held out for testing randomly selected 10% of the documents. We perform document retrieval by selecting queries from the test set and ordering other test documents according to the similarity of the inferred codes. We use Hamming distance for binary codes and cosine similarity for real-valued representations. Results are averaged over queries. We assess the performance of our models with precision-recall curves and two popular information retrieval metrics, namely mean average precision (MAP) and the normalized discounted cumulative gain at the 10th result (NDCG@10) (Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002). The results depend, of course, on the chosen document relevancy measure. Relevancy measure for the 20 Newsgroups dataset is straightforward: a retrieved document is relevant to the query if they both belong to the same newsgroup. In RCV1 each document belongs to a hierarchy of topics,\n1Available at http://qwone.com/\u02dcjason/ 20Newsgroups\n2Available at http://trec.nist.gov/data/ reuters/reuters.html\n3A snapshot from April 5th, 2016\nmaking the definition of relevancy less obvious. In this case we adopted the relevancy measure used by Salakhutdinov and Hinton (2009). That is, the relevancy is calculated as the fraction of overlapping labels in a retrieved document and the query document. Overall, our selection of test datasets and relevancy measures for 20 Newsgroups and RCV1 follows Salakhutdinov and Hinton (2009), enabling comparison with semantic hashing codes. To assess the relevancy of articles in English Wikipedia we can employ categories assigned to them. However, unlike in RCV1, Wikipedia categories can have multiple parent categories and cyclic dependencies. Therefore, for this dataset we adopted a simplified relevancy measure: two articles are relevant if they share at least one category. We also removed from the test set categories with less than 20 documents as well as documents that were left with no categories. Overall, the relevancy is measured over more than 11, 800 categories, making English Wikipedia harder than the other two benchmarks.\nWe use AdaGrad (Duchi et al., 2011) for training and inference in all experiments reported in this work. During training we employ dropout (Srivastava et al., 2014) in the embedding layer. To facilitate models with large vocabularies, we approximate the gradients with respect to the softmax logits using the method described by Cho et al. (2015). Binary PV-DM networks use the same number of dimensions for document codes and word embeddings.\nPerformance of 128- and 32-bit binary paragraph vector codes is reported in Table 1 and in Figure 4. For comparison we also report performance of real-valued paragraph vectors. Note that the binary codes perform very well, despite their far lower capacity: on 20 Newsgroups and RCV1 the 128-bit Binary PV-DBOW trained with bigrams approaches the performance of the real-valued paragraph vectors, while on English Wikipedia its performance is slightly lower. Furthermore, Binary PV-DBOW with bigrams outperforms semantic hashing codes: comparison of precision-recall curves from Figures 4a and 4b with Salakhutdinov and Hinton (2009, Figures 6 & 7) shows that 128-bit codes learned with this model outperform 128-bit semantic hashing codes on 20 Newsgroups and RCV1. Moreover, the 32- bit codes from this model outperform 128-bit semantic hashing codes on the RCV1 dataset, and\non the 20 Newsgroups dataset give similar precision up to approximately 3% recall and better precision for higher recall levels. Note that the difference in this case lies not only in retrieval precision: the short 32-bit Binary PV-DBOW codes are more efficient for indexing than long 128-bit semantic hashing codes.\nWe also compared binary paragraph vectors against codes constructed by first inferring short, real-valued paragraph vectors and then using a separate hashing algorithm for binarization. When the dimensionality of the paragraph vectors is equal to the size of binary codes, the number of network parameters in this approach is similar to that of Binary PV models. We experimented with two standard hashing algorithms, namely random hyperplane projection (Charikar, 2002) and iterative quantization (Gong and Lazebnik, 2011). Paragraph vectors in these experiments were inferred using PV-DBOW with bigrams. Results reported in Table 2 show no benefit from using a separate algorithm for binarization. On the 20 Newsgroups and RCV1 datasets Binary PV-DBOW yielded higher MAP than the two baseline approaches. On English Wikipedia iterative quantization achieved MAP equal to Binary PV-DBOW, while random hyperplane projec-\ntion yielded lower MAP. Some gain in precision of top hits can be observed for iterative quantization, as indicated by NDCG@10. However, precision of top hits can also be improved by querying with Real-Binary PV-DBOW model (Section 3.2). It is also worth noting that end-to-end inference in Binary PV models is more convenient than inferring real-valued vectors and then using another algorithm for hashing.\nLi et al. (2015) argue that PV-DBOW outperforms PV-DM on a sentiment classification task, and demonstrate that the performance of PVDBOW can be improved by including bigrams in the vocabulary. We observed similar results with Binary PV models. That is, including bigrams in the vocabulary usually improved retrieval precision. Also, codes learned with Binary PV-DBOW provided higher retrieval precision than Binary PV-DM codes. Furthermore, to choose the context size for the Binary PV-DM models, we evaluated several networks on validation sets taken out of the training data. The best results were obtained with a minimal one-word, one-sided context window. This is the distributed memory architecture most similar to the Binary PV-DBOW model."}, {"heading": "3.1 Transfer learning", "text": "In the experiments presented thus far we had at our disposal training sets with documents similar to the documents for which we inferred binary codes. One could ask a question, if it is possible to use binary paragraph vectors without collecting a domain-specific training set? For example, what if we needed to hash documents that are not associated with any available domain-specific corpus? One solution could be to train the model with a big generic text corpus, that covers a wide variety of domains. Lau and Baldwin (2016) evaluated this approach for real-valued paragraph vectors, with promising results. It is not obvious, however, whether short binary codes would also perform well in similar settings. To shed light on this question we trained Binary PV-DBOW with\nbigrams on the English Wikipedia, and then inferred binary codes for the test parts of the 20 Newsgroups and RCV1 datasets. The results are presented in Table 3 and in Figure 5. The model trained on an unrelated text corpus gives lower retrieval precision than models with domain-specific training sets, which is not surprising. However, it still performs remarkably well, indicating that the semantics it captured can be useful for different text collections. Importantly, these results were obtained without domain-specific finetuning."}, {"heading": "3.2 Retrieval with Real-Binary models", "text": "As pointed out by Salakhutdinov and Hinton (2009), when working with large text collections one can use short binary codes for indexing and a representation with more capacity for ranking. Following this idea, we proposed Real-Binary PVDBOW model (Section 2) that can simultaneously learn short binary codes and high-dimensional real-valued representations. We begin evaluation of this model by comparing retrieval precision of real-valued and binary representations learned by it. To this end, we trained a Real-Binary PVDBOW model with 28-bit binary codes and 300- dimensional real-valued representations on the 20 Newsgroups and RCV1 datasets. Results are reported in Figure 6. The real-valued representations learned with this model give lower precision than PV-DBOW vectors but, importantly, improve precision over binary codes for top ranked documents. This justifies their use alongside binary codes.\nUsing short binary codes for initial filtering of documents comes with a tradeoff between the retrieval performance and the recall level. For example, one can select a small subset of similar documents by using 28\u201332 bit codes and retrieving documents within small Hamming distance to the query. This will improve retrieval performance, and possibly also precision, at the cost of recall. Conversely, short codes provide a less finegrained hashing and can be used to index documents within larger Hamming distance to the\nquery. They can therefore be used to improve recall at the cost of retrieval performance, and possibly also precision. For these reasons, we evaluated Real-Binary PV-DBOW models with different code sizes and under different limits on the Hamming distance to the query. In general, we cannot expect these models to achieve 100% recall under the test settings. Furthermore, recall will vary on query-by-query basis. We therefore decided to focus on the NDCG@10 metric in this evaluation, as it is suited for measuring model performance when a short list of relevant documents is sought, and the recall level is not known. MAP and precision-recall curves are not applicable in\nthese settings. Information retrieval results for Real-Binary PV-DBOW are summarized in Table 4. The model gives higher NDCG@10 than 32-bit Binary PVDBOW codes (Table 1). The difference is large when the initial filtering is restrictive, e.g. when using 28-bit codes and 1-2 bit Hamming distance limit. Real-Binary PV-DBOW can therefore be useful when one needs to quickly find a short list of relevant documents in a large text collection, and the recall level is not of primary importance. If needed, precision can be further improved by using plain Binary PV-DBOW codes for filtering and standard DBOW representation for raking (Table 4, column B). Note, however, that PV-DBOW model would then use approximately 10 times more parameters than Real-Binary PV-DBOW."}, {"heading": "4 Conclusion", "text": "In this article we presented simple neural networks that learn short binary codes for text documents. Our networks extend Paragraph Vector by introducing a sigmoid nonlinearity before the softmax that predicts words in documents. Binary codes inferred with the proposed networks achieve higher retrieval precision than semantic hashing codes on two popular information retrieval benchmarks. They also retain a lot of their precision when trained on an unrelated text corpus. Finally, we presented a network that simultaneously learns short binary codes and longer, real-valued representations.\nThe best codes in our experiments were inferred with Binary PV-DBOW networks. The Binary PV-DM model did not perform so well. Li et al. (2015) made similar observations for Paragraph Vector models, and argue that in distributed memory model the word context takes a lot of the burden of predicting the central word from the document code. An interesting line of future research could, therefore, focus on models that account for word order, while learning good binary codes. It is also worth noting that Le and Mikolov (2014) constructed paragraph vectors by combining DM and DBOW representations. This strategy may proof useful also with binary codes, when employed with hashing algorithms designed for longer codes, e.g. with multi-index hashing (Norouzi et al., 2012)."}, {"heading": "Acknowledgments", "text": "This research is supported by National Science Centre, Poland grant no. 2013/09/B/ST6/01549 \u201cInteractive Visual Text Analytics (IVTA): Development of novel, user-driven text mining and visualization methods for large text corpora exploration.\u201d This research was carried out with the support of the \u201cHPC Infrastructure for Grand Challenges of Science and Engineering\u201d project, cofinanced by the European Regional Development Fund under the Innovative Economy Operational Programme. This research was supported in part by PL-Grid Infrastructure.\nA Visualization of Binary PV codes\nFor an additional comparison with semantic hashing, we used t-distributed Stochastic Neighbor Embedding (van der Maaten and Hinton, 2008) to construct two-dimensional visualizations of codes learned by Binary PV-DBOW with bigrams. We used the same subsets of newsgroups and RCV1 topics that were used by Salakhutdinov and Hinton (2009, Figure 5). Codes learned by Binary PV-DBOW (Figure 7) appear slightly more clustered."}], "references": [{"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron Courville."], "venue": "arXiv preprint arXiv:1308.3432 .", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Similarity estimation tech", "author": ["Moses S Charikar"], "venue": null, "citeRegEx": "Charikar.,? \\Q2002\\E", "shortCiteRegEx": "Charikar.", "year": 2002}, {"title": "On using very large", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2015\\E", "shortCiteRegEx": "Bengio.", "year": 2015}, {"title": "The digital universe in 2020: Big data, bigger digital shadows, and biggest growth in the far east", "author": ["John Gantz", "David Reinsel."], "venue": "Technical report, IDC.", "citeRegEx": "Gantz and Reinsel.,? 2012", "shortCiteRegEx": "Gantz and Reinsel.", "year": 2012}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Yunchao Gong", "Svetlana Lazebnik."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, pages 817\u2013824.", "citeRegEx": "Gong and Lazebnik.,? 2011", "shortCiteRegEx": "Gong and Lazebnik.", "year": 2011}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "International Conference on Artificial Intelligence and Statistics. pages 297\u2013304.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani."], "venue": "Proceedings of the thirtieth annual ACM symposium on Theory of computing. ACM, pages 604\u2013613.", "citeRegEx": "Indyk and Motwani.,? 1998", "shortCiteRegEx": "Indyk and Motwani.", "year": 1998}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen."], "venue": "ACM Transactions on Information Systems (TOIS) 20(4):422\u2013446.", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.,? 2002", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.", "year": 2002}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems. pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Using very deep autoencoders for content-based image retrieval", "author": ["Alex Krizhevsky", "Geoffrey E Hinton."], "venue": "Proceedings of the 19th European Symposium on Artificial Neural Networks. pages 489\u2013494.", "citeRegEx": "Krizhevsky and Hinton.,? 2011", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2011}, {"title": "An empirical evaluation of doc2vec with practical insights into document embedding generation", "author": ["Jey Han Lau", "Timothy Baldwin."], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP. Association for Computational Linguistics,", "citeRegEx": "Lau and Baldwin.,? 2016", "shortCiteRegEx": "Lau and Baldwin.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "Proceedings of The 31st International Conference on Machine Learning. pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Learning document embeddings by predicting n-grams for sentiment classification of long movie reviews", "author": ["Bofang Li", "Tao Liu", "Xiaoyong Du", "Deyuan Zhang", "Zhe Zhao."], "venue": "arXiv preprint arXiv:1512.08183 .", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Deep learning of binary hash codes for fast image retrieval", "author": ["Kevin Lin", "Huei Fang Yang", "Jen Hao Hsiao", "Chu Song Chen."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. pages 27\u201335.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Multimodal similarity-preserving hashing", "author": ["Jonathan Masci", "Michael M Bronstein", "Alexander M Bronstein", "J\u00fcrgen Schmidhuber."], "venue": "IEEE transactions on pattern analysis and machine intelligence 36(4):824\u2013830.", "citeRegEx": "Masci et al\\.,? 2014", "shortCiteRegEx": "Masci et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Fast search in hamming space with multiindex hashing", "author": ["Mohammad Norouzi", "Ali Punjani", "David J Fleet."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, pages 3108\u20133115.", "citeRegEx": "Norouzi et al\\.,? 2012", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "Semantic hashing", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton."], "venue": "International Journal of Approximate Reasoning 50(7):969\u2013978.", "citeRegEx": "Salakhutdinov and Hinton.,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research 9(Nov):2579\u20132605.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Hashing for similarity search: A survey", "author": ["Jingdong Wang", "Heng Tao Shen", "Jingkuan Song", "Jianqiu Ji."], "venue": "arXiv preprint arXiv:1408.2927 .", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Semantic hashing using tags and topic modeling", "author": ["Qifan Wang", "Dan Zhang", "Luo Si."], "venue": "Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval. ACM, pages 213\u2013222.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": ", Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al.", "startOffset": 29, "endOffset": 54}, {"referenceID": 20, "context": ", Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al., 2014).", "startOffset": 124, "endOffset": 143}, {"referenceID": 3, "context": "Gantz and Reinsel (2012), for example, claim that the amount of digital data in the world doubles every two years.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Gantz and Reinsel (2012), for example, claim that the amount of digital data in the world doubles every two years. This trend underpins efforts to develop algorithms that can efficiently search for relevant information in huge datasets. One class of such algorithms, represented by, e.g., Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al., 2014). The codes can then be used to group the data into buckets, thereby enabling sublinear search for relevant information, or for fast comparison of data items. Most of the algorithms from this family are data-oblivious, i.e. can generate hashes for any type of data. Nevertheless, some methods target specific kind of input data, like text or image. In this work we focus on learning binary codes for text documents. An important work in this direction has been presented by Salakhutdinov and Hinton (2009). Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation.", "startOffset": 0, "endOffset": 936}, {"referenceID": 3, "context": "Gantz and Reinsel (2012), for example, claim that the amount of digital data in the world doubles every two years. This trend underpins efforts to develop algorithms that can efficiently search for relevant information in huge datasets. One class of such algorithms, represented by, e.g., Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al., 2014). The codes can then be used to group the data into buckets, thereby enabling sublinear search for relevant information, or for fast comparison of data items. Most of the algorithms from this family are data-oblivious, i.e. can generate hashes for any type of data. Nevertheless, some methods target specific kind of input data, like text or image. In this work we focus on learning binary codes for text documents. An important work in this direction has been presented by Salakhutdinov and Hinton (2009). Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation. Salakhutdinov & Hinton report that binary codes allow for up to 20-fold improvement in document ranking speed, compared to real-valued representation of the same dimensionality. Moreover, they demonstrate that semantic hashing codes used as an initial document filter can improve precision of TFIDF-based retrieval. Learning binary representation from BOW, however, has its disadvantages. First, word-count representation, and in turn the learned codes, are not in itself stronger than TFIDF. Second, BOW is an inefficient representation: even for moderate-size vocabularies BOW vectors can have thousands of dimensions. Learning fully-connected autoencoders for such highdimensional vectors is impractical. Salakhutdinov & Hinton restricted the BOW vocabulary in their experiments to 2000 most frequent words. Binary codes have also been applied to crossmodal retrieval where text is one of the modalities. Specifically, Wang et al. (2013) incorporated tag information that often accompany text documents, while Masci et al.", "startOffset": 0, "endOffset": 2029}, {"referenceID": 3, "context": "Gantz and Reinsel (2012), for example, claim that the amount of digital data in the world doubles every two years. This trend underpins efforts to develop algorithms that can efficiently search for relevant information in huge datasets. One class of such algorithms, represented by, e.g., Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al., 2014). The codes can then be used to group the data into buckets, thereby enabling sublinear search for relevant information, or for fast comparison of data items. Most of the algorithms from this family are data-oblivious, i.e. can generate hashes for any type of data. Nevertheless, some methods target specific kind of input data, like text or image. In this work we focus on learning binary codes for text documents. An important work in this direction has been presented by Salakhutdinov and Hinton (2009). Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation. Salakhutdinov & Hinton report that binary codes allow for up to 20-fold improvement in document ranking speed, compared to real-valued representation of the same dimensionality. Moreover, they demonstrate that semantic hashing codes used as an initial document filter can improve precision of TFIDF-based retrieval. Learning binary representation from BOW, however, has its disadvantages. First, word-count representation, and in turn the learned codes, are not in itself stronger than TFIDF. Second, BOW is an inefficient representation: even for moderate-size vocabularies BOW vectors can have thousands of dimensions. Learning fully-connected autoencoders for such highdimensional vectors is impractical. Salakhutdinov & Hinton restricted the BOW vocabulary in their experiments to 2000 most frequent words. Binary codes have also been applied to crossmodal retrieval where text is one of the modalities. Specifically, Wang et al. (2013) incorporated tag information that often accompany text documents, while Masci et al. (2014) employed siamese neural networks to learn single binary representation for text and image data.", "startOffset": 0, "endOffset": 2121}, {"referenceID": 5, "context": "One can also use noise-contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) or importance sampling (Cho et al.", "startOffset": 46, "endOffset": 75}, {"referenceID": 13, "context": "Mikolov et al. (2013) proposed loglinear models that learn distributed representations of words by predicting a central word from its context (CBOW model) or by predicting context words given the central word (Skip-gram model).", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "The CBOW model was then extended by Le and Mikolov (2014) to learn distributed representations of documents.", "startOffset": 36, "endOffset": 58}, {"referenceID": 8, "context": "An alternative approach to learning representation of pieces of text has been recently described by Kiros et al. (2015). Networks proposed therein, inspired by the Skip-gram model, learn to predict surrounding sentences given the center sentence.", "startOffset": 100, "endOffset": 120}, {"referenceID": 13, "context": "One inspiration for binary paragraph vectors comes from a recent work by Lin et al. (2015) on learning binary codes for images.", "startOffset": 73, "endOffset": 91}, {"referenceID": 13, "context": "One inspiration for binary paragraph vectors comes from a recent work by Lin et al. (2015) on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While Lin et al. (2015) employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents.", "startOffset": 73, "endOffset": 684}, {"referenceID": 17, "context": "Salakhutdinov and Hinton (2009), for example, used semantic hashing codes for initial filtering and TF-IDF for ranking.", "startOffset": 0, "endOffset": 32}, {"referenceID": 11, "context": "Le and Mikolov (2014) suggest that in PV-DM, a context of the central word can be constructed by either concatenating or averaging the document vector and the embeddings of the surrounding words.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "In semantic hashing autoencoders Salakhutdinov and Hinton (2009) added noise to the sigmoid coding layer.", "startOffset": 33, "endOffset": 65}, {"referenceID": 7, "context": "Another approach was used by Krizhevsky and Hinton (2011) in autoencoders that learned binary codes for small images.", "startOffset": 29, "endOffset": 58}, {"referenceID": 0, "context": "We experimented with the methods used in semantic hashing and Krizhevsky\u2019s autoencoders, as well as with the two biased gradient estimators for stochastic binary neurons discussed by Bengio et al. (2013). We also investigated the slope annealing trick (Chung et al.", "startOffset": 183, "endOffset": 204}, {"referenceID": 12, "context": "Results reported by (Li et al., 2015) indicate that performance of PV-DBOW can be improved by including n-grams in the model.", "startOffset": 20, "endOffset": 37}, {"referenceID": 7, "context": "We assess the performance of our models with precision-recall curves and two popular information retrieval metrics, namely mean average precision (MAP) and the normalized discounted cumulative gain at the 10th result (NDCG@10) (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002).", "startOffset": 227, "endOffset": 258}, {"referenceID": 17, "context": "In this case we adopted the relevancy measure used by Salakhutdinov and Hinton (2009). That is, the relevancy is calculated as the fraction of overlapping labels in a retrieved document and the query document.", "startOffset": 54, "endOffset": 86}, {"referenceID": 17, "context": "In this case we adopted the relevancy measure used by Salakhutdinov and Hinton (2009). That is, the relevancy is calculated as the fraction of overlapping labels in a retrieved document and the query document. Overall, our selection of test datasets and relevancy measures for 20 Newsgroups and RCV1 follows Salakhutdinov and Hinton (2009), enabling comparison with semantic hashing codes.", "startOffset": 54, "endOffset": 340}, {"referenceID": 18, "context": "During training we employ dropout (Srivastava et al., 2014) in the embedding layer.", "startOffset": 34, "endOffset": 59}, {"referenceID": 18, "context": "During training we employ dropout (Srivastava et al., 2014) in the embedding layer. To facilitate models with large vocabularies, we approximate the gradients with respect to the softmax logits using the method described by Cho et al. (2015). Binary PV-DM networks use the same number of dimensions for document codes and word embeddings.", "startOffset": 35, "endOffset": 242}, {"referenceID": 1, "context": "We experimented with two standard hashing algorithms, namely random hyperplane projection (Charikar, 2002) and iterative quantization (Gong and Lazebnik, 2011).", "startOffset": 90, "endOffset": 106}, {"referenceID": 4, "context": "We experimented with two standard hashing algorithms, namely random hyperplane projection (Charikar, 2002) and iterative quantization (Gong and Lazebnik, 2011).", "startOffset": 134, "endOffset": 159}, {"referenceID": 10, "context": "Lau and Baldwin (2016) evaluated this approach for real-valued paragraph vectors, with promising results.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "As pointed out by Salakhutdinov and Hinton (2009), when working with large text collections one can use short binary codes for indexing and a representation with more capacity for ranking.", "startOffset": 18, "endOffset": 50}, {"referenceID": 16, "context": "with multi-index hashing (Norouzi et al., 2012).", "startOffset": 25, "endOffset": 47}, {"referenceID": 11, "context": "Li et al. (2015) made similar observations for Paragraph Vector models, and argue that in distributed memory model the word context takes a lot of the burden of predicting the central word from the document code.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "It is also worth noting that Le and Mikolov (2014) constructed paragraph vectors by combining DM and DBOW representations.", "startOffset": 29, "endOffset": 51}], "year": 2017, "abstractText": "Recently Le & Mikolov described two log-linear models, called Paragraph Vector, that can be used to learn state-ofthe-art distributed representations of documents. Inspired by this work, we present Binary Paragraph Vector models: simple neural networks that learn short binary codes for fast information retrieval. We show that binary paragraph vectors outperform autoencoder-based binary codes, despite using fewer bits. We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus. Results from these experiments indicate that binary paragraph vectors can capture semantics relevant for various domainspecific documents. Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations. This model can be used to rapidly retrieve a short list of highly relevant documents from a large document collection.", "creator": "LaTeX with hyperref package"}}}