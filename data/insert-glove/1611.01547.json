{"id": "1611.01547", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "j&s We pitre propose a ordinated language - agnostic mine-clearing way ongais of pete automatically generating mascarene sets 45.30 of 4b semantically coelestium similar cpsu clusters of lajcak entities along skolian with wansbeck sets of \" outlier \" gabay elements, tantalite which may varied then streater be blackball used nauset to jasin perform an intrinsic evaluation edule of misers word embeddings undercroft in catchall the outlier detection 66.5 task. extraordinary We ekin used orehek our methodology oceanographer to create chassidim a gold - amrish standard dataset, trita which we dubie call WikiSem500, and saloons evaluated multiple tubes state - of - the - mulherin art embeddings. verbandsliga The results show a respublika correlation archduke between keatley performance on this dataset and 17-15 performance radio-frequency on gasp\u00e9e sentiment three-movement analysis.", "histories": [["v1", "Fri, 4 Nov 2016 21:35:07 GMT  (2182kb,D)", "https://arxiv.org/abs/1611.01547v1", "12 pages"], ["v2", "Tue, 15 Nov 2016 13:21:17 GMT  (2182kb,D)", "http://arxiv.org/abs/1611.01547v2", "12 pages"], ["v3", "Fri, 9 Dec 2016 15:58:37 GMT  (5232kb,D)", "http://arxiv.org/abs/1611.01547v3", "12 pages"], ["v4", "Wed, 21 Dec 2016 17:51:57 GMT  (5233kb,D)", "http://arxiv.org/abs/1611.01547v4", "13 pages"], ["v5", "Wed, 5 Apr 2017 15:26:51 GMT  (5267kb,D)", "http://arxiv.org/abs/1611.01547v5", "Published as a workshop paper at ICLR 2017"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["philip blair", "yuval merhav", "joel barry"], "accepted": false, "id": "1611.01547"}, "pdf": {"name": "1611.01547.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Philip Blair", "Yuval Merhav"], "emails": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "High quality datasets for evaluating word and phrase representations are essential for building better models that can advance natural language understanding. Various researchers have developed and shared datasets for syntactic and semantic intrinsic evaluation. The majority of these datasets are based on word similarity (e.g., Finkelstein et al. (2001); Bruni et al. (2012); Hill et al. (2016)) and analogy tasks (e.g., Mikolov et al. (2013a;b)). While there has been a significant amount of work in this area which has resulted in a large number of publicly available datasets, many researchers have recently identified problems with existing datasets and called for further research on better evaluation methods (Faruqui et al., 2016; Gladkova et al., 2016; Hill et al., 2016; Avraham & Goldberg, 2016; Linzen, 2016; Batchkarov et al., 2016). A significant problem with word similarity tasks is that human bias and subjectivity result in low inter-annotator agreement and, consequently, human performance that is lower than automatic methods (Hill et al., 2016). Another issue is low or no correlation between intrinsic and extrinsic evaluation metrics (Chiu et al., 2016; Schnabel et al., 2015).\nRecently, Camacho-Collados & Navigli (2016) proposed the outlier detection task as an intrinsic evaluation method that improved upon some of the shortcomings of word similarity tasks. The task builds upon the \u201cword intrusion\u201d task initially described in Chang et al. (2009): given a set of words, the goal is to identify the word that does not belong in the set. However, like the vast majority of existing datasets, this dataset requires manual annotations that suffer from human subjectivity and bias, and it is not multilingual.\nInspired by Camacho-Collados & Navigli (2016), we have created a new outlier detection dataset that can be used for intrinsic evaluation of semantic models. The main advantage of our approach is that it is fully automated using Wikidata and Wikipedia, and it is also diverse in the number of included topics, words and phrases, and languages. At a high-level, our approach is simple: we view Wikidata as a graph, where nodes are entities (e.g., \u3008Chicago Bulls, Q128109\u3009, \u3008basketball team, Q13393265\u3009), edges represent \u201cinstance of\u201d and \u201csubclass of\u201d relations (e.g., \u3008Chicago Bulls, Q128109\u3009 is an instance of \u3008basketball team, Q13393265\u3009, \u3008basketball team, Q13393265\u3009 is a subclass of \u3008sports team, Q12973014\u3009), and the semantic similarity between two entities is inversely proportional to their graph distance (e.g., \u3008Chicago Bulls, Q128109\u3009 and \u3008Los Angeles Lakers, Q121783\u3009 are semantically similar since they are both instance of \u3008basketball team, Q13393265\u3009). This way we can form semantic clusters\nar X\niv :1\n61 1.\n01 54\n7v 5\n[ cs\n.C L\n] 5\nA pr\n2 01\nby picking entities that are members of the same class, and picking outliers with different notions of dissimilarity based on their distance from the cluster entities.\nWe release the first version of our dataset, which we call WikiSem500, to the research community. It contains around 500 per-language cluster groups for English, Spanish, German, Chinese, and Japanese (a total of 13,314 test cases). While we have not studied yet the correlation between performance on this dataset and various downstream tasks, our results show correlation with sentiment analysis. We hope that this diverse and multilingual dataset will help researchers to advance the state-of-the-art of word and phrase representations."}, {"heading": "2 RELATED WORK", "text": "Word similarity tasks have been popular for evaluating distributional similarity models. The basic idea is having annotators assigning similarity scores for word pairs. Models that can automatically assign similarity scores to the same word pairs are evaluated by computing the correlation between their and the human assigned scores. Schnabel et al. (2015) and Hill et al. (2016) review many of these datasets. Hill et al. (2016) also argue that the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. Their main argument is that many such benchmarks measure association and relatedness and not necessarily similarity, which limits their suitability for a wide range of applications. One of their motivating examples is the word pair \u201ccoffee\u201d and \u201ccup,\u201d which have high similarity ratings in some benchmarks despite not being very similar. Consequently, they developed guidelines that distinguish between association and similarity and used five hundred Amazon Mechanical Turk annotators to create a new dataset called SimLex-999, which has higher inter annotator agreement than previous datasets. Avraham & Goldberg (2016) improved this line of work further by redesigning the annotation task from rating scales to ranking, in order to alleviate bias, and also redefined the evaluation measure to penalize models more for making wrong predictions on reliable rankings than on unreliable ones.\nAnother popular task is based on word analogies. The analogy dataset proposed by Mikolov et al. (2013a) has become a standard evaluation set. The dataset contains fourteen categories, but only about half of them are for semantic evaluation (e.g. \u201cUS Cities\u201d, \u201cCommon Capitals\u201d, \u201cAll Capitals\u201d). In contrast, WikiSem500 contains hundreds of categories, making it a far more diverse and challenging dataset for the general-purpose evaluation of word representations. The Mikolov dataset has the advantage of additionally including syntactic categories, which we have left for future work.\nCamacho-Collados & Navigli (2016) addressed some of the issues mentioned previously by proposing the outlier detection task. Given a set of words, the goal is to identify the word that does not belong in the set. Their pilot dataset consists of eight different topics each made up of a cluster of eight words and eight possible outliers. Four annotators were used for the creation of the dataset. The main advantage of this dataset is its near perfect human performance. However, we believe a major reason for that is the specific choice of clusters and the small size of the dataset."}, {"heading": "3 GENERATING THE DATASET", "text": "In a similar format to the one used in the dataset furnished by Camacho-Collados & Navigli (2016), we generated sets of entities which were semantically similar to one another, known as a \u201ccluster\u201d, followed by up to three pairs (as available) of dissimilar entities, or \u201coutliers\u201d, each with different levels of semantic similarity to the cluster. The core thesis behind our design is that our knowledge base, Wikidata (2016), can be treated like a graph, where the semantic similarity between two elements is inversely proportional to their graph distance.\nInformally, we treat Wikidata entities which are instances of a common entity as a cluster (see Figure 1). Then, starting from that common entity (which we call a \u2018class\u2019), we follow \u201csubclass of\u201d relationships to find a sibling class (see \u201cAmerican Football Team\u201d in Figure 1). Two items which are instances of the sibling class (but not instances of the original class) are chosen as outliers. The process is then repeated with a \u2018cousin\u2019 class with a common grandparent to the original class (see \u201cIce Hockey Team\u201d in Figure 1). Finally, we choose two additional outliers by randomly selecting items which are a distance of at least 7 steps away from the original class. These three \u201coutlier classes\u201d are referred to as O1, O2, and O3 outlier classes, respectively.\nA full formalization of our approach is described in Appendix A."}, {"heading": "3.1 REFINING THE DATASET QUALITY", "text": "Prior to developing a framework to improve the quality of the generated dataset, we performed a small amount of manual pruning of our Wikidata graph. Disambiguation pages led to bizarre clusters of entities, for their associated relationships are not true semantic connections, but are instead artifacts of the structure of our knowledge base. As such, they were removed. Additionally, classes within a distance of three from the entity for \u201cEntity\u201d itself1 (Q35120) had instances which had quite weak semantic similarity (one example being \u201chuman\u201d). We decided that entities at this depth range ought to be removed from the Wikidata graph as well.\nOnce our Wikidata dump was pruned, we employed a few extra steps at generation time to further improve the quality of the dataset; first and foremost were how we chose representative instances and outliers for each class (see \u03c3i and \u03c3o in Appendix A). While \u201cSan Antonio Spurs\u201d and \u201cChicago Bulls\u201d may both be instances of \u201cbasketball team\u201d, so are \u201cBC Andorra\u201d and \u201cOlimpia Milano.\u201d We wanted the cluster entities to be as strongly related as possible, so we sought a class-agnostic heuristic to accomplish this. Ultimately, we found that favoring entities whose associated Wikipedia pages had higher sitelink counts gave us the desired effect.\nAs such, we created clusters by choosing the top eight instances of a given class, ranked by sitelink count. Additionally, we only chose items as outliers when they had at least ten sitelinks so as to remove those which were \u2018overly obscure,\u2019 for the ability of word embeddings to identify rare words (Schnabel et al., 2015) would artificially decrease the difficulty of such outliers.\nWe then noticed that many cluster entities had similarities in their labels that could be removed if a different label was chosen. For example, 80% of the entities chosen for \u201cassociation football club\u201d ended with the phrase \u201cF.C.\u201d This essentially invalidates the cluster, for the high degree of syntactic overlap artificially increases the cosine similarity of all cluster items in word-level embeddings. In order to increase the quality of the surface forms chosen for each entity, we modified our resolution of entity QIDs to surface forms (see \u03c4 in Appendix A) to incorporate a variant2 of the work from\n1Q35120 is effectively the \u201croot\u201d node of the Wikidata graph; 95.5% of nodes have \u201csubclass of\u201d chains which terminate at this node.\n2By \u2018variant,\u2019 we are referring to the fact that the dictionaries in which we perform the probability lookups are constructed for each language, as opposed to the cross-lingual dictionaries originally described by Spitkovsky & Chang (2012).\nSpitkovsky & Chang (2012):\n\u03c4(QID) = argmax s {P (s | wikipedia page(QID))} (1)\nThat is, the string for an entity is the string which is most likely to link to the Wikipedia page associated with that entity. For example, half of the inlinks to the page for Manchester United FC are the string \u201cManchester United,\u201d which is the colloquial way of referring to the team.\nNext, we filter out remaining clusters using a small set of heuristics. The following clusters are rejected:\n\u2022 Clusters with more than two items are identical after having all digits removed. This handles cases such as entities only differing by years (e.g. \u201cJanuary 2010,\u201d \u201cJanuary 2012,\u201d etc.).\n\u2022 Clusters with more than three elements have identical first or last six characters3. Characters are compared instead of words in order to better support inflected languages. This was inspired by clusters for classes such as \u201ccounties of Texas\u201d (Q11774097), where even the dictionary-resolved aliases have high degrees of syntactic overlap (namely, over half of the cluster items ended with the word \u201cCounty\u201d).\n\u2022 Clusters in which any item has an occurrence of a \u2018stop affix,\u2019 such as the prefix \u201cCategory:\u201d or the suffix \u201c\u4e00\u89a7\u201d (a Japanese Wikipedia equivalent of \u201cList of\u201d). In truth, this could be done during preprocessing, but doing it at cluster generation time instead has no bearing on the final results. These were originally all included under an additional stop class (\u201cWikimedia page outside the main knowledge tree\u201d) at prune time, but miscategorizations in the Wikidata hierarchy prevented us from doing so; for example, a now-removed link resulted in every country being pruned from the dataset. As such, we opted to take a more conservative approach and perform this on at cluster-generation time and fine tune our stoplist as needed.\n\u2022 Clusters with more than one entity with a string length of one. This prevents clusters such as \u201cletters of the alphabet\u201d being created. Note that this heuristic was disabled for the creation of Chinese and Japanese clusters.\n\u2022 Clusters with too few entities, after duplicates introduced by resolving entities to surface forms (\u03c4 ) are removed."}, {"heading": "3.2 THE WIKISEM500 DATASET", "text": "Using the above heuristics and preprocessing, we have generated a dataset, which we call WikiSem5004. Our dataset is formatted as a series of files containing test groups, comprised of a cluster and a series of outliers. Test cases can be constructed by taking each outlier in a given group with that group\u2019s cluster. Table 1 shows the number of included test groups and test cases for each language. Each group contains a cluster of 7-8 entities and up to two entities from each of the three outlier classes. Table 2 shows example clusters taken from the dataset."}, {"heading": "4 EVALUATION", "text": "For clarity, we first restate the definitions of the scoring metrics defined by Camacho-Collados & Navigli (2016) in terms of test groups (in contrast to the original definition, which is defined in terms of test cases). The way in which out-of-vocabulary entities are handled and scores are reported makes this distinction important, as will be seen in Section 4.3.\n3 For Chinese and Japanese, this is modified such that the at least six entities must have identical (non-kana) first or last characters, or more than three must have identical the same first or last two characters. Because English is not inflected, we simply use spaces as approximate word boundaries and check that the first or last of those does not occur too often.\n4The dataset is available for download at https://github.com/belph/wiki-sem-500\nThe core measure during evaluation is known as the compactness score; given a set W of words, it is defined as follows:\n\u2200w \u2208W, c(w) = 1 (|W | \u2212 1)(|W | \u2212 2) \u2211 wi\u2208W\\{w} \u2211 wj\u2208W\\{w}\nwj 6=wi\nsim(wi, wj) (2)\nwhere sim is a vector similarity measure (typically cosine similarity). Note that Camacho-Collados & Navigli (2016) reduces the asymptotic complexity of c(w) from O(n3) to O(n2). We denote P (W,w) to be the (zero-indexed) position of w in the list of elements of W , sorted by compactness score in descending order. From this, we can describe the following definition for Outlier Position (OP), where \u3008C,O\u3009 is a test group and o \u2208 O:\nOP (C \u222a {o}) = P (C \u222a {o}, o) (3)\nThis gives rise to the boolean-valued Outlier Detection (OD) function:\nOD(C \u222a {o}) = { 1 OP (C \u222a {o}) = |C| 0 otherwise\n(4)\nFinally, we can now describe the Outlier Position Percentage (OPP) and Accuracy scores:\nOPP (D) =\n\u2211 \u3008C,O\u3009\u2208D \u2211 o\u2208O\nOP (C\u222a{o}) |C|\u2211\n\u3008C,O\u3009\u2208D|O| (5)\nAccuracy(D) =\n\u2211 \u3008C,O\u3009\u2208D \u2211 o\u2208O OD(C \u222a {o})\u2211\n\u3008C,O\u3009\u2208D|O| (6)"}, {"heading": "4.1 HANDLING OUT-OF-VOCABULARY WORDS", "text": "One thing Camacho-Collados & Navigli (2016) does not address is how out-of-vocabulary (OOV) items should be handled. Because our dataset is much larger and contains a wider variety of words, we have extended their work to include additional scoring provisions which better encapsulate the performance of vector sets trained on different corpora.\nThere are two approaches to handling out-of-vocabulary entities: use a sentinel vector to represent all such entities or discard such entities entirely. The first approach is simpler, but it has a number of drawbacks; for one, a poor choice of sentinel can have a drastic impact on results. For example, an implementation which uses the zero vector as a sentinel and defines sim(~x,~0) = 0\u2200~x places many non-out-of-vocabulary outliers at a large disadvantage in a number of vector spaces, for we have found that negative compactness scores are rare. The second approach avoids deliberately introducing invalid data into the testing evaluation, but comparing scores across vector embeddings with different vocabularies is difficult due to them having different in-vocabulary subsets of the test set.\nWe have opted for the latter approach, computing the results on both the entire dataset and on only the intersection of in-vocabulary entities between all evaluated vector embeddings. This allows us to compare embedding performance both when faced with the same unknown data and when evaluated on the same, in-vocabulary data."}, {"heading": "4.2 HUMAN BASELINE", "text": "In order to gauge how well embeddings should perform on our dataset, we conducted a human evaluation. We asked participants to select the outlier from a given test case, providing us with a human baseline for the accuracy score on the dataset. We computed the non-out-of-vocabulary intersection of the embeddings shown in Table 4, from which 60 test groups were sampled. Due to the wide array of domain knowledge needed to perform well on the dataset, participants were allowed to refer to Wikipedia (but explicitly told not to use Wikidata). We collected 447 responses, with an overall precision of 68.9%.\nThe performance found is not as high as on the baseline described in Camacho-Collados & Navigli (2016), so we conducted a second human evaluation on a smaller hand-picked set of clusters in order to determine whether a lack of domain knowledge or a systemic issue with our method was to blame. We had 6 annotators fully annotate 15 clusters generated with our system. Each cluster had one outlier, with a third of the clusters having each of the three outlier classes. Human performance was at 93%, with each annotator missing exactly one cluster. Five out of the six annotators missed the same cluster, which was based on books and contained an O1 outlier (the most difficult class). We interviewed the annotators, and three of them cited a lack of clarity on Wikipedia over whether or not the presented outlier was a book (leading them to guess), while the other two cited a conflation with one of the book titles and a recently popular Broadway production.\nWith the exception of this cluster, the performance was near-perfect, with one annotator missing one cluster. Consequently, we believe that the lower human performance on our dataset is primarily a result of the dataset\u2019s broad domain."}, {"heading": "4.3 EMBEDDING RESULTS", "text": "We evaluated our dataset on a number of publicly available vector embeddings: the Google Newstrained CBOW model released by Mikolov et al. (2013a), the 840-billion token Common Crawl corpus-trained GloVe model released by Pennington et al. (2014), and the English, Spanish, German, Japanese, and Chinese MultiCCA vectors5 from Ammar et al. (2016), which are trained on a combination of the Europarl (Koehn, 2005) and Leipzig (Quasthoff et al., 2006) corpora. In ad-\n5The vectors are word2vec CBOW vectors, and the non-English vectors are aligned to the English vector space. Reproducing the original (unaligned) non-English vectors yields near-identical results to the aligned vectors.\ndition, we trained GloVe, CBOW, and Skip-Gram (Mikolov et al., 2013a) models on an identical corpus comprised of an English Wikipedia dump and Gigaword corpus6.\nThe bulk of the embeddings we evaluated were word embeddings (as opposed to phrase embeddings), so we needed to combine each embeddings\u2019 vectors in order to represent multi-word entities. If the embedding does handle phrases (only Google News), we perform a greedy lookup for the longest matching subphrase in the embedding, averaging the subphrase vectors; otherwise, we take a simple average of the vectors for each token in the phrase. If a token is out-of-vocabulary, it is ignored. If all tokens are out-of-vocabulary, the entity is discarded. This check happens as a preprocessing step in order to guarantee that a test case does not have its outlier thrown away. As such, we report the percentage of cluster entities filtered out for being out-of-vocabulary separately from the outliers which are filtered out, for the latter results in an entire test case being discarded.\nIn order to compare how well each vector embedding would do when run on unknown input data, we first collected the scores of each embedding on the entire dataset. Table 3 shows the Outlier Position Percentage (OPP) and accuracy scores of each embedding, along with the number of test groups which were skipped entirely7 and the mean percentage of out-of-vocabulary cluster entities and outliers among all test groups8. As in Camacho-Collados & Navigli (2016), we used cosine similarity for the sim measure in Equation 2.\nThe MultiCCA (Leipzig+Europarl) CBOW vectors have the highest rate of out-of-vocabulary entities, likely due in large part to the fact that its vocabulary is an order of magnitude smaller than the other embeddings (176,691, while the other embeddings had vocabulary sizes of over 1,000,000). Perhaps most surprising is the below-average performance of the Google News vectors. While attempting to understand this phenomenon, we noticed that disabling the phrase vectors boosted performance; as such, we have reported the performance of the vectors with and without phrase vectors enabled.\nInspecting the vocabulary of the Google News vectors, we have inferred that the vocabulary has undergone some form of normalization; performing the normalizations which we can be reasonably certain were done before evaluating has a negligible impact (\u2248 +0.01%) on the overall score. The Google News scores shown in Table 3 are with the normalization enabled. Ultimately, we hypothesize that the discrepancy in Google News scores comes down to the training corpus. We observe a bias in performance on our training set towards Wikipedia-trained vectors (discussed below; see Table 5), and, additionally, we expect that the Google News corpus did not have the wide regional\n6We used the July 2016 Wikipedia dump (Wikimedia, 2016) and the 2011 Gigaword corpus (Parker et al., 2011).\n7This happens when either all outliers are out-of-vocabulary or fewer than two cluster items are invocabulary. No meaningful evaluation can be performed on the remaining data, so the group is skipped.\n8This includes the out-of-vocabulary rates of the skipped groups.\ncoverage that Wikidata has, limiting the training exposure to many of the more niche classes in the training set.\nIn order to get a better comparison between the embeddings under identical conditions, we then took the intersection of in-vocabulary entities across all embeddings and reevaluated on this subset. 23.88% of cluster entities and 22.37% of outliers were out-of-vocabulary across all vectors, with 23 test groups removed from evaluation. Table 4 shows the results of this evaluation.\nThe scores appear to scale roughly linearly when compared to Table 3, but these results serve as a more reliable \u2018apples to apples\u2019 comparison of the algorithms and training corpora.\nBecause Wikidata was the source of the dataset, we analyzed how using Wikipedia as a training corpus influenced the evaluation results. We trained three GloVe models with smaller vocabularies: one trained on only Gigaword, one trained on only Wikipedia, and one trained on both. The results of evaluating on the embeddings\u2019 common intersection are shown in Table 5. We observe a slight (\u2248 3.15% relative change) bias in OPP scores with Wikipedia over Gigaword, while finding a significantly larger (\u2248 19.12% relative change) bias in accuracy scores. We believe that this bias is acceptable, for OPP scores (which we believe to be more informative) are not as sensitive to the bias and the numerous other factors involved in embedding generation (model, window size, etc.) can still be compared by controlling for the training corpora.\nAdditionally, we wanted to verify that the O1 outlier class (most similar) was the most difficult to distinguish from the cluster entities, followed by the O2 and O3 classes. We generated three separate datasets, each with only one class of outliers, and evaluated each embedding on each dataset. Figure 2 illustrates a strong positive correlation between outlier class and both OPP scores and accuracy.\nFinally, we used the non-English MultiCCA vectors (Ammar et al., 2016) to evaluate the multilingual aspect of our dataset. We expect to see Spanish and German perform similarly to the English Europarl+Leipzig vectors, for the monolingual training corpora used to generate them consisted of\nSpanish and German equivalents of the English training corpus. Table 6 shows the results of the non-English evaluations.\nWe observe a high degree of consistency with the results of the English vectors. The Japanese and Chinese scores are somewhat lower, but this is likely due to their having smaller training corpora and more limited vocabularies than their counterparts in other languages."}, {"heading": "4.4 CORRELATION WITH DOWNSTREAM PERFORMANCE", "text": "In light of recent concerns raised about the correlation between intrinsic word embedding evaluations and performance in downstream tasks, we sought to investigate the correlation between WikiSem500 performance and extrinsic evaluations. We used the embeddings from Schnabel et al. (2015) and ran the outlier detection task on them with our dataset.\nAs a baseline measurement of how well our dataset correlates with performance on alternative intrinsic tasks, we compared our evaluation with the scores reported in Schnabel et al. (2015) on the well-known analogy task (Mikolov et al., 2013a). Figure 3a illustrates strong correlations between analogy task performance and our evaluation\u2019s OPP scores and accuracy.\nFigure 3b displays the Pearson\u2019s correlation between the performance of each embedding on the WikiSem500 dataset and the extrinsic scores of each embedding on noun-phrase chunking and sentiment analysis reported in Schnabel et al. (2015).\nSimilar to the results seen in the paper, performance on our dataset correlates strongly with performance on a semantic-based task (sentiment analysis), with Pearson\u2019s correlation coefficients higher than 0.97 for both accuracy and OPP scores. On the other hand, we observe a weak-to-nonexistent correlation with chunking. This is expected, however, for the dataset we have constructed consists of items which differ in semantic meaning; syntactic meaning is not captured by the dataset. It is worth noting the inconsistency between this and the intrinsic results in Figure 3a, which indicate a stronger correlation with the syntactic subset of the analogy task than its semantic subset. This is\nexpected, for it agrees with the poor correlation between chunking and intrinsic performance shown in Schnabel et al. (2015)."}, {"heading": "5 FUTURE WORK", "text": "Due to the favorable results we have seen from the WikiSem500 dataset, we intend to release test groups in additional languages using the method described in this paper. Additionally, we plan to study further the downstream correlation of performance on our dataset with additional downstream tasks.\nMoreover, while we find a substantial correlation between performance on our dataset and on a semantically-based extrinsic task, the relationship between performance and syntactically-based tasks leaves much to be desired. We believe that the approach taken in this paper to construct our dataset could be retrofitted to a system such as WordNet (2010) or Wiktionary (2016) (for multilingual data) in order to construct syntactically similar clusters of items in a similar manner. We hypothesize that performance on such a dataset would correlate much more strongly with syntactically-based extrinsic evaluations such as chunking and part of speech tagging."}, {"heading": "6 CONCLUSION", "text": "We have described a language-agnostic technique for generating a dataset consisting of semantically related items by treating a knowledge base as a graph. In addition, we have used this approach to construct the WikiSem500 dataset, which we have released. We show that performance on this dataset correlates strongly with downstream performance on sentiment analysis. This method allows for creation of much larger scale datasets in a larger variety of languages without the time-intensive task of human creation. Moreover, the parallel between Wikidata\u2019s graph structure and the annotation guidelines from Camacho-Collados & Navigli (2016) preserve the simple-to-understand structure of the original dataset."}, {"heading": "A FORMALIZATION", "text": "We now provide a formal description of the approach taken to generate our dataset.\nLet V be the set of entities in Wikidata. For all v1, v2 \u2208 V , we denote the relations v1\u227aIv2 when v1 is an instance of v2, and v1\u227aSv2 when v1 is a subclass of v2. We then define I : V \u2192 V \u2217 as the following \u2018instances\u2019 mapping:\nI(v) = {v\u2032 \u2208 V | v\u2032\u227aIv} (7)\nFor convenience, we then denote C = {v \u2208 V | |I(v)| \u2265 2}; the interpretation being that C is the set of entities which have enough instances to possibly be viable clusters. We now formally state the following definition: Definition 1. A set A \u2286 V is a cluster if A = I(v) for some v \u2208 C. We additionally say that v is the class associated with the cluster A.\nLet P : V \u2192 V \u2217 be the following \u2018parent of\u2019 mapping:\nP (v) = {v\u2032 \u2208 V | v\u227aSv\u2032} (8)\nFurthermore, let P\u22121 : V \u2192 V \u2217 be the dual of P :\nP\u22121(v) = {v\u2032 \u2208 V | v\u2032\u227aSv} (9)\nFor additional convenience, we denote the following:\nP k(v) = { P (v) k = 1\u22c3\nv\u2032\u2208P (v) P k\u22121(v\u2032) k > 1\n(10)\nAs an abuse of notation, we define the following:\nI\u2217(v) = I(v) \u222a  \u22c3 v\u2032\u2208P\u22121(v) I\u2217(v)  (11) That is, I\u2217(v) is the set of all instances of v and all instances of anything that is a subclass of v (recursively).\nWe then define the measure d : V \u00d7 V \u2192 N to be the graph distance between any entities in V , using the following set of edges:\nESU = {(v1, v2) | v1\u227aSv2 \u2228 v2\u227aSv1} (12)\nFinally, we define9 three additional mappings for outliers parametrized10 by \u00b5 \u2208 N+:\nO1(v) =  \u22c3 p\u2208P (v)  \u22c3 c\u2208P\u22121(p)\\{v} I\u2217(c)  \\ I(v) (13) 9For the definition of O2, note that we do not say that it must be true that p \u2208 P 2(v) \\ P (v). In practice, however, avoiding (if not excluding) certain values of p in this manner can help improve the quality of resulting clusters, at the cost of reducing the number of clusters which can be produced.\n10The WikiSem500 dataset was generated with a value of \u00b5 = 7.\nO2(v) =  \u22c3 p\u2208P 2(v)  \u22c3 c\u2208P\u22121(p)\\{v} I\u2217(c)  \\ I(v) (14) O3(v) =\n \u22c3 p\u2208P (v) {e \u2208 I(v\u2032) | \u00b5 \u2264 d(p, v\u2032)}  \\ I(v) (15) To simplify the model, we assume that all three of the above sets are mutually exclusive. Given these, we can formally state the following definition: Definition 2. Let A = I(v) be a cluster based on a class v. An outlier for A is any o \u2208 O1(v) \u222a O2(v)\u222aO3(v). If o is in O1(v), O2(v), or O3(v), we denote the outlier class of o as O1, O2, or O3 (respectively).\nIntuitively, the three outlier classes denote different degrees of \u2018dissimilarity\u2019 from the original cluster; O1 outliers are the most challenging to distinguish, for they are semantically quite similar to the cluster. O2 outliers are slightly easier to distinguish, and O3 outliers should be quite simple to pick out.\nThe final dataset (a set of \u3008cluster, outliers\u3009 pairs) is then created by serializing the following:\nD = \u03c4 ( fD (\u22c3 c\u2208C \u3008fi(\u03c3i[I(c)]), fo (\u03c3o[O1(c)] \u222a \u03c3o[O2(c)] \u222a \u03c3o[O3(c)])\u3009 )) (16)\nWhere \u03c3i and \u03c3o are functions which select up to a given number of elements from the given set of instances and outliers (respectively), and fD, fi, and fo are functions which filter out dataset elements, instances, and outliers (respectively) based on any number of heuristics (see Section 3.1). Finally, \u03c4 takes the resulting tuples and resolves their QIDs to the appropriate surface strings.\nThe benefit of stating the dataset in the above terms is that it is highly configurable. In particular, different languages can be targeted by simply changing \u03c4 to resolve Wikidata entities to their labels in that language."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \u201coutlier\u201d elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "creator": "LaTeX with hyperref package"}}}