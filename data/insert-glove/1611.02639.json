{"id": "1611.02639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Gradients of Counterfactuals", "abstract": "Gradients have bergamasque been magdal\u00e9na used to dujarric quantify changxing feature purvanchal importance 1,261 in www.dell.com machine learning models. Unfortunately, rolling-stock in 28.97 nonlinear heavenly deep dinard networks, garten not only melaku individual euro422 neurons simulans but letowski also the gorgets whole network sharply can saturate, prewashed and as jus a result an taiseer important input piipari feature opaline can piombino have a tiny gradient. We demure study 7.5-billion various peetam networks, medevedev and lodja observe aw139 that this d\u2019or phenomena is indeed widespread, across southam many inputs.", "histories": [["v1", "Tue, 8 Nov 2016 18:10:44 GMT  (2193kb,D)", "http://arxiv.org/abs/1611.02639v1", null], ["v2", "Tue, 15 Nov 2016 19:55:26 GMT  (2193kb,D)", "http://arxiv.org/abs/1611.02639v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["mukund sundararajan", "ankur taly", "qiqi yan"], "accepted": false, "id": "1611.02639"}, "pdf": {"name": "1611.02639.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mukund Sundararajan", "Ankur Taly"], "emails": ["mukunds@google.com", "ataly@google.com", "qiqiyan@google.com"], "sections": [{"heading": null, "text": "Gradients have been used to quantify feature importance in machine learning models. Unfortunately, in nonlinear deep networks, not only individual neurons but also the whole network can saturate, and as a result an important input feature can have a tiny gradient. We study various networks, and observe that this phenomena is indeed widespread, across many inputs.\nWe propose to examine interior gradients, which are gradients of counterfactual inputs constructed by scaling down the original input. We apply our method to the GoogleNet architecture for object recognition in images, as well as a ligand-based virtual screening network with categorical features and an LSTM based language model for the Penn Treebank dataset. We visualize how interior gradients better capture feature importance. Furthermore, interior gradients are applicable to a wide variety of deep networks, and have the attribution property that the feature importance scores sum to the the prediction score.\nBest of all, interior gradients can be computed just as easily as gradients. In contrast, previous methods are complex to implement, which hinders practical adoption."}, {"heading": "1 INTRODUCTION", "text": "Practitioners of machine learning regularly inspect the coefficients of linear models as a measure of feature importance. This process allows them to understand and debug these models. The natural analog of these coefficients for deep models are the gradients of the prediction score with respect to the input. For linear models, the gradient of an input feature is equal to its coefficient. For deep nonlinear models, the gradient can be thought of as a local linear approximation (Simonyan et al. (2013)). Unfortunately, (see the next section), the network can saturate and as a result an important input feature can have a tiny gradient.\nWhile there has been other work (see Section 2.8) to address this problem, these techniques involve instrumenting the network. This instrumentation currently involves significant developer effort because they are not primitive operations in standard machine learning libraries. Besides, these techniques are not simple to understand\u2014they invert the operation of the network in different ways, and have their own pecularities\u2014for instance, the feature importances are not invariant over networks that compute the exact same function (see Figure 14).\nIn contrast, the method we propose builds on the very familiar, primitive concept of the gradient\u2014all it involves is inspecting the gradients of a few carefully chosen counterfactual inputs that are scaled versions of the initial input. This allows anyone who knows how to extract gradients\u2014presumably even novice practioners that are not very familiar with the network\u2019s implementation\u2014to debug the network. Ultimately, this seems essential to ensuring that deep networks perform predictably when deployed.\nar X\niv :1\n61 1.\n02 63\n9v 1\n[ cs\n.L G\n] 8\nN ov\n2 01\n6"}, {"heading": "2 OUR TECHNIQUE", "text": ""}, {"heading": "2.1 GRADIENTS DO NOT REFLECT FEATURE IMPORTANCE", "text": "Let us start by investigating the performance of gradients as a measure of feature importance. We use an object recognition network built using the GoogleNet achitecture (Szegedy et al. (2014)) as a running example; we refer to this network by its codename Inception. (We present applications of our techniques to other networks in Section 3.) The network has been trained on the ImageNet object recognition dataset (Russakovsky et al. (2015)). It is is 22 layers deep with a softmax layer on top for classifying images into one of the 1000 Imagenet object classes. The input to the network is a 224\u00d7 224 sized RGB image. Before evaluating the use of gradients for feature importance, we introduce some basic notation that is used throughout the paper.\nWe represent a 224\u00d7 224 sized RGB image as a vector in R224\u00d7224\u00d73. Let IncpL : R224\u00d7224\u00d73 \u2192 [0, 1] be the function represented by the Inception network that computes the softmax score for the object class labeled L. Let 5IncpL(img) be the gradients of IncpL at the input image img. Thus, the vector 5IncpL(img) is the same size as the image and lies in R224\u00d7224\u00d73. As a shorthand, we write5IncpLi,j,c(img) for the gradient of a specific pixel (i, j) and color channel c \u2208 {R,G,B}.\nWe compute the gradients of IncpL (with respect to the image) for the highest-scoring object class, and then aggregate the gradients5IncpL(img) along the color dimension to obtain pixel importance scores.1\n\u2200i, j : PLi,j(img) ::= \u03a3c\u2208{R,G,B}| 5 Incp L i,j,c(img)| (1)\nNext, we visualize pixel importance scores by scaling the intensities of the pixels in the original image in proportion to their respective scores; thus, higher the score brighter would be the pixel. Figure 1a shows a visualization for an image for which the highest scoring object class is \u201creflex camera\u201d with a softmax score of 0.9938.\n1 These pixel importance scores is similar to the gradient-based saliency map defined by Simonyan et al. (2013) with the difference being in how the gradients are aggregated along the color channel.\nIntuitively, one would expect the the high gradient pixels for this classification to be ones falling on the camera or those providing useful context for the classification (e.g., the lens cap). However, most of the highlighted pixels seem to be on the left or above the camera, which to a human seem not essential to the prediction. This could either mean that (1) the highlighted pixels are somehow important for the internal computation performed by the Inception network, or (2) gradients of the image fail to appropriately quantify pixel importance.\nLet us consider hypothesis (1). In order to test it we ablate parts of the image on the left and above the camera (by zeroing out the pixel intensities) and run the ablated image through the Inception network. See Figure 1b. The top predicted category still remains \u201creflex camera\u201d with a softmax score of 0.9966 \u2014 slightly higher than before. This indicates that the ablated portions are indeed irrelevant to the classification. On computing gradients of the ablated image, we still find that most of the high gradient pixels lie outside of the camera. This suggests that for this image, it is in fact hypothesis (2) that holds true. Upon studying more images (see Figure 4), we find that the gradients often fail to highlight the relevant pixels for the predicted object label."}, {"heading": "2.2 SATURATION", "text": "In theory, it is easy to see that the gradients may not reflect feature importance if the prediction function flattens in the vicinity of the input, or equivalently, the gradient of the prediction function with respect to the input is tiny in the vicinity of the input vector. This is what we call saturation, which has also been reported in previous work (Shrikumar et al. (2016), Glorot & Bengio (2010)).\nWe analyze how widespread saturation is in the Inception network by inspecting the behavior of the network on counterfactual images obtained by uniformly scaling pixel intensities from zero to their values in an actual image. Formally, given an input image img \u2208 R224\u00d7224\u00d73, the set of counterfactual images is\n{\u03b1 img | 0 \u2264 \u03b1 \u2264 1} (2)\nFigure 2a shows the trend in the softmax output of the highest scoring class, for thirty randomly chosen images form the ImageNet dataset. More specifically, for each image img, it shows the trend in IncpL(\u03b1 img) as \u03b1 varies from zero to one with L being the label of highest scoring object class for img. It is easy to see that the trend flattens (saturates) for all images \u03b1 increases. Notice that saturation is present even for images whose final score is significantly below 1.0. Moreover, for a majority of images, saturation happens quite soon when \u03b1 = 0.2.\nOne may argue that since the output of the Inception network is the result of applying the softmax function to a vector of activation values, the saturation is expected due to the squashing property of the softmax function. However, as shown in Figure 2b, we find that even the Pre-Softmax activation scores for the highest scoring class saturate.\nIn fact, to our surprise, we found that the saturation is inherently present in the Inception network and the outputs of the intermediate layers also saturate. We plot the distance between the intemediate layer neuron activations for a scaled down input image and the actual input image with respect to the scaling parameter, and find that the trend flattens. Due to lack of space, we provide these plots in Figure 12 in the appendix.\nIt is quite clear from these plots that saturation is widespread across images in the Inception network, and there is a lot more activity in the network for counterfactual images at relatively low values of the scaling parameter \u03b1. This observation forms the basis of our technique for quantifying feature importance.\nNote that it is well known that the saturation of gradients prevent the model from converging to a good quality minima (Glorot & Bengio (2010)). So one may expect good quality models to not have saturation and hence for the (final) gradients to convey feature importance. Clearly, our observations on the Inception model show that this is not the case. It has good prediction accuracy, but also exhibits saturation (see Figure 2). Our hypothesis is that the gradients of important features are not saturated early in the training process. The gradients only saturate after the features have been learnt adequeately, i.e., the input is far away from the decision boundary."}, {"heading": "2.3 INTERIOR GRADIENTS", "text": "We study the importance of input features in a prediction made for an input by examining the gradients of the counterfactuals obtained by scaling the input; we call this set of gradients interior gradients.\nWhile the method of examining gradients of counterfactual inputs is broadly applicable to a wide range of networks, we first explain it in the context of Inception. Here, the counterfactual image inputs we consider are obtained by uniformly scaling pixel intensities from zero to their values in the actual image (this is the same set of counterfactuals that was used to study saturation). The interior gradients are the gradients of these images.\nInteriorGrads(img) ::= {5Incp(\u03b1 img) | 0 \u2264 \u03b1 \u2264 1} (3)\nThese interior gradients explore the behavior of the network along the entire scaling curve depicted in Figure 2a, rather than at a specific point. We can aggregate the interior gradients along the color dimension to obtain interior pixel importance scores using equation 1.\nInteriorPixelImportance(img) ::= {P(\u03b1 img) | 0 \u2264 \u03b1 \u2264 1} (4)\nWe individually visualize the pixel importance scores for each scaling parameter \u03b1 by scaling the intensities of the pixels in the actual image in proportion to their scores. The visualizations show how the importance of each pixel evolves as we scale the image, with the last visualization being identical to one generated by gradients at the actual image. In this regard, the interior gradients offer strictly more insight into pixel importance than just the gradients at the actual image.\nFigure 3 shows the visualizations for the \u201creflex camera\u201d image from Figure 1a for various values of the scaling parameter \u03b1. The plot in the top right corner shows the trend in the absolute magnitude of the average pixel importance score. The magnitude is significantly larger at lower values of \u03b1 and nearly zero at higher values \u2014 the latter is a consequence of saturation. Note that each visualization is only indicative of the relative distribution of the importance scores across pixels and not the absolute magnitude of the scores, i.e., the later snapshots are responsible for tiny increases in the scores as the chart in the top right depicts.\nThe visualizations show that at lower values of \u03b1, the pixels that lie on the camera are most important, and as \u03b1 increases, the region above the camera gains importance. Given the high magnitude of gradients at lower values of \u03b1, we consider those gradients to be the primary drivers of the final prediction score. They are more indicative of feature importance in the prediction compared to the gradients at the actual image (i.e., when \u03b1 = 1).\nThe visualizations of the interior pixel gradients can also be viewed together as a single animation that chains the visualizations in sequence of the scaling parameter. This animation offers a concise yet complete summary of how pixel importance moves around the image as the scaling parameter increase from zero to one.\nRationale. While measuring saturation via counterfactuals seems natural, using them for quantifying feature importance deserves some discussion. The first thing one may try to identify feature importance is to examine the deep network like one would with human authored code. This seems hard; just as deep networks employ distributed representations (such as embeddings), they perform convoluted (pun intended) distributed reasoning. So instead, we choose to probe the network with several counterfactual inputs (related to the input at hand), hoping to trigger all the internal workings of the network. This process would help summarize the effect of the network on the protagonist input; the assumption being that the input is human understandable. Naturally, it helps to work with gradients in this process as via back propagation, they induce an aggregate view over the function computed by the neurons.\nInterior gradients use counterfactual inputs to artifactually induce a procedure on how the networks attention moves across the image as it compute the final prediction score. From the animation, we gather that the network focuses on strong and distinctive patterns in the image at lower values of the scaling parameter, and subtle and weak patterns in the image at higher values. Thus, we speculate that the network\u2019s computation can be loosely abstracted by a procedure that first recognize distinctive features of the image to make an initial prediction, and then fine tunes (these are small score jumps as the chart in Figure 3 shows) the prediction using weaker patterns in the image."}, {"heading": "2.4 CUMULATING INTERIOR GRADIENTS", "text": "A different summarization of the interior gradients can be obtained by cumulating them. While there are a few ways of cumulating counterfactual gradients, the approach we take has the nice attribution property (Proposition 1) that the feature importance scores approximately add up to the prediction score. The feature importance scores are thus also referred to as attributions.\nNotice that the set of counterfactual images {\u03b1 img | 0 \u2264 \u03b1 \u2264 1} fall on a straight line path in R224\u00d7224\u00d73. Interior gradients \u2014 which are the gradients of these counterfactual images \u2014 can be cumulated by integrating them along this line. We call the resulting gradients as integrated gradients. In what follows, we formalize integrated gradients for an arbitrary function F : Rn \u2192 [0, 1] (representing a deep network), and an arbitrary set of counterfactual inputs falling on a path in Rn.\nLet x \u2208 Rn be the input at hand, and \u03b3 = (\u03b31, . . . , \u03b3n) : [0, 1]\u2192 Rn be a smooth function specifying the set of counterfactuals; here, \u03b3(0) is the baseline input (for Inception, a black image), and \u03b3(1) is the actual input (for Inception, the image being studied). Specifically, {\u03b3(\u03b1) | 0 \u2264 \u03b1 \u2264 1} is the set of counterfactuals (for Inception, a series of images that interpolate between the black image and the acutal input).\nThe integrated gradient along the i-th dimension for an input x \u2208 Rn is defined as follows.\nIntegratedGradsi(x) ::= \u222b 1 \u03b1=0 \u2202F (\u03b3(\u03b1)) \u2202\u03b3i(\u03b1) \u2202\u03b3i(\u03b1) \u2202\u03b1 d\u03b1 (5)\nwhere \u2202F (x)\u2202xi is the gradient of F along the i th dimension at x.\nA nice technical property of the integrated gradients is that they add up to the difference between the output of F at the final counterfactual \u03b3(1) and the baseline counterfactual \u03b3(0). This is formalized by the proposition below, which is an instantiation of the fundamental theorem of calculus for path integrals.\nProposition 1 If F : Rn \u2192 R is differentiable almost everywhere 2, and \u03b3 : [0, 1] \u2192 Rn is smooth then\n\u03a3ni=1IntegratedGradsi(x) = F (\u03b3(1))\u2212 F (\u03b3(0))\nFor most deep networks, it is possible to choose countefactuals such that the prediction at the baseline counterfactual is near zero (F (\u03b3(0)) \u2248 0). For instance, for the Inception network, the counterfactual defined by the scaling path satisfies this property as Incp(0224\u00d7224\u00d73) \u2248 0. In such cases, it follows from the Proposition that the integrated gradients form an attribution of the prediction output F (x), i.e., they almost exactly distribute the output to the individual input features.\nThe additivity property provides a form of sanity checking for the integrated gradients and ensures that we do not under or over attribute to features. This is a common pitfall for attribution schemes based on feature ablations, wherein, an ablation may lead to small or a large change in the prediction score depending on whether the ablated feature interacts disjunctively or conjunctively to the rest of the features. This additivity is even more desirable when the networks score is numerically critical, i.e., the score is not used purely in an ordinal sense. In this case, the attributions (together with additivity) guarantee that the attributions are in the units of the score, and account for all of the score.\nWe note that these path integrals of gradients have been used to perform attribution in the context of small non-linear polynomials (Sun & Sundararajan (2011)), and also within the cost-sharing literature in economics where function at hand is a cost function that models the cost of a project as a function of the demands of various participants, and the attributions correspond to cost-shares. The specific path we use corresponds to a cost-sharing method called Aumann-Shapley (Aumann & Shapley (1974)).\nComputing integrated gradients. The integrated gradients can be efficiently approximated by Riemann sum, wherein, we simply sum the gradients at points occurring at sufficiently small intervals along the path of countefactuals.\nIntegratedGradsapproxi (x) ::= \u03a3 m k=1 \u2202F (\u03b3(k/m)) \u2202\u03b3i(\u03b1) (\u03b3( km )\u2212 \u03b3( k\u22121 m )) (6)\nHere m is the number of steps in the Riemman approximation of the integral. Notice that the approximation simply involves computing the gradient in a for loop; computing the gradient is central to deep learning and is a pretty efficient operation. The implementation should therefore be straightforward in most deep learning frameworks. For instance, in TensorFlow (ten), it essentially amounts to calling tf.gradients in a loop over the set of counterfactual inputs (i.e., \u03b3( km ) for k = 1, . . . ,m), which could also be batched. Going forward, we abuse the term \u201cintegrated gradients\u201d to refer to the approximation described above.\n2Formally, this means that the partial derivative of F along each input dimension satisfies Lebesgue\u2019s integrability condition, i.e., the set of discontinuous points has measure zero. Deep networks built out of Sigmoids, ReLUs, and pooling operators should satisfy this condition.\nIntegrated gradients for Inception. We compute the integrated gradients for the Inception network using the counterfactuals obtained by scaling the input image; \u03b3(\u03b1) = \u03b1 img where img is the input image. Similar to the interior gradients, the integrated gradients can also be aggregated along the color channel to obtain pixel importance scores which can then be visualized as discussed earlier. Figure 4 shows these visualizations for a bunch of images. For comparison, it also presents the corresponding visualization obtained from the gradients at the actual image. From the visualizations, it seems quite evident that the integrated gradients are better at capturing important features.\nAttributions are independent of network implementation. Two networks may be functionally equivalent3 despite having very different internal structures. See Figure 14 in the appendix for an example. Ideally, feature attribution should only be determined by the functionality of the network and not its implementation. Attributions generated by integrated gradients satisfy this property by definition since they are based only on the gradients of the function represented by the network.\nIn contrast, this simple property does not hold for all feature attribution methods known to us, including, DeepLift (Shrikumar et al. (2016)), Layer-wise relevance propagation (LRP) (Binder et al. (2016)), Deconvolution networks (DeConvNets) (Zeiler & Fergus (2014)), and Guided backpropagation (Springenberg et al. (2014)) (a counter-example is given in Figure 14). We discuss these methods in more detail in Section 2.8"}, {"heading": "2.5 EVALUATING OUR APPROACH", "text": "We discuss an evaluation of integrated gradients as a measure of feature importance, by comparing them against (final) gradients.\nPixel ablations. The first evaluation is based on a method by Samek et al. (2015). Here we ablate4 the top 5000 pixels (10% of the image) by importance score, and compute the score drop for the highest scoring object class. The ablation is performed 100 pixels at a time, in a sequence of 50 steps. At each perturbation step k we measure the average drop in score up to step k. This quantity is refered to a area over the perturbation curve (AOPC) by Samek et al. (2015).\nFigure 5 shows the AOPC curve with respect to the number of pertubation steps for integrated gradients and gradients at the image. AOPC values at each step represent the average over a dataset of 150 randomly chosen images. It is clear that ablating the top pixels identified by integrated gradients leads to a larger score drop that those identified by gradients at the image.\nHaving said that, we note an important issue with the technique. The images resulting from pixel perturbation are often unnatural, and it could be that the scores drop simply because the network has never seen anything like it in training.\nLocalization. The second evaluation is to consider images with human-drawn bounding boxes around objects, and compute the percentage of pixel attribution inside the bounding box. We use the 2012 ImageNet object localization challenge dataset to get a set of human-drawn bounding boxes. We run our evaluation on 100 randomly chosen images satisfying the following properties \u2014 (1) the total size of the bounding box(es) is less than two thirds of the image size, and (2) ablating the bounding box significantly drops the prediction score for the object class. (1) is for ensuring that the boxes are not so large that the bulk of the attribution falls inside them by definition, and (2) is for ensuring that the boxed part of the image is indeed responsible for the prediction score for the image. We find that on 82 images the integrated gradients technique leads to a higher fraction of the pixel attribution inside the box than gradients at the actual image. The average difference in the percentage pixel attribution inside the box for the two techniques is 8.4%.\nWhile these results are promising, we note the following caveat. Integrated gradients are meant to capture pixel importance with respect to the prediction task. While for most objects, one would expect the pixel located on the object to be most important for the prediction, in some cases the\n3Formally, two networks F and G are functionally equivalent if and only if \u2200 x : F (x) = G(x). 4Ablation in our setting amounts to zeroing out (or blacking out) the intensity for the R, G, B channels. We view this as a natural mechanism for removing the information carried by the pixel (than, say, randomizing the pixel\u2019s intensity as proposed by Samek et al. (2015), especially since the black image is a natural baseline for vision tasks.\ncontext in which the object occurs may also contribute to the prediction. The cabbage butterfly image from Figure 4 is a good example of this where the pixels on the leaf are also surfaced by the integrated gradients.\nEyeballing. Ultimately, it was hard to come up with a perfect evaluation technique. So we did spend a large amount of time applying and eyeballing the results of our technique to various networks\u2014 the ones presented in this paper, as well as some networks used within products. For the Inception network, we welcome you to eyeball more visualizations in Figure 11 in the appendix and also at: https://github.com/ankurtaly/Attributions. While we found our method to beat gradients at the image for the most part, this is clearly a subjective process prone to interpretation and cherry-picking, but is also ultimately the measure of the utility of the approach\u2014debugging inherently involves the human.\nFinally, also note that we did not compare against other whitebox attribution techniques (e.g., DeepLift (Shrikumar et al. (2016))), because our focus was on black-box techniques that are easy to implement, so comparing against gradients seems like a fair comparison."}, {"heading": "2.6 DEBUGGING NETWORKS", "text": "Despite the widespread application of deep neural networks to problems in science and technology, their internal workings largely remain a black box. As a result, humans have a limited ability to understand the predictions made by these networks. This is viewed as hindrance in scenarios where the bar for precision is high, e.g., medical diagnosis, obstacle detection for robots, etc. (dar (2016)). Quantifying feature importance for individual predictions is a first step towards understanding the behavior of the network; at the very least, it helps debug misclassified inputs, and sanity check the internal workings. We present evidence to support this below.\nWe use feature importance to debug misclassifications made by the Inception network. In particular, we consider images from the ImageNet dataset where the groundtruth label for the image not in the top five labels predicted by the Inception network. We use interior gradients to compute pixel importance scores for both the Inception label and the groundtruth label, and visualize them to gain insight into the cause for misclassification.\nFigure 6 shows the visualizations for two misclassified images. The top image genuinely has two objects, one corresponding to the groundtruth label and other corresponding to the Inception label. We find that the interior gradients for each label are able to emphasize the corresponding objects. Therefore, we suspect that the misclassification is in the ranking logic for the labels rather than the recognition logic for each label. For the bottom image, we observe that the interior gradients are largely similar. Moreover, the cricket gets emphasized by the interior gradients for the mantis (Inception label). Thus, we suspect this to be a more serious misclassification, stemming from the recognition logic for the mantis."}, {"heading": "2.7 DISCUSSION", "text": "Faithfullness. A natural question is to ask why gradients of counterfactuals obtained by scaling the input capture feature importance for the original image. First, from studying the visualizations in Figure 4, the results look reasonable in that the highlighted pixels capture features representative of the predicted class as a human would perceive them. Second, we confirmed that the network too seems to find these features representative by performing ablations. It is somewhat natural to expect that the Inception network is robust to to changes in input intensity; presumably there are some low brightness images in the training set.\nHowever, these counterfactuals seem reasonable even for networks where such scaling does not correspond to a natural concept like intensity, and when the counterfactuals fall outside the training set; for instance in the case of the ligand-based virtual screening network (see Section 3.1). We speculate that the reason why these counterfactuals make sense is because the network is built by composing ReLus. As one scales the input starting from a suitable baseline, various neurons activate, and the scaling process that does a somewhat thorough job of exploring all these events that contribute to the prediction for the input. There is an analogous argument for other operator such as max pool, average pool, and softmax\u2014here the triggering events arent discrete but the argument is analogous.\nLimtations of Approach. We discuss some limitations of our technique; in a sense these are limitations of the problem statement and apply equally to other techniques that attribute to base input features.\n\u2022 Inability to capture Feature interactions: The models could perform logic that effectively combines features via a conjuction or an implication-like operations; for instance, it could be that a molecule binds to a site if it has a certain structure that is essentially a conjunction of certains atoms and certain bonds between them. Attributions or importance scores have no way to represent these interactions.\n\u2022 Feature correlations: Feature correlations are a bane to the understandability of all machine learning models. If there are two features that frequently co-occur, the model is free to assign weight to either or both features. The attributions would then respect this weight assignment. But, it could be that the specific weight assignment chosen by the model is not human-intelligible. Though there have been approoaches to feature selection that reduce feature correlations (Yu & Liu (2003)), it is unclear how they apply to deep models on dense input."}, {"heading": "2.8 RELATED WORK", "text": "Over the last few years, there has been a vast amount work on demystifying the inner workings of deep networks. Most of this work has been on networks trained on computer vision tasks, and deals with understanding what a specific neuron computes (Erhan et al. (2009); Le (2013)) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi (2015); Dosovitskiy & Brox (2015); Yosinski et al. (2015)).\nOur work instead focusses on understanding the network\u2019s behavior on a specific input in terms of the base level input features. Our technique quantifies the importance of each feature in the prediction. Known approaches for accomplishing this can be divided into three categories.\nGradient based methods. The first approach is to use gradients of the input features to quantify feature importance (Baehrens et al. (2010); Simonyan et al. (2013)). This approach is the easies to implement. However, as discussed earlier, naively using the gradients at the actual input does not accurate quantify feature importance as gradients suffer from saturation.\nScore back-propagation based methods. The second set of approaches involve back-propagating the final prediction score through each layer of the network down to the individual features. These include DeepLift (Shrikumar et al. (2016)), Layer-wise relevance propagation (LRP) (Binder et al. (2016)), Deconvolution networks (DeConvNets) (Zeiler & Fergus (2014)), and Guided backpropagation (Springenberg et al. (2014)). These methods largely differ in the backpropagation logic for various non-linear activation functions. While DeConvNets, Guided back-propagation and LRP rely on the local gradients at each non-linear activation function, DeepLift relies on the deviation in the neuron\u2019s activation from a certain baseline input.\nSimilar to integrated gradients, the DeepLift and LRP also result in an exact distribution of the prediction score to the input features. Howevever, as shown by Figure 14, the attributions are not invariant across functionally equivalent networks. Besides, the primary advantage of our method over all these methods is its ease of implementation. The aforesaid methods require knowledge of the network architecture and the internal neuron activations for the input, and involve implementing a somewhat complicated back-propagation logic. On the other hand, our method is agnostic to the network architectures and relies only on computing gradients which can done efficiently in most deep learning frameworks.\nModel approximation based methods. The third approach, proposed first by Ribeiro et al. (2016a;b), is to locally approximate the behavior of the network in the vicinity of the input being explained with a simpler, more interpretable model. An appealing aspect of this approach is that it is completely agnostic to the structure of the network and only deals with its input-output behavior. The approximation is learned by sampling the network\u2019s output in the vicinity of the input at hand. In this sense, it is similar to our approach of using counterfactuals. Since the counterfactuals are chosen somewhat arbitrarily, and the approximation is based purely on the network\u2019s output at\nthe counterfactuals, the faithfullness question is far more crucial in this setting. The method is also expensive to implement as it requires training a new model locally around the input being explained."}, {"heading": "3 APPLICATIONS TO OTHER NETWORKS", "text": "The technique of quantifying feature importance by inspecting gradients of counterfactual inputs is generally applicable across deep networks. While for networks performing vision tasks, the counterfactual inputs are obtained by scaling pixel intensities, for other networks they may be obtained by scaling an embedding representation of the input.\nAs a proof of concept, we apply the technique to the molecular graph convolutions network of Kearnes et al. (2016) for ligand-based virtual screening and an LSTM model (Zaremba et al. (2014)) for the language modeling of the Penn Treebank dataset (Marcus et al. (1993))."}, {"heading": "3.1 LIGAND-BASED VIRTUAL SCREENING", "text": "The Ligand-Based Virtual Screening problem is to predict whether an input molecule is active against a certain target (e.g., protein or enzyme). The process is meant to aid the discovery of new drug molecules. Deep networks built using molecular graph convolutions have recently been proposed by Kearnes et al. (2016) for solving this problem.\nOnce a molecule has been identified as active against a target, the next step for medicinal chemists is to identify the molecular features\u2014formally, pharmacophores5\u2014that are responsible for the activity. This is akin to quantifying feature importance, and can be achieved using the method of integrated gradients. The attributions obtained from the method help with identifying the dominant molecular features, and also help sanity check the behavior of the network by shedding light on its inner workings. With regard to the latter, we discuss an anecdote later in this section on how attributions surfaced an anomaly in W1N2 network architecture proposed by Kearnes et al. (2016).\nDefining the counterfactual inputs. The first step in computing integrated gradients is to define the set of counterfactual inputs. The network requires an input molecule to be encoded by hand as a set of atom and atom-pair features describing the molecu Gradients have been used to quantify feature\n5A pharmacophore is the ensemble of steric and electronic features that is necessary to ensure the a molecule is active against a specific biological target to trigger (or to block) its biological response.\nimportance in machine le as an undirected graph. Atoms are featurized using a one-hot encoding specifying the atom type (e.g., C, O, S, etc.), and atom-pairs are featurized by specifying either the type of bond (e.g., single, double, triple, etc.) between the atoms, or the graph distance between them 6\nThe counterfactual inputs are obtained by scaling down the molecule features down to zero vectors, i.e., the set {\u03b1Features(mol) | 0 \u2264 \u03b1 \u2264 1} where Features(mol) is an encoding of the molecule into atom and atom-pair features.\nThe careful reader might notice that these counterfactual inputs are not valid featurizations of molecules. However, we argue that they are still valid inputs for the network. First, all operators in the network (e.g., ReLUs, Linear filters, etc.) treat their inputs as continuous real numbers rather than discrete zeros and ones. Second, all fields of the counterfactual inputs are bounded between zero and one, therefore, we dont expect them to appear spurious to the network. We discuss this further in section 2.7\nIn what follows, we discuss the behavior of a network based on the W2N2-simple architecture proposed by Kearnes et al. (2016). On inspecting the behavior of the network over counterfactual inputs, we observe saturation here as well. Figure 13a shows the trend in the softmax score for the task PCBA-588342 for twenty five active molecules as we vary the scaling parameter \u03b1 from zero to one. While the overall saturated region is small, saturation does exist near vicinity of the input (0.9 \u2264 \u03b1 \u2264 1). Figure 13b in the appendix shows that the total feature gradient varies significantly along the scaling path; thus, just the gradients at the molecule is fully indicative of the behavior of the network.\nVisualizing intergrated gradients. We cumulate the gradients of these counterfactual inputs to obtain an attribution of the prediction score to each atom and atom-pair feature. Unlike image inputs, which have dense features, the set of input features for molecules are sparse. Consequently, the attributions are sparse and can be inspected directly. Figure 7 shows heatmaps for the atom and atom-pair attributions for a specific molecule.\nUsing the attributions, one can easily identify the atoms and atom-pairs that that have a strongly positive or strongly negative contribution. Since the attributions add up to the final prediction score (see Proposition 1), the at- tribution magnitudes can be use for accounting the contributions of each feature. For instance, the atom-pairs that have a bond between them contribute cumulatively contribute 46% of the prediction score, while all other atom pairs cumulatively contribute \u22123%. We presented the attributions for 100 molecules active against a specific task to a few chemists. The chemists were able to immediately spot dominant functional groups (e.g., aromatic rings) being surfaced by the attributions. A next step could be cluster the aggregate the attributions across a large set of molecules active against a specific task to identify a common denominator of features shared by all active molecules.\nIdentifying Dead Features. We now discuss how attributions helped us spot an anomaly in the W1N2 architecture. On applying the integrated gradients method to the W1N2 network, we found that several atoms in the same molecule received the exact same attribution. For instance, for the molecule in Figure 7, we found that several carbon atoms at positions 2, 3, 14, 15, and 16 received the same attribution of 0.0043 despite being bonded to different atoms, for e.g., Carbon at position 3 is bonded to an Oxygen whereas Carbon at position 2 is not. This is surprising as one would expect two atoms with different neighborhoods to be treated differently by the network.\nOn investigating the problem further we found that since the W1N2 network had only one convolution layer, the atoms and atom-pair features were not fully convolved. This caused all atoms that have the same atom type, and same number of bonds of each type to contribute identically to the network. This is not the case for networks that have two or more convolutional layers.\nDespite the aforementioned problem, the W1N2 network had good predictive accuracy. One hypothesis for this is that the atom type and their neighborhoods are tightly correlated; for instance an outgoing double bond from a Carbon is always to another Carbon or Oxygen atom. As a result, given the atom type, an explicit encoding of the neighborhorhood is not needed by the network. This also\n6This featurization is referred to as \u201csimple\u201d input featurization in Kearnes et al. (2016).\nsuggests that equivalent predictive accuracy can be achieved using a simpler \u201cbag of atoms\u201d type model."}, {"heading": "3.2 LANGUAGE MODELING", "text": "To apply our technique for language modeling, we study word-level language modeling of the Penn TreeBank dataset (Marcus et al. (1993)), and apply an LSTM-based sequence model based on Zaremba et al. (2014). For such a network, given a sequence of input words, and the softmax prediction for the next word, we want to identify the importance of the preceding words for the score.\nAs in the case of the Inception model, we observe saturation in this LSTM network. To describe the setup, we choose 20 randomly chosen sections of the test data, and for each of them inspect the prediction score of the next word using the first 10 words. Then we give each of the 10 input words a weight of \u03b1 \u2208 [0, 1], which is applied to scale their embedding vectors. In Figure 8, we plot the prediction score as a function of \u03b1. For all except one curves, the curve starts near zero at \u03b1 = 0, moves around in the middle, stabilizes, and turns flat around \u03b1 = 1. For the interesting special case where softmax score is non-zero at \u03b1 = 0, it turns out that that the word being predicted represents out of vocabulary words. [!h]\nIn Table 9 and Table 10 we show two comparisons of gradients to integrated gradients. Due to saturation, the magnitudes of gradients are so small compared to the prediction scores that it is difficult to make sense of them. In comparison, (approximate) integrated gradients have a total amount close to the prediction, and seem to make sense. For example, in the first example, the integrated gradients attribute the prediction score of \u201cthan\u201c to the preceding word \u201cmore\u201d. This makes sense as \u201cthan\u201d often follows right after \u201cmore\u201c in English. On the other hand, standard gradient gives a slightly negative attribution that betrays our intuition. In the second example, in predicting the second \u201cual\u201d, integrated gradients are clearly the highest for the first occurrence of \u201cual\u201d, which is the only word that is highly predictive of the second \u201cual\u201d. On the other hand, standard gradients are not only tiny, but also similar in magnitude for multiple words."}, {"heading": "4 CONCLUSION", "text": "We present Interior Gradients, a method for quantifying feature importance. The method can be applied to a variety of deep networks without instrumenting the network, in fact, the amount of code required is fairly tiny. We demonstrate that it is possible to have some understanding of the\nperformance of the network without a detailed understanding of its implementation, opening up the possibility of easy and wide application, and lowering the bar on the effort needed to debug deep networks.\nWe also wonder if Interior Gradients are useful within training as a measure against saturation, or indeed in other places that gradients are used."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Patrick Riley and Christian Szegedy for their helpful feedback on the technique and on drafts of this paper."}, {"heading": "A APPENDIX", "text": ""}], "references": [{"title": "Values of Non-Atomic Games", "author": ["R.J. Aumann", "L.S. Shapley"], "venue": null, "citeRegEx": "Aumann and Shapley.,? \\Q1974\\E", "shortCiteRegEx": "Aumann and Shapley.", "year": 1974}, {"title": "How to explain individual classification decisions", "author": ["David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "KlausRobert M\u00fcller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Baehrens et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baehrens et al\\.", "year": 2010}, {"title": "Layer-wise relevance propagation for neural networks with local renormalization layers", "author": ["Alexander Binder", "Gr\u00e9goire Montavon", "Sebastian Bach", "Klaus-Robert M\u00fcller", "Wojciech Samek"], "venue": null, "citeRegEx": "Binder et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Binder et al\\.", "year": 2016}, {"title": "Inverting visual representations with convolutional networks, 2015. URL http://arxiv.org/abs/1506.02753", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": null, "citeRegEx": "Dosovitskiy and Brox.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy and Brox.", "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Technical Report 1341,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Artificial Intelligence and Statistics (AISTATS10),", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Molecular graph convolutions: moving beyond fingerprints", "author": ["Steven Kearnes", "Kevin McCloskey", "Marc Berndl", "Vijay Pande", "Patrick Riley"], "venue": "Journal of Computer-Aided Molecular Design,", "citeRegEx": "Kearnes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kearnes et al\\.", "year": 2016}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V. Le"], "venue": "In International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Le.,? \\Q2013\\E", "shortCiteRegEx": "Le.", "year": 2013}, {"title": "Understanding deep image representations by inverting them", "author": ["Aravindh Mahendran", "Andrea Vedaldi"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Mahendran and Vedaldi.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "why should I trust you?\u201d: Explaining the predictions of any classifier", "author": ["Marco T\u00falio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "venue": "In 22nd ACM International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Model-agnostic interpretability of machine learning", "author": ["Marco T\u00falio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "venue": "CoRR, 2016b. URL http://arxiv.org/abs/1606.05386", "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Evaluating the visualization of what a deep neural network has learned", "author": ["Wojciech Samek", "Alexander Binder", "Gr\u00e9goire Montavon", "Sebastian Bach", "Klaus-Robert M\u00fcller"], "venue": null, "citeRegEx": "Samek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Samek et al\\.", "year": 2015}, {"title": "Not just a black box: Learning important features through propagating activation differences", "author": ["Avanti Shrikumar", "Peyton Greenside", "Anna Shcherbina", "Anshul Kundaje"], "venue": null, "citeRegEx": "Shrikumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shrikumar et al\\.", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "maps. CoRR,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin A. Riedmiller"], "venue": "URL http://arxiv.org/abs/1412", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Axiomatic attribution for multilinear functions", "author": ["Yi Sun", "Mukund Sundararajan"], "venue": "In 12th ACM Conference on Electronic Commerce (EC), pp", "citeRegEx": "Sun and Sundararajan.,? \\Q2011\\E", "shortCiteRegEx": "Sun and Sundararajan.", "year": 2011}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski", "Jeff Clune", "Anh Mai Nguyen", "Thomas Fuchs", "Hod Lipson"], "venue": null, "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Feature selection for high-dimensional data: A fast correlation-based filter solution", "author": ["Lei Yu", "Huan Liu"], "venue": "In 20th International Conference on Machine Learning (ICML), pp", "citeRegEx": "Yu and Liu.,? \\Q2003\\E", "shortCiteRegEx": "Yu and Liu.", "year": 2003}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "CoRR,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus"], "venue": "In 13th European Conference on Computer Vision (ECCV),", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "For deep nonlinear models, the gradient can be thought of as a local linear approximation (Simonyan et al. (2013)).", "startOffset": 91, "endOffset": 114}, {"referenceID": 15, "context": "1 These pixel importance scores is similar to the gradient-based saliency map defined by Simonyan et al. (2013) with the difference being in how the gradients are aggregated along the color channel.", "startOffset": 89, "endOffset": 112}, {"referenceID": 14, "context": "This is what we call saturation, which has also been reported in previous work (Shrikumar et al. (2016), Glorot & Bengio (2010)).", "startOffset": 80, "endOffset": 104}, {"referenceID": 14, "context": "This is what we call saturation, which has also been reported in previous work (Shrikumar et al. (2016), Glorot & Bengio (2010)).", "startOffset": 80, "endOffset": 128}, {"referenceID": 13, "context": "In contrast, this simple property does not hold for all feature attribution methods known to us, including, DeepLift (Shrikumar et al. (2016)), Layer-wise relevance propagation (LRP) (Binder et al.", "startOffset": 118, "endOffset": 142}, {"referenceID": 2, "context": "(2016)), Layer-wise relevance propagation (LRP) (Binder et al. (2016)), Deconvolution networks (DeConvNets) (Zeiler & Fergus (2014)), and Guided backpropagation (Springenberg et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 2, "context": "(2016)), Layer-wise relevance propagation (LRP) (Binder et al. (2016)), Deconvolution networks (DeConvNets) (Zeiler & Fergus (2014)), and Guided backpropagation (Springenberg et al.", "startOffset": 49, "endOffset": 132}, {"referenceID": 2, "context": "(2016)), Layer-wise relevance propagation (LRP) (Binder et al. (2016)), Deconvolution networks (DeConvNets) (Zeiler & Fergus (2014)), and Guided backpropagation (Springenberg et al. (2014)) (a counter-example is given in Figure 14).", "startOffset": 49, "endOffset": 189}, {"referenceID": 13, "context": "The first evaluation is based on a method by Samek et al. (2015). Here we ablate4 the top 5000 pixels (10% of the image) by importance score, and compute the score drop for the highest scoring object class.", "startOffset": 45, "endOffset": 65}, {"referenceID": 13, "context": "The first evaluation is based on a method by Samek et al. (2015). Here we ablate4 the top 5000 pixels (10% of the image) by importance score, and compute the score drop for the highest scoring object class. The ablation is performed 100 pixels at a time, in a sequence of 50 steps. At each perturbation step k we measure the average drop in score up to step k. This quantity is refered to a area over the perturbation curve (AOPC) by Samek et al. (2015). Figure 5 shows the AOPC curve with respect to the number of pertubation steps for integrated gradients and gradients at the image.", "startOffset": 45, "endOffset": 454}, {"referenceID": 13, "context": "We view this as a natural mechanism for removing the information carried by the pixel (than, say, randomizing the pixel\u2019s intensity as proposed by Samek et al. (2015), especially since the black image is a natural baseline for vision tasks.", "startOffset": 147, "endOffset": 167}, {"referenceID": 14, "context": ", DeepLift (Shrikumar et al. (2016))), because our focus was on black-box techniques that are easy to implement, so comparing against gradients seems like a fair comparison.", "startOffset": 12, "endOffset": 36}, {"referenceID": 4, "context": "Most of this work has been on networks trained on computer vision tasks, and deals with understanding what a specific neuron computes (Erhan et al. (2009); Le (2013)) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi (2015); Dosovitskiy & Brox (2015); Yosinski et al.", "startOffset": 135, "endOffset": 155}, {"referenceID": 4, "context": "Most of this work has been on networks trained on computer vision tasks, and deals with understanding what a specific neuron computes (Erhan et al. (2009); Le (2013)) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi (2015); Dosovitskiy & Brox (2015); Yosinski et al.", "startOffset": 135, "endOffset": 166}, {"referenceID": 4, "context": "Most of this work has been on networks trained on computer vision tasks, and deals with understanding what a specific neuron computes (Erhan et al. (2009); Le (2013)) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi (2015); Dosovitskiy & Brox (2015); Yosinski et al.", "startOffset": 135, "endOffset": 272}, {"referenceID": 4, "context": "Most of this work has been on networks trained on computer vision tasks, and deals with understanding what a specific neuron computes (Erhan et al. (2009); Le (2013)) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi (2015); Dosovitskiy & Brox (2015); Yosinski et al.", "startOffset": 135, "endOffset": 299}, {"referenceID": 4, "context": "Most of this work has been on networks trained on computer vision tasks, and deals with understanding what a specific neuron computes (Erhan et al. (2009); Le (2013)) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi (2015); Dosovitskiy & Brox (2015); Yosinski et al. (2015)).", "startOffset": 135, "endOffset": 323}, {"referenceID": 1, "context": "The first approach is to use gradients of the input features to quantify feature importance (Baehrens et al. (2010); Simonyan et al.", "startOffset": 93, "endOffset": 116}, {"referenceID": 1, "context": "The first approach is to use gradients of the input features to quantify feature importance (Baehrens et al. (2010); Simonyan et al. (2013)).", "startOffset": 93, "endOffset": 140}, {"referenceID": 13, "context": "These include DeepLift (Shrikumar et al. (2016)), Layer-wise relevance propagation (LRP) (Binder et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 2, "context": "(2016)), Layer-wise relevance propagation (LRP) (Binder et al. (2016)), Deconvolution networks (DeConvNets) (Zeiler & Fergus (2014)), and Guided backpropagation (Springenberg et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 2, "context": "(2016)), Layer-wise relevance propagation (LRP) (Binder et al. (2016)), Deconvolution networks (DeConvNets) (Zeiler & Fergus (2014)), and Guided backpropagation (Springenberg et al.", "startOffset": 49, "endOffset": 132}, {"referenceID": 2, "context": "(2016)), Layer-wise relevance propagation (LRP) (Binder et al. (2016)), Deconvolution networks (DeConvNets) (Zeiler & Fergus (2014)), and Guided backpropagation (Springenberg et al. (2014)).", "startOffset": 49, "endOffset": 189}, {"referenceID": 13, "context": "Figure 5: AOPC (Samek et al. (2015)) for integrated gradients and gradients at image.", "startOffset": 16, "endOffset": 36}, {"referenceID": 6, "context": "As a proof of concept, we apply the technique to the molecular graph convolutions network of Kearnes et al. (2016) for ligand-based virtual screening and an LSTM model (Zaremba et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 6, "context": "As a proof of concept, we apply the technique to the molecular graph convolutions network of Kearnes et al. (2016) for ligand-based virtual screening and an LSTM model (Zaremba et al. (2014)) for the language modeling of the Penn Treebank dataset (Marcus et al.", "startOffset": 93, "endOffset": 191}, {"referenceID": 6, "context": "As a proof of concept, we apply the technique to the molecular graph convolutions network of Kearnes et al. (2016) for ligand-based virtual screening and an LSTM model (Zaremba et al. (2014)) for the language modeling of the Penn Treebank dataset (Marcus et al. (1993)).", "startOffset": 93, "endOffset": 269}, {"referenceID": 6, "context": "Deep networks built using molecular graph convolutions have recently been proposed by Kearnes et al. (2016) for solving this problem.", "startOffset": 86, "endOffset": 108}, {"referenceID": 6, "context": "Deep networks built using molecular graph convolutions have recently been proposed by Kearnes et al. (2016) for solving this problem. Once a molecule has been identified as active against a target, the next step for medicinal chemists is to identify the molecular features\u2014formally, pharmacophores5\u2014that are responsible for the activity. This is akin to quantifying feature importance, and can be achieved using the method of integrated gradients. The attributions obtained from the method help with identifying the dominant molecular features, and also help sanity check the behavior of the network by shedding light on its inner workings. With regard to the latter, we discuss an anecdote later in this section on how attributions surfaced an anomaly in W1N2 network architecture proposed by Kearnes et al. (2016).", "startOffset": 86, "endOffset": 816}, {"referenceID": 6, "context": "7 In what follows, we discuss the behavior of a network based on the W2N2-simple architecture proposed by Kearnes et al. (2016). On inspecting the behavior of the network over counterfactual inputs, we observe saturation here as well.", "startOffset": 106, "endOffset": 128}, {"referenceID": 6, "context": "This featurization is referred to as \u201csimple\u201d input featurization in Kearnes et al. (2016).", "startOffset": 69, "endOffset": 91}, {"referenceID": 6, "context": "Figure 7: Attribution for a molecule under the W2N2 network (Kearnes et al. (2016)).", "startOffset": 61, "endOffset": 83}, {"referenceID": 9, "context": "To apply our technique for language modeling, we study word-level language modeling of the Penn TreeBank dataset (Marcus et al. (1993)), and apply an LSTM-based sequence model based on Zaremba et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 9, "context": "To apply our technique for language modeling, we study word-level language modeling of the Penn TreeBank dataset (Marcus et al. (1993)), and apply an LSTM-based sequence model based on Zaremba et al. (2014). For such a network, given a sequence of input words, and the softmax prediction for the next word, we want to identify the importance of the preceding words for the score.", "startOffset": 114, "endOffset": 207}], "year": 2016, "abstractText": "Gradients have been used to quantify feature importance in machine learning models. Unfortunately, in nonlinear deep networks, not only individual neurons but also the whole network can saturate, and as a result an important input feature can have a tiny gradient. We study various networks, and observe that this phenomena is indeed widespread, across many inputs. We propose to examine interior gradients, which are gradients of counterfactual inputs constructed by scaling down the original input. We apply our method to the GoogleNet architecture for object recognition in images, as well as a ligand-based virtual screening network with categorical features and an LSTM based language model for the Penn Treebank dataset. We visualize how interior gradients better capture feature importance. Furthermore, interior gradients are applicable to a wide variety of deep networks, and have the attribution property that the feature importance scores sum to the the prediction score. Best of all, interior gradients can be computed just as easily as gradients. In contrast, previous methods are complex to implement, which hinders practical adoption.", "creator": "LaTeX with hyperref package"}}}