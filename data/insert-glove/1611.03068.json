{"id": "1611.03068", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Incremental Sequence Learning", "abstract": "Deep svelte learning research over cavaliers the vascones past years colovic has hi3g shown that motoko by increasing nitrogeno the efr scope pre-draft or zhucheng difficulty of the learning larger-than-life problem patrickswell over brosch time, joundi increasingly complex heartbreaking learning problems can be ojiya addressed. siddiq We masoretes study low-elevation incremental oluyemi learning in alstead the sincil context of late-18th sequence lochgelly learning, using generative RNNs 67.71 in upplands the aidells form maseratis of multi - layer recurrent Mixture Density ginned Networks. devale We introduce Incremental lre Sequence aldaco Learning, laments a non-classical simple incremental odah approach dumaresq to sequence r\u00e9is learning. Incremental prakan Sequence Learning celam starts out by using only the first few steps of avid each 1546 sequence as training geol data. salvano Each mouton time b\u00e9isbol a harardhere performance krokus criterion has been reached, the aiyangar length of charlton the zegarra parts constantin of the dacrydium sequences 3,500-ton used for 67.1 training semi-rural is increased. urango To evaluate Incremental Sequence Learning otolaryngology and comparison aping methods, we introduce and espressos make available a novel sequence motordrome learning task hikayat and arnout data set: a1c predicting 9:17 and classifying 3,355 MNIST pen stroke 1700s sequences, where cataloger the nasrani familiar cheteshwar handwritten simultaneously digit images have institucion been transformed affero to pen theodros stroke zell sequences mmcguire@timesunion.com representing the 1.438 skeletons of crissman the 95.63 digits. We find lacus that Incremental idylwyld Sequence recirculate Learning cannibalizing greatly azrael speeds poz up sequence euphronios learning and reaches the best corimon test performance interahamwe level blecha of regular summorum sequence aminu learning coming-of-age 20 cbs/sony times cruachan faster, valueclick reduces the test error by hias 74% , palisade and in general gorm\u00e9 performs more robustly; pitfall it gobinda displays domingos lower variance glaucoma and tecnica achieves hafen sustained progress capitation after recycled all plays three tthe comparison method innisfail have delineates stopped 75.26 improving. A trained buwa sequence prediction oord model is also used dankworth in transfer heroes learning to the shihri task hagfors of macaraig sequence birker classification, where it is corneaus found that kiyosato transfer shalem learning realizes btn improved tudu classification hybla performance compared 46.32 to methods auditors that sweet-natured learn 3100 to classify volesky from peoplemover scratch.", "histories": [["v1", "Wed, 9 Nov 2016 20:12:08 GMT  (5605kb,D)", "http://arxiv.org/abs/1611.03068v1", "18 pages"], ["v2", "Thu, 1 Dec 2016 21:33:19 GMT  (5706kb,D)", "http://arxiv.org/abs/1611.03068v2", "Updated version: Clarified the contribution (see abstract, intro, and conclusion); added figures to illustrate the architecture of the network and the difference between training and generation; different selection of experiments in Section 6.4; some textual edits"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["edwin d de jong"], "accepted": false, "id": "1611.03068"}, "pdf": {"name": "1611.03068.pdf", "metadata": {"source": "CRF", "title": "Incremental sequence learning", "authors": ["Edwin D. de Jong"], "emails": [], "sections": [{"heading": "1 Introduction", "text": ""}, {"heading": "1.1 Incremental Learning, Transfer Learning, and Representation Learning", "text": "Deep learning research over the past years has shown that by increasing the scope or difficulty of the learning problem over time, increasingly complex learning problems can be addressed. This principle has been described as Incremental learning by Elman (1991), and has a long history. Schlimmer and Granger (1986) described a pseudo-connectionist distributed concept learning approach involving incremental learning. Elman (1991) defined Incremental Learning as an approach where the training data is not presented all at once, but incrementally; see also Elman (1993). Giraud-Carrier (2000) defines Incremental Learning as follows: \u201cA learning task is incremental if the training examples used to solve it become available over time, usually one at a time.\u201c Bengio et al. (2009) introduced the framework of Curriculum Learning. The central idea behind this approach is that a learning system is guided by presenting gradually more and/or more complex concepts. A formal definition is provided specifying that the distribution over examples converges monotonically towards the target training distribution, and that the entropy of the distributions visited over time, and hence the diversity of training examples, increases.\nar X\niv :1\n61 1.\n03 06\n8v 1\n[ cs\n.L G\n] 9\nN ov\n2 01\nAn extension of the notion of incremental learning is to also let the learning task vary over time. This approach, known as Transfer Learning or Inductive Transfer, was first described by Pratt (1993). Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation. In coevolutionary algorithms, the coevolution of representations with solutions that employ them, see e.g. Moriarty (1997); de Jong and Oates (2002), provides another approach to representation learning. Representation learning can be seen as a special form of transfer learning, where one goal is to learn adequate representations, and the other goal, addressed in parallel or sequentially, is to use these representations to address the learning problem.\nSeveral of the recent successes of deep learning can be attributed to representation learning and incremental learning. Bengio et al. (2013) provide a review and insightful discussion of representation learning. Parisotto et al. (2015) report experiments with transfer learning across Atari 2600 arcade games where up to 5 million frames of training time in each game are saved. More recently, successful transfer of robot learning from the virtual to the real world was achieved using transfer learning, see Rusu et al. (2016). And at the annual ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), the depth of networks has steadily increased over the years, so far leading up to a network of 152 layers for the winning entry in the ILSVRC 2015 classification task; see He et al. (2015)."}, {"heading": "1.2 Sequence Learning", "text": "We study incremental learning in the context of sequence learning. The aim in sequence learning is to predict, given a step of the sequence, what the next step will be. By iteratively feeding the predicted output back into the network as the next input, the network can be used to produce a complete sequences of variable length. For a discussion of variants of sequence learning problems, see Sun and Giles (2001); a more recent treatment covering recurrent neural networks as used here is provided by Lipton (2015).\nAn interesting challenge in sequence learning is that for most sequence learning problems of interest, the next step in a sequence does not follow unambiguously from the previous step. If this were the case, i.e. if the underlying process generating the sequences satisfies the Markov property, the learning problem would be reduced to learning a mapping from each step to the next. Instead, steps in the sequence may depend on some or all of the preceding steps in the sequence. Therefore, a main challenge faced by a sequence learning model is to capture relevant information from the part of the sequence seen so far. This ability to capture relevant information about future sequences it may receive must be developed during training; the network must learn the ability to build up internal representations which encode relevant aspects of the sequence that is received."}, {"heading": "1.3 Incremental Sequence Learning", "text": "The dependency on the partial sequence received so far provides a special opportunity for incremental learning that is specific to sequence learning. Whereas the examples in a supervised learning problem bear no known relation to each other, the steps in a sequence have a very specific relation; later steps in the sequence can only be learned well once the network has learned to develop the appropriate internal state summarizing the part of the sequence seen so far. This observation leads to the idea that sequence learning may be expedited by learning to predict the first few steps in each sequence first and, once reasonable performance has been achieved and (hence) a suitable internal representation of the initial part of the sequences has been developed, gradually increasing the length of the partial sequences used for training.\nA prefix of a sequence is a consecutive subsequence (a substring) of the sequence starting from the first element; e.g. the prefix S3 of a sequence S consists of the first 3 steps of S. We define Incremental Sequence Learning as an approach to sequence learning whereby learning starts out by using only a short prefix of each sequence for training, and where the length of the prefixes used for training is gradually increased, up to the point where the complete sequences are used. The structure of sequence learning problems suggests that adequate modeling of the preceding part of the sequence is a requirement for learning later parts of the sequence; Incremental Sequence Learning draws the consequence of this by learning to predict the earlier parts of the sequences first."}, {"heading": "1.4 Related Work", "text": "In presenting the framework of Curriculum Learning, Bengio et al. (2009) provide an example within the domain of sequence learning, more specifically concerning language modeling. There, the vocabulary used for training on word sequences is gradually increased, i.e. the subset of sequences used for training is gradually increased. Another specialization of Curriculum Learning to the context of sequence learning described by Bengio et al. (2015) addresses the discrepancy between training, where the true previous step is presented as input, and inference, where the previous output from the network is used as input; with scheduled sampling, the probability of using the network output as input is adapted to gradually increase over time. Zaremba and Sutskever (2014) apply curriculum learning in a sequence-to-sequence learning context where a neural network learns to predict the outcome of Python programs. The generation of programs forming the training data is parameterized by two factors that control the complexity of the programs: the number of digits of the numbers used in the programs and the degree of nesting. While the idea of learning to predict the earlier parts of sequences first is straightforward and easy to implement, we are unaware of other work describing this approach."}, {"heading": "2 MNIST Handwritten Digits as Pen Stroke Sequences", "text": ""}, {"heading": "2.1 Motivation for representing digits as pen stroke sequences", "text": "The classification of MNIST digit images, see LeCun and Cortes (2010), is one example of a task on which the success of deep learning has been demonstrated convincingly; a test error rate of 0.23% was obtained by Ciresan et al. (2012) using Multi-column Deep Neural Networks. To obtain a sequence learning data set for evaluating Incremental Sequence Learning, we created a variant of the familiar MNIST handwritten digit data set provided by LeCun and Cortes (2010) where each digit image is transformed into a sequence of pen strokes that could have generated the digit.\nOne motivation for representing digits as strokes is the notion that when humans try to discern digits or letters that are difficult to read, it appears natural to trace the line so as to reconstruct what path the author\u2019s pen may have taken. Indeed, Hinton and Nair (2005) note that the idea that patterns can be recognized by figuring out how they were generated was already introduced in the 1950\u2019s, and describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program.\nPen stroke sequences also form a natural and efficient representation for digits; handwriting constitutes a canonical manifestation of the manifold hypothesis, according to which \u201creal-world data presented in high dimensional spaces are expected to concentrate in the vicinity of a manifoldM of much lower dimensionality dM, embedded in high dimensional input space Rdx\u201d; see Bengio et al. (2013). Specifically: (i) the vast majority of the pixels are white, (ii) almost all digit images consist of a single connected set of pixels, and (iii) the shapes mostly consist of smooth curved lines. This suggests that collections of pen strokes form a natural representation for the purpose of recognizing digits.\nThe relevance of the manifold hypothesis can also be appreciated by considering the space of all 2-D 28x28 binary pixel images; when sampling uniformly from this space, one is likely to only encounter images resembling TV noise, and the chances of observing any of the 70000 MNIST digit images is astronomically small. By contrast, a randomly generated pen stroke sequence is not unlikely to resemble a part of a digit, such as a short straight or curved line segment. This increased alignment of the digit data with its representation in the form of pen stroke sequences implies that the amount of computation required to address the learning problem can potentially be vastly reduced."}, {"heading": "2.2 Construction of the pen stroke sequence data set", "text": "The MNIST handwritten digit data set consists of 60000 training images and 10000 test images, each forming 28 x 28 bit map images of written numerical digits from 0 to 9. The digits are transformed into one or more pen strokes, each consisting of a sequence of pen offset pairs (dx, dy). To extract the pen stroke sequences, the following steps are performed:\n1. Incremental thesholding. Starting from the original MNIST grayscale image, the following characteristics are measured:\n\u2022 The number of nonzero pixels \u2022 The number of connected components, for both the 4-connected and 8-connected\nvariants. Starting from a thresholding level of zero, the thresholding level is increased stepwise, until either (A) the number of 4-connected or 8-connected components changes, (B) the number of remaining pixels drops below 50% of the original number, or (C) the thresholding level reaches a preselected maximum level (250). When any of these conditions occur, the previous level (i.e. the highest thresholding level for which none of these conditions occurred) is selected.\n2. A common method for image thinning, described by Zhang and Suen (1984), is applied. 3. After the thresholding and thinning steps, the result is a skeleton of the original digit image\nthat mostly consists of single-pixel-width lines. 4. Finding a pen stroke sequence that could have produced the digit skeleton can be viewed\nas a Traveling Salesman Problem where, starting from the origin, all points of the digit skeleton are visited. Each point is represented by the pen offset (dx, dy) from the previous to the current point. For any transition to a non-neighboring pixel (based on 8-connected distance), an extra step is inserted with (dx, dy) = (0, 0) and with eos = 1 (end-of-stroke), to indicate that the current stroke has ended and the pen is to be lifted off the paper. At the end of each sequence, a final step with values (0, 0, 1, 1) is appended. The fourth value represents eod, end-of-digit. This final tuple of the sequence marks that both the current stroke and the current sequence have ended, and forms a signal that the next input presented to the network will belong to another digit.\nIt is important to note that the thinning operation discards pixels and therefore information; this implies that the sequence learning problem constructed here should be viewed as a new learning problem, i.e. performance on this new task cannot be directly compared to results on the original MNIST classification task. While for many images the thinned skeleton is an adequate representation that retains original shape, in other cases relevant information is lost as part of the thinning process."}, {"heading": "3 Network Architecture", "text": "We adopt the approach to generative neural networks described by Graves (2013) which makes use of mixture density networks, introduced by Bishop (1994). One sequence corresponds to one complete\nimage of a digit skeleton, represented as a sequence of \u3008dx, dy, eos, eod\u3009 tuples, and may contain one or more strokes; see previous section.\nThe network has four input units, corresponding to these four input variables. To produce the input for the network, the (dx, dy) pairs are scaled to yield two real-valued input variables dx and dy. The variables indicating the end-of-stroke (EOS) and end-of-digit (EOD) are binary inputs. Two hidden LSTM layers, see Hochreiter and Schmidhuber (1997), of 200 units each are used.\nThe input units receive one step of a sequence at a time, starting with the first step. The goal for the output units is to predict the immediate next step in the sequence, but rather than trying to directly predict dx and dy, the output units represent a mixture of bivariate Gaussians. The output layer consists of the end of stroke signal (EOS), and a set of means \u00b5i , standard deviations \u03c3i, correlations \u03c1i, and mixture weights \u03c0i for each of theM mixture components, where the number of mixture componentsM = 17 was found empirically to yield good results and is used in the experiments presented here. Additionally, a binary indicator signaling the end of digit (EOD) is used, to mark the end of each sequence. In addition to these output elements for predicting the pen stroke sequences,\n10 binary class variable outputs are added, representing the 10 digit classes. This facilitates switching the task from sequence prediction to sequence classification, as will be discussed later; the output of these units is ignored in the sequence prediction experiments. The number of output units depends on the number of mixture components used, and equals 6M+ 2 + 10 = 114. For regularization, we found in early experiments that using the maximum weight as a regularization term produced better results than using the more common L-2 regularization. This approach can be viewed as L-\u221e-norm regularization, and has been used previously in the context of regularization, see e.g. Schmidt et al. (2008).\nThe definition of the sequence prediction loss LP follows Graves (2013), with the difference that terms for the eod and for the L-\u221e loss are included:\nL(x) = T\u2211\nt=1\n\u2212log \u2211 j \u03c0jtN (xt+1|\u00b5 j t , \u03c3 j t , \u03c1 j t )  \u2212 { log eost if (xt+1)3 = 1 log (1\u2212 eost) otherwise\n\u2212 {\nlog eodt if (xt+1)4 = 1\nlog (1\u2212 eodt) otherwise + \u03bb||w||\u221e"}, {"heading": "4 Incremental Sequence Learning and Comparison Methods", "text": "Below we describe Incremental Sequence Learning and three comparison methods, where two of the comparison methods are other instantiations of curriculum learning, and the third comparison is regular sequence learning without a curriculum learning aspect.\n\u2022 Regular sequence learning The baseline method is regular sequence learning; here, all training data is used from the outset.\n\u2022 Incremental Sequence Learning: increasing sequence length Predicting the second step of a sequence given the first step is a straightforward mapping problem that can be handled using regular supervised learning methods. The prediction of later steps in the sequence can potentially depend on all preceding steps, and for some cases may only be learned once an effective internal representation has been developed that summarizes relevant information present in the preceding part of the sequence. For predicting the 17th step for example, the available input consist of the previous 16 steps, and the network must learn to construct a compact representation of the preceding steps that have been seen. More specifically, it must be able to distinguish between subspaces of the sequence space that correspond to different distributions for the next step in the sequence. The number of possible contexts grows exponentially with the position in the sequence, and the task of summarizing the preceding sequence therefore potentially becomes more difficult as a function of the position within the sequence. The problem of learning to predict steps later on in the sequence is therefore potentially much harder than learning to predict the earlier steps. In Incremental Sequence Learning therefore, the length of sequences presented to the network is increased as learning progresses.\n\u2022 Increasing training set size Bengio et al. (2009) describe an application of curriculum learning to sequence learning, where the task is to predict the best word which can follow a given context of words in a correct English sentence. The curriculum strategy used there is to grow the vocabulary size. Transferring this to the context of pen stroke sequence generation, the most straightforward translation is to use subsets of the training data that grow in size, where the order of examples that are added to the training set is random.\n\u2022 Increasing number of classes The network is first presented with sequences from only one digit class; e.g. all zeros. The number of classes is increased until all 10 digit classes are represented in the training data.\nAll three curriculum learning methods employ a threshold criterion based on the training RMSE; once a specified level of the RMSE has been reached, the set of training examples (determined by the\nnumber of sequence steps used, the number of sequences used, or the number of digits) is increased. We note that many possible variants of this simple adaptive scheme are possible, some of which may provide improvements of the results."}, {"heading": "5 Experimental Settings", "text": "In this section, we describe the experimental setup in detail.\nThe configuration of the baseline method, regular sequence learning, is as follows. The number of mixture componentsM = 17, two hidden layers of size 200 are used. A batch size of 50 sequences per batch is used in these first experiments. The learning rate is \u03b1 = 0.0025, with a decay rate of 0.99995 per epoch. The order of training sequences (not steps within the sequences) is randomized. The weight of the regularization component \u03bb = 0.25. In these first experiments, a subset of 10 000 training sequences and 5 000 test sequences is used. The error measure in these figures is the RMSE of the pen offsets (unscaled) predicted by the network given the previous pen movement.\nThe RMSE is calculated based on the difference between the predicted and actual (dx, dy) pairs, scaled back to their original range of pixel units, so as to obtain an interpretable error; the eos and eod components of the error, which do form part of the loss, are not used in this error measure. For the method where the sequence length is varied, the number of individual points (input-target pairs) that must be processed per sequence varies over the course of a run. The number of sequences processed (or collections thereof such as batches or epochs) is therefore no longer an adequate measure of computational expense; performance is therefore reported as a function of the number of points processed.\nDetails per method:\n\u2022 Incremental Sequence Learning The initial sequence length is 2, meaning that the first two points of each sequence are used, i.e. after feeding the first point as input, the second point is to be predicted. Once the training RMSE drops below the threshold value of 4, the length is doubled, up to the point where it reaches the maximum sequence length.\n\u2022 Increasing training set size The initial training set size is 10. Each time the RMSE threshold of 4 is reached, this amount is doubled, up to the point where the complete set of training sequences is used.\n\u2022 Increasing number of digit classes The initial number of classes is 1, meaning that only sequences representing the first digit (zero) are used. Each time the RMSE threshold of 4 is reached, this amount is doubled, up to the point where all 10 digit classes are used."}, {"heading": "6 Experimental results", "text": ""}, {"heading": "6.1 Sequence Prediction: Comparison of the Methods", "text": "Figures 4 shows a comparison of the results of the four methods. The baseline method (in red) does not use curriculum learning, and is presented with the entire training set from the start. Incremental Sequence Learning (in green) performs markedly better than all comparison methods. It reaches the best test performance of the baseline methods twenty times faster; see the horizontal dotted black line. Moreover, Incremental Sequence Learning greatly improves generalization; on this subset of the data, the average test performance over 10 runs reaches 1.5 for Incremental Sequence Learning vs 3.9 for regular sequence learning, representing a reduction of the error of 74%.\nWe furthermore note that the variance of the test error is substantially lower than for each of the other methods, as seen in the performance graphs; and where the three comparison methods reach their best test error just before 4 \u00b7 106 processed sequence steps and then begin to deteriorate, the test error for incremental sequence learning continues to steadily decrease over the course of the run.\nThe two other curriculum methods do not provide any speedup or advantage compared to the baseline method, and in fact result in a higher test error; indiscriminate application of the curriculum learning\nprinciple apparently does not guarantee improved results and it is important therefore to discover which forms of curriculum learning can confer an advantage.\nTo explain the dramatic improvement achieved by Incremental Sequence Learning, we consider two possible hypotheses: H1: The number of sequences per batch is fixed (50), but the number of sequence steps or points varies, and is initially much smaller (2) for Incremental Sequence Learning. Thus, when measured in terms of the number of points that are being processed, the batch size for Incremental Sequence Learning is initially much smaller than for the remaining methods, and it increases adaptively over time. HypothesisH1 therefore is that (A) the smaller batch size improves performance, see Keskar et al. (2016) for earlier findings in this direction, and/or (B) the adaptive batch size aspect has a positive effect on performance. H2: Effectively learning later parts of the sequence requires an adequate internal representation of the preceding part of the sequence, which must be learned first; this formed the motivation for the Incremental Sequence Learning method.\nTo test the first hypothesis, H1, we design a second experiment where the batch size is no longer defined in terms of the number of sequences, but in terms of the number of points or sequence steps, where the number of points is chosen such that the expected total number of points for the baseline method remains the same. Thus, whereas a batch for regular sequence learning contains 50 sequences of length 40 on average yielding 2000 points, Incremental Sequence Learning will start out with batches containing 1000 sequences of 2 points each, yielding the same total number of points.\nFigure 5 shows the results. This change reduces the speedup during the earlier part of the runs, and thus partially explains the improvements observed with Incremental Sequence Learning. However, part of the speedup is still present, and moreover the three other observed improvements remain:\n\u2022 Incremental Sequence Learning still features strongly improved generalization performance \u2022 Incremental Sequence Learning still has a much lower variance of the test error \u2022 Incremental Sequence Learning still continues improving at the point where the test perfor-\nmance of all other methods starts deteriorating\nIn summary, the adaptive and initially smaller batch size of Incremental Sequence Learning explains part of the observed improvements, but not all. We therefore test to what extent hypothesisH2 plays a role. To see whether the ability to first learn a suitable representation based on the earlier parts of the sequences plays a role, we compare the situation where this effect is ruled out. A straightforward way to achieve this is to use Feed-Forward Neural Networks (FFNNs); whereas Recurrent Neural Networks (RNNs) are able to learn such a representation by learning to build up relevant internal state, FFNNs lack this ability. Therefore if any advantage of Incremental Sequence Learning is seen when using FFNNs, it cannot be due to hypothesis H2. Conversely, if using FFNNs removes the advantage, the advantage must have be due to the difference between FFNNs and RNNs, which exactly corresponds to the ability to build up an informative internal representation, i.e.H2. Since we want to explain the remaining part of the effect, we also use a batch size based on the number of points, as in Experiment 2.\nFigure 6 shows the results. As the figure shows, when using FFNNs, the advantage of Incremental Sequence Learning is entirely lost. This provides a clear demonstration that both of the hypothesesH1 andH2 play a role. Together the two hypotheses explain the total effect of the difference, suggesting that the proposed hypotheses are also the only explanatory factors that play a role.\nIt is interesting to compare the performance of the RNN and their FFNN variants, by comparing the results of Experiments 2 and 3. From this comparison, it is seen that for Incremental Sequence Learning, the RNN variant achieves improved performance compared to the FFNN variant, as would be expected, since a FFNN cannot make use of any knowledge of the preceding part of the sequence and is thus limited to learning a general mapping between two subsequent pen offsets pairs (dxk, dyk) and (dxk+1, dyk+1). However, it is the only method of the four to do so; for all three other methods, around the point where test performance for the RNN variants starts to deteriorate (after around 4 \u00b7 106 processed sequence steps), FFNN performance continues to improve and surpasses that of the RNN variants. This suggests that Incremental Sequence Learning is the only method that is able to utilize information about the preceding part of the sequence, and thereby surpass FFNN performance. In terms of absolute performance, a strong further improvement can be obtained by using the entire training set, as will be seen in the next section. These results suggest that learning the earlier parts of the sequence first can be instrumental in sequence learning."}, {"heading": "6.2 Loss as a function of sequence position", "text": "To further analyze why variation of the sequence length has a particularly strong effect on sequence learning, we evaluate how the relative difficulty of learning a sequence step relates to the position within the sequence. To do so, we measure the average loss contribution of the points or steps within a sequence as a function of their position within the sequence, as obtained with a learning method that learns entire sequences (no incremental learning), averaged over the first hundred epochs of training. Figure 7 shows the results.\nThe first steps are fundamentally unpredictable as the network cannot know which example it will receive next; accordingly, at the start of the sequence, the error is high, as the method cannot know in advance what the shape or digit class of the new sequence will be. Once the first steps of the sequence have been received and the context increasingly narrows down the possibilities, the loss for the prediction of the next steps steeply drops. Subsequently however, as the position in the sequence advances, the loss increases strongly, and exceeds the initial uncertainty of the first steps. This effect may be explained by the fact that the number of possible preceding contexts increases exponentially, thus posing stronger requirements on the learning system for steps later on in the sequence."}, {"heading": "6.3 Results on the Full MNIST Pen Stroke Sequence Data Set", "text": "The results reported so far were based on a subset of 10000 training sequences and 5000 test sequences, in order to complete a sufficient number of runs for each of the experiments within a reasonable amount of time. Given the positive results obtained with Incremental Sequence Learning, we now train this method on the full MNIST Pen Stroke Sequence Data Set, consisting of 60000 training sequences and 10000 test sequences (Experiment 4). In these experiments, a batch size of 500 sequences instead of 50 is used.\nFigure 8 shows the results. Compared to the performance of the above experiments, a strong improvement is obtained by training on this larger set of examples; whereas the best test error in the results above was slightly above 1.5, the test performance for this experiment drops below one; a test error of 0.972 on the full test data set is obtained. An strking finding is that while initially the test error is much larger than the train error, the test error continues to improve for a long time, and approaches the training error very closely; in other words, no overtraining is observed even for relatively long runs where the training performance appears to be nearly converged."}, {"heading": "6.4 Transfer Learning", "text": "The first task considered here was to perform sequence learning: predicting step t+1 of a sequence given step t. To adequately perform this task, the network must learn to detect which digit it is being fed; the initial part of a sequence representing a 2 or 3 for example is very similar, but as evidence is growing that the current sequence represents a 3, that information is vital in predicting how the stroke will continue.\nGiven that the network is expected to have built up some representation of what digit it is reading, an interesting test is to see whether it is able to switch to the task of sequence classification. The input presentation remains the same: at every time step, the recurrent neural network is fed one step of the sequence of pen movements representing the strokes of a digit. However, we now also read the output of the 10 binary class variable outputs. The target for these is a one-hot representation of the digit, i.e. the target value for the output corresponding to the digit is one, and all nine other target values are zero. To obtain the output, softmax is used, and the sequence classification loss LC for the classification outputs is the cross entropy, weighted by a factor \u03b3 = 10:\nLC = \u03b3 ( \u2212 1 N N\u2211 n=1 [ynlogy\u0302n + (1\u2212 yn)log(1\u2212 y\u0302n)] )\nIn the following experiments, the loss consists of the sequence classification loss LC , to which optionally the earlier sequence prediction loss LP is added, regulated by a binary parameter \u03b2:\nL = LC + \u03b2LP\nThe network is asked for a prediction of the digit class after each step it receives. Clearly, accurate classification is impossible during the first part of a sequence; before the first point is received, the sequence could represent any of the 10 digits with equal probability. As the sequence is received step by step however, the network receives more information. The prediction produced after receiving the one-but-last step of the sequence, i.e. at the point where the network was previously asked to predict the last step, is used as its final answer for predicting the digit class.\nWe compare the following variants:\n\u2022 Transfer learning: sequence classification and sequence prediction Starting from a trained sequence prediction model as obtained in Experiment 4, the earlier loss function is augmented with the sequence classification loss: L = LC + LP\n\u2022 Transfer Learning: sequence classification only Starting from a trained sequence prediction model, the loss function is switched such that it only reflects the classification performance, and no longer tracks the sequence prediction performance: L = LC\n\u2022 Learn from scratch, sequence classification, stroke variable present In this variant, learning starts from scratch, only sequence classification loss is used. As in the transfer learning configurations above, variables representing the four stroke sequence variables are present, and as in the the second transfer learning configuration, the outputs of these are ignored. L = LC\n\u2022 Learn from scratch, sequence classification only, no stroke variables This variant also starts from scratch and uses the classification loss only: L = LC . Since no transfer learning is used, the stroke variables can be (and are) left out.\nFigure 9 shows the results; indeed the network is able to build further on its ability to predict pen stroke sequences, and learns the sequence classification task faster and more accurately than an identical network that learns the sequence classification task from scratch; in this first and straightforward transfer learning experiment based on the MNIST stroke sequence data set, a classification accuracy of 96.0% is reached1. We note that performance on the MNIST sequence data cannot be compared to results obtained with the original MNIST data set, as the information in the input data is vastly reduced. This result sets a first baseline for the MNIST stroke sequence data set; we expect there is ample room for improvement. Simultaneously learning sequence prediction and sequence classification does not appear to provide an advantage, neither for transfer learning nor for learning from scratch."}, {"heading": "7 Generative results", "text": "To gain insight into what the network has learned, in this section we report examples of output of the network.\n1This performance was reached after training for 7 \u00b7 107 sequence steps, i.e. roughly twice as long as the run shown in the chart"}, {"heading": "7.1 Development during training", "text": "During training, the network receives each sequence step by step, and after each step, it outputs its expectation of the offset of the next point. In these figures and movies, we visualize the predictions of the network for a given sequence at different stages of the training process. All results have been obtained from a single run of Incremental Sequence Learning.\nAfter 80 batches After 140 batches After 530 batches After 570 batches After 650 batches"}, {"heading": "7.2 Unguided output generation, a.k.a. neural network hallucination", "text": "After training, the trained network can be used to generate output independently. The guidance that is present during training in the form of receiving each next step of the sequence following a prediction is not available here. Instead, the output produced by the network is fed back into the network as its next input; Figure 11 shows example results."}, {"heading": "7.3 Sequence classification", "text": "The third analysis of the behavior the trained network is to view what happens during sequence classification. At each step of the sequence, we monitor the ten class outputs and visualize their output. As more steps of the sequence are being received, the network receives more information, and adjusts its expectation of what digit class the sequence represents.\n\u25cf \u25cf\nMNIST stroke sequence test image 25\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n5 10 15 20 25 30 35\n0 2\n4 6\n8\nClassification output\nSequence step D\nig it\ncl as\ns\nClassification output for a sequence representing a 0. Initially, as the downward part of the curved stroke is being received, the network believes the sequences represents a 4. After passing the lowest point of the figure, it assigns higher likelihood to a 6. Only at the very end, just in time before the sequence ends, the prediction of the network switches for the last time, and a high probability is assigned to the correct class.\n\u25cf \u25cf\nMNIST stroke sequence test image 18\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\u25cf\u25cf\n10 20 30 40 50\n0 2\n4 6\n8\nClassification output\nSequence step\nD ig\nit cl\nas s\nClassification output for a sequence representing a 3. Initially, the networks estimates the sequence to represent a 7. Next, it expects a 2 is more likely. After 20 points have been received, it concludes correctly that the sequences represents a 3.\n\u25cf\u25cf\nMNIST stroke sequence test image 62\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n5 10 15 20 25 30 35\n0 2\n4 6\n8\nClassification output\nSequence step\nD ig\nit cl\nas s\nClassification output for a sequence representing a 9. While receiving the sequence, the dominant prediction of the network is that the sequence represents a five; the open loop of the 9 and the straight top line may contribute to this. When the last points are received, the network considers a 9 to be more likely, but some ambiguity remains."}, {"heading": "8 Conclusions", "text": "We have investigated an approach to sequence learning where the training data is initially limited to the first few steps of each sequence. Gradually, as the network learns to predict the early parts of the sequences, the length of the part of the sequences used for training is increased. We name this approach Incremental Sequence Learning, and find that it strongly improves sequence learning performance.\nA first observation was that with Incremental Sequence Learning, the time required to attain the best test performance level of regular sequence learning was much lower; on average, the method reached this level twenty times faster, thus achieving a significant speedup and reduction of the computational cost of sequence learning. More importantly, Incremental Sequence Learning was found to reduce the test error of regular sequence learning by 74%. Two other forms of curriculum sequence learning used for comparison did not display improvements compared to regular sequence learning.\nTo analyze the cause of the observed speedup and performance improvements, we first increase the number of sequences per batch for Incremental Sequence Learning, so that all methods use the same number of sequence steps per batch. This reduced the speedup, but the improvement of the generalization performance was maintained. We then replaced the RNN layers with feed forward network layers, so that the networks can no longer maintain information about the earlier part of the sequences. This completely removed the remaining advantage. This provides clear evidence that the improvement in generalization performance is due to the specific ability of an RNN to build up internal representations of the sequences it receives, and that the ability to develop these representations is aided by training on the early parts of sequences first.\nNext, we trained Incremental Sequence Learning on the full MNIST stroke sequence data set, and found that the use of this larger training set further improves sequence prediction performance. The trained model was then used as a starting point for transfer learning, where the task was switched from sequence prediction to sequence classification.\nWe conclude that Incremental Sequence Learning provides a simple and easily applicable approach to sequence learning that was found to produce large improvements in both computation time and generalization performance. The dependency of later steps in a sequence on the preceding steps is characteristic of virtually all sequence learning problems. We therefore expect that this approach can\nyield improvements for sequence learning applications in general, and recommend its usage, given that exclusively positive results were obtained with the approach so far."}, {"heading": "9 Resources", "text": "The Tensorflow implementation that was used to perform these experiments is available here: https: //github.com/edwin-de-jong/incremental-sequence-learning\nThe MNIST stroke sequence data set is available for download here: https: //github.com/edwin-de-jong/mnist-digits-stroke-sequence-data/wiki/ MNIST-digits-stroke-sequence-data\nThe code for transforming the MNIST digit data set to a pen stroke sequence data set has also been made available: https:// github.com/edwin-de-jong/mnist-digits-as-stroke-sequences/wiki/ MNIST-digits-as-stroke-sequences-(code)"}, {"heading": "Acknowledgments", "text": "The author would like to thank Max Welling, Dick de Ridder and Michiel de Jong for valuable comments and suggestions on earlier versions."}], "references": [{"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS\u201915, pages 1171\u20131179, Cambridge, MA, USA. MIT Press.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 41\u201348, New York, NY, USA. ACM.", "citeRegEx": "Bengio et al\\.,? 2009", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Mixture density networks", "author": ["C. Bishop"], "venue": "Technical Report NCRG/94/0041, Aston University.", "citeRegEx": "Bishop,? 1994", "shortCiteRegEx": "Bishop", "year": 1994}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Mach. Learn., 28(1):41\u201375.", "citeRegEx": "Caruana,? 1997", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "Multi-column deep neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "CoRR, abs/1202.2745.", "citeRegEx": "Ciresan et al\\.,? 2012", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "A coevolutionary approach to representation development", "author": ["E.D. de Jong", "T. Oates"], "venue": "Proceedings of the ICML-2002 Workshop on Development of Representations,", "citeRegEx": "Jong and Oates,? \\Q2002\\E", "shortCiteRegEx": "Jong and Oates", "year": 2002}, {"title": "Incremental learning, or the importance of starting small", "author": ["J.L. Elman"], "venue": "crl technical report 9101. Technical report, University of California, San Diego.", "citeRegEx": "Elman,? 1991", "shortCiteRegEx": "Elman", "year": 1991}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["J.L. Elman"], "venue": "Cognition, 48:781\u201399.", "citeRegEx": "Elman,? 1993", "shortCiteRegEx": "Elman", "year": 1993}, {"title": "A note on the utility of incremental learning", "author": ["C. Giraud-Carrier"], "venue": "AI Commun., 13(4):215\u2013223.", "citeRegEx": "Giraud.Carrier,? 2000", "shortCiteRegEx": "Giraud.Carrier", "year": 2000}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, abs/1308.0850.", "citeRegEx": "Graves,? 2013", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Inferring motor programs from images of handwritten digits", "author": ["G.E. Hinton", "V. Nair"], "venue": "Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada], pages 515\u2013522.", "citeRegEx": "Hinton and Nair,? 2005", "shortCiteRegEx": "Hinton and Nair", "year": 2005}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["N.S. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang"], "venue": "CoRR, abs/1609.04836.", "citeRegEx": "Keskar et al\\.,? 2016", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "A critical review of recurrent neural networks for sequence learning", "author": ["Z.C. Lipton"], "venue": "CoRR, abs/1506.00019.", "citeRegEx": "Lipton,? 2015", "shortCiteRegEx": "Lipton", "year": 2015}, {"title": "Symbiotic Evolution Of Neural Networks In Sequential Decision Tasks", "author": ["D.E. Moriarty"], "venue": "PhD thesis, Department of Computer Sciences, The University of Texas at Austin. Technical Report UT-AI97-257.", "citeRegEx": "Moriarty,? 1997", "shortCiteRegEx": "Moriarty", "year": 1997}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["E. Parisotto", "L.J. Ba", "R. Salakhutdinov"], "venue": "CoRR, abs/1511.06342.", "citeRegEx": "Parisotto et al\\.,? 2015", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Discriminability-based transfer between neural networks", "author": ["L.Y. Pratt"], "venue": "Advances in Neural Information Processing Systems 5, [NIPS Conference], pages 204\u2013211, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Pratt,? 1993", "shortCiteRegEx": "Pratt", "year": 1993}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "arxiv:1610.04286 [cs.ro]. Technical report, Deep Mind.", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Incremental learning from noisy data", "author": ["J.C. Schlimmer", "R.H. Granger"], "venue": "Machine Learning, 1(3):317\u2013354.", "citeRegEx": "Schlimmer and Granger,? 1986", "shortCiteRegEx": "Schlimmer and Granger", "year": 1986}, {"title": "Structure learning in random fields for heart motion abnormality detection", "author": ["M. Schmidt", "K. Murphy", "G. Fung", "R. Rosales"], "venue": "In CVPR.", "citeRegEx": "Schmidt et al\\.,? 2008", "shortCiteRegEx": "Schmidt et al\\.", "year": 2008}, {"title": "Sequence learning: from recognition and prediction to sequential decision making", "author": ["R. Sun", "C.L. Giles"], "venue": "IEEE Intelligent Systems, 16(4):67\u201370.", "citeRegEx": "Sun and Giles,? 2001", "shortCiteRegEx": "Sun and Giles", "year": 2001}, {"title": "Is learning the n-th thing any easier than learning the first", "author": ["S. Thrun"], "venue": "Advances in Neural Information Processing Systems, volume 8, pages 640\u2013646.", "citeRegEx": "Thrun,? 1996", "shortCiteRegEx": "Thrun", "year": 1996}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "CoRR, abs/1410.4615.", "citeRegEx": "Zaremba and Sutskever,? 2014", "shortCiteRegEx": "Zaremba and Sutskever", "year": 2014}, {"title": "A fast parallel algorithm for thinning digital patterns", "author": ["T.Y. Zhang", "C.Y. Suen"], "venue": "Commun. ACM, 27(3):236\u2013239.", "citeRegEx": "Zhang and Suen,? 1984", "shortCiteRegEx": "Zhang and Suen", "year": 1984}], "referenceMentions": [{"referenceID": 4, "context": "This principle has been described as Incremental learning by Elman (1991), and has a long history.", "startOffset": 61, "endOffset": 74}, {"referenceID": 4, "context": "This principle has been described as Incremental learning by Elman (1991), and has a long history. Schlimmer and Granger (1986) described a pseudo-connectionist distributed concept learning approach involving incremental learning.", "startOffset": 61, "endOffset": 128}, {"referenceID": 4, "context": "This principle has been described as Incremental learning by Elman (1991), and has a long history. Schlimmer and Granger (1986) described a pseudo-connectionist distributed concept learning approach involving incremental learning. Elman (1991) defined Incremental Learning as an approach where the training data is not presented all at once, but incrementally; see also Elman (1993).", "startOffset": 61, "endOffset": 244}, {"referenceID": 4, "context": "This principle has been described as Incremental learning by Elman (1991), and has a long history. Schlimmer and Granger (1986) described a pseudo-connectionist distributed concept learning approach involving incremental learning. Elman (1991) defined Incremental Learning as an approach where the training data is not presented all at once, but incrementally; see also Elman (1993). Giraud-Carrier (2000) defines Incremental Learning as follows: \u201cA learning task is incremental if the training examples used to solve it become available over time, usually one at a time.", "startOffset": 61, "endOffset": 383}, {"referenceID": 4, "context": "This principle has been described as Incremental learning by Elman (1991), and has a long history. Schlimmer and Granger (1986) described a pseudo-connectionist distributed concept learning approach involving incremental learning. Elman (1991) defined Incremental Learning as an approach where the training data is not presented all at once, but incrementally; see also Elman (1993). Giraud-Carrier (2000) defines Incremental Learning as follows: \u201cA learning task is incremental if the training examples used to solve it become available over time, usually one at a time.", "startOffset": 61, "endOffset": 406}, {"referenceID": 0, "context": "\u201c Bengio et al. (2009) introduced the framework of Curriculum Learning.", "startOffset": 2, "endOffset": 23}, {"referenceID": 10, "context": "This approach, known as Transfer Learning or Inductive Transfer, was first described by Pratt (1993). Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation.", "startOffset": 88, "endOffset": 101}, {"referenceID": 10, "context": "This approach, known as Transfer Learning or Inductive Transfer, was first described by Pratt (1993). Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation.", "startOffset": 88, "endOffset": 115}, {"referenceID": 1, "context": "Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation.", "startOffset": 127, "endOffset": 142}, {"referenceID": 1, "context": "Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation. In coevolutionary algorithms, the coevolution of representations with solutions that employ them, see e.g. Moriarty (1997); de Jong and Oates (2002), provides another approach to representation learning.", "startOffset": 127, "endOffset": 376}, {"referenceID": 1, "context": "Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation. In coevolutionary algorithms, the coevolution of representations with solutions that employ them, see e.g. Moriarty (1997); de Jong and Oates (2002), provides another approach to representation learning.", "startOffset": 127, "endOffset": 402}, {"referenceID": 0, "context": "Bengio et al. (2013) provide a review and insightful discussion of representation learning.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2013) provide a review and insightful discussion of representation learning. Parisotto et al. (2015) report experiments with transfer learning across Atari 2600 arcade games where up to 5 million frames of training time in each game are saved.", "startOffset": 0, "endOffset": 116}, {"referenceID": 0, "context": "Bengio et al. (2013) provide a review and insightful discussion of representation learning. Parisotto et al. (2015) report experiments with transfer learning across Atari 2600 arcade games where up to 5 million frames of training time in each game are saved. More recently, successful transfer of robot learning from the virtual to the real world was achieved using transfer learning, see Rusu et al. (2016). And at the annual ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), the depth of networks has steadily increased over the years, so far leading up to a network of 152 layers for the winning entry in the ILSVRC 2015 classification task; see He et al.", "startOffset": 0, "endOffset": 408}, {"referenceID": 0, "context": "Bengio et al. (2013) provide a review and insightful discussion of representation learning. Parisotto et al. (2015) report experiments with transfer learning across Atari 2600 arcade games where up to 5 million frames of training time in each game are saved. More recently, successful transfer of robot learning from the virtual to the real world was achieved using transfer learning, see Rusu et al. (2016). And at the annual ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), the depth of networks has steadily increased over the years, so far leading up to a network of 152 layers for the winning entry in the ILSVRC 2015 classification task; see He et al. (2015).", "startOffset": 0, "endOffset": 676}, {"referenceID": 21, "context": "For a discussion of variants of sequence learning problems, see Sun and Giles (2001); a more recent treatment covering recurrent neural networks as used here is provided by Lipton (2015).", "startOffset": 64, "endOffset": 85}, {"referenceID": 15, "context": "For a discussion of variants of sequence learning problems, see Sun and Giles (2001); a more recent treatment covering recurrent neural networks as used here is provided by Lipton (2015). An interesting challenge in sequence learning is that for most sequence learning problems of interest, the next step in a sequence does not follow unambiguously from the previous step.", "startOffset": 173, "endOffset": 187}, {"referenceID": 0, "context": "In presenting the framework of Curriculum Learning, Bengio et al. (2009) provide an example within the domain of sequence learning, more specifically concerning language modeling.", "startOffset": 52, "endOffset": 73}, {"referenceID": 0, "context": "In presenting the framework of Curriculum Learning, Bengio et al. (2009) provide an example within the domain of sequence learning, more specifically concerning language modeling. There, the vocabulary used for training on word sequences is gradually increased, i.e. the subset of sequences used for training is gradually increased. Another specialization of Curriculum Learning to the context of sequence learning described by Bengio et al. (2015) addresses the discrepancy between training, where the true previous step is presented as input, and inference, where the previous output from the network is used as input; with scheduled sampling, the probability of using the network output as input is adapted to gradually increase over time.", "startOffset": 52, "endOffset": 449}, {"referenceID": 0, "context": "In presenting the framework of Curriculum Learning, Bengio et al. (2009) provide an example within the domain of sequence learning, more specifically concerning language modeling. There, the vocabulary used for training on word sequences is gradually increased, i.e. the subset of sequences used for training is gradually increased. Another specialization of Curriculum Learning to the context of sequence learning described by Bengio et al. (2015) addresses the discrepancy between training, where the true previous step is presented as input, and inference, where the previous output from the network is used as input; with scheduled sampling, the probability of using the network output as input is adapted to gradually increase over time. Zaremba and Sutskever (2014) apply curriculum learning in a sequence-to-sequence learning context where a neural network learns to predict the outcome of Python programs.", "startOffset": 52, "endOffset": 772}, {"referenceID": 2, "context": "23% was obtained by Ciresan et al. (2012) using Multi-column Deep Neural Networks.", "startOffset": 20, "endOffset": 42}, {"referenceID": 2, "context": "23% was obtained by Ciresan et al. (2012) using Multi-column Deep Neural Networks. To obtain a sequence learning data set for evaluating Incremental Sequence Learning, we created a variant of the familiar MNIST handwritten digit data set provided by LeCun and Cortes (2010) where each digit image is transformed into a sequence of pen strokes that could have generated the digit.", "startOffset": 20, "endOffset": 274}, {"referenceID": 2, "context": "23% was obtained by Ciresan et al. (2012) using Multi-column Deep Neural Networks. To obtain a sequence learning data set for evaluating Incremental Sequence Learning, we created a variant of the familiar MNIST handwritten digit data set provided by LeCun and Cortes (2010) where each digit image is transformed into a sequence of pen strokes that could have generated the digit. One motivation for representing digits as strokes is the notion that when humans try to discern digits or letters that are difficult to read, it appears natural to trace the line so as to reconstruct what path the author\u2019s pen may have taken. Indeed, Hinton and Nair (2005) note that the idea that patterns can be recognized by figuring out how they were generated was already introduced in the 1950\u2019s, and describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program.", "startOffset": 20, "endOffset": 654}, {"referenceID": 0, "context": "Pen stroke sequences also form a natural and efficient representation for digits; handwriting constitutes a canonical manifestation of the manifold hypothesis, according to which \u201creal-world data presented in high dimensional spaces are expected to concentrate in the vicinity of a manifoldM of much lower dimensionality dM, embedded in high dimensional input space Rx\u201d; see Bengio et al. (2013). Specifically: (i) the vast majority of the pixels are white, (ii) almost all digit images consist of a single connected set of pixels, and (iii) the shapes mostly consist of smooth curved lines.", "startOffset": 375, "endOffset": 396}, {"referenceID": 25, "context": "A common method for image thinning, described by Zhang and Suen (1984), is applied.", "startOffset": 49, "endOffset": 71}, {"referenceID": 9, "context": "We adopt the approach to generative neural networks described by Graves (2013) which makes use of mixture density networks, introduced by Bishop (1994).", "startOffset": 65, "endOffset": 79}, {"referenceID": 3, "context": "We adopt the approach to generative neural networks described by Graves (2013) which makes use of mixture density networks, introduced by Bishop (1994). One sequence corresponds to one complete", "startOffset": 138, "endOffset": 152}, {"referenceID": 13, "context": "Two hidden LSTM layers, see Hochreiter and Schmidhuber (1997), of 200 units each are used.", "startOffset": 28, "endOffset": 62}, {"referenceID": 20, "context": "Schmidt et al. (2008). The definition of the sequence prediction loss LP follows Graves (2013), with the difference that terms for the eod and for the L-\u221e loss are included:", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "The definition of the sequence prediction loss LP follows Graves (2013), with the difference that terms for the eod and for the L-\u221e loss are included:", "startOffset": 58, "endOffset": 72}, {"referenceID": 0, "context": "\u2022 Increasing training set size Bengio et al. (2009) describe an application of curriculum learning to sequence learning, where the task is to predict the best word which can follow a given context of words in a correct English sentence.", "startOffset": 31, "endOffset": 52}, {"referenceID": 14, "context": "HypothesisH1 therefore is that (A) the smaller batch size improves performance, see Keskar et al. (2016) for earlier findings in this direction, and/or (B) the adaptive batch size aspect has a positive effect on performance.", "startOffset": 84, "endOffset": 105}], "year": 2017, "abstractText": "Deep learning research over the past years has shown that by increasing the scope or difficulty of the learning problem over time, increasingly complex learning problems can be addressed. We study incremental learning in the context of sequence learning, using generative RNNs in the form of multi-layer recurrent Mixture Density Networks. We introduce Incremental Sequence Learning, a simple incremental approach to sequence learning. Incremental Sequence Learning starts out by using only the first few steps of each sequence as training data. Each time a performance criterion has been reached, the length of the parts of the sequences used for training is increased. To evaluate Incremental Sequence Learning and comparison methods, we introduce and make available a novel sequence learning task and data set: predicting and classifying MNIST pen stroke sequences, where the familiar handwritten digit images have been transformed to pen stroke sequences representing the skeletons of the digits. We find that Incremental Sequence Learning greatly speeds up sequence learning and reaches the best test performance level of regular sequence learning 20 times faster, reduces the test error by 74%, and in general performs more robustly; it displays lower variance and achieves sustained progress after all three comparison method have stopped improving. A trained sequence prediction model is also used in transfer learning to the task of sequence classification, where it is found that transfer learning realizes improved classification performance compared to methods that learn to classify from scratch.", "creator": "LaTeX with hyperref package"}}}