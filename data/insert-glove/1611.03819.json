{"id": "1611.03819", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Recovery Guarantee of Non-negative Matrix Factorization via Alternating Updates", "abstract": "Non - dostoyevsky negative matrix factorization firm is macneish a heterogeneous popular tool appro for shiatzy decomposing data oberlander into fujinami feature pristinely and weight matrices under non - negativity constraints. It sankyo enjoys practical 19:31 success canoy but agah is poorly desta understood theoretically. terumah This naing paper proposes belka an algorithm that subdirectories alternates between lustrilanang decoding the weights balkanize and shots-24 updating benzie the features, ljungskog and adv29 shows that appleseed assuming theaetetus a dragicevic generative prest model anti-illegal of hoadly the fyllingen data, it re-assessment provably yeltin recovers scow the parentally ground - truth riisn\u00e6s under fairly kulis mild 114.06 conditions. menes In particular, jaula its only mg/ml essential requirement 15-billion on features k\u00e1\u0165a is beachcombers linear 16.33 independence. Furthermore, caricola the nisam algorithm larky uses ReLU bals to exploit laza the champing non - clogged negativity ebury for worthies decoding the weights, and thus can tolerate adversarial moorei noise that can kazaa potentially be hirsutism as large blowup as bequeathed the xinmin signal, and can carangidae tolerate blockbusting unbiased noise 5,888 much m\u00edo larger than guiraud the signal. The representa analysis relies on borgholm a carefully braeswood designed coupling lothaire between magnetized two strozzi potential functions, which we liangjiang believe babi\u0107 is gideons of independent #f interest.", "histories": [["v1", "Fri, 11 Nov 2016 19:13:37 GMT  (1430kb)", "http://arxiv.org/abs/1611.03819v1", "To appear in NIPS 2016. 8 pages of extended abstract; 48 pages in total"]], "COMMENTS": "To appear in NIPS 2016. 8 pages of extended abstract; 48 pages in total", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["yuanzhi li", "yingyu liang", "andrej risteski"], "accepted": true, "id": "1611.03819"}, "pdf": {"name": "1611.03819.pdf", "metadata": {"source": "CRF", "title": "Recovery Guarantee of Non-negative Matrix Factorization via Alternating Updates", "authors": ["Yuanzhi Li", "Yingyu Liang", "Andrej Risteski"], "emails": ["risteski}@cs.princeton.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n03 81\n9v 1\n[ cs\n.L G\n] 1\n1 N"}, {"heading": "1 Introduction", "text": "In this paper, we study the problem of non-negative matrix factorization (NMF), where given a matrix Y \u2208 Rm\u00d7N , the goal to find a matrix A \u2208 Rm\u00d7n and a non-negative matrix X \u2208 Rn\u00d7N such that Y \u2248 AX.1 A is often referred to as feature matrix and X referred as weights. NMF has been extensively used in extracting a parts representation of the data (e.g., [LS97, LS99, LS01]). Empirically it is observed that the non-negativity constraint on the coefficients forcing features to combine, but not cancel out, can lead to much more interpretable features and improved downstream performance of the learned features.\nDespite all the practical success, however, this problem is poorly understood theoretically, with only few provable guarantees known. Moreover, many of the theoretical algorithms are based on heavy tools from algebraic geometry (e.g., [AGKM12]) or tensors (e.g. [AKF+12]), which are still not as widely used in practice primarily because of computational feasibility issues or sensitivity to assumptions on A and X. Some others depend on specific structure of the feature matrix, such as separability [AGKM12] or similar properties [BGKP16].\nA natural family of algorithms for NMF alternate between decoding the weights and updating the features. More precisely, in the decoding step, the algorithm represents the data as a non-negative combination of the current set of features; in the updating step, it updates the features using the decoded representations. This meta-algorithm is popular in practice due to ease of implementation, computational efficiency, and empirical quality of the recovered features. However, even less theoretical analysis exists for such algorithms.\nThis paper proposes an algorithm in the above framework with provable recovery guarantees. To be specific, the data is assumed to come from a generative model y = A\u2217x\u2217 + \u03bd. Here, A\u2217\n1In the usual formulation of the problem, A is also assumed to be non-negative, which we will not require in this paper.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nis the ground-truth feature matrix, x\u2217 are the non-negative ground-truth weights generated from an unknown distribution, and \u03bd is the noise. Our algorithm can provably recover A\u2217 under mild conditions, even in the presence of large adversarial noise.\nOverview of main results. The existing theoretical results on NMF can be roughly split into two categories. In the first category, they make heavy structural assumptions on the feature matrix A\u2217 such as separability ([AGM12]) or allowing running time exponential in n ( [AGKM12]). In the second one, they impose strict distributional assumptions on x\u2217 ([AKF+12]), where the methods are usually based on the method of moments and tensor decompositions and have poor tolerance to noise, which is very important in practice.\nIn this paper, we present a very simple and natural alternating update algorithm that achieves the best of both worlds. First, we have minimal assumptions on the feature matrix A\u2217: the only essential condition is linear independence of the features. Second, it is robust to adversarial noise \u03bd which in some parameter regimes can potentially be on the same order as the signal A\u2217x\u2217, and is robust to unbiased noise potentially even higher than the signal by a factor of O( \u221a n). The algorithm does not require knowing the distribution of x\u2217, and allows a fairly wide family of interesting distributions. We get this at a rather small cost of a mild \u201cwarm start\u201d. Namely, we initialize each of the features to be \u201ccorrelated\u201d with the ground-truth features. This type of initialization is often used in practice as well, for example in LDA-c, the most popular software for topic modeling ([lda16]).\nA major feature of our algorithm is the significant robustness to noise. In the presence of adversarial noise on each entry of y up to level C\u03bd , the noise level \u2016\u03bd\u20161 can be in the same order as the signal A\n\u2217x\u2217. Still, our algorithm is able to output a matrix A such that the final \u2016A\u2217 \u2212A\u20161 \u2264 O(\u2016\u03bd\u20161) in the order of the noise in one data point. If the noise is unbiased (i.e., E[\u03bd|x\u2217] = 0), the noise level \u2016\u03bd\u20161 can be \u2126( \u221a n) times larger than the signal A\u2217x\u2217, while we can still guarantee \u2016A\u2217 \u2212A\u20161 \u2264\nO (\u2016\u03bd\u20161 \u221a n) \u2013 so our algorithm is not only tolerant to noise, but also has very strong denoising effect. Note that even for the unbiased case the noise can potentially be correlated with the groundtruth in very complicated manner, and also, all our results are obtained only requiring the columns of A\u2217 are independent.\nTechnical contribution. The success of our algorithm crucially relies on exploiting the nonnegativity of x\u2217 by a ReLU thresholding step during the decoding procedure. Similar techniques have been considered in prior works on matrix factorization, however to the best of our knowledge, the analysis (e.g., [AGMM15]) requires that the decodings are correct in all the intermediate iterations, in the sense that the supports of x\u2217 are recovered with no error. Indeed, we cannot hope for a similar guarantee in our setting, since we consider adversarial noise that could potentially be the same order as the signal. Our major technical contribution is a way to deal with the erroneous decoding throughout all the intermediate iterations. We achieve this by a coupling between two potential functions that capture different aspects of the working matrix A. While analyzing iterative algorithms like alternating minimization or gradient descent in non-convex settings is a popular topic in recent years, the proof usually proceeds by showing that the updates are approximately performing gradient descent on an objective with some local or hidden convex structure. Our technique diverges from the common proof strategy, and we believe is interesting in its own right.\nOrganization. After reviewing related work, we define the problem in Section 3 and describe our main algorithm in Section 4. To emphasize the key ideas, we first present the results and the proof sketch for a simplified yet still interesting case in Section 5, and then present the results under much more general assumptions in Section 6. The complete proof is provided in the appendix."}, {"heading": "2 Related work", "text": "Non-negative matrix factorization relates to several different topics in machine learning. We provide a high level review, and discuss in more details in the appendix.\nNon-negative matrix factorization. The area of non-negative matrix factorization (NMF) has a rich empirical history, starting with the practical algorithm of [LS97].On the theoretical side, [AGKM12] provides a fixed-parameter tractable algorithm for NMF, which solves algebraic equations and thus has poor noise tolerance. [AGKM12] also studies NMF under separability assumptions about the features. [BGKP16] studies NMF under heavy noise, but also needs assumptions related to separability, such as the existence of dominant features. Also, their noise model is different from ours.\nTopic modeling. A closely related problem to NMF is topic modeling, a common generative model for textual data [BNJ03, Ble12]. Usually, \u2016x\u2217\u20161 = 1 while there also exist work that assume x\u2217i \u2208 [0, 1] and are independent [ZX12]. A popular heuristic in practice for learning A\u2217 is variational inference, which can be interpreted as alternating minimization in KL divergence norm. On the theory front, there is a sequence of works by based on either spectral or combinatorial approaches, which need certain \u201cnon-overlapping\u201d assumptions on the topics. For example, [AGH+13] assume the topic-word matrix contains \u201canchor words\u201d: words which appear in a single topic. Most related is the work of [AR15] who analyze a version of the variational inference updates when documents are long. However, they require strong assumptions on both the warm start, and the amount of \u201cnon-overlapping\u201d of the topics in the topic-word matrix.\nICA. Our generative model for x\u2217 will assume the coordinates are independent, therefore our problem can be viewed as a non-negative variant of ICA with high levels of noise. Results here typically are not robust to noise, with the exception of [AGMS12] that tolerates Gaussian noise. However, to best of our knowledge, no result in this setting is provably robust to adversarial noise.\nNon-convex optimization. The framework of having a \u201cdecoding\u201d for the samples, along with performing an update for the model parameters has proven successful for dictionary learning as well. The original empirical work proposing such an algorithm (in fact, it suggested that the V1 layer processes visual signals in the same manner) was due to [OF97]. Even more, similar families of algorithms based on \u201cdecoding\u201d and gradient-descent are believed to be neurally plausible as mechanisms for a variety of tasks like clustering, dimension-reduction, NMF, etc ([PC15a, PC14]). A theoretical analysis came latter for dictionary learning due to [AGMM15] under the assumption that the columns of A\u2217 are incoherent. The technique is not directly applicable to our case, as we don\u2019t wish to have any assumptions on the matrix A\u2217. For instance, if A\u2217 is non-negative and columns with l1 norm 1, incoherence effectively means the the columns of A\u2217 have very small overlap."}, {"heading": "3 Problem definition and assumptions", "text": "Given a matrix Y \u2208 Rm\u00d7N , the goal of non-negative matrix factorization (NMF) is to find a matrix A \u2208 Rm\u00d7n and a non-negative matrix X \u2208 Rn\u00d7N , so that Y \u2248 AX. The columns of Y are called data points, those of A are features, and those of X are weights. We note that in the original NMF, A is also assumed to be non-negative, which is not required here. We also note that typically m \u226b n, i.e., the features are a few representative components in the data space. This is different from dictionary learning where overcompleteness is often assumed.\nThe problem in the worst case is NP-hard [AGKM12], so some assumptions are needed to design provable efficient algorithms. In this paper, we consider a generative model for the data point\ny = A\u2217x\u2217 + \u03bd (1)\nwhere A\u2217 is the ground-truth feature matrix, x\u2217 is the ground-truth non-negative weight from some unknown distribution, and \u03bd is the noise. Our focus is to recover A\u2217 given access to the data distribution, assuming some properties of A\u2217, x\u2217, and \u03bd. To describe our assumptions, we let [M]i denote the i-th row of a matrix M, [M]j its i-th column, Mi,j its (i, j)-th entry. Denote its column norm, row norm, and symmetrized norm as \u2016M\u20161 = maxj \u2211 i |Mi,j |, \u2016M\u2016\u221e = maxi \u2211 j |Mi,j |, and \u2016M\u2016s = max {\u2016M\u20161, \u2016M\u2016\u221e} , respectively. We assume the following hold for parameters C1, c2, C2, \u2113, C\u03bd to be determined in our theorems.\n(A1) The columns of A\u2217 are linearly independent.\n(A2) For all i \u2208 [n], x\u2217i \u2208 [0, 1], E[x\u2217i ] \u2264 C1n and c2n \u2264 E[(x\u2217i )2] \u2264 C2n , and x\u2217i \u2019s are independent. (A3) The initialization A(0) = A\u2217(\u03a3(0) + E(0)) + N(0), where \u03a3(0) is diagonal, E(0) is off-\ndiagonal, and\n\u03a3 (0) (1\u2212 \u2113)I, \u2225\u2225\u2225E(0) \u2225\u2225\u2225 s \u2264 \u2113.\nWe consider two noise models.\n(N1) Adversarial noise: only assume that maxi |\u03bdi| \u2264 C\u03bd almost surely.\n(N2) Unbiased noise: maxi |\u03bdi| \u2264 C\u03bd almost surely, and E[\u03bd|x\u2217] = 0.\nRemarks. We make several remarks about each of the assumptions. (A1) is the assumption about A\u2217. It only requires the columns of A\u2217 to be linear independent, which is very mild and needed to ensure identifiability. Otherwise, for instance, if (A\u2217)3 = \u03bb1(A\u2217)1 + \u03bb2(A \u2217)2, it is impossible to distinguish between the case when x\u22173 = 1 and the case when x \u2217 2 = \u03bb1 and x\u22171 = \u03bb2. In particular, we do not restrict the feature matrix to be non-negative, which is more general than the traditional NMF and is potentially useful for many applications. We also do not make incoherence or anchor word assumptions that are typical in related work.\n(A2) is the assumption on x\u2217. First, the coordinates are non-negative and bounded by 1; this is simply a matter of scaling. Second, the assumption on the moments requires that, roughly speaking, each feature should appear with reasonable probability. This is expected: if the occurrences of the features are extremely unbalanced, then it will be difficult to recover the rare ones. The third requirement on independence is motivated by that the features should be different so that their occurrences are not correlated. Here we do not stick to a specific distribution, since the moment conditions are more general, and highlight the essential properties our algorithm needs. Example distributions satisfying our assumptions will be discussed later.\nThe warm start required by (A3) means that each feature A(0)i has a large fraction of the groundtruth feature A\u2217i and a small fraction of the other features, plus some noise outside the span of the ground-truth features. We emphasize that N(0) is the component of A(0) outside the column space of A\u2217, and is not the difference between A(0) and A\u2217. This requirement is typically achieved in practice by setting the columns of A(0) to reasonable \u201cpure\u201d data points that contains one major feature and a small fraction of some other features (e.g. [lda16, AR15]); in this initialization, it is generally believed that N(0) = 0. But we state our theorems to allow some noise N(0) for robustness in the initialization.\nThe adversarial noise model (N1) is very general, only imposing an upper bound on the entry-wise noise level. Thus, \u03bd can be correlated with x\u2217 in some complicated unknown way. (N2) additionally requires it to be zero mean, which is commonly assumed and will be exploited by our algorithm to tolerate larger noise."}, {"heading": "4 Main algorithm", "text": "Algorithm 1 Purification\nInput: initialization A(0), threshold \u03b1, step size \u03b7, scaling factor r, sample size N , iterations T 1: for t = 0, 1, 2, ..., T \u2212 1 do 2: Draw examples y1, . . . , yN . 3: (Decode) Compute A\u2020, the pseudo-inverse of A(t) with minimum \u2016(A)\u2020\u2016\u221e.\nSet x = \u03c6\u03b1(A\u2020y) for each example y. // \u03c6\u03b1 is ReLU activation; see (2) for the definition\n4: (Update) Update the feature matrix A (t+1) = (1\u2212 \u03b7)A(t) + r\u03b7E\u0302 [ (y \u2212 y\u2032)(x\u2212 x\u2032)\u22a4 ]\nwhere E\u0302 is over independent uniform y, y\u2032 from {y1, . . . , yN}, and x, x\u2032 are their decodings. Output: A = A(T )\nOur main algorithm is presented in Algorithm 1. It keeps a working feature matrix and operates in iterations. In each iteration, it first compute the weights for a batch of N examples (decoding), and then uses the computed weights to update the feature matrix (updating).\nThe decoding is simply multiplying the example by the pseudo-inverse of the current feature matrix and then passing it through the rectified linear unit (ReLU) \u03c6\u03b1 with offset \u03b1. The pseudo-inverse with minimum infinity norm is used so as to maximize the robustness to noise (see the theorems). The ReLU function \u03c6\u03b1 operates element-wise on the input vector v, and for an element vi, it is defined as\n\u03c6\u03b1(vi) = max {vi \u2212 \u03b1, 0} . (2)\nTo get an intuition why the decoding makes sense, suppose the current feature matrix is the groundtruth. Then A\u2020y = A\u2020A\u2217x\u2217 + A\u2020\u03bd = x\u2217 + A\u2020\u03bd. So we would like to use a small A\u2020 and use threshold to remove the noise term.\nIn the encoding step, the algorithm move the feature matrix along the direction E [ (y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4 ] . To see intuitively why this is a good direction, note that when the decoding is perfect and there is no noise, E [ (y \u2212 y\u2032)(x\u2212 x\u2032)\u22a4 ] = A\u2217, and thus it is moving towards the ground-truth. Without those ideal conditions, we need to choose a proper step size, which is tuned by the parameters \u03b7 and r."}, {"heading": "5 Results for a simplified case", "text": "We will state and demonstrate our results and proof intuition in a simplified setting first, with assumptions (A1), (A2\u2019), (A3), and (N1), where\n(A2\u2019) x\u2217i \u2019s are independent, and x \u2217 i = 1 with probability s/n and 0 otherwise for a constant\ns > 0.\nFurthermore, we will assume N(0) = 0.\nNote this is a special case of our general assumptions, with C1 = c2 = C2 = s where s is the parameter in (A2\u2019). It is still an interesting setting: to the best of our knowledge there is no existing guarantee of alternating type algorithms for it. Moreover, we will present the general result in Section 6 which will be easier to digest after we have presented this simplified setting.\nFor notational convenience, let (A\u2217)\u2020 denote the matrix satisfying (A\u2217)\u2020A\u2217 = I. If there are multiple such matrices we let it denote the one with minimum \u2016(A\u2217)\u2020\u2016\u221e. Theorem 1 (Simplified case, adversarial noise). There exists an absolute constant G such that if Assumptions (A1),(A2\u2019),(A3) and (N1) are satisfied with l = 1/10, C\u03bd \u2264 Gcmax{m,n\u2016(A\u2217)\u2020\u2016\n\u221e } for\nsome 0 \u2264 c \u2264 1 and N(0) = 0, then there is a choice of parameters \u03b1, \u03b7, r such that for every 0 < \u01eb, \u03b4 < 1 and N = poly(n,m, 1/\u01eb, 1/\u03b4) the following holds with probability at least 1\u2212 \u03b4: After T = O ( ln 1\u01eb ) iterations, Algorithm 1 outputs a solution A = A\u2217(\u03a3 + E) +N where \u03a3 (1 \u2212 \u2113)I is diagonal, \u2016E\u20161 \u2264 \u01eb+ c is off-diagonal, and \u2016N\u20161 \u2264 c.\nRemarks. Consequently, when \u2016A\u2217\u20161 = 1, we can do normalization A\u0302i = Ai/\u2016Ai\u20161, and the normalized output A\u0302 satisfies \u2016A\u0302\u2212A\u2217\u20161 \u2264 \u01eb+ 2c. In particular, under mild conditions and with proper parameters, our algorithm recovers the groundtruth in a geometric rate. It can achieve arbitrary small recovery error in the noiseless setting, and achieve error up to the noise limit even with adversarial noise whose level is comparable to the signal.\nThe condition on \u2113 means that a constant warm start is sufficient for our algorithm to converge, which is much better than previous work such as [AR15]: indeed, there \u2113 depends on the dynamic range of the entries of A\u2217 which is problematic in practice.\nThe result implies that with large adversarial noise, the algorithm can still recover the features up to the noise limit. When m \u2265 n\u2016 (A\u2217)\u2020 \u2016\u221e, each data point has adversarial noise with \u21131 norm as large as \u2016\u03bd\u20161 = C\u03bdm = \u2126(c), which is in the same order as the signal \u2016A\u2217x\u2217\u20161 = O(1). Our algorithm still works in this regime. Furthermore, the final error \u2016A\u2212A\u2217\u20161 is O(c), in the same order as the adversarial noise in one data point.\nNote the appearance of \u2016 (A\u2217)\u2020 \u2016\u221e is not surprising. The case when the columns are the canonical unit vectors for instance, which corresponds to \u2016 (A\u2217)\u2020 \u2016\u221e = 1, is expected to be easier than the case when the columns are nearly the same, which corresponds to large \u2016 (A\u2217)\u2020 \u2016\u221e. A similar theorem holds for the unbiased noise model. Theorem 2 (Simplified case, unbiased noise). If Assumptions (A1),(A2\u2019),(A3) and (N2) are satisfied with C\u03bd = Gc\u221an max{m,n\u2016(A\u2217)\u2020\u2016\n\u221e } , then the same guarantee as Theorem 1 holds.\nRemarks. With unbiased noise which is commonly assumed in many applications, the algorithm can tolerate noise level \u221a n larger than the adversarial case. When m \u2265 n\u2016 (A\u2217)\u2020 \u2016\u221e, each data\npoint has noise with \u21131 norm as large as \u2016\u03bd\u20161 = C\u03bdm = \u2126(c \u221a n), which can be \u2126( \u221a n) times larger than the signal \u2016A\u2217x\u2217\u20161 = O(1). The algorithm can recover the ground-truth in this heavy noise regime. Furthermore, the final error \u2016A\u2212A\u2217\u20161 is O (\u2016\u03bd\u20161/ \u221a n), which is only O(1/ \u221a n) fraction of the noise in one data point. This is a strong denoising effect and a bit counter-intuitive. It is possible since we exploit averaging of the noise for cancellation, as well as thresholding to remove noise spread out in the coordinates."}, {"heading": "5.1 Analysis: intuition", "text": "A natural approach typically employed to analyze algorithms for non-convex problems is to define a function on the intermediate solution A and the ground-truth A\u2217 measuring their distance and then show that the function decreases at each step. However, a single potential function will not be enough in our case, as we argue below, so we introduce a novel framework of maintaining two potential functions which capture different aspects of the intermediate solutions.\nLet us denote the intermediate solution and the update as (omitting the superscript (t))\nA = A\u2217(\u03a3+E) +N, E\u0302[(y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4] = A\u2217(\u03a3\u0303+ E\u0303) + N\u0303, (3) where \u03a3 and \u03a3\u0303 are diagonal, E and E\u0303 are off-diagonal, and N and N\u0303 are the terms outside the span of A\u2217 which is caused by the noise. To cleanly illustrate the intuition behind ReLU and the coupled potential functions, we focus on the noiseless case and assume that we have infinite samples. Since Ai = \u03a3i,iA\u2217i + \u2211 j 6=i Ej,iA \u2217 j , if the ratio between \u2016Ei\u20161 = \u2211 j 6=i |Ej,i| and \u03a3i,i gets smaller, then the algorithm is making progress; if the ratio is large at the end, a normalization of Ai gives a good approximation of A\u2217i . So it suffices to show that \u03a3i,i is always about a constant while \u2016Ei\u20161 decreases at each iteration. We will focus on E and consider the update rule in more detail to argue this. After some calculation, we have\nE \u2190 (1\u2212 \u03b7)E+ r\u03b7E\u0303, E\u0303 = E[(x\u2217 \u2212 (x\u2032)\u2217) (x\u2212 x\u2032)\u22a4], (4) where x, x\u2032 are the decoding for x\u2217, (x\u2032)\u2217 respectively:\nx = \u03c6\u03b1 ( (\u03a3+E)\u22121x\u2217 ) , x\u2032 = \u03c6\u03b1 ( (\u03a3+E)\u22121(x\u2032)\u2217 ) . (5)\nTo see why the ReLU function matters, consider the case when we do not use it.\nE\u0303 = E(x\u2217 \u2212 (x\u2032)\u2217) [ A \u2020 A \u2217(x\u2217 \u2212 (x\u2032)\u2217) ]\u22a4 = E [ (x\u2217 \u2212 (x\u2032)\u2217)(x\u2217 \u2212 (x\u2032)\u2217)\u22a4 ] [ (\u03a3+E)\u22121 ]\u22a4\n\u221d [ (\u03a3+E)\u22121 ]\u22a4 \u2248 \u03a3\u22121 \u2212\u03a3\u22121E\u03a3\u22121. where we used Taylor expansion and the fact that E [ (x\u2217 \u2212 (x\u2032)\u2217)(x\u2217 \u2212 (x\u2032)\u2217)\u22a4 ] is a scaling of identity. Hence, if we think of \u03a3 as approximately I and take an appropriate r, the update to the matrix E is approximately E \u2190 E \u2212 \u03b7E\u22a4. Since we do not have control over the signs of E throughout the iterations, the problematic case is when the entries of E\u22a4 and E roughly match in signs, which would lead to the entries of E increasing.\nNow we consider the decoding to see why the ReLU is helpful. Ignoring the higher order terms and regarding \u03a3 = I, we have\nx = \u03c6\u03b1 ( (\u03a3+E)\u22121x\u2217 ) \u2248 \u03c6\u03b1 ( \u03a3 \u22121x\u2217 \u2212\u03a3\u22121E\u03a3\u22121x\u2217 ) \u2248 \u03c6\u03b1 (x\u2217 \u2212Ex\u2217) . (6)\nThe problematic term is Ex\u2217. These errors when summed up will be comparable or even larger than the signal, and the algorithm will fail. However, since the signal coordinates are non-negative and most coordinates with errors only have small values, the hope is that thresholding with ReLU can remove those errors while keeping a large fraction of the signal coordinates. This leads to large \u03a3\u0303i,i and small E\u0303j,i\u2019s, and then we can choose an r such that Ej,i\u2019s keep decreasing while \u03a3i,i\u2019s stay in a certain range.\nTo quantify the intuition above, we need to divide E into its positive part E+ and its negative part E\u2212:\n[E+]i,j = max {Ei,j , 0} , [E\u2212]i,j = max {\u2212Ei,j, 0} . (7)\nThe reason to do so is the following: when Ei,j is negative, by the Taylor expansion approximation,[ (\u03a3+E)\u22121x\u2217 ] i\nwill tend to be more positive and will not be thresholded to 0 by the ReLU most of the time. Therefore, Ej,i will become more positive at next iteration. On the other hand, when Ei,j is positive, [ (\u03a3+E)\u22121x\u2217\n] i\nwill tend to be more negative and zeroed out by the ReLU function. Therefore, Ej,i will not be more negative at next iteration. Informally, we will show for positive and negative parts of E:\npostive(t+1) \u2190 (1\u2212\u03b7)positive(t)+(\u03b7)negative(t), negative(t+1) \u2190 (1\u2212\u03b7)negative(t)+(\u03b5\u03b7)positive(t)\nfor a small \u03b5 \u226a 1. Due to the appearance of \u03b5 in the above updates, we can \u201ccouple\u201d the two parts, namely show that a weighted average of them will decrease, which implies that \u2016E\u2016s is small at the end. This leads to our coupled potential function.2"}, {"heading": "5.2 Analysis: proof sketch", "text": "We now provide a proof sketch for the simplified case presented above. The complete proof of the results for the general case (which is stated in the next section) is presented in the appendix. The lemmas here are direct corollaries of those in the appendix.\nOne iteration. We focus on one update and omit the superscript (t). Recall the definitions of E, \u03a3, N and E\u0303, \u03a3\u0303 and N\u0303 from (3). Our goal is to derive lower and upper bounds for E\u0303, \u03a3\u0303 and N\u0303, assuming that \u03a3i,i falls into some range around 1, while E and N are small. This will allow us to do induction on t.\nFirst, begin with the decoding. A simple calculation shows that the decoding for y = A\u2217x\u2217 + \u03bd is\nx = \u03c6\u03b1 (Zx \u2217 + \u03be) , where Z = (\u03a3+E)\u22121 , \u03be = \u2212A\u2020NZx\u2217 +A\u2020\u03bd. (8)\nNow, we can present our key lemmas bounding E\u0303, \u03a3\u0303, and N\u0303. Before doing this, we add that the particular value for r we will choose is r = ns (recalling s is the sparsity of x\n\u2217 according to Assumption (A2\u2019)). We also set the threshold of the ReLU as \u03c1 < \u03b1 \u226a sn . Then, we get: Lemma 3 (Simplified bound on E\u0303, informal). (1) if Zi,j < 0, then \u2223\u2223\u2223E\u0303j,i \u2223\u2223\u2223 \u2264 o ( s n (|Zi,j |+ \u03c1) ) , (2) if Zi,j \u2265 0, then \u2212O (( s n )2 Zi,j + \u03c1Zi,j ) \u2264 E\u0303j,i \u2264 O ( ( sn + \u03c1)|Zi,j | ) .\nNote that Z \u2248 \u03a3\u22121 \u2212 \u03a3\u22121E\u03a3\u22121, so Zi,j < 0 corresponds roughly to Ei,j > 0. In this case, keeping in mind that r = ns , the upper bound on |E\u0303j,i| is small enough to ensure |Ej,i| decreases, as described in the intuition.\nOn the other hand, when Zi,j \u2265 0 (roughly Ei,j < 0), the upper bound on E\u0303j,i is large enough that rE\u0303j,i can be on the same order as Ei,j , corresponding to the intuition that negative Ei,j can contribute a large positive value to Ej,i. Fortunately, the lower bound on E\u0303j,i is of much smaller absolute value, which allows us to show that a potential function that couples Case (1) and Case (2) in Lemma 3 actually decreases; see the induction below.\nLemma 4 (Simplified bound on \u03a3\u0303, informal). \u03a3\u0303i,i \u2265 \u2126((\u03a3\u22121i,i \u2212 \u03b1)/n). Lemma 5 (Simplified bound on N\u0303, adversarial noise, informal). \u2223\u2223\u2223N\u0303i,j \u2223\u2223\u2223 \u2264 O(C\u03bd/n).\nInduction by iterations. We now show how to use the three lemmas to prove the theorem for the adversarial noise. The proof for the unbiased noise statement is similar. Let at := \u2225\u2225\u2225E(t)+ \u2225\u2225\u2225 s and bt := \u2225\u2225\u2225E(t)\u2212 \u2225\u2225\u2225 s , and choose \u03b7 = \u2113/6. We begin with proving the following three claims by induction on t: at the beginning of iteration t,\n(1) (1\u2212 \u2113)I \u03a3(t) 2Note that since intuitively, Ei,j gets affected by Ej,i after an update, if we have a row which contains\nnegative entries, it is possible that \u2016Ai \u2212 A\u2217i \u20161 increases. So we cannot simply use maxi \u2016Ai \u2212 A \u2217 i \u20161 as a potential function.\n(2) \u2225\u2225E(t) \u2225\u2225 s \u2264 1/8, and if t > 0, then at + \u03b2bt \u2264 ( 1\u2212 125\u03b7 ) (at\u22121 + \u03b2bt\u22121) + \u03b7h, for some\n\u03b2 \u2208 (1, 8), and some small value h,\n(3) \u2225\u2225N(t) \u2225\u2225 s \u2264 c/10.\nThe most interesting part is the second claim. At a high level, by Lemma 3, we can show that\nat+1 \u2264 ( 1\u2212 3\n25 \u03b7\n) at + 7\u03b7bt + \u03b7h, bt+1 \u2264 ( 1\u2212 24\n25 \u03b7\n) bt + 1\n100 \u03b7at + \u03b7h.\nNotice that the contribution of bt to at+1 is quite large (due to the larger upper bound in Case (2) in Lemma 3), but the other contributions are all small. This allows to choose a \u03b2 \u2208 (1, 8) so that at+1 + \u03b2bt+1 leads to the desired recurrence in the second claim. In other words, at+1 + \u03b2bt+1 is our potential function which decreases at each iteration up to the level h. The other claims can also be proved by the corresponding lemmas. Then the theorem follows from the induction claims."}, {"heading": "6 More general results", "text": "More general weight distributions. Our argument holds under more general assumptions on x\u2217.\nTheorem 6 (Adversarial noise). There exists an absolute constant G such that if Assumption (A0)(A3) and (N1) are satisfied with l = 1/10, C2 \u2264 2c2, C31 \u2264 Gc22n, C\u03bd \u2264 { c22Gc C21m , c42Gc\nC51n\u2016(A\u2217)\u2020\u2016\u221e\n}\nfor 0 \u2264 c \u2264 1, and \u2225\u2225N(0) \u2225\u2225 \u221e \u2264 c22Gc C31\u2016(A\u2217)\u2020\u2016\u221e , then there is a choice of parameters \u03b1, \u03b7, r such that for every 0 < \u01eb, \u03b4 < 1 and N = poly(n,m, 1/\u01eb, 1/\u03b4), with probability at least 1\u2212 \u03b4 the following holds: After T = O ( ln 1\u01eb ) iterations, Algorithm 1 outputs a solution A = A\u2217(\u03a3 + E) +N where \u03a3 (1 \u2212 \u2113)I is diagonal, \u2016E\u20161 \u2264 \u01eb+ c/2 is off-diagonal, and \u2016N\u20161 \u2264 c/2.\nTheorem 7 (Unbiased noise). If Assumption (A0)-(A3) and (N2) are satisfied with C\u03bd = c2G \u221a cn C1 max{m,n\u2016(A\u2217)\u2020\u2016 \u221e } and the other parameters set as in Theorem 6, then the same guarantee holds.\nThe conditions on C1, c2, C2 intuitively mean that each feature needs to appear with reasonable probability. C2 \u2264 2c2 means that their proportions are reasonably balanced. This may be a mild restriction for some applications \u2013 however, we additionally propose a pre-processing step that can relax this in the following subsection.\nThe conditions allow a rather general family of distributions, so we point out an important special case to provide a more concrete sense of the parameters. For example, for the uniform independent distribution considered in the simplified case, we can actually allow s to be much larger than a constant; our algorithm just requires s \u2264 Gn for a fixed constant G. So it works for uniform sparse distributions even when the sparsity is linear, which is an order of magnitude larger than what can be achieved in the dictionary learning regime. Furthermore, the distributions of x\u2217i can be very different, since we only require C31 = O(c 2 2n). Moreover, all these can be handled without specific structural assumptions on A\u2217.\nMore general proportions. A mild restriction in Theorem 6 and 7 is that C2 \u2264 2c2, that is, maxi\u2208[n] E[(x \u2217 i )\n2] \u2264 2mini\u2208[n] E[(x\u2217i )2]. To relax this, we propose a pre-processing algorithm for balancing E[(x\u2217i ) 2]. The idea is quite natural: instead of solving Y \u2248 A\u2217X, we could also solve Y \u2248 [A\u2217D][(D)\u22121X] for a positive diagonal matrix D, where E[(x\u2217i )\n2]/D2i,i is with in a factor of 2 from each other. We show in the appendix that this can be done under assumptions as the above theorems, and additionally \u03a3 (1 + \u2113)I and E(0) \u2265 0 entry-wise. After balancing, one can use Algorithm 1 on the new ground-truth matrix [A\u2217D] to get the final result."}, {"heading": "7 Conclusion", "text": "A simple and natural algorithm that alternates between decoding and updating is proposed for nonnegative matrix factorization and theoretical guarantees are provided. The algorithm provably recovers a feature matrix close to the ground-truth and is robust to noise. Our analysis provides insights on the effect of the ReLU units in the presence of the non-negativity constraints, and the resulting interesting dynamics of the convergence."}, {"heading": "Acknowledgements", "text": "This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329."}, {"heading": "A Preliminary", "text": "Given a matrix Y \u2208 Rm\u00d7N , the goal of non-negative matrix factorization (NMF) is to find a matrix A \u2208 Rm\u00d7n and a non-negative matrix X \u2208 Rn\u00d7N , so that Y \u2248 AX. The columns of Y are called data points, those of A are features, and those of X are weights.\nThe notation [M]j denotes the j-th column of M, [M]i denotes the i-th row of M, and Mi,j denotes the element of M at the i-th row and j-th column. Furthermore, let M+ = denote the positive part of the matrix, and let M\u2212 denote the absolute value of the negative part of the matrix:\n[M+]i,j = { Mi,j if Mi,j \u2265 0, 0 if Mi,j < 0,\n[M\u2212]i,j = { 0 if Mi,j \u2265 0, |Mi,j | if Mi,j < 0.\nFor analysis, the following norms of the matrices are needed. Definition (l1 norm of a matrix (induced column norm)). The (induced) l1 norm of a matrix E \u2208 R n\u00d7n is\n\u2016E\u20161 = max i\u2208[n]    n\u2211\nj=1\n|Ej,i|    .\nDefinition (l\u221e norm of a matrix (induced row norm)). The (induced) l\u221e norm of a matrix E \u2208 R n\u00d7n is\n\u2016E\u2016\u221e = max i\u2208[n]    n\u2211\nj=1\n|Ei,j |    .\nThese two norms are related, and they enjoy the sub-multipicity property of the induced norm. Property 8 (dual norm). For a matrix E \u2208 Rn\u00d7n,\n\u2016E\u20161 = \u2016E\u22a4\u2016\u221e.\nNote that unlike l2 norm, it is possible that \u2016E\u20161 6= \u2016E\u22a4\u20161 or \u2016E\u2016\u221e 6= \u2016E\u22a4\u2016\u221e. Property 9 (induced norm of a matrix). Let E1,E2 \u2208 Rn\u00d7n be two matrices, then\n\u2016E1E2\u20161 \u2264 \u2016E1\u20161\u2016E2\u20161, \u2016E1E2\u2016\u221e \u2264 \u2016E1\u2016\u221e\u2016E2\u2016\u221e.\nThe following two kinds of norms are also useful for the analysis. Definition (symmetrized norm of a matrix). The symmetrized norm of a matrix E \u2208 Rn\u00d7n is\n\u2016E\u2016s = max(\u2016E\u20161, \u2016E\u2016\u221e).\nNote that \u2016E\u2016s is a norm since it\u2019s the maximum of two norms. Definition (max norm). The max norm of a matrix E \u2208 Rm\u00d7n is\n\u2016E\u2016max = maxi,j |Ei,j | .\nFor the function \u03c6\u03b1 used in our decoding algorithm, we frequently use the following properties in the analysis. Property 10 (ReLU). \u03c6\u03b1(z) = max (0, z \u2212 \u03b1) is non-decreasing. It is 1-Lipschitz, i.e.,\n|\u03c6\u03b1(z1)\u2212 \u03c6\u03b1(z2)| \u2264 |z1 \u2212 z2| . (9) It satisfies\n\u03c6\u03b1(z) \u2265 z \u2212 \u03b1, (10) \u03c6\u03b1(z) \u2264 |z \u2212 \u03b1| . (11)\nFurthermore, if \u03b1 > 0,\n\u03c6\u03b1(z) \u2264 |z| . (12)"}, {"heading": "B Proofs for main algorithm: Purification", "text": "Since NMF is NP-hard in the worst case, some assumptions are needed to make it tractable. In this paper, we consider a generative model for the data point y = A\u2217x\u2217 + \u03bd, where A\u2217 is the groundtruth feature matrix, x\u2217 is the ground-truth non-negative weight from some unknown distribution, and \u03bd is the noise. Our focus is to recover A\u2217 given access to the data distribution, assuming the following hold for parameters C1, c2, C2, \u2113, C\u03bd that will be determined in our theorems.\n(A1) The columns of A\u2217 are linearly independent.\n(A2) For all i \u2208 [n], x\u2217i \u2208 [0, 1], E[x\u2217i ] \u2264 C1n and c2n \u2264 E[(x\u2217i )2] \u2264 C2n , and x\u2217i \u2019s are independent. (A3) The initialization A(0) = A\u2217(\u03a3(0) + E(0)) + N(0), where \u03a3(0) is diagonal, E(0) is off-\ndiagonal, and\n\u03a3 (0) (1\u2212 \u2113)I, \u2225\u2225\u2225E(0) \u2225\u2225\u2225 s \u2264 \u2113.\nWe consider two noise models.\n(N1) Adversarial noise: only assume that maxi |\u03bdi| \u2264 C\u03bd almost surely. (N2) Unbiased noise: maxi |\u03bdi| \u2264 C\u03bd almost surely, and E[\u03bd|x\u2217] = 0.\nAlgorithm 1 Purification\nInput: initialization A(0), threshold \u03b1, step size \u03b7, scaling factor r, sample size N , iterations T 1: for t = 0, 1, 2, ..., T \u2212 1 do 2: Draw examples y1, . . . , yN . 3: (Decode) Compute A\u2020, the pseudo-inverse of A(t) with minimum \u2016(A)\u2020\u2016\u221e.\nSet x = \u03c6\u03b1(A\u2020y) for each example y. // \u03c6\u03b1 is ReLU activation; see (2) for the definition\n4: (Update) Update the feature matrix A (t+1) = (1\u2212 \u03b7)A(t) + r\u03b7E\u0302 [ (y \u2212 y\u2032)(x\u2212 x\u2032)\u22a4 ]\nwhere E\u0302 is over independent uniform y, y\u2032 from {y1, . . . , yN}, and x, x\u2032 are their decodings. Output: A = A(T )\nOur main algorithm is presented in Algorithm 1. It keeps a working feature matrix and operates in iterations. In each iteration, it first compute the weights for N examples (decoding), and then use the computed weights to update the feature matrix (updating).\nThe decoding is simply multiplying the example by the pseudo-inverse of the current feature matrix and then passing it through a one-sided threshold function \u03c6\u03b1. The pseudo-inverse with minimum infinity norm is used so as to maximize the robustness to noise (see the theorems). The one-sided threshold function operates element-wisely on the input vector v, and for an element vi, it is defined as\n\u03c6\u03b1(vi) = max {vi \u2212 \u03b1, 0} . This is just the rectified linear unit (ReLU) with offset \u03b1. To get some sense about the decoding, suppose the current feature matrix is the ground-truth. Then A\u2020y = A\u2020A\u2217x\u2217 +A\u2020\u03bd = x\u2217 +A\u2020\u03bd. So we would like to use a small A\u2020 and use threshold to remove the noise term.\nIn the encoding step, the algorithm move the feature matrix along the direction E [ (y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4 ] . Suppose we have independent x\u2217i \u2019s, perfect decoding and no noise,\nthen E [ (y \u2212 y\u2032)(x\u2212 x\u2032)\u22a4 ] = A\u2217, and thus it is moving towards the ground-truth. Without those ideal conditions, we need to choose a proper step size, which is tune by the parameters \u03b7 and r.\nAt the end, the algorithm simply outputs the scaled features with unit norm. The output enjoys the following guarantee in the adversarial noise model."}, {"heading": "B.1 Analysis of one update step", "text": "In this subsection, we focus on one update step, bounding the changes of \u03a3,E,N and some auxiliary variables, and then in the next subsection we put things together to prove the theorem. So through\nout this subsection we will focus on a particular iteration t and omit the superscript (t), while in the next subsection we will put back the superscript.\nFor analysis, denote A(t) as\nA = A\u2217(\u03a3+E) +N\nwhere \u03a3 is a diagonal matrix, E is an off-diagonal matrix, and N is the component of A that lies outside the span of A\u2217 (e.g., the noise caused by the noise in the sample).\nRecall the following notations:\nZ = (\u03a3+E) \u22121 ,\nV = Z\u2212\u03a3\u22121 = \u03a3\u22121 \u221e\u2211\nk=1\n(\u2212E\u03a3\u22121)k,\n\u03be = \u2212A\u2020NZx\u2217 +A\u2020\u03bd.\nConsider the update term E\u0302 [ (y \u2212 y\u2032)(x\u2212 x\u2032)\u22a4 ] and denote it as\n\u2206 = E\u0302 [ (y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4 ] = A\u2217(\u03a3\u0303+ E\u0303) + N\u0303\nwhere \u03a3\u0303 is a diagonal matrix, E\u0303 is an off-diagonal matrix, and N is the component of \u2206 that lies outside the span of A\u2217.\nSince we now use empirical average, we will have sampling noise. Denote it as\nNs = E\u0302[(y \u2212 y\u2032)(x\u2212 x\u2032)\u22a4]\u2212 E[(y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4].\nThen by definition, for y = A\u2217x\u2217 + \u03bd and y\u2032 = A\u2217(x\u2032)\u2217 + \u03bd\u2032, we have\nE\u0302[(y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4] = E[(y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4] +Ns = A\u2217 E [ (x\u2217 \u2212 (x\u2032)\u2217)(x \u2212 x\u2032)\u22a4\n] \ufe38 \ufe37\ufe37 \ufe38\n\u03a3\u0303+E\u0303\n+E [ (\u03bd \u2212 \u03bd\u2032)(x\u2212 x\u2032)\u22a4 ] +Ns\ufe38 \ufe37\ufe37 \ufe38\nN\u0303\n.\nOur goal is then bounding \u03a3\u0303, E\u0303, N\u0303 in terms of \u03a3,E,N. Before doing so, we present a lemma for the decoding. Lemma 11 (Main: Decoding). Let m \u2265 n be two positive integers. Let A \u2208 Rm\u00d7n be a matrix such that A = A\u2217(\u03a3 + E) +N where A\u2217 is full rank, \u03a3 is a diagonal matrix such that \u03a3 12I and \u2016E\u20161 < 12 . Then for y = A\u2217x\u2217 + \u03bd, the decoding is\nx = \u03c6\u03b1 (Zx \u2217 + \u03be)\n= \u03c6\u03b1 (( \u03a3 \u22121 +V ) x\u2217 + \u03be ) .\nProof of Lemma 11. Since A = A\u2217(\u03a3+E) +N, we have\nA \u2217 = (A\u2212N)(\u03a3+E)\u22121\ny = (A\u2212N)(\u03a3+E)\u22121x\u2217 + \u03bd. Plugging into the decoding we get the first statement.\nObserving that \u03a3 + E = (I + E\u03a3\u22121)\u03a3 and \u2016E\u03a3\u22121\u20161 \u2264 \u2016\u03a3\u22121\u20161\u2016E\u20161 \u2264 2\u2016E\u20161 < 1, we have (\u03a3+E)\u22121 = ( \u03a3 \u22121 +V ) , resulting in the second statement.\nLemma 12 (Main: Bound on \u03a3\u0303). Suppose |\u03bei| \u2264 \u03c1 < \u03b1 for any example and every i \u2208 [n], and suppose \u03a3 12I. Then for any i \u2208 [n],\n\u03a3\u0303i,i \u2265 E [ (x\u2217i ) 2 ] ( 2\u03a3\u22121i,i \u2212 2 |Vi,i| ) \u2212 2C1\nn\n( \u03b1+ 2\u03c1+\nC1 n \u03a3 \u22121 i,i + 2C1 n \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1 ) ,\n\u03a3\u0303i,i \u2264 E [ (x\u2217i ) 2 ] ( 2\u03a3\u22121i,i + 2 |Vi,i| ) +\n2C1 n\n( \u03c1+\nC1 n \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1 ) .\nProof of Lemma 12. According to the definition, we have\n\u03a3\u0303i,i = [ (A\u2217)\u2020E[(y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4] ] i,i\n= E [(x\u2217i \u2212 (x\u2032i)\u2217)(xi \u2212 x\u2032i)] = E [(x\u2217i \u2212 (x\u2032i)\u2217)xi] + E [((x\u2032i)\u2217 \u2212 x\u2217i )x\u2032i] .\nSince (x\u2217i \u2212 (x\u2032i)\u2217)xi and ((x\u2032i)\u2217 \u2212 x\u2217i )x\u2032i has the same distribution, and (x\u2032)\u2217, x\u2217 are i.i.d. , we have \u03a3\u0303i,i = 2E [(x \u2217 i \u2212 (x\u2032i)\u2217)xi] = 2E[x\u2217i xi]\u2212 2E[x\u2217i ]E[xi]. So it suffices to bound E[x\u2217i xi] and E[xi]. To do so, we first take a look at xi. By the decoding rule,\nxi = [ \u03c6\u03b1 (( \u03a3 \u22121 +V ) x\u2217 + \u03be )] i .\nSince \u03c6\u03b1 is 1-Lipschitz, denoting \u2206 = |[Vx\u2217]i + \u03bei| we have[ \u03c6\u03b1 ( \u03a3 \u22121x\u2217 )]\ni \u2212\u2206 \u2264 xi \u2264\n[ \u03c6\u03b1 ( \u03a3 \u22121x\u2217 )]\ni +\u2206. (13)\nFor [ \u03c6\u03b1 ( \u03a3 \u22121x\u2217 )]\ni , by the Property 10 of \u03c6\u03b1(z),\n\u03a3 \u22121 i,i x \u2217 i \u2212 \u03b1 \u2264 [ \u03c6\u03b1 ( \u03a3 \u22121x\u2217 )] i = \u03c6\u03b1 ( \u03a3 \u22121 i,i x \u2217 i ) \u2264 \u03a3\u22121i,i x\u2217i . (14)\nFor \u2206 = |[Vx\u2217]i + \u03bei|,\nE[\u2206] \u2264 E   \u2223\u2223\u2223\u2223\u2223\u2223 \u2211\nj\nVi,jx \u2217 j \u2223\u2223\u2223\u2223\u2223\u2223  + E [|\u03bei|]\n\u2264 E\n \u2211\nj\n|Vi,j |x\u2217j\n + \u03c1\n= \u2211\nj\n|Vi,j |E [ x\u2217j ] + \u03c1\n\u2264 C1 n \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1 + \u03c1 (15)\nwhere the second step follows from the assumption |\u03bei| \u2264 \u03c1, and the last step follows from Assumption (A2).\nBounding E[xi]. By (13),(14), and (15), we have\nE[xi] \u2264 E[\u03a3\u22121i,i x\u2217] + E[\u2206] \u2264 C1 n \u03a3 \u22121 i,i + C1 n \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1 + \u03c1.\nBounding E[x\u2217i xi]. First, note that\nE[x\u2217i\u2206] \u2264 E  x\u2217i \u2223\u2223\u2223\u2223\u2223\u2223 \u2211\nj\nVi,jx \u2217 j \u2223\u2223\u2223\u2223\u2223\u2223  + E [x\u2217i |\u03bei|]\n\u2264 E  x\u2217i \u2211\nj\nx\u2217j |Vi,j |\n + \u03c1C1\nn\n= \u2211\nj\nE [ x\u2217i x \u2217 j ] |Vi,j |+\n\u03c1C1 n\n= E [ (x\u2217i ) 2 ] |Vi,i|+ \u2211 j:j 6=i E [ x\u2217i x \u2217 j ] |Vi,j |+ \u03c1C1 n \u2264 E [ (x\u2217i ) 2 ] |Vi,i|+\nC21 n2\n\u2211\nj:j 6=i |Vi,j |+\n\u03c1C1 n\n\u2264 E [ (x\u2217i ) 2 ] |Vi,i|+\nC21 n2 \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1 ++ \u03c1C1 n ,\nwhere the second and the fifth steps follow from Assumption (A2). Therefore,\nE[x\u2217i xi] \u2265 E [ x\u2217i ( \u03a3 \u22121 i,i x \u2217 i \u2212 \u03b1\u2212\u2206 )] (16)\n\u2265 \u03a3\u22121i,i E [ (x\u2217i ) 2 ] \u2212 (\u03b1+ \u03c1)C1\nn \u2212 E\n[ (x\u2217i ) 2 ] |Vi,i| \u2212\nC21 n2 \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1 . (17)\nPutting together. For the first statement,\n\u03a3\u0303i,i = 2E[x \u2217 i xi]\u2212 2E[x\u2217i ]E[xi]\n\u2265 2\u03a3\u22121i,i E [ (x\u2217i ) 2 ] \u2212 2(\u03b1+ \u03c1)C1\nn \u2212 2E\n[ (x\u2217i ) 2 ] |Vi,i| \u2212 2\nC21 n2 \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1\n\u2212 2C 2 1\nn2 \u03a3\n\u22121 i,i \u2212 2 C21 n2 \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1 \u2212 2\u03c1C1 n\n\u2265 E [ (x\u2217i ) 2 ] ( 2\u03a3\u22121i,i \u2212 2 |Vi,i| ) \u2212 2C1\nn\n( \u03b1+ 2\u03c1+\nC1 n \u03a3 \u22121 i,i + 2C1 n \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1 ) .\nThe second statement follows from\n\u03a3\u0303i,i \u2264 2E[x\u2217i xi] \u2264 2E[x\u2217i (\u03a3\u22121i,i x\u2217i +\u2206)]\nand the bound on E[x\u2217i\u2206].\nLemma 13 (Main: Bound on E\u0303). Suppose |\u03bei| \u2264 \u03c1 < \u03b1 for any example and every i \u2208 [n]. Then for all i, j \u2208 [n] such that i 6= j, the following holds. (1) If Zi,j < 0, then \u2223\u2223\u2223E\u0303j,i \u2223\u2223\u2223 \u2264 4C 2 1\u2016Zi\u20161\nn2(\u03b1\u2212 \u03c1) (|Zi,j |+ \u03c1) .\n(2) If Zi,j \u2265 0, then\n\u2212 8C1\u03c1 n(\u03b1\u2212 \u03c1)\n( C1\u2016Zi\u20161\nn + Zi,j\n) \u22122C 2 1\nn2 Zi,j \u2264 E\u0303j,i \u2264\n8C1\u03c1\nn(\u03b1\u2212 \u03c1)\n( C1\u2016Zi\u20161\nn + Zi,j\n) +2E[(x\u2217j ) 2]Zi,j .\nProof of Lemma 13. Since i 6= j, we know that\nE\u0303j,i = E [ (x\u2217j \u2212 (x\u2032j)\u2217)(xi \u2212 x\u2032i) ]\n= E [ x\u2217j (xi \u2212 x\u2032i) ] + E [ (x\u2032j) \u2217(x\u2032i \u2212 xi) ] = 2E [ x\u2217j (xi \u2212 x\u2032i) ]\nwhere the last equality follows from that x\u2217j (xi \u2212 x\u2032i) and (x\u2032j)\u2217(x\u2032i \u2212 xi) has the same distribution. This quantity can be bounded by a coupling between xi and x\u2032i. Define a new variable x\u0303 \u2217 as\n[x\u0303\u2217]i = { x\u2217i , if i 6= j, (x\u2032j) \u2217, if i = j.\nBy Assumption (A2), conditional on x\u2217j , x\u0303 \u2217 has the same distribution as (x\u2032)\u2217. Therefore, consider the variable x\u0303 given by x\u0303 = \u03c6\u03b1(A\u2020(A\u2217x\u0303\u2217 + \u03bd\u2032)), we then have\nE [ x\u2217j (xi \u2212 x\u2032i) ] = E [ x\u2217j (xi \u2212 x\u0303i) ] .\nIn summary, we have E\u0303j,i = 2E[x \u2217 j (xi \u2212 x\u0303i)]\nwhere\nxi = [\u03c6\u03b1 (Zx \u2217 + \u03be)]i , \u03be = \u2212A\u2020NZx\u2217 +A\u2020\u03bd, x\u0303i = [ \u03c6\u03b1 ( Zx\u2217 + \u03be\u0303\n)] i , \u03be\u0303 = \u2212A\u2020NZx\u0303\u2217 +A\u2020\u03bd\u2032.\nIntroduce the notation w = Zi,ix \u2217 i + \u2211\nl 6=i,j Zi,lx\n\u2217 l .\nWe have\nxi = \u03c6\u03b1 ( w + Zi,jx \u2217 j + \u03bei ) ,\nx\u0303i = \u03c6\u03b1 ( w + Zi,j(x \u2032 j) \u2217 + \u03be\u0303i ) .\n(1) Since Zi,j < 0, |\u03bei| \u2264 \u03c1, and |\u03be\u0303i| \u2264 \u03c1, we know that when w < \u03b1\u2212 \u03c1, xi = x\u0303i = 0. Then E [ x\u2217j (xi \u2212 x\u0303i) ] = Pr[w \u2265 \u03b1\u2212 \u03c1] E [ x\u2217j (xi \u2212 x\u0303i)|w \u2265 \u03b1\u2212 \u03c1 ] . (18)\nBy Property 10, \u03c6\u03b1 (\u00b7) is 1-Lipschitz, so\n|xi \u2212 x\u0303i| \u2264 |Zi,j | \u2223\u2223x\u2217j \u2212 (x\u2032j)\u2217 \u2223\u2223+ \u2223\u2223\u2223\u03bei \u2212 \u03be\u0303i \u2223\u2223\u2223 ,\nwhich implies that\n\u2223\u2223E [ x\u2217j (xi \u2212 x\u0303i)|w \u2265 \u03b1\u2212 \u03c1 ]\u2223\u2223 \u2264 E [ x\u2217j |Zi,j | \u2223\u2223x\u2217j \u2212 (x\u2032j)\u2217 \u2223\u2223+ x\u2217j \u2223\u2223\u2223\u03bei \u2212 \u03be\u0303i \u2223\u2223\u2223 \u2223\u2223\u2223\u2223w \u2265 \u03b1\u2212 \u03c1 ]\n\u2264 |Zi,j |max {\u2223\u2223x\u2217j \u2212 (x\u2032j)\u2217 \u2223\u2223}E [ x\u2217j |w \u2265 \u03b1\u2212 \u03c1 ] + 2\u03c1E [ x\u2217j |w \u2265 \u03b1\u2212 \u03c1 ] \u2264 |Zi,j |max {\u2223\u2223x\u2217j \u2212 (x\u2032j)\u2217 \u2223\u2223}E [ x\u2217j ] + 2\u03c1E [ x\u2217j ] \u2264 2E [ x\u2217j ] (|Zi,j |+ \u03c1)\n\u2264 2C1 n (|Zi,j |+ \u03c1) . (19)\nNow consider Pr[w \u2265 \u03b1\u2212 \u03c1]. Since\nE |w| \u2264 |Zi,i|E[x\u2217i ] + \u2211\nl 6=i,j |Zi,l|E[x\u2217j ] \u2264 C1 n \u2016Zi\u20161,\nwe have that\nPr[w \u2265 \u03b1\u2212 \u03c1] \u2264 E |w| \u03b1\u2212 \u03c1 \u2264 C1\u2016Zi\u20161 n(\u03b1\u2212 \u03c1) (20)\nCombining (18)(19) and (20) together completes the proof for the case when Zi,j < 0.\n(2) Now consider the case when Zi,j \u2265 0. Again, we have xi = \u03c6\u03b1 ( w + Zi,jx \u2217 j + \u03bei ) ,\nx\u0303i = \u03c6\u03b1 ( w + Zi,j(x \u2032 j) \u2217 + \u03be\u0303i ) .\nFor the analysis, introduce a variable\nu\u0303i = \u03c6\u03b1 ( w + Zi,jx \u2217 j + \u03be\u0303i ) .\nIf (x\u2032j) \u2217 > x\u2217j , by Property 10 \u03c6\u03b1(\u00b7) is 1-Lipschitz, so\nx\u0303i \u2264 u\u0303i + Zi,j ( (x\u2032j) \u2217 \u2212 x\u2217j ) .\nIf (x\u2032j) \u2217 \u2264 x\u2217j , by Property 10 \u03c6\u03b1(\u00b7) is non-decreasing, then\nx\u0303i \u2264 u\u0303i. In any case,\nx\u0303i \u2264 u\u0303i + Zi,j(x\u2032j)\u2217.\nTherefore,\nE [ x\u2217j (xi \u2212 x\u0303i) ] \u2265 E [ x\u2217j (xi \u2212 u\u0303i) ] \u2212 E [ x\u2217jZi,j(x \u2032 j) \u2217]\n\u2265 E [ x\u2217j (xi \u2212 u\u0303i) ] \u2212 C 2 1\nn2 Zi,j .\nSo we only need to consider E [ x\u2217j (xi \u2212 u\u0303i) ] . Let G denote the event that xi 6= 0 or u\u0303i 6= 0. Then by conditioning on x\u2217j , we have\nE [ x\u2217j (xi \u2212 u\u0303i) ] = E [ x\u2217jE [ xi \u2212 u\u0303i \u2223\u2223\u2223\u2223x \u2217 j ]]\nand\nE [ xi \u2212 u\u0303i \u2223\u2223\u2223\u2223x \u2217 j ] = Pr [ G \u2223\u2223\u2223\u2223x \u2217 j ] E [ xi \u2212 u\u0303i \u2223\u2223\u2223\u2223x \u2217 j ,G ] .\nBy Property 10 \u03c6\u03b1(\u00b7) is 1-Lipschitz, so \u2223\u2223\u2223\u2223E [ xi \u2212 u\u0303i \u2223\u2223\u2223\u2223x \u2217 j ,G ]\u2223\u2223\u2223\u2223 \u2264 E [ |\u03bei|+ \u2223\u2223\u2223\u03be\u0303i \u2223\u2223\u2223 \u2223\u2223\u2223\u2223x \u2217 j ,G ] \u2264 2\u03c1.\nNow consider Pr [ G \u2223\u2223\u2223\u2223x\u2217j ] . We have\nE [\u2223\u2223w + Zi,jx\u2217j \u2223\u2223 \u2223\u2223\u2223\u2223x \u2217 j ] \u2264 E [ |w| \u2223\u2223\u2223\u2223x \u2217 j ] + Zi,j\n\u2264 C1 n \u2225\u2225Zi \u2225\u2225 1 + Zi,j ,\nwhere the first step follows from x\u2217j \u2264 1 and the second step follows from the conditional independence in Assumption (A2). Then by Markov\u2019s inequality,\nPr [ xi 6= 0 \u2223\u2223\u2223\u2223x \u2217 j ] \u2264 Pr [\u2223\u2223w + Zi,jx\u2217j \u2223\u2223 \u2265 \u03b1\u2212 \u03c1 \u2223\u2223\u2223\u2223x \u2217 j ]\n\u2264 1 \u03b1\u2212 \u03c1 ( C1 n \u2225\u2225Zi \u2225\u2225 1 + Zi,j ) .\nA similar argument leads to that\nPr [ u\u0303i 6= 0 \u2223\u2223\u2223\u2223x \u2217 j ] \u2264 1\n\u03b1\u2212 \u03c1 ( C1 n \u2225\u2225Zi \u2225\u2225 1 + Zi,j )\nand thus\nPr [ G \u2223\u2223\u2223\u2223x \u2217 j ] \u2264 2\n\u03b1\u2212 \u03c1 ( C1 n \u2225\u2225Zi \u2225\u2225 1 + Zi,j ) .\nPutting things together,\n\u2223\u2223E [ x\u2217j (xi \u2212 u\u0303i) ]\u2223\u2223 \u2264 4\u03c1 \u03b1\u2212 \u03c1\n( C1\u2016Zi\u20161\nn + Zi,j\n) E [ x\u2217j ]\n\u2264 4C1\u03c1 n(\u03b1\u2212 \u03c1)\n( C1\u2016Zi\u20161\nn + Zi,j\n) .\nThis completes the proof for the lower bound.\nSimilarly, for the upper bound, introduce\nui = \u03c6\u03b1 ( w + Zi,j(x \u2032 j) \u2217 + \u03bei ) .\nThen in any case,\nxi \u2264 ui + Zi,jx\u2217j\nand thus\nE [ x\u2217j (xi \u2212 x\u0303i) ] \u2264 E [ x\u2217j (ui \u2212 x\u0303i) ] + E [ (x\u2217j ) 2 ] Zi,j .\nThe same argument as above shows that\n\u2223\u2223E [ x\u2217j (ui \u2212 x\u0303i) ]\u2223\u2223 \u2264 4C1\u03c1 n(\u03b1\u2212 \u03c1)\n( C1\u2016Zi\u20161\nn + Zi,j\n) .\nThis completes the whole proof.\nLemma 14 (Main: Bound on N\u0303). Suppose \u2016E\u2016s \u2264 \u2113, \u03a3 (1\u2212 \u2113)I, and |\u03bej | \u2264 \u03c1 < \u03b1. (1) If the noise is correlated (Assumption (N1)), then\n\u2223\u2223\u2223N\u0303i,j \u2223\u2223\u2223 \u2264 4C\u03bdC1\n(1\u2212 2\u2113)2n(\u03b1\u2212 \u03c1) + |[Ns]i,j |\n(2) If the noise is unbiased (Assumption (N2)) and \u2016A\u2020\u03bd\u2016\u221e \u2264 \u03c1\u2032 < \u03b1, then \u2223\u2223\u2223N\u0303i,j \u2223\u2223\u2223 \u2264 2C1C\u03bd\u03c1 \u2032(1 + \u2016A\u2020N\u2016\u221e)\n(1\u2212 2\u2113)n(\u03b1\u2212 \u03c1\u2032) + \u2223\u2223\u2223[Ns]i,j \u2223\u2223\u2223 .\nProof of Lemma 14. (1) By the update rule,\nN\u0303 = 2E[\u03bd(x\u2212 x\u2032)\u22a4] +Ns.\nUnder Assumption (N1), we have that for every i \u2208 [n], j \u2208 [n],\n|N\u0303i,j | = |2E[\u03bdi(xj \u2212 x\u2032j)] + [Ns]i,j | \u2264 4C\u03bdE[xj ] + |[Ns]i,j | = 4C\u03bdE [\u03c6\u03b1 ([Zx\n\u2217]j + \u03bej)] + |[Ns]i,j |. since |\u03bdi| is bounded by C\u03bd . Now focus on the term E [\u03c6\u03b1 ([Zx\u2217]j + \u03bej)]. We have\n|[Zx\u2217]j | \u2264 \u2016Z\u2016\u221e\u2016x\u2217\u2016\u221e \u2264 \u2016Z\u2016\u221e \u2264 1\n1\u2212 2\u2113 by the fact that \u2016x\u2217\u2016\u221e \u2264 1 in Assumption (A2), and the assumptions of the lemma on \u03a3 and E. Then when [Zx\u2217]j + \u03bej \u2265 \u03b1,\n\u03c6\u03b1 ([Zx \u2217]j + \u03bej) \u2264 [Zx\u2217]j + \u03bej \u2212 \u03b1 \u2264\n1 1\u2212 2\u2113 + \u03c1\u2212 \u03b1 \u2264 1 1\u2212 2\u2113 ,\nand thus\nE [\u03c6\u03b1 ([Zx \u2217]j + \u03bej)] \u2264\n1\n1\u2212 2\u2113 Pr {[Zx \u2217]j + \u03bej \u2265 \u03b1}\n\u2264 1 1\u2212 2\u2113 Pr {|[Zx \u2217]j | \u2265 \u03b1\u2212 \u03c1}\n\u2264 1 1\u2212 2\u2113 E|[Zx\u2217]j | \u03b1\u2212 \u03c1\n\u2264 1 1\u2212 2\u2113 \u2016Z\u2016\u221e \u03b1\u2212 \u03c1 E [ x\u2217j ]\n\u2264 C1 (1 \u2212 2\u2113)2n(\u03b1\u2212 \u03c1)\nwhere the last step uses the bound on E [ x\u2217j ] in Assumption (A2). Therefore,\n|N\u0303i,j | \u2264 4C\u03bdC1\n(1\u2212 2\u2113)2n(\u03b1\u2212 \u03c1) + |[Ns]i,j |.\n(2) When the noise is unbiased, we have E[\u03bd|x\u2217] = 0. Then E[\u03bdix\u2032j ] = 0, and\u2223\u2223\u2223N\u0303i,j \u2223\u2223\u2223 = \u2223\u22232E[\u03bdi(xj \u2212 x\u2032j)] + [Ns]i,j \u2223\u2223 \u2264 2 |E[\u03bdixj ]|+ |[Ns]i,j | . (21)\nConsider the first term for a fixed x\u2217, i.e., consider the conditional expectation E[\u03bdixj | x\u2217]. For notational simplicity, let Z\u0303 = (Z\u2212A\u2020NZ) and \u03be\u0303 = A\u2020\u03bd. Then\nE[\u03bdixj | x\u2217] = E [\u03bdi\u03c6\u03b1 ([Zx\u2217]j + \u03bej) | x\u2217] = E [ \u03bdi\u03c6\u03b1 ( [Z\u0303x\u2217]j + \u03be\u0303j ) | x\u2217 ] .\nWe consider the following two cases about [Z\u0303x\u2217]j .\n(a) If [Z\u0303x\u2217]j \u2264 \u03b1\u2212 \u03c1\u2032, then \u03c6\u03b1 ( [Z\u0303x\u2217]j + \u03be\u0303j ) = 0 always holds, which implies that\n|E[\u03bdixj | x\u2217]| = E [ \u03bdi\u03c6\u03b1 ( [Z\u0303x\u2217]j + \u03be\u0303j ) | x\u2217 ] = 0.\n(b) If [Z\u0303x\u2217]j > \u03b1\u2212 \u03c1\u2032, then\n\u03c6\u03b1 ( [Z\u0303x\u2217]j + \u03be\u0303j ) \u2264 \u03c6\u03b1 ( [Z\u0303x\u2217]j + \u03c1 \u2032 ) \u2264 [Z\u0303x\u2217]j + \u03c1\u2032 \u2212 \u03b1.\nOn the other side, by Property 10,\n\u03c6\u03b1 ( [Z\u0303x\u2217]j + \u03be\u0303j ) \u2265 [Z\u0303x\u2217]j + \u03be\u0303j \u2212 \u03b1 \u2265 [Z\u0303x\u2217]j \u2212 \u03c1\u2032 \u2212 \u03b1.\nPutting together, we conclude that\n\u03bdi([Z\u0303x \u2217]j \u2212 \u03b1)\u2212 |\u03bdi\u03c1\u2032| \u2264 \u03bdi\u03c6\u03b1 ( [Z\u0303x\u2217]j + \u03be\u0303j ) \u2264 \u03bdi([Z\u0303x\u2217]j \u2212 \u03b1) + |\u03bdi\u03c1\u2032|.\nNote that E[\u03bdi([Z\u0303x\u2217]j \u2212 \u03b1)|x\u2217] = 0, so\n|E[\u03bdixj | x\u2217]| = \u2223\u2223\u2223E [ \u03bdi\u03c6\u03b1 ( [Z\u0303x\u2217]j + \u03be\u0303j ) | x\u2217 ]\u2223\u2223\u2223 \u2264 E[|\u03bdi\u03c1\u2032||x\u2217] \u2264 C\u03bd\u03c1\u2032.\nPutting case (a) and case (b) together, we have\n|E[\u03bdixj | x\u2217]| \u2264 C\u03bd\u03c1\u2032 Pr { [Z\u0303x\u2217]j > \u03b1\u2212 \u03c1\u2032 } \u2264 C\u03bd\u03c1\u2032 Pr {\u2223\u2223\u2223[Z\u0303x\u2217]j \u2223\u2223\u2223 > \u03b1\u2212 \u03c1\u2032 } .\nBy definition of Z\u0303 and the assumptions of the lemma on \u03a3 and E, \u2223\u2223\u2223[Z\u0303x\u2217]j \u2223\u2223\u2223 \u2264 (1 + \u2016A\u2020N\u2016\u221e) |[Zx\u2217]j | \u2264 (1 + \u2016A\u2020N\u2016\u221e) |Z|\u221e x\u2217j \u2264 1 + \u2016A\u2020N\u2016\u221e\n1\u2212 2\u2113 x \u2217 j . (22)\nThen\nPr {\u2223\u2223\u2223[Z\u0303x\u2217]j \u2223\u2223\u2223 > \u03b1\u2212 \u03c1\u2032 } \u2264 E\n\u2223\u2223\u2223[Z\u0303x\u2217]j \u2223\u2223\u2223\n\u03b1\u2212 \u03c1\u2032 \u2264 C1(1 + \u2016A\u2020N\u2016\u221e) (1\u2212 2\u2113)n(\u03b1\u2212 \u03c1\u2032) .\nThe lemma then follows from (21) and (22).\nThere are three terms Z, V and \u03be in the above lemmas that need to be bounded. Since Z = V+\u03a3\u22121, we only need to bound V and \u03be in the following two lemmas, respectively. Lemma 15 (Bound on V). Suppose \u2016E\u2016s < \u2113e and \u03a3 (1\u2212 \u2113)I. Then (1) \u2016V+\u2016s \u2264 1\u2212 \u2113e\n(1\u2212 \u2113)(1 \u2212 \u2113e \u2212 \u2113) \u2016E\u2212\u2016s +\n\u2113\n(1\u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) \u2016E+\u2016s ,\n(2) \u2016V\u2212\u2016s \u2264 1\u2212 \u2113e\n(1\u2212 \u2113)(1\u2212 \u2113e \u2212 \u2113) \u2016E+\u2016s +\n\u2113\n(1\u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) \u2016E\u2212\u2016s ,\n(3) \u2016V\u2016s \u2264 \u2113e(1\u2212 \u2113e)\n(1 \u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) ,\n(4) |Vi,i| \u2264 \u2113\u2113e\n(1 \u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) , \u2200i \u2208 [n].\nProof of Lemma 15. Denote T = \u03a3\u22121 \u2211\u221e\nk=2(\u2212E\u03a3\u22121)k, so that V = \u2212\u03a3\u22121E\u03a3\u22121 +T.\nThe following bound on \u2016T\u20161 will be useful.\n\u2016T\u20161 \u2264 \u2225\u2225\u03a3\u22121 \u2225\u2225 1\n\u221e\u2211\nk=2\n\u2225\u2225(E\u03a3\u22121)k \u2225\u2225 1\n\u2264 \u2225\u2225\u03a3\u22121 \u2225\u2225 1\n\u221e\u2211\nk=2\n\u2225\u2225E\u03a3\u22121 \u2225\u2225k 1\n\u2264 \u2225\u2225\u03a3\u22121 \u2225\u2225 1\n\u2225\u2225E\u03a3\u22121 \u2225\u22252 1\n1\u2212 \u2016E\u03a3\u22121\u20161 \u2264 1\n(1\u2212 \u2113)3 \u00d7 \u2113\u00d7 \u2016E\u20161\n1\u2212 \u2113e1\u2212\u2113 \u2264 \u2113\n(1\u2212 \u2113)2(1 \u2212 \u2113e \u2212 \u2113) \u2016E\u20161. (23)\n(1) We need to show the bound for both \u2016V+\u20161 and \u2016V+\u2016\u221e. By definition of V, for any i,\n\u2016V+\u20161 = \u2225\u2225\u2225 [ \u2212\u03a3\u22121E\u03a3\u22121 +T ] + \u2225\u2225\u2225 1 .\nSince for any A and B,\n\u2016[A+B]+\u20161 \u2264 \u2016[A]+\u20161 + \u2016[B]+\u20161, and \u2016[A]+\u20161 \u2264 \u2016A\u20161, we have\n\u2016[V+]i\u20161 \u2264 \u2225\u2225\u2225 [ \u2212\u03a3\u22121E\u03a3\u22121 ] + \u2225\u2225\u2225 1 + \u2016T+\u20161\n\u2264 1 (1\u2212 \u2113)2 \u2016E\u2212\u20161 + \u2016T\u20161. (24)\nBy (23),\n\u2016T\u20161 \u2264 \u2113\n(1\u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) \u2016E\u20161 \u2264\n\u2113\n(1 \u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) (\u2016E\u2212\u20161 + \u2016E+\u20161).\nCombined with (24), it implies\n\u2016[V+]i\u20161 \u2264 1\u2212 \u2113e\n(1\u2212 \u2113)2(1 \u2212 \u2113e \u2212 \u2113) \u2016E\u2212\u20161 +\n\u2113\n(1\u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) \u2016E+\u20161.\nSimilarly, we have \u2225\u2225\u2225[V+]i\n\u2225\u2225\u2225 1 \u2264 1\u2212 \u2113e (1 \u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) \u2016E\u2212\u2016\u221e +\n\u2113\n(1 \u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) \u2016E+\u2016\u221e.\nPutting things together we have\n\u2016V+\u2016s \u2264 1\u2212 \u2113e\n(1 \u2212 \u2113)(1\u2212 \u2113e \u2212 \u2113) \u2016E\u2212\u2016s +\n\u2113\n(1\u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) \u2016E+\u2016s .\n(2) The argument for \u2016V\u2212\u2016s is similar to that for \u2016V+\u2016s. (3) We need to show the bound for both \u2016V\u20161 and \u2016V\u2016\u221e.\n\u2016V\u20161 \u2264 \u2225\u2225\u2212\u03a3\u22121E\u03a3\u22121 \u2225\u2225 1 + \u2016T\u20161\n\u2264 \u2113e (1\u2212 \u2113)2 +\n\u2113\n(1\u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) \u2016E\u20161\n\u2264 \u2113e (1\u2212 \u2113)2 + \u2113\u2113e (1\u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) = \u2113e(1\u2212 \u2113e)\n(1\u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113)\nwhere the second step is by (23).\nSimilarly, \u2016V\u2016\u221e \u2264 \u2113e(1\u2212\u2113e) (1\u2212\u2113)2(1\u2212\u2113e\u2212\u2113) , so \u2016V\u2016s \u2264 \u2113e(1\u2212\u2113e) (1\u2212\u2113)2(1\u2212\u2113e\u2212\u2113) .\n(4) Now consider Vi,i. By definition of T.\nVi,i = [ \u2212\u03a3\u22121E\u03a3\u22121 ] i,i +Ti,i.\nNote that since Ei,i = 0, [ \u2212\u03a3\u22121E\u03a3\u22121 ] i,i = 0. Then\n|Vi,i| = |Ti,i| \u2264 \u2016T\u20161 \u2264 \u2113\n(1\u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113) \u2016E\u20161\n\u2264 \u2113\u2113e (1\u2212 \u2113)2(1\u2212 \u2113e \u2212 \u2113)\nwhere the third step is by (23). This completes the proof.\nLemma 16 (Bound on \u03be). Suppose \u2016E\u2016s < \u2113 \u2264 1/8 and \u03a3 (1\u2212 \u2113)I. Then for any i \u2208 [n],\n|\u03bei| \u2264 \u03b3 := 1 1\u2212 2\u2113 \u2225\u2225A\u2020 \u2225\u2225 \u221e\u2016N\u2016\u221e + C\u03bd \u2225\u2225A\u2020 \u2225\u2225 \u221e.\nIf furthermore, \u2016N\u2016\u221e \u2225\u2225\u2225(A\u2217)\u2020 \u2225\u2225\u2225 \u221e < 1/8, then\n\u2225\u2225A\u2020 \u2225\u2225 \u221e \u2264 2 \u2225\u2225\u2225(A\u2217)\u2020 \u2225\u2225\u2225 \u221e ,\n\u03b3 \u2264 3 \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e (\u2016N\u2016\u221e + C\u03bd) .\nProof of Lemma 16. First, we have\n\u2016\u03be\u2016\u221e \u2264 \u2225\u2225A\u2020NZx\u2217 \u2225\u2225 \u221e + \u2225\u2225A\u2020\u03bd \u2225\u2225 \u221e \u2264 \u2225\u2225A\u2020 \u2225\u2225 \u221e\u2016N\u2016\u221e\u2016Z\u2016\u221e\u2016x \u2217\u2016\u221e + \u2225\u2225A\u2020 \u2225\u2225 \u221e\u2016\u03bd\u2016\u221e.\nNote that \u2016x\u2217\u2016\u221e \u2264 1 and \u2016\u03bd\u2016\u221e \u2264 C\u03bd . Furthermore,\n\u2016Z\u2016\u221e \u2264 1\n1\u2212 2\u2113 .\nThe first statement follows from combining these terms.\nNow consider the second statement. We apply Lemma 17. Since\n\u03b6 = \u2016E\u03a3\u22121 + (A\u2217)\u2020N\u03a3\u22121\u2016\u221e \u2264 \u2016E\u03a3\u22121\u2016\u221e + \u2016(A\u2217)\u2020N\u03a3\u22121\u2016\u221e \u2264 1\n7 + \u2016(A\u2217)\u2020\u2016\u221e \u00d7 \u2016N\u2016\u221e \u00d7 \u2016\u03a3\u22121\u2016\u221e\n\u2264 2 7 ,\nLemma 17 implies that\n\u2016A\u2020\u2016\u221e \u2264 \u2016\u03a3\u22121\u2016\u221e 1\u2212 \u03b6 \u2016(A \u2217)\u2020\u2016\u221e \u2264 2\u2016(A\u2217)\u2020\u2016\u221e.\nThen \u03b3 is bounded by\n\u03b3 = 1 1\u2212 2\u2113 \u2225\u2225A\u2020 \u2225\u2225 \u221e\u2016N\u2016\u221e + C\u03bd \u2225\u2225A\u2020 \u2225\u2225 \u221e\n\u2264 1 1\u2212 2\u2113 \u00d7\n( 2\u2016(A\u2217)\u2020\u2016\u221e ) \u00d7 \u2016N\u2016\u221e + C\u03bd \u00d7 ( 2\u2016(A\u2217)\u2020\u2016\u221e )\n\u2264 3 \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e (\u2016N\u2016\u221e + C\u03bd) .\nThe following is the lemma about the norm of the pseudo-inverse, which is used in Lemma 16. Lemma 17 (Pseudo-inverse). Let A\u2217,N \u2208 Rm\u00d7n be two matrices with m \u2265 n. Let (A\u2217)\u2020 be one pseudo-inverse of A\u2217 such that (A\u2217)\u2020A\u2217 = I. Let A = A\u2217(\u03a3 + E) +N be another matrix, with \u03a3 being diagonal and \u03b6 := \u2016E\u03a3\u22121 + (A\u2217)\u2020N\u03a3\u22121\u2016\u221e. satisfies \u03b6 < 1. Then there exists a pseudo-inverse A\u2020 of A such that A\u2020A = I and\n\u2016A\u2020\u2016\u221e \u2264 \u2016\u03a3\u22121\u2016\u221e 1\u2212 \u03b6 \u2016(A \u2217)\u2020\u2016\u221e.\nProof of Lemma 17. Consider the matrix\nA \u2020 = (\u03a3+E+ (A\u2217)\u2020N)\u22121(A\u2217)\u2020.\nThen by definition,\nA \u2020 A = (\u03a3+E+ (A\u2217)\u2020N)\u22121(A\u2217)\u2020 (A\u2217(\u03a3+E) +N)\n= (\u03a3+E+ (A\u2217)\u2020N)\u22121(\u03a3+E+ (A\u2217)\u2020N)\n= I.\nWhat remains is to bound \u2016A\u2020\u2016\u221e. We have \u2016A\u2020\u2016\u221e \u2264 \u2016(\u03a3+E+ (A\u2217)\u2020N)\u22121\u2016\u221e\u2016(A\u2217)\u2020\u2016\u221e.\nBy Taylor expansion rule, the first term on the right-hand side is\n(\u03a3+E+ (A\u2217)\u2020N)\u22121 = (( I+E\u03a3\u22121 + (A\u2217)\u2020N\u03a3\u22121 ) \u03a3 )\u22121\n= \u03a3\u22121 ( I+E\u03a3\u22121 + (A\u2217)\u2020N\u03a3\u22121 )\u22121\n=\n\u221e\u2211\ni=0\n\u03a3 \u22121 (\u2212E\u03a3\u22121 \u2212 (A\u2217)\u2020N\u03a3\u22121 )i\nwhere we use the assumption that \u2016E\u03a3\u22121 + (A\u2217)\u2020N\u03a3\u22121\u2016\u221e = \u03b6 < 1. Therefore,\n\u2016(\u03a3+E+ (A\u2217)\u2020N)\u22121\u2016\u221e \u2264 \u2016\u03a3\u22121\u2016\u221e \u221e\u2211\ni=0\n\u03b6i = \u2016\u03a3\u22121\u2016\u221e 1\u2212 \u03b6 ."}, {"heading": "B.2 Putting things together", "text": "We are now ready to prove our main theorems.\nTheorem 6 (Adversarial noise). There exists an absolute constant G such that if Assumption (A0)(A3) and (N1) are satisfied with l = 1/10, C2 \u2264 2c2, C31 \u2264 Gc22n, C\u03bd \u2264 { c22Gc C21m , c42Gc\nC51n\u2016(A\u2217)\u2020\u2016\u221e\n}\nfor 0 \u2264 c \u2264 1, and \u2225\u2225N(0) \u2225\u2225 \u221e \u2264 c22Gc C31\u2016(A\u2217)\u2020\u2016\u221e , then there is a choice of parameters \u03b1, \u03b7, r such that for every 0 < \u01eb, \u03b4 < 1 and N = poly(n,m, 1/\u01eb, 1/\u03b4), with probability at least 1\u2212 \u03b4 the following holds: After T = O ( ln 1\u01eb ) iterations, Algorithm 1 outputs a solution A = A\u2217(\u03a3 + E) +N where \u03a3 (1 \u2212 \u2113)I is diagonal, \u2016E\u20161 \u2264 \u01eb+ c/2 is off-diagonal, and \u2016N\u20161 \u2264 c/2.\nProof of Theorem 6. We consider the following set of parameters\n\u03b1 = c2\n80C1 , r =\nn c2 , \u03b7 = \u2113 6 .\nFurthermore, set \u03c1 = B1 c22c C31 for a sufficiently small absolute constant B1. Since C1 \u2265 nE[x\u2217i ] \u2265 nE[(x\u2217i ) 2] \u2265 c2, this is small enough so that\n\u03c1 \u2264 min { \u03b1\n2 ,\nc2\u03b1\n2048C1 ,\nc2\u03b1\n8000\u00d7 100C21 ,\ncc2\u03b1\n48000C21\n}\nwhich will be used in the proof. The proof also needs C21 \u2264 B1c2n,C31 \u2264 B2c22n for sufficiently small absolute constants B1 and B2. Since C1 > c2, we only need C31 \u2264 Gc22n. Similarly, we need\nC\u03bd \u2264 B1 min { c(\u03b1\u2212 \u03c1)c2\nmC1 , (\u03b1\u2212 \u03c1)c2 nC1\u2016(A\u2217)\u2020\u2016\u221e , (\u03b1\u2212 \u03c1)c2\u03c1 nC1\u2016(A\u2217)\u2020\u2016\u221e ,\n\u03c1\n\u2016(A\u2217)\u2020\u2016\u221e\n}\nfor a sufficiently small absolute constant B1. This can be satisfied by setting G small enough in the theorem assumption.\nAfter setting the parameters needed, we now prove the theorem. We prove it by proving the following three claims by induction on t: at the beginning of iteration t,\n(1) (1\u2212 \u2113)I \u03a3(t), (2) \u2225\u2225E(t) \u2225\u2225 s \u2264 18 , and if t > 0\n\u2225\u2225\u2225E(t)+ \u2225\u2225\u2225 s + \u03b2 \u2225\u2225\u2225E(t)\u2212 \u2225\u2225\u2225 s \u2264 ( 1\u2212 1 25 \u03b7 )(\u2225\u2225\u2225[E](t\u22121)+ \u2225\u2225\u2225 s + \u03b2 \u2225\u2225\u2225[E](t\u22121)\u2212 \u2225\u2225\u2225 s ) + c 10 ,\nfor \u03b2 = \u221a 842+2800\u221284\n2 \u2208 (1, 8),\n(3) \u2225\u2225N(t) \u2225\u2225 \u221e \u2264 1 8\u2016(A\u2217)\u2020\u2016\u221e , and \u2016\u03be(t)\u2016\u221e \u2264 \u03c1.\nClaim (1) and (2) are clearly true at t = 0 by the assumption on initialization. The first part of Claim (3) is true because of the assumption that \u2225\u2225N(0) \u2225\u2225 \u221e \u2264 Gc 8\u00b53\u2016(A\u2217)\u2020\u2016\u221e\nand that \u00b5 = C1/c2 \u2265 1. Then the second part follows from Lemma 16.\nNow we assume they are true up to t, and show them for t+ 1.\n(1) First consider the diagonal terms. Combining Lemma 12 and Lemma 15, we have\n\u03a3\u0303 (t) i,i \u2265 E [ (x\u2217i ) 2 ] ( 2(\u03a3 (t) i,i ) \u22121 \u2212 2 \u2223\u2223\u2223V(t)i,i \u2223\u2223\u2223 ) \u2212 2C1\nn\n( \u03b1+ 2\u03c1+\nC1 n (\u03a3 (t) i,i ) \u22121 + 2C1 n \u2225\u2225\u2225\u2225 [ V (t) ]i\u2225\u2225\u2225\u2225\n1\n) .\n\u2265 2C2 n\n( 0\u2212 \u2113 2 (1\u2212 2\u2113)(1\u2212 \u2113)2 ) \u2212 2C1 n ( \u03b1+ \u03b1+ C1 n 1 1\u2212 \u2113 + 2C1 n\n\u2113\n(1 \u2212 \u2113)(1\u2212 2\u2113)\n) .\n= 2C2 n\n( 0\u2212 \u2113 2 (1\u2212 2\u2113)(1\u2212 \u2113)2 ) \u2212 2C1 n ( 2\u03b1+ C1 n(1\u2212 \u2113)(1\u2212 2\u2113) )\n> \u2212 c2 5n .\nThe first inequality uses \u03c1 < \u03b1/2 and the last inequality is due to \u03b1 \u2264 c280C1 and C 2 1 \u2264 c2n80 . Therefore, \u03a3\n(t+1) i,i = (1\u2212 \u03b7)\u03a3 (t) i,i + \u03b7r\u03a3\u0303 (t) i,i \u2265 (1\u2212 \u03b7)\u03a3 (t) i,i \u2212\n\u03b7 5 .\nAssume for contradiction \u03a3(t+1)i,i < 1\u2212 \u2113. Then by the above inequality,\n1\u2212 \u2113 > \u03a3(t+1)i,i \u2265 (1 \u2212 \u03b7)\u03a3 (t) i,i \u2212\n\u03b7 5 .\nwhich implies \u03a3(t)i,i \u2264 1\u2212 \u2113+ 2\u03b7. In this case, by Lemma 12 and Lemma 15,\n\u03a3\u0303 (t) i,i \u2265 E [ (x\u2217i ) 2 ] ( 2(\u03a3 (t) i,i ) \u22121 \u2212 2 \u2223\u2223\u2223V(t)i,i \u2223\u2223\u2223 ) \u2212 2C1\nn\n( \u03b1+ 2\u03c1+\nC1 n (\u03a3 (t) i,i ) \u22121 + 2C1 n \u2225\u2225\u2225\u2225 [ V (t) ]i\u2225\u2225\u2225\u2225\n1\n) .\n\u2265 2c2 n\n( 1\n1\u2212 \u2113+ 2\u03b7 \u2212 \u21132 (1 \u2212 2\u2113)(1\u2212 \u2113)2 ) \u2212 2C1 n ( 2\u03b1+ C1 n(1\u2212 \u2113)(1 \u2212 2\u2113) )\n> c2 n .\nThen \u03a3\n(t+1) i,i = (1\u2212 \u03b7)\u03a3 (t) i,i + \u03b7r\u03a3\u0303 (t) i,i = (1\u2212 \u03b7)\u03a3 (t) i,i + \u03b7 > \u03a3 (t) i,i ,\nwhich is a contradiction. Therefore, (1\u2212 \u2113)I \u03a3(t). (2) Now consider the off-diagonal terms. We shall split them into the positive part and the negative part. By the update rule, for any i \u2208 [n],\n\u2225\u2225\u2225 [ E (t+1) + ] i \u2225\u2225\u2225 1 \u2264 (1\u2212 \u03b7) \u2225\u2225\u2225 [ E (t) + ] i \u2225\u2225\u2225 1 + \u03b7r \u2225\u2225\u2225 [ E\u0303 (t) + ] i \u2225\u2225\u2225 1 .\nRecall the notations\nZ (t) = (\u03a3(t) +E(t))\u22121 = (\u03a3(t))\u22121 +V(t),\nV (t) = (\u03a3(t))\u22121\n\u221e\u2211\nk=1\n(\u2212E(t)(\u03a3(t))\u22121)k\nBy Lemma 13, we have \u2225\u2225\u2225 [ E\u0303 (t) + ] i \u2225\u2225\u2225 1 \u2264 \u2211\nj 6=i\n4C21 n2(\u03b1\u2212 \u03c1) \u2225\u2225\u2225\u2225 [ Z (t) ]i\u2225\u2225\u2225\u2225\n1\n(\u2223\u2223\u2223\u2223 [ Z (t) \u2212 ] i,j \u2223\u2223\u2223\u2223+ \u03c1 )\n\ufe38 \ufe37\ufe37 \ufe38 T1 + \u2211\nj 6=i\n8C1\u03c1\nn(\u03b1\u2212 \u03c1) ( C1 n \u2225\u2225\u2225\u2225 [ Z (t) ]i\u2225\u2225\u2225\u2225\n1\n+ \u2223\u2223\u2223\u2223 [ Z (t) + ] i,j \u2223\u2223\u2223\u2223 )\n\ufe38 \ufe37\ufe37 \ufe38 T2\n+ \u2211\nj 6=i 2E[(x\u2217j ) 2]\n\u2223\u2223\u2223\u2223 [ Z (t) + ] i,j \u2223\u2223\u2223\u2223 \ufe38 \ufe37\ufe37 \ufe38\nT3\n.\nFirst, by Lemma 15, \u2225\u2225\u2225\u2225 [ Z (t) ]i\u2225\u2225\u2225\u2225\n1\n\u2264 [( \u03a3 (t) )\u22121]\ni,i\n+ \u2225\u2225\u2225\u2225 [ V (t) ]i\u2225\u2225\u2225\u2225\n1\n\u2264 1 1\u2212 2\u2113\nNow consider Z(t)+ and Z (t) \u2212 . We have\n\u2211\nj:j 6=i\n\u2223\u2223\u2223\u2223 [ Z (t) \u2212 ] i,j \u2223\u2223\u2223\u2223 \u2264 \u2225\u2225\u2225 [ V (t) \u2212 ] i \u2225\u2225\u2225 1 , \u2211\nj:j 6=i\n\u2223\u2223\u2223\u2223 [ Z (t) + ] i,j \u2223\u2223\u2223\u2223 \u2264 \u2225\u2225\u2225 [ V (t) + ] i \u2225\u2225\u2225 1 .\nTherefore,\nT 1 \u2264 8C 2 1 n2(\u03b1\u2212 \u03c1) \u2225\u2225\u2225 [ V (t) \u2212 ] i \u2225\u2225\u2225 1 + 8C21\u03c1 n(\u03b1\u2212 \u03c1) ,\nT 2 \u2264 16C 2 1\u03c1 n(\u03b1\u2212 \u03c1) + 8C1\u03c1 n(\u03b1\u2212 \u03c1) \u2225\u2225\u2225 [ V (t) + ] i \u2225\u2225\u2225 1 ,\nT 3 \u2264 2C2 n\n\u2225\u2225\u2225 [ V (t) + ] i \u2225\u2225\u2225 1 .\nand thus we have \u2225\u2225\u2225 [ E\u0303 (t) + ] i \u2225\u2225\u2225 1 \u2264 8C 2 1 n2(\u03b1\u2212 \u03c1) \u2225\u2225\u2225 [ V (t) \u2212 ] i \u2225\u2225\u2225 1 + ( 2C2 n + 8C1\u03c1 n(\u03b1\u2212 \u03c1) )\u2225\u2225\u2225 [ V (t) + ] i \u2225\u2225\u2225 1 + 24C21\u03c1 n(\u03b1\u2212 \u03c1) .\nSimilarly, for any i \u2208 [n], \u2225\u2225\u2225\u2225 [ E\u0303 (t) +\n]i\u2225\u2225\u2225\u2225 1 \u2264 8C 2 1 n2(\u03b1\u2212 \u03c1) \u2225\u2225\u2225\u2225 [ V (t) \u2212 ]i\u2225\u2225\u2225\u2225 1 + ( 2C2 n + 8C1\u03c1 n(\u03b1\u2212 \u03c1) )\u2225\u2225\u2225\u2225 [ V (t) + ]i\u2225\u2225\u2225\u2225 1 + 24C21\u03c1 n(\u03b1\u2212 \u03c1) .\nPutting the two together, we have \u2225\u2225\u2225E\u0303(t)+ \u2225\u2225\u2225 s \u2264 8C 2 1 n2(\u03b1\u2212 \u03c1) \u2225\u2225\u2225V(t)\u2212 \u2225\u2225\u2225 s + ( 2C2 n + 8C1\u03c1 n(\u03b1\u2212 \u03c1) )\u2225\u2225\u2225V(t)+ \u2225\u2225\u2225 s + 24C21\u03c1 n(\u03b1\u2212 \u03c1) . (25)\nBy Lemma 15 and \u2113 \u2264 18 , we have: \u2225\u2225\u2225V(t)+ \u2225\u2225\u2225 s \u2264 32 21 \u2225\u2225\u2225E(t)\u2212 \u2225\u2225\u2225 s + 32 147 \u2225\u2225\u2225E(t)+ \u2225\u2225\u2225 s ,\n\u2225\u2225\u2225V(t)\u2212 \u2225\u2225\u2225 s \u2264 32 21 \u2225\u2225\u2225E(t)+ \u2225\u2225\u2225 s + 32 147 \u2225\u2225\u2225E(t)\u2212 \u2225\u2225\u2225 s\nSo (25) becomes \u2225\u2225\u2225E\u0303(t)+ \u2225\u2225\u2225 s \u2264 ( 64C2 147n + 256C1\u03c1 147n(\u03b1\u2212 \u03c1) + 256C21 21n2(\u03b1\u2212 \u03c1) )\u2225\u2225\u2225E(t)+ \u2225\u2225\u2225 s\n(26)\n+ ( 64C2 21n + 256C1\u03c1 21n(\u03b1\u2212 \u03c1) + 256C21 147n2(\u03b1 \u2212 \u03c1) )\u2225\u2225\u2225E(t)\u2212 \u2225\u2225\u2225 s + 24C21\u03c1 n(\u03b1\u2212 \u03c1) . (27)\nNow consider the negative part. The same argument as above leads to \u2225\u2225\u2225E\u0303(t)\u2212 \u2225\u2225\u2225 s \u2264 ( 64C21 147n2 + 256C1\u03c1 147n(\u03b1\u2212 \u03c1) + 256C21 21n2(\u03b1\u2212 \u03c1) )\u2225\u2225\u2225E(t)+ \u2225\u2225\u2225 s\n+ ( 64C21 21n2 + 256C1\u03c1 21n(\u03b1\u2212 \u03c1) + 256C21 147n2(\u03b1 \u2212 \u03c1) )\u2225\u2225\u2225E(t)\u2212 \u2225\u2225\u2225 s + 24C21\u03c1 n(\u03b1\u2212 \u03c1) . (28)\nNote the difference between (27) and (28): C2n in the former is replaced by C21 n2 in the latter, which is much smaller. This is crucial for our proof, which will be clear below.\nFor simplicity, we introduce the following notations:\nat := \u2225\u2225\u2225E(t)+ \u2225\u2225\u2225 s , bt := \u2225\u2225\u2225E(t)\u2212 \u2225\u2225\u2225 s .\nThen by the update rule, we have\nat+1 \u2264 (1\u2212 \u03b7)at + \u03b7r \u2225\u2225\u2225E\u0303(t)+ \u2225\u2225\u2225 s , bt+1 \u2264 (1\u2212 \u03b7)bt + \u03b7r \u2225\u2225\u2225E\u0303(t)\u2212 \u2225\u2225\u2225 s .\nPlugging in (27)and since r = nc2 \u2264 2n C2 , we have\nat+1 \u2264 (1\u2212 \u03b7)at + \u03b7 2n\nC2\n( 64C2 147n + 256C1\u03c1 147n(\u03b1\u2212 \u03c1) + 256C21 21n2(\u03b1\u2212 \u03c1) ) at\n+ \u03b7 2n\nC2\n( 64C2 21n + 256C1\u03c1 21n(\u03b1\u2212 \u03c1) + 256C21 147n2(\u03b1\u2212 \u03c1) ) bt + \u03b7 2n\nC2\n24C21\u03c1\nn(\u03b1\u2212 \u03c1)\nbt+1 \u2264 (1\u2212 \u03b7)bt + \u03b7 2n\nC2\n( 64C21 147n2 + 256C1\u03c1 147n(\u03b1\u2212 \u03c1) + 256C21 21n2(\u03b1\u2212 \u03c1) ) at\n+ \u03b7 2n\nC2\n( 64C21 21n2 + 256C1\u03c1 21n(\u03b1\u2212 \u03c1) + 256C21 147n2(\u03b1 \u2212 \u03c1) ) bt + \u03b7 2n\nC2\n24C21\u03c1\nn(\u03b1\u2212 \u03c1) .\nWhen 512C1\u03c1C2(\u03b1\u2212\u03c1) \u2264 1 2 and 512C21 C2n(\u03b1\u2212\u03c1) \u2264 1 14 ,\nat+1 \u2264 (1\u2212 \u03b7)at + 129\n147 \u03b7at +\n129\n21 \u03b7bt + \u03b7\n48C21\u03c1\nC2(\u03b1 \u2212 \u03c1)\n\u2264 ( 1\u2212 18\n147 \u03b7\n) at + 129\n21 \u03b7bt + \u03b7\n48C21\u03c1\nC2(\u03b1\u2212 \u03c1)\nSimilarly, when 512C1\u03c1C2(\u03b1\u2212\u03c1) \u2264 1 2 and 512C21 C2n(\u03b1\u2212\u03c1) \u2264 1 14 , and furthermore, 128C21 C2n \u2264 14 ,\nbt+1 \u2264 (1\u2212 \u03b7)bt + 1\n100 \u03b7at +\n1\n25 \u03b7bt + \u03b7\n48C21\u03c1\nC2(\u03b1 \u2212 \u03c1)\n\u2264 ( 1\u2212 24\n25 \u03b7\n) bt + 1\n100 \u03b7at + \u03b7\n48C21\u03c1\nC2(\u03b1 \u2212 \u03c1)\nLet h = 48C 2 1\u03c1\nC2(\u03b1\u2212\u03c1) , we then have:\nat+1 \u2264 ( 1\u2212 3\n25 \u03b7\n) at + 7\u03b7bt + \u03b7h,\nbt+1 \u2264 ( 1\u2212 24\n25 \u03b7\n) bt + 1\n100 \u03b7at + \u03b7h.\nNow set \u03b2 = \u221a 842+2800\u221284\n2 , so that\nat+1 + \u03b2bt+1 \u2264 ( 1\u2212 3\n25 \u03b7\n) at + 7\u03b7bt + \u03b7h+ ( \u03b2 \u2212 24\n25 \u03b7\u03b2\n) bt + \u03b2\n100 \u03b7at + \u03b7\u03b2h\n= ( 1\u2212 3\n25 \u03b7 +\n\u03b2\n100 \u03b7\n) (at + \u03b2bt) + \u03b7(1 + \u03b2)h\n\u2264 ( 1\u2212 1\n25 \u03b7\n) (at + \u03b2bt) + 9\u03b7h,\nwhere the last inequality follows from that \u03b2 < 8.\nNote that the recurrence is true up to t+ 1. Using Lemma 29 to solve this recurrence, we obtain\nat + bt \u2264 a0 + b0 + 250h \u2264 1 10 + 250h \u2264 1 8\nwhen 4000C 2 1\u03c1\nC2(\u03b1\u2212\u03c1) \u2264 1 100 . Moreover, we know that\n\u2225\u2225\u2225E(t+1) \u2225\u2225\u2225 s \u2264 at+1 + \u03b2bt+1 \u2264 ( 1\u2212 1 25 \u03b7 )t + 250h.\n(3) Finally, consider the noise term. Set the sample size N to be large enough, so that by Lemma 14, we have\n\u2223\u2223\u2223N\u0303(t)i,j \u2223\u2223\u2223 \u2264 4C\u03bdC1 (1\u2212 2\u00d7 \u2113)2n(\u03b1\u2212 \u03c1) + \u2223\u2223\u2223[N(t)s ]i,j \u2223\u2223\u2223\n\u2264 8C\u03bdC1 n(\u03b1\u2212 \u03c1) .\nThen by the update rule, we have \u2223\u2223\u2223N(t+1)i,j \u2223\u2223\u2223 \u2264 8C\u03bdC1(\u03b1\u2212\u03c1)c2 . Then \u2225\u2225\u2225N(t+1)\n\u2225\u2225\u2225 \u221e \u2264 nmax i,j \u2223\u2223\u2223N(t+1)i,j \u2223\u2223\u2223 \u2264 8nC\u03bdC1 (\u03b1\u2212 \u03c1)c2 \u2264 1\n8\u2016(A\u2217)\u2020\u2016\u221e where the last inequality is due to\nC\u03bd \u2264 (\u03b1 \u2212 \u03c1)c2\n64nC1\u2016(A\u2217)\u2020\u2016\u221e .\nOn the other hand, by Lemma 16, we have\n\u2016\u03be(t+1)\u2016\u221e \u2264 3\u2016(A\u2217)\u2020\u2016\u221e(\u2016N(t+1)\u2016\u221e + C\u03bd)\n\u2264 3\u2016(A\u2217)\u2020\u2016\u221e (\n8nC\u03bdC1 (\u03b1\u2212 \u03c1)c2 + C\u03bd\n) \u2264 \u03c1\nwhere the last inequality is due to\nC\u03bd \u2264 (\u03b1\u2212 \u03c1)c2\u03c1\n48nC1\u2016(A\u2217)\u2020\u2016\u221e , and C\u03bd \u2264\n\u03c1\n6\u2016(A\u2217)\u2020\u2016\u221e .\nWe also have (which will be useful in proving the final bound) \u2225\u2225\u2225N(t+1)\n\u2225\u2225\u2225 1 \u2264 mmax i,j \u2223\u2223\u2223N(t+1)i,j \u2223\u2223\u2223 \u2264 8mC\u03bdC1 (\u03b1\u2212 \u03c1)c2 \u2264 c 10\nwhere the last inequality is due to\nC\u03bd \u2264 c(\u03b1\u2212 \u03c1)c2 80mC1 .\nNow, we shall prove the theorem statements. Recall that solving the recurrence about at and bt leads to \u2225\u2225\u2225E(t+1)\n\u2225\u2225\u2225 s \u2264 at+1 + \u03b2bt+1 \u2264 ( 1\u2212 1 25 \u03b7 )t + 250h.\nSince the setting of \u03c1 makes sure h = O(c), when t = O ( ln 1\u01eb ) , we have the second statement\u2225\u2225\u2225E\u0302\n\u2225\u2225\u2225 s \u2264 \u01eb+ c2 . Note that\nA \u2217 \u03a3\u0302 = A\u2212A\u2217E\u0302\u2212 N\u0302\nand \u2225\u2225\u2225 [ A \u2217 \u03a3\u0302 ] i \u2225\u2225\u2225 = \u03a3\u0302i,i, \u2016A\u20161 = 1, \u2225\u2225\u2225A\u2217E\u0302 \u2225\u2225\u2225 1 = \u2225\u2225\u2225E\u0302 \u2225\u2225\u2225 1 , so we have\n\u03a3\u0302i,i \u2265 \u2016A\u20161 \u2212 \u2225\u2225\u2225E\u0302 \u2225\u2225\u2225 1 \u2212 \u2225\u2225\u2225N\u0302 \u2225\u2225\u2225 1\n\u2265 1\u2212 \u01eb\u2212 c. Similarly,\n\u03a3\u0302i,i \u2264 \u2016A\u20161 + \u2225\u2225\u2225E\u0302 \u2225\u2225\u2225 1 + \u2225\u2225\u2225N\u0302 \u2225\u2225\u2225 1\n\u2264 1 + \u01eb+ c. Then the final statement of the theorem follows by replacing c with c/4. This completes the proof.\nTheorem 7 (Unbiased noise). If Assumption (A0)-(A3) and (N2) are satisfied with C\u03bd = c2G \u221a cn C1 max{m,n\u2016(A\u2217)\u2020\u2016 \u221e } and the other parameters set as in Theorem 6, then the same guarantee holds.\nProof. The proof is similar to that of Theorem 6, except using the second bound for unbiased noise in Lemma 14. We highlight the different part, that is, the induction on the noise term.\nIn the induction, by Lemma 14 we have when N is large enough, \u2223\u2223\u2223N\u0303(t)i,j \u2223\u2223\u2223 \u2264 2C1C\u03bd\u03c1 \u2032(1 + \u2016A\u2020N(t)\u2016\u221e) (1\u2212 2\u2113)n(\u03b1\u2212 \u03c1\u2032) + \u2223\u2223\u2223\u2223 [ N (t) s ] i,j \u2223\u2223\u2223\u2223 \u2264 3C1C\u03bd\u03c1 \u2032(1 + \u2016A\u2020N(t)\u2016\u221e) n(\u03b1\u2212 \u03c1\u2032) .\nBy Lemma 16 and the induction, we have \u2016A\u2020N(t)\u2016\u221e \u2264 1/4. Furthermore, \u03c1\u2032 \u2264 C\u03bd\u2016A\u2020\u2016\u221e \u2264 2C\u03bd\u2016(A\u2217)\u2020\u2016\u221e and the parameter setting makes sure \u03c1\u2032 \u2264 \u03b1/2. Then\n\u2223\u2223\u2223N\u0303(t)i,j \u2223\u2223\u2223 \u2264\n16C2\u03bdC1 \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e\nn\u03b1 .\nThen by the update rule, we have \u2223\u2223\u2223N(t+1)i,j \u2223\u2223\u2223 \u2264 32C2\u03bdC1 \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e\nc2\u03b1\nand \u2225\u2225\u2225N(t+1)\n\u2225\u2225\u2225 \u221e \u2264 32nC2\u03bdC1\n\u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e\nc2\u03b1 \u2264 1 8\u2016(A\u2217)\u2020\u2016\u221e (29)\nby the definition of \u03b1, and C\u03bd \u2264 1256 c2C1 \u221a n n\u2016(A\u2217)\u2020\u2016\u221e . This completes the induction for the noise.\nAlso, in proving the final bounds, we have \u2225\u2225\u2225N(t+1)\n\u2225\u2225\u2225 1 \u2264\n32mC2\u03bdC1 \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e\nc2\u03b1 \u2264 c 10 (30)\nby the definition of \u03b1, and\nC\u03bd \u2264 c\n320 c2 C1\n\u221a n\nmax {m,n\u2016(A\u2217)\u2020\u2016\u221e} \u2264\n\u221a c\n320 c2 C1 1\u221a m\u2016(A\u2217)\u2020\u2016\u221e\nwhere the last inequality can be shown by consider the two cases when \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e \u2264 m/n and\u2225\u2225(A\u2217)\u2020\n\u2225\u2225 \u221e \u2265 m/n. The rest of the proof is the same as in Theorem 6."}, {"heading": "C Results for general proportions: Equilibration", "text": "Algorithm 2 ColumnUpdate Input: A matrix A, a threshold value \u03b1, a step size \u03b7, ratios {rj : j \u2208 [n]}, iteration number T , a\nsubset S \u2286 [n], sample size N 1: Set A(0) = A 2: for t = 0 \u2192 T \u2212 1 do 3:\n\u2200i \u2208 S, [A(t+1)]i = [ (1\u2212 \u03b7)A(t) + ri\u03b7E\u0303 [ (y \u2212 y\u2032)(x\u2212 x\u2032)\u22a4 ]] i\n(31)\nOutput: A\u0302 = A(T )\nAlgorithm 3 Rescale Input: A matrix A, a threshold value \u03b1, a step size \u03b7, ratios {rj : j \u2208 [n]}, iteration number T , and\na set S \u2286 [n], \u01eb \u2208 (0, 1). 1: Let A\u0303 = ColumnUpdate(A, \u03b1, \u03b7, {rj}j, T, S,N) 2: for i \u2208 S do 3: Set [A\u0302]i = 11\u2212\u01eb [A\u0303]i"}, {"heading": "Output: A\u0302", "text": "Algorithm 4 Equilibration Input: A, \u03b1, \u03b7, T , and \u01eb \u2208 (0, 1), \u03bb,N\n1: S \u2190 \u2205, D \u2190 I 2: while |S| \u2264 n do 3: mj \u2190 E\u0302[x2j ] for j 6\u2208 S using N examples 4: while maxj 6\u2208S mj < \u03bb do 5: A \u2190 Rescale(A, \u03b1, \u03b7, {3/(5mj) : j \u2208 [n]}, T, S, \u01eb,N) 6: \u03bb \u2190 (1\u2212 \u01eb)\u03bb, Dj,j \u2190 Dj,j/(1\u2212 \u01eb) 7: mj \u2190 (1\u2212 \u01eb)2mj for j \u2208 S, and mj \u2190 E\u0302[x2j ] for j 6\u2208 S using N examples 8: S \u2190 S \u222a {j : mj \u2265 \u03bb}\nOutput: A\nWhen the feature have various proportions (i.e., E[(x\u2217i ) 2] varies for different i), we propose Algorithm 4 for balancing them. The idea is quite simple: instead of solving Y \u2248 A\u2217X, we could also solve Y \u2248 [A\u2217D][(D)\u22121X] for a positive diagonal matrix D. Our goal is to find A = A\u2217D(\u03a3+ E) +N so that \u03a3 is large, E,N are small, while E[(x\u2217i )\n2]/D2i,i is with in a factor of 2 from each other.\nThe algorithm works at stages and keeps a working set S of column index i such that E[(x\u2217i ) 2]/D2i,i is above a threshold \u03bb. At each stage, it only updates the columns in S; at the end of the stage, it increases these columns by a small factor so that E[(x\u2217i )\n2]/D2i,i decreases. Then it decreases the threshold \u03bb, and add more columns to the working set and repeat. In this way, E[(x\u2217i )\n2]/D2i,i(i \u2208 S) are always balanced; in particular, they are balanced at the end when S = [n]. Formally,\nTheorem 18 (Main: Equilibration). If there exists an absolute constant G such that Assumption (A1)-(A3) and (N1) are satisfied with l = 1/50, C31 \u2264 Gc22n, max { C\u03bd , \u2016N(0)\u2016\u221e } \u2264 Gc 4 2\nC51n\u2016(A\u2217)\u2020\u2016\u221e ,\nand additionally \u03a3(0) (1 \u2212 \u2113)I, and E \u2265 0 entry-wise, then there exist \u03b1, \u03b7, T, \u03bb such that for sufficiently small \u01eb > 0 and sufficiently large N = poly(n,m, 1/\u01eb, 1/\u03b4) the following hold with probability at least 1\u2212\u03b4: Algorithm 4 outputs a solutionA = A\u2217D(\u03a3+E)+Nwhere \u03a3 (1\u2212\u2113)I is diagonal, \u2016E\u2016s \u2264 \u03b3\u2113 is off-diagonal, \u2016N\u2016\u221e \u2264 2\u2016N(0)\u2016\u221e, and D is diagonal and satisfies\nmaxi\u2208[n] 1\nD2 i,i\nE[(x\u2217i ) 2]\nminj\u2208[n] 1\nD2 j,j\nE[(x\u2217j ) 2]\n\u2264 2.\nIf Assumption (A1)-(A3) and (N2) are satisfied with the same parameters except max { C\u03bd , \u2016N(0)\u2016\u221e } \u2264 min {\u221a Gc42 C51n 1 \u2016(A\u2217)\u2020\u2016\u221e , Gc22 C31\u2016(A\u2217)\u2020\u2016\u221e } , then the same guarantees hold.\nNow, we can view A\u2217D as the ground-truth feature matrix and D\u22121x\u2217 as the weights. Then applying Algorithm 1 with A can recover A\u2217D, and after normalization we get A\u2217.\nThe initialization condition of the theorem can be achieved by the popular practical heuristic that sets the columns of A(0) to reasonable almost pure data points. It is generally believed that it gives E\n(0) i,j \u2265 0 andN(0) = 0. We note that the parameters are not optimized; the algorithm can potentially\ntolerate much better initialization.\nIntuition. Before delving into the specifics of the algorithm, it will be useful to provide a highlevel outline of the proof. As described above, the algorithm makes use of the fact that samples from a ground truth matrix A\u2217 and distribution x\u2217 can equivalently be viewed as coming from the ground truth matrix A\u2217D and distribution D\u22121x\u2217, for some diagonal matrix D. Therefore, the goal is to find a D such that the features are balanced:\nmaxi\u2208[n] E[(x\u2217i ) 2]\nD2 i,i\nmini\u2208[n] E[(x\u2217 i )2]\nD2 i,i\n\u2264 \u03ba.\nThe algorithm will implicitly calculate such a D gradually. Namely, at any point in time, the algorithm will have an active set S \u2286 [n] of features, which are balanced, i.e.\nmaxi\u2208[n] E[(x\u2217i ) 2]\nD2 i,i\nmini\u2208S E[(x\u2217 i )2]\nD2 i,i\n\u2264 \u03ba. (32)\nIt is clear that when S = [n] the algorithm achieves the goal. Our algorithm begins with S = \u2205 and gradually increase S until S = [n].\nThe mechanism for increasing S will be as follows. Given S, A is of the form\nA = A\u2217D(\u03a3+E) +N\nwith\nE =\n[ E1,1 E1,2\nE2,1 E2,2\n]\nwhere the columns of A are sorted such that the first |S| columns correspond to the features of S, and E1,1 \u2208 R|S|\u00d7|S|, E2,1 \u2208 R(n\u2212|S|)\u00d7|S|, E1,2 \u2208 R|S|\u00d7(n\u2212|S|), E2,2 \u2208 R(n\u2212|S|)\u00d7(n\u2212|S|). Then scaling up the columns of A indexed by S by a factor of 11\u2212\u01eb is equivalent to\n(1) scaling up the columns of D indexed by S by a factor of 11\u2212\u01eb and\n(2) scaling up the columns of E2,1 by a factor of 11\u2212\u01eb and (3) scaling down the columns of E1,2 by a factor of 1\u2212 \u01eb.\nTherefore, to increase the set S, the algorithm will scale up the columns of A indexed by S, until some j 6\u2208 S satisfies\nmax i\u2208[n]\nE[(x\u2217i ) 2]\nD2i,i\n\u2264 \u03ba E[(x\u2217j ) 2]\nD2j,j\n.\nThen it can add j into S while keeping the corresponding features balanced as in (32). Note that we do not need to explicitly maintain D, though it can be calculated along with the scaling. Further note that the values of E[(x\u2217i ) 2] are not known but they can be estimated using the current A.\nHowever, there is still one caveat: E should be kept small, so that at the end of the algorithm, we still have a good initialization A. For this reason, the algorithm additionally maintains that for a small constant 1 < \u03b3 < 2,\n\u2016E1,1\u2016s \u2264 \u03b3\u2113, \u2016E1,2\u2016s \u2264 \u2113, \u2016E2,1\u2016s \u2264 \u03b3\u2113, \u2016E2,2\u2016s \u2264 \u2113. (33)\nSince scaling up A will scale up E2,1, we will need to first decrease \u2016E2,1\u2016s before the scaling step. The key observation is that by applying our training algorithm only on the columns indexed by S, \u2016E1,1\u2016s and \u2016E2,1\u2016s will be decreased, while \u2016E1,2\u2016s and \u2016E2,2\u2016s unchanged. On a high level, using the fact that the matrix E1,2 has no negative entries (which we get by virtue of our initialization), and the fact that the contribution in the updates to the entry (E1,1)i,j mostly comes from (E1,1)j,i (i.e. the matrix E1,1 in the first order contribution \u201cupdates itself\u201d), and the fact that the features in S are balanced, we can show that after sufficiently many updates, the symmetric norm of E1,1 and E2,1 drops by a reasonable amount: \u2016E1,1\u2016s \u2264 (\u03b3\u2212 1)\u2113 and \u2016E2,1\u2016s \u2264 (1\u2212 \u01eb)(\u03b3\u2212 1)\u2113. Now, we can do the scaling step without hurting the invariant 33.\nOrganization. The result of the section is as follows. We first prove in Section C.1 that applying our training algorithm only on the columns indexed by S will decrease \u2016E1,1\u2016s and \u2016E2,1\u2016s. Then in Section C.2 we analyze the scaling step, and show that the invariant (33) is maintained. In Section C.3, we show how to increase S while maintaining the invariant (32), where the main technical details are about how to estimate E[(x\u2217i ) 2]."}, {"heading": "C.1 Equilibration: ColumnUpdate", "text": "In this subsection, we focus on the update step, bounding the changes of \u03a3,E, and N.\nFirst recall some notations. Let A = A\u2217(\u03a3+E)+N where \u03a3 is diagonal, E is off diagonal, and N is the component outside the span of A\u2217.3 Given the set S \u2286 [n] and a matrix M \u2208 Rn\u00d7n, let M1,1 denote the submatrix indexed by S \u00d7 S, and M2,1 denote the submatrix indexed by ([n] \u2212 S)\u00d7 S, M1,2 denote the submatrix indexed by S \u00d7 ([n] \u2212 S), and M2,2 denote the submatrix indexed by ([n]\u2212 S)\u00d7 ([n]\u2212 S). 4 In the special case when S = [s] where s = |S|,\nM =\n[ M1,1 M1,2\nM2,1 M2,2\n] .\nAlso, let MS denote the submatrix formed by the columns indexed by S, and M\u2212S the submatrix formed by the other columns. 5\nThe input A(0) of Algorithm 2 can be written as A(0) = A\u2217(\u03a3(0) + E(0)) + N(0) where \u03a3(0) is diagonal, and E(0) is off diagonal. Define E(0)1,1,E (0) 1,2,E (0) 2,1 and E\u03022,2 as described above. Similarly, define E\u03021,1, E\u03021,2, E\u03022,1 and E\u03022,2 for the output A\u0302 = A\u2217(\u03a3\u0302+E\u0302)+N\u0302 of Algorithm 2. Finally, define N (0) S , N (0) \u2212S , N\u0302S , and N\u0302\u2212S as described above.\nThe main result of the subsection is Lemma 19.\n3Note that A\u2217 here can be any ground-truth matrix; in particular, later Lemma 19 will be applied where A\u2217\nin the lemma corresponds to A\u2217D in the intuition described above. 4These notations will be used for M = E, M = E\u0303, and related matrices. 5These notations will be used for M = N or M = N\u0303, and related matrices.\nLemma 19 (Main: ColumnUpdate). Define\nRj = E[(x \u2217 j ) 2], R = max j\u2208[n] Rj , r = max j\u2208S rj , (34)\nh1 = r 8C1(C1 + 1)\u03c1 (1 \u2212 \u2113\u2212 \u03b2\u2113)n(\u03b1\u2212 \u03c1) + 4C21 (1\u2212 \u2113\u2212 \u03b2\u2113)n2(\u03b1\u2212 \u03c1)r (\n1 (1 \u2212 \u2113\u2212 \u03b2\u2113) + 1 ) , (35)\nh2 = r R\u03b22\u21132 (1 \u2212 \u2113)2(1\u2212 \u2113\u2212 \u03b2\u2113) + 12C1(C1 + 1) n2(\u03b1\u2212 \u03c1)(1 \u2212 \u2113\u2212 \u03b2\u2113)\n( 1 1\u2212 \u2113\u2212 \u03b2\u2113 + n\u03c1 ) r, (36)\nh = h1 + h2, (37)\nUa = 8rC\u03bdC1 \u03b1\u2212 \u03c1 , (38)\nUn = 10rC1C\n2 \u03bd \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e\n(1\u2212 2\u2113)(\u03b1\u2212 2C\u03bd\u2016(A\u2217)\u2020\u2016\u221e) . (39)\nSuppose \u2113 \u2264 1/8, \u03b2 is a constant with \u03b2\u2113 \u2264 1/2, \u03b3 \u2208 (1, 2), \u01eb \u2208 (0, 1). The initialization satisfies (1 \u2212 \u2113)I \u03a3(0), \u2225\u2225\u2225E(0)1,1 \u2225\u2225\u2225 s \u2264 \u03b3\u2113, \u2225\u2225\u2225E(0)2,1 \u2225\u2225\u2225 s \u2264 \u03b3\u2113, \u2225\u2225\u2225(E(0)1,2;E (0) 2,2) \u2225\u2225\u2225 s \u2264 \u2113, E(0)1,2 \u2265 0 and E (0) 2,2 \u2265 0 entrywise, and \u2016N(0)\u2212S\u2016\u221e \u2264 U and \u2016N (0) S \u2016\u221e \u2264 2U \u2264 1/(16\u2016(A\u2217)\u2020\u2016\u221e). Furthermore, the parameters satisfy that for any i \u2208 S,\n\u03b7 ( 1 + 2riRi\n1 (1\u2212 \u2113)2 \u03b2\u21132 1\u2212 \u03b2\u2113\u2212 \u2113 + ri 2C1 n\n( \u03b1+ 2\u03c1+\nC1 n + 2C1 n \u03b2\u2113(1\u2212 \u03b2\u2113) (1\u2212 \u2113)2(1\u2212 \u03b2\u2113 \u2212 \u2113) )) \u2264 \u2113 (40)\nriRi\n( 2\u2212 2 1\n(1\u2212 \u2113)2 \u03b2\u21132 1\u2212 \u03b2\u2113 \u2212 \u2113\n) \u2212 ri ( 2C1 n ( \u03b1+ 2\u03c1+ C1 n 1 1\u2212 \u2113 + 2C1 n \u03b2\u2113(1\u2212 \u03b2\u2113) (1 \u2212 \u2113)2(1\u2212 \u03b2\u2113\u2212 \u2113) )) \u2265 1\u2212 \u2113\n(41)\nh1 \u2264 \u2113, ( rR (1 \u2212 \u2113)2 + 1 ) (\u01eb+ h1) + (\u01eb+ h2) \u2264 (\u03b3 \u2212 1)\u2113 (42)\n\u01eb+ h2 \u2264 (1\u2212 \u01eb)(\u03b3 \u2212 1)\u2113 (43)\nh1 + \u2113 \u2264 (\u03b2 \u2212 1)\u2113, h2 + ( rR (1 \u2212 \u2113)2 + 1 ) \u2113 \u2264 (\u03b2 \u2212 1)\u2113 (44)\n3\u2016(A\u2217)\u2020\u2016\u221e (3U + C\u03bd) \u2264 \u03c1 < \u03b1. (45) If we have adversarial noise (Assumption (N1)), assume\n\u01eb\u2032 + Ua \u2264 (1 \u2212 \u01eb)U, and 3\u2016(A\u2217)\u2020\u2016\u221e (2U + Ua + C\u03bd) \u2264 \u03c1 < \u03b1 < 1. (46) If we have unbiased noise (Assumption (N2)), assume\n\u01eb\u2032 + Un \u2264 (1\u2212 \u01eb)U. (47) Finally, let N = poly (n,m, 1/\u03b4, 1/\u01eb) sufficiently large.\nThen with probability at least 1 \u2212 \u03b4, after 2 ln(\u01eb/(\u03b3\u2113))ln(1\u2212\u03b7) + ln(\u01eb\u2032/U) ln(1\u2212\u03b7) iterations, the output of Algorithm"}, {"heading": "2 is A\u0302 = A\u2217(\u03a3\u0302+ E\u0302) + N\u0302 satisfying", "text": "(1\u2212 \u2113)I \u03a3\u0302 uI, \u2016E\u03021,1\u2016s \u2264 (\u03b3 \u2212 1)\u2113, \u2016E\u03022,1\u2016s \u2264 (1\u2212 \u01eb)(\u03b3 \u2212 1)\u2113, \u2016(E\u03021,2; E\u03022,2)\u2016s \u2264 \u2113, and E\u03021,2 \u2265 0 and E\u03022,2 \u2265 0 entry-wise. Furthermore, \u2016N\u0302\u2212S\u2016\u221e \u2264 U and \u2016N\u0302S\u2016\u221e \u2264 (1\u2212 \u01eb)U.\nProof of Lemma 19. It follows from Lemma 22 and the conditions (42) and (43).\nTo prove Lemma 22, we will first consider how E changes after one update step, and then derive the recurrence for all steps in Lemma 22."}, {"heading": "C.1.1 One update step of E", "text": "In this subsection, we focus on one update step, bounding the change of E. So through out this subsection we will focus on a particular iteration t and omit the superscript (t), while in the next subsection we will put back the superscript.\nFor analysis, denote A(t) as\nA = A\u2217(\u03a3+E) +N\nwhere \u03a3 is a diagonal matrix, E is an off-diagonal matrix, and N is the component of A that lies outside the span of A\u2217 (e.g., the noise caused by the noise in the sample).\nRecall the following notations:\nZ = (\u03a3+E)\u22121 ,\nV = Z\u2212\u03a3\u22121 = \u03a3\u22121 \u221e\u2211\nk=1\n(\u2212E\u03a3\u22121)k,\n\u03be = \u2212A\u2020NZx\u2217 +A\u2020\u03bd.\nConsider the update term E\u0302 [ (y \u2212 y\u2032)(x\u2212 x\u2032)\u22a4 ] and denote it as\n\u2206 = E\u0302 [ (y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4 ] = A\u2217(\u03a3\u0303+ E\u0303) + N\u0303\nwhere \u03a3\u0303 is a diagonal matrix, E\u0303 is an off-diagonal matrix, and N is the component of \u2206 that lies outside the span of A\u2217.\nSince we now use empirical average, we will have sampling noise. Denote it as\nNs = E\u0302[(y \u2212 y\u2032)(x\u2212 x\u2032)\u22a4]\u2212 E[(y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4].\nThen by definition, for y = A\u2217x\u2217 + \u03bd and y\u2032 = A\u2217(x\u2032)\u2217 + \u03bd\u2032, we have\nE\u0302[(y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4] = E[(y \u2212 y\u2032)(x \u2212 x\u2032)\u22a4] +Ns = A\u2217 E [ (x\u2217 \u2212 (x\u2032)\u2217)(x \u2212 x\u2032)\u22a4\n] \ufe38 \ufe37\ufe37 \ufe38\n\u03a3\u0303+E\u0303\n+E [ (\u03bd \u2212 \u03bd\u2032)(x\u2212 x\u2032)\u22a4 ] +Ns\ufe38 \ufe37\ufe37 \ufe38\nN\u0303\n.\nRecall the definition of E1,1, i.e., it is the submatrix of E indexed by S \u00d7 S. Define E\u03031,1 similarly, i.e., it is the submatrix of E\u0303 indexed by S \u00d7 S. Define E\u03031,2, E\u03032,1 and E\u03032,2 accordingly. So in the special case when S = [s] where s = |S|,\nE\u0303 =\n[ E\u03031,1 E\u03031,2\nE\u03032,1 E\u03032,2\n] .\nWe also use the notation M+ or M\u2212 to denote the positive or negative part of a matrix M.\nLemma 20 (Update E\u03031,1). Let E\u03031,1 be defined as above. If \u2016\u03be\u2016\u221e \u2264 \u03c1 < \u03b1 < 1 and \u03a3 (1\u2212 \u2113)I, then (1). Negative entries:\n\u2016E\u0303\u22121,1\u2016s \u2264 4C21\u2016Z\u2016s(\u2016Z\u2016s + 1) n2(\u03b1\u2212 \u03c1) + 8C1(C1 + 1)\u03c1\u2016Z\u2016s n(\u03b1\u2212 \u03c1) .\n(2) Positive entries:\n\u2016E\u0303+1,1\u2016s \u2264 12C1(C1 + 1)\u2016Z\u2016s\nn2(\u03b1\u2212 \u03c1) (\u2016Z\u2016s + n\u03c1) + 2maxj\u2208[n]{E[(x \u2217 j )\n2]} (\n1\n(1\u2212 \u2113)2 \u2016E \u2212 1,1\u2016s + \u2016E\u20162s (1\u2212 \u2113)2(1\u2212 \u2113\u2212 \u2016E\u2016s)\n) .\nProof of Lemma 20. (1) By Lemma 13, we have\n\u2016E\u0303\u22121,1\u2016s \u2264 max { 4C21\u2016Z\u2016s n2(\u03b1\u2212 \u03c1) \u2016Z\u2016s + 4C21\u2016Z\u2016s n2(\u03b1\u2212 \u03c1)n\u03c1, 8C1\u03c1 n(\u03b1\u2212 \u03c1) (C1 + 1)\u2016Z\u2016s + 2C21 n2 \u2016Z\u2016s } .\nObserve that for \u03b1 < 1,\n4C21\u2016Z\u2016s(\u2016Z\u2016s + 1) n2(\u03b1\u2212 \u03c1) \u2265 max { 4C21\u2016Z\u20162s n2(\u03b1\u2212 \u03c1) , 2C21 n2 \u2016Z\u2016s } .\nMoreover,\n8C1\u03c1 n(\u03b1\u2212 \u03c1) (C1 + 1)\u2016Z\u2016s \u2265 4C21\u2016Z\u2016s n2(\u03b1 \u2212 \u03c1)n\u03c1.\nTherefore,\n\u2016E\u0303\u22121,1\u2016s \u2264 4C21\u2016Z\u2016s n2(\u03b1\u2212 \u03c1) + 8C1(C1 + 1)\u03c1\u2016Z\u2016s n(\u03b1\u2212 \u03c1) .\n(2) By Lemma 13, when Zi,j < 0,\nE\u0303j,i \u2264 4C21\u2016Zi\u20161 n2(\u03b1\u2212 \u03c1) (|Zi,j |+ \u03c1) .\nWhen Zi,j \u2265 0,\nE\u0303j,i \u2264 8C1\u03c1\nn(\u03b1\u2212 \u03c1)\n( C1\u2016Zi\u20161\nn + Zi,j\n) + 2E[(x\u2217j ) 2]Zi,j\nConsider a fixed i. Let G = {j \u2208 S,Zi,j \u2265 0} and let Gc = S \u2212G. We know that\n\u2016[E\u0303+1,1]i\u20161 = \u2211\nj\u2208[n] [E\u0303+1,1]j,i\n\u2264 \u2211\nj\u2208Gc\n4C21\u2016Zi\u20161 n2(\u03b1\u2212 \u03c1) (|Zi,j |+ \u03c1)\n+ \u2211\nj\u2208G\n( 8C1\u03c1\nn(\u03b1\u2212 \u03c1)\n( C1\u2016Zi\u20161\nn + Zi,j\n) + 2E[(x\u2217j ) 2]Zi,j )\n\u2264 4C 2 1\u2016Z\u2016s n2(\u03b1\u2212 \u03c1) (\u2016Z\u2016s + n\u03c1) + 8C1(C1 + 1)\u03c1 n(\u03b1\u2212 \u03c1) \u2016Z\u2016s + \u2211\nj\u2208G 2E[(x\u2217j ) 2]Zi,j\n\u2264 4C 2 1\u2016Z\u20162s n2(\u03b1\u2212 \u03c1) + 4C21\u2016Z\u2016s n2(\u03b1 \u2212 \u03c1)n\u03c1+ 8C1(C1 + 1)\u03c1 n(\u03b1\u2212 \u03c1) \u2016Z\u2016s + \u2211\nj\u2208S 2E[(x\u2217j ) 2]Zi,j\n\u2264 12C1(C1 + 1)\u2016Z\u2016s n2(\u03b1\u2212 \u03c1) (\u2016Z\u2016s + n\u03c1) +\n\u2211 j\u2208G 2E[(x\u2217j ) 2]Zi,j .\nA similar bound holds for \u2016[E\u0303+1,1]i\u20161. By the definition of Z, we know that\nZ = (\u03a3+E)\u22121\n= \u03a3\u22121 \u221e\u2211\nk=0\n(\u2212E\u03a3\u22121)k\n= \u03a3\u22121 \u2212\u03a3\u22121E\u03a3\u22121 +\u03a3\u22121 \u221e\u2211\nk=2\n(\u2212E\u03a3\u22121)k.\nTherefore, we know that for i 6= j,\nZi,j \u2264 \u2212[\u03a3\u22121E\u03a3\u22121]i,j + | \u221e\u2211\nk=2\n\u03a3 \u22121[(\u2212E\u03a3\u22121)k]i,j |.\nThis implies that\n\u2211 j\u2208G Zi,j \u2264 \u2211 j\u2208G\n( \u2212[\u03a3\u22121E\u03a3\u22121]i,j + \u221e\u2211\nk=2\n\u2223\u2223\u03a3\u22121[(\u2212E\u03a3\u22121)k]i,j \u2223\u2223 )\n\u2264 1 (1\u2212 \u2113)2 \u2016E \u2212 1,1\u2016s + 1 1\u2212 \u2113\n\u2016E\u20162s (1\u2212\u2113)2\n1\u2212 \u2016E\u2016s1\u2212\u2113\n\u2264 1 (1\u2212 \u2113)2 \u2016E \u2212 1,1\u2016s + \u2016E\u20162s (1\u2212 \u2113)2(1\u2212 \u2113\u2212 \u2016E\u2016s) .\nPutting together, we complete the proof.\nLemma 21 (Update E\u03032,1). Let E\u03032,1 be defined as above, and suppose \u2016\u03be\u2016\u221e \u2264 \u03c1 < \u03b1 < 1, \u03a3 (1\u2212 \u2113)I and E1,2 \u2265 0, then we have\n\u2016E\u03032,1\u2016s \u2264 12C1(C1 + 1)\u2016Z\u2016s\nn2(\u03b1 \u2212 \u03c1) (\u2016Z\u2016s + n\u03c1) + 2maxj\u2208[n]{E[(x \u2217 j ) 2]} ( \u2016E\u20162s (1\u2212 \u2113)2(1 \u2212 \u2113\u2212 \u2016E\u2016s) ) .\nProof of Lemma 21. The proof is almost the same as that of Lemma 20, combined with the fact that E1,2 \u2265 0 entry-wise."}, {"heading": "C.1.2 Recurrence", "text": "Recall that A = A\u2217(\u03a3+E) +N\nand recall that E1,1 is the submatrix indexed by S \u00d7 S, and E1,2,E2,1,E2,2 are defined according. Recall that MS denote the submatrix of M formed by columns indexed by S, and let M\u2212S denote the submatrix formed by the other columns.\nLemma 22 (Recurrence). Suppose the conditions in Lemma 19 hold. Then with probability at least 1\u2212 \u03b4, after 2 ln(\u01eb/(\u03b3\u2113))ln(1\u2212\u03b7) iterations,\n(1\u2212 \u2113)I \u03a3(t), \u2016(E(t)1,1)\u2212\u2016s \u2264 \u01eb+ h1,\n\u2016(E(t)1,1)+\u2016s \u2264 rR\n(1\u2212 \u2113)2 (\u01eb + h1) + h2 + \u01eb,\n\u2016(E(t)2,1)\u2016s \u2264 \u01eb+ h2.\nAlso, after ln(\u01eb\u2032/U) ln(1\u2212\u03b7) iterations, for both adversarial and unbiased noise,\n\u2225\u2225\u2225N(t)\u2212S \u2225\u2225\u2225 \u221e \u2264 U, \u2225\u2225\u2225N(t)S \u2225\u2225\u2225 \u221e \u2264 (1 \u2212 \u01eb)U.\nProof of Lemma 22. We first prove the following claims by induction. (1) (1\u2212 \u2113)I \u03a3(t), (2)\n\u2016(E\u22121,1)(t)\u2016s \u2264 \u03b3\u2113\n\u2016(E+1,1)(t)\u2016s \u2264 rR\n(1\u2212 \u2113)2 \u03b3\u2113+ h2\n\u2016E(t)2,1\u2016s \u2264 \u03b3\u2113 \u2016E(t)1,2\u2016s \u2264 \u2113 \u2016E(t)2,2\u2016s \u2264 \u2113,\n(3) \u2016E(t)\u2016s \u2264 \u03b2\u2113, (4) for adversarial noise, \u2225\u2225\u2225N(t)S \u2225\u2225\u2225 \u221e \u2264 U +Ua, and \u2016\u03be(t)\u2016\u221e \u2264 \u03c1; or for unbiased noise, \u2225\u2225\u2225N(t)S \u2225\u2225\u2225 \u221e\n\u2264 U + Uu.\nThe basis case for t = 0 is trivial by assumptions. Now assume they are true for iteration t and show that they are true for iteration t+ 1.\n(1) By the update of \u03a3, we have\n\u03a3 (t+1) = (1\u2212 \u03b7)\u03a3(t) + \u03b7r\u03a3\u0303(t).\nTo lower bound \u03a3(t+1)i,i , we will consider two cases, \u03a3 (t) i,i \u2265 1 and \u03a3 (t) i,i \u2264 1.\nFor \u03a3(t)i,i \u2265 1, by Lemma 12,\n\u03a3\u0303i,i \u2265 E [ (x\u2217i ) 2 ] ( 2\u03a3\u22121i,i \u2212 2 |Vi,i| ) \u2212 2C1\nn\n( \u03b1+ 2\u03c1+\nC1 n \u03a3 \u22121 i,i + 2C1 n \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1\n)\n\u2265 \u22122Ri |Vi,i| \u2212 ( 2C1 n ( \u03b1+ 2\u03c1+ C1 n \u03a3 \u22121 i,i + 2C1 n \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1 )) .\nHence,\n\u03a3 (t+1) i,i \u2265 (1\u2212 \u03b7)\u03a3 (t) i,i \u2212 \u03b7 ( 2riRi \u2223\u2223\u2223V(t)i,i \u2223\u2223\u2223+ ri ( 2C1 n ( \u03b1+ 2\u03c1+ C1 n (\u03a3 (t) i,i ) \u22121 + 2C1 n \u2225\u2225\u2225\u2225 [ V (t) ]i\u2225\u2225\u2225\u2225\n1\n)))\n\u2265 1\u2212 \u03b7 ( 1 + 2riRi \u2223\u2223\u2223V(t)i,i \u2223\u2223\u2223+ ri\n2C1 n\n( \u03b1+ 2\u03c1+\nC1 n + 2C1 n \u2225\u2225\u2225\u2225 [ V (t) ]i\u2225\u2225\u2225\u2225\n1\n))\n\u2265 1\u2212 \u03b7 ( 1 + 2riRi\n1 (1\u2212 \u2113)2 \u03b2\u21132 1\u2212 \u03b2\u2113 \u2212 \u2113 + ri 2C1 n\n( \u03b1+ 2\u03c1+\nC1 n + 2C1 n \u03b2\u2113(1\u2212 \u03b2\u2113) (1\u2212 \u2113)2(1\u2212 \u03b2\u2113 \u2212 \u2113)\n)) .\nwhere we use the bound on V(t). By condition (40), the claim follows.\nFor \u03a3(t)i,i \u2264 1, again by Lemma 12,\n\u03a3\u0303 (t) i,i \u2265 E [ (x\u2217i ) 2 ] ( 2\u2212 2 \u2223\u2223\u2223V(t)i,i \u2223\u2223\u2223 ) \u2212 ( 2C1 n ( \u03b1+ 2\u03c1+ C1 n (\u03a3 (t) i,i ) \u22121 + 2C1 n \u2225\u2225\u2225\u2225 [ V (t) ]i\u2225\u2225\u2225\u2225\n1\n)) .\nHence,\n\u03a3 (t+1) i,i = (1\u2212 \u03b7)\u03a3 (t) i,i + \u03b7r\u03a3\u0303 (t) i,i\n\u2265 (1\u2212 \u03b7)(1 \u2212 \u2113)\n+ \u03b7 ( riRi ( 2\u2212 2 \u2223\u2223\u2223V(t)i,i \u2223\u2223\u2223 ) \u2212 ri ( 2C1 n ( \u03b1+ 2\u03c1+ C1 n (\u03a3 (t) i,i ) \u22121 + 2C1 n \u2225\u2225\u2225\u2225 [ V (t) ]i\u2225\u2225\u2225\u2225\n1\n)))\n\u2265 (1\u2212 \u03b7)(1 \u2212 \u2113) + \u03b7riRi ( 2\u2212 2 1\n(1\u2212 \u2113)2 \u03b2\u21132 1\u2212 \u03b2\u2113 \u2212 \u2113\n)\n\u2212 \u03b7ri ( 2C1 n ( \u03b1+ 2\u03c1+ C1 n 1 1\u2212 \u2113 + 2C1 n \u03b2\u2113(1\u2212 \u03b2\u2113) (1 \u2212 \u2113)2(1\u2212 \u03b2\u2113\u2212 \u2113) )) .\nBy condition (41), the claim follows.\n(2) By Lemma 20,\n\u2016(E\u0303(t+1)1,1 )\u2212\u2016s \u2264 8C1(C1 + 1)\u03c1\u2016Z(t)\u2016s n(\u03b1\u2212 \u03c1) + 4C21\u2016Z(t)\u2016s(\u2016Z(t)\u2016s + 1) n2(\u03b1\u2212 \u03c1) ,\n\u2016(E\u0303(t+1)1,1 )+\u2016s \u2264 R (1 \u2212 \u2113)2 \u2016(E \u2212 1,1) (t)\u2016s + R\u2016E(t)\u20162s (1\u2212 \u2113)2(1 \u2212 \u2113\u2212 \u2016E(t)\u2016s)\n+ 12C1(C1 + 1)\u2016Z(t)\u2016s n2(\u03b1\u2212 \u03c1) (\u2225\u2225\u2225Z(t) \u2225\u2225\u2225 s + n\u03c1 ) .\nBy the update rule, we have\n\u2016(E(t+1)1,1 )\u2212\u2016s \u2264 (1\u2212 \u03b7)\u2016(E (t) 1,1) \u2212\u2016s\n+ r\u03b7 8C1(C1 + 1)\u03c1 (1 \u2212 \u2113\u2212 \u03b2\u2113)n(\u03b1\u2212 \u03c1) + 4C21 (1\u2212 \u2113\u2212 \u03b2\u2113)n2(\u03b1\u2212 \u03c1)\n( 1 (1\u2212 \u2113\u2212 \u03b2\u2113) + 1 ) r\u03b7,\n\u2264 (1\u2212 \u03b7)\u2016(E(t)1,1)\u2212\u2016s + \u03b7h1 (48)\n\u2016(E(t+1)1,1 )+\u2016s \u2264 (1\u2212 \u03b7)\u2016(E (t) 1,1) +\u2016s + r\u03b7 R\n(1 \u2212 \u2113)2 \u2016(E (t) 1,1) \u2212\u2016s\n+ r\u03b7 R\u03b22\u21132\n(1\u2212 \u2113)2(1 \u2212 \u2113\u2212 \u03b2\u2113)\n+ 12C1(C1 + 1)\nn2(\u03b1\u2212 \u03c1)(1 \u2212 \u2113\u2212 \u03b2\u2113)\n( 1 1\u2212 \u2113\u2212 \u03b2\u2113 + n\u03c1 ) r\u03b7\n\u2264 (1\u2212 \u03b7)\u2016(E(t)1,1)+\u2016s + r\u03b7 R\n(1 \u2212 \u2113)2 \u2016(E (t) 1,1) \u2212\u2016s + \u03b7h2 (49)\nwhere we use \u2225\u2225E(t) \u2225\u2225 s \u2264 \u03b2\u2113 and \u2016Z(t)\u2016s \u2264 11\u2212\u2113\u2212\u03b2\u2113 .\nThe claim on \u2016(E(t+1)1,1 )\u2212\u2016s follows from (48) and the condition (42).\nFor \u2016(E(t+1)1,1 )+\u2016s, by induction (49) becomes\n\u2016(E(t+1)1,1 )+\u2016s \u2264 (1 \u2212 \u03b7)\u2016(E (t) 1,1) +\u2016s + r\u03b7 R (1 \u2212 \u2113)2 \u03b3\u2113+ \u03b7h2 \u2264 rR (1 \u2212 \u2113)2 \u03b3\u2113+ h2.\nNow we consider \u2016(E(t+1)2,1 )\u2016s. By Lemma 21,\n\u2016(E(t+1)2,1 )\u2016s \u2264 (1\u2212 \u03b7)\u2016(E (t) 2,1)\u2016s\n+ r\u03b7 R\u03b22\u21132\n(1\u2212 \u2113)2(1\u2212 \u2113\u2212 \u03b2\u2113)\n+ 12C1(C1 + 1)\nn2(\u03b1\u2212 \u03c1)(1\u2212 \u2113 \u2212 \u03b2\u2113)\n( 1 1\u2212 \u2113\u2212 \u03b2\u2113 + n\u03c1 ) r\u03b7\n= (1\u2212 \u03b7)\u2016(E(t)2,1)\u2016s + \u03b7h2 (50) \u2264 \u03b3\u2113\nwhere the last line follows by condition (43) and induction. Finally, clearly we have \u2225\u2225\u2225E(t+1)1,2 \u2225\u2225\u2225 s \u2264 \u2113 and \u2225\u2225\u2225E(t+1)2,2 \u2225\u2225\u2225 s \u2264 \u2113, since they are not updated.\n(3) Note that (48) (49) hold for all iterations up to t+ 1. Then by Lemma 28, we have\n\u2016(E(t+1)1,1 )\u2212\u2016s + \u2016(E (t+1) 1,1 ) +\u2016s\n\u2264 max { \u2016(E(0)1,1)\u2212\u2016s + \u2016(E (0) 1,1) +\u2016s, \u2016(E(0)1,1)+\u2016s + h1, h2 + ( rR (1\u2212 \u2113)2 + 1 ) \u2016(E(0)1,1)\u2212\u2016s, h2 + ( rR (1 \u2212 \u2113)2 + 1 ) h1 } .\nSince h1 \u2264 \u2113 and h2 \u2264 \u2113 by (42)(43), and \u2016(E(0)1,1)\u2212\u2016s + \u2016(E (0) 1,1) +\u2016s \u2264 \u2113 by assumption, we have\n\u2016(E(t+1)1,1 )\u2212\u2016s + \u2016(E (t+1) 1,1 ) +\u2016s \u2264 max { \u2113+ h1, h2 + ( rR (1 \u2212 \u2113)2 + 1 ) \u2113 } . (51)\nThen we have by condition (44),\n\u2016(E(t+1)1,1 )\u2212\u2016s + \u2016(E (t+1) 1,1 ) +\u2016s \u2264 (\u03b2 \u2212 1)\u2113, \u2016E(t+1)\u2016s \u2264 \u03b2\u2113.\n(4) Finally, we consider the noise. We first consider the adversarial noise. Set the sample size N to be large enough, so that by Lemma 14, we have\n\u2223\u2223\u2223N\u0303(t)i,j \u2223\u2223\u2223 \u2264 4C\u03bdC1 (1 \u2212 2\u2113)2n(\u03b1\u2212 \u03c1) + \u2223\u2223\u2223[N\u0303(t)s ]i,j \u2223\u2223\u2223 \u2264 8C\u03bdC1 n(\u03b1\u2212 \u03c1)\nand thus \u2225\u2225\u2225N(t+1)\n\u2225\u2225\u2225 \u221e \u2264 (1\u2212 \u03b7) \u2225\u2225\u2225N(t) \u2225\u2225\u2225 \u221e + \u03b7 8rC\u03bdC1 \u03b1\u2212 \u03c1 . (52)\nThen for any t \u2265 0, \u2225\u2225\u2225N(t)\n\u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225N(0) \u2225\u2225\u2225 \u221e + 8rC\u03bdC1 \u03b1\u2212 \u03c1 \u2264 U + 8rC\u03bdC1 \u03b1\u2212 \u03c1 \u2264 2U + Ua\nwhere the last inequality is by the definition of Ua. On the other hand, by Lemma 16, we have\n\u2016\u03be(t)\u2016\u221e \u2264 3\u2016(A\u2217)\u2020\u2016\u221e(\u2016N(t)\u2016\u221e + C\u03bd)\n\u2264 3\u2016(A\u2217)\u2020\u2016\u221e ( 2U +\n8rC\u03bdC1 \u03b1\u2212 \u03c1 + C\u03bd\n)\n\u2264 \u03c1 where the last inequality is due to condition (46).\nWe now consider the unbiased noise, where the proof is similar. Set the sample size N to be large enough, so that by Lemma 14, we have\n\u2223\u2223\u2223N\u0303(t)i,j \u2223\u2223\u2223 \u2264 2C1C\u03bd\u03c1 \u2032(1 + \u2016A\u2020N(t)\u2016\u221e) (1\u2212 2\u2113)n(\u03b1\u2212 \u03c1\u2032) + \u2223\u2223\u2223[Ns]i,j \u2223\u2223\u2223\n\u2264 8C1C\n2 \u03bd \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e\n(1 \u2212 2\u2113)n(\u03b1\u2212 2C\u03bd\u2016(A\u2217)\u2020\u2016\u221e) + \u2223\u2223\u2223[Ns]i,j \u2223\u2223\u2223\n\u2264 10C1C\n2 \u03bd \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e\n(1 \u2212 2\u2113)n(\u03b1\u2212 2C\u03bd\u2016(A\u2217)\u2020\u2016\u221e) ,\nand thus \u2225\u2225\u2225N(t+1)S \u2225\u2225\u2225 \u221e \u2264 (1\u2212 \u03b7) \u2225\u2225\u2225N(t)S \u2225\u2225\u2225 \u221e + \u03b7 10rC1C 2 \u03bd \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e (1\u2212 2\u2113)(\u03b1\u2212 2C\u03bd\u2016(A\u2217)\u2020\u2016\u221e) . (53)\nThen for any t \u2265 0, \u2225\u2225\u2225N(t)S \u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225NS(0) \u2225\u2225\u2225 \u221e + 10rC1C 2 \u03bd \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e (1\u2212 2\u2113)(\u03b1\u2212 2C\u03bd\u2016(A\u2217)\u2020\u2016\u221e) \u2264 2U +\n10rC1C 2 \u03bd \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e\n(1\u2212 2\u2113)(\u03b1\u2212 2C\u03bd\u2016(A\u2217)\u2020\u2016\u221e) \u2264 2U + Un\nwhere the last inequality is by the definition of Un. This completes the proof for the claims.\nNow, after proving the claims, we are ready to prove the last statement of the lemma. First, by (48) and Lemma 29, we have that after ln(\u01eb/(\u03b3\u2113))ln(1\u2212\u03b7) iterations,\n\u2016(E(t)1,1)\u2212\u2016s \u2264 \u01eb+ h1.\nNow (49) becomes\n\u2016(E(t+1)1,1 )+\u2016s \u2264 (1\u2212 \u03b7)\u2016(E (t) 1,1) +\u2016s + r\u03b7 R\n(1 \u2212 \u2113)2 (\u01eb + h1) + \u03b7h2 (54)\nAfter an additional ln(\u01eb/(\u03b3\u2113))ln(1\u2212\u03b7) iterations, by Lemma 29,\n\u2016(E(t)1,1)+\u2016s \u2264 rR\n(1\u2212 \u2113)2 (\u01eb + h1) + h2 + \u01eb\nSimilarly, Lemma 29 and (50), after ln(\u01eb/(\u03b3\u2113))ln(1\u2212\u03b7) iterations,\n\u2016(E(t)2,1)\u2016s \u2264 \u01eb+ h2. \u2225\u2225\u2225N(t)\u2212S \u2225\u2225\u2225 \u221e does not change since it is not updated. Now consider \u2225\u2225\u2225N(t)S \u2225\u2225\u2225 \u221e .\nFor the adversarial noise, by (52) and Lemma 29, after ln(\u01eb\u2032/U) ln(1\u2212\u03b7) iterations,\n\u2225\u2225\u2225N(t)S \u2225\u2225\u2225 \u221e \u2264 \u01eb\u2032 + 8rC\u03bdC1 \u03b1\u2212 \u03c1 \u2264 (1\u2212 \u01eb)U\nwhere the last inequality is due to condition (46).\nFor the unbiased noise, by (53) and Lemma 29, after ln(\u01eb\u2032/U) ln(1\u2212\u03b7) iterations,\n\u2225\u2225\u2225N(t)S \u2225\u2225\u2225 \u221e \u2264 \u01eb\u2032 + 10rC1C 2 \u03bd\n\u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e\n(1\u2212 2\u2113)(\u03b1\u2212 2C\u03bd\u2016(A\u2217)\u2020\u2016\u221e) \u2264 (1\u2212 \u01eb)U\nwhere the last inequality is due to condition (47).\nThis completes the proof."}, {"heading": "C.2 Equilibration: Rescale", "text": "The input of of Algorithm 3 can be written as A(0) = A\u2217(\u03a3(0) + E(0)) +N(0). The output A\u0302 can be written as A\u0302 = (A\u2217D)(\u03a3\u0302 + E\u0302) + N\u0302 where \u03a3\u0302 is diagonal, and E\u0302 is off diagonal, and D is a diagonal matrix with Di,i = 11\u2212\u01eb for i \u2208 S and the rest being 1. Recall that for a matrix M, let M1,1 denote the submatrix of M indexed by S \u00d7 S, and define M1,2,M2,1 and M2,2 accordingly. Also recall that MS denote the submatrix of M formed by columns indexed by S, and let M\u2212S denote the submatrix formed by the other columns. Lemma 23 (Main: Rescale). Let A(0) = A\u2217(\u03a3(0)+E(0))+N(0) satisfies the condition in Lemma 19 and \u01eb be defined as in Lemma 19. Then the output of Algorithm 3 is A\u0302 = (A\u2217D)(\u03a3\u0302 + E\u0302) + N\u0302 satisfying\n(1\u2212 \u2113)I \u03a3\u0302, \u2016E\u03021,1\u2016s \u2264 (\u03b3 \u2212 1)\u2113, \u2016E\u03022,1\u2016s \u2264 (\u03b3 \u2212 1)\u2113, \u2016(E\u03021,2, E\u03022,2)\u2016s \u2264 \u2113, \u2016N\u0302S\u2016\u221e \u2264 U, \u2016N\u0302\u2212S\u2016\u221e \u2264 U. Moreover, E\u03021,2 \u2265 0 and E\u03022,2 \u2265 0 entry-wise.\nProof of Lemma 23. Note that A\u0303 = A\u2217(\u03a3\u0303+ E\u0303) + N\u0303 for a diagonal matrix \u03a3\u0303, off-diagonal matrix E\u0303 and error matrix N\u0303. By lemma 19, we have \u03a3\u0303 (1\u2212 \u2113)I, error matrix \u2016N\u0303S\u2016\u221e \u2264 (1\u2212 \u01eb)U and\n\u2016E\u03031,1\u2016s \u2264 (\u03b3 \u2212 1)\u2113, \u2016E\u03032,1\u2016s \u2264 (1\u2212 \u01eb)(\u03b3 \u2212 1)\u2113, \u2016(E\u03031,2; E\u03032,2)\u2016s \u2264 \u2113 and E\u03031,2 \u2265 0 and E\u03032,2 \u2265 0 entry-wise. Therefore, by the rescaling rule:\nA\u0302 = A\u0303D = A\u2217(\u03a3\u0303+ E\u0303)D+ N\u0303D\n= A\u2217D(\u03a3\u0303+D\u22121E\u0303D) + N\u0303D.\nTherefore, \u03a3\u0302 = \u03a3\u0303 (1 \u2212 \u2113)I, \u2016N\u0302S\u2016\u221e \u2264 11\u2212\u01eb\u2016N\u0303S\u2016\u221e \u2264 U . \u2016N\u0302\u2212S\u2016\u221e = \u2016N\u0303\u2212S\u2016\u221e \u2264 U since it is not updated. For the E\u0302 term, denote D1 = Diag ( 1 1\u2212\u01eb , . . . , 1 1\u2212\u01eb ) \u2208 Rs\u00d7s. We know that\nE\u03021,1 = D \u22121 1 E\u03031,1D1 = E\u03031,1\nE\u03022,1 = E\u03032,1D1 = 1\n1\u2212 \u01ebE\u03032,1\nE\u03021,2 = D \u22121 1 E\u03031,2 = (1\u2212 \u01eb)E\u03031,2\nE\u03022,2 = E\u03032,2.\nThis leads to\n\u2016E\u03021,1\u2016s \u2264 (\u03b3 \u2212 1)\u2113, \u2016E\u03022,1\u2016s \u2264 (\u03b3 \u2212 1)\u2113, \u2016(E\u03021,2, E\u03022,2)\u2016s \u2264 \u2113,\nwith E\u03021,2, E\u03022,2 \u2265 0. This completes the proof."}, {"heading": "C.3 Equilibration: Main algorithm", "text": "Lemma 24 (Main: Equilibration). Suppose the conditions in Lemma 23 each time Algorithm 3. Additionally, there exists constant 0 < b < 1, \u03ba > 1 and u > 1 such that b\u03ba > 1 such that the initial \u03bb \u2265 maxi\u2208[n] E[(x\u2217i )2]/b, and the initial \u03a3 uI. Furthermore, for any \u03bb \u2265 mini\u2208[n] E[(x\u2217i )2]/\u03ba,\n( 1 1\u2212 \u2113 + h6 )2 b\u03bb+ h25b\u03ba\u03bb+ h3 \u2264 ( 1\u2212 1 100 ) \u03bb, (55)\n( 1\nu \u2212 h6\n)2 (1\u2212 \u01eb)b\u03ba\u03bb\u2212 h25b\u03ba\u03bb\u2212 h4 \u2265 ( 1 + 1\n100\n) \u03bb, 1\nu > h6 (56)\nh3 \u2264 1\n200 min i\u2208[n]\nE[(x\u2217i ) 2], (57)\nh4 \u2264 1\n200 min i\u2208[n]\nE[(x\u2217i ) 2], (58)\nwhere\nh3 = C21 n2 h5\n( h5 + 2\n1\u2212 \u2113\n) ,\nh4 = C21 n2 h5\n( h5 + 2\n1\u2212 \u2113\n) +\n2(\u03b1+ \u03c1)C1 n(1\u2212 \u2113) ,\nh5 = (\u03b3 + 1)\u2113(1\u2212 (\u03b3 + 1)\u2113) (1\u2212 \u2113)2(1\u2212 (\u03b3 + 2)\u2113) ,\nh6 = (\u03b3 + 1)\u21132\n(1\u2212 \u2113)2(1\u2212 (\u03b3 + 2)\u2113) ."}, {"heading": "Finally, set N = poly(1/mini\u2208[n] E[(x\u2217i )", "text": "2], n, 1/\u03b4) large enough.\nThen with probability at least 1 \u2212 \u03b4, the following hold. During the execution of the algorithm, for any j \u2208 S, ((\n1 u \u2212 h6\n)2 \u2212 \u03bah25 \u2212 1\n100\n) E[(x\u2217j ) 2]\n(Dj,j)2 \u2264 mj \u2264\n(( 1 1\u2212 \u2113 + h6 )2 + \u03bah25 + 1 100 ) E[(x\u2217j ) 2] (Dj,j)2 .\nFurthermore, the output of Algorithm 4 is A = A\u2217D(\u03a3 + E) + N where \u03a3 is diagonal and (1 \u2212 \u2113)I \u03a3, E is off diagonal and \u2016E\u2016s \u2264 \u03b3\u2113, N satisfies \u2016N\u2016\u221e \u2264 2U , and\nmaxi\u2208[n] 1\nD2 i,i\nE[(x\u2217i ) 2]\nminj\u2208[n] 1\nD2 j,j\nE[(x\u2217j ) 2]\n\u2264 \u03ba.\nProof of Lemma 24. We prove the lemma by induction. For notational convenience, let us introduce a counter (p) denoting the number of times the inner while cycle has been executed, and denote A as A(p). Recall that for a matrix M \u2208 Rn\u00d7n and index set S \u2286 [n], let M1,1 denote the submatrix indexed by S \u00d7 S, and M1,2,M2,1 and M2,2 are defined accordingly. Also, let MS denote the submatrix formed by the columns indexed by S, and M\u2212S the submatrix formed by the other columns.\nOur inductive claims are as follows. At the beginning of each inner while cycle,\nA (p) = A\u2217D(p) ( \u03a3 (p) +E(p) ) +N(p)\nwhere D(p) and \u03a3(p) are diagonal, E(p) are off diagonal satisfying\n(1) (1\u2212 \u2113)I \u03a3(p),\n(2) E(p)1,2 \u2265 0 and E (p) 2,2 \u2265 0 entry-wise and\n\u2225\u2225\u2225E(p)1,1 \u2225\u2225\u2225 s \u2264 \u03b3\u2113, \u2225\u2225\u2225E(p)2,1 \u2225\u2225\u2225 s \u2264 \u03b3\u2113,\n\u2225\u2225\u2225(E(p)1,2;E (p) 2,2) \u2225\u2225\u2225 s \u2264 \u2113,\n(3) N(p)\u2212S \u2264 U and N (p) S \u2264 2U ,\n(4) We have\n(a) When E[(x\u2217j ) 2] < b\u03bb(p), j /\u2208 S, then mj \u2264 \u03bb(p), (b) When E[(x\u2217j ) 2] \u2265 (1\u2212 \u01eb)b\u03ba\u03bb(p), j /\u2208 S, then mj > \u03bb(p),\nand consequently,\n(c) \u2200i \u2208 S, b\u03bb(p) \u2264 E[(x \u2217 i ) 2]( D\n(p) i,i\n)2 ,\n(d) \u2200i \u2208 [n], E[(x \u2217 i ) 2]( D\n(p) i,i\n)2 \u2264 b\u03ba\u03bb(p).\nThe claims are trivially true at initialization, so we proceed to the induction. Assume the claim is true at time p, we proceed to show it is true at time p+ 1.\nFirst, consider (1), (2) and (3). By Lemma 23, after applying the rescaling algorithm, (1\u2212\u2113)I \u03a3(p) and\n\u2016E(p)1,1\u2016s \u2264 (\u03b3 \u2212 1)\u2113, \u2016E (p) 2,1\u2016s \u2264 (\u03b3 \u2212 1)\u2113, \u2016(E (p) 1,2,E (p) 2,2)\u2016s \u2264 \u2113, \u2016N (p) S \u2016\u221e \u2264 U, \u2016N (p) \u2212S\u2016\u221e \u2264 U.\nMoreover, E(p)1,2 \u2265 0 and E (p) 2,2 \u2265 0 entry-wise. Observe that when moving from time p to p + 1, potentially the algorithm includes new elements in S. Then\n\u2016E(p+1)1,1 \u2016s \u2264 \u2016E (p) 1,1\u2016s +max{\u2016E (p) 2,1\u2016s, \u2016E (p) 1,2\u2016s} \u2264 (\u03b3 \u2212 1)\u2113+ \u2113 = \u03b3\u2113\nWhere the last inequality used the fact that \u03b3 < 2. Similarly,\n\u2016E(p+1)2,1 \u2016s \u2264 \u2016E (p) 2,1\u2016s + \u2016E (p) 2,2\u2016s \u2264 (\u03b3 \u2212 1)\u2113+ \u2113 = \u03b3\u2113.\nAlso, \u2016(E(p+1)1,2 ,E (p+1) 2,2 )\u2016s \u2264 \u2016(E (p) 1,2,E (p) 2,2)\u2016s \u2264 \u2113, and (E (p+1) 1,2 ,E (p+1) 2,2 ) \u2265 0 entry-wise. Furthermore, \u2016N(p+1)\u2212S \u2016\u221e \u2264 \u2016N (p) \u2212S\u2016\u221e \u2264 U and\n\u2016N(p+1)S \u2016\u221e \u2264 \u2016N (p) S \u2016\u221e + \u2016N (p) \u2212S\u2016\u221e \u2264 2U.\nHence, (1), (2) and (3) are also true at time (p+ 1).\nFinally, we proceed to (4). Since (a)(b) are true at time p, (c)(d) are true at time p+1. 6 Furthermore, when \u03bb \u2264 mini\u2208[n] E[(x\u2217i )2]/\u03ba, it is guaranteed that all [n] \u2286 S, so we only need to prove that when \u03bb \u2265 mini\u2208[n] E[(x\u2217i )2]/\u03ba, (a)(b) are also true at time p+ 1. To prove (a)(b) are true at time p + 1, we will use Lemma 25. Note that since A has been scaled, so A\u2217D should be regarded as the ground truth matrix A\u2217 in Lemma 25. We first make sure its assumption is satisfied. First, \u2016N\u2016\u221e \u2264 3U and \u2225\u2225(A\u2217D)\u2020 \u2225\u2225 \u221e \u2264 \u2225\u2225(A\u2217)\u2020 \u2225\u2225 \u221e. By Lemma 16 and condition (45), the assumption in Lemma 25 is satisfied.\nWe are now ready to prove (a). By Lemma 25,\nE[x2j ] \u2264 ( \u03a3 \u22121 j,j + |Vj,j | )2 E[(x\u2217j )2] D\n(p+1) j,j\n+ \u2016[V]j\u201622 max k\u2208[n]\nE[(x\u2217k) 2]\nD (p+1) k,k\n+ \u2016[V]j\u20161 ( \u2016[V]j\u20161 + 2\u03a3\u22121j,j ) C21 n2 .\n6Note that in (b), the factor (1\u2212 \u01eb) is needed to ensure (d) is true at time p+ 1.\nBy Lemma 15, |Vj,j | \u2264 h6, \u2016[V]j\u201622 \u2264 \u2016[V]j\u201621 \u2264 h25, so\nE[x2j ] \u2264 ( 1 1\u2212 \u2113 + h6 )2 E[(x\u2217j ) 2]\n(D (p+1) j,j )\n2 + h25 max k\u2208[n]\nE[(x\u2217k) 2] (D (p+1) k,k ) 2 + h3."}, {"heading": "By (d), maxk\u2208[n]", "text": "E[(x\u2217k) 2]\n(D (p+1) k,k\n)2 \u2264 b\u03ba\u03bb, so for any j /\u2208 S with E[(x\u2217j )2] =\nE[(x\u2217j ) 2]\n(D (p+1) j,j\n)2 < b\u03bb, we have\nE[x2j ] \u2264 ( 1 1\u2212 \u2113 + h6 )2 b\u03bb+ h25b\u03ba\u03bb+ h3.\nBy using large enough sample, with high probability, the empirical estimation\nE\u0302[x2j ] \u2264 E[x2j ] + 1\n100 \u03bb \u2264 \u03bb\nwhere the last step is by condition (55).\nAs for (b), by Lemma 25 we have\nE[x2j ] \u2265 ( \u03a3 \u22121 j,j \u2212 |Vj,j | )2 E[(x\u2217j )2] (D\n(p+1) j,j )\n2 \u2212 \u2016[V]j\u201622 max k\u2208[n]\nE[(x\u2217k) 2] (D (p+1) k,k ) 2 \u2212 ( C21 n2 \u2016[V]j\u20161(\u2016[V]j\u20161 + 2\u03a3\u22121j,j ) + 2(\u03b1+ \u03c1)C1 n \u03a3 \u22121 j,j )\n\u2265 ( 1\nu \u2212 h6\n)2 E[(x\u2217j ) 2]\n(D (p+1) j,j )\n2 \u2212 h25 max k\u2208[n]\nE[(x\u2217k) 2] (D (p+1) k,k ) 2 \u2212 h4.\nThe last step uses that \u03a3\u22121j,j \u2264 u, which is by the initial condition assumed and that it is not updated for j 6\u2208 S. Putting in the bound that E[(x \u2217 k) 2]\n(D (p+1) k,k\n)2 \u2264 b\u03ba\u03bb, then for any j /\u2208 S with E[(x\u2217j )2] =\nE[(x\u2217j ) 2]\n(D (p+1) j,j\n)2 \u2265 (1\u2212 \u01eb)b\u03ba\u03bb, we have\nE[x2j ] \u2265 ( 1\nu \u2212 h6\n)2 (1 \u2212 \u01eb)b\u03ba\u03bb\u2212 h25b\u03ba\u03bb\u2212 h4.\nAgain, use large enough sample to ensure that with high probability\nE\u0303[x2j ] \u2265 E[x2j ]\u2212 1\n100 \u03bb \u2265 \u03bb\nwhere the last step follows from condition (56). This completes the proof of the induction.\nWe now prove the statements of the lemma. The statement about the output follows from the above claims. What is left is to prove that mj(j \u2208 S) approximates E[(x\u2217j )2] well. Since mj for j \u2208 S is updated along with Dj,j , we only need to check the right after adding j to S, the statement holds. Suppose the time point is p, we have\nE[x2j ] \u2264 ( 1 1\u2212 \u2113 + h6 )2 E[(x\u2217j ) 2]\n(D (p) j,j )\n2 + h25 max k\u2208[n]\nE[(x\u2217k) 2] (D (p) k,k) 2 + h3.\nSince j is in S, by the claims (c)(d) we have\nmax k\u2208[n]\nE[(x\u2217k) 2] (D (p) k,k) 2 \u2264 b\u03ba\u03bb(p) \u2264 \u03ba E[(x\u2217j ) 2] (D (p) j,j ) 2 .\nSince N is large enough so that\nE[x2j ] \u2264 E[(x\u2217j )2] ( 1 + 1\n200\n) .\nCombined these with the condition (57), we have\nmj \u2264 (( 1 1\u2212 \u2113 + h6 )2 + \u03bah25 + 1 100 ) E[(x\u2217j ) 2] (Dj,j)2 .\nThe upper bound on mj can be bounded similarly. This completes the proof of the lemma.\nThe following is the lemma used in the proof of Lemma 24.\nLemma 25 (Estimate of feature weight). Suppose |\u03bei| \u2264 \u03c1 < \u03b1 for any example and every i \u2208 [n], and suppose \u03a3 12I. Then\nE[x2i ] \u2265 ( \u03a3 \u22121 i,i \u2212 |Vi,i| )2 E[(x\u2217i )\n2]\u2212 \u2016[V]i\u201622max j\u2208[n] E[(x\u2217j ) 2]\n\u2212 ( C21 n2 \u2016[V]i\u20161(\u2016[V]i\u20161 + 2\u03a3\u22121i,i ) + 2(\u03b1+ \u03c1)C1 n \u03a3 \u22121 i,i )\nE[x2i ] \u2264 ( \u03a3 \u22121 i,i + |Vi,i| )2 E[(x\u2217i )\n2] + \u2016[V]i\u201622max j\u2208[n] E[(x\u2217j ) 2] + \u2016[V]i\u20161\n( \u2016[V]i\u20161 + 2\u03a3\u22121i,i ) C21 n2 .\nProof of Lemma 25. By the decoding rule,\nxi = [ \u03c6\u03b1(A \u2020[A\u2217x\u2217 + \u03bd]) ] i\n= [ \u03c6\u03b1 (( \u03a3 \u22121 +V ) x\u2217 + \u03be )] i .\nLet [V]i = v and \u03a3\u22121i,i = \u03c3, then we can rewrite above as\nxi = \u03c6\u03b1(\u03c3x \u2217 i + \u3008v, x\u2217\u3009+ \u03bei)\nwhich implies that\n\u03c3x\u2217i + \u3008v, x\u2217\u3009 \u2212 \u03c1\u2212 \u03b1 \u2264 xi \u2264 |\u03c3x\u2217i + \u3008v, x\u2217\u3009| . (59)\nFirst, consider the lower bound.\nE[x2i ] \u2265 E [(\u03c3x\u2217i + \u3008v, x\u2217\u3009 \u2212 \u03c1\u2212 \u03b1)\u03c6\u03b1(\u03c3x\u2217i + \u3008v, x\u2217\u3009+ \u03bei)]\nThe following simple lemma is useful.\nClaim 26. Let \u03c7 be a variable such that |\u03c7| \u2264 \u03b1, then for every w \u2208 Rn, k \u2208 [n],\nE[x\u2217k\u03c6\u03b1(\u3008w, x\u2217\u3009+ \u03c7)] \u2264 |wk|E[(x\u2217k)2] + C21 n2\n\u2211 j 6=k |wj | (60)\n\u2264 |wk|E[(x\u2217k)2] + C21 n2 \u2016w\u20161. (61)\nProof. The proof is a direct observation that when |\u03c7| < \u03b1, \u03c6\u03b1(\u3008w, x\u3009 + \u03c7) \u2264 |\u3008w, x\u3009| \u2264 \u3008|w|, x\u3009\nwhere |w| is the entry wise absolute value.\nTherefore, we can obtain the following bounds.\n(1). By (17) in Lemma 12, we have\nE[x\u2217i \u03c6\u03b1(\u03c3x \u2217 i + \u3008v, x\u2217\u3009+ \u03bei)] \u2265 \u03a3\u22121i,i E [ (x\u2217i ) 2 ] \u2212 (\u03b1 + \u03c1)C1\nn \u2212 E\n[ (x\u2217i ) 2 ] |Vi,i| \u2212\nC21 n2 \u2225\u2225\u2225[V]i \u2225\u2225\u2225 1 ,\n(2). By (61) in the above claim,\nE[x\u2217j\u03c6\u03b1(\u03c3x \u2217 i + \u3008v, x\u2217\u3009+ \u03bei)] \u2264 |vj |E[(x\u2217j )2] + C21 n2 (\u2016v\u20161 + \u03c3),\n(3). By (59), for j 6= i,\nE[\u03c6\u03b1(\u03c3x \u2217 i + \u3008v, x\u2217\u3009+ \u03bei)] \u2264 E[|\u03c3x\u2217i + \u3008v, x\u2217\u3009|] \u2264 (\u03c3 + \u2016v\u20161)C1 n .\nPutting together, we can obtain\nE[x2i ] \u2265 ( \u03a3 \u22121 i,i \u2212 |Vi,i| )2 E[(x\u2217i )\n2]\u2212 \u2016[V]i\u201622 max j\u2208[n] E[(x\u2217j ) 2]\n\u2212 ( C21 n2 \u2016[V]i\u20161(\u2016[V]i\u20161 + 2\u03a3\u22121i,i ) + 2(\u03b1+ \u03c1)C1 n \u03a3 \u22121 i,i ) .\nSecond, we proceed to the upper bound. Similarly as the lower bound, by (59), we have\nE[x2i ] \u2264 E    (|vi|+ \u03c3)x\u2217i + \u2211\nj 6=i |vj |x\u2217j\n \u03c6\u03b1(\u03c3x\u2217i + \u3008v, x\u2217\u3009+ \u03bei)  \n= (|vi|+ \u03c3)E[x\u2217i \u03c6\u03b1(\u03c3x\u2217i + \u3008v, x\u2217\u3009+ \u03bei)] + \u2211\nj 6=i |vj |E[x\u2217j\u03c6\u03b1(\u03c3x\u2217i + \u3008v, x\u2217\u3009+ \u03bei)].\nFor the first summand, same as in (2), by (61) in the above claim we get\nE[x\u2217i \u03c6\u03b1(\u03c3x \u2217 i + \u3008v, x\u2217\u3009+ \u03bei)] \u2264 (\u03c3 + |vi|)E[(x\u2217i )2] + C21 n2 \u2016v\u20161,\nE[x\u2217j\u03c6\u03b1(\u03c3x \u2217 i + \u3008v, x\u2217\u3009+ \u03bei)] \u2264 |vj |E[(x\u2217j )2] + C21 n2 (\u2016v\u20161 + \u03c3).\nTherefore, we get\nE[x2i ] \u2264 ( \u03a3 \u22121 i,i + |Vi,i| )2 E[(x\u2217i ) 2] + \u2016[V]i\u20161(\u2016[V]i\u20161 + 2\u03a3\u22121i,i ) C21 n2 + \u2016[V]i\u201622max j\u2208[n] E[(x\u2217j ) 2].\nwhich completes the proof."}, {"heading": "C.4 Main theorem", "text": "Theorem 18 (Main: Equilibration). If there exists an absolute constant G such that Assumption (A1)-(A3) and (N1) are satisfied with l = 1/50, C31 \u2264 Gc22n, max { C\u03bd , \u2016N(0)\u2016\u221e } \u2264 Gc 4 2\nC51n\u2016(A\u2217)\u2020\u2016\u221e ,\nand additionally \u03a3(0) (1 \u2212 \u2113)I, and E \u2265 0 entry-wise, then there exist \u03b1, \u03b7, T, \u03bb such that for sufficiently small \u01eb > 0 and sufficiently large N = poly(n,m, 1/\u01eb, 1/\u03b4) the following hold with probability at least 1\u2212\u03b4: Algorithm 4 outputs a solutionA = A\u2217D(\u03a3+E)+Nwhere \u03a3 (1\u2212\u2113)I is diagonal, \u2016E\u2016s \u2264 \u03b3\u2113 is off-diagonal, \u2016N\u2016\u221e \u2264 2\u2016N(0)\u2016\u221e, and D is diagonal and satisfies\nmaxi\u2208[n] 1\nD2 i,i\nE[(x\u2217i ) 2]\nminj\u2208[n] 1\nD2 j,j\nE[(x\u2217j ) 2]\n\u2264 2.\nIf Assumption (A1)-(A3) and (N2) are satisfied with the same parameters except max { C\u03bd , \u2016N(0)\u2016\u221e } \u2264 min {\u221a Gc42 C51n 1 \u2016(A\u2217)\u2020\u2016\u221e , Gc22 C31\u2016(A\u2217)\u2020\u2016\u221e } , then the same guarantees hold.\nProof of Theorem 18. The theorem follows from Lemma 24 (taking union bound over all the iterations and setting a proper \u03b4), if the conditions are satisfied. So in the following, we first specify the parameters and then verify the conditions in Lemma 19 and Lemma 24.\nRecall that \u2113 = 1/50. Define u = 1 + \u2113, \u03b3 = 3/2, \u03b2 = 4, \u03ba = 2, b = 3/4, and let \u01eb < 1/1000.\nConditions in Lemma 19. For (40), we need to compute riRi and the the third term. Note that by the induction in Lemma 24, the mj is an good approximation of E[(x\u2217j ) 2]/(Dj,j) 2. Furthermore, when Lemma 19 is applied in Lemma 24, it is applied on the ground-truth matrix (A\u2217)\u2032 = A\u2217D and (x\u2217j ) \u2032 = x\u2217j/Dj,j , so mj is a good approximation of E[((x \u2217 j ) \u2032)2]. Then\nriRi = 3E[((x\u2217i ) \u2032)2]\n5mi \u2264 3 5 ((\n1 u \u2212 h6 )2 \u2212 \u03bah25 \u2212 1100 ) .\nFor the third term, first note that C31 \u2264 Gc22n, and thus C21 \u2264 Gc2n by C1 > c2. Furthermore, ri = O(1/mi) = O(n/c2) for i \u2208 S. Plugging in the parameters, we know that the third term is less than 1/1000 when G is sufficiently small. Then (40) can be verified by plugging the parameters. Similarly, for (41), we can compute riRi and let G small enough so that the second term is less than 1/1000, and then verify the condition.\nFor (42) (43) and (44), we need to bound h1 and h2, which in turn relies on r and rR. Since for i \u2208 S, ri = O(n/c2), r = O(n/c2). Then similar to the argument as above, h1 < 2/10000 when G is sufficiently small. when Lemma 19 is applied in Lemma 24, it is applied on the ground-truth matrix (A\u2217)\u2032 = A\u2217D and (x\u2217j ) \u2032 = x\u2217j/Dj,j . By the induction claims there, maxj\u2208[n] E[((x \u2217 j )\n\u2032)2] differ from minj\u2208S E[((x\u2217j )\n\u2032)2] by a factor of at most \u03ba, so rR \u2264 3\u03ba5 . So the first term can be computed. The second term is less than 1/10000 when G is small enough. Then h2 can be computed. And the conditions can be verified.\nCondition (45) is true since max {C\u03bd , \u2016N\u2016\u221e} = O( c 2 2\nC31\u2016(A\u2217)\u2020\u2016\u221e ). Condition (46) is true by setting\n\u01eb\u2032 < U/8 and by Ua < U/8 and U = \u2016N\u2016\u221e \u2264 O( c 2 2\nC31\u2016(A\u2217)\u2020\u2016\u221e ). Similarly, condition (46) is true\nby setting \u01eb\u2032 < U/8 and by Un < U/8 and \u2016N\u2016\u221e is sufficiently small.\nConditions in Lemma 24. First, consider (57) and (58). As mentioned above, since C31 = O(c 2 2n) and C21 = O(c2n), then h3 and h4 can be made sufficiently small to satisfy the conditions. (55) and (56) can be verified by plugging (57) and (58) and the assumption that \u03bb \u2265 mini\u2208[n] E[(x\u2217i )2]/\u03ba. This completes the proof."}, {"heading": "D Auxiliary lemmas for solving recurrence", "text": "The following lemmas are used when solving recurrence in our analysis.\nLemma 27 (Coupling update rule). Let {at}\u221et=0, {bt}\u221et=0 be sequences of non-negative numbers such that for fixed values h \u2265 0, \u03b7 \u2208 [0, 1], R > 4r > 0:\nat+1 \u2264 (1\u2212 \u03b7)at + \u03b7rbt + \u03b7h bt+1 \u2264 (1\u2212 \u03b7)bt + \u03b7\nR at + \u03b7h\nThen the following two properties holds:\n1.\n\u2200t \u2265 0, at + bt \u2264 a0 + b0 + Rr + 2R+ 1\nR\u2212 r h\n2. For all \u01eb > 0, when t \u2265 ln a0+b08\u03b7\u01eb , we have:\nat \u2264 R(r + 1) R\u2212 r h+ \u01eb, bt \u2264 R+ 1 R\u2212 r h+ \u01eb\nProof of Lemma 27. Observe that the update rule is equivalent to ( at+1 \u2212 R(r + 1) R\u2212 r h ) \u2264 (1 \u2212 \u03b7) ( at \u2212 R(r + 1) R\u2212 r h ) + \u03b7r ( bt \u2212 R+ 1 R\u2212 r h )\n( bt+1 \u2212 R+ 1 R\u2212 r h ) \u2264 (1 \u2212 \u03b7) ( bt \u2212 R + 1 R \u2212 rh ) + \u03b7 R ( at \u2212 R(r + 1) R\u2212 r h )\nTherefore, define ct = at \u2212 R(r+1)R\u2212r h and dt = bt \u2212 R+1R\u2212rh, we can rewrite above as:\nct+1 \u2264 (1\u2212 \u03b7)ct + \u03b7rdt dt+1 \u2264 (1\u2212 \u03b7)dt + \u03b7\nR ct\nSince we just need to upper bound ct, dt. without lose of generality, we can assume that\nct+1 = (1\u2212 \u03b7)ct + \u03b7rdt dt+1 = (1\u2212 \u03b7)dt + \u03b7\nR ct\nWhich implies that ( ct+1 + \u221a R\nr dt+1\n) = ( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)( ct + \u221a R\nr dt\n)\n( ct+1 \u2212 \u221a R\nr dt+1\n) = ( 1\u2212 \u03b7 \u2212 \u03b7 \u221a r\nR\n)( ct \u2212 \u221a R\nr dt\n)\nWhich can be simplified to ( ct + \u221a R\nr dt\n) = ( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t ( c0 + \u221a R\nr d0\n)\n( ct \u2212 \u221a R\nr dt\n) = ( 1\u2212 \u03b7 \u2212 \u03b7 \u221a r\nR\n)t ( c0 \u2212 \u221a R\nr d0\n)\nTherefore, we can solve\nct = 1\n2\n[( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t + ( 1\u2212 \u03b7 \u2212 \u03b7 \u221a r\nR\n)t] c0+ 1\n2\n\u221a R\nr\n[( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t \u2212 ( 1\u2212 \u03b7 \u2212 \u03b7 \u221a r\nR\n)t] d0\ndt = 1\n2\n\u221a r\nR\n[( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t \u2212 ( 1\u2212 \u03b7 \u2212 \u03b7 \u221a r\nR\n)t] c0+ 1\n2\n[( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t + ( 1\u2212 \u03b7 \u2212 \u03b7 \u221a r\nR\n)t] d0"}, {"heading": "Observe that for every t \u2265 0, a \u2265 b \u2265 0, at \u2212 bt \u2264 (a\u2212 b)tat\u22121", "text": "Which implies: ( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t \u2212 ( 1\u2212 \u03b7 \u2212 \u03b7 \u221a r\nR\n)t \u2264 2t\u03b7 \u221a r\nR\n( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t\u22121\nTherefore, when c0, d0 \u2265 0,\nct \u2264 ( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t c0 + t\u03b7 ( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t\u22121 d0\nMoreover,\ndt \u2264 r\nR \u03b7\n( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t c0 + ( 1\u2212 \u03b7 + \u03b7 \u221a r\nR\n)t d0\nTaking the optimal t, we obtain ct + dt \u2264 c0 + d0, which implies that\nat + bt \u2264 a0 + b0 + Rr + 2R+ 1\nR\u2212 r h\nOn the other hand, when t \u2265 ln c0+d08\u03b7\u01eb , ct, dt \u2264 \u01eb, which implies that\nat \u2264 R(r + 1) R\u2212 r h+ \u01eb, bt \u2264 R+ 1 R\u2212 r h+ \u01eb.\nLemma 28 (Simple coupling). Let {at}\u221et=0, {bt}\u221et=0 be sequences of non-negative numbers such that for fixed values h1, h2 \u2265 0, \u03b7 \u2208 [0, 1], r > 0:\nat+1 \u2264 (1 \u2212 \u03b7)at + \u03b7h1 bt+1 \u2264 (1 \u2212 \u03b7)bt + \u03b7sat + \u03b7h2\nThen at \u2264 ua := max {a0, h1} , bt \u2264 max {b0, h2 + sua} .\nProof. We have (at+1 \u2212 h1) \u2264 (1\u2212 \u03b7)(at \u2212 h1) (bt+1 \u2212 h2) \u2264 (1\u2212 \u03b7)(bt \u2212 h2) + \u03b7sat\nSolving the first one gives at \u2264 ua := max {a0, h1} .\nThen (bt+1 \u2212 h2) \u2264 (1 \u2212 \u03b7)(bt \u2212 h2) + \u03b7sua\nleads to bt \u2264 max {b0, h2 + sua} .\nLemma 29 (Simple recursion). Let {at}\u221et=0 be a sequences of non-negative numbers such that for fixed values h \u2265 0, \u03b7 \u2208 [0, 1], at+1 \u2264 (1\u2212 \u03b7)at + \u03b7h. Then, at \u2264 (1\u2212 \u03b7)ta0 + h, and thus for t \u2265 ln(\u01eb/a0)ln(1\u2212\u03b7) , we have\nat \u2264 \u01eb+ h.\nProof. We will prove by induction that at \u2264 (1 \u2212 \u03b7)ta0 + h, which implies the statement of the lemma. The base case is trivial, so we proceed to the induction:\nat+1 \u2264 (1\u2212 \u03b7) ( (1\u2212 \u03b7)ta0 + h ) + \u03b7h \u2264 (1\u2212 \u03b7)t+1a0 + h\nas we need."}, {"heading": "E Detailed discussion about related work", "text": ""}, {"heading": "E.1 Non-negative matrix factorization", "text": "The area of non-negative matrix factorization (henceforth NMF) has a rich empirical history, starting with the work of [LS99]. In that paper, the authors propose two algorithms based on alternating minimization, one in KL divergence norm, and the other in Frobenius norm. They observe that these heuristics work quite well in practice, but no theoretical understanding of it is provided.\nOn the theoretical side, [AGKM12] provide a fixed-parameter tractable algorithm for NMF: namely when if the matrix A \u2208 Rm\u00d7nand X \u2208 Rn\u00d7N , they provide an algorithm that runs in time (mN)n. This is prohibitive unless n is extremely small. Furthermore, the algorithm is based on routines from algebraic geometry, so its tolerance to noise is fairly weak. More precisely, if there are matrices A\u2217, X\n\u2217, s.t. \u2016Y \u2212A\u2217X\u2217\u2016F \u2264 \u01ebY\ntheir algorithm produces matrices A,X, s.t. \u2016Y \u2212A\u2217X\u2217\u2016F \u2264 O(\u01eb1/2n1/4)Y They further provide matching hardness results: namely they show there is no algorithm running in time (mN)o(n) unless there is a sub-exponential running time algorithm for 3-SAT. They also study the problem under separability assumptions about the feature matrix. [BGKP16] studies the problem under heavy noise setting, but also needs assumptions related to separability, such as the existence of dominant features. Also, their noise model is different from ours."}, {"heading": "E.2 Topic modeling", "text": "A closely related problem is topic modeling. Topic models are a generative model for text data, using the common bag-of-words assumption. In this case, the columns of the matrix A\u2217 (which have norm 1) can naturally be interpreted as topics, with the entries being the emmision probabilities of words in that topic. The vectors x\u2217 in this case also will have norm 1, and can be viewed as distributions over topics. In this way, y\u2217 = A\u2217x\u2217 can be viewed as the vector describing the emission probabilities of words in a given document: first a topic i is selected according to the distribution x\u2217, then a word is selected from topic i according to the distribution in column [A\u2217]i. There also exist work that assume x\u2217i \u2208 [0, 1] and are independent (e.g., [ZX12]), which is closely related to our model. The distinction from NMF is that when documents are fairly short, the empirical frequencies of the words in the document might be very far from y\u2217. For this reason, typicall the algorithms with provable guarantees look at the empirical covariance matrix of the words, which will concentrate to the true one when the number of documents grows, even if the documents are very short. This, however, results in algorithms that scale quadratically in the vocabulary size, which often is prohibitive in practice. Also note that since x\u2217 is assumed to have norm 1 in topic modeling, it does not satisfy our assumption (A2). However, there also exist work on topic modeling [ZX12] that do not restrict x\u2217 is assumed to have norm 1 and can satisfying our assumption.\nThere is a rich body of empirical work on topic models, starting from the seminal work on LDA due to [BNJ03]. Typically in empirical papers the matrices A\u2217, as well as the vectors x\u2217 are learned using variational inference, which can be interpreted as a kind of alternating minimization in KL divergence norm, and in the limit of infinite-length documents converges to the [LS99] updates ([AR15]).\nFrom the theoretical side, there was a sequence of works by [AGM12],[AGH+13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14]. All of these works are based on either spectral or combinatorial (overlapping clustering) approaches, and need certain \u201cnon-overlapping\u201dassumptions on the topics. For example, [AGM12] and [AGH+13] assume that the topic-word matrix contains \u201canchor words\u201d. This means that each topic has a word which appears in that topic, and no other. [AHJK13] on the other hand work with a certain expansion assumption on the word-topic graph, which says that if one takes a subset S of topics, the number of words in the support of these topics should be at least |S|+ smax, where smax is the maximum support size of any topic. Finally, in the paper [AR15] a version of the standard variational inference updates is analyzed in the limit of infinite length documents. The algorithm there also involves a step of \u201cdecoding\u201d, which recovers correctly the support of a given sample, and a \u201cgradient descent\u201d step, which updates A\u2217 in the direction of the gradient of a KL-divergence based objective function. However, [AR15] requires quite strong assumptions on both the warm start, and the amount of \u201cnon-overlapping\u201d of the topics in the topic-word matrix.\nE.3 ICA\nIn the problem of independent component analysis (henceforth ICA, also known as blind-source separation), one is given samples y = A\u2217x\u2217 + \u03b7, where the distribution on the samples x\u2217 is independent for each coordinate, the 4-th moment of x\u2217i is strictly smaller than that of a Gaussian and A\u2217 has full rank. The classic papers [Com94] and [FJK96] solved this problem in the noiseless case, with an approach based on cumulants, and [AGMS12] solved it in another special case, when the noise \u03b7 is Gaussian (albeit with an unknown covariance matrix).\nOur approach is significantly more robust to noise than these prior approaches, since it can handle both adversarial noise and zero mean noise. This is extremely important in practice, as often the nature of the noise may not be precisely known, let alone exactly Gaussian.\nE.4 Non-convex optimization via gradient descent\nThe framework of having a \u201cdecoding\u201d for the samples, along with performing a gradient descentlike update for the model parameters has proven successful for dictionary learning as well, which is the problem of recovering the matrixA\u2217 from samples y = A\u2217x\u2217+\u03b7, where the matrixA\u2217 \u2208 Rm\u00d7n is typically long (i.e. n \u226b m) and x\u2217 is sparse. (No non-negativity constraints are imposed on either\nA or x\u2217.) In this scenario, the columns of A\u2217 are thought of as a dictionary, and each sample y is generated as a (noisy) sparse combination of the columns of the dictionary.\nThe original empirical work which proposed an algorithm like this (in fact, it suggested that the V1 layer processes visual signals in the same manner) was due to [OF97]. In fact, it is suggest that similar families of algorithms based on \u201cdecoding\u201d and gradient-descent are neurally plausible as mechanisms for a variety of tasks like clustering, dimension-reduction, NMF, etc. ([PC15a, PC15b, HPC14, PC14])\nA theoretical analysis of it came latter due to [AGMM15]. They showed that with a suitable warm start, the gradient calculated from the \u201cdecoding\u201d of the samples is sufficiently correlated with the gradient calculated with the correct value x\u2217, therefore allowing them to show the algorithm converges to a matrix A close to the ground truth A\u2217. However, the assumption made in [AGMM15] is that the columns of A\u2217 are incoherent, which means that they have l2 norm bounded by 1, and inner products bounded by O( 1\u221a\nm ). 7\nThe above techniques are not directly applicable to our case, as we don\u2019t wish to have any assumptions on the matrix A\u2217. Additionally, the incoherence assumptions on the matrix A\u2217 used in [AGMM15], in the case when A\u2217 needs to be non-negative and has l1 column-wise norm would effectively imply that the columns of A\u2217 have very small overlap.\n7This is satisfied when the columns are random unit vectors, and intuitively says the columns of the dictionary are not too correlated."}], "references": [{"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "ICML", "citeRegEx": "AGH13", "shortCiteRegEx": null, "year": 2013}, {"title": "Computing a nonnegative matrix factorization\u2013provably", "author": ["Sanjeev Arora", "Rong Ge", "Ravindran Kannan", "Ankur Moitra"], "venue": "STOC, pages 145\u2013162. ACM,", "citeRegEx": "AGKM12", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning topic models \u2013 going beyond svd", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "FOCS", "citeRegEx": "AGM12", "shortCiteRegEx": null, "year": 2012}, {"title": "Simple", "author": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"], "venue": "efficient, and neural algorithms for sparse coding. In COLT", "citeRegEx": "AGMM15", "shortCiteRegEx": null, "year": 2015}, {"title": "with implications for gaussian mixtures and autoencoders", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra", "Sushant Sachdeva. Provable ica with unknown gaussian noise"], "venue": "NIPS, pages 2375\u20132383,", "citeRegEx": "AGMS12", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning latent bayesian networks and topic models under expansion constraints", "author": ["A. Anandkumar", "D. Hsu", "A. Javanmard", "S. Kakade"], "venue": "ICML", "citeRegEx": "AHJK13", "shortCiteRegEx": null, "year": 2013}, {"title": "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation", "author": ["A. Anandkumar", "S. Kakade", "D. Foster", "Y. Liu", "D. Hsu"], "venue": "Technical report", "citeRegEx": "AKF12", "shortCiteRegEx": null, "year": 2012}, {"title": "In NIPS", "author": ["Pranjal Awasthi", "Andrej Risteski. On some provably correct cases of variational inference for topic models"], "venue": "pages 2089\u20132097,", "citeRegEx": "AR15", "shortCiteRegEx": null, "year": 2015}, {"title": "A provable svd-based algorithm for learning topics in dominant admixture corpus", "author": ["T. Bansal", "C. Bhattacharyya", "R. Kannan"], "venue": "NIPS", "citeRegEx": "BBK14", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonnegative matrix factorization under heavy noise", "author": ["Chiranjib Bhattacharyya", "Navin Goyal", "Ravindran Kannan", "Jagdeep Pani"], "venue": "Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "BGKP16", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic topic models", "author": ["David M Blei"], "venue": "Communications of the ACM,", "citeRegEx": "Ble12", "shortCiteRegEx": null, "year": 2012}, {"title": "JMLR", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan. Latent dirichlet allocation"], "venue": "3:993\u20131022,", "citeRegEx": "BNJ03", "shortCiteRegEx": null, "year": 2003}, {"title": "a new concept? Signal processing", "author": ["Pierre Comon. Independent component analysis"], "venue": "36(3):287\u2013314,", "citeRegEx": "Com94", "shortCiteRegEx": null, "year": 1994}, {"title": "Topic discovery through data dependent and random projections", "author": ["W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama"], "venue": "arXiv preprint arXiv:1303.3664", "citeRegEx": "DRIS13", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient distributed topic modeling with provable guarantees", "author": ["W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama"], "venue": "AISTAT, pages 167\u2013175", "citeRegEx": "DRIS14", "shortCiteRegEx": null, "year": 2014}, {"title": "page 359", "author": ["Alan Frieze", "Mark Jerrum", "Ravi Kannan. Learning linear transformations. In focs"], "venue": "IEEE,", "citeRegEx": "FJK96", "shortCiteRegEx": null, "year": 1996}, {"title": "A hebbian/anti-hebbian network for online sparse dictionary learning derived from symmetric matrix factorization", "author": ["Tao Hu", "Cengiz Pehlevan", "Dmitri B Chklovskii"], "venue": "Asilomar Conference on Signals, Systems and Computers, pages 613\u2013619. IEEE,", "citeRegEx": "HPC14", "shortCiteRegEx": null, "year": 2014}, {"title": "NIPS", "author": ["Daniel D Lee", "H Sebastian Seung. Unsupervised learning by convex", "conic coding"], "venue": "pages 515\u2013521,", "citeRegEx": "LS97", "shortCiteRegEx": null, "year": 1997}, {"title": "Nature", "author": ["Daniel D Lee", "H Sebastian Seung. Learning the parts of objects by non-negative matrix factorization"], "venue": "401(6755):788\u2013791,", "citeRegEx": "LS99", "shortCiteRegEx": null, "year": 1999}, {"title": "In NIPS", "author": ["Daniel D Lee", "H Sebastian Seung. Algorithms for non-negative matrix factorization"], "venue": "pages 556\u2013562,", "citeRegEx": "LS01", "shortCiteRegEx": null, "year": 2001}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research", "author": ["Bruno A Olshausen", "David J Field"], "venue": "37(23):3311\u20133325,", "citeRegEx": "OF97", "shortCiteRegEx": null, "year": 1997}, {"title": "A hebbian/anti-hebbian network derived from online non-negative matrix factorization can cluster and discover sparse features", "author": ["Cengiz Pehlevan", "Dmitri B Chklovskii"], "venue": "Asilomar Conference on Signals, Systems and Computers, pages 769\u2013775. IEEE,", "citeRegEx": "PC14", "shortCiteRegEx": null, "year": 2014}, {"title": "In NIPS", "author": ["Cengiz Pehlevan", "Dmitri Chklovskii. A normative theory of adaptive dimensionality reduction in neural networks"], "venue": "pages 2260\u20132268,", "citeRegEx": "PC15a", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimization theory of hebbian/antihebbian networks for pca and whitening", "author": ["Cengiz Pehlevan", "Dmitri B Chklovskii"], "venue": "arXiv preprint arXiv:1511.09468,", "citeRegEx": "PC15b", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse topical coding", "author": ["Jun Zhu", "Eric P Xing"], "venue": "arXiv preprint arXiv:1202.3778,", "citeRegEx": "ZX12", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": ", [AGKM12]) or tensors (e.", "startOffset": 2, "endOffset": 10}, {"referenceID": 6, "context": "[AKF12]), which are still not as widely used in practice primarily because of computational feasibility issues or sensitivity to assumptions on A and X.", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "Some others depend on specific structure of the feature matrix, such as separability [AGKM12] or similar properties [BGKP16].", "startOffset": 85, "endOffset": 93}, {"referenceID": 9, "context": "Some others depend on specific structure of the feature matrix, such as separability [AGKM12] or similar properties [BGKP16].", "startOffset": 116, "endOffset": 124}, {"referenceID": 2, "context": "In the first category, they make heavy structural assumptions on the feature matrix A\u2217 such as separability ([AGM12]) or allowing running time exponential in n ( [AGKM12]).", "startOffset": 109, "endOffset": 116}, {"referenceID": 1, "context": "In the first category, they make heavy structural assumptions on the feature matrix A\u2217 such as separability ([AGM12]) or allowing running time exponential in n ( [AGKM12]).", "startOffset": 162, "endOffset": 170}, {"referenceID": 6, "context": "In the second one, they impose strict distributional assumptions on x\u2217 ([AKF12]), where the methods are usually based on the method of moments and tensor decompositions and have poor tolerance to noise, which is very important in practice.", "startOffset": 72, "endOffset": 79}, {"referenceID": 3, "context": ", [AGMM15]) requires that the decodings are correct in all the intermediate iterations, in the sense that the supports of x\u2217 are recovered with no error.", "startOffset": 2, "endOffset": 10}, {"referenceID": 17, "context": "The area of non-negative matrix factorization (NMF) has a rich empirical history, starting with the practical algorithm of [LS97].", "startOffset": 123, "endOffset": 129}, {"referenceID": 1, "context": "On the theoretical side, [AGKM12] provides a fixed-parameter tractable algorithm for NMF, which solves algebraic equations and thus has poor noise tolerance.", "startOffset": 25, "endOffset": 33}, {"referenceID": 1, "context": "[AGKM12] also studies NMF under separability assumptions about the features.", "startOffset": 0, "endOffset": 8}, {"referenceID": 9, "context": "[BGKP16] studies NMF under heavy noise, but also needs assumptions related to separability, such as the existence of dominant features.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "Usually, \u2016x\u2217\u20161 = 1 while there also exist work that assume xi \u2208 [0, 1] and are independent [ZX12].", "startOffset": 91, "endOffset": 97}, {"referenceID": 0, "context": "For example, [AGH13] assume the topic-word matrix contains \u201canchor words\u201d: words which appear in a single topic.", "startOffset": 13, "endOffset": 20}, {"referenceID": 7, "context": "Most related is the work of [AR15] who analyze a version of the variational inference updates when documents are long.", "startOffset": 28, "endOffset": 34}, {"referenceID": 4, "context": "Results here typically are not robust to noise, with the exception of [AGMS12] that tolerates Gaussian noise.", "startOffset": 70, "endOffset": 78}, {"referenceID": 20, "context": "The original empirical work proposing such an algorithm (in fact, it suggested that the V1 layer processes visual signals in the same manner) was due to [OF97].", "startOffset": 153, "endOffset": 159}, {"referenceID": 3, "context": "A theoretical analysis came latter for dictionary learning due to [AGMM15] under the assumption that the columns of A\u2217 are incoherent.", "startOffset": 66, "endOffset": 74}, {"referenceID": 1, "context": "The problem in the worst case is NP-hard [AGKM12], so some assumptions are needed to design provable efficient algorithms.", "startOffset": 41, "endOffset": 49}, {"referenceID": 7, "context": "The condition on l means that a constant warm start is sufficient for our algorithm to converge, which is much better than previous work such as [AR15]: indeed, there l depends on the dynamic range of the entries of A\u2217 which is problematic in practice.", "startOffset": 145, "endOffset": 151}, {"referenceID": 0, "context": "References [AGH13] S.", "startOffset": 11, "endOffset": 18}, {"referenceID": 1, "context": "[AGKM12] Sanjeev Arora, Rong Ge, Ravindran Kannan, and Ankur Moitra.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[AGM12] S.", "startOffset": 0, "endOffset": 7}, {"referenceID": 3, "context": "[AGMM15] S.", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "[AGMS12] Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "[AHJK13] A.", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "[AKF12] A.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[AR15] Pranjal Awasthi and Andrej Risteski.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[BBK14] T.", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[BGKP16] Chiranjib Bhattacharyya, Navin Goyal, Ravindran Kannan, and Jagdeep Pani.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "[Ble12] David M Blei.", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "[BNJ03] David M Blei, Andrew Y Ng, and Michael I Jordan.", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[Com94] Pierre Comon.", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "[DRIS13] W.", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "[DRIS14] W.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[FJK96] Alan Frieze, Mark Jerrum, and Ravi Kannan.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[HPC14] Tao Hu, Cengiz Pehlevan, and Dmitri B Chklovskii.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[LS97] Daniel D Lee and H Sebastian Seung.", "startOffset": 0, "endOffset": 6}, {"referenceID": 18, "context": "[LS99] Daniel D Lee and H Sebastian Seung.", "startOffset": 0, "endOffset": 6}, {"referenceID": 19, "context": "[LS01] Daniel D Lee and H Sebastian Seung.", "startOffset": 0, "endOffset": 6}, {"referenceID": 20, "context": "[OF97] Bruno A Olshausen and David J Field.", "startOffset": 0, "endOffset": 6}, {"referenceID": 21, "context": "[PC14] Cengiz Pehlevan and Dmitri B Chklovskii.", "startOffset": 0, "endOffset": 6}, {"referenceID": 22, "context": "[PC15a] Cengiz Pehlevan and Dmitri Chklovskii.", "startOffset": 0, "endOffset": 7}, {"referenceID": 23, "context": "[PC15b] Cengiz Pehlevan and Dmitri B Chklovskii.", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "[ZX12] Jun Zhu and Eric P Xing.", "startOffset": 0, "endOffset": 6}, {"referenceID": 18, "context": "The area of non-negative matrix factorization (henceforth NMF) has a rich empirical history, starting with the work of [LS99].", "startOffset": 119, "endOffset": 125}, {"referenceID": 1, "context": "On the theoretical side, [AGKM12] provide a fixed-parameter tractable algorithm for NMF: namely when if the matrix A \u2208 Rm\u00d7nand X \u2208 Rn\u00d7N , they provide an algorithm that runs in time (mN).", "startOffset": 25, "endOffset": 33}, {"referenceID": 9, "context": "[BGKP16] studies the problem under heavy noise setting, but also needs assumptions related to separability, such as the existence of dominant features.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": ", [ZX12]), which is closely related to our model.", "startOffset": 2, "endOffset": 8}, {"referenceID": 24, "context": "However, there also exist work on topic modeling [ZX12] that do not restrict x\u2217 is assumed to have norm 1 and can satisfying our assumption.", "startOffset": 49, "endOffset": 55}, {"referenceID": 11, "context": "There is a rich body of empirical work on topic models, starting from the seminal work on LDA due to [BNJ03].", "startOffset": 101, "endOffset": 108}, {"referenceID": 18, "context": "Typically in empirical papers the matrices A\u2217, as well as the vectors x\u2217 are learned using variational inference, which can be interpreted as a kind of alternating minimization in KL divergence norm, and in the limit of infinite-length documents converges to the [LS99] updates ([AR15]).", "startOffset": 263, "endOffset": 269}, {"referenceID": 7, "context": "Typically in empirical papers the matrices A\u2217, as well as the vectors x\u2217 are learned using variational inference, which can be interpreted as a kind of alternating minimization in KL divergence norm, and in the limit of infinite-length documents converges to the [LS99] updates ([AR15]).", "startOffset": 279, "endOffset": 285}, {"referenceID": 2, "context": "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].", "startOffset": 60, "endOffset": 67}, {"referenceID": 0, "context": "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].", "startOffset": 68, "endOffset": 75}, {"referenceID": 5, "context": "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].", "startOffset": 88, "endOffset": 96}, {"referenceID": 13, "context": "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].", "startOffset": 98, "endOffset": 106}, {"referenceID": 14, "context": "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].", "startOffset": 108, "endOffset": 116}, {"referenceID": 8, "context": "From the theoretical side, there was a sequence of works by [AGM12],[AGH13], as well as [AHJK13], [DRIS13], [DRIS14] and [BBK14].", "startOffset": 121, "endOffset": 128}, {"referenceID": 2, "context": "For example, [AGM12] and [AGH13] assume that the topic-word matrix contains \u201canchor words\u201d.", "startOffset": 13, "endOffset": 20}, {"referenceID": 0, "context": "For example, [AGM12] and [AGH13] assume that the topic-word matrix contains \u201canchor words\u201d.", "startOffset": 25, "endOffset": 32}, {"referenceID": 5, "context": "[AHJK13] on the other hand work with a certain expansion assumption on the word-topic graph, which says that if one takes a subset S of topics, the number of words in the support of these topics should be at least |S|+ smax, where smax is the maximum support size of any topic.", "startOffset": 0, "endOffset": 8}, {"referenceID": 7, "context": "Finally, in the paper [AR15] a version of the standard variational inference updates is analyzed in the limit of infinite length documents.", "startOffset": 22, "endOffset": 28}, {"referenceID": 7, "context": "However, [AR15] requires quite strong assumptions on both the warm start, and the amount of \u201cnon-overlapping\u201d of the topics in the topic-word matrix.", "startOffset": 9, "endOffset": 15}, {"referenceID": 12, "context": "The classic papers [Com94] and [FJK96] solved this problem in the noiseless case, with an approach based on cumulants, and [AGMS12] solved it in another special case, when the noise \u03b7 is Gaussian (albeit with an unknown covariance matrix).", "startOffset": 19, "endOffset": 26}, {"referenceID": 15, "context": "The classic papers [Com94] and [FJK96] solved this problem in the noiseless case, with an approach based on cumulants, and [AGMS12] solved it in another special case, when the noise \u03b7 is Gaussian (albeit with an unknown covariance matrix).", "startOffset": 31, "endOffset": 38}, {"referenceID": 4, "context": "The classic papers [Com94] and [FJK96] solved this problem in the noiseless case, with an approach based on cumulants, and [AGMS12] solved it in another special case, when the noise \u03b7 is Gaussian (albeit with an unknown covariance matrix).", "startOffset": 123, "endOffset": 131}, {"referenceID": 20, "context": "The original empirical work which proposed an algorithm like this (in fact, it suggested that the V1 layer processes visual signals in the same manner) was due to [OF97].", "startOffset": 163, "endOffset": 169}, {"referenceID": 3, "context": "([PC15a, PC15b, HPC14, PC14]) A theoretical analysis of it came latter due to [AGMM15].", "startOffset": 78, "endOffset": 86}, {"referenceID": 3, "context": "However, the assumption made in [AGMM15] is that the columns of A\u2217 are incoherent, which means that they have l2 norm bounded by 1, and inner products bounded by O( 1 \u221a m ).", "startOffset": 32, "endOffset": 40}, {"referenceID": 3, "context": "Additionally, the incoherence assumptions on the matrix A\u2217 used in [AGMM15], in the case when A\u2217 needs to be non-negative and has l1 column-wise norm would effectively imply that the columns of A\u2217 have very small overlap.", "startOffset": 67, "endOffset": 75}], "year": 2016, "abstractText": "Non-negative matrix factorization is a popular tool for decomposing data into feature and weight matrices under non-negativity constraints. It enjoys practical success but is poorly understood theoretically. This paper proposes an algorithm that alternates between decoding the weights and updating the features, and shows that assuming a generative model of the data, it provably recovers the ground-truth under fairly mild conditions. In particular, its only essential requirement on features is linear independence. Furthermore, the algorithm uses ReLU to exploit the nonnegativity for decoding the weights, and thus can tolerate adversarial noise that can potentially be as large as the signal, and can tolerate unbiased noise much larger than the signal. The analysis relies on a carefully designed coupling between two potential functions, which we believe is of independent interest.", "creator": "LaTeX with hyperref package"}}}