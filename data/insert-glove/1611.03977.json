{"id": "1611.03977", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "A Review on Algorithms for Constraint-based Causal Discovery", "abstract": "Causal h\u00fcrriyet discovery ljubi\u0161a studies the problem of cattanach mining edlife causal cheeked relationships between variables from data, which is theatergoing of primary fuel-injected interest 290 in native-born science. miedema During vulnificus the agerskov past decades, mikitenko significant agassi amount of progresses gudbrandsdalen have been made b\u00f6rse toward behra this fundamental 23-28 data bursera mining paradigm. blogspot.com Recent waldensian years, as the availability of jaga abundant sauerbraten large - sized trodden and u2 complex koyambedu observational data, the modernised constrain - nestled based approaches thunder have dilman gradually garroting attracted a parkway lot .689 of stechford interest utf-8 and guin\u00e9e have been nursing widely saca applied to mrfm many otisville diverse sexting real - sitars world problems plessur due to skitters the eustathios fast formula running benefaction speed missles and et/pt easy generalizing 46.39 to k\u00f6ln the problem 14.31 of causal insufficiency. 74.39 In 1079 this paper, bounces we nippon-ham aim motru to review staveley the krusen constraint - simorgh based crypto causal bunjevci discovery algorithms. timesharing Firstly, we discuss narwa the learning umaine paradigm of wattana the constraint - globecast based nucleation approaches. Secondly roselli and akerlof primarily, minugua the (801) state - of - the - http://web.coxnews.net art helldorado constraint - based casual inference wind-up algorithms carpeted are surveyed with tobia the detailed gentlest analysis. Thirdly, several zeitels related qt open - source gilberte software packages and benchmark biddeford data repositories goldfine are albin briefly gammon summarized. As c\u0113sis a 3.15 conclusion, hawkesworth some open problems in pyote constraint - khazanah based causal kuspit discovery westmeath are 1.3-percent outlined for returnable future research.", "histories": [["v1", "Sat, 12 Nov 2016 09:25:38 GMT  (241kb)", "http://arxiv.org/abs/1611.03977v1", null], ["v2", "Thu, 24 Nov 2016 22:33:25 GMT  (0kb,I)", "http://arxiv.org/abs/1611.03977v2", "This paper has been withdrawn by the author due to further improvement"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kui yu", "jiuyong li", "lin liu"], "accepted": false, "id": "1611.03977"}, "pdf": {"name": "1611.03977.pdf", "metadata": {"source": "CRF", "title": "A Review on Algorithms for Constraint-based Causal Discovery", "authors": ["Kui Yu", "Jiuyong Li"], "emails": ["Lin.Liu}@unisa.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n03 97\n7v 1\n[ cs\n.A I]\n1 2\nN ov\n2 01\n6 1\nIndex Terms\u2014Causal discovery, Causal sufficiency, Bayesian networks, Directed acyclic graph, Maximal ancestral graph\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "Discovering causal relationships between variables from data is fundamental in all areas of sciences, such as computer science, medicine, statistics, economy, and social sciences [29, 44, 52, 69, 74, 90, 114]. Causal discovery from observational data has been widely accepted as the best alternative to randomized controlled experiments [44, 53, 83, 117], since observational data can often be collected cheaply and is abundant, while in many settings, randomized controlled experiments are unethical or impossible [38, 70, 82]. Over the last two decades, many algorithms have been developed using observational data to infer causal relations, particularly in the area of graphical causal modeling [34, 44, 74, 87]. The most frequently used causal models belong to two broad families: (1) causal Bayesian networks [18, 72, 87], and (2) structural equation models (functional causal models) [26, 107].\nIn this paper, we focus on reviewing the algorithms for casual discovery using causal Bayesian networks. The algorithms for learning Bayesian networks can be mainly grouped in two categories: score-based and constraintbased algorithms. Score-based algorithms assign a score to each candidate Bayesian network for measuring how well the candidate Bayesian network fits a data set [15, 21]. Constraint-based algorithms learn Bayesian networks with conditional independence tests through analyzing the probabilistic relations entailed by the Markov property of Bayesian networks [72, 74].\nThe main advantage of the constraint-based algorithms over the score-based methods are illustrated as follows. Firstly, a constraint-based algorithm is easier to generalize to the situation where the observed variables are not causally sufficient than a score-based method. Causal sufficiency\nassumes that there are no latent common causes, for two or more of the observed variables in the underlying datagenerating process. Without assuming casual sufficiency, the score-based methods need to pre-determine the number of latent variables and the exact locations of the latent variables with the observed variables in data before learning starts [85]. However, these knowledge is hard to be achieved in advance. The constraint-based algorithms can deal with the problem without those knowledge in advance. Secondly, a constraint-based algorithm is much faster than a scorebased method [14]. The computational intractability is a key drawback of the score-based methods [18].\nWith the exponential growth of electronically accessible information (i.e. big data) in recent year, abundant largesized and complex observational data provide critical challenges in casual discovery. Recent years, due to the advantage mentioned above, the constraint-based algorithms have opened up new possibilities to develop scalable and accurate causal discovery algorithms to deal with largesized, complex, and high-dimensional data, especially in the situation of local learning, multiple data sets, and the violation of casual sufficiency. Therefore, the constraintbased approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].\nAlthough some reviews on causal inference have been proposed [8, 25, 45, 58, 73, 88], those reviews attempt to introduce types of causal discovery methods in a conceptual level or limited algorithmic discussion. There is no paper for comprehensive reviewing the constraint-based algorithms. In this paper, we focus on the constraint-based algorithms that are categorized in Figure 1. There are two research lines for constraint-based causal inference. One is to learn\n2\na DAG (directed acyclic graph) to represent casual relations between variables under the causal sufficiency assumption. In this line, there includes the following approaches to discover causal relationships.\n\u2022 Global approach. The approach learns a whole skeleton involving all variables in a data set, then learn the causal structure (represented by a completed partially directed acyclic graph, CPDAG) from the skeleton. \u2022 Local-to-global approach. The approach first mines a local skeleton around each variable, then constructs a global skeleton by integrating these local skeletons, finally, learns a CPDAG through the global skeleton. \u2022 Active-learning approach. This approach only focuses on edge orientations. It first learns a whole CPDAG using the global or local-to-global method, then uses the experimental data to orient the remaining undirected edges in the CPDAG. \u2022 Local causal discovery. Instead of discovering the whole causal structure, this approach only studies direct causal relationships around a given target variable.\nWith assuming causal sufficiency, the idea behind learning a DAG from multiple data is that we first discover a sub CPDAG from each data set, then integrate those CPDAG to obtain a causal model to fit all data sets.\nThe second research line is to learn a MAG (maximal ancestral graph) [81] to represent casual relations when causal sufficiency is not assumed. When dealing with a single data set, all developed constraint-based methods attempt to learn the whole MAG (i.e. the global approach). As to handle multiple data sets, all algorithms adopt a style of the local-to-global approach, that is, first use an existing learning algorithm to learn a sub MAG for each data set, then design an effective strategy, such as conflict resolution in discovered (in)dependence constraints, to integrate those sub MAGs to achieve a causal model to fit all data sets."}, {"heading": "2 LEARNING PARADIGM", "text": "In the section, we will discuss (1) learning framework in Section 3.1, (2) key assumptions in constraint-based learning in Section 3.2, (3) conditional independence test in Section 3.3, (4) evaluation metrics in Section 3.4, and (5) key challenges in Section 3.5. Table 1 summarizes some mathematical notations frequently used in this paper."}, {"heading": "2.1 Learning Framework", "text": "Given an observational data set D defined on a variable set V , the constraint-based algorithms consists of three key steps: (1) uncovering the entire skeleton, (2) discovering vstructures, and (3) orienting edges as many as possible, as shown in Figure 2.\nStep 1: Skeleton identification. There are two approaches employed in skeleton identification, global approach and local approach. The global approach attempts to discover the skeleton including all variables [13, 90, 90, 106], while the local method mines local skeletons, i.e., the set of adjacent variables (parents and children) or Markov blanket of each vertex, then constructs a global skeleton by local skeletons [65, 75].\nStep 2: V-structure discovery. This identifies v-structures by the found separation sets (i.e. conditional sets) between variables.\nStep 3: Edge orientation After obtaining v-structures, the main orientation methods are listed as the followings.\n\u2022 The orientation rules (i.e. Meek rules and Zhang Jiji\u2019s rules) defined in [67, 118] using observational data. \u2022 Experimental orientation rules by using experimental data. The method involves manipulating a variable and assessing its statistical association with the undirected neighbors in order to determine the orientation [22, 97]. \u2022 Edge orientation using both observational data and experimental data, which first applies the rules in (1) to orient edges as many as possible, then orients the remaining unoriented edges with the method in (2) [37].\n3"}, {"heading": "2.2 Key Assumptions.", "text": "There are three key assumptions are assumed to develop and analyze a constraint-based algorithm.\n\u2022 Assumption 1: causal sufficiency. The set V (observed variables) satisfies the causal sufficiency assumption. \u2022 Assumption 2: faithfulness and Markov condition. The distribution P over V is faithful to a DAG of the causal structure (faithfulness) and satisfies the Markov condition. \u2022 Assumption 3: reliable independence tests. The statistical tests employed by algorithms are correct and reliable."}, {"heading": "2.3 Conditional Independence Tests", "text": "In constraint-based algorithms, the independence tests generally can be implemented using the G2 test [1], mutual information [59, 108], and Fisher\u2019s Z-test [77]. The the G2 test and mutual information are both for dealing with discrete (categorical) data, while the Fisher\u2019s Z-test for handling continuous (numeric) data (The details of these tests are given in Appendix)."}, {"heading": "2.4 Evaluation Metrics", "text": "The frequently usedmetrics to evaluate the constraint-based algorithms are summarized as the following.\n\u2022 Missing edges. The number edges that are present in the original structure but are missing in the learned structure. \u2022 Extra edges. The number of edges that are found in the learned structure but are not present in the original structure. \u2022 Correct undirected edges. The number of undirected edges that are present both in the original structure and the learned structure. \u2022 Correct directed edges. The number of directed edges that are present both in the original structure and the learned structure with the same orientations. \u2022 Correct directed edges. The number of directed edges in the learned structure that are oriented correctly. \u2022 Incorrect directed edges. directed edge in the learned structure that are oriented incorrectly. \u2022 Computational efficiency. There are two metrics always used, run times and number of statistical tests."}, {"heading": "2.5 Key Challenges", "text": "Reliability. (1) By performing a reliable conditional independence test between Vi and Vj given Vk, the average number of instances per cell of the contingency table of {Vi, Vj}\u222aVk must be at least \u03d5, i.e.,N/((ri\u22121)(rj\u22121)rk) \u2265 \u03d5 (N is the total number of instances, ri is the number of distinct values of Vi, and \u03d5 is often set to 5 or 10) [1]. Thus, independence tests of high order can be unreliable unless the size of the sample is huge. Unreliable tests not only produce errors in the resulting causal model structure, but also produce cascading errors. That is to say, an error early on in the search can have a cascading effect that causesmany errors to be present in the final graph. (2) Constraint-based\nmethods suffer the problem of multiple testing, that is, type I error and type II error, and the multiple testing is sensitive to error propagation. (3) Using G2 test, Fisher\u2018s Z-test, or mutual information, we need to choose a threshold (or the significance level) to decide whether the independence test is passed or not. It is hard to select a suitable threshold or significance level, thus, the resulting networks are not easily evaluated in terms of reliability [51].\nScalability. Learning the whole causal network which includes all variables involved in data is computational expensive or even infeasible in large-scale data mining applications with thousands of variables, thus it is challenging to develop algorithms scaling up to real-world data with high dimensionality [86].\nContradictory independence constraints. The conflict (in)dependences discovered from multiple data sets are common in constraint-based algorithms. For example, Vi is a parent of Vj in one DAG but a child of Vj in another DAG, resulting from statistical test errors or when the input samples are not identifically distributed. It is challenging to solve the conflicts from different data sets to result in a consistent casual structure.\nViolation of the faithfullness assumption.The Faithfullness assumption is a key assumption to design causal structure learning methods, but in many real-world applications, such as biomedical science [92] and climate research [115, 116], the assumption is often violated. Thus it is challenging to design robust causal inference algorithms by relaxing this assumption [119].\nViolation of the causal sufficiency assumption. In realworld applications, such as medical science, epidemiology, and sociology, it is impossible to ensure that all common causes are measured in study data [17]. Thus, it is a challenging task to deal with causal inference without assuming causal sufficiency."}, {"heading": "3 LEARNING WITH CAUSAL SUFFICIENCY", "text": ""}, {"heading": "3.1 Problem Definition and Algorithm Overview", "text": "Problem Definition. Given an observational data set D defined on a variable set V and its underlying DAG G, under the assumptions of the faithfullness and causal sufficiency, the constraint-based search algorithms learn a CPDAG ofG.\nAlgorithm Overview. Table 2 gives an overview of constraint-based algorithms under the assumptions of causal sufficiency and faithfullness. In Table 2, \u201cglobal\u201d means that an algorithm uses a global search strategy to find a whole skeleton while \u201clocal\u201d represents that an algorithm employ a local discovery approach to discover skeletons. In the table, a tick mark indicates that the main modification of the corresponding algorithm, and \u201c-\u201d denotes the step has not changes. In Table 2, except for SLPR, the other algorithms all deal with a single data sets."}, {"heading": "3.2 Global algorithm", "text": ""}, {"heading": "3.2.1 SGS Algorithm", "text": "Description of SGS. The SGS (SGS stands for Spirtes, Glymour and Scheines who invented this algorithm) algorithm is one of the earliest constraint-based algorithms proposed\n4\nby Spirtes et. al. [90], which provides a theoretical framework for learning structures of causal models. Algorithm 1 gives the description of the idea of the SGS algorithm. In Algorithm 1, if there exists a subset S \u2286 V \\{Vi, Vj} to make the term Vi \u22a5 Vj |S hold, the set S is called a separation set of Vi \u2208 V and Vj \u2208 V .\nAlgorithm 1: The SGS Algorithm\nInput: Data set D with a set of variables V Output: CPDAG /*Step 1: Skeleton discovery*/ (i) Form the complete undirected graph UG on the vertex set V. (ii) For each pair of variables Vi and Vj in UG, if there exists a subset S \u2286 V \\{Vi, Vj}, such that the term Vi \u22a5 Vj |S holds, then remove the edge between Vi and Vj from UG. /*Step 2: V-structure identification*/ Let UG1 be undirected graph resulted from Step 1. (i) In the UG1, for each pair of non-adjacent variables Vi and Vj with a common neighbor Vk (i.e. Vi \u2212 Vk \u2212 Vj), check if Vk \u2208 S where S is the separation set of Vi and Vj . If so, then continue Step 2; (ii) otherwise, discover a v-structure, i.e., Vi \u2192 Vk \u2190 Vj . /*Step 3: Edge orientation*/ repeat\n/*Rule 1: avoiding yielding a new v-structure/* if Vi \u2192 Vj , Vj and Vk are adjacent, Vi and Vk are not adjacent, and there is no arrowhead at Vj , then orient Vj \u2212 Vk as Vj \u2192 Vk; /*Rule 2: avoiding generating a directed cycle/* If there is a directed path from Vi to Vj , and an edge between Vi and Vj , then orient Vi \u2212 Vj as Vi \u2192 Vj .\nuntil no more edges can be oriented; Output CPDAG\nIn the original version of the SGS algorithm [90], at the step of edge orientation, Rule 1 is used to avoid any alternative orientation would yield a new v-structure, while Rule\n2 is employed to avoid any alternative orientations would generate a directed cycle. An orientation rule is sound if and only if any orientation other than the orientation indicated by the rule would lead to a new unshielded collider or a directed cycle [67]. Later, Meek [67] extended the rules by the additional two rules (noted as Rule 3 and Rule 4) which are summarized as the following.\n\u2022 Rule 1. If Vi \u2192 Vj , Vj and Vk are adjacent, Vi and Vk are not adjacent, and there is no arrowhead at Vj , then orient Vj \u2212 Vk as Vj \u2192 Vk; \u2022 Rule 2. If there is a directed path from Vi to Vj , and an edge between Vi and Vj , then orient Vi \u2212 Vj as Vi \u2192 Vj . \u2022 Rule 3. If there are two chains, Vi \u2212 Vk \u2192 Vj and Vi \u2212 Vh \u2192 Vj such that Vk and Vh are not adjacent, then orient Vi \u2212 Vj as Vi \u2192 Vj . \u2022 Rule 4. If there are two chains, Vi \u2212 Vk \u2192 Vh and Vk \u2192 Vh \u2192 Vj , Vk such that Vj are not adjacent and Vi and Vh are adjacent, then orient Vi\u2212Vj as Vi \u2192 Vj .\nMeek [67] proved that these four rules are sufficient in terms of the soundness and completeness (please refer to Theorem 2 and Theorem 3 in Section 2 in [67]). In other words, given a correct skeleton and v-structures of a DAG G, the repeated application of the four rules will eventually orient all arrows that are common to the Markov equivalence class of G.\nRational and correctness of SGS. Based on the Assumptions 1 to 2 mentioned above, the rational of the SGS algorithm is validated by the following three properties of d-separation [87].\n\u2022 Vi and Vj are adjacent in DAG G if and only if \u2200S \u2286 V \\ {Vi, Vj}, Vi is not d-separated from Vj conditioned on S. \u2022 If < Vi, Vk, Vj > is a v-structure in DAG G, then \u2200S \u2286 V \\ {Vi, Vj}, Vi is not d-separated from Vj conditioned on S that contains Vk.\n5 \u2022 If < Vi, Vk, Vj > (i 6= j 6= k) is not a v-structure in DAGG, then \u2200S \u2286 V \\{Vi, Vj}, Vi is not d-separated from Vj conditioned on S that does not contain Vk.\nTheorem 3.1. [90] Given the distribution P over set of variables V and its corresponding DAG G, under Assumptions 1 to 3, the output of the SGS algorithm is the CPDAG that represents G.\nDrawback of SGS. By Theorem 3.1, to determine whether Vi and Vj are adjacent, SGS needs to test Vi \u22a5 Vj |S for all possible subsets S \u2286 V \\{Vi, Vj}. Such an exhaustive search quickly becomes computationally infeasible for a large number of variables. Besides the problem of computational feasibility, the algorithm suffers the problem of reliability when the number of data instances is small."}, {"heading": "3.2.2 PC algorithm", "text": "The PC (PC stands for Peter Spirtes and Clark Glymour who invented this algorithm) algorithm [89, 90] was proposed to improve learning efficiency in skeleton identification (Step 1 in Algorithm 1). Instead of checking every possible conditioning set at every order of conditioning, the PC algorithm tests only conditioning sets involving variables that are adjacent to the variables under testing. The main idea of the PC algorithm in skeleton identification is summareized as the following.\n\u2022 The PC algorithm starts by a complete undirected graph, then refines the graph by removing edges with an empty condition set for independence tests. If a pair of variables is found to be independent, the edge between them is removed. \u2022 Next, for each pair of nodes (Vi,Vj) that are still adjacent, it tests conditional independence of the corresponding variables conditioned on all possible subsets of size 1 of adj(Vi, G\n\u2217)\\Xj and of adj(Vj , G \u2217)\\Vi, where G \u2217 is the current graph and adj(Vi, G \u2217) represents the variables adjacent to Vi in G\u2217. The edge will be removed if such a conditional independence is concluded to be true. \u2022 The PC algorithm continues in this way by performing level by level of the size of conditioning sets, until the size of the current conditioning set is larger than the size of the adjacency sets of the variables.\nThis procedure gives the correct skeleton when using perfect conditional independence information (the tests are reliable and the data instances are enough large). The PC algorithm uses the same procedure as the SGS algorithm to identify v-structures. After applying the edge orientation rules, i.e., Rules 1 to 4, the output of the PC algorithm is a CPDAG. The complexity of the algorithm for a DAG G is bounded by the largest degree over all pairs of vertices inG. Let k be the maximal order of the conditional independence relations that need be tested, the computational requirements increase exponentially with k, as the the number of vertices becomes large.\nRational and correctness of PC. The PC algorithm can significantly improve the computational feasibility of SGS using Theorem 3.2 below.\nTheorem 3.2. [2, 90] Under Assumptions 1 to 2, in a DAG G, there is an edge between the pair of variables Vi \u2208 V and\nVj \u2208 V , if and only if Vi \u22a56 Vj |S, for all S \u2286 pa(Vi)\\{Vj} and S \u2286 pa(Vj)\\{Vi}.\nTheorem 3.3. [89, 90] Under Assumptions 1 to 3, the PC algorithm gives the same output as the SGS algorithm.\nConsistency of the PC Algorithm. Whether causal effects can be consistently estimated from observational studies alone is a central problem in causal inference. As a pioneer work, under the framework in which causal relationships are represented by edges in a DAG, Spirtes et.al. [87] and Pearl [74] described asymptotically consistent procedures for determining features of causal structure from data under Assumptions 1 and 2 in Section 3.2. Spirtes et.al. [87] presented a pointwise consistent estimator of the Markov equivalence class of any causal structure that can be represented by a DAG for any parametric family with a uniformly consistent test of conditional independence under Assumptions 1 and 2. Kalisch and Bu\u0308hlmann [42] showed the PC-algorithm is asymptotically consistent for the equivalence class of the DAG and its skeleton with corresponding high-dimensional and sparse Gaussian distribution. They proved that the uniform consistency of the PC algorithm for high-dimensional and sparse DAGs, when the number of variables is allowed to quickly grow with sample size N , as fast as O(Na) for any 0 \u2264 a \u2264 \u221e. Harris and Drton [36] studied the PC algorithm in the nonparanormal distributions, i.e., continuous distributions with Gaussian copula, and employed the rank-based partial correlation estimates for conditional independence tests. The study presented that the PC algorithm has high-dimensional consistency properties when the observations follow a nonparanormal distribution and the rank-based measures of correlation are used to assess conditional independence."}, {"heading": "3.2.3 Conservative SGS and PC algorithms", "text": "In this section, we will review some variants of SGS and PC by relaxing the assumption of faithfullness, i.e., Assumption 2. To the beginning, we introduce two consequences of the faithfulness assumption proposed by [80], i.e., the Adjacency-Faithfulness assumption and the OrientationFaithfulness assumption, that justify the step of inferring adjacency and the step of inferring edge orientations, respectively1.\nDefinition 3.1 (Adjacency-Faithfulness assumption). Given a set of variables V in DAG G, if two variables Vi \u2208 V and Vj \u2208 V are adjacent in G, then \u2200S \u2286 V \\ {Vi, Vj}, Vi \u22a5 Vj |S holds.\nDefinition 3.2 (Orientation-Faithfulness assumption). Given a set of variables V in DAG G, let < Vi, Vk, Vj > be any unshielded triple in G:\n\u2022 If Vi \u2192 Vk \u2190 Vj exists, Vi and Vj are not independent given \u2200S \u2286 V \\ {Vi, Vj} that contains Vk; \u2022 Otherwise, Vi and Vj are not independent given \u2200S \u2286 V \\ {Vi, Vj} that does not contain Vk.\nUnder the Adjacency-Faithfulness assumption, any edge removed in the step of skeleton identification is correctly\n1. The Adjacency-Faithfulness and the Orientation-Faithfulness both are consequences of the faithfulness assumption, but they together do not imply the faithfulness assumption [119].\n6 removed. If the Orientation-Faithfulness assumption is true, for any unshielded triple < Vi, Vk, Vj >, we need check only the separation set S found in the step of skeleton identification that renders Vi and Vj independent. The triple is a v-structure if and only if the set S does not contain Vk [80, 119]. The SGS and PC algorithms both adopt the approach in Step 2 for inferring v-structures.\nConservative PC algorithm. The CPC (Conservative PC) algorithm was proposed by Ramsey et. al. [80] for improving the robustness of the PC algorithm in the orientation phase. Assuming the Adjacency-Faithfulness condition is true, Ramsey et al. [80] extended the PC algorithm to capture violations of the Orientation-Faithfulness assumption. If the Orientation-Faithfulness does not hold in the step of vstructure discovery, the CPC algorithm is able to avoid drawing certain wrong conclusions that the PC algorithm would draw.\nTo improve the robustness of Step 2 for the v-structure discovery in PC, the CPC algorithm replaces this step in the PC algorithm with the Step 2\u2019 as the following.\nStep 2\u2019 in CPC. Let G\u2032 be the graph skeleton resulting from Step 1 in PC. For each unshielded triple< Vi, Vk, Vj >, check all subsets of Vi\u2019s potential parents and of Vj \u2019s potential parents:\n\u2022 If Vk is not in any such set conditioned on which Vi and Vj are independent, orient Vi \u2212 Vk \u2212 Vj as Vi \u2192 Vk \u2190 Vj ; \u2022 If Vk is in all such sets conditioned on which Vi and Vj are independent, leave Vi \u2212 Vk \u2212 Vj as it is, i.e., a non v-structure; \u2022 Otherwise, mark the triple as \u201cunfaithful\u201d by underlining the triple, Vi\u2212Vk\u2212Vj . An unfaithful triple is a triple which we cannot qualify as being a v-structure or a Markov chain.\nIn the step of edge orientation (Step 3), the CPC algorithm does not apply the orientation rules to the ambiguous triples. The output of the CPC-algorithm is a partially directed graph in which ambiguous triples are marked. The rational of the CPC algorithm is based on Theorem 3.4.\nTheorem 3.4. [119] Assuming the causal Markov condition and the Adjacency-Faithfulness condition hold, any violation of the Orientation-Faithfulness condition is detectable.\nVery Conservative SGS Algorithm. Zhang and Spirtes [119] proved that given the Causal Markov, Minimality, and Triangle-Faithfulness assumptions, any violations of faithfulness are detectable. They concluded that the Triangle-Faithfulness Condition is a consequence of the Adjacency-Faithfulness condition and is strictly weaker than the latter.2 Then Spirtes and Zhang [91] proposed the VCSGS algorithm under those two weaker assumptions of faithfullness. Compared to the CPC algorithm, the VCSGS algorithm relaxes both the Adjacency-Faithfulness assumption and the Orientation-Faithfulness assumption.\nSpirtes and Zhang [91] only analyzed the soundness of the VCSGS algorithm in theory without any experimental\n2. More explanations and examples for the Causal Minimality assumption, Triangle-Faithfulness Condition, and the AdjacencyFaithfulness condition, please refer to the work [119].\nvalidation, and they theoretically described a uniformly consistent estimator of both the Markov equivalence class of a linear Gaussian causal structure and the identifiable structural coefficients in the Markov equivalence class under the Markov assumption and the Triangle-Faithfulness assumption.\nAdjacency Conservative PC algorithm. In practice, as an another extention of the CPC algorithm by relaxing the Adjacency-Faithfulness assumption, Lemeire et. al. [50] proposed two specific violations of the Adjacency-Faithfulness assumption, i.e., pseudo-independent relations and equivalent edges. Both PC and CPC algorithm fail to detect those two violations in skeleton identification. By relaxing the assumption from the Adjacency-Faithfulness assumption to the Triangle-Faithfulness assumption, Lemeire et. al. [50] developed the Adjacency Conservative PC (ACPC) algorithm to deal with the two violations. Their experimental results illustrated that the ACPC algorithm is superior to the PC and CPC algorithms."}, {"heading": "3.2.4 PC-stable algorithm", "text": "The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional data. However, the order-dependence can become very severe in data with high-dimensionality and a small sample size, and lead to highly variant results and conclusions for different variable orderings [17].\nPC-stable algorithm. Colombo and Maathuis [17] proposed the PC-stable algorithm and its variants to address this order-dependence problem. The main difference between PC and PC-stable lies in the skeleton discovery step. Compared to the PC algorithm, at each level of d (the size of the current conditioning set), PC-stable records which edges should be removed, but for keeping the adjacency sets unchanged for other pairs of variables at this level of d, PC-stable removes these edges only when it goes to the next value of d. These stored adjacency sets are used whenever we search for conditioning sets of this given size. Consequently, an edge deletion no longer affects which conditional independencies are checked for other pairs of variables at this level of d.\nUnder Assumptions 1 to 3, PC-stable is sound and complete, and yields order-independent skeletons by the following theorems. However, the PC-stable algorithm requires more independence tests and therefore results in even longer running time than the PC algorithm.\nTheorem 3.5. [17] Let the distribution of P be faithful to a DAG G, and assume that we are given perfect conditional independence information about all pairs of variables (Fi, Fj) in P given subsets S \u2286 F \\{Fi\u222aFj}. Then the output of the PC-stable algorithm is the CPDAG that represents G.\nTheorem 3.6. [17] The skeleton resulting of the PC-stable algorithm is order-independent.\nCPC/MPC stable algorithm. The CPC-stable algorithm [17] is the combination of the PC-stable algorithm and the CPC algorithm, that is, using the idea of the CPC algorithm to identify v-structures. Since the CPC algorithm is very conservative, in the sense that very few unshielded\n7 triples are unambiguous in real-world data, Colombo and Maathuis [17] proposed the majority rule PC-stable algorithm (MPC-stable). As in CPC-stable, we first determine all separating sets Z of adj(UG,Fi) and of adj(UG,Fk) satisfying Fi \u22a5 Fk|Z where UG is the skeleton computed from Step 1. We then label the triple < Fi, Fj , Fk > as unambiguous if at least one such separating set is found and Fj is not in exactly 50% of those separating sets. Otherwise it is labeled as ambiguous. Clearly, we can also use different cut-offs to declare ambiguous and non-ambiguous triples. If a triple is unambiguous, it is oriented as v-structure if and only if Fj is in less than half of the separating sets. As the same as CPC-stable, the orientation rules (Step 3) are adapted so that only unambiguous triples are oriented, and the output is a partially directed graph in which ambiguous triples are marked.\nTheorem 3.7. [17] Let the distribution of P be faithful to a DAG G, and assume that we are given perfect conditional independence information about all pairs of variables (Fi;Fj) in P given subsets S \u2286 F \\ {Fi \u222a Fj}. Then the output of the CPC/MPC-stable algorithm is the CPDAG that represents G.\nTheorem 3.8. [17] The decisions about v-structures in the sample versions of the CPC/MPC-stable algorithms are order-independent."}, {"heading": "3.2.5 Parallel PC algorithm", "text": "It is known that in the worst case, the PC algorithm is exponential to the number of variables, and thus it is computational inefficiency when applying to a high dimensional data set involved thousands of variables. The work [47, 48] proposed the parallelized versions of the PC algorithm and the PC-stable algorithm using the parallel computing technique, and showed that the parallel PC and PC-stable algorithms produce the same outputs as the PC and PCstable algorithms, but much more efficient in running time.\n3.2.6 PCfdr-skeleton algorithm In multiple hypothesis testing for learning skeletons (Step 2), a type I error occurs when the variables are independent but the test indicates that they are dependent, while a type II error occurs when the variables are dependent but the statistical test indicates independence. The false discovery rate (FDR) [10, 94] is a criterion to assess the errors when multiple hypotheses are simultaneously tested. It is the expected ratio of the number of falsely claimed positive results to that of all those claimed to be positive. Colombo and Maathuis [16] proposed the PCfdr-skeleton algorithm by embedding an FDR-control procedure into the PC algorithm to curb the error rate of the skeleton of the learned PDAGs.\nInstead of individually controlling the type I error rate of each hypothesis test, the FDR-control procedure considers the hypothesis tests together to correct the effect of simultaneously testing the existence of multiple edges. Since the PCfdr-skeleton algorithm built within the framework of the PC algorithm, the PCfdr-skeleton algorithm provides the PC algorithm with the ability to control the FDR over the skeleton of the recovered network. ThePCfdr-skeleton algorithm controls the FDRwhile the standard PC algorithm controls the type I error rate. It is not only applicable to both\nthe Gaussian model and any models for which conditionalindependence tests are available, such as discrete models.\nColombo and Maathuis proved that the PCfdr-skeleton algorithm can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level. The algorithm can be applicable to any models for which statistical tests of conditional independence are available."}, {"heading": "3.2.7 AIT Framework", "text": "Against the PCfdr-skeleton algorithm, Bromberg and Margaritis [12] proposed the AIT (Argumentative Independence Test) framework to improve the reliability of the constraintbased algorithms when independence tests on small data sets are not reliable. The AIT framework modeled the problem of unreliability of statistical independence tests as a knowledge base containing a set of independence facts that are related through Pearls well-known axioms [72], as shown as follows.\n\u2022 Symmetry. (X \u22a5 Y |Z) =\u21d2 (Y \u22a5 X |Z) \u2022 Decomposition. (X \u22a5 Y \u222aW |Z) =\u21d2 (X \u22a5 Y |Z) \u2227\n(X \u22a5 W |Z) \u2022 Weak Union. (X \u22a5 Y \u222aW |Z) =\u21d2 (X \u22a5 Y |Z \u222aW ) \u2022 Contraction. (X\u22a5 Y |Z)\u2227(X\u22a5 W |Z\u222aY ) =\u21d2 (X\u22a5\nY \u222aW |Z) \u2022 Intersection. (X \u22a5 Y |Z \u222aW )\u2227 (X \u22a5 W |Z \u222a Y ) =\u21d2\n(X \u22a5 Y \u222aW |Z)\nStatistical tests on finite data sets may result in errors in these tests and inconsistencies in the knowledge base. For example, consider an independence-based knowledge base that contains the following propositions, obtained through statistical tests on data.\n\u2022 P1: ({0} \u22a5 {1}|{2}) \u2022 P2: ({0} \u22a56 {3}|{2}) \u2022 P3: ({0} \u22a5 {3}|{1, 2})\nSuppose that P3 is in fact wrong. Such an error can be avoided if there exists a constraint involving these independence propositions. For example, suppose that we also know that the following rule holds in the domain (this is an instance of an application of the Contraction and Decomposition axioms:\nP4: ({0} \u22a5 {1}|{2}) \u2227 ({0} \u22a56 {3}|{2}) =\u21d2 ({0} \u22a56 {3}|{1, 2})\nP4, together with the independence proposition in P1 and the dependence proposition in P2, contradict the independence proposition in P3, resulting in an inconsistent knowledge base. If P4 and the propositions in P1 and P2 are accepted, then the proposition (3) in P3 must be rejected, correcting the error in this case.\nIn the AIT framework, these inconsistencies are solved through the argumentative independence test, which is a defeasible logic proposed by Amgoud and Cayrol [5] to reason about and correct errors. Thus, in the argumentation framework, the outcomes of independence tests are not themselves independent but are constrained by the outcomes of other tests through Pearls properties of the conditional independence relation. The constraints can correct certain\n8 inconsistent test outcomes and select the outcome that can be inferred by tests that do not result in contradictions. Experimental evaluation shows significant improvements in the accuracy of the argumentative independence test over other simple statistical tests (up to 13%), and improvements on the accuracy of the PC algorithms s (up to 20%).\nA disadvantage with this approach is that, as it is a propositional formalism, it requires to propositionalizing the set of rules of Pearl, which are first-order. As these are rules for super-sets and sub-sets of variables, its propositionalization involves an exponential number of propositions, and then, the exact argumentative algorithm proposed is exponential."}, {"heading": "3.2.8 TPDA algorithm", "text": "The TPDA (three phase dependency analysis) algorithm was proposed by Cheng et. al. [13]. Compared to the PC algorithm, TPDA employs mutual information for independence tests to learn graph skeletons from data and divides the structure learning process into three phases, i.e., drafting, thickening, and thinning. There are two versions of the TPDA algorithm, TPDA, which learns DAGs when a node ordering is not given, and TPDA-II, when the node ordering is given. When the correct node ordering is available, TPDAII requires only the standard DAG-faithfulness, while TPDA is correct under a sufficient quantity of training data and the monotone DAG faithfulness condition.\nInstead of starting by a complete undirected graph, TPDA begins with a \u201cdrafting\u201d phase, which produces an initial set of edges based on pairwise mutual information tests. Then, in the \u201cthickening\u201d phase, TPDA adds edges to the current graph when the pairs of nodes cannot be separated using a set of relevant independence tests. The graph produced by this phase will contain all the edges of the underlying dependency model when the underlying model is DAG-faithful. Next, in the \u201cthinning\u201d phase, each edge is examined and it will be removed if the two corresponding nodes are found to be conditionally independent. Finally, TPDA orients edges of the learned graph using Rules 1 to 4.\nAs for TPDA, for each pair of variables, each decision in the \u201cdrafting\u201d phase requires one independence test and each decision in \u201cthickening\u201d phase requires O(N2) independence tests, where N is the number of variables. The \u201cthinning\u201d then requires O(N2) independence tests tests to verify each edge proposed in these two phases. Thus, TPDA makes O(N2) correct decisions to produce the DAG structure, and each such decision requires O(N2) independence tests. In total, TPDA requires at most O(N4) independence tests to learn an DAGwithN variables. When a correct node ordering is given, the TPDA-II algorithm requiresO(N2) tests and is correct whenever the underlying model is DAG-faithful."}, {"heading": "3.3 Local-to-global approach", "text": "The main problem of the global constraint-based algorithms is their inefficiency and inaccuracy in performing conditional independence tests for large condition sets. For example, the PC and TPDA algorithms requires an exponentially growing number of independence tests with the number of nodes to learn a skeleton. Their time complexity can\nbe can be reduced to polynomial by fixing the number of parents (PC algorithm) or using a threshold to control the number of mutual information tests and some strong assumptions (TPDA algorithm). In addition, the TPDA algorithm is ignoring the problem of the curse-of-dimensionality by not limiting the size of the condition set. To reduce the conditional sets for testing independence, in the section, we review the causal discovery algorithms using a Local-toglobal learning approach."}, {"heading": "3.3.1 Decomposition approaches", "text": "Geng et. al [33] proposed a decomposition method for simplifying the search for v-structures for efficient structure learning. The key idea is that given an undirected moral graph, they first decomposed it into several subgraphs, then search for v-structures in each subgraphs separately. The rational behind the idea is that if vertices Vi and Vj are adjacent in the moral graph but there exists a vertex set S such that Vi \u22a5 Vj |S, then there exists a decomposed subgraph that contains Vi, Vj and a variable set S\n\u2032 such that Vi \u22a5 Vj |S\n\u2032, and vice versa [33]. To decompose a graph into subgraphs, the approach in [33] needs a moral graph and requires that each dseparator (conditional sets) has a complete subgraph in the moral graph. To solve the condition, Xie et al. [110] proposed an improved decomposition approach using d-separation tree instead of a moral graph. But this algorithm performs decomposition only based on the entire undirected independence graph containing all vertices in V and cannot perform decomposition of undirected independence subgraphs.\nLater, Xie et al. [109] proposed a recursive method for learning structures of DAGs. Specifically, the algorithm mainly includes the top-down step and the bottom-up step. At the top-down step, the independence graph at the top is decomposed into two small subsets, each of which is decomposed recursively into two smaller subsets until each node cannot be decomposed further at the bottom of the tree. At each step, the decomposition is achieved by learning an independence graph for a variable subset. Then, at the bottom-up step, the skeletons of leaf nodes are first constructed, and then a pair of child skeletons are combined together into a large subgraph at their parent node until the entire graph is constructed at the top of the tree. By recursively decomposing the entire variable set into small subsets, search for d-separators in a large network is localized to small subgraphs, and thus this approach improved the efficiency of searches and the power of statistical tests for structure learning."}, {"heading": "3.3.2 Markov blanket approach", "text": "The composition approaches mentioned above need to construct the undirected independence graph at first. For a small set of variables, the composition approaches start with a complete undirected graph, and then check an edge between each pair of vertices X and Y . The edge X \u2212 Y is removed if X and Y are independent conditionally on the set of all other variables. However, it is a computational challenge to construct undirected network structure with a large number of variables. In the section, we review two algorithms using Markov blankets to allevate the computational problem. The key idea of the two algorithms is that\n9 firstly it learns a Markov blanket for each variable, secondly constructs the local skeletons around each variable through the learned Markov blankets, thirdly builds the global skeleton by those local skeletons, and at last, discovers vstructures and orients edge directions. The main advantage of the algorithm comes through the use of Markov blankets to restrict the size of the conditioning sets and localize learning the skeleton of each variable.\nGSBN algrithm.Margaritis and Thrun [66] proposed the Grow-Shrink Bayesain network learning (GSBN) algorithm for constrain-based causal network learning. In the GSBN algorithm, Margaritis and Thrun first invented a sound and efficient algorithm, the GSMB (Grow-Shrink Markov Blanket discovery) algorithm, for computing the Markov blanket of a node under the failthful condition, and then used the properties of the Markov blanket to facilitate fast reconstruction of the local skeleton around each variable under assumptions of bounded neighborhood size. But the GSMB algorithm requires exponential number of data instances to the size of the Markov blanket, thus impractical for many real data sets. To conquer this drawback of the GSMB algorithm, Tsamardinos and Aliferis [100] proposed a modified version of the GSMB algorithm, called the IAMB algorithm, which guarantees to find the actual Markov blanket given enough training data and the method is more sample efficient than GSMB, and its variants, such as inter-IAMB [102], Fast-IAMB [102], and IAMBFDR [77]. However, the IAMB algorithm still requires a sample size exponential in the size of a Markov blanket. Thus, HITONMB [2, 4] and MMMB [101, 104] were introduced to mitigate the problem of data inefficiency. Different from GSMB and IAMB, HITON-MB and MMMB take two steps to find the Markov blanket of a target node: (1) discovering the parents and children of the target node; and then (2) identifying its spouses based on Step 1. As an efficient implementation of Step 1, two major algorithms HITON-PC [2, 4] and MMPC were introduced [101, 103]. Following the ideas above, PCMB [78], IPCMB [30], STMB [32], WLCMB [57] was also proposed to efficient and effective discovery of Markov blankets. Note that both those Markov blanket discovery algorithms do not distinguish causes from effects between variables in the discovered Markov blankets.\nTotal Conditioning (TC) algorithm Tsamardinos and Aliferis [100] associated Markov blankets in Bayesian networks with strongly relevant variables defined by Kohavi and John in feature selection, then transferred the feature selection task to the discovery of Markov blankets in Bayesin networks. Based on the work, in contrast, Pellet and Elisseeff [75] connected the problem of learning causal structures with a problem of feature selection in data mining. They proposed the Total Conditioning (TC) algorithm, and its improved version, the TCbw algorithm, using the Recursive Feature Elimination (RFE) algorithm [35]. The work illustrated that under the Markov and faithfulness assumptions, the smallest set of variables relevant to predicting variable X is the Markov blanket of X , and we can use existing feature-selection techniques to design scalable structure learning algorithms in high-dimensional data under different assumptions."}, {"heading": "3.3.3 Recursive Autonomy Identification", "text": "The RAI (Recursive Autonomy Identification) algorithm was presented in [111, 112] which uses a structure decomposition approach. While the RAI (Recursive Autonomy Identification) algorihtm combines the skeleton discovery and edge orientation, all algorithms mentioned above all separate the skeleton discovery and edge orientation into the independent steps. Beginning with a complete undirected graph and proceeding from low to high sizes of condition sets, the RAI algorithm uncovers a casual structure by performing the following sequence of operations: (1) pairwise independence test between nodes, followed by the removal of edges related to independence, (2) edge direction according to orientation rules, and (3) structure decomposition into autonomous sub-structures. For each autonomous substructure, the RAI algorithm is applied recursively, while increasing the order of independence testing.\nDefinition 3.3 (exogenous cause.). [72] A node Y in G(V,E) is an exogenous cause to G(V ,E), where V \u2282 V and E \u2282 E, if Y /\u2208 V and \u2200X \u2208 V , Y \u2208 Pa(X,G) or Y /\u2208 Adj(X,G).\nDefinition 3.4 (autonomous sub-structure.). [112] In a DAG G(V,E), a sub-structureGA(V A, EA) such that V A \u2282 V and EA \u2282 E is said to be autonomous in G given a set Vex \u2282 V of exogenous causes to G\nA if \u2200X \u2208 V A, Pa(X,G) \u2282 {V A \u222a Vex}. If Vex is empty, we say the sub-structure is (completely) autonomous.\nDue to structure decomposition into separated, smaller, autonomous sub-structures, RAI requires a small number of independence tests and the small sizes of condition sets used in these tests. This reduces computational complexity and increases the accuracy of the learned structure. Moreover, by combining skeleton discovery and edge orientation, additional edges can be directed, which also reduce the complexity of independence tests of the subsequent iterations."}, {"heading": "3.4 Active learning-based approaches", "text": "The existing methods for casual discovery fully rely on observational data to discover causal relations between variables up to a Markov equivalence class, and thus leave many edge directions undetermined [90]. In general, complete identification of all edge directions requires manipulation/experimentation to augment discoveries from observational data. This has led to the recent development of several methods for active learning of causal networks that utilize both observational and experimental data.\nThe active learning approach first learns a CPDAG from observational data, then orients remaining undirected edges in the resulting CPDAG via intervention experiments. The type of methods includes the work of [22, 27, 68, 97] and the method proposed by He and Geng [37]. The main idea behind these approaches is to employ the PC algorithm to obtain an undirected or partially directed graph from observational data, then to use some decision criteria to select a variable for manipulation, with the goal of maximizing the number of edges that are oriented after the experiment. The ALCBN algorithm [68] uses either the mini-max, maximin or Laplace decision criteria, whereas the approach proposed by He and Geng [37] uses either the maxi-min\n10\nor maximum entropy criteria. Once the variable is selected and manipulated, they perform a statistical independence test between the manipulated variable and each of its unoriented adjacency in the graph, using experimental data. The method in [68] repeats this process until all edges in the graph are oriented. The algorithm in [37] first partitions the graph into chain components which are only connected by directed edges and orients each of these components separately."}, {"heading": "3.5 Local discovery of direct causes and effects", "text": "Many causal learning algorithms generally learn the global causal structures in the form of CPDAGs to find causal relationships between variables. While these algorithms are effective, it is often unnecessary and wasteful to find the global structures when we are only interested in the causal relationships around a target variable. Below we discuss the algorithms for distinguishing causes from effects to a target variable, which is more scalable than global methods and more efficient than local-to-global methods.\nLCD algorithm. The LCD (Local Causal Discovery) algorithm and its variants [19, 61, 86] aim to find causal edges by testing the dependence/independence relationships among every four-variable set in a causal network. Bayesian Local Causal Discovery (BLCD) [63] and CD-B/H algorithm [60] explores the Y-structures among the nodes in a Markov blanket to infer causal edges using Baysain scoring-based method. To discover the effects of a node X, BLCD estimates the Markov blanket of a variable and uses it for the identification of Y structures from sets of four variables. BLCD does not specifically identify the sets of parents and children from the Markov blanket. While LCD/BLCD algorithms aim to identify a subset of causal edges via special structures among all variables, not distinguishing causes from effects of a target variable in which we are interested.\nPCD-by-PCD approach. The general idea of the PCDby-PCD approach (PCD denotes Parents, Children and some Descendants) is to discover the Markov blanket of a target variable of interest, then to orient the edges connected to the target [113]. But it is not sufficient to orient the edges connected to the target using only the variables in the Markov blanket. To solve this problem, the PCD-by-PCD algorithm first discovers PCD(T ) of the target T and PCD(X) for variable X \u2208 PCD(T ), then sequentially finds PCD(X) for a variable X which is contained in the previous PCD\u2019s. During the sequential process, the PCD-by-PCD algorithm finds local v-structures and try to orient the edges connected to the target T as much as possible. When all of the edges connected to the target T are oriented, the PCD-by-PCD algorithm stops the process and obtain all direct causes and effects of the target T .\nSuppose that a causal network is faithful to a probability distribution and that independence tests are correctly performed, the PCD-by-PCD algorithm was proved that it obtains edges connected to the target T , and further it returns the same orientations of these edges as a partially directed graph for the Markov equivalence class of the underlying global causal network.\nCMB algorithm. Based on the idea of the PCD-by-PCD algorithm, Tian and Ji [31] propose a new Causal Markov\nBlanket (CMB) discovery algorithm to identify the direct causes and effects of a target variable of interest. The CMB algorithm has three major steps: 1) to find the Markov blanket set of the target and to identify some direct causes and effects by tracking the independence relationship changes among the parent and child nodes of a target before and after conditioning on the target node, 2) to repeat Step 1 but conditioned on one parent and child node\u2019s MB set, and 3) to repeat Step 1 and 2 with unidentified neighboring nodes as new targets to identify more direct causes and effects of the original target until all edges adjacent to the target are oriented.\nThe CMB algorithm was proved that it is soundness and completeness. It can identify the same causal structure as the global and local-to-global causal discovery algorithms with the same identification condition, but uses a fraction of the cost of the global and local-to-global approaches.\nODLP algorithm. The local causal discovery methods discussed above are fully dependent on observational data and may not orient all edge directions with observational data alone [3, 93]. Statnikov et. al. [93] introduced a local causal discovery method, called ODLP, for discovery of local causal pathways around the target variable of interest (i.e., direct causes and direct effects of the target) using observational and experimental data, rather than learning a entire causal network with both observational and experimental data. The ODLP algorithm discovers members of the local causal pathway of a target variable (i.e., the set of parents and children of the target) using observational data, while all orientation decisions are based on experimental data exclusively. In the edge orientation step, the ODLP algorithm uses single-variable manipulation experiments to determine the sequence of manipulation for the variables that belong to the local causal pathway of the target. By using the local discovery strategy and single-variable manipulation experiments, the ODLP algorithm reduces the number of required experiments and scales to high-dimensional data with thousands of variables. There two versions of the ODLP algorithm, ODLP* and ODLP.\nUnder the faithfullness assumption, the ODLP* algorithm was proposed. The ODLP* algorithm first uses the HITON-PC or MMPC algorithm to discover the set of parents and children of the given target with observational data, then orient edge directions with experimental data to distinguish direct causes from direct effects of the target. The ODLP* algorithm is sound and complete under the assumptions of Assumption 1 to 3.\nIn order to increase the discovery accuracy, by relaxing the faithfullness assumption, the ODLP algorithm was presented by employing the TIE* algorithm to discover multiple local casual path ways (i.e., multiple sets of parents and children) of a target, then to find the true local casual path way of the target from the discovered the multiple local casual path ways with experimental data. The rational behind the idea is that a variable may have multiple sets of parents and children when the faithfullness assumption is violated [92], while the TIE* algorithm was designed to discover multiple Markov blanket of a target as the assumption does not hold in data. Assuming a relaxation of local adjacency faithfulness to allow for target information equivalency relations (near faithfullness), the ODLP algo-\n11\nrithm is sound under Assumptions 1 and 3."}, {"heading": "3.6 Learning from overlapping variable sets", "text": "Instead of learning with a single observational data set, sometimes we need to deal with several data sets that do not share the same set of variables, but share significant overlap variable sets due to privacy or ethics [23, 24]. In this setting, co-analyzing a collection of data sets and learning causal structures over the joint set of observed variables are an active research field.\nAs a pioneering work, Danks [23] proposed the SLPR (Structure learning using prior results) algorithm to learning the causal structure over the joint set of observed variables. The SLPR algorithm first learns the a sub CPDAG for each data set using the existing constrain-based algorithm. Then beginning with a complete undirected graph, the SLPR algorithm uses each sub CPDAG and edge removal rules to get a full CPDAG. In particular, the SLPR algorithm designs some edge removal rules to extract casual information about the true (unknown) Bayesian network from the previously learned sub CPDAG. Finally, the SLPR algorithm employs orientation rules to orient as many as rules. The problem for the SLPR algorithm is that it is hard to guarantee that all data sets satisfy the assumption of causal sufficiency, thus in addition to Assumption 1 and 3, the SLPR algorithm works at the assumption of the joint causal sufficiency, that is, the joint set of observed variables are assumed to be causally sufficient."}, {"heading": "4 LEARNING WITHOUT CAUSAL SUFFICIENCY", "text": "In real-world applications, such as medical science, epidemiology, and sociology, it is impossible to ensure that all common causes are measured in real-world data [6, 17]. Thus, the assumption of causal sufficiency is often violated in many situations. There has been some work on learning a Bayesian network with latent variables. However, this type of approaches (mainly based on score-based methods) still deals with latent variables under a DAG model and assumes causal sufficiency, since those algorithms need to pre-determine the number of latent variables and the exact locations of the latent variables with the observed variables in data before learning starts. In this case, by involved latent variable into the structure to be learnt, these methods then transfer the problem of latent variable learning to the problem of missing data completing [85]. In practice, since the latent variables are not observed, both the number of those variables and their locations with the observed variables in data are unclear in advance. Moreover, with latent variables, the space of DAGs is not closed under marginalization [17].\nInstead of DAGs, using a MAG (maximal ancestral graph) [81] model to represent latent common causes is an emerging research direction in causal discovery without the assumption of causal sufficiency [11, 17, 85]. The key difference between DAGs and MAGs is that a MAG includes bidirected edges between pairs of the observed variables, in addition to directed edges. Any latent common cause causing two or more of the observed variables will be represented implicitly by bidirected edges between all pairs of the affected variables in MAGs, instead of explicitly\nmarking the given latent common causes in structures. In the section, we will review some representative constraintbased algorithms designed for learning MAGs without assuming the causal sufficiency."}, {"heading": "4.1 Problem Definition and Algorithm Overview", "text": "Problem Definition. Given a perfect oracle of conditional independence and faithfulness, the algorithm for Learning causal structures without the assumption of causal sufficiency is to find a PAG of the Markov equivalence class of the true causal MAG.\nAlgorithm Overview. Tabel 3 gives an overview of constraint-based algorithms under the assumptions of causal sufficiency and faithfullness. In Table 3, \u201csingle or multiple\u201d indicates whether an algorithm is able to deal with a single data set or multiple data sets.\nDifferent from a DAG, a mixed graph can contain three types of edges: directed edges (\u2190 or \u2192) , bidirected edges (\u2194), and undirected edges (\u2212). In a mixed graph, if the edge Vi \u2192 Vj is present, Vi is a parent of Vj ; if Vi \u2194 Vj appears, Vi is called a spouse of Vj (and Vj is a spouse of Vi); Vi is a neighbor of Vj if Vi \u2212 Vj is present. Vi is an ancestor of Vj if there is a directed path from Vi to Vj .\nLet an(Vi) be the set of ancestors of Vi, a directed cycle occurs in a mixed graph when Vj \u2192 Vi and Vi \u2208 an(Vj). An almost directed cycle occurs when Vj \u2194 Vi appears and Vi \u2208 an(Vj), i.e., removing the arrowhead at Vj results in a directed cycle. In the following, we use the symbol \u201c\u2217\u201d to denote an arbitrary edge mark > or \u2212.\nDefinition 4.1 (ancestral graph). [81] A mixed graph is ancestral if the conditions hold that (1) it does not contain directed cycles, (2) it does not contain almost directed cycles, and (3) for any undirected edge Vi \u2212 Vj , Vi and Vj have no parents or spouses.\nSo in an ancestral graph, Vi \u2192 Vj means that Vi is a cause (ancestor) of Vj , while Vj is not a cause (ancestor) of Vj . Vi \u2194 Vj denotes that Vi is not a cause of Vj and Vj is not a cause of Vi, which implies that there is a latent common cause of Vi and Vj .\nDefinition 4.2 (m-separation). [81] In an ancestral graph, Vi and Vj are m-separated by Z \u2286 V \\{Vi, Vj}, if every path between Vi and Vj is blocked by Z . A path \u03c4 in the ancestral graph is said to be blocked by Z if and only if one of the conditions holds.\n\u2022 \u03c4 contains a subpath < Vi, Vj , Vk > such that variable Vj is a noncollider on this path and Vj \u2208 Z ;\n12\n\u2022 \u03c4 contains a v-structure Vi\u2217 \u2192 Vj \u2190 \u2217Vk such that Vj /\u2208 Z and no descendants of Vj is in Z .\nDefinition 4.3 (maximal ancestral graph, MAG). [81] For any two non-adjacent variables in an ancestral graph, if there exists a set of variables that m-separates them, then the ancestral graph is said to be maximal.\nProposition 4.1. TwoMAGs over the same set of vertices are Markov equivalent if and only if (1) they have the same adjacency; (2) they have the same unshielded colliders; (3) if a path is a discriminating path for a vertex Vi in both graphs, then Vi is a collider on the path in one graph if and only if it is a collider on the path in the other.\nSeveral MAGs can describe exactly the same conditional independence relationships and such MAGs form a Markov equivalence class which can be represented by a partial ancestral graph (PAG).\nDefinition 4.4 (partial ancestral graph.). Let [G] be the Markov equivalence class of a MAG G. A partial ancestral graph (PAG) for [G] is a graph P with possibly three kinds of marks (and hence six kinds of edges: \u2212, \u2190, \u2194, \u25e6\u2212, \u25e6\u2212\u25e6, \u25e6 \u2190), such that (1) P has the same adjacency as G (and anymember of [G]) does; and (2) every non-circle mark in P is an invariant mark in [G]."}, {"heading": "4.2 Learning from a single data set", "text": "IC* algorithms. The IC* (Inductive Causation) algorithm [74] is the pioneer work to learn causal structures without assuming causal sufficiency. It starts with an empty graph, and performs the following steps.\n\u2022 Step 1: Find the undirected structure: add an undirected edge between X and Y if no set SXY \u2286 V \\ {X,Y } can be found such that X \u22a5 Y |SXY . \u2022 Step 2: Determine the v-structures. For each connected triple X \u2212 Z \u2212 Y where X and Y are nonadjacent, direct the edges and add a v-structure X \u2190 Z \u2192 Y if Z /\u2208 SXY , i.e., if and only if X and Y are dependent given Z. \u2022 Step 3: Orient the directions of the remaining edges.\nStep 1 is identical to that of the SGS algorithm, but IC* uses the following rules to deal with the adjacency between variables and edge orientation. In the edge orientation, IC* just adds arrow heads into the identified colliders, independently of what the other arrow endpoints.\n\u2022 Adjacency: insert the \u201cagnostic link\u201d \u25e6\u2212\u25e6, if \u2200SXY \u2286 V \\ {X,Y } : (X \u22a56 Y |SXY ); \u2022 V-structures: for all triples X \u2217 \u2212 \u2217 Z \u2217 \u2212 \u2217 Y where X and Y are nonadjacent, add arrowheads into Z as X\u2217 \u2190 Z \u2192 \u2217Y if Z /\u2208 SXY ; \u2022 Orientations: use the following rules to further orient \u201cagnostic\u201d endpoints wherever possible. R1: for each Vi \u2217 \u2212 \u25e6 Vj such that there is a directed path from Vi to Vj , then orient as Vi\u2217 \u2192 Vj . R2: for each Vi\u2217 \u2192 Vk \u25e6 \u2212 \u2217 Vj , Vi and Vj are not adjacent, then orient the triple as Vi\u2217 \u2192 Vk\u25e6 \u2192 Vj\nRule 1 preserves acyclicity, while Rule 2 honors the noncollider constraint when one of the two endpoints is an\narrowhead. Like SGS, Step 1 of IC* requires a subset search, which has an exponential time complexity.\nFCI algorithm. To improve the efficiency of the IC* algorithm, the FCI algorithm [90] was proposed which finds an graph skeleton in Step 1 using the PC algorithm. Before we give the details of the FCI algorithm, we give the following definition.\nDefinition 4.5. Let C be a graph with any of the following edge types: \u25e6 \u2212 \u25e6, \u25e6 \u2192, \u2194, Possible-D-SEP(Vi,Vj) in C, denoted in shorthand by pds(C, Vi, Vj), is defined as follows: Vk \u2208 pds(C, Vi, Vj) if and only if there is a path \u03c0 between Vi and Vj in C such that for every subpath < Vm, Vl, Vh > of \u03c0, Vl is a collider on the subpath in C or< Vm, Vl, Vh > is a triangle in C.\nThe FCI algorithm includes the following steps:\n\u2022 Step 1: Finding the graph skeleton (called C1) using the PC algorithm and every edge in C1 is reoriented as \u25e6 \u2212 \u25e6. \u2022 Step 2: Orienting unshielded triples Vi\u2217\u2212\u25e6Vj \u25e6\u2212\u2217Vk as v- structures Vi\u2217 \u2190 Vj \u2192 \u2217Vk if and only if Vj is not in sepset(Vi, Vk) and sepset(Vk, Vi). \u2022 Step 3: Computing pds(C2, Vi, )\u0307 for every Vi \u2208 X suppose C2 is the graph achieved from Step 2. For every element Vj in adj(C2, Vi), the algorithm tests whether Vi \u22a5 Vj |Z) for every subset Z of\npds(C2, Vi, )\u0307\\{Vi, Vj} and of pds(C2, Vj , )\u0307\\{Vj , Vi}. If there exists a set Z that makes Vi and Vj conditionally independent given Z , the edge between Vi and Vj is removed and the setZ is saved as the separation set in sepset(Vi, Vj) and sepset(Vj, Vi). \u2022 Step 4: Orienting the v-structures again based on the updated skeleton and the updated information in sepset. \u2022 Step 5: Replacing as many circles as possible by arrowheads and tails using the R1-R10 orientation rules (in total 10 rules) described by [118].\nFor sparse graphs, Step 3 of the FCI algorithm dramatically increases the computational complexity of the algorithm when compared to the PC algorithm. The additional computational effort can be divided in two parts: computing the Possible-D-SEP sets, and testing conditional independence given all subsets of these sets. The latter part\nis computationally infeasible when the sets pds(C2, Xi, )\u0307 are large, containing, say, more than 30 vertices. Thus, the size of the Possible-D-SEP sets plays such an important role in the complexity of the FCI algorithm.\nRFCI algorithm. The RFCI (Really Fast Causal Inference) algorithm [17] is a modification of the FCI algorithm. The step 1 of the RFCI algorithm is identical to Step 1 of the FCI algorithm, and is used to find an initial skeleton. The main difference between RFCI and FCI is that the RFCI algorithm avoids preforming the conditional independence tests given subsets of Possible-D-SEP sets in Step 3 of the FCI algorithm. Instead, in Step 2, RFCI performs some additional tests before orienting v-structures and discriminating paths in order to ensure soundness. In Step 2, RFCI orients all unshielded triples as v-structures or non-v-structures. In Step 3, the RFCI algorithm orients as many further edges as possible.\n13\nMBCS* algorithm. The main difference between the MBCS* algorithm [76] and the FCI algorithm exists in Step 1. Both FCI and RFCI need to construct a graph skeleton (undirected graph) at first. But it is a computational challenge to construct undirected structure with a large number of variables. Different from FCI and RFCI, the MBCS* algorithm first learns a Markov blanket of each variable, then constructs the global skeleton of graph by the learned Markov blankets. By localizing learning the skeleton of each node, the MBCS* algorithm restricts the size of the conditioning sets, and thus performs much fewer conditional-independence tests than FCI and RFCI in Step 1. By the learned skeleton, the MBCS* algorithm examines the triangle structures to identify colliders and noncolliders. Finally, it uses the same orientation rules as FCI to obtain the maximally oriented PAG."}, {"heading": "4.3 Learning with overlapping variable sets", "text": "In many situations, we may have several data sets with overlapping variable sets. Those data sets can be observational data, experiment data, or both. In this setting, it is hard to assume that the causal sufficiency assumption holds in a data set. To deal with learningMAGs frommultiple data sets without assuming causal sufficiency, some methods have been designed for learning MAGs from overlapping variable sets.\nION algorithm. The ION (Integration of Overlapping Networks) algorithm [95] is the first and an asymptotically correct algorithm for discovering the complete set of causal DAG structures that are consistent with multiple observational data with overlapping variables. The ION algorithm first uses FCI to learning the common characteristics a sub MAG from each data set. Then beginning with the complete graph with all variables, the algorithmn takes these PAGs as input to learn a set of PAGs that is consistent with the underlying MAG through a series of designed graph operations.\nBut the ION algorithm requires exponential number of tests with respect to the number of variables, and thus is often computationally intractable even for a small number of variables. Moreover, since each PAG in the input set is learned independently using a different data set, it is often the case that different PAGs in the input set may entail contrary conditional independences and dependences. For example, Vi is a parent of Vj in one PAG but a child of Vj in another PAG, resulting from statistical errors. But the ION algorithm ignores such contradictory information rather than attempting to resolve it. When ION encounters contradictory inputs, it may either produce inaccurate results or no results since it cannot find any structures which are consistent with the entire input set.\nIOD algorithm. To deal with the problem of conflict information resulted from different data sets, the IOD (integration of overlapping datasets) algorithm [96] was proposed which is asymptotically correct and complete. The IOD algorithm requires significantly less memory and computation time than ION. Rather than learning PAGs for each data set independently, the IOD algorithm learns PAGs directly from the multiple data sets. This avoids the problem of contradictory inputs and also results in a more robust\nlearning procedure since the algorithm can often use more than one data set at a time when performing any statistical tests, which results in a more accurate test.\ncSAT+ algorithm. Triantafillou et al. [99] also presented a cSAT+ algorithm to improve the efficiency of the ION algorithm. The cSAT+ algorithm exploits the power of a general computer science technique called SAT-solving (satisfiability of logical statements) for integrating the independence constraints discovered from each data set into a MAG consistent with all available data sets. The basic idea of the method is to convert the (in)dependence constraints found in the data into logical constraints on the presence and absence of certain pathways in the underlying causal structure, and to use a SAT-solver to find the causal structures consistent with all constraints. Experiments show that the cSAT+ algorithm is more efficient than the ION algorithm. Latter, Hyttinen et al. [40] proposed a general SAT-based procedure by transforming the observed dependences and independences constraints into a SAT instance. This approach is able to deal with cyclic structures, but assumes lack of statistical errors and corresponding conflicts. Thus, it relies on an oracle of conditional independence and cannot be directly applied to real problems. Instead of the SAT solver, recently, Hyttinen et al. [39] presented an ASP-based constraint optimization approach to handle inconsistent (in)dependence constraints obtained from overlapping experimental or observational data sets. But the scalability of the approach is still quite limited.\nINCA approach. Tsamardinos et. al. [105] unified the prior work above and proposed the INCA (Integrative Causal Analysis) framework for the co-analysis of heterogeneous data sets with overlapping variables. The framework can predict the presence and strength of conditional and unconditional dependencies between two variables never jointly measured based on multiple data sets. To implement this idea, three algorithms were proposed, the Full-Testing Rule (FTR), the Minimal-Testing Rule (MTR), and FTRS. The FTR and MTR algorithms both were presented to predict the presence unconditional and conditional dependencies between variables not jointly measured, but the FTR algorihtm is much better than the MTR algorihtm. Meanwhile, the FTR-S predicts the strength of the dependence.\nFTR is a greedy algorithm that sacrifices completeness to improve computational efficiency and scalability, while the ION, IOD, and cSAT+ algorithms are complete algorithms and could make more predictions, and more general types of predictions (e.g., also predicting independencies) than FTR. But the computational costs of the ION, IOD, and cSAT+ algorithms in high-dimensional data sets are expensive or even prohibitive.\nCOmbINE algorihtm. The COmbINE (Causal discovery from Overlapping INtErventions) algorithm [98] builds upon the ideas in the cSAT+ algorithm [99] and is the first algorithm to address both overlapping variables and multiple (hard) interventions for acyclic structures without relying on specific parametric assumptions or requiring an oracle of conditional independence. The algorithm converts the observed statistical dependencies and independencies in study data into path constraints on the data-generating causal model and encodes them as a SAT instance. However, due to statistical errors in the determination of dependencies\n14\nand independencies, conflicting constraints may arise. In this case, the SAT instance is unsolvable and no useful information can be inferred. To address the problem, the COmbINE algorithm proposed a conflict resolution technique that ranks dependencies and independencies discovered according to confidence as a function of their p-values. The constraints are added to the SAT instance in decreasing order of confidence, and the ones that conflict with the set of higher-ranked constraints are discarded. This technique allows it to be applicable on real data that may present conflicting constraints. The algorithm is proved to be sound and complete in the sample limit under different interventions in acyclic domains [98]. The limits of COmbINE may be that it only scales up to 100 variables in sparse graphs.\nRecently, a number of algorithms have emerged which use non-Gaussianity [Shimizu et al., 2006] and nonlinearity [Hoyer et al., 2009] observed in the data to learn causal structures. These algorithms typically produce an equivalence class that is much smaller than the Markov equivalence class, often a unique structure. Thus, an open problem is how to adapt such methods to use multiple data sets with overlapping variables and produce an equivalence class that is significantly smaller than the equivalence classes produced by IOD and ION. Another open problem is how to use background knowledge about the true data generating process to efficiently guide the IOD search procedure."}, {"heading": "5 PUBLIC SOFTWARE AND BENCHMARK DATA", "text": "In the section, we list some main public software packages related to constraint-based methods in Table 4. The frequently used benchmark Bayesian networks are from the Bayesian Network Repository3. We can also use the software package above to generate synthetic Bayesian networks. Specially, the Causal Explore package provides a bn tiling algorithm to generate an arbitrary size of Bayesian networks."}, {"heading": "6 CONCLUSION AND DISCUSSION", "text": "In this paper, we present a comprehensive survey on algorithms for constraint-based causal discovery. We first introduce the learn paradigm of the constraint-based methods. Then, we give the problem definitions and discuss the details of the state-of-the-art causal structure learning algorithms. Finally, we briefly mention publicly-available software and data sets for constraint-based causal discovery. As we discussed above, the emerging of big data brings not only opportunities to constraint-basedmethods, but also challenges.\nFirstly, large-sized and high dimensional data present an immediate challenge to learning a whole casual structure due to the computational feasibility, although the current local causal discovery techniques can scale up to hundreds of thousands of variables. Furthermore, the current constraintbased methods all assume the causal structure to be learnt is spare, that is, limiting the maximum neighborhood size for each node (i.e. the number of parents and children), since the number of independence tests is exponential to the neighborhood size. How to deal with causal structure\n3. http://www.bnlearn.com/bnrepository/\nlearning with large neighborhood sizes is still an open problem.\nSecondly, in big data analytics, the availability of multiple heterogeneous data sets makes us be able to acquire causal knowledge from multiple data sets instead of any individual data source alone [8]. Although significant efforts have been spent on this research direction, there are still two problem unsolved. One is the efficiency problem. The current methods only deal with data with the number of variables no more than 100. And the other is that the current methods focus on the theoretical foundations of this field, and thus, bridging the gap between theory and practice is crucial of causal discovery in multiple data sets. Recent, the idea of invariant causal inference may open a new way to handle those issues [79].\nThirdly, the MAGmodels provide a different perspective in causal inference without assuming causal sufficiency, compared to DAGs [81]. Using MAGs to represent latent variables is an emerging direction in causal discovery without the assumption of causal sufficiency [17, 46, 85]. Thus, developing scalable and accurate global and local methods to learn MAGs is an emerging research topic.\nFinally, except for latent variables, a sample selection bias represents an another major obstacle to causal discovery [7]. Although some existing work described the selection biases, such as FCI and RFCI, those work in fact focused on handling latent variables. Recently, some work has been proposed to address the problem of selection bias in causal discovery, but those work is still in a theoretical aspect [7, 9, 41]. As emerging multiple data sets, it is an open problem to design algorithms to deal with the problem of selection bias and apply to real applications."}], "references": [{"title": "Categorical data analysis", "author": ["A. Agresti", "M. Kateri"], "venue": "Springer,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Local causal and markov blanket induction for causal discovery and feature selection for classification part i: Algorithms and empirical evaluation", "author": ["C.F. Aliferis", "A. Statnikov", "I. Tsamardinos", "S. Mani", "X.D. Koutsoukos"], "venue": "Journal of Machine Learning Research, 11:171\u2013234,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Local causal and markov blanket induction for causal discovery and feature selection for classification part ii: Analysis and extensions", "author": ["C.F. Aliferis", "A. Statnikov", "I. Tsamardinos", "S. Mani", "X.D. Koutsoukos"], "venue": "Journal of Machine Learning Research, 11(Jan):235\u2013284,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Hiton: a novel markov blanket algorithm for optimal variable selection", "author": ["C.F. Aliferis", "I. Tsamardinos", "A. Statnikov"], "venue": "AMIA Annual Symposium Proceedings, volume 2003, page 21. American Medical Informatics Association,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A reasoning model based on the production of acceptable arguments", "author": ["L. Amgoud", "C. Cayrol"], "venue": "Annals of Mathematics and Artificial Intelligence, 34(1-3):197\u2013215,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning linear bayesian networks with latent variables", "author": ["A. Anandkumar", "D. Hsu", "A. Javanmard", "S. Kakade"], "venue": "Proceedings of The 30th International Conference on Machine Learning, pages 249\u2013 257,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Controlling selection bias in causal inference", "author": ["E. Bareinboim", "J. Pearl"], "venue": "AISTATS, pages 100\u2013108,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Causal inference and the datafusion problem", "author": ["E. Bareinboim", "J. Pearl"], "venue": "Proceedings of the National Academy of Sciences, 113(27):7345\u20137352,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Recovering causal effects from selection bias", "author": ["E. Bareinboim", "J. Tian"], "venue": "AAAI, pages 3475\u20133481,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The control of the false discovery rate in multiple testing under dependency", "author": ["Y. Benjamini", "D. Yekutieli"], "venue": "Annals of statistics, pages 1165\u20131188,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Incorporating causal prior knowledge as path-constraints in bayesian networks and maximal ancestral graphs", "author": ["G. Borboudakis", "I. Tsamardinos"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML\u201912), pages 1799\u20131806,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving the reliability of causal discovery from small data sets using argumentation", "author": ["F. Bromberg", "D. Margaritis"], "venue": "Journal of Machine Learning Research, 10(Feb):301\u2013340,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning bayesian networks from data: An information-theory based approach", "author": ["J. Cheng", "D.A. Bell", "W. Liu"], "venue": "Artificial Intelligence, 137(May):43\u201390,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning Bayesian networks from data", "author": ["D.M. Chickering"], "venue": "University of California, Los Angeles,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning equivalence classes of bayesiannetwork structures", "author": ["D.M. Chickering"], "venue": "Journal of machine learning research, 2(Feb):445\u2013498,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Order-independent constraintbased causal structure learning", "author": ["D. Colombo", "M.H. Maathuis"], "venue": "The Journal of Machine Learning Research, 15(1):3741\u20133782,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning high-dimensional directed acyclic graphs with latent and selection variables", "author": ["D. Colombo", "M.H. Maathuis", "M. Kalisch", "T.S. Richardson"], "venue": "The Annals of Statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "An overview of the representation and discovery of causal relationships using bayesian networks", "author": ["G. Cooper"], "venue": "Computation, causation, and discovery, pages 4\u201362,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "A simple constraint-based algorithm for efficiently mining observational databases for causal relationships", "author": ["G.F. Cooper"], "venue": "Data Mining and Knowledge Discovery, 1(2):203\u2013224,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "The center for causal discovery of biomedical knowledge from big data", "author": ["G.F. Cooper", "I. Bahar", "M.J. Becich", "P.V. Benos", "J. Berg", "J.U. Espino", "C. Glymour", "R.C. Jacobson", "M. Kienholz", "A.V. Lee"], "venue": "Journal of the American Medical Informatics Association, page ocv059,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "A bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Machine learning, 9(4):309\u2013347,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1992}, {"title": "Causal discovery from a mixture of experimental and observational data", "author": ["G.F. Cooper", "C. Yoo"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 116\u2013125. Morgan Kaufmann Publishers Inc.,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning the causal structure of overlapping variable sets", "author": ["D. Danks"], "venue": "International Conference on Discovery Science, pages 178\u2013 191. Springer,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Scientific coherence and the fusion of experimental results", "author": ["D. Danks"], "venue": "The British Journal for the Philosophy of Science, 56(4):791\u2013 807,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Structure learning in graphical modeling", "author": ["M. Drton", "M.H. Maathuis"], "venue": "arXiv preprint arXiv:1606.02359,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Introduction to structural equation models", "author": ["O.D. Duncan"], "venue": "Elsevier,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "On the number of experiments sufficient and in the worst case necessary to identify all causal relations among n variables", "author": ["F. Eberhardt", "C. Glymour", "R. Scheines"], "venue": "In UAI,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Causal discovery for climate research using graphical models", "author": ["I. Ebert-Uphoff", "Y. Deng"], "venue": "Journal of Climate, 25(17):5648\u2013 5665,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Inferring cellular networks using probabilistic graphical models", "author": ["N. Friedman"], "venue": "Science, 303(5659):799\u2013805,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast markov blanket discovery algorithm via local learning within single pass", "author": ["S. Fu", "M.C. Desmarais"], "venue": "Conference of the Canadian Society for Computational Studies of Intelligence, pages 96\u2013107. Springer,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Local causal discovery of direct causes and effects", "author": ["T. Gao", "Q. Ji"], "venue": "NIPS\u201915, pages 2503\u20132511,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient markov blanket discovery and its application", "author": ["T. Gao", "Q. Ji"], "venue": "IEEE Transactions on Cybernetics, DOI: 10.1109/TCYB.2016.2539338,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Decomposition of search for vstructures in dags", "author": ["Z. Geng", "C. Wang", "Q. Zhao"], "venue": "Journal of Multivariate Analysis, 96(2):282\u2013294,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Computation, causation, and discovery", "author": ["C.N. Glymour", "G.F. Cooper"], "venue": "Aaai Press,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1999}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine learning, 46(1-3):389\u2013422,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Pc algorithm for nonparanormal graphical models", "author": ["N. Harris", "M. Drton"], "venue": "Journal of Machine Learning Research, 14(1):3365\u20133383,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning of causal networks with intervention experiments and optimal designs", "author": ["Y.-B. He", "Z. Geng"], "venue": "Journal of Machine Learning Research, 9(Nov):2523\u20132547,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Experiment selection for causal discovery", "author": ["A. Hyttinen", "F. Eberhardt", "P.O. Hoyer"], "venue": "Journal of Machine Learning Research, 14(1):3041\u20133071,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Constraint-based causal discovery: Conflict resolution with answer set programming", "author": ["A. Hyttinen", "F. Eberhardt", "M. J\u00e4rvisalo"], "venue": "Proc. UAI, pages 340\u2013349,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Discovering cyclic causal models with latent variables: A general sat-based procedure", "author": ["A. Hyttinen", "P.O. Hoyer", "F. Eberhardt", "M. J\u00e4rvisalo"], "venue": "Uncertainty in Artificial Intelligence, page 301. Citeseer,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "The directions of selection bias", "author": ["Z. Jiang", "P. Ding"], "venue": "arXiv preprint arXiv:1609.07834,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Estimating high-dimensional directed acyclic graphs with the pc-algorithm", "author": ["M. Kalisch", "P. B\u00fchlmann"], "venue": "Journal of Machine Learning Research, 8(Mar):613\u2013636,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "A review of causal inference for biomedical informatics", "author": ["S. Kleinberg", "G. Hripcsak"], "venue": "Journal of biomedical informatics, 44(6):1102\u20131112,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "A review of bayesian networks and structure learning", "author": ["T.J. Koski", "J.M. Noble"], "venue": "Mathematica Applicanda, 40(1):53\u2013103,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic computational causal discovery for systems biology", "author": ["V. Lagani", "S. Triantafillou", "G. Ball", "J. Tegn\u00e9r", "I. Tsamardinos"], "venue": "Uncertainty in Biology, pages 33\u201373. Springer,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Parallelpc: an r package for efficient constraint based causal exploration", "author": ["T.D. Le", "T. Hoang", "J. Li", "L. Liu", "S. Hu"], "venue": "arXiv preprint arXiv:1510.03042,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast pc algorithm for high dimensional causal discovery with multi-core pcs", "author": ["T.D. Le", "T. Hoang", "J. Li", "L. Liu", "H. Liu"], "venue": "arXiv preprint arXiv:1502.02454,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring microrna\u2013mrna causal regulatory relationships from expression data", "author": ["T.D. Le", "L. Liu", "A. Tsykin", "G.J. Goodall", "B. Liu", "B.-Y. Sun", "J. Li"], "venue": "Bioinformatics, 29(6):765\u2013771,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Conservative independence-based causal structure learning in absence of adjacency faithfulness", "author": ["J. Lemeire", "S. Meganck", "F. Cartella", "T. Liu"], "venue": "International Journal of Approximate Reasoning, 53(9):1305\u20131325,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive thresholding in structure learning of a bayesian network", "author": ["B. Lerner", "M. Afek", "R. Bojmel"], "venue": "IJCAI,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "From observational studies to causal rule mining", "author": ["J. Li", "T.D. Le", "L. Liu", "J. Liu", "Z. Jin", "B. Sun", "S. Ma"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 7(2):14,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical approaches to causal relationship exploration", "author": ["J. Li", "L. Liu", "T. Le"], "venue": "Springer,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Controlling the false discovery rate of the association/causality structure learned with the pc algorithm", "author": ["J. Li", "Z.J. Wang"], "venue": "Journal of Machine Learning Research, 10(Feb):475\u2013514,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Big data problems on discovering and analyzing causal relationships in epidemiological data", "author": ["Y. Liang", "A.R. Mikler"], "venue": "Big Data, 2014 IEEE International Conference on, pages 11\u201318. IEEE,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Determining molecular predictors of adverse drug reactions with causality analysis based on structure learning", "author": ["M. Liu", "R. Cai", "Y. Hu", "M.E. Matheny", "J. Sun", "J. Hu", "H. Xu"], "venue": "Journal of the American Medical Informatics Association, 21(2):245\u2013251,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2014}, {"title": "Swamping and masking in markov boundary discovery", "author": ["X. Liu", "X. Liu"], "venue": "Machine Learning, pages 1\u201330,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "A review of some recent advances in causal inference", "author": ["M.H. Maathuis", "P. Nandy"], "venue": "Handbook of Big Data, page 387,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "A new class of non-shannon-type inequalities for entropies", "author": ["K. Makarychev", "Y. Makarychev", "A. Romashchenko", "N. Vereshchagin"], "venue": "Communications in Information and Systems, 2(2):147\u2013 166,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2002}, {"title": "Bayesian algorithms for causal data mining", "author": ["S. Mani", "C.F. Aliferis", "A.R. Statnikov", "M. NYU"], "venue": "NIPS Causality: Objectives and Assessment, pages 121\u2013136,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "A study in causal discovery from population-based infant birth and death records", "author": ["S. Mani", "G.F. Cooper"], "venue": "Proceedings of the AMIA Symposium, page 315. American Medical Informatics Association,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1999}, {"title": "Causal discovery from medical textual data", "author": ["S. Mani", "G.F. Cooper"], "venue": "Proceedings of the AMIA Symposium, page 542. American Medical Informatics Association,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2000}, {"title": "Causal discovery using a bayesian local causal discovery algorithm", "author": ["S. Mani", "G.F. Cooper"], "venue": "Medinfo, 11(Pt 1):731\u2013735,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2004}, {"title": "A constraint-based modelling approach to metabolic dysfunction in parkinson\u2019s disease", "author": ["L. Mao", "A. Nicolae", "M.A. Oliveira", "F. He", "S. Hachi", "R.M. Fleming"], "venue": "Computational and structural biotechnology journal, 13:484\u2013491,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian network induction via local neighborhoods", "author": ["D. Margaritis", "S. Thrun"], "venue": "Technical report,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2000}, {"title": "Bayesian network induction via local neighborhoods", "author": ["D. Margaritis", "S. Thrun"], "venue": "Advances in Neural Information Processing Systems, pages 505\u2013511,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2000}, {"title": "Causal inference and causal explanation with background knowledge", "author": ["C. Meek"], "venue": "Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, pages 403\u2013410. Morgan Kaufmann Publishers Inc.,", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning causal bayesian networks from observations and experiments: A decision theoretic approach", "author": ["S. Meganck", "P. Leray", "B. Manderick"], "venue": "International Conference on Modeling Decisions for Artificial Intelligence, pages 58\u201369. Springer,", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning bayesian networks", "author": ["R.E. Neapolitan"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2004}, {"title": "Causal inference with observational data", "author": ["A. Nichols"], "venue": "Stata Journal,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2007}, {"title": "Causation in epidemiology", "author": ["M. Parascandola", "D.L. Weed"], "venue": "Journal of Epidemiology and Community Health, 55(12):905\u2013912,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2001}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1988}, {"title": "An introduction to causal inference", "author": ["J. Pearl"], "venue": "The international journal of biostatistics, 6(2),", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2010}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["J. Pearl"], "venue": "Second Edition, Cambridge University Press,", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2014}, {"title": "Using markov blankets for causal structure learning", "author": ["J.-P. Pellet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research, 9(Jul):1295\u20131342,", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2008}, {"title": "Finding latent causes in causal networks: an efficient approach based on markov blankets", "author": ["J.-P. Pellet", "A. Elisseeff"], "venue": "NIPS\u201909, pages 1249\u20131256,", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning gaussian graphical models of gene networks with false discovery rate control", "author": ["J.M. Pe\u00f1a"], "venue": "European conference on evolutionary computation, machine learning and data mining in bioinformatics, pages 165\u2013176. Springer,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2008}, {"title": "Towards scalable and data efficient learning of markov boundaries", "author": ["J.M. Pe\u00f1a", "R. Nilsson", "J. Bj\u00f6rkegren", "J. Tegn\u00e9r"], "venue": "International Journal of Approximate Reasoning, 45(2):211\u2013232,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2007}, {"title": "Causal inference using invariant prediction: identification and confidence intervals", "author": ["J. Peters", "P. B\u00fchlmann", "N. Meinshausen"], "venue": "arXiv preprint arXiv:1501.01332,", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2015}, {"title": "Adjacency-faithfulness and conservative causal inference", "author": ["J. Ramsey", "J. Zhang", "P.L. Spirtes"], "venue": "UAI\u201906, pages 401\u2013408,", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2006}, {"title": "Ancestral graph markov models", "author": ["T. Richardson", "P. Spirtes"], "venue": "Annals of Statistics, pages 962\u20131030,", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2002}, {"title": "Estimating causal effects of treatments in randomized and nonrandomized studies", "author": ["D.B. Rubin"], "venue": "Journal of educational Psychology, 66(5):688,", "citeRegEx": "82", "shortCiteRegEx": null, "year": 1974}, {"title": "Causal inference using potential outcomes", "author": ["D.B. Rubin"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2011}, {"title": "Identifying causal gateways and mediators in complex spatio-temporal systems", "author": ["J. Runge", "V. Petoukhov", "J.F. Donges", "J. Hlinka", "N. Jajcay", "M. Vejmelka", "D. Hartman", "N. Marwan", "M. Palu\u0161", "J. Kurths"], "venue": "Nature communications, 6,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2015}, {"title": "The hidden life of latent variables: Bayesian learning with mixed graph models", "author": ["R. Silva", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research, 10(Jun):1187\u20131238,", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2009}, {"title": "Scalable techniques for mining causal structures", "author": ["C. Silverstein", "S. Brin", "R. Motwani", "J. Ullman"], "venue": "Data Mining and Knowledge Discovery, 4(2-3):163\u2013192,", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2000}, {"title": "Introduction to causal inference", "author": ["P. Spirtes"], "venue": "Journal of Machine Learning Research, 11(May):1643\u20131662,", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2010}, {"title": "An algorithm for fast recovery of sparse causal graphs", "author": ["P. Spirtes", "C. Glymour"], "venue": "Social science computer review, 9(1):62\u201372,", "citeRegEx": "89", "shortCiteRegEx": null, "year": 1991}, {"title": "Causation, prediction, and search", "author": ["P. Spirtes", "C.N. Glymour", "R. Scheines"], "venue": "MIT press,", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2000}, {"title": "A uniformly consistent estimator of causal effects under the k-triangle-faithfulness assumption", "author": ["P. Spirtes", "J. Zhang"], "venue": "Statistical Science,", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2014}, {"title": "Algorithms for discovery of multiple markov boundaries", "author": ["A. Statnikov", "J. Lemeir", "C.F. Aliferis"], "venue": "The Journal of Machine Learning Research, 14(1):499\u2013566,", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2013}, {"title": "Ultra-scalable and efficient methods for hybrid observational and experimental local causal pathway discovery", "author": ["A. Statnikov", "S. Ma", "M. Henaff", "N. Lytkin", "E. Efstathiadis", "E.R. Peskin", "C.F. Aliferis"], "venue": "J Mach Learn Res,", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2015}, {"title": "A direct approach to false discovery rates", "author": ["J.D. Storey"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(3):479\u2013498,", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2002}, {"title": "Integrating locally learned causal structures with overlapping variables", "author": ["R.E. Tillman", "D. Danks", "C. Glymour"], "venue": "Advances in Neural Information Processing Systems, pages 1665\u20131672,", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning equivalence classes of acyclic models with latent and selection variables from multiple datasets with overlapping variables", "author": ["R.E. Tillman", "P. Spirtes"], "venue": "AISTATS, pages 3\u201315,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2011}, {"title": "Active learning for structure in bayesian networks", "author": ["S. Tong", "D. Koller"], "venue": "International joint conference on artificial intelligence, volume 17, pages 863\u2013869. LAWRENCE ERLBAUM AS- SOCIATES LTD,", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2001}, {"title": "Constraint-based causal discovery from multiple interventions over overlapping variable sets", "author": ["S. Triantafillou", "I. Tsamardinos"], "venue": "J Machine Learn Res, 16:2147\u20132205,", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning causal structure from overlapping variable sets", "author": ["S. Triantafilou", "I. Tsamardinos", "I.G. Tollis"], "venue": "AISTATS, pages 860\u2013 867,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards principled feature selection: relevancy, filters and wrappers", "author": ["I. Tsamardinos", "C.F. Aliferis"], "venue": "AISTATS,", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2003}, {"title": "Time and sample efficient discovery of markov blankets and direct causal relations", "author": ["I. Tsamardinos", "C.F. Aliferis", "A. Statnikov"], "venue": "Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 673\u2013678. ACM,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2003}, {"title": "Algorithms for large scale markov blanket discovery", "author": ["I. Tsamardinos", "C.F. Aliferis", "A.R. Statnikov", "E. Statnikov"], "venue": "FLAIRS conference, volume 2,", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2003}, {"title": "Bounding the false discovery rate in local bayesian network learning", "author": ["I. Tsamardinos", "L.E. Brown"], "venue": "AAAI, pages 1100\u2013 1105,", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2008}, {"title": "The max-min hillclimbing bayesian network structure learning algorithm", "author": ["I. Tsamardinos", "L.E. Brown", "C.F. Aliferis"], "venue": "Machine learning, 65(1):31\u201378,", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards integrative causal analysis of heterogeneous data sets and studies", "author": ["I. Tsamardinos", "S. Triantafillou", "V. Lagani"], "venue": "Journal of Machine Learning Research, 13(Apr):1097\u20131157,", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2012}, {"title": "Equivalence and synthesis of causal models", "author": ["T. Vermal", "J. Pearl"], "venue": "Proceedings of Sixth Conference on Uncertainty in Artijicial Intelligence, pages 220\u2013227,", "citeRegEx": "106", "shortCiteRegEx": null, "year": 1991}, {"title": "Correlation and causation", "author": ["S. Wright"], "venue": "Journal of agricultural research, 20(7):557\u2013585,", "citeRegEx": "107", "shortCiteRegEx": null, "year": 1921}, {"title": "A definition of conditional mutual information for arbitrary ensembles", "author": ["A.D. Wyner"], "venue": "Information and Control, 38(1):51\u201359,", "citeRegEx": "108", "shortCiteRegEx": null, "year": 1978}, {"title": "A recursive method for structural learning of directed acyclic graphs", "author": ["X. Xie", "Z. Geng"], "venue": "Journal of Machine Learning Research, 9(Mar):459\u2013483,", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2008}, {"title": "Decomposition of structural learning about directed acyclic graphs", "author": ["X. Xie", "Z. Geng", "Q. Zhao"], "venue": "Artificial Intelligence, 170(4):422\u2013439,", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2006}, {"title": "Recursive autonomy identification for bayesian network structure learning", "author": ["R. Yehezkel", "B. Lerner"], "venue": "AISTATS, pages 429\u2013436. Citeseer,", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2005}, {"title": "Bayesian network structure learning by recursive autonomy identification", "author": ["R. Yehezkel", "B. Lerner"], "venue": "Journal of Machine Learning Research, 10(Jul):1527\u20131570,", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2009}, {"title": "Partial orientation and local structural learning of causal networks for prediction", "author": ["J. Yin", "Y. Zhou", "C. Wang", "P. He", "C. Zheng", "Z. Geng"], "venue": "WCCI Causation and Prediction Challenge, pages 93\u2013 105,", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2008}, {"title": "Bridging causal relevance and pattern discriminability: Mining emerging patterns from high-dimensional data", "author": ["K. Yu", "W. Ding", "H. Wang", "X. Wu"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 25(12):2721\u20132739,", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2013}, {"title": "Tornado forecasting with multiple markov boundaries", "author": ["K. Yu", "D. Wang", "W. Ding", "J. Pei", "D.L. Small", "S. Islam", "X. Wu"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2237\u20132246. ACM,", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2015}, {"title": "Markov blanket feature selection using representative sets", "author": ["K. Yu", "X. Wu", "W. Ding", "Y. Mu", "H. Wang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, DOI: 10.1109/TNNLS.2016.2602365,", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring causal relationships with streaming features", "author": ["K. Yu", "X. Wu", "W. Ding", "H. Wang"], "venue": "The Computer Journal, 55(9):1103\u2013 1117,", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2012}, {"title": "On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias", "author": ["J. Zhang"], "venue": "Artificial Intelligence, 172(16):1873\u20131896,", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2008}, {"title": "Detection of unfaithfulness and robust causal inference", "author": ["J. Zhang", "P. Spirtes"], "venue": "Minds and Machines, 18(2):239\u2013271,", "citeRegEx": "119", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 28, "context": "Discovering causal relationships between variables from data is fundamental in all areas of sciences, such as computer science, medicine, statistics, economy, and social sciences [29, 44, 52, 69, 74, 90, 114].", "startOffset": 179, "endOffset": 208}, {"referenceID": 43, "context": "Discovering causal relationships between variables from data is fundamental in all areas of sciences, such as computer science, medicine, statistics, economy, and social sciences [29, 44, 52, 69, 74, 90, 114].", "startOffset": 179, "endOffset": 208}, {"referenceID": 51, "context": "Discovering causal relationships between variables from data is fundamental in all areas of sciences, such as computer science, medicine, statistics, economy, and social sciences [29, 44, 52, 69, 74, 90, 114].", "startOffset": 179, "endOffset": 208}, {"referenceID": 68, "context": "Discovering causal relationships between variables from data is fundamental in all areas of sciences, such as computer science, medicine, statistics, economy, and social sciences [29, 44, 52, 69, 74, 90, 114].", "startOffset": 179, "endOffset": 208}, {"referenceID": 73, "context": "Discovering causal relationships between variables from data is fundamental in all areas of sciences, such as computer science, medicine, statistics, economy, and social sciences [29, 44, 52, 69, 74, 90, 114].", "startOffset": 179, "endOffset": 208}, {"referenceID": 88, "context": "Discovering causal relationships between variables from data is fundamental in all areas of sciences, such as computer science, medicine, statistics, economy, and social sciences [29, 44, 52, 69, 74, 90, 114].", "startOffset": 179, "endOffset": 208}, {"referenceID": 112, "context": "Discovering causal relationships between variables from data is fundamental in all areas of sciences, such as computer science, medicine, statistics, economy, and social sciences [29, 44, 52, 69, 74, 90, 114].", "startOffset": 179, "endOffset": 208}, {"referenceID": 43, "context": "alternative to randomized controlled experiments [44, 53, 83, 117], since observational data can often be collected cheaply and is abundant, while in many settings, randomized controlled experiments are unethical or impossible [38, 70, 82].", "startOffset": 49, "endOffset": 66}, {"referenceID": 52, "context": "alternative to randomized controlled experiments [44, 53, 83, 117], since observational data can often be collected cheaply and is abundant, while in many settings, randomized controlled experiments are unethical or impossible [38, 70, 82].", "startOffset": 49, "endOffset": 66}, {"referenceID": 82, "context": "alternative to randomized controlled experiments [44, 53, 83, 117], since observational data can often be collected cheaply and is abundant, while in many settings, randomized controlled experiments are unethical or impossible [38, 70, 82].", "startOffset": 49, "endOffset": 66}, {"referenceID": 115, "context": "alternative to randomized controlled experiments [44, 53, 83, 117], since observational data can often be collected cheaply and is abundant, while in many settings, randomized controlled experiments are unethical or impossible [38, 70, 82].", "startOffset": 49, "endOffset": 66}, {"referenceID": 37, "context": "alternative to randomized controlled experiments [44, 53, 83, 117], since observational data can often be collected cheaply and is abundant, while in many settings, randomized controlled experiments are unethical or impossible [38, 70, 82].", "startOffset": 227, "endOffset": 239}, {"referenceID": 69, "context": "alternative to randomized controlled experiments [44, 53, 83, 117], since observational data can often be collected cheaply and is abundant, while in many settings, randomized controlled experiments are unethical or impossible [38, 70, 82].", "startOffset": 227, "endOffset": 239}, {"referenceID": 81, "context": "alternative to randomized controlled experiments [44, 53, 83, 117], since observational data can often be collected cheaply and is abundant, while in many settings, randomized controlled experiments are unethical or impossible [38, 70, 82].", "startOffset": 227, "endOffset": 239}, {"referenceID": 33, "context": "particularly in the area of graphical causal modeling [34, 44, 74, 87].", "startOffset": 54, "endOffset": 70}, {"referenceID": 43, "context": "particularly in the area of graphical causal modeling [34, 44, 74, 87].", "startOffset": 54, "endOffset": 70}, {"referenceID": 73, "context": "particularly in the area of graphical causal modeling [34, 44, 74, 87].", "startOffset": 54, "endOffset": 70}, {"referenceID": 17, "context": "The most frequently used causal models belong to two broad families: (1) causal Bayesian networks [18, 72, 87], and (2) structural equation models (functional causal models) [26, 107].", "startOffset": 98, "endOffset": 110}, {"referenceID": 71, "context": "The most frequently used causal models belong to two broad families: (1) causal Bayesian networks [18, 72, 87], and (2) structural equation models (functional causal models) [26, 107].", "startOffset": 98, "endOffset": 110}, {"referenceID": 25, "context": "The most frequently used causal models belong to two broad families: (1) causal Bayesian networks [18, 72, 87], and (2) structural equation models (functional causal models) [26, 107].", "startOffset": 174, "endOffset": 183}, {"referenceID": 105, "context": "The most frequently used causal models belong to two broad families: (1) causal Bayesian networks [18, 72, 87], and (2) structural equation models (functional causal models) [26, 107].", "startOffset": 174, "endOffset": 183}, {"referenceID": 14, "context": "Score-based algorithms assign a score to each candidate Bayesian network for measuring how well the candidate Bayesian network fits a data set [15, 21].", "startOffset": 143, "endOffset": 151}, {"referenceID": 20, "context": "Score-based algorithms assign a score to each candidate Bayesian network for measuring how well the candidate Bayesian network fits a data set [15, 21].", "startOffset": 143, "endOffset": 151}, {"referenceID": 71, "context": "with conditional independence tests through analyzing the probabilistic relations entailed by the Markov property of Bayesian networks [72, 74].", "startOffset": 135, "endOffset": 143}, {"referenceID": 73, "context": "with conditional independence tests through analyzing the probabilistic relations entailed by the Markov property of Bayesian networks [72, 74].", "startOffset": 135, "endOffset": 143}, {"referenceID": 84, "context": "of latent variables and the exact locations of the latent variables with the observed variables in data before learning starts [85].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "based method [14].", "startOffset": 13, "endOffset": 17}, {"referenceID": 17, "context": "The computational intractability is a key drawback of the score-based methods [18].", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "based approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].", "startOffset": 146, "endOffset": 154}, {"referenceID": 83, "context": "based approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].", "startOffset": 146, "endOffset": 154}, {"referenceID": 42, "context": "based approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].", "startOffset": 171, "endOffset": 179}, {"referenceID": 48, "context": "based approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].", "startOffset": 171, "endOffset": 179}, {"referenceID": 19, "context": "based approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].", "startOffset": 197, "endOffset": 209}, {"referenceID": 55, "context": "based approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].", "startOffset": 197, "endOffset": 209}, {"referenceID": 61, "context": "based approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].", "startOffset": 197, "endOffset": 209}, {"referenceID": 54, "context": "based approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].", "startOffset": 228, "endOffset": 240}, {"referenceID": 63, "context": "based approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].", "startOffset": 228, "endOffset": 240}, {"referenceID": 70, "context": "based approaches have gradually attracted a lot of recent interest and has been widely applied to diverse real-world problems in climate research [28, 84], bioinformatics [43, 49], medical science [20, 56, 62], and epidemiology [55, 64, 71].", "startOffset": 228, "endOffset": 240}, {"referenceID": 7, "context": "Although some reviews on causal inference have been proposed [8, 25, 45, 58, 73, 88], those reviews attempt to", "startOffset": 61, "endOffset": 84}, {"referenceID": 24, "context": "Although some reviews on causal inference have been proposed [8, 25, 45, 58, 73, 88], those reviews attempt to", "startOffset": 61, "endOffset": 84}, {"referenceID": 44, "context": "Although some reviews on causal inference have been proposed [8, 25, 45, 58, 73, 88], those reviews attempt to", "startOffset": 61, "endOffset": 84}, {"referenceID": 57, "context": "Although some reviews on causal inference have been proposed [8, 25, 45, 58, 73, 88], those reviews attempt to", "startOffset": 61, "endOffset": 84}, {"referenceID": 72, "context": "Although some reviews on causal inference have been proposed [8, 25, 45, 58, 73, 88], those reviews attempt to", "startOffset": 61, "endOffset": 84}, {"referenceID": 86, "context": "Although some reviews on causal inference have been proposed [8, 25, 45, 58, 73, 88], those reviews attempt to", "startOffset": 61, "endOffset": 84}, {"referenceID": 80, "context": "cestral graph) [81] to represent casual relations when causal sufficiency is not assumed.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "to discover the skeleton including all variables [13, 90, 90, 106], while the local method mines local skeletons, i.", "startOffset": 49, "endOffset": 66}, {"referenceID": 88, "context": "to discover the skeleton including all variables [13, 90, 90, 106], while the local method mines local skeletons, i.", "startOffset": 49, "endOffset": 66}, {"referenceID": 88, "context": "to discover the skeleton including all variables [13, 90, 90, 106], while the local method mines local skeletons, i.", "startOffset": 49, "endOffset": 66}, {"referenceID": 104, "context": "to discover the skeleton including all variables [13, 90, 90, 106], while the local method mines local skeletons, i.", "startOffset": 49, "endOffset": 66}, {"referenceID": 64, "context": ", the set of adjacent variables (parents and children) or Markov blanket of each vertex, then constructs a global skeleton by local skeletons [65, 75].", "startOffset": 142, "endOffset": 150}, {"referenceID": 74, "context": ", the set of adjacent variables (parents and children) or Markov blanket of each vertex, then constructs a global skeleton by local skeletons [65, 75].", "startOffset": 142, "endOffset": 150}, {"referenceID": 66, "context": "Meek rules and Zhang Jiji\u2019s rules) defined in [67, 118] using observational data.", "startOffset": 46, "endOffset": 55}, {"referenceID": 116, "context": "Meek rules and Zhang Jiji\u2019s rules) defined in [67, 118] using observational data.", "startOffset": 46, "endOffset": 55}, {"referenceID": 21, "context": "entation [22, 97].", "startOffset": 9, "endOffset": 17}, {"referenceID": 95, "context": "entation [22, 97].", "startOffset": 9, "endOffset": 17}, {"referenceID": 36, "context": "\u2022 Edge orientation using both observational data and experimental data, which first applies the rules in (1) to orient edges as many as possible, then orients the remaining unoriented edges with the method in (2) [37].", "startOffset": 213, "endOffset": 217}, {"referenceID": 0, "context": "In constraint-based algorithms, the independence tests generally can be implemented using the G test [1], mutual information [59, 108], and Fisher\u2019s Z-test [77].", "startOffset": 101, "endOffset": 104}, {"referenceID": 58, "context": "In constraint-based algorithms, the independence tests generally can be implemented using the G test [1], mutual information [59, 108], and Fisher\u2019s Z-test [77].", "startOffset": 125, "endOffset": 134}, {"referenceID": 106, "context": "In constraint-based algorithms, the independence tests generally can be implemented using the G test [1], mutual information [59, 108], and Fisher\u2019s Z-test [77].", "startOffset": 125, "endOffset": 134}, {"referenceID": 76, "context": "In constraint-based algorithms, the independence tests generally can be implemented using the G test [1], mutual information [59, 108], and Fisher\u2019s Z-test [77].", "startOffset": 156, "endOffset": 160}, {"referenceID": 0, "context": ",N/((ri\u22121)(rj\u22121)rk) \u2265 \u03c6 (N is the total number of instances, ri is the number of distinct values of Vi, and \u03c6 is often set to 5 or 10) [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 50, "context": "evaluated in terms of reliability [51].", "startOffset": 34, "endOffset": 38}, {"referenceID": 85, "context": "includes all variables involved in data is computational expensive or even infeasible in large-scale data mining applications with thousands of variables, thus it is challenging to develop algorithms scaling up to real-world data with high dimensionality [86].", "startOffset": 255, "endOffset": 259}, {"referenceID": 90, "context": "such as biomedical science [92] and climate research [115, 116], the assumption is often violated.", "startOffset": 27, "endOffset": 31}, {"referenceID": 113, "context": "such as biomedical science [92] and climate research [115, 116], the assumption is often violated.", "startOffset": 53, "endOffset": 63}, {"referenceID": 114, "context": "such as biomedical science [92] and climate research [115, 116], the assumption is often violated.", "startOffset": 53, "endOffset": 63}, {"referenceID": 117, "context": "Thus it is challenging to design robust causal inference algorithms by relaxing this assumption [119].", "startOffset": 96, "endOffset": 101}, {"referenceID": 16, "context": "In realworld applications, such as medical science, epidemiology, and sociology, it is impossible to ensure that all common causes are measured in study data [17].", "startOffset": 158, "endOffset": 162}, {"referenceID": 88, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 127, "endOffset": 131}, {"referenceID": 88, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 144, "endOffset": 148}, {"referenceID": 79, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 164, "endOffset": 168}, {"referenceID": 89, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 186, "endOffset": 190}, {"referenceID": 49, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 207, "endOffset": 211}, {"referenceID": 15, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 231, "endOffset": 235}, {"referenceID": 15, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 262, "endOffset": 266}, {"referenceID": 53, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 283, "endOffset": 287}, {"referenceID": 21, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 301, "endOffset": 317}, {"referenceID": 26, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 301, "endOffset": 317}, {"referenceID": 67, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 301, "endOffset": 317}, {"referenceID": 95, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 301, "endOffset": 317}, {"referenceID": 11, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 360, "endOffset": 364}, {"referenceID": 32, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 404, "endOffset": 417}, {"referenceID": 32, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 404, "endOffset": 417}, {"referenceID": 107, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 404, "endOffset": 417}, {"referenceID": 64, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 441, "endOffset": 445}, {"referenceID": 74, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 467, "endOffset": 471}, {"referenceID": 109, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 498, "endOffset": 508}, {"referenceID": 110, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 498, "endOffset": 508}, {"referenceID": 18, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 525, "endOffset": 537}, {"referenceID": 60, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 525, "endOffset": 537}, {"referenceID": 85, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 525, "endOffset": 537}, {"referenceID": 111, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 561, "endOffset": 566}, {"referenceID": 30, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 583, "endOffset": 587}, {"referenceID": 91, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 652, "endOffset": 656}, {"referenceID": 22, "context": "Algorithm approach Skeleton v-structure edge orientation Reference SGS global \u221a \u221a Rules 1 to 4 (meek rules) [90] TPDA global \u221a [13] PC global \u221a [90] CPC global \u221a \u221a [80] VCSGS global \u221a \u221a [91] ACPC global \u221a \u221a [50] PC-stable global \u221a [16] CPC/MPC-stable global \u221a \u221a [16] PC-FDR global \u221a [54] AIT global \u221a [22, 27, 68, 97] ALCBN global \u221a (hybrid orientation rules) [12] Xie-geng algorithm local-to-global \u221a \u221a [33, 33, 109] GSBN local-to-global \u221a [65] TC local-to-global \u221a [75] RAI local-to-global \u221a \u221a \u221a [111, 112] LCD local \u221a \u221a \u221a [19, 61, 86] PCD-by-PCD local \u221a \u221a \u221a [113] CMB local \u221a \u221a \u221a [31] ODLP local \u221a ignore the step \u221a (experimental orientation rules) [93] SLPR global (multiple data) \u221a \u221a [23]", "startOffset": 689, "endOffset": 693}, {"referenceID": 88, "context": "[90], which provides a theoretical framework for learning structures of causal models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 88, "context": "In the original version of the SGS algorithm [90], at the step of edge orientation, Rule 1 is used to avoid any alternative orientation would yield a new v-structure, while Rule 2 is employed to avoid any alternative orientations would generate a directed cycle.", "startOffset": 45, "endOffset": 49}, {"referenceID": 66, "context": "only if any orientation other than the orientation indicated by the rule would lead to a new unshielded collider or a directed cycle [67].", "startOffset": 133, "endOffset": 137}, {"referenceID": 66, "context": "Later, Meek [67] extended the rules by the additional two rules (noted as Rule 3 and Rule 4) which are summarized as the following.", "startOffset": 12, "endOffset": 16}, {"referenceID": 66, "context": "Meek [67] proved that these four rules are sufficient in terms of the soundness and completeness (please refer", "startOffset": 5, "endOffset": 9}, {"referenceID": 66, "context": "to Theorem 2 and Theorem 3 in Section 2 in [67]).", "startOffset": 43, "endOffset": 47}, {"referenceID": 88, "context": "[90] Given the distribution P over set of variables V and its corresponding DAG G, under Assumptions 1 to 3, the output of the SGS algorithm is the CPDAG that represents G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 87, "context": "The PC (PC stands for Peter Spirtes and Clark Glymour who invented this algorithm) algorithm [89, 90] was proposed to improve learning efficiency in skeleton identification (Step 1 in Algorithm 1).", "startOffset": 93, "endOffset": 101}, {"referenceID": 88, "context": "The PC (PC stands for Peter Spirtes and Clark Glymour who invented this algorithm) algorithm [89, 90] was proposed to improve learning efficiency in skeleton identification (Step 1 in Algorithm 1).", "startOffset": 93, "endOffset": 101}, {"referenceID": 1, "context": "[2, 90] Under Assumptions 1 to 2, in a DAG G, there is an edge between the pair of variables Vi \u2208 V and Vj \u2208 V , if and only if Vi \u22a56 Vj |S, for all S \u2286 pa(Vi)\\{Vj} and S \u2286 pa(Vj)\\{Vi}.", "startOffset": 0, "endOffset": 7}, {"referenceID": 88, "context": "[2, 90] Under Assumptions 1 to 2, in a DAG G, there is an edge between the pair of variables Vi \u2208 V and Vj \u2208 V , if and only if Vi \u22a56 Vj |S, for all S \u2286 pa(Vi)\\{Vj} and S \u2286 pa(Vj)\\{Vi}.", "startOffset": 0, "endOffset": 7}, {"referenceID": 87, "context": "[89, 90] Under Assumptions 1 to 3, the PC algorithm gives the same output as the SGS algorithm.", "startOffset": 0, "endOffset": 8}, {"referenceID": 88, "context": "[89, 90] Under Assumptions 1 to 3, the PC algorithm gives the same output as the SGS algorithm.", "startOffset": 0, "endOffset": 8}, {"referenceID": 73, "context": "[87] and Pearl [74] described asymptotically consistent", "startOffset": 15, "endOffset": 19}, {"referenceID": 41, "context": "Kalisch and B\u00fchlmann [42] showed the PC-algorithm is asymptotically consistent for the equivalence class of the DAG and its skeleton with corresponding high-dimensional and sparse Gaussian distribution.", "startOffset": 21, "endOffset": 25}, {"referenceID": 35, "context": "Harris and Drton [36] studied the PC algorithm in the nonparanormal distributions, i.", "startOffset": 17, "endOffset": 21}, {"referenceID": 79, "context": "To the beginning, we introduce two consequences of the faithfulness assumption proposed by [80], i.", "startOffset": 91, "endOffset": 95}, {"referenceID": 117, "context": "The Adjacency-Faithfulness and the Orientation-Faithfulness both are consequences of the faithfulness assumption, but they together do not imply the faithfulness assumption [119].", "startOffset": 173, "endOffset": 178}, {"referenceID": 79, "context": "The triple is a v-structure if and only if the set S does not contain Vk [80, 119].", "startOffset": 73, "endOffset": 82}, {"referenceID": 117, "context": "The triple is a v-structure if and only if the set S does not contain Vk [80, 119].", "startOffset": 73, "endOffset": 82}, {"referenceID": 79, "context": "[80] for improving the robustness of the PC algorithm in the orientation phase.", "startOffset": 0, "endOffset": 4}, {"referenceID": 79, "context": "[80] extended the PC algorithm to capture violations of the Orientation-Faithfulness assumption.", "startOffset": 0, "endOffset": 4}, {"referenceID": 117, "context": "[119] Assuming the causal Markov condition and the Adjacency-Faithfulness condition hold, any violation of the Orientation-Faithfulness condition is detectable.", "startOffset": 0, "endOffset": 5}, {"referenceID": 117, "context": "Zhang and Spirtes [119] proved that given the Causal Markov, Minimality, and Triangle-Faithfulness assumptions, any violations of faithfulness are detectable.", "startOffset": 18, "endOffset": 23}, {"referenceID": 89, "context": "Then Spirtes and Zhang [91] proposed the VCSGS algorithm under those two weaker assumptions of faithfullness.", "startOffset": 23, "endOffset": 27}, {"referenceID": 89, "context": "Spirtes and Zhang [91] only analyzed the soundness of", "startOffset": 18, "endOffset": 22}, {"referenceID": 117, "context": "More explanations and examples for the Causal Minimality assumption, Triangle-Faithfulness Condition, and the AdjacencyFaithfulness condition, please refer to the work [119].", "startOffset": 168, "endOffset": 173}, {"referenceID": 49, "context": "[50] proposed two specific violations of the Adjacency-Faithfulness assumption, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[50] developed the Adjacency Conservative PC (ACPC) algorithm to deal with the two violations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "and conclusions for different variable orderings [17].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Colombo and Maathuis [17] proposed the PC-stable algorithm and its variants to address this order-dependence problem.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "[17] Let the distribution of P be faithful to a DAG G, and assume that we are given perfect conditional independence information about all pairs of variables (Fi, Fj) in P given subsets S \u2286 F \\{Fi\u222aFj}.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] The skeleton resulting of the PC-stable algorithm is order-independent.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The CPC-stable algorithm [17] is the combination of the PC-stable algorithm and the CPC algorithm, that is, using the idea of the CPC algorithm to identify v-structures.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "Maathuis [17] proposed the majority rule PC-stable algorithm (MPC-stable).", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "[17] Let the distribution of P be faithful to a DAG G, and assume that we are given perfect conditional independence information about all pairs of variables (Fi;Fj) in P given subsets S \u2286 F \\ {Fi \u222a Fj}.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] The decisions about v-structures in the sample versions of the CPC/MPC-stable algorithms are", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "The work [47, 48] proposed the parallelized versions of the PC algorithm and the PC-stable algorithm using the parallel computing technique, and showed that the parallel PC and PC-stable algorithms produce the same outputs as the PC and PC-", "startOffset": 9, "endOffset": 17}, {"referenceID": 47, "context": "The work [47, 48] proposed the parallelized versions of the PC algorithm and the PC-stable algorithm using the parallel computing technique, and showed that the parallel PC and PC-stable algorithms produce the same outputs as the PC and PC-", "startOffset": 9, "endOffset": 17}, {"referenceID": 9, "context": "The false discovery rate (FDR) [10, 94] is a criterion to assess the errors when multiple hypotheses are simultaneously tested.", "startOffset": 31, "endOffset": 39}, {"referenceID": 92, "context": "The false discovery rate (FDR) [10, 94] is a criterion to assess the errors when multiple hypotheses are simultaneously tested.", "startOffset": 31, "endOffset": 39}, {"referenceID": 15, "context": "Colombo and Maathuis [16] proposed the PCfdr-skeleton algorithm by embedding an FDR-control procedure into the PC algorithm to curb the error rate of the skeleton of the learned PDAGs.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "Against the PCfdr-skeleton algorithm, Bromberg and Margaritis [12] proposed the AIT (Argumentative Independence Test) framework to improve the reliability of the constraintbased algorithms when independence tests on small data sets are not reliable.", "startOffset": 62, "endOffset": 66}, {"referenceID": 71, "context": "a knowledge base containing a set of independence facts that are related through Pearls well-known axioms [72], as shown as follows.", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "feasible logic proposed by Amgoud and Cayrol [5] to reason about and correct errors.", "startOffset": 45, "endOffset": 48}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "al [33] proposed a decomposition method for", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "The rational behind the idea is that if vertices Vi and Vj are adjacent in the moral graph but there exists a vertex set S such that Vi \u22a5 Vj |S, then there exists a decomposed subgraph that contains Vi, Vj and a variable set S \u2032 such that Vi \u22a5 Vj |S , and vice versa [33].", "startOffset": 267, "endOffset": 271}, {"referenceID": 32, "context": "To decompose a graph into subgraphs, the approach in [33] needs a moral graph and requires that each dseparator (conditional sets) has a complete subgraph in the moral graph.", "startOffset": 53, "endOffset": 57}, {"referenceID": 108, "context": "[110] proposed", "startOffset": 0, "endOffset": 5}, {"referenceID": 107, "context": "[109] proposed a recursive method for learning structures of DAGs.", "startOffset": 0, "endOffset": 5}, {"referenceID": 65, "context": "Margaritis and Thrun [66] proposed the Grow-Shrink Bayesain network learning (GSBN) algorithm for constrain-based causal network learning.", "startOffset": 21, "endOffset": 25}, {"referenceID": 98, "context": "To conquer this drawback of the GSMB algorithm, Tsamardinos and Aliferis [100] proposed a modified version of the GSMB algorithm, called the IAMB algorithm, which guarantees to find the actual Markov", "startOffset": 73, "endOffset": 78}, {"referenceID": 100, "context": "blanket given enough training data and the method is more sample efficient than GSMB, and its variants, such as inter-IAMB [102], Fast-IAMB [102], and IAMBFDR [77].", "startOffset": 123, "endOffset": 128}, {"referenceID": 100, "context": "blanket given enough training data and the method is more sample efficient than GSMB, and its variants, such as inter-IAMB [102], Fast-IAMB [102], and IAMBFDR [77].", "startOffset": 140, "endOffset": 145}, {"referenceID": 76, "context": "blanket given enough training data and the method is more sample efficient than GSMB, and its variants, such as inter-IAMB [102], Fast-IAMB [102], and IAMBFDR [77].", "startOffset": 159, "endOffset": 163}, {"referenceID": 1, "context": "Thus, HITONMB [2, 4] and MMMB [101, 104] were introduced to mitigate", "startOffset": 14, "endOffset": 20}, {"referenceID": 3, "context": "Thus, HITONMB [2, 4] and MMMB [101, 104] were introduced to mitigate", "startOffset": 14, "endOffset": 20}, {"referenceID": 99, "context": "Thus, HITONMB [2, 4] and MMMB [101, 104] were introduced to mitigate", "startOffset": 30, "endOffset": 40}, {"referenceID": 102, "context": "Thus, HITONMB [2, 4] and MMMB [101, 104] were introduced to mitigate", "startOffset": 30, "endOffset": 40}, {"referenceID": 1, "context": "As an efficient implementation of Step 1, two major algorithms HITON-PC [2, 4] and MMPC", "startOffset": 72, "endOffset": 78}, {"referenceID": 3, "context": "As an efficient implementation of Step 1, two major algorithms HITON-PC [2, 4] and MMPC", "startOffset": 72, "endOffset": 78}, {"referenceID": 99, "context": "were introduced [101, 103].", "startOffset": 16, "endOffset": 26}, {"referenceID": 101, "context": "were introduced [101, 103].", "startOffset": 16, "endOffset": 26}, {"referenceID": 77, "context": "Following the ideas above, PCMB [78], IPCMB [30], STMB [32], WLCMB [57] was also proposed to efficient and effective discovery of Markov blankets.", "startOffset": 32, "endOffset": 36}, {"referenceID": 29, "context": "Following the ideas above, PCMB [78], IPCMB [30], STMB [32], WLCMB [57] was also proposed to efficient and effective discovery of Markov blankets.", "startOffset": 44, "endOffset": 48}, {"referenceID": 31, "context": "Following the ideas above, PCMB [78], IPCMB [30], STMB [32], WLCMB [57] was also proposed to efficient and effective discovery of Markov blankets.", "startOffset": 55, "endOffset": 59}, {"referenceID": 56, "context": "Following the ideas above, PCMB [78], IPCMB [30], STMB [32], WLCMB [57] was also proposed to efficient and effective discovery of Markov blankets.", "startOffset": 67, "endOffset": 71}, {"referenceID": 98, "context": "Total Conditioning (TC) algorithm Tsamardinos and Aliferis [100] associated Markov blankets in Bayesian networks with strongly relevant variables defined by Kohavi and John in feature selection, then transferred the feature selection task to the discovery of Markov blankets in", "startOffset": 59, "endOffset": 64}, {"referenceID": 74, "context": "Based on the work, in contrast, Pellet and Elisseeff [75] connected the problem of learning causal structures with a problem of feature selection in data mining.", "startOffset": 53, "endOffset": 57}, {"referenceID": 34, "context": "They proposed the Total Conditioning (TC) algorithm, and its improved version, the TCbw algorithm, using the Recursive Feature Elimination (RFE) algorithm [35].", "startOffset": 155, "endOffset": 159}, {"referenceID": 109, "context": "The RAI (Recursive Autonomy Identification) algorithm was presented in [111, 112] which uses a structure decomposition approach.", "startOffset": 71, "endOffset": 81}, {"referenceID": 110, "context": "The RAI (Recursive Autonomy Identification) algorithm was presented in [111, 112] which uses a structure decomposition approach.", "startOffset": 71, "endOffset": 81}, {"referenceID": 71, "context": "[72] A node Y in G(V,E) is an exogenous cause to G(V ,E), where V \u2282 V and E \u2282 E, if Y / \u2208 V and \u2200X \u2208 V , Y \u2208 Pa(X,G) or Y / \u2208 Adj(X,G).", "startOffset": 0, "endOffset": 4}, {"referenceID": 110, "context": "[112] In a DAG G(V,E), a sub-structureG(V , E) such that V A \u2282 V and E \u2282 E is said to be autonomous in G given a set Vex \u2282 V of exogenous causes to G A if \u2200X \u2208 V , Pa(X,G) \u2282 {V A \u222a Vex}.", "startOffset": 0, "endOffset": 5}, {"referenceID": 88, "context": "The existing methods for casual discovery fully rely on observational data to discover causal relations between variables up to a Markov equivalence class, and thus leave many edge directions undetermined [90].", "startOffset": 205, "endOffset": 209}, {"referenceID": 21, "context": "The type of methods includes the work of [22, 27, 68, 97] and the method proposed by He and Geng [37].", "startOffset": 41, "endOffset": 57}, {"referenceID": 26, "context": "The type of methods includes the work of [22, 27, 68, 97] and the method proposed by He and Geng [37].", "startOffset": 41, "endOffset": 57}, {"referenceID": 67, "context": "The type of methods includes the work of [22, 27, 68, 97] and the method proposed by He and Geng [37].", "startOffset": 41, "endOffset": 57}, {"referenceID": 95, "context": "The type of methods includes the work of [22, 27, 68, 97] and the method proposed by He and Geng [37].", "startOffset": 41, "endOffset": 57}, {"referenceID": 36, "context": "The type of methods includes the work of [22, 27, 68, 97] and the method proposed by He and Geng [37].", "startOffset": 97, "endOffset": 101}, {"referenceID": 67, "context": "The ALCBN algorithm [68] uses either the mini-max, maximin or Laplace decision criteria, whereas the approach proposed by He and Geng [37] uses either the maxi-min", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": "The ALCBN algorithm [68] uses either the mini-max, maximin or Laplace decision criteria, whereas the approach proposed by He and Geng [37] uses either the maxi-min", "startOffset": 134, "endOffset": 138}, {"referenceID": 67, "context": "The method in [68] repeats this process until all edges in the graph are oriented.", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "The algorithm in [37] first partitions the graph into chain components which are only connected", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "The LCD (Local Causal Discovery) algorithm and its variants [19, 61, 86] aim to find causal edges by testing the dependence/independence relationships among", "startOffset": 60, "endOffset": 72}, {"referenceID": 60, "context": "The LCD (Local Causal Discovery) algorithm and its variants [19, 61, 86] aim to find causal edges by testing the dependence/independence relationships among", "startOffset": 60, "endOffset": 72}, {"referenceID": 85, "context": "The LCD (Local Causal Discovery) algorithm and its variants [19, 61, 86] aim to find causal edges by testing the dependence/independence relationships among", "startOffset": 60, "endOffset": 72}, {"referenceID": 62, "context": "Bayesian Local Causal Discovery (BLCD) [63] and CD-B/H algorithm [60] explores the Y-structures among the nodes in a Markov blanket to infer causal edges using Baysain scoring-based method.", "startOffset": 39, "endOffset": 43}, {"referenceID": 59, "context": "Bayesian Local Causal Discovery (BLCD) [63] and CD-B/H algorithm [60] explores the Y-structures among the nodes in a Markov blanket to infer causal edges using Baysain scoring-based method.", "startOffset": 65, "endOffset": 69}, {"referenceID": 111, "context": "target [113].", "startOffset": 7, "endOffset": 12}, {"referenceID": 30, "context": "Based on the idea of the PCD-by-PCD algorithm, Tian and Ji [31] propose a new Causal Markov Blanket (CMB) discovery algorithm to identify the direct", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "and may not orient all edge directions with observational data alone [3, 93].", "startOffset": 69, "endOffset": 76}, {"referenceID": 91, "context": "and may not orient all edge directions with observational data alone [3, 93].", "startOffset": 69, "endOffset": 76}, {"referenceID": 91, "context": "[93] introduced a local causal discovery method, called ODLP, for discovery of local causal pathways around the target variable of interest (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 90, "context": "of parents and children when the faithfullness assumption is violated [92], while the TIE* algorithm was designed to discover multiple Markov blanket of a target as the assumption does not hold in data.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "Instead of learning with a single observational data set, sometimes we need to deal with several data sets that do not share the same set of variables, but share significant overlap variable sets due to privacy or ethics [23, 24].", "startOffset": 221, "endOffset": 229}, {"referenceID": 23, "context": "Instead of learning with a single observational data set, sometimes we need to deal with several data sets that do not share the same set of variables, but share significant overlap variable sets due to privacy or ethics [23, 24].", "startOffset": 221, "endOffset": 229}, {"referenceID": 22, "context": "As a pioneering work, Danks [23] proposed the SLPR (Structure learning using prior results) algorithm to learning the causal structure over the joint set of observed variables.", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "In real-world applications, such as medical science, epidemiology, and sociology, it is impossible to ensure that all common causes are measured in real-world data [6, 17].", "startOffset": 164, "endOffset": 171}, {"referenceID": 16, "context": "In real-world applications, such as medical science, epidemiology, and sociology, it is impossible to ensure that all common causes are measured in real-world data [6, 17].", "startOffset": 164, "endOffset": 171}, {"referenceID": 84, "context": "In this case, by involved latent variable into the structure to be learnt, these methods then transfer the problem of latent variable learning to the problem of missing data completing [85].", "startOffset": 185, "endOffset": 189}, {"referenceID": 16, "context": "Moreover, with latent variables, the space of DAGs is not closed under marginalization [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 80, "context": "Instead of DAGs, using a MAG (maximal ancestral graph) [81] model to represent latent common causes is an emerging research direction in causal discovery without the assumption of causal sufficiency [11, 17, 85].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Instead of DAGs, using a MAG (maximal ancestral graph) [81] model to represent latent common causes is an emerging research direction in causal discovery without the assumption of causal sufficiency [11, 17, 85].", "startOffset": 199, "endOffset": 211}, {"referenceID": 16, "context": "Instead of DAGs, using a MAG (maximal ancestral graph) [81] model to represent latent common causes is an emerging research direction in causal discovery without the assumption of causal sufficiency [11, 17, 85].", "startOffset": 199, "endOffset": 211}, {"referenceID": 84, "context": "Instead of DAGs, using a MAG (maximal ancestral graph) [81] model to represent latent common causes is an emerging research direction in causal discovery without the assumption of causal sufficiency [11, 17, 85].", "startOffset": 199, "endOffset": 211}, {"referenceID": 88, "context": "Algorithm single or multiple Reference IC* single data set [90] FCI single data set [90] RFCI single data set [17] MBCS* single data set [76] ION multiple data set [95] IOD multiple data set [96] cSAT+ multiple data set [99] INCA multiple data set [105] COmbINE multiple data set [98]", "startOffset": 59, "endOffset": 63}, {"referenceID": 88, "context": "Algorithm single or multiple Reference IC* single data set [90] FCI single data set [90] RFCI single data set [17] MBCS* single data set [76] ION multiple data set [95] IOD multiple data set [96] cSAT+ multiple data set [99] INCA multiple data set [105] COmbINE multiple data set [98]", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "Algorithm single or multiple Reference IC* single data set [90] FCI single data set [90] RFCI single data set [17] MBCS* single data set [76] ION multiple data set [95] IOD multiple data set [96] cSAT+ multiple data set [99] INCA multiple data set [105] COmbINE multiple data set [98]", "startOffset": 110, "endOffset": 114}, {"referenceID": 75, "context": "Algorithm single or multiple Reference IC* single data set [90] FCI single data set [90] RFCI single data set [17] MBCS* single data set [76] ION multiple data set [95] IOD multiple data set [96] cSAT+ multiple data set [99] INCA multiple data set [105] COmbINE multiple data set [98]", "startOffset": 137, "endOffset": 141}, {"referenceID": 93, "context": "Algorithm single or multiple Reference IC* single data set [90] FCI single data set [90] RFCI single data set [17] MBCS* single data set [76] ION multiple data set [95] IOD multiple data set [96] cSAT+ multiple data set [99] INCA multiple data set [105] COmbINE multiple data set [98]", "startOffset": 164, "endOffset": 168}, {"referenceID": 94, "context": "Algorithm single or multiple Reference IC* single data set [90] FCI single data set [90] RFCI single data set [17] MBCS* single data set [76] ION multiple data set [95] IOD multiple data set [96] cSAT+ multiple data set [99] INCA multiple data set [105] COmbINE multiple data set [98]", "startOffset": 191, "endOffset": 195}, {"referenceID": 97, "context": "Algorithm single or multiple Reference IC* single data set [90] FCI single data set [90] RFCI single data set [17] MBCS* single data set [76] ION multiple data set [95] IOD multiple data set [96] cSAT+ multiple data set [99] INCA multiple data set [105] COmbINE multiple data set [98]", "startOffset": 220, "endOffset": 224}, {"referenceID": 103, "context": "Algorithm single or multiple Reference IC* single data set [90] FCI single data set [90] RFCI single data set [17] MBCS* single data set [76] ION multiple data set [95] IOD multiple data set [96] cSAT+ multiple data set [99] INCA multiple data set [105] COmbINE multiple data set [98]", "startOffset": 248, "endOffset": 253}, {"referenceID": 96, "context": "Algorithm single or multiple Reference IC* single data set [90] FCI single data set [90] RFCI single data set [17] MBCS* single data set [76] ION multiple data set [95] IOD multiple data set [96] cSAT+ multiple data set [99] INCA multiple data set [105] COmbINE multiple data set [98]", "startOffset": 280, "endOffset": 284}, {"referenceID": 80, "context": "[81] A mixed graph is ancestral if the conditions hold that (1) it does not contain directed cycles, (2) it does not contain almost directed cycles, and (3) for any undirected edge Vi \u2212 Vj , Vi and Vj have no parents or spouses.", "startOffset": 0, "endOffset": 4}, {"referenceID": 80, "context": "[81] In an ancestral graph, Vi and Vj are m-separated by Z \u2286 V \\{Vi, Vj}, if every path between Vi and Vj is blocked by Z .", "startOffset": 0, "endOffset": 4}, {"referenceID": 80, "context": "[81] For any two non-adjacent variables in an ancestral graph, if there exists a set of variables that m-separates them, then the ancestral graph is said to be maximal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "The IC* (Inductive Causation) algorithm [74] is the pioneer work to learn causal structures without assuming causal sufficiency.", "startOffset": 40, "endOffset": 44}, {"referenceID": 88, "context": "To improve the efficiency of the IC* algorithm, the FCI algorithm [90] was proposed which finds an graph skeleton in Step 1 using the PC algorithm.", "startOffset": 66, "endOffset": 70}, {"referenceID": 116, "context": "\u2022 Step 5: Replacing as many circles as possible by arrowheads and tails using the R1-R10 orientation rules (in total 10 rules) described by [118].", "startOffset": 140, "endOffset": 145}, {"referenceID": 16, "context": "algorithm [17] is a modification of the FCI algorithm.", "startOffset": 10, "endOffset": 14}, {"referenceID": 75, "context": "MBCS* algorithm [76] and the FCI algorithm exists in Step 1.", "startOffset": 16, "endOffset": 20}, {"referenceID": 93, "context": "The ION (Integration of Overlapping Networks) algorithm [95] is the first and an asymptotically correct algorithm for discovering the complete set of causal", "startOffset": 56, "endOffset": 60}, {"referenceID": 94, "context": "To deal with the problem of conflict information resulted from different data sets, the IOD (integration of overlapping datasets) algorithm [96] was", "startOffset": 140, "endOffset": 144}, {"referenceID": 97, "context": "[99] also presented a cSAT+ algorithm to improve the efficiency of the ION algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] proposed a general SAT-based procedure by transforming the observed dependences and independences constraints into a SAT instance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] presented an ASP-based constraint optimization approach to handle inconsistent (in)dependence constraints", "startOffset": 0, "endOffset": 4}, {"referenceID": 103, "context": "[105] unified the prior work above and proposed the INCA (Integrative Causal Analysis) framework for the co-analysis of heteroge-", "startOffset": 0, "endOffset": 5}, {"referenceID": 96, "context": "The COmbINE (Causal discovery from Overlapping INtErventions) algorithm [98] builds upon the ideas in the cSAT+ algorithm [99] and is the first algorithm to address both overlapping variables and multiple (hard) interventions for acyclic structures without", "startOffset": 72, "endOffset": 76}, {"referenceID": 97, "context": "The COmbINE (Causal discovery from Overlapping INtErventions) algorithm [98] builds upon the ideas in the cSAT+ algorithm [99] and is the first algorithm to address both overlapping variables and multiple (hard) interventions for acyclic structures without", "startOffset": 122, "endOffset": 126}, {"referenceID": 96, "context": "The algorithm is proved to be sound and complete in the sample limit under different interventions in acyclic domains [98].", "startOffset": 118, "endOffset": 122}, {"referenceID": 7, "context": "causal knowledge from multiple data sets instead of any individual data source alone [8].", "startOffset": 85, "endOffset": 88}, {"referenceID": 78, "context": "Recent, the idea of invariant causal inference may open a new way to handle those issues [79].", "startOffset": 89, "endOffset": 93}, {"referenceID": 80, "context": "Thirdly, the MAGmodels provide a different perspective in causal inference without assuming causal sufficiency, compared to DAGs [81].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "Using MAGs to represent latent variables is an emerging direction in causal discovery without the assumption of causal sufficiency [17, 46, 85].", "startOffset": 131, "endOffset": 143}, {"referenceID": 45, "context": "Using MAGs to represent latent variables is an emerging direction in causal discovery without the assumption of causal sufficiency [17, 46, 85].", "startOffset": 131, "endOffset": 143}, {"referenceID": 84, "context": "Using MAGs to represent latent variables is an emerging direction in causal discovery without the assumption of causal sufficiency [17, 46, 85].", "startOffset": 131, "endOffset": 143}, {"referenceID": 6, "context": "ery [7].", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "Recently, some work has been proposed to address the problem of selection bias in causal discovery, but those work is still in a theoretical aspect [7, 9, 41].", "startOffset": 148, "endOffset": 158}, {"referenceID": 8, "context": "Recently, some work has been proposed to address the problem of selection bias in causal discovery, but those work is still in a theoretical aspect [7, 9, 41].", "startOffset": 148, "endOffset": 158}, {"referenceID": 40, "context": "Recently, some work has been proposed to address the problem of selection bias in causal discovery, but those work is still in a theoretical aspect [7, 9, 41].", "startOffset": 148, "endOffset": 158}], "year": 2016, "abstractText": "Causal discovery studies the problem of mining causal relationships between variables from data, which is of primary interest in science. During the past decades, significant amount of progresses have been made toward this fundamental data mining paradigm. Recent years, as the availability of abundant large-sized and complex observational data, the constrain-based approaches have gradually attracted a lot of interest and have been widely applied to many diverse real-world problems due to the fast running speed and easy generalizing to the problem of causal insufficiency. In this paper, we aim to review the constraint-based causal discovery algorithms. Firstly, we discuss the learning paradigm of the constraint-based approaches. Secondly and primarily, the state-of-the-art constraint-based casual inference algorithms are surveyed with the detailed analysis. Thirdly, several related open-source software packages and benchmark data repositories are briefly summarized. As a conclusion, some open problems in constraint-based causal discovery are outlined for future research.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}