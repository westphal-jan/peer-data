{"id": "1611.05340", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Approximating Wisdom of Crowds using K-RBMs", "abstract": "moskvy An important way arkaig to make bernardoni large training moloi sets is 1966-69 to backside gather noisy brice labels from burtis crowds tiefenbach of non experts. We propose solter a method e9 to leatherbacks aggregate 76.44 noisy pok labels journal collected manj from a ebisawa crowd workaholics of dearing workers or murupara annotators. Eliciting labels is important 550-foot in dustjacket tasks such inchworm as judging then-husband web search quartetto quality gairaigo and rating sociopolitical products. Our method po\u00e8tes assumes that labels prts are murder-suicide generated agglutinative by cassoni a probability 42.83 distribution stagy over sonangol items and 75-billion labels. We irey formulate plasman the kampf method by drawing battaglia parallels ascendance between malesani Gaussian Mixture rongzhen Models (danshuei GMMs) kookie and oberkfell Restricted fairhair Boltzmann Machines (RBMs) coelacanths and hauliers show that the problem anticholinergics of alternatively vote aggregation can t\u00e9 be viewed golfe as one isoflavone of clustering. We use jordana K - RBMs to jenabi perform clustering. pratylenchus We distractor finally show haqqania some empirical evaluations revisitation over lifts real singtel datasets.", "histories": [["v1", "Wed, 16 Nov 2016 16:01:48 GMT  (28kb,D)", "http://arxiv.org/abs/1611.05340v1", "8 pages, 1 figure"], ["v2", "Thu, 17 Nov 2016 02:48:04 GMT  (28kb,D)", "http://arxiv.org/abs/1611.05340v2", "8 pages, 1 figure"]], "COMMENTS": "8 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["abhay gupta"], "accepted": false, "id": "1611.05340"}, "pdf": {"name": "1611.05340.pdf", "metadata": {"source": "CRF", "title": "Vote Aggregation as a Clustering Problem", "authors": ["Abhay Gupta"], "emails": ["abhgup@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "There has been considerable amount of work on learning when labeling is expensive, such as techniques on transductive inference and active learning. With the emergence of crowdsourcing services, like Amazon Mechanical Turk, labeling costs in many applications have dropped dramatically. Large amounts of labeled data can now be gathered at low price. Due to a lack of domain expertise and misaligned incentives, however, labels provided by crowdsourcing workers are often noisy. To overcome the quality issue, each item is usually simultaneously labeled by several workers, and then we aggregate the multiple labels with some manner, for instance, majority voting.\nAn advanced approach for label aggregation is suggested by Dawid and Skene[1]. They assume that each worker has a latent confusion matrix for labeling. The off-diagonal elements represent the probabilities that a worker mislabels an arbitrary item from one class to another while the diagonal elements correspond to her accuracy in each class. Worker confusion matrices and true labels are jointly estimated by maximizing the likelihood of observed labels. One may further assume a prior distribution over worker confusion matrices and perform Bayesian inference [2][3][4].\nThe method of Dawid-Skene (1979) implicitly assumes that a worker performs equally well across all items in a common class. In practice, however, it is often the case that one item is more difficult to label than another. To address this heterogeneous issue, Zhou et al.(2012)[5] propose a minimax entropy principle for crowdsourcing. It results in that each item is associated with a latent confusion vector besides a latent confusion matrix for each worker. Observed labels are determined jointly by worker confusion matrices and item confusion vectors through an exponential family model. Moreover, it turns out that the probabilistic labeling model can be equivalently derived from a natural assumption of objective measurements of worker ability and item difficulty. Such kinds of objectivity arguments have been widely discussed in the literature of mental test theory [6][7]. All the above approaches are for aggregating multiclass labels and In many scenarios, the labels are ordinal. Zhou et. al. (2014)[8] proposed a work to aggregate votes using minimax conditional entropy for ordinal labels.\nMost of the methods use statistical methods to aggregate the observed labels by transforming them to some probability or entropy measures. But, there has been no work that operates directly on the observed labels. We present a method to learn the aggregates of the votes using clustering. We first show the formulation that allows us to use clustering as an approximation of the vote aggregation\nar X\niv :1\n61 1.\n05 34\n0v 1\n[ cs\n.L G\n] 1\n6 N\nov 2\n01 6\nstratagem. We first draw a parallel between the Restricted Boltzmann Machine (RBM) learning and the Expectation Maximization (EM) algorithm of the David-Skene algorithm and then show that Gaussian-Softmax RBMs[9] can be approximated by a Gaussian Mixture Model (GMM), whose specific conditions lead to a direct mapping to the traditional K-means algorithm[10][11].\nTo then elucidate the clustering paradigm, we perform clustering using theK-RBM model as proposed in [14]."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Restricted Boltzmann Machines", "text": "The restricted Boltzmann machine is a bipartite, undirected graphical model with visible (observed) units and hidden (latent) units. The RBM can be understood as an MRF with latent factors that explains the input visible data using binary latent variables. The RBM consists of visible data v of dimension L that can take real values or binary values, and stochastic binary variables h of dimension K. The parameters of the model are the weight matrix W \u2208 RLxK that defines a potential between visible input variables and stochastic binary variables, the biases c \u2208 RL for visible units, and the biases b \u2208 RK for hidden units. When the visible units are real-valued, the model is called the Gaussian RBM, and its joint probability distribution can be defined as follows:\nP (v, h) = 1\nZ exp(\u2212E(v, h)) (1)\nE(v, h) = 1 2\u03c32 ||v \u2212 c||2 \u2212 1 \u03c3 vTWh\u2212 bTh (2)\nwhere Z is a normalization constant. The conditional distribution of this model can be written as:\nP (vi|h) = N(vi;\u03c3 \u2211 j Wijhj + ci, \u03c3 2) (3)\nP (hj = 1|v) = sigm( 1\n\u03c3 \u2211 i Wijvi + bj) (4)\nwhere sigm(s) = 11+exp(\u2212s) is the sigmoid function, and N(.; ., .) is a Gaussian distribution. Here, the variables in a layer (given the other layers) are conditionally independent, and thus we can perform block Gibbs sampling in parallel. The RBM can be trained using sampling-based approximate maximum-likelihood, e.g., contrastive divergence approximation [12]. After training the RBM, the posterior (Equation 2) of the hidden units (given input data) can be used as feature representations for classification tasks."}, {"heading": "2.2 Gaussian-softmax RBMs", "text": "We define the Gaussian-softmax RBM as the Gaussian RBM with a constraint that at most one hidden unit can be activated at a time given the input, i.e., \u2211 j hj \u2264 1. The energy function of the Gaussian-softmax RBM can be written in a vectorized form as follows:\nE(v, h) = 1 2\u03c32 ||v \u2212 c||2 \u2212 1 \u03c3 vTWh\u2212 bTh (5)\nsubject to \u2211 j hj \u2264 1. For this model, the conditional probabilities of visible or hidden units given the other layer can be computed as:\nP (v|h) = N(v;\u03c3Wh+ c, \u03c32I) (6)\nP (hj = 1|v) = exp( 1\u03c3w T j v + bj) 1 + \u2211 j\u2032 exp( 1 \u03c3w T j\u2032v + bj\u2032)\n(7)\nwhere wj is the j-th column of the W matrix, often denoted as a \u201cbasis\u201d vector for the j-th hidden unit. In this model, there are K + 1 possible configurations (i.e., all hidden units are 0, or only one hidden unit hj is 1 for some j)."}, {"heading": "2.3 K-Means", "text": "The k-means clustering is an unsupervised algorithm that assigns clusters to data points. The algorithm can be written as\n\u2022 Randomly choose k cluster centers, \u00b5(0) = \u00b5(0)1 , \u00b5 (0) 2 , \u00b7 \u00b7 \u00b7 , \u00b5 (0) k .\n\u2022 Assign an incoming data point xj to the nearest cluster center C(t)(j) = mini ||\u00b5i \u2212 xj ||2 \u2022 \u00b5i becomes centroid of the cluster. \u00b5t+1i = min\u00b5 \u2211 j:C(j)=i ||\u00b5\u2212 xj ||2\nThe procedure is repeated till convergence, that is all points have been assigned the best cluster centers and over many trials r, we take the best possible solution as the cluster assignment."}, {"heading": "2.4 Gaussian Mixture Models", "text": "The Gaussian mixture model is a directed graphical model where the likelihood of visible units is expressed as a convex combination of Gaussians. The likelihood of a GMM with K + 1 Gaussians can be written as follows:\nP (v) = K\u2211 k=0 \u03c0kN(v;\u00b5k,\u03a3k) (8)\nFor the rest of the paper, we denote the GMM with shared spherical covariance as GMM(\u00b5k, \u03c32I), when \u03a3k = \u03c32I for all k \u2208 {0, 1, ...,K}. For the GMM with arbitrary positive definite covariance matrices, we will use the shorthand notation GMM(\u00b5k,\u03a3k).\nK-means can be understood as a special case of GMM with spherical covariance by letting \u03c3 \u2192 0 [13]. Compared to GMM, the training of K-means is highly efficient; therefore it is plausible to train K-means to provide an initialization of a GMM. Then the GMM is trained with EM algorithm. The EM algorithm learns the parameters to maximize the likelihood of the data as described by Equation 8."}, {"heading": "3 Vote Aggregation as a Clustering Problem", "text": "The following section outlines the proof for vote aggregation as a special case of clustering problems, when trying to model the problem using RBMs."}, {"heading": "3.1 Vote Aggregation using RBMs", "text": "RBMs are learned in a manner so as to minimize the negative log-likelihood of the data. In a vote aggregation setup, the data is the observed labels. Thus, we can see that learning from RBMs are similar to aggregating votes from the Dawid-Skene algorithm which also minimizes the negative log-likelihood of the observed labels. But in the training of RBMs, we often encounter the normalization constant Z, which is intractable and this makes it difficult to train an RBM, and we need to approximate Z to learn the ideal parameters for the same. Hence, it becomes difficult to directly apply RBMs to aggregate votes."}, {"heading": "3.2 Equivalence between Gaussian Mixture Models and RBMs with a softmax constraint", "text": "In this section, we show that a Gaussian RBM with softmax hidden units can be converted into a Gaussian mixture model, and vice versa. This connection between mixture models and RBMs with a softmax constraint completes the chain of links between K-means, GMMs and Gaussian-softmax RBMs and helps us to visualize vote aggregation as a clustering problem.\nAs Equation 6 shows, the conditional probability of visible units given the hidden unit activations for Gaussian-softmax RBM follows a Gaussian distribution. From this perspective, the Gaussian-softmax RBM can be viewed as a mixture of Gaussians whose mean components correspond to possible hidden unit configurations. In this section, we show an explicit equivalence between these two models by formulating the conversion equations between GMM(\u00b5k, \u03c32I) with K + 1 Gaussian components and the Gaussian-softmax RBM with K hidden units.\nProposition The mixture of K + 1 Gaussians with shared spherical covariance of \u03c32I is equivalent to the Gaussian-softmax RBM with K hidden units. We prove the following conversions by showing the following conversions."}, {"heading": "1. From Gaussian-softmax RBM to GMM(\u00b5k, \u03c32I):", "text": "We begin by decomposing the chain rule:\nP (v, h) = P (v|h)(h), (9)\nwhere\nP (h) = 1\nZ\n\u222b exp(\u2212E(v, h))dv (10)\nSince there are only a finite number of hidden unit configurations, we can explicitly enumerate the prior probabilities:\nP (hj = 1) =\n\u222b exp(\u2212E(hj = 1, v))dv\u2211\nj\u2032\n\u222b exp(\u2212E(hj\u2032 = 1, v))dv\n(11)\nIf we define \u03c0\u0302j = \u222b exp(\u2212E(v, hj = 1))dv, then we have P (hj = 1) = \u03c0\u0302j\u2211\nj\u2032 \u03c0\u0302j\u2032 \u223c= \u03c0j . In fact, \u03c0\u0302j\ncan be calculated analytically,\n\u03c0\u0302j = \u222b exp(\u2212E(v, hj = 1))dv\n= exp( 1 2\u03c32 ||v \u2212 c||2 \u2212 1 \u03c3 vTwj \u2212 bj) = ( \u221a 2\u03c0\u03c3)Lexp(bj + 1\n2 ||wj ||2 +\n1 \u03c3 cTwj)\nUsing this definition, we can show the equivalence as, P (v) = \u2211 j \u03c0jN(v;\u03c3wj + c;\u03c3 2I) (12)"}, {"heading": "2. From GMM(\u00b5k, \u03c32I) to Gaussian-softmax RBM:", "text": "We will also show this by construction. Suppose we have the following Gaussian mixture with K + 1 components and the shared spherical covariance \u03c32I:\nP (v) = K\u2211 j=0 \u03c0jN(v;\u00b5j , \u03c3 2I) (13)\nThis GMM can be converted to a Gaussian-softmax RBM with the following transformations:\nc = \u00b50 (14)\nwj = 1\n\u03c3 (\u00b5j \u2212 c) (15)\nb = log \u03c0j \u03c00 \u2212 1 2 ||wj ||2 \u2212 1 \u03c3 wTj c (16)\nIt is easy to see that the conditional distribution P (v|hj = 1) can be formulated as a Gaussian distribution with mean \u00b5j = \u03c3wj + c, which is identical to that of the Gaussian-softmax RBM.\nFurther, we can recover the posterior probabilities of the hidden units given visible units as the follows:\nP (hj = 1|v) = \u03c0jexp(\n\u22121 2\u03c32 ||v \u2212 \u03c3wj \u2212 c|| 2)\n\u03a0Kj\u2032=0\u03c0j\u2032exp( \u22121 2\u03c32 ||v \u2212 \u03c3wj\u2032 \u2212 c||2)\n= exp( 1\u03c3w T j v + bj) 1 + \u2211 j\u2032 exp( 1 \u03c3w T j\u2032v + bj\u2032)\nTherefore, a Gaussian mixture can be converted to an equivalent Gaussian RBM with a softmax constraint."}, {"heading": "3.3 From GMMs to Clustering Assignments", "text": "GMMs learn a density function over the data, while trying to maximize its likelihood. From maximum likelihood estimation, the equation a GMM tries to learn is max \u03a0jP (xj , yj). But since we do not know yj\u2019s, we maximize the marginal likelihood, which is given by max \u03a0jP (xj) = max \u03a0j \u2211k i=1 P (yj = i, xj) ,where k is the number of clusters.\nFrom the Gaussian Bayes Classifier, P (y = i|xj) = P (xj |y=i)P (y=i)/P (xj), that is,\nP (y = i|xj) \u221d 1\n(2\u03c0)m/2||\u03a3i||1/2 exp\n[ \u2212 1\n2 (xj \u2212 \u00b5i)T\u03a3\u22121i (xj \u2212 \u00b5i)\n] P (y = i) (17)\nWhen P (x|y = i) is spherical, with same \u03c3 for all classes, P (xj |y = i) \u221d exp [ \u22121 2\u03c32 ||xj \u2212 \u00b5i|| 2 ] . If\neach xj belongs to one class Cj , marginal likelihood is given by:\n\u03a0mj=1 k\u2211 i=1 P (xj , y = i) \u221d \u03a0mj=1exp [ \u22121 2\u03c32 ||xj \u2212 \u00b5Cj ||2 ] (18)\nFor estimating the parameters, we maximize the log-likelihood with respect to all clusters and this gives,\nmax m\u2211 j=1 log( k\u2211 i=1 P (xj , y = i)) \u221d max m\u2211 j=1 [ \u22121 2\u03c32 ||xj \u2212 \u00b5Cj ||2 ] (19)\nEquivalently, minimizing the negative log-likelihood gives,\nmin m\u2211 j=1 \u2212log( k\u2211 i=1 P (xj , y = i)) \u221d min m\u2211 j=1 [ 1 2\u03c32 ||xj \u2212 \u00b5Cj ||2 ] (20)\nwhich is the same as the k-means function. We thus show that the vote aggregation methodolody when applied from an RBM model perspective can be viewed as a clustering problem, one of K-means specifically.\nThus, we can consider vote aggregation learned by maximizing the likelihood of observed labels to be a clustering problem."}, {"heading": "4 Clustering using K-RBMs", "text": "Our framework uses K component RBMs. Each component RBM learns one non-linear subspace. The visible units vi , i = 1, \u00b7 \u00b7 \u00b7 , I correspond to an I dimensional visible (input) space and the hidden units hj , j = 1, \u00b7 \u00b7 \u00b7 , J correspond to a learned non-linear J-dimensional subspace."}, {"heading": "4.1 K-RBM Model", "text": "The K-RBM model has K component RBMs. Each of these maps a set of sample points xn \u2208 RI to a projection in RJ . Each component RBM has a set of symmetric weights (and asymmetric biases) wk \u2208 R(I+1)x(J+1) that learns a non-linear subspace. Note that these weights include the forward and backward bias terms. The error of reconstruction for a sample xn given by the kth RBM is simply the squared Euclidean distance between the data point xn and its reconstruction by the kth RBM, computed using. We denote this error by kn. The total reconstruction error t in any iteration t is given by \u2211N n=1 mink kn.\nThe K RBMs are trained simultaneously. During the RBM training, we associate data points with RBMs based on how well each component RBM is able to reconstruct the data points. A component RBM is trained only on the training data points associated with it. The component RBMS are given random initial weights wk, k = 1, \u00b7 \u00b7 \u00b7 ,K."}, {"heading": "4.2 Methodology", "text": "As in traditional K-means clustering, the algorithm alternates between two steps: (1) Computing association of a data point with a cluster and (2) updating the cluster parameters. In K-RBMs, the nth data point is associated with the kth RBM (cluster) if its reconstruction error from that RBM is lowest compared to other RBMS, i.e. if kn < k\u2032n\u2200k 6= k\u2032, k, k\u2032 \u2208 {1, \u00b7 \u00b7 \u00b7 ,K}. Once all the points are associated with one of the RBMS the weights of the RBMS are learnt in a batch update. In hard clustering the data points are partitioned into the clusters exhaustively (i.e. each data point must be associated with some cluster) and disjointly (i.e. each data point is associated with only one cluster). In contrast with K-means where the update of the cluster center is a closed form solution given the data association with clusters, in K-RBMs the weights are learned iteratively."}, {"heading": "5 Experimental Results", "text": "In this section, we report empirical results of our method on real crowdsourced data. We consider the L0 error metric. Let us denote by y the true rating and yb the estimate. The error metrics is defined as: (1) L0 = I(y 6= yb). All the research (code and datasets) is reproducible and is available at: https://github.com/gupta-abhay/deep-voteaggregate."}, {"heading": "5.1 Data", "text": "Web search relevance rating The web search relevance rating dataset contains 2665 query-URL pairs and 177 workers[5]. Give a query-URL pair, a worker is required to provide a rating to measure how the URL is relevant to the query. The rating scale is 5-level: perfect, excellent, good, fair, or bad. On average, each pair was labeled by 6 different workers, and each worker labeled 90 pairs. More than 10 workers labeled only one pair.\nDog Image Labeling We chose the images of 4 breeds of dogs from the Stanford dogs dataset [8]: Norfolk Terrier (172), Norwich Terrier (185), Irish Wolfhound (218), and Scottish Deerhound (232). The numbers of the images for each breed are in the parentheses. There are 807 images in total. A worker labeled an image at most once, and each image was labeled 10 times."}, {"heading": "5.2 Architectures", "text": "There are four architectures considered for both the datasets. We consider two RBMs, binary-binary RBMs and gaussian-binary RBMs. The architectures are the following:"}, {"heading": "5.2.1 Web search relevance rating", "text": "1. Binary-Binary RBM with 30 visible units and 5 hidden units. 2. Binary-Binary RBM with 18 visible units and 3 hidden units. 3. Gaussian-Binary RBM with 6 visible units and 5 hidden units. 4. Gaussian-Binary RBM with 6 visible units and 3 hidden units."}, {"heading": "5.2.2 Dog Image Labeling", "text": "1. Binary-Binary RBM with 40 visible units and 4 hidden units.\n2. Binary-Binary RBM with 20 visible units and 2 hidden units.\n3. Gaussian-Binary RBM with 10 visible units and 4 hidden units.\n4. Gaussian-Binary RBM with 10 visible units and 2 hidden units."}, {"heading": "5.3 Results", "text": "We report the results, both L0 and L1 errors of the architectures considered in Tables 1 and 2. The L0 error of the Dawid-Skene model on the web search data is 0.17 and the error on the dog data is 0.21."}, {"heading": "5.4 Dicussion and Analysis", "text": "All the results are done over and average of 20 runs. We see from the results that the results of one-hot encodings are the best among all the proposed architectures, for both the web and dog datasets. This can be because RBMs capture binary data and thus it is able to capture the one-hot encodings in a good manner. Also, we see that in the web data, when we use Gaussian binary RBMs, we get 100% error. This may be because Gaussian sampling of the data is not ideal for this dataset. On trying CD-k above k = 2, we get huge reconstruction errors for every data point. However, between CD-1 and CD-2, CD-2 outperforms CD-1. Also, PCD gives huge reconstruction errors for the web dataset, but gave results comparable to CD-1 for the dog dataset. We give a plot for the average reconstruction error per sample as the RBM proceeds for the web dataset in Figure 1."}], "references": [{"title": "Maximum likeihood estimation of observer error-rates using the EM algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1979}, {"title": "Learning from crowds", "author": ["V.C. Raykar", "S. Yu", "L.H. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Variational inference for crowdsourcing", "author": ["Q. Liu", "J. Peng", "A. Ihler"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Optimistic knowledge gradient policy for optimal budget allocation in crowdsourcing", "author": ["X. Chen", "Q. Lin", "D. Zhou"], "venue": "In Proceedings of the 30th International Conferences on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning from the wisdom of crowds by minimax entropy", "author": ["D. Zhou", "J.C. Platt", "S. Basu", "Y. Mao"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "On general laws and the meaning of measurement in psychology", "author": ["G. Rasch"], "venue": "In Proceedings of the 4th Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1961}, {"title": "Statistical theories of mental test scores", "author": ["F.M. Lord", "M.R. Novick"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1968}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "November. Efficient learning of sparse, distributed, convolutional feature representations for object recognition", "author": ["K. Sohn", "D.Y. Jung", "H. Lee", "A.O. Hero III"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning multiple non-linear sub-spaces using k-rbms", "author": ["S. Chandra", "S. Kumar", "C.V. Jawahar"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2778-2785)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "An advanced approach for label aggregation is suggested by Dawid and Skene[1].", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "One may further assume a prior distribution over worker confusion matrices and perform Bayesian inference [2][3][4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "One may further assume a prior distribution over worker confusion matrices and perform Bayesian inference [2][3][4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "One may further assume a prior distribution over worker confusion matrices and perform Bayesian inference [2][3][4].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "(2012)[5] propose a minimax entropy principle for crowdsourcing.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Such kinds of objectivity arguments have been widely discussed in the literature of mental test theory [6][7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "Such kinds of objectivity arguments have been widely discussed in the literature of mental test theory [6][7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "We first draw a parallel between the Restricted Boltzmann Machine (RBM) learning and the Expectation Maximization (EM) algorithm of the David-Skene algorithm and then show that Gaussian-Softmax RBMs[9] can be approximated by a Gaussian Mixture Model (GMM), whose specific conditions lead to a direct mapping to the traditional K-means algorithm[10][11].", "startOffset": 198, "endOffset": 201}, {"referenceID": 8, "context": "We first draw a parallel between the Restricted Boltzmann Machine (RBM) learning and the Expectation Maximization (EM) algorithm of the David-Skene algorithm and then show that Gaussian-Softmax RBMs[9] can be approximated by a Gaussian Mixture Model (GMM), whose specific conditions lead to a direct mapping to the traditional K-means algorithm[10][11].", "startOffset": 344, "endOffset": 348}, {"referenceID": 10, "context": "To then elucidate the clustering paradigm, we perform clustering using theK-RBM model as proposed in [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 9, "context": ", contrastive divergence approximation [12].", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "1 Data Web search relevance rating The web search relevance rating dataset contains 2665 query-URL pairs and 177 workers[5].", "startOffset": 120, "endOffset": 123}], "year": 2017, "abstractText": "An important way to make large training sets is to gather noisy labels from crowds of non experts. We propose a method to aggregate noisy labels collected from a crowd of workers or annotators. Eliciting labels is important in tasks such as judging web search quality and rating products. Our method assumes that labels are generated by a probability distribution over items and labels. We formulate the method by drawing parallels between Gaussian Mixture Models (GMMs) and Restricted Boltzmann Machines (RBMs) and show that the problem of vote aggregation can be viewed as one of clustering. We use K-RBMs to perform clustering. We finally show some empirical evaluations over real datasets.", "creator": "LaTeX with hyperref package"}}}