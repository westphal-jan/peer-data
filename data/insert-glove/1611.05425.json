{"id": "1611.05425", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "ProjE: Embedding Projection for Knowledge Graph Completion", "abstract": "hansom With the shockwave.com large attributor volume of new cleveland-based information created samardzic every day, determining femicide the rostraver validity eschatology of belated information in non-hungarian a derm knowledge graph and goco filling in 121.20 its missing deadpans parts are crucial a\u0107imovi\u0107 tasks kaffeeklatsch for baldeo many researchers baobabs and 1:21 practitioners. 199.4 To address goals this valinor challenge, set-list a x100 number africaines of 232.2 knowledge graph completion methods have been developed using low - brioschi dimensional rylands graph okaka embeddings. euclidean Although adapazar\u0131 researchers gibsonia continue grantmaking to improve almanor these models using aerith an increasingly complex feature agribusinesses space, moderna we second-lowest show schevchenko that lamech simple sneed changes in rur the architecture hartness of the 2-yard underlying pittoni model can outperform state - weizsacker of - handspring the - art ranging models without obligado the exclusivamente need for m20 complex opec feature neca engineering. In this work, 341,000 we present 11.57 a shared b-24s variable antofagasta neural haggler network model called forming ProjE deception that fills - in urus missing shifting information peral in negad a knowledge graph wagons by learning jshaa joint barsalou embeddings kodr\u0105b of the wiling knowledge graph ' euro33 s sapidus entities huaraches and 107.40 edges, and through subtle, pseudoscientific but important, changes to the ternera standard kudos loss obediah function. In realigns doing sukhorukov so, coexisted ProjE lough has a nws.globe.com parameter eurogroup size lotbini\u00e8re that 68.56 is smaller langille than duhart 11 out fastens of 15 inflexion existing methods while performing $ belligerents 37 \\% $ better than maren the current - ib\u00e9rico best 16.96 method volkspartij on engorged standard fort\u00e9 datasets. We kunen also gorchakov show, via essie a new fact unico checking task, pollocks that miot ProjE is 1.4700 capable tyramine of accurately 22-16 determining the kandace veracity of many 7-foot-4 declarative statements.", "histories": [["v1", "Wed, 16 Nov 2016 20:09:08 GMT  (1337kb,D)", "http://arxiv.org/abs/1611.05425v1", "14 pages, Accepted to AAAI 2017"]], "COMMENTS": "14 pages, Accepted to AAAI 2017", "reviews": [], "SUBJECTS": "cs.AI stat.ML", "authors": ["baoxu shi", "tim weninger"], "accepted": true, "id": "1611.05425"}, "pdf": {"name": "1611.05425.pdf", "metadata": {"source": "META", "title": "ProjE: Embedding Projection for Knowledge Graph Completion", "authors": ["Baoxu Shi", "Tim Weninger"], "emails": [], "sections": [{"heading": null, "text": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few. In our view, KGs are an example of a heterogeneous information network containing entity-nodes and relationship-edges corresponding to RDF-style triples \u3008h, r, t\u3009 where h represents a head entity, and r is a relationship that connects h to a tail entity t.\nKGs are widely used for many practical tasks, however, their correctness and completeness are not guaranteed. Therefore, it is necessary to develop knowledge graph completion (KGC) methods to find missing or errant relationships with the goal of improving the general quality of KGs, which, in turn, can be used to improve or create interesting downstream applications.\nThe KGC task can be divided into two non-mutually exclusive sub-tasks: (i) entity prediction and (ii) relationship prediction. The entity prediction task takes a partial triple \u3008h, r, ?\u3009 as input and produces a ranked list of candidate entities as output:\nDefinition 1. (Entity Ranking Problem) Given a Knowledge Graph G = {E,R} and an input triple \u3008h, r, ?\u3009, the entity ranking problem attempts to find the optimal ordered list such that \u2200ej\u2200ei ((ej \u2208 E\u2212 \u2227 ei \u2208 E+)\u2192 ei \u227a ej), where E+ = {e \u2208 {e1, e2, . . . , el}|\u3008h, r, e\u3009 \u2208 G} and E\u2212 = {e \u2208 {el+1, el+2, . . . , e|E|}|\u3008h, r, e\u3009 /\u2208 G}.\nDistinguishing between head and tail-entities is usually arbitrary, so we can easily substitute \u3008h, r, ?\u3009 for \u3008?, r, t\u3009.\nThe relationship prediction task aims to find a ranked list of relationships that connect a head-entity with a tail-entity, i.e., \u3008h, ?, t\u3009. When discussing the details of the present work, we focus specifically on the entity prediction task; however, it is straightforward to adapt the methodology to the relationship prediction task by changing the input.\nA number of KGC algorithms have been developed in recent years, and the most successful models all have one thing in common: they use low-dimensional embedding vectors to represent entities and relationships. Many embedding models, e.g., Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t. In these models the loss functions are all the same, so models differ in how they transform the\nar X\niv :1\n61 1.\n05 42\n5v 1\n[ cs\n.A I]\n1 6\nN ov\n2 01\nentity embeddings h and t with respect to the relationship embeddings r. Instead of simply adding h + r, more expressive combination operators are learned by Knowledge Vault [8] and HolE [29] in order to predict the existence of \u3008h, r, t\u3009 in the KG.\nOther models, such as the Neural Tensor Network (NTN) [33] and the Compositional Vector Space Model (CVSM) [27], incorporate a multilayer neural network solution into the existing models. Unfortunately, due to their extremely large parameter size, these models either (i) do not scale well or (2) consider only a single relationship at a time [10] thereby limiting their usefulness on large, real-world KGs.\nDespite their large model size, the aforementioned methods only use singleton triples, i.e., length-1 paths in the KG. PTransE [24] and RTransE [10] employ extended path information from 2 and 3-hop trails over the knowledge graph. These extended models achieve excellent performance due to the richness of the input data; unfortunately, their model-size grows exponentially as the path-length increases, which further exacerbates the scalability issues associated with the already high number of parameters of the underlying-models.\nAnother curious finding is that some of the existing models are not self-contained models, i.e., they require pre-trained KG embeddings (RTransE, CVSM), pre-selected paths (PTransE, RTransE), or pre-computed content embeddings of each node (DKRL [36]) before their model training can even begin. TransR and TransH are self-contained models, but their experiments only report results using pre-trained TransE embeddings as input.\nWith these considerations in mind, in the present work we rethink some of the basic decisions made by previous models to create a projection embedding model (ProjE) for KGC. ProjE has four parts that distinguish it from the related work:\n1. Instead of measuring the distance between input triple \u3008h, r, ?\u3009 and entity candidates on a unified or a relationship-specific plane, we choose to project the entity candidates onto a target vector representing the input data. 2. Unlike existing models that use transformation matrices, we combine the embedding vectors representing the input data into a target vector using a learnable combination operator. This avoids the addition of a large number of transformation matrices by reusing the entity-embeddings. 3. Rather than optimizing the margin-based pairwise ranking loss, we optimize a ranking loss of the list of candidate-entities (or relationships) collectively. We further use candidate sampling to handle very large data sets. 4. Unlike many of the related models that require pre-trained data from prerequisite models or explore expensive multi-hop paths through the knowledge graph, ProjE is a self-contained model over length-1 edges."}, {"heading": "1 Related Work", "text": "A variety of low-dimensional representation-based methods have been developed to work on the KGC task. These methods usually learn continuous, low-dimensional vector representations (i.e., embeddings) for entities WE and relationships WR by minimizing a margin-based pairwise ranking loss [24].\nThe most widely used embedding model in this category is TransE [4], which views relationships as translations from a head entity to a tail entity on the same low-dimensional plane. The energy function of TransE is defined as\nE(h, r, t) =\u2016 h + r\u2212 t \u2016Ln , (1) which measures the Ln-distance between a translated head entity h+r and some tail entity t. The Unstructured model [3] is a special case of TransE where r = 0 for all relationships.\nBased on the initial idea of treating two entities as a translation of one another (via their relationship) in the same embedding plane, several models have been introduced to improve the initial TransE model. The newest contributions in this line of work focus primarily on the changes in how the embedding planes are computed and/or how the embeddings are combined. For example, the entity translations in TransH [35] are computed on a hyperplane that is perpendicular to the relationship embedding. In TransR [25] the entities and relationships are embedded on separate planes and then the entity-vectors are translated to the relationship\u2019s plane. Structured Embedding (SE) [5] creates two translation matrices for each relationship and applies them\nto head and tail entities separately. Knowledge Vault [8] and HolE [29], on the other hand, focus on learning a new combination operator instead of simply adding two entity embeddings element-wise.\nThe aforementioned models are all geared toward link prediction in KGs, and they all minimize a marginbased pairwise ranking loss function L over the training data S:\nL(S) = \u03a3(h,r,t)\u2208S[\u03b3 + E(h, r, t)\u2212 E(h\u2032, r\u2032, t\u2032)]+, (2) where E(h, r, t) is the energy function of each model, \u03b3 is the margin, and (h\u2032, r\u2032, t\u2032) denotes some \u201ccorrupted\u201d triple which does not exist in S. Unlike aforementioned models that focus on different E(h, r, t), TransA [19] introduces an adaptive local margin approach that determines \u03b3 by a closed set of entity candidates. Other similar models include RESCAL [30], Semantic Matching Energy (SME) [3], and the Latent Factor Model (LFM) [18].\nThe Neural Tensor Network (NTN) model [33] is an exception to the basic energy function in Eq. 1. Instead, NTN uses an energy function\nE(h, r, t) = uTr f(h TWrt +Wrhh +Wrtt + br), (3)\nwhere ur, Wr, Wrh, and Wrt are all relationship-specific variables. As a result, the number of parameters in NTN is significantly larger than other methods. This makes NTN unsuitable for networks with even a moderate number of relationships.\nSo far, the related models have only considered triples that contain a single relationship. More complex models have been introduced to leverage path and content information in KGs. For instance, the Compositional Vector Space Model (CVSM) [27] composes a sequence of relationship embeddings into a single path embedding using a Recurrent Neural Network (RNN). However, this has two disadvantages: (i) CVSM needs pre-trained relationship embeddings as input, and (ii) each CVSM is specifically trained for only a single relationship type. This makes CVSM perform well in specific tasks, but unsuitable for generalized entity and relationship prediction tasks. RTransE [10] solves the relationship-specific problem in CVSM by using entity and relationship embeddings learned from TransE. However, it is hard to compare RTransE with existing methods because it requires unambiguous, pre-selected paths as inputs called quadruples \u3008h, r1, r2, t\u3009 further complicating the model. DKRL, like NTN, uses word embeddings of entity-content in addition to multi-hop paths, but relies on the machinery of a Convolution Neural Network (CNN) to learn entity and relationship embeddings.\nPTransE [24] is another path-based method that uses path information in its energy function. Simply put, PTransE doubles the number of edges in the KG by creating reverse relationships for every existing relationship in the KG. Then PTransE uses PCRA [37] to select input paths within a given length constraint.\nTable 1 shows a breakdown of the parameter complexity of each model. As is typical, we find that more complex models achieve better prediction accuracy, but are also more difficult to train and have trouble scaling. The proposed method, ProjE, has a number of parameters that is smaller than 11 out of 15 methods and does not require any prerequisite training."}, {"heading": "2 Methodology", "text": "The present work views the KGC problem as a ranking task and optimizes the collective scores of the list of candidate entities. Because we want to optimize the ordering of candidate entities collectively, we need to project the candidate entities onto the same embedding vector. For this task we learn a combination operator that creates a target vector from the input data. Then, the candidate entities are each projected onto the same target vector thereby revealing the candidate\u2019s similarity score as a scalar.\nIn this section we describe the ProjE architecture, followed by two proposed variants, their loss functions, and our choice of candidate sampling method. In the experiments section we demonstrate that ProjE outperforms all existing methods despite having a relatively small parameter space. A detailed algorithm description can be found in the Supplementary Material."}, {"heading": "2.1 Model Architecture", "text": "The main insight in the development of ProjE is as follows: given two input embeddings, we view the prediction task as ranking problem where the top-ranked candidates are the correct entities. To generate this ordered list, we project each of the candidates onto a target vector defined by two input embeddings through a combination operator.\nExisting models, such as Knowledge Vault, HolE, and NTN, define specific matrix combination operators that combine entities and/or relationships. In common practice, these matrices are expected to be sparse. Because we believe it is unnecessary to have interactions among different feature dimensions at this early stage, we constraint our matrices to be diagonal, which are inherently sparse. The combination operator is therefore defined as\ne\u2295 r = Dee + Drr + bc, (4) where De and Dr are k\u00d7k diagonal matrices which serve as global entity and relationship weights respectively, and bc \u2208 Rk is the combination bias.\nUsing this combination operator, we can define the embedding projection function as\nh(e, r) = g(Wcf(e\u2295 r) + bp), (5) where f and g are activation functions that we define later, Wc \u2208 Rs\u00d7k is the candidate-entity matrix, bp is the projection bias, and s is the number of candidate-entities. h(e, r) represents the ranking score vector, where each element represents the similarity between some candidate entity in Wc and the combined input embedding e\u2295 r.\nAlthough s is relatively large, due to the use of shared variables, Wc is the candidate-entity matrix that contains s rows that exist in the entity embedding matrix WE . Simply put, Wc does not introduce any new variables into the model. Therefore, compared to simple models like TransE, ProjE only increases the number of parameters by 5k + 1, where 1, 4k, and k are introduced as the projection bias, combination weights, and combination bias respectively. Later we show that by changing different activation functions, ProjE can be either a pointwise ranking model or a listwise ranking model.\nProjE can be viewed as a neural network with a combination layer and a projection (i.e., output) layer. Figure 1 illustrates this architecture by way of an example. Given a tail entity Illinois and a relationship CityOf, our task is to calculate the scores of each head entity. The blue nodes are row vectors from the entity embedding matrix WE , and the green nodes are row vectors from the relationship embedding matrix WR; the orange nodes are the combination operators as diagonal matrices. For clarity we only illustrate two candidates in Fig. 1, however Wc may contain an arbitrary number of candidate-entities.\nThe next step is to define the loss functions used in ProjE."}, {"heading": "2.2 Ranking Method and Loss Function", "text": "As defined in Defn. 1, we view the KGC problem as a ranking task where all positive candidates precede all negative candidates and train our model accordingly. Typically there are two ways to obtain such an ordering: with either 1) the pointwise method, or 2) the listwise method [31]. Although most existing KGC models, including TransE, TransR, TransH, and HolE use a pairwise ranking loss function during training, their ranking score is calculated independently in what is essentially a pointwise method when deployed. Based on the architecture we described in previous section, we propose two methods: 1) ProjE_pointwise, and 2) ProjE_listwise through the use of different activation functions for g(\u00b7) and f(\u00b7) in Eq. 5.\nFirst we describe the ProjE_pointwise ranking method. Because the relative order inside each entity set does not affect the prediction power, we can create a binary label vector in which all entities in E\u2212 have a score of 0, and all entities in E+ have a score of 1. Because we maximize the likelihood between the ranking score vector h(e, r) and the binary label vector, it is intuitive to view this task as a multi-class classification problem. Therefore, the loss function of ProjE_pointwise can be defined in a familiar way:\nL(e, r,y) = \u2212 \u2211\ni\u2208{i|yi=1} log(h(e, r)i)\n\u2212 \u2211\nm\nEj\u223cPy log(1\u2212 h(e, r)j), (6)\nwhere e and r are the input embedding vectors of a training instance in S, y \u2208 Rs is a binary label vector where yi = 1 means candidate i represents a positive label, m is the number of negative samples drawn from a negative candidate distribution Ej\u223cPy (described in next section). Because we view ProjE_pointwise as a multiclass classification problem, we use the sigmoid and tanh activation functions as our choice for g(\u00b7) and f(\u00b7) respectively. When deployed, the ranking score of the ith candidate-entity is:\nh(e, r)i = sigmoid ( Wc[i,:]tanh (e\u2295 r) + bp ) , (7)\nwhere Wc[i,:] represents i th candidate in the candidate-entity matrix.\nRecently, softmax regression loss has achieved good results in multi-label image annotation tasks [12, 11]. This is because multi-label image annotation, as well as many other classification tasks, should consider their predicted scores collectively. Inspired by this way of thinking, we employ the softmax activation function in order to classify candidate-entities collectively, i.e., using a listwise method. In this case we define the loss function of ProjE_listwise as:\nL(e, r,y) = \u2212 |y|\u2211\ni\n1(yi = 1)\n\u03a3i1(yi = 1) log (h(e, r)i) , (8)\nwhere the target probability (i.e., the target score) of a positive candidate is 1 / (total number of positive candidates of the input instance). Similar to Eq. 7, we replace g(\u00b7) and f(\u00b7) as softmax and tanh respectively, which can be written equivalently as:\nh(e, r)i = exp(Wc[i,:]tanh(e\u2295 r) + bp)\u2211 j exp(W c [j,:]tanh(e\u2295 r) + bp) . (9)\nLater, we perform a comprehensive set of experiments that compare ProjE with more than a dozen related models and discuss the proposed ProjE_pointwise and ProjE_listwise variants in depth."}, {"heading": "2.3 Candidate Sampling", "text": "Although ProjE limits the number of additional parameters, the projection operation may be costly due to the large number of candidate-entities (i.e., the number of rows in Wc). If we reduce the number of candidate-entities in the training phrase, we could create a smaller working set that only contains a subset of the embedding matrix WE . With this in mind, we use candidate sampling to reduce the number of candidate-entities. Candidate sampling is not a new problem; many recent works have addressed this problem in interesting ways [16, 26, 13]. We experimented with many choices, and found that the negative sampling used in Word2Vec [26] resulted the best performance.\nFor a given entity e, relationship r, and a binary label vector y, we compute the projection with all of the positive candidates and only a sampled subset of negative candidates from Py following the convention of Word2Vec. For simplicity, Py can be replaced by a (0, 1) binomial distribution B(1, py) shared by all training instances, where py is the probability that a negative candidate is sampled and 1\u2212 py is the probability that a negative candidate is not sampled. For every negative candidate in y we sample a value from B(1, py) to determine whether we include this candidate in the candidate-entity matrix Wc or not.\nIn the Supplementary Material we evaluate the performance of ProjE with different candidate sampling rates py \u2208 {5%, 25%, 50%, 75%, 95%}. Our experiments show relatively consistent performance using negative sampling rates as low as 25%."}, {"heading": "3 Experiments", "text": "We evaluate the ProjE model with entity prediction and relationship prediction tasks, and compare the performance against several existing methods using experimental procedures, datasets, and metrics established in the related work. The FB15K dataset is a 15,000-entity subset of Freebase; the Semantic MEDLINE Database (SemMedDB) is a KG extracted from all of PubMed [20]; and DBpedia is KG extracted from Wikipedia infoboxes [23]. Using DBpedia and SemMedDB, we also introduce a new fact checking task for a practical case study on the usefulness of these models. ProjE is implemented in Python using TensorFlow [1]; the code and data are available at https://github.com/nddsg/ProjE."}, {"heading": "3.1 Settings", "text": "For both entity and relationship prediction tasks, we use Adam [21] as the stochastic optimizer with default hyper-parameter settings: \u03b21 = 0.9, \u03b22 = 0.999, and = 1e\u22128. During the training phrase, we apply an L1 regularizer to all parameters in ProjE and a dropout layer on top of the combination operator to prevent over-fitting.\nThe hyper-parameters in ProjE are the learning rate lr, embedding size k, mini-batch size b, regularizer weight \u03b1, dropout probability pd, and success probability for negative candidate sampling py . We set lr = 0.01, b = 200, \u03b1 = 1e\u22125, and pd = 0.5 for both tasks, k = 200, py = 0.5 for the entity prediction task and k = 100, py = 0.75 for the relationship prediction task.\nFor all tasks, ProjE was trained for at most 100 iterations, and all parameters were initialized from a uniform distribution U [\u2212 6\u221a\nk , 6\u221a k ] as suggested by TransE [4]. ProjE can also be initialized with pre-trained\nembeddings."}, {"heading": "3.2 Entity and Relationship Prediction", "text": "We evaluated ProjE\u2019s performance on entity and relationship prediction tasks using the FB15K dataset following the experiment settings in TransE [4] and PTransE [24]. For entity prediction, we aim to predict a missing h (or t) for a given triple \u3008h, r, t\u3009 by ranking all of the entities in the KG. To create a test set we replaced the head or tail-entity with all entities in the KG, and rank these replacement entities in descending order. For relationship prediction, we replaced the relationship of each test triple with all relationships in the KG, and rank these replacement relationships in descending order.\nFollowing convention, we use mean rank and HITS@k as evaluation metrics. Mean rank measures the average rank of correct entities/relationships. HITS@k measures if correct entities/relationships appear within the top-k elements. The filtered mean rank and filtered HITS@k ignore all other true entities/relationships in the result and only look at the target entity/relationship. For example, if the target relationship between \u3008Springfield, ?, Illinois\u3009 is locatedIn, and the top-2 ranked relationships are capitalOf and locatedIn, then the raw mean rank and HITS@1 of this example would be 2 and 0 respectively, but the filtered mean rank and HITS@1 would both be 1 because the filtered mean rank and filtered HITS@k ignore the correct capitalOf relationship in the results set.\nIn addition to ProjE_pointwise and ProjE_listwise, we also evaluate ProjE_wlistwise, which is a slight variation of ProjE_listwise that incorporates instance-level weights (\u03a3i1(yi = 1)) to increase the importance of N-to-N and N-to-1 (1-to-N) relationships.\nTable 2 and Tab. 3 show that the three ProjE variants outperform existing methods in most cases. Table 3 contains fewer models than Tab. 2 because many models do not perform the relationship prediction task. We also adapt the pointwise and listwise ranking methods to TransE using the same hyperparameter settings, but the performance does not improve significantly and is not shown here. This indicates that the pointwise and listwise ranking methods are not merely simple tricks that can be added to any model to improve performance.\nSurprisingly, although softmax is usually used in mutually exclusive multi-class classification problems and sigmoid is a more natural choice for non-exclusive cases like the KGC task, our results show that both ProjE_listwise and ProjE_wlistwise perform better than ProjE_pointwise in most cases.\nThis is because KGC is a special ranking task, where a good model ought to have the following properties: 1) the score of all positive candidates should be maximized and the score of all negative candidates should be minimized, and 2) the number of positive candidates that are ranked above negative candidates should be maximized. By maximizing the similarity between the ranking score vector and the binary label vector, ProjE_pointwise meets the first property but fails to meet the second, i.e., ProjE_pointwise does not addresses the ranking order of all candidates collectively, because sigmoid is applied to each candidate individually. On the other hand, ProjE_listwise and ProjE_wlistwise successfully addresses both properties by maximizing the similarity between the binary label vector and the ranking score vector, which is an exponential-normalized ranking score vector that imposes an explicit ordering to the candidate-entities collectively.\nIn the Supplementary Material we also examine the stability of the proposed ProjE model and demonstrate that the performance of ProjE increases steadily and smoothly during training."}, {"heading": "3.3 Fact Checking", "text": "Unlike the entity prediction and relationship prediction tasks that predict randomly sampled triples, we employ a new fact checking task that tests the predictive power of various models on real world questions. We view the fact checking task as a type of link prediction problem because a fact statement \u3008h, r, t\u3009 can be naturally considered as an edge in a KG.\nWe use ProjE_wlistwise with a small change: rather than using entity embeddings directly, the input vector of ProjE consists of the predicate paths between the two entities [32]. We learn the entity-embeddings by adding an input layer that converts input predicate paths into the entity-embedding.\nWe employ the experimental setup and question set from Shi and Weninger (2016) on the DBPedia and SemMedDB data sets. Specifically, we remove all edges having the same label as the input relationship r and perform fact checking on the modified KG by predicting the existence of r on hundreds of variations of 7 types of questions. For example, the CapitalOf question checks various claims of the capitals of US states. In this case, we check if each of the 5 most populous cities within each state is its capital. This results in about 5\u00d7 5 = 250 checked facts with an 20/80 positive to negative label ratio. The odds that some fact statement is true is equivalent to the odds that the fact\u2019s triple is missing from the KG (rather than purposefully omitted, i.e., a true negative). Results in Tab. 4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type."}, {"heading": "4 Conclusions and Future Work", "text": "To recap, the contributions of the present work are as follows: 1) we view the KGC task as a ranking problem and project candidate-entities onto a vector representing a combined embedding of the known parts of an input triple and order the ranking score vector in descending order; 2) we show that by optimizing the ranking score vector collectively using the listwise ProjE variation, we can significantly improve prediction performance; 3) ProjE uses only directly connected, length-1 paths during training, and has a relatively simple 2-layer structure, yet outperforms complex models that have a richer parameter or feature set; and 4) unlike other models (e.g., CVSM, RTransE, DKRL), the present work does not require any pre-trained embeddings and has many fewer parameters than related models. We finally show that ProjE can outperform existing methods on fact checking tasks.\nFor future work, we will adapt more complicated neural network models such RNN and CNN with the embedding projection model presented here. It is also possible to incorporate rich feature sets from length-2 and length-3 paths, but these would necessarily add additional complexity. Instead, we plan to use information from complex paths in the KG to clearly summarize the many complicated ways in which entities are connected."}, {"heading": "A Appendix", "text": "In this supplement we provide a detailed algorithm description of the proposed ProjE_wlistwise model in Alg. 1, of which ProjE_listwise is a special case. Next, two more experiments are shown to demonstrate the training stability and scaling potential of ProjE.\nA.1 Training ProjE In Alg. 1, we describe the training process of ProjE_wlistwise. For a given training triple set S, we first construct the actual training set by randomly corrupting either the head entity h or tail entity t, and then generate the corresponding positive and negative candidates from S using candidate sampling if requested. Then for each mini-batch in the newly generated training data set, we calculate the loss and update the parameters accordingly.\nA.2 Model Stability In order to assess the training stability, we plotted the mean rank, filtered mean rank, HITS@10 and filtered HITS@10 over the first 25 training iterations on the FB15K dataset. For the purpose of illustration, we also draw three dashed lines representing the top-3 existing models that achieved the best performance in each metric.\nAs shown in Fig. 2, the performance of all three ProjE variants become stable after the first few iterations due to the use of Adam optimizer. The score variation between each iteration is also low, indicating stable training progress. The ProjE_wlistwise variant performed the best across all tests, followed by ProjE_listwise and ProjE_pointwise respectively.\nInput: Training triples S = {(h, r, t)}, entities E, relations R, embedding dimension k, dropout probability pd, candidate sampling rate py , regularizer parameter \u03b1. initialize embedding matrices WE, WR, combination operators (diagonal matrices) Deh, Drh, Det, Drt with uniform(\u2212 6\u221ak , 6\u221a k\n) Loop /* A training iteration/epoch */\nSh \u2190 {}, Th \u2190 {}, St \u2190 {}, Tt \u2190 {}; /* training data */ for (h, r, t) \u2208 S do /* construct training data using all training triples */ e\u2190 random(h, t); if e == h then /* tail is missing */\nSh.add([e, r]); /* all positive tails from S and some sampled negative\ncandidates */ Th.add({t\u2032|(h, r, t\u2032) \u2208 S} \u222a sample(E, py)); else /* head is missing */ St.add([e, r]); /* all positive heads from S and some sampled negative\ncandidates */ Tt.add({h\u2032|(h\u2032, r, t) \u2208 S} \u222a sample(E, py)); end end for each (Shb,Thb,Stb,Ttb) \u2282 (Sh,Th,St,Tt) do /* mini-batches */\nl\u2190 0; for (sh, th, st, tt) \u2208 (Shb,Thb,Stb,Ttb) do /* training instance */\noh \u2190 softmax(WE[tt,:] \u00d7 tanh(dropout(pd,Det \u00d7 (WE[st[0],:])T + Drt \u00d7 (WR[st[1],:])T + bc)) + bp); ot \u2190 softmax(WE[th,:] \u00d7 tanh(dropout(pd,Deh \u00d7 (WE[sh[0],:])T + Drh \u00d7 (WR[sh[1],:])\nT + bc)) + bp); l = l \u2212 \u03a3({1((h, st[1], st[0]) \u2208 S)|h \u2208 tt} \u25e6 log(oh))\u2212 \u03a3({1((sh[0], sh[1], t) \u2208 S)|t \u2208 th} \u25e6 log(ot));\nend /* L1 loss */ lr \u2190 Regu1(WE)+Regu1(WR)+Regu1(Deh)+Regu1(Drh)+Regu1(Det)+Regu1(Drt);\nupdate all parameters w.r.t. l + \u03b1lr; end\nEndLoop Algorithm 1: Algorithm of ProjE_wlistwise Training. \u25e6 is Hadamard product and \u00d7 is matrix product.\nA.3 Candidate Sampling In order to evaluate the relationship between the sampling rate and the model performance, we plotted five different py rates from 5% to 95% using the ProjE_wlistwise variant. All settings except py = 5% achieved better performance than the top-3 existing methods in each metric. These results demonstrate that that we can use ProjE with a relatively small sampling rate (25%), but it also demonstrates that ProjE is robust in the presence of different positive-to-negative training data ratios. Indeed, we find that the best results are often achieved under the 25% sampling ratio. This robustness provides ProjE the ability to handle very large datasets by significantly reducing the active working set."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Friends and neighbors on the Web", "author": ["L.A. Adamic", "E. Adar"], "venue": "Social Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Translating Embeddings for Modeling Multi-relational Data", "author": ["A. Bordes", "N. Usunier", "A. Garc\u00eda-Dur\u00e1n", "J. Weston", "O. Yakhnenko"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning Structured Embeddings of Knowledge Bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Computational fact checking from knowledge networks", "author": ["G.L. Ciampaglia", "P. Shiralkar", "L.M. Rocha", "J. Bollen", "F. Menczer", "A. Flammini"], "venue": "PLoS ONE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Large-scale named entity disambiguation based on wikipedia data", "author": ["S. Cucerzan"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Knowledge vault: a web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "In SIGKDD,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "AMIE: association rule mining under incomplete evidence in ontological knowledge bases", "author": ["L.A. Gal\u00e1rraga", "C. Teflioudi", "K. Hose", "F. Suchanek"], "venue": "In WWW,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Composing Relationships with Translations", "author": ["A. Garc\u00eda-Dur\u00e1n", "A. Bordes", "N. Usunier"], "venue": "EMNLP, pages 286\u2013290,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep convolutional ranking for multilabel image annotation", "author": ["Y. Gong", "Y. Jia", "T. Leung", "A. Toshev", "S. Ioffe"], "venue": "arXiv preprint arXiv:1312.4894,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation", "author": ["M. Guillaumin", "T. Mensink", "J. Verbeek", "C. Schmid"], "venue": "In ICCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "In AISTATS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Evaluating entity linking with wikipedia", "author": ["B. Hachey", "W. Radford", "J. Nothman", "M. Honnibal", "J.R. Curran"], "venue": "AI, 194:130\u2013150,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Topic-sensitive pagerank", "author": ["T.H. Haveliwala"], "venue": "In WWW, pages 517\u2013526,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "SimRank: a measure of structural context similarity", "author": ["G. Jeh", "J. Widom"], "venue": "In KDD,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N. Le Roux", "A. Bordes", "G. Obozinski"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Locally Adaptive Translation for Knowledge Graph Embedding", "author": ["Y. Jia", "Y. Wang", "H. Lin", "X. Jin", "X. Cheng"], "venue": "In AAAI,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "SemMedDB: a PubMed-scale repository of biomedical semantic predications", "author": ["H. Kilicoglu", "D. Shin", "M. Fiszman", "G. Rosemblat", "T.C. Rindflesch"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["N. Lao", "W.W. Cohen"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "DBpedia - a large-scale, multilingual knowledge base extracted from Wikipedia", "author": ["J. Lehmann", "R. Isele", "M. Jakob"], "venue": "Semantic Web,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "author": ["Y. Lin", "Z. Liu", "M. Sun"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "In AAAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Compositional Vector Space Models for Knowledge Base Inference", "author": ["A. Neelakantan", "B. Roth", "A. McCallum"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "A review of relational machine learning for knowledge graphs: From multi-relational link prediction to automated knowledge graph construction", "author": ["M. Nickel", "K. Murphy", "V. Tresp", "E. Gabrilovich"], "venue": "arXiv preprint arXiv:1503.00759,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Holographic Embeddings of Knowledge Graphs", "author": ["M. Nickel", "L. Rosasco", "P. Tomaso"], "venue": "In AAAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In ICML,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "A representation theory for ranking functions", "author": ["H.H. Pareek", "P.K. Ravikumar"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Fact checking in heterogeneous information networks", "author": ["B. Shi", "T. Weninger"], "venue": "In WWW,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Template-based question answering over rdf data", "author": ["C. Unger", "L. B\u00fchmann", "J. Lehmann", "A.-C. Ngonga Ngomo", "D. Gerber", "P. Cimiano"], "venue": "In WWW,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Representation Learning of Knowledge Graphs with Entity Descriptions", "author": ["R. Xie", "Z. Liu", "J. Jia", "H. Luan", "M. Sun"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Bipartite network projection and personal recommendation", "author": ["T. Zhou", "J. Ren", "M. Medo", "Y.-C. Zhang"], "venue": "Phys. Rev. E,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}], "referenceMentions": [{"referenceID": 33, "context": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few.", "startOffset": 173, "endOffset": 177}, {"referenceID": 6, "context": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few.", "startOffset": 201, "endOffset": 204}, {"referenceID": 13, "context": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few.", "startOffset": 227, "endOffset": 231}, {"referenceID": 31, "context": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few.", "startOffset": 247, "endOffset": 251}, {"referenceID": 27, "context": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few.", "startOffset": 273, "endOffset": 277}, {"referenceID": 2, "context": ", Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": ", Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t.", "startOffset": 27, "endOffset": 30}, {"referenceID": 34, "context": ", Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": ", Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "Instead of simply adding h + r, more expressive combination operators are learned by Knowledge Vault [8] and HolE [29] in order to predict the existence of \u3008h, r, t\u3009 in the KG.", "startOffset": 101, "endOffset": 104}, {"referenceID": 28, "context": "Instead of simply adding h + r, more expressive combination operators are learned by Knowledge Vault [8] and HolE [29] in order to predict the existence of \u3008h, r, t\u3009 in the KG.", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Other models, such as the Neural Tensor Network (NTN) [33] and the Compositional Vector Space Model (CVSM) [27], incorporate a multilayer neural network solution into the existing models.", "startOffset": 54, "endOffset": 58}, {"referenceID": 26, "context": "Other models, such as the Neural Tensor Network (NTN) [33] and the Compositional Vector Space Model (CVSM) [27], incorporate a multilayer neural network solution into the existing models.", "startOffset": 107, "endOffset": 111}, {"referenceID": 9, "context": "Unfortunately, due to their extremely large parameter size, these models either (i) do not scale well or (2) consider only a single relationship at a time [10] thereby limiting their usefulness on large, real-world KGs.", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "PTransE [24] and RTransE [10] employ extended path information from 2 and 3-hop trails over the knowledge graph.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "PTransE [24] and RTransE [10] employ extended path information from 2 and 3-hop trails over the knowledge graph.", "startOffset": 25, "endOffset": 29}, {"referenceID": 35, "context": ", they require pre-trained KG embeddings (RTransE, CVSM), pre-selected paths (PTransE, RTransE), or pre-computed content embeddings of each node (DKRL [36]) before their model training can even begin.", "startOffset": 151, "endOffset": 155}, {"referenceID": 23, "context": ", embeddings) for entities W and relationships W by minimizing a margin-based pairwise ranking loss [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "The most widely used embedding model in this category is TransE [4], which views relationships as translations from a head entity to a tail entity on the same low-dimensional plane.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "The Unstructured model [3] is a special case of TransE where r = 0 for all relationships.", "startOffset": 23, "endOffset": 26}, {"referenceID": 34, "context": "For example, the entity translations in TransH [35] are computed on a hyperplane that is perpendicular to the relationship embedding.", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "In TransR [25] the entities and relationships are embedded on separate planes and then the entity-vectors are translated to the relationship\u2019s plane.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "Structured Embedding (SE) [5] creates two translation matrices for each relationship and applies them", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "Knowledge Vault [8] and HolE [29], on the other hand, focus on learning a new combination operator instead of simply adding two entity embeddings element-wise.", "startOffset": 16, "endOffset": 19}, {"referenceID": 28, "context": "Knowledge Vault [8] and HolE [29], on the other hand, focus on learning a new combination operator instead of simply adding two entity embeddings element-wise.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "Unlike aforementioned models that focus on different E(h, r, t), TransA [19] introduces an adaptive local margin approach that determines \u03b3 by a closed set of entity candidates.", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "Other similar models include RESCAL [30], Semantic Matching Energy (SME) [3], and the Latent Factor Model (LFM) [18].", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "Other similar models include RESCAL [30], Semantic Matching Energy (SME) [3], and the Latent Factor Model (LFM) [18].", "startOffset": 73, "endOffset": 76}, {"referenceID": 17, "context": "Other similar models include RESCAL [30], Semantic Matching Energy (SME) [3], and the Latent Factor Model (LFM) [18].", "startOffset": 112, "endOffset": 116}, {"referenceID": 32, "context": "The Neural Tensor Network (NTN) model [33] is an exception to the basic energy function in Eq.", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "For instance, the Compositional Vector Space Model (CVSM) [27] composes a sequence of relationship embeddings into a single path embedding using a Recurrent Neural Network (RNN).", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "RTransE [10] solves the relationship-specific problem in CVSM by using entity and relationship embeddings learned from TransE.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "PTransE [24] is another path-based method that uses path information in its energy function.", "startOffset": 8, "endOffset": 12}, {"referenceID": 36, "context": "Then PTransE uses PCRA [37] to select input paths within a given length constraint.", "startOffset": 23, "endOffset": 27}, {"referenceID": 30, "context": "Typically there are two ways to obtain such an ordering: with either 1) the pointwise method, or 2) the listwise method [31].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "Recently, softmax regression loss has achieved good results in multi-label image annotation tasks [12, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 10, "context": "Recently, softmax regression loss has achieved good results in multi-label image annotation tasks [12, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 15, "context": "Candidate sampling is not a new problem; many recent works have addressed this problem in interesting ways [16, 26, 13].", "startOffset": 107, "endOffset": 119}, {"referenceID": 25, "context": "Candidate sampling is not a new problem; many recent works have addressed this problem in interesting ways [16, 26, 13].", "startOffset": 107, "endOffset": 119}, {"referenceID": 12, "context": "Candidate sampling is not a new problem; many recent works have addressed this problem in interesting ways [16, 26, 13].", "startOffset": 107, "endOffset": 119}, {"referenceID": 25, "context": "We experimented with many choices, and found that the negative sampling used in Word2Vec [26] resulted the best performance.", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "The FB15K dataset is a 15,000-entity subset of Freebase; the Semantic MEDLINE Database (SemMedDB) is a KG extracted from all of PubMed [20]; and DBpedia is KG extracted from Wikipedia infoboxes [23].", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "The FB15K dataset is a 15,000-entity subset of Freebase; the Semantic MEDLINE Database (SemMedDB) is a KG extracted from all of PubMed [20]; and DBpedia is KG extracted from Wikipedia infoboxes [23].", "startOffset": 194, "endOffset": 198}, {"referenceID": 0, "context": "ProjE is implemented in Python using TensorFlow [1]; the code and data are available at https://github.", "startOffset": 48, "endOffset": 51}, {"referenceID": 20, "context": "For both entity and relationship prediction tasks, we use Adam [21] as the stochastic optimizer with default hyper-parameter settings: \u03b21 = 0.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "For all tasks, ProjE was trained for at most 100 iterations, and all parameters were initialized from a uniform distribution U [\u2212 6 \u221a k , 6 \u221a k ] as suggested by TransE [4].", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "We evaluated ProjE\u2019s performance on entity and relationship prediction tasks using the FB15K dataset following the experiment settings in TransE [4] and PTransE [24].", "startOffset": 145, "endOffset": 148}, {"referenceID": 23, "context": "We evaluated ProjE\u2019s performance on entity and relationship prediction tasks using the FB15K dataset following the experiment settings in TransE [4] and PTransE [24].", "startOffset": 161, "endOffset": 165}, {"referenceID": 31, "context": "We use ProjE_wlistwise with a small change: rather than using entity embeddings directly, the input vector of ProjE consists of the predicate paths between the two entities [32].", "startOffset": 173, "endOffset": 177}, {"referenceID": 1, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}, {"referenceID": 5, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}, {"referenceID": 16, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}, {"referenceID": 8, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}, {"referenceID": 14, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}, {"referenceID": 21, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}], "year": 2016, "abstractText": "With the large volume of new information created every day, determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners. To address this challenge, a number of knowledge graph completion methods have been developed using low-dimensional graph embeddings. Although researchers continue to improve these models using an increasingly complex feature space, we show that simple changes in the architecture of the underlying model can outperform state-of-the-art models without the need for complex feature engineering. In this work, we present a shared variable neural network model called ProjE that fills-in missing information in a knowledge graph by learning joint embeddings of the knowledge graph\u2019s entities and edges, and through subtle, but important, changes to the standard loss function. In doing so, ProjE has a parameter size that is smaller than 11 out of 15 existing methods while performing 37% better than the current-best method on standard datasets. We also show, via a new fact checking task, that ProjE is capable of accurately determining the veracity of many declarative statements. Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few. In our view, KGs are an example of a heterogeneous information network containing entity-nodes and relationship-edges corresponding to RDF-style triples \u3008h, r, t\u3009 where h represents a head entity, and r is a relationship that connects h to a tail entity t. KGs are widely used for many practical tasks, however, their correctness and completeness are not guaranteed. Therefore, it is necessary to develop knowledge graph completion (KGC) methods to find missing or errant relationships with the goal of improving the general quality of KGs, which, in turn, can be used to improve or create interesting downstream applications. The KGC task can be divided into two non-mutually exclusive sub-tasks: (i) entity prediction and (ii) relationship prediction. The entity prediction task takes a partial triple \u3008h, r, ?\u3009 as input and produces a ranked list of candidate entities as output: Definition 1. (Entity Ranking Problem) Given a Knowledge Graph G = {E,R} and an input triple \u3008h, r, ?\u3009, the entity ranking problem attempts to find the optimal ordered list such that \u2200ej\u2200ei ((ej \u2208 E\u2212 \u2227 ei \u2208 E+)\u2192 ei \u227a ej), where E+ = {e \u2208 {e1, e2, . . . , el}|\u3008h, r, e\u3009 \u2208 G} and E\u2212 = {e \u2208 {el+1, el+2, . . . , e|E|}|\u3008h, r, e\u3009 / \u2208 G}. Distinguishing between head and tail-entities is usually arbitrary, so we can easily substitute \u3008h, r, ?\u3009 for \u3008?, r, t\u3009. The relationship prediction task aims to find a ranked list of relationships that connect a head-entity with a tail-entity, i.e., \u3008h, ?, t\u3009. When discussing the details of the present work, we focus specifically on the entity prediction task; however, it is straightforward to adapt the methodology to the relationship prediction task by changing the input. A number of KGC algorithms have been developed in recent years, and the most successful models all have one thing in common: they use low-dimensional embedding vectors to represent entities and relationships. Many embedding models, e.g., Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t. In these models the loss functions are all the same, so models differ in how they transform the 1 ar X iv :1 61 1. 05 42 5v 1 [ cs .A I] 1 6 N ov 2 01 6 entity embeddings h and t with respect to the relationship embeddings r. Instead of simply adding h + r, more expressive combination operators are learned by Knowledge Vault [8] and HolE [29] in order to predict the existence of \u3008h, r, t\u3009 in the KG. Other models, such as the Neural Tensor Network (NTN) [33] and the Compositional Vector Space Model (CVSM) [27], incorporate a multilayer neural network solution into the existing models. Unfortunately, due to their extremely large parameter size, these models either (i) do not scale well or (2) consider only a single relationship at a time [10] thereby limiting their usefulness on large, real-world KGs. Despite their large model size, the aforementioned methods only use singleton triples, i.e., length-1 paths in the KG. PTransE [24] and RTransE [10] employ extended path information from 2 and 3-hop trails over the knowledge graph. These extended models achieve excellent performance due to the richness of the input data; unfortunately, their model-size grows exponentially as the path-length increases, which further exacerbates the scalability issues associated with the already high number of parameters of the underlying-models. Another curious finding is that some of the existing models are not self-contained models, i.e., they require pre-trained KG embeddings (RTransE, CVSM), pre-selected paths (PTransE, RTransE), or pre-computed content embeddings of each node (DKRL [36]) before their model training can even begin. TransR and TransH are self-contained models, but their experiments only report results using pre-trained TransE embeddings as input. With these considerations in mind, in the present work we rethink some of the basic decisions made by previous models to create a projection embedding model (ProjE) for KGC. ProjE has four parts that distinguish it from the related work: 1. Instead of measuring the distance between input triple \u3008h, r, ?\u3009 and entity candidates on a unified or a relationship-specific plane, we choose to project the entity candidates onto a target vector representing the input data. 2. Unlike existing models that use transformation matrices, we combine the embedding vectors representing the input data into a target vector using a learnable combination operator. This avoids the addition of a large number of transformation matrices by reusing the entity-embeddings. 3. Rather than optimizing the margin-based pairwise ranking loss, we optimize a ranking loss of the list of candidate-entities (or relationships) collectively. We further use candidate sampling to handle very large data sets. 4. Unlike many of the related models that require pre-trained data from prerequisite models or explore expensive multi-hop paths through the knowledge graph, ProjE is a self-contained model over length-1 edges.", "creator": "LaTeX with hyperref package"}}}