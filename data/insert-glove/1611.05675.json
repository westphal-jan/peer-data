{"id": "1611.05675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Study on Feature Subspace of Archetypal Emotions for Speech Emotion Recognition", "abstract": "Feature on-the-air subspace selection is an tsaban important part cornishmen in dokan speech kargin emotion cossiga recognition. coatepec Most of cooker the studies http://www.whitehouse.gov are roca devoted bugling to soun finding a motori feature suvi subspace for macher representing dang all 31.24 emotions. However, some jurgis studies have indicated rowand that the il-2 features associated baserunner with \u014dbaku different 55-acre emotions s\u00e1enz are menominee not nasdr exactly 54sec the gbv same. Hence, traditional purling methods masseria may fail gilsa to hotfix distinguish some of mcgerr the emotions with just one pazz global feature terminological subspace. In terumo this work, we muster propose a ghaus new siok divide mulde and conquer lycus idea melo to solve the raghuraman problem. First, the feature subspaces cathail are elkington constructed for natatorium all the combinations veselka of ehf every two eshnunna different 90-78 emotions (emotion - pair ). Bi - classifiers leonean are then trained on these feature suranand subspaces outworld respectively. The vehicross final suenos emotion recognition kinzie result pavagadh is mcmurphy derived by ecsa the longstanding voting isarn and tz competition peyote method. Experimental kambakhsh results demonstrate charites that the daff proposed meador method owenite can xlviii get better results a-grade than larco the pope\u015fti traditional fugs multi - tsf classification 85,000-seat method.", "histories": [["v1", "Thu, 17 Nov 2016 13:32:59 GMT  (527kb)", "http://arxiv.org/abs/1611.05675v1", "5 pages, 4 figures, ICASSP-2017"]], "COMMENTS": "5 pages, 4 figures, ICASSP-2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["xi ma", "zhiyong wu", "jia jia", "mingxing xu", "helen meng", "lianhong cai"], "accepted": false, "id": "1611.05675"}, "pdf": {"name": "1611.05675.pdf", "metadata": {"source": "CRF", "title": "STUDY ON FEATURE SUBSPACE OF ARCHETYPAL EMOTIONS FOR SPEECH EMOTION RECOGNITION", "authors": ["Xi Ma", "Zhiyong Wu", "Jia Jia", "Mingxing Xu", "Helen Meng", "Lianhong Cai"], "emails": ["max15@mails.tsinghua.edu.cn,", "zywu@se.cuhk.edu.hk,", "hmmeng@se.cuhk.edu.hk,", "jjia@tsinghua.edu.cn", "xumx@tsinghua.edu.cn", "clh-dcs@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n05 67\n5v 1\n[ cs\n.L G\n] 1\n7 N\nIndex Terms\u2014 speech emotion recognition, feature subspace, emotion pair"}, {"heading": "1. INTRODUCTION", "text": "Emotion recognition plays an important role in many applications, especially in human-computer interaction systems that are increasingly common today. As one of the main communication media between human beings, voice has received widespread attention from researchers [1]. Speech contains a wealth of emotional information, how to extract such information from the original speech signal is of great importance for speech emotion recognition.\nAs an important part of speech emotion recognition, the selection of feature subspace has attracted lot of research interests. Existing researches on feature subspace selection can be divided into three categories, including the artificial selection of emotion related features, the automatic feature selection algorithms to select feature subset from a large set of numerous feature candidates, and the transformation method to map the original feature space to the new one in favor of emotion recognition. Most of these researches are devoted to finding a common and global feature subspace that can represent all kinds of emotions. However, studies have already indicated that the features associated with different emotions are not exactly the same. In other words, if we can divide the whole emotions space into several subspaces and find the\nfeatures that are most distinguishable for each subspace separately, the emotion recognition performance on the whole space might be boosted. Motivated by this, we propose a divide and conquer idea for emotion recognition by leveraging feature subspaces. The feature subspaces are first constructed for every two different emotions (i.e. emotion-pair); bi-classifier are then used to distinguish the emotions for each emotion-pair from the feature subspace; and the final emotion recognition result is derived by voting and competition method.\nThe reset of the paper is organized as follows. Section 2 summarizes previous related work on feature selection. Our proposed method is then detailed in Section 3. Experiments and results are presented in Section 4. Section 5 concludes the paper."}, {"heading": "2. RELATED WORK", "text": "As a common issue for many classification problems [2], feature selection aims to pick a subset of features that are most relevant to the target concept [3] or to reduce the dimension of features for reducing computational time as well as improving the performance [4]. There have been many studies on feature selection for speech emotion recognition. In [5\u20137], prosody-based acoustic features, including pitchrelated, energy-related and timing features have been widely used for recognizing speech emotion. Spectral-based acoustic features also play an important role in emotion recognition, such as Linear Predictor Coefficients (LPC) [8], Linear Predictor Cepstral Coefficients (LPCC) [9] and Mel-frequency Cepstral Coefficients (MFCC) [10]. In [11], voice quality features have also been shown to be related to emotions.\nBesides manual selection, there have also many automatic feature selection algorithms been proposed. For example, Sequential Floating Forward Selection (SFFS) [12] is an iterative method that can find a subset of features near to the optimal one. Some evolutionary algorithms such as Genetic Algorithm (GA) [13] are often used in feature selection. Feature space transformation is another type of method, including Principal Component Analysis (PCA) [4] and Neural Network (NN) [14].\nTo describe emotions, some studies have used the psychological dimensional space such as the 2-dimensional arousalvalence model and the 3-dimensional valence-activation-\ndominance model [15]. Besides, discrete emotional labels, the so-called archetypal emotions [16], are common used in speech emotion recognition. Different archetypal emotions are located at different locations in the dimensional space. [17] has proposed a hierarchical approach to classify the speech emotions with the dimensional model. However, the selection of emotions at different stages is too subjective, and the used feature sets may not have a good match to the psychological emotional model."}, {"heading": "3. METHOD", "text": "Our study is based on archetypal emotions. The emotionpair is composed of two different kinds of archetypal emotion, like Anger and Happiness. For all possible combinations of archetypal emotion-pairs, the bi-classification and voting method is used to distinguish every emotion-pairs and to derive the final emotion recognition result, As shown in Figure 1, the whole method involves four steps: feature extraction, feature subspace selection, emotion classification and voting decision."}, {"heading": "3.1. Feature Extraction", "text": "The used acoustic features include the following low-level descriptors (LLDs): Intensity, Loudness, 12 MFCC, Pitch (F0), Probability of voicing, F0 envelope, 8 LSF (Line Spectral Frequencies), Zero-Crossing Rate. Delta regression coefficients are computed from these LLDs, and the following statistical functionals are applied to the LLDs and delta coefficients: Max./Min. value and respective relative position within input, range, arithmetic mean, 2 linear regression coefficients and linear and quadratic error, standard deviation, skewness, kurtosis, quartile 1-3, and 3 inter-quartile ranges. All the features are utterance-level features. In feature selection stage, the relevant feature subset or relevant feature space will be derived from the above large feature set."}, {"heading": "3.2. Feature Subspace Selection", "text": "Different from traditional methods that distinguish all emotions with just one global feature subspace, this work selects different feature subspaces for different combination of emotion-pairs. For a specific emotion-pair, its corresponding feature subspace should be of the best power in distinguishing the two emotions of the pair. In order to verify the generality of our idea, the methods of feature subset selection and feature space transformation has been considered. Genetic algorithm (GA) is used for feature subset selection, while neural network (NN) is used for feature space transformation.\nGA is a kind of stochastic searching and optimizing algorithm, that simulates the natural evolution process. We use a fixed number of features to form a vector (so called individual), and a fixed number of individuals to form the first population. Crossover and mutation operation is then used to generate a new individual. New population will be selected by comparing fitness. The \u201cWrapper\u201d method is used to calculate the fitness of individuals, i.e. the accuracy of the classifier is used as the fitness. The above procedures are repeated until the average fitness of population reaches the threshold or the evolutionary generation reaches the threshold. Compared to some other heuristic searching algorithm, such as Sequential Floating Forward Selection (SFFS), it is more flexible to control the computing time for GA, especially when the feature set is relatively large."}, {"heading": "3.3. Emotion Classification", "text": "By using the feature subspace obtained in the previous step, a particular classifier can be trained for a specific emotion-pair and be designated to distinguish the emotions in that emotionpair. As each classifier is only related to a specific emotionpair, we call it bi-classifier. For feature subset selection, two basic classification algorithms are used, including Logistic Regression (LR) and Support Vector Machine (SVM). For feature space transformation, neural network (NN) is used as the classifier."}, {"heading": "3.4. Voting Decision", "text": "After getting the emotion distinguishing result for each emotion-pair in the previous emotion classification step, a voting and competition method is finally used to integrate the emotion classification results for all emotion-pairs to derive the final emotion recognition result. The voting decision process is summarized in Algorithm 1.\nIt can be proved that the final emotion recognition result can be correctly derived by the above voting decision algorithm given that all the bi-classifiers give the correct distinguishing result for each emotion-pair. The theorem and proof procedure is described in Theorem 1.\nTheorem 1. Voting decision will be able to derive the correct result given all bi-classifiers are in correct situation.\nProof. Given the symbol definitions in Algorithm 1, let ei be the target emotion and ei \u2208 E. So,\nR is correct \u21d2 nei = M \u2212 1\n\u21d2 nej < M \u2212 1, ej 6= ei\n\u21d2 em = ei\nAlgorithm 1 Voting Decision Algorithm Require:\nInput: M : the number of emotions E = {ei|i = 1, 2, ...,M}: emotion set R = {reiej |ei 6= ej; reiej , ei, ej \u2208 E}: classification result of bi-classifier Ensure: 1: Compute the number of different emotions in R: Ne =\n{nei |ei \u2208 R} 2: Create an emotion set with the maximum number in Ne:\nEmax = {mk|mk \u2208 argmaxnek E; k = 1, 2, ...,K}\n3: em := m1 4: if K = 1 then 5: return em 6: else 7: for k = 2 to K do 8: em := remmk 9: end for\n10: return em 11: end if"}, {"heading": "4. EXPERIMENT", "text": ""}, {"heading": "4.1. Experimental Setting", "text": "In this study, we used the well known Berlin emotional database (EmoDB) [18]. Ten actors (5 male and 5 female) simulated the emotions, producing 10 German sentences (5 short and 5 longer). EmoDB comprises 535 utterances that cover 6 archetypal emotions and 1 neutral emotion from everyday communication, namely, Anger, Fear, Happiness, Sadness, Disgust, Boredom and Neutral. Our work focuses on speaker independent emotion recognition, hence the samples of 8 actors (4 males and 4 females) are used as the training set, and the samples from the other 2 actors (1 male and 1 female) are used as the test set. The 5-fold-cross-validation method is used to conduct the experiments. OpenSmile toolkit [19] is used to extract acoustic features, and a total of 988 features are obtained.\nTwo experiment are conducted. In the first one, GA is used to select feature subset for each emotion-pair. As for emotion classification, the same emotion classifier is used for all emotion-pairs, but trained with different features subsets associated with different emotion-pairs. Furthermore, the same classifier is also used to recognize the emotions from the feature subsets associated with all emotions. This experiment is to verify that selecting the feature subset associated with emotion-pairs is better than the feature subset associated with all emotions using the same classifier. The details of parameter setting for GA are as follows: individual size 50, population size 100, two-point crossover with crossover probability 0.8, substitution mutation with mutation probability 0.1. If the generation number reaches 300 or the fitness value does not improve for the last 100 generation, the GA algorithm stops. In this experiment, 50 most representative features are selected by the GA algorithm to form the feature subset for not only every emotion-pair but also all emotions. Furthermore, two different classifiers (LR and SVM) are tested.\nIn the second experiment, neural network (NN) is used not only for feature space transformation but also as the classifier. This experiment is to verify that feature space transformation\nto the feature space associated with emotion-pairs is better than the feature space associated with all emotions using the same feature space transformation method. For experimental settings, the neural network has a 988-unit input layer corresponding to the dimensionality of original feature vector, and one 50-unit hidden layer corresponding to the dimensionality of feature subset in the feature selection method. Batch gradient descend method is used to learn the weights and the activation function is the sigmoid function. The learning rate is set to 0.1."}, {"heading": "4.2. Experimental Result", "text": ""}, {"heading": "4.2.1. Feature Selection Method", "text": "We first conduct the feature selection experiment by comparing the similarity degree of the feature subspace for each emotion-pair and the global feature subspace for all emotions. Figure 2 depicts the number of the common features (vertical axis) that are shared between the feature subspace for a specific emotion-pair and the global feature subspace for all emotions, where the horizontal axis represents different emotion-pairs (e.g. N-A is the Neutral-Anger pair). It should be noted that all feature subspaces (including emotion-pair specific feature subspace, and the global feature subspace for all emotions) contain 50 selected features, as described in Section 4.1. From the figure, it can be seen that the number of the common features between each emotion-pair and all emotions is no more than 5 (5 out of 50). This indicates that the feature subspace of each emotion-pair is quite different from the global feature space of all emotions. This further confirms the necessity to perform pair-wised emotion classification with feature subspace related to emotion-pairs.\nWe further conducted emotion recognition experiment to compare the emotion recognition accuracies of different feature selection criterions between the proposed method and the\ntraditional method. Experimental results are shown in Table 1, where \u201cBi-classification and voting\u201d is the proposed method, while \u201cMulti-classification\u201d is the traditional method using the global feature subspace for all emotions. As can be seen, the recognition accuracy obtained by \u201cBi-classification and voting\u201d is significantly higher than that using the \u201cMulticlassification\u201d method (P < 0.05 by T-test).\nThe emotion recognition accuracy (recognition rate) of different emotions are further computed and shown in Figure 3, where Bi-clf and voting represents \u201cBi-classification and voting\u201d, Multi-clf represents \u201cMulti-classification\u201d. It should be noted that the result about Disgust is not depicted because there are only quite few utterances with Disgust emotion. The experimental results indicate that the \u201cBiclassification and voting\u201d method achieve better performance than the \u201cMulti-classification\u201d method for all emotions using both classifiers (LR and SVM). This result further proves that the priori information of the emotion-pair is helpful to feature selection and can bring further performance improvements for emotion recognition."}, {"heading": "4.2.2. Feature Space Transformation", "text": "Similarly, we also conduct experiments in the feature space transformation scenario to validate the efficiency of our proposed divide and conquer idea for emotion recognition. The emotion recognition accuracy (recognition rate) by using different classification criterions withe feature space transformation are shown in Table 2, and the recognition of different emotions are showed in Figure 4.\nFrom the experimental results, we can get the same conclusion as the feature selection method. It is indicated that\nTable 2. Comparison of emotion recognition accuracy by using feature space transformation.(P < 0.05)\nNeural Network Bi-classification and voting 0.652\nMulti-classification 0.552\nNeutral Anger Boredom Fear Happiness Sadness\nemotion\n0\n0.5\n1\nac cu\nra cy\nBi-clf and voting(NN) Multi-clf(NN)\nFig. 4. Emotion recognition accuracy of different emotions for different classification methods by using feature space transformation.\nthe method of \u201cBi-classification and voting\u201d is also effective in feature space transformation. This confirms the generality of our proposed method. For details of the experiment, please refer to our code and document on GitHub1."}, {"heading": "5. CONCLUSION", "text": "In this paper we present a \u201cBi-classification and voting\u201d method by distinguishing different emotion-pairs in different feature space. The experimental results have proved that this method can get better result compared to the traditional multi-classification method. In addition, our method is a kind of divide and conquer algorithm which converts a complex multi-classification problem into many simple bi-classification problems. This idea makes it possible to boost the multi-class emotion recognition performance by optimizing the emotion classification performance for each emotion-pair. Hence, our future work will be devoted to the classifier optimization of different emotion-pairs."}, {"heading": "6. ACKNOWLEDGEMENT", "text": "This work is supported by National High Technology Research and Development Program of China (2015AA016305), National Natural Science Foundation of China (NSFC) (61375027, 61433018 and 61370023), joint fund of NSFCRGC (Research Grant Council of Hong Kong) (61531166002, N CUHK404/15) and Major Program for National Social Science Foundation of China (13&ZD189).\n1git@github.com : mxmaxi007/Emotion Recognition.git"}, {"heading": "7. REFERENCES", "text": "[1] M.S. Kamel M. El Ayadi and F. Karray, \u201cSurvey on speech emotion recognition: features, classification schemes and database,\u201d Pattern Recognition, vol. 44, no. 3, pp. 572\u2013587, 2011.\n[2] M. Dash and H. Liu, \u201cFeature selection for classification,\u201d Intelligent Data Analysis, vol. 1, no. 3, pp. 131\u2013 156, 1997.\n[3] I. Guyon and A. Elisseeff, \u201cAn introduction to variable and feature selection,\u201d Journal of machine learning research, , no. 3, pp. 1157\u20131182, 2003.\n[4] J.P. Hespanha P.N. Belhumeur and D.J. Kriegman, \u201cEigenfaces vs. fisherfaces: Recognition using class specific linear projection,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 19, no. 7, pp. 711\u2013720, 1997.\n[5] S. Lee C. Busso and S. Narayanan, \u201cAnalysis of emotionally salient aspects of fundamental frequency for emotion detection,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 4, pp. 582\u2013596, 2009.\n[6] E. Douglas-Cowie R. Cowie and N. Tsapatsoulis, \u201cEmotion recognition in human-computer interaction,\u201d IEEE Signal Processing Magazine, vol. 18, no. 1, pp. 32\u201380, 2001.\n[7] J. Kortelainen E. Vayrynen and T. Seppanen, \u201cClassifier-based learning of nonlinear feature manifold for visualization of emotional speech prosody,\u201d IEEE Transactions on Affective Computing, vol. 4, no. 1, pp. 47\u201356, 2013.\n[8] L.R. Rabiner and R.W. Schafer, Digital Processing of Speech Signals, Prentice Hall, Upper Saddle River, New Jersey 07458, USA, 1978.\n[9] B.S. Atal, \u201cEffectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification,\u201d the Journal of the Acoustical Society of America, vol. 55, no. 6, pp. 1304\u20131312, 1974.\n[10] S. Davis and P. Mermelstein, \u201cEffectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 28, no. 4, pp. 357\u2013366, 1980.\n[11] C. Gobl and A. N. Chasaide, \u201cThe role of voice quality in communicating emotion, mood and attitude,\u201d Speech Communication, vol. 40, no. 1-2, pp. 189\u2013212, 2003.\n[12] D. Ververidis and C. Kotropoulos, \u201cFast and accurate sequential floating forward feature selection with the bayes classifier applied to speech emotion recognition,\u201d Signal Processing, vol. 88, no. 12, pp. 2956\u20132970, 2008.\n[13] V. Kadirkamanathan F. J. Ferri and J. Kittler, \u201cFeature subset search using genetic algorithms,\u201d in in: IEE/IEEE Workshop on Natural Algorithms in Signal Processing, IEE. 1993, Press.\n[14] M.L. Seltzer D. Yu and J. Li, \u201cFeature learning in deep neural networks - studies on speech recognition tasks,\u201d arXiv:1301.3605, 2013.\n[15] H. Schlosberg, \u201cThree dimensions of emotion,\u201d Psychological Review, vol. 61, no. 2, pp. 81\u201388, 1954.\n[16] A. Ortony and T.J. Turner, \u201cWhat\u2019s basic about basic emotions,\u201d Psychological Review, vol. 97, no. 3, pp. 315\u2013331, 1990.\n[17] M. Lugger and B. Yang, Psychological Motivated MultiStage Emotion Classification Exploiting Voice Quality Features, Speech Recognition, France Mihelic and Janez Zibert (Ed.), InTech, DOI: 10.5772/6383., 2008.\n[18] A. Paeschke F. Burkhardt and M. Rolfes, \u201cA database of german emotional speech,\u201d in Proceedings Interspeech 2005, 2005, pp. 1517\u20131520.\n[19] F. Weninger F. Eyben and F. Gross, \u201cRecent developments in opensmile, the munich open-source multimedia feature extractor,\u201d in Proceedings of the 21st ACM international conference on Multimedia. 2013, pp. 835\u2013838, ACM New York, ISBN: 978-1-4503-2404-5 DOI:10.1145/2502081.2502224."}], "references": [{"title": "Survey on speech emotion recognition: features, classification schemes and database", "author": ["M.S. Kamel M. El Ayadi", "F. Karray"], "venue": "Pattern Recognition, vol. 44, no. 3, pp. 572\u2013587, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature selection for classification", "author": ["M. Dash", "H. Liu"], "venue": "Intelligent Data Analysis, vol. 1, no. 3, pp. 131\u2013 156, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of machine learning research, , no. 3, pp. 1157\u20131182, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Eigenfaces vs. fisherfaces: Recognition using class specific linear projection", "author": ["J.P. Hespanha P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 19, no. 7, pp. 711\u2013720, 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Analysis of emotionally salient aspects of fundamental frequency for emotion detection", "author": ["S. Lee C. Busso", "S. Narayanan"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 4, pp. 582\u2013596, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Emotion recognition in human-computer interaction", "author": ["E. Douglas-Cowie R. Cowie", "N. Tsapatsoulis"], "venue": "IEEE Signal Processing Magazine, vol. 18, no. 1, pp. 32\u201380, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Classifier-based learning of nonlinear feature manifold for visualization of emotional speech prosody", "author": ["J. Kortelainen E. Vayrynen", "T. Seppanen"], "venue": "IEEE Transactions on Affective Computing, vol. 4, no. 1, pp. 47\u201356, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification", "author": ["B.S. Atal"], "venue": "the Journal of the Acoustical Society of America, vol. 55, no. 6, pp. 1304\u20131312, 1974.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1974}, {"title": "Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["S. Davis", "P. Mermelstein"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 28, no. 4, pp. 357\u2013366, 1980.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1980}, {"title": "The role of voice quality in communicating emotion, mood and attitude", "author": ["C. Gobl", "A.N. Chasaide"], "venue": "Speech Communication, vol. 40, no. 1-2, pp. 189\u2013212, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Fast and accurate sequential floating forward feature selection with the bayes classifier applied to speech emotion recognition", "author": ["D. Ververidis", "C. Kotropoulos"], "venue": "Signal Processing, vol. 88, no. 12, pp. 2956\u20132970, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Feature subset search using genetic algorithms", "author": ["V. Kadirkamanathan F.J. Ferri", "J. Kittler"], "venue": "in: IEE/IEEE Workshop on Natural Algorithms in Signal Processing, IEE. 1993, Press.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Feature learning in deep neural networks - studies on speech recognition tasks", "author": ["M.L. Seltzer D. Yu", "J. Li"], "venue": "arXiv:1301.3605, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Three dimensions of emotion", "author": ["H. Schlosberg"], "venue": "Psychological Review, vol. 61, no. 2, pp. 81\u201388, 1954.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1954}, {"title": "What\u2019s basic about basic emotions", "author": ["A. Ortony", "T.J. Turner"], "venue": "Psychological Review, vol. 97, no. 3, pp. 315\u2013331, 1990.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1990}, {"title": "Psychological Motivated Multi- Stage Emotion Classification Exploiting Voice Quality Features, Speech Recognition, France", "author": ["M. Lugger", "B. Yang"], "venue": "Mihelic and Janez Zibert (Ed.), InTech,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "A database of german emotional speech", "author": ["A. Paeschke F. Burkhardt", "M. Rolfes"], "venue": "Proceedings Interspeech 2005, 2005, pp. 1517\u20131520.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Recent developments in opensmile, the munich open-source multimedia feature extractor", "author": ["F. Weninger F. Eyben", "F. Gross"], "venue": "Proceedings of the 21st ACM international conference on Multimedia. 2013, pp. 835\u2013838, ACM New York, ISBN: 978-1-4503-2404-5 DOI:10.1145/2502081.2502224.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "As one of the main communication media between human beings, voice has received widespread attention from researchers [1].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "As a common issue for many classification problems [2], feature selection aims to pick a subset of features that are most relevant to the target concept [3] or to reduce the dimension of features for reducing computational time as well as improving the performance [4].", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "As a common issue for many classification problems [2], feature selection aims to pick a subset of features that are most relevant to the target concept [3] or to reduce the dimension of features for reducing computational time as well as improving the performance [4].", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "As a common issue for many classification problems [2], feature selection aims to pick a subset of features that are most relevant to the target concept [3] or to reduce the dimension of features for reducing computational time as well as improving the performance [4].", "startOffset": 265, "endOffset": 268}, {"referenceID": 4, "context": "In [5\u20137], prosody-based acoustic features, including pitchrelated, energy-related and timing features have been widely used for recognizing speech emotion.", "startOffset": 3, "endOffset": 8}, {"referenceID": 5, "context": "In [5\u20137], prosody-based acoustic features, including pitchrelated, energy-related and timing features have been widely used for recognizing speech emotion.", "startOffset": 3, "endOffset": 8}, {"referenceID": 6, "context": "In [5\u20137], prosody-based acoustic features, including pitchrelated, energy-related and timing features have been widely used for recognizing speech emotion.", "startOffset": 3, "endOffset": 8}, {"referenceID": 7, "context": "Spectral-based acoustic features also play an important role in emotion recognition, such as Linear Predictor Coefficients (LPC) [8], Linear Predictor Cepstral Coefficients (LPCC) [9] and Mel-frequency Cepstral Coefficients (MFCC) [10].", "startOffset": 180, "endOffset": 183}, {"referenceID": 8, "context": "Spectral-based acoustic features also play an important role in emotion recognition, such as Linear Predictor Coefficients (LPC) [8], Linear Predictor Cepstral Coefficients (LPCC) [9] and Mel-frequency Cepstral Coefficients (MFCC) [10].", "startOffset": 231, "endOffset": 235}, {"referenceID": 9, "context": "In [11], voice quality features have also been shown to be related to emotions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "For example, Sequential Floating Forward Selection (SFFS) [12] is an iterative method that can find a subset of features near to the optimal one.", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "Some evolutionary algorithms such as Genetic Algorithm (GA) [13] are often used in feature selection.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "Feature space transformation is another type of method, including Principal Component Analysis (PCA) [4] and Neural Network (NN) [14].", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "Feature space transformation is another type of method, including Principal Component Analysis (PCA) [4] and Neural Network (NN) [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "dominance model [15].", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Besides, discrete emotional labels, the so-called archetypal emotions [16], are common used in speech emotion recognition.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "[17] has proposed a hierarchical approach to classify the speech emotions with the dimensional model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In this study, we used the well known Berlin emotional database (EmoDB) [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "OpenSmile toolkit [19] is used to extract acoustic features, and a total of 988 features are obtained.", "startOffset": 18, "endOffset": 22}], "year": 2016, "abstractText": "Feature subspace selection is an important part in speech emotion recognition. Most of the studies are devoted to finding a feature subspace for representing all emotions. However, some studies have indicated that the features associated with different emotions are not exactly the same. Hence, traditional methods may fail to distinguish some of the emotions with just one global feature subspace. In this work, we propose a new divide and conquer idea to solve the problem. First, the feature subspaces are constructed for all the combinations of every two different emotions (emotion-pair). Bi-classifiers are then trained on these feature subspaces respectively. The final emotion recognition result is derived by the voting and competition method. Experimental results demonstrate that the proposed method can get better results than the traditional multi-classification method.", "creator": "LaTeX with hyperref package"}}}