{"id": "1611.06321", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2016", "title": "Learning the Number of Neurons in Deep Networks", "abstract": "parkinson Nowadays, genego the number carpegiani of layers vaez and tajan of neurons eden in each layer 56-40 of a deep network are 69-73 typically set manually. While danspace very cadeby deep indo-parthian and wide co-offensive networks have proven effective in qd7 general, they sturcken come at biopesticide a 76714 high memory and 5-cube computation cost, thus making them 413th impractical agbo for ruas constrained boldenone platforms. These gyulai networks, feldheim however, are known to have shots-22 many mishmar redundant parameters, and could dandie thus, cherusci in workhouse principle, be replaced nonsmoker by kmrl more joerger compact architectures. In piolo this paper, we amil introduce an retired approach 83.67 to automatically determining sdeh the wayfarers number of ill-founded neurons in satoshi each 8-5-1 layer vano of aregawi a deep network during transcend learning. reinhard To 1,735 this end, steenbok we mogilevich propose to make klaassen use smethwick of lws a group sparsity vaduz regularizer on widdle the syncom parameters gr\u00f6nwall of rishawi the pseudonym network, where tally each cornas group russack is blickenstaff defined to act atzerodt on klimont\u00f3w a groveport single neuron. Starting n\u00e1jera from turon an overcomplete network, terusuke we overshadowing show superstition that 3,000-kilometer our weijden approach gald\u00f3s can 27-aug reduce 450s the dahod number 248.5 of parameters leycester by up tizer to ebz 80 \\% while weyand retaining 30-and or incontrovertible even khasi improving teresi the post-independence network accuracy.", "histories": [["v1", "Sat, 19 Nov 2016 07:18:17 GMT  (208kb,D)", "https://arxiv.org/abs/1611.06321v1", "NIPS 2016"], ["v2", "Fri, 13 Jan 2017 05:21:29 GMT  (342kb,D)", "http://arxiv.org/abs/1611.06321v2", "NIPS 2016"]], "COMMENTS": "NIPS 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jose m alvarez", "mathieu salzmann"], "accepted": true, "id": "1611.06321"}, "pdf": {"name": "1611.06321.pdf", "metadata": {"source": "CRF", "title": "Learning the Number of Neurons in Deep Networks", "authors": ["Jose M. Alvarez", "Mathieu Salzmann"], "emails": ["jose.alvarez@data61.csiro.au", "mathieu.salzmann@epfl.ch"], "sections": [{"heading": "1 Introduction", "text": "Thanks to the growing availability of large-scale datasets and computation power, Deep Learning has recently generated a quasi-revolution in many fields, such as Computer Vision and Natural Language Processing. Despite this progress, designing a deep architecture for a new task essentially remains a dark art. It involves defining the number of layers and of neurons in each layer, which, together, determine the number of parameters, or complexity, of the model, and which are typically set manually by trial and error.\nA recent trend to avoid this issue consists of building very deep [Simonyan and Zisserman, 2014] or ultra deep [He et al., 2015] networks, which have proven more expressive. This, however, comes at a significant cost in terms of memory requirement and speed, which may prevent the deployment of such networks on constrained platforms at test time and complicate the learning process due to exploding or vanishing gradients.\nAutomatic model selection has nonetheless been studied in the past, using both constructive and destructive approaches. Starting from a shallow architecture, constructive methods work by incrementally incorporating additional parameters [Bello, 1992] or, more recently, layers to the network [Simonyan and Zisserman, 2014]. The main drawback of this approach stems from the fact that shallow networks are less expressive than deep ones, and may thus provide poor initialization when adding new layers. By contrast, destructive techniques exploit the fact that very deep models include a significant number of redundant parameters [Denil et al., 2013, Cheng et al., 2015], and thus, given an initial deep network, aim at reducing it while keeping its representation power. Originally, this has been achieved by removing the parameters [LeCun et al., 1990, Hassibi et al., 1993] or the neurons [Mozer and Smolensky, 1988, Ji et al., 1990, Reed, 1993] that have little influence on the output. While effective this requires analyzing every parameter/neuron independently, e.g., via the network Hessian, and thus does not scale well to large architectures. Therefore, recent trends\n\u2217http://www.josemalvarez.net.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n06 32\n1v 2\n[ cs\n.C V\n] 1\n3 Ja\nn 20\nto performing network reduction have focused on training shallow or thin networks to mimic the behavior of large, deep ones [Hinton et al., 2014, Romero et al., 2015]. This approach, however, acts as a post-processing step, and thus requires being able to successfully train an initial deep network.\nIn this paper, we introduce an approach to automatically selecting the number of neurons in each layer of a deep architecture simultaneously as we learn the network. Specifically, our method does not require training an initial network as a pre-processing step. Instead, we introduce a group sparsity regularizer on the parameters of the network, where each group is defined to act on the parameters of one neuron. Setting these parameters to zero therefore amounts to canceling the influence of a particular neuron and thus removing it entirely. As a consequence, our approach does not depend on the success of learning a redundant network to later reduce its parameters, but instead jointly learns the number of relevant neurons in each layer and the parameters of these neurons.\nWe demonstrate the effectiveness of our approach on several network architectures and using several image recognition datasets. Our experiments demonstrate that our method can reduce the number of parameters by up to 80% compared to the complete network. Furthermore, this reduction comes at no loss in recognition accuracy; it even typically yields an improvement over the complete network. In short, our approach not only lets us automatically perform model selection, but it also yields networks that, at test time, are more effective, faster and require less memory."}, {"heading": "2 Related work", "text": "Model selection for deep architectures, or more precisely determining the best number of parameters, such as the number of layers and of neurons in each layer, has not yet been widely studied. Currently, this is mostly achieved by manually tuning these hyper-parameters using validation data, or by relying on very deep networks [Simonyan and Zisserman, 2014, He et al., 2015], which have proven effective in many scenarios. These large networks, however, come at the cost of high memory footprint and low speed at test time. Furthermore, it is well-known that most of the parameters in such networks are redundant [Denil et al., 2013, Cheng et al., 2015], and thus more compact architectures could do as good a job as the very deep ones.\nWhile sparse, some literature on model selection for deep learning nonetheless exists. In particular, a forerunner approach was presented in [Ash, 1989] to dynamically add nodes to an existing architecture. Similarly, [Bello, 1992] introduced a constructive method that incrementally grows a network by adding new neurons. More recently, a similar constructive strategy was successfully employed by [Simonyan and Zisserman, 2014], where their final very deep network was built by adding new layers to an initial shallower architecture. The constructive approach, however, has a drawback: Shallow networks are known not to handle non-linearities as effectively as deeper ones [Montufar et al., 2014]. Therefore, the initial, shallow architectures may easily get trapped in bad optima, and thus provide poor initialization for the constructive steps.\nIn contrast with constructive methods, destructive approaches to model selection start with an initial deep network, and aim at reducing it while keeping its behavior unchanged. This trend was started by [LeCun et al., 1990, Hassibi et al., 1993] to cancel out individual parameters, and by [Mozer and Smolensky, 1988, Ji et al., 1990, Reed, 1993], and more recently [Liu et al., 2015], when it comes to removing entire neurons. The core idea of these methods consists of studying the saliency of individual parameters or neurons and remove those that have little influence on the output of the network. Analyzing individual parameters/neurons, however, quickly becomes computationally expensive for large networks, particularly when the procedure involves computing the network Hessian and is repeated multiple times over the learning process. As a consequence, these techniques have no longer been pursued in the current large-scale era. Instead, the more recent take on the destructive approach consists of learning a shallower or thinner network that mimics the behavior of an initial deep one [Hinton et al., 2014, Romero et al., 2015], which ultimately also reduces the number of parameters of the initial network. The main motivation of these works, however, was not truly model selection, but rather building a more compact network.\nAs a matter of fact, designing compact models also is an active research focus in deep learning. In particular, in the context of Convolutional Neural Networks (CNNs), several works have proposed to decompose the filters of a pre-trained network into low-rank filters, thus reducing the number of parameters [Jaderberg et al., 2014b, Denton et al., 2014, Gong et al., 2014]. However, this approach, similarly to some destructive methods mentioned above, acts as a post-processing step, and\nthus requires being able to successfully train an initial deep network. Note that, in a more general context, it has been shown that a two-step procedure is typically outperformed by one-step, direct training [Srivastava et al., 2015]. Such a direct approach has been employed by [Weigend et al., 1991] and [Collins and Kohli, 2014] who have developed regularizers that favor eliminating some of the parameters of the network, thus leading to lower memory requirement. The regularizers are minimized simultaneously as the network is learned, and thus no pre-training is required. However, they act on individual parameters. Therefore, similarly to [Jaderberg et al., 2014b, Denton et al., 2014] and to other parameter regularization techniques [Krogh and Hertz, 1992, Bartlett, 1996], these methods do not perform model selection; the number of layers and neurons per layer is determined manually and won\u2019t be affected by learning.\nBy contrast, in this paper, we introduce an approach to automatically determine the number of neurons in each layer of a deep network. To this end, we design a regularizer-based formulation and therefore do not rely on any pre-training. In other words, our approach performs model selection and produces a compact network in a single, coherent learning framework. To the best of our knowledge, only three works have studied similar group sparsity regularizers for deep networks. However, [Zhou et al., 2016] focuses on the last fully-connected layer to obtain a compact model, and [S. Tu, 2014] and [Murray and Chiang, 2015] only considered small networks. Our approach scales to datasets and architectures two orders of magnitude larger than in these last two works with minimum (and tractable) training overhead. Furthermore, these three methods define a single global regularizer. By contrast, we work in a per-layer fashion, which we found more effective to reduce the number of neurons by large factors without accuracy drop."}, {"heading": "3 Deep Model Selection", "text": "We now introduce our approach to automatically determining the number of neurons in each layer of a deep network while learning the network parameters. To this end, we describe our framework for a general deep network, and discuss specific architectures in the experiments section.\nA general deep network can be described as a succession of L layers performing linear operations on their input, intertwined with non-linearities, such as Rectified Linear Units (ReLU) or sigmoids, and, potentially, pooling operations. Each layer l consists of Nl neurons, each of which is encoded by parameters \u03b8nl = [w n l , b n l ], where w n l is a linear operator acting on the layer\u2019s input and b n l is a bias. Altogether, these parameters form the parameter set \u0398 = {\u03b8l}1\u2264l\u2264L, with \u03b8l = {\u03b8nl }1\u2264n\u2264Nl . Given an input signal x, such as an image, the output of the network can be written as y\u0302 = f(x,\u0398), where f(\u00b7) encodes the succession of linear, non-linear and pooling operations. Given a training set consisting of N input-output pairs {(xi, yi)}1\u2264i\u2264N , learning the parameters of the network can be expressed as solving an optimization of the form\nmin \u0398\n1\nN N\u2211 i=1 `(yi, f(xi,\u0398)) + r(\u0398) , (1)\nwhere `(\u00b7) is a loss that compares the network prediction with the ground-truth output, such as the logistic loss for classification or the square loss for regression, and r(\u00b7) is a regularizer acting on the network parameters. Popular choices for such a regularizer include weight-decay, i.e., r(\u00b7) is the (squared) `2-norm, of sparsity-inducing norms, e.g., the `1-norm.\nRecall that our goal here is to automatically determine the number of neurons in each layer of the network. We propose to do this by starting from an overcomplete network and canceling the influence of some of its neurons. Note that none of the standard regularizers mentioned above achieve this goal: The former favors small parameter values, and the latter tends to cancel out individual parameters, but not complete neurons. In fact, a neuron is encoded by a group of parameters, and our goal therefore translates to making entire groups go to zero. To achieve this, we make use of the notion of group sparsity [Yuan and Lin., 2007]. In particular, we write our regularizer as\nr(\u0398) = L\u2211 l=1 \u03bbl \u221a Pl Nl\u2211 n=1 \u2016\u03b8nl \u20162 , (2)\nwhere, without loss of generality, we assume that the parameters of each neuron in layer l are grouped in a vector of size Pl, and where \u03bbl sets the influence of the penalty. Note that, in the general case,\nthis weight can be different for each layer l. In practice, however, we found most effective to have two different weights: a relatively small one for the first few layers, and a larger weight for the remaining ones. This effectively prevents killing too many neurons in the first few layers, and thus retains enough information for the remaining ones.\nWhile group sparsity lets us effectively remove some of the neurons, exploiting standard regularizers on the individual parameters has proven effective in the past for generalization purpose [Bartlett, 1996, Krogh and Hertz, 1992, Theodoridis, 2015, Collins and Kohli, 2014]. To further leverage this idea within our automatic model selection approach, we propose to exploit the sparse group Lasso idea of [Simon et al., 2013]. This lets us write our regularizer as\nr(\u0398) = L\u2211 l=1\n( (1\u2212 \u03b1)\u03bbl \u221a Pl\nNl\u2211 n=1 \u2016\u03b8nl \u20162 + \u03b1\u03bbl\u2016\u03b8l\u20161\n) , (3)\nwhere \u03b1 \u2208 [0, 1] sets the relative influence of both terms. Note that \u03b1 = 0 brings us back to the regularizer of Eq. 2. In practice, we experimented with both \u03b1 = 0 and \u03b1 = 0.5.\nTo solve Problem (1) with the regularizer defined by either Eq. 2 or Eq. 3, we follow a proximal gradient descent approach [Parikh and Boyd, 2014]. In our context, proximal gradient descent can be thought of as iteratively taking a gradient step of size t with respect to the loss \u2211N i=1 `(yi, f(xi,\u0398)) only, and, from the resulting solution, applying the proximal operator of the regularizer. In our case, since the groups are non-overlapping, we can apply the proximal operator to each group independently. Specifically, for a single group, this translates to updating the parameters as\n\u03b8\u0303nl = argmin \u03b8nl\n1 2t \u2016\u03b8nl \u2212 \u03b8\u0302nl \u201622 + r(\u0398) , (4)\nwhere \u03b8\u0302nl is the solution obtained from the loss-based gradient step. Following the derivations of [Simon et al., 2013], and focusing on the regularizer of Eq. 3 of which Eq. 2 is a special case, this problem has a closed-form solution given by\n\u03b8\u0303nl =\n( 1\u2212 t(1\u2212 \u03b1)\u03bbl \u221a Pl\n||S(\u03b8\u0302nl , t\u03b1\u03bbl)||2) ) + S(\u03b8\u0302nl , t\u03b1\u03bbl) , (5)\nwhere + corresponds to taking the maximum between the argument and 0, and S(\u00b7) is the softthresholding operator defined elementwise as\n(S(z, \u03c4))j = sign(zj)(|zj | \u2212 \u03c4)+ . (6)\nThe learning algorithm therefore proceeds by iteratively taking a gradient step based on the loss only, and updating the variables of all the groups according to Eq. 5. In practice, we follow a stochastic gradient descent approach and work with mini-batches. In this setting, we apply the proximal operator at the end of each epoch and run the algorithm for a fixed number of epochs.\nWhen learning terminates, the parameters of some of the neurons will have gone to zero. We can thus remove these neurons entirely, since they have no effect on the output. Furthermore, when considering fully-connected layers, the neurons acting on the output of zeroed-out neurons of the previous layer also become useless, and can thus be removed. Ultimately, removing all these neurons yields a more compact architecture than the original, overcomplete one."}, {"heading": "4 Experiments", "text": "In this section, we demonstrate the ability of our method to automatically determine the number of neurons on the task of large-scale classification. To this end, we study three different architectures and analyze the behavior of our method on three different datasets, with a particular focus on parameter reduction. Below, we first describe our experimental setup and then discuss our results."}, {"heading": "4.1 Experimental setup", "text": "Datasets: For our experiments, we used two large-scale image classification datasets, ImageNet [Russakovsky et al., 2015] and Places2-401 [Zhou et al., 2015]. Furthermore, we conducted\nadditional experiments on the character recognition dataset of [Jaderberg et al., 2014a]. ImageNet contains over 15 million labeled images split into 22, 000 categories. We used the ILSVRC-2012 [Russakovsky et al., 2015] subset consisting of 1000 categories, with 1.2 million training images and 50, 000 validation images. Places2-401 [Zhou et al., 2015] is a large-scale dataset specifically created for high-level visual understanding tasks. It consists of more than 10 million images with 401 unique scene categories. The training set comprises between 5,000 and 30,000 images per category. Finally, the ICDAR character recognition dataset of [Jaderberg et al., 2014a] consists of 185,639 training and 5,198 test samples split into 36 categories. The training samples depict characters collected from text observed in a number of scenes and from synthesized datasets, while the test set comes from the ICDAR2003 training set after removing all non-alphanumeric characters.\nArchitectures: For ImageNet and Places2-401, our architectures are based on the VGG-B network (BNet) [Simonyan and Zisserman, 2014] and on DecomposeMe8 (Dec8) [Alvarez and Petersson, 2016]. BNet consists of 10 convolutional layers followed by three fully-connected layers. In our experiments, we removed the first two fully-connected layers. As will be shown in our results, while this reduces the number of parameters, it maintains the accuracy of the original network. Below, we refer to this modified architecture as BNetC . Following the idea of low-rank filters, Dec8 consists of 16 convolutional layers with 1D kernels, effectively modeling 8 2D convolutional layers. For ICDAR, we used an architecture similar to the one of [Jaderberg et al., 2014b]. The original architecture consists of three convolutional layers with a maxout layer [Goodfellow et al., 2013] after each convolution, followed by one fully-connected layer. [Jaderberg et al., 2014b] first trained this network and then decomposed each 2D convolution into 2 1D kernels. Here, instead, we directly start with 6 1D convolutional layers. Furthermore, we replaced the maxout layers with max-pooling. As shown below, this architecture, referred to as Dec3, yields similar results as the original one, referred to as MaxOut.\nImplementation details: For the comparison to be fair, all models including the baselines were trained from scratch on the same computer using the same random seed and the same framework. More specifically, for ImageNet and Places2-401, we used the torch-7 multi-gpu framework [Collobert et al., 2011] on a Dual Xeon 8-core E5-2650 with 128GB of RAM using three Kepler Tesla K20m GPUs in parallel. All models were trained for a total of 55 epochs with 12, 000 batches per epoch and a batch size of 48 and 180 for BNet and Dec8, respectively. These variations in batch size were mainly due to the memory and runtime limitations of BNet. The learning rate was set to an initial value of 0.01 and then multiplied by 0.1. Data augmentation was done through random crops and random horizontal flips with probability 0.5. For ICDAR, we trained each network on a single Tesla K20m GPU for a total 45 epochs with a batch size of 256 and 1,000 iterations per epoch. In this case, the learning rate was set to an initial value of 0.1 and multiplied by 0.1 in the second, seventh and fifteenth epochs. We used a momentum of 0.9. In terms of hyper-parameters, for large-scale classification, we used \u03bbl = 0.102 for the first three layers and \u03bbl = 0.255 for the remaining ones. For ICDAR, we used \u03bbl = 5.1 for the first layer and \u03bbl = 10.2 for the remaining ones.\nEvaluation: We measure classification performance as the top-1 accuracy using the center crop, referred to as Top-1. We compare the results of our approach with those obtained by training the same architectures, but without our model selection technique. We also provide the results of additional, standard architectures. Furthermore, since our approach can determine the number of neurons per layer, we also computed results with our method starting for different number of neurons, referred to as M below, in the overcomplete network. In addition to accuracy, we also report, for the convolutional layers, the percentage of neurons set to 0 by our approach (neurons), the corresponding percentage of zero-valued parameters (group param), the total percentage of 0 parameters (total param), which additionally includes the parameters set to 0 in non-completely zeroed-out neurons, and the total percentage of zero-valued parameters induced by the zeroed-out neurons (total induced), which additionally includes the neurons in each layer, including the last fully-connected layer, that have been rendered useless by the zeroed-out neurons of the previous layer."}, {"heading": "4.2 Results", "text": "Below, we report our results on ImageNet and ICDAR. The results on Places-2 are provided as supplementary material.\nImageNet: We first start by discussing our results on ImageNet. For this experiment, we used BNetC and Dec8, both with the group sparsity (GS) regularizer of Eq. 2. Furthermore, in the case of\na Trained over 55 epochs using a batch size of 128 on two TitanX with code publicly available.\nDec8, we evaluated two additional versions that, instead of the 512 neurons per layer of the original architecture, have M = 640 and M = 768 neurons per layer, respectively. Finally, in the case of M = 640, we further evaluated both the group sparsity regularizer of Eq. 2 and the sparse group Lasso (SGL) regularizer of Eq. 3 with \u03b1 = 0.5. Table 1 compares the top-1 accuracy of our approach with that of the original architectures and of other baselines. Note that, with the exception of Dec8-768, all our methods yield an improvement over the original network, with up to 1.6% difference for BNetC and 2.45% for Dec8-640. As an additional baseline, we also evaluated the naive approach consisting of reducing each layer in the model by a constant factor of 25%. The corresponding two instances, Dec25%8 and Dec 25% 8 -640, yield 64.5% and 65.8% accuracy, respectively.\nMore importantly, in Figure 1 and Figure 2, we report the relative saving obtained with our approach in terms of percentage of zeroed-out neurons/parameters for BNetC and Dec8, respectively. For BNetC , in Figure 1, our approach reduces the number of neurons by over 12%, while improving its generalization ability, as indicated by the accuracy gap in the bottom row of the table. As can be seen from the bar-plot, the reduction in the number of neurons is spread all over the layers with the largest difference in the last layer. As a direct consequence, the number of neurons in the subsequent fully connected layer is significantly reduced, leading to 27% reduction in the total number of parameters. For Dec8, in Figure 2, we can see that, when considering the original architecture with 512 neurons per layer, our approach only yields a small reduction in parameter numbers with minimal gain in performance. However, when we increase the initial number of neurons in each layer, the benefits of our approach become more significant. For M = 640, when using the group sparsity regularizer, we see a reduction of the number of parameters of more than 19%, with improved generalization ability. The reduction is even larger, 23%, when using the sparse group Lasso regularizer. In the case of M = 768, we managed to remove 26% of the neurons, which translates to 48% of the parameters. While, here, the accuracy is slightly lower than that of the initial network, it is in fact higher than that of the original Dec8 network, as can be seen in Table 1.\nInterestingly, during learning, we also noticed a significant reduction in the training-validation accuracy gap when applying our regularization technique. For instance, for Dec8-768, which zeroes out 48.2% of the parameters, we found the training-validation gap to be 28.5% smaller than in the original network (from 14% to 10%). We believe that this indicates that networks trained using our approach have better generalization ability, even if they have fewer parameters. A similar phenomenon was also observed for the other architectures used in our experiments.\nWe now analyze the sensitivity of our method with respect to \u03bbl (see Eq. (2)). To this end, we considered Dec8 \u2212 768GS and varied the value of the parameter in the range \u03bbl = [0.051..0.51]. More specifically, we considered 20 different pairs of values, (\u03bb1, \u03bb2), with the former applied to the\nfirst three layers and the latter to the remaining ones. The details of this experiment are reported in supplementary material. Altogether, we only observed small variations in validation accuracy (std of 0.33%) and in number of zeroed-out neurons (std of 1.1%). ICDAR: Finally, we evaluate our approach on a smaller dataset where architectures have not yet been heavily tuned. For this dataset, we used the Dec3 architecture, where the last two layers initially contain 512 neurons. Our goal here is to obtain an optimal architecture for this dataset. Figure 3 summarizes our results using GS and SGL regularization and compares them to state-of-the-art baselines. From the comparison between MaxPool2Dneurons and Dec3, we can see that learning 1D filters leads to better performance than an equivalent network with 2D kernels. More importantly, our algorithm reduces by up to 80% the number of parameters, while further improving the performance of the original network. We believe that these results evidence that our algorithm effectively performs automatic model selection for a given (classification) task."}, {"heading": "4.3 Benefits at test time", "text": "We now discuss the benefits of our algorithm at test time. For simplicity, our implementation does not remove neurons during training. However, these neurons can be effectively removed after training, thus yielding a smaller network to deploy at test time. Not only does this entail benefits in terms of memory requirement, as illustrated above when looking at the reduction in number of parameters, but it also leads to speedups compared to the complete network. To demonstrate this, in Table 2, we report the relative runtime speedups obtained by removing the zeroed-out neurons. For BNet and Dec8, these speedups were obtained using ImageNet, while Dec3 was tested on ICDAR. Note that significant speedups can be achieved, depending on the architecture. For instance, using BNetC , we achieve a speedup of up to 13% on ImageNet, while with Dec3 on ICDAR the speedup reaches almost 50%. The right-hand side of Table 2 shows the relative memory saving of our networks. These numbers were computed from the actual memory requirements in MB of the networks. In terms of parameters, for ImageNet, Dec8-768 yields a 46% reduction, while Dec3 increases this saving to more than 80%. When looking at the actual features computed in each layer of the network, we reach a 10% memory saving for Dec8-768 and a 25% saving for Dec3. We believe that these numbers clearly evidence the benefits of our approach in terms of speed and memory footprint at test time.\nNote also that, once the models are trained, additional parameters can be pruned using, at the level of individual parameters, `1 regularization and a threshold [Liu et al., 2015]. On ImageNet, with our\nDec8-768GS model and the `1 weight set to 0.0001 as in [Liu et al., 2015], this method yields 1.34M zero-valued parameters, compared to 7.74M for our approach, i.e., a 82% relative reduction in the number of individual parameters for our approach."}, {"heading": "5 Conclusions", "text": "We have introduced an approach to automatically determining the number of neurons in each layer of a deep network. To this end, we have proposed to rely on a group sparsity regularizer, which has allowed us to jointly learn the number of neurons and the parameter values in a single, coherent framework. Not only does our approach estimate the number of neurons, it also yields a more compact architecture than the initial overcomplete network, thus saving both memory and computation at test time. Our experiments have demonstrated the benefits of our method, as well as its generalizability to different architectures. One current limitation of our approach is that the number of layers in the network remains fixed. To address this, in the future, we intend to study architectures where each layer can potentially be bypassed entirely, thus ultimately canceling out its influence. Furthermore, we plan to evaluate the behavior of our approach on other types of problems, such as regression networks and autoencoders."}, {"heading": "Acknowledgments", "text": "The authors thank John Taylor and Tim Ho for helpful discussions and their continuous support through using the CSIRO high-performance computing facilities. The authors also thank NVIDIA for generous hardware donations."}], "references": [{"title": "Decomposeme: Simplifying convnets for end-to-end learning", "author": ["J.M. Alvarez", "L. Petersson"], "venue": null, "citeRegEx": "Alvarez and Petersson.,? \\Q2016\\E", "shortCiteRegEx": "Alvarez and Petersson.", "year": 2016}, {"title": "Dynamic node creation in backpropagation networks", "author": ["T. Ash"], "venue": "Connection Science,", "citeRegEx": "Ash.,? \\Q1989\\E", "shortCiteRegEx": "Ash.", "year": 1989}, {"title": "For valid generalization the size of the weights is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "In NIPS,", "citeRegEx": "Bartlett.,? \\Q1996\\E", "shortCiteRegEx": "Bartlett.", "year": 1996}, {"title": "Enhanced training algorithms, and integrated training/architecture selection for multilayer perceptron networks", "author": ["M.G. Bello"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bello.,? \\Q1992\\E", "shortCiteRegEx": "Bello.", "year": 1992}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Cheng", "Felix X. Yu", "Rog\u00e9rio Schmidt Feris", "Sanjiv Kumar", "Alok N. Choudhary", "Shih-Fu Chang"], "venue": "In ICCV,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M.A. Ranzato", "N. de Freitas"], "venue": "CoRR, abs/1306.0543,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E. L Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "In NIPS", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L.D. Bourdev"], "venue": "In CoRR, volume abs/1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Optimal brain surgeon and general network pruning", "author": ["B. Hassibi", "D.G. Stork", "G.J. Wolff"], "venue": "In ICNN,", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In CoRR, volume abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["G.E. Hinton", "O. Vinyals", "J. Dean"], "venue": "In arXiv,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Deep features for text spotting", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "In ECCV,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "In BMVC,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Generalizing smoothness constraints from discrete samples", "author": ["C. Ji", "R.R. Snapp", "D. Psaltis"], "venue": "Neural Computation,", "citeRegEx": "Ji et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Ji et al\\.", "year": 1990}, {"title": "A simple weight decay can improve generalization", "author": ["A. Krogh", "J.A. Hertz"], "venue": "In NIPS,", "citeRegEx": "Krogh and Hertz.,? \\Q1992\\E", "shortCiteRegEx": "Krogh and Hertz.", "year": 1992}, {"title": "Optimal brain damage", "author": ["Y. LeCun", "J.S. Denker", "S.A. Solla"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Sparse convolutional neural networks", "author": ["B. Liu", "M. Wang", "H. Foroosh", "M. Tappen", "M. Penksy"], "venue": "In CVPR,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "On the number of linear regions of deep neural networks", "author": ["G. F Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "In NIPS", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Skeletonization: A technique for trimming the fat from a network via relevance assessment", "author": ["M. Mozer", "P. Smolensky"], "venue": "In NIPS,", "citeRegEx": "Mozer and Smolensky.,? \\Q1988\\E", "shortCiteRegEx": "Mozer and Smolensky.", "year": 1988}, {"title": "Auto-sizing neural networks: With applications to n-gram language models", "author": ["K. Murray", "D. Chiang"], "venue": "CoRR, abs/1508.05051,", "citeRegEx": "Murray and Chiang.,? \\Q2015\\E", "shortCiteRegEx": "Murray and Chiang.", "year": 2015}, {"title": "Proximal algorithms. Found", "author": ["N. Parikh", "S. Boyd"], "venue": "Trends Optim.,", "citeRegEx": "Parikh and Boyd.,? \\Q2014\\E", "shortCiteRegEx": "Parikh and Boyd.", "year": 2014}, {"title": "Pruning algorithms-a survey", "author": ["R. Reed"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Reed.,? \\Q1993\\E", "shortCiteRegEx": "Reed.", "year": 1993}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Romero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2015}, {"title": "Learning block group sparse representation combined with convolutional neural networks for rgb-d object recognition", "author": ["J. Wang X. Huang X. Zhang S. Tu", "Y. Xue"], "venue": "Journal of Fiber Bioengineering and Informatics,", "citeRegEx": "Tu and Xue.,? \\Q2014\\E", "shortCiteRegEx": "Tu and Xue.", "year": 2014}, {"title": "A sparse-group lasso", "author": ["N. Simon", "J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Simon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simon et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Training very deep networks", "author": ["R. K Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "In NIPS,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Machine Learning, A Bayesian and Optimization Perspective, volume", "author": ["S. Theodoridis"], "venue": null, "citeRegEx": "Theodoridis.,? \\Q2015\\E", "shortCiteRegEx": "Theodoridis.", "year": 2015}, {"title": "Generalization by weight-elimination with application to forecasting", "author": ["A.S. Weigend", "D. Rumelhart", "B.A. Huberman"], "venue": "In NIPS,", "citeRegEx": "Weigend et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Weigend et al\\.", "year": 1991}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Yuan and Lin.,? \\Q2007\\E", "shortCiteRegEx": "Yuan and Lin.", "year": 2007}, {"title": "Places: An image database for deep scene understanding", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Torralba", "A. Oliva"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Less is more: Towards compact CNNs", "author": ["H. Zhou", "J.M. Alvarez", "F. Porikli"], "venue": "In ECCV,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 26, "context": "A recent trend to avoid this issue consists of building very deep [Simonyan and Zisserman, 2014] or ultra deep [He et al.", "startOffset": 66, "endOffset": 96}, {"referenceID": 10, "context": "A recent trend to avoid this issue consists of building very deep [Simonyan and Zisserman, 2014] or ultra deep [He et al., 2015] networks, which have proven more expressive.", "startOffset": 111, "endOffset": 128}, {"referenceID": 3, "context": "Starting from a shallow architecture, constructive methods work by incrementally incorporating additional parameters [Bello, 1992] or, more recently, layers to the network [Simonyan and Zisserman, 2014].", "startOffset": 117, "endOffset": 130}, {"referenceID": 26, "context": "Starting from a shallow architecture, constructive methods work by incrementally incorporating additional parameters [Bello, 1992] or, more recently, layers to the network [Simonyan and Zisserman, 2014].", "startOffset": 172, "endOffset": 202}, {"referenceID": 1, "context": "In particular, a forerunner approach was presented in [Ash, 1989] to dynamically add nodes to an existing architecture.", "startOffset": 54, "endOffset": 65}, {"referenceID": 3, "context": "Similarly, [Bello, 1992] introduced a constructive method that incrementally grows a network by adding new neurons.", "startOffset": 11, "endOffset": 24}, {"referenceID": 26, "context": "More recently, a similar constructive strategy was successfully employed by [Simonyan and Zisserman, 2014], where their final very deep network was built by adding new layers to an initial shallower architecture.", "startOffset": 76, "endOffset": 106}, {"referenceID": 18, "context": "The constructive approach, however, has a drawback: Shallow networks are known not to handle non-linearities as effectively as deeper ones [Montufar et al., 2014].", "startOffset": 139, "endOffset": 162}, {"referenceID": 17, "context": ", 1990, Reed, 1993], and more recently [Liu et al., 2015], when it comes to removing entire neurons.", "startOffset": 39, "endOffset": 57}, {"referenceID": 27, "context": "Note that, in a more general context, it has been shown that a two-step procedure is typically outperformed by one-step, direct training [Srivastava et al., 2015].", "startOffset": 137, "endOffset": 162}, {"referenceID": 29, "context": "Such a direct approach has been employed by [Weigend et al., 1991] and [Collins and Kohli, 2014] who have developed regularizers that favor eliminating some of the parameters of the network, thus leading to lower memory requirement.", "startOffset": 44, "endOffset": 66}, {"referenceID": 32, "context": "However, [Zhou et al., 2016] focuses on the last fully-connected layer to obtain a compact model, and [S.", "startOffset": 9, "endOffset": 28}, {"referenceID": 20, "context": "Tu, 2014] and [Murray and Chiang, 2015] only considered small networks.", "startOffset": 14, "endOffset": 39}, {"referenceID": 30, "context": "To achieve this, we make use of the notion of group sparsity [Yuan and Lin., 2007].", "startOffset": 61, "endOffset": 82}, {"referenceID": 25, "context": "To further leverage this idea within our automatic model selection approach, we propose to exploit the sparse group Lasso idea of [Simon et al., 2013].", "startOffset": 130, "endOffset": 150}, {"referenceID": 21, "context": "3, we follow a proximal gradient descent approach [Parikh and Boyd, 2014].", "startOffset": 50, "endOffset": 73}, {"referenceID": 25, "context": "Following the derivations of [Simon et al., 2013], and focusing on the regularizer of Eq.", "startOffset": 29, "endOffset": 49}, {"referenceID": 31, "context": ", 2015] and Places2-401 [Zhou et al., 2015].", "startOffset": 24, "endOffset": 43}, {"referenceID": 31, "context": "Places2-401 [Zhou et al., 2015] is a large-scale dataset specifically created for high-level visual understanding tasks.", "startOffset": 12, "endOffset": 31}, {"referenceID": 26, "context": "Architectures: For ImageNet and Places2-401, our architectures are based on the VGG-B network (BNet) [Simonyan and Zisserman, 2014] and on DecomposeMe8 (Dec8) [Alvarez and Petersson, 2016].", "startOffset": 101, "endOffset": 131}, {"referenceID": 0, "context": "Architectures: For ImageNet and Places2-401, our architectures are based on the VGG-B network (BNet) [Simonyan and Zisserman, 2014] and on DecomposeMe8 (Dec8) [Alvarez and Petersson, 2016].", "startOffset": 159, "endOffset": 188}, {"referenceID": 5, "context": "More specifically, for ImageNet and Places2-401, we used the torch-7 multi-gpu framework [Collobert et al., 2011] on a Dual Xeon 8-core E5-2650 with 128GB of RAM using three Kepler Tesla K20m GPUs in parallel.", "startOffset": 89, "endOffset": 113}, {"referenceID": 10, "context": "1 ResNet50a [He et al., 2015] 67.", "startOffset": 12, "endOffset": 29}, {"referenceID": 12, "context": "1% a Results from Jaderberg et al. [2014a] using MaxOut layer instead of MaxPooling and decompositions as post-processing step b Results from Jaderberg et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 12, "context": "1% a Results from Jaderberg et al. [2014a] using MaxOut layer instead of MaxPooling and decompositions as post-processing step b Results from Jaderberg et al. [2014a] Figure 3: Experimental results on ICDAR using Dec3.", "startOffset": 18, "endOffset": 167}, {"referenceID": 17, "context": "Note also that, once the models are trained, additional parameters can be pruned using, at the level of individual parameters, `1 regularization and a threshold [Liu et al., 2015].", "startOffset": 161, "endOffset": 179}, {"referenceID": 17, "context": "0001 as in [Liu et al., 2015], this method yields 1.", "startOffset": 11, "endOffset": 29}], "year": 2017, "abstractText": "Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80% while retaining or even improving the network accuracy.", "creator": "LaTeX with hyperref package"}}}