{"id": "1611.06453", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Fast Video Classification via Adaptive Cascading of Deep Models", "abstract": "medpac Recent vilify advances have enabled \" oracle \" classifiers pirithous that can shahanshah classify growly across inzamam-ul-haq many polsat classes widowhood and palicki input distributions with high worldview accuracy without retraining. 121.13 However, these classifiers are clubgoers relatively phosphorylate heavyweight, lill so 33-0 that applying healthcare.gov them karlovich to classify 67-65 video is bjelasnica costly. macguffin We quetzaltenango show that kairys day - to - santol day phongthep video nikica exhibits pof highly astonishingly skewed gottlieb class distributions over the short term, encina and matsunaga that paekdu these distributions swooping can be saint-nicolas classified saltine by duro much veteris simpler models. carrom We 1973-1978 formulate the problem lader of detecting nehr the short - term skews diablerets online stresemann and sentrachem exploiting models based on saracoglu it 40,000-strong as bonna a new sequential eppinger decision geingob making krassimir problem sender dubbed utley the Online trapasso Bandit pm2 Problem, apanage and redlich present hazaaron a clausura new alkanes algorithm jheri to socioeconomics solve bolts it. When 26-kilometer applied redskins to arminianism recognizing faces in schulz TV jurgensen shows rysselberghe and science-fictional movies, we realize end - to - cookeville end classification taks speedups of airside 2. tabatha 5 - sainovic 8. 5x / ferri\u00e8res 2. calligrapher 8 - vaitupu 12. 7x (on GPU / 10.80 CPU) 13.19 relative goiskoye to starchy a -8:30 state - 93.4 of - the - deconstructed art marinova convolutional melanesians neural network, altidore at competitive accuracy.", "histories": [["v1", "Sun, 20 Nov 2016 00:21:32 GMT  (4052kb,D)", "http://arxiv.org/abs/1611.06453v1", null], ["v2", "Sun, 2 Jul 2017 02:17:00 GMT  (4068kb,D)", "http://arxiv.org/abs/1611.06453v2", "Accepted at IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["haichen shen", "seungyeop han", "matthai philipose", "arvind krishnamurthy"], "accepted": false, "id": "1611.06453"}, "pdf": {"name": "1611.06453.pdf", "metadata": {"source": "CRF", "title": "Fast Video Classification via Adaptive Cascading of Deep Models", "authors": ["Haichen Shen", "Seungyeop Han", "Matthai Philipose", "Arvind Krishnamurthy"], "emails": ["arvind}@cs.washington.edu,", "seungyeop.han@rubrik.com,", "matthaip@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "Consider recognizing entities such as objects, people, scenes and activities in every frame of video footage of day-to-day life. Such footage may come, for instance, from the media, wearable cameras, movies, or surveillance cameras. In principle, these entities could be drawn from thousands of classes: many of us encounter hundreds to thousands of distinct people, objects, scenes and activities through our life. Over short intervals such as minutes, however, we tend to encounter a very small subset of classes of entities. For instance, a wearable camera may see the same set of objects from our desk at work for an hour, a movie may focus only on cooking-related activities through a five-minute kitchen sequence, and media footage of an event may focus on only those celebrities participating in the event. In this paper, we characterize and exploit such short-term class skew to significantly reduce the latency of classifying video using Convolutional Neural Networks (CNNs).\nSince the seminal work of Viola and Jones [23] on face detection, one of the best-known techniques to speed\nup classification has been to structure the classifier as a cascade (or tree [26]) of simple classifiers such that \u201ceasy\u201d examples lead to early exits and are therefore classified faster. Cascaded classifiers require that training and test data are strongly (and identically) biased toward a small number of easy to detect classes. In the (binary) face detection task, for example, the class \u201cnot a face\u201d is both (i) by far more common than \u201cface\u201d, and (ii) often quite easy to classify via a small number of comparisons of inexpensive Haar-style features. In fact, traditional cascades are most applicable in detection tasks [4], where the background is both much more common and easier to classify than the foreground.\nDistinct from the (two-class) detection setting in traditional cascading, recent advances in convolutional neural networks (CNNs) [16,21,29] have opened up the possibility of using a single, pre-trained \u201coracle\u201d classifier to recognize thousands of classes such as people, objects and scenes. When training such oracle classifiers, such as GoogLeNet [21] of VGGFace [16]), a small number of classes do not usually dominate the training set: for broad applicability, the classifier is trained assuming that all classes are more or less equally likely. Even if such a skew toward such small classes existed, there is no a priori reason that these dominant classes are fast to classify. It may seem therefore that cascading is not a promising optimization for improving the speed of entity recognition in video via CNNs.\nWe demonstrate, however, that for many recognition tasks, day-to-day video often exhibits significant shortterm skews in class distribution. We present measurements on a diverse set of videos that show, for instance, that in over 90% of 1-minute windows, at least 90% of objects interacted with by humans belong to a set of 25 or fewer objects. The underlying ImageNet-based recognizer, on the other hand, can recognize up to 1000 objects. We show that similar skews hold for faces and scenes in videos.\nEven if such skew exists, to our knowledge, it has not been shown that distributions skewed toward small sets of classes can be classified accurately by simpler\n1\nar X\niv :1\n61 1.\n06 45\n3v 1\n[ cs\n.C V\n] 2\n0 N\nov 2\n01 6\nCNNs than uniformly distributed ones. We therefore also demonstrate that when class distribution is highly skewed, \u201cspecialized\u201d CNNs trained to classify inputs from this distribution can be much more compact than the oracle classifier. For instance, we present a CNN that executes 60\u00d7 fewer FLOPs than the state-of-the art VGGFace [16] model, but has comparable accuracy when over 50% of faces come from the same 10 or fewer people. We present similar order-of-magnitude faster specialized CNNs for object and scene recognition.\nGiven the ability to produce fast, accurate versions of CNNs specialized for particular test-time skews, we seek to estimate the (possibly non-stationary) skew at testtime, produce a specialized model if appropriate, exploit the model as long as the skew lasts, detect when the skew disappears and then revert to the oracle model. As with standard \u201cbandit\u201d-style sequential decision-making problems, the challenge is in balancing exploration (i.e., using the expensive oracle to estimate the skew) with exploitation (i.e., using a model specialized to the current best available estimate of the skew). We formalize this problem as the Oracle Bandit Problem and propose a new exploration/exploitation-based algorithm we dub Windowed -Greedy (WEG) to address it.\nUsing a combination of synthetic data and real-world videos, we empirically validate the WEG algorithm. In particular, we show that WEG can reduce the end-toend classification overhead of face recognition on TV episodes and movies by 2.5-8.5\u00d7 relative to unspecialized classification using the VGGFace classifier on a GPU (2.8-12.7\u00d7 on a CPU). We show via synthetic data that similar gains are to be had on object and scene recognition as well. We provide a detailed analysis of WEG\u2019s functioning, including an accounting of how much its key features contribute. To our knowledge our system is the first to use test-time sequential class skews in video to produce faster classifiers."}, {"heading": "2. Related work", "text": "There is a long line of work on cost-sensitive classification, the epitome of which is perhaps the cascaded classification work of Viola and Jones [23]. The essence of this line of work [25, 27] is to treat classification as a sequential process that may exit early if it is confident in its inference, typically by learning sequences that have low cost in expectation over training data. Recent work [15] has even proposed cascading CNNs as we do. All these techniques assume that testing data is i.i.d. (i.e., not sequential), that all training happens before any testing, and rely on skews in training data to capture cost structure. As such, they are not equipped to exploit short-term class skews in test data.\nTraditional sequential models such as probabilis-\ntic models [3, 17, 24] and Recurrent Neural Networks (RNNs) [5,11] are aimed at classifying instances that are not independent of each other. Given labeled sequences as training data, these techniques learn more accurate classifiers than those that treat sequence elements as independent. However, to our knowledge, none of these approaches produces classifiers that yield less expensive classification in response to favorable inputs, as we do.\nSimilar to adaptive cascading, online learning methods [9, 12, 22] customize models at test time. For training, they use labeled data from a sequential stream that typically contains both labeled and unlabeled data. As with adaptive cascading, the test-time cost of incrementally training the model in these systems needs to be low. A fundamental difference in our work is that we make no assumption that our input stream is partly labeled. Instead, we assume the availability of a large, resource-hungry model that we seek to \u201ccompress\u201d into a resource-light cascade stage.\nEstimating distributions in sequential data and exploiting it is the focus of the multi-armed bandit (MAB) community [1, 13]. The Oracle Bandit Problem (OBP) we define differs from the classic MAB setting in that in MAB the set of arms over which exploration and exploitation happen are the same, whereas in OBP only the oracle \u201carm\u201d allows exploration whereas specialized models allow exploitation. Capturing the connection between these arms is the heart of the OBP formulation. Our Windowed -Greedy algorithm is strongly informed by the use of windows in [6] to handle non-stationarities and the well-known [20] -greedy scheme to balance exploration and exploitation.\nFinally, much recent work has focused on reducing the resource consumption of (convolutional) neural networks [2, 7, 8, 18]. These techniques are oblivious to test-time data skew and are complementary to specialization. We expect that even more pared-down versions of these optimized models will provide good accuracy when specialized at test-time."}, {"heading": "3. Class skew in day-to-day video", "text": "Specialization depends on skew (or bias) in the temporal distribution of classes presented to the classifier. In this section, we analyze the skew in videos of day-to-day life culled from YouTube. We assembled a set of 30 videos of length 3 minutes to 20 minutes from five classes of daily activities: socializing, home repair, biking around urban areas, cooking, and home tours. We expect this kind of footage to come from a variety of sources such as movies, amateur productions of the kind that dominate YouTube and wearable videos.\nWe sample one in three frames uniformly from these videos and apply state-of-the-art face (derived from\n[16]), scene [29] and object recognizers [19] to every sampled frame. Note that these \u201coracle\u201d recognizers can recognize up to 2622 faces, 205 scenes and 1000 objects respectively. For face recognition, we record the top-scoring label for each face detected, and for the others, we record only the top-scoring class on each frame. For object recognition in particular, this substantially undercounts objects in the scene; our count (and specialization) applies to applications that identify some distinctive subset of objects (e.g., all objects \u201chandled\u201d by a person). We seek to compare these numbers to the number of distinct recognized faces, scenes and objects that dominate \u201cepochs\u201d of \u03c4 = 10 seconds, 1 minute and 3 minutes.\nFigure 1 shows the results for object recognition and scene recognition. We partition the sequence of frames into segments of length \u03c4 and show one plot per segment length. Each line in the plot corresponds to percentage skew s \u2208 {60, 70, 80, 90}. Each line in the plots shows the cumulative distribution representing the fraction of all segments where n labels comprised more than s percent of all labels in the segment. For instance, for 10-second segments (Figure 1(a)), typically roughly 100 frames, 5 objects comprised 90% of all objects in a segment 60% of the time (cyan line), whereas they comprise 60% of objects 90% of the time (dark blue).\nIn practice, detecting skews and training models to exploit them within 10 seconds is often challenging. As figures (b) and (c) show, the skew is less pronounced albeit still very significant for longer segments. For instance, in 90% of 3-minute segments, the top 15 objects comprise 90% of objects seen. The trend is similar with faces and scenes, with the skew significantly more pronounced, as is apparent from comparing figures (b)\nand (d); e.g. the cyan line in (d) dominates that in (b). We expect that if we ran a hand-detector and only recognized objects in the hand (analogously to recognizing detected faces), the skew would be much sharper.\nSpecialized models must exploit skews such as these to deliver appreciable speedups over the oracle. Typically, they should be generated in much less than a minute, handle varying amounts of skew gracefully, and deliver substantial speedups when inputs belong to subsets of 20 classes or fewer out of a possible several hundred in the oracle."}, {"heading": "4. Specializing Models", "text": "In order to exploit skews in the input, we cascade the expensive but comprehensive oracle model with a (hopefully much) less expensive \u201ccompact\u201d model. This cascaded classifier is designed so that if its input belongs to the frequent classes in the incoming distribution it will return early with the classification result of compact model, else it will invoke the oracle model. Thus if the skew dictates that n frequent classes, or dominant classes, comprise percentage p of the input, or skew, model execution will cost the overhead of just executing compact model roughly p% of the time, and the overhead of executing compact model and oracle sequentially the rest of the time. When p is large, the lower cost compact model will be incurred with high probability.\nTo be more concrete, we use state of the art convolutional neural networks (CNNs) for oracles. In particular, we use the GoogLeNet [21] as our oracle model, for object recognition; the VGG Net 16-layer version for scene recognition [29]; and the VGGFace network [16] for face recognition. The compact models are also CNNs. For these, we use architectures derived from the corresponding oracles by systematically (but manually) removing layers, decreasing kernel sizes, increasing kernel strides, and reducing the size of fully-connected layers. The\nend results are architectures (O[1|2] for objects, S[1|2] for scenes and F[1|2] for faces) that use noticeably less resources (Table 1), but also yield significantly lower average accuracy when trained and validated on unskewed data, i.e., the standard training and validation sets. For instance, O1 requires roughly 4\u00d7 fewer FLOPs to execute than VGGFace, but achieves roughly 70% of its accuracy.\nHowever, in our approach, we train these compact models to classify skewed distributions observed during execution, denoted by specialized classifier, and their performance on skewed distributions is the critical measure. In particular, to generate a specialized model, we create a new training dataset with the data from the n dominant classes of the original data, and a randomly chosen subset from the remaining classes with label \u201cother\u201d such that the dominant classes comprise p percent of the new data set. We train the compact architecture with this new dataset.\nFigure 2 shows how compact models trained on skewed data and cascaded with their oracles perform on validation data of different skews. Figure 2(a) ana-\nlyzes the case where n = 10, for various combinations of training and validation skews for model O1. Recall from Table 1 that O1 delivers only 70% of its accuracy on unskewed inputs. However, when training and testing is on skewed inputs, the numbers are much more favorable. When O1 is trained on p=90% skewed data with n=10 dominant classes, it delivers over 84% accuracy on average (the left-most dark-blue bar). This is significantly higher than the oracle\u2019s average of 68.9% (top-1 accuracy), denoted by the horizontal black line. We also observed from Figure 2(a) that when O1 is trained on 60% skewed data, the cascaded classifier maintains high accuracy across a wide range of testing skews from 90% to 50%. Therefore, in what follows, we use 60% skew as fixed training skew to specialize object compact models in the rest of paper (similarly 70% fixed skew for scene and 50% for face). Figure 2(b) shows that, where n is varied for O1, the cascaded classifier degrades very gracefully with n. Finally, Figure 3, which reports similar measurements on compact models S[1|2] and F[1|2] shows that these trends carry over to scene and face recognition.\nFinally, we note that since skews are only evident at test-time, specialized models must be trained extremely fast (ideally a few seconds at most). We use two techniques to accomplish this. First, before we begin processing any inputs, we train all model architectures on the full, unskewed datasets of their oracles. At test time, when the skew n, p and the identity of dominant classes is available, we only retrain the top (fully connected and softmax) layers of the compact model. The lower layers, being \u201cfeature calculation\u201d layers do not need to change with skew. Second, as a pre-processing step, we run all inputs in the training dataset through the lower feature-calculation layers, so that when re-training the top layers at test time, we can\navoid doing so. This combination of techniques allows us to re-train the specialized model in roughly 4s for F1 and F2 and 14s for O1/O2, many orders of magnitude faster than fully re-training these models."}, {"heading": "5. Sequential Model Specialization", "text": ""}, {"heading": "5.1. The Oracle Bandit Problem (OBP)", "text": "Let x1, x2, . . . , xi, . . . \u2208 X = Rn be a stream of images to be classified. Let y1, y2, . . . , yi, . . . \u2208 Y = [1, . . . , k] be the corresponding classification results. Let \u03c0 : I+ \u2192 I+ be a partition over the values. Associate the distribution Tj with partition j, so that each pair (xi, yi) is sampled from T\u03c0(i). Intuitively, \u03c0 partitions, or segments, . . . , xi, . . . into a sequence of \u201cepochs\u201d, where elements from each epoch j are drawn independently from the corresponding stationary distribution Tj . Thus, for the overall series, samples are drawn from an abruptlychanging, piece-wise stationary distribution. At test time, neither results yi nor partitions \u03c0 are known.\nLet h\u2217 : X \u2192 Y be a classifier, designated the \u201coracle\u201d classifier, trained on distribution T \u2217. Intuitively T \u2217 is a mixture of all distributions comprising the oracle\u2019s input stream: T \u2217 = \u2211 j Tj . Let R(h\n\u2217) be the cost (e.g., some combination of execution cycles and accuracy), assumed invariant across X, needed to execute h\u2217 on any x \u2208 X. Executing h\u2217 incurs a cost of R(h\u2217) with probability 1. At test time, on each input xi, we can always consult h\u2217 at cost R(h\u2217) to get a label yi with some (high) accuracy a\u2217.\nLet the dominant classes DTp \u2286 Y of distribution T be the distinct set of classes in the top p fraction of T \u2019s cumulative distribution. For each set of dominant classesDTp, the corresponding specialized classifier hDTp is trained on a dataset that draws fraction p of its examples from classes in DTp and the rest from Y \u2212DTp with a common label \u201cother\u201d. On input x \u2208 X, hDTp is converted into a cascaded classifier h\u0302DTp , if y = hDTp(x) \u2208 DTp, return y, otherwise return h\u2217(x).\nGiven examples drawn from some distribution T \u2032 (which may be the same as T ), each classifier h\u0302DTp has a cost distribution RT \u2032(h\u0302DTp) attached to it. Let \u00b5(R) be the mean of distribution R. Also suppose ||T \u2032, T || is the distributional similarity (e.g., symmetric KullbackLeibler divergence) between T \u2032 and T . We require that R satisfies the following distributional monotonicity (DM) property. For any two distributions T \u2032 and T \u2032\u2032, if ||T \u2032, T || \u2264 ||T \u2032\u2032, T ||, \u00b5(RT \u2032\u2032(h\u0302DT )) \u2264 \u00b5(RT \u2032(h\u0302DT )) for any h\u0302DT . Further, we require that the accuracy of h\u0302DT satisfy the DM property. Finally, the first time h\u0302DTp is used, we charge a flat (and usually high) \u201ctraining\u201d cost for R0.\nIntuitively, the DM property ensures that the more\nsimilar the current distribution of data is to the distribution that a classifier was specialized on, the less need for the classifier to \u201ccascade\u201d to the oracle (giving a lower average cost of classification), and the more accurate the classifier as a whole.\nNow consider a policy P that, for each incoming image xi, selects a classifier h\u0302PiDT (for some set choice of DT ), and applies it to xi. The classifier selected could also include the oracle. The expected total cost of this policy Rp = | \u222ai {h\u0302PiDT }|R0 + \u03a3i\u00b5(RT\u03c0(i)(h\u0302 P iDT\n)). We seek a policy P \u2217 that minimizes this cost: P \u2217 = arg minP RP . Note that since the cost is typically a combination of accuracy and resource use (e.g., latency), reducing cost can optimize along both these fronts."}, {"heading": "5.2. The Windowed -Greedy (WEG) Algorithm", "text": "The DM property is at the heart of minimizing cost RT \u2032(h\u0302DTp). Essentially, if we can empirically estimate the current distribution T \u2032 and ensure that the classifier h\u0302DTp currently in use is based on distribution T that is not \u201ctoo different from\u201d T \u2032, then classification costs will be low. Note that the (high) one-time cost R0 of specialization forbids us from training a new h\u0302DTp every time T \u2032 changes.\nThe above suggests that a suitable policy might alternate between exploring to estimate the current distribution and exploiting a specialized classifier h\u0302D when available. Unlike standard \u201cbandit\u201d settings [14], where the actions for exploration and exploitation belong to the same set, so that exploration would involve trying out random other specialized classifiers, in our \u201coracle bandit\u201d setting other random specialized classifiers are likely to have both low accuracy and high execution cost. We therefore always explore by consulting oracle h\u2217, which will give a good empirical estimate of the current distribution albeit at high cost.\nAnother way in which the OBP differs from the standard bandit setting is that the underlying distribution is (piece-wise) non-stationary. When empirically estimating the current distribution therefore, we need a notion of the current epoch (or \u201cpiece\u201d of distribution). Borrowing from the literature on non-stationary bandits [6], we therefore maintain a window Sj of samples initialized when we determine a new epoch j has started.\nAlgorithm 1 details our solution, the Windowed - Greedy (WEG) algorithm. WEG uses heuristics to switch between exploration and exploitation and doesn\u2019t guarantee to find the optimal policy. While processing the incoming stream xi, WEG alternates between three phases. In the window initialization phase, it uses oracle h\u2217 to accumulate a minimum number wmin of samples of the current epoch j. If these samples are distributionally close enough to those from the previous epoch,\nAlgorithm 1 Windowed -Greedy (WEG) 1: j, S0 \u2190 1, []\n. Note: \u03c4s and below are hyper-parameters. Window Initialization Phase\n2: Repeat for wmin times 3: yt \u2190 h\u2217(xt) 4: Sj \u2190 Sj \u2295 [yt] . Append new sample 5: if ||DomClasses(Sj\u22121),DomClasses(Sj)|| \u2265 \u03c4r then . dominant classes match sufficiently, old epoch continues 6: Sj \u2190 Sj\u22121 \u2295 Sj 7: w \u2190 |Sj | and go to Line 8\nUnspecialized Classification Phase 8: D \u2190 DomClasses(last w elements in Sj) 9: Estimate the true skew p\u2217 from empirical skew p\u0302\u2217 =\nP{last w classifications in Sj \u2208 D} using Equation 1 10: Estimate acc. ah\u0302D of h\u0302D with p\n\u2217 using Equation 2 11: if ah\u0302D \u2265 a\n\u2217 + \u03c4a then 12: train specialized classifier hD on dominant classes D 13: go to Line 17 . Exploit cascaded classifier h\u0302D 14: yt \u2190 h\u2217(xt) . Else, continue exploring with oracle 15: Sj \u2190 Sj \u2295 [yt] 16: go to Line 8\nSpecialized Classification Phase 17: nc, n\u2217, S \u2190 0, 0, Sj 18: yt, c\u2190 h\u0302D(xt) . exploit; c = 0|1 if|if-not cascaded to oracle 19: n\u2217 \u2190 (c or rand() \u2265 ) ? n\u2217 : n\u2217 + (h\u2217(xt) 6= yt) 20: nc \u2190 nc + c . Increment if h\u0302D did not use oracle 21: Estimate the true skew p from empirical skew p\u0302 = P{last w\nclassifications in S didn\u2019t use oracle} using Equation 1 22: Estimate acc. ah\u0302D of h\u0302D with p using Equation 2 23: if a\u2217 \u2212 ah\u0302D > \u03c4a or n\u2217 nc\u00b7 > \u03c4FP then . Exit specialized classification 24: j \u2190 j + 1 . Potentially start new epoch j 25: go to Line 2 . Go back to check if distribution has changed 26: else 27: S \u2190 S \u2295 [yt]; go to Line 18\nwe infer the previous epoch is continuing and merge the corresponding sample sets (Line 6). In the next (unspecialized classification) phase, WEG estimates whether a cascaded model h\u0302D based on the dominant classes D in the current epoch will be more accurate than h\u2217 (Line 11). If so, it trains hD and transitions to specialized classification; if not, it uses the oracle.\nThe specialized classification phase simply applies the current cascaded model h\u0302D to inputs (Line 18) until it determines that the distribution it was trained on (as represented by D) does not adequately match the current distribution. This determination is non-trivial because in the specialization phase, we wish to avoid consulting the oracle in order to reduce costs. However, the oracle is the only unbiased source of samples from the current distribution. We therefore use two proxy measures of increasing expense (Line 23).\nFirst, we use the fraction of times h\u0302D avoided using the oracle in the last w classifications as empirical skew p\u0302 and estimate the true current skew p (discussed below) (Line 21). A straightforward rule to exit the specialized classification phase is when current skew p deviates too far from the skew p\u2217 that triggered entry\nto the phase. However, this is often too conservative. As we observed in Section 4, cascaded classifiers can maintain high accuracy across a wide range of skews. We therefore re-estimate accuracy based on the current skew p, thus avoiding a premature exit from specialized classification and subsequent excess specialization. Second, our accuracy estimate is prone to failure in the case where hD routinely confuses elements of the dominant class D. To handle this case, we have no option but to consult h\u2217. We do so with a small probability (Line 19) and exit if the oracle confirms that dominant-class confusion is too frequent.\nBecause both oracle and specialized classifier make mistakes in classification, the empirical skew p\u0302 observed at runtime can differ from the true skew p of the distribution. Thus, we adopt a simple model to estimate the true skew from empirical skew. Suppose N is total number of classes that the oracle is trained to classify, n is dominant classes size |D|, and a\u2217 is accuracy of oracle. Assuming the confusion matrix is uniform, empirical skew p\u0302 when using oracle classifier is:\np\u0302 = p \u00b7 a\u2217 + p(1\u2212 a\u2217) n\u2212 1 N \u2212 1 + (1\u2212 p)(1\u2212 a\u2217) n N \u2212 1\n(1)\nThe first term handles the case that the input belongs to D and is correctly classified. The second term handles the case that the input belongs to D but is confused with the other classes in D. The third term assumes the input does not belong to D but is confused with classes in D. Given that we know p\u0302, we can derive the true skew p by solving Equation 1. Similarly we can also estimate true skew p when using cascaded classifier.\nAnother key technique in Algorithm 1 is to estimate the accuracy of a cascaded classifier. Suppose we already know the true skew p, the accuracy of cascaded classifier h\u0302D can be estimated by:\nah\u0302D = p \u00b7 ain + p \u00b7 ein\u2192out \u00b7 a \u2217 + (1\u2212 p) \u00b7 aout \u00b7 a\u2217 (2)\nwhere ain is the accuracy of specialized classifier hD on n dominant classes, ein\u2192out is the fraction of dominant inputs that hD classifies as non-dominant ones, and aout is the fraction of non-dominant inputs that hD classifies as non-dominant (note that these inputs will be cascaded to the oracle). We have observed that these parameters ain, ein\u2192out, aout of specialized classifier hD are mainly affected only by the size of the dominant class D, not the identity of elements in it. Thus, we pre-compute these parameters for a fixed set of values of n (averaging over 10 samples of D for each n), and use linear interpolation for other ns at test time."}, {"heading": "6. Evaluation", "text": "We implemented the WEG algorithm with a classification runtime based on Caffe [10]. The system can be fed with videos to produce classification results by recognizing frames. Our goal was to measure both how well the large specialized model speedups of Table 1 translated to speedups in diverse settings and on long, real videos. Further we wished to characterize the extent to which elements of our design contributed to these speedups."}, {"heading": "6.1. Synthetic experiments", "text": "First, we evaluate our system with synthetically generate data in order to study diverse settings. For this experiment, we generate a time-series of images picked from standard large validation sets of CNNs we use. Each test set comprises of one or two segments where a segment is defined by the number of dominant classes, the skew, and the duration in minutes. For each segment, we assume that images appear at a fixed interval (1/6 seconds) and that each image is picked from the testing set based on the skew of the segment. For an example of a segment with 5 dominant classes and 90% skew, we pre-select 5 classes as dominant classes and pick an image with 90% probability from the dominant classes and an image with 10% probability from the other classes at each time of image arrival over 5 minutes duration. Images in a class are picked in a uniform random way. We also generate traces with two consecutive segments with different configurations to study the effect of moving from one context to the other.\nTable 2 shows the average accuracies and per-image processing latencies using GPU for the recognition tasks with and without the specializer enabled. The results are averaged over 5 iterations for each experiment. The specializer was configured to use the compact classifiers O2 for objects, S2 for scenes, and F2 for face recognition from Table 1.\nThe following points are worth noting. (i) (Row 1 and it\u2019s sub-rows) WEG is able to detect and exploit skews over 5-minute intervals and get significant speedups over the oracle while preserving accuracy. For the single segment cases, the GPU latency speedup per-image was 1.5\u00d7 to 2.0\u00d7, 1.5\u00d7 to 2.3\u00d7, and 3.3\u00d7 to 5.9\u00d7, for object, scene, and face, respectively. However, due to WEG\u2019s overhead these numbers are noticeably lower than the raw speedups of specialized models (Table 1). When the number of dominant classes increase, the specializer latency increases because it alternates between exploration and exploitation to recognizes more dominant classes (compare row 1/2 or 3/4 for object and scene recognition, and row 2/3 for face recognition). The latency also increases when the skew of dominant classes decreases because specializer cascades more times to oracle model when using the cascaded classifier (compare row 2/3 for object and scene recognition, and row 1/2 for face recognition). (ii) (Row 2) WEG is quite stable in handling random inputs, essentially resorting to the oracle so that accuracy and latency are unchanged. (iii) (Rows 3 and 4) WEG is able to detect abrupt input distribution changes as the accuracy remains comparable\nto oracle accuracy, but with significant speedups when the distribution is skewed (Row 4)."}, {"heading": "6.2. Video experiments", "text": "We now turn to evaluating WEG on real videos. We hand-labeled video clips from three movies, one TV show, and an interview and manually labeled the faces in the videos. The names of video clips with lengths are listed in Table 3. Note that we used the entire videos for Friends and Ellen Show, while we used a video chunk for the movies. For these experiments, we used F2 as the compact classifier.\nTable 3 shows the average accuracies and average latencies for processing a frame for 5 videos. We generated these by first extracting all faces from the videos to disk using the Viola Jones detector. We then ran WEG on these faces and measured the total execution time. Dividing by the number of faces gave the average numbers shown here. The most important point is that even on real-world videos, WEG is able to achieve very significant speedups over the oracle, ranging from 2.8\u00d7-12.7\u00d7 (CPU) and 2.5\u00d7-8.5\u00d7 (GPU).\nTo understand the speedup, we summarize the statistics of WEG execution in Table 3. \u201cSpecial rate\u201d indicates the percentage of time that specializer exploits the cascaded classifier to reduce the latency, while cascade rate reveals the percentage of time that a cascaded classifier cascades to the oracle classifier, thus hurting performance. Higher special rate and lower cascade rate yield more speedup. The cascade rate of \u201cOcean\u2019s Eleven\u201d is significantly higher than that of other videos. We investigated this and found that the specialized compact CNN repeatedly made mistakes on one person in the video, which led to a high cascade rate. \u201cTrans. special\u201d counts the number of times WEG needed to switch between specialized and unspecialized classification to handle the distribution changes and insufficient exploration. The average dominant classes sizes (\u201cdom. size\u201d) show that the real videos are skewed to fewer dominant\nclasses than the configurations used in the synthetic experiment. This explains why our system achieved higher speedup on real videos than on synthetic data. Overall, the statistics show that the dataset exercise WEG features such as skew estimation, cascading and specialization.\nTo understand better the utility of WEG\u2019s features, we performed an ablation study: (a) We disable the adaptive window exploration (Line 5-6 in Algorithm 1), and use a fixed window size of 30 and 60. (b) We use the skew of dominant classes in the input distribution as the training skew for specializing compact CNNs instead of using the fixed (50%) training skew suggested in Section 4. (c) We apply a simple (but natural) criterion to exit from the specialized classification phase: WEG now exits when the current skew is lower than the skew when it entered into specialized classification phase instead of using the estimated accuracy as soft indicator.\nFigure 4 shows the comparison between these variants and our specializer in accuracy and CPU / GPU speedups when recognizing faces on Friends video. In the figure we show the absolute differences in accuracy and relative differences in CPU / GPU speedup. (a) Fixed window size (30 and 60) variants achieve similar accuracy but lower speedup. As table 3 (\u201cwindow size\u201d column) shows, the adaptively estimated size for the window is between 30 and 60. In general, too small a window fails to capture the full dominant classes, yielding specializers that exit prematurely. Too large a window requires more work by the oracle to fill up the window. (b) Using variable rather than fixed skew for training achieves more speedup, but suffers from 30% loss in accuracy. This is because the training skew is usually very high. As discussed in Section 4, training on highly skewed data produces models vulnerable to false positives in \u201cother\u201d classes. (c) The simple exit variant achieves almost comparable accuracy while the latency is more than 50% higher than our system. It demonstrates the value of our accuracy estimate in modeling the accuracy of cascaded classifiers and to prevent premature exit from the specialized classification phase. In summary, the key design elements of WEG each have a role in producing fast and accurate results."}, {"heading": "7. Conclusion", "text": "We characterize skew in day-to-day video, showed that skewed distributions need much simpler CNNs, and developed new very fast specialized CNNs. We formalize the \u201cbandit\u201d-style sequential model selection as Oracle Bandit Problem and provide a new exploration/exploitation based algorithm, Windowed - Greedy (WEG). Our solution speeds up face recognition on TV episodes and movies by 2.5-8.5\u00d7 on a GPU (2.8-\n12.7\u00d7 on a CPU) with little loss in accuracy relative to a modern convolutional neural network."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Mach. Learn.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Do deep nets really need to be deep? In Advances in neural information processing", "author": ["J. Ba", "R. Caruana"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Detecting actions, poses, and objects with relational phraselets", "author": ["C. Desai", "D. Ramanan"], "venue": "In Proceedings of the 12th European Conference on Computer Vision - Volume Part IV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Fast feature pyramids for object detection", "author": ["P. Doll\u00e1r", "R. Appel", "S. Belongie", "P. Perona"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "On upper-confidence bound policies for non-stationary bandit problems", "author": ["A. Garivier", "E. Moulines"], "venue": "In Proceedings of the 22nd International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Proceedings of the Twentyninth Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P.H.S. Torr"], "venue": "In 2011 International Conference on Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia (MM),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Learning from a single labeled face and a stream of unlabeled data", "author": ["B. Kveton", "M. Valko"], "venue": "In Automatic Face and Gesture Recognition (FG),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T. Lai", "H. Robbins"], "venue": "Adv. Appl. Math.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1985}, {"title": "A convolutional neural network cascade for face detection", "author": ["H. Li", "Z. Lin", "X. Shen", "J. Brandt", "G. Hua"], "venue": "In CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Deep face recognition", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "In BMVC,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Detecting activities of daily living in first-person camera views", "author": ["H. Pirsiavash", "D. Ramanan"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "XNOR-Net: ImageNet Classification", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "Using Binary Convolutional Neural Networks", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Reinforcement Learning, an introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press/Bradford Books,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CoRR, abs/1409.4842,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Online semi-supervised learning on quantized graphs", "author": ["M. Valko", "B. Kveton", "L. Huang", "D. Ting"], "venue": "In Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "A scalable approach to activity recognition based on object use", "author": ["J. Wu", "A. Osuntogun", "T. Choudhury", "M. Philipose", "J.M. Rehg"], "venue": "IEEE 11th International Conference on Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Cost-sensitive tree of classifiers", "author": ["Z. Xu", "M. Kusner", "M. Chen", "K.Q. Weinberger"], "venue": "In ICML,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Classifier cascades and trees for minimizing feature evaluation cost", "author": ["Z.E. Xu", "M.J. Kusner", "K.Q. Weinberger", "M. Chen", "O. Chapelle"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Testcost sensitive classification on data with missing values", "author": ["Q. Yang", "C.X. Ling", "X. Chai", "R. Pan"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Places: An image database for deep scene understanding", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Torralba", "A. Oliva"], "venue": "arXiv preprint arXiv:1610.02055,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In Proceedings of the Twentyeighth Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Since the seminal work of Viola and Jones [23] on face detection, one of the best-known techniques to speed up classification has been to structure the classifier as a cascade (or tree [26]) of simple classifiers such that \u201ceasy\u201d examples lead to early exits and are therefore classified faster.", "startOffset": 42, "endOffset": 46}, {"referenceID": 25, "context": "Since the seminal work of Viola and Jones [23] on face detection, one of the best-known techniques to speed up classification has been to structure the classifier as a cascade (or tree [26]) of simple classifiers such that \u201ceasy\u201d examples lead to early exits and are therefore classified faster.", "startOffset": 185, "endOffset": 189}, {"referenceID": 3, "context": "In fact, traditional cascades are most applicable in detection tasks [4], where the background is both much more common and easier to classify than the foreground.", "startOffset": 69, "endOffset": 72}, {"referenceID": 15, "context": "Distinct from the (two-class) detection setting in traditional cascading, recent advances in convolutional neural networks (CNNs) [16,21,29] have opened up the possibility of using a single, pre-trained \u201coracle\u201d classifier to recognize thousands of classes such as people, objects and scenes.", "startOffset": 130, "endOffset": 140}, {"referenceID": 20, "context": "Distinct from the (two-class) detection setting in traditional cascading, recent advances in convolutional neural networks (CNNs) [16,21,29] have opened up the possibility of using a single, pre-trained \u201coracle\u201d classifier to recognize thousands of classes such as people, objects and scenes.", "startOffset": 130, "endOffset": 140}, {"referenceID": 28, "context": "Distinct from the (two-class) detection setting in traditional cascading, recent advances in convolutional neural networks (CNNs) [16,21,29] have opened up the possibility of using a single, pre-trained \u201coracle\u201d classifier to recognize thousands of classes such as people, objects and scenes.", "startOffset": 130, "endOffset": 140}, {"referenceID": 20, "context": "When training such oracle classifiers, such as GoogLeNet [21] of VGGFace [16]), a small number of classes do not usually dominate the training set: for broad applicability, the classifier is trained assuming that all classes are more or less equally likely.", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "When training such oracle classifiers, such as GoogLeNet [21] of VGGFace [16]), a small number of classes do not usually dominate the training set: for broad applicability, the classifier is trained assuming that all classes are more or less equally likely.", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "For instance, we present a CNN that executes 60\u00d7 fewer FLOPs than the state-of-the art VGGFace [16] model, but has comparable accuracy when over 50% of faces come from the same 10 or fewer people.", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "There is a long line of work on cost-sensitive classification, the epitome of which is perhaps the cascaded classification work of Viola and Jones [23].", "startOffset": 147, "endOffset": 151}, {"referenceID": 24, "context": "The essence of this line of work [25, 27] is to treat classification as a sequential process that may exit early if it is confident in its inference, typically by learning sequences that have low cost in expectation over training data.", "startOffset": 33, "endOffset": 41}, {"referenceID": 26, "context": "The essence of this line of work [25, 27] is to treat classification as a sequential process that may exit early if it is confident in its inference, typically by learning sequences that have low cost in expectation over training data.", "startOffset": 33, "endOffset": 41}, {"referenceID": 14, "context": "Recent work [15] has even proposed cascading CNNs as we do.", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "Traditional sequential models such as probabilistic models [3, 17, 24] and Recurrent Neural Networks (RNNs) [5,11] are aimed at classifying instances that are not independent of each other.", "startOffset": 59, "endOffset": 70}, {"referenceID": 16, "context": "Traditional sequential models such as probabilistic models [3, 17, 24] and Recurrent Neural Networks (RNNs) [5,11] are aimed at classifying instances that are not independent of each other.", "startOffset": 59, "endOffset": 70}, {"referenceID": 23, "context": "Traditional sequential models such as probabilistic models [3, 17, 24] and Recurrent Neural Networks (RNNs) [5,11] are aimed at classifying instances that are not independent of each other.", "startOffset": 59, "endOffset": 70}, {"referenceID": 4, "context": "Traditional sequential models such as probabilistic models [3, 17, 24] and Recurrent Neural Networks (RNNs) [5,11] are aimed at classifying instances that are not independent of each other.", "startOffset": 108, "endOffset": 114}, {"referenceID": 10, "context": "Traditional sequential models such as probabilistic models [3, 17, 24] and Recurrent Neural Networks (RNNs) [5,11] are aimed at classifying instances that are not independent of each other.", "startOffset": 108, "endOffset": 114}, {"referenceID": 8, "context": "Similar to adaptive cascading, online learning methods [9, 12, 22] customize models at test time.", "startOffset": 55, "endOffset": 66}, {"referenceID": 11, "context": "Similar to adaptive cascading, online learning methods [9, 12, 22] customize models at test time.", "startOffset": 55, "endOffset": 66}, {"referenceID": 21, "context": "Similar to adaptive cascading, online learning methods [9, 12, 22] customize models at test time.", "startOffset": 55, "endOffset": 66}, {"referenceID": 0, "context": "Estimating distributions in sequential data and exploiting it is the focus of the multi-armed bandit (MAB) community [1, 13].", "startOffset": 117, "endOffset": 124}, {"referenceID": 12, "context": "Estimating distributions in sequential data and exploiting it is the focus of the multi-armed bandit (MAB) community [1, 13].", "startOffset": 117, "endOffset": 124}, {"referenceID": 5, "context": "Our Windowed -Greedy algorithm is strongly informed by the use of windows in [6] to handle non-stationarities and the well-known [20] -greedy scheme to balance exploration and exploitation.", "startOffset": 77, "endOffset": 80}, {"referenceID": 19, "context": "Our Windowed -Greedy algorithm is strongly informed by the use of windows in [6] to handle non-stationarities and the well-known [20] -greedy scheme to balance exploration and exploitation.", "startOffset": 129, "endOffset": 133}, {"referenceID": 1, "context": "Finally, much recent work has focused on reducing the resource consumption of (convolutional) neural networks [2, 7, 8, 18].", "startOffset": 110, "endOffset": 123}, {"referenceID": 6, "context": "Finally, much recent work has focused on reducing the resource consumption of (convolutional) neural networks [2, 7, 8, 18].", "startOffset": 110, "endOffset": 123}, {"referenceID": 7, "context": "Finally, much recent work has focused on reducing the resource consumption of (convolutional) neural networks [2, 7, 8, 18].", "startOffset": 110, "endOffset": 123}, {"referenceID": 17, "context": "Finally, much recent work has focused on reducing the resource consumption of (convolutional) neural networks [2, 7, 8, 18].", "startOffset": 110, "endOffset": 123}, {"referenceID": 15, "context": "[16]), scene [29] and object recognizers [19] to every sampled frame.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[16]), scene [29] and object recognizers [19] to every sampled frame.", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": "[16]), scene [29] and object recognizers [19] to every sampled frame.", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "Object (1000 classes) [21] 68.", "startOffset": 22, "endOffset": 26}, {"referenceID": 27, "context": "Scene (205) [28] 58.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Face (2622) [16] 95.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "Execution time is feedforward time of a single image without batching on Caffe [10], a Linux server with a 24-core Intel Xeon E5-2620 and an NVIDIA K20c GPU.", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "In particular, we use the GoogLeNet [21] as our oracle model, for object recognition; the VGG Net 16-layer version for scene recognition [29]; and the VGGFace network [16] for face recognition.", "startOffset": 36, "endOffset": 40}, {"referenceID": 28, "context": "In particular, we use the GoogLeNet [21] as our oracle model, for object recognition; the VGG Net 16-layer version for scene recognition [29]; and the VGGFace network [16] for face recognition.", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "In particular, we use the GoogLeNet [21] as our oracle model, for object recognition; the VGG Net 16-layer version for scene recognition [29]; and the VGGFace network [16] for face recognition.", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "Unlike standard \u201cbandit\u201d settings [14], where the actions for exploration and exploitation belong to the same set, so that exploration would involve trying out random other specialized classifiers, in our \u201coracle bandit\u201d setting other random specialized classifiers are likely to have both low accuracy and high execution cost.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "Borrowing from the literature on non-stationary bandits [6], we therefore maintain a window Sj of samples initialized when we determine a new epoch j has started.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "We implemented the WEG algorithm with a classification runtime based on Caffe [10].", "startOffset": 78, "endOffset": 82}], "year": 2017, "abstractText": "Recent advances have enabled \u201coracle\u201d classifiers that can classify across many classes and input distributions with high accuracy without retraining. However, these classifiers are relatively heavyweight, so that applying them to classify video is costly. We show that day-to-day video exhibits highly skewed class distributions over the short term, and that these distributions can be classified by much simpler models. We formulate the problem of detecting the short-term skews online and exploiting models based on it as a new sequential decision making problem dubbed the Online Bandit Problem, and present a new algorithm to solve it. When applied to recognizing faces in TV shows and movies, we realize end-toend classification speedups of 2.5-8.5\u00d7/2.8-12.7\u00d7 (on GPU/CPU) relative to a state-of-the-art convolutional neural network, at competitive accuracy.", "creator": "LaTeX with hyperref package"}}}