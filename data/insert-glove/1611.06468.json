{"id": "1611.06468", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Generating machine-executable plans from end-user's natural-language instructions", "abstract": "stature It 0.060 is bathymetric critical cogema for 76.1 advanced manufacturing perron machines amoebozoa to autonomously 10.125 execute veena a haversham task by following 46-game an end - gewog user ' s natural language (NL) lithographic instructions. However, quartus NL instructions are parsa usually deaf ambiguous espectaculo and abstract liatsos so g-1 that purani the dewoskin machines rhizomes may canings misunderstand starting and walsch incorrectly panchayaths execute unfailingly the photomasks task. To bo\u0161njak address inhalations this eiseley NL - 84.15 based orma human - machine 47.43 communication engberg problem one-race and dazzles enable clogged the machines 61.44 to appropriately execute t43 tasks by 4d following gear-driven the zyklon end - user ' seastar s branchflower NL conticchio instructions, santanas we duodu developed a Machine - 48.63 Executable - venders Plan - desist Generation (exePlan) method. heavyhanded The basils exePlan homewood method conducts task - 2xcd centered woodsfield semantic gawaine analysis geping to extract 15.42 task - related information zhensheng from chowed ambiguous NL md-11 instructions. octal In grossi addition, protocols the despoiling method specifies sleepers machine roselyn execution bagwell parameters 55a to generate wtrg a machine - unleased executable plan by strikers interpreting keystrokes abstract kunarac NL instructions. oga To o'shea evaluate the exePlan method, www.haplecrone.com an industrial robot sforzesco Baxter johan was pwn instructed by 58,200 NL to perform holzman three types of industrial 1887-88 tasks {' bmm drill a hole ', ' clean a spot ', ' install a 17.27 screw ' }. wassel The 2,066 experiment shakib results sukhteh proved that the exePlan deferments method rafa was effective parlement in generating machine - \u0f53 executable cagers plans from the end - user ' meteoroids s bsec NL colomb instructions. Such koszics a thamud method has the espousing promise to tadzhuddin endow fenninger a sobanska machine praharaj with 17.7 the sold ability lemonis of snjezana NL - orph\u00e9e instructed task zhuravleva execution.", "histories": [["v1", "Sun, 20 Nov 2016 04:06:47 GMT  (1155kb)", "http://arxiv.org/abs/1611.06468v1", "16 pages, 10 figures, article submitted to Robotics and Computer-Integrated Manufacturing, 2016 Aug"]], "COMMENTS": "16 pages, 10 figures, article submitted to Robotics and Computer-Integrated Manufacturing, 2016 Aug", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.RO", "authors": ["rui liu", "xiaoli zhang"], "accepted": false, "id": "1611.06468"}, "pdf": {"name": "1611.06468.pdf", "metadata": {"source": "CRF", "title": "Generating machine-executable plans from end-user\u2019s natural-language instructions", "authors": ["Rui Liu", "Xiaoli Zhang"], "emails": ["xlzhang}@mines.edu"], "sections": [{"heading": null, "text": "an end-user\u2019s natural language (NL) instructions. However, NL instructions are usually ambiguous and abstract so that the machines may misunderstand and incorrectly execute the task. To address this NL-based human-machine communication problem and enable the machines to appropriately execute tasks by following the end-user\u2019s NL instructions, we developed a Machine-Executable-Plan-Generation (exePlan) method. The exePlan method conducts task-centered semantic analysis to extract task-related information from ambiguous NL instructions. In addition, the method specifies machine execution parameters to generate a machine-executable plan by interpreting abstract NL instructions. To evaluate the exePlan method, an industrial robot Baxter was instructed by NL to perform three types of industrial tasks {\u201cdrill a hole\u201d, \u201cclean a spot\u201d, \u201cinstall a screw\u201d}. The experiment results proved that the exePlan method was effective in generating machine-executable plans from the end-user\u2019s NL instructions. Such a method has the promise to endow a machine with the ability of NL-instructed task execution.\nIndex Terms-Advanced manufacturing machine, machine-executable plan, natural language instruction,\nsemantic analysis, task execution.\nIntroduction\nHuman-machine collaborative manufacturing combines human intelligence on high-level task planning and the robot physical capability (e.g., precision and speed) on low-level task execution [1]. Toward this direction, intuitive and natural communication between the human and the machine has been an active research area in the last decade with the goal to enable seamless human-machine cooperation [2][3]. Natural-Language-instructed human-machine interaction is expected to enable an advanced manufacturing machine, such as a Computer Numerical Control machine or an industrial robot, to autonomously perform tasks such as rough/fine finishing [4][5], assembly [2][6] and packaging [7][8] according to the end-user\u2019s NL instructions, which are given based on the user\u2019s judgement of the task progress and environmental situations. Compared with other input methods, including human hand force [9][10], hand gesture [11][12], and body motions [13][14][15][16], the NL instruction method has two main advantages. First, NL instruction provides a natural, human-like, face-to-face communication manner. Non-expert users without prior programming training could command a machine to perform their desired tasks [17][18]. Second, the inherent linguistic structure of NL, as a predefined information encoder, provides a standard, informative data source to generate structured machine language [19][20]. In contrast, the aforementioned existing methods require extra translations among discrepant data patterns. These two advantages make NL a superior means for the end-user to naturally and efficiently communicate with manufacturing machines.\nCurrently, typical industrial applications involving NL include NL-based control in which the working statuses such as \u201con/off\u201d and \u201cquickly/slowly\u201d are selected orally to control a machine in navigation [3][21], NL-based task execution in which the task operation methods such as \u201cgoTo + Location; then drop + object\u201d is described orally to help a machine with object finding/placing [22][23], and NL-based execution personalization in which human\u2019s preferences and moods in oral dialogs were considered to adjust a machine\u2019s execution manners [24][25].\nHowever, there is still a long way to apply NL-instructed machines in practical manufacturing applications. First, NL is variable and ambiguous. NL is usually polysemous, homophonic and expression-manner diverse so that the same meaning could be expressed in various ways, and different meanings could be expressed in similar ways [8][26]. For example, \u201cdrill a hole\u201d could be expressed as \u201cbore one hole\u201d, \u201cdrilling one bore\u201d, \u201ccreate an unthreaded hole\u201d, and so on [27]. In addition, humans usually use referring, outlining, and omitting in NL instructions [22][28]. For\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Corresponding author: Tel.:+1-303-384-2343; fax:+1-303-273-3602, email:xlzhang@mines.edu.\nexample, in an instruction \u201cat the center point\u201d, information such as \u201cwhich object in which place has the center point\u201d cannot be known merely from a word \u2018the\u2019 [27]. It is challenging to extract task-related information such as task goals and detailed execution procedures from NL instructions. Second, human instruction is abstract [23][29]. Even when a complete execution procedure for a task is instructed, the generated plan is still non-executable for a machine. For example, the abstract instructions \u2018clean the surface\u2019 are still machine-non-executable for that the execution-related specific knowledge such as \u201ctool: brush; action: moveDown \u2192 sweep \u2192 moveUp; ...\u201d is missing [27]. In addition to specific-knowledge missing, a reasonable and flexible knowledge structure, which is implicitly embedded in NL descriptions to guide correct task execution, is difficult to extract [30][31][32]. By obeying the human instructions, one task could be flexibly executed by several methods, which were formulated according to an individual\u2019s cognitive logics [33][34]. However, usually these cognitive logics in NL instructions are difficult to understand as to a machine, for that literal information directly extracted from NL instructions is insufficient to explain the logics [2][35]. Taking the task \u201cdeliver a drink\u201d as an example, the potential methods could either be \u201cfetch a cup + fill the cup with water + place it on table\u201d or \u201cplace a cup on the table + fill cup with water\u201d. The logics such as {CupAvailability(yes) ^ WaterAvailability(yes) \u2192 CupBeingFilledFeasibility(yes)} behind the task executions have not been described explicitly in NL instruction while these logics are important in deciding what kind of procedures are feasible and reasonable in execution and in assessing whether a task could be executable or not. It is challenging for a machine to perform a task without knowing the task-related logics.\nTo address these problems and enable NL-instructed manufacturing in practical industrial tasks, we developed a machine-executable-plan-generation (exePlan) method to \u201ctranslate\u201d the ambiguous and abstract NL instructions into machine-executable plans. In this paper, we mainly have two contributions, shown as follows.\n A task-centered semantic analysis method is developed for processing ambiguous NL instructions into task-related information including task goal, sub-goals, and execution logic relations. Instead of using basic linguistic features such as keywords/Part-of-Speech(PoS), the task-related semantic features, such as actions/tools/execution logics were considered to extract the task-related information from ambiguous NL instructions.\n A machine-execution-specification method is developed for interpreting abstract human instructions into machine-executable plans. With this method, each abstract sub-goal in the NL instruction is firstly specified into an executable sub-goal by adding the machine-execution specification (MES) parameters such as location, action, and human requirements. Then a machine-executable plan is specified by exploring the weighted logic relations among the task-related execution procedures. Different from the first-order logic in which all the logic relations are inviolable and plans using first-order logic have binary executability {executable, non-executable}, weighted logic relations could be violable with a corresponding weighted decrease of plan executability and the plans using weighted logics have a range of acceptable executability. A plan is flexibly made by organizing reasonable logic procedures with the executability greater than a threshold value.\nRelated Work\nTo disambiguate NL instructions in task execution, special grammars were designed to identify the task-related entities based on specific keyword involvements and their PoS tags. For example, in the sentence \u201cbring the can in the trash bin\u201d the task goal \u201cin the trash bin\u201d was extracted based on the keywords \u201cbring, can\u201d and their corresponding PoS tags \u201cVB, NN\u2019[8]. Ontology relations among the interested entities were used for mutual disambiguation. For example, to describe a cup, the description was likely to be \u201ccontainer with handle attached\u201d. \u201cAttached\u201d was the constraint relation between the object \u201ccontainer\u201d and object part \u201chandle\u201d [36][37]. When an entity was ambiguously mentioned, the ambiguous entity could be explained by mutually co-referring. Take sentences \u201cGo to the second crate on the right. Pick it up\u201d for an example, with co-reference resolution the uncertain expression \u201cit\u201d was identified as \u201cthe second crate on the right\u201d [22][38]. When the NL descriptions such as \u201cpick up the pallet\u201d were too ambiguous for a robot, a query such as \u201cwhich pallet?\u201d was launched to ask the human for disambiguation [2][39]. By exploring the features such as perceivable properties \u201ccylindrical\u201d and \u201cround\u201d, the ambiguous descriptions \u201ccylindrical container with a round handle attached on one side\u201d for the object \u201ccontainer\u201d was understood [36]. By exploring the spatial relations \u201cbehind\u201d in NL descriptions \u201cNavigate to the building behind the pole\u201d, named entities \u201cbuilding, pole\u201d were identified in the real world [40][41]. To disambiguate the NL instructions, these methods explored context evidences for a single entity. Evidences include semantic relations, human explanations, and spatial constrains. However, these methods only focused on using one single type of evidences such as basic linguistic feature keywords/PoS or semantic features co-referring and perceivable properties, without combining the multiple types of features together to perform a comprehensive semantic analysis. In addition, these methods aimed to identify an entity such as \u201ccan\u201d or \u201ctrash bin\u201d\nseparately without considering entity correlations such as \u201ccan\u2014trash bin\u201d, which are informative in instruction disambiguation. The above mentioned features are important for semantic analysis, however have not been well investigated.\nTo interpret abstract expressions in NL instructions, motion grammars were first designed for establishing the word-action correlations such as word \u201cgrasp\u201d \u2014 action \u201cGrasp\u201d [3][21][23]. Real-world preconditions such as \u201cstay in the kitchen\u201d were defined for triggering specific types of executions such as \u201cvisiting the kitchen\u201d [17]. The NL descriptions were marked by landmark objects such as \u201cstaircase, box\u201d in the real world to enable the execution of tasks such as \u201creach in a spot\u201d [42][43]. With these methods, abstract NL descriptions are interpreted into executable commands to some extent. However these methods do not make the NL command truly machine-executable for that the critical execution parameters, including the tool usage, real-world precondition, action sequence, and human requirements, are still missing or insufficient for supporting a robot\u2019s executions in practical situations.\nTo interpret implicit cognitive logics embedded in NL instructions, probabilistic graphical models were designed to explore the knowledge importance with the consideration of its probability distributions for plan making. For example, in the NL descriptions \u201cgo to the second crate on the right. Pick it up.\u201d, the procedures could be modeled as {goTo create (p=0.50), PickUp crate (p=0.50)}[22][43]. In [46][47], a semantic topological model was developed to explore the internal logic correlations of sub-steps in a reasonable task-execution plan. For example, in the path \u201cfirst go to the hallway; the cafeteria is down the hallway\u201d the hallway could be replaced by \u201chall, corridor, walkway\u201d and the \u201ccafeteria\u201d could be replaced by \u201cdining hall\u201d according to the semantic topology. The procedures with any combinations of the elements in the topology structure were considered as reasonable task execution plans. However, these plans are not truly executable for a machine. Probabilistic graphical models merely describe the importance of the sub-steps in a plan, ignoring their internal logics without which a plan is non-executable in the real world. On the other hand, topological models describe the logic relations among procedures; however, the logic constrains are hard without discriminative descriptions of the involved logic relations. If one hard logic relation is unsatisfied the whole plan is not executable. Both probability-based methods and logic-based methods should be considered to solve the plan-making problem. However, they have not been well investigated.\nAs Fig. 1 shows, the exePlan method transfers NL instructions (input) into machine task execution plans (output). The exePlan method includes three critical steps: instruction disambiguation, instruction interpretation, and plan executability assessment. Instruction disambiguation conducts task-centered semantic analysis on NL instructions to extract task-related information including task goal, sub-goals, and execution logic relations. Instruction interpretation generates machine-executable plans from abstract task-related information by specifying MESs for each sub-goal and organizing the sub-goals into a logic-based plan. The main types of MES parameters include working-spot locations,\nworking-spot statuses, action sequences, tool usages, and operation preconditions. A logic-based plan represents a reasonable execution procedure in a weighted logic manner. Executability assessment checks feasibility of generated sub-goals, sub-goal transitions, and the MES availability in the practical situations, enabling the plan to be machine-executable.\n3.1. Instruction Disambiguation Initial natural language processing (NLP) is conducted for processing NL instructions into hierarchical syntax trees with linguistic features such as words, PoS tags, word dependencies, and independent sentences. First, English oral instructions are recognized as text corpus by the speech recognition tool SpeechRecognition [48]. Second, text corpus is split into independent sentences, words, PoS tags and dependences by using the NLP tool Stanford CoreNLP [20]. Then word normalization is conducted to normalize the interested keywords such as \u201cdrilling, drills and drill\u201d to unified morphologies such as \u2018drill\u2019. Unified formats of the linguistic features are for preparation of feature-based semantic analysis. Last, based on the linguistic features, sentence structures are analyzed by generating the hierarchical syntax tree, shown in Fig. 2 with {root: sentence root, dobj: direct object dependency, nomd: norm modifier dependency, amod: adjectival modifier, case: case dependency, det: determiner dependency, VB: verb, NN: noun, DT: determiner, JJ: adjective, IN: preposition}.\nTask-centered semantic analysis aims to identify task-related entities such as execution sub-goals and sub-goals\u2019 logic relations indicated by ambiguous NL instructions. A task-centered feature space is designed by using both the basic linguistic features {keyword (w), PoS (pos), word dependencies (wd), general words\u2019 PoS dependencies (posd)} and the task-related semantic features {task-related keyword (KW), previous/next sub-goal (preSubg/nextSubg), previous/next noun of a task-related entity (preN/nextN), previous/next word of a task-related entity (preW/nextW), dependency of the task-related keywords (KWD), task-related keywords\u2019 PoS dependency (posD), dependency of the task-related norn (NND), and task-related context (context)}. By projecting the NL instructions into this task-centered feature space and performing classifications, sub-goals in an instructed execution plan are identified. Then based on the identified sub-goals, logic transition relations among the sub-goals are detected according to their involvement sequence. Taking the instruction \u201cDrill a hole at the board center. Then clean the dust with the brush.\u201d as an example, the sub-goal \u201cclean\u201d is identified by using features {KW: clean, pos: VB, KWD: clean+dust, posD: dobj(VB+NN), preSubg:(drill), context: (dust, brush)}. Then based on the sub-goals\u2019 temporal sequence in human instructions, the logic transition \u201cdrill\u2192clean\u201d is detected, shown in Fig. 3.\nKeywords for the task-related entities are defined in the local database. When keywords are detected in NL instructions, semantic analysis is triggered for knowledge extraction. Given that the amount of the text corpus is potentially large and the desired output predictions (sub-goals involved in task plans) are multiple, a semi-supervised multi-class Support Vector Machine algorithm (smSVM) [44] is adopted. A semi-supervised classification method merely needs a small amount of labeled samples (usually 1%~5% of the total samples) for classifier training [32]. During the semi-supervised training, new features are learned for better entity-extraction performances. The detailed process of the smSVM-supported entity detection is shown in Algorithm 1. First, by using the manually labeled sentence vectors (P\ud835\udc58, Q\ud835\udc58), a classifier h for an entity is initially trained. (P\ud835\udc58, Q\ud835\udc58) is represented by (\ud835\udc5d1:\ud835\udc58, \ud835\udc5e1:\ud835\udc58), where \ud835\udc5d\ud835\udc58 denotes the k-th feature vector of a sentence in human instructions and \ud835\udc5e\ud835\udc58 denotes the corresponding entity label. Second, the initially-trained classifier h is adopted to classify testing samples \ud835\udc5e\ud835\udc58+1:\ud835\udc5a into label \ud835\udc5d\ud835\udc58+1:\ud835\udc5a one by one.\nAfter a classification for a single sentence, the classified sample (\ud835\udc5d\ud835\udc58+1, \ud835\udc5e\ud835\udc58+1) is used to update the existing training sample set into a new set T, based on which the classifier h is updated. In the new training process, new semantic features for a type of classification are collected to improve the classifier\u2019s performance. The core algorithm for smSVM is shown in equations (1) and (2). wsm is the weight for an input feature vector, \ud835\udf09\ud835\udc56 denotes the acceptable classification error, and C \ud835\udc60\ud835\udc5a is the tradeoff parameter balancing error and margin. The slop value \ud835\udc64\ud835\udc60\ud835\udc5a \u2217 and the intercept value \ud835\udc4f\ud835\udc60\ud835\udc5a \u2217 are solved by a quadratic programming (QP) solver [45] and they define an optimal hyperplane to conduct classification for extracting the task-related entity \ud835\udc53\ud835\udc60\ud835\udc5a.\nmin 1\n2 ||wsm||\n2 + C \ud835\udc60\ud835\udc5a \u2211 \ud835\udf09\ud835\udc56 \ud835\udc3e \ud835\udc56=1\n(1)\n\ud835\udc53\ud835\udc60\ud835\udc5a(\ud835\udc65) = \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b{\ud835\udc64\ud835\udc60\ud835\udc5a \u2217 \ud835\udc47 \u00b7 \ud835\udc65 + \ud835\udc4f\ud835\udc60\ud835\udc5a \u2217 } (2)\n3.2. Instruction Interpretation 1). Sub-goal Interpretation An abstract sub-goal is interpreted into an executable operation by specifying the corresponding MES parameters. A complete MES consists of operation preconditions (precon), working spot locations (loc), action sequence (act), tool usages (tool), and human requirements (req), shown in Fig. 4. The MES parameters are hard constraints for a sub-goal that only when all the hard constraints are satisfied, a sub-goal is executable with interpretation value 1; otherwise the interpretation value is 0. As shown in equation (3), \ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc56 denotes a MES parameter for sub-goal i of task j, n is the total number of MES parameters for sub-goal i, and \ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc54\ud835\udc5c\ud835\udc4e\ud835\udc59\ud835\udc56 denotes the binary executability for the i-th sub-goal. In the practical implementation process, NL instructions specify some MES parameters, and the interpretation module recommends the remaining MES parameters.\n\u2200\ud835\udc56, \ud835\udc57 [ \u22c0 \ud835\udc8e\ud835\udc86\ud835\udc94\ud835\udc56\n\ud835\udc8f\n\ud835\udc8a=\ud835\udfcf\n\u2192 \ud835\udc94\ud835\udc96\ud835\udc83\ud835\udc88\ud835\udc90\ud835\udc82\ud835\udc8d\ud835\udc56(\ud835\udc61\ud835\udc4e\ud835\udc60\ud835\udc58 \ud835\udc57) ]\n(3)\nFig.4. Sub-goal interpretation by machine execution specifications (MES)\n(2) Execution Planning An execution workflow is the machine-executable task procedure consisting of each executable sub-goal generated from the previous step. The generation of this workflow relies on the task-centered cognitive logics that are learned from NL instructions. These logics are the key to enable the exploration of the operation correlations among the task sub-procedures given specific environmental conditions and human preferences. The task-centered logic framework is modeled by a Markov Logic Network (MLN) algorithm [49][50] (shown in equation (4)). In this algorithm, each sub-goal and sub-goal transition are expressed as logic clauses \ud835\udc53\ud835\udc56 with weighted contribution \ud835\udc64\ud835\udc56 towards task execution, \ud835\udc5b\ud835\udc56 denotes the total number of clauses \ud835\udc53\ud835\udc56 which are satisfied by the real-world condition x, the clause set is denoted by \u2131, and z is the total number of all the possible worlds of grounding the logic clauses with the real-world condition x. Different from hard constraints in first-order logic, soft constraints in a MLN describe the contribution of logic relations in a weighted manner, making the task execution flexible. This means, if some human-instructed logic transitions are inconsistent with the pre-trained logic transitions, the task is still likely to be executable with a lower value of executability, instead of being non-executable., including all logic clauses. A potential executable plan is generated by combining the grounded logic clauses according to human instructions. The optimal execution plan is the logic flow with the maximum weight accumulation. A sample MLN logic flow for the\ntask \u201cdrill\u2192clean\u201d is shown in equation (5). The corresponding execution feasibility is calculated by equation (6),\nwhere the numbers denote the corresponding contributions of the logic formulas \u201cDrill, TransitionFeasible, Clean\u201d towards the task \u201cdrill\u2192clean\u201d. Task \u201cdrill\u2192clean\u201d is executable only when its significant logics \u201cdrill is executed (Drill), clean is executed (Clean), and transition among drill and clean is feasible (TransitionFeasible)\u201d are satisfied, the task is executable with the executability 0.9.\n\ud835\udc43(\ud835\udc4b = \ud835\udc65)= 1\n\ud835\udc4d exp (\u2211 \ud835\udc64\ud835\udc56\ud835\udc5b\ud835\udc56(\ud835\udc65)\ud835\udc53\ud835\udc56\u2208\u2131 )\n(4)\nDrill(\ud835\udc5a\ud835\udc52\ud835\udc601)^TransitionFeasible(\ud835\udc5a\ud835\udc52\ud835\udc601,\ud835\udc5a\ud835\udc52\ud835\udc602)^Clean(\ud835\udc5a\ud835\udc52\ud835\udc602) \u21d2 Task(\ud835\udc5a\ud835\udc52\ud835\udc601,\ud835\udc5a\ud835\udc52\ud835\udc602) (5)\n0.3+0.3+0.3\u21d20.9 (6)\nThe most likely execution plan could be made by mapping the human-instructed procedure into MLN structures for different tasks and selecting the most reasonable procedure with the greatest execution feasibility. Execution planning is formally expressed by equation (7). As Fig. 5 shows, the green plan is the mapped human-described plan for a specific task.\n\ud835\udc66\ud835\udc5a\ud835\udc4e\ud835\udc65 = arg \ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc66\n1 \ud835\udc4d exp (\u2211 \ud835\udc64\ud835\udc56\ud835\udc5b\ud835\udc56(\ud835\udc65, \ud835\udc66)\ud835\udc53\ud835\udc56\u2208\u2131 )\n= arg \ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc66 \u2211 \ud835\udc64\ud835\udc56\ud835\udc5b\ud835\udc56(\ud835\udc65, \ud835\udc66)\ud835\udc53\ud835\udc56\u2208\u2131 (7)\nLearning the MLN representation for a task\u2019s execution includes two steps. The first step is defining the logic formulas for a task execution by domain experts. Detailed formula definitions are shown in section 4.1 task knowledge collection, Table 1. The second step is adopting a Structural Support Vector Machine (SSVM) method [50] to learn the formula involvements and their corresponding weights given a task. SSVM is good at learning complex structures, which are constructed by knowledge entity involvements and their weights [51][52]. In this paper given each task, each sub-goal only needs to be involved once to meet the basic execution requirements of a task. \ud835\udc5b\ud835\udc56 in equation (7) is represented as feature function \ud835\udef9(\ud835\udc65, \ud835\udc66) which means when a logic clause is true the feature function value is 1, otherwise 0. With real-world constraints, the weight learning problem of the MLN is formulized as a Quadratic Programming (QP) problem in which the optimal weights are solved to achieve the maximum mutual task differences. A SSVM aims to learn two important aspects of the MLN representation, logic formula involvements and their weights. Logic formulas for sub-goal transitions are defined by human experts and extracted by the task-centered semantic analysis. In MLN learning, our objective is to learn the optimal MLN structure S: \ud835\udcb3 \u2192 \ud835\udcb4 (training samples (x, y) \u2208 (\ud835\udcb3 \u00d7 \ud835\udcb4)\ud835\udc5b ), based on which the most executable plan \ud835\udcb4 (output) is predicted from NL-instructed task knowledge \ud835\udcb3 (input) with the smallest error.\nGiven that a plan\u2019s executability is accumulated from the products of feature involvements and their weights, equation (4) could be simplified into a discriminant function \ud835\udc53\ud835\udc64, shown in equation (8) where W is a weight matrix \u211b \ud835\udc41 and \ud835\udef9 is the feature function expressed by x and y. Then the MLN learning aims to find the weights W for minimizing the prediction error of equation (7). This weight learning problem could be formulized as a 1-slack SSVM problem and solved by an efficient cutting plane method [51], formulized by the objective function in equation (9), where C denotes the trade-off parameters between the error \ud835\udf09 and the margin W. Equation (10) denotes the constraint for the cutting-plane optimization problem to ensure that the discriminative value of the correct prediction \ud835\udc66\ud835\udc56 is greater than the discriminative value of the incorrect predictions \ud835\u0305\udc66\ud835\udc56. To simplify the learning process, the calculation is only conducted between the correctly-predicted task \ud835\udc66\ud835\udc56 and the most-executable incorrectly-predicted task \ud835\u0305\udc66\ud835\udc56, defined by equation (11). \u2206(\ud835\udc66\ud835\udc56 , \ud835\u0305\udc66\ud835\udc56) is the loss function, which is defined as the executability difference between tasks \ud835\udc66\ud835\udc56 \ud835\udc4e\ud835\udc5b\ud835\udc51 \ud835\u0305\udc66\ud835\udc56.\n\ud835\udc53\ud835\udc64(\ud835\udc65, \ud835\udc66)=\ud835\udc4a \ud835\udc47\ud835\udef9(\ud835\udc65, \ud835\udc66) (8)\nmin\n\ud835\udc4a,\ud835\udf09\u22650\n1 2 \ud835\udc4a\ud835\udc47\ud835\udc4a + \ud835\udc36 \ud835\udf09\n(9)\ns.t. \u2200(\ud835\udc66 1 , \ud835\udc66 2 , \u2026, \ud835\udc66 \ud835\udc5b ) \u2208 \ud835\udcb4\ud835\udc5b,\n1 \ud835\udc5b \ud835\udc4a\ud835\udc47 \u2211 (\ud835\udef9(\ud835\udc65\ud835\udc56 , \ud835\udc66\ud835\udc56) \u2212 \ud835\udef9(\ud835\udc65\ud835\udc56 , \ud835\u0305\udc66\ud835\udc56)) \ud835\udc5b \ud835\udc56=1 \u2265 1 \ud835\udc5b \u2211 \u2206(\ud835\udc66\ud835\udc56 , \ud835\u0305\udc66\ud835\udc56) \u2212 \ud835\udc5b \ud835\udc56=1 \ud835\udf09\n(10)\n\ud835\u0305\udc66\ud835\udc56 = argmax y\u2208{\ud835\udc661,\ud835\udc662,\u2026 }\n\ud835\udc64\ud835\udc47\u03a8(\ud835\udc65\ud835\udc56 , y)+\u2206((\ud835\udc66\ud835\udc56 , y)) (11)\n3.3. Executability Assessment To make the human-instructed plan executable in practical situations, executability for sub-goals, sub-goal\ntransitions and MES availabilities were assessed. If the MES is incomplete or the overall execution likelihood is lower than a threshold \u22020, the task is non-executable and the instruction interpretation process is triggered to make the involved sub-goals executable by filling in the missing MES parameters; otherwise the task will be executed. After interpretation, if the executability is still lower than the threshold \u22020, a human instructor will be notified of the execution failure.\nEvaluation\nTo evaluate exePlan\u2019s effectiveness in generating machine-executable plans, an advanced machine, the humanoid robot Baxter, was instructed to execute three types of tasks {drill, clean, install} at three different locations {center spot, upper-right spot, bottom-right spot} in a lab environment. The experiment platform was set up as shown in Fig. 6, where a human instructor orally commanded a robot to execute tasks on 3 targeted points on a working surface. The available tools included a microphone, a Kinect sensor, a brush, a screwdriver, and a driller. The detailed human-robot interaction process is described in the caption of Fig. 6. The three types of tasks {drill, clean, install} were assigned by the instructor based on his execution and expression habits. Depending on different task situations and different instructors, the NL instructions could vary, such as {clean a spot, drill a hole, install a screw, drill a hole \u2192 clean the dust, install a screw \u2192 clean the dust, drill a hole \u2192 install a screw, drill a hole \u2192 install a screw \u2192 clean the dust, drill a hole \u2192 clean the dust \u2192 install a screw, \u2026}. Therefore due to their variability, these three types of tasks were selected to evaluate the effectiveness of the exePlan to generate machine-executable plans in NL-based human-machine interaction.\nWith the above-mentioned experiment setup, we aimed to evaluate two main aspects of our exePlan method. First, the accuracy of task-related information extraction evaluated the effectiveness of instruction disambiguation. Second, the plan identification accuracy and plan executability evaluated the effectiveness of instruction interpretation."}, {"heading": "4.1. Task Knowledge Collection", "text": "To collect the MLN-based task knowledge for machines\u2019 executions, 800 volunteers were recruited on the\ncrowdsourcing website Amazon Mechanical Turk (AMT) [53] to give 2400 copies of NL instructions (about 4100 task-related sentences) for executing the three types of tasks mentioned in prior experiment setup. Two types of questions were asked in the instruction collection process. One type of questions was at the overall plan level, such as \u201cFor instructing a robot to perform the task \u2018install a screw at the upper-right spot\u2019, please give your step\u2013by-step instructions\u201d. The complete execution plan and some of the important execution parameters such as working spot location, necessary tools, and actions were provided by the volunteers. The second type of questions was at the MES level, such as \u201cplease describe the five types of MES parameters {location, actions sequence, tool usage, human requirement, working precondition} according to your expression habits\u201d. In this question, different MES parameters and their various expression manners for the three tasks were collected. With these instructions, the MLN structure and the related MES parameters were learned to represent the targeted tasks. In both MES collection and implementation, MES parameters were detected by the related keywords, such as \u201cscrewdriver, screw drive, screwdrivers, \u2026\u201d for the tool \u201cscrewdriver\u201d and \u201cclean, cleans, sweep, sweeping, remove, \u2026\u201d for the action \u201cclean\u201d.\nThe likely logic formulas for MLN representations of the three types of tasks were defined by two expert volunteers, shown in Table 1. With these logics, a task was decomposed into small sub-goals. To finish a task, some of these goals must be achieved by satisfying the corresponding logics. The weights of these predefined clauses were learned by a SSVM to construct the MLN representations. These logic formulas consist of three basic execution steps {DrillHole, CleanSpot, InstallScrew} and their mutual logic transitions. Given that one task could be executed by multiple execution manners, MLN learning aims to learn the relative importance of the logic formulas given a task. The benefits of learning formulas\u2019 relative importance is that some important formulas could be flexibly involved with the consideration of both their importance and internal logic relations in task planning, increasing task planning flexibility and accuracy.\nIn the semi-supervised entity detection, classifiers for three basic formulas {DrillHole, CleanSpot, InstallScrew} were trained. To filter out the irrelevant instructions, a fourth type of basic formula \u201cOther\u201d was added to denote the instructions that donot include the three basic logic formulas. A set of 50 sentence samples are manually labeled as seed samples for initially training the classifiers. A set of 4000 unlabeled sentence samples was prepared for further training the classifiers. Another new set of 50 samples was manually labeled for evaluating the classifiers\u2019 real-time performances during the semi-supervised learning process. After training, the top 20 influential semantic features for the classifiers of the four formulas are shown in Fig. 7. The influential features such as keywords \u201cdrill\u201d, keyword dependency \u201cmake+hole\u201d and next sub-goal \u201cclean\u201d given a basic formulas such as \u201cDrillHole\u201d are reasonable according to our NL expression habits.\nBased on the trained classifiers of the three basic formulas {DrillHole, CleanSpot, InstallScrew} as well as the 2400 copies of NL instructions, the weights of the defined formulas given each task were learned. The MLN representation for the three tasks is shown in Fig. 8. The corresponding formula features and their weights for different tasks are shown in Fig. 9. Taking the task type \u201cinstall\u201d as an example, the relative important logic formulas are \u201cInstallScrew\u201d and \u201ctransCI\u201d which denote that \u201cscrew\u201d and \u201ccleaning the surface\u201d are relatively important in finishing the \u201cinstall\u201d task. This result is consistent with our daily common sense. The collected MES parameters for the three basic formulas are shown in Table 2."}, {"heading": "4.2. Evaluation of Intruction Disambiguation", "text": "A new set of 20 volunteers were recruited to instruct the Baxter robot in the lab environment (shown in Fig. 6). Each of the volunteer was required to naturally describe three tasks for the robot to execute, shown in Fig. 10. Supported by the exePlan method, the Baxter robot was able to understand task-related logics from the ambiguous human instructions. To evaluate the performances of the task-centered semantic analysis by using the exePlan method, an algorithm-level baseline was selected as Na\u00efve Bayesian (NB) [55] which is efficient and classic in performing classification and a method-level baseline was selected as nonTC [8][54] which only considers the keyword-related features in semantic understanding. Accuracy of task-related logic formula detection was used to evaluate exePlan\u2019s effectiveness in understanding a task from the ambiguous NL instructions. Accuracy was assessed by precision and recall. Recall was calculated by the percentage of the machine-extracted entities in the overall involved entities. Precision was calculated by the percentage of correctly identified entities in the overall identified entities. As Table 3 shows, TC\u2019s performance was good in both the SVM-based method and NB-based method, with recall higher than 0.97 and precision higher than 0.96. The TC method consistently outperformed the nonTC method in both recall\n\u201cclean, drill, install\u201d. For a type of task such as \u201cdrill\u201d, different logic formulas {CleanSpot, DrillHole, InstallScrew, TranCD, TranDC, TranCI, TranDI, TranID} contribute to the task executability with different weights {0.009\uff0c 0.243\uff0c 0.077\uff0c 0.169\uff0c 0.126\uff0c 0.104\uff0c 0.165\uff0c 0.108\uff0c 0.002}. The formulas \u201cdrill a hole (DrillHole), transition from \u2018clean\u2019 to \u2018drill\u2019 (transCD)\u201d are relatively important for a \u201cdrill\u201d task.\n(greater than 0.03) and precision (greater than 0.02) under the support of either the SVM algorithm or NB algorithm. Both the good performance and the relative advantages proved the effectiveness of exePlan in instruction disambiguation. In addition, SVM-based TC outperformed NB-based TC, proving SVM was more suitable than NB in understanding various task expressions. The disambiguation samples of task-related logic formulas are shown in Table 4. Based on keywords such as \u201cremove\u201d, the potential sentences were located. Then based on the task-centered semantic analysis, the task-related sub-goals such as \u201cCleanSpot\u201d in sentence \u201cremove the dust in middle with a brush\u201d were identified due to the linguistic features such as {key word: remove, dust; pos:vb, nn; \u2026} and semantic features {MES action \u201cremove\u201d+ corresponding context \u201cdust\u201d}; while the irrelevant sentences such as \u201cwait, I need\nto remove the tools on the surface\u201d were filtered out due to the semantic features {incorrect task context: \u201ctools\u201d, \u201ctools+on surface\u201d}."}, {"heading": "4.3. Evaluation of Intruction Interpretation and Executability Assessment", "text": "Based on the extracted task-related logic formulas in instruction disambiguation, instruction interpretation was conducted by using exePlan to \u201ctranslate\u201d abstract NL instructions into machine-executable plans. Using these plans, the Baxter robot executed the instructor-assigned tasks. To evaluate the performance of the machine-execution-specification method included in exePlan, a baseline method was selected as Literally Interpretation method (\u201cLI\u201d for short), which was implemented in recent research of NL-based human-robot interactions [8][54]. In the LI method a plan was interpreted by literally understanding NL instructions, and the plan type was identified by strictly following human instructions in a first-order logic manner. For example, drill\u2192clean denotes task type \u201cclean\u201d. MES detection in this baseline method was keyword-based. Different from the LI method with literally task understanding, the exePlan method classified the plan type first and then selected the appropriate MES parameters for each logic formulas included in the identified plan. Each method\u2019s performance of instruction interpretation was assessed by identification precision/recall and MES/plan interpretation accuracy.\nIn plan identification, recall was calculated by the percentage of correctly-identified plans in all the volunteer-instructed plans. Precision was calculated by the percentage of correctly-identified specific-type plans in all the predicted specific-type plans. As Table 4 shows, exePlan\u2019s performance was good with an average precision of 0.93 and recall of 0.95. It outperformed the LI method by recall of 0.26 and precision of 0.28, proving the effectiveness of exePlan in identifying the task type.\nIn MES/plan interpretation evaluation, both the MES mapping accuracy and executable-plan proportion were calculated. MES mapping recall denoted the percentage of correctly-extracted MES in all the instructed MES and MES mapping precision denoted the percentage of correctly-extracted MES in all the extracted MES. Executable plan proportion denoted the percentage of the machine-executable plans after the plan interpretation. The executability threshold \u22020 of the generated plan was 0.5, only greater than which a plan could be considered as machine-executable. As Table 5 shows, in both interpretation methods, MES mapping accuracy was good while the exePlan outperformed the LI method by precision of 0.3 and recall of 0.15, proving both the keyword-based method and the classification-based method could accurately find the available MES parameters. For the executable plan proportion, exePlan\u2019s performance was good and largely outperformed the LI method. The big difference in executable-plan generation could be explained by that usually in NL instructions due to the instructor\u2019s expression habits, some crucial MES parameters was missing. Even if all the mentioned MES parameters could be identified accurately, the generated\nplan was still machine-non-executable without instruction interpretation. The better performance in MES mapping and machine-executable plan generation proved that exePlan was effective in instruction interpretation.\nThe purpose of designing an executability assessment module is to detect non-executable plans and sub-goals. The detection accuracy depends on the threshold setting. Therefore, in the evaluation of the executability assessment module, only two general aspects were assessed: 1). the necessity of checking plan/sub-goal executability. 2). the effectiveness of checking plan/sub-goal executability. Based on the literal interpretation method that generated plans/sub-goals based on the information literally contained in the NL instructions, only 3% of the plans are executable. After being assessed by the executability module and launching the plan/sub-goal interpretation, the proportion of executable plans was increased to 95%. This significant increase proved that the module was necessary and effective in detecting the non-executable plans/sub-goals.\nConclusions & Future Work\nIn this paper, we have developed an exePlan method to generate machine-executable plans from ambiguous and abstract NL instructions. Two main problems have been solved. One is understanding of ambiguous instructions. To solve this problem, a task-centered semantic analysis method was developed. By understanding NL instructions given by 20 volunteers to an advanced Baxter robot, the effectiveness of semantic analysis of the exePlan method was validated. The second problem is interpretation of abstract NL instructions. To make the abstract task instruction executable by a machine, a machine-execution-specification method was developed to map the abstract NL\ninstructions with the correct execution plan and the detailed MES parameters. By interpreting the abstract task instructions into machine-executable plans in the experiments, the effectiveness of this interpretation method was validated.\nIn future work, focus will be placed on knowledge mapping from NL instruction to the real world by considering\nmore complete constraints such as human/environmental factors, further making our exePlan practical.\nAcknowledgements\nWe would like to thank Dr. Hao Zhang for providing the Baxter robot to help us with the experiments. We also\nwould like to thank Mr. Jeremy Webb for assisting us with the experiment setting up."}], "references": [{"title": "Collaborative Manufacturing with physical human-robot interaction", "author": ["A. Cherubini", "R. Passama", "A. Crosnier", "A. Lasnier", "P. Fraisse"], "venue": "Robotics and Computer-Integrated Manufacturing,vol.40, pp. 1-13, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Recovering from failure by asking for help.", "author": ["R.A. Knepper", "S. Tellex", "A. Li", "N. Roy", "D. Rus"], "venue": "Autonomous Robots,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The Motion Grammar: Analysis of a Linguistic Method for Robot Control", "author": ["N. Dantam", "M. Stilman"], "venue": "IEEE Transactions on Robotics, vol. 29, no. 3, pp. 704-718, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Facilitating knowledge sharing and reuse in building and construction domain: an ontology-based approach.", "author": ["R. Costa", "C. Lima", "J. Sarraipa", "R. Jardim-Gon\u00e7alves"], "venue": "Journal of Intelligent Manufacturing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Automated generation of training sets for object recognition in robotic applications", "author": ["M. Schoeler", "F. W\u00f6rg\u00f6tter", "M.J. Aein", "T. Kulvicius"], "venue": "Robotics in Alpe-Adria-Danube Region 23rd International Conference, 2014, pp 1-7.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language programming of industrial robots", "author": ["M. Stenmark", "P. Nugues"], "venue": "In Robotics 44th International Symposium, 2013, pp. 1-5.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Effective and Robust Natural Language Understanding for Human-Robot Interaction", "author": ["E. Bastianelli", "G. Castellucci", "D. Croce", "R. Basili", "D. Nardi"], "venue": "ECAI, 2014, pp. 57-62.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Hand force adjustment: robust control of force-coupled human\u2013robot-interaction in assembly processes", "author": ["J. Kruger", "D. Surdilovic"], "venue": "CIRP Annals - Manufacturing Technology, vol. 57, no. 1, pp. 41-44, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "An Analysis of Contact Instability in Terms of Passive Physical Equivalents", "author": ["E. Colgate", "N. Hogan"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 404-409, 1989.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1989}, {"title": "Hand Gesture-Based Manipulation of a Personalized Avatar Robot in Remote Communication", "author": ["T. Ito"], "venue": "Symposium on Human Interface, pp. 425-434, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "A Proposed Gesture Set for the Control of Industrial Collaborative Robots", "author": ["P. Barattini", "C. Morand", "N. Robertson"], "venue": "IEEE International Symposium on Robot and Human Interactive Communication (IEEE RO-MAN), PP. 132-137, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Augmented reality applications in design and manufacturing", "author": ["A. Nee", "S. Ong", "G. Chryssolouris", "D. Mourtzis"], "venue": "CIRP Annals - Manufacturing Technology, vol. 61, no. 2, pp. 657-679, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Assembly Strategy Modeling and Selection for Human and Robot Coordinated Cell Assembly", "author": ["F. Chen", "K. Sekiyama", "H. Sasaki", "J. Huang", "B. Sun", "T. Fukuda"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4670-4675, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding Human Behaviors with an Object Functional Role Perspective for Robotics", "author": ["R. Liu", "X. Zhang"], "venue": "IEEE Transactions on Cognitive and Developmental Systems, vol. 8, no. 2, pp. 115-127, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Fuzzy-Context-Specific Intention Inference for Robotic Caregiving", "author": ["R. Liu", "X. Zhang"], "venue": "International Journal of Advanced Robotic System, DOI: 10.1177/1729881416662780, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Make it so: Continuous, flexible natural language interaction with an autonomous robot", "author": ["D.J. Brooks", "C. Lignos", "C. Finucane", "M.S. Medvedev", "I. Perera", "V. Raman", "H. Kress-Gazit", "M. Marcus", "H.A. Yanco"], "venue": "AAAI, pp. 2-8, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "An effective personal mobile robot agent through symbiotic human-robot interaction", "author": ["S. Rosenthal", "J. Biswas", "M. Veloso"], "venue": "AAMAS 2010, pages 915-922, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Open Language Learning for Information Extraction", "author": ["M. Schmitz", "R. Bart", "S. Soderland", "O. Etzioni"], "venue": "2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 523-534, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit", "author": ["C. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S. Bethard", "D. McClosky"], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstration, pp. 55-60, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Action database for categorizing and inferring human poses from video sequences", "author": ["W. Takano", "Y. Nakamura"], "venue": "Robotics and Autonomous Systems, vol. 70, pp. 116-125, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "A Probabilistic Approach for Enabling Robots to Acquire Information From Human Partners Using Language", "author": ["S. Tellex", "P. Thaker", "R. Deits", "D. Simeonov", "T. Kollar", "N. Roy"], "venue": "http://hdl.handle.net/1721.1/68651, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Nardi,\u201cTeaching Robots Parametrized Executable Plans Through Spoken Interaction,", "author": ["G. Gemignani", "E. Bastianelli"], "venue": "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Spoken Language Processing in a Conversational System for Child-Robot Interaction", "author": ["I. Kruijff-Korbayova", "H. Cuayahuitl", "B. Kiefer", "M. Schroder", "P. Cosi", "G. Paci", "G. Sommavilla", "F. Tesser", "H.Sahli", "G. Athanasopoulos", "W. Wang"], "venue": "Workshop on Child, Computer and Interaction (WOCCI), pp. 32-39, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "An Emotional Storyteller Robot", "author": ["A. Chella", "R.E. Barone", "G. Pilato", "R. Sorbello"], "venue": "AAAI Spring Symposium: Emotion, Personality, and Social Behavior, pp.17-22, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Toward Open Knowledge Enabling for Human- Robot Interaction", "author": ["X. Chen", "J. Xie", "J. Ji", "Z. Sui"], "venue": "Journal of Human-Robot Interaction,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Natural-Language-Instructed Industrial Task Execution", "author": ["R. Liu", "J. Webb", "X. Zhang"], "venue": "Proceedings of the ASME 2016 International Design Engineering Technical Conferences & Computers & Information in Engineering Conference (IDETC/CIE 2016), August 21-24, 2016, Charlotte, North Carolina, USA.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "The production and comprehension of referring expressions", "author": ["E. Graf", "C. Davies"], "venue": "Pragmatic Development in First Language Acquisition, pp. 161, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Going Beyond Literal Command-Based Instructions: Extending Robotic Natural Language Interaction Capabilities", "author": ["T. Williams", "G. Briggs", "B. Oosterveld", "M. Scheutz"], "venue": "AAAI, pp. 1387-1393, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Tell me Dave: Context-sensitive grounding of natural language to mobile manipulation instructions", "author": ["D.K. Misra", "J. Sung", "K. Lee", "A. Saxena"], "venue": "Robotics: Science and Systems, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Context-specific intention awareness through web query in robotic caregiving", "author": ["R. Liu", "X. Zhang", "J. Webb", "S. Li"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 1962-1967, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1962}, {"title": "Context-Specific Grounding of Web Natural Descriptions to Human-centered Situations", "author": ["R. Liu", "X. Zhang"], "venue": "Knowledge-based Systems, in press July 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "An interactive approach for situated task specification through verbal instructions", "author": ["C. Meri\u00e7li", "S.D. Klee", "M.J. Paparian", "Veloso"], "venue": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, pp. 1069-1076, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Inferring maps and behaviors from natural language instructions", "author": ["F. Duvallet", "M.R. Walter", "T. Howard", "S. Hemachandra", "J. Oh", "S. Teller", "N. Roy", "A. Stentz"], "venue": "Experimental Robotics, pp. 373-388, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning the spatial semantics of manipulation actions through preposition grounding", "author": ["K. Zampogiannis", "Y. Yang", "C. Fermuller", "Y. Aloimonos"], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference, pp. 1389-1396, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to recognize novel objects in one shot through human-robot interactions in natural language dialogues", "author": ["E.A. Krause", "M. Zillich", "T.E. Williams", "M. Sche utz"], "venue": "AAAI, pp. 2796-2802, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic representation for navigation in large-scale environments", "author": ["R. Drouilly", "P. Rives", "B. Morisset"], "venue": "IEEE International Conference Robotics and Automation (ICRA), pp. 1106-1111, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech and language Processing, 2nd ed", "author": ["D. Jurafsky", "J.H. Martin"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Toward interpreting spatial language discourse with grounding graphs", "author": ["D. Simeonov", "S. Tellex", "T. Kollar", "N. Roy"], "venue": "RSS Workshop on Grounding Human-Robot Dialog for Spatial Tasks, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Cakmak,\u201cRobot programming by demonstration with situated spatial language understanding,", "author": ["M. Forbes", "R.P. Rao", "L. Zettlemoyer"], "venue": "IEEE Conference,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Grounding verbs of motion in natural language commands to robots", "author": ["T. Kollar", "S. Tellex", "D. Roy", "N. Roy"], "venue": "Experimental Robotics, Springer Berlin Heidelberg, pp. 31-47, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Using semantic fields to model dynamic spatial relations in a robot architecture for natural language instruction of service robots", "author": ["J. Fasola", "M.J. Mataric"], "venue": "Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference, pp. 143-150, 2013.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Imitation learning for natural language direction following through unknown environments", "author": ["F. Duvallet", "T. Kollar", "A. Stentz"], "venue": "Robotics and Automation (ICRA), 2013 IEEE International Conference, pp. 1047-1053, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised and Semi-supervised Multi-class Support Vector Machines", "author": ["L. Xu", "D. Schuurmans"], "venue": "AAAI Conference on Artificial Intelligence, vol. 5, 2005.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "SVM soft margin classifiers: liner programming versus quadratic programming", "author": ["Q. Wu", "D. Zhou"], "venue": "Neural Computation, vol. 17, no. 5, pp. 11160-1187, 2005.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning spatial-semantic representations from natural language descriptions and scene classifications", "author": ["S. Hemachandra", "M.R. Walter", "S. Tellex", "S. Teller"], "venue": "IEEE International Conference Robotics and Automation (ICRA), pp. 2623-2630, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning semantic maps from natural language descriptions", "author": ["M.R. Walter", "S. Hemachandra", "B. Homberg", "S. Tellex", "S. Teller"], "venue": "Proc. Robotics: Science and Systems (RSS), 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Markov Logic Networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning, vol. 62, no. 1-2, pp. 107-136, 2006.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}, {"title": "Max-Margin Weight Learning for Markov Logic Networks", "author": ["T. Huynh", "R. mooney"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 564-579, 2009.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Cutting-Plane Training of Structural SVM", "author": ["T. Joachims", "T. Finley"], "venue": "Machine Learning, vol. 77, no. 1, pp.27-59, 2009.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "Training Structural SVMs when Exact Inference is Intractable", "author": ["T. Joachims", "T. Finley"], "venue": "International conference on Machine learning, pp. 304-311, 2008.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to Parse Natural Language to a Robot Control System", "author": ["C. Matuszek", "E. Herbst", "L. Zettlemoyer", "D. Fox"], "venue": "Experimental Robotics, pp. 403-415, 2013.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": ", precision and speed) on low-level task execution [1].", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Toward this direction, intuitive and natural communication between the human and the machine has been an active research area in the last decade with the goal to enable seamless human-machine cooperation [2][3].", "startOffset": 204, "endOffset": 207}, {"referenceID": 2, "context": "Toward this direction, intuitive and natural communication between the human and the machine has been an active research area in the last decade with the goal to enable seamless human-machine cooperation [2][3].", "startOffset": 207, "endOffset": 210}, {"referenceID": 3, "context": "Natural-Language-instructed human-machine interaction is expected to enable an advanced manufacturing machine, such as a Computer Numerical Control machine or an industrial robot, to autonomously perform tasks such as rough/fine finishing [4][5], assembly [2][6] and packaging [7][8] according to the end-user\u2019s NL instructions, which are given based on the user\u2019s judgement of the task progress and environmental situations.", "startOffset": 242, "endOffset": 245}, {"referenceID": 1, "context": "Natural-Language-instructed human-machine interaction is expected to enable an advanced manufacturing machine, such as a Computer Numerical Control machine or an industrial robot, to autonomously perform tasks such as rough/fine finishing [4][5], assembly [2][6] and packaging [7][8] according to the end-user\u2019s NL instructions, which are given based on the user\u2019s judgement of the task progress and environmental situations.", "startOffset": 256, "endOffset": 259}, {"referenceID": 4, "context": "Natural-Language-instructed human-machine interaction is expected to enable an advanced manufacturing machine, such as a Computer Numerical Control machine or an industrial robot, to autonomously perform tasks such as rough/fine finishing [4][5], assembly [2][6] and packaging [7][8] according to the end-user\u2019s NL instructions, which are given based on the user\u2019s judgement of the task progress and environmental situations.", "startOffset": 259, "endOffset": 262}, {"referenceID": 5, "context": "Natural-Language-instructed human-machine interaction is expected to enable an advanced manufacturing machine, such as a Computer Numerical Control machine or an industrial robot, to autonomously perform tasks such as rough/fine finishing [4][5], assembly [2][6] and packaging [7][8] according to the end-user\u2019s NL instructions, which are given based on the user\u2019s judgement of the task progress and environmental situations.", "startOffset": 277, "endOffset": 280}, {"referenceID": 6, "context": "Natural-Language-instructed human-machine interaction is expected to enable an advanced manufacturing machine, such as a Computer Numerical Control machine or an industrial robot, to autonomously perform tasks such as rough/fine finishing [4][5], assembly [2][6] and packaging [7][8] according to the end-user\u2019s NL instructions, which are given based on the user\u2019s judgement of the task progress and environmental situations.", "startOffset": 280, "endOffset": 283}, {"referenceID": 7, "context": "Compared with other input methods, including human hand force [9][10], hand gesture [11][12], and body motions [13][14][15][16], the NL instruction method has two main advantages.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "Compared with other input methods, including human hand force [9][10], hand gesture [11][12], and body motions [13][14][15][16], the NL instruction method has two main advantages.", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "Compared with other input methods, including human hand force [9][10], hand gesture [11][12], and body motions [13][14][15][16], the NL instruction method has two main advantages.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "Compared with other input methods, including human hand force [9][10], hand gesture [11][12], and body motions [13][14][15][16], the NL instruction method has two main advantages.", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "Compared with other input methods, including human hand force [9][10], hand gesture [11][12], and body motions [13][14][15][16], the NL instruction method has two main advantages.", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "Compared with other input methods, including human hand force [9][10], hand gesture [11][12], and body motions [13][14][15][16], the NL instruction method has two main advantages.", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Compared with other input methods, including human hand force [9][10], hand gesture [11][12], and body motions [13][14][15][16], the NL instruction method has two main advantages.", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "Compared with other input methods, including human hand force [9][10], hand gesture [11][12], and body motions [13][14][15][16], the NL instruction method has two main advantages.", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "Non-expert users without prior programming training could command a machine to perform their desired tasks [17][18].", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "Non-expert users without prior programming training could command a machine to perform their desired tasks [17][18].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "Second, the inherent linguistic structure of NL, as a predefined information encoder, provides a standard, informative data source to generate structured machine language [19][20].", "startOffset": 171, "endOffset": 175}, {"referenceID": 18, "context": "Second, the inherent linguistic structure of NL, as a predefined information encoder, provides a standard, informative data source to generate structured machine language [19][20].", "startOffset": 175, "endOffset": 179}, {"referenceID": 2, "context": "Currently, typical industrial applications involving NL include NL-based control in which the working statuses such as \u201con/off\u201d and \u201cquickly/slowly\u201d are selected orally to control a machine in navigation [3][21], NL-based task execution in which the task operation methods such as \u201cgoTo + Location; then drop + object\u201d is described orally to help a machine with object finding/placing [22][23], and NL-based execution personalization in which human\u2019s preferences and moods in oral dialogs were considered to adjust a machine\u2019s execution manners [24][25].", "startOffset": 204, "endOffset": 207}, {"referenceID": 19, "context": "Currently, typical industrial applications involving NL include NL-based control in which the working statuses such as \u201con/off\u201d and \u201cquickly/slowly\u201d are selected orally to control a machine in navigation [3][21], NL-based task execution in which the task operation methods such as \u201cgoTo + Location; then drop + object\u201d is described orally to help a machine with object finding/placing [22][23], and NL-based execution personalization in which human\u2019s preferences and moods in oral dialogs were considered to adjust a machine\u2019s execution manners [24][25].", "startOffset": 207, "endOffset": 211}, {"referenceID": 20, "context": "Currently, typical industrial applications involving NL include NL-based control in which the working statuses such as \u201con/off\u201d and \u201cquickly/slowly\u201d are selected orally to control a machine in navigation [3][21], NL-based task execution in which the task operation methods such as \u201cgoTo + Location; then drop + object\u201d is described orally to help a machine with object finding/placing [22][23], and NL-based execution personalization in which human\u2019s preferences and moods in oral dialogs were considered to adjust a machine\u2019s execution manners [24][25].", "startOffset": 385, "endOffset": 389}, {"referenceID": 21, "context": "Currently, typical industrial applications involving NL include NL-based control in which the working statuses such as \u201con/off\u201d and \u201cquickly/slowly\u201d are selected orally to control a machine in navigation [3][21], NL-based task execution in which the task operation methods such as \u201cgoTo + Location; then drop + object\u201d is described orally to help a machine with object finding/placing [22][23], and NL-based execution personalization in which human\u2019s preferences and moods in oral dialogs were considered to adjust a machine\u2019s execution manners [24][25].", "startOffset": 389, "endOffset": 393}, {"referenceID": 22, "context": "Currently, typical industrial applications involving NL include NL-based control in which the working statuses such as \u201con/off\u201d and \u201cquickly/slowly\u201d are selected orally to control a machine in navigation [3][21], NL-based task execution in which the task operation methods such as \u201cgoTo + Location; then drop + object\u201d is described orally to help a machine with object finding/placing [22][23], and NL-based execution personalization in which human\u2019s preferences and moods in oral dialogs were considered to adjust a machine\u2019s execution manners [24][25].", "startOffset": 545, "endOffset": 549}, {"referenceID": 23, "context": "Currently, typical industrial applications involving NL include NL-based control in which the working statuses such as \u201con/off\u201d and \u201cquickly/slowly\u201d are selected orally to control a machine in navigation [3][21], NL-based task execution in which the task operation methods such as \u201cgoTo + Location; then drop + object\u201d is described orally to help a machine with object finding/placing [22][23], and NL-based execution personalization in which human\u2019s preferences and moods in oral dialogs were considered to adjust a machine\u2019s execution manners [24][25].", "startOffset": 549, "endOffset": 553}, {"referenceID": 6, "context": "NL is usually polysemous, homophonic and expression-manner diverse so that the same meaning could be expressed in various ways, and different meanings could be expressed in similar ways [8][26].", "startOffset": 186, "endOffset": 189}, {"referenceID": 24, "context": "NL is usually polysemous, homophonic and expression-manner diverse so that the same meaning could be expressed in various ways, and different meanings could be expressed in similar ways [8][26].", "startOffset": 189, "endOffset": 193}, {"referenceID": 25, "context": "For example, \u201cdrill a hole\u201d could be expressed as \u201cbore one hole\u201d, \u201cdrilling one bore\u201d, \u201ccreate an unthreaded hole\u201d, and so on [27].", "startOffset": 127, "endOffset": 131}, {"referenceID": 20, "context": "In addition, humans usually use referring, outlining, and omitting in NL instructions [22][28].", "startOffset": 86, "endOffset": 90}, {"referenceID": 26, "context": "In addition, humans usually use referring, outlining, and omitting in NL instructions [22][28].", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "cannot be known merely from a word \u2018the\u2019 [27].", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "Second, human instruction is abstract [23][29].", "startOffset": 38, "endOffset": 42}, {"referenceID": 27, "context": "Second, human instruction is abstract [23][29].", "startOffset": 42, "endOffset": 46}, {"referenceID": 25, "context": "\u201d is missing [27].", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "In addition to specific-knowledge missing, a reasonable and flexible knowledge structure, which is implicitly embedded in NL descriptions to guide correct task execution, is difficult to extract [30][31][32].", "startOffset": 195, "endOffset": 199}, {"referenceID": 29, "context": "In addition to specific-knowledge missing, a reasonable and flexible knowledge structure, which is implicitly embedded in NL descriptions to guide correct task execution, is difficult to extract [30][31][32].", "startOffset": 199, "endOffset": 203}, {"referenceID": 30, "context": "In addition to specific-knowledge missing, a reasonable and flexible knowledge structure, which is implicitly embedded in NL descriptions to guide correct task execution, is difficult to extract [30][31][32].", "startOffset": 203, "endOffset": 207}, {"referenceID": 31, "context": "By obeying the human instructions, one task could be flexibly executed by several methods, which were formulated according to an individual\u2019s cognitive logics [33][34].", "startOffset": 159, "endOffset": 163}, {"referenceID": 32, "context": "By obeying the human instructions, one task could be flexibly executed by several methods, which were formulated according to an individual\u2019s cognitive logics [33][34].", "startOffset": 163, "endOffset": 167}, {"referenceID": 1, "context": "However, usually these cognitive logics in NL instructions are difficult to understand as to a machine, for that literal information directly extracted from NL instructions is insufficient to explain the logics [2][35].", "startOffset": 211, "endOffset": 214}, {"referenceID": 33, "context": "However, usually these cognitive logics in NL instructions are difficult to understand as to a machine, for that literal information directly extracted from NL instructions is insufficient to explain the logics [2][35].", "startOffset": 214, "endOffset": 218}, {"referenceID": 6, "context": "For example, in the sentence \u201cbring the can in the trash bin\u201d the task goal \u201cin the trash bin\u201d was extracted based on the keywords \u201cbring, can\u201d and their corresponding PoS tags \u201cVB, NN\u2019[8].", "startOffset": 185, "endOffset": 188}, {"referenceID": 34, "context": "\u201cAttached\u201d was the constraint relation between the object \u201ccontainer\u201d and object part \u201chandle\u201d [36][37].", "startOffset": 95, "endOffset": 99}, {"referenceID": 35, "context": "\u201cAttached\u201d was the constraint relation between the object \u201ccontainer\u201d and object part \u201chandle\u201d [36][37].", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "Pick it up\u201d for an example, with co-reference resolution the uncertain expression \u201cit\u201d was identified as \u201cthe second crate on the right\u201d [22][38].", "startOffset": 137, "endOffset": 141}, {"referenceID": 36, "context": "Pick it up\u201d for an example, with co-reference resolution the uncertain expression \u201cit\u201d was identified as \u201cthe second crate on the right\u201d [22][38].", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "a robot, a query such as \u201cwhich pallet?\u201d was launched to ask the human for disambiguation [2][39].", "startOffset": 90, "endOffset": 93}, {"referenceID": 37, "context": "a robot, a query such as \u201cwhich pallet?\u201d was launched to ask the human for disambiguation [2][39].", "startOffset": 93, "endOffset": 97}, {"referenceID": 34, "context": "By exploring the features such as perceivable properties \u201ccylindrical\u201d and \u201cround\u201d, the ambiguous descriptions \u201ccylindrical container with a round handle attached on one side\u201d for the object \u201ccontainer\u201d was understood [36].", "startOffset": 218, "endOffset": 222}, {"referenceID": 38, "context": "By exploring the spatial relations \u201cbehind\u201d in NL descriptions \u201cNavigate to the building behind the pole\u201d, named entities \u201cbuilding, pole\u201d were identified in the real world [40][41].", "startOffset": 173, "endOffset": 177}, {"referenceID": 39, "context": "By exploring the spatial relations \u201cbehind\u201d in NL descriptions \u201cNavigate to the building behind the pole\u201d, named entities \u201cbuilding, pole\u201d were identified in the real world [40][41].", "startOffset": 177, "endOffset": 181}, {"referenceID": 2, "context": "To interpret abstract expressions in NL instructions, motion grammars were first designed for establishing the word-action correlations such as word \u201cgrasp\u201d \u2014 action \u201cGrasp\u201d [3][21][23].", "startOffset": 174, "endOffset": 177}, {"referenceID": 19, "context": "To interpret abstract expressions in NL instructions, motion grammars were first designed for establishing the word-action correlations such as word \u201cgrasp\u201d \u2014 action \u201cGrasp\u201d [3][21][23].", "startOffset": 177, "endOffset": 181}, {"referenceID": 21, "context": "To interpret abstract expressions in NL instructions, motion grammars were first designed for establishing the word-action correlations such as word \u201cgrasp\u201d \u2014 action \u201cGrasp\u201d [3][21][23].", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "Real-world preconditions such as \u201cstay in the kitchen\u201d were defined for triggering specific types of executions such as \u201cvisiting the kitchen\u201d [17].", "startOffset": 143, "endOffset": 147}, {"referenceID": 40, "context": "The NL descriptions were marked by landmark objects such as \u201cstaircase, box\u201d in the real world to enable the execution of tasks such as \u201creach in a spot\u201d [42][43].", "startOffset": 154, "endOffset": 158}, {"referenceID": 41, "context": "The NL descriptions were marked by landmark objects such as \u201cstaircase, box\u201d in the real world to enable the execution of tasks such as \u201creach in a spot\u201d [42][43].", "startOffset": 158, "endOffset": 162}, {"referenceID": 20, "context": "50)}[22][43].", "startOffset": 4, "endOffset": 8}, {"referenceID": 41, "context": "50)}[22][43].", "startOffset": 8, "endOffset": 12}, {"referenceID": 44, "context": "In [46][47], a semantic topological model was developed to explore the internal logic correlations of sub-steps in a reasonable task-execution plan.", "startOffset": 3, "endOffset": 7}, {"referenceID": 45, "context": "In [46][47], a semantic topological model was developed to explore the internal logic correlations of sub-steps in a reasonable task-execution plan.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "Second, text corpus is split into independent sentences, words, PoS tags and dependences by using the NLP tool Stanford CoreNLP [20].", "startOffset": 128, "endOffset": 132}, {"referenceID": 42, "context": "Given that the amount of the text corpus is potentially large and the desired output predictions (sub-goals involved in task plans) are multiple, a semi-supervised multi-class Support Vector Machine algorithm (smSVM) [44] is adopted.", "startOffset": 217, "endOffset": 221}, {"referenceID": 30, "context": "A semi-supervised classification method merely needs a small amount of labeled samples (usually 1%~5% of the total samples) for classifier training [32].", "startOffset": 148, "endOffset": 152}, {"referenceID": 43, "context": "intercept value bsm \u2217 are solved by a quadratic programming (QP) solver [45] and they define an optimal hyperplane to", "startOffset": 72, "endOffset": 76}, {"referenceID": 46, "context": "The task-centered logic framework is modeled by a Markov Logic Network (MLN) algorithm [49][50] (shown in equation (4)).", "startOffset": 87, "endOffset": 91}, {"referenceID": 47, "context": "The task-centered logic framework is modeled by a Markov Logic Network (MLN) algorithm [49][50] (shown in equation (4)).", "startOffset": 91, "endOffset": 95}, {"referenceID": 47, "context": "The second step is adopting a Structural Support Vector Machine (SSVM) method [50] to learn the formula involvements and their corresponding weights given a task.", "startOffset": 78, "endOffset": 82}, {"referenceID": 48, "context": "SSVM is good at learning complex structures, which are constructed by knowledge entity involvements and their weights [51][52].", "startOffset": 118, "endOffset": 122}, {"referenceID": 49, "context": "SSVM is good at learning complex structures, which are constructed by knowledge entity involvements and their weights [51][52].", "startOffset": 122, "endOffset": 126}, {"referenceID": 48, "context": "This weight learning problem could be formulized as a 1-slack SSVM problem and solved by an efficient cutting plane method [51], formulized by the objective function in equation (9), where C denotes the trade-off parameters between the error \u03be and the margin W.", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "To evaluate the performances of the task-centered semantic analysis by using the exePlan method, an algorithm-level baseline was selected as Na\u00efve Bayesian (NB) [55] which is efficient and classic in performing classification and a method-level baseline was selected as nonTC [8][54] which only considers the keyword-related features in semantic understanding.", "startOffset": 276, "endOffset": 279}, {"referenceID": 50, "context": "To evaluate the performances of the task-centered semantic analysis by using the exePlan method, an algorithm-level baseline was selected as Na\u00efve Bayesian (NB) [55] which is efficient and classic in performing classification and a method-level baseline was selected as nonTC [8][54] which only considers the keyword-related features in semantic understanding.", "startOffset": 279, "endOffset": 283}, {"referenceID": 6, "context": "machine-execution-specification method included in exePlan, a baseline method was selected as Literally Interpretation method (\u201cLI\u201d for short), which was implemented in recent research of NL-based human-robot interactions [8][54].", "startOffset": 222, "endOffset": 225}, {"referenceID": 50, "context": "machine-execution-specification method included in exePlan, a baseline method was selected as Literally Interpretation method (\u201cLI\u201d for short), which was implemented in recent research of NL-based human-robot interactions [8][54].", "startOffset": 225, "endOffset": 229}], "year": 2016, "abstractText": "It is critical for advanced manufacturing machines to autonomously execute a task by following an end-user\u2019s natural language (NL) instructions. However, NL instructions are usually ambiguous and abstract so that the machines may misunderstand and incorrectly execute the task. To address this NL-basedso that the machines may misunderstand and incorrectly execute the task. To address this NL-based human-machine communication problem and enable the machines to appropriately execute tasks by following the end-user\u2019s NL instructions, we developed a Machine-Executable-Plan-Generation (exePlan) method. The exePlan method conducts task-centered semantic analysis to extract task-related information from ambiguous NL instructions. In addition, the method specifies machine execution parameters to generate a machine-executable plan by interpreting abstract NL instructions. To evaluate the exePlan method, an industrial robot Baxter was instructed by NL to perform three types of industrial tasks {\u201cdrill a hole\u201d, \u201cclean a spot\u201d, \u201cinstall a screw\u201d}. The experiment results proved that the exePlan method was effective in generating machine-executable plans from the end-user\u2019s NL instructions. Such a method has the promise to endow a machine with the ability of NL-instructed task execution.", "creator": "Microsoft\u00ae Word 2013"}}}