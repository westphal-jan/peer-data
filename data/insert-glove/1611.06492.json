{"id": "1611.06492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Recurrent Memory Addressing for describing videos", "abstract": "Deep Neural Network reacting architectures gangbangers with external berkely memory smola components allow 300mm the model dogging to appropriators perform inference deshler and capture palette long term dewaniya dependencies, fowling by storing information solutes explicitly. In multiphasic this paper, we benefitted generalize schoenberger Key - mulay Value jaaf Memory Networks michoacana to a 4-door multimodal trawl setting, ghouls introducing lazarevski a novel heafy key - colonial addressing mechanism kanuri to 71.80 deal with stiansen sequence - to - 140-kilometer sequence brewpubs models. 191.8 The asle advantages aryan of caravan the mosimann framework sabhal are zaporizhya demonstrated hia on the trope task fleetstreet of video captioning, i. e kronberger generating gallagher natural language ubs4 descriptions silverbell for videos. Conditioning on lyondthzi the previous remini time - 2-50 step reverb attention distributions leakes for the key - guillemont value memory slots, giudecca we kosmala introduce gadebusch a non-moving temporal doublemint structure in j7 the makadmeh memory addressing childline schema. 26-page The zonata proposed model taiping naturally 53.53 decomposes the propeller-driven problem of video captioning hotkey into vision primerica and language dented segments, baldly dealing codman with siropulo them as u.f.o. key - value duranti pairs. norske More radleys specifically, we learn a uparaja semantic chiaramonte embedding (ardross v) corresponding natinal to whateley each frame (gasimov k) trenched in the m-66 video, thereby creating (131.00 k, zeitz v) memory slots. This allows bhoomi us streeterville to exploit carmiel the kriz temporal rutt dependencies at multiple hierarchies (kumbh in cooling the http://www.wto.org recurrent key - addressing; and amtran in the swac language 3,731 decoder ). 3,191 Exploiting mr2 this froid flexibility brini of salene the framework, we additionally capture altercations spatial moretz dependencies ndiema while mapping deaf-blind from elefant the ident visual to semantic pupillary embedding. milosavljevic Extensive circumventing experiments on lason the kolokotronis Youtube2Text dataset buhrer demonstrate duwe usefulness barc of cuchi recurrent key - nell addressing, ilmi while mainshock achieving competitive 52kg scores estaba on nancheng BLEU @ dekaser 4, ergodic METEOR albinus metrics barawa against state - omega of - the - art models.", "histories": [["v1", "Sun, 20 Nov 2016 10:07:54 GMT  (1454kb,D)", "http://arxiv.org/abs/1611.06492v1", "Under review at CVPR 2017"], ["v2", "Thu, 23 Mar 2017 14:01:20 GMT  (1455kb,D)", "http://arxiv.org/abs/1611.06492v2", null]], "COMMENTS": "Under review at CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["arnav kumar jain", "abhinav agarwalla", "kumar krishna agrawal", "pabitra mitra"], "accepted": false, "id": "1611.06492"}, "pdf": {"name": "1611.06492.pdf", "metadata": {"source": "CRF", "title": "Recurrent Memory Addressing for describing videos", "authors": ["Kumar Krishna Agrawal", "Arnav Kumar Jain", "Abhinav Agarwalla", "Pabitra Mitra"], "emails": ["pabitra}@iitkgp.ac.in", "BLEU@4,"], "sections": [{"heading": null, "text": "The advantages of the framework are demonstrated on the task of video captioning, i.e generating natural language descriptions for videos. Conditioning on the previous time-step attention distributions for the key-value memory slots, we introduce a temporal structure in the memory addressing schema. The proposed model naturally decomposes the problem of video captioning into vision and language segments, dealing with them as key-value pairs. More specifically, we learn a semantic embedding (v) corresponding to each frame (k) in the video, thereby creating (k, v) memory slots. This allows us to exploit the temporal dependencies at multiple hierarchies (in the recurrent keyaddressing; and in the language decoder). Exploiting this flexibility of the framework, we additionally capture spatial dependencies while mapping from the visual to semantic embedding. Extensive experiments on the Youtube2Text dataset demonstrate usefulness of recurrent key-addressing, while achieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art models."}, {"heading": "1. Introduction", "text": "Generating natural language descriptions for images and videos is a long-standing problem, in the intersection of computer vision and natural language processing. Solving the problem requires developing powerful models capable of extracting visual information about various objects in an image, while deriving semantic relationships between them in natural language. For video captioning, the models are additionally required to find compact representations of the video which capture the temporal dynamics across image frames.\nThe recent advances in training deep neural architectures have significantly improved in the state-of-the-art across computer vision and natural language understanding. With impressive results in object detection and scene understanding, Convolution Neural Networks (CNNs) [23] have become the staple for extracting feature representations from images. Recurrent Neural Networks (RNNs) with Long Short Term Memory (LSTM) [15] units or Gated Recurrent Units (GRUs)[10], have similarly emerged as generative models of choice for dealing with sequences in domains ranging from language modeling, machine translation to speech recognition. Advancements in these fundamental problems make tackling challenging problems, like captioning [16, 45], dialogue [32] and visual question answering [1] more viable.\nDespite the fundamental complexities of these problems, there has been increasing interest in solving them. A common underlying approach in these proposed models is the notion of \u201dattention mechanisms\u201d, which refers to selectively focusing on segments of sequences [3, 47] or images [35] to generate corresponding outputs. Such attention based approaches are specially attractive for captioning problems, since they allow the network to focus on patches of the image conditioned on the previously generated tokens [45, 16], often referred to as spatial attention.\nModels with spatial attention, however cannot be readily used for video description. For instance, in the Youtube2Text dataset, a video clip stretches around 10 sec-\n1\nar X\niv :1\n61 1.\n06 49\n2v 1\n[ cs\n.C V\n] 2\n0 N\nov 2\n01 6\nonds, or around 150 frames. Applying attention on patches in these individual frames provides the network with local spatial context. This however, does not take ordering of the frame sequence or events ranging across frames, into consideration. To incorporate this temporal attention into the model, [47, 49, 27] extend this soft alignment to video captioning. Most of these approaches, treat the problem of video captioning in the sequence-to-sequence paradigm [37] with attentive encoders and decoders. This requires them to find a compact representation of the video, which is passed as context to the RNN decoder.\nHowever, we identify two primary issues with these approaches. For one, applying attention sequentially provides the model with local context at the generative decoder [46]. As a result the decoder would be unable to deal with longterm dependencies while captioning videos of longer duration. Secondly, these models jointly learn the multimodal embedding in a visual-semantic space [28, 49] at the RNN decoder. With the annotated sentences being the only supervisory signal, learning a mapping from a sequence of images to a sequence of words, is difficult. This is specially true for dealing with video sequences, as the underlying probability distribution is distinctively multimodal. While [28] tries to address this issue with an auxiliary loss, the model suffers from the first drawback.\nTo address the aforementioned issues, we introduce a model which generalizes Key-Value Memory Networks [25] to a multimodal setting for video captioning. The proposed model, naturally tackles the problem of maintaining long-term temporal dependencies in videos, by explicitly providing all image frames for selection at each time step. We also propose a novel key-addressing scheme (see Section 4), which keeps track of previous attention distributions, allowing us to exploit temporal structure at multiple hierarchies and provide a global context. At the same time, the framework provides an effective way to deal with the complex transformation from the visual to language domain. Using a pre-trained model to explicitly transform individual frames (keys) to semantic embedding (values), we construct memory slots with each slot being a tuple (key, value). This allows to provide a weighted pooling of textual features as context to the decoder RNN, which is closer to the language model.\nIn summary, our key contributions are following :\n\u2022 We generalize Key-Value Memory Networks (KVMemNN) in a multimodal setting to generate natural descriptions for videos, and more generally deal with sequence-to-sequence models.\n\u2022 We propose a novel key-addressing schema, conditioning on the previous time-step attention distributions for the key-value pairs. This allows us to exploit spatial and temporal structure at multiple hierarchies.\n\u2022 The proposed model is evaluated on the YouTube dataset [7], where we outperform strong baselines while reporting competitive results against state-of-art models.\nThe remaining part of paper is organized as following. Section 2 provides a brief literature review of related work. In Section 3 we present details of our proposed model with recurrent key-addressing. Section 4 demonstrates the advantages of the model with experimental details and empirical analysis, followed by discussions and conclusions in Section 5."}, {"heading": "2. Related Work", "text": "Following the success of end-to-end neural architectures and attention mechanisms, there is a growing body of literature for captioning tasks, in images and more recently videos. To deal with the multimodal nature of the problem, classical approaches relied on manually engineered templates [19, 11]. And while some recent approaches in this direction show promise [13], the models lack generalization to deal with complex scenes, videos.\nAs an alternative approach, [14, 18] suggest learning a joint visual-semantic embedding, effectively a mapping from the visual to language space. The motivation of our work is strongly aligned with [31], who generate semantic representations for images using CRF models, as context for the language decoder. However, our approach significantly differs in the essence that we capture spatio-temporal dynamics in videos while generating the text description.\nBuilding on this, and Encoder-Decoder [3, 9] models for machine translation , [42, 41] develop models which compute average fixed-length representations (for images, videos respectively) from image features. These context vectors are provided at each time step to the decoder language model, for generating descriptions. The visual representations for the images are usually transferred from pretrained convolution networks [33, 38].\nAn immediate drawback of the above approach is collapsing features across image frames to fixed vector by mean-pooling. For one, this loses the temporal structure across frames by treating them as \u201dbag-of-images\u201d model. Addressing this, [37] propose Sequence-to-Sequence models for accounting for the temporal structure, and [40] extend it to a video-captioning setting. However, passing a fixed vector as context at each time step, creates a bottleneck for the flow of gradients using Backpropagate Through Time (BPTT) [43] at the encoder.\nMeanwhile, the notion of visual attention, has a rich literature in Psychology and Neuroscience, and has recently found application in Computer Vision [26] and Machine Translation [3]. Allowing the network to selectively focus on the patches of images or segments of the input se-\nquences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain. In essence, with the decoder output selectively conditioned on the encoder states the issues of fixed length representation and gradient bottleneck are largely solved:\np(yi|yi\u22121, ..., y1, x) = g(yi\u22121, si, ci) (1)\nwhere,yi is the readout, ci is the context from the encoder and si = f(si\u22121, yi\u22121, ci), is the hidden state of decoder RNN (See [3] for details).\nHowever, as discussed in Section 1, sequential attention provides the decoder with local context [46]. Additionally, providing a semantic input which is closer to language space, as context to the decoder significantly improves on the capability of the model [48]. Our work closely brings these advances together in a Memory Networks framework [44, 36, 20]. While we introduce Key-Value Memory Networks [25] in a multimodal setting, there are several other key differences from previous works. For one, to our knowledge this is the first work which introduces video captioning in light of Memory Networks. This automatically deals with problems of maintaining long-term dependencies in the input stream by explicitly storing image representations. Meanwhile we also tackle the \u201dvanishing gradient problem\u201d typical with training RNN Encoder-Decoder modules for long input sequences.\nKey-Value MemNNs [25] are originally proposed for QA task in the language domain, providing the last timestep hidden state, as input to the classifier. In this work, we address a more complex problem of video captioning by proposing a novel key-addressing scheme (details in Section 4) and (key, value) setup for exploiting the hierarchical spatio-temporal structures. The model tracks the attention distribution at previous time steps, thereby providing a strong context on where to attend on the complete video sequence. This implicitly provides a global temporal structure\nat each readout. While similar in motivation to [48, 30], the model architecture and domain of application, especially on capturing global temporal dynamics in videos as opposed to images or entailment, is significantly different."}, {"heading": "3. Recurrent Memory Addressing for videos", "text": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting. The encoder network learns a mapping from the input sequence to a fixed-length vector representation, which is used by the decoder to generate output sequences. Unlike standard Encoder-Decoder architectures, our model (see Fig. 2) comprises an encoder module, keyvalue memories and a decoder module."}, {"heading": "3.1. Encoder", "text": "The encoder network E maps a given sequence X = {I1, ..., IT } to the corresponding sequence of context representation vectors {k1, ..., kT }. As we are dealing with videos (sequence of images), we define two different encoders to achieve the mapping.\nCNN Encoder: Given an input image It \u2208 RNxM , the CNN encoders learn a mapping f : RNxM \u2192 RD , if we consider the output of fully-connected layers in standard ConvNet architectures [33]. In case of convolution outputs, the encoders learn f : RNxM \u2192 RLxD, where L is the number of context vectors of dimensionality D.\nRNN Encoder: Consider an input sequence of length T . The RNN encoder processes the input X sequentially, generating hidden states ht at each time step, where\nht = g(f(It), gt\u22121) (2)\nWhile maintaining temporal dependencies, this allows us to map variable length sequences to fixed length context vector. In this work we use modified version of an LSTM [15] unit, as proposed in [50] to implement g.\nLearning a good representation for the keys is imperative to good performance of the model. We experiment with different settings of the encoder, using features directly from the CNN Encoders, or stacking an RNN Encoder on these extracted feature vectors."}, {"heading": "3.2. Key-Value Memories", "text": "The model is built around a Key-Value Memory Network [25] with memory slots as vector pairs (k1, v1), ..., (kn, vn). The keys and values serve the purpose of transforming visual space context into language space, and effectively capture the relationships between the video features and textual descriptions. The memory definition, addressing and reading schema is outlined below :\nKeys (K): In our model, feature vectors for each frame It of the video are extracted using CNN Encoders. To incorporate sequential structure (video being a sequence of images), these appearance feature vectors are passed through a RNN Encoder, and hidden state at each timestep is extracted as key kt.\nkt = g(f(It), kt\u22121) (3)\nThis allows the model to capture the temporal variation between frames, and take the ordering of actions and events into consideration. Implicitly this also helps, preserving high level information about motion in the video [4].\nValues (V ): Jointly learning visual-semantic embedding with supervisory signal only from annotated descriptions in Encoder-Decoder models is difficult [48]. To mitigate this, and to improve the context provided to the language model at decoder, we explicitly precompute semantic embedding corresponding to individual frames in the video.\nFor each image frame It we generate a semantic embedding vt, which represents the textual meaning of that frame. This could be obtained from any pretrained model which jointly models visual and semantic embedding for images [45, 16, 42]. In this work we use a pretrained DenseCap model [16], to obtain semantic embedding corresponding to every image. Therefore,\nvi = \u03c8(It) (4)\nwhere, \u03c8 is the intermediate visual-semantic embedding from a pretrained image captioning model. This gives for each frame in the video a key-value memory slot (kt, vt).\nKey Hashing: This refers to retrieving a subset of the relevant memory slots for a given query vector q. For each video clip (typically with 120-240 frames), key-value pairs of T equally spaced image frames are extracted using pretrained models and are stored in the memory.\nKey Addressing: This corresponds to the soft-attention mechanism deployed to assign a relevance probability \u03b1i to each of the retrieved memory slots. These relevant probabilities are used for value reading (details in Section 4).\nValue Reading: The value reading from the memory slots, returns the weighted sum of the key-value feature vectors: \u03c6t(K) and \u03c6t(V ) at each time step. \u03c6t(K) is used for key addressing at the next time step and \u03c6t(V ) is taken as input to the decoder RNN for generating the next word.\n\u03c6t(V ) = N\u2211 i=1 \u03b1 (t) i vi (5)"}, {"heading": "3.3. Decoder", "text": "The joint probability over generating the output sequence can be decomposed as:\np(y|x1, ..., xT ) = N\u220f t=1 p(yt|hT , y1, .., yt\u22121) (6)\nwhere hT is the context vector representing X = {x1, ..., xT } and y = {y1, ..., yN} is the output sequence. To train the network, the overall objective is therefore to maximize the log-likelihood of the output word sequence.\nTo this effect, Recurrent Neural Networks have been widely used for natural language generation tasks like Machine Translation, Image Captioning and Video Description generation. Vanilla RNNs are difficult to train for long range dependencies as they suffer from the vanishing gradient problem [5]. Thus, we use Long Short Term Memory(LSTM) [15] as they are known to memorize context for longer period of time using controllable memory units.\nThe LSTM model has a memory cell ct in addition to the hidden state ht of RNNs, which effectively summarizes the information observed up to that time step. There are primarily three gates which control the flow of information i.e (input, output, forget). The input gate it controlling the current input xt, forget gate ft adaptively allowing to forget old memory and output gate ot deciding the extent of transfer of cell memory to hidden state. The recurrences at the decoder are defined as:\nit = \u03c3(Wiht\u22121 +Uixt +Ai\u03c6t(V ) + bi) (7) ft = \u03c3(Wfht\u22121 +Ufxt +Af\u03c6t(V ) + bf ) (8) ot = \u03c3(Woht\u22121 +Uoxt +Ao\u03c6t(V ) + bo) (9) c\u0303t = tanh(Wcht\u22121 +Ucxt +Ac\u03c6t(V ) + bc) (10) ct = it c\u0303t + ft ct\u22121 (11) ht = ot ct (12)\nwhere is an element wise multiplication, \u03c3 is the sigmoidal non-linearity. Wi,Ux,Ax and bx, are the weight matrices for the previous hidden state, input, value context and bias respectively. Here \u03c6t(V ) represents the context vector to the decoder at each time step.\nFollowing standard sequence-to-sequence models with generative decoders, we apply a single layer network con-\nditioned on the hidden state ht.\nst = tanh(Ws[ht, xt, \u03c6t(V )] + bs) (13) pt = softmax(Upst + bp) (14)\nHere st refers to the pre-softmax hidden representation with concatenated inputs ht, xt, \u03c6t(V ) and pt is the probability distribution over the vocabulary for sampling the current word."}, {"heading": "4. Key Addressing", "text": "Soft attention mechanism have recently been successful in Image Captioning[45] and Video Description Generation [47] tasks because they help to focus on relevant parts of the features rather than mean pooling. In [41], the feature vectors were simply averaged, leading to loss of the temporal relationships between frames of the video. Soft attention mechanism weights each frame allowing to exploit the temporal structure in video. They were also used in the Key-Value Memory Networks[25] to focus on more relevant keys and reads the weighted sum of values.\nWe propose a recurrent key addressing mechanism which looks at the previous attention distribution over keys to obtain the new relevance scores. This helps to exploit multiple hierarchies in sequence-to-sequence architectures as the model is selecting relevant frames based on the key distribution seen so far and the previously generated words. There is a Key Addressing RNN (referred to as Memory LSTM in Fig. 4) to keep track of the previous attention distributions over keys and uses the hidden state to find the new attention weights \u03b1i. The current hidden state of the Memory LSTM takes as input the previous relevance distribution over keys \u03c6t\u22121(K):\nhkt = f k(\u03c6t\u22121(K), h k t\u22121) (15)\nwhere fk is the recurrent unit. The relevance distribution on the memory slots is evaluated using a query vector q, which summarizes the frames seen so far and the output generated. The query vector q is a weighted combination of the decoder and key-addressing hidden states.\nq =Wkh k t +Wdht\u22121 (16)\nFor obtaining the attention weights, the relevance score eti of i-th temporal feature is obtained using the decoder RNN hidden state ht\u22121, key addressing RNN hidden state hkt and the i-th key vector ki:\neti = wt tanh(q+Uaki) (17)\nwhere wt, Wd, Wk and Ua are parameters of the model. This allows us take into consideration the previously generated words, the attention distribution on previous time steps and the individual key representations.\nThese relevance scores are normalised using a softmax function which gives the attention distribution \u03b1ti using:\n\u03b1ti = exp{eti}\u2211N j=1 exp{etj}\n(18)\nThe proposed model is flexible, because it allows multiple hops for iterative inference just like in the original framework. The query vector q at initial time step is a mean-pooled average of all the keys ki. The segregation of the vision and language components into key-value pairs provides a better context for the RNN decoder. Also, the explicit memory structure provides access to the image frames at all time steps allowing the model to assign weights to the key-frames without losing information."}, {"heading": "5. Experimental Setup", "text": ""}, {"heading": "5.1. Dataset", "text": "Youtube2Text The proposed approach is benchmarked on the Youtube2Text[7] dataset which consists of 1,970 Youtube videos with multiple descriptions annotated through Amazon Mechanical Turk. The videos are generally short (9 seconds on an average), and depict a single activity. Activities depicted are open domain ranging from everyday objects to animals, scenarios, actions, landscapes. etc. The dataset consists of 80,839 annotations and vocabulary of 16,000 words approximately, with an average of 41 annotations per clip and 8 words per sentence respectively. The training, validation and test sets have 1,200, 100 and 670 videos respectively which is exactly the same splits as in previous work on video captioning [47, 4, 27].\nKey-Value Memories We select 28 equally spaced frames and pass them through a pretrained VGG-16[33] and GoogleNet[38] because of their state of the art performance in object detection on Imagenet[12] database.\nFor an input image of size WXH , visual features with shape (bW16 c, b W 16 c, C) with C as 512 are extracted from the conv5 3 layer of VGG-16. We simply average over a feature map which results in a feature of length C and are then used as keys. The visual features extracted from the pool5/7x7 s1 layer of GoogLeNet is a 1024 dimensional vector and are also used as keys.\nThe values are generated from a pre-trained image captioning module following [45, 16], which identifies salient regions in an image, and generates a caption for each of these regions. Densecap jointly models the task of object localization as well as description using a Fully Convolutional Localization Network (FCLN). After passing the image through CNN and Localization layers, Recognition network is used which is essentially a fully connected layer. We extract output from this layer which is encoded as region codes of size BxD, where B is the number of salient regions or boxes, and D is the representation with dimension 4096. Along with the features, a score S is assigned to each of the regions which denotes its confidence. A weighted sum of features of top 5 scores is calculated to get values with dimension D.\nPreprocessing: The video descriptions are tokenized using the wordpunct tokenizer from the NLTK toolbox[24]. The number of unique words were 15,903 in the Youtube2Text dataset."}, {"heading": "5.2. Model Specifications", "text": "We vary the underlying CNN for feature encoding, and conduct ablation studies with key addressing with and without LSTM which results in four model variations. The results from different variations are presented in Table 1. VGG-Encoder uses features encoded from the last convolution layer in VGG-16 CNN [33] along with a LSTM decoder. Here, we define q = ht\u22121 for the key addressing following [25]. GoogLeNet-Encoder is exactly the same as VGG Encoder with the only difference being GoogLeNet used for feature encoding. t-KeyAddressing adds to GoogLeNet-Encoder by addressing keys through\nboth the last hidden state ht\u22121 and the previous time-step attention distribution on the keys. Finally, m-KeyAddressing summarizes all the previous attention distributions through a LSTM, after which its hidden state is used for key addressing along with last hidden state of the Decoder. These variations help us identify architecture changes that lead to large improvements on the evaluation metric. Detailed comparison of the results is provided in Section 5.6."}, {"heading": "5.3. Model Comparisons", "text": "We compare the model performance with previous state of the art approaches and some strong baselines. Pan et al. [28] explicitly learn a visual-semantic joint embedding model for exploiting the relationship between visual features and generated language, which is then used in a similar encoder-decoder framework. Yao et al.[47] utilizes a temporal attention mechanism for global attention apart from local attention using 3-D Convolution Networks. Ballas et al.[4] proposed an encoder to learn spatial-temporal features across frames, introducing a variant GRU with convolution operations (GRU-RCN). In the current state-of-art Yu et al. [49] model the decoder as a paragraph generator, describing the videos over multiple sentences using stacked LSTMs."}, {"heading": "5.4. Evaluation Metrics", "text": "We evaluate our approach using standard evaluation metrics for video captioning tasks, namely BLEU [29], METEOR [22] and CIDEr[39] to compare the generated sequences with the human annotations. Treating this as a Sequence-to-Sequence model, and comparing against given ground truth, higher scores on the metrics indicate better performance. The metrics score are calculated based on alignment and similarity between the generated and candidate reference descriptions. We use the code accompanying the Microsoft COCO Evaluation script [8] to obtain the results reported in the paper."}, {"heading": "5.5. Training Details", "text": "The model predicts the next output word given the previous words and the input video. Thus, the goal is to maximize the log likelihood of the loss function:\nL = 1\nN N\u2211 i=1 |yi|\u2211 j=1 log p(yij |yi<j ,xn, \u03b8) (19)\nwhere N is the total number of video-description pairs and length of each description yi is |yi|. Here xn refers to the input video provided as context to the decoder. We train our network parameters \u03b8 through first order stochastic gradient-based optimization with an adaptive learning rate using the Adadelta [51] optimizer. We set the batch size to be 64 in our implementation. We optimize hyperparameters, namely number of hidden units in Decoder LSTM, key addressing LSTM, learning rate, word embedding dimension for the log loss using random search [6]."}, {"heading": "5.6. Results", "text": "Table 5.1 summarizes the performance of our approach, and compares it with various state of the art techniques. We test our proposed model in the basic setting with visual features extracted from a pretrained VGG-16 [33] and GoogLeNet[38] models, as depicted by the first two lines of Table 1. The key addressing in these models, follows the basic approach as in [25], i.e without tracking the last time steps. These models are able to outperform the baselines of S2VT [40], and the Basic Encoder-Decoder model introduced in [47] on all three metrics. Using the recurrent key-addressing, we see further boost in performance. We observe that using features from pretrained GoogleNet rather than VGG-16 improves on the results, following [42].\nt-KeyAddressing refers to using only the last time attention distribution over keys in the query vector. Using GoogleNet to extract features, we achieve significantly better results compared to the temporal attention introduced by [47], where they do not utilize attention distribution from previous time steps further in the model. Additionally, [47]\nuse 3D Convolution Networks to extract local temporal features, while we work directly on the individual frame features.\nUsing a recurrent addressing scheme; an LSTM for storing previous attention distributions, leads to huge improvement on all the metrics and demonstrates the effectiveness of the proposed framework. This model, referred to as m-KeyAddressing, outperforms strong baselines from [27] by a significant margin on BLEU@4. While the improvements on METEOR are significant compared to tKeyAddressing, [27] learns a hierarchical representation of the video, thereby performing slightly better. In the current setting, our model is unable to outperform [49] which uses a stacked LSTM at the decoder and generates more granular descriptions. A possible approach for improvement in this direction is using more sophisticated regularizers for training the decoder, as proposed in [2, 21].\nIn Table 5.1 we provide comparison on whether the models use finetuning on the CNN encoder (represented by Fine) or if they use external features, like on action recognition, optical flow (represented by Feat). It is to be noted, that we do not finetune the encoder, or the image captioning module as compared to [4] which finetunes the encoder CNN on UCF101 action recognition set[34]. Also, no additional features are extracted for gaining more information about motion, actions etc. as in [49], [4], [40].\nIn Fig 4, we show examples of some of the input frames and generated outputs, with ground truths. Some of the examples demonstrate the fact that the model is able to infer the activities from the video frames, like \u201dswimming\u201d, \u201driding\u201d and \u201dflying\u201d which is often distributed across multiple frames."}, {"heading": "6. Conclusion", "text": "We introduce a recurrent memory addressing model for video captioning. By decomposing the visual and language components, we explicitly exploit temporal structure a multiple hierarchies. Extensive experiments on the proposed model outperform strong baselines across several metrics, and achieve competitive scores against the state-of-the-art benchmarks. To the best of our knowledge this is the first proposed work for video-captioning in a Memory Networks setting, and does not rely on heavily annotated videos to generate intermediate semantic-embedding for supporting the decoder. Further work would be exploring the effectiveness of the model on longer videos, and generating finegrained descriptions with more sophisticated decoders."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425\u20132433,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Layer normalization", "author": ["J.L. Ba", "J.R. Kiros", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1607.06450,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Delving deeper into convolutional networks for learning video representations", "author": ["N. Ballas", "L. Yao", "C. Pal", "A. Courville"], "venue": "arXiv preprint arXiv:1511.06432,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE transactions on neural networks, 5(2):157\u2013166,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Random search for hyperparameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research, 13(Feb):281\u2013305,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["D.L. Chen", "W.B. Dolan"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 190\u2013200. Association for Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar, Oct.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["P. Das", "C. Xu", "R.F. Doell", "J.J. Corso"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2634\u20132641,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR09,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1511.07571,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(12):2891\u2013 2903,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Professor Forcing: A New Algorithm for Training Recurrent Networks", "author": ["A. Lamb", "A. Goyal", "Y. Zhang", "S. Zhang", "A. Courville", "Y. Bengio"], "venue": "ArXiv e-prints, Oct.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Meteor universal: language specific translation evaluation for any target language", "author": ["M.D.A. Lavie"], "venue": "ACL 2014, page 376,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Nltk: The natural language toolkit", "author": ["E. Loper", "S. Bird"], "venue": "Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics - Volume 1, ETMTNLP \u201902, pages 63\u201370, Stroudsburg, PA, USA,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Key-value memory networks for directly reading documents", "author": ["A. Miller", "A. Fisch", "J. Dodge", "A.-H. Karimi", "A. Bordes", "J. Weston"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["P. Pan", "Z. Xu", "Y. Yang", "F. Wu", "Y. Zhuang"], "venue": "arXiv preprint arXiv:1511.03476,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "arXiv preprint arXiv:1505.01861,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Reasoning about entailment with neural attention", "author": ["T. Rockt\u00e4schel", "E. Grefenstette", "K.M. Hermann", "T. Ko\u010disk\u1ef3", "P. Blunsom"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 433\u2013440,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI- 16),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["K. Soomro", "A.R. Zamir", "M. Shah"], "venue": "arXiv preprint arXiv:1212.0402,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["M.F. Stollenga", "J. Masci", "F. Gomez", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems, pages 3545\u20133553,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566\u20134575,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence \u2013 video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "NAACL HLT,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE, 78(10):1550\u2013 1560,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1990}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Encode, review, and decode: Reviewer module for caption generation", "author": ["Z. Yang", "Y. Yuan", "Y. Wu", "R. Salakhutdinov", "W.W. Cohen"], "venue": "arXiv preprint arXiv:1605.07912,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Describing videos by exploiting temporal  structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 4507\u20134515,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "arXiv preprint arXiv:1603.03925,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": "arXiv preprint arXiv:1510.07712,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 22, "context": "With impressive results in object detection and scene understanding, Convolution Neural Networks (CNNs) [23] have become the staple for extracting feature representations from images.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "Recurrent Neural Networks (RNNs) with Long Short Term Memory (LSTM) [15] units or Gated Recurrent Units (GRUs)[10], have similarly emerged as generative models of choice for dealing with sequences in domains ranging from language modeling, machine translation to speech recognition.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "Recurrent Neural Networks (RNNs) with Long Short Term Memory (LSTM) [15] units or Gated Recurrent Units (GRUs)[10], have similarly emerged as generative models of choice for dealing with sequences in domains ranging from language modeling, machine translation to speech recognition.", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": "Advancements in these fundamental problems make tackling challenging problems, like captioning [16, 45], dialogue [32] and visual question answering [1] more viable.", "startOffset": 95, "endOffset": 103}, {"referenceID": 44, "context": "Advancements in these fundamental problems make tackling challenging problems, like captioning [16, 45], dialogue [32] and visual question answering [1] more viable.", "startOffset": 95, "endOffset": 103}, {"referenceID": 31, "context": "Advancements in these fundamental problems make tackling challenging problems, like captioning [16, 45], dialogue [32] and visual question answering [1] more viable.", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "Advancements in these fundamental problems make tackling challenging problems, like captioning [16, 45], dialogue [32] and visual question answering [1] more viable.", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "A common underlying approach in these proposed models is the notion of \u201dattention mechanisms\u201d, which refers to selectively focusing on segments of sequences [3, 47] or images [35] to generate corresponding outputs.", "startOffset": 157, "endOffset": 164}, {"referenceID": 46, "context": "A common underlying approach in these proposed models is the notion of \u201dattention mechanisms\u201d, which refers to selectively focusing on segments of sequences [3, 47] or images [35] to generate corresponding outputs.", "startOffset": 157, "endOffset": 164}, {"referenceID": 34, "context": "A common underlying approach in these proposed models is the notion of \u201dattention mechanisms\u201d, which refers to selectively focusing on segments of sequences [3, 47] or images [35] to generate corresponding outputs.", "startOffset": 175, "endOffset": 179}, {"referenceID": 44, "context": "Such attention based approaches are specially attractive for captioning problems, since they allow the network to focus on patches of the image conditioned on the previously generated tokens [45, 16], often referred to as spatial attention.", "startOffset": 191, "endOffset": 199}, {"referenceID": 15, "context": "Such attention based approaches are specially attractive for captioning problems, since they allow the network to focus on patches of the image conditioned on the previously generated tokens [45, 16], often referred to as spatial attention.", "startOffset": 191, "endOffset": 199}, {"referenceID": 46, "context": "To incorporate this temporal attention into the model, [47, 49, 27] extend this soft alignment to video captioning.", "startOffset": 55, "endOffset": 67}, {"referenceID": 48, "context": "To incorporate this temporal attention into the model, [47, 49, 27] extend this soft alignment to video captioning.", "startOffset": 55, "endOffset": 67}, {"referenceID": 26, "context": "To incorporate this temporal attention into the model, [47, 49, 27] extend this soft alignment to video captioning.", "startOffset": 55, "endOffset": 67}, {"referenceID": 36, "context": "Most of these approaches, treat the problem of video captioning in the sequence-to-sequence paradigm [37] with attentive encoders and decoders.", "startOffset": 101, "endOffset": 105}, {"referenceID": 45, "context": "For one, applying attention sequentially provides the model with local context at the generative decoder [46].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "Secondly, these models jointly learn the multimodal embedding in a visual-semantic space [28, 49] at the RNN decoder.", "startOffset": 89, "endOffset": 97}, {"referenceID": 48, "context": "Secondly, these models jointly learn the multimodal embedding in a visual-semantic space [28, 49] at the RNN decoder.", "startOffset": 89, "endOffset": 97}, {"referenceID": 27, "context": "While [28] tries to address this issue with an auxiliary loss, the model suffers from the first drawback.", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "To address the aforementioned issues, we introduce a model which generalizes Key-Value Memory Networks [25] to a multimodal setting for video captioning.", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "\u2022 The proposed model is evaluated on the YouTube dataset [7], where we outperform strong baselines while reporting competitive results against state-of-art models.", "startOffset": 57, "endOffset": 60}, {"referenceID": 18, "context": "To deal with the multimodal nature of the problem, classical approaches relied on manually engineered templates [19, 11].", "startOffset": 112, "endOffset": 120}, {"referenceID": 10, "context": "To deal with the multimodal nature of the problem, classical approaches relied on manually engineered templates [19, 11].", "startOffset": 112, "endOffset": 120}, {"referenceID": 12, "context": "And while some recent approaches in this direction show promise [13], the models lack generalization to deal with complex scenes, videos.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "As an alternative approach, [14, 18] suggest learning a joint visual-semantic embedding, effectively a mapping from the visual to language space.", "startOffset": 28, "endOffset": 36}, {"referenceID": 17, "context": "As an alternative approach, [14, 18] suggest learning a joint visual-semantic embedding, effectively a mapping from the visual to language space.", "startOffset": 28, "endOffset": 36}, {"referenceID": 30, "context": "The motivation of our work is strongly aligned with [31], who generate semantic representations for images using CRF models, as context for the language decoder.", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "Building on this, and Encoder-Decoder [3, 9] models for machine translation , [42, 41] develop models which compute average fixed-length representations (for images, videos respectively) from image features.", "startOffset": 38, "endOffset": 44}, {"referenceID": 8, "context": "Building on this, and Encoder-Decoder [3, 9] models for machine translation , [42, 41] develop models which compute average fixed-length representations (for images, videos respectively) from image features.", "startOffset": 38, "endOffset": 44}, {"referenceID": 41, "context": "Building on this, and Encoder-Decoder [3, 9] models for machine translation , [42, 41] develop models which compute average fixed-length representations (for images, videos respectively) from image features.", "startOffset": 78, "endOffset": 86}, {"referenceID": 40, "context": "Building on this, and Encoder-Decoder [3, 9] models for machine translation , [42, 41] develop models which compute average fixed-length representations (for images, videos respectively) from image features.", "startOffset": 78, "endOffset": 86}, {"referenceID": 32, "context": "The visual representations for the images are usually transferred from pretrained convolution networks [33, 38].", "startOffset": 103, "endOffset": 111}, {"referenceID": 37, "context": "The visual representations for the images are usually transferred from pretrained convolution networks [33, 38].", "startOffset": 103, "endOffset": 111}, {"referenceID": 36, "context": "Addressing this, [37] propose Sequence-to-Sequence models for accounting for the temporal structure, and [40] extend it to a video-captioning setting.", "startOffset": 17, "endOffset": 21}, {"referenceID": 39, "context": "Addressing this, [37] propose Sequence-to-Sequence models for accounting for the temporal structure, and [40] extend it to a video-captioning setting.", "startOffset": 105, "endOffset": 109}, {"referenceID": 42, "context": "However, passing a fixed vector as context at each time step, creates a bottleneck for the flow of gradients using Backpropagate Through Time (BPTT) [43] at the encoder.", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "Meanwhile, the notion of visual attention, has a rich literature in Psychology and Neuroscience, and has recently found application in Computer Vision [26] and Machine Translation [3].", "startOffset": 151, "endOffset": 155}, {"referenceID": 2, "context": "Meanwhile, the notion of visual attention, has a rich literature in Psychology and Neuroscience, and has recently found application in Computer Vision [26] and Machine Translation [3].", "startOffset": 180, "endOffset": 183}, {"referenceID": 44, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 46, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 15, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 3, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 48, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 26, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 27, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 2, "context": "where,yi is the readout, ci is the context from the encoder and si = f(si\u22121, yi\u22121, ci), is the hidden state of decoder RNN (See [3] for details).", "startOffset": 128, "endOffset": 131}, {"referenceID": 45, "context": "However, as discussed in Section 1, sequential attention provides the decoder with local context [46].", "startOffset": 97, "endOffset": 101}, {"referenceID": 47, "context": "Additionally, providing a semantic input which is closer to language space, as context to the decoder significantly improves on the capability of the model [48].", "startOffset": 156, "endOffset": 160}, {"referenceID": 43, "context": "Our work closely brings these advances together in a Memory Networks framework [44, 36, 20].", "startOffset": 79, "endOffset": 91}, {"referenceID": 35, "context": "Our work closely brings these advances together in a Memory Networks framework [44, 36, 20].", "startOffset": 79, "endOffset": 91}, {"referenceID": 19, "context": "Our work closely brings these advances together in a Memory Networks framework [44, 36, 20].", "startOffset": 79, "endOffset": 91}, {"referenceID": 24, "context": "While we introduce Key-Value Memory Networks [25] in a multimodal setting, there are several other key differences from previous works.", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "Key-Value MemNNs [25] are originally proposed for QA task in the language domain, providing the last timestep hidden state, as input to the classifier.", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": "While similar in motivation to [48, 30], the model architecture and domain of application, especially on capturing global temporal dynamics in videos as opposed to images or entailment, is significantly different.", "startOffset": 31, "endOffset": 39}, {"referenceID": 29, "context": "While similar in motivation to [48, 30], the model architecture and domain of application, especially on capturing global temporal dynamics in videos as opposed to images or entailment, is significantly different.", "startOffset": 31, "endOffset": 39}, {"referenceID": 8, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 66, "endOffset": 84}, {"referenceID": 2, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 66, "endOffset": 84}, {"referenceID": 44, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 66, "endOffset": 84}, {"referenceID": 41, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 66, "endOffset": 84}, {"referenceID": 16, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 66, "endOffset": 84}, {"referenceID": 43, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 107, "endOffset": 115}, {"referenceID": 24, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 107, "endOffset": 115}, {"referenceID": 32, "context": "CNN Encoder: Given an input image It \u2208 R , the CNN encoders learn a mapping f : R \u2192 R , if we consider the output of fully-connected layers in standard ConvNet architectures [33].", "startOffset": 174, "endOffset": 178}, {"referenceID": 14, "context": "In this work we use modified version of an LSTM [15] unit, as proposed in [50] to implement g.", "startOffset": 48, "endOffset": 52}, {"referenceID": 49, "context": "In this work we use modified version of an LSTM [15] unit, as proposed in [50] to implement g.", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "The model is built around a Key-Value Memory Network [25] with memory slots as vector pairs (k1, v1), .", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "Implicitly this also helps, preserving high level information about motion in the video [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 47, "context": "Values (V ): Jointly learning visual-semantic embedding with supervisory signal only from annotated descriptions in Encoder-Decoder models is difficult [48].", "startOffset": 152, "endOffset": 156}, {"referenceID": 44, "context": "This could be obtained from any pretrained model which jointly models visual and semantic embedding for images [45, 16, 42].", "startOffset": 111, "endOffset": 123}, {"referenceID": 15, "context": "This could be obtained from any pretrained model which jointly models visual and semantic embedding for images [45, 16, 42].", "startOffset": 111, "endOffset": 123}, {"referenceID": 41, "context": "This could be obtained from any pretrained model which jointly models visual and semantic embedding for images [45, 16, 42].", "startOffset": 111, "endOffset": 123}, {"referenceID": 15, "context": "In this work we use a pretrained DenseCap model [16], to obtain semantic embedding corresponding to every image.", "startOffset": 48, "endOffset": 52}, {"referenceID": 4, "context": "Vanilla RNNs are difficult to train for long range dependencies as they suffer from the vanishing gradient problem [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 14, "context": "Thus, we use Long Short Term Memory(LSTM) [15] as they are known to memorize context for longer period of time using controllable memory units.", "startOffset": 42, "endOffset": 46}, {"referenceID": 44, "context": "Soft attention mechanism have recently been successful in Image Captioning[45] and Video Description Generation [47] tasks because they help to focus on relevant parts of the features rather than mean pooling.", "startOffset": 74, "endOffset": 78}, {"referenceID": 46, "context": "Soft attention mechanism have recently been successful in Image Captioning[45] and Video Description Generation [47] tasks because they help to focus on relevant parts of the features rather than mean pooling.", "startOffset": 112, "endOffset": 116}, {"referenceID": 40, "context": "In [41], the feature vectors were simply averaged, leading to loss of the temporal relationships between frames of the video.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "They were also used in the Key-Value Memory Networks[25] to focus on more relevant keys and reads the weighted sum of values.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "Youtube2Text The proposed approach is benchmarked on the Youtube2Text[7] dataset which consists of 1,970 Youtube videos with multiple descriptions annotated through Amazon Mechanical Turk.", "startOffset": 69, "endOffset": 72}, {"referenceID": 46, "context": "The training, validation and test sets have 1,200, 100 and 670 videos respectively which is exactly the same splits as in previous work on video captioning [47, 4, 27].", "startOffset": 156, "endOffset": 167}, {"referenceID": 3, "context": "The training, validation and test sets have 1,200, 100 and 670 videos respectively which is exactly the same splits as in previous work on video captioning [47, 4, 27].", "startOffset": 156, "endOffset": 167}, {"referenceID": 26, "context": "The training, validation and test sets have 1,200, 100 and 670 videos respectively which is exactly the same splits as in previous work on video captioning [47, 4, 27].", "startOffset": 156, "endOffset": 167}, {"referenceID": 32, "context": "Key-Value Memories We select 28 equally spaced frames and pass them through a pretrained VGG-16[33] and GoogleNet[38] because of their state of the art performance in object detection on Imagenet[12] database.", "startOffset": 95, "endOffset": 99}, {"referenceID": 37, "context": "Key-Value Memories We select 28 equally spaced frames and pass them through a pretrained VGG-16[33] and GoogleNet[38] because of their state of the art performance in object detection on Imagenet[12] database.", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "Key-Value Memories We select 28 equally spaced frames and pass them through a pretrained VGG-16[33] and GoogleNet[38] because of their state of the art performance in object detection on Imagenet[12] database.", "startOffset": 195, "endOffset": 199}, {"referenceID": 46, "context": "[47]) 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27]) 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47]) 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[49] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40]) .", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4]) 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 44, "context": "The values are generated from a pre-trained image captioning module following [45, 16], which identifies salient regions in an image, and generates a caption for each of these regions.", "startOffset": 78, "endOffset": 86}, {"referenceID": 15, "context": "The values are generated from a pre-trained image captioning module following [45, 16], which identifies salient regions in an image, and generates a caption for each of these regions.", "startOffset": 78, "endOffset": 86}, {"referenceID": 23, "context": "Preprocessing: The video descriptions are tokenized using the wordpunct tokenizer from the NLTK toolbox[24].", "startOffset": 103, "endOffset": 107}, {"referenceID": 32, "context": "VGG-Encoder uses features encoded from the last convolution layer in VGG-16 CNN [33] along with a LSTM decoder.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "Here, we define q = ht\u22121 for the key addressing following [25].", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "[28] explicitly learn a visual-semantic joint embedding model for exploiting the relationship between visual features and generated language, which is then used in a similar encoder-decoder framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47] utilizes a temporal attention mechanism for global attention apart from local attention using 3-D Convolution Networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] proposed an encoder to learn spatial-temporal features across frames, introducing a variant GRU with convolution operations (GRU-RCN).", "startOffset": 0, "endOffset": 3}, {"referenceID": 48, "context": "[49] model the decoder as a paragraph generator, describing the videos over multiple sentences using stacked LSTMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "We evaluate our approach using standard evaluation metrics for video captioning tasks, namely BLEU [29], METEOR [22] and CIDEr[39] to compare the generated sequences with the human annotations.", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "We evaluate our approach using standard evaluation metrics for video captioning tasks, namely BLEU [29], METEOR [22] and CIDEr[39] to compare the generated sequences with the human annotations.", "startOffset": 112, "endOffset": 116}, {"referenceID": 38, "context": "We evaluate our approach using standard evaluation metrics for video captioning tasks, namely BLEU [29], METEOR [22] and CIDEr[39] to compare the generated sequences with the human annotations.", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "We use the code accompanying the Microsoft COCO Evaluation script [8] to obtain the results reported in the paper.", "startOffset": 66, "endOffset": 69}, {"referenceID": 50, "context": "We train our network parameters \u03b8 through first order stochastic gradient-based optimization with an adaptive learning rate using the Adadelta [51] optimizer.", "startOffset": 143, "endOffset": 147}, {"referenceID": 5, "context": "We optimize hyperparameters, namely number of hidden units in Decoder LSTM, key addressing LSTM, learning rate, word embedding dimension for the log loss using random search [6].", "startOffset": 174, "endOffset": 177}, {"referenceID": 32, "context": "We test our proposed model in the basic setting with visual features extracted from a pretrained VGG-16 [33] and GoogLeNet[38] models, as depicted by the first two lines of Table 1.", "startOffset": 104, "endOffset": 108}, {"referenceID": 37, "context": "We test our proposed model in the basic setting with visual features extracted from a pretrained VGG-16 [33] and GoogLeNet[38] models, as depicted by the first two lines of Table 1.", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "The key addressing in these models, follows the basic approach as in [25], i.", "startOffset": 69, "endOffset": 73}, {"referenceID": 39, "context": "These models are able to outperform the baselines of S2VT [40], and the Basic Encoder-Decoder model introduced in [47] on all three metrics.", "startOffset": 58, "endOffset": 62}, {"referenceID": 46, "context": "These models are able to outperform the baselines of S2VT [40], and the Basic Encoder-Decoder model introduced in [47] on all three metrics.", "startOffset": 114, "endOffset": 118}, {"referenceID": 41, "context": "We observe that using features from pretrained GoogleNet rather than VGG-16 improves on the results, following [42].", "startOffset": 111, "endOffset": 115}, {"referenceID": 46, "context": "Using GoogleNet to extract features, we achieve significantly better results compared to the temporal attention introduced by [47], where they do not utilize attention distribution from previous time steps further in the model.", "startOffset": 126, "endOffset": 130}, {"referenceID": 46, "context": "Additionally, [47]", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "This model, referred to as m-KeyAddressing, outperforms strong baselines from [27] by a significant margin on BLEU@4.", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "While the improvements on METEOR are significant compared to tKeyAddressing, [27] learns a hierarchical representation of the video, thereby performing slightly better.", "startOffset": 77, "endOffset": 81}, {"referenceID": 48, "context": "In the current setting, our model is unable to outperform [49] which uses a stacked LSTM at the decoder and generates more granular descriptions.", "startOffset": 58, "endOffset": 62}, {"referenceID": 1, "context": "A possible approach for improvement in this direction is using more sophisticated regularizers for training the decoder, as proposed in [2, 21].", "startOffset": 136, "endOffset": 143}, {"referenceID": 20, "context": "A possible approach for improvement in this direction is using more sophisticated regularizers for training the decoder, as proposed in [2, 21].", "startOffset": 136, "endOffset": 143}, {"referenceID": 3, "context": "It is to be noted, that we do not finetune the encoder, or the image captioning module as compared to [4] which finetunes the encoder CNN on UCF101 action recognition set[34].", "startOffset": 102, "endOffset": 105}, {"referenceID": 33, "context": "It is to be noted, that we do not finetune the encoder, or the image captioning module as compared to [4] which finetunes the encoder CNN on UCF101 action recognition set[34].", "startOffset": 170, "endOffset": 174}, {"referenceID": 48, "context": "as in [49], [4], [40].", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "as in [49], [4], [40].", "startOffset": 12, "endOffset": 15}, {"referenceID": 39, "context": "as in [49], [4], [40].", "startOffset": 17, "endOffset": 21}], "year": 2017, "abstractText": "Deep Neural Network architectures with external memory components allow the model to perform inference and capture long term dependencies, by storing information explicitly. In this paper, we generalize Key-Value Memory Networks to a multimodal setting, introducing a novel keyaddressing mechanism to deal with sequence-to-sequence models. The advantages of the framework are demonstrated on the task of video captioning, i.e generating natural language descriptions for videos. Conditioning on the previous time-step attention distributions for the key-value memory slots, we introduce a temporal structure in the memory addressing schema. The proposed model naturally decomposes the problem of video captioning into vision and language segments, dealing with them as key-value pairs. More specifically, we learn a semantic embedding (v) corresponding to each frame (k) in the video, thereby creating (k, v) memory slots. This allows us to exploit the temporal dependencies at multiple hierarchies (in the recurrent keyaddressing; and in the language decoder). Exploiting this flexibility of the framework, we additionally capture spatial dependencies while mapping from the visual to semantic embedding. Extensive experiments on the Youtube2Text dataset demonstrate usefulness of recurrent key-addressing, while achieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art models.", "creator": "LaTeX with hyperref package"}}}