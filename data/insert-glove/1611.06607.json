{"id": "1611.06607", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs", "abstract": "recieving Recent queene progress on image captioning has made 60f it possible to yugansk generate novel sentences squeamish describing images lgus in afpfl natural himowitz language, hicks but handique compressing an image into a mannin single eichst\u00e4tt sentence barkdoll can describe evon visual coutaz content in 172 only poona coarse detail. yuanpei While tantrism one new yeshua captioning farrago approach, votta dense eru captioning, tiru can potentially describe cayes images in bargh finer mid-month levels hosannas of gianniotis detail w2 by 3rs captioning many regions patristics within an image, 4,578 it in sepak turn is unable snus to produce tehrani a coherent story spinefarm for lauryn an image. incommensurate In naudero this paper dillion we overcome bajnoksag these monocerotis limitations annalee by muzik generating segregate entire paragraphs reconvenes for describing images, which can hornblower tell maylin detailed, unified scheibner stories. We develop a model that mish decomposes autoinfobank both images and totes paragraphs into their constituent parts, tgm detecting 14,300 semantic cardstock regions rabbet in images zimbabweans and using programmable a rubey hierarchical soedergren recurrent generalisation neural peugot network to reason vecchione about khloe language. Linguistic analysis confirms wisch the taik complexity composition of the paragraph abrahamson generation task, sedley and thorough helotiaceae experiments commodores on gabros a myalgic new dataset of image two-wheel and qube paragraph worldnetdaily pairs demonstrate azra the unclassified effectiveness of pupils our approach.", "histories": [["v1", "Sun, 20 Nov 2016 23:10:51 GMT  (1782kb,D)", "http://arxiv.org/abs/1611.06607v1", null], ["v2", "Mon, 10 Apr 2017 17:59:15 GMT  (1784kb,D)", "http://arxiv.org/abs/1611.06607v2", "CVPR 2017 spotlight"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["jonathan krause", "justin johnson", "ranjay krishna", "li fei-fei"], "accepted": false, "id": "1611.06607"}, "pdf": {"name": "1611.06607.pdf", "metadata": {"source": "CRF", "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs", "authors": ["Jonathan Krause", "Justin Johnson", "Ranjay Krishna", "Li Fei-Fei"], "emails": ["jkrause@cs.stanford.edu", "jcjohns@cs.stanford.edu", "ranjaykrishna@cs.stanford.edu", "feifeili@cs.stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "Vision is the primary sensory modality for human perception, and language is our most powerful tool for communicating with the world. Building systems that can simultaneously understand visual stimuli and describe them in natural language is therefore a core problem in both computer vision and artificial intelligence as a whole. With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27]. While the success of these methods is encouraging, they all share one key limitation: detail. By only describing images with a single high-level sentence, there is a fundamental upper-bound on the quantity and quality of information approaches can produce.\nOne recent alternative to sentence-level captioning is the task of dense captioning [10], which overcomes this limitation by detecting many regions of interest in an image and describing each with a short phrase. By extending the task of object detection to include natural language description,\ndense captioning describes images in considerably more detail than standard image captioning. However, this comes at a cost: descriptions generated for dense captioning are not coherent, i.e. they do not form a cohesive whole describing the entire image.\nIn this paper we address the shortcomings of both traditional image captioning and the recently-proposed dense\nar X\niv :1\n61 1.\n06 60\n7v 1\n[ cs\n.C V\n] 2\n0 N\nov 2\n01 6\nimage captioning by introducing the task of generating paragraphs that richly describe images (Fig. 1). Paragraph generation combines the strengths of these tasks but does not suffer from their weaknesses \u2013 like traditional captioning, paragraphs give a coherent natural language description for images, but like dense captioning, they can do so in finegrained detail.\nGenerating paragraphs for images is challenging, requiring both fine-grained image understanding and long-term language reasoning. To overcome these challenges, we propose a model that decomposes images and paragraphs into their constituent parts: We break images into semantically meaningful pieces by detecting objects and other regions of interest, and we reason about language with a hierarchical recurrent neural network, decomposing paragraphs into their corresponding sentences. In addition, we also demonstrate for the first time the ability to transfer visual and linguistic knowledge from large-scale region captioning [15], which we show has the ability to improve paragraph generation.\nTo validate our method, we collected a dataset of image and paragraph pairs, which complements the whole-image and region-level annotations of MS COCO [19] and Visual Genome [15]. To validate the complexity of the paragraph generation task, we performed a linguistic analysis of our collected paragraphs, comparing them to sentence-level image captioning. We compare our approach with numerous baselines, showcasing the benefits of hierarchical modeling for generating descriptive paragraphs.\nThe rest of this paper is organized as follows: Sec. 2 overviews related work in image captioning and hierarchical RNNs, Sec. 3 introduces the paragraph generation task, describes our newly-collected dataset, and performs a simple linguistic analysis on it, Sec. 4 details our model for paragraph generation, Sec. 5 contains experiments, and Sec. 6 concludes with discussion."}, {"heading": "2. Related Work", "text": "Image Captioning Building connections between visual and textual data has been a longstanding goal in computer vision. One line of work treats the problem as a ranking task, using images to retrieve relevant captions from a database and vice-versa [7, 9, 12]. Due to the compositional nature of language, it is unlikely that any database will contain all possible image captions; therefore another line of work focuses on generating captions directly. Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text. Similar methods have also been applied to generate captions for videos [5, 29, 32].\nA handful of approaches to image captioning reason not only about whole images but also image regions. Xu et al. [28] generate captions using a recurrent network with\nattention, so that for each generated word the model produces a distribution over image regions. In contrast to their work, which uses cells in a coarse grid as image regions, we use semantically meaningful regions of interest. Karpathy and Fei-Fei [11] use a ranking loss to align image regions with sentence fragments but do not do generation with the model. Johnson et al. [10] introdue the task of dense captioning, which detects and describes regions of interest, but these descriptions are independent and do not form a coherent whole.\nHierarchical Recurrent Networks In order to generate a paragraph description, a model must reason about longterm linguistic structures spanning multiple sentences. Due to vanishing gradients, recurrent neural networks trained with stochastic gradient descent often struggle to learn longterm dependencies. Alternative recurrent architectures such as long-short term memory (LSTM) [8] help alleviate this problem through a gating mechanism that improves gradient flow. Another solution is a hierarchical recurrent network, where the architecture is designed such that different parts of the model operate on different time scales.\nEarly pioneering work applied hierarchical recurrent networks to simple algorithmic problems [6]. The Clockwork RNN [14] uses a related technique for audio signal generation, spoken word classification, and handwriting recognition; a similar hierarchical architecture was also used in [1] for speech recognition. In these approaches, each recurrent unit is updated on a fixed schedule: some units are updated on every timestep, while other units might be updated every other or every fourth timestep. This type of hierarchy helps reduce the vanishing gradient problem, but the hierarchy of the model does not directly reflect the hierarchy of the output sequence.\nMore related to our work are hierarchical architectures that directly mirror the hierarchy of language. Li et al. [17] introduce a hierarchical autoencoder, and Lin et al. [18] use different recurrent units to model sentences and words. Most similar to our work is Yu et al. [32], who generate multisentence descriptions for cooking videos using a different hierarchical model. In their setting, strong temporal dependencies are present, which allows them to generate coherent paragraphs. In contrast, due to the less constrained setting in our work, our method has to learn in a much more generic fashion and has been made simpler as a result, relying more on learning the interplay between sentences. Additionally, our method reasons about semantic regions in images, which both enables the transfer of information from these regions and leads to more interpretability in generation."}, {"heading": "3. Paragraphs are Different", "text": "To what extent does describing images with paragraphs differ from sentence-level captioning? To answer this ques-\ntion, we collected a novel dataset of paragraph annotations, comparised of 19,551 MS COCO [19] and Visual Genome [15] images, where each image has been annotated with a paragraph description. Annotations were collected on Amazon Mechanical Turk, using U.S. workers with at least 5,000 accepted HITs and an acceptance rate of 98% or greater1, and were additionally subject to automatic and manual spot checks on quality. Fig. 1 demonstrates an example, comparing our collected paragraph with the five corresponding sentence-level captions from MS COCO. Though it is clear that the paragraph is longer and more descriptive than any one sentence, we note further that a single paragraph can be more detailed than all five sentence captions, even when combined. This occurs because of redundancy in sentencelevel captions \u2013 while each caption might use slightly different words to describe the image, since all sentence captions have the goal of describing the image as a whole, they are fundamentally limited in terms of both diversity and their total information.\nWe quantify these observations along with various other statistics of language in Tab. 1. For example, we find that each paragraph is roughly six times as long as the average sentence caption, and the individual sentences in each paragraph are of comparable length as sentence-level captions. To examine the issue of sentence diversity, we compute the average CIDEr [26] similarity between COCO sentences for each image and between the individual sentences in each collected paragraph, defining the final diversity score as 100 minus the average CIDEr similarity. Viewed through this metric, the difference in diversity is striking \u2013 sentences within paragraphs are substantially more diverse than sentence captions, with a diversity score of 70.49 compared to only 19.01. This quantifiable evidence demonstrates that sentences in paragraphs provide significantly more information\n1To be publically released.\nabout images. Diving deeper, we performed a simple linguistic analysis on COCO sentences and our collected paragraphs, comprised of annotating each word with a part of speech tag from Penn Treebank via Stanford CoreNLP [20] and aggregating parts of speech into higher-level linguistic categories. A few common parts of speech are given in Tab. 1. As a proportion, paragraphs have somewhat more verbs and pronouns, a comparable frequency of adjectives, and somewhat fewer nouns. Given the nature of paragraphs, this makes sense \u2013 longer descriptions go beyond the presence of a few salient objects and include information about their properties and relationships. We also note but do not quantify that paragraphs exhibit higher frequencies of more complex linguistic phenomena, e.g. coreference occurring in Fig. 1, wherein sentences refer to either \u201ctwo children\u201d, \u201cone little girl and one little boy\u201d, \u201cthe girl\u201d, or \u201cthe boy.\u201d We belive that these types of long-range phenomena are a fundamental property of descriptive paragraphs with human-like language and cannot be adequately explored with sentence-level captions."}, {"heading": "4. Method", "text": "Overview Our model takes an image as input, generating a natural-language paragraph describing it, and is designed to take advantage of the compositional structure of both images and paragraphs. Fig. 2 provides an overview. We first decompose the input image by detecting objects and other regions of interest, then aggregate features across these regions to produce a pooled representation richly expressing the image semantics. This feature vector is taken as input by a hierarchical recurrent neural network composed of two levels: a sentence RNN and a word RNN. The sentence RNN receives the image features, decides how many sentences to generate in the resulting paragraph, and produces an input topic vector for each sentence. Given this topic vector, the word RNN generates the words of a single sentence. We also show how to transfer knowledge from a dense image captioning [10] task to our model for paragraph generation."}, {"heading": "4.1. Region Detector", "text": "The region detector receives an input image of size 3\u00d7 H \u00d7W , detects regions of interest, and produces a feature vector of dimension D = 4096 for each region. Our region detector follows the design of [24, 10], and we provide a summary here for completeness: The image is resized so that its longest edge is 720 pixels, and is then passed through a convolutional network initialized from the 16-layer VGG network [25]. The resulting feature map is processed by a region proposal network [24], which regresses from a set of anchors to propose regions of interest. These regions of interest are projected onto the convolutional feature map, and the corresponding region of the feature map is reshaped to a fixed size using bilinear interpolation and processed by\ntwo fully-connected layers to give a vector of dimension D for each region.\nGiven a dataset of images and ground-truth regions of interest, the region detector can be trained in an end-to-end fashion as in [24] for object detection and [10] for dense captioning. Since paragraph descriptions do not have annotated groundings to regions of interest, we use a region detector trained for dense image captioning on the Visual Genome dataset [15], using the publicly available implementation of [10]. This produces M = 50 detected regions.\nOne alternative worth noting is to use a region detector trained strictly for object detection, rather than dense captioning. Although such an approach would likely capture many salient objects in an image, its paragraphs would suffer: an ideal paragraph describes not only objects, but also scenery and relationships, which are better captured by dense captioning task that captures all noteworthy elements of a scene."}, {"heading": "4.2. Region Pooling", "text": "The region detector produces a set of vectors v1, . . . , vM \u2208 RD, each describing a different region in the input image. We wish to aggregate these vectors into a single pooled vector vp \u2208 RP that compactly describes the content of the image. To this end, we learn a projection matrix Wpool \u2208 RP\u00d7D and bias bpool \u2208 RP ; the pooled vector vp is computed by projecting each region vector using Wpool and taking an elementwise maximum, so that vp = max M i=1(Wpoolvi + bpool). Alternative approaches for representing collections of regions, such as spatial attention [28], may also be possible, and are complementary to the model proposed in this paper."}, {"heading": "4.3. Hierarchical Recurrent Network", "text": "The pooled region vector vp \u2208 RP is given as input to a hierarchical neural language model composed of two\nmodules: a sentence RNN and a word RNN. The sentence RNN is responsible for deciding the number of sentences S that should be in the generated paragraph and for producing a P -dimensional topic vector for each of these sentences. Given a topic vector for a sentence, the word RNN generates the words of that sentence. We adopt the standard LSTM architecture [8] for both the word RNN and the sentence RNN.\nAs an alternative to this hierarchical approach, one could instead use a non-hierarchical recurrent language model to directly generate the words of a paragraph, treating the endof-sentence token as another word in the vocabulary. Our hierarchical model is advantageous because it reduces the length of time over which the recurrent networks must reason. Our paragraphs contain an average of 67.5 words (Tab. 1), so a non-hierarchical approach must reason over dozens of time steps, which is extremely difficult for language models. However, since our paragraphs contain an average of 5.7 sentences, each with an average of 11.9 words, both the paragraph and sentence RNNs need only reason over much shorter time-scales, making learning an appropriate representation much more tractable.\nSentence RNN The sentence RNN is a single-layer LSTM with hidden size H = 512 and initial hidden and cell states set to zero. At each time step, the sentence RNN receives the pooled region vector vp as input, and in turn produces a sequence of hidden states h1, . . . , hS \u2208 RH , one for each sentence in the paragraph. Each hidden state hi is used in two ways: First, a linear projection from hi and a logistic classifier are used to produce a distribution pi over the two states {CONTINUE = 0,STOP = 1} which determine whether the ith sentence is the last sentence in the paragraph. Second, the hidden state hi is fed through a two-layer fullyconnected network to produce the topic vector ti \u2208 RP for the ith sentence of the paragraph, which is the direct input to the word RNN.\nWord RNN The word RNN is a two-layer LSTM with hidden size H = 512, which, given a topic vector ti \u2208 RP from the sentence RNN, is responsible for generating the words of a sentence. We follow the input formulation of [27]: the first and second inputs to the RNN are the topic vector and a special START token, and subsequent inputs are learned embedding vectors for the words of the sentence. At each timestep the hidden state of the last LSTM layer is used to predict a distribution over the words in the vocabulary, and a special END token signals the end of a sentence. After each Word RNN has generated the words of their respective sentences, these sentences are finally concatenated to form the generated paragraph."}, {"heading": "4.4. Training and Sampling", "text": "Training data consists of pairs (x, y), with x an image and y a ground-truth paragraph description for that image, where y has S sentences, the ith sentence has Ni words, and yij is the jth word of the ith sentence. After computing the pooled region vector vp for the image, we unroll the sentence RNN for S timesteps, giving a distribution pi over the {CONTINUE,STOP} states for each sentence. We feed the sentence topic vectors to S copies of the word RNN, unrolling the ith copy for Ni timesteps, producing distributions pij over each word of each sentence. Our training loss `(x, y) for the example (x, y) is a weighted sum of two cross-entropy terms: a sentence loss `sent on the stopping distribution pi, and a word loss `word on the word distribution pij :\n`(x, y) =\u03bbsent S\u2211 i=1 `sent(pi, I [i = S]) (1)\n+\u03bbword S\u2211 i=1 Ni\u2211 j=1 `word(pij , yij) (2)\nTo generate a paragraph for an image, we run the sentence RNN forward until the stopping probability pi(STOP) exceeds a threshold TSTOP or after SMAX sentences, whichever comes first. We then sample sentences from the word RNN, choosing the most likely word at each timestep and stopping after choosing the STOP token or after NMAX words. We set the parameters TSTOP = 0.5, SMAX = 6, and NMAX = 50 based on validation set performance."}, {"heading": "4.5. Transfer Learning", "text": "Transfer learning has become pervasive in computer vision. For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such\nas the 16-layer VGG network [25]. Initializing from a pretrained convolutional network allows a form of knowledge transfer from large classification datasets, and is particularly effective on datasets of limited size. Might transfer learning also be useful for paragraph generation?\nWe propose to utilize transfer learning in two ways. First, we initialize our region detection network from a model trained for dense image captioning [10]; although our model is end-to-end differentiable, we keep this sub-network fixed during training both for efficiency and also to prevent overfitting. Second, we can also initialize the word embedding vectors, recurrent network weights, and output linear projection of the word RNN from a language model that had been trained on region-level captions [10], fine-tuning these parameters during training to be better suited for the task of paragraph generation. Parameters for tokens not present in the region model are initialized from the parameters for the UNK token. This initialization strategy allows our model to utilize linguistic knowledge learned on large-scale region caption datasets [15] to produce better paragraph descriptions, and we validate the efficacy of this strategy in our experiments."}, {"heading": "5. Experiments", "text": "In this section we describe our paragraph generation experiments on the collected data described in Sec. 3, which we divide into 14,575 training, 2,487 validation, and 2,489 testing images."}, {"heading": "5.1. Baselines", "text": "Sentence-Concat: To demonstrate the difference between sentence-level and paragraph captions, this baseline samples and concatenates five sentence captions from a model [11] trained on MS COCO captions [19]. The first sentence uses beam search (beam size = 2) and the rest are sampled. The motivation for this is as follows: the image captioning model first produces the sentence that best describes the image as a whole (according to the model), and subsequent sentences use sampling in order to generate a diverse range of sentences, since the alternative is to repeat the same sentence from beam search. We have validated that this approach works better than using either only beam search or only sampling, as the intent is to make the strongest possible comparison at a task-level to standard image captioning. We also note that, while Sentence-Concat is trained on MS COCO, all images in our dataset are also in MS COCO our descriptions were also written by users on Amazon Mechanical Turk.\nImage-Flat: This model uses a flat representation for both images and language, and is equivalent to the standard image captioning model NeuralTalk [11]. It takes the whole image\nas input, and decodes into a paragraph token by token. We use the publically available implementation of [11], which uses the 16-layer VGG network [25] to extract CNN features and projects them as input into an LSTM [8], training the whole model jointly end-to-end.\nTemplate: This method represents a very different approach to generating paragraphs, similar in style to an openworld version of more classical methods like BabyTalk [16], which converts a structured representation of an image into text via a handful of manually specified templates. The first step of our template-based baseline is to detect and describe many regions in a given target image using a pre-trained dense captioning model [10], which produces a set of region descriptions tied with bounding boxes and detection scores. The region descriptions are parsed into a set of subjects, verbs, objects, and various modifiers according to part of speech tagging and a handful of TokensRegex [2] rules, which we find suffice to parse the vast majority (\u2265 99%) of the fairly simplistic and short region-level descriptions.\nEach parsed word is scored by the sum of its detection score and the log probability of the generated tokens in the original region description. Words are then merged into a coherent graph representing the scene, where each node combines all words with the same text and overlapping bounding boxes. Finally, text is generated using the topN = 25 scored nodes, prioritizing subject-verb-object triples first in generation, and representing all other nodes with existential \u201cthere is/are\u201d statements.\nOther Baselines: \u201cRegions-Flat-Scratch\u201d uses a flat language model for decoding and initializes it from scratch. The language model input is the projected and pooled regionlevel image features. \u201cRegions-Flat-Pretrained\u201d uses a pretrained language model. These baselines are included to show the benefits of decomposing the image into regions and pre-training the language model."}, {"heading": "5.2. Implementation Details", "text": "All baseline neural language models use two layers of LSTM [8] units with 512 dimensions. The feature pooling\ndimension P is 1024, and we set \u03bbsent = 5.0 and \u03bbword = 1.0 based on validation set performance. Training is done via stochastic gradient descent with Adam [13] learning rate updates, implemented in Torch. Of critical note is that model checkpoint selection is based on the best combined METEOR and CIDEr score on the validation set \u2013 although models tend to minimize validation loss fairly quickly, it takes much longer training for METEOR and CIDEr scores to stop improving."}, {"heading": "5.3. Main Results", "text": "We present our main results at generating paragraphs in Tab. 2, which are evaluated across six language metrics: CIDEr [26], METEOR [4], and BLEU-{1,2,3,4} [23]. The Sentence-Concat method performs poorly, achieving the lowest scores across all metrics. Its lackluster performance provides further evidence of the stark differences between single-sentence captioning and paragraph generation. Surprisingly, the hard-coded template-based approach performs reasonably well, particularly on CIDEr, METEOR, and BLEU-1, where it is competitive with some of the neural approaches. This makes sense: the template approach is provided with a strong prior about image content since it receives region-level captions [10] as input, and the many expletive \u201cthere is/are\u201d statements it makes, though uninteresting, are safe, resulting in decent scores. However, its relatively poor performance on BLEU-3 and BLEU-4 highlights the limitation of reasoning about regions in isolation \u2013 it is unable to produce much text relating regions to one another, and further suffers from a lack of \u201cconnective tissue\u201d that transforms paragraphs from a series of disconnected thoughts into a coherent whole.\nImage-Flat, trained on the task of paragraph generation, outperforms Sentence-Concat, and the region-based reasoning of Regions-Flat-Scratch improves results further on all metrics. Pre-training results in improvements on all metrics, and our full model, Regions-Hierarchical, achieves the highest scores among all methods on every metric except BLEU-4. One hypothesis for the mild superiority of Regions-Flat-Pretrained on BLEU-4 is that it is better able to reproduce words immediately at the end and beginning of\nsentences more exactly due to their non-hierarchical structure, providing a slight boost in BLEU scores.\nTo make these metrics more interpretable, we performed a human evaluation by collecting an additional paragraph for 500 randomly chosen images, with results in the last row of Tab. 2. As expected, humans produce superior descriptions to any automatic method, performing better on all language metrics considered. Of particular note is the large gap between humans our the best model on CIDEr and METEOR, which are both designed to correlate well with human judgment [26, 4]."}, {"heading": "5.4. Qualitative Results", "text": "We present qualitative results from our model and the Sentence-Concat and Template baselines in Fig. 3. Some interesting properties of our model\u2019s predictions include its use of coreference in the first example (\u201ca bus\u201d, \u201cit\u201d, \u201cthe bus\u201d) and its ability to capture relationships between objects in the second example. Also of note is the order in which our model chooses to describe the image: the first sentence tends to be fairly high level, middle sentences give some details about scene elements mentioned earlier in the description, and the last sentence often describes something in the background, which other methods are not able to capture. Anecdotally, we observed that this follows the same order with which most of our human annotators tended to describe images.\nThe failure case in the last row highlights another interesting phenomenon: even though our model was generally wrong about the semantics of the image, calling the girl\n\u201ca woman\u201d, it has learned that \u201cwoman\u201d is consistently associated with female pronouns (\u201cshe\u201d, \u201cshe\u201d, \u201cher hand\u201d, \u201cbehind her\u201d).\nIt is also worth noting the general behavior of the two baselines. Paragraphs from Sentence-Concat tend to be repetitive in sentence structure and are often simply inaccurate due to the sampling required to generate multiple sentences. On the other hand, the Template baseline is largely accurate, but has uninteresting language and lacks the ability to determine which things are most important to describe. In contrast, Regions-Hierarchical stays relevant for the most part and furthermore exhibits more interesting patterns of language."}, {"heading": "5.5. Paragraph Language Analysis", "text": "To shed a quantitative light on the linguistic phenomena generated, in Tab. 3 we show statistics of the language produced by a representative spread of methods.\nOur hierarchical approach generates text of similar average length and variance as human descriptions, with Sentence-Concat and the Template approach somewhat shorter and less varied in length. Sentence-Concat is also the least diverse method, though all automatic methods remain far less diverse than human sentences, indicating ample opportunity for improvement. According to this diversity metric, it can be observed that the Template approach is actually the most diverse automatic method, which may be attributed to how the method is hard-coded to sequentially describe each region in the scene in turn, regardless of importance or how interesting such an output may be (see Fig. 3). While\nboth our hierarchical approach and the Template method produce text with similar portions of nouns and verbs as human paragraphs, only our approach was able to generate a reasonable quantity of pronouns. Our hierarchical method also had a much wider vocabulary compared to the Template approach, though Sentence-Concat, trained on hundreds of thousands of MS COCO [19] captions, is a bit larger."}, {"heading": "5.6. Generating Paragraphs from Fewer Regions", "text": "As an exploratory experiment in order to highlight the interpretability of our model, we investigate generating paragraphs from a smaller number of regions than the M = 50 used in the rest of this work. Instead, we only give our method access to the top few detected regions as input, with the hope that the generated paragraph focuses only on those particularly regions, preferring not to describe other parts of the image. The results for a handful of images are shown in Fig. 4. Although the input is extremely out of sample compared to the training data, the results are still quite reasonable \u2013 the model generates paragraphs describing the detected regions without much mention of objects or scenery outside of the detections. Taking the top-right image as an example, despite a few linguistic mistakes, the paragraph generated\nby our model mentions the batter, catcher, dirt, and grass, which all appear in the top detected regions, but does not pay heed to the pitcher or the umpire in the background."}, {"heading": "6. Conclusion", "text": "In this paper we have introduced the task of describing images with long, descriptive paragraphs, and presented a hierarchical neural approach for generation that leverages the compositional structure of both images and language. We have shown that paragraph generation is different from traditional image captioning and have tailored our model to suit these differences. Experimentally, we have demonstrated the advantages of our approach over traditional image captioning methods and shown how region-level knowledge can be effectively transferred to the paragraph captioning task. We have also demonstrated the benefits of our model in interpretability, showing how to generate descriptive paragraphs using only a subset of image regions. We anticipate further opportunities for knowledge transfer in tasks at the intersection of vision and language, and project that visual and lingual compositionality will continue to lie at the heart of effective paragraph generation."}], "references": [{"title": "Listen", "author": ["W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals"], "venue": "attend, and spell: A neural network for large vocabulary conversational speech recognition. In ICASSP", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Tokensregex: Defining cascaded regular expressions over tokens", "author": ["A.X. Chang", "C.D. Manning"], "venue": "Technical report, CSTR 2014-02, Department of Computer Science, Stanford University", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "CVPR", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["M. Denkowski", "A. Lavie"], "venue": "EACL Workshop on Statistical Machine Translation", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["S. El Hihi", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853\u2013 899", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "DenseCap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "NIPS", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A clockwork RNN", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "ICML", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1602.07332", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "M.-T. Luong", "D. Jurafsky"], "venue": "ACL", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["R. Lin", "S. Liu", "M. Yang", "M. Li", "M. Zhou", "S. Li"], "venue": "EMNLP", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "The stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J.R. Finkel", "S. Bethard", "D. McClosky"], "venue": "ACL (System Demonstrations), pages 55\u201360", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2):313\u2013330", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1993}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "ACL, pages 311\u2013318. Association for Computational Linguistics", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "CVPR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "Y. Bengio"], "venue": "attend, and tell: Neural image caption generation with visual attention. In ICML", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "ICCV", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "CVPR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": "CVPR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 84, "endOffset": 99}, {"referenceID": 30, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 84, "endOffset": 99}, {"referenceID": 8, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 84, "endOffset": 99}, {"referenceID": 14, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 84, "endOffset": 99}, {"referenceID": 2, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 178, "endOffset": 196}, {"referenceID": 4, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 178, "endOffset": 196}, {"referenceID": 10, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 178, "endOffset": 196}, {"referenceID": 20, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 178, "endOffset": 196}, {"referenceID": 26, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 178, "endOffset": 196}, {"referenceID": 9, "context": "One recent alternative to sentence-level captioning is the task of dense captioning [10], which overcomes this limitation by detecting many regions of interest in an image and describing each with a short phrase.", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "Here we show an image with its sentence-level captions from MS COCO [19] (top) and the paragraph used in this work (bottom).", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "In addition, we also demonstrate for the first time the ability to transfer visual and linguistic knowledge from large-scale region captioning [15], which we show has the ability to improve paragraph generation.", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "To validate our method, we collected a dataset of image and paragraph pairs, which complements the whole-image and region-level annotations of MS COCO [19] and Visual Genome [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": "To validate our method, we collected a dataset of image and paragraph pairs, which complements the whole-image and region-level annotations of MS COCO [19] and Visual Genome [15].", "startOffset": 174, "endOffset": 178}, {"referenceID": 6, "context": "One line of work treats the problem as a ranking task, using images to retrieve relevant captions from a database and vice-versa [7, 9, 12].", "startOffset": 129, "endOffset": 139}, {"referenceID": 8, "context": "One line of work treats the problem as a ranking task, using images to retrieve relevant captions from a database and vice-versa [7, 9, 12].", "startOffset": 129, "endOffset": 139}, {"referenceID": 11, "context": "One line of work treats the problem as a ranking task, using images to retrieve relevant captions from a database and vice-versa [7, 9, 12].", "startOffset": 129, "endOffset": 139}, {"referenceID": 15, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 4, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 10, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 20, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 26, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 29, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 4, "context": "Similar methods have also been applied to generate captions for videos [5, 29, 32].", "startOffset": 71, "endOffset": 82}, {"referenceID": 28, "context": "Similar methods have also been applied to generate captions for videos [5, 29, 32].", "startOffset": 71, "endOffset": 82}, {"referenceID": 31, "context": "Similar methods have also been applied to generate captions for videos [5, 29, 32].", "startOffset": 71, "endOffset": 82}, {"referenceID": 27, "context": "[28] generate captions using a recurrent network with attention, so that for each generated word the model produces a distribution over image regions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Karpathy and Fei-Fei [11] use a ranking loss to align image regions with sentence fragments but do not do generation with the model.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "[10] introdue the task of dense captioning, which detects and describes regions of interest, but these descriptions are independent and do not form a coherent whole.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Alternative recurrent architectures such as long-short term memory (LSTM) [8] help alleviate this problem through a gating mechanism that improves gradient flow.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Early pioneering work applied hierarchical recurrent networks to simple algorithmic problems [6].", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": "The Clockwork RNN [14] uses a related technique for audio signal generation, spoken word classification, and handwriting recognition; a similar hierarchical architecture was also used in [1] for speech recognition.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "The Clockwork RNN [14] uses a related technique for audio signal generation, spoken word classification, and handwriting recognition; a similar hierarchical architecture was also used in [1] for speech recognition.", "startOffset": 187, "endOffset": 190}, {"referenceID": 16, "context": "[17] introduce a hierarchical autoencoder, and Lin et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] use different recurrent units to model sentences and words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32], who generate multisentence descriptions for cooking videos using a different hierarchical model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Sentences COCO [19] Paragraphs Ours", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "between sentences of the same image, and part of speech distributions are aggregated from Penn Treebank [22] part of speech tags.", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "tion, we collected a novel dataset of paragraph annotations, comparised of 19,551 MS COCO [19] and Visual Genome [15] images, where each image has been annotated with a paragraph description.", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "tion, we collected a novel dataset of paragraph annotations, comparised of 19,551 MS COCO [19] and Visual Genome [15] images, where each image has been annotated with a paragraph description.", "startOffset": 113, "endOffset": 117}, {"referenceID": 25, "context": "To examine the issue of sentence diversity, we compute the average CIDEr [26] similarity between COCO sentences for each image and between the individual sentences in each collected paragraph, defining the final diversity score as 100 minus the average CIDEr similarity.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "Diving deeper, we performed a simple linguistic analysis on COCO sentences and our collected paragraphs, comprised of annotating each word with a part of speech tag from Penn Treebank via Stanford CoreNLP [20] and aggregating parts of speech into higher-level linguistic categories.", "startOffset": 205, "endOffset": 209}, {"referenceID": 9, "context": "We also show how to transfer knowledge from a dense image captioning [10] task to our model for paragraph generation.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "Our region detector follows the design of [24, 10], and we provide a summary here for completeness: The image is resized so that its longest edge is 720 pixels, and is then passed through a convolutional network initialized from the 16-layer VGG network [25].", "startOffset": 42, "endOffset": 50}, {"referenceID": 9, "context": "Our region detector follows the design of [24, 10], and we provide a summary here for completeness: The image is resized so that its longest edge is 720 pixels, and is then passed through a convolutional network initialized from the 16-layer VGG network [25].", "startOffset": 42, "endOffset": 50}, {"referenceID": 24, "context": "Our region detector follows the design of [24, 10], and we provide a summary here for completeness: The image is resized so that its longest edge is 720 pixels, and is then passed through a convolutional network initialized from the 16-layer VGG network [25].", "startOffset": 254, "endOffset": 258}, {"referenceID": 23, "context": "The resulting feature map is processed by a region proposal network [24], which regresses from a set of anchors to propose regions of interest.", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "Given a dataset of images and ground-truth regions of interest, the region detector can be trained in an end-to-end fashion as in [24] for object detection and [10] for dense captioning.", "startOffset": 130, "endOffset": 134}, {"referenceID": 9, "context": "Given a dataset of images and ground-truth regions of interest, the region detector can be trained in an end-to-end fashion as in [24] for object detection and [10] for dense captioning.", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "Since paragraph descriptions do not have annotated groundings to regions of interest, we use a region detector trained for dense image captioning on the Visual Genome dataset [15], using the publicly available implementation of [10].", "startOffset": 175, "endOffset": 179}, {"referenceID": 9, "context": "Since paragraph descriptions do not have annotated groundings to regions of interest, we use a region detector trained for dense image captioning on the Visual Genome dataset [15], using the publicly available implementation of [10].", "startOffset": 228, "endOffset": 232}, {"referenceID": 27, "context": "Alternative approaches for representing collections of regions, such as spatial attention [28], may also be possible, and are complementary to the model proposed in this paper.", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "We adopt the standard LSTM architecture [8] for both the word RNN and the sentence RNN.", "startOffset": 40, "endOffset": 43}, {"referenceID": 26, "context": "We follow the input formulation of [27]: the first and second inputs to the RNN are the topic vector and a special START token, and subsequent inputs are learned embedding vectors for the words of the sentence.", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 35, "endOffset": 39}, {"referenceID": 4, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 61, "endOffset": 76}, {"referenceID": 10, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 61, "endOffset": 76}, {"referenceID": 26, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 61, "endOffset": 76}, {"referenceID": 27, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 61, "endOffset": 76}, {"referenceID": 24, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 318, "endOffset": 322}, {"referenceID": 9, "context": "First, we initialize our region detection network from a model trained for dense image captioning [10]; although our model is end-to-end differentiable, we keep this sub-network fixed during training both for efficiency and also to prevent overfitting.", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "Second, we can also initialize the word embedding vectors, recurrent network weights, and output linear projection of the word RNN from a language model that had been trained on region-level captions [10], fine-tuning these parameters during training to be better suited for the task of paragraph generation.", "startOffset": 200, "endOffset": 204}, {"referenceID": 14, "context": "This initialization strategy allows our model to utilize linguistic knowledge learned on large-scale region caption datasets [15] to produce better paragraph descriptions, and we validate the efficacy of this strategy in our experiments.", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "Sentence-Concat: To demonstrate the difference between sentence-level and paragraph captions, this baseline samples and concatenates five sentence captions from a model [11] trained on MS COCO captions [19].", "startOffset": 169, "endOffset": 173}, {"referenceID": 18, "context": "Sentence-Concat: To demonstrate the difference between sentence-level and paragraph captions, this baseline samples and concatenates five sentence captions from a model [11] trained on MS COCO captions [19].", "startOffset": 202, "endOffset": 206}, {"referenceID": 10, "context": "Image-Flat: This model uses a flat representation for both images and language, and is equivalent to the standard image captioning model NeuralTalk [11].", "startOffset": 148, "endOffset": 152}, {"referenceID": 10, "context": "38 Image-Flat ([11]) 12.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "We use the publically available implementation of [11], which uses the 16-layer VGG network [25] to extract CNN features and projects them as input into an LSTM [8], training the whole model jointly end-to-end.", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "We use the publically available implementation of [11], which uses the 16-layer VGG network [25] to extract CNN features and projects them as input into an LSTM [8], training the whole model jointly end-to-end.", "startOffset": 92, "endOffset": 96}, {"referenceID": 7, "context": "We use the publically available implementation of [11], which uses the 16-layer VGG network [25] to extract CNN features and projects them as input into an LSTM [8], training the whole model jointly end-to-end.", "startOffset": 161, "endOffset": 164}, {"referenceID": 15, "context": "Template: This method represents a very different approach to generating paragraphs, similar in style to an openworld version of more classical methods like BabyTalk [16], which converts a structured representation of an image into text via a handful of manually specified templates.", "startOffset": 166, "endOffset": 170}, {"referenceID": 9, "context": "The first step of our template-based baseline is to detect and describe many regions in a given target image using a pre-trained dense captioning model [10], which produces a set of region descriptions tied with bounding boxes and detection scores.", "startOffset": 152, "endOffset": 156}, {"referenceID": 1, "context": "The region descriptions are parsed into a set of subjects, verbs, objects, and various modifiers according to part of speech tagging and a handful of TokensRegex [2] rules, which we find suffice to parse the vast majority (\u2265 99%) of the fairly simplistic and short region-level descriptions.", "startOffset": 162, "endOffset": 165}, {"referenceID": 7, "context": "All baseline neural language models use two layers of LSTM [8] units with 512 dimensions.", "startOffset": 59, "endOffset": 62}, {"referenceID": 12, "context": "Training is done via stochastic gradient descent with Adam [13] learning rate updates, implemented in Torch.", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "2, which are evaluated across six language metrics: CIDEr [26], METEOR [4], and BLEU-{1,2,3,4} [23].", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "2, which are evaluated across six language metrics: CIDEr [26], METEOR [4], and BLEU-{1,2,3,4} [23].", "startOffset": 71, "endOffset": 74}, {"referenceID": 22, "context": "2, which are evaluated across six language metrics: CIDEr [26], METEOR [4], and BLEU-{1,2,3,4} [23].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "This makes sense: the template approach is provided with a strong prior about image content since it receives region-level captions [10] as input, and the many expletive \u201cthere is/are\u201d statements it makes, though uninteresting, are safe, resulting in decent scores.", "startOffset": 132, "endOffset": 136}, {"referenceID": 25, "context": "Of particular note is the large gap between humans our the best model on CIDEr and METEOR, which are both designed to correlate well with human judgment [26, 4].", "startOffset": 153, "endOffset": 160}, {"referenceID": 3, "context": "Of particular note is the large gap between humans our the best model on CIDEr and METEOR, which are both designed to correlate well with human judgment [26, 4].", "startOffset": 153, "endOffset": 160}, {"referenceID": 18, "context": "Our hierarchical method also had a much wider vocabulary compared to the Template approach, though Sentence-Concat, trained on hundreds of thousands of MS COCO [19] captions, is a bit larger.", "startOffset": 160, "endOffset": 164}], "year": 2016, "abstractText": "Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.", "creator": "LaTeX with hyperref package"}}}