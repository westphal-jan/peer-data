{"id": "1611.06757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Non-Local Color Image Denoising with Convolutional Neural Networks", "abstract": "accomack We propose a pedauye novel deep network architecture bertens for bowes grayscale and color glamorizes image denoising kassi that clemens is based on kh\u0101n a humoring non - abcs local pneumatics image thygesen model. usagi Our osoba motivation for garey the overall design of suevic the proposed network mosso stems from variational methods removal that vinegary exploit the nelia inherent non - bathypelagic local self - similarity property of gugl natural images. clamshell We dobrzanski build on 21-9 this concept ibad and jews introduce deep networks fireteam that moodna perform 27-time non - pinneberg local dogubayazit processing and 1970-1980 at erv the same gollob time they voxels significantly benefit derivable from interring discriminative learning. chmielewski Experiments on hairpin the amoron Berkeley segmentation dataset, comparing cellar several warckie state - of - gundem the - art methods, hyperventilation show that bahiya the 28-11 proposed pino non - associa\u00e7\u00e3o local garnaoui models thinprep achieve polymathic the productivity best l\u1ee3i reported denoising performance both for monomeric grayscale and color images for all the 12.5-percent tested lochridge noise levels. It parslow is wart also procardia worth taura noting prue that 3.925 this increase in repentigny performance verdelho comes 500-m at buckland no literalism extra gilbride cost on the gpmg capacity assab of phosphine the bullrich network compared to phonemes existing alternative klees deep diversify network embeddedness architectures. lmt In delma addition, cams we highlight a averil direct 3-g link esquirol of the proposed nspcc non - daning local models dispenser to convolutional 62-60 neural networks. 42.90 This motherhouse connection mbalula is junkers of pietrus significant importance since it allows our models yunxia to take olum full advantage piezoelectric of himyarite the ulaanbaatar latest advances on 1944-1945 GPU presdient computing 9-for-9 in deep learning vanara and makes mbalula them nagavalli amenable to sogod efficient implementations student-edited through their inherent parallelism.", "histories": [["v1", "Mon, 21 Nov 2016 12:36:10 GMT  (7536kb,D)", "http://arxiv.org/abs/1611.06757v1", "15 pages"], ["v2", "Mon, 10 Jul 2017 20:06:26 GMT  (7537kb,D)", "http://arxiv.org/abs/1611.06757v2", "15 pages, accepted to CVPR 2017"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["stamatios lefkimmiatis"], "accepted": false, "id": "1611.06757"}, "pdf": {"name": "1611.06757.pdf", "metadata": {"source": "CRF", "title": "Non-local Color Image Denoising with Convolutional Neural Networks", "authors": ["Stamatios Lefkimmiatis"], "emails": ["s.lefkimmiatis@skoltech.ru"], "sections": [{"heading": "1. Introduction", "text": "Deep learning methods have been successfully applied in various computer vision tasks, including image classification [16,20] and object detection [11,29], and have dramatically improved the performance of these systems, setting the new state-of-the-art. Recently, very promising results have also been reported for image processing applications such as image restoration [5, 39], super-resolution [18] and optical flow [1].\nThe significant boost in performance achieved by deep networks can be mainly attributed to their advanced modeling capabilities, thanks to their deep structure and the presence of non-linearities that are combined with discriminative learning on large training datasets. However, most of the current deep learning methods developed for im-\nage restoration tasks are based on general network architectures that do not fully exploit problem-specific knowledge. It is thus reasonable to expect that incorporating such information could lead to further improvements in performance. Only very recently, Schmidt and Roth [34] and Chen and Pock [6] introduced deep networks whose architecture is specifically tailored to certain image restoration problems. However, even in these cases, the resulting models are local ones and do not take into account the inherent non-local self-similarity property of natural images. On the other hand, conventional methods that have exploited this property have been shown to gain significant improvements compared to standard local approaches. A notable example is the Block Matching and 3D Collaborative Filtering (BM3D) method [7] which is a very efficient and highly engineered approach that held the state-of-the-art record in image denoising for almost a decade.\nIn this work, motivated by the recent advances in deep learning and relying on the rich body of algorithmic ideas\nar X\niv :1\n61 1.\n06 75\n7v 1\n[ cs\n.C V\n] 2\n1 N\nov 2\n01 6\nthat have been developed in the past for tackling image reconstruction problems, we study deep network architectures for image denoising. Inspired by non-local variational methods and other related approaches, we design a network that performs non-local processing and at the same time it significantly benefits from discriminative learning. Specifically, our strategy is instead of manually designing a nonlocal regularization functional, to learn the non-local regularization operator and the potential function following a loss-based training approach.\nOur contributions in this work can be summarized as follows: (1) We propose a novel deep network architecture that is discriminatively trained for image denoising. As opposed to the existing deep-learning methods for image restoration, which are based on local models, our network explicitly models the non-local self-similarity property of natural images through a grouping operation of similar patches and a joint filtering. (2) We unroll a proximal gradient method into a deep network and learn the relevant parameters using a simple yet effective back-propagation strategy. (3) In contrast to the majority of recent denoising methods that are designed for processing single-channel images, we introduce a variation of our network that applies to color images and leads to state-of-the-art results. (4) We highlight a direct link of our proposed non-local networks with convolutional neural networks (CNNs). This connection allows our models to take full advantage of the latest advances on GPU computing in deep learning and makes them amenable to efficient implementations through their inherent parallelism."}, {"heading": "2. Variational Image Restoration Revisited", "text": "The goal of image denoising is the restoration of a grayscale or color image X from a corrupted observation Y, with the later obtained according to the observation model\ny = x + n . (1)\nIn this setting, y, x \u2208 RN \u00b7C are the vectorized versions of the observed and latent images, respectively, N is the number of pixels, C the number of image channels, and n is assumed to be i.i.d Gaussian noise with variance \u03c32.\nDue to the ill-posedness of the studied problem [38], Eq. (1) that relates the latent image to the observation cannot uniquely characterize the solution. This implies that in order to obtain a physically or statistically meaningful solution, the image evidence must be combined with suitable image priors.\nAmong the most popular and powerful strategies available in the literature for combining the observation and prior information is the variational approach. In this framework the recovery of x from y heavily relies on the formation of an objective function\nE (x) = D (x,y) + \u03bbJ (x) , (2)\nwhose role is to quantify the quality of the solution. Typically the objective function consists of two terms, namely the data fidelity term D (x,y), which measures the proximity of the solution to the observation, and the regularizer J (x) which constrains the set of plausible solutions by penalizing those that do not exhibit the desired properties. The regularization parameter \u03bb \u2265 0 balances the contributions of the two terms. Then, the restoration task is cast as the minimization of this objective function and the minimizer corresponds to the restored image. Note that for the problem under consideration and since the noise corrupting the observation is i.i.d Gaussian, the data term should be equal to 12 \u2016y \u2212 x\u2016 2 2. This variational restoration approach has also direct links to Bayesian estimation methods and can be interpreted either as a penalized maximum likelihood or a maximum a posteriori (MAP) estimation problem [2, 13]."}, {"heading": "2.1. Image Regularization", "text": "The choice of an appropriate regularizer is very important, since it is one of the main factors that determine the quality of the restored image. For this reason, a lot of effort has been made to design novel regularization functionals that can model important image properties and consequently lead to improved reconstruction results. Most of the existing regularization methods are based either on a synthesis- or an analysis-based approach. Synthesis-based regularization takes place in a sparsifying-domain, such as the wavelet basis, and the restored image is obtained by applying an inverse transform [13]. On the other hand, analysis-based regularization involves regularizers that are directly applied on the image one aims to restore. For general inverse problems, the latter regularization strategy has been reported to lead to better reconstruction results [9, 35] and therefore is mostly preferred.\nThe analysis-based regularizers are typically defined as:\nJ (x) =\nR\u2211\nr=1\n\u03c6 (Lrx) , (3)\nwhere L : RN 7\u2192 RR\u00d7D is the regularization operator (Lrx denotes the D-dimensional r-th entry of the result obtained by applying L to the image x) and \u03c6 : RD 7\u2192 R is the potential function. Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein). For the potential function \u03c6 the most popular choices are vector and matrix norms, but other type of functions are also frequently used such as the `0 pseudo-norm and the logarithm. Combinations of the above regularization operators and potential functions lead to existing regularization functionals that have been proven very effective in several inverse problems, including image denoising. A notable representative\nof the above regularizers is the Total Variation (TV) [31], where the regularization operator corresponds to the gradient and the potential function to the `2 vector norm.\nTV regularization and similar methods that penalize derivatives are essentially local methods, since they involve operators that act on a restricted region of the image domain. More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40]. The resulting non-local methods are well-suited for image processing and computer-vision applications and produce very competitive results. The reason is that they allow long-range dependencies between image points and are able to exploit the inherent non-local self-similarity property of natural images. This property implies that images often consist of localized patterns that tend to repeat themselves possibly at distant locations in the image domain.\nIt is worth noting that alternative image denoising methods that do not fall in the category of analysis-based regularization schemes but still exploit the self-similarity property have been developed and produce excellent results. A nonexhaustive list of these methods is the non-local means filter (NLM) [4], BM3D [7], the Learned Simultaneous Sparse Coding (LSSC) [25], and the Weighted Nuclear Norm Minimization (WNNM) [15]."}, {"heading": "2.2. Objective Function Minimization", "text": "Besides the formulation of the objective function and the proper selection of the regularizer, another important aspect in the variational approach is the minimization strategy that will be employed to obtain the solution. For the case under study, the solution to the image denoising problem can be mathematically formulated as:\nx\u2217 = argmin a\u2264xn\u2264b\n1 2 \u2016y \u2212 x\u201622 + \u03bb\nR\u2211\nr=1\n\u03c6 (Lrx)\n= argmin x\n1 2 \u2016y \u2212 x\u201622 + \u03bb\nR\u2211\nr=1\n\u03c6 (Lrx) + \u03b9C (x) (4)\nwhere \u03b9C is the indicator function of the convex set C ={ x \u2208 RN |xn \u2208 [a, b]\u2200n = 1, . . . N } . The indicator function \u03b9C takes the value 0 if x \u2208 C and +\u221e otherwise. The presence of this additional term in Eq. (4) stems from the fact that these type of constraints on the image intensities arise naturally. For example it is reasonable to require that the intensity of the restored image should either be nonnegative (non-negativity constraint with a = 0, b = +\u221e) or its values should lie in a specific range (box-constraint)."}, {"heading": "2.3. Proximal Gradient Method", "text": "There is a variety of powerful optimization strategies for dealing with Eq. (4). The simplest approach however,\nwhich we will follow in this work, is to directly use a gradient-descent algorithm. Since the indicator function \u03b9C is non-smooth, instead of the classical gradient descent algorithm we employ the proximal gradient method [28]. According to this method, the objective function is split into two terms, one of which is differentiable. Here we assume that the potential function \u03c6 is smooth and therefore we can compute its partial derivatives. In this case, the splitting that we choose for the objective function has the form E (x) = f (x) + \u03b9C (x), where f (x) is defined as\nf (x) = 1\n2 \u2016y \u2212 x\u201622 + \u03bb\nR\u2211\nr=1\nD\u2211\nd=1\n\u03c6d ((Lrx)d) . (5)\nNote that in the above definition we have gone one step further and we have expressed the multivariable potential function \u03c6 as the sum of D single-variable functions,\n\u03c6 (z) =\nD\u2211\nd=1\n\u03c6d (zd) . (6)\nAs it will become clear later, this choice will allows us to reduce significantly the computational cost for training our network and will make the learning process feasible. It is also worth noting that this decoupled formulation of the potential function is met frequently in image regularization, as in wavelet regularization [13], anisotropic TV [12] and Field-of-Experts (FoE) [30].\nAfter the splitting of the objective function, the proximal gradient method recovers the solution in an iterative fashion, using the updates\nxt = prox\u03b3t\u03b9C ( xt\u22121 \u2212 \u03b3t\u2207xf ( xt\u22121 )) , (7)\nwhere \u03b3t is a step size and prox\u03b3t\u03b9C is the proximal operator [28] related to the indicator function \u03b9C . The proximal map in this case corresponds to the orthogonal projection of the input onto C, and hereafter will be denoted as PC .\nGiven that the gradient of f is computed as\n\u2207xf (x) = x\u2212 y + \u03bb R\u2211\nr=1\nLTr\u03c8 (Lrx) , (8)\nwhere \u03c8 (z) = [ \u03c81 (z1) \u03c82 (z2) . . . \u03c8D (zD) ]T and \u03c8d (z) = d\u03c6d dz (z), each proximal gradient iteration can be finally re-written as\nxt=PC ( xt\u22121 ( 1\u2212\u03b3t ) + \u03b3ty\u2212\u03b1t R\u2211\nr=1\nLTr\u03c8 ( Lrx t\u22121) ) , (9)\nwhere \u03b1t = \u03bb\u03b3t. In order to obtain the solution of the minimization problem in Eq. (4) using this iterative scheme, a large number of\niterations is required. In addition, the exact form of the operator L and the potential function \u03c6 must be specified. Determining appropriate values for these quantities is in general a very difficult task. This has generated increased research interest and a lot of effort has been made for designing regularization functionals that can lead to good reconstruction results."}, {"heading": "3. Proposed Non-Local Network", "text": "In this work, we pursue a different approach than conventional regularization methods and instead of handpicking the exact forms of the potential function and the regularization operator, we design a network that has the capacity to learn these quantities directly from training data. The core idea is to unroll the proximal gradient method and use a limited number of the iterations derived in Eq. (9) to construct the graph of the network. Then, we learn the relevant parameters by training the network using pairs of corrupted and ground-truth data.\nNext, we describe in detail the overall architecture of the proposed network, which is trained discriminatively for image denoising. First we motivate and derive its structure for processing grayscale images, and then we explain the necessary modifications for processing color images."}, {"heading": "3.1. Non-Local Regularization Operator", "text": "As mentioned earlier, non-local regularization methods have been shown to produce superior reconstruction results than their local counterparts [14,22] for several inverse problems, including image denoising. Their superiority in performance is mainly attributed to their ability of modeling complex image structures by allowing long-range dependencies between points in the image domain. This fact highly motivates us to explore the design of a network that will exhibit a similar behavior. To this end, our starting point is the definition of a non-local operator that will serve as the backbone of our network structure.\nLet us consider a single-channel image X of size Nx \u00d7 Ny and let x \u2208 RN , where N = Nx \u00b7Ny , be the vector that is formed by stacking together the columns of X. Further, we consider image patches of size Px \u00d7 Py and we denote by xr \u2208 RP , with P = Px \u00b7 Py , the vector whose elements correspond to the pixels of the r-th image patch extracted from X. The vector xr is derived from x as xr = Prx,\nwhere Pr is a P \u00d7 N binary matrix that indicates which elements of x belong to xr. For each one of the R extracted image patches, its K closest neighbors are selected. Let ir = {ir,1, ir,2, . . . , ir,K}, with r = 1, . . . , R, be the set of indices of the K most similar patches to the r-th patch xr1. Next, a two-dimensional transform is applied to every patch xr. The patch transform can be represented by a matrixvector multiplication fr = Fxr where F \u2208 RF\u00d7P . Note that if F > P then the patch representation in the transform domain is redundant. In this work, we focus on the nonredundant case where F = P . For the transformed patch fr, a group is formed using the K-closest patches. This is denoted as\nfir = [ fTir,1 f T ir,2 . . . fTir,K ]T \u2208 RF \u00b7K . (10)\nThe final step of the non-local operator involves collaborating filtering among the group, which can be expressed as zr = Wfir , where W \u2208 RF\u00d7(F \u00b7K) is a weighting matrix and is constructed by retaining the first F rows of a circulant matrix. The first row of this matrix corresponds to the vector r = [ w1 . . . wK ] \u2208 RF \u00b7K , where\nwi = [ wi 0 . . . 0 ] \u2208 RF . This collaborative filtering amounts to performing a weighted sum of the K transformed patches in the group, i.e.\nzr =\nK\u2211\nk=1\nwkfir,k . (11)\nBased on the above, the non-local operator acting on an image patch xr can be expressed as the composition of three linear operators, that is\nLr x = ( WF\u0303Pir ) x, (12)\nwhere Pir = [ PTir,1 P T ir,2 . . . PTir,K ]T\nand F\u0303 \u2208 R(F \u00b7K)\u00d7(P \u00b7K) is a block diagonal matrix whose diagonal elements correspond to the patch-transform matrix F. The non-local operator L : RN 7\u2192 RR\u00b7F described above bears strong resemblance to the BM3D analysis operator studied in [8]. The main difference between the two is that for the proposed operator in (12) a weighted average of the transformed patches in the group takes place, as described in\n1The convention used here is that the set ir includes the reference patch, i.e. ir,1 = r.\nEq. (11), while for the operator of [8] a 1D Haar wavelet transform is applied on the group. Our decision for this particular set-up of the non-local operator was mainly based on computational considerations and for decreasing the memory requirements of the network that we propose next.\nDue to the specific structure of the non-local operator Lr (composition of linear operators) it is now easy to derive its adjoint as\nLTr = P T ir F\u0303 TWT. (13)\nThe adjoint of the non-local operator is an important component of our network since it provides a reverse mapping from the transformed patch domain to the original image domain, that is LT : RR\u00b7F 7\u2192 RN ."}, {"heading": "3.1.1 Convolutional Implementation of the Non-Local Operator", "text": "As we explain next, both the non-local operator defined in (12) and its adjoint defined in (13) can be computed using convolution operations and their transpose. Therefore, they can be efficiently implemented using modern software libraries such as OMP and cuDNN that support multi-threaded CPU and parallel GPU implementations.\nConcretely, the image patch extraction and the 2D patch transform, fr = FPrx, can be combined and computed by passing the image X from a convolutional layer. In order to obtain the desired output, the filterbank should consist of as many 2D filters as the number of coefficients in the transform domain. In addition, the support of these filters should match the size of the image patches. This implies that in our case F filters with a support of Px\u00d7Py should be used. Also note that based on the desired overlap between consecutive image patches, an appropriate stride for the convolution layer should be chosen. Finally, the non-local weighted sum operation of (11) can also be computed using convolutions. In particular, following the grouping operation of the similar transformed patches, which is completely defined\nby the set I = {ir : r = 1 . . . R}, the desired output can be obtained by convolving the grouped data with a single 3D filter of support 1 \u00d7 1 \u00d7 K. The necessary steps for computing the non-local operator using convolutional layers are illustrated in Fig. 2. To compute the adjoint of the non-local operator one simply has to follow the opposite direction of the graph shown in Fig. 2 and replace the convolution and patch grouping operations with their transpose operations."}, {"heading": "3.2. Parameterization of the Potential Function", "text": "Besides the non-local operator L, we further need to model the potential function \u03c6. We do this indirectly by representing its partial derivatives \u03c8i as a linear combination of Radial Basis Functions (RBFs), that is\n\u03c8i (x) =\nM\u2211\nj=1\n\u03c0ij\u03c1j (|x\u2212 \u00b5j |) , (14)\nwhere \u03c0ij are the expansion coefficients and \u00b5j are the centers of the basis functions \u03c1j . There are a few radial functions to choose from [17], but in this work we use Gaussian RBFs, \u03c1j (r) = exp ( \u2212\u03b5jr2 ) . For our network we employ M = 63 Gaussian kernels whose centers are distributed equidistantly and they all share the same precision parameter \u03b5. The representation of \u03c8i using mixtures of RBFs is very powerful and allow us to approximate with high accuracy arbitrary non-linear functions. This is an important advantage over conventional regularization methods that mostly rely on a limited set of potential functions such as the ones reported in Section 2.1. Also note that this parameterization of the potential gradient \u03c8 would have been computationally very expensive if we had not adopted the decoupled formulation of Eq. (6) for the potential function.\nHaving all the pieces of the puzzle in order, the architecture of a single \u201citeration\u201d of our network, which we will refer to it as stage, is depicted in Fig. 3. We note that our network follows very closely the proximal gradient iteration in Eq. (9). The only difference is that the parameter\n\u03b1t has been absorbed by the potential gradient \u03c8, whose representation is learned. We further observe that every stage of the network consists of both convolutional and deconvolutional layers and in between there is a layer of trainable non-linear functions."}, {"heading": "3.3. Color Image Denoising", "text": "The architecture of the proposed network as shown in Fig. 3 can only handle grayscale images. To deal with RGB color images, a simple approach would be to use the same network to process each image channel independently. However, this would result to a sub-optimal restoration performance since the network would not be able to explore the existing correlations between the different channels.\nTo circumvent this limitation, we follow a similar strategy as in [7] and before we feed the noisy color image to the network, we apply the same opponent color transformation which results to one luminance and two chrominance channels. Due to the nature of the color transform, the luminance channel contains most of the valuable information about primitive image structures and has a higher signal-tonoise-ratio (SNR) than the two chroma channels. We take advantage of this fact and since the block-matching operation can be sensitive to the presence of noise, we perform the grouping of the patches only from the luminance channel. Then, we use exactly the same set of group indices I = {ir : r = 1 . . . R} for the other two image channels. Another important modification that we make to the original network is that for every image channel we learn a different RBF mixture. The reason for this is that due to the color transformation the three resulting channels have different SNRs that need to be correctly accounted for. Finally, it is important to note that all the image channels share the same filters of the convolutional and weighted-sum layers and their transposes. The reasoning here is that this way the network can better exploit the channel correlations. A by-product of the specific network design is that the search for similar patches needs to be performed only once compared to the naive implementation that would demand it to be computed independently for each channel. In addition, since this operation is computed only once from the noisy input and then it is re-used in all the network stages, the processing of the color channels can take place in a completely decoupled way and therefore the network admits a very efficient parallel implementation."}, {"heading": "4. Discriminative Network Training", "text": "We train our network, which consists of S stages, for grayscale and color image denoising, where the images are corrupted by i.i.d Gaussian noise. The network parameters \u0398 = [ \u03981, . . . ,\u0398S ] , where \u0398t = {\u03b3t,\u03c0t,Ft,Wt} denotes the set of parameters for the t-th stage, are learned using a loss-minimization strategy given Q pairs of train-\ning data { y(q),x(q) }Q q=1\n, where y(q) is a noisy input and x(q) is the corresponding ground-truth image. To achieve an increased capacity for the network, we learn different parameters for each stage. Therefore, the overall architecture of the network does not exactly map to the proximal gradient method but rather to an adaptive version. Nevertheless, in each stage the convolution and deconvolution layers share the same filter parameters and, thus, they correspond to proper proximal gradient iterations.\nSince the objective function that we need to minimize is non-convex, in order to avoid getting stuck in a bad local-minima but also to speed-up the training, initially we learn the network parameters by following a greedy-training strategy. The same approach has been followed in [6, 34]. In this case, we minimize the cost\nL ( \u0398t ) = Q\u2211\nq=1\n` ( x\u0302t(q),x(q) ) , (15)\nwhere x\u0302t(q) is the output of the t-th stage and the loss function ` corresponds to the negative peak signal-to-noise-ratio (PSNR). This is computed as\n` (y,x) = \u221220 log10\n( Pint \u221a N\n\u2016y \u2212 x\u20162\n) , (16)\nwhere N is the total number of pixels of the input images and Pint is the maximum intensity level (i.e. Pint = 255 for grayscale images and Pint = 1 for color images).\nTo minimize the objective function in Eq. (15) w.r.t the parameters \u0398t we employ the L-BFGS algorithm [27] (we use the available implementation of [33]). The L-BFGS is a Quasi-Newton method and therefore it requires the gradient of L w.r.t \u0398t. This can be computed using the chain-rule as\n\u2202L (\u0398t) \u2202\u0398t =\nQ\u2211\nq=1\n\u2202x\u0302t(q) \u2202\u0398t \u00b7 \u2202` ( x\u0302t(q),x(q) )\n\u2202x\u0302t(q) (17)\nwhere \u2202`(y,x)\u2202y = 20 log 10 (y\u2212x) \u2016y\u2212x\u201622 , and \u2202x\u0302t(q) \u2202\u0398t is the Jacobian of the output of the t-th stage, which can be computed using Eq. (9). We omit the details about the computation of the\nderivatives w.r.t specific network parameters and we provide their derivations in the appendix. Here, it suffices to say that the gradient of the loss function can be efficiently computed using the back-propagation algorithm [32], which is a clever implementation of the chain-rule.\nFor the greedy-training we run 100 L-BFGS iterations to learn the parameters of each stage independently. Then we use the learned parameters as initialization of the network and we train all the stages jointly. The joint training corresponds to minimizing the cost function\nL (\u0398) = Q\u2211\nq=1\n` ( x\u0302S(q),x(q) ) , (18)\nw.r.t to all the parameters of the network \u0398. This cost function does not take into account anymore the intermediate results but only depends on the final output of the network x\u0302S(q). In this case we run 400 L-BFGS iterations to refine the result that we have obtained from the greedy-training. Similarly to the previous case, we still employ the backpropagation algorithm to compute the required gradients."}, {"heading": "5. Experiments", "text": "To train our grayscale and color non-local models we generated the training data using the Berkeley segmentation dataset (BSDS) [26] which consists of 500 images. We\nsplit these images in two sets, a training set which consists of 400 images and the validation/test set which consists of the remaining 100 images. All the images were randomly cropped and their resulting size was 180 \u00d7 180 pixel. We note that the 68 BSDS images of [30] that are used for the comparisons reported in Tables 1 and 2 are strictly excluded from the training set. The proposed models were trained on a NVIDIA Tesla K-40 GPU and the software we used for training and testing was built on top of MatConvnet [36].\nGrayscale denosing Following the strategy described in Section 4, we have trained 5 stages of two different variations of our model, which we will refer to as NLNet55\u00d75 and NLNet57\u00d77. The main difference between them is the configuration of the non-local operator. For the first network we considered patches of size 5\u00d75 while for the second one we have considered slightly larger patches of size 7\u00d77. In both cases, the patch stride is one, that is every pixel in the image is considered as the center of a patch. Consequently, the input images at each network stage are padded accordingly, using symmetric boundaries. In addition, a non-redundant patch-transform, which was learned by training, is applied to every image-patch2 and the group is formed using the K = 8 closest neighbors. The similar patches are searched\n2Similarly to variational methods, we do not penalize the DC component of the patch-transform. Therefore, the number of the transformdomain coefficients for a patch of size P is equal to P \u2212 1.\non the noisy input of the network in a window of 31 \u00d7 31 centered around each pixel. The same group indices are then used for all the stages of the network.\nIn Table 1 we report comparisons of our proposed NLNet55\u00d75 and NLNet 5 7\u00d77 models with several recent stateof-the-art denoising methods on the standard evaluation dataset of 68 images [30]. From these results we observe that both our non-local models lead to the best overall performance, with the only exception being the case of \u03c3 = 50 where the MLP denoising method [5] achieves a slightly better average PSNR compared to that of NLNet55\u00d75. It worths noting that while NLNet55\u00d75 has a lower capacity (it uses approximately half of the parameters) than both CSF57\u00d77 and TNRD 5 7\u00d77, it still produces better restoration results in all tested cases. This is attributed to the non-local information that exploits, as opposed to CSF57\u00d77 and TNRD57\u00d77 which are local models. Representative grayscale denoising results that demonstrate visually the restoration quality of the proposed models are shown in Fig. 4. Color denoising Given that in the grayscale case the use of 7\u00d77 patches did not bring any substantial improvements compared to the use of 5\u00d7 5 patches, for the color case we have trained a single configuration of our model, considering only color image patches of size 5\u00d75. Besides the standard differences, as they are described in Section 3.3, between the color and the grayscale versions of the NLNet55\u00d75 model, the rest of the parameters about the size of the patchgroup and the search window remain the same.\nAn important remark to make here is that most of the denoising methods that were considered previously have been explicitly designed to treat single-channel images, with the most notable exception being the BM3D, for which it indeed exists a color-version (CBM3D) [7]. In practice, this means that if we need to restore color-images then each of these methods should be applied independently on every image channel. In this case however, their denoising performance does not anymore correspond to state-of-theart. The reason is that due to their single-channel design they fail to capture the existing correlations between the image channels, and this limitation has a direct impact in the final restoration quality. This fact is also verified by the color denoising comparisons reported in Table 2. From\nthese results we observe that the TNRD and MLP models, which outperform BM3D for single-channel images, fall behind in restoration performance by more than 1.3 dBs. In fact, for low noise levels CBM3D, which currently produces state-of-the-art results, leads to PSNR gains that exceed 2 dBs. Comparing the proposed non-local model with CBM3D, we observe that CNLNet55\u00d75 manages to provide better restoration results for all the reported noise levels, with the PSNR gain ranging approximately between 0.2-0.3 dBs. We are not aware of any other color-denoising method that manages to compete with CBM3D on such large set of images. For a visual inspection of the color restoration performance of CNLNet55\u00d75 we refer to Figs. 1 and 9."}, {"heading": "6. Conclusions and Future Work", "text": "In this work we have proposed a novel network architecture for grayscale and color image denoising. The design of the resulting models has been inspired by non-local variational methods and it exploits the non-local self-similarity property of natural images. We believe that non-local modeling coupled with discriminative learning are the key factors of the improved restoration performance that our models achieve compared to several recent state-of-the-art methods. Meanwhile, the proposed models have direct links to convolutional neural networks and therefore can take full advantage of all the latest advances on parallel GPU computing in deep learning.\nWe are confident that image restoration is just one of the many inverse imaging problems that our non-local networks can successfully handle. We believe that a very interesting research direction is to investigate the necessary modifications on the design of our current non-local models that would allow them to be efficiently applied to other important reconstruction problems. Another very relevant research question is if it is possible to train a single model that can handle all noise levels."}, {"heading": "A. Derivative Calculations", "text": "In this section we provide the necessary derivations for the gradients of the loss function of the network w.r.t the parameters \u0398. We note that for all the derivative cal-\nculations we use the denominator layout notation3. Further, we recall that in order to learn the parameters \u0398 = {\u03b3t,\u03c0t,Ft,Wt}St=1 of the network, which consists of S stages, we use two different strategies, namely greedy and joint training. During greedy training we learn the parameters \u0398t of each stage t of the network independently from the parameters of the other stages by minimizing the loss function of Eq. (15). On the other hand, in joint training the complete set of the network parameters is learned simultaneously by minimizing the loss function given in Eq. (18).\nA.1. Single-Stage Parameter Learning\nFirst we will consider the greedy training scheme. The results computed here will also be useful in the joint estimation scheme. Since the gradient of the overall loss L in Eq. (15) is decomposed as:\n\u2202L (\u0398t) \u2202\u0398t =\nQ\u2211\nq=1\n\u2202` ( x\u0302t(q),x(q) )\n\u2202\u0398t , (19)\nhereafter we will consider the case of a single training example x\u0302t. In order to retain the notation simplicity, in the following computations we will also drop the superscript t from all the variables and use it only when it is necessary.\nAs we mentioned earlier, to compute the gradients w.r.t the network parameters we rely on the chain rule and we get\n\u2202` (x\u0302,x)\n\u2202\u0398 =\n\u2202x\u0302 \u2202\u0398 \u00b7 \u2202` (x\u0302,x) \u2202x\u0302 , (20)\nwhere\n\u2202` (x\u0302,x)\n\u2202x\u0302 =\n20\nlog 10 (x\u0302\u2212 x) \u2016x\u0302\u2212 x\u201622 , (21)\nis a vector of size N \u00d71. Now we focus on the computation of the Jacobian of the output of the stage, x\u0302, w.r.t the stage parameters. Before doing so, we recall that the output, x\u0302, of a stage given an input z, is computed according to the mapping\nx\u0302 = PC\n( z (1\u2212 \u03b3) + \u03b3y \u2212 R\u2211\nr=1\nLTr\u03c8 (Lrz)\n) . (22)\nNote that the Eq. (22) is just a modified version of Eq. (9), where the variable \u03b1 is absorbed by the function \u03c8.\nThe Jacobian of x\u0302 w.r.t the parameters of the stage, \u0398, can now be expressed as\n\u2202x\u0302 \u2202\u0398 = \u2202u \u2202\u0398 \u2202PC (u) \u2202u , (23)\n3For the details of this notation we refer to https://en.wikipedia. org/wiki/Matrix_calculus#Denominator-layout_notation.\nwhere\nu = z (1\u2212 \u03b3) + \u03b3y \u2212 R\u2211\nr=1\nLTr\u03c8 (Lrz) . (24)\nRegarding the projection operator PC (u), this is applied element-wise to the vector u and it is defined as:\nPC (u) =    u, if a \u2264 u \u2264 b a, if u < a b, if u > b.\n(25)\nThe derivative of PC (u) w.r.t u is computed as:\ndPC (u) du = { 1, if a \u2264 u \u2264 b 0, elsewhere,\n(26)\nand therefore the Jacobian \u2202PC(u)\u2202u corresponds to a binary diagonal matrix of size N \u00d7 N , whose diagonal elements are non-zero only if the corresponding values of u are in the range [ a b ] . Now, let us denote as e the N \u00d7 1 vector obtained by the matrix vector product of the Jacobian \u2202PC(u) \u2202u with the gradient \u2202`(x\u0302,x) \u2202x\u0302 , that is\ne = \u2202PC (u) \u2202u \u00b7 \u2202` (x\u0302,x) \u2202x\u0302 . (27)\nWeight parameter \u03b3 : Using Eq. (24) it is straightforward to show that\n\u2202u \u2202\u03b3 = (y \u2212 z)T (28)\nand thus \u2202`(x\u0302,x)\u2202\u03b3 is computed as\n\u2202` (x\u0302,x)\n\u2202\u03b3 = (y \u2212 z)T \u00b7 e. (29)\nExpansion coefficients \u03c0 : To compute the gradient of the loss function ` w.r.t to the expansion coefficients \u03c0 of the mixture of Gaussian RBFs, we first express the output of the RBF mixture as a vector inner product. Specifically, it holds that\n\u03c8i (x) =\nM\u2211\nj=1\n\u03c0ij\u03c1j (|x\u2212 \u00b5j |) = \u03c1T (x)\u03c0i, (30)\nwhere \u03c1 (x) = [ \u03c1 (|x\u2212 \u00b51|) . . . \u03c1 (|x\u2212 \u00b5M |) ]T \u2208 RM . We note that in the definition of \u03c1 (x) we have dropped the subscript j from the Gaussian RBF \u03c1j (x) = exp ( \u2212\u03b5jx2 ) , since we use a common precision parameter for all the mixture components, i.e. \u03b5 = \u03b5j , \u2200j.\nBased on this notation we can further express \u03c8 (x) =[ \u03c81 (x1) \u03c82 (x2) . . . \u03c8F (xF ) ]T as\n\u03c8 (x) = RT (x)\u03c0 (31)\nwhere \u03c0 = [ \u03c0T1 . . . \u03c0 T F ]T , x \u2208 RF and\nRT (x) =   \u03c1T (x1) 0 . . . 0 0 \u03c1T (x2) 0 ...\n. . . 0 . . . 0 \u03c1T (xF )\n  \u2208 R F\u00d7(M \u00b7F ).\n(32)\nNow, using Eqs. (24), (30) and (31) we have\nu = z (1\u2212 \u03b3) + \u03b3y \u2212 R\u2211\nr=1\nLTrR T (Lrz)\u03c0 (33)\nwhich directly leads us to compute the Jacobian \u2202u\u2202\u03c0 as\n\u2202u \u2202\u03c0 = \u2212\nR\u2211\nr=1\nR (Lrz)Lr. (34)\nFinally, combining Eqs. (27) and (34) we get\n\u2202` (x\u0302,x)\n\u2202\u03c0 = \u2212\nR\u2211\nr=1\nRT (Lrz)Lre. (35)\nWeighted sum coefficients W : To simplify the computation of the gradient of the loss function w.r.t W, first we obtain an equivalent expression for the non-local operator Lr defined in Eq. (12). Indeed, the non-local operator can be re-written as\nLr =\nK\u2211\nk=1\nwkFPir,k =\nK\u2211\nk=1\nwkTir,k . (36)\nPlugging the new expression of Lr into Eq. (24) we get\nu = z (1\u2212 \u03b3) + \u03b3y \u2212 R\u2211\nr=1\nK\u2211\nk=1\nwkT T ir,k \u03c8\n( K\u2211\nk=1\nwkzir,k\n) ,\n(37)\nwhere zir,k = Tir,kz. Now, it is straightforward to compute the partial derivative of u w.r.t each wi. Based on Eq. (37), we obtain\n\u2202u \u2202wi = \u2212\nR\u2211\nr=1\n\u2202\n\u2202wi\n   wiTTir,i + \u2211\nk 6=i wkT\nT ir,k  \u03c8 (zir )  \n= \u2212 R\u2211\nr=1\n( \u03c8T (zir )Tir,i + K\u2211\nk=1\nwkz T ir,i\n\u2202\u03c8 (zir )\n\u2202zir Tir,k\n) ,\n(38)\nwhere zir = K\u2211 k=1 wkzir,k . Note that due to the decoupled formulation of \u03c8, the Jacobian \u2202\u03c8(zir )\u2202zir is a diagonal matrix of the form:\n\u2202\u03c8 (x)\n\u2202x =   \u2202\u03c81(x1) \u2202x1\n0 . . . 0\n0 \u2202\u03c82(x2)\u2202x2 0 ...\n. . . 0 . . . 0 \u2202\u03c8F (xF )\u2202xF\n  , (39)\nwhere\n\u2202\u03c8i (x)\n\u2202x = \u22122\u03b5\nM\u2211\nj=1\n\u03c0ij (x\u2212 \u00b5j) exp ( \u2212\u03b5 (x\u2212 \u00b5j)2 ) .\n(40)\nCombining Eqs (27) and (38) we obtain:\n\u2202` (x\u0302,x)\n\u2202wi = \u2212\nR\u2211\nr=1\n( \u03c8T (zir )Tir,i + K\u2211\nk=1\nwkz T ir,i\n\u2202\u03c8 (zir )\n\u2202zir Tir,k\n) e.\n(41)\nPatch-transform coefficients F : Let us express the matrix F \u2208 RF\u00d7P in terms of its column vectors, i.e. F =[ f1 . . . fF ]T with fi \u2208 RP \u2200 i = 1, . . . , F . Now, let us also re-write Lrz as\nLrz =\n( K\u2211\nk=1\nwkF Pir,k\n) z\n= F\n( K\u2211\nk=1\nwkPir,k ) z = F (Brz)\n= Fz\u0303r =   fT1 z\u0303r ...\nfTF z\u0303r\n  . (42)\nNext, we use Eq. (42) to re-write Eq. (24) as\nu = z (1\u2212 \u03b3) + \u03b3y \u2212 R\u2211\nr=1\nBTr [ f1 . . . fF ] \u03c8     fT1 z\u0303r ...\nfTF z\u0303r\n   \n= z (1\u2212 \u03b3) + \u03b3y \u2212 R\u2211\nr=1\nBTr [ f1 . . . fF ]   \u03c81 ( fT1 z\u0303r ) ...\n\u03c8F ( fTF z\u0303r )\n \n= z (1\u2212 \u03b3) + \u03b3y \u2212 R\u2211\nr=1\nBTr\n  F\u2211\nj=1\nfj\u03c8j ( fTj z\u0303r )   .\n(43)\nThis last reformulation of u greatly facilitates the computation of its Jacobian w.r.t fi, \u2200i = 1, . . . , F . Now, we can show that\n\u2202u \u2202fi = \u2212\nR\u2211\nr=1\n( IP \u00b7 \u03c8i (fiz\u0303r) + fiz\u0303Tr \u00b7 \u2202\u03c8i (fiz\u0303r)\n(fiz\u0303r)\n) Br,\n(44)\nwhere IP \u2208 RP\u00d7P is the identity matrix. Consequently, it holds\n\u2202` (x\u0302,x)\n\u2202fi = \u2212\nR\u2211\nr=1\n( IP \u00b7 \u03c8i (fiz\u0303r) + fiz\u0303Tr \u00b7 \u2202\u03c8i (fiz\u0303r)\n(fiz\u0303r)\n) Bre.\n(45)\nA.2. Joint Parameter Learning\nIn the joint-training scheme the parameters of all the stages of the network are learned simultaneously by minimizing the loss function of Eq. (18) which depends only on the final output of the network x\u0302S . In this case we need to compute the gradient of the loss function ` ( x\u0302S ,x ) w.r.t the parameters \u0398t of each stage t. Using the chain-rule this can be computed as\n\u2202` ( x\u0302S ,x )\n\u2202\u0398t =\n\u2202x\u0302t\n\u2202\u0398t \u00b7 \u2202x\u0302\nS\n\u2202x\u0302t \u00b7 \u2202`\n( x\u0302S ,x ) \u2202x\u0302S , (46)\nwhere \u2202x\u0302 t\n\u2202\u0398t is calculated by combining Eq. (23) and the re-\nsults of Section A.1, while \u2202`(x\u0302S ,x) \u2202x\u0302S\nis given by Eq. (21). Therefore, the only remaining Jacobian that we need to compute is \u2202x\u0302 S\n\u2202x\u0302t . This quantity can be computed recursively as\n\u2202x\u0302S \u2202x\u0302t = \u2202x\u0302t+1 \u2202x\u0302t \u00b7 \u2202x\u0302 t+2 \u2202x\u0302t+1 \u00b7 \u00b7 \u00b7 \u2202x\u0302 S \u2202x\u0302S\u22121 . (47)\nConsequently, it suffices to derive the expression for the Jacobian \u2202x\u0302 t+1\n\u2202x\u0302t where x\u0302 t+1 is obtained from x\u0302t according to\nx\u0302t+1 = PC ( x\u0302t ( 1\u2212 \u03b3t+1 ) + \u03b3t+1y\n\u2212 R\u2211\nr=1\n( Lt+1r )T \u03c8t+1 ( Lt+1r x\u0302\nt ) )\n= PC ( ut+1 ) . (48)\nUsing Eq. (48), we finally get\n\u2202x\u0302t+1 \u2202x\u0302t = \u2202ut+1 \u2202x\u0302t \u00b7 \u2202PC\n( ut+1 )\n\u2202ut+1\n= ( IN ( 1\u2212 \u03b3t+1 )\n\u2212 R\u2211\nr=1\n( Lt+1r )T \u2202\u03c8t+1 (x\u0302tr) \u2202x\u0302tr Lt+1r\n) Pt+1, (49)\nwhere IN \u2208 RN\u00d7N is the identity matrix, x\u0302tr = Lt+1r x\u0302t and Pt+1 = PC(ut+1) \u2202ut+1 ."}, {"heading": "B. Grayscale and Color Image Denoising Comparisons", "text": "In this section we provide additional grayscale and color image denoising results for different noise levels. For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7]. Besides the visual comparisons, in the captions of the figures we provide the PSNR score (in dB) of each method to also allow a quantitative comparison."}, {"heading": "C. Acknowledgments", "text": "The author would like to thank NVIDIA for supporting this work by donating a Tesla K-40 GPU."}], "references": [{"title": "Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation", "author": ["C. Bailer", "B. Taetz", "D. Stricker"], "venue": "Proc. IEEE Int. Conf. on Computer Vision, pages 4015\u20134023", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Introduction to Inverse Problems in Imaging", "author": ["M. Bertero", "P. Boccacci"], "venue": "IOP Publishing", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Total generalized variation", "author": ["K. Bredies", "K. Kunisch", "T. Pock"], "venue": "SIAM J. Imaging Sci., 3:492\u2013526", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Image denoising methods", "author": ["A. Buades", "B. Coll", "J.-M. Morel"], "venue": "A new nonlocal principle. SIAM review, 52:113\u2013147", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Image denoising: Can plain neural networks compete with bm3d? In Proc", "author": ["H.C. Burger", "C.J. Schuler", "S. Harmeling"], "venue": "IEEE Int. Conf. Computer Vision and Pattern Recognition, pages 2392\u20132399", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration", "author": ["Y. Chen", "T. Pock"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Image denoising by sparse 3-d transform-domain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Trans. Image Process., 16(8):2080\u20132095", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Bm3d frames and variational image deblurring", "author": ["A. Danielyan", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Trans. Image Process., 21(4):1715\u20131728", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis versus synthesis in signal priors", "author": ["M. Elad", "P. Milanfar", "R. Rubinstein"], "venue": "Inverse problems, 23(3):947", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlocal discrete regularization on weighted graphs: a framework for image and manifold processing", "author": ["A. Elmoataz", "O. Lezoray", "S. Bougleux"], "venue": "IEEE Trans. Image Proces., 17:1047\u20131060", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Scalable object detection using deep neural networks", "author": ["D. Erhan", "C. Szegedy", "A. Toshev", "D. Anguelov"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, pages  2147\u20132154", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Majorization\u2013minimization algorithms for wavelet-  based image restoration", "author": ["M. Figueiredo", "J. Bioucas-Dias", "R. Nowak"], "venue": "IEEE Trans. Image Process., 16:2980\u20132991", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlocal operators with applications 13  (a)  (b)  (c) (d) Figure 8", "author": ["G. Gilboa", "S. Osher"], "venue": "Color image denoising. (a) Original image, (b) Noisy image corrupted with Gaussian noise (\u03c3 = 25) ; PSNR = 20.34 dB. (c) Denoised image using  CNLNet5\u00d75 ; PSNR = 31.14 dB. (d) Denoised image using CBM3D [7] ; PSNR = 30.75 dB. Images are best viewed magnified on screen. Note the differences of the denoised results in the highlighted region. to image processing. Multiscale Model. Simul., 7:1005\u2013 1028", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Weighted nuclear norm minimization with application to image denoising", "author": ["S. Gu", "L. Zhang", "W. Zuo", "X. Feng"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, pages 2862\u20132869", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Handbook of neural network signal processing", "author": ["Y.H. Hu", "J.-N. Hwang"], "venue": "CRC press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Accurate image superresolution using very deep convolutional networks", "author": ["J. Kim", "K. Lee", "K.M. Lee"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, pages 1646\u20131654", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deblurring and denoising of images by nonlocal functionals", "author": ["S. Kindermann", "S. Osher", "P.W. Jones"], "venue": "Multiscale Model. Simul., 4:1091\u20131115", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Hessianbased norm regularization for image restoration with biomedical applications", "author": ["S. Lefkimmiatis", "A. Bourquard", "M. Unser"], "venue": "IEEE Trans. Image Process., 21(3):983\u2013995", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-local Structure Tensor functionals for image regularization", "author": ["S. Lefkimmiatis", "S. Osher"], "venue": "IEEE Trans. Comput. Imaging, 1:16\u201329", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Structure tensor total variation", "author": ["S. Lefkimmiatis", "A. Roussos", "P. Maragos", "M. Unser"], "venue": "SIAM J. Imaging Sci., 8:1090\u20131122", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Hessian Schattennorm regularization for linear inverse problems", "author": ["S. Lefkimmiatis", "J. Ward", "M. Unser"], "venue": "IEEE Trans. Image Process., 22(5):1873\u20131888", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Non-local sparse models for image restoration", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Proc. IEEE Int. Conf. Computer Vision, pages 2272\u20132279", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "Proc. IEEE Int. Conf. Computer Vision, pages 416\u2013423", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Numerical optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "Springer Science & Business Media", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Proximal Algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Now Publishers", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "Advances in neural information processing systems, pages 91\u201399", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Fields of experts", "author": ["S. Roth", "M.J. Black"], "venue": "International Journal of Computer Vision, 82(2):205\u2013229", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D, 60:259\u2013268", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323(6088):533\u2013536", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1986}, {"title": "minFunc: unconstrained differentiable multivariate optimization in Matlab", "author": ["M. Schmidt"], "venue": "http://www.cs.ubc. ca/ \u0303schmidtm/Software/", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Shrinkage fields for effective image restoration", "author": ["U. Schmidt", "S. Roth"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, pages 2774\u20132781", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal restoration with overcomplete wavelet transforms: Comparison of analysis and synthesis priors", "author": ["I. Selesnick", "M. Figueiredo"], "venue": "SPIE ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Gaussian conditional random field network: A model-based deep network for discriminative denoising", "author": ["R. Vemulapalli", "O. Tuzel", "M.-Y. Liu"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, pages 4801\u2013 4809", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Computational Methods for Inverse Problems", "author": ["C.R. Vogel"], "venue": "SIAM", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["J. Xie", "L. Xu", "E. Chen"], "venue": "Advances in Neural Information Processing Systems, pages 341\u2013349", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularization on discrete spaces", "author": ["D. Zhou", "B. Sch\u00f6lkopf"], "venue": "Pattern Recognition, pages 361\u2013368. Springer", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "Proc. IEEE Int. Conf. Computer Vision, pages 479\u2013486. IEEE", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "Deep learning methods have been successfully applied in various computer vision tasks, including image classification [16,20] and object detection [11,29], and have dramatically improved the performance of these systems, setting the new state-of-the-art.", "startOffset": 118, "endOffset": 125}, {"referenceID": 18, "context": "Deep learning methods have been successfully applied in various computer vision tasks, including image classification [16,20] and object detection [11,29], and have dramatically improved the performance of these systems, setting the new state-of-the-art.", "startOffset": 118, "endOffset": 125}, {"referenceID": 10, "context": "Deep learning methods have been successfully applied in various computer vision tasks, including image classification [16,20] and object detection [11,29], and have dramatically improved the performance of these systems, setting the new state-of-the-art.", "startOffset": 147, "endOffset": 154}, {"referenceID": 27, "context": "Deep learning methods have been successfully applied in various computer vision tasks, including image classification [16,20] and object detection [11,29], and have dramatically improved the performance of these systems, setting the new state-of-the-art.", "startOffset": 147, "endOffset": 154}, {"referenceID": 4, "context": "Recently, very promising results have also been reported for image processing applications such as image restoration [5, 39], super-resolution [18] and optical flow [1].", "startOffset": 117, "endOffset": 124}, {"referenceID": 37, "context": "Recently, very promising results have also been reported for image processing applications such as image restoration [5, 39], super-resolution [18] and optical flow [1].", "startOffset": 117, "endOffset": 124}, {"referenceID": 16, "context": "Recently, very promising results have also been reported for image processing applications such as image restoration [5, 39], super-resolution [18] and optical flow [1].", "startOffset": 143, "endOffset": 147}, {"referenceID": 0, "context": "Recently, very promising results have also been reported for image processing applications such as image restoration [5, 39], super-resolution [18] and optical flow [1].", "startOffset": 165, "endOffset": 168}, {"referenceID": 32, "context": "Only very recently, Schmidt and Roth [34] and Chen and Pock [6] introduced deep networks whose architecture is specifically tailored to certain image restoration problems.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "Only very recently, Schmidt and Roth [34] and Chen and Pock [6] introduced deep networks whose architecture is specifically tailored to certain image restoration problems.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "A notable example is the Block Matching and 3D Collaborative Filtering (BM3D) method [7] which is a very efficient and highly engineered approach that held the state-of-the-art record in image denoising for almost a decade.", "startOffset": 85, "endOffset": 88}, {"referenceID": 36, "context": "Due to the ill-posedness of the studied problem [38], Eq.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "This variational restoration approach has also direct links to Bayesian estimation methods and can be interpreted either as a penalized maximum likelihood or a maximum a posteriori (MAP) estimation problem [2, 13].", "startOffset": 206, "endOffset": 213}, {"referenceID": 11, "context": "This variational restoration approach has also direct links to Bayesian estimation methods and can be interpreted either as a penalized maximum likelihood or a maximum a posteriori (MAP) estimation problem [2, 13].", "startOffset": 206, "endOffset": 213}, {"referenceID": 11, "context": "Synthesis-based regularization takes place in a sparsifying-domain, such as the wavelet basis, and the restored image is obtained by applying an inverse transform [13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 8, "context": "For general inverse problems, the latter regularization strategy has been reported to lead to better reconstruction results [9, 35] and therefore is mostly preferred.", "startOffset": 124, "endOffset": 131}, {"referenceID": 33, "context": "For general inverse problems, the latter regularization strategy has been reported to lead to better reconstruction results [9, 35] and therefore is mostly preferred.", "startOffset": 124, "endOffset": 131}, {"referenceID": 2, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 102, "endOffset": 109}, {"referenceID": 29, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 102, "endOffset": 109}, {"referenceID": 21, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 132, "endOffset": 136}, {"referenceID": 19, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 168, "endOffset": 175}, {"referenceID": 22, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 168, "endOffset": 175}, {"referenceID": 11, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 250, "endOffset": 254}, {"referenceID": 29, "context": "of the above regularizers is the Total Variation (TV) [31], where the regularization operator corresponds to the gradient and the potential function to the `2 vector norm.", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40].", "startOffset": 151, "endOffset": 171}, {"referenceID": 12, "context": "More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40].", "startOffset": 151, "endOffset": 171}, {"referenceID": 17, "context": "More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40].", "startOffset": 151, "endOffset": 171}, {"referenceID": 20, "context": "More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40].", "startOffset": 151, "endOffset": 171}, {"referenceID": 38, "context": "More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40].", "startOffset": 151, "endOffset": 171}, {"referenceID": 3, "context": "A nonexhaustive list of these methods is the non-local means filter (NLM) [4], BM3D [7], the Learned Simultaneous Sparse Coding (LSSC) [25], and the Weighted Nuclear Norm Minimization (WNNM) [15].", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "A nonexhaustive list of these methods is the non-local means filter (NLM) [4], BM3D [7], the Learned Simultaneous Sparse Coding (LSSC) [25], and the Weighted Nuclear Norm Minimization (WNNM) [15].", "startOffset": 84, "endOffset": 87}, {"referenceID": 23, "context": "A nonexhaustive list of these methods is the non-local means filter (NLM) [4], BM3D [7], the Learned Simultaneous Sparse Coding (LSSC) [25], and the Weighted Nuclear Norm Minimization (WNNM) [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "A nonexhaustive list of these methods is the non-local means filter (NLM) [4], BM3D [7], the Learned Simultaneous Sparse Coding (LSSC) [25], and the Weighted Nuclear Norm Minimization (WNNM) [15].", "startOffset": 191, "endOffset": 195}, {"referenceID": 26, "context": "Since the indicator function \u03b9C is non-smooth, instead of the classical gradient descent algorithm we employ the proximal gradient method [28].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "It is also worth noting that this decoupled formulation of the potential function is met frequently in image regularization, as in wavelet regularization [13], anisotropic TV [12] and Field-of-Experts (FoE) [30].", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "It is also worth noting that this decoupled formulation of the potential function is met frequently in image regularization, as in wavelet regularization [13], anisotropic TV [12] and Field-of-Experts (FoE) [30].", "startOffset": 207, "endOffset": 211}, {"referenceID": 26, "context": "where \u03b3 is a step size and prox\u03b3t\u03b9C is the proximal operator [28] related to the indicator function \u03b9C .", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "As mentioned earlier, non-local regularization methods have been shown to produce superior reconstruction results than their local counterparts [14,22] for several inverse problems, including image denoising.", "startOffset": 144, "endOffset": 151}, {"referenceID": 20, "context": "As mentioned earlier, non-local regularization methods have been shown to produce superior reconstruction results than their local counterparts [14,22] for several inverse problems, including image denoising.", "startOffset": 144, "endOffset": 151}, {"referenceID": 7, "context": "The non-local operator L : R 7\u2192 RR\u00b7F described above bears strong resemblance to the BM3D analysis operator studied in [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "(11), while for the operator of [8] a 1D Haar wavelet transform is applied on the group.", "startOffset": 32, "endOffset": 35}, {"referenceID": 15, "context": "There are a few radial functions to choose from [17], but in this work we use Gaussian RBFs, \u03c1j (r) = exp ( \u2212\u03b5jr ) .", "startOffset": 48, "endOffset": 52}, {"referenceID": 6, "context": "To circumvent this limitation, we follow a similar strategy as in [7] and before we feed the noisy color image to the network, we apply the same opponent color transformation which results to one luminance and two chrominance channels.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": ") TNRD7\u00d77 [6] MLP [5] CBM3D [7] CNLNet5\u00d75 15 31.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": ") TNRD7\u00d77 [6] MLP [5] CBM3D [7] CNLNet5\u00d75 15 31.", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": ") TNRD7\u00d77 [6] MLP [5] CBM3D [7] CNLNet5\u00d75 15 31.", "startOffset": 28, "endOffset": 31}, {"referenceID": 28, "context": "Color image denoising comparisons for three different noise levels over the standard set of 68 [30] Berkeley images.", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "The same approach has been followed in [6, 34].", "startOffset": 39, "endOffset": 46}, {"referenceID": 32, "context": "The same approach has been followed in [6, 34].", "startOffset": 39, "endOffset": 46}, {"referenceID": 25, "context": "t the parameters \u0398 we employ the L-BFGS algorithm [27] (we use the available implementation of [33]).", "startOffset": 50, "endOffset": 54}, {"referenceID": 31, "context": "t the parameters \u0398 we employ the L-BFGS algorithm [27] (we use the available implementation of [33]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "(d) Denoised image using TNRD7\u00d77 [6] ; PSNR = 29.", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "(e) Denoised image using MLP [5] ; PSNR = 29.", "startOffset": 29, "endOffset": 32}, {"referenceID": 13, "context": "(f) Denoised image using WNNM [15] ; PSNR = 29.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "(d) Denoised image using CBM3D [7] ; PSNR = 25.", "startOffset": 31, "endOffset": 34}, {"referenceID": 30, "context": "Here, it suffices to say that the gradient of the loss function can be efficiently computed using the back-propagation algorithm [32], which is a clever implementation of the chain-rule.", "startOffset": 129, "endOffset": 133}, {"referenceID": 24, "context": "To train our grayscale and color non-local models we generated the training data using the Berkeley segmentation dataset (BSDS) [26] which consists of 500 images.", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "We note that the 68 BSDS images of [30] that are used for the comparisons reported in Tables 1 and 2 are strictly excluded from the training set.", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "The proposed models were trained on a NVIDIA Tesla K-40 GPU and the software we used for training and testing was built on top of MatConvnet [36].", "startOffset": 141, "endOffset": 145}, {"referenceID": 6, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 7, "endOffset": 10}, {"referenceID": 23, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 16, "endOffset": 20}, {"referenceID": 39, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 61, "endOffset": 64}, {"referenceID": 35, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 72, "endOffset": 76}, {"referenceID": 4, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 81, "endOffset": 84}, {"referenceID": 28, "context": "Grayscale image denoising comparisons for three different noise levels over the standard set of 68 [30] Berkeley images.", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "[6], while the results of DGCRF8 are taken from [37] .", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "[6], while the results of DGCRF8 are taken from [37] .", "startOffset": 48, "endOffset": 52}, {"referenceID": 28, "context": "In Table 1 we report comparisons of our proposed NLNet5\u00d75 and NLNet 5 7\u00d77 models with several recent stateof-the-art denoising methods on the standard evaluation dataset of 68 images [30].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "From these results we observe that both our non-local models lead to the best overall performance, with the only exception being the case of \u03c3 = 50 where the MLP denoising method [5] achieves a slightly better average PSNR compared to that of NLNet5\u00d75.", "startOffset": 179, "endOffset": 182}, {"referenceID": 6, "context": "An important remark to make here is that most of the denoising methods that were considered previously have been explicitly designed to treat single-channel images, with the most notable exception being the BM3D, for which it indeed exists a color-version (CBM3D) [7].", "startOffset": 264, "endOffset": 267}, {"referenceID": 5, "context": "For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 39, "context": "For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7].", "startOffset": 110, "endOffset": 114}, {"referenceID": 6, "context": "For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7].", "startOffset": 229, "endOffset": 232}], "year": 2017, "abstractText": "We propose a novel deep network architecture for grayscale and color image denoising that is based on a non-local image model. Our motivation for the overall design of the proposed network stems from variational methods that exploit the inherent non-local self-similarity property of natural images. We build on this concept and introduce deep networks that perform non-local processing and at the same time they significantly benefit from discriminative learning. Experiments on the Berkeley segmentation dataset, comparing several state-of-the-art methods, show that the proposed non-local models achieve the best reported denoising performance both for grayscale and color images for all the tested noise levels. It is also worth noting that this increase in performance comes at no extra cost on the capacity of the network compared to existing alternative deep network architectures. In addition, we highlight a direct link of the proposed non-local models to convolutional neural networks. This connection is of significant importance since it allows our models to take full advantage of the latest advances on GPU computing in deep learning and makes them amenable to efficient implementations through their inherent parallelism.", "creator": "LaTeX with hyperref package"}}}