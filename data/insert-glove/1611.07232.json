{"id": "1611.07232", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Compositional Learning of Relation Path Embedding for Knowledge Base Completion", "abstract": "Nowadays, pattle large - cakmakoglu scale knowledge bases larrieux containing 2126 billions chrysippus of facts have mastered reached 37.01 impressive sizes; however, two-hundred they are 41/m still far up-and-comer from shammai completion. zeybek In soccer-specific addition, brohd most alprazolam existing methods only consider the direct links cr\u00e9quy between entities, heur ignoring saravan the tolan vital limann impact about the bostan semantic of relation fasting paths. In apollonia this waz paper, doksone we study the problem of sourozh how to tegula better embed mundhir entities ilanthirayan and relations alshehri into different low blindness dimensional maranhao spaces. bmcc A generalissimo compositional learning yoichiro model co-ordinator of armanda relation rajdeep paths jaric embedding (coconspirators RPE) is proposed greenhaus to take sindel full begrudging advantage lakeshores of additional abstinence semantic ivani\u0107 information expressed by relation paths. More myerses specifically, ahlam using 69-72 corresponding projection feign matrices, RPE tpdf can goree simultaneously zildjian embed hoglan entities into promontory corresponding nuyorican relation capelet and path spaces. hejduk It municipally is cosin also 818,000 suggested longifolia that chromatically type rosvooruzheniye constraints depetro could oot be aerodrome extended wormsley from j\u00edbaro traditional relation - specific to kirkmichael the new jwrc proposed path - injunctive specific sk\u00ebnderbeu ones. ulundurpet Both of the bantam two snicket type 1,374 constraints lovren can be seamlessly incorporated sunjic into RPE and combretaceae decrease the errors dimboola in gwon prediction. Experiments are conducted on miev the vedi benchmark datasets and co-songwriter the proposed yanai model achieves significant and consistent crisanto improvements compared rupertswood with idalia the state - laka of - the - darts art cities/abc algorithms asclepius for centrepieces knowledge peikoff base 65-point completion.", "histories": [["v1", "Tue, 22 Nov 2016 10:11:56 GMT  (1910kb)", "https://arxiv.org/abs/1611.07232v1", "9 Pages,1 figure"], ["v2", "Mon, 28 Nov 2016 13:07:30 GMT  (1910kb)", "http://arxiv.org/abs/1611.07232v2", "9 pages,1 figure"], ["v3", "Tue, 20 Dec 2016 07:11:26 GMT  (1911kb)", "http://arxiv.org/abs/1611.07232v3", "9 pages,1 figure"], ["v4", "Fri, 24 Feb 2017 04:12:56 GMT  (1910kb)", "http://arxiv.org/abs/1611.07232v4", "7 pages,1 figure"]], "COMMENTS": "9 Pages,1 figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xixun lin", "yanchun liang", "fausto giunchiglia", "xiaoyue feng", "renchu guan"], "accepted": false, "id": "1611.07232"}, "pdf": {"name": "1611.07232.pdf", "metadata": {"source": "CRF", "title": "Compositional Learning of Relation Path Embedding for Knowledge Base Completion", "authors": ["Xixun Lin", "Yanchun Liang", "Fausto Giunchiglia", "Xiaoyue Feng", "Renchu Guan"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n07 23\n2v 4\n[ cs\n.C L\n] 2\n4 Fe\nb 20"}, {"heading": "1 Introduction", "text": "Large-scale knowledge bases (KBs), such as Freebase [Bollacker et al., 2008], WordNet [Miller, 1995], Yago [Suchanek et al., 2007], and NELL [Carlson et al., 2010], are critical to natural language processing applications, e.g., question answering [Dong et al., 2015], relation extraction [Riedel et al., 2013], and language modeling [Ahn et al., 2016]. These KBs generally contain billions of facts, and each fact is organized into a triple base format (head entity, relation, tail entity), abbreviated as (h,r,t). However, the coverage of such KBs is still far from complete compared with real-world knowledge [Dong et al., 2014]. Traditional KB completion approaches, such as Markov logic networks [Richardson and Domingos, 2006], suffer from feature sparsity and low efficiency. Recently, encoding the entire knowledge base into a lowdimensional vector space to learn latent representations of entity and relation has attracted widespread attention. These knowledge embedding models yield better performance in\nterms of low complexity and high scalability compared with previous works. Among these methods, TransE [Bordes et al., 2013] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)=\u2016h+ r \u2212 t\u2016 to measure the plausibility for triples. TransH [Wang et al., 2014] and TransR [Lin et al., 2015b] are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects.\nHowever, the majority of these approaches only exploit direct links that connect head and tail entities to predict potential relations between entities. These approaches do not explore the fact that relation paths, which are denoted as the sequences of relations, i.e., p=(r1, r2, . . ., rm), play an important role in knowledge base completion. For example, the sequence of triples (J.K. Rowling, CreatedRole, Harry Potter), (Harry Potter, Describedin, Harry Potter and the Philosopher\u2019s Stone) can be used to infer the new fact (J.K. Rowling, WroteBook, Harry Potter and the Philosopher\u2019s Stone), which does not appear in the original KBs. Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings [Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016].\nFor a relation path, consistent semantics is a semantic interpretation via composition of the meaning of the component elements. Each relation path contains its respective consistent semantics. However, the consistent semantics expressed by some relation paths p is unreliable for reasoning new facts of that entity pair [Lin et al., 2015a]. For instance, there is a common relation path h HasChildren \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 t\u2032 GraduatedFrom \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 t, but this path is meaningless for inferring additional relationships between h and t. Therefore, reliable relation paths are urgently needed. Moreover, their consistent semantics, which is essential for knowledge representation learning, is consistent with the semantics of relation r. Based on this intuition, we propose a compositional learning model of relation path embedding (RPE), which extends the projection and type constraints of the specific relation to the specific path. As the path ranking algorithm (PRA) [Lao et al., 2011] suggests, relation paths that end in many possible tail entities are more likely to be unreliable for the entity pair. Reliable relation paths can thus be filtered using PRA. Figure 1 illustrates the basic idea for relation-specific and path-specific projections. Each entity is projected byMr andMp into the corresponding\nrelation and path spaces. These different embedding spaces hold the following hypothesis: in the relation-specific space, relation r is regarded as a translation from head hr to tail tr; likewise, p\u2217, the path representation by the composition of relation embeddings, is regarded as a translation from head hp to tail tp in the path-specific space. We design two types of compositions to dynamically construct the path-specific projection Mp without extra parameters. Moreover, with slight changes on negative sampling, we also propose that relationspecific and path-specific type constraints can be seamlessly incorporated into our model. Our main contributions are as follows: 1) To reinforce the reasoning ability of knowledge embedding models, the consistent semantics and the path spaces are introduced. 2) The path-specific type constraints generated from path space can help to improve the model\u2019s discriminability. 3) Compared with the pure data-driven mechanism in the knowledge embeddingmodels used, the way in which we utilize PRA to find reliable relation paths improves the knowledge representation learning interpretability. The remainder of this paper is organized as follows. We first provide a brief review of related knowledge embedding models in Section 2. The details of RPE are introduced in Section 3. The experiments and analysis are reported in Section 4. Conclusions and directions for future work are reported in the final section."}, {"heading": "2 Related Work", "text": "We first concentrate on three classical translation-based models that only consider direct links between entities. The bold lowercase letter v denotes a column vector, and the bold uppercase letter M denotes a matrix. The first translationbased model is TransE, and it defines the score function as S(h,r,t)=\u2016h + r \u2212 t\u2016 for each triple (h,r,t). The score will become smaller if the triple (h,r,t) is correct; otherwise, the score will become higher. The embeddings are\nlearned by optimizing a global margin-loss function. This assumption is clearly simple and cannot address more complex relation attributes well, i.e., 1-to-N, N-to-1, and N-toN. To alleviate this problem, TransH projects entities into a relation-dependent hyperplane by the normal vector wr: hh=h-w T r hwr and th=t-w T r twr (restrict \u2016wr\u20162=1). The corresponding score function is S(h,r,t)=\u2016hh + r \u2212 th\u2016. TransE and TransH achieve translations on the same embedding space, whereas TransR assumes that each relation should be used to project entities into different relation-specific embedding spaces since different relations may place emphasis on different entity aspects. The projected entity vectors are hr=Mrh and tr=Mrt; thus, the new score function is defined as S(h,r,t)=\u2016hr + r\u2212 tr\u2016.\nAnother research direction focuses on improving the prediction performance by using prior knowledge in the form of relation-specific type constraints [Krompass et al., 2015; Chang et al., 2014; Wang et al., 2015]. Note that each relation should possess Domain and Range fields to indicate the subject and object types, respectively. For example, the relation haschildren\u2019s Domain and Range types both belong to a person. By exploiting these limited rules, the harmful influence of a merely data-driven pattern can be avoided. Typeconstrained TransE [Krompass et al., 2015] imposes these constraints on the global margin-loss function to better distinguish similar embeddings in latent space.\nA third current related work is PTransE [Lin et al., 2015a] and the path ranking algorithm (PRA) [Lao et al., 2011]. PTransE considers relation paths as translations between head and tail entities and primarily addresses two problems: 1) exploit a variant of PRA to select reliable relation paths, and 2) explore three path representations by compositions of relation embeddings. PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention [Lao et al., 2015; Gardner and Mitchell, 2015; Wang et al., 2016; Nickel et al., 2016]. PRA uses the path-constrained random walk probabilities as path features to train linear classifiers for different relations. In large-scale KBs, relation paths have great significance for enhancing the reasoning ability for more complicated situations. However, none of the aforementioned models take full advantage of the consistent semantics of relation paths."}, {"heading": "3 Our Model", "text": "The consistent semantics expressed by reliable relation paths has a significant impact on learning meaningful embeddings. Here, we propose a compositional learning model of relation path embedding (RPE), which includes novel path-specific projection and type constraints. All entities constitute the entity set \u03b6, and all relations constitute the relation set R. RPE uses PRA to select reliable relation paths. Precisely, for a triple (h,r,t), Pall={p1,p2,. . .,pk} is the path set for the entity pair (h,t). PRA calculates P(t|h, pi), the probability of reaching t from h following the sequence of relations indicated by pi, which can be recursively defined as follows:\nIf pi is an empty path,\nP (t|h, pi) =\n{\n1 if h = t 0 otherwise (1)\nIf pi is not an empty path, then p \u2032 i is defined as r1,. . .,rm\u22121; subsequently,\nP (t|h, pi) = \u2211\nt\u2032\u2208Ran(p\u2032i)\nP (t\u2032|h, p\u2032i) \u00b7 P (t|t \u2032, rm) (2)\nRan(p\u2032i) is the set of ending nodes of p \u2032 i. RPE obtains the reliable relation paths set Pfilter={p1,p2,. . .,pz} by selecting relation paths whose probabilities are above a certain threshold \u03b7."}, {"heading": "3.1 Path-specific Projection", "text": "The key idea of RPE is that the consistent semantics of reliable relation paths is similar to the semantics of the relation between an entity pair. For a triple (h,r,t), RPE exploits projection matrices Mr, Mp \u2208 R\nm\u00d7n to project entity vectors h, t \u2208 Rn in entity space into the corresponding relation and path spaces simultaneously (m is the dimension of relation embeddings, n is the dimension of entity embeddings, and m may differ from n). The projected vectors (hr, hp, tr, tp) in their respective embedding spaces are denoted as follows:\nhr = Mrh, hp = Mph (3)\ntr = Mrt, tp = Mpt (4)\nBecause relation paths are those sequences of relations p=(r1, r2, . . ., rm), we dynamically use Mr to construct Mp to decrease the model complexity. Subsequently, we explore two compositions for the formation of Mp, which are formulated as follows:\nMp =Mr1 +Mr2 + . . .+Mrm (addition composition)\n(5)\nMp =Mr1 \u00d7Mr2 \u00d7 . . .\u00d7Mrm (multiplication composition)\n(6)\nwhere addition composition (ACOM) and multiplication composition (MCOM) represent cumulative addition and multiplication for path projection. Matrix normalization is applied on Mp for both compositions. The new score function is defined as follows:\nG(h,r,t) = S(h,r,t) + \u03bb \u00b7 S(h,p,t) = \u2016hr + r\u2212 tr\u2016+\n\u03bb\nZ\n\u2211\npi\u2208Pfilter\nP (t|h, pi) \u00b7 Pr(r|pi) \u00b7 \u2016hp + p \u2217 i \u2212 tp\u2016 (7)\nFor path representation p\u2217, we use p\u2217=r1+r2+. . . +rm, as suggested by PTransE. \u03bb is the hyper-parameter used to balance the knowledge embedding score S(h,r,t) and the relation path embedding score S(h,p,t). Z= \u2211\npi\u2208pfilter P(t|h, pi) is the nor-\nmalization factor, and Pr(r|pi) = Pr(r, pi) / Pr(pi) is used to assist in calculating the reliability of relation paths. In the experiments, we increase the limitation on these embeddings, i.e., \u2016h\u20162 6 1, \u2016t\u20162 6 1, \u2016r\u20162 6 1, \u2016hr\u20162 6 1, \u2016tr\u20162 6 1, \u2016hp\u20162 6 1, and \u2016tp\u20162 6 1. By exploiting the consistent semantics of reliable relation paths, RPE embeds entities into the relation and path spaces simultaneously. This method improves the flexibility of RPE when modeling more complicated relation attributes."}, {"heading": "3.2 Path-specific Type Constraints", "text": "In RPE, based on the semantic similarity between relations and reliable relation paths, we can naturally extend the relation-specific type constraints to novel path-specific type constraints. In type-constrained TransE, the distribution of corrupted triples is a uniform distribution. In our model, we borrow the idea from TransH, incorporating the two type constraints with a Bernoulli distribution. For each relation r, we denote the Domainr and Ranger to indicate the subject and object types of relation r. \u03b6Domainr is the entity set whose entities conform to Domainr, and \u03b6Ranger is the entity set whose entities conform to Ranger. We calculate the average numbers of tail entities for each head entity, named teh, and the average numbers of head entities for each tail entity, named het. The Bernoulli distribution with parameter teh\nteh+het for each relation r is incorporated with the two type constraints, which can be defined as follows: RPE samples entities from \u03b6Domainr to replace the head entity with probability teh teh+het , and it samples entities from \u03b6Ranger to replace the tail entity with probability het teh+het . The objective function for RPE is defined as follows:\nL = \u2211\n(h,r,t)\u2208C\n[ L(h, r, t) + \u03bb\nZ\n\u2211\npi\u2208Pfilter\nP (t|h, pi)\u00b7\nPr(r|pi)L(h, pi, t) ]\n(8)\nL(h,r,t) is the loss function for triples, and L(h,pi,t) is the loss function for relation paths.\nL(h, r, t) = \u2211\n(h\u2032,r,t\u2032)\u2208C\u2032\u2032\nmax(0, S(h, r, t) + \u03b31\u2212\nS(h\u2032, r, t\u2032))\n(9)\nL(h, pi, t) = \u2211\n(h\u2032,r,t\u2032)\u2208C\u2032\u2032\nmax(0, S(h, pi, t) + \u03b32\u2212\nS(h\u2032, pi, t \u2032))\n(10)\nWe denoteC={(hi,ri,ti) | i=1,2. . .,t} as the set of all observed triples and C\u2032={(h\u2032i,ri,ti) \u222a (hi,ri,t \u2032\ni) | i=1,2. . .,t} as the set of corrupted triples, where each element of C\u2032 is obtained by randomly sampling from \u03b6. C\u2032\u2032, whose element conforms to the two type constraints with a Bernoulli distribution, is a subset of C\u2032. The Max(0, x) returns the maximum between 0 and x. \u03b3 is the hyper-parameter of margin, which separates corrected triples and corrupted triples. By exploiting these two prior knowledges, RPE could better distinguish similar embeddings in different embedding spaces, thus allowing it to achieve better prediction."}, {"heading": "3.3 Training Details", "text": "We adopt stochastic gradient descent (SGD) to minimize the objective function. TransE or RPE (initial) can be exploited for the initializations of all entities and relations. The score function of RPE (initial) is as follows:\nG(h,r,t) = S(h,r,t) + \u03bb \u00b7 S(h,p,t) = \u2016h+ r\u2212 t\u2016+\n\u03bb\nZ\n\u2211\npi\u2208Pfilter\nP (t|h, pi) \u00b7 Pr(r|pi) \u00b7 \u2016h+ p \u2217 i \u2212 t\u2016 (11)\nWe also employ this score function in our experiment as a baseline. The projection matrices Ms are initialized as identity matrices. RPE holds the local closed-world assumption (LCWA) [Dong et al., 2014], where each relation\u2019s domain and range types are based on the instance level. Their type information is provided by KBs or the entities shown in observed triples.\nNote that each relation r has the reverse relation r\u22121; therefore, to increase supplemental path information, RPE utilizes the reverse relation paths. For example, for the relation path LeBron James PlayFor \u2212\u2212\u2212\u2212\u2212\u2212\u2192 Cleveland Cavaliers BelongTo \u2212\u2212\u2212\u2212\u2212\u2212\u2192 NBA, its reverse relation path can be defined as NBA BelongTo\u22121 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 Cleveland Cavaliers PlayFor\u22121\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 LeBron James.\nFor each iteration, we randomly sample a correct triple (h,r,t) with its reverse (t,r\u22121,h), and the final score function of our model is defined as follows:\nF (h,r,t) = G(h,r,t) +G(t,r\u22121,h) (12)\nTheoretically, we can arbitrarily set the length of the relation path, but in the implementation, we prefer to take a smaller value to reduce the time required to enumerate all relation paths. Moreover, as suggested by the path-constrained random walk probability P(t|h, p), as the path length increases, P(t|h, p) will become smaller and the relation path will more likely be cast off."}, {"heading": "4 Experiments", "text": "To examine the retrieval and discriminative ability of our model, RPE is evaluated on two standard subtasks of knowledge base completion: link prediction and triple classification. We also present a case study on consistent semantics learned by our method to further highlight the importance of relation paths for knowledge representation learning."}, {"heading": "4.1 Datasets", "text": "We evaluate our model on two classical large-scale knowledge bases: Freebase and WordNet. Freebase is a large collaborative knowledge base that contains billions of facts about the real world, such as the triple (Beijing, Locatedin, China), which describes the fact that Beijing is located in China. WordNet is a large lexical knowledge base of English, in which each entity is a synset that expresses a distinct concept, and each relationship is a conceptual-semantic or lexical relation. We use two subsets of Freebase, FB15K and FB13 [Bordes et al., 2013], and one subset of WordNet, WN11 [Socher et al., 2013]. Table 1 presents the statistics of the datasets, where each column represents the numbers of entity type, relation type and triples that have been split into training, validation and test sets.\nIn our model, each triple has its own reverse triple for increasing the reverse relation paths. Therefore, the total number of triples is twice the number used in the original datasets. Our model exploits the LCWA. In this case, we utilize the type information provided by [Xie et al., 2016] for FB15K. As for FB13 and WN11, we do not depend on the auxiliary data, and the domain and range of each relation are approximated by the triples from the original datasets."}, {"heading": "4.2 Link Prediction", "text": "The link prediction task consists of predicting the possible h or t for test triples when h or t is missed. FB15K is employed for this task."}, {"heading": "Evaluation Protocol", "text": "We follow the same evaluation procedures as used in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b]. First, for each test triple (h,r,t), we replace h or t with every entity in \u03b6. Second, each corrupted triple is calculated by the corresponding score function S(h,r,t). The final step is to rank the original correct entity with these scores in descending order. Two evaluation metrics are reported: the average rank of correct entities (Mean Rank) and the proportion of correct entities ranked in the top 10 (Hits@10). Note that if a corrupted triple already exists in the knowledge base, then it should not be considered to be incorrect. We prefer to remove these corrupted triples from our dataset, and call this setting \u201dFilter\u201d. If these corrupted triples are reserved, then we call this setting \u201dRaw\u201d. In both settings, if the latent representations of entity and relation are better, then a lower mean rank and a higher Hits@10 should be achieved. Because we use the same dataset, the baseline results reported in [Lin et al., 2015b; Lin et al., 2015a; Ji et al., 2016] are directly used for comparison."}, {"heading": "Implementation", "text": "We set the dimension of entity embeddingm and relation embedding n among {20, 50, 100, 120}, the margin \u03b31 among {1, 2, 3, 4, 5}, the margin \u03b32 among {3, 4, 5, 6, 7, 8}, the learning rate \u03b1 for SGD among {0.01, 0.005, 0.0025, 0.001, 0.0001}, the batch size B among {20, 120, 480, 960, 1440, 4800}, and the balance factor \u03bb among {0.5, 0.8, 1,1.5, 2}. The threshold \u03b7 was set in the range of {0.01, 0.02, 0.04, 0.05} to reduce the calculation of meaningless paths.\nGrid search is used to determine the optimal parameters. The best configurations for RPE (ACOM) are n=100, m=100, \u03b31=2, \u03b32=5, \u03b1=0.0001, B=4800, \u03bb=1, and \u03b7=0.05. We select RPE (initial) to initialize our model, set the path length as 2, take L1 norm for the score function, and traverse our model for 500 epochs."}, {"heading": "Analysis of Results", "text": "Table 2 reports the results of link prediction, in which the first column is translation-based models, the variants of PTransE, and our models. The numbers in bold are the best performance, and n-hop indicates the path length n that PTransE exploits. We denote RPE only with path-specific constraints as RPE (PC), and from the results, we can observe the following. 1) Our models significantly outperform the classical knowledge embedding models (TransE, TransH, TransR, and TranSparse) and PTransE on FB15K with the metrics of mean rank and Hits@10 (filter). The results demonstrate that the path-specific projection can explore further implications on relation paths, which are crucial for knowledge base completion. 2) RPE (PC) improves slightly compared with the baselines. We believe that this result is primarily because RPE (PC) only focuses on local information provided by related embeddings, ignoring some global information compared with the approach of randomly selecting corrupted entities. In terms of mean rank, RPE (ACOM) achieves the best performance with 14.5% and 24.1% error reduction compared with PTransE\u2019s performance in the raw and filter settings, respectively. In terms of Hits@10, RPE (ACOM) brings few improvements. RPE with path-specific type constraints and projection (RPE (PC + ACOM) and RPE (PC +MCOM)) is a compromise between RPE (PC) and RPE (ACOM).\nTable 3 presents the separated evaluation results by mapping properties of relations on FB15K. Mapping properties of relations follows the same rules in [Bordes et al., 2013], and the metrics are Hits@10 on head and tail entities. From Table 3, we can conclude that 1) RPE (ACOM) outperforms all baselines in all mapping properties of relations. In particular, for the 1-to-N, N-to-1, and N-to-N types of relations\nthat plague knowledge embeddingmodels, RPE (ACOM) improves 4.1%, 4.6%, and 4.9% on head entity\u2019s prediction and 6.9%, 7.0%, and 5.1% on tail entity\u2019s prediction compared with previous state-of-the-art performances achieved by PTransE (ADD, 2-hop). 2) RPE (MCOM) does not perform as well as RPE (ACOM), and we believe that this result is because RPE\u2019s path representation is not consistent with RPE (MCOM)\u2019s composition of projections. Although RPE (PC) improves little compared with PTransE, we will indicate the effectiveness of relation-specific and pathspecific type constraints in triple classification. 3) We use the relation-specific projection to construct path-specific ones dynamically; then, entities are encoded into relation-specific and path-specific spaces simultaneously. The experiments are similar to link prediction, and the results of experiments results further demonstrate the better expressibility of our model."}, {"heading": "4.3 Triple Classification", "text": "We conduct the task of triple classification on three benchmark datasets: FB15K, FB13 and WN11. Triple classification aims to predict whether a given triple (h,r,t) is true, which is a binary classification problem."}, {"heading": "Evaluation Protocol", "text": "We set different relation-specific thresholds {\u03b4r} to perform this task. For a test triple (h,r,t), if its score S(h,r,t) is below \u03b4r, then we predict it as a positive one; otherwise, it is negative. {\u03b4r} is obtained by maximizing the classification accuracies on the valid set."}, {"heading": "Implementation", "text": "We directly compare our model with prior work using the results about knowledge embedding models reported in [Lin et al., 2015b] for WN11 and FB13. Because [Lin et al., 2015a] does not evaluate PTransE\u2019s performance on this task, we use the code of PTransE that is released in [Lin et al., 2015a] to complete it. FB13 and WN11 already contain negative samples. For FB15K, we use the same process to produce negative samples, as suggested by [Socher et al., 2013]. The hyper-parameter intervals are the same as link prediction. The best configurations for RPE (PC + ACOM) are as follows: n=50, m=50, \u03b31=5, \u03b32=6, \u03b1=0.0001, B=1440, \u03bb=0.8, and \u03b7=0.05, taking the L1 norm on WN11; n=100, m=100, \u03b31=3, \u03b32=6, \u03b1=0.0001, B=960, \u03bb=0.8, and \u03b7=0.05, taking the L1 norm on FB13; and n=100, m=100, \u03b31=4, \u03b32=5, \u03b1=0.0001, B=4800, \u03bb=1, and \u03b7 =0.05, taking the L1 norm on FB15K. We exploit RPE (initial) for initiation, and we set the path length as 2 and the maximum epoch as 500."}, {"heading": "Analysis of Results", "text": "Table 4 lists the results for triple classification on different datasets, and the evaluation metric is classification accuracy. The results demonstrate that 1) RPE (PC + ACOM) achieves the best performance on all datasets, which takes good advantage of path-specific projection and type constraints; 2) RPE (PC) improves the performance of RPE (initial) by 4.5%, 6.0%, and 13.2%, particularly on FB15K; thus, we consider that lengthening the distances for similar entities in embedding space is essential to specific problems. The results of experiments also indicate that although LCWA can compensate for the loss for type information, real relation-type information is predominant."}, {"heading": "4.4 Case Study of Consistent Semantics", "text": "As shown in Table 5, for two entity pairs (sociology, George Washington University) and (Planet of the Apes, art director) from Freebase, RPE provides two relations and four most relevant relation paths (each relation is mapped to two relation paths, denoted as a and b), which are considered as having\nsimilar semantics to their respective relations. However, this type of consistent semantics of reliable relation paths cannot be achieved by translation-based models, such as Trans(E, H, R), because translation-based models only exploit the direct links and do not consider relation path information, such as reliable relation paths in line 3 and line 6 in Table 5. In contrast, RPE can obtain reliable relation paths with their consistent semantics, and it extends the projection and type constraints of the specific relation to the specific path. Furthermore, the experimental results demonstrate that by explicitly using the additional semantics, RPE consistently and significantly outperforms state-of-the-art knowledge embedding models in the two benchmark tasks (link prediction and triple classification)."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper, we propose a compositional learning model of relation path embedding (RPE) for knowledge base completion. To the best of our knowledge, this is the first time that a path-specific projection has been proposed, and it simultaneously embeds each entity into relation and path spaces to learn more meaningful embeddings. Moreover, We also put forward the novel path-specific type constraints based on relation-specific constraints to better distinguish similar embeddings in the latent space.\nIn the future, we plan to 1) incorporate other potential semantic information into the relation path modeling, such as the information provided by those intermediate nodes connected by relation paths, and 2) explore relation path embedding in other applications associated with knowledge bases, such as distant supervision for relation extraction and question answering over knowledge bases."}, {"heading": "Acknowledgments", "text": "The authors are grateful for the support of the National Natural Science Foundation of China (No. 61572228, No. 61472158, No. 61300147, and No. 61602207), the Science Technology Development Project from Jilin Province (No. 20140520070JH and No. 20160101247JC), the PremierDiscipline Enhancement Scheme supported by Zhuhai Government and Premier Key-Discipline Enhancement Scheme supported by Guangdong Government Funds, and the Smart Society Collaborative Project funded by the European Commission\u2019s 7th Framework ICT Programme for Research and\nTechnological Development under Grant Agreement No. 600854."}], "references": [{"title": "CoRR", "author": ["Sungjin Ahn", "Heeyoul Choi", "Tanel P\u00e4rnamaa", "Yoshua Bengio. A neural knowledge language model"], "venue": "abs/1608.00318,", "citeRegEx": "Ahn et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "Proceedings of KDD, pages 1247\u20131250,", "citeRegEx": "Bollacker et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Alberto Garc\u0131\u0301a-Dur\u00e1n", "author": ["Antoine Bordes", "Nicolas Usunier"], "venue": "Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multirelational data. In Proceedings of NIPS, pages 2787\u20132795,", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of AAAI", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam Hruschka", "Tom Mitchell. Toward an architecture for never-ending language learning"], "venue": "pages 1306\u2013 1313,", "citeRegEx": "Carlson et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of EMNLP", "author": ["Kai-Wei Chang", "Wen tau Yih", "Bishan Yang", "Christopher Meek. Typed tensor decomposition of knowledge bases for relation extraction"], "venue": "pages 1568\u20131579,", "citeRegEx": "Chang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowledge vault: a web-scale approach to probabilistic knowledge fusion", "author": ["Dong et al", "2014] Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": "Proceedings of KDD,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "In Proceedings of ACL", "author": ["Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu. Question answering over freebase with multicolumn convolutional neural networks"], "venue": "pages 260\u2013269,", "citeRegEx": "Dong et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of EMNLP", "author": ["Matthew Gardner", "Tom Mitchell. Efficient", "expressive knowledge base completion using subgraph feature extraction"], "venue": "pages 1488\u20131498,", "citeRegEx": "Gardner and Mitchell. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of EMNLP", "author": ["Kelvin Guu", "JohnMiller", "Percy Liang. Traversing knowledge graphs in vector space"], "venue": "pages 318\u2013327,", "citeRegEx": "Guu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of AAAI", "author": ["Guoliang Ji", "Kang Liu", "Shizhu He", "Jun Zhao. Knowledge graph completion with adaptive sparse transfer matrix"], "venue": "pages 985\u2013991,", "citeRegEx": "Ji et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of ISWC", "author": ["Denis Krompass", "Stephan Baier", "Volker Tresp. Type-constrained representation learning in knowledge graphs"], "venue": "pages 640\u2013655,", "citeRegEx": "Krompass et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "andWilliamW", "author": ["Ni Lao", "TomMitchell"], "venue": "Cohen. Random walk inference and learning in a large scale knowledge base. In Proceedings of EMNLP, pages 529\u2013 539,", "citeRegEx": "Lao et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "andWilliamW", "author": ["Ni Lao", "EinatMinkov"], "venue": "Cohen. Learning relational features with backward random walks. In Proceedings of ACL, pages 666\u2013675,", "citeRegEx": "Lao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of EMNLP", "author": ["Yankai Lin", "Zhiyuan Liu", "HuanBo Luan", "Maosong Sun", "Siwei Rao", "Song Liu. Modeling relation paths for representation learning of knowledge bases"], "venue": "pages 705\u2013714,", "citeRegEx": "Lin et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "In AAAI", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu. Learning entity", "relation embeddings for knowledge graph completion"], "venue": "pages 2181\u20132187,", "citeRegEx": "Lin et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "Wordnet: a lexical database for english", "author": ["George A. Miller"], "venue": "Communications of the ACM, 38:39\u201341,", "citeRegEx": "Miller. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "In Proceedings of ACL", "author": ["Arvind Neelakantan", "Benjamin Roth", "AndrewMcCallum. Compositional vector space models for knowledge base completion"], "venue": "pages 156\u2013166,", "citeRegEx": "Neelakantan et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Proceedings of the IEEE", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich. A review of relational machine learning for knowledge graphs"], "venue": "104:11\u201333,", "citeRegEx": "Nickel et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Machine Learning", "author": ["Matthew Richardson", "Pedro M. Domingos. Markov logic networks"], "venue": "62:107\u2013136,", "citeRegEx": "Richardson and Domingos. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "In Proceedings of HLT-NAACL", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M. Marlin. Relation extraction with matrix factorization", "universal schemas"], "venue": "pages 74\u201384,", "citeRegEx": "Riedel et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of NIPS", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Yago: a core of semantic knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "Proceedings of WWW, pages 697\u2013706,", "citeRegEx": "Suchanek et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "In Proceedings of ACL", "author": ["Kristina Toutanova", "Victoria Lin", "Wen tau Yih", "Hoifung Poon", "Chris Quirk. Compositional learning of embeddings for relation paths in knowledge base", "text"], "venue": "pages 1434\u2013 1444,", "citeRegEx": "Toutanova et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of AAAI", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen. Knowledge graph embedding by translating on hyperplanes"], "venue": "pages 1112\u20131119,", "citeRegEx": "Wang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of IJCAI", "author": ["Quan Wang", "Bin Wang", "Li Guo. Knowledge base completion using embeddings", "rules"], "venue": "pages 1859\u20131865,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of ACL", "author": ["Quan Wang", "Jing Liu", "Yuanfei Luo", "Bin Wang", "Chin-Yew Lin. Knowledge base completion via coupled path ranking"], "venue": "pages 1308\u20131318,", "citeRegEx": "Wang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Representation learning of knowledge graphs with hierarchical types", "author": ["Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"], "venue": "Proceedings of IJCAI,", "citeRegEx": "Xie et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Large-scale knowledge bases (KBs), such as Freebase [Bollacker et al., 2008], WordNet [Miller, 1995], Yago [Suchanek et al.", "startOffset": 52, "endOffset": 76}, {"referenceID": 15, "context": ", 2008], WordNet [Miller, 1995], Yago [Suchanek et al.", "startOffset": 17, "endOffset": 31}, {"referenceID": 21, "context": ", 2008], WordNet [Miller, 1995], Yago [Suchanek et al., 2007], and NELL [Carlson et al.", "startOffset": 38, "endOffset": 61}, {"referenceID": 3, "context": ", 2007], and NELL [Carlson et al., 2010], are critical to natural language processing applications, e.", "startOffset": 18, "endOffset": 40}, {"referenceID": 6, "context": ", question answering [Dong et al., 2015], relation extraction [Riedel et al.", "startOffset": 21, "endOffset": 40}, {"referenceID": 19, "context": ", 2015], relation extraction [Riedel et al., 2013], and language modeling [Ahn et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 0, "context": ", 2013], and language modeling [Ahn et al., 2016].", "startOffset": 31, "endOffset": 49}, {"referenceID": 18, "context": "Traditional KB completion approaches, such as Markov logic networks [Richardson and Domingos, 2006], suffer from feature sparsity and low efficiency.", "startOffset": 68, "endOffset": 99}, {"referenceID": 2, "context": "Among these methods, TransE [Bordes et al., 2013] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)=\u2016h+ r \u2212 t\u2016 to measure the plausibility for triples.", "startOffset": 28, "endOffset": 49}, {"referenceID": 23, "context": "TransH [Wang et al., 2014] and TransR [Lin et al.", "startOffset": 7, "endOffset": 26}, {"referenceID": 14, "context": ", 2014] and TransR [Lin et al., 2015b] are representative variants of TransE.", "startOffset": 19, "endOffset": 38}, {"referenceID": 16, "context": "Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings [Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016].", "startOffset": 104, "endOffset": 172}, {"referenceID": 8, "context": "Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings [Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016].", "startOffset": 104, "endOffset": 172}, {"referenceID": 22, "context": "Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings [Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016].", "startOffset": 104, "endOffset": 172}, {"referenceID": 13, "context": "However, the consistent semantics expressed by some relation paths p is unreliable for reasoning new facts of that entity pair [Lin et al., 2015a].", "startOffset": 127, "endOffset": 146}, {"referenceID": 11, "context": "As the path ranking algorithm (PRA) [Lao et al., 2011] suggests, relation paths that end in many possible tail entities are more likely to be unreliable for the entity pair.", "startOffset": 36, "endOffset": 54}, {"referenceID": 10, "context": "Another research direction focuses on improving the prediction performance by using prior knowledge in the form of relation-specific type constraints [Krompass et al., 2015; Chang et al., 2014; Wang et al., 2015].", "startOffset": 150, "endOffset": 212}, {"referenceID": 4, "context": "Another research direction focuses on improving the prediction performance by using prior knowledge in the form of relation-specific type constraints [Krompass et al., 2015; Chang et al., 2014; Wang et al., 2015].", "startOffset": 150, "endOffset": 212}, {"referenceID": 24, "context": "Another research direction focuses on improving the prediction performance by using prior knowledge in the form of relation-specific type constraints [Krompass et al., 2015; Chang et al., 2014; Wang et al., 2015].", "startOffset": 150, "endOffset": 212}, {"referenceID": 10, "context": "Typeconstrained TransE [Krompass et al., 2015] imposes these constraints on the global margin-loss function to better distinguish similar embeddings in latent space.", "startOffset": 23, "endOffset": 46}, {"referenceID": 13, "context": "A third current related work is PTransE [Lin et al., 2015a] and the path ranking algorithm (PRA) [Lao et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 11, "context": ", 2015a] and the path ranking algorithm (PRA) [Lao et al., 2011].", "startOffset": 46, "endOffset": 64}, {"referenceID": 12, "context": "PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention [Lao et al., 2015; Gardner and Mitchell, 2015; Wang et al., 2016; Nickel et al., 2016].", "startOffset": 128, "endOffset": 214}, {"referenceID": 7, "context": "PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention [Lao et al., 2015; Gardner and Mitchell, 2015; Wang et al., 2016; Nickel et al., 2016].", "startOffset": 128, "endOffset": 214}, {"referenceID": 25, "context": "PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention [Lao et al., 2015; Gardner and Mitchell, 2015; Wang et al., 2016; Nickel et al., 2016].", "startOffset": 128, "endOffset": 214}, {"referenceID": 17, "context": "PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention [Lao et al., 2015; Gardner and Mitchell, 2015; Wang et al., 2016; Nickel et al., 2016].", "startOffset": 128, "endOffset": 214}, {"referenceID": 2, "context": "We use two subsets of Freebase, FB15K and FB13 [Bordes et al., 2013], and one subset of WordNet, WN11 [Socher et al.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": ", 2013], and one subset of WordNet, WN11 [Socher et al., 2013].", "startOffset": 41, "endOffset": 62}, {"referenceID": 2, "context": "Metric Mean Rank Hits@10(%) Raw Filter Raw Filter TransE [Bordes et al., 2013] 243 125 34.", "startOffset": 57, "endOffset": 78}, {"referenceID": 23, "context": "1 TransH (unif) [Wang et al., 2014] 211 84 42.", "startOffset": 16, "endOffset": 35}, {"referenceID": 23, "context": "5 TransH (bern) [Wang et al., 2014] 212 87 45.", "startOffset": 16, "endOffset": 35}, {"referenceID": 14, "context": "4 TransR (unif) [Lin et al., 2015b] 226 78 43.", "startOffset": 16, "endOffset": 35}, {"referenceID": 14, "context": "5 TransR (bern) [Lin et al., 2015b] 198 77 48.", "startOffset": 16, "endOffset": 35}, {"referenceID": 13, "context": "7 PTransE (ADD, 2-hop) [Lin et al., 2015a] 200 54 51.", "startOffset": 23, "endOffset": 42}, {"referenceID": 13, "context": "4 PTransE (MUL, 2-hop) [Lin et al., 2015a] 216 67 47.", "startOffset": 23, "endOffset": 42}, {"referenceID": 13, "context": "7 PTransE (ADD, 3-hop) [Lin et al., 2015a] 207 58 51.", "startOffset": 23, "endOffset": 42}, {"referenceID": 9, "context": "6 TranSparse (separate, S, unif) [Ji et al., 2016] 211 63 50.", "startOffset": 33, "endOffset": 50}, {"referenceID": 9, "context": "9 TranSparse (separate, S, bern) [Ji et al., 2016] 187 82 53.", "startOffset": 33, "endOffset": 50}, {"referenceID": 26, "context": "In this case, we utilize the type information provided by [Xie et al., 2016] for FB15K.", "startOffset": 58, "endOffset": 76}, {"referenceID": 2, "context": "Evaluation Protocol We follow the same evaluation procedures as used in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b].", "startOffset": 72, "endOffset": 131}, {"referenceID": 23, "context": "Evaluation Protocol We follow the same evaluation procedures as used in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b].", "startOffset": 72, "endOffset": 131}, {"referenceID": 14, "context": "Evaluation Protocol We follow the same evaluation procedures as used in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b].", "startOffset": 72, "endOffset": 131}, {"referenceID": 14, "context": "Because we use the same dataset, the baseline results reported in [Lin et al., 2015b; Lin et al., 2015a; Ji et al., 2016] are directly used for comparison.", "startOffset": 66, "endOffset": 121}, {"referenceID": 13, "context": "Because we use the same dataset, the baseline results reported in [Lin et al., 2015b; Lin et al., 2015a; Ji et al., 2016] are directly used for comparison.", "startOffset": 66, "endOffset": 121}, {"referenceID": 9, "context": "Because we use the same dataset, the baseline results reported in [Lin et al., 2015b; Lin et al., 2015a; Ji et al., 2016] are directly used for comparison.", "startOffset": 66, "endOffset": 121}, {"referenceID": 2, "context": "Relation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N TransE [Bordes et al., 2013] 43.", "startOffset": 81, "endOffset": 102}, {"referenceID": 23, "context": "0 TransH (unif) [Wang et al., 2014] 66.", "startOffset": 16, "endOffset": 35}, {"referenceID": 23, "context": "8 TransH (bern) [Wang et al., 2014] 66.", "startOffset": 16, "endOffset": 35}, {"referenceID": 14, "context": "2 TransR (unif) [Lin et al., 2015b] 76.", "startOffset": 16, "endOffset": 35}, {"referenceID": 14, "context": "1 TransR (bern) [Lin et al., 2015b] 78.", "startOffset": 16, "endOffset": 35}, {"referenceID": 13, "context": "1 PTransE (ADD, 2-hop) [Lin et al., 2015a] 91.", "startOffset": 23, "endOffset": 42}, {"referenceID": 13, "context": "4 PTransE (MUL, 2-hop) [Lin et al., 2015a] 89.", "startOffset": 23, "endOffset": 42}, {"referenceID": 13, "context": "4 PTransE (ADD, 3-hop) [Lin et al., 2015a] 90.", "startOffset": 23, "endOffset": 42}, {"referenceID": 9, "context": "7 TranSparse (separate, S, unif) [Ji et al., 2016] 82.", "startOffset": 33, "endOffset": 50}, {"referenceID": 9, "context": "1 TranSparse (separate, S, bern) [Ji et al., 2016] 86.", "startOffset": 33, "endOffset": 50}, {"referenceID": 2, "context": "Mapping properties of relations follows the same rules in [Bordes et al., 2013], and the metrics are Hits@10 on head and tail entities.", "startOffset": 58, "endOffset": 79}, {"referenceID": 2, "context": "TransE (unif) [Bordes et al., 2013] 75.", "startOffset": 14, "endOffset": 35}, {"referenceID": 2, "context": "8 TransE (bern) [Bordes et al., 2013] 75.", "startOffset": 16, "endOffset": 37}, {"referenceID": 23, "context": "3 TransH (unif) [Wang et al., 2014] 77.", "startOffset": 16, "endOffset": 35}, {"referenceID": 23, "context": "4 TransH (bern) [Wang et al., 2014] 78.", "startOffset": 16, "endOffset": 35}, {"referenceID": 14, "context": "8 TransR (unif) [Lin et al., 2015b] 85.", "startOffset": 16, "endOffset": 35}, {"referenceID": 14, "context": "2 TransR (bern) [Lin et al., 2015b] 85.", "startOffset": 16, "endOffset": 35}, {"referenceID": 13, "context": "0 PTransE (ADD, 2-hop) [Lin et al., 2015a] 80.", "startOffset": 23, "endOffset": 42}, {"referenceID": 13, "context": "4 PTransE (MUL, 2-hop) [Lin et al., 2015a] 79.", "startOffset": 23, "endOffset": 42}, {"referenceID": 13, "context": "3 PTransE (ADD, 3-hop) [Lin et al., 2015a] 80.", "startOffset": 23, "endOffset": 42}, {"referenceID": 14, "context": "We directly compare our model with prior work using the results about knowledge embedding models reported in [Lin et al., 2015b] for WN11 and FB13.", "startOffset": 109, "endOffset": 128}, {"referenceID": 13, "context": "Because [Lin et al., 2015a] does not evaluate PTransE\u2019s performance on this task, we use the code of PTransE that is released in [Lin et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 13, "context": ", 2015a] does not evaluate PTransE\u2019s performance on this task, we use the code of PTransE that is released in [Lin et al., 2015a] to complete it.", "startOffset": 110, "endOffset": 129}, {"referenceID": 20, "context": "For FB15K, we use the same process to produce negative samples, as suggested by [Socher et al., 2013].", "startOffset": 80, "endOffset": 101}], "year": 2017, "abstractText": "Large-scale knowledge bases have currently reached impressive sizes; however, these knowledge bases are still far from complete. In addition, most of the existing methods for knowledge base completion only consider the direct links between entities, ignoring the vital impact of the consistent semantics of relation paths. In this paper, we study the problem of how to better embed entities and relations of knowledge bases into different low-dimensional spaces by taking full advantage of the additional semantics of relation paths, and we propose a compositional learning model of relation path embedding (RPE). Specifically, with the corresponding relation and path projections, RPE can simultaneously embed each entity into two types of latent spaces. It is also proposed that type constraints could be extended from traditional relation-specific constraints to the new proposed path-specific constraints. The results of experiments show that the proposed model achieves significant and consistent improvements compared with the state-of-the-art algorithms.", "creator": "LaTeX with hyperref package"}}}