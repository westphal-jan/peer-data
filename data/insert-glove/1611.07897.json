{"id": "1611.07897", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Learning Generic Sentence Representations Using Convolutional Neural Networks", "abstract": "pequena We blackall propose copy a mohseni new mckee encoder - junid decoder approach seedings to smurfette learn distributed ronaldsway sentence vestita representations fadilah from varadaraja unlabeled 74.98 sentences. ju-52 The word - lovingly to - vector representation is used, and convolutional sphingomyelin neural networks are employed songkok as kodava sentence gristle encoders, mapping an glenolden input tuberosity sentence maunoury into a cossington fixed - length dunxin vector. This skocpol representation is decoded merchandises using long 1991-93 short - term cottier memory nogai recurrent reinsch neural 12mo networks, translators considering several tasks, such as reconstructing quincey the palmerton input millstein sentence, or hice predicting the future carnoy sentence. We disavow further describe 960s a hdw hierarchical encoder - decoder sivits model z\u00fcndel to roundworms encode a sentence to predict funderburke multiple future sentences. By training timika our models 9,010 on zanele a demands large chas collection superpower of nuriel novels, we obtain opryland a 78.8 highly six-day generic naameh convolutional 50-4 sentence sitte encoder responsa that fabrega performs babyfirsttv well qurabi in kpt practice. Experimental results entonces on kortchmar several paravac benchmark datasets, mattingly and endosomes across absenting a broad range terblanche of applications, ecsenius demonstrate the superiority starbreaker of% the proposed model over issoudun competing screwed methods.", "histories": [["v1", "Wed, 23 Nov 2016 17:32:23 GMT  (213kb,D)", "http://arxiv.org/abs/1611.07897v1", null], ["v2", "Wed, 26 Jul 2017 20:48:52 GMT  (275kb,D)", "http://arxiv.org/abs/1611.07897v2", "Accepted by EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhe gan", "yunchen pu", "ricardo henao", "chunyuan li", "xiaodong he", "lawrence carin"], "accepted": true, "id": "1611.07897"}, "pdf": {"name": "1611.07897.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning of Sentence Representations using Convolutional Neural Networks", "authors": ["Zhe Gan", "Yunchen Pu", "Ricardo Henao", "Chunyuan Li", "Xiaodong He", "Lawrence Carin"], "emails": ["lcarin}@duke.edu", "xiaohe@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Learning sentence representations is central to many natural language applications. The aim of a model for such task is to learn fixed-length feature vectors that encode the semantic and syntactic properties of sentences. Deep learning techniques have shown promising performance on sentence modeling, via recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [2, 3], and recursive neural networks [4]. Most of these models are trained in a supervised manner, with a fully-connected layer at the top of the neural network. However, large-scale labeled datasets are often difficult to acquire, motivating the need for unsupervised learning methods to learn structure in sentences. Further, sentences contain a large amount of linguistic regularity, which makes them particularly well suited as a domain for building unsupervised learning models.\nSeveral approaches have been proposed for unsupervised sentence modeling. The paragraph-vector model of [5] incorporates a global context vector into the log-linear neural language model [6] to learn the sentence representations; however, at prediction time, one needs to perform gradient descent to compute a new vector. The skip-thought model of [7] describes an encoder-decoder model to reconstruct the surrounding sentences of an input sentence, where both the encoder and decoder are modeled as RNNs. The sequence autoencoder of [8] is a simple variant of [7], in which the decoder is used to reconstruct the input sentence itself. Most recently, [9] proposed a sentence-level log-linear bag-of-words (BoW) model, where a BoW representation of an input sentence is used to predict adjacent sentences that are also represented as BoW. CNNs have recently achieved excellent results in various supervised natural language applications [2, 3, 10]. However, CNN-based unsupervised sentence modeling has previously not been explored. This paper seeks to fill that gap.\nRelated to but distinct from the skip-thought model of [7], we propose to use a CNN encoder for unsupervised learning of sentence representations within the framework of encoder-decoder models\nar X\niv :1\n61 1.\n07 89\n7v 1\n[ cs\n.C L\n] 2\n3 N\nov 2\nproposed by [11, 12]. A CNN performs successive convolution and pooling operations on an input sentence, then uses a fully-connected layer to produce a fixed-length encoding of the sentence. This encoding vector is then fed into a long short-term memory (LSTM) recurrent network to produce a target sentence. Depending on the task, we propose three models: (i) CNN-LSTM autoencoder: this model seeks to reconstruct the original input sentence, by capturing the intra-sentence information; (ii) CNN-LSTM future predictor: this model aims to predict a future sentence, by leveraging intersentence information; (iii) CNN-LSTM composite model: in this case, there are two LSTMs, decoding the representation to the input sentence itself and a future sentence, respectively. This composite model aims to learn a sentence encoder that captures both intra- and inter-sentence information.\nThe proposed CNN-LSTM future predictor model only considers the immediately subsequent sentence as context. In order to capture longer-term dependencies between sentences, we further introduce a hierarchical encoder-decoder model. This model abstracts the RNN language model of [13] to the sentence level. That is, instead of using the current word in a sentence to predict its future words (sentence continuation), we encode a sentence to predict multiple future sentences (paragraph continuation). This model is termed hierarchical CNN-LSTM model.\nCompared with the LSTM encoders used in [7, 8, 9], a CNN encoder may have the following advantages. First, the sparse connectivity of a CNN, which indicates fewer parameters are required, typically improves its statistical efficiency as well as reduces memory requirements. For example, excluding the number of parameters used in the word embeddings, our trained sentence encoder has 3 million parameters, while the skip-thought vector of [7] contains 40 million parameters. Second, a CNN is able to encode regional (n-gram) information containing rich linguistic patterns. Further, an LSTM encoder might be disproportionately influenced by words appearing later in the sentence, while the CNN gives largely uniform importance to the signal coming from each of the words in the sentence. This makes the LSTM excellent at language modeling (decoding), but potentially suboptimal at encoding n-gram information placed further back into the sentence.\nAs in [7], we first train our proposed models on a large collection of novels, and then evaluate the CNN sentence encoder as a generic feature extractor for 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks. In these experiments, we train a linear classifier on top of the extracted sentence features, without additional fine-tuning of the CNN. We show that our trained sentence encoder yields generic representations that perform as well as, or better than those of [7, 8, 9], in all the tasks considered. Note that the LSTM decoder is not evaluated, and can be discarded after the encoder-decoder model has been trained.\nSummarizing, the main contribution of this paper is a new class of CNN-LSTM encoder-decoder models that is able to leverage the vast quantity of unlabeled text for unsupervised learning of sentence representations."}, {"heading": "2 Model description", "text": ""}, {"heading": "2.1 CNN-LSTM model", "text": "Assume we are given a sentence pair (sx, sy). The encoder, a CNN, encodes the first sentence sx into a feature vector z, which is then fed into a LSTM decoder that predicts the second sentence sy. Let wtx and w t y denote the t-th word in sentences sx and sy , respectively. Each word w t x is embedded into a k-dimensional word vector xt = We[wtx], where We \u2208 Rk\u00d7V is a word embedding matrix (to be learned), V is the vocabulary size, and notation [v] denotes the index for the v-th column of a matrix. Similarly, we let yt = We[wty]. Next we describe the model in three parts: encoder, decoder and applications.\nCNN encoder The CNN architecture in [3, 14] is used for sentence encoding, which consists of a convolution layer and a max-pooling operation over the entire sentence for each feature map. A sentence of length T (padded where necessary) is represented as a matrix X \u2208 Rk\u00d7T , by concatenating its word embeddings as columns, i.e., the t-th column of X is xt.\nA convolution operation involves a filter Wc \u2208 Rk\u00d7h, applied to a window of h words to produce a new feature. According to [14], we can induce one feature map c = f(X \u2217Wc + b) \u2208 RT\u2212h+1, where f(\u00b7) is a nonlinear activation function such as the hyperbolic tangent used in our experiments, b \u2208 RT\u2212h+1 is a bias vector, and \u2217 denotes the convolutional operator. Convolving the same filter\nwith the h-gram at every position in the sentence allows the features to be extracted independently of their position in the sentence. We then apply a max-over-time pooling operation [14] to the feature map and take its maximum value, i.e., c\u0302 = max{c}, as the feature corresponding to this particular filter. This pooling scheme tries to capture the most important feature, i.e., the one with the highest value, for each feature map, effectively filtering out less informative compositions of words. Further, this pooling scheme also guarantees that the extracted features are independent of the length of the input sentence.\nThe above process describes how one feature is extracted from one filter. In practice, the model uses multiple filters with varying window sizes. Each filter can be considered as a linguistic feature detector that learns to recognize a specific class of n-grams (or h-grams, in the above notation). Assume we have m window sizes, and for each window size, we use d filters; then we obtain a md-dimensional vector to represent a sentence.\nThere exist other CNN architectures in the literature [2, 10, 15]. We adopt the CNN model in [3, 14] due to its simplicity and excellent performance on classification. Empirically, we found that it can extract high-quality sentence representations in our unsupervised models.\nLSTM decoder The CNN encoder maps sentence sx into a vector z. We now describe the LSTM decoder that translates z into the sentence sy. The probability of a length-T sentence sy given the encoded feature vector z is defined as\np(sy|z) = p(w1y|z) T\u220f t=2 p(wty|w<ty , z) (1)\nSpecifically, we generate the first word w1y from z, with p(w 1 y|z) = softmax(Vh1), where h1 = tanh(Cz). Bias terms are omitted for simplicity. All other words in the sentence are then sequentially generated using the RNN, until the end-sentence symbol is generated. Each conditional p(wty|w<ty , z), where< t = {1, . . . , t\u22121}, is specified as softmax(Vht), where ht, the hidden units, are recursively updated through ht = H(yt\u22121,ht\u22121, z). V is a weight matrix used for computing a distribution over words.\nThe transition functionH(\u00b7) is implemented with an LSTM [1]. Each LSTM unit has a cell containing a state ct at time t. Reading or writing the memory unit is controlled through sigmoid gates, namely, input gate it, forget gate ft, and output gate ot. The hidden units ht are updated as follows:\nit = \u03c3(Wiyt\u22121 +Uiht\u22121 +Ciz) ft = \u03c3(Wfyt\u22121 +Ufht\u22121 +Cfz) (2) ot = \u03c3(Woyt\u22121 +Uoht\u22121 +Coz) c\u0303t = tanh(Wcyt\u22121 +Ucht\u22121 +Ccz) (3) ct = ft ct\u22121 + it c\u0303t ht = ot tanh(ct) (4)\nwhere \u03c3(\u00b7) denotes the logistic sigmoid function, and represents the element-wise multiply operator (Hadamard product). W{i,f,o,c},U{i,f,o,c}, C{i,f,o,c}, V and C are the set of parameters. Note that z is used as an explicit input at each time step of the LSTM to guide the generation of sy .\nGiven the sentence pair (sx, sy), the objective function is the sum of the log-probabilities of the target sentence conditioned on the encoder representation in (1): log p(w1y|z) + \u2211T t=2 log p(w t y|w<ty , z). The total objective is the above objective summed over all the sentence pairs.\nApplications Inspired by [16], we propose three models: (i) an autoencoder, (ii) a future predictor, and (iii) the composite model. These models share the same CNN-LSTM model architecture, but are different in terms of the choices of the target sentence. An illustration of the proposed encoder-decoder models is shown in Figure 1(Left).\nThe autoencoder (i) aims to reconstruct the same sentence as the input. The intuition behind this is that an autoencoder learns to represent the data using features that explain its own important factors of variation, and hence model the internal structure of sentences, effectively capturing the intra-sentence information. Another natural unsupervised learning task is encoding the input sentence, namely, the sentence sent to the encoder, to predict the subsequent sentence w.r.t the input. The future predictor (ii) achieves this, effectively capturing the inter-sentence information, which has been shown to be useful to learn the semantics of a sentence [7]. These two tasks can be combined to create a composite model (iii), where the CNN encoder is asked to learn a feature vector that is useful to simultaneously reconstruct the input sentence and predict a future sentence. This composite model encourages the sentence encoder to incorporate contextual information both within and beyond the sentence.\nVocabulary expansion In our experiments, the CNN-LSTM models are trained with a vocabulary size of 22,154 words. In order to learn a generic sentence encoder that can encode a large number of possible words, we describe two methods to expand our encoder\u2019s vocabulary to words that have not been seen during training. Suppose we have a large pretrained word embedding matrix, such as the publicly available word2vec vectors that have dimensionality 300 and were trained using a continuous bag-of-words architecture [6].\nThe first method is the same as described in [7]. A linear mapping between the word2vec embedding space Vw2v and the CNN word embedding space Vcnn is learned by solving a linear regression problem. Thus, any word from Vw2v can be mapped into Vcnn for encoding sentences. The second method is conceptually simpler. We initialize the word vectors in Vcnn as the corresponding word vectors in Vw2v , and do not update the word embedding parameters during training. Thus, any word vector from Vw2v can be naturally used to encode sentences. We examine both methods in our experiments, and after vocabulary expansion, our trained sentence encoder can successfully encode 931,331 words.\nRelated work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others. However, most of the previous work focuses on utilizing LSTMs for both the encoder and decoder. The combination of CNN and LSTM has been considered in image captioning [17], and in some recent work on machine translation [18, 19]. Our utilization of a CNN is different, and more importantly, the ultimate goal of our model is different. Our work is the first to use a CNN for unsupervised learning of sentence representations."}, {"heading": "2.2 Hierarchical CNN-LSTM model", "text": "The future predictor described in Section 2.1 only considers the immediately subsequent sentence as context. By utilizing a larger surrounding context, it is likely that we can learn even higher-quality sentence representations. Inspired by the standard RNN-based language model [13] that uses the current word to predict future words, we propose a hierarchical encoder-decoder model that encodes the current sentence to predict multiple future sentences. An illustration of the hierarchical model is shown in Figure 1(Right).\nA CNN is used for sentence encoding, an LSTM is used for sentence decoding, and another LSTM on top of the CNN model is used for paragraph generation, capturing the dependencies between sentences. This encourages the current sentence to be responsible for generating all the future\nsentences, and hence enables the CNN encoder to leverage longer-term cross-sentence contextual information, when compared with the previously described future predictor.\nOur proposed hierarchical model characterizes the hierarchy word-sentence-paragraph. A paragraph is modeled as a sequence of sentences, and each sentence is modeled as a sequence of words. Specifically, assume we are given a paragraph D = (s1, . . . , sL), that consists of L sentences. The probability for a paragraph is then defined as\np(D) = L\u220f `=1 p(s`|s<`) p(s`|s<`) = T\u220f\u0300 t=1 p(w`,t|w`,<t, s<`) (5)\nwhere T` is the length of sentence `, and w`,t denotes the t-th word in sentence `. Inspecting (5), the future predictor can be considered as a special case of the hierarchical model, by making the Markov assumption p(D) = \u220fL `=1 p(s`|s`\u22121). In (5), each p(w`,t|w`,<t, s<`) is calculated as\np(w`,t|w`,<t, s<`) = softmax(Vh(s)`,t ) h (s) `,t = LSTM1(h (s) `,t\u22121,x`,t\u22121,h (p) ` ) (6)\nh (p) ` = LSTM2(h (p) `\u22121, z`) z` = CNN(s`\u22121) (7)\nwhere h(s)`,t denotes the t-th hidden state of the LSTM decoder for sentence `, h (p) ` denotes the `-th hidden state of the LSTM paragraph generator, see Figure 1(Right), and z` denotes the extracted feature vector for sentence `\u2212 1. Besides, x`,t\u22121 denotes the word embedding for w`,t\u22121, and V is a weight matrix used for computing distribution over words. As can be seen, h(p)` summarizes all the previous `\u2212 1 sentences, and is used to guide the generation of the `-th sentence. Note that the generation of s` implicitly depends on all the previous sentences s<`, rather than a single s`\u22121.\nRelated work We use our hierarchical CNN-LSTM model to extract meaningful sentence representations. However, our model can also be considered as a hierarchical language model for paragraphs. Similar work can be found in [20, 21, 22]. However, [20, 21] uses a LSTM for the sentence encoder, while [22] uses a bag-of-words to represent sentences. Our work is unique in that we use a CNN for sentence encoding, in an unsupervised setting."}, {"heading": "3 Experiments", "text": "We first provide qualitative analysis of our CNN encoder, and then present experimental results on 8 tasks: 5 classification benchmarks, paraphrase detection, semantic relatedness and image-sentence ranking. As in [7], we evaluate the capabilities of our encoder as a generic feature extractor. To further demonstrate the advantage of unsupervised training, we also fine-tune our trained sentence encoder on the 5 classification benchmarks. All the CNN-LSTM models are trained using the BookCorpus dataset [23], which consists of 70 million sentences from over 7000 books.\nWe train four models in total: (i) an autoencoder, (ii) a future predictor, (iii) the composite model, and (iv) the hierarchical model1. For the CNN encoder, we employ filter windows (h) of sizes {3,4,5} with 800 feature maps each, hence each sentence is represented as a 2400-dimensional vector. For both, the LSTM sentence decoder and paragraph generator, we use one hidden layer of 600 units.\nFor training, all weights in the CNN and non-recurrent weights in the LSTM are initialized from a uniform distribution in [-0.01,0.01]. Orthogonal initialization is employed on the recurrent matrices in the LSTM. All bias terms are initialized to zero. The initial forget gate bias for LSTM is set to 3. We initialize the word embeddings using word2vec vectors [6] and consider the two vocabulary expansion methods described in Section 2.1. Gradients are clipped if the norm of the parameter vector exceeds 5 [11]. The Adam algorithm [24] with learning rate 2\u00d710\u22124 is utilized for optimization. For all the CNN-LSTM models, we use mini-batches of size 64. For the hierarchical CNN-LSTM model, we use mini-batches of size 8, and each paragraph is composed of 8 sentences. All experiments are implemented in Theano [25], using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory. For reference, the CNN-LSTM composite model was trained for roughly two weeks."}, {"heading": "3.1 Qualitative analysis", "text": "We first demonstrate that the sentence representation learned by our model exhibits a linear structure that makes it possible to perform analogical reasoning using simple vector arithmetics, as illustrated in Table 1. It demonstrates that the arithmetic operations on the sentence representations correspond to word-level addition and subtractions. For instance, in the 3rd example, our encoder captures that the difference between sentence B and C is \u201cyou\" and \u201chim\", so that the former word in sentence A is replaced by the latter (i.e., \u201cyou\u201d-\u201cyou\u201d+\u201chim\u201d=\u201chim\u201d), resulting in sentence D.\nIn order to further demonstrate the different properties of the CNN and LSTM encoder, we train a CNNLSTM autoencoder and an LSTMLSTM autoencoder (LSTM encoder, and LSTM decoder), and empirically compare their sentence retrieval results. During training, the input sentences sent to the LSTM encoder are reversed. Given a query sentence, we retrieve its nearest neighbor when CNN and LSTM are used to encode the sentence. Nearest neighbors are scored by cosine similarity from a ran-\ndom sample of 1 million sentences from the BookCorpus dataset.\nThree examples are provided in Table 2. As can be seen, the CNN encoder captures information uniformly across the sentence, while the LSTM encoder tends to focus more on the beginning of a sentence. For instance, in the 1st example, the CNN encoder captures the 5-gram information \u201con the couch and watched\u201d, while the LSTM encoder emphasizes on the first two words \u201cwe sat\u201d. Further, from the 2nd and 3rd examples, we can see that our CNN encoder is capable of encoding the global semantic properties of a sentence, while the LSTM only uses the phrases appearing at the beginning of a sentence to determine semantic similarity. We provide additional examples of sentence retrieval results in the Supplementary Material."}, {"heading": "3.2 Quantitative evaluations", "text": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30]. A detailed description of the datasets is provided in the Supplementary Material. On all the datasets, we simply train a logistic regression model on top of the extracted sentence features. We restrict our comparison to unsupervised methods that do not require label information during training for fair comparison. Results are summarized in Table 3. As can be seen, our CNN encoder provides better results than the combine-skip model of [7] on 4 out of 5 datasets.\nWe highlight some observations. First, the autoencoder performs better than the future predictor, indicating that the intra-sentence information may be more important for classification than the inter-sentence information. Second, the hierarchical model performs better than the future predictor, demonstrating the importance of capturing long-term dependencies across multiple sentences. Our\n1We also tried the skip-thought construction [7] of our CNN-LSTM model, but empirically we did not observe better performance than the composite model, hence no results are reported.\ncombined model, which concatenates the feature vectors learned from both the hierarchical model and the composite model, performs the best. This is not surprising since in this setting, both intraand long-term inter-sentence information are leveraged.\nTo further demonstrate the advantage of unsupervised training, we train a CNN classifier (i.e., a CNN encoder with a logistic regression model on top) with two different initialization strategies: random initialization and initialization with trained parameters from the CNN-LSTM composite model. Results are shown in Figure 2 (Left). As can be seen, unsupervised pretraining provides substantial improvements (3.52% on average) over random initialization of CNN parameters. Figure 2 (Right) shows the effect of pretraining as the number of labeled sentences is varied. For the TREC dataset, the performance improves from 79.7% to 84.1% when only 10% sentences are labeled. As the size of the set of labeled sentences grows, the improvement becomes smaller as expected.\nParaphrase detection Now we consider paraphrase detection on the MSRP dataset [31]. On this task, one needs to predict whether or not two sentences are paraphrases. The training set consists of 4076 sentence pairs, and the test set has 1725 pairs. As in [32], given two sentence representations zx\nand zy , we first compute their element-wise product zx zy and their absolute difference |zx \u2212 zy|, and then concatenate them together. A logistic regression model is trained on top of the concatenated features to predict whether two sentences are paraphrases. We present our results on the last column of Table 3. Our best result is better than all the reported results, except the sequential denoising autoencoder (SDAE) used in [9]. However, SDAE performs poorly on all the classification datasets, while our model provides consistently competitive results.\nImage-sentence ranking We consider the task of image-sentence ranking, which aims to retrieve items in one modality given a query from the other. We use the Microsoft (MS) COCO dataset [33], which contains 123287 images each with 5 captions. For development and testing we use the same splits as [17]. The development and test sets each contain 1000 images and 5000 captions. Performance is evaluated using Recall@K, which measures the average times a correct item is found within the top-K retrieved results. We also report the median rank of the closest ground truth result in the ranked list.\nWe represent images using 4096-dimensional feature vectors from VggNet [34]. Each caption is encoded using our trained CNN encoder. The training objective we use is the same pairwise ranking loss as used in [7], which takes the form of max(0, \u03b1 \u2212 f(xn, yn) + f(xn, ym)), where f(\u00b7, \u00b7) is the image-sentence score. (xn, yn) denotes the related image-sentence pair, and (xn, ym) is the randomly sampled unrelated image-sentence pair with n 6= m. For image retrieval from sentences, x denotes the caption, y denotes the image, and vice versa. The objective is to force the matching score of the related pair (xn, yn) to be greater than the unrelated pair (xn, ym) by a margin \u03b1, which is set to 0.1 in our experiments. Detailed setup is provided in the Supplementary Material.\nhierarchical model+emb. 0.8352 0.7588 0.3152 composite model+emb. 0.8425 0.7742 0.3005 combine+emb. 0.8554 0.7893 0.2789\nTable 4 shows our results. We empirically found that the encoder trained using the fixed word embedding performed better on this task, hence only results using the second vocabulary expansion method are reported. As can be seen, we obtain the same median rank as in [7], indicating that our encoder is as competitive as the skip-thought vectors [7]. The performance gain between our encoder and the combine-skip model of [7] on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on retrieving the most correct item than a LSTM encoder.\nSemantic relatedness For our final experiment, we consider the task of semantic relatedness on the SICK dataset [35], consisting of 9927 sentence pairs. Given two sentences, our goal is to produce a realvalued score between [1, 5] to indicate how semantically related two sentences are, based on human generated scores. We compute a feature vector representing the pair of sentences in the same way as on the MSRP dataset. We follow the method in [32], and use the cross-entropy loss for training. Details are provided in the Supplementary Material. Results are summarized in Table 5. Our best\nresult is competitive with, though slightly worse than, the combine-skip model of [7]. This may suggest that CNN is good at discriminative classification tasks, but suboptimal at matching human relatedness judgements."}, {"heading": "4 Conclusion", "text": "We presented a new class of CNN-LSTM encoder-decoder models to learn sentence representations from unlabeled text. Our trained convolutional encoder is highly generic, and can be an alternative to the skip-thought vectors of [7]. Compelling experimental results on several tasks demonstrated the advantages of our approach.\nIn future work, we aim to use more advanced CNN architectures [10] for sentence encoding. Further, our proposed models can be used for other natural language applications. For example, the basic CNN-LSTM model can be used for machine translation [19], while the hierarchical CNN-LSTM model can be extended to learn document embeddings [20]."}, {"heading": "A Additional results", "text": "A.1 Vocabulary expansion\nIn the first vocabulary expansion method, a linear mapping between the word2vec embedding space Vw2v and the CNN word embedding space Vcnn is learned by solving a linear regression problem. We show in Table 6 examples of nearest neighbor words based on the word embeddings we have learned after using the first vocabulary expansion method.\nA.2 Detailed accuracies from Figure 2\nThe details of the accuracies shown in Figure 2(Left) and Figure 2(Right) can be found in Table 7 and Table 8, respectively.\nA.3 Sentence retrieval\nTable 9 shows nearest neighbors of sentences from a CNN-LSTM autoencoder trained on the BookCorpus dataset. As can be seen, our encoder learns to accurately capture semantic and syntax of the sentences."}, {"heading": "B Experimental details", "text": "B.1 Classification datasets\nWe test our CNN encoder on several benchmark datasets for sentence classification. Summary statistics of the datasets are in Table 10. A detailed description of the datasets is as follow. (i) TREC: This task involves classifying a question into 6 types [30]. (ii) MR: Movie reviews with one sentence per review. Classification involves predicting positive/negative reviews [26]. (iii) SUBJ: Subjectivity dataset where the task is to classify a sentence as being subjective or objective [28]. (iv) CR: Customer reviews of various products. This task is about predicting positive/negative reviews [27]. (v) MPQA: Opinion polarity detection subtask of the MPQA dataset [29].\nB.2 Image-sentence ranking method\nWe use the same method as in [7] for image-sentence ranking. The training objective is given by\n\u2211 x \u2211 k max(0, \u03b1\u2212 f(Ux,Vy) + f(Ux,Vyk)) + \u2211 y \u2211 k max(0, \u03b1\u2212 f(Vy,Ux) + f(Vy,Uxk))\nwhere x is the extracted image feature vector, y is the sentence feature vector, yk are vectors for contrastive (incorrect) sentences, and xk are vectors for contrastive images. f(\u00b7, \u00b7) is the imagesentence score using cosine similarity. U and V are the image embedding matrix and sentence embedding matrix (to be learned), respectively. We use a 1000-dimensional embedding, margin \u03b1 = 0.1 and k = 50 contrastive terms. The model was trained for 15 epochs.\nB.3 Semantic relatedness method\nWe follow the method in [32] for semantic relatedness. Let r> = [1, . . . , 5] be an integer vector from 1 to 5. Given two sentence representations zx and zy, we predict the similarity score y\u0302 = r>\u03c0\u0302\u03b8 , where\n\u03c0\u0302\u03b8 = softmax(W \u00b7 (zx zy) +U \u00b7 |zx \u2212 zy|+ b) (8)\nThis corresponds to a categorical distribution p\u0302\u03b8 = Cat(\u03c0\u0302\u03b8). \u03b8 = {W,U, b} is the set of parameters to be learned.\nGiven a similarity score y, we define a target distribution p = Cat(\u03c0) as follow\n\u03c0i = { y \u2212 byc, i = byc+ 1 byc \u2212 y + 1, i = byc 0 otherwise\nfor 1 \u2264 i \u2264 5. byc denotes the largest integer y\u0302 such that y\u0302 \u2264 y. The cost function is as follow\nJ\u03b8 = 1\nN N\u2211 n=1 KL ( p(n)||p\u0302(n)\u03b8 ) (9)\nwhere N is the number of sentence pairs; p(n) and p\u0302(n)\u03b8 denote the categorical distributions for pair n, defined above."}], "references": [{"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "ACL", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "EMNLP", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Q. Le", "T. Mikolov"], "venue": "ICML", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised sequence learning", "author": ["A. Dai", "Q. Le"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": "NAACL", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "NIPS", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "NAACL HLT", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "ICML", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["F. Meng", "Z. Lu", "M. Wang", "H. Li", "W. Jiang", "Q. Liu"], "venue": "ACL", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "M. Luong", "D. Jurafsky"], "venue": "ACL", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "and Ji", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J. Grue Simonsen"], "venue": "Nie. A hierarchical recurrent encoder-decoder for generative context-aware query suggestion. In CIKM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["R. Lin", "S. Liu", "M. Yang", "M. Li", "M. Zhou", "S. Li"], "venue": "EMNLP", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "ICCV", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "arXiv:1211.5590", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "ACL", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "SIGKDD", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "ACL", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["J. Wiebe", "T. Wilson", "C. Cardie"], "venue": "Language resources and evaluation", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning question classifiers", "author": ["X. Li", "D. Roth"], "venue": "ACL", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["B. Dolan", "C. Quirk", "C. Brockett"], "venue": "COLING", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K. Tai", "R. Socher", "C. Manning"], "venue": "ACL", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning techniques have shown promising performance on sentence modeling, via recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [2, 3], and recursive neural networks [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Deep learning techniques have shown promising performance on sentence modeling, via recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [2, 3], and recursive neural networks [4].", "startOffset": 159, "endOffset": 165}, {"referenceID": 2, "context": "Deep learning techniques have shown promising performance on sentence modeling, via recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [2, 3], and recursive neural networks [4].", "startOffset": 159, "endOffset": 165}, {"referenceID": 3, "context": "Deep learning techniques have shown promising performance on sentence modeling, via recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [2, 3], and recursive neural networks [4].", "startOffset": 197, "endOffset": 200}, {"referenceID": 4, "context": "The paragraph-vector model of [5] incorporates a global context vector into the log-linear neural language model [6] to learn the sentence representations; however, at prediction time, one needs to perform gradient descent to compute a new vector.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "The paragraph-vector model of [5] incorporates a global context vector into the log-linear neural language model [6] to learn the sentence representations; however, at prediction time, one needs to perform gradient descent to compute a new vector.", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "The skip-thought model of [7] describes an encoder-decoder model to reconstruct the surrounding sentences of an input sentence, where both the encoder and decoder are modeled as RNNs.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "The sequence autoencoder of [8] is a simple variant of [7], in which the decoder is used to reconstruct the input sentence itself.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "The sequence autoencoder of [8] is a simple variant of [7], in which the decoder is used to reconstruct the input sentence itself.", "startOffset": 55, "endOffset": 58}, {"referenceID": 8, "context": "Most recently, [9] proposed a sentence-level log-linear bag-of-words (BoW) model, where a BoW representation of an input sentence is used to predict adjacent sentences that are also represented as BoW.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "CNNs have recently achieved excellent results in various supervised natural language applications [2, 3, 10].", "startOffset": 98, "endOffset": 108}, {"referenceID": 2, "context": "CNNs have recently achieved excellent results in various supervised natural language applications [2, 3, 10].", "startOffset": 98, "endOffset": 108}, {"referenceID": 9, "context": "CNNs have recently achieved excellent results in various supervised natural language applications [2, 3, 10].", "startOffset": 98, "endOffset": 108}, {"referenceID": 6, "context": "Related to but distinct from the skip-thought model of [7], we propose to use a CNN encoder for unsupervised learning of sentence representations within the framework of encoder-decoder models ar X iv :1 61 1.", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "proposed by [11, 12].", "startOffset": 12, "endOffset": 20}, {"referenceID": 11, "context": "proposed by [11, 12].", "startOffset": 12, "endOffset": 20}, {"referenceID": 12, "context": "This model abstracts the RNN language model of [13] to the sentence level.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "Compared with the LSTM encoders used in [7, 8, 9], a CNN encoder may have the following advantages.", "startOffset": 40, "endOffset": 49}, {"referenceID": 7, "context": "Compared with the LSTM encoders used in [7, 8, 9], a CNN encoder may have the following advantages.", "startOffset": 40, "endOffset": 49}, {"referenceID": 8, "context": "Compared with the LSTM encoders used in [7, 8, 9], a CNN encoder may have the following advantages.", "startOffset": 40, "endOffset": 49}, {"referenceID": 6, "context": "For example, excluding the number of parameters used in the word embeddings, our trained sentence encoder has 3 million parameters, while the skip-thought vector of [7] contains 40 million parameters.", "startOffset": 165, "endOffset": 168}, {"referenceID": 6, "context": "As in [7], we first train our proposed models on a large collection of novels, and then evaluate the CNN sentence encoder as a generic feature extractor for 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "We show that our trained sentence encoder yields generic representations that perform as well as, or better than those of [7, 8, 9], in all the tasks considered.", "startOffset": 122, "endOffset": 131}, {"referenceID": 7, "context": "We show that our trained sentence encoder yields generic representations that perform as well as, or better than those of [7, 8, 9], in all the tasks considered.", "startOffset": 122, "endOffset": 131}, {"referenceID": 8, "context": "We show that our trained sentence encoder yields generic representations that perform as well as, or better than those of [7, 8, 9], in all the tasks considered.", "startOffset": 122, "endOffset": 131}, {"referenceID": 2, "context": "CNN encoder The CNN architecture in [3, 14] is used for sentence encoding, which consists of a convolution layer and a max-pooling operation over the entire sentence for each feature map.", "startOffset": 36, "endOffset": 43}, {"referenceID": 13, "context": "CNN encoder The CNN architecture in [3, 14] is used for sentence encoding, which consists of a convolution layer and a max-pooling operation over the entire sentence for each feature map.", "startOffset": 36, "endOffset": 43}, {"referenceID": 13, "context": "According to [14], we can induce one feature map c = f(X \u2217Wc + b) \u2208 RT\u2212h+1, where f(\u00b7) is a nonlinear activation function such as the hyperbolic tangent used in our experiments, b \u2208 RT\u2212h+1 is a bias vector, and \u2217 denotes the convolutional operator.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "We then apply a max-over-time pooling operation [14] to the feature map and take its maximum value, i.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "There exist other CNN architectures in the literature [2, 10, 15].", "startOffset": 54, "endOffset": 65}, {"referenceID": 9, "context": "There exist other CNN architectures in the literature [2, 10, 15].", "startOffset": 54, "endOffset": 65}, {"referenceID": 14, "context": "There exist other CNN architectures in the literature [2, 10, 15].", "startOffset": 54, "endOffset": 65}, {"referenceID": 2, "context": "We adopt the CNN model in [3, 14] due to its simplicity and excellent performance on classification.", "startOffset": 26, "endOffset": 33}, {"referenceID": 13, "context": "We adopt the CNN model in [3, 14] due to its simplicity and excellent performance on classification.", "startOffset": 26, "endOffset": 33}, {"referenceID": 0, "context": "The transition functionH(\u00b7) is implemented with an LSTM [1].", "startOffset": 56, "endOffset": 59}, {"referenceID": 15, "context": "Applications Inspired by [16], we propose three models: (i) an autoencoder, (ii) a future predictor, and (iii) the composite model.", "startOffset": 25, "endOffset": 29}, {"referenceID": 6, "context": "The future predictor (ii) achieves this, effectively capturing the inter-sentence information, which has been shown to be useful to learn the semantics of a sentence [7].", "startOffset": 166, "endOffset": 169}, {"referenceID": 5, "context": "Suppose we have a large pretrained word embedding matrix, such as the publicly available word2vec vectors that have dimensionality 300 and were trained using a continuous bag-of-words architecture [6].", "startOffset": 197, "endOffset": 200}, {"referenceID": 6, "context": "The first method is the same as described in [7].", "startOffset": 45, "endOffset": 48}, {"referenceID": 10, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 119, "endOffset": 127}, {"referenceID": 11, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 119, "endOffset": 127}, {"referenceID": 15, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 167, "endOffset": 171}, {"referenceID": 6, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 205, "endOffset": 214}, {"referenceID": 7, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 205, "endOffset": 214}, {"referenceID": 8, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 205, "endOffset": 214}, {"referenceID": 16, "context": "The combination of CNN and LSTM has been considered in image captioning [17], and in some recent work on machine translation [18, 19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "The combination of CNN and LSTM has been considered in image captioning [17], and in some recent work on machine translation [18, 19].", "startOffset": 125, "endOffset": 133}, {"referenceID": 18, "context": "The combination of CNN and LSTM has been considered in image captioning [17], and in some recent work on machine translation [18, 19].", "startOffset": 125, "endOffset": 133}, {"referenceID": 12, "context": "Inspired by the standard RNN-based language model [13] that uses the current word to predict future words, we propose a hierarchical encoder-decoder model that encodes the current sentence to predict multiple future sentences.", "startOffset": 50, "endOffset": 54}, {"referenceID": 19, "context": "Similar work can be found in [20, 21, 22].", "startOffset": 29, "endOffset": 41}, {"referenceID": 20, "context": "Similar work can be found in [20, 21, 22].", "startOffset": 29, "endOffset": 41}, {"referenceID": 21, "context": "Similar work can be found in [20, 21, 22].", "startOffset": 29, "endOffset": 41}, {"referenceID": 19, "context": "However, [20, 21] uses a LSTM for the sentence encoder, while [22] uses a bag-of-words to represent sentences.", "startOffset": 9, "endOffset": 17}, {"referenceID": 20, "context": "However, [20, 21] uses a LSTM for the sentence encoder, while [22] uses a bag-of-words to represent sentences.", "startOffset": 9, "endOffset": 17}, {"referenceID": 21, "context": "However, [20, 21] uses a LSTM for the sentence encoder, while [22] uses a bag-of-words to represent sentences.", "startOffset": 62, "endOffset": 66}, {"referenceID": 6, "context": "As in [7], we evaluate the capabilities of our encoder as a generic feature extractor.", "startOffset": 6, "endOffset": 9}, {"referenceID": 22, "context": "All the CNN-LSTM models are trained using the BookCorpus dataset [23], which consists of 70 million sentences from over 7000 books.", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "We initialize the word embeddings using word2vec vectors [6] and consider the two vocabulary expansion methods described in Section 2.", "startOffset": 57, "endOffset": 60}, {"referenceID": 10, "context": "Gradients are clipped if the norm of the parameter vector exceeds 5 [11].", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "The Adam algorithm [24] with learning rate 2\u00d710\u22124 is utilized for optimization.", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "All experiments are implemented in Theano [25], using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory.", "startOffset": 42, "endOffset": 46}, {"referenceID": 25, "context": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30].", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30].", "startOffset": 115, "endOffset": 119}, {"referenceID": 28, "context": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30].", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "As can be seen, our CNN encoder provides better results than the combine-skip model of [7] on 4 out of 5 datasets.", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "We also tried the skip-thought construction [7] of our CNN-LSTM model, but empirically we did not observe better performance than the composite model, hence no results are reported.", "startOffset": 44, "endOffset": 47}, {"referenceID": 8, "context": "ParagraphVec DM [9] 61.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "9 SDAE [9] 67.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "[9] 74.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "7 FastSent [9] 70.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "uni-skip [7] 75.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "9 bi-skip [7] 73.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "2 combine-skip [7] 76.", "startOffset": 15, "endOffset": 18}, {"referenceID": 30, "context": "Paraphrase detection Now we consider paraphrase detection on the MSRP dataset [31].", "startOffset": 78, "endOffset": 82}, {"referenceID": 31, "context": "As in [32], given two sentence representations zx", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "uni-skip [7] 30.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "7 4 bi-skip [7] 32.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "2 4 combine-skip [7] 33.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "Our best result is better than all the reported results, except the sequential denoising autoencoder (SDAE) used in [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 32, "context": "We use the Microsoft (MS) COCO dataset [33], which contains 123287 images each with 5 captions.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "For development and testing we use the same splits as [17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 33, "context": "We represent images using 4096-dimensional feature vectors from VggNet [34].", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "The training objective we use is the same pairwise ranking loss as used in [7], which takes the form of max(0, \u03b1 \u2212 f(xn, yn) + f(xn, ym)), where f(\u00b7, \u00b7) is the image-sentence score.", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "uni-skip [7] 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "2872 bi-skip [7] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "2995 combine-skip [7] 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": "As can be seen, we obtain the same median rank as in [7], indicating that our encoder is as competitive as the skip-thought vectors [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "As can be seen, we obtain the same median rank as in [7], indicating that our encoder is as competitive as the skip-thought vectors [7].", "startOffset": 132, "endOffset": 135}, {"referenceID": 6, "context": "The performance gain between our encoder and the combine-skip model of [7] on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on retrieving the most correct item than a LSTM encoder.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "Given two sentences, our goal is to produce a realvalued score between [1, 5] to indicate how semantically related two sentences are, based on human generated scores.", "startOffset": 71, "endOffset": 77}, {"referenceID": 4, "context": "Given two sentences, our goal is to produce a realvalued score between [1, 5] to indicate how semantically related two sentences are, based on human generated scores.", "startOffset": 71, "endOffset": 77}, {"referenceID": 31, "context": "We follow the method in [32], and use the cross-entropy loss for training.", "startOffset": 24, "endOffset": 28}, {"referenceID": 6, "context": "result is competitive with, though slightly worse than, the combine-skip model of [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "Our trained convolutional encoder is highly generic, and can be an alternative to the skip-thought vectors of [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "In future work, we aim to use more advanced CNN architectures [10] for sentence encoding.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "For example, the basic CNN-LSTM model can be used for machine translation [19], while the hierarchical CNN-LSTM model can be extended to learn document embeddings [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "For example, the basic CNN-LSTM model can be used for machine translation [19], while the hierarchical CNN-LSTM model can be extended to learn document embeddings [20].", "startOffset": 163, "endOffset": 167}], "year": 2016, "abstractText": "We propose a new encoder-decoder approach to learn distributed sentence representations from unlabeled sentences. The word-to-vector representation is used, and convolutional neural networks are employed as sentence encoders, mapping an input sentence into a fixed-length vector. This representation is decoded using long short-term memory recurrent neural networks, considering several tasks, such as reconstructing the input sentence, or predicting the future sentence. We further describe a hierarchical encoder-decoder model to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.", "creator": "LaTeX with hyperref package"}}}