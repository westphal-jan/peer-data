{"id": "1612.00155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Adversarial Images for Variational Autoencoders", "abstract": "orbitz.com We gaute investigate adversarial attacks chudinov for bei autoencoders. purusha We propose cross-dressers a procedure eastview that anti-aliasing distorts the unheralded input milawa image to mislead the reconnecting autoencoder re-enacts in electrum reconstructing a completely different target vtt image. We attack bonzo the internal latent representations, yo-yos attempting rehabs to make euro297 the adversarial input produce an brougher internal representation lague as breezin similar buvaysa as possible anonyme as the 29,035 target ' 211th s. 79-74 We 7,240 find that posluszny autoencoders are coleophora much more radivojevic robust galante to the attack 18.59 than snfu classifiers: male-female while some prospering examples have tolerably small 2008-2013 input t-series distortion, person-to-person and reasonable similarity voznesensky to \u0161ar the bloomingdales target litany image, there is a mandatorily quasi - tayfur linear hewitt trade - anglosphere off 100-million between weeda those aims. We 43-6 report nemser results on chandon MNIST and aspinall SVHN vend\u00f4me datasets, fichman and photochemistry also yeoville test scalpay regular deterministic autoencoders, racc reaching oi similar sportsline conclusions ehe in all bibit cases. ballenas Finally, we keinath show g.hn that sirolimus the usual adversarial attack for classifiers, matej while being much easier, also gherardini presents a akhlaq direct w-l proportion between urzica distortion on oxidized the acca input, and tordenskjold misdirection nijhoff on the isospin output. bourbourg That 71.13 proportionality vampiric however is hidden orgy by the salukvadze normalization of the 24-16 output, j\u00e4germeister which akamine maps a arnoun linear layer canonize into liposuction non - meitetsu linear selfless probabilities.", "histories": [["v1", "Thu, 1 Dec 2016 05:59:57 GMT  (1331kb,D)", "http://arxiv.org/abs/1612.00155v1", "Workshop on Adversarial Training, NIPS 2016, Barcelona, Spain"]], "COMMENTS": "Workshop on Adversarial Training, NIPS 2016, Barcelona, Spain", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["pedro tabacof", "julia tavares", "eduardo valle"], "accepted": false, "id": "1612.00155"}, "pdf": {"name": "1612.00155.pdf", "metadata": {"source": "CRF", "title": "Adversarial Images for Variational Autoencoders", "authors": ["Pedro Tabacof", "Julia Tavares"], "emails": ["dovalle}@dca.fee.unicamp.br"], "sections": [{"heading": "1 Introduction", "text": "Adversarial attacks expressly optimize the input to \u201cfool\u201d models, e.g., in image classification, the adversarial input \u2014 while visually tantamount to an ordinary original image \u2014 leads to mislabelling with high confidence.\nHere, we explore adversarial images for autoencoders \u2014 models optimized to reconstruct their inputs from compact internal representations. In an autoencoder, the attack targets not a single label, but a whole reconstruction. Our contributions include:\n\u2022 An adversarial attack on variational \u2014 and, for comparison, deterministic \u2014 autoencoders. Our attack aims not only at disturbing the reconstruction, but at fooling the autoencoder into reconstructing a completely different target image;\n\u2022 A comparison between attacks for autoencoders and for classifiers, showing that while the former is much harder, in both cases the amount of distortion on the input is proportional to the amount of misdirection on the output. For classifiers, however, such proportionality is hidden by the normalization of the output, which maps a linear layer into non-linear probabilities.\nEvaluating generative models is hard [1], there are no clear-cut success criteria for autoencoder reconstruction, and therefore, neither for the attack. We attempt to bypass that difficulty by analyzing how inputs and outputs differ across varying regularization constants.\nThe seminal article of Szegedy et al. [2] introduced adversarial images, showing how to force a deep network to misclassify an image by applying nearly imperceptible distortions. Goodfellow et al. [3] exploited the linear nature of deep convolutional networks to both attempt explaining how adversarial samples arise, and to propose a much faster technique to create them. Tabacof and Valle [4] explored\nWorkshop on Adversarial Training, NIPS 2016, Barcelona, Spain.\nar X\niv :1\n61 2.\n00 15\n5v 1\n[ cs\n.N E\n] 1\nD ec\n2 01\nthe geometry of adversarial regions, showing that they appear in relatively dense regions of the input space, and that shallow, simple classifiers tend to be more robust to them.\nThe existence of adversarial images lead to interesting questions on their significance, and even usefulness. Training models to resist adversarial attacks was advanced as a form of regularization [3, 5]. Gu et al. [6] used autoencoders to pre-process the input and try to reinforce the network against adversarial attacks, finding that although in some cases resistance improved, attacks with small distortions remained possible. A more recent trend is training adversarial models, in which one attempts to generate \u201cartificial\u201d samples (from a generative model) and the other attempts to recognize those samples [7]. Makhzani et al. [8] employ such scheme to train an autoencoder.\nAlthough autoencoders appear in the literature of adversarial images as an attempt to obtain robustness to the attacks [6], and in the literature of adversarial training as models that can be trained with the technique [8], we are unaware of any attempts to create attacks targeted to them. In the closest related literature, Sara Sabour et al. [9] show that adversarial attacks can not only lead to mislabelling, but also manipulate the internal representations of the network. In this paper, we show that an analogous manipulation allows us to attack autoencoders, but that those remain much more resistant than classifiers to such attacks."}, {"heading": "2 Autoencoders and Variational Autoencoders", "text": "Autoencoders are models that map their input into a compact latent representation, and then, from such representation, build back the input (discounting some distortion). Therefore, autoencoders are trained to minimize the distortion between their input and their (reconstructed) output \u2014 plus regularization terms. The model comprises two parts: an encoder, which maps the input into the latent representation; and a decoder, which maps such representation into an output as close to the input as possible. In regular autoencoders, the training loss function may be as simple as the `2-distance between input and output.\nFamous variants include sparse autoencoders, which use `1-regularization [10], and denoising autoencoders, which use implicit regularization by feeding noise to the input, while keeping the original input in the reconstruction loss term [11]. An important offshoot are models with similar encoder\u2013decoder structure, but which seek not to reconstruct the input, but to produce an output related to it (e.g., a segmentation map) [12].\nA modern variant of growing popularity, variational autoencoders [13] interpret the latent representation through a Bayesian lens, thus offering a theoretical foundation for the reconstruction and regularization objectives. Variational autoencoders are probabilistic generative models, where we find the probability distribution of the data by marginalizing over the latent variables:\np\u03b8(x) = \u222b p\u03b8(x, z)dz = \u222b p\u03b8(x|z)p(z)dz (1)\nThe likelihood p\u03b8(x|z) is the probabilistic explanation of the observed data: in practice, often it is simply the output of the decoder network under a noise consideration (e.g. additive Gaussian noise for\nRGB pixels). The subscript \u03b8 comprises all decoder parameters, while z is the latent representation, over which we marginalize. The representation prior p(z) is often the standard normal N (0, I) [13], but might be instead a discrete distribution (e.g. Bernoulli) [14], or even some distribution with geometric interpretation (\u201cwhat\u201d and \u201cwhere\u201d latent variables) [15]. Since the integration above is often intractable, we maximize its variational lower bound...\nEq\u03c6(z|x)[log p\u03b8(x|z)]\u2212KL(q\u03c6(z|x) \u2016 p(z)) = \u2212KL(q\u03c6(z|x) \u2016 p(z|x)) [\u2264 log p(x)] (2)\n...which is the Kullback\u2013Leibler (KL) divergence between the approximate and the (unknown) exact posterior. Thus, maximizing the variational lower bound may also be interpreted as finding the best posterior approximation. In the context of variational autoencoders, such approximate posterior is usually an uncorrelated multivariate normal determined by the encoder network (with parameters \u03c6):\nq\u03c6(z|x) = N (\u00b5\u03c6(x), exp(\u03c32\u03c6(x))) (3)\nWe can approximate the likelihood expectation Eq\u03c6(z|x)[log p\u03b8(x|z)] by Monte Carlo. As the prior and the approximated posterior are normal distributions, their KL divergence has analytic form [13]. We can use the reparameterization trick to reduce the variance of the gradient estimator [16].\nThe encoder and the decoder may be any neural network: a multilayer perceptron [13], a convolutional network [17], or even LSTMs. The latter are a recent development \u2014 recurrent variational autoencoders \u2014 which use soft attention to encode and decode patches from the input image [18, 19]. Simulating a chain of samples from the latent variables and likelihood allows to denoise images, or to impute missing data (inpaint images) [20]. The latent variables of a variational autoencoder also allow visual analogy and interpolation [17]."}, {"heading": "3 Adversarial Images for Autoencoders", "text": "Adversarial procedures minimize an adversarial loss to mislead the model (e.g., misclassification), while distorting the input as little as possible. If the attack is successful, humans should hardly be able to distinguish between the adversarial and the regular inputs [2, 4]. We can be even more strict, and only allow a distortion below the input quantization noise [3, 9].\nTo build adversarial images for classification, one can maximize the misdirection towards a certain wrong label [2, 4] or away from the correct one [3]. The distortion can be minimized [2, 4] or constrained to be small [3, 9]. Finally, one often requires that images stay within their valid space (i.e., no pixels \u201cbelow black or above white\u201d).\nIn autoencoders, there is not a single class output to misclassify, but instead a whole image output to scramble. The attack attempts to mislead the reconstruction: if a slightly altered image enters the autoencoder, but the reconstruction is wrecked, then the attack worked. A more dramatic attack \u2014 the one we attempt in this paper \u2014 would be to change slightly the input image and make the autoencoder reconstruct a completely different valid image (Fig. 2).\nOur attack consists in selecting an original image and a target image, and then feeding the network the original image added to a small distortion, optimized to get an output as close to the target image as possible (Fig. 2). Our attempts to attack the output directly failed: minimizing its distance to the target only succeeded in blurring the reconstruction. As autoencoders reconstruct from the latent representation, we can attack it instead. The latent layer is the information bottleneck of the autoencoder, and thus particularly convenient to attack. We used the following adversarial optimization:\nmin d\n\u2206(za,zt) +C\u2016d\u2016\ns.t. L \u2264 x+d \u2264 U za = encoder(x+d)\n(4)\nwhere d is the adversarial distortion; za and zt are the latent representations, respectively, for the adversarial and the target images; x is the original image; x+ d is the adversarial image; L and U\nare the bounds on the input space; and C is the regularizing constant the balances reaching the target and limiting the distortion.\nWe must choose a function \u2206 to compare representations. For regular autoencoders a simple `2- distance sufficed; however, for variational autoencoders, the KL-divergence between the distributions induced by the latent variables not only worked better, but also offered a sounder justification. In our variational autoencoders, the z\u2217 are uncorrelated multivariate normal distributions with parameters given by the encoder:\nencoder(x) \u223c N (M\u03c6(x),\u03a3\u03c6(x)) (5)\nwhere M and \u03a3 are the representation mean vector, and (diagonal) covariance matrix output by the last layer of the encoder network; while \u03c6 are the autoencoder parameters \u2014 learned previously by training it for its ordinary task of reconstruction. During the entire adversarial procedure, \u03c6 remains fixed."}, {"heading": "4 Data and Methods", "text": "We worked on the binarized MNIST [21] and SVHN datasets [22]. The former allows for very fast experiments and very controlled conditions; the latter, while still allowing to manage a large number of experiments, provides much more noise and variability. Following literature [13], we modeled pixel likelihoods as independent Bernoullis (for binary images), or as independent normals (for RGB images). We used Parmesan and Lasagne [23] for the implementation1.\nThe loss function to train the variational autoencoder (equation 2) is the expectation of the likelihood under the approximated posterior plus the KL divergence between the approximated posterior and the prior. We approximate the expectation of the likelihood with one sample of the posterior. We extract the gradients of the lower bound using automatic differentiation and maximize it using stochastic gradient ascent via the ADAM algorithm [24]. We used 20 and 100 latent variables for MNIST and SVHN, respectively. We parameterized the encoder and decoder as fully-connected networks in the MNIST case, and as convolutional and deconvolutional [25] networks in the SVHN case. After the training is done, we can use the autoencoder to reconstruct some image samples through the latent variables, which are the learned representation of the images. An example of a pair of input image/reconstructed output appears in Fig. 1.\n1The code for the experiments can be found at https://github.com/tabacof/adv_vae\nFor classification tasks, the regularization term C (Eq. 4) may be chosen by bisection as the smallest constant that still leads to success [4]. Autoencoders complicate such choice, for there is no longer a binary criterion for success. Goodfellow et al. [3] and Sabour et al.[9] optimize differently, choosing for \u2206 an `\u221e-norm constrained to make the distortion imperceptible, while maximizing the misdirection. We found such solution too restrictive, leading to reconstructions visually too distinct from the target images. Our solution was instead to forgo a single choice for C, and analyze the behavior of the system throughout a series of values.\nIn our experiments, we pick at random 25 pairs of original/target images (axis \u201cexperiment\u201d in graphs). For each pair, we span 100 different values for the regularization constant C in a logarithmic scale (from 2\u221220 to 220), measuring the `2-distance between the adversarial input and the original image (axis \u201cdistortion\u201d), and the `2-distance between the reconstructed output and the target image (axis \u201cadversarial\u2212target\u201d). The \u201cdistortion\u201d axis is normalized between 0.0 (no attack) and the `2-distance between the original and target images in the pair (a large distortion that could reach the target directly). The \u201cadversarial\u2212target\u201d is normalized between the `2-distance of the reconstruction of the target and the target (the best expected attack) and the `2-distance of the reconstruction of the original and the target (the worst expected attack). The geometry of such normalization is illustrated by the colored lines in the graphs of Fig. 3. For variational autoencoders, the reconstruction is stochastic: therefore, each data point is sampled 100 times, and the average is reported.\nFor comparison purposes, we use the same protocol above to generate a range of adversarial images for the usual classification tasks on the same datasets. The aim is to contrast the behavior of adversarial attacks across the two tasks (autoencoding / classification). In those experiments we pick pairs of original image / adversarial class (axis \u201cexperiment\u201d), and varying C (from 2\u221210 to 220), we measure the distortion as above, and the probability (with corresponding logit) attributed to the adversarial (red lines) and to the original classes (blue lines). The axes here are no longer normalized, but we center at 0 in the \u201cdistortion\u201d axis the transition point between attack failure and success \u2014 the point where red and blue lines cross."}, {"heading": "5 Results and Discussion", "text": "We found that generating adversarial images for autoencoders is a much harder task than for classifiers. If we apply little distortion (comparable to those used for misleading classifiers), the reconstructions stay essentially untouched. To get reconstructions very close to the target\u2019s, we have to apply heavy distortions to the input. However, by hand-tuning the regularization parameter, it is possible to find trade-offs where the reconstruction approaches the target\u2019s and the adversarial image will still resemble the input (two examples in Fig. 3).\nThe plots for the full set of 25 original/target image pairs appear in Fig. 4. All series saturate when the latent representation of the adversarial image essentially equals the target\u2019s. That saturation appears well before the upper distortion limit of 1.0, and provides a measure of how resistant the model is to the attack: Variational Autoencoders appear slightly more resistant than Deterministic Autoencoders, and MNIST much more resistant than SVHN. The latter is not surprising, since large complex models seem, in general, more susceptible to adversarial attacks. Before the \u201chinge\u201d where the attack saturates, there is a quasi-linear trade-off between input distortion and output similarity to target, for all combinations of dataset and autoencoder choice. We were initially hoping for a more non-linear behavior, with a sudden drop at some point in the scale, but data suggests that there is a give-and-take for attacking autoencoders: each gain in the attack requires a proportional increase in distortion.\nThe comparison with the (much better-studied) attacks for classifiers, showed, at the beginning, a much different behavior: when we contrasted the probability attributed to the adversarial class vs. the distortion imposed on the input, we observed the non-linear, sudden change we were expecting (left column of Fig. 6). The question remained, however whether such non-linearity was intrinsic, or whether it was due to the highly non-linear nature of the probability scale. The answer appears in the right column of Fig. 6, where, with a logit transformation of the probabilities, the linear behavior appears again. It seems that the attack on classifiers show, internally, the same linear give-and-take present in autoencoders, but that the normalization of the outputs of the last layer into valid probabilities aids the attack: changes in input lead to proportional changes in logit, but to much larger changes in probability. That makes feasible for the attack on classifiers to find much better\nsweet spots than the attack on autoencoders (Fig. 5). Goodfellow et al. [3] suggested that the linearity of deep models make them susceptible to adversarial attacks. Our results seems to reinforce that such linearity plays indeed a critical role, with \u201cinternal\u201d success of the attack being proportional to the distortion on inputs. On classification networks, however, which are essentially piecewise linear until the last layer, the non-linearity of the latter seems to compound the problem."}, {"heading": "6 Conclusion", "text": "We proposed an adversarial method to attack autoencoders, and evaluated their robustness to such attacks. We showed that there is a linear trade-off between how much the adversarial input is similar to the original input, and how much the adversarial reconstruction is similar to the target reconstruction \u2014 frustrating the hope that a small change in the input could lead to drastic changes in the reconstruction. Surprisingly, such linear trade-off also appears for adversarial attacks on classification networks, if we \u201cundo\u201d the non-linearity of the last layer. In the future, we intend to extend our empirical results to datasets with larger inputs and more complex networks (e.g. ImageNet) \u2014 as well as to different autoencoder architectures. For example, the DRAW variational autoencoder [18] uses feedback from the reconstruction error to improve the reconstruction \u2014 and thus could be more robust to attacks. We are also interested in advancing theoretical explanations to illuminate our results."}, {"heading": "Acknowledgments", "text": "We thank Brazilian agencies CAPES, CNPq and FAPESP for financial support. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research. Eduardo Valle is partially supported by a Google Awards LatAm 2016 grant, and by a CNPq PQ-2 grant (311486/2014-2)."}], "references": [{"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "A\u00e4ron van den Oord", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Exploring the space of adversarial images", "author": ["Pedro Tabacof", "Eduardo Valle"], "venue": "arXiv preprint arXiv:1510.05328,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Distributional smoothing by virtual adversarial examples", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii"], "venue": "arXiv preprint arXiv:1507.00677,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "arXiv preprint arXiv:1412.5068,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Adversarial manipulation of deep representations", "author": ["Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J Fleet"], "venue": "arXiv preprint arXiv:1511.05122,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Sparse autoencoder", "author": ["Andrew Ng"], "venue": "CS294A Lecture notes,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["Hyeonwoo Noh", "Seunghoon Hong", "Bohyung Han"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["SM Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa", "Koray Kavukcuoglu", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1603.08575,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Diederik P Kingma", "Tim Salimans", "Max Welling"], "venue": "arXiv preprint arXiv:1506.02557,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Towards conceptual compression", "author": ["Karol Gregor", "Frederic Besse", "Danilo Jimenez Rezende", "Ivo Danihelka", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1604.08772,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Lasagne: First release", "author": ["Sander Dieleman", "Jan Schl\u00fcter", "Colin Raffel", "Eben Olson", "S\u00f8ren Kaae S\u00f8nderby", "Daniel Nouri", "Daniel Maturana", "Martin Thoma", "Eric Battenberg", "Jack Kelly"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Deconvolutional networks", "author": ["Matthew D Zeiler", "Dilip Krishnan", "Graham W Taylor", "Rob Fergus"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Evaluating generative models is hard [1], there are no clear-cut success criteria for autoencoder reconstruction, and therefore, neither for the attack.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "[2] introduced adversarial images, showing how to force a deep network to misclassify an image by applying nearly imperceptible distortions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] exploited the linear nature of deep convolutional networks to both attempt explaining how adversarial samples arise, and to propose a much faster technique to create them.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Tabacof and Valle [4] explored", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "Training models to resist adversarial attacks was advanced as a form of regularization [3, 5].", "startOffset": 87, "endOffset": 93}, {"referenceID": 4, "context": "Training models to resist adversarial attacks was advanced as a form of regularization [3, 5].", "startOffset": 87, "endOffset": 93}, {"referenceID": 5, "context": "[6] used autoencoders to pre-process the input and try to reinforce the network against adversarial attacks, finding that although in some cases resistance improved, attacks with small distortions remained possible.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "A more recent trend is training adversarial models, in which one attempts to generate \u201cartificial\u201d samples (from a generative model) and the other attempts to recognize those samples [7].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "Although autoencoders appear in the literature of adversarial images as an attempt to obtain robustness to the attacks [6], and in the literature of adversarial training as models that can be trained with the technique [8], we are unaware of any attempts to create attacks targeted to them.", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "[9] show that adversarial attacks can not only lead to mislabelling, but also manipulate the internal representations of the network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Famous variants include sparse autoencoders, which use `1-regularization [10], and denoising autoencoders, which use implicit regularization by feeding noise to the input, while keeping the original input in the reconstruction loss term [11].", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "Famous variants include sparse autoencoders, which use `1-regularization [10], and denoising autoencoders, which use implicit regularization by feeding noise to the input, while keeping the original input in the reconstruction loss term [11].", "startOffset": 237, "endOffset": 241}, {"referenceID": 10, "context": ", a segmentation map) [12].", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "A modern variant of growing popularity, variational autoencoders [13] interpret the latent representation through a Bayesian lens, thus offering a theoretical foundation for the reconstruction and regularization objectives.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "The representation prior p(z) is often the standard normal N (0, I) [13], but might be instead a discrete distribution (e.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "Bernoulli) [14], or even some distribution with geometric interpretation (\u201cwhat\u201d and \u201cwhere\u201d latent variables) [15].", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "Bernoulli) [14], or even some distribution with geometric interpretation (\u201cwhat\u201d and \u201cwhere\u201d latent variables) [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "As the prior and the approximated posterior are normal distributions, their KL divergence has analytic form [13].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "We can use the reparameterization trick to reduce the variance of the gradient estimator [16].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "The encoder and the decoder may be any neural network: a multilayer perceptron [13], a convolutional network [17], or even LSTMs.", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "The encoder and the decoder may be any neural network: a multilayer perceptron [13], a convolutional network [17], or even LSTMs.", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "The latter are a recent development \u2014 recurrent variational autoencoders \u2014 which use soft attention to encode and decode patches from the input image [18, 19].", "startOffset": 150, "endOffset": 158}, {"referenceID": 17, "context": "The latter are a recent development \u2014 recurrent variational autoencoders \u2014 which use soft attention to encode and decode patches from the input image [18, 19].", "startOffset": 150, "endOffset": 158}, {"referenceID": 18, "context": "Simulating a chain of samples from the latent variables and likelihood allows to denoise images, or to impute missing data (inpaint images) [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "The latent variables of a variational autoencoder also allow visual analogy and interpolation [17].", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": "If the attack is successful, humans should hardly be able to distinguish between the adversarial and the regular inputs [2, 4].", "startOffset": 120, "endOffset": 126}, {"referenceID": 3, "context": "If the attack is successful, humans should hardly be able to distinguish between the adversarial and the regular inputs [2, 4].", "startOffset": 120, "endOffset": 126}, {"referenceID": 2, "context": "We can be even more strict, and only allow a distortion below the input quantization noise [3, 9].", "startOffset": 91, "endOffset": 97}, {"referenceID": 7, "context": "We can be even more strict, and only allow a distortion below the input quantization noise [3, 9].", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "To build adversarial images for classification, one can maximize the misdirection towards a certain wrong label [2, 4] or away from the correct one [3].", "startOffset": 112, "endOffset": 118}, {"referenceID": 3, "context": "To build adversarial images for classification, one can maximize the misdirection towards a certain wrong label [2, 4] or away from the correct one [3].", "startOffset": 112, "endOffset": 118}, {"referenceID": 2, "context": "To build adversarial images for classification, one can maximize the misdirection towards a certain wrong label [2, 4] or away from the correct one [3].", "startOffset": 148, "endOffset": 151}, {"referenceID": 1, "context": "The distortion can be minimized [2, 4] or constrained to be small [3, 9].", "startOffset": 32, "endOffset": 38}, {"referenceID": 3, "context": "The distortion can be minimized [2, 4] or constrained to be small [3, 9].", "startOffset": 32, "endOffset": 38}, {"referenceID": 2, "context": "The distortion can be minimized [2, 4] or constrained to be small [3, 9].", "startOffset": 66, "endOffset": 72}, {"referenceID": 7, "context": "The distortion can be minimized [2, 4] or constrained to be small [3, 9].", "startOffset": 66, "endOffset": 72}, {"referenceID": 19, "context": "We worked on the binarized MNIST [21] and SVHN datasets [22].", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "We worked on the binarized MNIST [21] and SVHN datasets [22].", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "Following literature [13], we modeled pixel likelihoods as independent Bernoullis (for binary images), or as independent normals (for RGB images).", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "We used Parmesan and Lasagne [23] for the implementation1.", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "We extract the gradients of the lower bound using automatic differentiation and maximize it using stochastic gradient ascent via the ADAM algorithm [24].", "startOffset": 148, "endOffset": 152}, {"referenceID": 23, "context": "We parameterized the encoder and decoder as fully-connected networks in the MNIST case, and as convolutional and deconvolutional [25] networks in the SVHN case.", "startOffset": 129, "endOffset": 133}, {"referenceID": 3, "context": "4) may be chosen by bisection as the smallest constant that still leads to success [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "[3] and Sabour et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] optimize differently, choosing for \u2206 an `\u221e-norm constrained to make the distortion imperceptible, while maximizing the misdirection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] suggested that the linearity of deep models make them susceptible to adversarial attacks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "For example, the DRAW variational autoencoder [18] uses feedback from the reconstruction error to improve the reconstruction \u2014 and thus could be more robust to attacks.", "startOffset": 46, "endOffset": 50}], "year": 2016, "abstractText": "We investigate adversarial attacks for autoencoders. We propose a procedure that distorts the input image to mislead the autoencoder in reconstructing a completely different target image. We attack the internal latent representations, attempting to make the adversarial input produce an internal representation as similar as possible as the target\u2019s. We find that autoencoders are much more robust to the attack than classifiers: while some examples have tolerably small input distortion, and reasonable similarity to the target image, there is a quasi-linear trade-off between those aims. We report results on MNIST and SVHN datasets, and also test regular deterministic autoencoders, reaching similar conclusions in all cases. Finally, we show that the usual adversarial attack for classifiers, while being much easier, also presents a direct proportion between distortion on the input, and misdirection on the output. That proportionality however is hidden by the normalization of the output, which maps a linear layer into non-linear probabilities.", "creator": "LaTeX with hyperref package"}}}