{"id": "1612.00240", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "An Evaluation of Models for Runtime Approximation in Link Discovery", "abstract": "muridke Time - kwanzaa efficient matted link self-loading discovery 5l is huahine of central importance to lasik implement the vision mawyer of nanan the 75,000-seat Semantic Web. bujji Some of registered the most rapid Link sohaemus Discovery approaches euro693 rely internally 58.74 on planning to updater execute link specifications. romper In newer genet works, linear contorta models rambeau have bastard been used drue to estimate tankful the brl runtime the baader fastest planners. manolescu However, no other gattelli category ifes of hatosy models has been updrafts studied holloway for this deisler purpose ardsnet so enlisted far. nusser In inskeep this nasya paper, we dinitz study non - zver linear runtime estimation functions srirot for labarthe runtime azriel estimation. In particular, marciniak we study pekar exponential and logics mixed participaciones models r\u00e5sunda for falsettos the estimation of forebrain the runtimes of planners. To 3,271 this concertgoers end, we evaluate tutorial three different guerreiro models 46.99 for runtime on 16.81 six internal datasets using 3,245 400 tok103 link left-wing specifications. We lutfullah show herrara that exponential and willems mixed matzinger models achieve better fits reclassification when trained but invap are only to alquerque be preferred undermine in some cases. albright-knox Our cassone evaluation stirland also 0.975 shows that kondh the herey use nonworking of better runtime playback approximation ismm models has bicho a exantus positive meindert impact hanfu on redbreast the overall ahk execution of 5,833 link specifications.", "histories": [["v1", "Thu, 1 Dec 2016 13:33:03 GMT  (31kb)", "http://arxiv.org/abs/1612.00240v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kleanthi georgala", "micheal hoffmann", "axel-cyrille ngonga ngomo"], "accepted": false, "id": "1612.00240"}, "pdf": {"name": "1612.00240.pdf", "metadata": {"source": "CRF", "title": "An Evaluation of Models for Runtime Approximation in Link Discovery", "authors": ["Kleanthi Georgala", "Micheal Hoffmann", "Axel-Cyrille Ngonga Ngomo"], "emails": ["georgala@informatik.uni-leipzig.de", "mhoffmann@informatik.uni-leipzig.de", "ngonga@informatik.uni-leipzig.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 2.\n00 24\n0v 1\n[ cs\n.A I]\n1 D\nec 2"}, {"heading": "1 Introduction", "text": "Link discovery frameworks are of utmost importance during the creation of Linked Data [1]. This is due to their being the key towards the implementation of the fourth Linked Data principle, i.e., the provision of links between datasets.1 Two main challenges need to be addressed by Link Discovery frameworks [13,15]. First, they need to address the accuracy challenge, i.e., they need to generate correct links. A plethora of approaches have been developed for this purpose and contain algorithms ranging from genetic programming to probabilistic models. In addition to addressing the need for accurate links, link discovery frameworks need to address the challenge of time efficiency. This challenge comes about because of the mere size of knowledge bases that need to be linked. In particular, large knowledge bases such as LinkedTCGA [18] contain more than 20 billion triples.\nOne of the approaches to improving the scalability of link discovery frameworks is to use planning algorithms in a manner akin (but not equivalent to) their use in databases [15]. In general, planners rely on cost functions to estimate the runtime of particular portions of link specifications. So far, it has been assumed that this cost function is linear in the parameters of the planning, i.e., in the size\n1 https://www.w3.org/DesignIssues/LinkedData.html\n2 of the datasets and the similarity threshold. However, this assumption has never been verified. In this paper, we address exactly this research gap and study how well other models for runtime approximation perform. In particular, we study linear, exponential and mixed models for runtime estimation. The contributions of this paper are thus as follows:\n\u2013 We present three different models for runtime approximation in planning for Link Discovery. \u2013 We compare these models on six different datasets and study how well they can approximate runtimes of specifications as well as with respect to how well they generalize across datasets. \u2013 We integrate the models with the Helios planner for Link Discovery as described in [15] and compare their performance using 400 specifications.\nThe rest of the paper is structured as follows: In Section 2, we present the concept and notations necessary to understand this work. The subsequent section, Section 3, presents the runtime approximation problem and how it can be addressed by different models. We then delve into a thorough evaluation of these models in Section 4 and compare the expected runtimes generated by the models at hand with the real runtimes of the Link Discovery framework. We also study the transferability of the results we achieve and their performance when planning whole link specifications. Finally, we recapitulate our results and conclude."}, {"heading": "2 Preliminaries", "text": "In this section, we present the necessary concepts and notations to understand the rest of the paper. We begin by giving a description of a knowledge base K and Link Discovery (LD), we continue by providing a formal definition of a link specification (LS) and its semantics and we finish our preliminary section with an explanatory presentation of a plan, its components and its relation to a LS.\nKnowledge Base. A knowledge base K is a set of triples (s, p, o) \u2208 (R \u222a B) \u00d7 P \u00d7 (R \u222a B \u222a L), where R is the set of all RDF resources, P \u2286 R is the set of all RDF properties, B is the set of all RDF blank nodes and L is the set of all literals.\nLink Discovery. Given two (not necessarily distinct) sets of RDF resources S and T and a relation R (e.g, directorOf, owl:sameAs), the main goal of LD is to discover the set (mapping) {(s, t) \u2208 S \u00d7 T : R(s, t)}. Given that this task can be very tedious (especially when S and T are large), LD frameworks are commonly used to achieve this computation.\nLink Specification. Declarative LD frameworks use link specifications (LSs) to describe the conditions for which R(s, t) holds for a pair (s, t) \u2208 S \u00d7 T . A LS consists of two basic components:\n3 \u2013 similarity measures which allow the comparison of property values of resources found in the input data sets S and T . We define an atomic similarity measure m \u2208 M as a function m : S\u00d7T\u00d7P2 \u2192 [0, 1]. We write m(s, t, ps, pt) to signify the similarity of s and t w.r.t. their properties ps resp. pt. \u2013 operators op \u2208 {\u2294,\u2293, \\} that allow the combination of two similarity measures.\nAn atomic LS consists of one similarity measure and has the form (m(ps, pt), \u03b8) where \u03b8 \u2208 [0, 1]. A complex LS L = op(L1, L2) consists of two LS, L1 and L2. We call L1 the left sub-specification and L2 the right sub-specification of L. We denote the semantics (i.e., the results of a LS for given sets of resources S and T ) of a LS L as [[L]] and call it a mapping. We begin by assuming the natural semantics of the combinations of measures. Filters are pairs (f, \u03c4), where (1) f is either empty (denoted \u01eb) or a combination of similarity measures by means of specification operators and (2) \u03c4 is a threshold. Note that an atomic specification can be regarded as a filter (f, \u03c4,X) with [[X ]] = S \u00d7 T . We will thus use the same graphical representation for filters and atomic specifications. We call (f, \u03c4) the filter of L and denote it with \u03d5(L). For our example L in Fig. 1, \u03d5(L) = (\u01eb, 0.7). We denote the operator of a LS L with op(L). For L = (f, \u03c4, \u03c9(L1, L2)), op(L) = \u03c9. The operator of the LS shown in our example is \u2294. The semantics of LSs are then as shown in Table 1.\nExecution Plan. To compute the mapping [[L]] (which corresponds to the output of L for a given pair (S, T )), LD frameworks implement (at least partly) a generic architecture consisting of a rewriter (optional), a planner (optional) and an execution engine (necessary). The rewriter performs algebraic operations to transform the input LS L into a LS L\u2032 (with [[L]] = [[L]]\u2032) that is potentially faster to execute. The most common planner is the canonical planner (dubbed Canonical), which simply traverses L in post-order and has its results computed in that order by the execution engine.2 For the LS shown in Fig. 1, the execution plan returned by Canonical would thus foresee to first compute the mapping M1 = [[(trigrams(:title, :title), 0.48)]] of pairs of resources whose property title has a cosine similarity greater or equal to 0.48. The computation of M2 = [[(levenSim(:label, :label), 0.46)]] would follow. Step 3 would be to compute M3 = M1 \u2293 M2 while abiding by the semantics described in Table 1. Step 4 would be to obtain M4 by filtering the results and keeping only the pairs that have a similarity above 0.5. Step 5 would be M5 = [[(cosine(:name, :name), 0.78)]] and Step 6 would be to compute M6 = M4 \u2294M5. Finally, Step 7 would be to filter out the pairs of links in M6 that have a similarity less than 0.8. Given that there is a 1-1 correspondence between LS and the plan generated by the canonical planner, we will reuse the representation of LS devised above for plans. The sequence of steps for such a plan is then to be understood as the sequence of steps that would be derived by Canonical for the LS displayed.\n2 Note that the planner and engine are not necessarily distinct in existing implementations.\n4"}, {"heading": "3 Runtime Estimation", "text": "In general, planners aims to estimate the cost of the leaves of a plan, i.e., the runtime of atomic link specifications. So far, linear models [15] have been used for this purpose but the appropriateness of other models has never been evaluated. Hence, in this work, we compare non-linear models with linear models to approximate the runtime of of atomic link specifications. Like in previous works, we follow a sampling-based approach. First, given a particular similarity measure m (e.g., Levenshtein) and an implementation of the said measure (e.g., Ed-Join [22]), we begin by collecting sample of runtimes for a given measure with varying values of |S|, |T | and \u03b8.3 These samples can be regarded as the output of a function that can predict the runtime of the implementation of m for which we were given samples. The major question that is to be answered is hence what is the shape of the runtime evaluation function?\nWe tried fitting functions of different shapes to the previously measured runtimes in order to compare their performance when planning the execution of link specifications. Formally, these functions are mappings \u03c6 : N \u00d7 N \u00d7 (0, 1] 7\u2192 R, whose value at (|S|, |T |, \u03b8) is an approximation of the runtime for the link specification with these parameters. If R = (R1, . . . , Rn) are the measured runtimes for\n3 We also experimented with the number of trigrams contained in S and T but found that they do not affect the models we considered. An exploration of other parameters remains future work.\n5 the parameters S = (|S1|, . . . , |Sn|), T = (|T1|, . . . , |Tn|) and \u03b8 = (\u03b81, . . . , \u03b8n), then we constrain the mapping \u03c6 to be a local minimum of the L2-Loss:\nE(S,T , \u03b8, r) := \u2016R\u2212 \u03c6(S,T , \u03b8)\u20162, (1)\nwriting \u03c6(S,T , \u03b8) = (\u03c6(|S1|, |T1|, \u03b81), . . . , \u03c6(|Sn|, |Tn|, \u03b8n)). Within this paper, we consider the following parametrized families of functions:\n\u03c61(S, T, \u03b8) = a+ b|S|+ c|T |+ d\u03b8 (2) \u03c62(S, T, \u03b8) = exp (a+ b|S|+ c|T |+ d\u03b8 + e\u03b8 2) (3) \u03c63(S, T, \u03b8) = a+ (b + c|S|+ d|T |+ e|S||T |) exp (f\u03b8 + g\u03b8 2) (4)\nThe parameters are then determined by\na\u2217, b\u2217, \u00b7 \u00b7 \u00b7 = argminE(S,T , \u03b8,R)(a, b, . . . ) (5)\nfor some local minimum. In the case of \u03c61 and \u03c62 this problem is linear in nature and we solved it using the pseudo-inverse of the associated Vandermonde matrix. For \u03c63 we used the Levenberg-Marquardt Algorithm [11] for nonlinear least squares problems, using 1 as initial guess for all parameters.\nWe chose \u03c61 as the baseline linear fit. \u03c62 is the standard log-linear fit, except for the \u03b82 term. We included this term during a grid search for polynomials to perform a log-polynomial fit. Higher orders of |S| or |T | or \u03b8 did not contribute to a better fit. \u03c63 can be interpreted as an interpolation of \u03c61 and \u03c62 with a constant offset a.\nTo exemplify our approach for \u03c62, assume we have measured S = (458, 458, 358, 58),T = (512, 404, 317, 512) and \u03b8 = (0.5, 0.9, 0.6, 0.7). Inserting into eq. (1) and taking the logarithm, one arrives at the optimization problem\nmin a,b,c,d,e \u2016\n\n  \n1 458 512 0.5 0.52 1 458 404 0.9 0.92 1 358 317 0.6 0.62 1 58 512 0.7 0.72\n\n  \n\n    \na b c d e\n\n     \u2212\n\n   log(67) log(4) log(4) log(1)\n\n   \u20162\nThe solution to this least squares problem also is the unique solution of its normal equations:\n\n     1 1 1 1 458 458 358 58 512 404 317 512 0.5 0.9 0.6 0.7 0.52 0.92 0.62 0.72\n\n    \n\n  \n1 458 512 0.5 0.52 1 458 404 0.9 0.92 1 358 317 0.6 0.62 1 58 512 0.7 0.72\n\n  \n\n    \na b c d e\n\n    \n=\n\n     1 1 1 1 458 458 358 58 512 404 317 512 0.5 0.9 0.6 0.7 0.52 0.92 0.62 0.72\n\n    \n\n   log(67) log(4) log(4) log(1)\n\n  \n6 By multiplying and inverting matrices, we arrive at the linear equation\n\n    \na b c d e\n\n     =\n\n  \n1 458 512 0.5 0.52 1 458 404 0.9 0.92 1 358 317 0.6 0.62 1 58 512 0.7 0.72\n\n  \n+ \n   log(67) log(4) log(4)\n0\n\n   ,\nwhere A+ denotes the Moore-Penrose pseudo inverse of A [5]. Multiplying the matrices, we arrive at\n\n    \na b c d e\n\n     =\n\n     \u22121.028 0.009 0.010 9.821 \u22129.053\n\n     .\nThus we have found the coefficients of the fit function."}, {"heading": "4 Evaluation", "text": ""}, {"heading": "4.1 Experimental Setup", "text": "We evaluated the three runtime estimation models using six data sets. The first three are the benchmark data sets for LD dubbed Amazon-Google Products, DBLP-ACM and DBLP-Scholar described in [10]. We also created two larger additional data sets (MOVIES and VILLAGES, see Table 2) from the data sets DBpedia, LinkedGeodata and LinkedMDB. 4 5 The sixth dataset was the set of all English labels from DBpedia 2014. Table 2 describes the characteristics of the datasets and presents the properties used when linking the retrieved resources for the first four datasets. The mapping properties were provided to the link discovery algorithms underlying our results.\nEach of our experiments consisted of two phases: During the training phase, we trained each of the models independently. For each model, we computed the set of coefficients for each of the approximation models that minimized the root mean squared error (RMSE) on the training data provided. The aim of the subsequent test phase was to evaluate the accuracy of the runtime estimation provided by each model and the performance of the currently best LD planner, Helios [15], when it relied of each of the three models for runtime approximations. Throughout our experiments, we used the algorithms Ed-Join [23] (which implements the Levenshtein string distance) and PPJoin+ [24] (which implements the Jaccard, Overlap, Cosine and Trigrams string similarity measures) to execute atomics specifications. As thresholds \u03b8 we used random values between 0.5 and 1.\n4 http://www.linkedmdb.org/ 5 The new data sets as well as a description of how they were constructed are available at http://titan.informatik.uni-leipzig.de/kgeorgala/DATA/.\n7 The aim of our evaluation was to answer the following set of questions regarding the performance of the three models exp, linear and mixed :\n\u2013 Q1: How do our models fit each class separately? To answer this question, we began by splitting the source and target data of each of our datasets into two non-overlapping parts of equal size. We used the first half of each source and each target for training and the second half for testing.\n\u2022 Training: We trained the three models on each dataset. For each model, dataset and mapper, we a) selected 15 source and 15 target random samples of random sizes from the first half of a dataset (Amazon-Google Products, DBLP-ACM, DBLP-Scholar, MOVIES and VILLAGES) and b) compared each source sample with each target sample 3 times. Note that we used the same samples across all models for the sake of fairness. Overall, we ran 675 training experiments to train each model on each dataset. \u2022 Testing: To test the accuracy of each model, we ran the corresponding algorithm (Ed-Join and PPJoin+) with a random threshold between 0.5 and 1 and recorded the real runtime of the approach and the runtimes predicted by our three models. Each approach was executed 100 times against the whole of the second half of the same dataset.\n\u2013 Q2: How do our models generalize across classes, i.e., can a model trained on data from one class be used to predict runtimes accurately on another class?\n\u2022 Training: We trained each model in the same manner as forQ1 on exactly the same five datasets with the sole difference that the samples were selected randomly from the whole dataset. \u2022 Testing: Like in the previous series of experiments, we ran Ed-Join and PPJoin+ with a random threshold between 0.5 and 1. Each of the algorithms was executed 100 times against the remaining four datasets.\n\u2013 Q3: How do our models perform when trained on a large dataset?\n\u2022 Training: We trained in the same fashion as to answer Q1 with the sole differences that (1) we used 15 source and 15 target random samples of various sizes between 10, 000 and 100, 000 from (2) the English labels of DBpedia to train our model. \u2022 Testing: We learned 100 LSs for the Amazon-GP, DBLP-ACM, MOVIES and VILLAGES datasets using the unsupervised version of the EAGLE algorithm [12]. We chose this algorithm because it was shown to generate meaningful specifications that return high-quality links in previous works. For each dataset, we ran the set of 100 specifications learned by EAGLE on the given dataset by using each of the models during the execution in combination with the HELIOS planning algorithm [15], which was shown to outperforms the canonical planner w.r.t. runtime while producing exactly the same results.\nThroughout our experiments, we configured Eagle by setting the number of generations and population size to 20, mutation and crossover rates were set\n8 to 0.6. All experiments for all implementations were carried out on the same 20- core Linux Server running OpenJDK 64-Bit Server 1.8.0 74 on Ubuntu 14.04.4 LTS on Intel(R) Xeon(R) CPU E5-2650 v3 processors clocked at 2.30GHz. Each train experiment and each test experiment for Q3 was repeated three times. As evaluation measure, we computed root mean square error (RMSE ) between the expected runtime and the average execution runtime required to run each LS. We report all three numbers for each model and dataset."}, {"heading": "4.2 Results", "text": "To address Q1, we evaluated the performance of our models when trained and tested on the same class. We present the results of this series of experiments in Table 3. For PPJoin+ (in particular the trigrams measure), the mixed model achieved the lowest error when tested upon Amazon-GP and DBLP-Scholar, whereas the linear model was able to approximate the expected runtime with higher accuracy on the MOVIES and VILLAGES datasets. On average, linear model was able to achieve a lower RMSE compared to the other two models. For the Ed-Join, the mixed model outperformed linear and exp in the majority of datasets (DBLP-Scholar, MOVIES and VILLAGES) and obtained the lowest RMSE on average. As we observe in Table 3, for both measures, the exp model retrieved the highest error on average and is thus the model less suitable for runtime approximations. Especially, for the Ed-Join, exp had the worst performance in four out of the five datasets and retrieved the highest RMSE among the different test datasets for VILLAGES. This clearly answers our first questions: the linear and mixed approximation models are able achieve the smallest error when trained on the class on which they are tested.\nTo continue with Q2, we conducted a set of experiments in order to observe how well each model could generalize among the different classes included in our evaluation data. Tables 5, 4, 6, 7 and 8 present the results of training on one dataset and testing the resulting models on the set of the remaining classes. The highest RMSE error was achieved when both measures were tested using the exp\n9\nmodel in all datasets but VILLAGES. However, Table 8 shows that the fitting error when trained on VILLAGES is relatively low among all three models. Additionally, we observe that the exp model\u2019s RMSE increased exponentially as the quantity of the training data decreased, which constitutes this model as inadequate and unreliable for runtime approximations. By observing Tables 5 and 6, we see that the RMSE of the exp model increased by 38 orders of magnitude for Ed-Join.\nFor both measures, the linear model outperformed the other two models on average when trained on the Amazon-GP, DBLP-ACM and DBLP-Scholar datasets and achieved the lowest RMSE when trained on MOVIES for Ed-Join compared to exp and mixed. Both linear and mixed achieved minuscule approximation errors compared to exp, but linear was able to produce at least 35% less RMSE compared to mixed. Therefore, we can answer Q2 by stating that the linear model is the most suitable and sufficient model that can generalize among different classes.\nFor our last question, we tested the performance of the different models when trained on a bigger and more diverse dataset. Table 9 shows the results of our evaluation, where each model was trained on DBpedia english labels and tested on the the four evaluation datasets. The linear model error was 1 order of magnitude less than the RMSE obtained by exp and 3 orders of magnitude less compared to the mixed error. In all four datasets, the mixed model produced the highest RMSE. For the VILLAGES dataset, the mixed model\u2019s error was 1916 and 214 times higher compared to linear and exp resp. Figs. 2 and 3 present the plans produces by Helios for the LS MINUS(AND(levenshtein(x.description,y.description)|0.5045,trigrams( x.title, y.name)|0.4871)|0.2925,OR(levenshtein(x.description,y.descri ption)|0.5045,trigrams(x.title, y.name)|0.4871)| 0.2925)>=0.2925 of the Amazon-GP dataset, if the planner used the exp model and the linear or the mixed model resp. For the child LS AND(levenshtein(x.description,y.descri\n10\nption)|0.5045,trigrams(x.title, y.name)|0.4871)|0.2925, the linear and the mixed model chose to execute only trigrams(x.title, y.name)|0.4871) and use the other child as a filter. Moreover, the plan retrieved by using the exp model for runtime approximations aims to execute both children LSs, which results into an overhead in the execution of the LS. It is obvious that the linear model achieved by far the lowest RMSE on average compared to the other two models, which concludes the answer to Q3."}, {"heading": "5 Related Work", "text": "The task of efficient query execution in database systems is similar to the task of execution optimization using runtime approximations in LD frameworks. Efficient and scalable data management has been of central importance in database\n11\nsystems [6]. Over the past few years, there has been an extensive work on query optimization in databases that is based on statistical information about relations and intermediate results [19]. The author of [3] gives an analytic overview regarding the procedure of query optimization and the different approaches used at each step of the process.\nA novel approach in this field was presented by [7], in which the proposed approach introduced the concept of parametric query optimization. In this work, the authors provided the necessary formalization of the aforementioned concept and conducted a set of experiments using the buffer size as parameter. In order to minimize the total cost of generating all possible alternative execution plans, they used a set of randomized algorithms. On a similar manner, the authors of [20] introduced the idea of Multi-Objective Parametric query optimization (MPQ), where the cost of plan is associated with multiple cost functions and\n12\neach cost function is associated with various parameters. Their experimental results showed however that the MPQ method performs an exhaustive search of the solution space which addresses this approach computationally inefficient.\nAnother set of approaches in the field of query optimization have focused on creating dynamic execution plans. Dynamic planning is based on the idea that the execution engine of a framework knows more than the planner itself. Therefore, information generated by the execution engine is used to re-evaluate the plans generated by the optimizer. There has been a vast amount of approaches towards dynamic query optimization such as query scrambling for initial delays [21], dynamic planning in compile-time [4], adaptive query operators [9] and re-ordering of operators [2].\nMoreover, the problem addressed in this work focus on identifying scalable and time-efficient solutions towards LD. A large number of frameworks were developed to assist this issue, such as SILK [8], Limes [14], KnoFuss [16] and Zhishi.links [17]. SILK and KnoFuss implement blocking approaches in order to achieve efficient linking between resources. SILK framework incorporates a\n13\nrough index pre-match, whereas KnoFuss blocking technique is highly influenced from databases systems techniques. To this end, the only LD framework that provides both theoretical and practical guarantees towards scalable and accurate LD is Limes. As we mentioned throughout this work,Limes execution strategy incorporates the Helios planner [15] which is (to the best of our knowledge) the first execution optimizer in LD. Helios is able to provide accurate runtime approximations, which we have extended in this work, and is able to find the least costly execution plan for a LS, consuming a minute portion of the overall execution runtime."}, {"heading": "6 Conclusion", "text": "In this paper, we studied approximation functions that allow predicting the runtime of link specifications. We showed that on average, linear models are indeed the approach to chose to this end as they seem to overfit the least. Still,\n14\nmixed models also perform in a satisfactory manner. Exponential models either fit very well or not at all and are thus not to be used. In future work, we will study further models for the evaluation of runtime and improve upon existing planning mechanisms for the declarative LD. In particular, we will consider other features when approximation runtimes, e.g., the distribution of characters in the strings to compare."}], "references": [{"title": "Introduction to linked data and its lifecycle on the web", "author": ["S. Auer", "J. Lehmann", "A.-C. Ngonga Ngomo", "A. Zaveri"], "venue": "In Reasoning Web. Semantic Technologies for Intelligent Data Access,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Eddies: Continuously adaptive query processing", "author": ["R. Avnur", "J.M. Hellerstein"], "venue": "In Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "An overview of query optimization in relational systems", "author": ["S. Chaudhuri"], "venue": "In Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Optimization of dynamic query evaluation plans", "author": ["R.L. Cole", "G. Graefe"], "venue": "In Proceedings of the 1994 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Fast computation of moore-penrose inverse matrices", "author": ["P. Courrieu"], "venue": "arXiv preprint arXiv:0804.4809,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Query evaluation techniques for large databases", "author": ["G. Graefe"], "venue": "ACM Comput. Surv.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "Parametric query optimization", "author": ["Y.E. Ioannidis", "R.T. Ng", "K. Shim", "T.K. Sellis"], "venue": "In Proceedings of the 18th International Conference on Very Large Data Bases,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Efficient Multidimensional Blocking for Link Discovery without losing Recall", "author": ["R. Isele", "A. Jentzsch", "C. Bizer"], "venue": "In WebDB,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "An adaptive query execution system for data integration", "author": ["Z.G. Ives", "D. Florescu", "M. Friedman", "A. Levy", "D.S. Weld"], "venue": "In Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Evaluation of entity resolution approaches on real-world match problems", "author": ["H. K\u00f6pcke", "A. Thor", "E. Rahm"], "venue": "PVLDB, 3(1):484\u2013493,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "The levenberg-marquardt algorithm: implementation and theory", "author": ["J.J. Mor\u00e9"], "venue": "In Numerical analysis,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1978}, {"title": "Eagle: Efficient active learning of link specifications using genetic programming", "author": ["A.-C.N. Ngomo", "K. Lyko"], "venue": "In The Semantic Web: Research and Applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "On link discovery using a hybrid approach", "author": ["A.-C. Ngonga Ngomo"], "venue": "Journal on Data Semantics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "On link discovery using a hybrid approach", "author": ["A.-C. Ngonga Ngomo"], "venue": "Journal on Data Semantics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "HELIOS - Execution Optimization for Link Discovery", "author": ["A.-C. Ngonga Ngomo"], "venue": "In The Semantic Web - ISWC 2014 - 13th International Semantic Web Conference, Riva del Garda, Italy, October", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Unsupervised learning of link discovery configuration", "author": ["A. Nikolov", "M. d\u2019Aquin", "E. Motta"], "venue": "In 9th Extended Semantic Web Conference (ESWC", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Zhishi.links results for oaei", "author": ["X. Niu", "S. Rong", "Y. Zhang", "H. Wang"], "venue": "In OM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Big linked cancer data: Integrating linked tcga and pubmed", "author": ["M. Saleem", "M.R. Kamdar", "A. Iqbal", "S. Sampath", "H.F. Deus", "A.-C.N. Ngomo"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Database Systems Concepts", "author": ["A. Silberschatz", "H. Korth", "S. Sudarshan"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Multi-objective parametric query optimization", "author": ["I. Trummer", "C. Koch"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Cost-based query scrambling for initial delays", "author": ["T. Urhan", "M.J. Franklin", "L. Amsaleg"], "venue": "In Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Ed-join: an efficient algorithm for similarity joins with edit distance constraints", "author": ["C. Xiao", "W. Wang", "X. Lin"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Ed-join: An efficient algorithm for similarity joins with edit distance constraints", "author": ["C. Xiao", "W. Wang", "X. Lin"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Efficient similarity joins for near duplicate detection", "author": ["C. Xiao", "W. Wang", "X. Lin", "J.X. Yu"], "venue": "In Proceedings of the 17th International Conference on World Wide Web, WWW", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Link discovery frameworks are of utmost importance during the creation of Linked Data [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "Two main challenges need to be addressed by Link Discovery frameworks [13,15].", "startOffset": 70, "endOffset": 77}, {"referenceID": 14, "context": "Two main challenges need to be addressed by Link Discovery frameworks [13,15].", "startOffset": 70, "endOffset": 77}, {"referenceID": 17, "context": "In particular, large knowledge bases such as LinkedTCGA [18] contain more than 20 billion triples.", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "One of the approaches to improving the scalability of link discovery frameworks is to use planning algorithms in a manner akin (but not equivalent to) their use in databases [15].", "startOffset": 174, "endOffset": 178}, {"referenceID": 14, "context": "\u2013 We integrate the models with the Helios planner for Link Discovery as described in [15] and compare their performance using 400 specifications.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "We define an atomic similarity measure m \u2208 M as a function m : S\u00d7T\u00d7P \u2192 [0, 1].", "startOffset": 71, "endOffset": 77}, {"referenceID": 0, "context": "An atomic LS consists of one similarity measure and has the form (m(ps, pt), \u03b8) where \u03b8 \u2208 [0, 1].", "startOffset": 90, "endOffset": 96}, {"referenceID": 14, "context": "So far, linear models [15] have been used for this purpose but the appropriateness of other models has never been evaluated.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": ", Ed-Join [22]), we begin by collecting sample of runtimes for a given measure with varying values of |S|, |T | and \u03b8.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "For \u03c63 we used the Levenberg-Marquardt Algorithm [11] for nonlinear least squares problems, using 1 as initial guess for all parameters.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "where A denotes the Moore-Penrose pseudo inverse of A [5].", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "The first three are the benchmark data sets for LD dubbed Amazon-Google Products, DBLP-ACM and DBLP-Scholar described in [10].", "startOffset": 121, "endOffset": 125}, {"referenceID": 14, "context": "The aim of the subsequent test phase was to evaluate the accuracy of the runtime estimation provided by each model and the performance of the currently best LD planner, Helios [15], when it relied of each of the three models for runtime approximations.", "startOffset": 176, "endOffset": 180}, {"referenceID": 22, "context": "Throughout our experiments, we used the algorithms Ed-Join [23] (which implements the Levenshtein string distance) and PPJoin+ [24] (which implements the Jaccard, Overlap, Cosine and Trigrams string similarity measures) to execute atomics specifications.", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "Throughout our experiments, we used the algorithms Ed-Join [23] (which implements the Levenshtein string distance) and PPJoin+ [24] (which implements the Jaccard, Overlap, Cosine and Trigrams string similarity measures) to execute atomics specifications.", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "\u2022 Testing: We learned 100 LSs for the Amazon-GP, DBLP-ACM, MOVIES and VILLAGES datasets using the unsupervised version of the EAGLE algorithm [12].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "For each dataset, we ran the set of 100 specifications learned by EAGLE on the given dataset by using each of the models during the execution in combination with the HELIOS planning algorithm [15], which was shown to outperforms the canonical planner w.", "startOffset": 192, "endOffset": 196}, {"referenceID": 5, "context": "systems [6].", "startOffset": 8, "endOffset": 11}, {"referenceID": 18, "context": "Over the past few years, there has been an extensive work on query optimization in databases that is based on statistical information about relations and intermediate results [19].", "startOffset": 175, "endOffset": 179}, {"referenceID": 2, "context": "The author of [3] gives an analytic overview regarding the procedure of query optimization and the different approaches used at each step of the process.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "A novel approach in this field was presented by [7], in which the proposed approach introduced the concept of parametric query optimization.", "startOffset": 48, "endOffset": 51}, {"referenceID": 19, "context": "On a similar manner, the authors of [20] introduced the idea of Multi-Objective Parametric query optimization (MPQ), where the cost of plan is associated with multiple cost functions and", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "There has been a vast amount of approaches towards dynamic query optimization such as query scrambling for initial delays [21], dynamic planning in compile-time [4], adaptive query operators [9] and re-ordering of operators [2].", "startOffset": 122, "endOffset": 126}, {"referenceID": 3, "context": "There has been a vast amount of approaches towards dynamic query optimization such as query scrambling for initial delays [21], dynamic planning in compile-time [4], adaptive query operators [9] and re-ordering of operators [2].", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "There has been a vast amount of approaches towards dynamic query optimization such as query scrambling for initial delays [21], dynamic planning in compile-time [4], adaptive query operators [9] and re-ordering of operators [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 1, "context": "There has been a vast amount of approaches towards dynamic query optimization such as query scrambling for initial delays [21], dynamic planning in compile-time [4], adaptive query operators [9] and re-ordering of operators [2].", "startOffset": 224, "endOffset": 227}, {"referenceID": 7, "context": "A large number of frameworks were developed to assist this issue, such as SILK [8], Limes [14], KnoFuss [16] and Zhishi.", "startOffset": 79, "endOffset": 82}, {"referenceID": 13, "context": "A large number of frameworks were developed to assist this issue, such as SILK [8], Limes [14], KnoFuss [16] and Zhishi.", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "A large number of frameworks were developed to assist this issue, such as SILK [8], Limes [14], KnoFuss [16] and Zhishi.", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "links [17].", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "As we mentioned throughout this work,Limes execution strategy incorporates the Helios planner [15] which is (to the best of our knowledge) the first execution optimizer in LD.", "startOffset": 94, "endOffset": 98}], "year": 2016, "abstractText": "Time-efficient link discovery is of central importance to implement the vision of the Semantic Web. Some of the most rapid Link Discovery approaches rely internally on planning to execute link specifications. In newer works, linear models have been used to estimate the runtime the fastest planners. However, no other category of models has been studied for this purpose so far. In this paper, we study non-linear runtime estimation functions for runtime estimation. In particular, we study exponential and mixed models for the estimation of the runtimes of planners. To this end, we evaluate three different models for runtime on six datasets using 400 link specifications. We show that exponential and mixed models achieve better fits when trained but are only to be preferred in some cases. Our evaluation also shows that the use of better runtime approximation models has a positive impact on the overall execution of link specifications.", "creator": "LaTeX with hyperref package"}}}