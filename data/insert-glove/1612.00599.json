{"id": "1612.00599", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Communication Lower Bounds for Distributed Convex Optimization: Partition Data on Features", "abstract": "bamut Recently, there has 96-page been leonen an cedaw increasing interest hauler in stylites designing distributed biswal convex eyaktek optimization algorithms under the setting where 301-seat the data pays-de-la-loire matrix is partitioned on features. forechecking Algorithms associ\u00e9s under ac30 this setting sometimes rodion have hegel many advantages over voalavo those under f-14b the setting nagor where superscripts data is partitioned syndactyly on vishwanath samples, 1977-1988 especially when kacang the arpaia number extra-base of 1985-87 features autocrats is huge. 207.9 Therefore, it miscalculations is pred important to anaia understand the inherent limitations of 52-member these 86.68 optimization atmel problems. miuccia In wistert this paper, a39 with certain restrictions napredak on the jarmila communication mulatta allowed jaeckel in madobe the slavko procedures, we 0710 develop tight mamer lower bounds on communication 53.51 rounds encke for 29b a broad class bioaccumulate of non - pierce-arrow incremental algorithms under 69-73 this 56.54 setting. hatam We also provide ghah a lower blokker bound 50-nation on communication al-gama rounds for mgd a gilan class buddhi of (cincinnati/northern randomized) nordre incremental algorithms.", "histories": [["v1", "Fri, 2 Dec 2016 09:01:57 GMT  (13kb)", "http://arxiv.org/abs/1612.00599v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["zihao chen", "luo luo", "zhihua zhang"], "accepted": true, "id": "1612.00599"}, "pdf": {"name": "1612.00599.pdf", "metadata": {"source": "CRF", "title": "Communication Lower Bounds for Distributed Convex Optimization: Partition Data on Features", "authors": ["Zihao Chen"], "emails": ["z.h.chen@sjtu.edu.cn", "ricky@sjtu.edu.cn", "zhzhang@math.pku.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 2.\n00 59\n9v 1\n[ cs\n.L G"}, {"heading": "1 Introduction", "text": "In this paper, we consider the following distributed convex optimization problem over m machines:\nmin w\u2208Rd f(w; \u03b8).\nEach machine knows the form of f but only has some partial information of \u03b8. In particular, we mainly consider the case of empirical risk minimization (ERM) problems. Let A \u2208 Rn\u00d7d be a matrix containing n data samples with d features and Aj: be the j-th row of the data matrix (corresponding to the j-th data sample). Then f has the form:\nf(w) = 1\nn\nn \u2211\nj=1\n\u03c6(w,Aj:), (1)\nwhere \u03c6 is some kind of convex loss. In the past few years, many distributed optimizations algorithms have been proposed. Many of them are under the setting where data is partitioned on samples, i.e. each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10]. Meanwhile, as the dimension d can be enormously large, there has been\nan increasing interest in designing algorithms with the setting where the data is partitioned on features, i.e., each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7]. Compared with algorithms under the sample partition setting, these algorithms have relatively less communication cost when d is large. In addition, as there is often no master machine in these algorithms, they tend to have more balanced workload on each machine, which is also an important factor affecting the performance of a distributed algorithm.\nAs communication is usually the bottleneck of distributed optimization algorithms, it is important to understand the fundamental limits of distributed optimization algorithms, i.e., how much communication an algorithm must need to reach an \u01eb-approximation of the minimum value. Studying fundamental communication complexity of the distributed computing without any assumption is quite hard, letting alone continuous optimization. As for optimization, one alternative way is to derive lower bounds on communication rounds. The number of communication rounds is also an important metric as in many algorithms, faster machines often need to pause and wait for slower ones before they communicate, which can be a huge waste of time. Recently, putting some restrictions on communication allowed in each iteration, Arjevani and Shamir (2015) developed lower bounds on communication rounds for a class of distributed optimization algorithms under the sample partition setting, and these lower bounds can be matched by some existing algorithms. However, optimization problems under the feature partition setting are still not well understood.\nConsidering the increasing interest and importance of designing distributed optimization algorithms under the feature partition setting, in our paper, we develop tight lower bounds on communication rounds for a broad class of distributed optimization algorithms under this setting. Our results can provide deeper understanding and insights for designing optimization algorithms under this setting. To define the class of algorithms, we put some constraints on the form and amount of communication in each round, while keeping restrictions mild and applying to many distributed algorithms. We summarize our contributions as follows:\n\u2022 For the class of smooth and \u03bb-strongly convex functions with condition number \u03ba, we develop a tight lower bound of \u2126 (\u221a \u03ba log(\u03bb\u2016w\n\u2217\u2212w0\u2016 \u01eb ) ) , which can be matched by a straightforward distributed\nversion of accelerated gradient decent [13] and also DISCO-F for quadratics [9], an variant of DISCO [16] under the feature partition setting.\n\u2022 For the class of smooth and (non-strongly) convex functions with Lipschitz smooth parameter L, we develop a tight lower bound of \u2126 ( \u221a\nL \u01eb \u2016w\u2217 \u2212 w0\u2016\n)\n, which is also matched by the distributed\naccelerated gradient decent.\n\u2022 By slightly modifying the definitions of the algorithms, we define a class of incremental/stochastic algorithms under the feature partition setting and develop a lower bound of \u2126 ( ( \u221a n\u03ba+ n) log(\u2016w\n\u2217\u2212w0\u2016\u03bb \u01eb ) )\nfor \u03bb-strongly convex functions with condition number \u03ba.\nRelated Work The most revelant work should be [1], which studied lower bounds under the sample partition setting, and provided tight lower bounds on communication rounds for convex smooth optimization after putting some mild restrictions. More recently, Lee et al. (2015) provided a lower bound for another class of algorithms under that setting. Both these work as well as ours are based on some techniques used in non-distributed optimization lower bound analysis [13, 6]."}, {"heading": "2 Notations and Preliminaries", "text": "We use \u2016 \u00b7 \u2016 as Euclidean norm throughout the paper. For a vector w \u2208 Rd, we denote w(i) as the i-th coordinate of the vector w \u2208 Rd. We let the coordinate index set [d] be partitioned into m disjoint sets\nS1,S2, . . . ,Sm with \u2211m i=1 di = d and Sj = {k \u2208 [d] \u2223 \u2223 \u2211 i<j di < k \u2264 \u2211 i\u2264j di} for j = 1, 2, . . . ,m. For a vector w \u2208 Rd, denote w[j] as a vector in Rdj which equals to the segment of w on coordinates Sj . For a set of vectors V \u2286 Rd, we define V [j] = {v[j] \u2223\n\u2223v \u2208 V} \u2286 Rdj . Then we define f \u2032j(x) := \u2202f(w)\n\u2202w[j]\n\u2223 \u2223 \u2223\nw=x and\nf \u2032\u2032ij(x) := \u22022f(w)\n\u2202w[i]\u2202w[j]\n\u2223 \u2223 \u2223\nw=x .\nThrough the whole paper, we use partition-on-sample and partition-on-feature to describe distributed algorithms or communication lower bounds under the settings where data is partitioned on samples and features respectively.\nThen we list several preliminaries:\n1. Lipschitz continuity. A function h is called Lipschitz continuous with constant L if\n\u2200x, y \u2208 dom h \u2016h(x) \u2212 h(y)\u2016 \u2264 L\u2016x\u2212 y\u2016.\n2. Lipschitz smooth and strongly convex. A function h is called L-smooth and \u03bb-strongly convex if\n\u03bb 2 \u2016x\u2212 y\u20162 \u2264 h(y) \u2212 h(x) \u2212 (x\u2212 y)T\u2207h(y) \u2264 L 2 \u2016x\u2212 y\u20162\n3. Communication operations. Here we list some common MapReduce types of communication operations [4] in an abstract level:\n(a) One-to-all broadcast. One-to-all broadcast is an operation that one machine sends identical data to all other machines.\n(b) All-to-all broadcast. All-to-all broadcast is an operation that each machines performs a one-toall broadcast simultaneously.\n(c) Reduce. Consider the setting where each processor has p units of data, Reduce is an operation that combines the data items piece-wise (using some associative operator, such as addition or min), and make the result available at a target machine. For example, if each machine owns an R d vector, then computing the average of the vectors needs a Reduce operation of an Rd vector.\n(d) ReduceAll. A ReduceAll operation can be viewed as a combination of a Reduce operation and a one-to-all broadcast operation."}, {"heading": "3 Definitions and Framework", "text": "In this section we first describe a family of distributed optimization algorithms using m machines, and then modify it to get a family of incremental algorithms. At the beginning, the feature coordinates are partitioned into m sets and each machine owns the data columns corresponding to its coordinate set. We model the algorithms as iterative processes in multiple rounds and each round consists of a computation phase followed by a communication phase. For each machine we define a feasible set and during the computation phase, each machine can do some \u201ccheap\" communication and add some vectors to it. During the communication phase each machine can broadcast some limited number of points to all other machines. We also assume the communication operations are the common operations like broadcast, Reduce and ReduceAll."}, {"heading": "3.1 Non-incremental Algorithm Family", "text": "Here we define the non-incremental algorithm class in a formal way:\nDefinition 1 (partition-on-feature distributed optimization algorithm family F\u03bb,L ). We say an algorithm A for solving (1) with m machines belongs to the family F\u03bb,L of distributed optimization algorithms for minimizing L-smooth and \u03bb-strongly convex functions (\u03bb = 0 for non-strongly-convex functions) with the form (1) if the data is partitioned as follows:\n\u2022 Let the coordinate index set [d] be partitioned into m disjoint sets S1,S2, . . . ,Sm with \u2211m\ni=1 di = d and Sj = {k \u2208 [d] \u2223 \u2223 \u2211 j<i di < k \u2264 \u2211\nj\u2264i di} for j = 1, 2, . . . ,m. The data matrix A \u2208 Rn\u00d7d is partitioned column-wise as A = [A1, . . . , Am], where Aj consists of columns i \u2208 Sj . Each machine j stores Aj .\nand the machines do the following operations in each round:\n1. For each machine j, define a feasible set of vectors Wj \u2286 Rdj initialized to be W(0)j = {0}. Denote W(k)j as machine j\u2019s feasible set Wj in the k-th round.\n2. Assumption on feasible sets. In the k-th round, initially W(k)j = W (k\u22121) j . Then for a constant number\nof times, each machine j can add any wj to W(k)j if wj satisfies\nwj \u2208 span { uj , f \u2032 j(u), (f \u2032\u2032 jj(u) +D)vj , f \u2032\u2032 ji(u)vi\n\u2223 \u2223 \u2223 uT = [u1 T , . . . , um T ],\nuj \u2208 W(k)j , vj \u2208 W (k) j , ui \u2208 W (k\u22121) i , vi \u2208 W (k\u22121) i , i 6= j, D is diagonal\n}\n. (2)\n3. Computation phase. Machines do local computations and can perform no more than some constant times of Reduce/ReduceAll operations of an Rn vector or constant during the computation phase.\n4. Communication phase. At the end of each round, each machine j can simultaneously broadcast no more than constant number of Rdj vectors.\n5. The final output after R rounds is wT = [wT1 , . . . , w T m] satisfies wj \u2208 W (R) j .\nWe have several remarks on the defined class of algorithms:\n\u2022 As described above, during the iterations to find w\u2217, machine j can only do updates on coordinates Sj . This restriction is natural because machine j does not have much information about function f on other coordinates. Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].\n\u2022 Similar to [1], we use Wj to define the restriction on the updates. This is not an explicit part of algorithms and machines do not necessarily need to store it, nor do they need to evaluate the points every time they add to the feasible sets. Although in most algorithms belonging to this family, each machine broadcasts the points it has added to the feasible set and store what other machines have broadcast in the communication phase (a simple example is the straightforward distributed implementation of gradient decent), we choose not to define feasible sets as physical sets that machines need to store to keep our results general.\n\u2022 The assumption on the updates is mild and it applies to many partition-on-feature algorithms. It allows machines to perform preconditioning using local second order information or use partial gradient to update. It also allows to compute and utilize global second order information, since the span we define includes the f \u2032\u2032ji(u)vi term. Besides, we emphasize that putting such structural assumptions is necessary. Even if we could develop some assumption-free communication lower bounds, they might be too weak and have a large gap with upper bounds provided by existing algorithms, thus becoming less meaningful and cannot provide deeper understanding or insights for designing algorithms.\n\u2022 During the computation phase, we assume each local machines can perform unbounded amount of computation and only limited amount of communication. Note that this part of communication is a must in many partition-on-feature algorithms, usually due to the need of computing partial gradients f \u2032j(w). However, for some common loss functions \u03c6, such as (regularized) squared loss, logistic loss and squared hinge loss, computing f \u2032j(w) for all j \u2208 [m] in total only needs a ReduceAll operation of an Rn vector [14]. In some gradient (or partial gradient) based algorithms [14, 11], communication to compute partial gradients are the only need of communication in the computation phase. Besides, some algorithms like DISCO-F [9] need to compute (\u2207f(w)u)[j]. For loss functions like squared loss, logistic loss and squared hinge loss, it only requires the same amount of communication as computing partial gradients, i.e. a ReduceAll operation of an Rn vector [9].\n\u2022 Here we summarize the total communication allowed in each round. We use O\u0303 to denote asymptotic bounds hiding constants and factors logarithmic in the required accuracy of the solution. During the computation phase, each machine can do constant times of ReduceAll operations of O\u0303(n) bits. During the communication phase, each machine j can broadcast O\u0303(dj) bits, this can be viewed as performing constant times of ReduceAll operation of an Rd vector. Therefore, the total communication allowed in each round is no more than ReduceAll operations of O\u0303(n+ d) bits. The amount of communication allowed in our partition-on-feature algorithm class is relatively small, compared with the partitionon-sample algorithm class described in [1], which allows O\u0303(md) bits of one-to-all broadcast in each round. This is due to the partition-on-feature algorithms\u2019 advantage on communication cost.\n\u2022 We also emphasize that the communication allowed in the entire round is moderate. On one hand, if we only allow too little communication then the information exchange between machines is not enough to perform efficient optimization. As a result, hardly no practical distributed algorithm could satisfy the requirement, which will diminish the generality of our results. On the other hand, if we allow too much communication in each round, our assumption on the feasible sets can be too strong. For example if we allow enough communication for all machines to broadcast their entire local data, then the machines only need one communication round to find out a solution up to any accuracy \u01eb.\n\u2022 Our assumption that Wj is initialized as {0} is merely for convenience and we just need to shift the function via f\u0304(w) = f(w + w0) for another starting point w0."}, {"heading": "3.2 Incremental Algorithm Family", "text": "To define the class of incremental/stochastic algorithms I\u03bb,L under the feature partition setting, we slightly modify the definition of F\u03bb,L by replacing assumption on feasible set (2) with the following while keeping the rest unchanged:\nAssumption on feasible sets for I\u03bb,L. In the k-th round, initially W(k)j = W (k\u22121) j . Next for a constant number of times, each machine j chooses g(w) := \u03c6(w,Al:) for some (possibly random) l and adds any wj\nto W(k)j if wj satisfies\nwj \u2208 span { uj, g \u2032 j(u), (g \u2032\u2032 jj(u) +D)vj, g \u2032\u2032 ji(u)vi\n\u2223 \u2223 \u2223 uT = [u1 T , . . . , um T ],\nuj \u2208 W(k)j , vj \u2208 W (k) j , ui \u2208 W (k\u22121) i , vi \u2208 W (k\u22121) i , i 6= j, D is diagonal\n}\n. (3)"}, {"heading": "4 Main Results", "text": "In this section, we present our main theorems followed by some discussions on their implications. First, we present a lower bound on communication rounds for algorithms in F\u03bb,L:\nTheorem 2. For any number m of machines, any constants \u03bb,L, \u01eb > 0, and any distributed optimization algorithm A \u2208 F\u03bb,L, there exists a \u03bb-strongly convex and L-smooth function f(w) with condition number \u03ba := L\n\u03bb over Rd such that if w\u2217 = argminw\u2208Rd f(w), then the number of communication rounds to obtain\nw\u0302 satisfying f(w\u0302)\u2212 f(w\u2217) \u2264 \u01eb is at least\n\u2126 (\u221a \u03ba log (\u2016w\u2217\u2016\u03bb \u01eb ))\n(4)\nfor sufficiently large d.\nSimilarly, we have a lower bound for algorithms in F0,L for minimizing smooth convex functions:\nTheorem 3. For any number m of machines, any constants L > 0, \u01eb > 0, and any distributed optimization algorithm A \u2208 F0,L, there exists a L-smooth convex function f(w) over Rd such that if w\u2217 = argminw\u2208Rd f(w), then the number of communication rounds to obtain w\u0302 satisfying f(w\u0302) \u2212 f(w\u2217) \u2264 \u01eb is at least\n\u2126\n(\n\u221a\nL \u01eb \u2016w\u2217\u2016\n)\n(5)\nfor sufficiently large d.\nThen we contrast our lower bound for smooth strongly convex functions with some existing algorithms and the upper bounds provided by their convergence rate. The comparisons indicate that our lower bounds are tight. For some common loss functions, this can be matched by a straightforward distributed implementation of accelerated gradient decent [13] and it is easy to verify that it satisfies our definition: let all machines compute their own partial gradients and aggregate to form a gradient. This straightforward distributed version of accelerated gradient decent achieves a round complexity of O (\u221a \u03ba log(\u2016w \u2217\u2212w0\u2016\u03bb\n\u01eb ) ) , which matches\nour lower bound exactly. Similarly, our lower bound on smooth (non-strongly) convex functions can also be matched by the distributed accelerated gradient decent.\nRecall that our definition of the algorithm class includes some types of distributed second order algorithms, for example DISCO-F [9]. The number of communication rounds DISCO-F needs to minimize general quadratic functions is O (\u221a\n\u03ba log(\u2016w \u2217\u2212w0\u2016 \u01eb ) ) , which also matches our bound with respect to \u03ba and\n\u01eb. This indicates that distributed second order algorithms may not achieve a faster convergence rate than first order ones if only linear communication is allowed.\nNext we present a lower bound on communication rounds for algorithms in I\u03bb,L:\nTheorem 4. For any number m of machines, any constants \u03bb,L, \u01eb > 0, and any distributed optimization algorithm A \u2208 I\u03bb,L, there exists a \u03bb-strongly convex and L-smooth function f(w) with condition number \u03ba := L\n\u03bb over Rd such that if w\u2217 = argminw\u2208Rd f(w), then the number of communication rounds to obtain\nw\u0302 satisfying E [f(w\u0302)\u2212 f(w\u2217)] \u2264 \u01eb is at least\n\u2126\n( (\u221a n\u03ba+ n ) log (\u2016w\u2217\u2016\u03bb \u01eb ))\n(6)\nfor sufficiently large d."}, {"heading": "5 Proof of Main Results", "text": "In this section, we provide proofs of Theorem 2 and Theorem 4. The proof framework of these theorems are based on [13]. As Theorem 3 can be obtained by replacing [13, Lemma 2.1.3] with Corollary 6 (see below) in the proof of [13, Theorem 2.1.6], we will not discuss it here for simplicity."}, {"heading": "5.1 Proof of Theorem 2", "text": "The idea is to construct a \u201chard\" function so that all algorithms in the class we defined could not optimize well in a small number of rounds: in each round only one of the machines can do a constant steps of \u201cprogress\" while other machines stay \u201ctrapped\" (see Lemma 5).\nWithout loss of generality, we assume that in every round, each machine only add one vector to its feasible set and the bound does not change asymptotically.\nFirst, we construct the following function like [6]\nf(w) = \u03bb(\u03ba\u2212 1)\n4\n[1\n2 wTAw \u2212 \u3008e1, w\u3009\n] + \u03bb\n2 \u2016w\u20162, (7)\nwhere A is a tridiagonal matrix in Rd\u00d7d with the form\nA =\n\n        2 \u22121 0 \u00b7 \u00b7 \u00b7 0 0 0 \u22121 2 \u22121 \u00b7 \u00b7 \u00b7 0 0 0 0 \u22121 2 \u00b7 \u00b7 \u00b7 0 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 0 \u00b7 \u00b7 \u00b7 \u22121 2 \u22121 0 0 0 \u00b7 \u00b7 \u00b7 0 \u22121\n\u221a \u03ba+3\u221a \u03ba+1\n\n        .\nIt is easy to verify that f(w) is \u03bb-strongly convex with condition number \u03ba. After K rounds of iteration, let Et,d := {x \u2208 Rd \u2223\n\u2223x(i) = 0, t + 1 \u2264 i \u2264 d} and W(K) := {[wT1 , . . . , wTm] T \u2223 \u2223wj \u2208 W(K)j , j = 1, . . . ,m}.\nThen we have the following lemma:\nLemma 5. If W(K) \u2286 EK,d for some K \u2264 d\u2212 1, then we have W(K+1) \u2286 EK+1,d. Proof. First we recall the assumption on Wj\u2019s in (2):\nwj \u2208 span { uj, f \u2032 j(u), (f \u2032\u2032 jj(u) +D)vj , f \u2032\u2032 ji(u)vi\n\u2223 \u2223 \u2223 uT = [u1 T , . . . , um T ],\nuj \u2208 W(k)j , vj \u2208 W (k) j , ui \u2208 W (k\u22121) i , vi \u2208 W (k\u22121) i , i 6= j, D is diagonal\n}\n.\nWe just need to prove that for any vector u, v \u2208 W(K) \u2286 EK,d,\nuj, f \u2032 j(u), (f \u2032\u2032 jj(u) +D)vj , f \u2032\u2032 ji(u)vi \u2208 E [j] K+1,d\nFor convenience of the proof, we partition A as follow\nA =\n\n A11 A12 A13 A21 A22 A23 A31 A32 A33\n\n , (8)\nwhere A11 \u2208 Ra\u00d7a, A22 \u2208 Rb\u00d7b, A33 \u2208 Rc\u00d7c and a = \u2211 i<j di, b = dj , c = \u2211 i>j di . Let x1 = [uT1 , u T 2 , . . . , u T j\u22121] T , x2 = uj , and x3 = [uTj+1, . . . , u T m]\nT . Then we obtain\nf \u2032j(u) =\n\n \n \n[\n\u03bb(\u03ba\u22121) 2 A22 + \u03bbI\n]\nx2 + \u03bb(\u03ba\u22121)\n2 (A21x1 +A23x3) j 6= 1 [\n\u03bb(\u03ba\u22121) 2 A22 + \u03bbI\n]\nx2 + \u03bb(\u03ba\u22121) 2 (A21x1 +A23x3)\u2212 \u03bb(\u03ba\u22121) 4 e [1] 1 j = 1\nand\nf \u2032\u2032jj(u) = \u03bb(\u03ba\u2212 1)\n4 A22 + \u03bbI.\nAs f \u2032\u2032jj(u) +D is a tridiagonal matrix, we have\n(f \u2032\u2032jj(u) +D)vj \u2208 E [j] K+1,d. (9)\nUsing the fact that\nA21 =\n\n   0 \u00b7 \u00b7 \u00b7 0 \u22121 0 \u00b7 \u00b7 \u00b7 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 0 0\n\n   A23 =\n\n   0 0 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 \u00b7 \u00b7 \u00b7 0 \u22121 0 \u00b7 \u00b7 \u00b7 0\n\n   ,\nwe can rewrite f \u2032j as\nf \u2032j(u) =\n\n \n \n[\n\u03bb(\u03ba\u22121) 2 A22 + \u03bbI\n]\nx2 + \u03bb(\u03ba\u22121) 2 [x1(a), 0, . . . , 0, x3(1)] T , j 6= 1\n[\n\u03bb(\u03ba\u22121) 2 A22 + \u03bbI\n]\nx2 + \u03bb(\u03ba\u22121) 2 [ 1 2 , 0, . . . , 0, x3(1)] T . j = 1\nAs \u03bb(\u03ba\u22121)2 A22 + \u03bbI is a tridiagonal matrix, then\n[\u03bb(\u03ba\u2212 1) 2 A22 + \u03bbI ] x2 \u2208 E [j]K+1,d.\nWe can also obtain the following by some simple discussions on different cases:\n[ 1\n2 , 0, . . . , 0, x3(1)]\nT , [x1(a), 0, . . . , 0, x3(1)] T \u2208 E [j]K+1,d\nTherefore, we conclude that f \u2032j(w) \u2208 E [j] K+1,d. (10)\nBesides, note that\nf \u2032\u2032ji(u)vi =\n\n \n  [0, . . . , 0,\u2212vi(1)]T i = j + 1 [\u2212vi(di), 0, . . . , 0]T i = j \u2212 1 [0, 0, . . . , 0]T otherwise\nSimilarly, using some simple discussions on different cases we obtain\nf \u2032\u2032ji(u)vi \u2208 E [j] K+1,d (11)\nHence when W(K) \u2286 EK,d, combining (9) (10) and (11) we have for all u \u2208 WT and j\nuj, f \u2032 j(u), (f \u2032\u2032 jj(u) +D)vj, f \u2032\u2032 ji(u)vi \u2208 E [j] K+1,d.\nwhich implies the newly added wj satisfies\nwj \u2208 E [j]K+1,d\nTherefore, we prove that W(K+1) \u2286 EK+1,d.\nApplying Lemma 5 recursively we can get the following corollary:\nCorollary 6. After K \u2264 d rounds, we have W(K) \u2286 EK,d.\nWith Corollary 6, we now proceed to finish the proof of Theorem 2. First, we can find w\u2217 by the first order optimality condition\nf \u2032(w\u2217) = (\u03bb(\u03ba\u2212 1)\n4 A+ \u03bbI\n) w\u2217 \u2212 \u03bb(\u03ba\u2212 1) 4 = 0,\nwhich implies (\nA+ 4 \u03ba\u2212 1I ) w\u2217 = e1.\nThe coordinate form of above equation is\n2 \u03ba+ 1 \u03ba\u2212 1w \u2217(1)\u2212 w\u2217(2) = 1,\nw\u2217(k + 1)\u2212 2\u03ba+ 1 \u03ba\u2212 1w \u2217(k) +w\u2217(k \u2212 1) = 0, 2 \u2264 k \u2264 d\u2212 2,\n\u2212x\u2217(d\u2212 1) + ( 4 \u03ba\u2212 1 + \u221a \u03ba+ 3\u221a \u03ba+ 1 ) x\u2217(d) = 0,\nand let q be the smallest root of the following equation\nq2 \u2212 2\u03ba+ 2 \u03ba\u2212 1 q + 1 = 0,\nthat is q = \u221a \u03ba\u22121\u221a \u03ba+1 . Then w\u2217 satisfies w\u2217(i) = qi for 1 \u2264 i \u2264 d. Hence,\n\u2016w\u2217\u20162 = d \u2211\ni=1\n[w\u2217(i)]2 = d \u2211\ni=1\nq2i = q2(1\u2212 q2d)\n1\u2212 q2\nLet w(k) be any point in W(k) after k rounds iterations (k \u2264 d), applying Corollary 6 we have\n\u2016w(k) \u2212 w\u2217\u20162 \u2265 d \u2211\ni=k+1\n[w\u2217(i)]2 = d \u2211\ni=k+1\nq2i\n= q2(k+1)[1\u2212 q2(d\u2212k+2)]\n1\u2212 q2\n= 1\u2212 q2(d\u2212k+2)\n1\u2212 q2d q 2k\u2016w\u2217\u20162\n\u2265 1\u2212 q 4\n1\u2212 q2d q 2k\u2016w\u2217\u20162\n\u2265 1\u2212 q 1 q2k\u2016w\u2217\u20162\nCombing the above inequality and the optimal condition of strongly-convex function, we obtain\nf(w(k))\u2212 f(w\u2217) \u2265 \u03bb 2 \u2016w(k) \u2212 w\u2217\u20162\n\u2265 \u03bb(1\u2212 q) 2 q2k\u2016w\u2217\u20162\n= \u03bb\n2 2\u221a \u03ba+ 1 (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 )2k \u2016w\u2217\u20162\n\u2265 \u03bb\u221a \u03ba+ 1 exp\n(\n\u2212 4k\u221a \u03ba+ 1\n)\n\u2016w\u2217\u20162.\nThus if f(w(k))\u2212 f(w\u2217) \u2264 \u01eb, then we have\nk \u2265 \u221a \u03ba\u2212 1 4 log ( \u03bb\u2016w\u2217\u20162 ( \u221a \u03ba+ 1)\u01eb ) = \u2126 (\u221a \u03ba log ( \u03bb\u2016w\u2217\u2016 \u01eb )) ,\nwhich completes the proof."}, {"heading": "5.2 Proof of Theorem 4", "text": "With a slight abuse of notation, we construct the following separable strongly convex function:\nf(w) := 1\nm\nm \u2211\nj=1\n\u03c6j(wj), (12)\nwhere wT = [wT1 , . . . , w T m] and \u03c6j(wj) is also a separable strongly convex function with form of\n\u03c6j(wj) =\nn m \u2211\ni=1\n[\n\u03bb(\u03ba\u2212 1) 4\n(\n1 2 wTj,iAj,iwj,i \u2212 \u3008e1, wj,i\u3009\n)\n+ \u03bb\n2 \u2016wj,i\u20162\n]\n, (13)\nwhere wTj = [w T j,1, . . . , w T j,n] and Aj,i is a tridiagonal matrix in R\ndj\u00d7dj given by\nAj,i =\n\n        2 \u22121 0 \u00b7 \u00b7 \u00b7 0 0 0 \u22121 2 \u22121 \u00b7 \u00b7 \u00b7 0 0 0 0 \u22121 2 \u00b7 \u00b7 \u00b7 0 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 0 \u00b7 \u00b7 \u00b7 \u22121 2 \u22121 0 0 0 \u00b7 \u00b7 \u00b7 0 \u22121\n\u221a \u03ba+3\u221a \u03ba+1\n\n        .\nIt is simple to veriy that f is a \u03bb-strongly convex function with condition number \u03ba. Note that f is a quadratic function with its coefficient matrix being a block diagonal matrix, in which each block matrix is tridiagonal. As f is separable, the setting here can be viewed as each machine j has all information (all data samples in all coordinates) of f \u2019s component \u03c6j , and all machines simultaneously do non-distributed incremental opitmizaion over their local data. To get the lower bound, note that in \u201cclever\" algorithms, machine j only chooses data samples among the ones corresponding to \u03c6j in each round, then we have\nE[\u2016w(k) \u2212 w\u2217\u20162]\n\u2265 E\n\n\nm \u2211\nj=1\n\u2016w(k)j \u2212 w\u2217j\u20162  \n\u2265 m \u2211\nj=1\n[\n1 2 exp\n( \u2212 4k \u221a \u03ba\nn( \u221a \u03ba+ 1)2 \u2212 4\u221a\u03ba\n) \u2016w\u2217j\u20162 ]\n= 1\n2 exp\n( \u2212 4k \u221a \u03ba\nn( \u221a \u03ba+ 1)2 \u2212 4\u221a\u03ba\n)\n\u2016w\u2217\u20162.\nThe second inequality is according to [6, Theorem 3] and Corollary 6. Finally by [6, Corollary 3], we get if E [ f(w(k))\u2212 f(w\u2217) ] \u2264 \u01eb, then\nk \u2265 \u2126 ( (\u221a n\u03ba+ n ) log (\u2016w\u2217\u2016\u03bb \u01eb )) , (14)\nfor sufficiently large d."}, {"heading": "6 Conclusion", "text": "In this paper we have defined two classes of distributed optimization algorithms under the setting where data is partitioned on features: one is a family of non-incremental algorithms and the other is incremental. We have presented tight lower bounds on communication rounds for non-incremental class of algorithms. We have also provided one lower bound for incremental class of algorithms but whether it is tight remains open.\nThe tightness informs that one should break our definition when trying to design optimization algorithms with less communication rounds than existing algorithms. We also emphasize that our lower bounds are important as they can provide deeper understanding on the limits of some ideas or techniques used in distributed optimization algorithms, which may provide some insights for designing better algorithms.\nTo the best of our knowledge, this is the first work to study communication lower bounds for distributed optimization algorithms under the setting where data is partitioned on features."}], "references": [{"title": "Communication complexity of distributed convex learning and optimization", "author": ["Y. Arjevani", "O. Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Distributed learning, communication complexity and privacy", "author": ["M.-F. Balcan", "A. Blum", "S. Fine", "Y. Mansour"], "venue": "arXiv preprint arXiv:1204.3514,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Communicationefficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Tak\u00e1c", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "An optimal randomized incremental gradient method", "author": ["G. Lan"], "venue": "arXiv preprint arXiv:1507.02000,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Distributed box-constrained quadratic optimization for dual linear svm", "author": ["C.-p. Lee", "D. Roth"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity", "author": ["J. Lee", "T. Ma", "Q. Lin"], "venue": "arXiv preprint arXiv:1507.07595,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Distributed inexact damped newton method: Data partitioning and loadbalancing", "author": ["C. Ma", "M. Tak\u00e1\u010d"], "venue": "arXiv preprint arXiv:1603.05191,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["C. Ma", "V. Smith", "M. Jaggi", "M.I. Jordan", "P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "arXiv preprint arXiv:1502.03508,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Distributed block coordinate descent for minimizing partially separable functions", "author": ["J. Mare\u010dek", "P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "In Numerical Analysis and Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Parallel coordinate descent methods for composite minimization: convergence analysis and error bounds", "author": ["I. Necoara", "D. Clipici"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Y. Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Distributed coordinate descent method for learning with big data", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "arXiv preprint arXiv:1310.2059,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Communication-efficient distributed optimization of self-concordant empirical loss", "author": ["Y. Zhang", "L. Xiao"], "venue": "arXiv preprint arXiv:1501.00263,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Y. Zhang", "M.J. Wainwright", "J.C. Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 16, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 1, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 2, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 14, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 7, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 4, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 9, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 13, "context": ", each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7].", "startOffset": 46, "endOffset": 64}, {"referenceID": 10, "context": ", each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7].", "startOffset": 46, "endOffset": 64}, {"referenceID": 8, "context": ", each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7].", "startOffset": 46, "endOffset": 64}, {"referenceID": 11, "context": ", each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7].", "startOffset": 46, "endOffset": 64}, {"referenceID": 6, "context": ", each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7].", "startOffset": 46, "endOffset": 64}, {"referenceID": 12, "context": ", which can be matched by a straightforward distributed version of accelerated gradient decent [13] and also DISCO-F for quadratics [9], an variant of DISCO [16] under the feature partition setting.", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": ", which can be matched by a straightforward distributed version of accelerated gradient decent [13] and also DISCO-F for quadratics [9], an variant of DISCO [16] under the feature partition setting.", "startOffset": 132, "endOffset": 135}, {"referenceID": 15, "context": ", which can be matched by a straightforward distributed version of accelerated gradient decent [13] and also DISCO-F for quadratics [9], an variant of DISCO [16] under the feature partition setting.", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "Related Work The most revelant work should be [1], which studied lower bounds under the sample partition setting, and provided tight lower bounds on communication rounds for convex smooth optimization after putting some mild restrictions.", "startOffset": 46, "endOffset": 49}, {"referenceID": 12, "context": "Both these work as well as ours are based on some techniques used in non-distributed optimization lower bound analysis [13, 6].", "startOffset": 119, "endOffset": 126}, {"referenceID": 5, "context": "Both these work as well as ours are based on some techniques used in non-distributed optimization lower bound analysis [13, 6].", "startOffset": 119, "endOffset": 126}, {"referenceID": 3, "context": "Here we list some common MapReduce types of communication operations [4] in an abstract level: (a) One-to-all broadcast.", "startOffset": 69, "endOffset": 72}, {"referenceID": 13, "context": "Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].", "startOffset": 87, "endOffset": 105}, {"referenceID": 10, "context": "Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].", "startOffset": 87, "endOffset": 105}, {"referenceID": 8, "context": "Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].", "startOffset": 87, "endOffset": 105}, {"referenceID": 11, "context": "Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].", "startOffset": 87, "endOffset": 105}, {"referenceID": 6, "context": "Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].", "startOffset": 87, "endOffset": 105}, {"referenceID": 0, "context": "\u2022 Similar to [1], we use Wj to define the restriction on the updates.", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": "However, for some common loss functions \u03c6, such as (regularized) squared loss, logistic loss and squared hinge loss, computing f \u2032 j(w) for all j \u2208 [m] in total only needs a ReduceAll operation of an R vector [14].", "startOffset": 209, "endOffset": 213}, {"referenceID": 13, "context": "In some gradient (or partial gradient) based algorithms [14, 11], communication to compute partial gradients are the only need of communication in the computation phase.", "startOffset": 56, "endOffset": 64}, {"referenceID": 10, "context": "In some gradient (or partial gradient) based algorithms [14, 11], communication to compute partial gradients are the only need of communication in the computation phase.", "startOffset": 56, "endOffset": 64}, {"referenceID": 8, "context": "Besides, some algorithms like DISCO-F [9] need to compute (\u2207f(w)u)[j].", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "a ReduceAll operation of an R vector [9].", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "The amount of communication allowed in our partition-on-feature algorithm class is relatively small, compared with the partitionon-sample algorithm class described in [1], which allows \u00d5(md) bits of one-to-all broadcast in each round.", "startOffset": 167, "endOffset": 170}, {"referenceID": 12, "context": "For some common loss functions, this can be matched by a straightforward distributed implementation of accelerated gradient decent [13] and it is easy to verify that it satisfies our definition: let all machines compute their own partial gradients and aggregate to form a gradient.", "startOffset": 131, "endOffset": 135}, {"referenceID": 8, "context": "Recall that our definition of the algorithm class includes some types of distributed second order algorithms, for example DISCO-F [9].", "startOffset": 130, "endOffset": 133}, {"referenceID": 12, "context": "The proof framework of these theorems are based on [13].", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "First, we construct the following function like [6] f(w) = \u03bb(\u03ba\u2212 1) 4 [1 2 wAw \u2212 \u3008e1, w\u3009 ]", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "[ \u03bb(\u03ba\u22121) 2 A22 + \u03bbI ] x2 + \u03bb(\u03ba\u22121) 2 (A21x1 +A23x3)\u2212 \u03bb(\u03ba\u22121) 4 e [1] 1 j = 1 and f \u2032\u2032 jj(u) = \u03bb(\u03ba\u2212 1) 4 A22 + \u03bbI.", "startOffset": 63, "endOffset": 66}], "year": 2016, "abstractText": "Recently, there has been an increasing interest in designing distributed convex optimization algorithms under the setting where the data matrix is partitioned on features. Algorithms under this setting sometimes have many advantages over those under the setting where data is partitioned on samples, especially when the number of features is huge. Therefore, it is important to understand the inherent limitations of these optimization problems. In this paper, with certain restrictions on the communication allowed in the procedures, we develop tight lower bounds on communication rounds for a broad class of non-incremental algorithms under this setting. We also provide a lower bound on communication rounds for a class of (randomized) incremental algorithms.", "creator": "LaTeX with hyperref package"}}}