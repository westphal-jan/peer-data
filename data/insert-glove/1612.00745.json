{"id": "1612.00745", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Cognitive Deep Machine Can Train Itself", "abstract": "Machine 9.325 learning smoldered is arrernte making substantial progress axley in diverse borowa applications. network-based The cut-price success dutrow is mostly due to advances yenny in deep orate learning. dainzin However, pampa deep darque learning 32.60 can bewail make mistakes vauban and if-then its generalization abilities etiology to grahn new 930,000 tasks are p25 questionable. faires We ask when and how one can combine network datacenters outputs, scarff when (gauld i) kwasi details isabela of aborn the observations cifuentes are evaluated by mesocarp learned nestorius deep trasporti components and (78.97 ii) pgi facts and confirmation rules ilayaraja are activesync available ra\u00eblian in knowledge khand based patriarchates systems. wjla We arbovirus show 34.07 that in dantewada limited festo contexts the required pentonville number schmitt of taobao training samples chahm can ishant be low isolde and bokm\u00e5l self - \u00ee improvement of pre - trained networks tenement in streptavidin more general 5:58 context is \u0cb5 possible. 2,910 We cental argue .509 that self-critical the combination of leira sparse outlier detection gulbene with birchenough deep gherardi components jocs that maude can cust\u00f3dio support rakshas each ethridge other .661 diminish showier the fragility of deep methods, an tyrannidae important requirement ambiguity for engineering tira applications. -------------------------------------------------- We argue that k\u00fchne supervised grigorovich learning of labels eew may leguminous be ne3 fully eliminated zinged under b.k. certain conditions: numbers a jacquinot component u-96 based architecture stewarts together phosphorylated with regenerating a knowledge based kurt\u00e1g system can valentini train arrojo itself and mestalla provide high quality answers. We demonstrate these concepts laino on antagonist the State kunce Farm Distracted Driver hamby Detection a4 benchmark. We zuo'er argue that the view intriguingly of derided the sorkh Study 18:30 Panel (2016) may botany overestimate unstaged the atty. requirements on ` bartov years sedbergh of focused research ' and ` nikki careful, unique lindman construction ' for ` korsak AI systems '.", "histories": [["v1", "Fri, 2 Dec 2016 16:49:07 GMT  (6235kb,D)", "http://arxiv.org/abs/1612.00745v1", "14 pages, 8 figures"]], "COMMENTS": "14 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["andr\\'as l\\h{o}rincz", "m\\'at\\'e cs\\'akv\\'ari", "\\'aron f\\'othi", "zolt\\'an \\'ad\\'am milacski", "r\\'as s\\'ark\\'any", "zolt\\'an t\\h{o}s\\'er"], "accepted": false, "id": "1612.00745"}, "pdf": {"name": "1612.00745.pdf", "metadata": {"source": "CRF", "title": "COGNITIVE DEEP MACHINE CAN TRAIN ITSELF", "authors": ["A. L\u0151rincz"], "emails": [], "sections": [{"heading": null, "text": "Keywords: deep learning, knowledge based system, recognition by components, episodic description"}, {"heading": "1 INTRODUCTION", "text": "Machine learning is progressing quickly due to deep learning. The key tool for deep learning is crowdsourcing, i.e., to the exploitation of human intelligence. Success stories demonstrate that superhuman performance can be reached this way (Schmidhuber, 2015). Still, the groundbreaking deep network approach seems limited as \u2018each application requires years of focused research and careful unique construction\u2019 (Study Panel, 2016). However, if we take a look at human information processing, for example, we learn that it has two basic routes: (i) holistic recognition (Tanaka & Gordon, 2011) and (ii) recognition by components (Biederman, 1987). These processing methodologies are competing and also complementing each other. Deep learning methods, on the other hand, tend to favor endto-end learning, which corresponds to holistic recognition and are missing the advantages of the component based approach.\nFurthermore, holistic recognition and thus end-to-end learning is fragile. The fragility has been shown in a number of studies., e.g., (i) deep networks can be fooled as described by Nguyen et al. (2015), when inputs that differ enormously for a human observer from a given class are assigned the label of that class with extremely high confidence, or as demonstrated in Sharif et al. (2016) showing that (ii) barely visible watermark-like modifications may change the class label, and that (iii) small additional components can be designed to change the class index to another desired one, making the network prone to attacks. One may admit that the highly sophisticated human visual recognition system is also prone to illusions and can misinterpret visual information.\nAn additional and apparent source of error is the dependency on the context. This is the tool that \u2013 eventually \u2013 we want to exploit for diminishing the problems mentioned above. We illustrate the problem through a few examples. Consider Fig. 1. Figure 1(a) looks like a deflated football, but\nar X\niv :1\n61 2.\n00 74\n5v 1\n[ cs\n.L G\n] 2\nD ec\n2 01\n6\nthe interpretation can be changed easily (see Fig. 1(b)) by varying the environment, it can also be a chair. Deep learning can make similar \u2018mistakes\u2019: we used the Faster R-CNN network that was pre-trained on the Pascal VOC database. The training set contains a horse (Fig. 1(c)). We changed the environment of the horse without adding any occlusion. The new backgrounds were chosen from the Visual Genome database. We provide a few samples of our results. For example, the best guess remained \u2018horse\u2019 when we put the horse along a road (Fig. 1(d)), but it changed to \u2018cow\u2019 upon mirroring the horse from left-to-right (Fig. 1(e)). Inserting the segmented horse to the bottom of an office desk, \u2018dog\u2019 was the best guess of the network (Fig. 1(f)). Intriguingly, these network proposals make sense due to the context: typically cows are crossing roads and dogs are in the office. Somehow, somewhere, such knowledge is implicitly embedded into the database. Our aim is to make it explicit.\nWhy is it so easy to change the output of the network trained on many examples? On the one end, it is good, since context can help, e.g., when words are ambiguous. On the other end, in case if the category is certain then such uncertainty seems worrying. We put forth the idea that holistic recognition can be disambiguated by the recognition of components, such as the mane or the hoof for the case of the horse in our example. Such component based reasoning can easily fix the output of the network. Furthermore, if time series are available, then the motion pattern is of help since it differs considerably for horses, dogs, and cows. Components have several advantages: (i) they are in smaller spaces, (ii) their environment may be correlated with them, like the example of face being the \u2018environment\u2019 of the eyes and the mouth, and (iii) temporal continuity, if present, may improve the precision of the observation process. Nonetheless, cooperation and competition between holistic information processing and component based inference may not overcome all problems as supported by different visual, or auditory illusions and certain combinations, like the McGurk effect (McGurk & MacDonald, 1976), too. Note also that from the general information processing point of view, components are useful; we can acquire additional knowledge by hearsay and can connect lower level component detectors and higher level symbols offering a solution to the symbol grounding problem emphasized by Harnad (2003). Holistic and component based recognition mechanisms that seem to compete and complement each other point to sophisticated hierarchical construction beyond present day deep neural network architectures.\nWe demonstrate that traditional knowledge based systems are capable of bridging and training deep neural networks working on different, but correlated components of a larger recognition problem. In\ngeneral, knowledge based systems may include reasoning tools, differential equations, knowledge about the physics of the world, ontologies, among others. An important ingredient of our approach is the condition that there are components that assume each other1. Our procedure has three building blocks. The first one is Robust Principal Component Analysis (RPCA) that filters out apparent components, which are, in fact, outliers. A related problem, Group Fused LASSO is capable of segmenting time series in the presence of non-Gaussian, but sparse deviations offering, e.g., the capability of grouping samples in time. The second part is self-training: the individual deep learning architectures pre-trained in general scenarios can fine-tune each other in a limited context. The third element is the derivation of rule based systems from the episodic labels that are given even if the meaning of the labels is hidden from us. We illustrate our approach on the State Farm Distracted Driver Scenario Kaggle benchmark.\nTheoretical background about the procedures we used is given in Sect. 2. We treat the driver monitoring benchmark in Sect. 3. We show that the deep components, the outlier detection and the rule based system together that we call Cognitive Deep Machine (CDM) improves performance. The discussion (Sect. 4) considers the generality of the results. In this section we argue that development of novel engineering solutions exploiting AI and deep learning may be much faster than expected by the Study Panel (2016). This is due to two reasons: deep networks can be (a) reused, can strengthen and train each other in narrow contexts and (b) connected to high level (classical) rule-based expert systems. In Section 4.4 we elaborate this concept within our framework. A short summary concludes this technical report (Sect. 5)"}, {"heading": "2 METHODS", "text": "Below, we describe the outlier detection and temporal segmentation schemes (Sect. 2.1\u20132.2). We discuss Optical Flow and its unsupervised way of finding components (Sect.2.3). It is followed by the list of pre-trained deep networks that we use and fine-tune through self-supervision (Sect. 2.4)."}, {"heading": "2.1 ROBUST PRINCIPAL COMPONENT ANALYSIS", "text": "It is well known that classical Principal Component Analysis (PCA) essentially works with \u2016 \u00b7 \u2016F (Frobenius) norm estimation, and hence it breaks down in presence of additional gross-but-sparse outliers. Cand\u00e8s et al. (2011) showed that it is possible to augment this architecture with a term that collects and thus separates such components, which they call Robust Principal Component Analysis (RPCA). Accordingly, given X \u2208 RD\u00d7T , one may define the following convex optimization problem:\nmin U,S\n1 2 \u2016X\u2212U\u2212 S\u20162F + \u03bb\u2016\u03c3(U)\u20161 + \u00b5\u2016vec(S)\u20161, (1)\ni.e., U \u2208 RD\u00d7T approximates X with low-rank (via the first `1 regularizer for its singular value vector \u03c3(U)), while S \u2208 RD\u00d7T represents an extra sparse outlier term (due to the second `1 regularizer). We utilized the Inexact Augmented Lagrangian Multiplier (Lin et al., 2010) solver for this problem implemented in Python2. The RPCA method does not require time series, but we utilized it this way."}, {"heading": "2.2 GROUP FUSED LASSO", "text": "Convex multiple change point detection for multivariate time series relies on `1,2 regularized Frobenius norm estimation as well, where the regularizer acts on the finite difference of the optimization variable, i.e., for input X \u2208 RD\u00d7T and weight matrix W \u2208 RD\u00d7T , solve:\nmin V\n1 2 \u2016W \u25e6 (X\u2212V)\u20162F + \u03bb T\u2212p\u2211 t=1 \u2016VQ.,t\u20162 (2)\nThis problem is often called the Group Fused LASSO (Bleakley & Vert, 2011). Here, \u25e6 indicates the elementwise product and Q \u2208 RT\u00d7T\u2212p is a finite differencing matrix that differentiates V \u2208 RD\u00d7T p times, yielding a piecewise polynomial model of degree p \u2212 1. The `1,2 regularizer promotes\n1The task of learning of such components has been tackled recently by Lo\u030brincz et al. (2016). 2https://kastnerkyle.github.io/posts/robust-matrix-decomposition/\njoint vanishing of finite difference components of individual time steps. We implemented the above problem in CVXPY3 (Diamond & Boyd, 2016)."}, {"heading": "2.3 OPTICAL FLOW AND COMPONENT SEARCH", "text": "We used Optical Flow (Lucas & Kanade, 1985) implemented in OpenCV4 for searching components across time via estimating bounding box motion between neighbouring frames in video. First, we applied an object detector (Sect. 2.4) to obtain the bounding box coordinates for each frame, then the boxes were scaled to identical sizes. Next, for the original frames, we estimated the motion of the feature points within the bounding boxes. This provided a similarity measure between consecutive frames. If similarity was above a threshold, we grouped bounding boxes together. We kept doing this for multiple time steps. Finally, we also merged groups that were similar.\nThe original method of (Lucas & Kanade, 1985) has been improved in many ways, including pyramidal evaluations that start from low resolution and work towards higher ones, Dense Optical Flow methods5 and deep architectures, also in combinations, see, e.g., the work of Fischer et al. (2015) and the references therein. The 30 years old method serves illustration purposes here."}, {"heading": "2.4 PRE-TRAINED SUPERVISED DEEP NETWORKS", "text": "Deep learning is thoroughly reviewed by Schmidhuber (2015). Introduction and details of the theory of the different networks can be found in the very recent book from Goodfellow et al. (2016). Consequently, we refer the interested reader to these excellent works and restrict this technical report to the collection of the references of the papers and the related software tools that we applied during the course of our work:\n1. Faster R-CNN6 (Ren et al., 2015) for object proposals;\n2. Region-based Fully Convolutional Network7 (Dai et al., 2016) for object and hand detection;\n3. Convolutional Pose Machine8 (Wei et al., 2016) for body pose detection (we also injected hand detection for improving the heat maps of this network);\n4. Libfacetracker (To\u030bs\u00e9r et al., 2016) for face detection.\nThe networks were pre-trained on the following databases:\n1. MS COCO9 (Lin et al., 2014) for object detection;\n2. PASCAL Visual Object Classes10 (Everingham et al., 2010) with twenty classes for training Faster R-CNN;\n3. Visual Genome11 (Krishna et al., 2016) for changing backgrounds;\n4. Mittal\u2019s Hand Dataset12 (Mittal et al., 2011) for hand detection;\n5. VIVA Hand Detection Dataset13 for left and right hand classification.\n3http://www.cvxpy.org/en/latest/ 4http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/\npy_video/py_lucas_kanade/py_lucas_kanade.html 5http://docs.opencv.org/3.0-beta/modules/optflow/doc/dense_optflow.html 6Code: https://github.com/rbgirshick/py-faster-rcnn 7Code: https://github.com/Orpine/py-R-FCN 8Code: https://github.com/shihenw/convolutional-pose-machines-release 9Database: http://mscoco.org/\n10Database: http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html 11Database: https://visualgenome.org/ 12Database: http://www.robots.ox.ac.uk:5000/~vgg/research/hands/index.html 13Database: http://cvrr.ucsd.edu/vivachallenge/index.php/hands"}, {"heading": "2.5 KAGGLE STATE FARM DISTRACTED DRIVER DETECTION DATASET", "text": "We performed and evaluated our methodology on the Kaggle State Farm Distracted Driver Detection challenge that was finished a few months ago on August 1, 201614. The database contains 26 subjects and 1 video for each. The original benchmark consisted of individual images but was later sorted and assembled into videos by Gilberto Titericz Junior15. The recordings consist of driving scenarios viewed from the passenger\u2019s seat with the camera facing the driver. The world outside the window is hazy and light. The chair, the body of the driver and the dashboard barely move.\nLabels of the State Farm Distracted Driver Detection challenge are ambiguous and can be misleading. For example, the image of Fig. 2 may correspond to safe driving when the driver is looking back to overcome the limits of the mirrors and monitor the blind spots of them directly. On the other hand, there are samples with safe driving labels, when the driver is looking at the passenger and is laughing, she is clearly distracted. A rule based system that takes into account if the driver is talking and the estimation of gaze direction would classify the latter as \u2018talking to passenger\u2019. Decision about the correct label of Fig. 2 may require more information about the past and the goals of the driver. For example, she may want to change lanes and may be asking the passenger on the back seat to move so she can see the blind spot of the mirror of the vehicle. There are other examples where labeling is ambiguous, e.g., if the driver is texting and talking simultaneously."}, {"heading": "3 EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "3.1 PIXEL BASED ROBUST PRINCIPAL COMPONENT ANALYSIS", "text": "Below, we show examples from the RPCA analysis (Sect. 2.1) applied for each grayscale video independently with default parameters. Figure 3 has four sets of three subfigures. The left, middle and right subfigures represent the original image, its low rank and its outlier components, respectively. A large portion of the images is stationary; these parts form the low dimensional subspace of the RPCA analysis. The outliers mostly correspond to short time intervals within driving episodes that are called, for example, \u2018operating the radio\u2019 (3(a)), \u2018texting with the right hand\u2019 (3(b)), \u2018drinking\u2019 (3(c)), \u2018talking to the passenger\u2019 (3(d)) and a few more. Note that the outlier parts as well as the low dimensional subspace parts differ: they are projections onto the PCA subspace and these projections depend on the input.\n14Database: https://www.kaggle.com/c/state-farm-distracted-driver-detection 15https://www.kaggle.com/titericz/state-farm-distracted-driver-detection/\njust-relax-and-watch-some-cool-movies/code"}, {"heading": "3.2 EPISODIC SEGMENTATION WITH GROUP FUSED LASSO", "text": "Similar to outliers being correlated with ground truth labels in Sect. 3.1, we note that driver body poses are also associated. Such information can be determined by means of a pose detector such as the Convolutional Pose Machine (Sect. 2.4).\nWe have applied the Convolutional Pose Machine for each original frame independently to extract arm joint coordinates. The network was pre-trained mostly for frontal 2D body samples, yet it still shows reasonable performance for side views and for upper body configurations. As network outputs were extremely noisy across time, we tried to extract the underlying temporal segments with constant pose using the Group Fused LASSO (Sect. 2.2). Pose outputs were normalized and weighted by their scores. As CVXPY (Diamond & Boyd, 2016) did not yield a truly sparse solution, change point strengths were further thresholded. The result \u2013 a nearly perfect match with ground truth labels, despite the large portion of noise \u2013 for one of the videos is shown in Fig. 4. Similar results can be achieved on the other videos. In all cases, the temporally segmented frames form groups that we shall discuss later."}, {"heading": "3.3 EPISODIC SEGMENTATION WITH OPTICAL FLOW", "text": "We applied Optical Flow (Sect. 2.3) with the Region-based Fully Convolutional Network handdetector (Sect. 2.4) on raw videos. Fig. 5 illustrates the results and our point. Subfigure (a) represents a close-to periodic motion within a larger image. It is labeled \u2018drinking\u2019 in the Kaggle database: the driver is holding a cup, while he starts and stops drinking in an alternating manner. As described in Sect. 2.3, the relevant portions of the images are segmented by our hand detector, the coherence of motion is made visible in subfigures (b)-(d) by the green arrows that show the motion of the individual feature points. The arrows change direction in the two drinking phases (not shown) and the Optical Flow itself can serve episodic segmentation, especially if Dense Optical Flow algorithms are used. Furthermore, similarly to the Group Fused LASSO example above, the bounding boxes connected by the optical flow form groups in a natural fashion, to be detailed in Sect. 4."}, {"heading": "3.4 RULE-BASED COMBINATION OF OBJECT DETECTION AND THE CONVOLUTIONAL POSE MACHINE", "text": "The episodic description can be detailed if facial information predictors or object detectors are included (Sect. 2.4), as shown in Fig. 2 and Fig. 6, respectively. Object detection is relatively weak for the Kaggle benchmark, as occluded objects are frequent, and there is a lack of rich datasets that would have such scenarios, e.g., objects in hand. Consider Fig. 7, showing test results for the Faster R-CNN network from MS COCO (Sect. 2.4). In Fig. 7(a) it is easy to recognize the cellphone. However, the\ncellphone can be inferred only from higher order knowledge in Fig. 7(b) and the classification can be questioned. Both images are from the test dataset and the higher order knowledge is implicit in the labeling of the training database.\nWe proceeded as follows. We \u2018know\u2019 that labels are about human activities that are manifested by gaze that may either search or be focused on something, hands that provide information about the actual manipulation, and speech that we don\u2019t have in this benchmark. We used Mittal\u2019s Hand Dataset for hand detection and the VIVA Hand Detection Dataset for left and right hand classification. We employed the Region-based Fully Convolutional Network for the localization of the hands and a vanilla Convolutional Network for classifying if a hand is left or right. Results were encouraging, but left and right hands were frequently misclassified. Then we combined the outputs with those of the Convolutional Pose Machine (Sect. 2.4). This is the direction of self-training; we/it can exploit inference methods based on our knowledge or knowledge bases that point to the strengths of classical artificial intelligence procedures.\nWithin the Kaggle database, there is a special scenario called \u2018Safe Driving\u2019. In this case, both hands are on the steering wheel and this is a limited spatial region. Furthermore, during all driving scenarios, one hand was always on that wheel; only very few frames are exceptions. The interplay between the Convolutional Pose Machine and the hand detectors together with backing knowledge forms the Cognitive Deep Machine as follows:\n1. the Convolutional Pose Machine gives high scores;\n2. the hand detector detects two or more hands with bounding boxes and the detectors have high scores;\n3. the node belonging to one of the wrists of the pose machine is close to one of the edges of the bounding box of the hand detector;\n4. the vector that starts at the node of the elbow and ends on the corresponding wrist points towards the center of one of the hand detectors;\n5. both hands are in the region of the wheel.\nIn case of doubt or if higher certainty is needed, then one can require the following:\n6. the scores of the hand detector should be high;\n7. the distance between the estimated point of the wrist and the bounding box of the hand should be small.\nFurther rigor could be introduced by means of the Optical Flow; one may require that the above conditions are fulfilled for a longer time interval, depending on the frame rate. Now, due to the high score of the Convolutional Pose Machine we know which hand is which, i.e., left or right. The variety of the set is limited and we can learn to recognize the left and the right hands on the steering wheel. One may proceed from here, since for other scenarios one of the hands is on the steering wheel. In turn, backing knowledge says that the other hand, which is not on the steering wheel should be the other hand, provided that the pose machine supports the proposition. We note that there are two more people in the car; one is next to the driver and is working with camera. The other person is sitting on the back seat and is taking notes, or making phone calls, or may be texting.\nIn the procedure we used one label set from the benchmark. Note, however, that this information can be dropped if we can recognize the steering wheel. The same procedure would follow. ImageNet has over 2,000 samples for this class16, so this training should be feasible.\nThe knowledge based method (one person has two hands and one of them is on the steering wheel) brings about good results for the Kaggle scenario. The information about the hands can be injected into the Convolutional Pose Machine that iterates heat maps about the suspected positions. The heat\n16http://image-net.org/synset?wnid=n04313503\nmaps can be modified and the left and right hand positions of high probabilities can be injected that constrains the iterative procedure of the machine. We show some results in Fig. 8.\nFinally, self-training can be continued: given the backing knowledge and the required high scores:\n8. hand recognition is improved within the Kaggle benchmark; 9. a training example is collected for further tuning of the Convolutional Pose Machine within\nthe scenarios of the benchmark."}, {"heading": "4 DISCUSSION", "text": ""}, {"heading": "4.1 SELF-TRAINING BY DEEP NETWORKS HAVING BACKING KNOWLEDGE BASES", "text": "Outputs of the different input-output systems (e.g., deep networks) can be used for self-training. Our example is the Convolutional Pose Machine and the hand detector: we improved their performances by joining them (Sect. 3.4). The Cognitive Machine can use high scoring poses and hand detector outputs for improving the recognition of the left and the right hands within the Kaggle scenarios. It may know that in the context of driving one hand is on the steering wheel in most of the time. It can use the information for giving proper labels to left and right hands and training itself. It can go further by using the same inference and inject the information about the left and/or the right hand into the Convolutional Pose Machine. The derived pose can serve further training for the Convolutional Pose Machine within the benchmark scenarios.\nWe note that the line of thoughts presented above is one option out of many. Other methods may serve the goals better, the goal being to have a joint agreement between the knowledge that concerns\nthe actual context, the segmentation procedures provided by Group Fused LASSO, the warning signs of the RPCA method about outliers and the deep networks trained on other, more general databases. The applied heuristics, although proved to be successful, it is still accidental. Nonetheless, it fits reinforcement learning as we discuss in Sect. 4.5 that we discuss below."}, {"heading": "4.2 THE ENGINEERING VIEW: BACK TO RULE BASED SYSTEMS", "text": "The machinery that we used simplified the Kaggle benchmark considerably. We can recognize the pose, separate the movements of the left and the right hands, one can track the face and the hand, too with high precision. Object detection can be raised to high levels, and thus the recognition of cell phones, cups/mugs should not be a big problem.\nThe outcome of these considerations is that the classification according to the Kaggle labels is a simple \u2014 rule-based \u2014 task that does not require any further training. The meaning of a label is sufficient for proper classification. Furthermore, one can filter the ambiguous cases, e.g., when talking to the passenger and texting occur simultaneously. This is main reason why we think that the Study Panel (2016) may overestimate the complexity of developing new applications. It is not deep technology alone that can serve new applications, but also the knowledge about the components and their relations, i.e., the semantic and ontological knowledge that have been collected over many years in history.\nOne question that emerges is the following: assuming that the labels are codes and thus have no meaning, could we tell what is happening? Temporal grouping by means of Group Fused LASSO and Optical Flow can help us. We have examples when the driver picks up the telephone, looks at the phone, manipulates the phone, lifts the phone to his ear and then talks. The interpretation of the scenario can be guessed by using ConceptNet17. In turn, we can derive a number of rules from the Kaggle scenarios. One particular example is that dialing and holding the phone involves the same hand in driving scenarios.\nComponent searches and learning, followed by inferences can take us back to traditional AI. They enable fast learning, since one can learn new categories and relations through hearsay. Zebra is related to our horse, cow, dog illustration (Sect. 1). A giraffe is similar, it has spots and not stripes and its neck is much longer. One can learn from the Kaggle labels, too; they also provide information. For example, in ordinary communication, the separation of class \u2018safe driving\u2019 may involve that the other categories are not safe. Similarly, we may suspect that there are phones, which are capable for text messaging, being common sense today, but not so about two decades ago. Such reasoning capabilities make decision making and, in turn, applications relatively easy to develop. If components are known then reasoning based upon them is sufficient in many cases. After all, what has been found by mankind over thousands of years, can be passed to a child in twenty years or so. The key, in our opinion, is in the knowledge of the components and deep neural networks can be trained to recognize them."}, {"heading": "4.3 THE GESTALT VIEW", "text": "The principles that we applied are the hundred year old Gestalt Principles (Todorovic, 2008), also called Gestalt Laws of Grouping, such as Proximity, Similarity, Continuity, Closure, and Connectedness. They can serve AI to discover spatio-temporal phenomena, or episodes, characterize and predict them, and if possible, compress them to meaningful concepts for human intelligence. Tacitly, we exploited determinism, when we used high scores and temporal relationships from Optical Flow and from Group Fused LASSO. The key hypothesis is that outlier detection and determinism together are powerful tools for learning if we can recognize components and have (episodic) knowledge about their spatio-temporal relationships.\nFrom the point of view of learning, if recognition has high score at a time, then it can help us to track the object and to learn its interactions, e.g., that a phone can be picked up. The more details we know, the less likely is that we make mistakes. This is also the Gestalt view.\n17http://conceptnet5.media.mit.edu/web/c/en/telephone"}, {"heading": "4.4 COGNITION IN THE COGNITIVE DEEP MACHINE", "text": "The word cognition is used in many ways, including information processing, or acquiring knowledge, among others. In our view, a deep network is not capable of cognizing. We think that cognition is more than input-output mapping, it includes the capability of reasoning and utilizes reasoning for acquiring knowledge. Note that we are not using the word understanding18, since it is beyond our present formulation.\nReasoning concerns considerations about the components, the reliability of the related observations as well as the spatio-temporal context. Cognition has at least five ingredients: (i) holistic and componentwise observations, e.g., by means of deep networks, (ii) outlier detection concerning those observations, (iii) reasoning about the best labels by means of the observed components, (iv) reasoning about the outliers and collecting such examples, and (v) self-training by means of the collected anomalies.\nWe went through such steps in the result section (Sect. 3) and that can highlight the milestones of our approach. We note that up to this point we provided no algorithmic method, but only the name. This is so, since we believe that cognition is meaningless without goals. Thus, in our view, cognition is closely linked to reinforcement learning."}, {"heading": "4.5 OUTLOOK: REINFORCEMENT LEARNING", "text": "Learning under supervision concerns input-output mapping. We think that there is a big difference between the supervised learning of components, like the case of the pose machine and the unsupervised searches for spatio-temporal structures. We think that the latter can be built stepwise from basic elements by means of Gestalt Principles, the example being the horse, the zebra, the giraffe, and the cow that look very similar, but differ in texture, the length of the neck, the shape of the body, including the mane and the hoof and the environment they live in (Sect. 1). The reduction of variables (here, components, or spatio-temporal structures) can put reinforcement learning into work, since reinforcement learning in the absence of such highly compressed factors restricted to those that might be relevant for decision making blows up exponentially. With such reduced spatio-temporal components, \u2018factored reinforcement learning\u2019 becomes feasible as summarized by To\u030bs\u00e9r & Lo\u030brincz (2015) and the cited references therein. Important ingredient of the approach is the assumption on close to deterministic episodes that can be concurrent, can start and stop. Such concepts have been formulated within the event learning framework for single episodic series Szita et al. (2002) and by Szita & Lo\u030brincz (2007) for multiple events.\nSuccess stories on reinforcement learning using deep networks are already numerous, including Atari games (Mnih et al., 2015) and the game Go (Silver et al., 2016). In turn, we conjecture that new applications built on deep learning technology based reinforcement learning, although technologically might be challenging regarding the sensory and the control systems, but may appear relatively fast as opposed to the view expressed in Study Panel (2016)."}, {"heading": "5 CONCLUSION", "text": "We considered the development of deep learning technology combined with traditional AI and component based reasoning. We provided examples through the State Farm Distracted Driver Detection benchmark. We argued that this benchmark requires components only and that they can be learned via deep learning in problems outside of the benchmark itself. Then, having these components, the benchmark requires no more learning, but reasoning, unsupervised anomaly detection, data collection related to the detected anomalies, searches for temporal segments, and finally self-training within the context of the Distracted Driver Detection problem. We conclude that novel applications can be tackled by AI and the bottleneck is in the sensory information and not in the learning system itself. We also argue that the component based construction has several advantages:\n\u2022 it can work if part of the information is missing, e.g., if the hand is not visible;\n\u2022 it can be combined with reinforcement learning for the optimization of decision making;\n18https://en.oxforddictionaries.com/definition/cognition\n\u2022 it can overcome the fragility of deep learning by means of component-wise reasoning; \u2022 it can detect anomalies by means of inverting its own input-output system; \u2022 it can look for spatio-temporal structures by means of the Gestalt Principles; \u2022 it can self-train itself via its component based reasoning capabilities.\nWe justify our conclusion by highlighting that applications aim high level of determinism, training concerns a limited context, ontology and rule based systems are warranted, operation and potential errors can be modeled and run in virtual reality19 and data can be collected in practice while searching for anomalies under less stringent conditions. The AI system can train itself step-by-step."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was partially supported by EIT Digital Grant No. 16527 on Cyber-Physical Systems for Smart Factories."}], "references": [{"title": "Recognition-by-components: a theory of human image understanding", "author": ["Irving Biederman"], "venue": "Psychological Review,", "citeRegEx": "Biederman.,? \\Q1987\\E", "shortCiteRegEx": "Biederman.", "year": 1987}, {"title": "The Group Fused LASSO for Multiple Change-point Detection", "author": ["Kevin Bleakley", "Jean-Philippe Vert"], "venue": "arXiv preprint arXiv:1106.4199,", "citeRegEx": "Bleakley and Vert.,? \\Q2011\\E", "shortCiteRegEx": "Bleakley and Vert.", "year": 2011}, {"title": "Robust Principal Component Analysis", "author": ["Emmanuel J Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "R-FCN: Object detection via region-based fully convolutional networks", "author": ["Jifeng Dai", "Yi Li", "Kaiming He", "Jian Sun"], "venue": "arXiv preprint arXiv:1605.06409,", "citeRegEx": "Dai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2016}, {"title": "CVXPY: A python-embedded modeling language for convex optimization", "author": ["Steven Diamond", "Stephen Boyd"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Diamond and Boyd.,? \\Q2016\\E", "shortCiteRegEx": "Diamond and Boyd.", "year": 2016}, {"title": "The Pascal Visual Object Classes (VOC) challenge", "author": ["Mark Everingham", "Luc Van Gool", "Christopher KI Williams", "John Winn", "Andrew Zisserman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["Philipp Fischer", "Alexey Dosovitskiy", "Eddy Ilg", "Philip H\u00e4usser", "Caner Hazirbas", "Vladimir Golkov", "Patrick van der Smagt", "Daniel Cremers", "Thomas Brox"], "venue": "IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Fischer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fischer et al\\.", "year": 2015}, {"title": "Deep learning. Book in preparation for MIT Press, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "URL http://www.deeplearningbook.org", "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "Krishna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2016}, {"title": "Microsoft COCO: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-rank Matrices", "author": ["Zhouchen Lin", "Minming Chen", "Yi Ma"], "venue": "arXiv preprint arXiv:1009.5055,", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Estimating Cartesian compression via deep learning", "author": ["Andr\u00e1s L\u0151rincz", "Andr\u00e1s S\u00e1rk\u00e1ny", "Zolt\u00e1n \u00c1 Milacski", "Zolt\u00e1n T\u0151s\u00e9r"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "L\u0151rincz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "L\u0151rincz et al\\.", "year": 2016}, {"title": "Optical navigation by the method of differences", "author": ["Bruce D Lucas", "Takeo Kanade"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Lucas and Kanade.,? \\Q1985\\E", "shortCiteRegEx": "Lucas and Kanade.", "year": 1985}, {"title": "Hearing lips and seeing", "author": ["Harry McGurk", "John MacDonald"], "venue": "voices. Nature,", "citeRegEx": "McGurk and MacDonald.,? \\Q1976\\E", "shortCiteRegEx": "McGurk and MacDonald.", "year": 1976}, {"title": "Hand detection using multiple proposals", "author": ["Arpit Mittal", "Andrew Zisserman", "Philip HS Torr"], "venue": "In British Machine Vision Conference,", "citeRegEx": "Mittal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mittal et al\\.", "year": 2011}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["Mahmood Sharif", "Sruti Bhagavatula", "Lujo Bauer", "Michael K Reiter"], "venue": "In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security,", "citeRegEx": "Sharif et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharif et al\\.", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Learning to play using low-complexity rule-based policies: Illustrations through Ms. Pac-Man", "author": ["Istv\u00e1n Szita", "Andr\u00e1s L\u0151rincz"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Szita and L\u0151rincz.,? \\Q2007\\E", "shortCiteRegEx": "Szita and L\u0151rincz.", "year": 2007}, {"title": "\u03b5-MDPs: Learning in varying environments", "author": ["Istv\u00e1n Szita", "B\u00e1lint Tak\u00e1cs", "Andr\u00e1s L\u0151rincz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Szita et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Szita et al\\.", "year": 2002}, {"title": "Features, configuration, and holistic face processing", "author": ["James W Tanaka", "Iris Gordon"], "venue": "The Oxford Handbook of Face Perception,", "citeRegEx": "Tanaka and Gordon.,? \\Q2011\\E", "shortCiteRegEx": "Tanaka and Gordon.", "year": 2011}, {"title": "The Cyber-Physical System approach towards artificial general intelligence", "author": ["Zolt\u00e1n T\u0151s\u00e9r", "Andr\u00e1s L\u0151rincz"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "T\u0151s\u00e9r and L\u0151rincz.,? \\Q2015\\E", "shortCiteRegEx": "T\u0151s\u00e9r and L\u0151rincz.", "year": 2015}, {"title": "Personalization of gaze direction estimation with deep learning. In Lecture Notes in Computer Science, volume 9904, chapter KI 2016", "author": ["Zolt\u00e1n T\u0151s\u00e9r", "R\u00f3bert A Rill", "Kinga Farag\u00f3", "L\u00e1szl\u00f3 A Jeni", "Andr\u00e1s L\u0151rincz"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "T\u0151s\u00e9r et al\\.,? \\Q2016\\E", "shortCiteRegEx": "T\u0151s\u00e9r et al\\.", "year": 2016}, {"title": "Convolutional pose machines", "author": ["Shih-En Wei", "Varun Ramakrishna", "Takeo Kanade", "Yaser Sheikh"], "venue": "arXiv preprint arXiv:1602.00134,", "citeRegEx": "Wei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Success stories demonstrate that superhuman performance can be reached this way (Schmidhuber, 2015).", "startOffset": 80, "endOffset": 99}, {"referenceID": 0, "context": "However, if we take a look at human information processing, for example, we learn that it has two basic routes: (i) holistic recognition (Tanaka & Gordon, 2011) and (ii) recognition by components (Biederman, 1987).", "startOffset": 196, "endOffset": 213}, {"referenceID": 0, "context": "However, if we take a look at human information processing, for example, we learn that it has two basic routes: (i) holistic recognition (Tanaka & Gordon, 2011) and (ii) recognition by components (Biederman, 1987). These processing methodologies are competing and also complementing each other. Deep learning methods, on the other hand, tend to favor endto-end learning, which corresponds to holistic recognition and are missing the advantages of the component based approach. Furthermore, holistic recognition and thus end-to-end learning is fragile. The fragility has been shown in a number of studies., e.g., (i) deep networks can be fooled as described by Nguyen et al. (2015), when inputs that differ enormously for a human observer from a given class are assigned the label of that class with extremely high confidence, or as demonstrated in Sharif et al.", "startOffset": 197, "endOffset": 681}, {"referenceID": 0, "context": "However, if we take a look at human information processing, for example, we learn that it has two basic routes: (i) holistic recognition (Tanaka & Gordon, 2011) and (ii) recognition by components (Biederman, 1987). These processing methodologies are competing and also complementing each other. Deep learning methods, on the other hand, tend to favor endto-end learning, which corresponds to holistic recognition and are missing the advantages of the component based approach. Furthermore, holistic recognition and thus end-to-end learning is fragile. The fragility has been shown in a number of studies., e.g., (i) deep networks can be fooled as described by Nguyen et al. (2015), when inputs that differ enormously for a human observer from a given class are assigned the label of that class with extremely high confidence, or as demonstrated in Sharif et al. (2016) showing that (ii) barely visible watermark-like modifications may change the class label, and that (iii) small additional components can be designed to change the class index to another desired one, making the network prone to attacks.", "startOffset": 197, "endOffset": 869}, {"referenceID": 2, "context": "Cand\u00e8s et al. (2011) showed that it is possible to augment this architecture with a term that collects and thus separates such components, which they call Robust Principal Component Analysis (RPCA).", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "We utilized the Inexact Augmented Lagrangian Multiplier (Lin et al., 2010) solver for this problem implemented in Python2.", "startOffset": 56, "endOffset": 74}, {"referenceID": 11, "context": "The `1,2 regularizer promotes The task of learning of such components has been tackled recently by L\u0151rincz et al. (2016). https://kastnerkyle.", "startOffset": 99, "endOffset": 121}, {"referenceID": 6, "context": ", the work of Fischer et al. (2015) and the references therein.", "startOffset": 14, "endOffset": 36}, {"referenceID": 17, "context": "Deep learning is thoroughly reviewed by Schmidhuber (2015). Introduction and details of the theory of the different networks can be found in the very recent book from Goodfellow et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 7, "context": "Introduction and details of the theory of the different networks can be found in the very recent book from Goodfellow et al. (2016). Consequently, we refer the interested reader to these excellent works and restrict this technical report to the collection of the references of the papers and the related software tools that we applied during the course of our work:", "startOffset": 107, "endOffset": 132}, {"referenceID": 17, "context": "Faster R-CNN6 (Ren et al., 2015) for object proposals; 2.", "startOffset": 14, "endOffset": 32}, {"referenceID": 3, "context": "Region-based Fully Convolutional Network7 (Dai et al., 2016) for object and hand detection; 3.", "startOffset": 42, "endOffset": 60}, {"referenceID": 26, "context": "Convolutional Pose Machine8 (Wei et al., 2016) for body pose detection (we also injected hand detection for improving the heat maps of this network); 4.", "startOffset": 28, "endOffset": 46}, {"referenceID": 25, "context": "Libfacetracker (T\u0151s\u00e9r et al., 2016) for face detection.", "startOffset": 15, "endOffset": 35}, {"referenceID": 9, "context": "MS COCO9 (Lin et al., 2014) for object detection; 2.", "startOffset": 9, "endOffset": 27}, {"referenceID": 5, "context": "PASCAL Visual Object Classes10 (Everingham et al., 2010) with twenty classes for training Faster R-CNN; 3.", "startOffset": 31, "endOffset": 56}, {"referenceID": 8, "context": "Visual Genome11 (Krishna et al., 2016) for changing backgrounds; 4.", "startOffset": 16, "endOffset": 38}, {"referenceID": 14, "context": "Mittal\u2019s Hand Dataset12 (Mittal et al., 2011) for hand detection; 5.", "startOffset": 24, "endOffset": 45}, {"referenceID": 15, "context": "Success stories on reinforcement learning using deep networks are already numerous, including Atari games (Mnih et al., 2015) and the game Go (Silver et al.", "startOffset": 106, "endOffset": 125}, {"referenceID": 20, "context": ", 2015) and the game Go (Silver et al., 2016).", "startOffset": 24, "endOffset": 45}, {"referenceID": 20, "context": "Such concepts have been formulated within the event learning framework for single episodic series Szita et al. (2002) and by Szita & L\u0151rincz (2007) for multiple events.", "startOffset": 98, "endOffset": 118}, {"referenceID": 20, "context": "Such concepts have been formulated within the event learning framework for single episodic series Szita et al. (2002) and by Szita & L\u0151rincz (2007) for multiple events.", "startOffset": 98, "endOffset": 148}, {"referenceID": 15, "context": "Success stories on reinforcement learning using deep networks are already numerous, including Atari games (Mnih et al., 2015) and the game Go (Silver et al., 2016). In turn, we conjecture that new applications built on deep learning technology based reinforcement learning, although technologically might be challenging regarding the sensory and the control systems, but may appear relatively fast as opposed to the view expressed in Study Panel (2016).", "startOffset": 107, "endOffset": 453}], "year": 2016, "abstractText": "Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning can make mistakes and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems. We show that in limited contexts the required number of training samples can be low and selfimprovement of pre-trained networks in more general context is possible. We argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods, an important requirement for engineering applications. We argue that supervised learning of labels may be fully eliminated under certain conditions: a component based architecture together with a knowledge based system can train itself and provide high quality answers. We demonstrate these concepts on the State Farm Distracted Driver Detection benchmark. We argue that the view of the Study Panel (2016) may overestimate the requirements on \u2018years of focused research\u2019 and \u2018careful, unique construction\u2019 for \u2018AI systems\u2019.", "creator": "LaTeX with hyperref package"}}}