{"id": "1612.00916", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2016", "title": "A Matrix Splitting Perspective on Planning with Options", "abstract": "hena We show that the Bellman festo operator flemings underlying the first-past-the-post options flypaper framework leads castellani to beckstrom a matrix splitting, sadikoglu an mattew approach centimetres traditionally used to 24.31 speed up convergence of iterative m\u00fcnter solvers for bo-105 large retama linear abubaker systems lissl of kahane equations. Based positivism on standard dist comparison theo - dofetilide rems for matrix i\u0307nan splittings, baptistry we then show how 630-nautical the 347th asymptotic previews rate gfsr of convergence elementos varies jnj as hwaseong a circumvented function butwal of d'urbervilles the inherent shared-use timescales 24.72 of the dorra options. This new sbinet perspective highlights a faton trade - 77a off leiferkus between asymptotic insititute performance and katiyar the scabies cost of computation associated with building srcs a good set csec of kivuitu options.", "histories": [["v1", "Sat, 3 Dec 2016 02:57:36 GMT  (12kb)", "https://arxiv.org/abs/1612.00916v1", "Accepted at the Continual Learning and Deep Networks Workshop, NIPS 2016"], ["v2", "Mon, 10 Jul 2017 19:28:32 GMT  (13kb)", "http://arxiv.org/abs/1612.00916v2", "The results presented in the previous version of this paper were found be applicable only to \"gating execution\" and not \"call-and-return\". We made this distinction clear in the text and added an extension to the call-and-return model"]], "COMMENTS": "Accepted at the Continual Learning and Deep Networks Workshop, NIPS 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["pierre-luc bacon", "doina precup"], "accepted": false, "id": "1612.00916"}, "pdf": {"name": "1612.00916.pdf", "metadata": {"source": "CRF", "title": "A Matrix Splitting Perspective on Planning with Options", "authors": [], "emails": ["dprecup}@cs.mcgill.ca", "@s"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 2.\n00 91\n6v 2\n[ cs\n.A I]\n1 0\nJu l 2\nWe show that the Bellman operator underlying the options framework leads to a matrix splitting, an approach traditionally used to speed up convergence of iterative solvers for large linear systems of equations. Based on standard comparison theorems for matrix splittings, we then show how the asymptotic rate of convergence varies as a function of the inherent timescales of the options. This new perspective highlights a trade-off between asymptotic performance and the cost of computation associated with building a good set of options."}, {"heading": "1 Introduction", "text": "The ability to represent and plan with temporally extended actions has long been recognized as an essential component for scaling up reinforcement learning systems. The options framework [Sutton et al., 1999; Precup, 2000] formalized this idea and showed how temporally extended actions can be used for learning and planning in reinforcement learning. Options can sometimes lead to better and faster exploration, learning, planning and transfer [Sutton et al., 1999] while being robust to model misspecifications and uncertainty [He et al., 2011]. While efficient algorithms for learning options have recently been proposed [Bacon et al., 2016; Mankowitz et al., 2016; Vezhnevets et al., 2016; Daniel et al., 2016], there is still no consensus on what constitute good options.\nIn this paper, we show that a choice of options is equivalent to a choice of iterative algorithm for solvingMarkov decision problems. We reach this conclusion by noting that the generalized Bellman operator underlying options and their models admits a linear representation as a matrix splitting [Varga, 1962; Young and Rheinboldt, 1971; Puterman, 1994], a notion which comes in pair with that of matrix preconditioning. As with options, the goal of these methods is to transform a linear system into one with the same solution but which is easier to solve. With this new perspective, the answer to what good options are becomes clearer while offering new theoretical tools to analyze them."}, {"heading": "2 Background and Notation", "text": "We restrict our attention to the class of discounted Markovian decision problems with finite state and action spaces. A discounted Markov Decision Process is specified by a finite set of states S, a finite set of actions A, a reward function r : S \u02c6 A \u00d1 R, a transition function P : S \u02c6 A \u00d1 pS \u00d1 r0, 1sq and a discount factor \u03b3 P r0, 1q. We write \u03c0 pa | sq to denote the probability of taking action a in state s under the stochastic policy \u03c0: that is \u0159 aPA \u03c0 pa | sq \u201c 1, @s P S. The value function v\u03c0 : S \u00d1 R represents the expected sum of discounted rewards encountered along the trajectories induced by the MDP and policy: v\u03c0psq 9\u201cE \u201c\u0159 k\u201c0 \u03b3 krpSk, Akq \u02c7\u030c S0 \u201c s, \u03c0 \u2030 . We also\nwrite r\u03c0psq 9\u201c \u0159 aPA \u03c0 pa | sq rps, aq and P\u03c0ps, s 1q 9\u201c \u0159 aPA \u03c0 pa | sqP ps\n1 | s, aq. The spectral radius of a n\u02c6 n matrix A with eigenvalues \u03bbi, i P r0, ns is \u03c1pAq 9\u201cmax1\u010fi\u010fn |\u03bbi|.\nAn option w P W is a triple pIw , \u03c0w, \u03b2wq where Iw \u010e S is the initiation set of w, \u03c0w is its policy, and \u03b2w : S \u00d1 r0, 1s is its termination function. In addition, there is a policy over options \u00b5 : S \u00d1 pW \u00d1 r0, 1sq whose role is to select an option whenever a termination event is sampled from the termination function of the current option. In the call-and-return model of execution, \u00b5 picks among tw P W : s P Iwu in state s and executes the policy of the selected option irrevocably until termination. We distinguish the call-and-return model from what we call gating execution, in which the choice of option is reconsidered at every step. The results in sections 3 and 4 apply only to the gating model. However, we also show in section 5 that the call-and-return execution model can be studied in the matrix splitting framework. For simplicity, we finally assume that options are available everywhere, that is : Iw \u201c S, @w P W ."}, {"heading": "3 Generalized Bellman Equations for Gating Execution", "text": "Planning with the value iteration algorithm over primitive actions usually involves a Bellman operator T of the form: pTvqpsq \u201c r\u03c0psq ` \u03b3 \u0159 s1PS P\u03c0ps, s\n1qvps1q. Rather than backing up values for only one step ahead, Sutton [1995] showed that multi-steps backups can equally be used for planning as long as the corresponding generalized Bellman operators satisfy the Bellman equations. We can extend this idea [Precup and Sutton, 1998] by making the number of Bellman backups a random variable which is determined by the termination events of an options-based process. Let K be a random variable representing the number of backups performed per iteration, we then define our generalized Bellman operator as follows:\npLvqpsq 9\u201cE\n\u00ab K\u00b41\u00ff\nk\u201c0\n\u03b3krpSk, Akq ` \u03b3 KvpSKq \u02c7\u030c \u02c7\u030c \u02c7 S0 \u201c s,W , \u00b5, v ff . (1)\nBy linearity and with Markov options, we can decompose L into a reward and a transition part. The reward model b : S \u00d1 R underlying L is the discounted sum of rewards until termination, averaged over all options:\nbpsq 9\u201cE\n\u00ab K\u00b41\u00ff\nk\u201c0\n\u03b3krpSk, Akq \u02c7\u030c \u02c7\u030c \u02c7 S0 \u201c s, v ff \u201c r\u03c3psq ` \u03b3 \u00ff\ns1\nP7ps, s 1qbps1q , (2)\nwhere \u03c3 and P7 are defined as follows: \u03c3 pa | sq 9\u201c \u00ff\nw\n\u00b5 pw | sq\u03c0w pa | sq , P7ps, s 1q 9\u201c\n\u00ff\nw\n\u00b5 pw | sq \u00ff\na\n\u03c0w pa | sqP ` s1 \u02c7\u030c s, a \u02d8 p1\u00b4 \u03b2wps 1qq .\nThe sharp 7 symbol here stands for \u201ccontinuation\u201d. Likewise, the transition model F : S \u00d1 pS \u00d1 r0, 1sq has the following recursive form:\nF ps, s1q 9\u201c\u03b3PKps, s 1q ` \u03b3\n\u00ff\ns\u0304\nP7ps, s\u0304qF ps\u0304, s 1q . (3)\nwhere PKps, s 1q 9\u201c \u0159 w \u00b5 pw | sq \u0159 a \u03c0w pa | sqP ps 1 | s, aq\u03b2wps 1q and K stands for \u201ctermination\u201d. The linear system of equations corresponding to the reward and transition models admits a solution given that the matrix I \u00b4 \u03b3P7 is nonsingular (which we prove below):\nb \u201c pI \u00b4 \u03b3P7q \u00b41r\u03c3, F \u201c pI \u00b4 \u03b3P7q \u00b41p\u03b3PKq . (4)\nThe generalized Bellman operator L then becomes:\nLv \u201c b` Fv \u201c pI \u00b4 \u03b3P7q \u00b41r\u03c3 ` \u03b3pI \u00b4 \u03b3P7q \u00b41PKv . (5)\nvan Nunen and Wessels [1976] showed that the basic iterative methods such as the Gauss-Seidel, Jacobi, Successive Overrelaxation, or Richardson\u2019s variants [Puterman, 1994] of value iteration can be obtained through different stopping time functions in an operator of the same form as (1), or equivalently, as a linear transformation of an MDP into an equivalent one. This perspective was leveraged by Porteus [1975] to derive better bounds by transforming an MDP into one which has the same optimal policy but whose spectral radius is smaller. The idea that stopping time functions (termination functions) lead to a transformation of an MDP is also what we just found by writing (1) as (5). Using Porteus\u2019 terminology, (5) in fact corresponds a pre-inverse transformation through the reward and transition models b and F . The pre-inverse transform as well as the basic iterative methods can also be studied more generally via the notion of matrix splittings [Varga, 1962] developed in the context of matrix iterative analysis."}, {"heading": "4 Matrix Splitting", "text": "For A \u201c M \u00b4 N P Rn\u02c6n and provided that A and M are nonsingular, Varga [1962] showed that the iterates xk`1 \u201c M \u00b41Nxk ` M \u00b41b converge to the unique solution of the linear system of equation Ax \u201c b. In the policy evaluation problem, we are working with the linear system of equations pI \u00b4 \u03b3P\u03c3qv \u201c r\u03c3 where \u03c3 is the target policy to evaluate. We now show that the reward and transition models (4) precisely corresponds to the notion of a matrix splitting for the matrix I \u00b4 \u03b3P\u03c3 .\nTheorem 1 (Matrix Splitting in the Gating Model). Let A 9\u201cI \u00b4 \u03b3P\u03c3, M 9\u201cI \u00b4 \u03b3P7 and N 9\u201c\u03b3PK, then A \u201c M \u00b4N is a regular splitting. Proof: M is an \u201cM-matrix\u201d (see chapter 6 of Berman and Plemmons [1979]). M-matrices have the property of being inverse-positive, that is M\u00b41 exists with M \u011b 0, fulfilling the definition of a regular splitting (see def. 3.5 of Varga [1962])\nCorollary 1. For the regular splitting of theorem 1,\n1. \u03c1p\u03b3pI \u00b4 \u03b3P7q \u00b41PKq \u0103 1\n2. The successive approximation method based on the generalized Bellman operator (5) converges for any initial vector v0.\nProof: This follows directly from the fact that options induce a regular splitting through the operator L in the gating model. See [Varga, 1962, Theorem 3.13] for a general proof.\nSince a set of options and the policy over them induce a matrix splitting, a choice of options is in fact a choice of algorithm for solving MDPs. An important property of an iterative solver, besides its computational efficiency, is that it should converge to a solution of the original problem. We should therefore ask ourselves whether the iterates corresponding to (5) converge to the true value function underlying a given target policy. Theorem 2 shows that the successive approximation method induced by a set of options and policy over them is consistent [Young and Rheinboldt, 1971] given that the marginal action probabilities is equal to the target policy.\nTheorem 2 (Consistency of Policy Evaluation in the Gating Model). The iterative method associated with the splitting (5)\nvk`1 \u201c pI \u00b4 \u03b3P7q \u00b41r\u03c3 ` \u03b3pI \u00b4 \u03b3P7q \u00b41PKvk, k \u011b 0\nis a consistent policy evaluation method in the gating model if the set of options and policy over them is such that \u03c3 pa | sq \u201c \u0159 w \u00b5 pw | sq\u03c0w pa | sq @a P A, s P S where \u03c3 is the target policy to be evaluated. Proof: Let vW,\u00b5 be the unique solution to the generalized Bellman equations (5), we have:\nvW,\u00b5 \u201c ` I \u00b4 \u03b3pI \u00b4 \u03b3P7q \u00b41PK \u02d8\u00b41 pI \u00b4 \u03b3P7q \u00b41r\u03c3\n\u201c ` pI \u00b4 \u03b3P7q \u00b41 ppI \u00b4 \u03b3P7q \u00b4 \u03b3PKq \u02d8\u00b41 pI \u00b4 \u03b3P7q \u00b41r\u03c3\n\u201c pI \u00b4 \u03b3P\u03c3q \u00b41 r\u03c3 \u201c v\u03c3\nTherefore vW,\u00b5 is also the solution to the policy evaluation problem for the policy \u03c3.\nWhile many set of options can satisfy the marginal condition in the gating model, not all of them would converge equally fast. Using comparison theorems for regular matrix splittings [Varga, 1962] we can better understand the effect of modelling the world at different timescales on the asymptotic performance of the induced algorithms.\nTheorem 3 (Predict further, plan faster). In the gating model, if a set of options \u0102W has the same intra-option policies and policy over options with some other setW but whose termination functions are such that \u03b2 rwpsq \u010f \u03b2wpsq @w P W , s P S, then 0 \u010f \u03c1p\u0102M\u00b41 rNq \u010f \u03c1pM\u00b41Nq \u0103 1. Proof: By theorem 1, the two sets of options induce a corresponding regular splitting. The claim then follows from [Varga, 1962, Theorem 3.32] since rN \u010f N (componentwise): rNps, s1q \u010f \u03b3 \u00ff\nw\n\u00b5 pw | sq \u00ff\na\n\u03c0w pa | sq \u00ff\ns1\nP ` s1 \u02c7\u030c s, a \u02d8 \u03b2wps 1q \u201c N .\nTheorem 3 consolidates the idea that modelling the world over longer time horizons increases the asymptotic rate of convergence. This also becomes apparent when writing (5) in the following form:\npI \u00b4 \u03b3P7qv \u201c pI \u00b4 \u03b3P7qv ` pr\u03c3 \u00b4 pI \u00b4 \u03b3P\u03c3qvq\nv \u201c v ` pI \u00b4 \u03b3P7q \u00b41pr\u03c3 \u00b4 pI \u00b4 \u03b3P\u03c3qvq (6)\nTherefore, options enter the linear system of equations pI\u00b4\u03b3P\u03c3qv \u201c r\u03c3 through the preconditioning matrixM [Saad, 2003] and yield the following transformed linear system of equations:\npI \u00b4 \u03b3P7q \u00b41pI \u00b4 \u03b3P\u03c3qv \u201c pI \u00b4 \u03b3P7q \u00b41r\u03c3 (7)\nAs the options timescales increase and \u03b2wpsq \u201c 0 @w P W , s P S, then P7 \u201c P\u03c3 and the solution is obtained directly on the right hand side of (7). The corresponding generalized Bellman operator also becomes Lp8qv 9\u201cpI \u00b4 \u03b3P\u03c3q \u00b41r\u03c3 and solves the original system in one iteration. On the other hand, if the termination functions are such that the options terminate after only one step, we get Lp0qv 9\u201cr\u03c3 ` \u03b3P\u03c3v, the usual one-step Bellman operator T of the value iteration algorithm. Since the spectral radius associated with matrix splitting methods is given by \u03c1pM\u00b41Nq, we also have the following: \u03c1pM\u00b41Nq \u201c \u03c1pM\u00b41pM \u00b4Aqq \u201c \u03c1 ` I \u00b4 pI \u00b4 \u03b3P7q \u00b41pI \u00b4 \u03b3P\u03c3q \u02d8 \u010f }I \u00b4 pI \u00b4 \u03b3P7q \u00b41pI \u00b4 \u03b3P\u03c3q} .\nThis suggests that in terms of asymptotic performances, a good set of options should be such that it induces a preconditioning matrixM that is close to I \u00b4 \u03b3P\u03c3 in some sense but whose inverseM \u00b41 is easier to compute."}, {"heading": "5 Call-and-Return Execution Model", "text": "The gating execution model assumed so far does not account for the notion of temporal commitment provided by the call-and-return model. Since a choice of option is made at every step in the gating model, the policy over option can be decoupled from the termination functions. This gives us the ability to express the value function as the solution of a matrix splitting over states. However, it is known [Sutton et al., 1999] that a set of options with call-and-return execution and an MDP induce a semi-Markov Decision Process (SMDP), even in the case of Markov options. This means that the trajectories over state and actions generated with options might no longer correspond to the dynamics of a Markov process. Hence, the existence of an equivalent marginal policy \u03c3pa|sq in theorem 2 cannot be guaranteed under the call-and-return model.\nTo restore the Markov property with call-and-return execution, the choice of option must be remembered as part of the state. The resulting process defines a Markov chain in an augmented state space over state-option pairs [Bacon et al., 2017]. This conditioning on both states and options is key to the derivation of the Bellman-like expressions of Sutton et al. [1999] for the reward and transition models of options. In the following, we show that the solution to these equations also yields a matrix splitting.\nSutton et al. [1999] showed that the reward model of an option can be written recursively as:\nbwpsq \u201c \u00ff\na\n\u03c0w pa | sq\n\u00ab rps, aq ` \u03b3 \u00ff\ns1\nP ` s1 \u02c7\u030c s, a \u02d8 \u03b2wps 1qbwps 1q ff .\nIf we define rwpsq 9\u201c \u0159 a \u03c0w pa | sq rps, aq and Pw,7,ps, s 1q 9\u201c \u0159 a \u03c0w pa | sqP ps 1 | s, aq p1 \u00b4 \u03b2wps 1q, we can see that the fixed point of these equations is :\nbw \u201c 8\u00ff\nt\u201c0\np\u03b3Pw,7q t rw \u201c pI \u00b4 \u03b3Pw,7q \u00b41 rw .\nSimilarly, the transition model of an option also admits a set of Bellman-like equations of the form:\nFwps, s 1q \u201c \u03b3\n\u00ff\na\n\u03c0w pa | sq \u00ff\ns\u0304\nP ps\u0304 | s, aq \u201c \u03b2wps\u0304q\u03b4s\u0304\u201cs1 ` p1 \u00b4 \u03b2wps\u0304qqFwps\u0304, s 1q \u2030 ,\nwhose fixed point can be written using the termination operator Pw,Kps, s 1q 9\u201c \u0159 a \u03c0w pa | sqP ps 1 | s, aq\u03b2wpsq:\nFw \u201c 8\u00ff\nt\u201c0\np\u03b3Pw,7q t p\u03b3Pw,Kq \u201c pI \u00b4 \u03b3Pw,7q \u00b41 p\u03b3Pw,Kq .\nTherefore, with Mw 9\u201cI \u00b4 \u03b3Pw,7 and Nw 9\u201c\u03b3Pw,K we haveMw \u00b4Nw \u201c I \u00b4 \u03b3P\u03c0w as a splitting."}, {"heading": "6 Implications", "text": "Given the interpretation of options in terms of matrix splittings, and consequently as preconditioners, it comes as no surprise that preconditioning methods share the same goals as options. Indeed in both cases we seek a representation of the problem which is easier to solve than the original one. As Herbert Simon wrote in his Sciences of the Artificial: \u201c[...] solving a problem simply means representing it so as to make the solution transparent\u201d [Simon, 1969]. Hence, the problem of finding good options or preconditioners is closely related to the representation learning problem [Minsky, 1961].\nAs with options, the design of general and fast preconditioners is a longstanding problem of numerical analysis. In some cases, good preconditioners can be found when problem-specific knowledge is available. However, manual design of preconditioners, and of options, quickly become a tedious process for large problems or when only partial knowledge about the domain is available. This is especially true in the context of reinforcement learning where the MDP is assumed to be unknown or too large to manipulate directly. When solving a single problemwith options, it is also clear from the connection with preconditioners that the initial setup cost and subsequent cost per preconditioned iteration should not outweigh the cost associated with the original problem. This leads to a fundamental tension between the improvement effort per iteration and the number of overall iterations. In our framework, this tension exists between the two poles Lp0q and Lp8q. While Lp8q has the fastest convergence, it also is just as expensive as solving the original problem directly. This is a reminder that modelling far comes with a cost: computational here but also statistical in the learning case. In fact, the choice of timescale for options also falls under the same bias-variance tradeoff as for the \u03bb-operator [Kearns and Singh, 2000] with Lp0q mirroring the choice \u03bb \u201c 0 and \u03bb \u201c 1 for Lp8q.\nComputational expenses associated with building preconditioners can be amortized throughout related problems. With options, this would corresponds to the typical case in which options are reused to speed up learning and planning in a transfer or continual learning setting. Reusability of options can take place for example when the transition structure remains fixed but the reward function on the right-hand side of (7) changes. The accounting exercise necessary to justify the use of options in a transfer setting was considered by Solway et al. [2014]. From a Bayesian perspective, the authors showed that a particular kind of bottleneck options is in fact optimal when a learning system must solve a series of related tasks. The idea of adapting the options structure to minimize the computational effort associated with solving a single task was also explored in Bacon and Precup [2015]. Using the bounded rationality framework, it was argued that options should primarily help computationally restricted adaptive systems: an idea which naturally fits with the preconditioning point of view."}], "references": [{"title": "Learning with options: Just deliberate and relax", "author": ["Pierre-Luc Bacon", "Doina Precup"], "venue": "In NIPS Bounded Optimality and Rational Metareasoning Workshop,", "citeRegEx": "Bacon and Precup.,? \\Q2015\\E", "shortCiteRegEx": "Bacon and Precup.", "year": 2015}, {"title": "The option-critic architecture", "author": ["Pierre-Luc Bacon", "Jean Harb", "Doina Precup"], "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February", "citeRegEx": "Bacon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2017}, {"title": "Nonnegative Matrices in the Mathematical Sciences", "author": ["A. Berman", "R.J. Plemmons"], "venue": null, "citeRegEx": "Berman and Plemmons.,? \\Q1979\\E", "shortCiteRegEx": "Berman and Plemmons.", "year": 1979}, {"title": "Probabilistic inference for determining options in reinforcement learning", "author": ["C. Daniel", "H. van Hoof", "J. Peters", "G. Neumann"], "venue": "Machine Learning, Special Issue,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Efficient planning under uncertainty with macroactions", "author": ["Ruijie He", "Emma Brunskill", "Nicholas Roy"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "He et al\\.,? \\Q2011\\E", "shortCiteRegEx": "He et al\\.", "year": 2011}, {"title": "Bias-variance error bounds for temporal difference updates", "author": ["Michael J. Kearns", "Satinder P. Singh"], "venue": "In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory,", "citeRegEx": "Kearns and Singh.,? \\Q2000\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2000}, {"title": "Adaptive skills, adaptive partitions (ASAP)", "author": ["Daniel J. Mankowitz", "Timothy Arthur Mann", "Shie Mannor"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mankowitz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mankowitz et al\\.", "year": 2016}, {"title": "Steps toward artificial intelligence", "author": ["Marvin Minsky"], "venue": "In Computers and Thought,", "citeRegEx": "Minsky.,? \\Q1961\\E", "shortCiteRegEx": "Minsky.", "year": 1961}, {"title": "Bounds and transformations for discounted finite markov decision chains", "author": ["Evan L. Porteus"], "venue": "Oper. Res.,", "citeRegEx": "Porteus.,? \\Q1975\\E", "shortCiteRegEx": "Porteus.", "year": 1975}, {"title": "Multi-time models for temporally abstract planning", "author": ["Doina Precup", "Richard S Sutton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Precup and Sutton.,? \\Q1998\\E", "shortCiteRegEx": "Precup and Sutton.", "year": 1998}, {"title": "Temporal abstraction in reinforcement learning", "author": ["Doina Precup"], "venue": "PhD thesis, University of Massachusetts,", "citeRegEx": "Precup.,? \\Q2000\\E", "shortCiteRegEx": "Precup.", "year": 2000}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["Martin L. Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q1994\\E", "shortCiteRegEx": "Puterman.", "year": 1994}, {"title": "Preconditioned Iterations, chapter 9, pages 261\u2013281", "author": ["Yousef Saad"], "venue": null, "citeRegEx": "Saad.,? \\Q2003\\E", "shortCiteRegEx": "Saad.", "year": 2003}, {"title": "The Sciences of the Artificial", "author": ["H.A. Simon"], "venue": "Karl Taylor Compton lectures. M.I.T. Press,", "citeRegEx": "Simon.,? \\Q1969\\E", "shortCiteRegEx": "Simon.", "year": 1969}, {"title": "Optimal behavioral hierarchy", "author": ["Alec Solway", "Carlos Diuk", "Natalia C\u00f3rdova", "Debbie Yee", "Andrew G. Barto", "Yael Niv", "Matthew M. Botvinick"], "venue": "PLoS Comput Biol,", "citeRegEx": "Solway et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Solway et al\\.", "year": 2014}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S. Sutton", "Doina Precup", "Satinder P. Singh"], "venue": "Artif. Intell.,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["Richard S. Sutton"], "venue": "In Machine Learning, Proceedings of the Twelfth International ConferenceVa on Machine Learning,", "citeRegEx": "Sutton.,? \\Q1995\\E", "shortCiteRegEx": "Sutton.", "year": 1995}, {"title": "Stopping times and markov programming", "author": ["J.A.E.E. van Nunen", "J. Wessels"], "venue": "Technical Report 76-22,", "citeRegEx": "Nunen and Wessels.,? \\Q1976\\E", "shortCiteRegEx": "Nunen and Wessels.", "year": 1976}, {"title": "Matrix iterative analysis", "author": ["Richard S. Varga"], "venue": "Prentice-Hall, Englewood Cliffs,", "citeRegEx": "Varga.,? \\Q1962\\E", "shortCiteRegEx": "Varga.", "year": 1962}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander (Sasha) Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Iterative solution of large linear systems", "author": [], "venue": null, "citeRegEx": "Rheinboldt.,? \\Q1971\\E", "shortCiteRegEx": "Rheinboldt.", "year": 1971}], "referenceMentions": [{"referenceID": 15, "context": "The options framework [Sutton et al., 1999; Precup, 2000] formalized this idea and showed how temporally extended actions can be used for learning and planning in reinforcement learning.", "startOffset": 22, "endOffset": 57}, {"referenceID": 10, "context": "The options framework [Sutton et al., 1999; Precup, 2000] formalized this idea and showed how temporally extended actions can be used for learning and planning in reinforcement learning.", "startOffset": 22, "endOffset": 57}, {"referenceID": 15, "context": "Options can sometimes lead to better and faster exploration, learning, planning and transfer [Sutton et al., 1999] while being robust to model misspecifications and uncertainty [He et al.", "startOffset": 93, "endOffset": 114}, {"referenceID": 4, "context": ", 1999] while being robust to model misspecifications and uncertainty [He et al., 2011].", "startOffset": 70, "endOffset": 87}, {"referenceID": 6, "context": "While efficient algorithms for learning options have recently been proposed [Bacon et al., 2016; Mankowitz et al., 2016; Vezhnevets et al., 2016; Daniel et al., 2016], there is still no consensus on what constitute good options.", "startOffset": 76, "endOffset": 166}, {"referenceID": 19, "context": "While efficient algorithms for learning options have recently been proposed [Bacon et al., 2016; Mankowitz et al., 2016; Vezhnevets et al., 2016; Daniel et al., 2016], there is still no consensus on what constitute good options.", "startOffset": 76, "endOffset": 166}, {"referenceID": 3, "context": "While efficient algorithms for learning options have recently been proposed [Bacon et al., 2016; Mankowitz et al., 2016; Vezhnevets et al., 2016; Daniel et al., 2016], there is still no consensus on what constitute good options.", "startOffset": 76, "endOffset": 166}, {"referenceID": 18, "context": "We reach this conclusion by noting that the generalized Bellman operator underlying options and their models admits a linear representation as a matrix splitting [Varga, 1962; Young and Rheinboldt, 1971; Puterman, 1994], a notion which comes in pair with that of matrix preconditioning.", "startOffset": 162, "endOffset": 219}, {"referenceID": 11, "context": "We reach this conclusion by noting that the generalized Bellman operator underlying options and their models admits a linear representation as a matrix splitting [Varga, 1962; Young and Rheinboldt, 1971; Puterman, 1994], a notion which comes in pair with that of matrix preconditioning.", "startOffset": 162, "endOffset": 219}, {"referenceID": 9, "context": "We can extend this idea [Precup and Sutton, 1998] by making the number of Bellman backups a random variable which is determined by the termination events of an options-based process.", "startOffset": 24, "endOffset": 49}, {"referenceID": 14, "context": "Rather than backing up values for only one step ahead, Sutton [1995] showed that multi-steps backups can equally be used for planning as long as the corresponding generalized Bellman operators satisfy the Bellman equations.", "startOffset": 55, "endOffset": 69}, {"referenceID": 11, "context": "van Nunen and Wessels [1976] showed that the basic iterative methods such as the Gauss-Seidel, Jacobi, Successive Overrelaxation, or Richardson\u2019s variants [Puterman, 1994] of value iteration can be obtained through different stopping time functions in an operator of the same form as (1), or equivalently, as a linear transformation of an MDP into an equivalent one.", "startOffset": 155, "endOffset": 171}, {"referenceID": 18, "context": "The pre-inverse transform as well as the basic iterative methods can also be studied more generally via the notion of matrix splittings [Varga, 1962] developed in the context of matrix iterative analysis.", "startOffset": 136, "endOffset": 149}, {"referenceID": 15, "context": "van Nunen and Wessels [1976] showed that the basic iterative methods such as the Gauss-Seidel, Jacobi, Successive Overrelaxation, or Richardson\u2019s variants [Puterman, 1994] of value iteration can be obtained through different stopping time functions in an operator of the same form as (1), or equivalently, as a linear transformation of an MDP into an equivalent one.", "startOffset": 4, "endOffset": 29}, {"referenceID": 8, "context": "This perspective was leveraged by Porteus [1975] to derive better bounds by transforming an MDP into one which has the same optimal policy but whose spectral radius is smaller.", "startOffset": 34, "endOffset": 49}, {"referenceID": 17, "context": "4 Matrix Splitting For A \u201c M  \u0301 N P R and provided that A and M are nonsingular, Varga [1962] showed that the iterates xk`1 \u201c M Nxk ` M b converge to the unique solution of the linear system of equation Ax \u201c b.", "startOffset": 81, "endOffset": 94}, {"referenceID": 2, "context": "Proof: M is an \u201cM-matrix\u201d (see chapter 6 of Berman and Plemmons [1979]).", "startOffset": 44, "endOffset": 71}, {"referenceID": 2, "context": "Proof: M is an \u201cM-matrix\u201d (see chapter 6 of Berman and Plemmons [1979]). M-matrices have the property of being inverse-positive, that is M exists with M \u011b 0, fulfilling the definition of a regular splitting (see def. 3.5 of Varga [1962]) Corollary 1.", "startOffset": 44, "endOffset": 237}, {"referenceID": 18, "context": "Using comparison theorems for regular matrix splittings [Varga, 1962] we can better understand the effect of modelling the world at different timescales on the asymptotic performance of the induced algorithms.", "startOffset": 56, "endOffset": 69}, {"referenceID": 12, "context": "This also becomes apparent when writing (5) in the following form: pI  \u0301 \u03b3P7qv \u201c pI  \u0301 \u03b3P7qv ` pr\u03c3  \u0301 pI  \u0301 \u03b3P\u03c3qvq v \u201c v ` pI  \u0301 \u03b3P7q pr\u03c3  \u0301 pI  \u0301 \u03b3P\u03c3qvq (6) Therefore, options enter the linear system of equations pI \u0301\u03b3P\u03c3qv \u201c r\u03c3 through the preconditioning matrixM [Saad, 2003] and yield the following transformed linear system of equations: pI  \u0301 \u03b3P7q pI  \u0301 \u03b3P\u03c3qv \u201c pI  \u0301 \u03b3P7q r\u03c3 (7) As the options timescales increase and \u03b2wpsq \u201c 0 @w P W , s P S, then P7 \u201c P\u03c3 and the solution is obtained directly on the right hand side of (7).", "startOffset": 265, "endOffset": 277}, {"referenceID": 15, "context": "However, it is known [Sutton et al., 1999] that a set of options with call-and-return execution and an MDP induce a semi-Markov Decision Process (SMDP), even in the case of Markov options.", "startOffset": 21, "endOffset": 42}, {"referenceID": 1, "context": "The resulting process defines a Markov chain in an augmented state space over state-option pairs [Bacon et al., 2017].", "startOffset": 97, "endOffset": 117}, {"referenceID": 1, "context": "The resulting process defines a Markov chain in an augmented state space over state-option pairs [Bacon et al., 2017]. This conditioning on both states and options is key to the derivation of the Bellman-like expressions of Sutton et al. [1999] for the reward and transition models of options.", "startOffset": 98, "endOffset": 245}, {"referenceID": 1, "context": "The resulting process defines a Markov chain in an augmented state space over state-option pairs [Bacon et al., 2017]. This conditioning on both states and options is key to the derivation of the Bellman-like expressions of Sutton et al. [1999] for the reward and transition models of options. In the following, we show that the solution to these equations also yields a matrix splitting. Sutton et al. [1999] showed that the reward model of an option can be written recursively as:", "startOffset": 98, "endOffset": 410}], "year": 2017, "abstractText": "We show that the Bellman operator underlying the options framework leads to a matrix splitting, an approach traditionally used to speed up convergence of iterative solvers for large linear systems of equations. Based on standard comparison theorems for matrix splittings, we then show how the asymptotic rate of convergence varies as a function of the inherent timescales of the options. This new perspective highlights a trade-off between asymptotic performance and the cost of computation associated with building a good set of options.", "creator": "LaTeX with hyperref package"}}}