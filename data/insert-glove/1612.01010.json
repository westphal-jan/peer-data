{"id": "1612.01010", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2016", "title": "DeepBach: a Steerable Model for Bach Chorales Generation", "abstract": "The brasseur composition of polyphonic chorale 2.01-meter music in the ratas style of foynes J. ninja S Bach has gopperth represented re-adopted a major midgrade challenge 52.93 in elfie automatic transition music 1,673 composition over youngest-ever the lambroughton last decades. dfc The sch\u00f6nhage art of Bach 160.1 chorales chechnya composition involves yushan combining four - part 110.01 harmony ervand with characteristic colic rhythmic patterns pomade and typical melodic fida movements to produce musical regreted phrases multirotor which nobert begin, thobias evolve stepanian and cheese-making end (1988/89 cadences) in dunseth a harmonious guyan way. To tehnika our waterhole knowledge, no x30 model so far ronggang was captives able to centcom solve 1.1933 all these problems simultaneously pha using 4,205 an agnostic gem machine - learning weiyi approach. changuinola This #ukqaqfqs paper introduces DeepBach, a pend statistical applecart model righthanded aimed ravensworth at pentateuch modeling jayawardene polyphonic 5:33 music and specifically four parts, hymn - delinquencies like \u03be pieces. sutrisno We claim interamericana that, after being trained on sheds the patting chorale 1040a harmonizations u.s.-controlled by skeksis Johann steinhauer Sebastian drafn Bach, mavuba our model is krbe capable of ted generating highly convincing hacienda chorales in anpp the xto style of Bach. naegle We bukovina evaluate how wrested indistinguishable our jiyong generated chorales bearings are bauchi from existing picuris Bach rebreathers chorales with nesterushkin a listening tetrachords test. tspm The alberoni results singapore corroborate wambach our claim. t\u014dt\u014dmi A sharmba key mulvaney strength of pathankot DeepBach 60-watt is that it is synodus agnostic blackshirts and stabilises flexible. Users ericsson can constrain the generation by kalana imposing some notes, akhlaq rhythms scooper or rmd cadences thyagarajan in 6,315 the generated ,8 score. This dictionary allows users to carbonyl reharmonize intersectionality user - lowu defined melodies. ihme DeepBach ' s ergui generation is ziketan fast, making antonine it usable pulpitis for quigo interactive run-scoring music composition applications. Several amidohydrolase generation glau examples 15,000-acre are fotyga provided and discussed lo-fi from a jeremih musical point exorcist of frescoed view.", "histories": [["v1", "Sat, 3 Dec 2016 19:17:29 GMT  (1459kb,D)", "http://arxiv.org/abs/1612.01010v1", "20 pages, 11 figures"], ["v2", "Sat, 17 Jun 2017 17:25:58 GMT  (2839kb,D)", "http://arxiv.org/abs/1612.01010v2", "10 pages, ICML2017 version"]], "COMMENTS": "20 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.AI cs.SD", "authors": ["ga\u00ebtan hadjeres", "fran\u00e7ois pachet", "frank nielsen"], "accepted": true, "id": "1612.01010"}, "pdf": {"name": "1612.01010.pdf", "metadata": {"source": "CRF", "title": "DeepBach: a Steerable Model for Bach chorales generation", "authors": ["Ga\u00ebtan Hadjeres", "Fran\u00e7ois Pachet"], "emails": ["gaetan.hadjeres@etu-upmc.fr", "pachetcsl@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "The corpus of the chorale harmonizations by Johann Sebastian Bach is remarkable by its homogeneity and its size (389 chorales in [5]). All these short pieces (approximately one minute long) are written for a four-part chorus (soprano, alto, tenor and bass) using similar compositional principles: the composer takes a well-known (at that time) melody from a Lutheran hymn and harmonizes it i.e. he composes the three lower parts (alto, tenor and bass) to be heard while the soprano (the highest part) sings the hymn, see Fig.1 for an example.\nMoreover, since the aim of reharmonizing a melody is to give more power or new insights to its text, the lyrics have to be understood clearly. We say that voices are in homophony, i.e. they articulate syllables at the same time. This implies characteristic rhythms, variety of harmonic ideas as well as characteristic\nar X\niv :1\n61 2.\n01 01\n0v 1\n[ cs\n.A I]\n3 D\nec 2\nmelodic movements which make the style of these chorale compositions easily distinguishable, even for non experts.\nThe difficulty, from a compositional point of view comes from the intricate interplay between harmony (notes sounding at the same time) and voice movements (how a single voice evolves through time). Furthermore, each voice has its own \u201cstyle\u201d and its own coherence. Finding a chorale-like reharmonization which combines Bach-like harmonic progressions with musically interesting melodic movements is a problem which often takes years of practice for musicians.\nFrom the point of view of automatic music generation , the first solution to this apparently highly combinatorial problem was proposed by [13] in 1988. This problem is seen as a constraint satisfaction problem, where the system must fulfill numerous hand-crafted constraints characterizing the style of Bach. It is a rule-based expert system which contains no less than 300 rules and tries to reharmonize a given melody with a generate-and-test method and intelligent backtracking. Among the short examples presented at the end of the paper, some are flawless. Drawbacks of this method are, as stated by the author, the considerable effort to generate the rule base and the fact that the harmonizations produced \u201cdo not sound like Bach, except for occasional Bachian patterns and cadence formulas\u201d. In our opinion, the requirement of an expert knowledge implies a lot of arbitrary choices. Furthermore, we have no idea about the variety and originality of the proposed solutions.\nA neural-network-based solution was later developed by [17]. This method relies on several neural networks, each one trained for solving a specific task: a harmonic skeleton is first computed then refined and ornamented. A similar approach is adopted in [3], but uses Hidden Markov Models (HMMs) instead\n3 https://www.youtube.com/watch?v=73WF0M99vlg\n3 of neural networks. Chords are represented as lists of intervals and form the states of the Markov models. These approaches produce interesting results even if they both use expert knowledge and bias the generation by imposing their compositional process. In [29,28], authors elaborate on those methods by introducing multiple viewpoints and variations on the sampling method (generated sequences which violate \u201crules of harmony\u201d are put aside for instance). However, this approach do not produce a convincing chorale-like texture, rhythmically as well as harmonically and the resort to hand-crafted criteria to assess the quality of the generated sequences might rule out many musically-interesting solutions.\nRecently, agnostic approaches (requiring no knowledge about harmony, or the music by Bach) using neural networks have been investigated with promising results. In [8], chords are modeled with Restricted Boltzmann Machines (RBMs). Their temporal dependencies are learned using Recurrent Neural Networks (RNNs). Variations of these architectures have been developed, based on Long Short-Term Memory (LSTM) units [23] or GRUs (Gated Recurrent Units) [10]. These models, which work on piano roll representations of the music, are in our opinion too general to capture the specificity of Bach chorales. But one of their main drawback is their lack of flexibility. Generation is performed from left to right. A user cannot interact with the system: it is impossible to do reharmonization for instance which is the essentially how the corpus of Bach chorales was composed. Moreover, their invention capacity and non-plagiarism abilities are not demonstrated.\nThe most recent advances in chorale harmonization is arguably the BachBot model [22], a LSTM-based approach specifically designed to deal with Bach chorales. This approach relies on little musical knowledge (all chorales are transposed in a common key) and is able to produce high-quality chorale harmonizations. However, compared to our approach, this model is less general (produced chorales are all in the C key for instance) and less flexible (only the soprano can be fixed). Similarly to and independently of our work, the authors evaluate their model with an online Turing test to assess the efficiency of their model with promising results. They also take into account the fermata symbols (Fig. 2) which are indicators of the structure of the chorales.\nIn this paper we introduce DeepBach, a LSTM-based model capable of producing musically-appealing four-part chorales in the style of Bach. Contrary to other models based on RNNs, we do not sample from left to right and model each voice separately. This allows us to enforce user-defined constraints such as rhythm, notes, parts, chords and cadences. DeepBach is able to produce coherent musical phrases and provides, for instance, varied reharmonizations of melodies without plagiarism. Its core features are its reliance upon no knowledge, its speed, the possible interaction with users and the richness of harmonic ideas it proposes. Its efficiency opens up new ways of creating interesting Bach-like chorales for non experts similarly to what is proposed in [26] for leadsheets.\nIn Sect. 2 we present the DeepBach architecture for four-part chorale generation.\n4 We discuss in Sect. 3 the results of two experimental studies we conducted to assess the quality of our model. Finally, we provide several annotated examples in Sect. 4. All examples can be heard on the accompanying web page4 and the code of our implementation is available on GitHub5."}, {"heading": "2 DeepBach", "text": "In this paper we introduce a new generative model which takes into account the distinction between voices. Sect. 2.1 indicates how we preprocessed the corpus of Bach chorale harmonizations and Sect. 2.2 presents the model\u2019s architecture."}, {"heading": "2.1 Data Representation", "text": "We represent a chorale as a tuple of six lists\n(V1,V2,V3,V4,S,F), (1)\nwhere the Vi\u2019s represent the four voices (soprano, alto, tenor and bass) to which we add two other lists: S the list of subdivisions and F the list of fermatas. All lists are indexed by a time index t and have equal size.\nSince Bach chorales contains only simple time signatures, we discretize time with sixteenth notes, which means that each beat is subdivided into four equal parts. Since there is no smaller subdivision in Bach chorales, there is no loss of information in this process.\nEach voice Vi contains the midi pitch of the played notes. It is a unique integer for each note, with no distinction between enharmonic equivalent notes. In order to represent rhythm in a compact way, we introduce an additional symbol to the pitches coding whether or not the preceding note is held. The subdivision list S contains the subdivision indexes of the beat. It is an integer between 1 and 4: there is no distinction between beats in a bar so that our model is able to deal with chorales with three and four beats per measure. The fermata list F indicates if there is a fermata symbol, see Fig. 2, over the current note, it is a Boolean value. If a fermata is placed over a note on the music sheet, we consider that it is active for all time indexes within the duration of the note.\nFig. 2: A fermata symbol\nOur choices are very general and do not involve expert knowledge about harmony or scales but are only mere observations of the corpus. The list S acts 4 http://www.flow-machines.com/deepbach-steerable-model-bach-chorales-\ngeneration/ 5 https://github.com/SonyCSL-Paris/DeepBach\nas a metronome. The list F is added since fermatas in Bach chorales indicate the end of each musical phrase. The use of fermata to this end is a specificity of Bach chorales that we want to take advantage of. Part 4 shows that this representation makes our model able to create convincing musical phrases in triple and quadruple simple time signatures."}, {"heading": "2.2 Model Architecture", "text": "For clarity, we suppose in this section that our dataset is composed of only one chorale written as in Eq. 1. We introduce a family of probabilistic models p parametrized by a parameter \u03b8 on our representation defined in Sect. 2.1. We do not model probabilistically the sequences S nor F but consider them fixed. The negative log-likelihood of our data is thus defined by\n\u2212 log p(V1,V2,V3,V4|S,F , \u03b8). (2)\nWe need to find a parameter \u03b8 which minimizes this loss. In order to have a computationally tractable training criterion, we introduce the pseudolikelihood of our data [7,4]. This approach was successful in many real-life problems [14] and consists in an approximation of the negative log-likelihood function by the sum over all variables:\n\u2212 \u2211 i (\u2211 t log p(Vti |V\\i,t,S,F , \u03b8) ) , (3)\nwhere Vti indicates the pitch of voice i at time index t and V\\i,t the union of all Vi\u2019s except from the variable Vti . This suggests to introduce four probabilistic models pi depending on parameter \u03b8i, one for each voice, and to minimize their negative log-likelihood independently using the pseudolikelihood criterion. We obtain four problems of the form:\u2211\nt\nlog pi(Vti |V\\i,t,S,F , \u03b8i), for i \u2208 [4]6. (4)\n6 We adopt the standard notation [N ] to denote the set of integers {1, . . . , N} for any integer N .\n6 The advantage with this formulation is that each model has to make predictions within a small range of integer values whose ranges correspond to the usual voice ranges.\nThe aim of these models is to predict the pitch of one note knowing the value of its neighboring notes, the subdivision of the beat it is on and the presence of fermatas. We implement them using neural networks based on LSTMs [18,24]. For accurate predictions, we choose to use four neural networks: two stacks of LSTMs, one summing up past information and another summing up information coming from the future together with a non-recurrent neural network for notes occurring at the same time. Their outputs are merged and passed as the input of a fourth neural network whose output is pi(Vti |V\\i,t,S,F , \u03b8). Figure 4a shows a graphical representation for one of these models. Details are provided in Sect. 2.4.\n7"}, {"heading": "2.3 Generation", "text": "Generation is performed using Gibbs sampling [15]. In our case, this consists in the following algorithm:\nAlgorithm 1 Gibbs sampling Require: Chorale length L, lists S and F of length L, probability distributions (p1, p2, p3, p4), maximum number of iterations M\n1: Create four lists (V1,V2,V3,V4) of length L . The lists are often initialized with random values drawn from the ranges of the\ncorresponding voices 2: for m from 1 to M do 3: Choose voice i uniformly between 1 and 4 4: Choose time t uniformly between 1 and L 5: Re-sample Vti from pi(Vti |V\\i,t,S,F , \u03b8i) 6: end for\nreturn (V1,V2,V3,V4)\nThe advantage of this method is that we can enforce user-defined constraints by tweaking Alg. 1:\n\u2013 instead of choosing voice i from 1 to 4 we can choose to fix the soprano and only resample voices from 2, 3 and 4 in step (3) in order to provide reharmonizations of the fixed melody \u2013 we can choose the fermata list F in order to impose end of musical phrases at some places \u2013 for any t and any i, we can fix specific ranges Rti , subsets of the range of voice i, to restrict ourselves to some specific chorales by re-sampling Vti from\npi(Vti |V\\i,t,S,F , \u03b8i,Vti \u2208 Rti)\nat step (5). This allows us for instance to fix rhythm (since the hold symbol is pitch), impose some chords in a soft manner or restrict the vocal ranges.\nNote that it is possible to make generation faster by making parallel Gibbs updates on GPU. Steps (3) to (5) from Alg. 1 can be run simultaneously to provide significant speedups. In Table 1 we show how the batch size (fixed number of parallel updates) influences the number of updates per second. Even if it known that this approach is biased [12] (since we can update simultaneously variables which are not conditionally independent), we experimentally observed that for small batch sizes (16 or 32), DeepBach still generates samples of great musicality while running ten times faster than the sequential version. This allows DeepBach to generate chorales in a few seconds.\nIt is also possible to use the hard-disk-configurations generation algorithm (Alg.2.9 in [20]) to appropriately choose all the time indexes at which we parallelly resample so that:\n\u2013 every time index is at distance at least \u03b4 from the other time indexes\n\u2013 configurations of time indexes satisfying the relation above are equally sampled.\nThis trick allows to assert that we do not update simultaneously a variable and its local context."}, {"heading": "2.4 Implementation Details", "text": "We implemented DeepBach using Keras [9] with the Tensorflow [1] backend. We used the database of chorale harmonizations by J.S. Bach included in the music21 [11] toolkit. After removing chorales with instrumental parts and chorales containing parts with two simultaneous notes (bass parts sometimes divide for the last chord), we ended up with 352 pieces. Contrary to other approaches which transpose all chorales to the same key (usually in C major or A minor), we choose to augment our dataset by adding all chorale transpositions which fit within the vocal ranges defined by the initial corpus. This gives us a corpus of 2503 chorales and split it between a training set (80%) and a validation set (20%) . The vocal ranges contains less than 30 different pitches for each voice (21, 21, 21, 28) for the soprano, alto, tenor and bass parts respectively.\nAs shown in Fig. 4, we model only local interactions between a note Vti and its context (V\\i,t, S , F) i.e. only elements with time index t between t\u2212\u2206t and t+\u2206t are taken as inputs of our model for some scope \u2206t.\nThe reported results, Sect. 3, and examples Sect. 4 were obtained with \u2206t = 16. We chose as the \u201cneural network brick\u201d in Fig. 4a a neural network with one hidden layer of size 200 and ReLU [25] nonlinearity and as the \u201cStacked LSTMs brick\u201d two LSTMs on top of each other, each one being of size 200 (see Fig. 2 (f) in [21]). We experimentally found that adding dropout or sharing weights between the embedding layers improved neither validation accuracy nor the musical quality of our generated chorales."}, {"heading": "3 Experimental Results", "text": "We now evaluate the quality of our model with two experiments: an online test conducted on human listeners and an analysis of the plagiarism in chorales generated by DeepBach.\n9"}, {"heading": "3.1 Listening Test", "text": "The online listening test consists in a perception test and discrimination test. For the parameters used in our experiments, see Sect 2.4. We compared our model with two other models: a Maximum Entropy model (MaxEnt) as in [16] (Fig. 4b) and a Multilayer Perceptron (MLP) model (Fig. 4c).\nThe Maximum Entropy model is a neural network with no hidden layer. It is given by:\npi(Vti |V\\i,t,S,F , Ai, bi) = Softmax(AX + b) (5)\nwhere X is a vector containing the elements in V\\i,t \u222a St \u222a Ft, Ai a (ni,mi) matrix and bi a vector of size mi with mi being the size of X, ni the number of pitches in the voice range i and Softmax the softmax function [30] given by\nSoftmax(z)j = ezj\u2211K k=1 e zk for j \u2208 [K],\nfor a vector z = (z1, . . . , zK). The Multilayer Perceptron model we chose takes as input elements in V\\i,t \u222a S \u222a F , is a neural network with one hidden layer of size 500 and uses a ReLU [25] nonlinearity.\nAll models are local and have the same scope \u2206t, see Sect. 2.4. Subjects were asked to give information about their musical expertise. They\ncould choose what category fits them best between:\n1. I seldom listen to classical music 2. Music lover or musician 3. Student in music composition or professional musician.\nThe musical extracts have been obtained by reharmonizing 50 chorales from the validation test by each of the three models (MaxEnt, MLP, DeepBach). We rendered the MIDI files using the Leeds Town Hall Organ soundfont7 and cut two extracts of 12 seconds from each chorale, which gives us 400 musical extracts for our test: 4 versions for each of the 100 melody chunks. We chose our rendering so that the generated parts (alto, tenor and bass) can be distinctly heard and differentiated from the soprano part (which is fixed and identical for all models): in our mix, dissonances are easily heard, the velocity is the same for all notes as in a real organ performance and the sound does not decay, which is important when evaluating the reharmonization of long notes.\nPerception Test In a first part, subjects were presented ten series of two reharmonizations of the same chorale melody and were asked \u201cwhich one sounds more like Bach to your ears\u201d. In order to give a general ranking from these binary confrontations, we used the Bradley-Terry model [27,2] to infer potentials \u03b2j\n7 https://www.samplephonics.com/products/free/sampler-instruments/the-\nleeds-town-hall-organ\n10\nreflecting the probability that the version j is better than another version. This is expressed as:\nP (version i is better than version j) = e\u03b2i\ne\u03b2i + e\u03b2j . (6)\nResults are plotted in Fig. 5. 1609 people took this test, 395 with musical expertise 1, 792 with musical\nexpertise 2 and 422 with musical expertise 3.\nExtracts generated from DeepBach are clearly recognized as being more Bach-like than the other models. The more musical expertise subjects have, the clearer is the distinction.\nDiscrimination Test: \u201cBach or Computer\u201d experiment In a second part, subjects were presented series of only one musical extract together with the binary choice \u201cBach\u201d or \u201cComputer\u201d8. Fig. 6 shows how the votes are distributed depending on the level of musical expertise of the subjects for each model. For this experiment, 1272 people took this test, 261 with musical expertise 1, 646 with musical expertise 2 and 365 with musical expertise 3.\nThe results are quite clear: the percentage of \u201cBach\u201d votes augment as the model\u2019s complexity increase. Furthermore, the distinction between computergenerated extracts and Bach\u2019s extracts is more accurate when the level of musical expertise is higher. When presented a DeepBach-generated extract, around 50% of the voters would judge it as composed by Bach. We consider this to be a good score knowing the complexity of Bach\u2019s compositions and the facility to detect badly-sounding chords even for non musicians.\nWe also plotted specific results for each of the 400 extracts. Fig. 7 shows for each reharmonization extract the percentage of Bach votes it collected: more than half of the DeepBach\u2019s automatically-composed extracts has a majority of\n8 This test is available at http://www.flow-machines.com:3010\n11\nvotes considering them as being composed by J.S. Bach while it is only a third for the MLP model."}, {"heading": "3.2 Plagiarism Analysis", "text": "We now evaluate the creativity and originality of DeepBach\u2019s productions. We use as a measure of plagiarism, for a given chorale, the length of the longest chorale subsequence which can be found identically in our training set. Fig. 8 shows histograms of this quantity for three different cases:\n\u2013 50 original (non transposed) J.S. Bach chorales from the test set \u2013 50 chorales (of the same length as the ones above) generated by DeepBach\nwithout constraints \u2013 50 reharmonizations by DeepBach where the soprano is constrained on the\nchorale melodies from the same 50 J.S. Bach chorales as above\nFor each case, we plot the length of the longest subsequence when considering only a given voice (\u201cSoprano\u201d, \u201cAlto\u201d, \u201cTenor\u201d and \u201cBass\u201d rows) and for all voices altogether (\u201cAll\u201d row).\nThe results show that DeepBach is not prone to plagiarize both in the unconstrained generation and in the reharmonization cases. Indeed, when considering voices taken separately, we see that the distribution of the lengths of the longest plagiarized subsequence peaks around 5 or 6 beats. This can be compared with the distributions obtained on our J.S. Bach test set: chorales from this test set tend to be more \u201cplagiaristic\u201d, with a higher mean length for the longest copied subsequences. This \u201cself-plagiarism\u201d is in fact characteristic of the style of the J.S. Bach chorales, with many typical movements, or cadences\n12\nrepeated exactly (up to transposition since we added all valid transpositions to our training dataset).\nThe peculiar histogram for the soprano voice for the J.S. Bach test set can be explained with two factors:\n\u2013 the extreme values are due to the fact that J.S. Bach reharmonized some chorale melodies several times. This results in the presence of long copied subsequences. \u2013 the central values are due to the particular combinatorics of the soprano voice. As chorale melodies are extracted from Lutheran hymns, their rhythm as well as the large use of step motions make them more prone to share common subsequences.\nWhen looking to all voices simultaneously, we see that DeepBach does not suffer from plagiarism with copied sequences of small maximum size (around 2 beats). Even during reharmonization, we see that DeepBach is original enough so that only small subsequences (chord transitions) are cited verbatim in the generated chorales. This enables DeepBach to propose interesting and different reharmonization ideas of the same melody (see Fig. 10 and 11)."}, {"heading": "4 Commented examples", "text": "We now provide three complete chorale reharmonizations composed by DeepBach. One is a reharmonization of a chorale melody used by Bach (see Fig. 1)\n13\n14\nWer nur den lieben Gott l\u00e4sst walten\nharmonization generated using DeepBach\n15\nAndante q = 72\n6\n11\nGa\u00ebtan Hadjeres\nGod Save the Queen Traditional, harmonization generated using DeepBach\nFig. 10: A reharmonization of \u201cGod Save the Queen\u201d by DeepBach. See Sect. 4 for comments on the annotations.\n16\nAndante q = 80\n5\n11\nGod Save the Queen\nGa\u00ebtan Hadjeres\nTraditional, harmonization generated using DeepBach\nFig. 11: A second reharmonization of \u201cGod Save the Queen\u201d by DeepBach. See Sect. 4 for comments on the annotations.\n17\nwhile the other two are different reharmonizations of the traditional hymn \u201cGod Save the Queen\u201d(see Fig. 10 and 11). These examples demonstrate the ability of DeepBach to learn and generate characteristic elements of J.S. Bach chorales while reharmonizing. To make our claim clearer, we highlighted particular aspects on the music sheets using three different colors:\n\u2013 in green, we indicated: \u2022 characteristic melodic movements: \u2217 Fig 9 bars 1, 3, 6, 7, 9, 14 \u2217 Fig 10 bars 13-14 \u2217 Fig 11 bars 5, 13-14\n\u2022 good voicings and voice leading: \u2217 Fig 9 bars 2, 11 \u2217 Fig 10 bars 2, 9 \u2217 Fig 11 bars 2, 4, 13 \u2022 characteristic suspensions9 and passing tones: \u2217 Fig 9 bars 4, 8, 8-9, 14 \u2217 Fig 10 bars 4, 13 \u2217 Fig 11 bar 4\n\u2013 in blue: \u2022 musically interesting ideas: \u2217 Fig 9: \u00b7 Starting on a dominant bar 1 \u00b7 Chromatic neighboring tone on the second degree bars 1, 13 \u00b7 Two different harmonizations between bars 1 and 8 \u00b7 Harmonization in A major bars 5-6 \u00b7 Bass in eighth notes bars 11-13 \u00b7 Cadence bar 12\n\u2217 Fig 10: \u00b7 Starting in E minor bar 1 \u00b7 Harmonization in G minor bar 5 \u00b7 Chromatic line bars 11-12 \u00b7 Proximity between F and F# bars 11-12 \u2217 Fig 11: \u00b7 Dominant of the sixth degree \u00b7 Minorization after a half cadence bars 6-7 \u00b7 Considering G at soprano to be an escape tone\n\u2013 in red : \u2022 parallel fifths and octaves indicated by lines \u2022 mistakes: \u2217 Fig 9: \u00b7 D missing bar 4\n9 For explanations for technical musical terms see https://en.wikipedia.org/wiki/Nonchord tone\n18\n\u00b7 C should resolve to B bar 9 \u2217 Fig 10: \u00b7 E should be removed in order to prevent parallel fifths bar 1 \u00b7 Seventh chord cannot be played without preparation bar 9 \u00b7 Repetition in eighth notes bar 11\n\u2217 Fig 11: \u00b7 Starting on an inverted chord of the first degree bar 1 \u00b7 Strange resolution for 9-8 suspension bar 10 \u00b7 Melodic movement is not Bach-like bar 11 (but it is imposed by\nthe user and not generated by DeepBach)\nDespite some compositional errors like parallel octaves, the musical analysis reveals that the DeepBach compositions reproduce typical Bach-like patterns, from characteristic cadences to the expressive use of nonchord tones. Furthermore, our model is able to propose varied and contrasting ideas when reharmonizing the same melody as can be seen by comparing the two versions of \u201cGod Save the Queen\u201d."}, {"heading": "5 Discussion and future work", "text": "We described DeepBach, a probabilistic model together with a sampling method which is flexible, efficient and provides musically convincing results even to the ears of professionals. The strength of our method, to our point of view, is the possibility to let users impose unary constraints, which is a feature often neglected in probabilistic models of music. We showed that DeepBach do not suffer from plagiarism while reproducing J.S. Bach\u2019s style and can be used to generate musically convincing harmonizations. We now plan to develop a music sheet graphical editor on top of the music21 toolkit in order to make interactive composition using DeepBach easier. This method is not only applicable to Bach chorales but embraces a wide range of polyphonic chorale music, from Palestrina to Take 6."}, {"heading": "Acknowledgment", "text": "We thank Emmanuel Deruty for the audio rendering part, Jason Sakellariou for the fruitful discussions we had and Frank Nielsen for his helpful comments.\nThis research is conducted within the Flow Machines project which received funding from the European Research Council under the European Unions Seventh Framework Programme (FP/2007-2013) / ERC Grant Agreement n. 291156.\n19"}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems (2015), http://tensorflow.org/, software available from tensorflow.org", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Categorical Data Analysis, pp. 206\u2013208", "author": ["A. Agresti", "M. Kateri"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Harmonising chorales by probabilistic inference", "author": ["M. Allan", "C.K. Williams"], "venue": "Advances in neural information processing systems 17, 25\u201332", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Pseudolikelihood estimation: some examples", "author": ["B.C. Arnold", "D. Strauss"], "venue": "Sankhy\u0101: The Indian Journal of Statistics, Series B pp. 233\u2013243", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "Chorales (Choral-Gesange): SATB (German Language Edition)", "author": ["J. Bach"], "venue": "Kalmus Classic Edition, Alfred Publishing Company", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1985}, {"title": "Music in Theory and Practice Volume 1", "author": ["B. Benward"], "venue": "McGraw-Hill Higher Education", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical analysis of non-lattice data", "author": ["J. Besag"], "venue": "The statistician pp. 179\u2013195", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1975}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-lewandowski", "Y. Bengio", "P. Vincent"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12). pp. 1159\u20131166", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "music21: A toolkit for computer-aided musicology and symbolic music data", "author": ["M.S. Cuthbert", "C. Ariza"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Ensuring rapid mixing and low bias for asynchronous gibbs sampling", "author": ["C. De Sa", "K. Olukotun", "C. R\u00e9"], "venue": "arXiv preprint arXiv:1602.07415", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "An expert system for harmonizing four-part chorales", "author": ["K. Ebcioglu"], "venue": "Computer Music Journal 12(3),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1988}, {"title": "Improved contact prediction in proteins: using pseudolikelihoods to infer potts models", "author": ["M. Ekeberg", "C. L\u00f6vkvist", "Y. Lan", "M. Weigt", "E. Aurell"], "venue": "Physical Review E 87(1), 012707", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on pattern analysis and machine intelligence (6), 721\u2013741", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1984}, {"title": "Style imitation and chord invention in polyphonic music with exponential families", "author": ["G. Hadjeres", "J. Sakellariou", "F. Pachet"], "venue": "arXiv preprint arXiv:1609.05152", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Harmonet: A neural net for harmonizing chorales in the style of js bach", "author": ["H. Hild", "J. Feulner", "W. Menzel"], "venue": "Advances in neural information processing systems. pp. 267\u2013274", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8), 1735\u20131780", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Trait\u00e9 de l\u2019harmonie: en 3 volumes, vol", "author": ["C. Koechlin"], "venue": "1. Eschig", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1928}, {"title": "Statistical Mechanics: Algorithms and Computations. Oxford Master Series in Physics, Oxford University Press, UK (2006), https://books.google", "author": ["W. Krauth"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition", "author": ["X. Li", "X. Wu"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 4520\u20134524. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Bachbot", "author": ["F. Liang"], "venue": "https://github.com/feynmanliang/bachbot", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Modelling high-dimensional sequences with lstm-rtrbm: application to polyphonic music generation", "author": ["Q. Lyu", "Z. Wu", "J. Zhu", "H. Meng"], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence. pp. 4138\u20134139. AAAI Press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning longer memory in recurrent neural networks", "author": ["T. Mikolov", "A. Joulin", "S. Chopra", "M. Mathieu", "M. Ranzato"], "venue": "arXiv preprint arXiv:1412.7753", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10). pp. 807\u2013814", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Assisted Lead Sheet Composition Using FlowComposer, pp. 769\u2013785", "author": ["A. Papadopoulos", "P. Roy", "F. Pachet"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "M.E.T.: Rank analysis of incomplete block designs: I. the method of paired comparisons", "author": ["Ralph Allan Bradley"], "venue": "Biometrika 39(3/4),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1952}, {"title": "Music generation from statistical models of harmony", "author": ["R.P. Whorley", "D. Conklin"], "venue": "Journal of New Music Research 45(2),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Multiple viewpoint systems: Time complexity and the construction of domains for complex musical viewpoints in the harmonization problem", "author": ["R.P. Whorley", "G.A. Wiggins", "C. Rhodes", "M.T. Pearce"], "venue": "Journal of New Music Research 42(3), 237\u2013 266", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "The corpus of the chorale harmonizations by Johann Sebastian Bach is remarkable by its homogeneity and its size (389 chorales in [5]).", "startOffset": 129, "endOffset": 132}, {"referenceID": 12, "context": "From the point of view of automatic music generation , the first solution to this apparently highly combinatorial problem was proposed by [13] in 1988.", "startOffset": 138, "endOffset": 142}, {"referenceID": 16, "context": "A neural-network-based solution was later developed by [17].", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": "A similar approach is adopted in [3], but uses Hidden Markov Models (HMMs) instead", "startOffset": 33, "endOffset": 36}, {"referenceID": 28, "context": "In [29,28], authors elaborate on those methods by introducing multiple viewpoints and variations on the sampling method (generated sequences which violate \u201crules of harmony\u201d are put aside for instance).", "startOffset": 3, "endOffset": 10}, {"referenceID": 27, "context": "In [29,28], authors elaborate on those methods by introducing multiple viewpoints and variations on the sampling method (generated sequences which violate \u201crules of harmony\u201d are put aside for instance).", "startOffset": 3, "endOffset": 10}, {"referenceID": 7, "context": "In [8], chords are modeled with Restricted Boltzmann Machines (RBMs).", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": "Variations of these architectures have been developed, based on Long Short-Term Memory (LSTM) units [23] or GRUs (Gated Recurrent Units) [10].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "Variations of these architectures have been developed, based on Long Short-Term Memory (LSTM) units [23] or GRUs (Gated Recurrent Units) [10].", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "The most recent advances in chorale harmonization is arguably the BachBot model [22], a LSTM-based approach specifically designed to deal with Bach chorales.", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "Its efficiency opens up new ways of creating interesting Bach-like chorales for non experts similarly to what is proposed in [26] for leadsheets.", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 1, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 2, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 3, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 0, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 1, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 2, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 3, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 0, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 1, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 2, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 3, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 0, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 1, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 2, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 3, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 200, "endOffset": 249}, {"referenceID": 0, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 250, "endOffset": 299}, {"referenceID": 0, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 250, "endOffset": 299}, {"referenceID": 0, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 250, "endOffset": 299}, {"referenceID": 0, "context": "[74,__,76,77,74,__,__,__,72,__,__,__,76,__,__,__] [69,__,__,__,67,__,65,__,64,__,__,__,64,__,__,__] [60,__,__,__,59,__,__,__,55,__,__,__,57,__,__,__] [53,__,50,__,55,__,__,__,48,__,__,__,49,__,__,__] [ 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]", "startOffset": 250, "endOffset": 299}, {"referenceID": 6, "context": "In order to have a computationally tractable training criterion, we introduce the pseudolikelihood of our data [7,4].", "startOffset": 111, "endOffset": 116}, {"referenceID": 3, "context": "In order to have a computationally tractable training criterion, we introduce the pseudolikelihood of our data [7,4].", "startOffset": 111, "endOffset": 116}, {"referenceID": 13, "context": "This approach was successful in many real-life problems [14] and consists in an approximation of the negative log-likelihood function by the sum over all variables:", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "t log pi(V i |V\\i,t,S,F , \u03b8i), for i \u2208 [4].", "startOffset": 39, "endOffset": 42}, {"referenceID": 17, "context": "We implement them using neural networks based on LSTMs [18,24].", "startOffset": 55, "endOffset": 62}, {"referenceID": 23, "context": "We implement them using neural networks based on LSTMs [18,24].", "startOffset": 55, "endOffset": 62}, {"referenceID": 14, "context": "Generation is performed using Gibbs sampling [15].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "Even if it known that this approach is biased [12] (since we can update simultaneously variables which are not conditionally independent), we experimentally observed that for small batch sizes (16 or 32), DeepBach still generates samples of great musicality while running ten times faster than the sequential version.", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "9 in [20]) to appropriately choose all the time indexes at which we parallelly resample so that:", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "We implemented DeepBach using Keras [9] with the Tensorflow [1] backend.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "We implemented DeepBach using Keras [9] with the Tensorflow [1] backend.", "startOffset": 60, "endOffset": 63}, {"referenceID": 10, "context": "Bach included in the music21 [11] toolkit.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "4a a neural network with one hidden layer of size 200 and ReLU [25] nonlinearity and as the \u201cStacked LSTMs brick\u201d two LSTMs on top of each other, each one being of size 200 (see Fig.", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "2 (f) in [21]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "We compared our model with two other models: a Maximum Entropy model (MaxEnt) as in [16] (Fig.", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "The Multilayer Perceptron model we chose takes as input elements in V\\i,t \u222a S \u222a F , is a neural network with one hidden layer of size 500 and uses a ReLU [25] nonlinearity.", "startOffset": 154, "endOffset": 158}, {"referenceID": 26, "context": "In order to give a general ranking from these binary confrontations, we used the Bradley-Terry model [27,2] to infer potentials \u03b2j 7 https://www.", "startOffset": 101, "endOffset": 107}, {"referenceID": 1, "context": "In order to give a general ranking from these binary confrontations, we used the Bradley-Terry model [27,2] to infer potentials \u03b2j 7 https://www.", "startOffset": 101, "endOffset": 107}], "year": 2016, "abstractText": "The composition of polyphonic chorale music in the style of J.S Bach has represented a major challenge in automatic music composition over the last decades. The art of Bach chorales composition involves combining four-part harmony with characteristic rhythmic patterns and typical melodic movements to produce musical phrases which begin, evolve and end (cadences) in a harmonious way. To our knowledge, no model so far was able to solve all these problems simultaneously using an agnostic machine-learning approach. This paper introduces DeepBach, a statistical model aimed at modeling polyphonic music and specifically four parts, hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. We evaluate how indistinguishable our generated chorales are from existing Bach chorales with a listening test. The results corroborate our claim. A key strength of DeepBach is that it is agnostic and flexible. Users can constrain the generation by imposing some notes, rhythms or cadences in the generated score. This allows users to reharmonize user-defined melodies. DeepBach\u2019s generation is fast, making it usable for interactive music composition applications. Several generation examples are provided and discussed from a musical point of view.", "creator": "LaTeX with hyperref package"}}}