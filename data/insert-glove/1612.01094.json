{"id": "1612.01094", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "Learning to superoptimize programs - Workshop Version", "abstract": "Superoptimization requires hurney the rm30 estimation umami of the 16,300 best u.s.-made program serifis for a given threskiornithidae computational rivet task. In order to savchuk deal freihofer with large programs, dhanusha superoptimization techniques perform intoxicant a stochastic search. This involves bogyoke proposing a arm\u00e9 modification arnthor of pacifici the daubeney current program, commerz which 9:48 is henschel accepted ao\u00fbt or 21-game rejected execrable based on arsacid the improvement achieved. The asaka state of gorgo the badfinger art loquasto method 1.4633 uses uniform proposal distributions, ceatec which gospodinov fails to 23.14 exploit the problem sankranthi structure to hilferty the hooda fullest. cerebus To alleviate this born deficiency, we learn debary a proposal a87 distribution over porte-cochere possible modifications ev-71 using saint-jean-sur-richelieu Reinforcement Learning. We 26.03 provide threatt convincing nitsch results on run-dmc the leapfrogged superoptimization utrillo of \" 2-0-6-0 Hacker ' nibbling s 671,000 Delight \" widdoes programs.", "histories": [["v1", "Sun, 4 Dec 2016 10:39:49 GMT  (758kb,D)", "http://arxiv.org/abs/1612.01094v1", "Workshop version for the NIPS NAMPI Workshop. Extended version atarXiv:1611.01787"]], "COMMENTS": "Workshop version for the NIPS NAMPI Workshop. Extended version atarXiv:1611.01787", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rudy bunel", "alban desmaison", "m pawan kumar", "philip h s torr", "pushmeet kohli"], "accepted": false, "id": "1612.01094"}, "pdf": {"name": "1612.01094.pdf", "metadata": {"source": "CRF", "title": "Learning to superoptimize programs", "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar"], "emails": ["rudy@robots.ox.ac.uk,", "alban@robots.ox.ac.uk,", "pawan@robots.ox.ac.uk,", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com", "@NIPS"], "sections": [{"heading": null, "text": "Superoptimization requires the estimation of the best program for a given computational task. In order to deal with large programs, superoptimization techniques perform a stochastic search. This involves proposing a modification of the current program, which is accepted or rejected based on the improvement achieved. The state of the art method uses uniform proposal distributions, which fails to exploit the problem structure to the fullest. To alleviate this deficiency, we learn a proposal distribution over possible modifications using Reinforcement Learning. We provide convincing results on the superoptimization of \u201cHacker\u2019s Delight\u201d programs."}, {"heading": "1 Introduction", "text": "Superoptimization requires us to obtain the optimal program for a computational task. While modern compilers implement a large set of rewrite rules, they fail to offer any guarantee of optimality. An alternative approach is to search over the space of all possible programs that are equivalent to the compiler output, and select the one that is the most efficient. If the search is carried out in a brute-force manner, we are guaranteed to achieve superoptimization. However, this approach quickly becomes computationally infeasible as the number of instructions and the length of the program grows. To address this issue, recent approaches have started to use a stochastic search procedure, inspired by Markov Chain Monte Carlo sampling [15]. One of the main factors that governs the efficiency of this stochastic search is the choice of a proposal distribution. Surprisingly, the state of the art method, Stoke [15] relies on uniform distributions for each of its components. We argue that this choice fails to fully exploit the power of stochastic search. To alleviate the aforementioned deficiency of Stoke, we build a reinforcement learning framework to estimate a more suitable proposal distribution for the task at hand. The quality of the distribution is measured as the expected quality of the program obtained via stochastic search. Using training data, which consists of a set of input programs, the parameters are learnt via the REINFORCE algorithm [18]. We demonstrate the efficacy of our approach on a set of \u201cHacker\u2019s Delight\u201d [17] programs. Preliminary results indicate that a learnt proposal distribution outperforms the uniform one on novel tasks that were previously unseen during training. 1st Workshop on Neural Abstract Machines & Program Induction (NAMPI), @NIPS 2016, Barcelona, Spain. ar X iv :1\n61 2.\n01 09\n4v 1\n[ cs\n.L G\n] 4"}, {"heading": "2 Related Works", "text": "The earliest approached for superoptimization relied on brute-force search. By sequentially enumerating all programs in increasing length orders [5, 12], the shortest program meeting the specification is guaranteed to be found. As expected, this approach scales poorly to longer programs or to large instruction sets. The longest reported synthesized program was 12 instructions long, on a restricted instruction set [12]. Trading off completeness for efficiency, stochastic methods [15] reduced the number of programs to test by guiding the exploration of the space, using the observed quality of programs encountered as hints. However, using a generic, unspecific exploratory policy made the optimization blind to the problem at hand. We propose to tackle this problem by learning the proposal distribution. Similar work was done to discover efficient implementation of computation of value of degree k polynomials [19]. Programs were generated from a grammar, using a learned policy to prioritize exploration. This particular approach of guided search looks promising to us, and is in spirit similar to our proposal, although applied on a very restricted case. Another approach to guide the exploration of the space of programs was to make use of the gradients of differentiable relaxation of programs. Bunel et al. [2] attempted this by simulating program execution using recurrent Neural Networks. This however provided no guarantee that the optimum found was going to correspond to a real program. Additionally, this method only had the possibility of performing very local moves, limiting the kind of discoverable transformations. Outside of program optimization, applying learning algorithms to improve optimization procedures, either in terms of results achieved or time taken, is a well studied subject. Doppa et al. [4] proposed methods to deal with structured output spaces, in a \u201cLearning to search\u201d framework. However, these approaches based on Imitation Learning are not directly applicable as we have access to a valid cost function, and therefore don\u2019t need to learn how to approximate it. More relevant is the recent literature on learning to optimize. Li and Malik [11] and Andrychowicz et al. [1] learns how to improve on first-order gradient descent algorithms, making use of neural networks. Our work is similar, as we aim to improve the optimization process. We differ in that our initial algorithm is a MCMC sampler, on a discrete space, as opposed to gradient descent on a continuous, unconstrained space. The training of a Neural Network to generate a proposal distribution to be used in sequential Monte-Carlo was also proposed by Paige and Wood [14] as a way to accelerate inference in graphical models. Additionally, similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible [8, 10, 20]."}, {"heading": "3 Learning Stochastic Superoptimization", "text": "Stoke performs black-box optimization of a cost function on the space of programs, represented as a series of instructions. Each instruction is composed of an opcode, specifying what to execute, and some operands, specifying the corresponding registers. Each given input program T defines a cost function. For a candidate program R called a rewrite, the associated cost is given by: cost (R, T ) = \u03c9e \u00d7 eq(R, T ) + \u03c9p \u00d7 perf(R) (1) The term eq(R; T ) measures how well do the outputs of the rewrite match with the outputs of the reference program when executed. This can be obtained either by running a symbolic validator or by running test cases, and accepting partial definition of correctness. The other term, perf(R) is a measure of the execution time of the program. An approximation can be the sum of the latency of all the instructions in the program. Alternatively, timing the program on some test cases can be used. To find the optimum of this cost function, Stoke runs an MCMC sampler, using the Metropolis algorithm. This allows to sample from the probability distribution induced by the the cost function:\np(R; T ) = 1 Z exp(\u2212cost (R, T ))), (2)\nwhere R is the proposed rewrite, T is the input program. The sampling is done by proposing random moves R \u2192 R?, sampled from a proposal distribution q(R?|R). An acceptance criterion is computed, and used as the parameter of a Bernoulli distribution, to decide whether or not the move is accepted.\n\u03b1(R \u2192 R?, T ) = min ( 1, p(R ?; T )\np(R; T )\n) . (3)\nThe proposal distribution q originally used in [15] is a hierarchical model, whose detailed structure distribution can be found in Appendix A. Uniform distributions were used for each of the elementary probability distributions the model sample from. This corresponds to a specific instantiation of the general approach. We propose to learn those probability distribution so as to maximize the probability of reaching the best programs. The cost function defined in equation (1) corresponds to what we want to optimize. Under a fixed computational budget to perform program superoptimization in less than T iterations, we are interested in having the lowest possible cost at the end. As different programs have different runtimes and therefore different associated costs, we need to perform normalization. As normalized loss function, we use the ratio between the best rewrite found and the cost of the initial unoptimized program R0. Given that our optimization procedure is stochastic, we will need to consider the expected cost as our loss. This expected loss is a function of the parameters \u03b8 of our proposal distribution. The objective function of our \u201cmeta-optimization\u201d problem is therefore:\nL(\u03b8) = E{Rt}\u223cq\u03b8 [\nmint=0..T cost (Rt, T ) cost (R0, T )\n] (4)\nOur chosen parameterization of q is to keep the hierarchical structure of the original work of Schkufza et al. [15], and parameterize all separate probability distributions (over the type of move, the opcodes, the operands, and the lines of the program) independently. In order to learn them, we will make use of unbiased estimators of the gradient. These can be obtained using the REINFORCE algorithm [18]. A helpful way to derive them is to consider the execution traces of the search procedure under the formalism of stochastic computation graphs [16]. The corresponding graph used can be found in Appendix B. By instrumenting the Stoke system of Schkufza et al. [15], we can collect the execution traces so as to compute gradients over the outputs of the probability distributions, which can then be back-propagated. In that way, we can perform Stochastic Gradient Descent (SGD) over our objective function 4."}, {"heading": "4 Experiments", "text": "We ran our experiments on the Hacker\u2019s delight [17] corpus, a collection of 25 bit-manipulation programs, used as benchmark in program synthesis [7, 9, 15]. A detailed description of the task is given in Appendix C. Some examples include identifying whether an integer is a power of two from its binary representation, counting the number of bits turned on in a register or computing the maximum of two integers. In order to have a larger corpus than the twenty-five programs initially obtained, we generate various starting points for each optimization. This is accomplished by running Stoke with a cost function where \u03c9p = 0 in (1), keeping only the correct programs and filtering out duplicates. This allows us to create a larger dataset. We divide the Hacker\u2019s Delight tasks into two sets. We train on the first set and only evaluate performance on the second so as to evaluate the generalization of our learned proposal distribution. We didn\u2019t attempt to learn the probability distribution over the operands and the program position, only learning the ones over opcodes and type of move to perform. The probability distribution learned here are simple categorical distribution. We learn the parameters of each separate distribution jointly, using a Softmax transformation to enforce that they are proper probability distribution. We initialize the training with uniform proposal distribution so the first datapoints on the graph corresponds to the original system of [15].\nIn our current experiment, the proposal distributions are not conditioned on the input program. Optimizing them corresponds to finding an ideal proposal distribution for Stoke. Figure 1a shows the results. Both the training and the test loss decreases and it can be observed that the optimization of program happens faster and that more programs reach the observed minimum."}, {"heading": "5 Conclusion", "text": "Within this paper, we have shown that learning the proposal distribution of the stochastic search can lead to significant performance improvement. It is interesting to compare our approach to the synthesis-style approaches that have been appearing recently in the Deep Learning community [6] that aim at learning programs directly using differentiable representations of programs. We note one advantage that such stochastic search-based approach yields is that the resulting program can be run independently from the Neural Network that was used to discover them. Several improvements are possible to the presented approach. Making the probability distribution a Neural Network conditioned on the initial input or on the current state of the rewrite would lead to a more expressive model, while essentially having similar training complexity. It will however be necessary to have a richer, more varied dataset to make any evaluation meaningful."}, {"heading": "A Generative model of the program transformations", "text": "In Stoke [15], the program transformation are sampled from a generative model. This process was analysed from the publicly available code [3]. First, a type of transformation is sampled uniformly from the following proposals method.\n1. Add a NOP instruction Add an empty instruction at a random position in the program.\n2. Delete an instruction Remove one of the instruction of the program. 3. Instruction Transform Replace one existing line (instruction + operands) by a\nnew one (New instruction and new operands). 4. Opcode Transform Replace one instruction by another one, keeping the same\noperands. The new instruction is sampled from the set of compatible instructions. 5. Opcode Width Transform Replace one instruction by another one, with the\nsame memonic. This means that those instructions do the same thing, except that they don\u2019t operate on the same part of the registers (for example, will replace movq that move 64-bit of data of the registers by movl that will move 32-bit of data)\n6. Operand Transform Replace the operand of a randomly selected instruction by another valid operand for the context, sampled at random.\n7. Local swap Transform Swap two instructions in the same \u201cblock\u201d. 8. Global Swap transform Swap any two instructions. 9. Rotate transform Draw two positions in the program, and rotate all the instruc-\ntions between the two (the last one becomes the first one of the series and all the others get pushed back).\nThen, once the type of move has been sampled, the actual move has to be sampled. To do that, a certain numbers of sampling steps need to happen. Let\u2019s take as example 3.\nTo perform an Instruction Transform,\n1. A line in the existing programs is uniformly chosen. 2. A new instruction is sampled, from the list of all possible instructions. 3. For each of the arguments of the instruction, sample from the acceptable value. 4. The chosen line is replaced by the new line that was sampled.\nThe sampling process of a move is therefore a hierarchy of sampling steps. A simple way to characterize it is as a generative model over the moves. Depending on what type of move is sampled, differents series of sampling steps will have to be performed. For a given move, all the probabilities are sampled independently so the probability of proposing the move is the product of the probability of picking each of the sampling steps. The generative model is defined in Figure 4. It is going to be parameterized by the the parameters of each specific probability distribution it samples from. The default Stoke version uses uniform probabilities over all of those elementary distributions. The criterion described in equation (3) is justified at the condition that the proposal distribution is symmetric, that is, q(R?|R) = q(R|R?). In that case, in the limit, the distribution of states visited by the sampler will be p, making the optimal program the most sampled [13]. By learning the proposal distribution, we won\u2019t necessarily maintain the symmetry property. Even when using only uniform elementary distributions as in [15], the proposal distribution is not symmetric. An example showing the non-symmetric characteristic is the case of the Instruction Transform move. If the proposal is to replace an instruction with two arguments by one with one argument, the probability of the proposal will be:\nq(R?|R) = 1 nmoves \u00d7 1 nopcodes \u00d7 1 noperands , (5)\nwhile the reverse proposal would be:\nq(R|R?) = 1 nmoves \u00d7 1 nopcodes \u00d7 1 n2operands , (6)\nAs a consequence, the proposal distribution is not symmetric and the properties of the Metropolis algorithm [13] won\u2019t apply. Even without guarantees in the limit, the whole process can still be understood as an hill-climbing algorithm with a stochastic component to avoid getting stuck in local maxima. Another potential solution would be to use the Metropolis-Hastings criterion to replace the simpler Metropolis criterion (3):\n\u03b1(R \u2192 R?, T ) = min ( 1, p(R ?; T )q(R|R?)\np(R; T )q(R?|R)\n) . (7)\nHowever, this involves developping an inverse model of the proposed moves to find for each move, the reverse move that would correspond to its undoing and estimate their probabilities. In the current form of the proposal distribution in Figure 4, not all moves have a direct reverse move. For example, Delete does not."}, {"heading": "B Metropolis algorithm as a Stochastic Computation Graph", "text": ""}, {"heading": "C Hacker\u2019s delight tasks", "text": "The 25 tasks of the Hacker\u2019s delight [17] datasets are the following:\n1. Turn off right-most one bit 2. Test whether an unsigned integer is of the form 2(n\u2212 1) 3. Isolate the right-most one bit 4. Form a mask that identifies right-most one bit and trailing zeros 5. Right propagate right-most one bit 6. Turn on the right-most zero bit in a word 7. Isolate the right-most zero bit 8. Form a mask that identifies trailing zeros 9. Absolute value function 10. Test if the number of leading zeros of two words are the same 11. Test if the number of leading zeros of a word is strictly less than of another work 12. Test if the number of leading zeros of a word is less than of another work 13. Sign Function 14. Floor of average of two integers without overflowing 15. Ceil of average of two integers without overflowing 16. Compute max of two integers 17. Turn off the right-most contiguous string of one bits 18. Determine if an integer is a power of two 19. Exchanging two fields of the same integer according to some input 20. Next higher unsigned number with same number of one bits 21. Cycling through 3 values 22. Compute parity 23. Counting number of bits 24. Round up to next highest power of two 25. Compute higher order half of product of x and y"}], "references": [{"title": "Learning to learn by gradient descent by gradient descent", "author": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W Hoffman", "David Pfau", "Tom Schaul", "Nando de Freitas"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Adaptive neural compilation", "author": ["Rudy Bunel", "Alban Desmaison", "Pushmeet Kohli", "Philip HS Torr", "M Pawan Kumar"], "venue": "In NIPS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Hc-search: A learning framework for search-based structured prediction", "author": ["Janardhan Rao Doppa", "Alan Fern", "Prasad Tadepalli"], "venue": "JAIR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Eliminating branches using a superoptimizer and the GNU C compiler", "author": ["Torbj\u00f6rn Granlund", "Richard Kenner"], "venue": "ACM SIGPLAN Notices,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "Synthesis of loop-free programs", "author": ["Sumit Gulwani", "Susmit Jha", "Ashish Tiwari", "Ramarathnam Venkatesan"], "venue": "In PLDI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "The informed sampler: A discriminative approach to bayesian inference in generative computer vision models", "author": ["Varun Jampani", "Sebastian Nowozin", "Matthew Loper", "Peter V Gehler"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Oracle-guided component-based program synthesis", "author": ["Susmit Jha", "Sumit Gulwani", "Sanjit A Seshia", "Ashish Tiwari"], "venue": "In International Conference on Software Engineering,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Picture: A probabilistic programming language for scene perception", "author": ["Tejas D Kulkarni", "Pushmeet Kohli", "Joshua B Tenenbaum", "Vikash Mansinghka"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Learning to optimize", "author": ["Ke Li", "Jitendra Malik"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Superoptimizer: A look at the smallest program", "author": ["Henry Massalin"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}, {"title": "Equation of state calculations by fast computing machines", "author": ["Nicholas Metropolis", "Arianna W Rosenbluth", "Marshall N Rosenbluth", "Augusta H Teller", "Edward Teller"], "venue": "The journal of chemical physics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1953}, {"title": "Inference networks for sequential Monte Carlo in graphical models", "author": ["Brookes Paige", "Frank Wood"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Learning to discover efficient mathematical identities", "author": ["Wojciech Zaremba", "Karol Kurach", "Rob Fergus"], "venue": "In NIPS", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Using training data, which consists of a set of input programs, the parameters are learnt via the REINFORCE algorithm [18].", "startOffset": 118, "endOffset": 122}, {"referenceID": 3, "context": "By sequentially enumerating all programs in increasing length orders [5, 12], the shortest program meeting the specification is guaranteed to be found.", "startOffset": 69, "endOffset": 76}, {"referenceID": 9, "context": "By sequentially enumerating all programs in increasing length orders [5, 12], the shortest program meeting the specification is guaranteed to be found.", "startOffset": 69, "endOffset": 76}, {"referenceID": 9, "context": "The longest reported synthesized program was 12 instructions long, on a restricted instruction set [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "Similar work was done to discover efficient implementation of computation of value of degree k polynomials [19].", "startOffset": 107, "endOffset": 111}, {"referenceID": 1, "context": "[2] attempted this by simulating program execution using recurrent Neural Networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] proposed methods to deal with structured output spaces, in a \u201cLearning to search\u201d framework.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Li and Malik [11] and Andrychowicz et al.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "[1] learns how to improve on first-order gradient descent algorithms, making use of neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "The training of a Neural Network to generate a proposal distribution to be used in sequential Monte-Carlo was also proposed by Paige and Wood [14] as a way to accelerate inference in graphical models.", "startOffset": 142, "endOffset": 146}, {"referenceID": 5, "context": "Additionally, similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible [8, 10, 20].", "startOffset": 151, "endOffset": 162}, {"referenceID": 7, "context": "Additionally, similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible [8, 10, 20].", "startOffset": 151, "endOffset": 162}, {"referenceID": 13, "context": "These can be obtained using the REINFORCE algorithm [18].", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "A helpful way to derive them is to consider the execution traces of the search procedure under the formalism of stochastic computation graphs [16].", "startOffset": 142, "endOffset": 146}, {"referenceID": 4, "context": "4 Experiments We ran our experiments on the Hacker\u2019s delight [17] corpus, a collection of 25 bit-manipulation programs, used as benchmark in program synthesis [7, 9, 15].", "startOffset": 159, "endOffset": 169}, {"referenceID": 6, "context": "4 Experiments We ran our experiments on the Hacker\u2019s delight [17] corpus, a collection of 25 bit-manipulation programs, used as benchmark in program synthesis [7, 9, 15].", "startOffset": 159, "endOffset": 169}, {"referenceID": 0, "context": "[1] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Rudy Bunel, Alban Desmaison, Pushmeet Kohli, Philip HS Torr, and M Pawan Kumar.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Janardhan Rao Doppa, Alan Fern, and Prasad Tadepalli.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Torbj\u00f6rn Granlund and Richard Kenner.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] Sumit Gulwani, Susmit Jha, Ashish Tiwari, and Ramarathnam Venkatesan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[8] Varun Jampani, Sebastian Nowozin, Matthew Loper, and Peter V Gehler.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[10] Tejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, and Vikash Mansinghka.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] Ke Li and Jitendra Malik.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] Henry Massalin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] Brookes Paige and Frank Wood.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[16] John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[18] Ronald J Williams.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[19] Wojciech Zaremba, Karol Kurach, and Rob Fergus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In that case, in the limit, the distribution of states visited by the sampler will be p, making the optimal program the most sampled [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "As a consequence, the proposal distribution is not symmetric and the properties of the Metropolis algorithm [13] won\u2019t apply.", "startOffset": 108, "endOffset": 112}], "year": 2016, "abstractText": "Superoptimization requires the estimation of the best program for a given computational task. In order to deal with large programs, superoptimization techniques perform a stochastic search. This involves proposing a modification of the current program, which is accepted or rejected based on the improvement achieved. The state of the art method uses uniform proposal distributions, which fails to exploit the problem structure to the fullest. To alleviate this deficiency, we learn a proposal distribution over possible modifications using Reinforcement Learning. We provide convincing results on the superoptimization of \u201cHacker\u2019s Delight\u201d programs.", "creator": "LaTeX with hyperref package"}}}