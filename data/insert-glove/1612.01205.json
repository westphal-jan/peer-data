{"id": "1612.01205", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits", "abstract": "We consider tiangong the sollee problem of bloodying off - avoyelles policy ritualism evaluation - - - yozo estimating the value of a .0 target policy polovina using lusting data -2.5 collected 390-7713 by reputations another toponymie policy - - - under rexburg the serf contextual pr\u00e9s bandit moeloek model. We bertuccelli establish bardy a minimax lower north-to-south bound westmin on the caudal mean squared maggots error (101.39 MSE ), truest and recrimination show that it ticketron is matched delneri up to stenella constant rapho factors warri by bhat the interlocutors inverse amadou propensity naosuke scoring (tamponade IPS) estimator. ignatov Since in yuran the multi - armed amrish bandit muazzam problem the unhappy IPS is suboptimal (co-curriculum Li proti\u0107 et. al, 2015 ), credit-card our qingzhu result perissodactyla highlights scrounger the missi difficulty of auditoria the turck contextual setting with kaleidoscape non - rados\u0142aw degenerate upei context pavane distributions. jaimie We self-referential further r\u00f9m consider fenfluramine improvements conservative-leaning on this bulimic minimax tacom MSE landfair bound, unkindly given tool-making access to altschuler a murawski reward model. dzongs We hge show that wipp the existing toona doubly cappadocia robust approach, which seku utilizes ardakan such a spinesi reward model, may continue 34,167 to tv-pg suffer from high variance even tenom when opportunities the asta reward dentyne model is sexing perfect. We propose a gurukula new dcx estimator called SWITCH liebers which more giblets effectively uses the reward italiano model 2,428 and 3,013 achieves hasao a superior bias - variance pilkington tradeoff columbine compared 55-44 with prior work. sambava We prove an upper bound on oak-hickory its al-thani MSE mahakali and adamo demonstrate pronator its benefits empirically sunde on canonicus a diverse collection 1,242 of datasets, often bmw seeing shadayid orders shoubaki of magnitude 29-9 improvements over a number of apne baselines.", "histories": [["v1", "Sun, 4 Dec 2016 23:24:17 GMT  (171kb,D)", "http://arxiv.org/abs/1612.01205v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yu-xiang wang", "alekh agarwal", "miroslav dud\u00edk"], "accepted": true, "id": "1612.01205"}, "pdf": {"name": "1612.01205.pdf", "metadata": {"source": "CRF", "title": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits", "authors": ["Yu-Xiang Wang", "Alekh Agarwal", "Miroslav Dud\u00edk"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Contextual bandits refer to a learning setting where the learner repeatedly observes a context, takes an action and observes a reward signal for the quality of the chosen action in the observed context. Crucially, there is no information on the quality of all the remaining actions that were not chosen for the context. As an example, consider online movie recommendation where the context describes information about a user, actions are possible movies to recommend and a reward can be whether the user enjoys the recommended movie. The framework applies equally well to several other applications such as online advertising, web search, personalized medical treatment, etc. The goal of the learner is to come up with a policy, that is a scheme for mapping contexts into actions. A common question which arises in such settings is, given a candidate target policy, what is the expected reward it obtains? A simple way of answering the question is by letting the policy choose actions (such as make movie recommendations to users), and compute the reward it obtains. Such online evaluation, is typically costly and time consuming since it involves exposing users to an untested experimental policy, and does not easily scale to evaluating the performance of many different policies.\nOff-policy evaluation refers to an alternative paradigm for answering the same question. Suppose we have existing logs from the existing system, which might be choosing actions from a very different logging policy than the one we seek to evaluate. Can we estimate the expected reward of the target policy? This question has been extensively researched in the contextual bandit model (see, e.g., [Li et al., 2011, Bottou et al., 2013, Oh and Scheuren, 1983, Little and Rubin, 2002] and references therein). In particular, there are several estimators which are unbiased under mild assumptions, such as inverse propensity scoring (IPS) [Horvitz and Thompson, 1952], and sharp estimates on their mean squared error (MSE) for policy evaluation are well-known [Dud\u00edk et al., 2014].\nar X\niv :1\n61 2.\n01 20\n5v 1\n[ st\nat .M\nL ]\n4 D\nec 2\nWhile the IPS-style methods make no attempt at all to model the underlying dependence of rewards on contexts and actions, such information is often available. The simplest approach to off-policy evaluation, given such a model, is to simply use the model to predict the reward for the target policy\u2019s action on each context. We call this estimator the model-based approach or the direct method (DM). The key drawback of DM is that it can be arbitrarily biased when the model is misspecified. Some approaches, such as the doubly-robust method (DR) [Dud\u00edk et al., 2014] (also see the references therein for its origin in statistics and application in causal inference, e.g., [Robins and Rotnitzky, 1995, Bang and Robins, 2005]), combine the model with an IPS-style unbiased estimation and remain consistent, with known estimates for their MSE.\nAll these works focus on developing specific methods alongside upper bounds on their MSE. Little work, on the other hand, exists on the question of the fundamental statistical hardness of off-policy evaluation and the optimality (or the lack of) of the existing methods. A notable exception is the recent work of Li et al. [2015], who study off-policy evaluation in multi-armed bandits\u2014a special case of our setting, without any contexts\u2014and provide a minimax lower bound on the MSE. Their result shows the suboptimality of IPS (and DR) due to an excessive variance of the importance weights. This result is rather intriguing as it hints at one of two possibilities: (i) IPS and variants are also suboptimal for contextual bandit setting and we should develop better estimators, or (ii) the contextual bandit setting has qualitatively different upper and lower bounds that match. In this quest, our paper makes the following key contributions:\n1. We provide the first rate-optimal lower bound on the MSE for off-policy evaluation in contextual bandits. In contrast with context-free multi-armed bandits [Li et al., 2015], our lower bound matches the MSE upper bound for IPS up to constants, so long as the contexts have a non-degenerate distribution. This highlights the challenges of the contextual setting; even if the reward as a function of contexts and actions has no variance, the lower bound stays non-trivial in contrast with context-free multi-armed bandits.\n2. We propose a new class of estimators called the SWITCH estimators, that adaptively interpolate between an available reward model and IPS. We show that SWITCH has MSE no worse than IPS in the worst case, but is robust to large importance weights. We also show that SWITCH can have a drastically smaller variance than alternatives for combining IPS with a reward model, such as DR.\n3. We conduct experiments showing that the new estimator performs significantly better than existing approaches on simulated contextual bandit problems using real-life multiclass classification data sets.\nThe rest of this paper is organized as follows. In Section 2 and 3, we recap the formal setup of off-policy evaluation and present a lower bound on the minimax risk for model-free off-policy evaluation in contextual bandits. In Section 4, we discuss the need to have adaptive estimators that can take advantage of a given reward model and present the SWITCH estimator along with its theoretical properties. Lastly, the experimental comparison of the proposed methods and existing approaches are given in Section 5."}, {"heading": "2 Setup and related work", "text": "We use the standard setup for off-policy evaluation in contextual bandits as in prior works Dud\u00edk et al. [2014]. We start with the formal setup and then discuss the related work."}, {"heading": "2.1 Formal setup", "text": "In contextual bandit problems, the learning agent observes a context x, takes an action a and observes a scalar reward measuring the quality of the chosen action for the context. Here the context x is a feature vector from a finite set X , possibly {0, 1}d or a machine-precision encoding of Rd for some large d. The stationary\ndistribution of contexts is denoted by \u03bb. Actions, denoted as a, are drawn from a finite set A. Rewards r have a distribution conditioned on x and a denoted by D(r|x, a). The decision rule of the agent is called a policy, which is a function from contexts to distributions over actions to allow for randomized action choice. We will use \u00b5(a|x) and \u03c0(a|x) to denote the logging and target policies respectively. Given a policy \u03c0 which is a distribution over actions given contexts, we extend it to a joint distribution over triples (x, a, r), where x is drawn according to \u03bb, action a according to \u03c0(a|x), and r according to D(r|x, a). With this notation, the goal of off-policy evaluation is to evaluate the expected reward of \u03c0, given samples where actions are chosen according to \u00b5. Formally, given n samples of the form (xi, ai, ri, pi), where pi = \u00b5(ai | xi), we wish to compute the value of \u03c0:\nv\u03c0 = E\u03c0[r], where we have x \u223c \u03bb, a \u223c \u03c0(\u00b7 | x), and r \u223c D(r | a, x). (1)\nIn order to correct for the mismatch in the action distributions under \u00b5 and \u03c0, it is typical to use importance weights, and it will be convenient to introduce the shorthand \u03c1(x, a) = \u03c0(a|x)/\u00b5(a|x). In order to facilitate consistent estimation, it is typical to assume that \u03c1(x, a) 6= \u221e, meaning that whenever \u03c0(a|x) > 0, then \u00b5(a|x) > 0 as well. We will make this same assumption throughout our upper and lower bounds. We conclude the setup by giving one concrete example for estimating the quantity v\u03c0 to aid the reader\u2019s intuition. The simplest estimator in this setting, called Inverse Propensity Scoring (IPS) [Horvitz and Thompson, 1952] is defined as:\nv\u0302\u03c0IPS = n\u2211 i=1 \u03c1(xi, ai)ri. (2)\nIn the sequel, we will discuss properties of this estimator, as well as improvements upon it for off-policy evaluation."}, {"heading": "2.2 Related work", "text": "We now review the related work. There are several works that focus on creating estimators for the off-policy evaluation problem, and provide upper bounds on the Mean Squared Error (MSE) of these estimators, as discussed in the introduction. The study of lower bounds on the MSE of any possible estimator for this problem is relatively new, on the other hand. Li et al. [2015] first studied this question under a minimax setup, but focused on the multi-arm bandits problem. As we will show, the naive extension of their results to contextual bandits yields a suboptimal lower bound. Jiang and Li [2016] built on top of Li et al. [2015] proves a Cramer-Rao-style lower bound for policy-evaluation in reinforcement learning, but the assumption of a small number of observed states precludes contextual bandits from being a special case. Cortes et al. [2010] studied the related covariate shift problem in the statistical learning setting and proved upper bounds and lower bounds of the minimax excess risk that depends on the Renyi divergence of the two covariate distributions. This is related to our problem in the following sense: we can formulate off-policy evaluation as a statistical learning problem of predicting reward r using feature vector (x, a), and off-policy evaluation corresponds to exactly the covariate shift problem (when we know how the covariate has shifted). However, as the minimax problems are defined differently1, their results (especially the lower bound) are not applicable to our problem.\nOn the complementary question of developing techniques for off-policy evaluation and understanding their theoretical properties, there is a considerably larger body of work in both the statistics and machine learning literature. In statistics, this is studied as the problem of mean estimation for treatment regimes in the causal inference literatre (see e.g. [Horvitz and Thompson, 1952, Holland, 1986, Bang and Robins, 2005, Rotnitzky et al., 2012] as well as the references in [Dud\u00edk et al., 2014]). The \u201cdoubly robust\u201d estimation techniques\n1Off policy evaluation focuses on estimating the policy-value, while statistical learning focuses on prediction accuracy measured by the expected loss (in terms of the excess risk).\n[Cassel et al., 1976, Robins and Rotnitzky, 1995], were recently used for off-policy value estimation in the contextual bandits problem [Dud\u00edk et al., 2011, 2014] and reinforcement learning [Jiang and Li, 2016]. These also provide error estimates for the proposed estimators, which we will compare with the lower bounds that we will obtain. The doubly robust techniques also have a flavor of incorporating existing reward models, an idea which we will also leverage in the development of the SWITCH estimator in Section 4. We note that similar ideas were also recently investigated in the context of reinforcement learning by Thomas and Brunskill [2016]."}, {"heading": "3 Limits of off-policy evaluation", "text": "In this section we will present our main result on lower bounds for off-policy evaluation. We first present the minimax framework to study such problems, before presenting the result and its implications."}, {"heading": "3.1 Minimax framework", "text": "Off-policy evaluation is a statistical estimation problem, where the goal is to estimate v\u03c0 given n iid samples generated according to a policy \u00b5. We study this problem in a standard minimax framework and seek to answer the following question. What is the smallest mean square error (MSE) that any estimator can achieve in the worst case over a large class of contextual bandit problems. As is usual in the minimax setting, we want the class of problems to be rich enough so that the estimation problem is not trivialized, and to be small enough so that the lower bounds are not driven by complete pathologies. In our problem, we make the choice to fix \u03bb, \u00b5 and \u03c0, and only take worst case over a class of reward distributions. This allows the upper and lower bounds, as well as the estimators to adapt with and depend on \u03bb, \u00b5 and \u03c0 in interesting ways. The family of reward distributions D(r | x, a) that we study is a natural generalization of the class studied by Li et al. [2015] for multi-armed bandits.\nTo formulate our class of reward distributions, assume we are given maps Rmax : X \u00d7A \u2192 R+ and \u03c3 : X \u00d7A \u2192 R+. The class of conditional distributionsR(\u03c3,Rmax) is defined as\nR(\u03c3,Rmax) := { D(r|x, a) : 0 \u2264 ED[r|x, a] \u2264 Rmax(x, a) and\nVarD[r|x, a] \u2264 \u03c32(x, a) for all x, a } .\nNote that \u03c3 and Rmax are allowed to change over contexts and actions. Formally, let an estimator be any function v\u0302 : (X \u00d7A\u00d7 R)n \u2192 R that takes n data points collected by \u00b5 and outputs an estimate of v\u03c0. The minimax risk of off-policy evaluation over the classR(\u03c32, Rmax) is defined as\nRn(\u03c0;\u03bb, \u00b5, \u03c3,Rmax) := inf v\u0302 sup D(r|x,a)\u2208R(\u03c3,Rmax)\nE [ (v\u0302 \u2212 v\u03c0)2 ] . (3)\nRecall that the expectation here is taken over the n samples collected according to \u00b5, along with any randomness in the estimator. The main goal of this section is to obtain a lower bound on the minimax risk. To state our bound, recall that \u03c1(x, a) = \u03c0(a | x)/\u00b5(a | x) is an importance weight at (x, a). We make the following technical assumption on our problem instances, described by tuples of the form (\u03c0, \u03bb, \u00b5, \u03c3,Rmax):\nAssumption 1. There exists > 0 such that E\u00b5 [ (\u03c1Rmax) 2+ ] and E\u00b5 [ (\u03c1\u03c3)2+ ] are finite.\nThis assumption is fairly mild, as it is only a slight strengthening of the assumption that E\u00b5[(\u03c1Rmax)2] and E\u00b5[(\u03c1\u03c3)2] be finite, which is required for consistency of IPS (see, e.g., the bound on the variance of IPS in Dud\u00edk et al. [2014], which assumes the finiteness of these second moments). Our assumption holds for instance whenever the context space is finite and \u03c0 cannot pick actions that receive zero probability under \u00b5,\nsince both \u03c1 and Rmax are bounded in this case. The latter assumption is quite standard and important in the off-policy evaluation literature as we observed in Section 2.1."}, {"heading": "3.2 Minimax lower bound for off-policy evaluation", "text": "With the minimax setup in place, we now give our main lower bound on the minimax risk of off-policy evaluation and discuss its conclusions. In order to describe our result, we define one convenient piece of notation\nC := 22+ \u00b7max\n{ E\u00b5 [ (\u03c1Rmax) 2+ ]2\nE\u00b5 [ (\u03c1Rmax)2\n]2+ , E\u00b5 [ (\u03c1\u03c3)2+ ]2 E\u00b5 [ (\u03c1\u03c3)2 ]2+ }\n(4)\nTheorem 1 (Minimax lower bound). For any problem instance satisfying Assumption 1 with and any n \u2265 max { 5C1/ , C2/ E\u00b5[\u03c32/R2max] } , the minimax risk satisfies the lower bound\nRn(\u03c0;\u03bb, \u00b5, \u03c3,Rmax) \u2265 1\n220n\n( E\u00b5 [ \u03c12\u03c32 ] + E\u00b5 [ \u03c12R2max ]( 1\u2212 110\u03bb0 log(4/\u03bb0) )) ,\nwhere \u03bb0 = maxx\u2208X \u03bb(x) is the largest probability of a single context.\nThe theorem is proved in Appendix A and uses a reduction to hypothesis testing, followed by an argument due to Le Cam. We now discuss several aspects of the above result.\nPre-conditions of the theorem The theorem assumes the existence of a (problem-dependent) constant C which depends on variance moments of the importance weighted rewards. The stated lower bound holds as long as the number of samples n is large enough relative to C and some other parameters. Assuming that the constant C is finite, then the condition on n is eventually satisfied as long as the random variable \u03c3/Rmax has a bounded variance. We believe this is quite reasonable, since the problems of interest typically have heavy tails in the importance weights but not the reward functions. In terms of the importance weights, we require the random variables \u03c1Rmax and \u03c1\u03c3 to have appropriate moment bounds. In particular, whenever Assumption 1 is satisfied, then it is easy to see that C < \u221e. In particular, under tail decay conditions on these two random variables, the precondition of the theorem is met for a reasonable number of samples. An extreme case where the preconditions will be violated are if Rmax \u2261 0, in which case the estimation problem is indeed trivial. For the remainder of this discussion, we will assume that n is appropriately large so that the preconditions of the theorem are met.\nComparison with upper bounds In typical contextual bandit settings, the contexts are drawn from either a continuous distribution, or an extremely large, but finite set. In either case, the probability of observing any particular context is vanishingly small, so that it is reasonable to expect \u03bb0 \u2192 0 as n\u2192\u221e. Consequently, the lower bound becomes \u2126 ( E\u00b5[\u03c12(\u03c32 +R2max)]/n ) . Comparing with the upper bounds on the MSE for existing estimators, we find that this lower bound precisely matches the upper bound for the IPS estimator [Horvitz and Thompson, 1952] (defined in Equation 2). We present the upper bound here for completeness.\nLemma 1 (IPS estimator [Horvitz and Thompson, 1952]). With v\u0302\u03c0IPS defined in Equation 2, we have\nE(v\u0302\u03c0IPS \u2212 v\u03c0)2 \u2264 1\nn\n( E\u00b5[\u03c12\u03c32] + 1\n4 E\u00b5[\u03c12R2max]\n) .\nWe conclude that IPS is unimprovable, in the worst case, beyond constant factors. Another implication is that the lower bound is sharp for our setting, and cannot be improved any further unless we change the minimax framework to consider smaller classes of reward distributions or allow other side information. In particular, we can combine the upper and lower bounds for a precise characterization of the minimax risk.\nCorollary 1. Under conditions of Theorem 1, for sufficiently small \u03bb0 and large enough n:\ninf v\u0302 sup D(r|a,x)\u2208R(\u03c32,Rmax)\nE\u00b5(v\u0302 \u2212 v\u03c0)2 = \u0398 [ 1\nn\n( E\u00b5[\u03c12\u03c32] + E\u00b5[\u03c12R2max] )] .\nComparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits. The somewhat surprising conclusion of their work was the sub-optimality of IPS, which might appear at odds with our conclusion regarding IPS above. However, this difference actually highlights the additional challenges in contextual bandits beyond multi-armed bandits. This is best illustrated in a noiseless setting, where \u03c3 = 0 in the rewards. This makes the multi-armed bandit problem trivial, we can just measure the reward of each arm with one pull and find out the optimal choice. However, there is still a non-trivial lower bound of \u2126(E\u00b5[\u03c12R2max]/n) in the contextual bandit setting, which is exactly the upper bound on the MSE of IPS when the rewards have no noise.\nThis difference crucially relies on \u03bb0 being suitably small relative to the sample size n. When the number of contexts is small, independent estimation for each context can be done in a noiseless setting as observed by Li et al. [2015]. However, once the context distribution is rich enough, then even with noiseless rewards, there is significant variance in the value estimates based on which contexts were observed. This distinction is further highlighted in the proof of Theorem 1, and is obtained by combining two separate lower bounds. The first lower bound considers the case of noisy rewards, and is a relatively straightforward generalization of the proof of Li et al. [2015]. The second lower bound focuses on noiseless rewards, and shows how the variance in a rich context distribution allows the environment to essentially simulate noisy rewards, even when the reward signal itself is noiseless."}, {"heading": "4 Incorporating model-based approaches in policy evaluation", "text": "Amongst our twin goals of optimal and adaptive estimators, the discussion so far has centered around the optimality of the IPS estimators in a minimax sense. However, real datasets seldom display worst-case behavior, and in this section we discuss approaches to leverage additional structure in the data, when such knowledge is available. We begin with the necessary setup, before introducing our new estimator and its properties. Throughout this section, we drop the superscript \u03c0 from value estimators, as the evaluation policy \u03c0 is fixed throughout this discussion."}, {"heading": "4.1 The need for model-based approaches", "text": "As we have seen in the last section, the model-free approach has a information-theoretic limit that depends quadratically on Rmax, \u03c3 and importance weight \u03c1. This is good news in that it implies the existence of an optimal estimator\u2013IPS. However, it also substantially limits what policies can be evaluated, due to the quadratic dependence on \u03c1. If \u00b5(a | x) is small for some actions, as is typical when number of actions is large or in real systems with a cost for exploration, the policies \u03c0 which can be reliably evaluated cannot put much mass on such actions either without resulting in unreliable value estimates. The key reason for this limitation is that the setup so far allows completely arbitrary reward models\u2014E[r | x, a] can change arbitrarily across different actions and contexts. Real data sets are seldom so pathological, and often we have substantial intuition about contexts and actions which obtain similar rewards, based on the specific application. It is natural to ask how can we leverage such prior information, and develop better estimators that improve upon the minimax risk in such favorable scenarios.\nIn the presence of additional domain information, a common approach is to construct an explicit model for the expected reward, given context and action. Based on the existing logs according to \u00b5: (xi, ai, ri)ni=1, one can then fit the parameters in such a model to form a reward estimator r\u0302(x, a). The task of policy evaluation is now simply performed by scoring \u03c0 according to r\u0302 as\nv\u0302DM = 1\nn n\u2211 i=1 \u2211 a\u2208A \u03c0(a|xi)r\u0302(xi, a), (5)\nwhere the DM stands for direct method, a name for this approach which has been used in prior works [Dud\u00edk et al., 2014]. This approach appears attractive when r\u0302 is a close to E[r | x, a]. Crucially, r\u0302 can be evaluated for any x, a pair, meaning that there is no need to do use importance weights unlike in IPS. This suggests that we might completely eliminate any dependence on \u03c1 using this approach., and it is easily seen that given any estimator r\u0302 such th which takes values in [0, Rmax(x, a)] for any x, a, the variance satisfies:\nVar(v\u0302DM) \u2264 1\nn E\u03c0[R2max] =\n1 n E\u00b5[\u03c1R2max]. (6)\nThat is, there is a linear, rather than quadratic dependence on \u03c1, unlike in the worst case minimax risk of Corollary 1. The catch, however, is that such estimators can have an uncontrolled bias of O(E\u03c0[Rmax]) in the worst case, meaning that even asymptotic consistency is not guaranteed.\nPrior works have addressed the bias of DM, while trying to reduce the variance of IPS by using doubly robust estimators. A concrete estimator previously used for off-policy evaluation in Dud\u00edk et al. [2014] is defined as\nv\u0302DR = 1\nn n\u2211 i=1\n[ (ri \u2212 r\u0302(xi, ai))\u03c0(ai)\n\u00b5(ai) + \u2211 a\u2208A \u03c0(a|xi)r\u0302(xi, ai)\n] . (7)\nThe theory of these methods suggests that the variance of v\u0302\u03c0DR can be smaller than IPS, if r\u0302 is a good reward estimator. Does this imply that DR is the right model-based adaptive estimator?\nAn interesting benchmark to consider is when one has a prefect reward estimator, that is r\u0302(x, a) = E[r | x, a] almost surely. In this case, v\u0302DM has no bias, and hence its MSE is identical to its variance, with a linear dependence on importance weights as in Equation 6. However, based on the results of Dud\u00edk et al. [2014], the MSE of the DR estimator in this special case is\nMSE(v\u0302DR) \u2264 1\nn\n( E\u00b5[\u03c12\u03c32] + E\u00b5[\u03c1R2max] ) . (8)\nCrucially, the MSE still has a dependence on \u03c12 in it as long as the variance \u03c3 is non-trivial. Comparing Equations 6 and 8, it would be better to use v\u0302DM instead in this case. Also note that \u03c1 instead of \u03c12 term multiplying Rmax does not contradict the lower bound in Theorem 1, since the bound (8) assumes access to the expected reward function which is information an estimator does not have in our minimax setup and which cannot be computed from the samples in general.\nWe conclude that while DR improves upon IPS in some cases, the estimator still does not adapt enough in the presence of good reward models, when the rewards have a non-trivial variance. We next present an estimator which utilizes the reward model in a different way, with an eye towards a more favorable bias-variance tradeoff."}, {"heading": "4.2 The SWITCH estimators to incorporate reward models", "text": "We now present a class of estimators which incorporate any available reward models very differently from the DR approach. The starting point for our method is the observation that insistence on maintaining unbiasedness\nputs the DR estimator at one extreme end of the bias-variance tradeoff. Prior works have considered ideas such as truncating the rewards, or importance weights when the importance weights get large (see e.g. Bottou et al. [2013] for a detailed discussion), which can often reduce the variance drastically at the cost of a little bias. We take the intuition a step further, and propose to estimate the rewards for actions differently, based on whether they have a large or a small importance weight given a context. When importance weights are small, we continue to use our favorite unbiased estimators, but switch to using the (potentially biased) reward model on the actions with large importance weights. Here small and large are defined via some threshold parameter \u03c4 . Varying this parameter between 0 and\u221e leads to a family of estimators which we call the SWITCH estimators as they switch between a model-free and model-based approach.\nWe now formalize this intuition, and begin by decomposing the value of \u03c0 according to importance weights:\nv\u03c0 = E\u03c0[r] = E\u03c0[r1(\u03c1 \u2264 \u03c4)] + E\u03c0[r1(\u03c1 > \u03c4)] = E\u00b5[\u03c1r1(\u03c1 \u2264 \u03c4)] + Ex\u223c\u03bb [\u2211 a\u2208A ED[r|x, a]\u03c0(a|x)1(\u03c1(x, a) > \u03c4) ] .\nConceptually, we split our problem into two. The first problem always has small importance weights, so we can use unbiased estimators such as IPS or DR as before. The second problem, where importance weights are large, is essentially addressed by DM. Writing this out leads to the following estimator:\nv\u0302SWITCH = 1\nn n\u2211 i=1 [ri\u03c1i1(\u03c1i \u2264 \u03c4)] + 1 n n\u2211 i=1 \u2211 a\u2208A r\u0302(xi, a)\u03c0(a|xi)1(\u03c1(xi, a) > \u03c4). (9)\nNote that the above estimator specifically uses IPS on the first part of the problem. We will mention an alternative using the DR estimator for the first part at the end of this section. We first present a bound on the MSE of the SWITCH estimator using IPS, for a given choice of the threshold \u03c4 .\nTheorem 2. Let (a, x) := r\u0302(a, x)\u2212 E[r|a, x] be the bias of r\u0302 and assume r\u0302(x, a) \u2208 [0, Rmax(x, a)] almost surely. Then for every n = 1, 2, 3, . . . , and for the \u03c4 > 0 used in Equation 9, we have\nMSE(v\u0302SWITCH) \u2264 2\nn\n{ E\u00b5 [( \u03c32 +R2max ) \u03c121(\u03c1 \u2264 \u03c4) ] +E\u00b5 [ \u03c1R2max1(\u03c1 > \u03c4) ] +E\u00b5 [ \u03c1 \u2223\u2223 \u03c1 > \u03c4]2\u03c0(\u03c1 > \u03c4)2} ,\nwhere quantities Rmax, \u03c3, \u03c1, and are functions of the random variables x and a, and we recall the use of \u03c0 and \u00b5 as joint distributions over (x, a, r) tuples.\nRemark 1. The proposed estimator is an interpolation of the DM and IPS estimators. By taking \u03c4 =, we get SWITCH coincides with DM while \u03c4 \u2192\u221e yields IPS. Several estimators related to SWITCH have been studied in the literature, and we discuss a couple of them here.\n\u2022 A special case of SWITCH uses r\u0302 \u2261 0, meaning that all the actions with large importance weights are essentially eliminated from consideration. This approach, with a specific choice of \u03c4 was described in Bottou et al. [2013] and will be evaluated in the experiments under the name Trimmed IPS.\n\u2022 Thomas and Brunskill [2016] study a similar estimator in the more general context of reinforcement learning. Their approach can be seen as using a number of candidate threshold \u03c4 \u2019s and then evaluating the policy as a weighted sum of the estimates corresponding to each \u03c4 . They address the questions of picking these thresholds and the weights in a specific manner for their estimator called MAGIC, and we discuss these aspects in more detail in the following subsection.\nRemark 2. The MSE bound in Theorem 2 is easily interpreted. The first term is the MSE of IPS on the region where it is used, while the second and third terms capture the variance and squared bias of DM on its region respective. Consequently, our matches the result for IPS when \u03c4 \u2192\u221e, meaning that the SWITCH estimators are also minimax optimal when \u03c4 is appropriately chosen. At the other extreme, we can consider the case of \u03c4 = 0 and a perfect reward predictor so that \u2261 0. In this case, the MSE bound matches that of DM in Equation 6. More generally, the estimators are robust to heavy-tails in the distribution of importance weights, unlike both IPS and DR estimators.\nRemark 3. The policy value in the region where the importance weights are small can be estimated using any unbiased approach rather than just IPS. For instance, we can use the DR, giving rise to the estimator, which we denote SWITCH-DR. For any reward estimator r\u0302\u2032 to construct the DR estimator, we get\nv\u0302SWITCH-DR = 1\nn n\u2211 i=1 [ (ri \u2212 r\u0302\u2032i)\u03c1i1(\u03c1i \u2264 \u03c4) + \u2211 a\u2208A r\u0302\u2032(xi, a)\u03c0(a|xi)1(\u03c1(xi, a) \u2264 \u03c4) ]\n+ 1\nn n\u2211 i=1 \u2211 a\u2208A r\u0302(xi, a)\u03c0(a|xi)1(\u03c1(xi, a) > \u03c4). (10)\nNote that r\u0302 and r\u0302\u2032 need not be the same direct estimators. When they are indeed the same, v\u0302SWITCH-DR can be rewritten as essentially a modified doubly robust estimator where all the importance weights above \u03c4 are clipped to zero, so that the importance weighted part of DR makes no contribution.\nThe analysis in Theorem 2 still applies, replacing the variance of IPS with that of DR from Dud\u00edk et al. [2014]. Since no independence was required in our analysis between the IPS and the DM parts of the estimator, the result is also robust to the use of a common data-dependent estimator r\u0302 = r\u0302\u2032 in SWITCH-DR (10)."}, {"heading": "4.3 Automatic parameter tuning", "text": "So far we have discussed the properties of the SWITCH estimators, assuming that the parameter \u03c4 is chosen well. On the other hand, the estimator can be as bad as DM in the worst-case if \u03c4 is poorly chosen, as evidenced by the bias term in Theorem 2. For the estimators to be useful in practice, it becomes essential to have a procedure to select a good value of \u03c4 . A natural criterion for selecting \u03c4 would be to pick one that minimizes the MSE of the resulting estimator. Since we do not know the precise MSE (as v\u03c0 is unknown), an alternative is to minimize a data-dependent estimate for it. Recalling that the MSE can be written as the sum of variance and squared bias, we estimate and bound the terms individually.\nWe can estimate the variance of the SWITCH estimator in a straightforward manner from the data. Let Yi(\u03c4) denote the estimated value that \u03c0 obtains on the data point xi according to the SWITCH estimator with the threshold \u03c4 , that is\nYi(\u03c4) := ri\u03c1i1(\u03c1i \u2264 \u03c4) + \u2211 a\u2208A r\u0302(xi, a)\u03c0(a|xi)1(\u03c1(xi, a) > \u03c4) and Y\u0304 (\u03c4) = 1 n n\u2211 i=1 Yi(\u03c4),\nSince the xi are i.i.d., the variance of v\u0302SWITCH can be simply estimated as\nVar(v\u0302SWITCH\u2212\u03c4 ) = 1\nn Var(v\u0302SWITCH\u2212\u03c4 (x1)) \u2248\n1\nn2 n\u2211 i=1 (Yi(\u03c4)\u2212 Y\u0304 (\u03c4))2 =: V\u0302ar\u03c4 , (11)\nwhere the approximation above is clearly consistent since the random variables Yi are appropriately bounded as long as the rewards are bounded, with the importance weights capped at the threshold \u03c4 . Note that we have explicitly denoted the dependence on \u03c4 in the SWITCH estimator above.\nNext we turn to the bias term. For understanding bias, we look at the MSE bound in Theorem 2, and observe that the last term in that theorem is precisely an upper bound on the bias. Rather than using a direct bias estimate, which would require knowledge of the error in r\u0302, we will upper bound this term. For now, let us assume that the function Rmax(x, a) is known. In many practical applications, this is not a limiting assumption at all, where an apriori bound on the rewards is known ahead of time anyways. Then we can upper bound the bias term as\nBias2(v\u0302SWITCH) \u2264 E\u00b5[\u03c1 2|\u03c1 > \u03c4 ]\u03c0(\u03c1 > \u03c4)2 \u2264 E\u00b5[\u03c1R2max|\u03c1 > \u03c4 ]\u03c0(\u03c1 > \u03c4)2\n\u2248\n[ 1\nn n\u2211 i=1 E\u03c0 ( R2max|\u03c1 > \u03c4, xi )] [ 1 n n\u2211 i=1 \u03c0(\u03c1 > \u03c4 |xi) ]2 =: B\u0302ias 2 \u03c4 . (12)\nNote that this estimate of the squared bias is not as straightforward as the one for variance, since we independently use sample-based unbiased approximations of the expectation of Rmax and the probability of large importance weights, and then multiply them. However, standard arguments can be used to show that the resulting estimates are still consistent under mild conditions. As a special case, if Rmax \u2261 R, where the uniform upper bound R is known, then the first average simplifies to R making the bias estimate rather straightforward.\nWith these estimates, we pick the value of \u03c4 that minimizes\n\u03c4\u0302 = argmin \u03c4\nV\u0302ar\u03c4 + B\u0302ias 2 \u03c4 . (13)\nNotice that the upper bound on the bias is rather conservative in that it upper bounds the error of DM at the largest possible value for every data point. This has the effect of favoring the use of the unbiased part in SWITCH whenever possible, unless the variance would overwhelm even an arbitrarily biased DM. While this might appear overly conservative, this does imply the minimax optimality of the SWITCH estimator using \u03c4\u0302 almost immediately: the incurred bias is no more than our upper bound, and we incur it only when the minimax optimal IPS estimator would be suffering an even larger variance.\nFinally, we should mention that this development is quite related to the MAGIC estimator of Thomas and Brunskill [2016], which was discussed following Theorem 2. The key differences are that we pick only one threshold \u03c4 , while they compute the estimates with many different \u03c4 \u2019s. Rather than picking just one of these estimates, they further learn a more general weighting over the estimates. In that sense, our estimator can be seen as a special case which puts the entire weight on one choice. Like us, they also pick this weighting function by optimizing a bias variance tradeoff. However, we use very different estimates for bias and variance than their estimator. In the experiments, we did evaluate their approach for picking the threshold \u03c4 and found that the choice \u03c4\u0302 in Equation 13 generally works better."}, {"heading": "5 Experiments", "text": "In this section, we conduct an experimental evaluation of the proposed SWITCH estimators. We will be using the same 10 UCI data sets that was used previously by Dud\u00edk et al. [2011] and convert the multi-class classification problem to contextual bandits by\n1. making it a sequential learning task, where the learner receives one example at a time, and guesses its label using policy \u00b5.\n2. providing a reward of 1 (and hence revealing the correct label) only if the guess is correct; otherwise providing a 0-reward. Note that this means Rmax \u2261 1 is a valid bound.\nThe target policy \u03c0 we used is the deterministic decision of a learned logistic regression classifier, while the logging policy \u00b5 we used is the probability estimates of a learned logistic regression classifier on a covariate-shifted data set. We simulate the covariate-shifted data set using the same technique as in Dud\u00edk et al. [2011], which follows standard practice as was described by Gretton et al. [2009].\nIn each data set with n rows, we treat the uniform distribution over the data set itself as a surrogate of the population distribution so that we know the ground truth of the rewards. Then, in the simulator, we randomly draw i.i.d. data sets of size [100, 200, 500, 1000, 2000, 5000, 10000, ...] until it reaches n, each repeating for 500 times, then compare different candidate estimators\u2019 MSE, estimated using the empirical averages of the square error over the 500 replicates, which we can calculate exactly since we know v\u03c0. It is worth pointing out that for some of the baselines that we compare to, e.g., IPS and DR, their distribution of square errors can have very large variance due to the potentially large importance weights. This leads to very large error bars if we aggregate their MSE with even 500 replicates. To circumvent this issue, we report a clipped version of the MSE that truncates any square errors to 1 if it is larger than 1, namely\nMSE = E[(v\u0302 \u2212 v\u03c0)2 \u2227 1].\nThis allows us to get valid confidence intervals for our empirical estimates of this quantity. Note that this does not change the MSE estimate of our approach at all, but is significantly more favorable towards IPS and DR. In this section, whenever we refer to\u201cMSE\u201d, we are referring to the truncated version.\nWe will compare the SWITCH-IPS and SWITCH-DR against the following baselines:\n1. IPS estimator\n2. Direct method via logistic regression\n3. Doubly robust estimator\n4. Truncated and reweighted IPS\n5. Trimmed IPS\nThe doubly robust is constructed by randomly splitting the data set into two halves, estimating DM on one half, combining with IPS on the other half, then switching the two sets and doing it again. The final estimate is the average of the two. This approach was suggested by Dud\u00edk et al. [2011], and is standard practice of doubly robust estimators when the direct method (or the oracle estimator) needs to be estimated from the data.\nWe also consider two additional variants of IPS, where importance weights are either capped at \u03c4 and renormalized [see, e.g., Bembom and van der Laan, 2008], or the terms with weights larger than \u03c4 are removed altogether as described in Bottou et al. [2013]. Note that the trimmed IPS is a special case of SWITCH when the direct estimator \u2261 0.\nFor both our methods (as well as the two IPS variants) we tune the parameter \u03c4 by the method described in Section 4.3. So everything is implementable in practice. As a reference, we are also including in the plot the results for the optimally tuned \u03c4 from hindsight, as well as the alternative ensemble approach via MAGIC. This allows us to gauge how well our automatic parameter tuning procedure works on these data sets.\nIn addition, we repeated the same experiments for a more challenging setting where the feedbacks are noisy. This is simulated by only revealing the correct reward with probability 0.5 and outputting a random coin toss with probability the other 0.5. Theoretically, this should lead to bigger \u03c32 and larger variance in all estimators.\nIn order to stay comparable across data sets, we use relative MSE with respect to the IPS. For each estimator v\u0302, report the relative MSE with respect to the IPS estimator\nRel.MSE(v\u0302) = MSE(v\u0302)\nMSE(v\u0302IPS) ,\nsuch that the numerical value is comparable across data sets and is independent to n. The results are summarized in Figure 12, in which methods that achieve smaller values of MSE are towards the top-left corner of the plot. In general, we want to maximize the area under the curve as these are CDF plots, and we notice the logarithmic scale for the relative MSE values on the X-axis. As we see, SWITCH-DR dominates all other methods and our empirical tuning of \u03c4 is not too far from the optimal possible. The advantage of SWITCH-DR is even stronger in the noisy-reward setting, where we add label noise to UCI data.\nIn Figure 2, we illustrate the convergence of MSE as n increases. In particular, we selected three data sets and show how SWITCH and SWITCH-DR perform against baselines in three typical cases: (i) when the direct method works better than IPS and DR; (ii) when the direct method works well initially but then as n gets large outperformed by IPS and DR; (iii) when the direct method works extremely poorly. In the first two cases, SWITCH-DR outperforms both DM and IPS, while DR only improves over IPS slightly. In the third case, SWITCH-DR performs about as well as IPS and DR despite that DM is very bad. In all cases, SWITCH-DR is robust to additional noise in the reward."}, {"heading": "6 Conclusion", "text": "In this paper we carried out minimax analysis of off-policy evaluation in contextual bandits and showed that IPS is optimal in the worst-case. This result highlights the need for using side information, potentially\n2For the clarity of the presentation, we excluded SWITCH, which also significantly outperforms either IPS but is dominated by SWITCH-DR. For the same reason, we combined the Trim and Truncated/reweighted IPS.\nprovided by modeling the reward directly, especially when importance weights are too large. Given this observation, we proposed a new class of estimators called SWITCH that can be used to combine any importance sampling estimators, including IPS and DR, with DM. The estimator involves adaptively switching to DM when the importance weights are large and switching to either IPS or DR when the importance weights are small. We showed that the new estimator has favorable theoretical properties and also works well on real-world data."}, {"heading": "Acknowledgments", "text": "The work was partially completed during YW\u2019s internship at Microsoft Research NYC from May 2016 - Aug 2016. The authors would like to thank Lihong Li and John Langford for helpful discussions, Edward Kennedy for bringing our attention to related problems and recent development in causal inference and Wei He for pointing us to [Sun, 2006] for rigorous a measure-theoretic treatment to a continuum of random variables and the corresponding law of large numbers.\nYW was supported by NSF Award BCS-0941518 to CMU Statistics, a grant by Singapore NRF under its International Research Centre @ Singapore Funding Initiative, and a Baidu Scholarship."}, {"heading": "A Proof of Theorem 1", "text": "In this appendix we prove the minimax bound of Theorem 1. The result is obtained by combining the following two lower bounds:\nTheorem 3 (Lower bound 1). For each problem instance such that E\u00b5[\u03c12\u03c32] <\u221e, we have\nRn(\u03c0;\u03bb, \u00b5, \u03c3,Rmax) \u2265 log 2E\u00b5[\u03c12\u03c32]\n8n\n1\u2212 E\u00b5[\u03c12\u03c321 ( \u03c1\u03c32 > Rmax \u221a nE\u00b5[\u03c12\u03c32]/(2 log 2) )] E\u00b5[\u03c12\u03c32] 2 . Theorem 4 (Lower bound 2). For each problem instance such that E\u00b5[\u03c12R2max] <\u221e, we have\nRn(\u03c0;\u03bb, \u00b5, \u03c3,Rmax)\n\u2265 log 2E\u00b5[\u03c1 2R2max]\n8n\n1\u2212 8\u03bb0 log(4/\u03bb0) log 2 \u2212 E\u00b5 [ \u03c12R2max1 ( \u03c1Rmax > \u221a nE\u00b5[\u03c12R2max]/(16 log 2) )] E\u00b5[\u03c12R2max] 2 , where \u03bb0 = maxx\u2208X \u03bb(x) is the largest probability of a single context.\nThe first bound captures the intrinsic difficulty in the variance of reward. The second result shows the additional dependence on R2max, even when \u03c3 \u2261 0, whenever the distribution \u03bb is sufficiently spread out over contexts, as quantified by the largest context probability \u03bb0. We next show how these two lower bounds yield Theorem 1 and then return to their proofs.\nProof of Theorem 1. We begin by simplifying the two lower bounds. Assume that Assumption 1 holds with and let p = 1 + /2 and q = 1 + 2/ , i.e., 1/p+ 1/q = 1. Then the definition of C means that\nC1/( q) = C1/(2+ ) = 2 \u00b7max E\u00b5 [ (\u03c12R2max) 2+ 2 ] 2 2+ E\u00b5 [ \u03c12R2max ] , E\u00b5[(\u03c12\u03c32) 2+ 2 ] 22+ E\u00b5 [ \u03c12\u03c32 ] \n= 2 \u00b7max\n{ E\u00b5 [ (\u03c12R2max) p ]1/p\nE\u00b5 [ \u03c12R2max ] , E\u00b5[(\u03c12\u03c32)p]1/p E\u00b5 [ \u03c12\u03c32 ] } , (14) and the assumption on n implies that\nn \u2265 max { (16 log 2)C1/ , (2 log 2)C2/ E\u00b5[\u03c32/R2max] } . (15)\nFirst, we simplify the correction term in the lower bound of Theorem 3. Using H\u00f6lder\u2019s inequality and Eq. (14), we have\nE\u00b5 [ \u03c12\u03c321 ( \u03c1\u03c32 > Rmax \u221a nE\u00b5[\u03c12\u03c32]/(2 log 2) )] \u2264 E\u00b5 [( \u03c12\u03c32\n)p]1/p \u00b7 P\u00b5[\u03c1\u03c32 > Rmax\u221anE\u00b5[\u03c12\u03c32]/(2 log 2)]1/q \u2264 1\n2 E\u00b5[\u03c12\u03c32] \u00b7 C1/( q) \u00b7 P\u00b5\n[ \u03c1\u03c32/Rmax > \u221a nE\u00b5[\u03c12\u03c32]/(2 log 2) ]1/q\nand by Markov\u2019s inequality, Cauchy-Schwartz inequality, and Eq. (15)\n\u2264 1 2 E\u00b5[\u03c12\u03c32] \u00b7 C1/( q) \u00b7\n( E\u00b5 [ \u03c1\u03c3 \u00b7 (\u03c3/Rmax) ]\u221a nE\u00b5[\u03c12\u03c32]/(2 log 2) )1/q\n\u2264 1 2 E\u00b5[\u03c12\u03c32] \u00b7 C1/( q) \u00b7\n(\u221a E\u00b5[\u03c12\u03c32] \u00b7 \u221a E\u00b5[\u03c32/R2max]\u221a\nnE\u00b5[\u03c12\u03c32]/(2 log 2)\n)1/q\n= 1\n2 E\u00b5[\u03c12\u03c32] \u00b7\n( C2/ \u00b7 2 log 2\nn \u00b7 E\u00b5[\u03c32/R2max]\n)1/2q \u2264 1\n2 E\u00b5[\u03c12\u03c32] . (16)\nFor the correction term in Theorem 4, we similarly have\nE\u00b5 [ \u03c12R2max1 ( \u03c1Rmax > \u221a nE\u00b5[\u03c12R2max]/(16 log 2) )] \u2264 E\u00b5 [( \u03c12R2max\n)p]1/p \u00b7 P\u00b5[\u03c1Rmax >\u221anE\u00b5[\u03c12R2max]/(16 log 2)]1/q \u2264 1\n2 E\u00b5[\u03c12R2max] \u00b7 C1/( q) \u00b7 P\u00b5\n[ \u03c12R2max > nE\u00b5[\u03c12R2max]/(16 log 2) ]1/q and by Markov\u2019s inequality and Eq. (15)\n\u2264 1 2 E\u00b5[\u03c12R2max] \u00b7 C1/( q) \u00b7\n( E\u00b5[\u03c12R2max]\nnE\u00b5[\u03c12R2max]/(16 log 2) )1/q = 1\n2 E\u00b5[\u03c12R2max] \u00b7\n( C1/ \u00b7 16 log 2\nn\n)1/q \u2264 1\n2 E\u00b5[\u03c12R2max] . (17)\nUsing Eq. (16), the bound of Theorem 3 simplifies as\nRn(\u03c0;\u03bb, \u00b5, \u03c3,Rmax)\n\u2265 log 2E\u00b5[\u03c1 2\u03c32]\n8n\n1\u2212 E\u00b5[\u03c12\u03c321 ( \u03c1\u03c32 > Rmax \u221a nE\u00b5[\u03c12\u03c32]/(2 log 2) )] E\u00b5[\u03c12\u03c32] 2\n\u2265 log 2E\u00b5[\u03c1 2\u03c32]\n8n\n( 1\u2212 1\n2\n)2 =\nlog 2E\u00b5[\u03c12\u03c32] 32n .\nSimilarly, by Eq. (17), Theorem 4 simplifies as\nRn(\u03c0;\u03bb, \u00b5, \u03c3,Rmax)\n\u2265 log 2E\u00b5[\u03c1 2R2max]\n8n\n1\u2212 8\u03bb0 log(4/\u03bb0) log 2 \u2212 E\u00b5 [ \u03c12R2max1 ( \u03c1Rmax > \u221a nE\u00b5[\u03c12R2max]/(16 log 2) )] E\u00b5[\u03c12R2max] 2\n\u2265 log 2E\u00b5[\u03c1 2R2max]\n8n\n[ 1\u2212 8\u03bb0 log(4/\u03bb0)\nlog 2 \u2212 1 2 ]2 \u2265 log 2E\u00b5[\u03c1 2R2max]\n8n\n(( 1\n2\n)2 \u2212 2 \u00b7 1\n2 \u00b7 8\u03bb0 log(4/\u03bb0) log 2\n) =\nlog 2E\u00b5[\u03c12R2max] 32n\n( 1\u2212 32\u03bb0 log(4/\u03bb0)\nlog 2\n) ,\nwhere in the last line we used (a\u2212 b)2 \u2265 a2 \u2212 2ab. Combining the two simplified bounds yields\nRn(\u03c0;\u03bb, \u00b5, \u03c3,Rmax)\n\u2265 1 2 \u00b7 log 2E\u00b5[\u03c1 2\u03c32] 32n + 1 2 \u00b7 log 2E\u00b5[\u03c1 2R2max] 32n\n( 1\u2212 32\u03bb0 log(4/\u03bb0)\nlog 2 ) \u2265 log 2\n64n\n[ E\u00b5[\u03c12\u03c32] + E\u00b5[\u03c12R2max] ( 1\u2212 32\nlog 2 \u00b7 \u03bb0 log(4/\u03bb0) )] \u2265 1\n220n\n[ E\u00b5[\u03c12\u03c32] + E\u00b5[\u03c12R2max] ( 1\u2212 110\u03bb0 log(4/\u03bb0) )] .\nIt remains to prove Theorems 3 and 4. They are both proved by a reduction to hypothesis testing, and invoke Le Cam\u2019s argument to lower-bound the error in this testing problem. As in most arguments of this nature, the key novelty lies in the construction of an appropriate testing problem that leads to the desired lower bounds. Before proving the theorems, we recall the basic result of Le Cam which underlies our proofs. We point the reader to the excellent exposition of Lafferty et al. [2008, Section 36.4] on more details about Le Cam\u2019s argument.\nTheorem 5 (Le Cam\u2019s method Lafferty et al., 2008, Theorem 36.8). Let P be a set of distributions. For any pair P0, P1 \u2208 P ,\ninf \u03b8\u0302 sup P\u2208P\nEP [d(\u03b8\u0302, \u03b8(P ))] \u2265 \u2206\n4\n\u222b [pn0 \u2227 pn1 ]dx \u2265 \u2206\n8 e\u2212nDKL(P0\u2016P1). (18)\nwhere \u2206 = d(\u03b8(P0), \u03b8(P1))\nWhile the proofs of the two theorems share a lot of similarities, they have to use reductions to slightly different testing problems given the different mean and variance constraints in the two results. We begin with the proof of Theorem 3, which has a simpler construction.\nA.1 Proof of Theorem 3\nThe basic idea of this proof is to reduce the problem of policy evaluation to that of Gaussian mean estimation where there is a mean associated with each x, a pair. We now describe our construction.\nCreating a family of problems Since we aim to show a lower bound on the hardness of policy evaluation in general, it suffices to show a particular family of hard problem instances, such that every estimator requires the stated number of samples on at least one of the problems in this family. Recall that our minimax setup assumes that \u03c0, \u00b5 and \u03bb are fixed and the only aspect of the problem which we can design is the conditional reward distribution D(r | x, a). For Theorem 3, this choice is further constrained to satisfy E[r | x, a] \u2264 Rmax(x, a) and Var(r | x, a) \u2264 \u03c32. In order to describe our construction, it will be convenient to define the shorthand E[r | x, a] = \u03b7(x, a). We will identify a problem in our family with the function \u03b7(x, a) as that will be the only changing element in our problems. For a chosen \u03b7, the policy evaluation question boils down to estimating v\u03c0\u03b7 = E[r(x, a)], where the contexts x are chosen according to \u03bb, actions are drawn from \u03c0(x, a) and the reward distribution D\u03b7(r | x, a) is a normal distribution with mean \u03b7(x, a)\nD\u03b7(r | x, a) = N (\u03b7(x, a), \u03c32).\nClearly this choice meets the variance constraint by construction, and satisfies the upper bound so long as \u03b7(x, a) \u2264 Rmax(x, a) almost surely. Since the evaluation policy \u03c0 is fixed throughout, we will drop the superscript and use v\u03b7 to denote ver\u03c0 in the remainder of the proofs. With some abuse of notation, we also\nuse E\u03b7[\u00b7] to denote expectations where contexts and actions are drawn based on the fixed choices \u03bb and \u00b5 corresponding to our data generating distribution, and the rewards drawn from \u03b7. We further use P\u03b7 to denote this entire joint distribution over (x, a, r) triples.\nGiven this family of problem instances, it is easy to see that for any pair of \u03b71, \u03b72 which are both pointwise upper bounded by Rmax, we have the lower bound:\nRn(\u03bb, \u03c0, \u00b5, \u03c3 2, Rmax) \u2265 inf\nv\u0302 max \u03b7\u2208\u03b71,\u03b72\nE\u03b7 [\n(v\u0302 \u2212 v\u03b7)2\ufe38 \ufe37\ufe37 \ufe38 `\u03b7(v\u0302)\n] ,\nwhere we have introduced the shorthand `\u03b7i(v\u0302) to denote the squared error of v\u0302 to v\u03b7i . For a parameter > 0 to be chosen later, we can further lower bound this risk as\nRn(\u03bb, \u03c0, \u00b5, \u03c3 2, Rmax) \u2265 max \u03b7i\u2208\u03b71,\u03b72 E\u03b7[`\u03b7(v\u0302)] \u2265 max \u03b7i\u2208\u03b71,\u03b72 P\u03b7(`\u03b7 \u2265 )\n\u2265 2 (P\u03b71(`\u03b71(v\u0302) \u2265 ) + P\u03b72(`\u03b72(v\u0302) \u2265 )) , (19)\nwhere the last inequality lower bounds the maximum by the average. So far we have been working with an estimation problem. We next describe how to reduce this to a hypothesis testing problem.\nReduction to hypothesis testing For turning our estimation problem into a testing problem, the idea is to identify a pair \u03b71, \u03b72 such that they are far enough from each other so that any estimator which gets a small estimation loss can essentially identify whether the data generating distribution corresponds to P\u03b71 or P\u03b72 . In order to do this, we take any estimator v\u0302 and identify a corresponding test statistic which maps v\u0302 into one of \u03b71, \u03b72. The way to do this is essentially identified in Equation 19, and we describe it next.\nNote that since we are constructing a hypothesis test for the specific family of distributions P\u03b71 and P\u03b72 , it is reasonable to consider test statistics which have knowledge of \u03b71 and \u03b72, and hence the corresponding distributions. Consequently, these tests also know the true policy values v\u03b71 and v\u03b72 and the only uncertainty is which of them gave rise to the observed data samples. Therefore, for any estimator v\u0302, we can a associate a statistic \u03c6(v\u0302) = argmin\u03b7 {`\u03b71(v\u0302), `\u03b72(v\u0302)}.\nGiven this hypothesis test, we are interested in its error rate P\u03b7(\u03c6(v\u0302) 6= \u03b7). We first relate the estimation error of v\u0302 to the error rate of the test. Suppose for now that\n`\u03b71(v\u0302) + `\u03b71(v\u0302) \u2265 2 , (20)\nso that at least one of the losses is at least . Suppose further that \u03c6(v\u0302) = \u03b71, so that we know `\u03b71(v\u0302) \u2264 `\u03b72(v\u0302). Then we are assured that `\u03b71(v\u0302) \u2264 assures the test\u2019s correctness by Equation 20, since the other loss is guaranteed to be at least in this condition. The reasoning for the loss under \u03b72 being small is similar, so that a valid upper bound on our test\u2019s error (under a uniform choice of \u03b71, \u03b72) is given by\nmax \u03b7\u2208\u03b71,\u03b72\nP\u03b7(\u03c6(v\u0302) 6= \u03b7) = 1\n2 (P\u03b71(\u03c6(v\u0302) 6= \u03b71) + P\u03b72(\u03c6(v\u0302) 6= \u03b72))\n\u2264 1 2 (P\u03b71(`\u03b71(v\u0302) \u2265 ) + P\u03b72(`\u03b72(v\u0302) \u2265 ) \u2264 1 Rn(\u03bb, \u03c0, \u00b5, \u03c3 2, Rmax), (21)\nwhere the final inequality uses our earlier lower bound in Equation 19.\nIt remains to establish our earlier supposition (20) to finish the relating our testing and estimation problems. Assume for now that \u03b71 and \u03b72 are chosen such that\n(v\u03b71 \u2212 v\u03b72) 2 \u2265 4 . (22)\nThen an application of Cauchy-Shwartz inequality (a+ b)2 \u2264 2a2 + 2b2 yields\n4 \u2264 (v\u03b71 \u2212 v\u03b72) 2 \u2264 2(v\u0302 \u2212 v\u03b71)2 + 2(v\u0302 \u2212 v\u03b72)2 \u2264 2`\u03b71(v\u0302) + 2`\u03b72(v\u0302),\nwhich yields the posited bound 19.\nInvoking Le Cam\u2019s argument So far we have identified a hypothesis testing problem and a test statistic whose error is upper bounded in terms of the minimax risk of our problem. In order to complete the proof, we now place a lower bound on the error of this test statistic. Recall the result of Le Cam 18, which places an upper bound on the attainable error in any testing problem. In our setting, this translates to\nmax \u03b7\u2208\u03b71,\u03b72 P\u03b7(\u03c6(v\u0302) 6= \u03b7) \u2265 exp (\u2212nDKL(P\u03b71 || P\u03b72)) .\nSince the distribution of the rewards is a spherical Gaussian, the KL-divergence is given by the squared distance between the means, scaled by the variance, that is\nDKL(P\u03b71 || P\u03b72) = E [ (\u03b71(x, a)\u2212 \u03b72(x, a))2\n2\u03c32\n] ,\nwhere we recall that the contexts and actions are drawn from \u03bb and \u00b5 respectively. Since we would like the probability of error in the test to be a constant, it suffices to choose \u03b71 and \u03b72 such that\nE [ (\u03b71(x, a)\u2212 \u03b72(x, a))2\n2\u03c32\n] \u2264 log 2\nn . (23)\nPicking the parameters So far, we have not made any concrete choices for \u03b71 and \u03b72, apart from some constraints which we have introduced along the way. Note that we have the constraints (22) and (23) which try to ensure that \u03b71 and \u03b72 are not too close that an estimator does not have to identify the true parameter, or too far that the testing problem becomes trivial. Additionally, we have the upper and lower bounds of 0 and Rmax on \u03b71 and \u03b72. In order to reason about these constraints, it is convenient to set \u03b72 \u2261 0, and pick \u03b71(x, a) = \u03b71(x, a)\u2212 \u03b72(x, a) = \u2206(x, a). We now write all our constraints in terms of \u2206.\nNote that v\u03b72 is now 0, so that the first constraint (22) is equivalent to\nv\u03b71 = E\u03b71 [\u03c1(x, a)r(x, a)] = E\u2206[\u03c1(x, a)r(x, a)] \u2265 2 \u221a ,\nwhere the importance weighting function \u03c1 is introduced since P\u03b71 is based on choosing actions according to \u00b5 and we seek to evaluate \u03c0. The second constraint (23) is also straightforward\nE [ \u22062\n2\u03c32\n] \u2264 log 2\nn .\nThe bound constraints are respected by imposing 0 \u2264 \u2206(x, a) \u2264 Rmax(x, a) almost surely. The minimax lower bound is then yielded by the largest in the constraint (22) such that the other two constraints can be satisfied. This gives rise to the following variational characterization of the minimax lower bound\nmax \u2206\nsuch that E\u2206[\u03c1(x, a)r(x, a)] \u2265 2 \u221a ,\nE [ \u22062\n2\u03c32\n] \u2264 log 2\nn and\n0 \u2264 \u2206(x, a) \u2264 Rmax(x, a).\nInstead of finding the optimal solution, we show a feasible setting of \u2206 here. We set\n\u2206 = min\n{ \u03b1\u03c32\u03c1\nE\u00b5[\u03c12\u03c32] , Rmax\n} , where \u03b1 = \u221a 2 log 2\nE\u00b5[\u03c12\u03c32] n . (24)\nThis setting satisfies the bound constraints by construction. A quick substitution also verifies that the second constraint is met. Consequently, it suffices to set to the value attained in the first constraint. Substituting the value of \u2206 in the constraint, we see that\nE\u2206[\u03c1(x, a)r(x, a)] = Ex\u223c\u03bb,a\u223c\u00b5[\u03c1(x, a)\u2206(x, a)] \u2265 Ex\u223c\u03bb,a\u223c\u00b5 [ \u03c1 \u03b1\u03c32\u03c1\nE\u00b5[\u03c12\u03c32] 1(\u03c1\u03c32\u03b1 \u2265 RmaxE\u00b5[\u03c12\u03c32]) ] = \u03b1 ( 1\u2212 E\u00b5[\u03c1\n2\u03c321(\u03c1\u03c32\u03b1 \u2264 RmaxE\u00b5[\u03c12\u03c32])] E\u00b5[\u03c12\u03c32]\n) .\nTaking squares and dividing by 4 (since this is the lower bound on 2 \u221a yields the statement of the theorem.\nA.2 Proof of Theorem 4\nWe now give the proof of Theorem 4. This shares a lot of common ideas with the proof of Theorem 3, but with a crucial difference. In Theorem 3, there is non-trivial noise in the reward function, unlike in Theorem 4. This allowed the proof to work with just two candidate mean reward functions, since any realization in the data is corrupted with noise. However, in the absence of added noise, the task of mean identification becomes rather trivial: an estimator can just check whether \u03b71 or \u03b72 matches the observations exactly.\nIn order to avoid this pathology, we instead construct a richer family of reward functions which will be described next. In this construction, rather than just two mean rewards, the construction will involve a randomized design of the expected reward function from an appropriate prior distribution. The draw of the mean reward from a prior will essentially generate noise even though any given problem is noiseless. The construction will also highlight the crucial sources of difference between the contextual and multi-armed bandit problems, since the arguments rely on having a rich context distribution.\nCreating a family of problems As before, we fix \u03c0, \u00b5 and \u03bb and parametrize the problem distribution in terms of the mean reward function \u03b7(x, a). However, now \u03b7(x, a) is itself a random variable, which is drawn from a prior distribution. The distribution of \u03b7(x, a) is a scaled Bernoulli, parametrized by a prior function \u03b8(x, a) as described below:\n\u03b7(x, a) =\n{ Rmax(x, a) with probability \u03b8(x, a)\n0 with probability 1\u2212 \u03b8(x, a) . (25)\nWe now set D\u03b7(r | x, a) = \u03b7(x, a). This clearly satisfies the constraints on the mean 0 \u2264 E[r | x, a] \u2264 Rmax(x, a) by construction, and also Var(r | x, a) = 0 as per the setting of Theorem 4. The goal of an estimator is to take n samples generated according with x \u223c \u03bb, a | x \u223c \u00b5 and r | x, a \u223c D\u03b7 and output an estimate v\u0302 such that E\u03b7[(v\u0302 \u2212 v\u03c0\u03b7 )2] is small. We recall our earlier shorthand v\u03b7 to denote the value of \u03c0 under the reward distribution generated by \u03b7. For showing a lower bound on this quantity, it is clearly sufficient to pick any prior distribution governed by a parameter \u03b8 as in Equation 25, and instead lower bound E\u03b8 [ E\u03b7[(v\u0302 \u2212 v\u03b7)2 | \u03b7] ] . If the expectation is large for an estimator v\u0302, then there must be at least one function \u03b7(x, a) which induces a large error for v\u0302 which is the goal of the lower bound. Consequently, we focus in the proof on lower bounding this quantity, which can be decomposed by Cauchy-Shwartz inequality into:\nE\u03b8 [ E\u03b7[(v\u0302 \u2212 v\u03b7)2 | \u03b7] ] \u2265 1 2 E\u03b8 [ E\u03b7[(v\u0302 \u2212 E\u03b8[v\u03b7])2 | \u03b7] ] \u2212 E\u03b8 [ (v\u03b7 \u2212 E\u03b8[v\u03b7])2 ] .\nThen we can further take a worst-case over all problems in the above inequality to obtain\nsup \u03b7 E\u03b7[(v\u0302 \u2212 v\u03b7)2] \u2265 sup \u03b8\nE\u03b8 [ E\u03b7[(v\u0302 \u2212 v\u03b7)2 | \u03b7] ] \u2265 sup\n\u03b8\n1 2 E\u03b8 [ E\u03b7[(v\u0302 \u2212 E\u03b8[v\u03b7])2 | \u03b7] ] \ufe38 \ufe37\ufe37 \ufe38\nT1\n\u2212E\u03b8 [ (v\u03b7 \u2212 E\u03b8[v\u03b7])2 ]\ufe38 \ufe37\ufe37 \ufe38 T2 . (26)\nThis decomposition is rather interesting. It says that the expected MSE of an estimator in estimating v\u03b7 can be related to the MSE of the same estimator in estimating the quantity E\u03b8[v\u03b7], as long as the variance of the quantity v\u03b7 under the distribution generated by \u03b8 is not too large. This is a very important observation, since we can now choose to instead study the MSE of an estimator in estimating E\u03b8[v\u03b7] as captured by T1. Unlike the distribution D\u03b7 which is degenerate, this problem has a non-trivial noise arising from the randomized draw of \u03b7 according to \u03b8. Thus we can use similar techniques as the proof of Theorem 4, albeit where the reward distribution is a scaled Bernoulli instead of Gaussian. For now, we focus on controlling T1, and T2 will be handled later.\nIn order to bound T1, we will consider two carefully designed choices \u03b81 and \u03b82 to induce two different problem instances and show that T1 is large for any estimator under one of the two parameters. In doing this, it would be convenient to use the additional shorthand `\u03b8(v\u0302) = (v\u0302 \u2212 E\u03b8[v\u03b7])2. Proceeding as in the proof of Theorem 3, we have\nT1 = sup \u03b8\nE\u03b8 [ E\u03b7[(v\u0302 \u2212 E\u03b8[v\u03b7])2 | \u03b7] ] \u2265 sup\n\u03b8 E\u03b8 [E\u03b7[`\u03b8(v\u0302) | \u03b7]]\n\u2265 sup \u03b8 P\u03b8 (`\u03b8(v\u0302) \u2265 ) \u2265 max \u03b8\u2208{\u03b81,\u03b82} P\u03b8 (`\u03b8(v\u0302) \u2265 )\n\u2265 2 [P\u03b81 (`\u03b81(v\u0302) \u2265 ) + P\u03b82 (`\u03b82(v\u0302) \u2265 )] .\nReduction to hypothesis testing As in the proof of Theorem 3, we now reduce the estimation problem into a hypothesis test for whether the data was generating according to the parameters \u03b81 or \u03b82. The arguments here are similar to the earlier proof, and we will be a bit terse in this presentation.\nAs before, our hypothesis test has entire knowledge ofD\u03b7 as well as \u03b81 and \u03b82. Consequently, we construct a test based on picking \u03b81 whenever `\u03b81(v\u0302) \u2264 `\u03b82(v\u0302). As before, we will ensure that |E\u03b81 [v\u03b7]\u2212E\u03b82 [v\u03b7]| \u2265 2 \u221a so that for any estimator v\u0302, we have `\u03b81(v\u0302) + `\u03b82(v\u0302) \u2265 2 .\nBy a similar argument, we further conclude that the error of our hypothesis test under a uniform prior on the choices of \u03b8 is at most\n1 2 [P\u03b81 (`\u03b81(v\u0302) \u2265 ) + P\u03b82 (`\u03b82(v\u0302) \u2265 )]\nInvoking Le Cam\u2019s argument Once again, we can lower bound the error rate of our test by invoking the result of Le Cam. Doing so, requires an upper bound on the KL-divergence DKL(P\u03b81\u2016P\u03b82). The only difference from our earlier argument is that these distributions are now Bernoullis instead of Normals based on the construction in Equation 25. More formally, we have\nDKL(P\u03b81\u2016P\u03b82) = \u222b log p(r; \u03b81(x, a))\np(r; \u03b82(x, a)) dp(r; \u03b81(x, a)) dp(x) d\u00b5(a|x)\n= EDKL(Ber(\u03b81(x, a)\u2016Ber(\u03b82(x, a)))).\nInvoking Lemma 4, we can further upper bound this as\nDKL(P\u03b81\u2016P\u03b82) \u2264 1 4 E\u00b5 [ (\u03b81(x, a)\u2212 \u03b82(x, a))2 ] .\nPicking the parameters It remains to carefully choose \u03b81 and \u03b82. We define \u03b82 to be a uniform prior: \u03b82(x, a) \u2261 0.5, and let \u03b81(x, a) = \u03b82(x, a) + \u2206(x, a), where \u2206(x, a) will be chosen to satisfy certain constraints as before. Collecting the constraints, we get the maximization problem\nmax \u2206\nsuch that E\u00b5[\u03c1(x, a)\u2206(x, a)Rmax(x, a)] \u2265 2 \u221a ,\n1 4 E [ \u2206(x, a)2 ] \u2264 log 2 n and 0 \u2264 \u2206(x, a) \u2264 0.5.\nFor some \u03b1 > 0 to be determined shortly, we pick\n\u2206 = min\n{ \u03c1Rmax\u03b1\nE\u00b5\u03c12R2max , 0.5\n} .\nThe bound constraints are satisfied by construction and we set \u03b1 = \u221a\n4 log 2E\u00b5\u03c12R2max/n to satisfy the second constraint. A feasible choice of then yields\n2 \u221a = E\u00b5[\u03c1(x, a)\u2206(x, a)Rmax(x, a)] \u2265 E\u00b5\n[ \u03c12R2max\u03b11(\u03c1Rmax \u2264 E\u00b5[\u03c12R2max]/2\u03b1)\nE\u00b5[\u03c12\u03b12] ] = \u03b1\u2212 \u03b1 E\u00b5 [ \u03c12R2max1(\u03c1Rmax \u2264 E\u00b5[\u03c12R2max]/2\u03b1)\n] E\u00b5[\u03c12R2max] .\nCollecting our arguments this far, we have established that\nT1 \u2265 2 [P\u03b81 (`\u03b81(v\u0302) \u2265 ) + P\u03b82 (`\u03b82(v\u0302) \u2265 )]\n\u2265 2 exp(\u2212nDKL(P\u03b81\u2016P\u03b82)) \u2265 4\n\u2265 log 2E\u00b5\u03c1 2R2max\n4n\n1\u2212 E\u00b5 ( \u03c12R2max1 ({ \u03c1Rmax > \u221a nE\u00b5(\u03c12R2max)/(16 log 2) })) E\u00b5\u03c12R2max 2\nIn order to complete the proof, we need to further upper bound T2 in the decomposition (26). This is done in Lemma 2, and plugging this upper bound in Equation 26 completes the proof.\nLemma 2. Let \u03bb(x) be any context distribution. Then for any distribution \u03b8 over the random variable \u03b7(x, a) = E[r | x, a], satisfying 0 \u2264 \u03b7(x, a) \u2264 Rmax(x, a), we have\nE\u03b8 [ (v\u03b7 \u2212 E\u03b8[v\u03b7])2 ] \u2264 \u03bb0 log(4/\u03bb0)E\u00b5[R2max\u03c12] ,\nwhere \u03bb0 = maxx\u2208X \u03bb(x).\nProof. We start by bounding the range of (v\u03b7 \u2212 E\u03b8[v\u03b7])2, viewed as a random variable under \u03b8. From the definition of v\u03b7,\n0 \u2264 v\u03b7 \u2264 E\u03c0[Rmax] = E\u00b5[\u03c1Rmax] ,\nso also 0 \u2264 E\u03b8[v\u03b7] \u2264 E\u00b5[\u03c1Rmax]. Hence, |v\u03b7 \u2212 E\u03b8[v\u03b7]| \u2264 E\u00b5[\u03c1Rmax], and we obtain the bound\n(v\u03b7 \u2212 E\u03b8[v\u03b7])2 \u2264 (E\u00b5[\u03c1Rmax])2 \u2264 E\u00b5[\u03c12R2max], (27)\nbecause \u03c1Rmax \u2265 0. The proof proceeds by applying Hoeffding\u2019s inequality to control the probability that (v\u03b7\u2212E\u03b8[v\u03b7])2 \u2265 t2 for a suitable t. Then we can, with high probability, use the bound (v\u03b7 \u2212 E\u03b8[v\u03b7])2 \u2265 t2, and with the remaining small probability apply the bound of Eq. (27).\nTo apply Hoeffding\u2019s inequality, we write v\u03b7 explicitly as v\u03b7 = \u2211 x\u2208X \u2211 a\u2208A \u03b7(x, a)\u03c1(x, a)\u00b5(a|x)\u03bb(x) =: \u2211 x\u2208X \u2211 a\u2208A Y (x, a).\nThat is, the quantity v\u03b7 can be written as a sum of a finite number of independent random variables, where we use the fact that the number of contexts is finite. Note that \u00b5(a|x) \u2264 1 and \u03bb(x) \u2264 \u03bb0, so each summand obeys\n0 \u2264 Y (x, a) \u2264 Rmax(x, a)\u03c1(x, a) \u221a \u00b5(a|x)\u03bb(x) \u221a \u03bb(x) \u2264 Rmax(x, a)\u03c1(x, a) \u221a \u00b5(a|x)\u03bb(x)\u03bb0 .\nBy Hoeffding\u2019s inequality, we have P(|v\u03b7 \u2212 E\u03b8v\u03b7| \u2265 t) \u2264 2 exp { \u2212 2t\n2\u2211 x\u2208X \u2211 a\u2208AR 2 max(x, a)\u03c1 2(x, a)\u00b5(a|x)\u03bb(x)\u03bb0 } = 2 exp { \u2212 2t 2\n\u03bb0E\u00b5[R2max\u03c12]\n} .\nTake t = \u221a \u03bb0 log(4/\u03bb0)E\u00b5[R2max\u03c12]/2 in the above bound, which yields\nP [ (v\u03b7 \u2212 E\u03b8v\u03b7)2 \u2265 t2 ] = P [ |v\u03b7 \u2212 E\u03b8v\u03b7| \u2265 t ] \u2264 \u03bb0\n2 .\nThus, using Eq. (27), we have\nE\u03b8 [ (v\u03b7 \u2212 E\u03b8v\u03b7)2 ] \u2264 t2P [ (v\u03b7 \u2212 E\u03b8v\u03b7)2 < t2 ] + E\u00b5[\u03c12R2max] \u00b7 P [ (v\u03b7 \u2212 E\u03b8v\u03b7)2 \u2265 t2 ] \u2264 t2 + E\u00b5[\u03c12R2max] \u00b7\n\u03bb0 2\n= \u03bb0 log(4/\u03bb0)E\u00b5[R2max\u03c12]\n2 + E\u00b5[\u03c12R2max] \u00b7 \u03bb0 2\n\u2264 \u03bb0 log(4/\u03bb0)E\u00b5[R2max\u03c12] ."}, {"heading": "B Other proofs", "text": "Proof of Lemma 1. The variance (therefore MSE) of the importance weighting estimator can be decomposed into two parts\nVar (v\u0302IPS) = E\u00b5(Var(v\u0302IPS|x, a) + Var\u00b5(E(v\u0302IPS|x, a)) = 1\nn E\u00b5[\u03c12Var(r|x, a)] +\n1 n Var\u00b5[\u03c1E(r|x, a)]\n\u2264 1 n E\u00b5[\u03c12\u03c32] + 1 n E\u00b5[\u03c12E(r|x, a)2] \u2264 1 n E\u00b5[\u03c12\u03c32] + 1 n E\u00b5[\u03c12R2max]\nProof of Theorem 2. Denote Ax \u2286 A to be Ax = { a \u2208 A \u2223\u2223\u2223\u03c0(a|x)\u00b5(a|x) \u2264 \u03c4}. For brevity, we also denote Ai := Axi . We decompose the mean square error and control each term separately.\nMSE(v\u0302SWITCH) = |Ev\u0302SWITCH \u2212 v\u03c0|2 + Var(v\u0302SWITCH).\nWe first calculate the bias. Note that bias is incurred only in the terms that fall in ACx , so that\nEv\u0302SWITCH \u2212 v\u03c0 = E \u2211 a\u2208Acx r\u0302(x, a)\u03c0(a|x) \u2212 E \u2211 a\u2208Acx E(r|x, a)\u03c0(a|x)  = E\u03c0 [(r\u0302(x, a)\u2212 E(r|x, a))1(a \u2208 Acx)] = E\u03c0 [r\u0302(x, a)\u2212 E(r|x, a)| a \u2208 Acx]P\u03c0(a \u2208 Acx) = E\u03c0[ (x, a)| a \u2208 Acx]P\u03c0(a \u2208 Acx),\nwhere we recall that (x, a) = r\u0302(x, a)\u2212 E[r| x, a]. Next we upper bound the variance. Note that the variance contributions from the IPS part and the DM part are not independent, since the indicators \u03c1(xi, a) > \u03c4 and \u03c1(xi, a) \u2264 \u03c4 are mutually exclusive. To simplify the analysis, we use the following inequality that holds for any random variable X and Y :\nVar(X + Y ) \u2264 2Var(X) + 2Var(Y ).\nThis allows us to calculate the variance of each part separately.\nVar(v\u0302SWITCH) \u22642 Var\n( 1\nn n\u2211 i=1 [ri\u03c1i1(ai \u2208 Ai)]\n) + 2 Var ( 1\nn n\u2211 i=1 \u2211 a\u2208A r\u0302(x, a)\u03c0(a|x)1(a \u2208 ACx )\n)\n= 2\nn Var\u00b5 [r\u03c11(a \u2208 Ax)] +\n2 n Var \u2211 a\u2208Acx r\u0302(x, a)\u03c0(a|x)  = 2\nn E\u00b5Var [r\u03c11(a \u2208 Ax)| x, a] +\n2 n Var\u00b5E[r\u03c11(a \u2208 Ax)| x, a] + 2 n Var \u2211 a\u2208Acx r\u0302(x, a)\u03c0(a|x)  \u2264 2 n E\u00b5Var [r\u03c11(a \u2208 Ax)| x, a] + 2 n E\u00b5(E[r\u03c11(a \u2208 Ax)| x, a])2 + 2 n E \u2211 a\u2208Acx r\u0302(x, a)\u03c0(a|x) 2 \u2264 2 n E\u00b5 [ \u03c32\u03c121(a \u2208 Ax) ] + 2 n E\u00b5 [ R2max\u03c1 21(a \u2208 Ax) ] + 2 n E (\u2211 a\u2208Ac r\u0302(x, a)\u03c0(a|x) )2 .\nTo complete the proof, note that the first term is further upper bounded using Jensen\u2019s inequality as\nE \u2211 a\u2208Acx r\u0302(x, a)\u03c0(a|x) 2 = E \u2211 a\u2208ACx \u03c0(a|x) 2( \u2211 a\u2208Aci r\u0302(x, a)\u03c0(a|x)\u2211 a\u2208ACx \u03c0(a|x) )2 \u2264 E\n\u2211 a\u2208ACx \u03c0(a|x) \u2211 a\u2208ACx r\u0302(x, a)2\u03c0(a|x)  \u2264 E\u03c0 [ R2max1(\u03c1 > \u03c4) ] ,\nwhere the final inequality uses \u2211\na\u2208ACx \u03c0(a|x) \u2264 1 and r\u0302(x, a) \u2208 [0, Rmax(x, a)] almost surely. Combining the bias and variance bounds, we get the stated MSE upper bound."}, {"heading": "C Utility Lemmas", "text": "Lemma 3 (Hoeffding, 1963, Theorem 2). Let Xi \u2208 [ai, bi] and X1, ..., Xn are drawn independently. Then the empirical mean X\u0304 = 1n(X1 + ...+Xn) obeys\nP(|X\u0304 \u2212 E[X\u0304]| \u2265 t) \u2264 2e \u2212 2n 2t2\u2211n i=1 (bi\u2212ai)2 .\nLemma 4 (Bernoulli KL-divergence). For 0 < p, q < 1, we have\nDKL(Ber(p)\u2016Ber(q)) \u2264 (p\u2212 q)2( 1\nq +\n1\n1\u2212 q ).\nProof.\nDKL(Ber(p)\u2016Ber(q)) = p log( p q ) + (1\u2212 p) log(1\u2212 p 1\u2212 q )\n\u2264 pp\u2212 q q + (1\u2212 p)q \u2212 p 1\u2212 q = (p\u2212 q)2 q + (p\u2212 q) + (p\u2212 q) 2 1\u2212 q + (q \u2212 p)\n= (p\u2212 q)2(1 q + 1 1\u2212 q )."}], "references": [{"title": "Doubly robust estimation in missing data and causal inference models", "author": ["Heejung Bang", "James M Robins"], "venue": null, "citeRegEx": "Bang and Robins.,? \\Q2005\\E", "shortCiteRegEx": "Bang and Robins.", "year": 2005}, {"title": "Data-adaptive selection of the truncation level for inverseprobability-of-treatment-weighted estimators", "author": ["Oliver Bembom", "Mark J van der Laan"], "venue": null, "citeRegEx": "Bembom and Laan.,? \\Q2008\\E", "shortCiteRegEx": "Bembom and Laan.", "year": 2008}, {"title": "Counterfactual reasoning and learning systems: the example of computational advertising", "author": ["L\u00e9on Bottou", "Jonas Peters", "Joaquin Quinonero Candela", "Denis Xavier Charles", "Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Y Simard", "Ed Snelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Some results on generalized difference estimation and generalized regression estimation for finite populations", "author": ["Claes M Cassel", "Carl E S\u00e4rndal", "Jan H Wretman"], "venue": null, "citeRegEx": "Cassel et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Cassel et al\\.", "year": 1976}, {"title": "Learning bounds for importance weighting", "author": ["Corinna Cortes", "Yishay Mansour", "Mehryar Mohri"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Doubly robust policy evaluation and learning", "author": ["Miroslav Dud\u00edk", "John Langford", "Lihong Li"], "venue": "In ICML,", "citeRegEx": "Dud\u00edk et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u00edk et al\\.", "year": 2011}, {"title": "Doubly robust policy evaluation and optimization", "author": ["Miroslav Dud\u00edk", "Dumitru Erhan", "John Langford", "Lihong Li"], "venue": "Statistical Science,", "citeRegEx": "Dud\u00edk et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dud\u00edk et al\\.", "year": 2014}, {"title": "Covariate shift by kernel mean matching", "author": ["Arthur Gretton", "Alex Smola", "Jiayuan Huang", "Marcel Schmittfull", "Karsten Borgwardt", "Bernhard Sch\u00f6lkopf"], "venue": "Dataset shift in machine learning,", "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Wassily Hoeffding"], "venue": "Journal of the American statistical association,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "Statistics and causal inference", "author": ["Paul W Holland"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Holland.,? \\Q1986\\E", "shortCiteRegEx": "Holland.", "year": 1986}, {"title": "A generalization of sampling without replacement from a finite universe", "author": ["Daniel G Horvitz", "Donovan J Thompson"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Horvitz and Thompson.,? \\Q1952\\E", "shortCiteRegEx": "Horvitz and Thompson.", "year": 1952}, {"title": "Doubly robust off-policy evaluation for reinforcement learning", "author": ["Nan Jiang", "Lihong Li"], "venue": "In ICML\u201916,", "citeRegEx": "Jiang and Li.,? \\Q2016\\E", "shortCiteRegEx": "Jiang and Li.", "year": 2016}, {"title": "Unbiased offline evaluation of contextual-banditbased news article recommendation algorithms", "author": ["Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Toward minimax off-policy value estimation", "author": ["Lihong Li", "R\u00e9mi Munos", "Csaba Szepesv\u00e1ri"], "venue": "In AISTATS,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Statistical analysis with missing data", "author": ["Roderick JA Little", "Donald B Rubin"], "venue": null, "citeRegEx": "Little and Rubin.,? \\Q2002\\E", "shortCiteRegEx": "Little and Rubin.", "year": 2002}, {"title": "Weighting adjustment for unit nonresponse", "author": ["H Lock Oh", "Frederick J Scheuren"], "venue": "Incomplete data in sample surveys,", "citeRegEx": "Oh and Scheuren.,? \\Q1983\\E", "shortCiteRegEx": "Oh and Scheuren.", "year": 1983}, {"title": "Semiparametric efficiency in multivariate regression models with missing data", "author": ["James M Robins", "Andrea Rotnitzky"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Robins and Rotnitzky.,? \\Q1995\\E", "shortCiteRegEx": "Robins and Rotnitzky.", "year": 1995}, {"title": "Improved double-robust estimation in missing data and causal inference models", "author": ["Andrea Rotnitzky", "Quanhong Lei", "Mariela Sued", "James M Robins"], "venue": null, "citeRegEx": "Rotnitzky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rotnitzky et al\\.", "year": 2012}, {"title": "The exact law of large numbers via fubini extension and characterization of insurable risks", "author": ["Yeneng Sun"], "venue": "Journal of Economic Theory,", "citeRegEx": "Sun.,? \\Q2006\\E", "shortCiteRegEx": "Sun.", "year": 2006}, {"title": "Data-efficient off-policy policy evaluation for reinforcement learning", "author": ["Philip S Thomas", "Emma Brunskill"], "venue": "In ICML\u201916,", "citeRegEx": "Thomas and Brunskill.,? \\Q2016\\E", "shortCiteRegEx": "Thomas and Brunskill.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Since in the multi-armed bandit problem the IPS is suboptimal [Li et al., 2015], our result highlights the difficulty of the contextual setting with non-degenerate context distributions.", "startOffset": 62, "endOffset": 79}, {"referenceID": 10, "context": "In particular, there are several estimators which are unbiased under mild assumptions, such as inverse propensity scoring (IPS) [Horvitz and Thompson, 1952], and sharp estimates on their mean squared error (MSE) for policy evaluation are well-known [Dud\u00edk et al.", "startOffset": 128, "endOffset": 156}, {"referenceID": 6, "context": "In particular, there are several estimators which are unbiased under mild assumptions, such as inverse propensity scoring (IPS) [Horvitz and Thompson, 1952], and sharp estimates on their mean squared error (MSE) for policy evaluation are well-known [Dud\u00edk et al., 2014].", "startOffset": 249, "endOffset": 269}, {"referenceID": 6, "context": "Some approaches, such as the doubly-robust method (DR) [Dud\u00edk et al., 2014] (also see the references therein for its origin in statistics and application in causal inference, e.", "startOffset": 55, "endOffset": 75}, {"referenceID": 0, "context": ", [Robins and Rotnitzky, 1995, Bang and Robins, 2005]), combine the model with an IPS-style unbiased estimation and remain consistent, with known estimates for their MSE. All these works focus on developing specific methods alongside upper bounds on their MSE. Little work, on the other hand, exists on the question of the fundamental statistical hardness of off-policy evaluation and the optimality (or the lack of) of the existing methods. A notable exception is the recent work of Li et al. [2015], who study off-policy evaluation in multi-armed bandits\u2014a special case of our setting, without any contexts\u2014and provide a minimax lower bound on the MSE.", "startOffset": 31, "endOffset": 501}, {"referenceID": 13, "context": "In contrast with context-free multi-armed bandits [Li et al., 2015], our lower bound matches the MSE upper bound for IPS up to constants, so long as the contexts have a non-degenerate distribution.", "startOffset": 50, "endOffset": 67}, {"referenceID": 5, "context": "We use the standard setup for off-policy evaluation in contextual bandits as in prior works Dud\u00edk et al. [2014]. We start with the formal setup and then discuss the related work.", "startOffset": 92, "endOffset": 112}, {"referenceID": 10, "context": "The simplest estimator in this setting, called Inverse Propensity Scoring (IPS) [Horvitz and Thompson, 1952] is defined as: v\u0302 IPS = n \u2211", "startOffset": 80, "endOffset": 108}, {"referenceID": 6, "context": ", 2012] as well as the references in [Dud\u00edk et al., 2014]).", "startOffset": 37, "endOffset": 57}, {"referenceID": 5, "context": "Li et al. [2015] first studied this question under a minimax setup, but focused on the multi-arm bandits problem.", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "Jiang and Li [2016] built on top of Li et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Jiang and Li [2016] built on top of Li et al. [2015] proves a Cramer-Rao-style lower bound for policy-evaluation in reinforcement learning, but the assumption of a small number of observed states precludes contextual bandits from being a special case.", "startOffset": 0, "endOffset": 53}, {"referenceID": 3, "context": "Cortes et al. [2010] studied the related covariate shift problem in the statistical learning setting and proved upper bounds and lower bounds of the minimax excess risk that depends on the Renyi divergence of the two covariate distributions.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": ", 2011, 2014] and reinforcement learning [Jiang and Li, 2016].", "startOffset": 41, "endOffset": 61}, {"referenceID": 3, "context": "[Cassel et al., 1976, Robins and Rotnitzky, 1995], were recently used for off-policy value estimation in the contextual bandits problem [Dud\u00edk et al., 2011, 2014] and reinforcement learning [Jiang and Li, 2016]. These also provide error estimates for the proposed estimators, which we will compare with the lower bounds that we will obtain. The doubly robust techniques also have a flavor of incorporating existing reward models, an idea which we will also leverage in the development of the SWITCH estimator in Section 4. We note that similar ideas were also recently investigated in the context of reinforcement learning by Thomas and Brunskill [2016].", "startOffset": 1, "endOffset": 654}, {"referenceID": 12, "context": "The family of reward distributions D(r | x, a) that we study is a natural generalization of the class studied by Li et al. [2015] for multi-armed bandits.", "startOffset": 113, "endOffset": 130}, {"referenceID": 5, "context": ", the bound on the variance of IPS in Dud\u00edk et al. [2014], which assumes the finiteness of these second moments).", "startOffset": 38, "endOffset": 58}, {"referenceID": 10, "context": "Comparing with the upper bounds on the MSE for existing estimators, we find that this lower bound precisely matches the upper bound for the IPS estimator [Horvitz and Thompson, 1952] (defined in Equation 2).", "startOffset": 154, "endOffset": 182}, {"referenceID": 10, "context": "Lemma 1 (IPS estimator [Horvitz and Thompson, 1952]).", "startOffset": 23, "endOffset": 51}, {"referenceID": 12, "context": "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al.", "startOffset": 50, "endOffset": 67}, {"referenceID": 12, "context": "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits.", "startOffset": 50, "endOffset": 133}, {"referenceID": 12, "context": "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits. The somewhat surprising conclusion of their work was the sub-optimality of IPS, which might appear at odds with our conclusion regarding IPS above. However, this difference actually highlights the additional challenges in contextual bandits beyond multi-armed bandits. This is best illustrated in a noiseless setting, where \u03c3 = 0 in the rewards. This makes the multi-armed bandit problem trivial, we can just measure the reward of each arm with one pull and find out the optimal choice. However, there is still a non-trivial lower bound of \u03a9(E\u03bc[\u03c1R max]/n) in the contextual bandit setting, which is exactly the upper bound on the MSE of IPS when the rewards have no noise. This difference crucially relies on \u03bb0 being suitably small relative to the sample size n. When the number of contexts is small, independent estimation for each context can be done in a noiseless setting as observed by Li et al. [2015]. However, once the context distribution is rich enough, then even with noiseless rewards, there is significant variance in the value estimates based on which contexts were observed.", "startOffset": 50, "endOffset": 1131}, {"referenceID": 12, "context": "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits. The somewhat surprising conclusion of their work was the sub-optimality of IPS, which might appear at odds with our conclusion regarding IPS above. However, this difference actually highlights the additional challenges in contextual bandits beyond multi-armed bandits. This is best illustrated in a noiseless setting, where \u03c3 = 0 in the rewards. This makes the multi-armed bandit problem trivial, we can just measure the reward of each arm with one pull and find out the optimal choice. However, there is still a non-trivial lower bound of \u03a9(E\u03bc[\u03c1R max]/n) in the contextual bandit setting, which is exactly the upper bound on the MSE of IPS when the rewards have no noise. This difference crucially relies on \u03bb0 being suitably small relative to the sample size n. When the number of contexts is small, independent estimation for each context can be done in a noiseless setting as observed by Li et al. [2015]. However, once the context distribution is rich enough, then even with noiseless rewards, there is significant variance in the value estimates based on which contexts were observed. This distinction is further highlighted in the proof of Theorem 1, and is obtained by combining two separate lower bounds. The first lower bound considers the case of noisy rewards, and is a relatively straightforward generalization of the proof of Li et al. [2015]. The second lower bound focuses on noiseless rewards, and shows how the variance in a rich context distribution allows the environment to essentially simulate noisy rewards, even when the reward signal itself is noiseless.", "startOffset": 50, "endOffset": 1579}, {"referenceID": 6, "context": "where the DM stands for direct method, a name for this approach which has been used in prior works [Dud\u00edk et al., 2014].", "startOffset": 99, "endOffset": 119}, {"referenceID": 5, "context": "where the DM stands for direct method, a name for this approach which has been used in prior works [Dud\u00edk et al., 2014]. This approach appears attractive when r\u0302 is a close to E[r | x, a]. Crucially, r\u0302 can be evaluated for any x, a pair, meaning that there is no need to do use importance weights unlike in IPS. This suggests that we might completely eliminate any dependence on \u03c1 using this approach., and it is easily seen that given any estimator r\u0302 such th which takes values in [0, Rmax(x, a)] for any x, a, the variance satisfies: Var(v\u0302DM) \u2264 1 n E\u03c0[R max] = 1 n E\u03bc[\u03c1R max]. (6) That is, there is a linear, rather than quadratic dependence on \u03c1, unlike in the worst case minimax risk of Corollary 1. The catch, however, is that such estimators can have an uncontrolled bias of O(E\u03c0[Rmax]) in the worst case, meaning that even asymptotic consistency is not guaranteed. Prior works have addressed the bias of DM, while trying to reduce the variance of IPS by using doubly robust estimators. A concrete estimator previously used for off-policy evaluation in Dud\u00edk et al. [2014] is defined as", "startOffset": 100, "endOffset": 1082}, {"referenceID": 5, "context": "However, based on the results of Dud\u00edk et al. [2014], the MSE of the DR estimator in this special case is", "startOffset": 33, "endOffset": 53}, {"referenceID": 2, "context": "Bottou et al. [2013] for a detailed discussion), which can often reduce the variance drastically at the cost of a little bias.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "This approach, with a specific choice of \u03c4 was described in Bottou et al. [2013] and will be evaluated in the experiments under the name Trimmed IPS.", "startOffset": 60, "endOffset": 81}, {"referenceID": 2, "context": "This approach, with a specific choice of \u03c4 was described in Bottou et al. [2013] and will be evaluated in the experiments under the name Trimmed IPS. \u2022 Thomas and Brunskill [2016] study a similar estimator in the more general context of reinforcement learning.", "startOffset": 60, "endOffset": 180}, {"referenceID": 5, "context": "The analysis in Theorem 2 still applies, replacing the variance of IPS with that of DR from Dud\u00edk et al. [2014]. Since no independence was required in our analysis between the IPS and the DM parts of the estimator, the result is also robust to the use of a common data-dependent estimator r\u0302 = r\u0302\u2032 in SWITCH-DR (10).", "startOffset": 92, "endOffset": 112}, {"referenceID": 19, "context": "Finally, we should mention that this development is quite related to the MAGIC estimator of Thomas and Brunskill [2016], which was discussed following Theorem 2.", "startOffset": 92, "endOffset": 120}, {"referenceID": 5, "context": "We will be using the same 10 UCI data sets that was used previously by Dud\u00edk et al. [2011] and convert the multi-class classification problem to contextual bandits by 1.", "startOffset": 71, "endOffset": 91}, {"referenceID": 5, "context": "We simulate the covariate-shifted data set using the same technique as in Dud\u00edk et al. [2011], which follows standard practice as was described by Gretton et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 5, "context": "We simulate the covariate-shifted data set using the same technique as in Dud\u00edk et al. [2011], which follows standard practice as was described by Gretton et al. [2009]. In each data set with n rows, we treat the uniform distribution over the data set itself as a surrogate of the population distribution so that we know the ground truth of the rewards.", "startOffset": 74, "endOffset": 169}, {"referenceID": 4, "context": "This approach was suggested by Dud\u00edk et al. [2011], and is standard practice of doubly robust estimators when the direct method (or the oracle estimator) needs to be estimated from the data.", "startOffset": 31, "endOffset": 51}, {"referenceID": 2, "context": ", Bembom and van der Laan, 2008], or the terms with weights larger than \u03c4 are removed altogether as described in Bottou et al. [2013]. Note that the trimmed IPS is a special case of SWITCH when the direct estimator \u2261 0.", "startOffset": 113, "endOffset": 134}, {"referenceID": 18, "context": "The authors would like to thank Lihong Li and John Langford for helpful discussions, Edward Kennedy for bringing our attention to related problems and recent development in causal inference and Wei He for pointing us to [Sun, 2006] for rigorous a measure-theoretic treatment to a continuum of random variables and the corresponding law of large numbers.", "startOffset": 220, "endOffset": 231}], "year": 2016, "abstractText": "We consider the problem of off-policy evaluation\u2014estimating the value of a target policy using data collected by another policy\u2014under the contextual bandit model. We establish a minimax lower bound on the mean squared error (MSE), and show that it is matched up to constant factors by the inverse propensity scoring (IPS) estimator. Since in the multi-armed bandit problem the IPS is suboptimal [Li et al., 2015], our result highlights the difficulty of the contextual setting with non-degenerate context distributions. We further consider improvements on this minimax MSE bound, given access to a reward model. We show that the existing doubly robust approach, which utilizes such a reward model, may continue to suffer from high variance even when the reward model is perfect. We propose a new estimator called SWITCH which more effectively uses the reward model and achieves a superior bias-variance tradeoff compared with prior work. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of datasets, often seeing orders of magnitude improvements over a number of baselines.", "creator": "LaTeX with hyperref package"}}}