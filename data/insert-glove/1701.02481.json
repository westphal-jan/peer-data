{"id": "1701.02481", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Implicitly Incorporating Morphological Information into Word Embedding", "abstract": "ijt In quik this paper, 51.43 we midsections implicitly incorporate morpheme information into capitis word trinley embedding. Based on acquaintance the strategy kotwali we utilize mindfulness the morpheme chinnaswamy information, three picea models chichagov are proposed. To obligado test blagoje the performances tabung of our brass models, helwig we 3-sphere conduct pez the xanthorrhoea word sangshad similarity edeka and radojko syntactic taitung analogy. bernardi The results cappy demonstrate sourdough the laryngoscope effectiveness of our methods. moulavi Our models tore beat briana the 35-acre comparative jeers baselines 7:07 on both tasks cerent to a great extent. On the 11:33 golden standard Wordsim - 353 kraters and RG - 65, botaneiates our models approximately kmgh outperform 179.2 CBOW for 5 and 7 percent, respectively. letraset In priori addition, 7 populates percent advantage outrebound is milady also achieved by our regie models on anandan syntactic analysis. According myrmidons to parameter analysis, accessibly our models garnet can ta\u00ef increase marcius the hernreich semantic information in okays the manjimup corpus cyberwar and cassette-only our espousing performances papaya on raspberry the smallest farriers corpus northborough are undeviating similar budarin to matsch the rummage performance of kidspost CBOW jovica on the corpus which 167.2 is trifu five trentini times ours. This an-2 property of affix our damiano methods industrie may suzlon have altemps some muqarnas positive effects stilted on pheng NLP researches horrida about jujutsu the merged corpus - limited languages.", "histories": [["v1", "Tue, 10 Jan 2017 08:59:38 GMT  (1446kb,D)", "http://arxiv.org/abs/1701.02481v1", "7 pages, 7 figures"], ["v2", "Thu, 26 Jan 2017 01:35:53 GMT  (1865kb,D)", "http://arxiv.org/abs/1701.02481v2", "7 pages, 7 figures"], ["v3", "Mon, 8 May 2017 03:19:20 GMT  (1865kb,D)", "http://arxiv.org/abs/1701.02481v3", "7 pages, 7 figures"]], "COMMENTS": "7 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yang xu", "jiawei liu"], "accepted": false, "id": "1701.02481"}, "pdf": {"name": "1701.02481.pdf", "metadata": {"source": "CRF", "title": "Implicitly Incorporating Morphological Information into Word Embedding", "authors": ["Yang Xu", "Jiawei Liu"], "emails": ["smallant@mail.ustc.edu.cn", "ustcljw@mail.ustc.edu.cn"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nWord embedding representing words into vector space has been a hot topic in the area of Natural Language Processing(NLP). By using learned word representations, a number of NLP tasks can be dealt with in other effective ways. For example, we can simply average the vectors of all tokens in a document and utilize some classifiers to do text classification [1]. In addition, word embedding are also used for information retrieval [2], sentiment analysis [3], etc. There are many classic word embedding methods including the Semantic Extraction using a Neural Network Architecture (SENNA) [4], Continuous Bag-of-Word (CBOW), Skip-gram [5], Global Vector (Glove) [6], etc. These models by learning semantic information from context, however, still have some weaknesses. All models mentioned before are actually wordlevel methods which ignore some internal structure of word. Try to improve the ability of word embedding models, a lot of strategy are proposed which can be divided into two main branches. The first is lexicon-based methods. These methods adjust the locations of word embeddings in vector space based on lexica like WordNet [7]. The next is inner structure-based methods. The basic idea of these methods is that the internal structure like Chinese character have some positive effects on the improvement of word embedding. For example, Chen [8] proposed a character-enhanced Chinese word embedding model by incorporating the character information of a single word into the input layer of CBOW.\nRecently, some researchers utilize the morpheme information to improve English language word embedding. In Luong\u2019s paper [9], a word is divided into the morpheme part and the basic part. Both of them are added into the firstly layer of their recursive neural network. Incorporating the morpheme information into word embedding is meaningful because every morpheme has its own meaning. For example, words starting with prefix \u201ca or an\u201d will have meaning of \u201cnot, without\u201d like \u201casexual\u201d and \u201canarchy\u201d. In addition, words ending with suffix \u201cable or ible\u201d will have the meaning of \u201ccapable\u201d like \u201cedible\u201d and \u201cvisible\u201d. However, these morpheme-enhanced word embedding methods only care about morphemes themselves rather than their meanings. For example, in their models, word \u201casexual\u201d is divided into prefix \u201ca\u201d, basic part \u201csexu\u201d and suffix \u201cal\u201d. The meaning \u201cwithout\u201d of prefix and the meaning \u201crelated to\u201d of suffix are ignored. Although these models have some positive effect, they are still ambiguous. In vector space, morpheme-similar words will have a tendency to stay together, which can benefits the syntactic parsing. However, the long distances between these words and their morphemes\u2019 meanings are still remained. We illustrate the basic idea of these models in the left part of Figure 1.\nIn this paper, we propose three more refined word embedding models based on the meanings of word morphemes rather than morphemes themselves. According to the nature of morphemes, they are divided into 3 parts including prefix, root and suffix. By using our models, morpheme-similar word in\nar X\niv :1\n70 1.\n02 48\n1v 1\n[ cs\n.C L\n] 1\n0 Ja\nn 20\n17\nvector space will not only have a tendency to gather together but also tend to locate close to their morphemes\u2019 meanings. For comparison, our models together with the comparative models are tested on the tasks of word similarity and syntactic analogy. More details are shown in the right part of Figure 1. The contribution of this paper are summarized as follows. \u2022 We implicitly incorporate the morpheme information into\nword embedding. Rather than the morphemes themselves, we utilize the morphemes\u2019 meanings. \u2022 Based on parameter analysis, our models can increase the semantic information in the corpus, which means our models are of great advantages to deal with some morpheme-rich and corpus-limited languages. The performances of our models are similar to the performance of CBOW on the corpus which is five times ours.\nThe rest of the paper is organized as follows. Some related work is given in Section 2. We introduce our models in Section 3. Experimental settings are described in the following section which includes corpus introduction, comparative baselines introduction and etc. In Section 5, we analyze the results of experiments and do parameter analysis. Conclusion is given in the last section."}, {"heading": "II. RELATED WORK", "text": "By representing words into low dimension vector space, word embedding models can extract the semantic information from context words, which has been a hot topic in the area of NLP. For its effectiveness in dealing with some tasks like text classification and sentiment analysis, a lot of word embedding models are proposed. Some of them are based on neural network while the others are based on matrix factorization. Among all of word embedding models, CBOW and Skipgram[5] are the most widely used and get some state-of-theart performances. CBOW utilizes the context to predict the target word while Skip-gram predicts the context by using the target word. Both of them suffer from the limitation of computational ability until Mikolov proposed two efficient solution, hierarchical softmax and negative sampling [10], to solve this problem. In order to combine the advantage of distributional word representation with the advantage of matrix factorization, Pennington proposed another famous word embedding model named Glove [6], which is reported to outperform the word2vec on some tasks.\nBased on the internal structure of word like Chinese character and English morpheme, a lot of enhanced models are proposed to improve the ability of word embedding. Chen proposed a character-enhanced Chinese word embedding model [8]. He modified the input layer of CBOW and added the character information in it. The basic idea of his model is that every Chinese character has its own meanings and these meanings will have a positive effect on improving the ability of word embedding. Based on recursive neural network, Luong split a word into morpheme part and basic part [9]. When computing the representation of a word on the input layer of his model, he weighed the two parts and utilized the sum of them to indicate the word. Recently, Kim has proposed a\ncharacter-level language model [11]. Their model is composed of several neural work layers and every character information of an English word is convoluted on the input layer and the result is sent to the highway network. It is reported this model has a great advantage to deal with the morphemerich language. However, with a huge size architecture, it is proven to be a time-consuming model. Besides, the structure of highway network is hard to determined. In his paper, he exploited three structures of highway network and two of them failed. No theoretical explanation is given."}, {"heading": "III. MORPHEME-ENHANCED WORD EMBEDDING", "text": "In this section, we proposed three refined morpheme meaning-enhanced word embedding models. Based on our models, the morpheme-similar words will not only tend to group together but also sit closed to morphemes\u2019 meanings. Firstly, we introduce some backgrounds of CBOW which is the fundamental model of our methods."}, {"heading": "A. CBOW with Negative Sampling", "text": "In the anterior section, the basic idea of CBOW has been discussed. With a slide window, CBOW utilizes the context words in the window to predict the target words. Given a sequence of tokens T = {t1, t2, \u00b7 \u00b7 \u00b7 , tn}, the objective of CBOW is to maximize the following average log probability equation.\nL = 1\nn n\u2211 i=1 log p(ti|context(ti)), (1)\nwhere context(ti) means the context words of ti in the slide window. Based on softmax, p(ti|context(ti)) is defined as follows.\nexp(vec \u2032 (ti) T \u2211 \u2212k\u2264j\u2264k,j 6=0 vec(ti+j))\u2211V\nx=1 exp(vec \u2032(tx)T \u2211 \u2212k\u2264j\u2264k,j 6=0 vec(ti+j)) , (2)\nwhere vec(t) and vec \u2032 (t) are the \u201cinput\u201d and \u201coutput\u201d vector representation of token t. k means the slide window size and V means the vocabulary size. Due to huge size of English vocabulary, however, Equation 2 can not be calculated in a tolerable time. Therefore, negative sampling and hierarchical softmax are proposed to solve this problem. Due to the efficiency of negative sampling, all related models are trained based on it. In terms of negative sampling, every item whose form is like log p(tO|tI) is defined as follows.\nlog \u03b4(vec \u2032 (tO)\nT vec(tI))+ m\u2211 i=1 log[1\u2212 \u03b4(vec \u2032 (ti) T vec(tI))], (3)\nwhere m indicates the number of negative samples. \u03b4(\u00b7) is the sigmoid function. The first item is the probability of target word when given its context. The second item denotes the probability that negative samples don\u2019t have the same context as the target word."}, {"heading": "B. Morpheme-Enhanced Word Embedding-Average (MWE-A)", "text": "In this model, we assume that all morphemes\u2019 meanings of token ti have equal contribution to ti. Given a sequence of tokens T = {t1, t2, \u00b7 \u00b7 \u00b7 , tn}, we assume that the morpheme\u2019s meaning set of ti, (i \u2208 [1, n]) is Mi. Mi can be divided into three part Pi, Ri and Si which indicate the prefix meaning set, root meaning set and suffix meaning set of ti, respectively. Hence, when ti is the context word of tj , the modified embedding of ti can be defined as follows.\nv\u0302ti = 1\n2 (vti +\n1\nNi \u2211 w\u2208Mi vw), (4)\nwhere vti is the original word embedding of ti, Ni denotes the length of Mi and vw indicates the vector of w. In this paper, we assume the original word embedding and the sum of its morphemes\u2019 meanings embedding have equal weight which is 1 2 . More details can be found in Figure 2.\nC. Morpheme-Enhanced Word Embedding-Similarity (MWES)\nWe observe that a lot of words have more than one morpheme\u2019s meanings. For word \u201canthropologist\u201d, its morpheme\u2019s meaning set is {man,human,who,which}. Obviously, \u201cman\u201d and \u201chuman\u201d are closer to \u201canthropologist\u201d. Motivated by this observation, we assume that different morphemes have different contributions. In this paper, we firstly train a part of word embedding based on the source code of word2vec1. Then, based on the pre-trained word embedding, the different contributions can be measured by calculating the cosine distances between target word and its morphemes\u2019 meanings. We illustrate the main idea of this model in Figure 3. We reuse the notation in MWE-A to describe MWE-S. Mathematical formation is given in Equation 5.\nv\u0302ti = 1\n2 (vti + \u2211 w\u2208Mi sim(vti , vw) vw), (5)\n1https://github.com/dav/word2vec\nwhere vti is the original vector of ti. sim(vti , vw) is the function used to denote the similarity between ti and its morphemes\u2019 meanings. We define function cos(vi, vj) to denote the cosine distance between vi, vj . sim(vti , vw) is normalized as follows.\nsim(vti , vw) = cos(vti , vw)\u2211\nw\u2208Mi cos(vti , vw) , w \u2208Mi, (6)"}, {"heading": "D. Morpheme-Enhanced Word Embedding-Max (MWE-M)", "text": "From the collections of morphemes and their meanings, we find that except for multiple meanings of a morpheme, the corresponding meanings of a token\u2019s prefix, root and suffix are usually different. However, some meanings are not close to the target word. To achieve better improvement of the target word\u2019s representation, we only exploit the morpheme\u2019s meaning whose distance to target word is the max. In Figure 4, the suffix\u2019s meaning of \u201canthropologist\u201d is \u201cwho\u201d and \u201cwhich\u201d. According to our description, the word \u201cwhich\u201d in red will not be applied for its shorter distance to \u201canthropologist\u201d. More details are shown in Figure 4. For clarity, we also utilize the notation in MWE-A. For a single token ti, the new morphemes\u2019 meanings set is defined as M imax = {P imax, Rimax, Simax} where P imax, Rimax, Simax are defined as follows.\nP imax = argmax w cos(vti , vw), w \u2208 Pi, Rimax = argmax w cos(vti , vw), w \u2208 Ri, (7) Simax = argmax w cos(vti , vw), w \u2208 Si,\nwhere cos(\u00b7) stands for cosine distance. Hence, MWE-M is defined mathematically as follows.\nv\u0302ti = 1\n2 (vti + \u2211 w\u2208Mimax sim(vti , vw) vw), (8)\nwhere sim(\u00b7) indicates the similarity between two words and needs to be normalized like Equation 6 shows."}, {"heading": "E. Strategy to Match the Morpheme", "text": "Now, we need to discuss the way to find the morphemes in a single token. Firstly, it is obvious that a number of words contain more than one prefix, root and suffix. For example, \u201canthropologist\u201d has the prefix \u201ca, an, anthrop\u201d. The first question is which one should be used. Secondly, some words\u2019 morphemes\u2019 meanings have nothing to do with the word. For example, although \u201capple\u201d has the prefix \u201ca\u201d, it doesn\u2019t have the meanings of \u201cwithout, not\u201d. To solve these problems, some rules are defined as follows. \u2022 When matching the morphemes of word ti, the longest\ncharacter sequence will be viewed as the morpheme. For example, the prefix of word \u201canthropologist\u201d is \u201canthrop\u201d not \u201ca or an\u201d. \u2022 We define a threshold \u03bb. If the cosine distance between word ti and its morpheme mi, (mi \u2208Mi) is larger than \u03bb, mi will be remained or abandoned otherwise."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": "In order to evaluate the performances of our models, we test them together with the comparative baselines on some semantic-related and syntactic-related tasks. From anterior sections, our models incorporating morphological information are considered to have great advantage to deal with the morphemerich languages. To demonstrate the supposition, a mediumsized English corpus is chosen to evaluate our models. For clarity, some experimental settings are introduced firstly."}, {"heading": "A. Corpus and Morphemes", "text": "In this paper, we utilize a medium-sized English corpus to train all word embedding models. The corpus stems from the 2013 ACL Workshop on Machine Translation2 and is used in [11]. The website lists a number of corpus ordered by year when they are released. We choose the news corpus of 2009 whose size is about 1.7GB. It contains approximately 500 millions of tokens and 600 thousands of vocabularies. For better quality of all word embeddings, we filter all numbers and some punctuation out of the corpora. All of morphemes\n2http://www.statmt.org/wmt13/translation-task.html\nused in this paper and their meanings are both collected from the website3. We get 90 prefixes, 241 roots and 64 suffixes."}, {"heading": "B. Baselines", "text": "For comparison, we choose three competitive baselines including CBOW, Skip-gram [5] and Glove [6]. All of baselines hold some state-of-the-art performances on the tasks about NLP. CBOW maximizes the probability of target word by giving its context information while Skip-gram maximizes the product of the context words\u2019 probabilities by giving the target word. Both of the models can be solved via negative sampling and hierarchical softmax [10]. CBOW and Skipgram acts as the baselines in a number of papers [12]. Different from CBOW and Skip-gram, Glove combines not only the context information but also the advantage of global matrix factorization. It is reported that Glove outperforms CBOW and Skip-gram in some NLP tasks [6]. In this paper, we utilize the source code of word2vec to train CBOW and Skip-gram. Glove is trained based on the code4. We modify the source of word2vec and train our models."}, {"heading": "C. Parameter Settings", "text": "Parameter settings have a great effect on the performance of word embedding. Obviously, larger context window size means more context information will be captured. For fairness, all models are trained based on equal parameter settings. In order to accelerate the training process, CBOW, Skipgram together with our models are trained by using negative sampling. It is reported that the number of negative samples in the range 5-20 is useful for small corpus. If huge-sized corpus is used, the number of negative samples should be chosen from 2 to 5 [10]. According to the corpus we used, the number of negative samples is set as 20 in this paper. The dimension of word embedding is set as 200 like that in [12]. We set the context window size as 5 which is equal to the setting in [10]."}, {"heading": "D. Evaluation Benchmarks", "text": "In this section, we test all word embeddings from several aspects including word similarity, syntactic analogy.\n1) Word Similarity: This experiment is used to evaluate word embedding\u2019s ability to capture semantic information from corpus. Each dataset is divided into three column. The first two columns stand for word pairs and the last column is human score. On the task, we need to solve two problems. One is how to calculate the distance between two words. The other is how to evaluate the similarity between our results and human scores. For the first problem, we utilize the cosine distance to measure the distance between two words. This strategy is used in many papers [10], [6]. The second problem is solved via Spearman\u2019s rank correlation coefficient (\u03c1). Higher \u03c1 means better performance. For English word similarity, there are two golden standard datasets including Wordsim-353 [13] and RG-65 [14]. To avoid occasionality, however, we utilize some other widely-used datasets including\n3https://msu.edu/ defores1/gre/roots/gre rts afx1.htm 4http://nlp.stanford.edu/projects/glove/\nRare-Word [9], SCWS [15], Men-3k [16] and WS-353-Related [17]. More details of these datasets are shown in Table 1.\n2) Syntactic Analogy: Based on the learned word embedding, the core task of syntactic analogy is to answer the analogy questions \u201ca is to b as c is to \u201d. In this paper, we utilize the Microsoft Research Syntactic Analogies dataset. This dataset with size of 8000 is created by Mikolov [18]. In this experiment, we also utilize the cosine distance to measure the distance between two words. To answer the syntactic analogy questions \u201ca is to b as c is to d\u201d where d is unknown, we assume the word representations of a, b, c, d are va, vb, vc, vd, respectively. To get d, we firstly calculate v\u0302d = vb\u2212va+vc. Then, we find the word d\u0302 whose cosine distance to v\u0302d is the largest. Finally, we set d as d\u0302. For clarity, we describe it as following equation. In this experiments, accuracy on the whole dataset will be reported.\nd = argmax w vwv\u0302d \u2016 vw \u2016\u2016 v\u0302d \u2016 , w \u2208 vocabulary (9)"}, {"heading": "V. RESULTS AND ANALYSIS", "text": ""}, {"heading": "A. Word Similarity", "text": "Word similarity is conducted to test the semantic information which is contained in word embedding. From Table 2, we observe that our models beat the comparative baselines on five datasets. It is remarkable that our models approximately outperform CBOW which is the base model of ours for 5 percent and 7 percent on the golden standard Wordsim-353 and RG-65, respectively. On WS-353-REL, the difference between CBOW and MEW-S even reaches to 8 percent. The advantage of our models indicates the validity and effectiveness of our methods. We give some empirical explanations of the huge promotion. Based on our strategy, more information will be captured in corpus. Like the example in Figure 2, the semantic information captured by CBOW only stems from sequence \u201che is a good anthropologist\u201d. In our model, however, not only the semantic information in the original sequence but also the information in sequence \u201che is a good human\u201d and \u201che is a good man\u201d will be captured. Obviously, more semantic information means better performance. Specially, because of medium-size corpus and experimental settings, Glove doesn\u2019t perform as well as it in other papers [6]."}, {"heading": "B. Syntactic Analogy", "text": "In [18], they divided the dataset into adjectives, nouns and verbs. In this paper, we report performance on the whole dataset. In Table 2, all of our models beat the comparative\nbaselines to a great extent. Compared with CBOW, the advantage of MEW-A even reaches to 7 percent. The result corresponds to our expectation. In Mikolov\u2019s dataset, we observe that the suffix of \u201cb\u201d usually is the same as the suffix of \u201cd\u201d when answering question \u201ca is to b as c is to d\u201d. Based on our strategy, morpheme-similar words will not only gather together but also have a tend to group near the morpheme\u2019s meanings, which makes our embeddings have the advantage to deal with the syntactic analogy problems. However, it is abnormal that all results are lower than the state-of-the-art results in [18]. We give some empirical explanations of this problem. Firstly, we utilize different corpus which is the key factor of word embedding\u2019s quality. Secondly, different parameter settings can affect the performance of word embeddings which can be seen in the following sections. However, this experiment is also meaningful. What we care about is the differences between the base model CBOW and our models. The advantages of our models indicate the validity of our methodology."}, {"heading": "C. Parameter Analysis", "text": "It is obvious that parameter settings will affect the performance of word embedding. For example, the corpus with larger token size contains more semantic information, which can improve the performance on word similarity. In this paper, we analyze the effects of token size and window size on the performance of word embedding.\nIn the analysis of token size, we set the same parameter settings as what we did in the anterior section. The sizes of corpora used in this analysis are the 1/5, 2/5, 3/5, 4/5 and 5/5 of the corpus mentioned before, respectively. We utilize the result of word similarity on Wordsim-353 as the evaluation criterion. From Figure 5, we observe several phenomenons. Firstly, the performance of CBOW is sensitive to the token size. However, our models\u2019 performances seem more stable than CBOW. Like what we said in word similarity analysis, our methods can increase the semantic information of the corpus, which is a great property for dealing with some corpus-limited languages. It is worth noting that our performances on the\nsmallest corpus are even better than CBOW\u2019s performance on the largest corpus. Secondly, the performances of our models are all better than CBOW which indicates the effectiveness and validity of our methods.\nIn the analysis of window size, we observe that the performances of all word embeddings have a tend to ascend with the increasing of window size in Figure 6. Like the result in Figure 5, our models outperform CBOW under all pre-set conditions. Our worst performances are approximately equal to the best performance of CBOW, which may stem from the increasing semantic information property of our methods.\nTo further demonstrate the validity and effectiveness of our method, the dimension of word embedding is reduced from 200 to 2 with Principal Component Analysis (PCA). We randomly select several words from the embedding of MWEA and illustrate them in Figure 7. Different colors stand for words with different morphemes. It is apparent that words with similar morpheme have a tend to group together."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we proposed a novel method to implicitly incorporate the morpheme information into word embedding. In our models, the morpheme\u2019s meanings are added into the input layer of CBOW rather than morphemes themselves. Based on this strategy, morpheme-similar words will not only gather together but also have a tend to group near their morphemes\u2019 meanings. Based on the different strategy to\nadd the morphemes\u2019 meanings, three models named MWEA, MWE-S and MWE-M were built. To test performance of our embeddings, we utilized three comparative baselines and tested them on the word similarity and syntactic analogy tasks. The results demonstrate the effectiveness of models. Our models beat the baselines on five word similarity datasets. On the golden standard Wordsim-353 and RG-65, our models even approximately outperform CBOW for 5 percent and 7 percent, respectively. On syntactic analogy task, our models also beat the baselines to a great extent. Compare with CBOW, MWE-A approximately outperforms it for 7 percent. Last but not the least, our models seem to have great advantages to train some corpus-limited languages according to the observation of token size analysis. The conspicuous advantages demonstrate the effectiveness of our models. In this paper, all word embeddings are trained based on medium-size corpus. In future work, we will conduct all experiments on huge-size corpus and test our models\u2019 ability to deal with some morpheme-rich languages like Germany and French."}], "references": [{"title": "Topical word embeddings", "author": ["Y. Liu", "Z. Liu", "T.-S. Chua", "M. Sun"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "Cambridge university press Cambridge,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Lexicon integrated cnn models with attention for sentiment analysis", "author": ["B. Shin", "T. Lee", "J.D. Choi"], "venue": "2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast semantic extraction using a novel neural network architecture", "author": ["R. Collobert", "J. Weston"], "venue": "ACL 2007, Proceedings of the Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague, Czech Republic, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP, vol", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith"], "venue": "arXiv preprint arXiv:1411.4166, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint learning of character and word embeddings", "author": ["X. Chen", "L. Xu", "Z. Liu", "M. Sun", "H. Luan"], "venue": "International Conference on Artificial Intelligence, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["T. Luong", "R. Socher", "C.D. Manning"], "venue": "Conference, 2013, pp. 104\u2013113.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "J. Dean"], "venue": "Advances in neural information processing systems, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "Computer Science, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Eigenwords: Spectral word embeddings", "author": ["P.S. Dhillon", "D.P. Foster", "L.H. Ungar"], "venue": "The Journal of Machine Learning Research, vol. 16, no. 1, pp. 3035\u20133078, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "Proceedings of the 10th international conference on World Wide Web. ACM, 2001, pp. 406\u2013414.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Contextual correlates of synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Communications of the ACM, vol. 8, no. 10, pp. 627\u2013633, 1965.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1965}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Meeting of the Association for Computational Linguistics: Long Papers, 2012, pp. 873\u2013882.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal distributional semantics.", "author": ["E. Bruni", "N.-K. Tran", "M. Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A study on similarity and relatedness using distributional and wordnetbased approaches", "author": ["E. Agirre", "E. Alfonseca", "K. Hall", "J. Kravalova", "M. Pa\u015fca", "A. Soroa"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2009, pp. 19\u201327.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.T. Yih", "G. Zweig"], "venue": "HLT-NAACL, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "For example, we can simply average the vectors of all tokens in a document and utilize some classifiers to do text classification [1].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "In addition, word embedding are also used for information retrieval [2], sentiment analysis [3], etc.", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "In addition, word embedding are also used for information retrieval [2], sentiment analysis [3], etc.", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "There are many classic word embedding methods including the Semantic Extraction using a Neural Network Architecture (SENNA) [4], Continuous Bag-of-Word (CBOW), Skip-gram [5], Global Vector (Glove) [6], etc.", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "There are many classic word embedding methods including the Semantic Extraction using a Neural Network Architecture (SENNA) [4], Continuous Bag-of-Word (CBOW), Skip-gram [5], Global Vector (Glove) [6], etc.", "startOffset": 170, "endOffset": 173}, {"referenceID": 5, "context": "There are many classic word embedding methods including the Semantic Extraction using a Neural Network Architecture (SENNA) [4], Continuous Bag-of-Word (CBOW), Skip-gram [5], Global Vector (Glove) [6], etc.", "startOffset": 197, "endOffset": 200}, {"referenceID": 6, "context": "These methods adjust the locations of word embeddings in vector space based on lexica like WordNet [7].", "startOffset": 99, "endOffset": 102}, {"referenceID": 7, "context": "For example, Chen [8] proposed a character-enhanced Chinese word embedding", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "In Luong\u2019s paper [9], a word is divided into the morpheme part and the basic part.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "Among all of word embedding models, CBOW and Skipgram[5] are the most widely used and get some state-of-theart performances.", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "Both of them suffer from the limitation of computational ability until Mikolov proposed two efficient solution, hierarchical softmax and negative sampling [10], to solve this problem.", "startOffset": 155, "endOffset": 159}, {"referenceID": 5, "context": "In order to combine the advantage of distributional word representation with the advantage of matrix factorization, Pennington proposed another famous word embedding model named Glove [6], which is reported to outperform the word2vec on some tasks.", "startOffset": 184, "endOffset": 187}, {"referenceID": 7, "context": "Chen proposed a character-enhanced Chinese word embedding model [8].", "startOffset": 64, "endOffset": 67}, {"referenceID": 8, "context": "Based on recursive neural network, Luong split a word into morpheme part and basic part [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "Recently, Kim has proposed a character-level language model [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "The corpus stems from the 2013 ACL Workshop on Machine Translation2 and is used in [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "For comparison, we choose three competitive baselines including CBOW, Skip-gram [5] and Glove [6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "For comparison, we choose three competitive baselines including CBOW, Skip-gram [5] and Glove [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 9, "context": "Both of the models can be solved via negative sampling and hierarchical softmax [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "CBOW and Skipgram acts as the baselines in a number of papers [12].", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "It is reported that Glove outperforms CBOW and Skip-gram in some NLP tasks [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "If huge-sized corpus is used, the number of negative samples should be chosen from 2 to 5 [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": "The dimension of word embedding is set as 200 like that in [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "We set the context window size as 5 which is equal to the setting in [10].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "This strategy is used in many papers [10], [6].", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "This strategy is used in many papers [10], [6].", "startOffset": 43, "endOffset": 46}, {"referenceID": 12, "context": "For English word similarity, there are two golden standard datasets including Wordsim-353 [13] and RG-65 [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "For English word similarity, there are two golden standard datasets including Wordsim-353 [13] and RG-65 [14].", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "Rare-Word [9], SCWS [15], Men-3k [16] and WS-353-Related [17].", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "Rare-Word [9], SCWS [15], Men-3k [16] and WS-353-Related [17].", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "Rare-Word [9], SCWS [15], Men-3k [16] and WS-353-Related [17].", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "Rare-Word [9], SCWS [15], Men-3k [16] and WS-353-Related [17].", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "This dataset with size of 8000 is created by Mikolov [18].", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "Specially, because of medium-size corpus and experimental settings, Glove doesn\u2019t perform as well as it in other papers [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 17, "context": "In [18], they divided the dataset into adjectives, nouns and verbs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "However, it is abnormal that all results are lower than the state-of-the-art results in [18].", "startOffset": 88, "endOffset": 92}], "year": 2017, "abstractText": "In this paper, we implicitly incorporate morpheme information into word embedding. Based on the strategy we utilize the morpheme information, three models are proposed. To test the performances of our models, we conduct the word similarity and syntactic analogy. The results demonstrate the effectiveness of our methods. Our models beat the comparative baselines on both tasks to a great extent. On the golden standard Wordsim-353 and RG-65, our models approximately outperform CBOW for 5 and 7 percent, respectively. In addition, 7 percent advantage is also achieved by our models on syntactic analysis. According to parameter analysis, our models can increase the semantic information in the corpus and our performances on the smallest corpus are similar to the performance of CBOW on the corpus which is five times ours. This property of our methods may have some positive effects on NLP researches about the corpus-limited languages.", "creator": "LaTeX with hyperref package"}}}