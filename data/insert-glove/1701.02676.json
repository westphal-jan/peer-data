{"id": "1701.02676", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Unsupervised Image-to-Image Translation with Generative Adversarial Networks", "abstract": "liberia It ' j.k. s awfi useful to borzoi automatically reevaluating transform cragg an image churchgate from aberlin its original iza form to some causant\u00edn synthetic form (style, partial starbase contents, etc. ), fluvoxamine while kasich keeping kcal the 1741 original tsu structure nasolacrimal or veis semantics. schnieder We define maximizes this requirement as the \" image - labute to - image translation \" substrate problem, and propose a stricto general approach kubot to achieve algeciras it, toyokawa based ubos on mi\u0161ko deep convolutional and conditional command-line generative adversarial neo-conservatives networks (stancik GANs ), which has panagia gained transshipment a p\u0142o\u0144sk phenomenal rentzias success a-lifetime to 449 learn indicus mapping odinism images quintessentially from kojic noise input game.com since face-offs 2014. walsenburg In this work, we freret develop smoker a two dans step (unsupervised) 124.44 learning method 4,140 to gel-like translate images between different lamentable domains by using ristorante unlabeled images ditommaso without 3,687 specifying skabo any homology correspondence between them, guiding so scarred that to non-recurring avoid the russom cost quack of guly\u00e1s acquiring labeled york-presbyterian data. Compared sliva with awesomeness prior meaties works, we chapatis demonstrated self-consciously the perturbing capacity of lunge generality olaya in our nfca model, neotropics by which fazzani variance mainwheels of translations can be cup conduct by df-1 a 109.67 single levox type of zippel model. marzuban Such s capability urbanite is desirable in applications bryden like 2,633 bidirectional translation", "histories": [["v1", "Tue, 10 Jan 2017 16:43:03 GMT  (1484kb,D)", "http://arxiv.org/abs/1701.02676v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hao dong", "paarth neekhara", "chao wu", "yike guo"], "accepted": false, "id": "1701.02676"}, "pdf": {"name": "1701.02676.pdf", "metadata": {"source": "META", "title": "Unsupervised Image-to-Image Translation with Generative Adversarial Networks", "authors": ["Hao Dong", "Chao Wu", "Yike Guo"], "emails": ["hao.dong11@imperial.ac.uk", "pniitucs@iitr.ac.in", "chao.wu@imperial.ac.uk", "y.guo@imperial.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "In this work we are interested in the problem of translating images from one domain to the images of other domains e.g. face of a person to another, aerial image to map, edges image to photos, black and white image to color image. Such translation is useful in creating synthetic images for various purpose. For example, by translating real images to synthetic counterparts, we can easily create balanced sizes of data for image classification tasks. We might also use this method to modify the context of photos (such as weather and background).\nThe community has been put a lot effort in this direction e.g. face swapping [9], changing time of day for outdoor image [19], changing weather of outdoor image [10], com-\nposed photo from sketch [3], but each of these tasks has been tackled with separate, special-purpose machinery. It would be more preferable if there is an universal model to achieve all these functions.\nFortunately, based on Generative Adversarial Networks (GANs) [5], a study [7] develop a common framework suitable for different problems, the generator in [7] is an encoder-decoder network which tries to synthesize fake image conditioned on a given image to fool discriminator, while the discriminator tries to identify the fake image by comparing with the corresponding target image. However, this work is supervised learning, therefore, it needs intensive input of labeled data for training, which is not always applicable in real cases.\n1\nar X\niv :1\n70 1.\n02 67\n6v 1\n[ cs\n.C V\n] 1\n0 Ja\nRecently, another work [20] demonstrated an unsupervised approach for image-to-image translation, the training images do not need to have a specific input and target. It employed a Domain Transfer Network to learn a generative network to map images from source domain to images in target domain. It also adopted a function from both domain to some metric, which is invariant under G. The method is general in its scope and does not rely on a predefined family of perceptual losses. Therefore, it can also be used for unsupervised domain adaption [2, 4].\nIn this work we purposed a two step learning method that utilize conditional GAN to learn the global feature of different domains, and learn to synthesize plausible image from random noise and given class/domain label. After that, a deep convolutional encoder is used to learn a mapping from an image to it\u2019s latent representation (noise). We demonstrated that using the trained deep convolutional encoder and conditional generator, images of different domains can be translated bidirectionally. Besides unsupervised learning, our method proposed a new way of training image encoder. To encourage the community to train different dataset, we will make code publicly available at 1.\nOne significant feature of our model is that it has preliminarily capability of one learning algorithm. According to one learning algorithm hypothesis, popularized by Jeff Hawkins [6], a universal learning machine is a powerful and general model for intelligent agents like human, in which the same network structure can serve for different learning purposes. Such capability is an desirable extension of deep learning algorithms, so that a universal algorithm as along with its deployment can adapt to the various tasks on hand. Compared to prior works in images translation, our method has such universal feature to support different learning scenarios"}, {"heading": "2. Background", "text": "In this section, we discuss the related work and the techniques be used in our work. A generative adversarial network (GAN) consists of a generator G and a discriminator D [5]. The discriminator is trained to maximize the probability of assigning the correct label to both training samples and samples generated from generator. While, the generator is trained to minimize the probability of assigning a samples from generator to be fake, so as to generate plausible samples conditioned on the random noise vector z. Following this, deep convolutional GAN [17] learns to synthesize images from unlabeled data, then an image can be represented by a fixed number of latent variables, and each of variable reflects a certain feature [17] e.g. controlling the window size of bathroom, smile of face and so on. The loss functions of D and G are shown as below, where P (s|X)\n1https://github.com/coming-soon\nindicates the score of a image to be real and Xfake = G(z).\nLD = log(P (s|D(Xreal))) + log(1\u2212 P (s|D(Xfake))) LG = log(P (s|D(Xfake))) (1)\nFollowing this, mass of studies show that images can be synthesized based on conditions such as sentence [18], class label [13], image inpainting [16] and image manipulation by constraints [21] etc. To synthesize image conditioned on a given class label c, auxiliary classifier GAN [15] demonstrate how to synthesize high quality image by using a new structure and loss function. In the generator G, input the class label c and the noise vector z, then a deconvolutional network [17] performs image generation conditioned on both class label c and the noise vector z. The discriminator is a convolutional network, which outputs the class label for the training data, that except requiring generate plausible image, the generator also trained to maximize the log-likelihood of the synthesize image to the correct class [14]. The loss functions as below, where Xfake = G(z, c):\nLD = log(P (s|D(Xreal))) + log(1\u2212 P (s|D(Xfake))) +log(P (c|D(Xreal))) LG = log(P (s|D(Xfake))) + log(P (c|D(Xfake))) (2) To synthesize an image from another image, the input image need to be projected into latent representation. In [21], author introduced a method to train a feedforward network for projecting an image to latent variables. The loss consists of two parts, mean-square-error (MSE) of input and output image for pixel-wise level reconstruction and the loss of hidden outputs for feature level reconstruction. Compared with previous works, we introduced a new method to train an image encoder specially for generator.\nThe one learning algorithm is a concept in machine learning and neuroscience that has been studied extensively by Je Hawkins [6]. It\u2019s based on a memory-prediction framework. The framework is based upon the work on columnar organization of the cerebral cortex which shows that all regions of the cortex perform the same operation."}, {"heading": "3. Method", "text": "Our image translation model is constituted with both deep convolutional encoder and decoder. It has input of images from source domain and output images of target domain(s), which conditioned on a given class/domain label. The training process has two steps: first step for image generator G for all domains, and second step for image encoder E for all domains as well. During the translation process, the conditional class label is applied to input image from source domain and go through the trained network for the output image in target domain."}, {"heading": "3.1. Learning shared feature", "text": "To utilize both the capability of unsupervised feature learning of GANs, and the representation capability of convolutional encoder, we separated the learning process into two steps.\nIn first step, as the left hand side of Figures 2 shows, we used auxiliary classifier GAN [15] to learn the global shared features of the images sampled from different domains. The shared features are represented as a latent variable (noise) vector z. In order to limit the variables into a specific range, we defined it as z \u2208 R \u223c U(\u22121, 1).\nThe idea is based on the fact that class-independent information contains global structure about the synthesized image [15]. Therefore, if the domains are semantically same, or similar (like different faces of peoples, different weather of environment), we expect the common features across the domains (like smile, face pose, background etc) to be captured by the same latent representation in the domains. We leverage this feature for translating the global structure from one domain to the other.\nConditional GANs usually use an embedding layer [18], we found same performance between embedding layer and one-hot format input. Although through experiment, we found the embedding layer contributed less to the performance. We believe this is due to the limit size of sampling domains in our experiment. If there are increasing size of domains (along with their relationships as word2vec [12]), such embedding layer can contribute more to the performance, compared with one-hot format."}, {"heading": "3.2. Learning image encoder", "text": "After the first step, the generator is able to generate corresponding images for different domains by keeping the latent variables fixed and changing the class label. In second step, we learn a mapping from image to global latent variable representation z.\nInstead of applying trained generator G on top of image encoder E [21] and except that the output image looks the same with the input image, we introduce a new method that applies E after the G from the first step as the middle of Figure 2 shows. We then except the network outputs the same variables with the input variables. Mean-squareerror (MSE) between input latent variables (noise) and output variable is used as the loss. We found this method is not only able to reconstruct the detailed feature but it also speed up the training.\nThe reason of this method working well is that: as the trained generator is able to synthesize image from arbitrary noise, we use the infinite synthesize images from the trained generator instead of the finite real image from dataset, so that we can have much more training data. Such feature is crucial since to train a image encoder with similar level of representation capacity as generator, the training datasets used should be at similar scale.\nBesides, there are two keys points about training: firstly, instead of passing more than just one class of images, we passed images of all classes, specifying the class along with it, this enables the image encoder which is able to work for all classes of image so as to achieve bidirectional translation; Secondly, as the noise z has uniform distribution, the outputs of E are scaled into [\u22121, 1] by tanh activation function."}, {"heading": "3.3. Translation", "text": "After finishing the training of generator and image encoder, as the right of Figure 2 shows, given an image and desired class/domain label, the network firstly maps the image to global latent representation z and then synthesizes the image of target domain by using conditional generator."}, {"heading": "4. Experiments", "text": "In this section, we firstly present results on the celebA face dataset for gender transformation and the presidential debate video for face swapping. The celebA face dataset [11] contains 84434 male images and 118165 female images, which has variety of backgrounds and faces. For the second data set, we detected and extracted face images of Obama and Hillary from two short presidential debate videos, there are 8452 images for Obama and 5065 images for Hillary, which has variety of face expression.\nWe used the same GAN and image encoder architecture for both datasets. The image size was 64 x 64, with 3 channel colors. We set the number of latent variables z to 100 and embedded the class label to a vector of 5 values. As in auxiliary classifier GAN [15], we used the learning rate of\n0.0002, and used the Adam optimizer [8] with momentum of 0.5 for both step 1 and 2. During training, we used batch size of 64, trained for 100 epochs for step 1 and 20,000 steps for step 2. Besides, we randomly flip, zoom in, crop and rotate the training images for data augmentation in step 1. The implementation was built with TensorLayer2 library on top of TensorFlow [1]."}, {"heading": "4.1. Gender transformation", "text": "As Figure 3 illustrated, the proposed method not only learned the characteristics and expressions of human face, but also learn to reconstruct background in some degree. On one hand, the translation has achieved desired performance, with the synthetic image having similar image quality as input images. On the other hand, the network can successfully translate face images for both female to male and male to female, which means, our method is bidirectional."}, {"heading": "4.2. Face swapping", "text": "Figure 4 shows the proposed method learns the expressions and face orientation very efficiently, so it does not violate the context. Such feature is very useful if we want to swap the faces in video."}, {"heading": "5. Conclusion", "text": "In this work, we proposed a simple and effective two step learning method for universal unsupervised image-toimage translation. In next step, we aim to further scale up\n2https://github.com/zsdonghao/tensorlayer\nthe network for higher resolution and try on more datasets. In the future, it might be interesting to combine symbolic features (like edge detection) with the original data, so that the performance of translation can be further improved. We will also try to make the method more general, to give it better characteristic of one learning algorithm, and apply it to various different other applications. We are considering to proposed an paradigm of \u201dmodular learning\u201d which conducts continuous recursive self-improvement with regards to the utility function (reward system). We can view this as second (and higher) order optimization."}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["A. Agarwal", "P. Barham", "e. a. Brevdo", "Eugene"], "venue": "Learning on Heterogeneous Distributed Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": "arXiv preprint arXiv:1206.4683", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Sketch2photo: internet image montage", "author": ["T. Chen", "M.-M. Cheng", "P. Tan", "A. Shamir", "S.-M. Hu"], "venue": "ACM Transactions on Graphics (TOG), 28(5):124", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Domainadversarial training of neural networks", "author": ["Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1505.07818", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "On intelligence", "author": ["J. Hawkins", "S. Blakeslee"], "venue": "Macmillan", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Imageto-image translation with conditional adversarial networks", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": "arXiv preprint arXiv:1611.07004", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "International Conference on Learning Representations", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast faceswap using convolutional neural networks", "author": ["I. Korshunova", "W. Shi", "J. Dambre", "L. Theis"], "venue": "arXiv preprint arXiv:1611.09577", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Transient attributes for high-level understanding and editing of outdoor scenes", "author": ["P.-Y. Laffont", "Z. Ren", "X. Tao", "C. Qian", "J. Hays"], "venue": "ACM Transactions on Graphics (TOG), 33(4):149", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "Proceedings of International Conference on Computer Vision (ICCV)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning with generative adversarial networks", "author": ["A. Odena"], "venue": "arXiv preprint arXiv:1606.01583", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "author": ["A. Odena", "C. Olah", "J. Shlens"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Krahenbuhl", "J. Donahue", "T. Darrell", "A.A. Efros"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial text to image synthesis", "author": ["S. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"], "venue": "ICML", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Data-driven hallucination of different times of day from a single outdoor photo", "author": ["Y. Shih", "S. Paris", "F. Durand", "W.T. Freeman"], "venue": "ACM Transactions on Graphics (TOG), 32(6):200", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised crossdomain image generation", "author": ["Y. Taigman", "A. Polyak", "L. Wolf"], "venue": "arXiv preprint arXiv:1611.02200", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["J.-Y. Zhu", "P. Kr\u00e4henb\u00fchl", "E. Shechtman", "A.A. Efros"], "venue": "European Conference on Computer Vision, pages 597\u2013613. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "face swapping [9], changing time of day for outdoor image [19], changing weather of outdoor image [10], comFigure 1.", "startOffset": 14, "endOffset": 17}, {"referenceID": 18, "context": "face swapping [9], changing time of day for outdoor image [19], changing weather of outdoor image [10], comFigure 1.", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "face swapping [9], changing time of day for outdoor image [19], changing weather of outdoor image [10], comFigure 1.", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "posed photo from sketch [3], but each of these tasks has been tackled with separate, special-purpose machinery.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "Fortunately, based on Generative Adversarial Networks (GANs) [5], a study [7] develop a common framework suitable for different problems, the generator in [7] is an encoder-decoder network which tries to synthesize fake image conditioned on a given image to fool discriminator, while the discriminator tries to identify the fake image by comparing with the corresponding target image.", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "Fortunately, based on Generative Adversarial Networks (GANs) [5], a study [7] develop a common framework suitable for different problems, the generator in [7] is an encoder-decoder network which tries to synthesize fake image conditioned on a given image to fool discriminator, while the discriminator tries to identify the fake image by comparing with the corresponding target image.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "Fortunately, based on Generative Adversarial Networks (GANs) [5], a study [7] develop a common framework suitable for different problems, the generator in [7] is an encoder-decoder network which tries to synthesize fake image conditioned on a given image to fool discriminator, while the discriminator tries to identify the fake image by comparing with the corresponding target image.", "startOffset": 155, "endOffset": 158}, {"referenceID": 19, "context": "Recently, another work [20] demonstrated an unsupervised approach for image-to-image translation, the training images do not need to have a specific input and target.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "Therefore, it can also be used for unsupervised domain adaption [2, 4].", "startOffset": 64, "endOffset": 70}, {"referenceID": 3, "context": "Therefore, it can also be used for unsupervised domain adaption [2, 4].", "startOffset": 64, "endOffset": 70}, {"referenceID": 5, "context": "According to one learning algorithm hypothesis, popularized by Jeff Hawkins [6], a universal learning machine is a powerful and general model for intelligent agents like human, in which the same network structure can serve for different learning purposes.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "A generative adversarial network (GAN) consists of a generator G and a discriminator D [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "Following this, deep convolutional GAN [17] learns to synthesize images from unlabeled data, then an image can be represented by a fixed number of latent variables, and each of variable reflects a certain feature [17] e.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Following this, deep convolutional GAN [17] learns to synthesize images from unlabeled data, then an image can be represented by a fixed number of latent variables, and each of variable reflects a certain feature [17] e.", "startOffset": 213, "endOffset": 217}, {"referenceID": 17, "context": "Following this, mass of studies show that images can be synthesized based on conditions such as sentence [18], class label [13], image inpainting [16] and image manipulation by constraints [21] etc.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "Following this, mass of studies show that images can be synthesized based on conditions such as sentence [18], class label [13], image inpainting [16] and image manipulation by constraints [21] etc.", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "Following this, mass of studies show that images can be synthesized based on conditions such as sentence [18], class label [13], image inpainting [16] and image manipulation by constraints [21] etc.", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": "Following this, mass of studies show that images can be synthesized based on conditions such as sentence [18], class label [13], image inpainting [16] and image manipulation by constraints [21] etc.", "startOffset": 189, "endOffset": 193}, {"referenceID": 14, "context": "To synthesize image conditioned on a given class label c, auxiliary classifier GAN [15] demonstrate how to synthesize high quality image by using a new structure and loss function.", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "In the generator G, input the class label c and the noise vector z, then a deconvolutional network [17] performs image generation conditioned on both class label c and the noise vector z.", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "The discriminator is a convolutional network, which outputs the class label for the training data, that except requiring generate plausible image, the generator also trained to maximize the log-likelihood of the synthesize image to the correct class [14].", "startOffset": 250, "endOffset": 254}, {"referenceID": 20, "context": "In [21], author introduced a method to train a feedforward network for projecting an image to latent variables.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "The one learning algorithm is a concept in machine learning and neuroscience that has been studied extensively by Je Hawkins [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 14, "context": "In first step, as the left hand side of Figures 2 shows, we used auxiliary classifier GAN [15] to learn the global shared features of the images sampled from different domains.", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "The idea is based on the fact that class-independent information contains global structure about the synthesized image [15].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "Conditional GANs usually use an embedding layer [18], we found same performance between embedding layer and one-hot format input.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "If there are increasing size of domains (along with their relationships as word2vec [12]), such embedding layer can contribute more to the performance, compared with one-hot format.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Instead of applying trained generator G on top of image encoder E [21] and except that the output image looks the same with the input image, we introduce a new method that applies E after the G from the first step as the middle of Figure 2 shows.", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "The celebA face dataset [11] contains 84434 male images and 118165 female images, which has variety of backgrounds and faces.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "As in auxiliary classifier GAN [15], we used the learning rate of", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "0002, and used the Adam optimizer [8] with momentum of 0.", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "The implementation was built with TensorLayer2 library on top of TensorFlow [1].", "startOffset": 76, "endOffset": 79}], "year": 2017, "abstractText": "It\u2019s useful to automatically transform an image from its original form to some synthetic form (style, partial contents, etc.), while keeping the original structure or semantics. We define this requirement as the \u201dimage-to-image translation\u201d problem, and propose a general approach to achieve it, based on deep convolutional and conditional generative adversarial networks (GANs), which has gained a phenomenal success to learn mapping images from noise input since 2014. In this work, we develop a two step (unsupervised) learning method to translate images between different domains by using unlabeled images without specifying any correspondence between them, so that to avoid the cost of acquiring labeled data. Compared with prior works, we demonstrated the capacity of generality in our model, by which variance of translations can be conduct by a single type of model. Such capability is desirable in applications like bidirectional translation", "creator": "LaTeX with hyperref package"}}}