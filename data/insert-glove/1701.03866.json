{"id": "1701.03866", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2017", "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory", "abstract": "palmilla Credit assignment chishima in traditional recurrent coste neural networks d'aoust usually involves metru back - 116-110 propagating through gatka a long quain chain kunuk of tied weight hadlock matrices. sanzone The \u043d length of maccormack this gjirokaster chain earlville scales natalegawa linearly with shikigami the number ryuji of nsf time - steps pile-up as the same network tonsillectomies is run sydkraft at nilgiris each time - phonograph step. kellermann This creates many problems, such autolite as vla vanishing gradients, disbarred that have been tightrope well studied. voltz In baiocchi contrast, a non-perturbative NNEM ' s architecture recurrent zora activity simonon doesn ' t colborn involve eclair a \u00fabeda long northlight chain heartily of 27b activity (i-1 though raindrop some suzaan architectures torcida such williams-sonoma as worthwhile the nadery NTM heilbrun do slaby utilize a traditional highland recurrent 4,080 architecture as viikmae a controller ). bad Rather, the whiterock externally stored pigou embedding vectors are peter used dna-based at pirker each jajoo time - 225.6 step, but newsworthiness no messages aj are osieck passed from previous equipe time - songaila steps. carlyn This jiminy means 116.8 that double-barrelled vanishing gradients aren ' t 6ft a sadhana problem, 2,705 as all sony of the necessary gradient paths are g\u0142uch\u00f3w short. However, gemologists these paths are meet-and-greet extremely listens numerous (one corgan per embedding vector in th\u00f6k\u00f6ly memory) 18.53 and edicts reused for bu a very long 63-60 time (filho until it physostigmine leaves outdistanced the memory ). Thus, the bazin forward - mandora pass phar information fidgety of acorus each memory sytems must be dominican stored lillenas for the seree entire orofino duration kasas of 86.3 the refusing memory. mediterannean This shumen is englewood problematic as hongyun this additional storage far 125.70 surpasses omi that of the curley actual gonadotropins memories, vajradhara to the extent comparators that garsten large memories on infeasible aptos to daihatsu back - scribal propagate through 20.3 in houti high dimensional woerle settings. .470 One ensnared way to 81-member get around the deighton need to hold bottlenose onto narvel forward - pass waterland information hotting is clowns to recalculate razim the forward - kunichika pass agelena whenever kvalheim gradient information is available. c/k However, if 1.3995 the rodeheaver observations 1,268 are too gelfond large to store hydrogenation in smokies the umbilicate domain tijani of kosice interest, direct pampeago reinstatement of a 39.6 forward ecis pass enesco cannot 34.5 occur. Instead, we rely expertise on a learned autoencoder alvira to tyme reinstate \u59ec the observation, kunta and carnivory then use the embedding goms network arise to kuchak recalculate survey the tonganoxie forward - pass. diyarbak\u0131rspor Since nygaard the recalculated embedding internacia vector upright is unlikely to perfectly match pujara the one stored in off-loaded memory, splenda we mixteca try asmi out bloembergen 2 jltv approximations bullit to ekadashi utilize error watercolor gradient w. .0216 r. t. contactable the vector 100.3 in bloodsucker memory.", "histories": [["v1", "Sat, 14 Jan 2017 01:47:54 GMT  (164kb,D)", "http://arxiv.org/abs/1701.03866v1", "Accepted into the NIPS 2016 workshop on Continual Learning"]], "COMMENTS": "Accepted into the NIPS 2016 workshop on Continual Learning", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["steven stenberg hansen"], "accepted": false, "id": "1701.03866"}, "pdf": {"name": "1701.03866.pdf", "metadata": {"source": "CRF", "title": "Long Timescale Credit Assignment in Neural Networks with External Memory", "authors": ["Steven S. Hansen"], "emails": ["sshansen@stanford.edu"], "sections": [{"heading": null, "text": "Long Timescale Credit Assignment in Neural Networks with External Memory\nSteven S. Hansen Department of Psychology\nStanford University Stanford, CA 94303\nsshansen@stanford.edu"}, {"heading": "1 Introduction", "text": "Neural networks with external memories (NNEM),such as the Neural Turing Machine [1] and Memory Networks [2], have often been compared to the human hippocampus in their ability to maintain episodic information over long timescales [3]. But so far, these networks have only been trained on tasks requiring memory storage comparable to a few minutes, whereas the hippocampus stores information on the order of years. It is well known that this sort of long-term episodic storage is vital for overcoming the problem of catastrophic interference [4], which is central to the challenge of continual learning.\nWhile some of this discrepancy can be attributed to the short-term nature of current benchmark tasks, a major bottleneck comes form the reliance on using back-propagation-through-time to learn the embedding function that maps the high-dimensional observations onto lower dimensional representations suitable for storage in the external memory. This is problematic because memories might be used an extremely long time after the embedding function was called, and back-propagation-through-time would require us to hold onto the forward activations of this function for the entire duration if we want to back-propagate through it.\nConsidering that in a continual learning setup, an external memory store on the order of hundreds of thousands, or even millions, of separate embeddings could be needed, storing all of the intermediate embedding activations is woefully intractable. A naive solution would be to simply store all of the memories in observation space in addition to embedding space and recompute the embeddings as needed for credit assignment, but a significant computational burden would result from these redundant forward-passes. More importantly, the high dimensionality of the observation space would make storing these additional memories take an unreasonable amount of memory. Indeed, Blundell et al [5] noted that even in the relatively modest Atari domain, storing a few million observations in pixel-space would require upwards of 300 gigabytes1."}, {"heading": "2 Methods", "text": ""}, {"heading": "2.1 Long Timescale Credit Assignment in NNEMs", "text": "Credit assignment in traditional recurrent neural networks usually involves back-propagating through a long chain of tied weight matrices. The length of this chain scales linearly with the number of time-steps as the same network is run at each time-step. This creates many problems, such as vanishing gradients, that have been well studied [7]. In contrast, a NNEM\u2019s architecture recurrent\n1While their work utilizes a long timescale external memory, it does not address the problems raised here, as the embedding function was not optimized for usage in an external memory. Rather, it was either a random projection or pre-trained on a separate objective. Indeed, the low performance of the latter suggests that end-to-end optimization might be necessary for adaptive embedding functions to be worthwhile in this context.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n70 1.\n03 86\n6v 1\n[ cs\n.A I]\n1 4\nJa n\n20 17\nactivity doesn\u2019t involve a long chain of activity (though some architectures such as the NTM do utilize a traditional recurrent architecture as a controller). Rather, the externally stored embedding vectors are used at each time-step, but no messages are passed from previous time-steps. This means that vanishing gradients aren\u2019t a problem, as all of the necessary gradient paths are short. However, these paths are extremely numerous (one per embedding vector in memory) and reused for a very long time (until it leaves the memory). Thus, the forward-pass information of each memory must be stored for the entire duration of the memory. This is problematic as this additional storage far surpasses that of the actual memories, to the extent that large memories on infeasible to back-propagate through in high dimensional settings."}, {"heading": "2.2 Synthetic Gradients", "text": "Synthetic gradients are a recent technique created to deal with situation whether a part of a network is \u2018blocked\u2019 from further usage [6], such as in the case previously discussed whereby information must be stored until all gradient calculations are complete. Their way around this problem is to treat gradient signal as just another function to be approximated. Namely, a function mapping from an activation vector, and any contextual information, onto to the error gradient w.r.t. that activation vector. This auxiliary network can be used to calculate an estimate of the gradient before the true gradient is available. This synthetic gradient producing network can be trained in a purely supervised manner by comparing its estimates to the true gradient.\nAs shown in Figure 1B, we can apply this approach to NNEMs by updating the embedding network with a synthetic gradient immediately after the embedding vector is calculated. True gradient signals are available for all of the previously embedded memories, and these can be used to train the synthetic gradient network."}, {"heading": "2.3 Reinstatement for Credit Assignment", "text": "One way to get around the need to hold onto forward-pass information is to recalculate the forwardpass whenever gradient information is available. However, as previously mentioned, even the observations are too large to store in our domain of interest, which prevents a direct reinstatement of a forward pass from occurring. Instead, we rely on a learned autoencoder to reinstate the observation, and then use the embedding network to recalculate the forward-pass. Since the recalculated embedding vector is unlikely to perfectly match the one stored in memory, it is non-obvious how to utilize error gradient w.r.t. the vector in memory.\nThere are two distinct possibilities: ignore the mismatch and directly use the memory\u2019s gradient, or use the fresh embedding instead of the memory during inference. These two methods are illustrated in Figure 1C and D, and while they aren\u2019t mutually exclusive, they are treated as such for the purposes of this paper, to aid in interpreting the initial results. The latter option is a true gradient signal whereas the former is only an approximation, but it relies on having a good enough reconstruction to not harm the inference process."}, {"heading": "3 Results", "text": "To test our model, we used the standard MNIST benchmark, the objective of which is to correctly classify images of numerals by the integer they represent. While this task is trivial for modern networks with the appropriate architecture, we use it here as it represents one of the simplest tasks that could, under certain conditions, still encounter the credit assignment problem previously described.\nSince our aim is to tractably train embedding functions within a network with an external memory, we constructed a simple NNEM consisting of an embedding network and an external store that holds the embeddings of the 5000 most recently encountered images along with their correct labels. Classifications are made by taking the cosine similarity between the embedding of the current image and all of the embeddings in memory, and using the normalized result to weight class labels associated with the memorized embeddings. As our focus is on weight updates derived from the memorized embeddings, we stop the gradient going into the embedding function coming from the current image, as these are sufficient for good performance in this simplified setting. In addition, we found that completely unsupervised learning of an autoencoder was sufficient to yield decent performance. Since we\u2019re primarily interested in getting the supervised signal to travel through the stored embeddings, we only allow the reconstruction error to modify the decoder\u2019s weights, as the decoder isn\u2019t used for inference like the encoder/embedding function is.\nThe results are shown in Figure2. All conditions utilize the same network architecture2 and hyperparameters, different only in their mechanism for credit assignment. The reinstatement method with exact gradients performed the best, but requires the most additional computation. Synthetic gradient performance is perhaps lower than expected, but it is possible that this is due to the network architecture. The surrounding memories affect the gradient of any individual memory, but these can\u2019t\n2a single hidden layer for each piece of the network, with dimension 256. ADAM was used for optimization with a learning rate of 1e-4 elsewhere\nbe an input to the synethic gradient network, as future memories aren\u2019t observable. The approximate gradient version of the reinstatement method was the most prone to divergence, which makes sense given the approximation involved."}, {"heading": "4 Future Directions", "text": "The absolute performance on this MNIST task isn\u2019t useful in itself, as the performance was purposely sabotaged to isolate the novel mechanisms under discussion. However, while the relative performance changes demonstrate the promise of reinstatement based credit assignment, the contrived domain necessitates further work to confirm these results on large scale problems. One possibility would be tackle reinforcement learning problems like Atari, using a variant of the model-free episodic control network [5] modified such that the embedding function is learned on-line. While the exact modifications needed to make that setting compatible with this approach to credit assignment is outside the scope of this paper, it is clear that this domain is one of the few present in the current literature that calls for a long term episodic store.\nIn a more complex environment, additional nuances emerge which might improve credit assignment performance. Given the reliance on accurate decoding for proper reinstatement, pre-training the autoencoder offline might be crucial. Vanilla autoencoders also tend to overfit, which might slow performance when used to assign credit to new examples. Variational autoencoders have been shown to be more robust [8] and also provide uncertainty estimates that could be useful when deciding which embeddings to discard. A hybrid approach utilizing synthetic gradients and both forms of AE based credit assignment might also be worthwhile, as these approaches appear to have complementary weaknesses.\nWhile the focus has been on credit assignment, adaptive long-term episodic memories raise other concerns, such as the fact that stored embedding go \u2018stale\u2019 rapidly, as the embedding function\u2019s parameters change. If the rate of change is slow enough, then operating the memories as a queue (first in, first out) should limit the impact this has on performance. But in many setups, embeddings are discarded as a function of their usage [5] such that frequently needed embeddings are kept around long enough for staleness to become problematic. While only speculative, an autoencoder could also be used to \u2018freshen\u2019 these embeddings by re-encoding them using the current parameters."}], "references": [{"title": "Hybrid computing using a neural network", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwi\u0144ska", "A.P. Badia"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory", "author": ["J.L. McClelland", "B.L. McNaughton", "R.C. O\u2019Reilly"], "venue": "Psychological review,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Model-free episodic control.", "author": ["Blundell", "Charles"], "venue": "arXiv preprint arXiv:1606.04460", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Decoupled neural interfaces using synthetic gradients. arXiv preprint arXiv:1608.05343", "author": ["M. Jaderberg", "W.M. Czarnecki", "S. Osindero", "O. Vinyals", "A. Graves", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["S. Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Neural networks with external memories (NNEM),such as the Neural Turing Machine [1] and Memory Networks [2], have often been compared to the human hippocampus in their ability to maintain episodic information over long timescales [3].", "startOffset": 230, "endOffset": 233}, {"referenceID": 1, "context": "It is well known that this sort of long-term episodic storage is vital for overcoming the problem of catastrophic interference [4], which is central to the challenge of continual learning.", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Indeed, Blundell et al [5] noted that even in the relatively modest Atari domain, storing a few million observations in pixel-space would require upwards of 300 gigabytes1.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "This creates many problems, such as vanishing gradients, that have been well studied [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "Synthetic gradients are a recent technique created to deal with situation whether a part of a network is \u2018blocked\u2019 from further usage [6], such as in the case previously discussed whereby information must be stored until all gradient calculations are complete.", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "One possibility would be tackle reinforcement learning problems like Atari, using a variant of the model-free episodic control network [5] modified such that the embedding function is learned on-line.", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Variational autoencoders have been shown to be more robust [8] and also provide uncertainty estimates that could be useful when deciding which embeddings to discard.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "But in many setups, embeddings are discarded as a function of their usage [5] such that frequently needed embeddings are kept around long enough for staleness to become problematic.", "startOffset": 74, "endOffset": 77}], "year": 2017, "abstractText": "Neural networks with external memories (NNEM),such as the Neural Turing Machine [1] and Memory Networks [2], have often been compared to the human hippocampus in their ability to maintain episodic information over long timescales [3]. But so far, these networks have only been trained on tasks requiring memory storage comparable to a few minutes, whereas the hippocampus stores information on the order of years. It is well known that this sort of long-term episodic storage is vital for overcoming the problem of catastrophic interference [4], which is central to the challenge of continual learning.", "creator": "LaTeX with hyperref package"}}}