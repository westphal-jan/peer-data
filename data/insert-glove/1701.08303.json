{"id": "1701.08303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jan-2017", "title": "Drug-Drug Interaction Extraction from Biomedical Text Using Long Short Term Memory Network", "abstract": "A tessmer drug ellingson can katmandu affect the activity lobbering of 56.93 other drugs, when administered together, in both 52.54 synergistic wooter or vosganian antagonistic lebor ways. abarbanel In d'alimonte one hand synergistic tufo effects sansapor lead 917k to improved therapeutic outcomes, antagonistic trofim consequences can tr\u01b0\u01a1ng be life - \u010drnomelj threatening, leading valorie to increased healthcare 109.95 cost, or may even gooz cause death. d/s Thus, caracappa identification of unknown chakras drug - drug re-branding interaction (harrisons DDI) is an pahala important lavatories concern pepped for ferrey efficient and toral effective word-initial healthcare. mani Although there sabse exist anatoma multiple outdo resources manacles for DDI, they atanu often ultor unable to chalkias keep pace with rich amount of ailments information gryphus available uncoiling in fast thefacebook.com growing advisers biomedical tolj texts including komisarjevsky literature. 62.26 Most izi existing reloaded methods hardy model thunderclan DDI lunchroom extraction groome from sedately text as classification fangire problem and mainly otlk rely pompeiian on kriegsmarine handcrafted buts features. Some 8,833 of these features further depends on ventas domain specific brive-la-gaillarde tools. trashers Recently lalgarh neural \u043e network 99ers models using sel latent carr\u00e0 features has non-users shown bullheads to sany\u014d be tribals perform molehill similar or counter-proposal better than knbr the other existing models domela using sally handcrafted mccains features. neifi In 38.48 this paper, 49b we samguk present three models 1,296 namely, sonsonate B - zuckerbrod LSTM, AB - formula LSTM morarjee and thresh Joint AB - frot LSTM buttimer based krinsky on artley long dissonances short - term memory (LSTM) network. pinstech All balaur three gratified models 4.60 utilize word 29-story and demetrious position lagan embedding lasater as latent 16 features and doukhobor thus euromed do not ameli rely rudes on 4,067 feature polytonic engineering. Further 14-nation use breast of bidirectional long tarki short - term memory (tsay Bi - wallone LSTM) networks manotoc allow u.n.-led to thmei extract optimal salsify features penne from sarkander the talo whole sentence. otolith The 16o two r.winters models, abderraouf AB - LSTM and Joint AB - allografts LSTM baijal also tacoma use attentive synovate pooling in the cinnamomea output of Bi - vays LSTM layer to vilkkumaa assign blames weights avgerinos to features. Our experimental overlaid results teletypewriter on 0:54 the SemEval - 2013 DDI hardees extraction dataset shows battistini that 2.31 the matsu Joint AB - world-leading LSTM 170.3 model bal\u0161i\u0107 outperforms selje all \u03b5 the existing methods, gasp\u00e9e including those euro89 relying pickfair on handcrafted ancilla features. The other vancouver-based two proposed models altazimuth also reassembles perform monothelitism competitively logsdon with vasilyevich state - sub-watershed of - the - sezer art methods.", "histories": [["v1", "Sat, 28 Jan 2017 17:04:21 GMT  (352kb,D)", "http://arxiv.org/abs/1701.08303v1", "10 pages, 3 figures"], ["v2", "Sun, 13 Aug 2017 12:56:03 GMT  (444kb,D)", "http://arxiv.org/abs/1701.08303v2", "Under review to the Journal of Biomedical Informatics"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sunil kumar sahu", "ashish anand"], "accepted": false, "id": "1701.08303"}, "pdf": {"name": "1701.08303.pdf", "metadata": {"source": "CRF", "title": "Drug-Drug Interaction Extraction from Biomedical Text Using Long Short Term Memory Network", "authors": ["Sunil Kumar Sahu", "Ashish Anand"], "emails": ["sunil.sahu@iitg.ernet.in", "anand.ashish@iitg.ernet.in", "permissions@acm.org."], "sections": [{"heading": "Keywords", "text": "Information Extraction, Recurrent Neural Network, Long Short Term Memory, Attention Model"}, {"heading": "1. INTRODUCTION", "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\nc\u00a9 2017 ACM. ISBN 978-1-4503-2138-9. DOI: 10.1145/1235\nRecent times has seen a significant rise in number of people taking multiple drugs at the same time. According to the numbers released in 2010 by the US Centers for Disease Control and Prevention, one in ten Americans is on five or more medications. Similar statistics can be expected from other countries as well. When multiple drugs are administered together, there is an inevitable risk of a drug affecting the activity of other drugs. Effect of DDI can be either synergistic or antagonistic. Adverse drug reaction (ADR) is an example of antagonistic effect causing immense health risks and sometimes even leading to death [6]. With the rise in people taking multiple drugs, it is very important to have DDI information available in structured form. Although there exist multiple knowledge base (KB) such as DrugBank1, Stockley2 but manually keeping them update is time consuming and labor intensive. As a result, they fail to keep pace with information available in exponentially growing biomedical texts including research articles. This lead to surge in interest in automatic detection and extraction of DDI information from biomedical texts to extend the available KBs.\nIdentifying DDIs in text is the process of recognizing how two drugs in a given sentence are related [25]. We illustrate different interaction types between two drugs through examples in Table1. The two pairs (Fluoxetine, Phenelzine) and (Crocin, Phenelzine ) in the sentence [S1] are advised not to be taken together. So two drugs within a pair is interacting and in both cases interaction type is Advice as its only a suggestion to be not taken together. Similarly, drugs (PGF2alpha, Oxytocin) and (Ketamine, Halothane) are interacting and type of interactions are Effect and Mechanism in [S2] and [S3] respectively since impact and mechanism of impact are present in the respective sentences. Sentence [S4] does not say anything more than that the two drugs (Warfarin and Rifampin) are interacting, so it falls into Interaction (Int) class. Identifying this kind of information can also be useful for other applications such as drug repurposing, semantic search and other information retrieval tasks.\nRealizing the importance of interaction information between drugs, two challenges were organized as part of SemEval, an annually organized international workshop on semantic evaluation in 2011 and 2013. While SemEval 2011 task [26] was focused on DDI detection assuming that drugs were already recognized, SemEval 2013 task-9 [25] was de-\n1https://www.drugbank.ca/ 2https://www.medicinescomplete.com/mc/alerts/current/druginteractions.htm\nar X\niv :1\n70 1.\n08 30\n3v 1\n[ cs\n.C L\n] 2\n8 Ja\nn 20\n17\nsigned as two tasks: first task concentrated on drug name recognition and their classification, and second was on DDI extraction from biomedical texts, same as in SemEval 2011 task. In this work, we focus on the second task of the SemEval 2013 task-9 only. These two challenges spurred the development of models for DDI extraction. Existing methods can be classified into two categories: 1-stage and 2-stage methods. In 1-stage methods [3, 13, 32], a multi-class classifier is used to map a sentence with two target drugs either into one of the interacting classes or into the negative class. On the other hand 2-stage methods [23, 2] breaks the problem into 2 steps. The first step builds a binary classifier to determine whether there is an interaction exists between two target drugs or not. Only sentences with target drug pairs are considered, which fell into positive category of the binary classifier in the previous step, as input to multi-class classifier of the second step. These methods can further be divided into two categories, methods relying on handcrafted features and methods using latent features. In the first category mainly support vector machines (SVMs) with linear or non-linear kernels have been used in many studies [8, 4, 16]. All of these methods are dependent on manually engineered features such as PoS tag, chunk tag, trigger words, shortest dependency tree and syntax tree. Method using non-linear kernels map structure features (dependency tree and syntax tree) in to real values. Although such methods have been shown to perform well, it required carefully crafting features. Extraction of these features are however dependent on other NLP tools, and hence get adversely affected by noise and cost of such tools. Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24]. Methods using latent features and belonging to the second category, are result of re-emergence of deep learning models as a powerful alternative to conventional feature based models. Some notable studies [18, 36] for the DDT extraction tasks are based on convolution neural networks (CNNs) and have been shown to achieving superior performance than the existing state-of-the-art methods. We discuss about these methods again in section 4.2.\nIn this work, we also rely on latent features learned by neural network models. As opposed to works in [18, 36], we use LSTM based neural network models [15] instead of CNN. CNN models require pooling over the whole sentence based on continuous n grams to obtain constant length features. Here n is the length of convolution or filter. It may cause problems for the sentences of large length and/or having important clues lying far away from each other. To overcome this issue we use Bi-LSTM with pooling techniques for encoding variable length features. Theoretically a Bi-LSTM can preserve information about past and future words while reading [15]. Therefore when we apply pooling on the out-\nput of Bi-LSTM, we can get optimal features from whole sentence which contained information about complete context rather than n gram of the sentence. With this intuition we propose three models namely: B-LSTM, AB-LSTM and Joint AB-LSTM for the DDI extraction task. Here B-LSTM and AB-LSTM uses a Bi-LSTM for encoding word and position features. B-LSTM uses max pooling and AB-LSTM uses attentive pooling on the outputs of Bi-LSTM to get fixed length features over complete sentence. On the other hand, Joint AB-LSTM being an ensemble of B-LSTM and AB-LSTM uses two Bi-LSTMs, one with max pooling and another with attentive pooling. In each of these models we use fully connected neural network in output layer.\nAll the three proposed models give either competitive performance to the existing methods or have achieved new stateof-the-art performance. The two important features of the models are: all of them belong to 1-stage category and use simple features. None of the chosen feature explicitly extract syntactic information hidden in sentence. Among the three proposed models, Joint AB-LSTM outperformed all existing models for the DDI extraction tasks."}, {"heading": "2. MODEL ARCHITECTURE", "text": "We present three LSTM based models namely B-LSTM, AB-LSTM and Joint AB-LSTM for the DDI extraction task. We assume that the two targeted drug names are given in a sentence and model has to either detect whether there is an interaction or not, or classify them into one of the categories: Advice, Effect, Mechanism, Int, Negative, depending on the task. We describe about the tasks in detail in the section 3. Architecture of the three proposed models are shown in figure 1. Each model uses embedding features as input in first layer and learn fixed length vector representation through subsequent layers. Score for each possible class is computed in the final layer and final decision is made using this score. Training of the model happen in end-to-end manner such a way that correct class will get high score after training. We now briefly explain each components of the three models."}, {"heading": "Feature Layer", "text": "We represent each word in the sentence with 3 discrete features namely: word (W), distance1 (P1), distance2 (P2). Here W is exact word appeared in the sentence. P1 is distance (in terms of words) from the first drug name [9, 30]. This value would be zero for first targeted drug name word. P2 is similar to P1 but considers distance from the second targeted drug name. This way a word w \u2208 D1 \u00d7D2 \u00d7D3, where Di is the dictionary for ith local features. This feature layer constitute the first layer for all models."}, {"heading": "Embedding Layer", "text": "In embedding layer, each discrete feature is mapped to a real-valued vector representation using a lookup or embed-\nding matrix. Lets say M i is the embedding matrix for ith feature. Here each column of M i is a vector for the value in ith feature. Mapping can be done by taking product of one hot vector of feature value with its embedding matrix [9]. Suppose a (i) j is the one hot vector for j th feature value of ith feature then embedding layer will output:\nf (i) j = M i.a (i) j (1)\nxi = f (i) 1 \u2295 f (i) 2 \u2295 f (i) 3 (2)\nHere \u2295 is concatenation operation so xi \u2208 R(n1+....n3) is feature vector for ith word in sentence and nk is dimension of kth feature. Pre-trained word vectors are used for word embedding matrix and other feature matrices are initialized with random values."}, {"heading": "Bi-LSTM Layer", "text": "Recurrent neural network is a powerful model for modeling sequential data [19]. It is a network with loop, allowing information to persist through out the sequence. However because of long sequence it may suffer with vanishing or exploding gradient problems [10, 20]. LSTM aims to overcome this problem by gating and memory mechanism. LSTM\nlayer is just another way to compute a hidden state which introduces a new structure called a memory cell (ct) and three gates called as input (it), output (ot) and forget (ft) gates. These gates are composed of sigmoid activation function and responsible for regulating information in memory cell and final output of LSTM will be calculated on the basis of new states of cell.\nConsider x1x2.....xm is the sequence of feature vectors of a sentence, where m is the length of sentence and xt \u2208 Rd is a vector obtained by concatenating all feature vector of tth word. Let h (t\u22121) l and c (t\u22121) l is previous hidden and cell state of LSTMl respectively then computation of current hidden state (h (t) l ), cell state c (t) l and output of Bi-LSTM (z(t)) would be.\ni (t) l = \u03c3(U (i) l x (t) +W (i) l h (t\u22121) l + b i l) f (t) l = \u03c3(U (f) l x (t) +W (f) l h (t\u22121) l + b f l ) o (t) l = \u03c3(U (o) l x (t) +W (o) l h (t\u22121) l + b o l ) g (t) l = tanh(U (g) l x (t) +W (g) l h (t\u22121) l + b g l ) c (t) l = c (t\u22121) l \u2217 f (t) l + g (t) l \u2217 i (t) l h (t) l = tanh(c (t) l ) \u2217 o (t) l\nWhere \u03c3 is sigmoid activation function, \u2217 is a element wise product, U\n(i) l , U (f) l , U (o) l , U (g) l \u2208 R N\u00d7d, W (i) l , W (o) l ,\nW (f) l , W (g) l \u2208 R N\u00d7N , bil, b f l , b o l , b g l \u2208 R N , h (0) l , c (0) l \u2208 R N are learning parameters for LSTMl. Here d is dimension of input feature vector and N is hidden layer size. h (t) l is output of LSTMl at time step t. We compute h (t) r in similar manner as h (t) l by reversing the words of sentence. A separate LSTMr is used for this calculation. The final output for tth word by Bi-LSTM would be:\nz(t) = (h (t) l \u2295 h (t) r ) (3)"}, {"heading": "Pooling Layer", "text": "The idea of pooling layer is to get a fixed length optimal features from variable length word features. We experiment with two different kind of pooling schemes:\n(A) Max Pooling The intuition behind using Max pooling is, taking one optimal over entire sequence. Bi-LSTM accumulate information in both forward and backward direction, each node is assumed to have entire information of sentence. Max pooling takes the maximum over entire sentence assuming all important and relevant information are accumulated in that position. Let z1z2...zm (zi \u2208 RN ) be the sequence of vectors obtained after concatenating forward, backward LSTM output of each word then:\nz = max 1\u2264i\u2264(m)\n[zi] (4)\nWhere z \u2208 RN is dimension wise max of entire zi\u2019s.\n(B) Attentive Pooling Max pooling may fail to perform well when important clues for the DDIs are present in different clauses or are faraway in the sentence. To overcome this issue we use attentive pooling which take optimal based on weighted linear combination of feature vectors. Weights of the feature vectors are computed through attention mechanism which assign weights based on importance of that features [1, 35, 37]. The attention mechanism produces a vector \u03b1 of size equal to length of sentence. The values in this vector is the weights we would assign to each word feature vectors. Weighted linear combination of Bi-LSTM outputs and attention weights are the output of attentive pooling layer. Let Z \u2208 RN\u00d7m be the matrix of outputs, obtained by Bi-LSTM then output of attentive pooling would be:\nH = tanh(W aZ)\n\u03b1 = Softmax(waTH)\nz = \u03b1ZT (5)\nWhere W a \u2208 RN\u00d7N , wa \u2208 RN are learning parameters, \u03b1 \u2208 Rm is attention weights and z \u2208 RN would be the output of attentive pooling layer. Thing should be noted that for every sentence \u03b1 would be different."}, {"heading": "Fully Connected and Softmax", "text": "Output of pooling layer would be fixed length vector, which we non-linearize by using tanh activation and then fed to\nfully connected neural layer. In fully connected layer we maintain number of node equals to number of class.\nh3 = tanh(h2)\np(y|x) = Softmax(W oh3 + bo) (6)\nHere h2 would be the output of pooling layer, W o \u2208 RN\u00d7C , bo \u2208 RC are parameters of fully connected neural network and C is number of class in the our model. To make classification we use softmax function in the output of fully connected layer. Softmax will give normalized probability score for each class."}, {"heading": "Training and Implementation", "text": "All three models use cross entropy loss function for training the entire network. Adam\u2019s technique [17] is used for optimization. We use batch size 500 for entire training in each model. Implementation is done in python language using Tensorflow3 package."}, {"heading": "2.1 B-LSTM Model", "text": "B-LSTM is similar to the model proposed in [18] for DDI extraction task. Here we use Bi-LSTM in place of convolution neural network used in [18]. As shown in figure 1a, this model apply Max pooling in the output of Bi-LSTM to get optimal fixed length features. Max pooling are obtained through Equation 4 for every instance. These features are then fed to fully connected followed by softmax layer to obtain final classification."}, {"heading": "2.2 AB-LSTM Model", "text": "Figure 1b is the pictorial representation of AB-LSTM model. In this case we apply attentive pooling in the output BiLSTM. Attention weights are obtained through Equation 5 for each sentence. Output of attentive pooling layer was used as features to make classifier by feeding this to fully connected and softmax layers."}, {"heading": "2.3 Joint AB-LSTM Model", "text": "The idea of using Joint AB-LSTM is to take the advantage of both max and attentive pooling. As shown in figure 1c Joint AB-LSTM model uses two separate modules each with a Bi-LSTM network. Both Bi-LSTM take same feature vectors as input and produce output for every word in the sentence. We applied Max pooling on the first and attentive pooling on second Bi-LSTM layer to get optimal features from both the modules. Concatenation of both optimal features are used for classification through fully connected and softmax layers."}, {"heading": "3. DATASET DESCRIPTION", "text": "We obtain the dataset from the shared challenge SemEval2013 task-9 [25]. This dataset contains annotated sentences from two sources, Medline abstracts and DrugBank database. MedLine contains biomedical research articles and DrugBank contains documents written by medical practitioner. The dataset is annotated with following four kinds of interactions:\n3https://www.tensorflow.org\nAdvice: The text states an opinion or a consultation related to the simultaneous use of the two drugs, e.g.\u201calphablockers should not be combined with uroxatral\u201d.\nEffect : The sentence notes the effect of the drug-drug interaction together with pharmacodynamic effect or mechanism of interaction. For example \u201cWarfarin users who initiated fluoxetine had an increased risk of hospitalization for gastrointestinal bleeding\u201d.\nMechanism : The sentence describes a pharmacokinetic mechanism, as in \u201cParoxetine reduce the plasma concentration of endoxifen by about 20%\u201d .\nInt : The text mentions a drug interaction without providing any other information. For example, \u201cThis is typical of the interaction of meperidine and MAOIs.\u201d.\nDataset provides the training and test instances by sentences. If a sentence has more than two drug names, all possible pairs of drugs in the sentence have been separately annontated. This way single sentence having multiple drug names leads to separate instances of drug pairs and corresponding interaction. Statistics of the dataset is shown in Table 2. We use dataset in two separate tasks, namely, Task-A and Task-B. Task-A corresponds to detection of drug interaction in a given sentence for a given pair of drugs. For this, we ignore the true class of interaction type and keep them in positive class and no interaction instances are kept in negative class. However in Task-B, we have to decide the exact class of interaction (one of the four types) or no interaction."}, {"heading": "3.1 Pre-processing", "text": "The following pre-processing is done in the dataset before using it in our model:\n\u2022 Words are tokenized using genia tagger4 tool. All digits are normalized by replacing them with a special token DG and all letters are changed to lowercase.\n\u2022 The two targeted drug names are replaced with DRUGA and DRUG-B respectively, and other drug names in the same sentence are replaced with DRUG-N. Similar strategies were followed in earlier studies [23, 18] as well."}, {"heading": "3.2 Negative Instance Filtering", "text": "Since we consider all possible pairs of drug names in a sentence as separate instances for our model, the resultant dataset becomes very imbalance. We have 1:6.9 ratio on positive and negative instances. However, one can adopt some strategies to remove negative instances. Earlier studies\n4http://www.nactem.ac.uk/GENIA/tagger/\n[36, 16, 18] have shown positive impact of negative instance filtering. We filter negative samples based on the following rules:\n1. If both targeted drug mentions have the same name, remove the corresponding instance. Assumption behind this rule is drug doesn\u2019t interact with itself. We use string matching on both drug names to identify such cases.\n2. If one drug is a kind of or a special case of the other one in a given instance. To identify such cases, we use regular expression by observing patterns in the dataset. \u201cDRUG-A (DRUG-B)\u201d, \u201cDRUG-A such as DRUG-B\u201d are examples of such patterns.\n3. If the both target drugs appear in same coordinate structure. To filter such instances, we again use several regular expressions based on observing the patterns in training set. Examples of one such patterns is\u201cDRUG-A , (DRUG-N , )+DRUG-B\u201d.\nSimilar to [18] Our rules have not eliminated any positive instances from the test set. However, 144 positive instances (54 Mechanism, 65 Effects, 49 Int and 6 Advice) are removed from the training set. Table 2 summarizes statistics of dataset before and after filtering."}, {"heading": "4. EXPERIMENT DESIGN", "text": "We train and evaluate our proposed model separately for the two tasks. As mentioned earlier, if a sentence contains more than two drug names then all possible pairs with the sentence constitute separate instances/samples. We use the same evaluation scheme as used in the challenge [25]."}, {"heading": "4.1 Hyperparameters", "text": "In all our proposed models, we use pre-trained word embedding of 100 dimensions, distance embedding of dimension 10, keep size of hidden layers in B-LSTM and AB-LSTM as 200, and 150 for Joint AB-LSTM. Word embedding are obtained using GloVe tool [21] on a corpus of PubMed open source articles [31]. We use both l2 regularization and dropout [28] techniques for regularization. We apply dropout only on the output of the pooling layers. Different values of the regularization parameters are shown in the table 3."}, {"heading": "Task Models Dropout l2 regu.", "text": ""}, {"heading": "4.2 Baseline Methods for comparison", "text": "We compare performance of the three proposed models with several baseline methods that include approaches based on conventional features, kernel methods and on neural networks. Below we describe briefly about the baseline methods, where superscript one (\u22171) indicates one stage and superscript two (\u22172) indicates two stages methods:\n\u2022 Linear Methods: In this class of methods, a linear classifier is used to identify correct class of interaction for each instance. All instances are generally represented by a vector of manually designed features.\nUTurku1 used Turku event extraction system (TEES) [2] for drug interaction extraction. The major features used by TEES comes from dependency parsing and domain dependent resources such as MetaMap. UWMTRIADS2 [23] and Kim2 [16] are two stage methods. In both the stages they used SVM with contextual, lexical, semantic and tree structured features.\n\u2022 Kernel Methods: Kernel methods are powerful techniques for utilizing graph based features in any natural language processing task. WBI-DDI2 and FBK irst2 are two stage methods [7, 32] for DDI extraction. First stage of both models used different kernels methods for utilizing syntax tree and dependency tree features. In stage two WBI-DDI2 used TEES and FBK irst2 used SVM with non-linear kernel for classification of interaction types. NIL UCM1 used multi-class SVM as kernel methods in one stage framework.\n\u2022 Neural Network Methods: Neural network or deep learning methods use latent features in place of manually designed features. This class of algorithms use neural network for encoding word level features with sentence level features. Final classification happens with sentence level features. CNN1 [18] and SCNN1,2 [36] used convolution neural network with max pooling layer to learn higher level discriminative features over entire sentence. SCNN also utilized PoS tags and dependency tree based features apart from latent word embedding and distance embedding features. MVRNN1 [29] is also a neural network base model they used recursive neural network [27] for learning embedding of sentence or part of sentence recursively and final vector will use for classification."}, {"heading": "5. RESULTS AND DISCUSSIONS", "text": ""}, {"heading": "5.1 Effect of Negative Instance Filtering", "text": "Following our filtering rules, a large amount of negative instances along with few positive instances are removed from the dataset. However none of the positive instances from test set are removed. Table 4 shows performance of different models on test set while training is done using either complete or filtered dataset. Although all three models gave improved performance in terms of F1 score for both the tasks, the relative improvement is higher for the task B than for the task A. This is quite expected. Performance improvement can be attributed to two factors: cleaner data and reduction of imbalance. Both these factors are result of the data filtering step. Now, for the task A, relative change in imbalance (ratio changed from 1:5.9 to 1:3.3) ratio is insignificant in comparison to the change in imbalance ratio\nfor each of the interaction class for the task B. For example, imbalance ratio changed from 1:32.6 to 1:19.1 for the interaction class Advice and from 1:146 to 1:116.8 for the interaction class Int. To check how these changes improved the performance, we compare confusion matrices (Tables 5 and 6) obtained before and after filtering steps by the best performing model Joint AB-LSTM. We observe that there were 242 instances, belonging to one of the four interaction class, incorrectly classified into negative class before the filtering step. But after filtering step only 218 such instances were misclassified into negative class. This improvement is likely due to noisy examples, belonging to negative instances and lying near the decision boundaries, being filtered out and thus making easier for classifiers to find better decision boundaries. However filtering step is only likely to effect the boundaries between interaction class and the negative class and very little, if any, direct effect on boundaries among the interaction classes.\nType Neg Adv Mech. Effect Int Negation 4562 39 39 83 14 Advice 47 166 0 6 2 Mech. 82 8 205 7 0 Effect 94 12 6 247 1\nInt 19 0 3 37 37\nTable 5: Confusion Matrix of the results of Joint ABLSTM while dataset was not filtered with negative instance\nType Neg. Adv Mech. Effect Int Negation 2878 48 41 73 6 Advice 28 191 2 0 0 Mech. 69 9 216 8 0 Effect 97 12 4 247 0\nInt 24 0 1 43 28\nTable 6: Confusion Matrix of the results of Joint ABLSTM while dataset was filtered with negative instance\nAdvice and Mechanism category benefited with the filtering step better than the other two categories. This could be due to the adverse consequence of removal of negative instance filtering. As we mentioned earlier total of 144 positive instances (54 Mechanism, 65 Effects, 49 Int and 6 Advice) were removed from the training data. Instances for Int class was already quite low (180) in the complete dataset. Deletion of another 49 instances gave model very few examples to learn about this class. Further few more instances of Int\nclass were incorrectly classified into the Effect class and thus adversely affecting the performance of model on this class.\nFurther we compare the relative improvement shown by the three models for both the tasks. The main motivation was to see if one model gets more benefit than the other due to filtering step. This would indicate which model is able to handle imbalance and noise better than others. B-LSTM has the least relative improvement for both tasks A and B, indicating that it is relatively more robust to handle imbalance. On the other hand Joint AB-LSTM had performance improvement of around 2% and 3% for task A and B respectively, thus indicating its performance is dependent on data imbalance. However, we can not generalize this observation that B-LSTM model is more robust than the Joint AB-LSTM for imbalance classification, unless further verified on other datasets."}, {"heading": "5.2 Comparison with Baseline Methods", "text": "Comparison on the complete dataset: First we compare the three proposed models with other existing models on the complete dataset, i.e., without using the filtering step. Results are shown in the table 8. Here, we have included only those methods which explicitly mentions about the similar steps and report corresponding results. B-LSTM and Joint AB-LSTM models outperformed all other models. This indicates the superior performance of the proposed models even in presence of noise and high imbalance.\nComparison on the filtered dataset: Table 7 provides a detail comparison of our models with previous approaches. We observe that the Joint AB-LSTM model gave performance similar to state-of-the-art performance obtained by FBK irst2 for the task A and achieves new state-of-theart performance for the task B. While FBK irst2 rely on\nmanually designed features and use kernel methods, Joint AB-LSTM depends on latent features and based on neural network framework. In comparison to the existing neural network models using latent features, all three proposed models gave at least similar or better results. Joint ABLSTM, the best model among the three proposed model, gave about 4.0% of relative improvement in F1 score when compared with the SCNN2, best performing neural network based models.\nFor task-B, Joint AB-LSTM model obtained the best F1 score of 71.48%, which is approximately 2.5% relative improvement over earlier state-of-the-art method CNN1. There is 6.7% of relative improvement in F1 score when comparison is performed with best performing method (Kim2 method) among feature based linear and kernel methods. In comparison to SCNN2 model, all three proposed models gave at least similar or better results for this task as well. SCNN2, a convolution neural network based model uses higher order grammatical features based on parts of speech and shortest dependency path apart from pre-trained word embedding and position embedding features. If we remove shortest dependency path features from SCNN2 model, its performance decreases to 63.8% from 68.6%. On the other hand, apart from Joint AB-LSTM, the other two models also gave relatively much better performance than SCNN2 with only using simple features.\nAmong the three proposed models, Joint AB-LSTM always performed better than the other two methods in both task-A and task-B. It gives an indication that combined features obtained by max and attentive pooling are better than the features obtained by just one of them.\nClass wise Performance Analysis: We compare class wise performance of the proposed models with existing models. We observe that the Joint AB-LSTM obtained the best performance for Advice and Mechanism classes, whereas FBK irst and CNN obtained the best performance for Int and Effect classes respectively. Even using macro-average F1 score, Joint AB-LSTM outperformed all other models. All models find it easier to detect Advice interaction types compared to the instances of the other three interaction types. Similarly all models find it most difficult to detect Int interaction types. While the worse performance on the Int class can be attributed to insufficient training data, the relatively better performance on Advice interaction class can\nbe attributed to the origin of data. One should recall that imbalance ratio for the Advice class was higher than the Effect and Mechanism. As most of the instances of Advice class are from DrugBank, where sentences are written by medical practitioners in relatively shorter sentences and in concise manner compared to sentences in Medline abstracts, all models could detect this class relatively easily. Effect interaction class was found to be the second most difficult class to detect by almost all models (8 out of the 10 models) compared here. Again, one should recall that the Effect class has the least imbalance ratio. We looked at the confusion matrix (Table 5 and 6) to get some clue for the best performing model Joint AB-LSTM. We observe that around 44.8% times model assign instances of Int class to Effect and thus adversely affecting precision of the Effect class."}, {"heading": "5.3 Feature Analysis", "text": "In order to validate the importance of each features we further analyzed the performance of Joint AB-LSTM model by removing features one by one. It can be observed through table 10 that, use of pre-trained word embedding is important feature for both task-A and task-B. In case of task-A, 2.5% and for task-B, 5.6% of relative decrements are observed if model uses random vectors for words instead of pre-trained word embedding. This clearly indicates the importance of word embedding features. On the other hand, removal of position features did not affect the performance (less than 1% relative change) of the model for the task A, but for\nthe task B, there was relative decrements of approximately 2.6%."}, {"heading": "Tasks Models Precision Recall F Score", "text": ""}, {"heading": "5.4 Error vs Sentence Length", "text": "Apart from imbalance issue, we try to find out whether there is any other factor which is adversely affecting performance of models. For this we look at average sentence lengths of correctly and incorrectly classified instances for both tasks and for all three proposed models. We observe that average sentence length for incorrectly classified instances are always high compared to the correctly classified sentences. For example, for task A, average sentence length of correctly (incorrectly) classified instances by Joint AB-LSTM and B-LSTM are 30(38.84) and 29.79(39.61) respectively. Similarly for the task B, average sentence length of correctly (incorrectly) classified instances by Joint ABLSTM and B-LSTM are 29.43(37.93) and 28.02(40.04) respectively. This clearly indicates that all models gets confused with increasing sentence length and hence a better strategy is required to deal with it. Considering limited context can be one way to deal with extra large sentences. One such strategy can be as follows: consideration of all words between two target drugs, and only fixed number of words before the first target drug and after the second target drug."}, {"heading": "5.5 Visual Analysis", "text": "In order to confirm that the model is able to learn attention weights based on importance of words, we visualize attention weights of some of the sentences after training Joint AB-LSTM. Figure 2 is the heat map of attention weights for 6 instances of test set. Here every line is a sentence with two targeted drug names replaced with special tokens DRUG-A and DRUG-B and darkness in Red color indicate heedfulness. Figure shows that our model can select impor-\ntant words based on the task, for example in the sentence DRUG-A may enhance the effects of DRUG-B , DRUG-N and other DRUG-N, model is able to assign high weights to may enhance the effects very well. Similarly, in the sentence DRUG-N and DRUG-A increase the effects of DRUG-B, our model assigning high weights to the words increase and effect."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "In this work we proposed three LSTM based models BLSTM, AB-LSTM and Joint AB-LSTM for DDI extraction task. All the three models use word and distance embedding as feature and learn higher level features representation through Bi-LSTM network. Two of our proposed models also utilized neural attention mechanism to get higher level features representation. To the best of our knowledge, it is the first study to use LSTM and attention mechanism for DDI extraction task. Performance of all three models are compared with existing methods on SemEval-2013 DDI extraction dataset. Joint AB-LSTM model achieve state-ofthe-art for both DDI detection and classification tasks. Performance of the other two models, B-LSTM and AB-LSTM, are also found to be competitive in both tasks. Analysis of the results indicates the following important points: imbalance and noise adversely affect all models, Advice interaction class is easiest to predict, and models are likely to make incorrect classification for large sentences."}, {"heading": "7. ADDITIONAL AUTHORS", "text": ""}, {"heading": "8. REFERENCES", "text": "[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine\ntranslation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\n[2] J. Bjo\u0308rne, S. Kaewphan, and T. Salakoski. Uturku: drug named entity recognition and drug-drug interaction extraction using svm classification and domain knowledge. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), pages 651\u2013659, 2013.\n[3] T. Bobic, J. Fluck, and M. Hofmann-Apitius. Scai: Extracting drug-drug interactions using a rich feature vector. Atlanta, Georgia, USA, page 675, 2013.\n[4] B. Bokharaeian and A. D\u0131az. Nil ucm: Extracting drug-drug interactions from text through combination of sequence and tree kernels. 2013.\n[5] A\u0300. Bravo, J. Pin\u0303ero, N. Queralt-Rosinach, M. Rautschka, and L. I. Furlong. Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research. BMC bioinformatics, 16(1):1, 2015.\n[6] R. Businaro. Why we need an efficient and careful pharmacovigilance. J Pharmacovigilance, 1(4):1000e110, 2013.\n[7] M. F. M. Chowdhury and A. Lavelli. Exploiting the scope of negations and heterogeneous features for relation extraction: A case study for drug-drug interaction extraction. In HLT-NAACL, pages 765\u2013771, 2013.\n[8] M. F. M. Chowdhury and A. Lavelli. Fbk-irst: a multi-phase kernel based approach for drug-drug\ninteraction detection and classification that exploits linguistic information. Atlanta, Georgia, USA, 351:53, 2013.\n[9] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493\u20132537, Nov. 2011.\n[10] A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\n[11] H. Gurulingappa, A. Mateen-Rajpu, and L. Toldo. Extraction of potential adverse drug events from medical case reports. Journal of biomedical semantics, 3(1):1, 2012.\n[12] H. Gurulingappa, L. Toldo, A. M. Rajput, J. A. Kors, A. Taweel, and Y. Tayrouz. Automatic detection of adverse events to predict drug label changes using text and data mining techniques. Pharmacoepidemiology and drug safety, 22(11):1189\u20131194, 2013.\n[13] N. D. Hailu, L. E. Hunter, and K. B. Cohen. Ucolorado som: extraction of drug-drug interactions from biomedical text using knowledge-rich and knowledge-poor features. Atlanta, Georgia, USA, page 684, 2013.\n[14] R. Harpaz, A. Callahan, S. Tamang, Y. Low, D. Odgers, S. Finlayson, K. Jung, P. LePendu, and N. H. Shah. Text mining for adverse drug events: the promise, challenges, and state of the art. Drug safety, 37(10):777\u2013790, 2014.\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735\u20131780, Nov. 1997.\n[16] S. Kim, H. Liu, L. Yeganova, and W. J. Wilbur. Extracting drug\u2013drug interactions from literature using a rich feature-based linear kernel approach. Journal of biomedical informatics, 55:23\u201330, 2015.\n[17] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[18] S. Liu, B. Tang, Q. Chen, and X. Wang. Drug-drug interaction extraction via convolutional neural networks. Computational and mathematical methods in medicine, 2016, 2016.\n[19] T. Mikolov, M. Karafia\u0301t, L. Burget, J. Cernocky\u0301, and S. Khudanpur. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045\u20131048, 2010.\n[20] R. Pascanu, T. Mikolov, and Y. Bengio. Understanding the exploding gradient problem. CoRR, abs/1211.5063, 2012.\n[21] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Proceedings of EMNLP, 2014.\n[22] L. Qian and G. Zhou. Tree kernel-based\nproteina\u0302A\u0306S\u0327protein interaction extraction from biomedical literature. Journal of Biomedical Informatics, 45(3):535 \u2013 543, 2012.\n[23] M. Rastegar-Mojarad, R. D. Boyce, and R. Prasad. Uwm-triads: classifying drug-drug interactions with two-stage svm and post-processing. In Proceedings of the 7th International Workshop on Semantic Evaluation, pages 667\u2013674, 2013.\n[24] B. Rink, S. Harabagiu, and K. Roberts. Automatic extraction of relations between medical concepts in clinical texts. Journal of the American Medical Informatics Association, 18(5):594\u2013600, 2011.\n[25] I. Segura Bedmar, P. Mart\u0301\u0131nez, and M. Herrero Zazo. Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013). Association for Computational Linguistics, 2013.\n[26] I. Segura Bedmar, P. Martinez, and D. Sa\u0301nchez Cisneros. The 1st ddiextraction-2011 challenge task: Extraction of drug-drug interactions from biomedical texts. 2011.\n[27] R. Socher, C. C.-Y. Lin., C. D. Manning, and A. Y. Ng. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In ICML, 2011.\n[28] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.\n[29] V. Sua\u0301rez-Paniagua and I. Segura-Bedmar. Extraction of drug-drug interactions by recursive matrix-vector spaces. In 6thInternational Workshop on Combinations of Intelligent Methods and Applications (CIMA 2016), page 65, 2016.\n[30] K. O. N. G. Sunil Kumar Sahu, Ashish Anand. Relation extraction from clinical texts using domain invariant convolutional neural network. In Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 206\u2013215, 2016.\n[31] M. TH, S. Sahu, and A. Anand. Evaluating distributed word representations for capturing semantics of biomedical concepts. In Proceedings of BioNLP 15, pages 158\u2013163, Beijing, China, July 2015. Association for Computational Linguistics.\n[32] P. Thomas, M. Neves, T. Rockta\u0308schel, and U. Leser. Wbi-ddi: drug-drug interaction extraction using majority voting. In Second Joint Conference on Lexical and Computational Semantics (* SEM), volume 2, pages 628\u2013635, 2013.\n[33] R. Xu and Q. Wang. Large-scale automatic extraction of side effects associated with targeted anticancer drugs from full-text oncological articles. Journal of biomedical informatics, 55:64\u201372, 2015.\n[34] M. Yang, M. Kiang, and W. Shang. Filtering big data from social media\u2013building an early warning system for adverse drug reactions. Journal of biomedical informatics, 54:230\u2013240, 2015.\n[35] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy. Hierarchical attention networks for document classification.\n[36] Z. Zhao, Z. Yang, L. Luo, H. Lin, and J. Wang. Drug drug interaction extraction from biomedical literature using syntax convolutional neural network. Bioinformatics, page btw486, 2016.\n[37] P. Zhou, W. Shi, J. Tian, Z. Qi, B. Li, H. Hao, and B. Xu. Attention-based bidirectional long short-term memory networks for relation classification. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 207\u2013212, Berlin, Germany, August 2016. Association for Computational\nLinguistics."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Uturku: drug named entity recognition and drug-drug interaction extraction using svm classification and domain knowledge", "author": ["J. Bj\u00f6rne", "S. Kaewphan", "T. Salakoski"], "venue": "In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Scai: Extracting drug-drug interactions using a rich feature vector", "author": ["T. Bobic", "J. Fluck", "M. Hofmann-Apitius"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "D\u0131az. Nil ucm: Extracting drug-drug interactions from text through combination of sequence and tree kernels", "author": ["A.B. Bokharaeian"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research", "author": ["\u00c0. Bravo", "J. Pi\u00f1ero", "N. Queralt-Rosinach", "M. Rautschka", "L.I. Furlong"], "venue": "BMC bioinformatics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Why we need an efficient and careful pharmacovigilance", "author": ["R. Businaro"], "venue": "J Pharmacovigilance,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Exploiting the scope of negations and heterogeneous features for relation extraction: A case study for drug-drug interaction extraction", "author": ["M.F.M. Chowdhury", "A. Lavelli"], "venue": "In HLT-NAACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Fbk-irst: a multi-phase kernel based approach for drug-drug  interaction detection and classification that exploits linguistic information", "author": ["M.F.M. Chowdhury", "A. Lavelli"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Extraction of potential adverse drug events from medical case reports", "author": ["H. Gurulingappa", "A. Mateen-Rajpu", "L. Toldo"], "venue": "Journal of biomedical semantics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Automatic detection of adverse events to predict drug label changes using text and data mining techniques", "author": ["H. Gurulingappa", "L. Toldo", "A.M. Rajput", "J.A. Kors", "A. Taweel", "Y. Tayrouz"], "venue": "Pharmacoepidemiology and drug safety,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Ucolorado som: extraction of drug-drug interactions from biomedical text using knowledge-rich and knowledge-poor features", "author": ["N.D. Hailu", "L.E. Hunter", "K.B. Cohen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Text mining for adverse drug events: the promise, challenges, and state of the art", "author": ["R. Harpaz", "A. Callahan", "S. Tamang", "Y. Low", "D. Odgers", "S. Finlayson", "K. Jung", "P. LePendu", "N.H. Shah"], "venue": "Drug safety,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Extracting drug\u2013drug interactions from literature using a rich feature-based linear kernel approach", "author": ["S. Kim", "H. Liu", "L. Yeganova", "W.J. Wilbur"], "venue": "Journal of biomedical informatics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Drug-drug interaction extraction via convolutional neural networks", "author": ["S. Liu", "B. Tang", "Q. Chen", "X. Wang"], "venue": "Computational and mathematical methods in medicine,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Understanding the exploding gradient problem", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "CoRR, abs/1211.5063,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Tree kernel-based protein\u00e2\u0102\u015eprotein interaction extraction from biomedical literature", "author": ["L. Qian", "G. Zhou"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Uwm-triads: classifying drug-drug interactions with two-stage svm and post-processing", "author": ["M. Rastegar-Mojarad", "R.D. Boyce", "R. Prasad"], "venue": "In Proceedings of the 7th International Workshop on Semantic Evaluation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Automatic extraction of relations between medical concepts in clinical texts", "author": ["B. Rink", "S. Harabagiu", "K. Roberts"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction", "author": ["I. Segura Bedmar", "P. Mart\u0301\u0131nez", "M. Herrero Zazo"], "venue": "Association for Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "The 1st ddiextraction-2011 challenge task: Extraction of drug-drug interactions from biomedical texts", "author": ["I. Segura Bedmar", "P. Martinez", "D. S\u00e1nchez Cisneros"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["R. Socher", "C.C.-Y. Lin", "C.D. Manning", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1929}, {"title": "Extraction of drug-drug interactions by recursive matrix-vector spaces", "author": ["V. Su\u00e1rez-Paniagua", "I. Segura-Bedmar"], "venue": "In 6thInternational Workshop on Combinations of Intelligent Methods and Applications (CIMA 2016),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Relation extraction from clinical texts using domain invariant convolutional neural network", "author": ["K.O.N.G. Sunil Kumar Sahu", "Ashish Anand"], "venue": "In Proceedings of the 15th Workshop on Biomedical Natural Language Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Evaluating distributed word representations for capturing semantics of biomedical concepts", "author": ["M. TH", "S. Sahu", "A. Anand"], "venue": "In Proceedings of BioNLP", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Wbi-ddi: drug-drug interaction extraction using majority voting", "author": ["P. Thomas", "M. Neves", "T. Rockt\u00e4schel", "U. Leser"], "venue": "In Second Joint Conference on Lexical and Computational Semantics (* SEM),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Large-scale automatic extraction of side effects associated with targeted anticancer drugs from full-text oncological articles", "author": ["R. Xu", "Q. Wang"], "venue": "Journal of biomedical informatics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Filtering big data from social media\u2013building an early warning system for adverse drug reactions", "author": ["M. Yang", "M. Kiang", "W. Shang"], "venue": "Journal of biomedical informatics,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Drug drug interaction extraction from biomedical literature using syntax convolutional neural network. Bioinformatics, page btw486, 2016", "author": ["Z. Zhao", "Z. Yang", "L. Luo", "H. Lin", "J. Wang"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Attention-based bidirectional long short-term memory networks for relation classification", "author": ["P. Zhou", "W. Shi", "J. Tian", "Z. Qi", "B. Li", "H. Hao", "B. Xu"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Adverse drug reaction (ADR) is an example of antagonistic effect causing immense health risks and sometimes even leading to death [6].", "startOffset": 130, "endOffset": 133}, {"referenceID": 24, "context": "Identifying DDIs in text is the process of recognizing how two drugs in a given sentence are related [25].", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "While SemEval 2011 task [26] was focused on DDI detection assuming that drugs were already recognized, SemEval 2013 task-9 [25] was de-", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "While SemEval 2011 task [26] was focused on DDI detection assuming that drugs were already recognized, SemEval 2013 task-9 [25] was de-", "startOffset": 123, "endOffset": 127}, {"referenceID": 2, "context": "In 1-stage methods [3, 13, 32], a multi-class classifier is used to map a sentence with two target drugs either into one of the interacting classes or into the negative class.", "startOffset": 19, "endOffset": 30}, {"referenceID": 12, "context": "In 1-stage methods [3, 13, 32], a multi-class classifier is used to map a sentence with two target drugs either into one of the interacting classes or into the negative class.", "startOffset": 19, "endOffset": 30}, {"referenceID": 31, "context": "In 1-stage methods [3, 13, 32], a multi-class classifier is used to map a sentence with two target drugs either into one of the interacting classes or into the negative class.", "startOffset": 19, "endOffset": 30}, {"referenceID": 22, "context": "On the other hand 2-stage methods [23, 2] breaks the problem into 2 steps.", "startOffset": 34, "endOffset": 41}, {"referenceID": 1, "context": "On the other hand 2-stage methods [23, 2] breaks the problem into 2 steps.", "startOffset": 34, "endOffset": 41}, {"referenceID": 7, "context": "In the first category mainly support vector machines (SVMs) with linear or non-linear kernels have been used in many studies [8, 4, 16].", "startOffset": 125, "endOffset": 135}, {"referenceID": 3, "context": "In the first category mainly support vector machines (SVMs) with linear or non-linear kernels have been used in many studies [8, 4, 16].", "startOffset": 125, "endOffset": 135}, {"referenceID": 15, "context": "In the first category mainly support vector machines (SVMs) with linear or non-linear kernels have been used in many studies [8, 4, 16].", "startOffset": 125, "endOffset": 135}, {"referenceID": 11, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 131, "endOffset": 147}, {"referenceID": 10, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 131, "endOffset": 147}, {"referenceID": 13, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 131, "endOffset": 147}, {"referenceID": 32, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 131, "endOffset": 147}, {"referenceID": 33, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 183, "endOffset": 187}, {"referenceID": 21, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 259, "endOffset": 270}, {"referenceID": 4, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 259, "endOffset": 270}, {"referenceID": 23, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 259, "endOffset": 270}, {"referenceID": 17, "context": "Some notable studies [18, 36] for the DDT extraction tasks are based on convolution neural networks (CNNs) and have been shown to achieving superior performance than the existing state-of-the-art methods.", "startOffset": 21, "endOffset": 29}, {"referenceID": 34, "context": "Some notable studies [18, 36] for the DDT extraction tasks are based on convolution neural networks (CNNs) and have been shown to achieving superior performance than the existing state-of-the-art methods.", "startOffset": 21, "endOffset": 29}, {"referenceID": 17, "context": "As opposed to works in [18, 36], we use LSTM based neural network models [15] instead of CNN.", "startOffset": 23, "endOffset": 31}, {"referenceID": 34, "context": "As opposed to works in [18, 36], we use LSTM based neural network models [15] instead of CNN.", "startOffset": 23, "endOffset": 31}, {"referenceID": 14, "context": "As opposed to works in [18, 36], we use LSTM based neural network models [15] instead of CNN.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "Theoretically a Bi-LSTM can preserve information about past and future words while reading [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "P1 is distance (in terms of words) from the first drug name [9, 30].", "startOffset": 60, "endOffset": 67}, {"referenceID": 29, "context": "P1 is distance (in terms of words) from the first drug name [9, 30].", "startOffset": 60, "endOffset": 67}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "Recurrent neural network is a powerful model for modeling sequential data [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "However because of long sequence it may suffer with vanishing or exploding gradient problems [10, 20].", "startOffset": 93, "endOffset": 101}, {"referenceID": 19, "context": "However because of long sequence it may suffer with vanishing or exploding gradient problems [10, 20].", "startOffset": 93, "endOffset": 101}, {"referenceID": 0, "context": "Weights of the feature vectors are computed through attention mechanism which assign weights based on importance of that features [1, 35, 37].", "startOffset": 130, "endOffset": 141}, {"referenceID": 35, "context": "Weights of the feature vectors are computed through attention mechanism which assign weights based on importance of that features [1, 35, 37].", "startOffset": 130, "endOffset": 141}, {"referenceID": 16, "context": "Adam\u2019s technique [17] is used for optimization.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "B-LSTM is similar to the model proposed in [18] for DDI extraction task.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "Here we use Bi-LSTM in place of convolution neural network used in [18].", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "We obtain the dataset from the shared challenge SemEval2013 task-9 [25].", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "Similar strategies were followed in earlier studies [23, 18] as well.", "startOffset": 52, "endOffset": 60}, {"referenceID": 17, "context": "Similar strategies were followed in earlier studies [23, 18] as well.", "startOffset": 52, "endOffset": 60}, {"referenceID": 34, "context": "uk/GENIA/tagger/ [36, 16, 18] have shown positive impact of negative instance filtering.", "startOffset": 17, "endOffset": 29}, {"referenceID": 15, "context": "uk/GENIA/tagger/ [36, 16, 18] have shown positive impact of negative instance filtering.", "startOffset": 17, "endOffset": 29}, {"referenceID": 17, "context": "uk/GENIA/tagger/ [36, 16, 18] have shown positive impact of negative instance filtering.", "startOffset": 17, "endOffset": 29}, {"referenceID": 17, "context": "Similar to [18] Our rules have not eliminated any positive instances from the test set.", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "We use the same evaluation scheme as used in the challenge [25].", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "Word embedding are obtained using GloVe tool [21] on a corpus of PubMed open source articles [31].", "startOffset": 45, "endOffset": 49}, {"referenceID": 30, "context": "Word embedding are obtained using GloVe tool [21] on a corpus of PubMed open source articles [31].", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "We use both l2 regularization and dropout [28] techniques for regularization.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "UTurku used Turku event extraction system (TEES) [2] for drug interaction extraction.", "startOffset": 49, "endOffset": 52}, {"referenceID": 22, "context": "UWMTRIADS [23] and Kim [16] are two stage methods.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "UWMTRIADS [23] and Kim [16] are two stage methods.", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "WBI-DDI and FBK irst are two stage methods [7, 32] for DDI extraction.", "startOffset": 43, "endOffset": 50}, {"referenceID": 31, "context": "WBI-DDI and FBK irst are two stage methods [7, 32] for DDI extraction.", "startOffset": 43, "endOffset": 50}, {"referenceID": 17, "context": "CNN [18] and SCNN [36] used convolution neural network with max pooling layer to learn higher level discriminative features over entire sentence.", "startOffset": 4, "endOffset": 8}, {"referenceID": 34, "context": "CNN [18] and SCNN [36] used convolution neural network with max pooling layer to learn higher level discriminative features over entire sentence.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "MVRNN [29] is also a neural network base model they used recursive neural network [27] for learning embedding of sentence or part of sentence recursively and final vector will use for classification.", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "MVRNN [29] is also a neural network base model they used recursive neural network [27] for learning embedding of sentence or part of sentence recursively and final vector will use for classification.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "Linear Methods UTurku [2] 85.", "startOffset": 22, "endOffset": 25}, {"referenceID": 22, "context": "4 UWM-TRIADS [23] 43.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "0 Kim [16] 77.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "Kernel Methods NIL UCM [4] 60.", "startOffset": 23, "endOffset": 26}, {"referenceID": 31, "context": "7 WBI-DDI [32] 80.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "9 FBK irst [8] 79.", "startOffset": 11, "endOffset": 14}, {"referenceID": 17, "context": "Neural Network CNN [18] 75.", "startOffset": 19, "endOffset": 23}, {"referenceID": 34, "context": "75 SCNN [36] 74.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "0 SCNN [36] 77.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "6 MV-RNN [29] 52.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Models Precision Recall F1 Score CNN [18] 75.", "startOffset": 37, "endOffset": 41}, {"referenceID": 34, "context": "01 SCNN [36] 68.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "UTurku [2] 63.", "startOffset": 7, "endOffset": 10}, {"referenceID": 22, "context": "UWM-TRIADS[23] 53.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "Kim [16] 72.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "FBK irst [8] 69.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "NIL UCM [4] 61.", "startOffset": 8, "endOffset": 11}, {"referenceID": 31, "context": "WBI-DDI[32] 63.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "CNN [18] 77.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "91 MV-RNN [29] 57.", "startOffset": 10, "endOffset": 14}], "year": 2017, "abstractText": "A drug can affect the activity of other drugs, when administered together, in both synergistic or antagonistic ways. In one hand synergistic effects lead to improved therapeutic outcomes, antagonistic consequences can be life-threatening, leading to increased healthcare cost, or may even cause death. Thus, identification of unknown drug-drug interaction (DDI) is an important concern for efficient and effective healthcare. Although there exist multiple resources for DDI, they often unable to keep pace with rich amount of information available in fast growing biomedical texts including literature. Most existing methods model DDI extraction from text as classification problem and mainly rely on handcrafted features. Some of these features further depends on domain specific tools. Recently neural network models using latent features has shown to be perform similar or better than the other existing models using handcrafted features. In this paper, we present three models namely, B-LSTM, ABLSTM and Joint AB-LSTM based on long short-term memory (LSTM) network. All three models utilize word and position embedding as latent features and thus do not rely on feature engineering. Further use of bidirectional long short-term memory (Bi-LSTM) networks allow to extract optimal features from the whole sentence. The two models, AB-LSTM and Joint AB-LSTM also use attentive pooling in the output of Bi-LSTM layer to assign weights to features. Our experimental results on the SemEval-2013 DDI extraction dataset shows that the Joint AB-LSTM model outperforms all the existing methods, including those relying on handcrafted features. The other two proposed models also perform competitively with state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}