{"id": "1701.08511", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Binary adaptive embeddings from order statistics of random projections", "abstract": "babayev We use lipica some salvadora of carton the carfagna largest order statistics khula of the vegetables random substitutes projections of diageo a 6-for-7 reference 1,536 signal to jablonski construct glaister a binary embedding that is adapted to samra signals correlated with such signal. The 3,070 embedding capaci is noncommital characterized from 75,500 the analytical standpoint and shown to zurek provide dunay improved sina performance kurka on tasks such fishmonger as ferromagnetism classification juli in panoramic a guddu reduced - dimensionality bcw space.", "histories": [["v1", "Mon, 30 Jan 2017 08:37:25 GMT  (358kb,D)", "http://arxiv.org/abs/1701.08511v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["diego valsesia", "enrico magli"], "accepted": false, "id": "1701.08511"}, "pdf": {"name": "1701.08511.pdf", "metadata": {"source": "CRF", "title": "Binary adaptive embeddings from order statistics of random projections", "authors": ["Diego Valsesia", "Enrico Magli"], "emails": [], "sections": [{"heading": null, "text": "Keywords\u2014Binary Embeddings, Random projections\nI. INTRODUCTION The ever-increasing amount of information that is produced in the age of Big Data calls for efficient techniques for storage and processing of a large number of high-dimensional signals. Compact representations can be obtained with different methods depending whether the particular task requires signal reconstruction (e.g., image and video compression for delivery and visualization) or the goal is to infer some information from the signals (e.g., in classification, regression, information retrieval, etc.). Embeddings provide compact representations of signals for the latter tasks. Formally, an embedding is a transformation that maps a set of signals in a high dimensional space to a lower dimensional space, in such a way that the geometry of the set is approximately preserved. The concept of embedding has been successfully used in the context of information retrieval [1], where it is usually called \u201chashing\u201d. An important class of signal embeddings are those preserving the distances among pairs of signals. Johnson and Lindenstrauss [2] famously stated that an embedding can be realized with a Lipschitz mapping to approximately preserve Euclidean distances with a dimension of the embedding space that only depends on the desired distortion and logarithmically in the number of signals to be embedded. Random projections have been shown to implement such embedding with high probability. Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7]. Recently, some works have studied learning embeddings [8]\u2013[10] from training data to derive compact codes by exploiting the particular geometry of the dataset (e.g., signals living close to a manifold).\nIn this paper we propose an approach to construct an embedding that is not based on learning and does not require a training set of data, but rather is adapted to a single reference signal. This choice maintains to some degree the universality of random projections and it is useful when the data present no particular structure. Jegou et al. [11] empirically explored\nThe authors are with Politecnico di Torino, Torino, Italy.This work was supported by the European Research Council through the European Community Seventh Framework Programme (FP7/2007-2013) under Grant 279848.\na similar idea by proposing to choose hash functions with a robustness criterion, that essentially measures how far a random projection falls from the edges of a quantization interval. Our work presents a rigorous analytical treatment of a binary embedding obtained from the selection of the random projections of a reference signal with largest magnitude. The analysis of the embedding provides insights on its advantages, particularly in mitigating the difficulty of low-contrast nearest neighbor problems and superior performance on classification tasks, e.g. in a neural network."}, {"heading": "II. PROPOSED METHOD", "text": ""}, {"heading": "A. Preliminaries", "text": "Definition 1. A mapping \u03c6 : X \u2192 Y of metric spaces, endowed with distances dX and dY is called an embedding with distortion C > 0 if LdX (u,v) \u2264 dY(\u03c6(u), \u03c6(v)) \u2264 CLdX (u,v) for some constant L > 0 and for all u,v \u2208 X .\nA well-known binary embedding is the sign random projections [3] where a random matrix \u03a6 made of independent and identically distributed (i.i.d.) Gaussian entries is used to compute some random projections, which are then quantized to a binary representation by keeping their sign. The Hamming distance between the binary vectors approximately preserves the angle between the signals in the original space [3], i.e., P (sign(\u03a6iu) = sign(\u03a6iv)) = 1 \u2212 \u03b8\u03c0 , being \u03b8 = cos\u22121 ( uTv \u2016u\u2016\u2016v\u2016 ) and \u03a6i the i-th row of \u03a6."}, {"heading": "B. Proposed adaptive embedding", "text": "A reference signal u is used to generate the adaptive embedding in the following way. A number mpool of random projections is computed by means of an i.i.d. Gaussian matrix \u03a6 \u2208 Rmpool\u00d7n as y = \u03a6u. The m entries with largest magnitude are identified and their locations stored in vector l. The m-bit resulting binary code is:\np = sign(\u03a6lu) , (1)\nwhere \u03a6l is the matrix \u03a6 restricted to the rows indexed by l. The locations vector l is saved as side information of the embedding and used as in (1) whenever a new signal is to be embedded, i.e., q = sign(\u03a6lv), where v is a generic signal and q its embedding.\nThe first theorem we prove confirms that the Hamming distance dH(p,q) = 1m \u2211 i pi\u2295qi, i.e. the number of differing entries, between binary codes obtained with the adaptive embedding concentrates around its expected value.\nTheorem 2. Let X \u2282 Rn be a set of N signals and u,v \u2208 X . Let \u03a6 \u2208 Rmpool\u00d7n with \u03a6i,j \u223c N (0, \u03c32), y = \u03a6u and the locations l of the m \u2264 mpool entries in y with largest\nar X\niv :1\n70 1.\n08 51\n1v 1\n[ cs\n.L G\n] 3\n0 Ja\nn 20\n17\n2 magnitude be known. Let p = sign(\u03a6lu) and q = sign(\u03a6lv). Then for 0 < \u03b5 \u2264 2e\u2212 1,\nP (|dH(p,q)\u2212 \u00b5| > \u00b5) < e\u2212\u00b5 \u03b52 2 + e\u2212\u00b5 \u03b52 4 (2)\nwith \u00b5 = E [dH(p,q)] = 1\nm m\u2211 i=1 pi\npi = 1\n2 +\n1 2 erf\n( \u2212yli\nuTv\u221a 2\u03c3\u2016u\u2016 \u221a \u2016u\u20162\u2016v\u20162 \u2212 (uTv)2 ) Proof: Let us consider a single measurement in the li location yi = \u03a6liu and zi = \u03a6liv. Then \u03b6 = [yi, zi] is a bivariate Gaussian with zero mean and covariance \u03a3 = \u03c32 [ \u2016u\u20162 uTv uTv \u2016v\u20162. ] . Suppose that yi is observed to be yi = \u03c4i, then the conditional distribution of zi given yi = \u03c4i is \u03b7\u03c4i = (zi|yi = \u03c4i) \u223c N ( \u03c4i uTv \u2016u\u20162 , \u03c3 2 ( \u2016v\u20162 \u2212 (u Tv)2 \u2016u\u20162 )) .\nAfter quantization of the measurements, the probability of mismatching bits in position li is pi = P (\u03b7\u03c4 \u2264 0|\u03c4i > 0)\n= 1\n2 +\n1 2 erf\n( \u2212\u03c4i\nuTv\u221a 2\u03c3\u2016u\u2016 \u221a \u2016u\u20162\u2016v\u20162 \u2212 (uTv)2 ) Define the following random variable\nEi = { 0 with probability 1\u2212 pi 1 with probability pi . (3)\nThen, DH = 1m \u2211m i=1Ei is a Poisson Binomial random variable measuring the Hamming distance between p and q. Eq. (2) can be readily obtained using Chernoff bounds [12] for the tails of DH .\nThe previous theorem holds for a fixed pair of signals. It is customary to derive an asymptotic result on the number of measurements needed to provide a distortion \u03b4 around the expectation when signals are drawn from a finite set of cardinality N . Standard derivation using a union bound on Eq. (2) yields m = O(\u03b4\u22122 logN), which is exactly the same as classic results on non-adaptive random projections [2]. The advantages of the proposed method are, in fact, due to the modified expected value rather than the variance.\nMoreover, the previous theorem supposed we knew the values of the projections of the reference signal at the locations kept as side information. This allows us to compute the exact expected value of the Hamming distance in the embedded space as function of the inner product (or correlation coefficient) between the original signals. However, it might be useful to have some a-priori knowledge about the embedding without the need to know the reference signal. The following theorem approximately bounds the expected value of the embedding by characterizing the order statistics of |y|. The k-th order statistic of a statistical sample is equal to its kth-smallest value.\nLet us call fk(\u03c4) the probability density function of the k-th order statistic of |y|. We could then in principle compute the probability of bit mismatch as\npi = \u222b P (\u03b7\u03c4 \u2264 0|\u03c4) fmpool\u2212i+1(\u03c4)d\u03c4 for i = 1, 2, . . . ,m\nand then repeat the same Poisson binomial argument as before. However, this is cumbersome to compute and we instead derive some bounds. Theorem 3. Under the same assumptions as Theorem 1, and being e2(mpool\u2212m+1);2mpool the expected value of the 2(mpool \u2212 m+ 1)-th order statistic of a sample of N (0, \u03c32\u2016u\u20162) of size 2mpool:\nE [dH(p,q)] \u2264 1\n2 +\n1 2 erf\n( \u2212e2(mpool\u2212m+1);2mpooluTv\u221a\n2\u03c3\u2016u\u2016 \u221a \u2016u\u20162\u2016v\u20162 \u2212 (uTv)2 ) Proof: We first notice that pi \u2264 pm, \u2200i = 1, . . . ,m\nso that E [dH(p,q)] = 1m \u2211m i=1 pi \u2264 pm. Then, pm = E\u03c4 [g(\u03c4)] \u2264 g(E[\u03c4 ]) by applying Jensen\u2019s inequality to g(\u03c4) = P (\u03b7\u03c4 \u2264 0|\u03c4), i.e., the same Gaussian tail probability as before. Also notice that the convexity of g allows us to use Jensen\u2019s inequality. E[\u03c4 ] = e\u0303(mpool\u2212m+1);mpool is the expected value of the (mpool \u2212 m + 1)-th order statistic of a sample of size mpool from a half Gaussian (since we consider |y|). We then notice that that is equivalent [13] to e2(mpool\u2212m+1);2mpool , i.e., the 2(mpool\u2212m+1)-th order statistic of a sample of size 2mpool of a full Gaussian with zero mean and \u03c32\u2016u\u20162 variance.\nAs a further remark, according to [14] an approximation of the expected value of the desired order statistic is:\ne2(mpool\u2212m+1);2mpool \u2248 F \u22121 (\n2(mpool \u2212m+ 1)\u2212 \u03b1 2mpool \u2212 2\u03b1+ 1 ) being \u03b1 = 0.375 and F\u22121 the inverse CDF of a normal distribution with zero mean and variance \u03c32\u2016u\u20162.\nSo far we considered distances between a test signal and the reference used to adapt the embedding. We will now consider what happens to the distance between any arbitrary pair of signals v and w. Qualitatively, we can say that the curve of the expected value of the Hamming distance in the embedded space as function of the original distance between v and w will be somewhere between the one predicted by Theorem 2 and the one of non-adaptive sign random projections depending on how much w is close to the reference u. The following theorem formalizes this concept.\nTheorem 4. Let X \u2282 Rn be a set of N signals and u,v,w \u2208 X . Let \u03a6 \u2208 Rmpool\u00d7n with \u03a6i,j \u223c N (0, \u03c32), y = \u03a6u and the locations l of the m \u2264 mpool entries in y with largest magnitude be known. Let p = sign(\u03a6lu), q = sign(\u03a6lv), and r = sign(\u03a6lw). Then for 0 < \u03b5 \u2264 2e\u2212 1,\nP (|dH(q, r)\u2212 \u00b5| > \u00b5) < e\u2212\u00b5 \u03b52 2 + e\u2212\u00b5 \u03b52 4 (4)\nwith \u00b5 = E [dH(q, r)] = 1\nm m\u2211 i=1 pi\npi =\n[ F ([ 0\n+\u221e\n]) \u2212 F ([ 0 0 ])] [ 1\u2212 F ([ +\u221e 0 ])] + [ F ([ +\u221e\n0\n]) \u2212 F ([ 0 0 ])] F ([ +\u221e 0 ]) ,\nbeing F the CDF of a bivariate Gaussian\nwith mean \u00b5\u2032 = yli\u2016u\u20162 [ uTv uTw ] and covariance\n3 \u03a3\u2032 = \u03c32 [ \u2016v\u20162 \u2212 (u Tv)2 \u2016u\u20162 v Tw \u2212 (u Tv)(uTw) \u2016u\u20162\nvTw \u2212 (u Tv)(uTw) \u2016u\u20162 \u2016w\u2016 2 \u2212 (u Tw)2 \u2016u\u20162\n] .\nProof: The proof is similar to the proof of Theorem 2. Let us consider a single measurement in the li location yi = \u03a6liu, zi = \u03a6liv, ai = \u03a6liw. Then \u03b6 = [zi, ai, yi] is Gaussian with\nzero mean and covariance \u03a3 = \u03c32 \u2016u\u20162 uTv uTwuTv \u2016v\u20162 vTw uTw vTw \u2016w\u20162. . Suppose that yi is observed to be yi = \u03c4i, then the conditional distribution of [zi, ai] given yi = \u03c4i is \u03b7 = ([zi, ai]|yi = \u03c4i) \u223c N (\u00b5\u2032,\u03a3\u2032) with \u00b5\u2032 = yli \u2016u\u20162 [ uTv uTw ] and covariance \u03a3\u2032 =\n\u03c32\n[ \u2016v\u20162 \u2212 (u Tv)2\n\u2016u\u20162 v Tw \u2212 (u Tv)(uTw) \u2016u\u20162\nvTw \u2212 (u Tv)(uTw) \u2016u\u20162 \u2016w\u2016 2 \u2212 (u Tw)2 \u2016u\u20162\n] .\nAfter quantization of the measurements, the probability of mismatching bits in position li is pi = P (\u03b71 < 0|\u03b72 > 0)P (\u03b72 > 0) + P (\u03b71 > 0|\u03b72 < 0)P (\u03b72 < 0)\nDefine the random variable Ei as in (3), then DH = 1 m \u2211m i=1Ei is a Poisson Binomial random variable measuring the Hamming distance between q and r. Eq. (4) can be readily obtained using Chernoff bounds [12] for the tails of DH .\nThe key distinction between sign random projections [3] and the method presented in this paper is that the former provides a linear relationship between the Hamming distance in the embedded space and the angle in the original space. On the other hand, the binary adaptive embedding provides a nonlinear relationship between the two distances, with the important property that the Hamming distances observed with the adaptive embedding are always smaller than those observed with sign random projections. This property is at the core of the improved performance of the embedding for tasks such as binary classification, as discussed in Sec. III. Fig. 1 shows the Hamming distance in the embedded space as function of the inner product between the test signal and the reference signal in the original space. It can be noticed how the curve of the adaptive embedding always lies below the one for the nonadaptive embedding. For a fixed value of mpool increasing the umber of measurements m will reduce the variance and move the expected value towards that of the non-adaptive embedding. Viceversa, for a fixed m increasing mpool will lower the curve. Fig. 2 shows the Hamming distance between the first test signal and the second test signal in the embedded space as function of the inner product between the first test signal and the second test signal (vTw) and between the second test signal and the reference signal (uTw) in the original space. Notice how for decreasing uTw the shape of the embedding tends to the one of a non-adaptive embedding."}, {"heading": "III. APPLICATIONS AND EXPERIMENTS", "text": ""}, {"heading": "A. Low-contrast approximate nearest neighbors", "text": "A measure of difficulty [15], [16] of a nearest neighbor search problem is the contrast r, which is defined as the ratio between the distance of the closest false neighbor and that of the farthest true neighbor. Locality sensitive hashing [17] is\nFig. 1. Hamming distance in the embedding a function of inner product between reference and test signals. Unit norm signals, m = 800, mpool = 5000. Non-adaptive curve uses sign random projections [3].\nFig. 2. Hamming distance in the embedding a function of inner product between reference and test signals and between both test signals. Unit norm signals, m = 800, mpool = 5000.\nable to find approximate nearest neighbors in a time O(Nr) that is sublinear in the database size N but that degenerates to linear search as the contrast approaches 1. The curse of dimensionality makes low contrast more probable when highdimensional spaces are considered. The adaptive embedding presented in this paper can be used as a dimensionality reduction technique to solve approximate nearest neighbor search more efficiently in low-contrast scenarios.\nIn order to show this we develop an experiment in a highdimensional space where the goal is to find the nearest neighbors of a given signal within a certain radius. True neighbors are generated as standard Gaussian vectors with n = 8192 i.i.d. entries with an expected correlation coefficient equal to 0.07 to a reference that is also used as query. Disturbing signals are i.i.d. Gaussian with zero expected correlation. Notice that they are almost orthogonal to each other but the contrast is low because the true neighbours are weakly correlated with our query. This problem is not unrealistic and, in fact, as an example, it occurs in the detection of photoresponse non-uniformity artifacts from camera sensors [18] [19], used to attribute a given picture to a given camera sensor. In this experiment all the signals in the database are adaptively embedded, i.e., the locations of the m entries of largest magnitude are identified and stored with the binary code. The random projections of the query are computed and\n4\nappropriately subsampled according to the locations stored for each database signal under test. The storage requirement for each database entry is the sum of the bits needed by the binary measurements and the overhead due to the adaptively chosen locations and it amounts to m + log2 [( mpool m )] bits. Fig. 3 shows the Receiver Operating Characteristic (ROC) showing the probability of detection of a true neighbor against the probability of false alarm. We notice that the adaptive embedding provides a performance closer to the uncompressed case. Two non-adaptive strategies are presented for a fair comparison. A non-adaptive method using the same storage as the adaptive method would use m\u2032 = m+log2 [( mpool m )] binary random projections, with the drawback of increased computational complexity in the Hamming distance evaluation. The second strategy equalizes computational complexity, thus using m\u2032 = m non-adaptive measurements. This is advantageous in terms of storage but it performs significantly worse. Finally, we compared the proposed method with the Universal Embedding of Boufounos et al. [5] which is a kind of adaptive embedding where the quantizer can be parametrized in order to distort the expected value of signal distances, similarly to the embedding proposed in this paper. The Universal Embedding bounds the maximum distance that is embedded, beyond which points become indistinguishable. It is therefore expected to have poor performance in low-contrast, low-correlation scenarios, as it appears from Fig. 3., where the quantization step is \u2206 = 2."}, {"heading": "B. Multiclass linear classifiers", "text": "In this section we apply the adaptive embedding to a multiclass linear classifier in order to improve its storage and computational efficiency. Linear classifiers are widely used in the context of deep neural networks, where the layers of the network are trained to disentangle the features of each class and a simple linear classification layer provides the class labels. A k-class linear classifier can be written as l = arg maxi={1,...,k}wix, being l the class label, wi the weights vectors and x a feature vector. The weights are learned during the training phase using a suitable loss function such as the hinge loss [20] for support vector machines or the softmax cross-entropy [21] for multinomial logistic regression which is more popular in deep neural networks. Since the feature vectors may be high dimensional and the number of classes\nlarge, this operation may require significant storage space for the real-valued weights as well as computational resources to compute all the inner products. This can be overcome using an embedding such as sign random projections. After the training phase is completed, the weights are embedded in a compact binary code for each class. During predictions the feature vectors are also embedded, the Hamming distance with the weights is computed and the class label corresponding to the minimum distance is selected, i.e. l = arg mini={1,...,k} dH(\u03c9i,y), being \u03c9i = sign (\u03a6w) and y = sign (\u03a6x). Replacing sign random projections with the proposed adaptive embedding can improve the classification performance of the compressed system. Overall, there are as many adaptive embeddings as the number of classes. The random projections of each wi are used to compute a different set of locations li for each class. At test time, the feature vector is embedded k times to generate yi (this amounts to generating mpool non-adaptive projections and then subsampling according to the corresponding li). Hence, the class label is given by l = arg mini={1,...,k} dH(\u03c9i,yi), being \u03c9i = sign (\u03a6liw) and yi = sign (\u03a6lix).\nThe following experiment is a classification problem on the CIFAR-10 dataset [22] comprising 10 classes. We implemented the same convolutional neural network architecture presented in [23]. This network is composed of 8 convolutional layers followed by 2 fully connected layers all with ReLU activation units [24] and a final linear layer. The last linear layer outputs one of the k = 10 class labels from a n = 1024- dimensional input feature vector. After conventional training of the network, we replaced the layer weights with its embedded codes as explained above. For the adaptive method we used mpool = 1024. Table I shows the classification accuracy as function of the number of measurements used by the embedding. It can be noticed that the adaptive embedding allows to achieve a significant dimensionality reduction at a negligible loss in terms of classification accuracy, with respect to both sign random projections [3] and the universal embedding [5]. The quantization step size of the universal embedding has been optimized via cross validation to value \u2206 = 180."}, {"heading": "IV. CONCLUSIONS", "text": "This paper presented a technique to generate compact binary codes from high-dimensional signals adapting them to a reference signal. The resulting embedding displays interesting properties that allow to improve performance in classification tasks when those are performed in the reduced-dimensionality domain. Future work will focus on generalizing the approach to sub-Gaussian and structured sensing matrices.\n5"}], "references": [{"title": "Database-friendly random projections: Johnson- Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of computer and System Sciences, vol. 66, no. 4, pp. 671\u2013687, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary Mathematics, vol. 26, 1984.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1984}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "Proceedings of the Thiry-fourth Annual ACM Symposium on Theory of Computing, ser. STOC \u201902. New York, NY, USA: ACM, 2002, pp. 380\u2013388. [Online]. Available: http://doi.acm.org/10.1145/509907.509965", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust 1bit compressive sensing via binary stable embeddings of sparse vectors", "author": ["L. Jacques", "J.N. Laska", "P.T. Boufounos", "R.G. Baraniuk"], "venue": "IEEE Trans. Inf. Theory, vol. 59, no. 4, pp. 2082\u20132102, April 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient coding of signal distances using universal quantized embeddings", "author": ["P.T. Boufounos", "S. Rane"], "venue": "Data Compression Conference (DCC), 2013, March 2013, pp. 251\u2013260.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "On the resemblance and containment of documents", "author": ["A. Broder"], "venue": "Proceedings of the Compression and Complexity of Sequences 1997. Washington, DC, USA: IEEE Computer Society, 1997, pp. 21\u201329. [Online]. Available: http://dl.acm.org/citation.cfm?id=829502.830043", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Sparse- Hash: Embedding Jaccard Coefficient between Supports of Signals", "author": ["D. Valsesia", "S. Fosson", "C. Ravazzi", "T. Bianchi", "E. Magli"], "venue": "Proc. of MM-SPARSE Workshop at ICME 2016, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Hamming distance metric learning", "author": ["M. Norouzi", "D.J. Fleet", "R.R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2012, pp. 1061\u20131069.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems 21, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, Eds. Curran Associates, Inc., 2009, pp. 1753\u20131760. [Online]. Available: http://papers.nips.cc/ paper/3383-spectral-hashing.pdf", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "K-means hashing: An affinity-preserving quantization method for learning binary compact codes", "author": ["K. He", "F. Wen", "J. Sun"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Query adaptative locality sensitive hashing", "author": ["H. Jegou", "L. Amsaleg", "C. Schmid", "P. Gros"], "venue": "2008 IEEE International Conference on Acoustics, Speech and Signal Processing, March 2008, pp. 825\u2013828.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Probability and computing: Randomized algorithms and probabilistic analysis", "author": ["M. Mitzenmacher", "E. Upfal"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Order statistics & inference: estimation methods", "author": ["N. Balakrishnan", "A.C. Cohen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Expected values of normal order statistics", "author": ["H.L. Harter"], "venue": "Biometrika, vol. 48, no. 1/2, pp. 151\u2013165, 1961. [Online]. Available: http: //www.jstor.org/stable/2333139", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1961}, {"title": "On the difficulty of nearest neighbor search", "author": ["J. He", "S. Kumar", "S.-F. Chang"], "venue": "ICML 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "When Is \u201cNearest Neighbor", "author": ["K. Beyer", "J. Goldstein", "R. Ramakrishnan", "U. Shaft"], "venue": "Meaningful? Berlin, Heidelberg: Springer Berlin Heidelberg,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM, vol. 51, no. 1, pp. 117\u2013122, Jan. 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Compressed fingerprint matching and camera identification via random projections", "author": ["D. Valsesia", "G. Coluccia", "T. Bianchi", "E. Magli"], "venue": "IEEE Transactions on Information Forensics and Security, vol. 10, no. 7, pp. 1472\u20131485, July 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale image retrieval based on compressed camera identification", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 9, pp. 1439\u2013 1449, Sept 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "Pattern recognition", "author": ["C.M. Bishop"], "venue": "Machine Learning, vol. 128, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The concept of embedding has been successfully used in the context of information retrieval [1], where it is usually called \u201chashing\u201d.", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "Johnson and Lindenstrauss [2] famously stated that an embedding can be realized with a Lipschitz mapping to approximately preserve Euclidean distances with a dimension of the embedding space that only depends on the desired distortion and logarithmically in the number of signals to be embedded.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7].", "startOffset": 166, "endOffset": 169}, {"referenceID": 5, "context": "Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7].", "startOffset": 204, "endOffset": 207}, {"referenceID": 6, "context": "Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7].", "startOffset": 209, "endOffset": 212}, {"referenceID": 7, "context": "Recently, some works have studied learning embeddings [8]\u2013[10] from training data to derive compact codes by exploiting the particular geometry of the dataset (e.", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "Recently, some works have studied learning embeddings [8]\u2013[10] from training data to derive compact codes by exploiting the particular geometry of the dataset (e.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "[11] empirically explored", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "A well-known binary embedding is the sign random projections [3] where a random matrix \u03a6 made of independent and identically distributed (i.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "The Hamming distance between the binary vectors approximately preserves the angle between the signals in the original space [3], i.", "startOffset": 124, "endOffset": 127}, {"referenceID": 11, "context": "(2) can be readily obtained using Chernoff bounds [12] for the tails of DH .", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "(2) yields m = O(\u03b4\u22122 logN), which is exactly the same as classic results on non-adaptive random projections [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 12, "context": "We then notice that that is equivalent [13] to e2(mpool\u2212m+1);2mpool , i.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "As a further remark, according to [14] an approximation of the expected value of the desired order statistic is:", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "(4) can be readily obtained using Chernoff bounds [12] for the tails of DH .", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "The key distinction between sign random projections [3] and the method presented in this paper is that the former provides a linear relationship between the Hamming distance in the embedded space and the angle in the original space.", "startOffset": 52, "endOffset": 55}, {"referenceID": 14, "context": "A measure of difficulty [15], [16] of a nearest neighbor search problem is the contrast r, which is defined as the ratio between the distance of the closest false neighbor and that of the farthest true neighbor.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "A measure of difficulty [15], [16] of a nearest neighbor search problem is the contrast r, which is defined as the ratio between the distance of the closest false neighbor and that of the farthest true neighbor.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Locality sensitive hashing [17] is Fig.", "startOffset": 27, "endOffset": 31}, {"referenceID": 2, "context": "Non-adaptive curve uses sign random projections [3].", "startOffset": 48, "endOffset": 51}, {"referenceID": 17, "context": "This problem is not unrealistic and, in fact, as an example, it occurs in the detection of photoresponse non-uniformity artifacts from camera sensors [18] [19], used to attribute a given picture to a given camera sensor.", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "This problem is not unrealistic and, in fact, as an example, it occurs in the detection of photoresponse non-uniformity artifacts from camera sensors [18] [19], used to attribute a given picture to a given camera sensor.", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "[5] which is a kind of adaptive embedding where the quantizer can be parametrized in order to distort the expected value of signal distances, similarly to the embedding proposed in this paper.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "The weights are learned during the training phase using a suitable loss function such as the hinge loss [20] for support vector machines or the softmax cross-entropy [21] for multinomial logistic regression which is more popular in deep neural networks.", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "The weights are learned during the training phase using a suitable loss function such as the hinge loss [20] for support vector machines or the softmax cross-entropy [21] for multinomial logistic regression which is more popular in deep neural networks.", "startOffset": 166, "endOffset": 170}, {"referenceID": 21, "context": "The following experiment is a classification problem on the CIFAR-10 dataset [22] comprising 10 classes.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "We implemented the same convolutional neural network architecture presented in [23].", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "This network is composed of 8 convolutional layers followed by 2 fully connected layers all with ReLU activation units [24] and a final linear layer.", "startOffset": 119, "endOffset": 123}, {"referenceID": 2, "context": "It can be noticed that the adaptive embedding allows to achieve a significant dimensionality reduction at a negligible loss in terms of classification accuracy, with respect to both sign random projections [3] and the universal embedding [5].", "startOffset": 206, "endOffset": 209}, {"referenceID": 4, "context": "It can be noticed that the adaptive embedding allows to achieve a significant dimensionality reduction at a negligible loss in terms of classification accuracy, with respect to both sign random projections [3] and the universal embedding [5].", "startOffset": 238, "endOffset": 241}], "year": 2017, "abstractText": "We use some of the largest order statistics of the random projections of a reference signal to construct a binary embedding that is adapted to signals correlated with such signal. The embedding is characterized from the analytical standpoint and shown to provide improved performance on tasks such as classification in a reduced-dimensionality space. Keywords\u2014Binary Embeddings, Random projections", "creator": "LaTeX with hyperref package"}}}