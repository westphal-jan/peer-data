{"id": "1701.08694", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "A Comparative Study on Different Types of Approaches to Bengali document Categorization", "abstract": "Document 1965-66 categorization is maheswaran a technique sacco where sumberg the theonym category turbe of ithier a document 117-page is osraige determined. siahaan In 1610 this corbel paper vistas three well - known eddington supervised learning techniques which alinea are epatha Support Vector 133.50 Machine (modine SVM ), 19.0 Na \\ \" therapeutically ive Bayes (zandberg NB) kurstin and Stochastic bickler Gradient nobelity Descent (d'aquili SGD) emulator compared attorney-general for Bengali ginned document categorization. Besides p80 classifier, religioso classification boult also gerberga depends p300 on piergiorgio how feature is asesinos selected ibises from kaspar dataset. For eskandarian analyzing those classifier monocots performances lautman on predicting otori a document 369 against twelve kvitov\u00e1 categories getrag several gravemind feature roye selection techniques are also memebers applied repton in narrowness this article namely Chi madaki square distribution, bergsma normalized fric TFIDF (term frequency - koz inverse opposer document funded frequency) with www.nytsyn word analyzer. kaisi So, we attempt a-300 to 22,500 explore the cusd efficiency shoven of blocq those three - ambras classification algorithms by pinelands using two different feature selection lathwell techniques in dunvant this wharfedale article.", "histories": [["v1", "Fri, 27 Jan 2017 13:08:08 GMT  (528kb)", "http://arxiv.org/abs/1701.08694v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["md saiful islam", "fazla elahi md jubayer", "syed ikhtiar ahmed"], "accepted": false, "id": "1701.08694"}, "pdf": {"name": "1701.08694.pdf", "metadata": {"source": "CRF", "title": "A Comparative Study on Different Types of Approaches to Bengali document Categorization", "authors": ["Md. Saiful Islam", "Fazla Elahi", "Md Jubayer", "Syed Ikhtiar Ahmed"], "emails": ["saiful-cse@sust.edu"], "sections": [{"heading": null, "text": "* Corresponding author: * saiful-cse@sust.edu\nKeywords:\n NB;\n SVM;\n SGD;\n LED;\n TFIDF;\n N-Gram;\n Supervised\nLearning.\ntechniques which are Support Vector Machine(SVM), Na\u00efve Bayes(NB) and Stochastic Gradient Descent(SGD) compared for Bengali document categorization. Besides classifier, classification also depends on how feature is selected from dataset. For analyzing those classifier performances on predicting a document against twelve categories several feature selection techniques are also applied in this article namely Chi square distribution, normalized TFIDF (term frequency-inverse document frequency) with word analyzer. So, we attempt to explore the efficiency of those three-classification algorithms by using two different feature selection techniques in this article."}, {"heading": "1. INTRODUCTION", "text": "Content based classification is emerging nowadays due to continuous growth of electronic data. It is the process of grouping documents into different classes or categories. So document categorization plays an important role in natural language processing, computer science and information science. In digital library system, search engine, and document management system this automatic categorization can be used. Spam filtering (Sahami et al., 1998 ), online news filtering (Chan et al., 2008 ), social media analytics (Melville et al., 2009), survey data grouping etc. are some applications of document categorization.\nExtensive research has already been done in this field for various languages. There are several powerful techniques offered by natural language processing. Three ways through which a document can be classified are unsupervised, supervised and semi-supervised techniques. In this paper we use supervised learning. Supervised learning algorithm is a technique which analyzes labeled training data sets and produces a function by which it can map new examples for prediction. The main focus of this article is categorize Bangla documents. Bangla is one of the most popular languages throughout the world. According to the consensus of the total number of native speakers, it is the 7th most spoken language (Islam, 2009). So it is very much needed to organize and categorize Bengali documents automatically so that users can easily find required and related information. For this purpose, this paper describes how to automatically categorize Bengali documents using the supervised learning technique. The result achieved for twelve categories by using the formula described later in this paper, is quite promising as it is better than the existing research on Bangla document classification.\n.2. RELATED WORKS\nFrom literacy review we see that many research works have been done in text or document categorization for English languages. Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.(1992) used N-grams,Quinlan (1986) used Decision Tree(DT), Sebastiani (2002) used Neural Network(NNet), Cortes and Vapnik (1995) used SVM. There are fair amounts of comparative.\nstudy also done in English DC. Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news.\nBesides English document there also done good amount of research on other languages too. KOURDI et al. (2004) used SVM , Mesleh (2008) used NB for automatic text classification in Arabic languages. For Tamil (South Indian language) languages Rajan et al. (2009) used Na\u00efve Bayes and Gupta (2012) used Neural networks for this task.\nIn addition, few works have been done on document categorization for Bangla languages. Mandal and Sen(2014) compared four supervised learning techniques for labeled web documents into five categories."}, {"heading": "3. FEATURE SELECTION ALGORITHM:", "text": "In this section, we briefly describe TF-IDF and Chi-Square distribution methods used in this study."}, {"heading": "TF-IDF:", "text": "Textual representation is converted to vector space model consisting of term frequency. The fundamental issue with the term-frequency method is that it scales up frequent terms as a result importance of rare terms is ignored, though low frequency terms are significant for distinguish between classes. Where TFIDF approaches scale down the frequent terms while scaling up the uncommon or rare terms. If a term occurs in 10 documents among total 15 documents then it is not as important as which is occur less than 10 document or occur only one document. That\u2019s why TFIDF uses the logarithm scale to do that tricks. Next, IDF computed using the following formula:\nln ( \ud835\udc41 + 1\n\ud835\udc37\ud835\udc39 + 1 ) + 1"}, {"heading": "CHI-SQUARE DISTRIBUTION:", "text": "Chi-square distribution is a simple statistical approach. In paper[25] authors used this method to determine important words in a document. Mathematically chi-square distribution can be written as:\n\ud835\udf122 =\u2211 {\ud835\udc5c\ud835\udc56 \u2212 \ud835\udf00\ud835\udc56}\n2\n\ud835\udf00\ud835\udc56\n\ud835\udc5b\n\ud835\udf04=0\n3 | Md. Saiful Islam et. al., I C E R I E 2 0 1 7\nWhere \ud835\udc4b2 represents chi-square value, \ud835\udc5c\ud835\udc56 represents observed frequency, \ud835\udf00\ud835\udc56 represents expected frequency and degree is freedom is 1. For calculating chi value of each term in a document the equation can be modified as:\n\ud835\udc4b2(\ud835\udc64) =\u2211 {\ud835\udc53\ud835\udc5f\ud835\udc52\ud835\udc5e(\ud835\udc64, \ud835\udc54) \u2212 \ud835\udc43\ud835\udc54\ud835\udc5b\ud835\udc64}\n2\n\ud835\udc43\ud835\udc54\ud835\udc5b\ud835\udc64\ud835\udc54\ud835\udf16\ud835\udc3a\nWhere, \ud835\udc43\ud835\udc54 = TF of a word/ total number of word in the document. \ud835\udc5b\ud835\udc64 = TF of a word w in a sentence where w appears. \ud835\udc43\ud835\udc54\ud835\udc5b\ud835\udc64 = Frequency of word co-occurrence. So, if a word appears in a long length sentence in a document it threated as an important term as they those words likely to co-occur with many other terms."}, {"heading": "4. CLASSIFICATION ALGORITHM:", "text": ""}, {"heading": "NA\u00cfVE BAYES:", "text": "Na\u00efve Bayes algorithm is one kind of probabilistic approach, based on applying Bayes theorem. Contingent upon the exact way of the likelihood show, this classifier can be prepared productively in a supervised learning methods."}, {"heading": "STOCHASTIC GRADIENT DESCENT:", "text": "Stochastic Gradient Descent (SGD) is a straightforward yet extremely productive way to deal with discriminative learning of linear classifiers under curved misfortune capacities, for example, (linear) Support Vector Machines and Logistic Regression [26]. Despite the fact that SGD has been around in the machine learning group for quite a while, it has gotten a lot of consideration only as of late with regards to expansive scale learning. It is effectively applied to large-scale and sparse machine learning issues frequently experienced in DC. For sparse data, it easily scale to problems with more than 10^5 training samples and more than 10^5 features.\nSUPPORT VECTOR MACHINE:\nAnother supervised learning algorithm is Support Vector Machine(SVM) which is widely and effectively used for DC. While training a classifier it needs to manage a lot of features. For our case it was 216576 features from 28717 train documents (table II) which was calculated after preprocessing and feature selection. Since SVMs use overfitting protection, which does not necessarily depend on the number of features, they have the potential to handle these large feature spaces. One of the important properties of text categorization is that most of the DC problems are linearly separable. SVMs algorithm can able to find such linear as well as polynomial, RBF separator. Basically SVM act as binary classifier. If n is the number of total features, then SVM plot each data item as a point in n-dimensional space where the value of each features represent the value of a particular coordinate. Then for separate two classes performing this classification algorithm find a hyperplane. More specifically, a SVM create a hyperplane or a set of hyperplane in a n-dimensional space, which can be used to classify. Theoretically, if x is the training data set and w is n weighted vector in Rn then the learning function of SVM is f(x) = sign(wx + b)."}, {"heading": "5. EXPERIMENTAL SETUP AND RESULTS:", "text": ""}, {"heading": "DATASET USED IN THIS EXPERIMENT:", "text": "All the documents that are used in these experiments are collected from a Bengali document corpus [27]. All documents are collected from various Bangladeshi newspapers such as http://prothom-alo.com, http://bdnews24.com, http://dailykalerkantha.com etc. and are labeled with their corresponding category name. From this corpus, we collect\nalmost same number of documents for training and testing for each category which are represented in table I. There are twelve categories."}, {"heading": "PREPROCESS DATASET:", "text": "Before using the documents, training the classifier model preprocessing data set is a crucial step. It is needed because it removes recurring words or symbols from each document which are common for all documents and plays an important role in dimension reduction of feature space and increase in the performance. So by doing this, it will find those important words only which are relevant for the information of data set, from which classifier can determine the label of document. The steps are (i)Tokenization (ii) Remove frequent symbol (iii) Stemming and (iv) Remove all pronouns and conjunction."}, {"heading": "APPLY FEATURE SELECTION AND TRAIN MODEL:", "text": "After preprocess done, two different feature engineering techniques as described above were applied separately with SVM, SGD, and NB classifier and compare performance with each other.\nFor chi-square distribution system, we sorted all features in a document in ascending order after this top 30% feature selected. Total 91503 features selected from total 28717 training documents using preprocess+chi-square method. For TFIDF we used unigram as word analyzer and normalized it weighting with length normalization. Total 216576 features selected from the same size train documents as previously used while using preprocess+TFIDF which is almost 2.3 times higher number of features then we found from preprocess+chi-square method.\nWhile using NB classifier, alpha value fixed to 0.01 and SGD classifier tuned with hinge as loss function, l2 regularization as penalty, 0.0001 as alpha value and 50 as number of iteration. For implementing SVM, LIBSVM was used which is a library for SVM. It supports multiclass classification. With the help of this library we used C-SVC method with linear kernel for classifier.\nUsing table I training data set and above configuration we performed CHI-SQUARE+NB, CHI-SQUARE+SGD, CHISQUARE+SVM, TFIDF+NB, TFIDF+SGD, TFIDF+SVM.\nprecision, recall. F-measure(macro average) and accuracy are calculated for those methodologies. Precision represents how many labels were correctly predicted (for a given class A) from all predicted label. Recall represents how many labels were correctly predicted from all instances that should have a label A. F-measure represents the weighted average of the precision and recall.\n5 | Md. Saiful Islam et. al., I C E R I E 2 0 1 7"}, {"heading": "6. CONCLUSION:", "text": "After calculating the performance of different approaches it is found that SVM classifier obtained the highest F1score(92.56%) while normalized TFIDF used as feature selection and CHI-SQUARE+NB was the lowest with F1score 83.36% . It is also observed that all the classifier performed better with TFIDF feature selection technique than chi-square distribution method. Below chart shows the comparisons between them:"}, {"heading": "8. REFERENCES", "text": "M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A Bayesian approach to filtering junk email., AAAI Workshop on Learning for Text Categorization, July 1998, Madison, Wisconsin. AAAI Technical Report WS-98- 05.\nC. Chan, A. Sun, and E. Lim, \"Automated Online News Classification with Personalization,\" in 4th International\nConference of Asian Digital Library, 2001.\nP. Melville, V. Sindhwani, and R. Lawrence, \"Social media analytics: Channeling the power of the blogosphere\nfor marketing insight.,\" in Workshop on Information in Networks, 2009.\nM. Islam, \"Research on Bangla language processing in Bangladesh: progress and challenges,\" 8th ILDC, Dhaka,\nBangladesh (June 2009), 2009.\nT. Denoeux, \"A k-nearest neighbor classification rule based on Dempster-Shafer theory,\" Systems, Man and\nCybernetics, IEEE Transactions on, vol. 25, pp. 804-813, 1995.\nJ. Chen, H. Huang, S. Tian, and Y. Qu, \"Feature selection for text classification with Na\u00efve Bayes,\"Expert Systems\nwith Applications, vol. 36, pp. 5432-5435, 2009.\nP. F. Brown, P. V. Desouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai, \"Class-based n-gram models of natural\nlanguage,\" Computational linguistics, vol. 18, pp. 467-479, 1992.\nJ. R. Quinlan, \"Induction of decision trees,\" Machine learning, vol. 1, pp. 81-106, 1986. F. Sebastiani, \"Machine learning in automated text categorization,\" ACM computing surveys (CSUR), vol. 34, pp.\n1-47, 2002.\nC. Cortes and V. Vapnik, \"Support-vector networks,\" Machine learning, vol. 20, pp. 273-297, 1995. A. S. Patil and B. Pawar, \"Automated classification of web sites using Naive Bayesian algorithm,\" in Proceedings\nof the International MultiConference of Engineers and Computer Scientists, 2012, pp. 14-16.\nV. Bijalwan, V. Kumar, P. Kumari, and J. Pascual, \"KNN based Machine Learning Approach for Text and\nDocument Mining,\" International Journal of Database Theory and Application, vol. 7, pp. 61-70,2014.\nTam, Santoso A and Setiono R., \u201cA comparative study of centroid-based, neighborhood-based and statistical approaches for effective document categorization\u201d, ICPR '02 Proceedings of the 16 th International Conference on Pattern Recognition (ICPR'02) ,vol.4 , no. 4 , 2002, pp.235\u2013238.\nPratiksha Y. Pawar and S. H. Gawande, \"A Comparative Study on Different Types of Approaches to Text\nCategorization\" International Journal of Machine Learning and Computing, Vol. 2, No. 4, August 2012\nL. Zhijie, L. Xueqiang, L. Kun, and S. Shuicai, \"Study on SVM Compared with the other Text Classification Methods,\" in Education Technology and Computer Science (ETCS), 2010 Second International Workshop on, 2010, pp. 219-222.\nThorsten Joachims. 1998. Text categorization with Support Vector Machines: Learning with many relevant\nfeatures. In Proceedings of the 10th European Conference on Machine Learning, ECML\u201998, pages 137\u2013142.\nT. Zakzouk and H. Mathkour, \"Text Classifiers for Cricket Sports News,\" in proceedings of International\nConference on Telecommunications Technology and Applications ICTTA, 2011, pp. 196-201.\nMohamed EL KOURDI, Amine BENSAID, Tajje-eddine RACHIDI, \"Automatic Arabic Document Categorization Based on the Na\u00efve Bayes Algorithm,\" in proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages, 2004, pp. 51-58.\nA. Moh\u2019d Mesleh, \"Support vector machines based Arabic language text classification system: feature selection comparative study,\" in Advances in Computer and Information Sciences and Engineering, ed: Springer, 2008, pp. 11- 16.\nK. Rajan, V. Ramalingam, M. Ganesan, S. Palanivel, and B. Palaniappan, \"Automatic classification of Tamil documents using vector space model and artificial neural network,\" Expert Systems with Applications, vol. 36, pp. 10914-10918, 2009.\nN. a. V. Gupta, \"Algorithm for Punjabi Text Classification,\" International Journal of Computer Applications, vol.\n37, pp. 30-35, 2012.\nAshis Kumar Mandal, Rikta Sen, \"Supervised Learning Methods For Bangla Web Document Categorization,\"\nInternational Journal of Artificial Intelligence & Applications (IJAIA), Vol. 5, No. 5, September 2014\nY. Matsuo, and M. Ishizuka, \u201cKeyword Extraction from a Single Document using Word Co-occurrence Statistical\nInformation\u201d, International journal on Artificial Intelligence Tools, vol.13, no.1, pp.157-169, 2004.\nT. Zhang, \u201cSolving large scale linear prediction problems using stochastic gradient descent algorithms,\u201d in Proceedings of the Twenty-first International Conference on Machine Learning, ser. ICML \u201904. New York, NY, USA: ACM, 2004, pp. 116\u2013. [Online]. Available: http://doi.acm.org/10.1145/1015330.1015332.\nhttp://scdnlab.com. Bangla Corpus[Online]. Available: http://scdnlab.com/corpus/ [Accessed: Retrieved Oct 13,\n2016]."}], "references": [{"title": "A Bayesian approach to filtering junk email., AAAI Workshop on Learning for Text Categorization, July 1998, Madison, Wisconsin", "author": ["M. Sahami", "S. Dumais", "D. Heckerman", "E. Horvitz"], "venue": "AAAI Technical Report", "citeRegEx": "Sahami et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sahami et al\\.", "year": 2001}, {"title": "Social media analytics: Channeling the power of the blogosphere for marketing insight.,", "author": ["P. Melville", "V. Sindhwani", "R. Lawrence"], "venue": "in Workshop on Information in Networks,", "citeRegEx": "Melville et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Melville et al\\.", "year": 2009}, {"title": "A k-nearest neighbor classification rule based on Dempster-Shafer theory,\" Systems, Man and Cybernetics", "author": ["T. Denoeux"], "venue": "IEEE Transactions on,", "citeRegEx": "Denoeux,? \\Q1995\\E", "shortCiteRegEx": "Denoeux", "year": 1995}, {"title": "Feature selection for text classification with Na\u00efve Bayes,\"Expert", "author": ["J. Chen", "H. Huang", "S. Tian", "Y. Qu"], "venue": "Systems with Applications,", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Class-based n-gram models of natural language,", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Induction of decision trees,", "author": ["J.R. Quinlan"], "venue": "Machine learning,", "citeRegEx": "Quinlan,? \\Q1986\\E", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "Machine learning in automated text categorization,", "author": ["F. Sebastiani"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "Sebastiani,? \\Q2002\\E", "shortCiteRegEx": "Sebastiani", "year": 2002}, {"title": "Automated classification of web sites using Naive Bayesian algorithm,", "author": ["A.S. Patil", "B. Pawar"], "venue": "Proceedings of the International MultiConference of Engineers and Computer Scientists,", "citeRegEx": "Patil and Pawar,? \\Q2012\\E", "shortCiteRegEx": "Patil and Pawar", "year": 2012}, {"title": "KNN based Machine Learning Approach for Text and Document Mining,", "author": ["V. Bijalwan", "V. Kumar", "P. Kumari", "J. Pascual"], "venue": "International Journal of Database Theory and Application,", "citeRegEx": "Bijalwan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bijalwan et al\\.", "year": 2014}, {"title": "A comparative study of centroid-based, neighborhood-based and statistical approaches for effective document categorization", "author": ["Tam", "A Santoso", "R. Setiono"], "venue": "Proceedings of the 16 th International Conference on Pattern Recognition", "citeRegEx": "Tam et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tam et al\\.", "year": 2002}, {"title": "A Comparative Study on Different Types of Approaches to Text Categorization", "author": ["Pratiksha Y. Pawar", "S.H. Gawande"], "venue": "International Journal of Machine Learning and Computing,", "citeRegEx": "Pawar and Gawande,? \\Q2012\\E", "shortCiteRegEx": "Pawar and Gawande", "year": 2012}, {"title": "Shuicai, \"Study on SVM Compared with the other Text Classification Methods,\" in Education", "author": ["L. Zhijie", "L. Xueqiang", "L. Kun"], "venue": "Technology and Computer Science (ETCS),", "citeRegEx": "Zhijie et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhijie et al\\.", "year": 2010}, {"title": "Text categorization with Support Vector Machines: Learning with many relevant features", "author": ["Thorsten Joachims."], "venue": "Proceedings of the 10th European Conference on Machine Learning, ECML\u201998, pages 137\u2013142.", "citeRegEx": "Joachims.,? 1998", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Text Classifiers for Cricket", "author": ["T. Zakzouk", "H. Mathkour"], "venue": "Sports News,\" in proceedings of International Conference on Telecommunications Technology and Applications ICTTA,", "citeRegEx": "Zakzouk and Mathkour,? \\Q2011\\E", "shortCiteRegEx": "Zakzouk and Mathkour", "year": 2011}, {"title": "RACHIDI, \"Automatic Arabic Document Categorization Based on the Na\u00efve Bayes Algorithm,\" in proceedings of the Workshop on Computational Approaches to Arabic", "author": ["Mohamed EL KOURDI", "Amine BENSAID", "Tajje-eddine"], "venue": "Script-based Languages,", "citeRegEx": "KOURDI et al\\.,? \\Q2004\\E", "shortCiteRegEx": "KOURDI et al\\.", "year": 2004}, {"title": "Support vector machines based Arabic language text classification system: feature selection comparative study,\" in Advances in Computer and Information Sciences and Engineering", "author": ["A. Moh\u2019d Mesleh"], "venue": null, "citeRegEx": "Mesleh,? \\Q2008\\E", "shortCiteRegEx": "Mesleh", "year": 2008}, {"title": "Automatic classification of Tamil documents using vector space model and artificial neural network,", "author": ["K. Rajan", "V. Ramalingam", "M. Ganesan", "S. Palanivel", "B. Palaniappan"], "venue": "Expert Systems with Applications,", "citeRegEx": "Rajan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rajan et al\\.", "year": 2009}, {"title": "Algorithm for Punjabi Text Classification,", "author": ["N. a. V. Gupta"], "venue": "International Journal of Computer Applications,", "citeRegEx": "Gupta,? \\Q2012\\E", "shortCiteRegEx": "Gupta", "year": 2012}, {"title": "Supervised Learning Methods For Bangla Web Document Categorization,", "author": ["Ashis Kumar Mandal", "Rikta Sen"], "venue": "International Journal of Artificial Intelligence & Applications (IJAIA),", "citeRegEx": "Mandal and Sen,? \\Q2014\\E", "shortCiteRegEx": "Mandal and Sen", "year": 2014}, {"title": "Keyword Extraction from a Single Document using Word Co-occurrence Statistical Information", "author": ["Y. Matsuo", "M. Ishizuka"], "venue": "International journal on Artificial Intelligence Tools, vol.13,", "citeRegEx": "Matsuo and Ishizuka,? \\Q2004\\E", "shortCiteRegEx": "Matsuo and Ishizuka", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": ", 2008 ), social media analytics (Melville et al., 2009), survey data grouping etc.", "startOffset": 33, "endOffset": 56}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al.", "startOffset": 80, "endOffset": 95}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.", "startOffset": 80, "endOffset": 143}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.(1992) used N-grams,Quinlan (1986) used Decision Tree(DT), Sebastiani (2002) used Neural Network(NNet), Cortes and Vapnik (1995) used SVM.", "startOffset": 80, "endOffset": 183}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.(1992) used N-grams,Quinlan (1986) used Decision Tree(DT), Sebastiani (2002) used Neural Network(NNet), Cortes and Vapnik (1995) used SVM.", "startOffset": 80, "endOffset": 211}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.(1992) used N-grams,Quinlan (1986) used Decision Tree(DT), Sebastiani (2002) used Neural Network(NNet), Cortes and Vapnik (1995) used SVM.", "startOffset": 80, "endOffset": 253}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.(1992) used N-grams,Quinlan (1986) used Decision Tree(DT), Sebastiani (2002) used Neural Network(NNet), Cortes and Vapnik (1995) used SVM.", "startOffset": 80, "endOffset": 305}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task.", "startOffset": 0, "endOffset": 164}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document.", "startOffset": 0, "endOffset": 335}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM.", "startOffset": 0, "endOffset": 431}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB.", "startOffset": 0, "endOffset": 678}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC.", "startOffset": 0, "endOffset": 800}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news.", "startOffset": 0, "endOffset": 910}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news. Besides English document there also done good amount of research on other languages too. KOURDI et al. (2004) used SVM , Mesleh (2008) used NB for automatic text classification in Arabic languages.", "startOffset": 0, "endOffset": 1063}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news. Besides English document there also done good amount of research on other languages too. KOURDI et al. (2004) used SVM , Mesleh (2008) used NB for automatic text classification in Arabic languages.", "startOffset": 0, "endOffset": 1088}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news. Besides English document there also done good amount of research on other languages too. KOURDI et al. (2004) used SVM , Mesleh (2008) used NB for automatic text classification in Arabic languages. For Tamil (South Indian language) languages Rajan et al. (2009) used Na\u00efve Bayes and Gupta (2012) used Neural networks for this task.", "startOffset": 0, "endOffset": 1215}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news. Besides English document there also done good amount of research on other languages too. KOURDI et al. (2004) used SVM , Mesleh (2008) used NB for automatic text classification in Arabic languages. For Tamil (South Indian language) languages Rajan et al. (2009) used Na\u00efve Bayes and Gupta (2012) used Neural networks for this task.", "startOffset": 0, "endOffset": 1249}, {"referenceID": 18, "context": "Mandal and Sen(2014) compared four supervised learning techniques for labeled web documents into five categories.", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "Learning. Abstract: Document categorization is a technique where the category of a document is determined. In this paper three well-known supervised learning techniques which are Support Vector Machine(SVM), Na\u00efve Bayes(NB) and Stochastic Gradient Descent(SGD) compared for Bengali document categorization. Besides classifier, classification also depends on how feature is selected from dataset. For analyzing those classifier performances on predicting a document against twelve categories several feature selection techniques are also applied in this article namely Chi square distribution, normalized TFIDF (term frequency-inverse document frequency) with word analyzer. So, we attempt to explore the efficiency of those three-classification algorithms by using two different feature selection techniques in this article.", "creator": "Microsoft\u00ae Word 2016"}}}