{"id": "1701.08702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language Model", "abstract": "guera In rjm this paper, trybuna we defacto describe caama a research winfree method clathrus that ingens generates templum Bangla nordhoff word clusters on putter the reimburse basis of kleinbahn relating coritiba to paino meaning in ubiquitin language and contextual rozes similarity. The urman importance exotics of rh\u00f4ne-poulenc word clustering gandel is lantawan in parts of undergarments speech (akwuegbu POS) tagging, word itzik sense posidonius disambiguation, wildwood text classification, kondos recommender system, spell checker, thing-in-itself grammar savielly checker, armfield knowledge uncalculated discover bodenham and juha for post-it many others Natural Language serbsky Processing (marauder NLP) applications. In l'atlantique the libis history of word bramwell clustering, English ex-im and some other languages analyzers have already makani implemented some brockhampton methods on megillat word allotropes clustering brn efficiently. adge But bagosora due to f37 lack notagonum of the f\u00fcrstenwalde resources, word clustering resides in 498 Bangla prosecuter has not been still disorientating implemented vot efficiently. kalishnikov Presently, michiko its implementation 63.45 is ltt in the burberry beginning orentlicher stage. In h.s some corridore research propranolol of word mkr clustering in English fiberoptic based 71.64 on straightaway preceding marojejy and wood next kdc five words of shangdong a key word geetmala they found hakims an efficient result. Now, tisin we are trying aspheric to implement 11:04 the tevin tri - gram, ferments 4 - gram and ossified 5 - gram model intoxication of word clustering bogush for Bangla senja to observe which one is battlefield the casl best lagun among them. darlie We have lanao started 1.4775 our research honko with 265th quite a \u2015 large tr\u1ea5n corpus concetta of approximate 1 lakh Bangla voltage-controlled words. We tairrie are bloembergen using a 1-76 machine miloon learning anthelmintic technique mahboob in ch\u00e3 this binyamin research. We will wadesboro generate word grouting clusters niels-henning and analyze the clusters lma by werknummer testing generators some 1,600-meter different threshold yamam values.", "histories": [["v1", "Fri, 27 Jan 2017 18:43:31 GMT  (281kb)", "http://arxiv.org/abs/1701.08702v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dipaloke saha", "md saddam hossain", "md saiful islam", "sabir ismail"], "accepted": false, "id": "1701.08702"}, "pdf": {"name": "1701.08702.pdf", "metadata": {"source": "CRF", "title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language Model", "authors": ["Dipaloke Saha", "Md Saddam Hossain", "Saiful Islam", "Sabir Ismail"], "emails": ["dipsustcse12@gmail.com,", "mshossaincse@gmail.com", "saiful-cse@sust.edu", "sabir.ismail01@gmail.com", "dipsustcse12@gmail.com"], "sections": [{"heading": null, "text": "Keywords:\n Word Cluster,\nNatural\nLanguage\nProcessing,\nMachine\nLearning,\nN-gram Model,\nTerm\nFrequency (tf).\n SUST, ICERIE."}, {"heading": "1. INTRODUCTION", "text": "Though Bangla is a widely spoken language, it has lack of resources in its research field. Recently, a new research dimension in Bangla is added called word clustering. In this paper, the research of word clustering for Bangla language is trying to be extended. For this, a large Bangla corpus containing 97,971 individual words is compiled to generate the word clusters. In this paper, an unsupervised machine learning technique and a method are proposed to cluster Bangla words on the basis of similarity in semantics and contexts.\nIn language processing word cluster has a wide range of applications. POS tag is one of them. Same clustered words usually contain the same POS tag. Word clustering can produce suggestions for an inaccurately typed word which is very much helpful for spell checker. Word sense disambiguation, sentence structure with grammatical mistakes can also be solvable using clustered words. In the case of recommender system if related products of the same category are clustered in the same group, more feasible suggestion can be produced. This type of work is also useful for Bangla search engine to find the appropriate content. So, there is a huge importance of word clustering in the field of natural language processing.\n* dipsustcse12@gmail.com"}, {"heading": "2. RELATED WORK", "text": "In Bangla the implementation of word clustering is in the neophyte stage. A previous work on Bangla word clustering exists in which an unsupervised machine learning technique is used to implement the bigram model by Sabir Ismail and M. Shahidur Rahman. In many other languages different types of techniques are used for word clustering. Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster. However, with the use of delete interpolation method by Mori, Nishimura and Itoh (1998) they got the better result than the Brown, Desouza\u2019s method. This was done for Japanese and English language. Besides these, there exists quite a good number of researches of word clustering for some other languages like Russian, Arabic, Chinese etc."}, {"heading": "3. PROBLEM DEFINITION", "text": "Clustering is an unsupervised machine learning technique that does not require any type of rules or predefined conditions. Items which are much similar either in semantically or contextually are grouped in the same cluster and which are dissimilar are in different clusters. The introduced method in this problem is concentrating on two types of similarity such as semantics and contextual similarity.\nConsider the following four sentences:\nand are similar in semantic meaning in sentence 1 and 2 and there is similarity in and\nin sentence 3 and 4. Here, the theory of N-gram model is implemented . Probability distribution is used\nhere to define n-th item in a sequence form previous or next (n-1) items. Tri-gram, 4 th and 5 th gram model is defined as size of 3, 4 and 5 of N-gram respectively. In this research, word clusters will be generated by implementing tri, 4 th and 5 th\ngram model. After finding the word clusters the most efficient model will be found out based on those clustering words."}, {"heading": "4. METHODOLOGY", "text": "Firstly, quite a large corpus of 97,971 individual words Wi is used in this research. Next, a list of previous three words of a specific word for tri-gram, four words for 4-gram, five words for 5-gram are prepared. Similarly, a list of next three words of a specific word for tri-gram, four words for 4-gram, five words for 5- gram are prepared. Next, similarity between a pair of words to be included in the same cluster based on preceding three words, four words and five words are determined as follows:\nIn tri-gram for every pair of words Wi, Wj the number of matched preceding words from list\nlist(Wi-3, Wi-2 , Wi-1) and list(Wj-3, Wj-2, Wj-1)\nP(Wi,Wj)=(Count(match(list(Wi-3,Wi-2,Wi-1),list(Wj-3,Wj-2,Wj-1)))/((Count(list(Wi-3,Wi-2,Wi1))+Count(list(Wj-3,Wj-2,Wj-1 )))\nSimilarly, calculation for the 4-gram model is:\nP(Wi,Wj) = (Count(match(list(Wi-4,Wi-3,Wi-2 ,Wi-1),list(Wj-4,Wj-3,Wj-2,Wj-1)))/((Count(list(Wi-4,Wi-3,Wi-2, Wi-1))+Count(list(Wj-4,Wj-3,Wj-2,Wj-1)))\n3 | Dipaloke et. al., I C E R I E 2 0 1 7\nand for 5-gram model is:\nP(Wi,Wj)=(Count(match(list(Wi-5,Wi-4,Wi-3,Wi-2,Wi-1),list(Wj-5,Wj-4,Wj-3,Wj-2,Wj-1)))/((Count(list(Wi5,Wi-4,Wi-3,Wi-2, Wi-1))+Count(list(Wj-5,Wj-4,Wj-3,Wj-2,Wj-1)))\nAgain similarly, between a pair of words to be included in the same cluster based on following three, four and five words are determined as follows, For tri-gram,\nP(Wi,Wj)=(Count(match(list(Wi+3,Wi+2,Wi+1),list(Wj+3,Wj+2,Wj+1)))/((Count(list(Wi+3,Wi+2,Wi+1)) +Count(list(Wj+3, Wj+2, Wj+1)))\nSimilarly, calculation for the 4-gram model is:\nP(Wi,Wj)=(Count(match(list(Wi+4,Wi+3,Wi+2,Wi+1),list(Wj+4,Wj+3,Wj+2,Wj+1)))/((Count(list(Wi+4,Wi+3 ,Wi+2, Wi+1))+Count(list(Wj+4,Wj+3,Wj+2,Wj+1)))\nand for 5-gram model is:\nP(Wi,Wj)=(Count(match(list(Wi+5,Wi+4,Wi+3,Wi+2,Wi+1),list(Wj+5,Wj+4,Wj+3,Wj+2,Wj+1)))/((Count(list (Wi+5,Wi+4,Wi+3,Wi+2,Wi+1))+Count(list(Wj+5,Wj+4,Wj+3,Wj+2,Wj+1)))\nIf the above equations of a particular model yield values greater than a predefined threshold value they\nare grouped into the same cluster for that model. For example, to implement the tri-gram model some of the following phrases are :\nFor word preceding three words list:\nlist(Wi-3, Wi-2, Wi-1) =\nCount (list(Wi-3, Wi-2, Wi-1)) = 3\nFor word Following three words list :\nlist(Wi+3, Wi+2, Wi+1) = Count (list(Wi+3, Wi+2, Wi+1)) = 3\nFor word preceding three words list:\nlist(Wj-3, Wj-2, Wj-1) = Count (list(Wj-3, Wj-2, Wj-1)) = 3\nFor word following three words list:\nlist(Wj+3, Wj+2, Wj+1) = Count(list(Wj+3, Wj+2, Wj+1)) = 3\nNumber of matched words for word with based on preceding three words :\nCount(match(list(Wi-3, Wi-2 , Wi-1),list(Wj-3, Wj-2 , Wj-1))) = 2 Count (list(Wi-3, Wi-2, Wi-1)) + Count (list(Wj-3, Wj-2, Wj-1)) = 6\nSimilarity between words and based on preceding three words:\nP(Wi, Wj) = 2/6 = 0.33\nNumber of matched words for word and based on following three words:\nCount(match(list(Wi+3, Wi+2 , Wi+1),list(Wj+3, Wj+2 , Wj+1))) = 2 Count(list(Wi+3, Wi+2, Wi+1)) + Count(list(Wj+3, Wj+2, Wj+1)) = 6\nSimilarity between words and based on following three words:\nP(Wi, Wj) = 2/6 = 0.33\nSimilarly, 4 th and 5 th gram model can be implemented in the same way.\nThe value of similarity between words with when considering preceding three words is 0.33 and considering following three words it is also 0.33. Different types of threshold values are experimented and best result is earned with 0.20. Both words are grouped in the same cluster when all the probability scores are greater than this threshold value."}, {"heading": "5. RESULT ANALYSIS", "text": "In the tri, 4 th and 5 th gram model we derive 2215, 3327 and 5730 word clusters in total respectively. Some\nclusters randomly from each of the model are represented here in the following tables:\n5 | Dipaloke et. al., I C E R I E 2 0 1 7\nAfter analyzing the word clusters of all the three models we find poor similarity in some word clusters such as 266 for tri-gram, 300 for 4 th gram and 360 for 5 th gram. So, we find 1949, 3027 and 5370 clusters in strong similarity for the tri, 4 th and 5 th gram model respectively. So, the accuracy for strong similarity in\nTri-gram :- 88%\n4 th gram :- 91%\n5 th gram :- 93%\nSo, it is observed that 4 th gram is is better than tri-gram and 5 th gram is the best in all of them."}, {"heading": "6. CONCLUSION", "text": "Word clustering is important for various types of purpose for any language. For this reason in Bangla, trigram, 4 th gram and 5 th gram model is implemented here to proceed the previous work on word clustering. The analysis and result presented above on quite a large Bangla corpus has helped us to find the efficiency among the three mentioned models for word clustering. On the basis of the observation, it can be said that better efficiency is in the higher orders than the preceding orders of the N-gram model."}, {"heading": "H A S\u00e1nchez, A P Porrata and R B Llavori. \u201cWord sense disambiguation based on word sense clustering\u201d.", "text": "Advances in Artificial Intelligence,Springer Berlin Heidelberg, 2006. P: 472-481.\nSabir Ismail, M. Shahidur Rahman. https://www.researchgate.net/publication/261551758_Bangla_Word_Cl ustering_Based_on_N-gram_Language_Model , in press."}, {"heading": "S Finch and N Chater. \u201cAutomatic methods for finding linguisticcategories\u201d. In Igor Alexander and John", "text": "Taylor, editors,ArtificialNeural Networks, Volume 2. Elsevier Science Publishers, 1992.\nP F Brown, P V Desouza, R L Mercer, V J D Pietra, V J Della. and J CLai. \u201cClass-based N-gram Models of Natural Language\u201d.Computationalinguistics, 18 No: 4, 1992, P: 467-479.\nEEKorkmaz. \u201cA method for improving automatic wordcategorization\u201d. Doctoral dissertation, Middle East Technical University, 1997, in press."}, {"heading": "S Mori, M Nishimura and N Itoh. \u201cWord clustering for a word bi - gramModel\u201d. International Conference on", "text": "Spoken Language Processing, 1998, in press.\nClustering \u2013 Introduction, http://home.deib.polimi.it/ matteucc/Clustering/tutorial_html.\nClustering \u2013 Introduction, \u201chttp://www.stanford.edu/class/cs345a/slides/12-clustering.pdf \u201c.Stanford UniversityClustering.\nSimilarity in semantics and contexts, http://www.ilc.cnr.it/EAGLES96/rep2/node37.html"}], "references": [{"title": "Word sense disambiguation based on word sense clustering", "author": ["H A S\u00e1nchez", "A P Porrata", "R B Llavori"], "venue": "Advances in Artificial Intelligence,Springer Berlin Heidelberg,", "citeRegEx": "S\u00e1nchez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "S\u00e1nchez et al\\.", "year": 2006}, {"title": "Automatic methods for finding linguisticcategories", "author": ["S Finch", "N Chater"], "venue": "Elsevier Science Publishers,", "citeRegEx": "Finch and Chater.,? \\Q1992\\E", "shortCiteRegEx": "Finch and Chater.", "year": 1992}, {"title": "Word clustering for a word bi - gramModel", "author": ["S Mori", "M Nishimura", "N Itoh"], "venue": "Technical University,", "citeRegEx": "Mori et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Mori et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster.", "startOffset": 0, "endOffset": 225}, {"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster.", "startOffset": 0, "endOffset": 292}, {"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster. However, with the use of delete interpolation method by Mori, Nishimura and Itoh (1998) they got the better result than the Brown, Desouza\u2019s method.", "startOffset": 0, "endOffset": 478}], "year": 2016, "abstractText": "\uf0b7 SUST, ICERIE. Abstract: \u2014 In this paper, we describe a research method that generates Bangla word clusters on the basis of relating to meaning in language and contextual similarity. The importance of word clustering is in parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. In the history of word clustering, English and some other languages have already implemented some methods on word clustering efficiently. But due to lack of the resources, word clustering in Bangla has not been still implemented efficiently. Presently, it\u2019s implementation is in the beginning stage. In some research of word clustering in English based on preceding and next five words of a key word they found an efficient result. Now, we are trying to implement the tri-gram, 4-gram and 5-gram model of word clustering for Bangla to observe which one is the best among them. We have started our research with quite a large corpus of approximate 1 lakh Bangla words. We are using a machine learning technique in this research. We will generate word clusters and analyze the clusters by testing some different threshold values.", "creator": "Microsoft\u00ae Office Word 2007"}}}