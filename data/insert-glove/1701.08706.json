{"id": "1701.08706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Document Decomposition of Bangla Printed Text", "abstract": "Today vidgen all kind arcona of information is ratomir getting aiva digitized marstrand and battleaxe along with untainted all this digitization, vengi the 98-yard huge hit archive 19th of calceolaria various kinds mullaghmore of brannan documents sarangani is being digitized semi-trailers too. We know that, Optical Character fatimie Recognition is the method incae through 23.1 which, newspapers ravenhill and christian other paper documents convert elaltuntas into malton digital 120.92 resources. declension But, it marett is pilgrim a fact that this method yankalilla works merklein on nestorianism texts maingain only. As sinne a rewrite result, if caray we goldbeck try to 147.6 process expressive any euro350 document which contains 52km non - landy textual zones, then thoreau we engine-powered will 2,172 get garbage magnesia texts kungyangon as nagan output. That is penglai why; rejuvenating in order munto to track-by-track digitize bellus documents 50-54 properly they should be all-embracing prepossessed oxenberg carefully. 105.90 And while reshaping preprocessing, barcelonnette segmenting a document fairer in different regions according to fashanu the 37.50 category doyline properly cuney is most usery important. lobregat But, the Optical myspace.com Character cholan Recognition equestre processes even-toed available for natar Bangla sensitized language have no such algorithm that can kngwarreye categorize a 168.75 newspaper / zaharia book hanneman page fully. erba So rice we petaflops worked to crowland decompose a larnaca document sahng into its moderato several e-melli parts committed like headlines, fdgb sub headlines, columns, laer images indeni etc. And revenue-generating if banni the input is hauke skewed and rotated, then the krais input lyonia was 17:14 also countervailing deskewed ol' and mercanti de - rotated. To voo decompose any Bangla document 6.125 we skateboard found 1,310 out ufp the 13-7 edges jerrycan of the input ld50 image. Then assa we geers find flaccus out estern the horizontal and lehighton vertical mowat area of every nishimoto pixel where it lies mirrabooka in. procaine Later silverfish on wiens the yarnton input image was 39.30 cut according to apriyantono these areas. Then we rede pick 2,366 each extra-parliamentary and trackable every sub keratoconus image and found 34-11 out d'exploitation their synovus height - corleto width quintaglie ratio, line schnapps height. Then surreality according biomedical to these values the sub ashtar images were chignon categorized. selecting To deskew silton the deyoung image activists we found out kuda the manassa skew schizoid angle 1.5465 and de wect skewed the image basc according to delasau this angle. To de - oak rotate the scoutisme image we radar-guided used repacking the meiktila line height, windermere matra squid-like line, pixel canteen ratio of ferruzzi matra mj-12 line.", "histories": [["v1", "Fri, 27 Jan 2017 12:54:52 GMT  (797kb)", "http://arxiv.org/abs/1701.08706v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["md fahad hasan", "tasmin afroz", "sabir ismail", "md saiful islam"], "accepted": false, "id": "1701.08706"}, "pdf": {"name": "1701.08706.pdf", "metadata": {"source": "CRF", "title": "Document Decomposition of Bangla Printed Text", "authors": ["Md. Fahad Hasan", "Tasmin Afroz", "Sabir Ismail", "Saiful Islam"], "emails": [], "sections": [{"heading": null, "text": "Keywords:\n Document\ndecomposition,\nCanny image, Pixel\nvalue according to\nthe horizontal and\nvertical line,\nHorizontal and\nvertical element\nseparators,\nDiscerning\nelements, Auto de-\nskew, Auto rotation.\ntoo. We know that, Optical Character Recognition is the method through which, newspapers and other paper documents convert into digital resources. But, it is a fact that this method works on texts only. As a result, if we try to process any document which contains non-textual zones, then we will get garbage texts as output. That is why; in order to digitize documents properly they should be preprocessed carefully. And while preprocessing, segmenting document in different regions according to the category properly is most important. But, the Optical Character Recognition processes available for Bangla language have no such algorithm that can categorize a newspaper/book page fully. So we worked to decompose a document into its several parts like headlines, sub headlines, columns, images etc. And if the input is skewed and rotated, then the input was also deskewed and de-rotated. To decompose any Bangla document we found out the edges of the input image. Then we find out the horizontal and vertical area of every pixel where it lies in. Later on the input image was cut according to these areas. Then we pick each and every sub image and found out their height-width ratio, line height. Then according to these values the sub images were categorized. To de skew the image we found out the skew angle and de skewed the image according to this angle. To de-rotate the image we used the line height, matra line, pixel ratio of matra line."}, {"heading": "1. INTRODUCTION", "text": "Today all kind of information is getting digitized and along with all this digitization, the huge archive various kinds of documents are being digitized too. It is known that, optical character recognition (OCR) is the main medium through which any document is transformed into digital data. Now it is a fact that, OCR can process texts only and for non-textual input, it will give garbage text output. So, if we want OCR to work perfectly is must be ensured that it is not fed any kind of non-textual input. Therefore, with a view to transforming document archives into digital collections smoothly and properly, documents are to be sectioned and decomposed category wise. Newspaper page has a complex layout. It contains images along with texts. Again, here the textual zones are divided in various parts like headlines, sub-headlines, columns. Though a few works on decomposing Bangla newspaper/document was done but, these approaches works only if the headline is on the top of the columns, most importantly none of them has handled the situation where a document contains images. So keeping all these in mind we proceeded with a bunch of algorithms, to decompose Bangla newspaper/documents section wise properly. In our process, at first we found the edge of the input image by\n\u201cCanny\u201d [3] algorithm. Then we took every pixel of this edge detected image and found out the horizontal and vertical area where it resides. Then converted the input in such a way that, the entire textual zone and image portion became black, and non-textual zones became white. Actually we had to categorize these black portions. This paper is sectioned in following chapters. Chapter 2 sums up some related work on document decomposition and their accuracy. Chapter 3 describes our document decomposition and recognition system, chapter 4 contains our result analysis, and chapter 5 focuses on the future plans and conclusion. For our work, we have used the scanned images collected from some Bangla newspapers."}, {"heading": "2. RELATED WORKS", "text": "Hadjar et al. (2001) worked on newspaper page decomposition [1]. They implemented a split and merge approach.They decomposed English newspapers.\nGatos et al. (1999) also worked on newspaper decomposition and article tracing. Recall and precision of their\ntracing texts and images were about 96% [2] . They moved ahead with a rule based approach to identify the segments of document.\nOmee et al. (2012) worked on decomposing Bangla newspaper. [4] But, their approach works if and only if,\nthere is no image in the input and the headline is on the top of the input.\nBansal et al. also worked on newspaper article extraction. They used hierarchical fixed point model. The accuracy of their labeling headline, sub headline and text blocks form English newspapers are about 96%, 82% and 97% respectively [5] .\nHa et al. (1995) worked on document page decomposition. They applied bounding box projection technique [6]\n.\nGao et al. (2007) also decomposed document [7]\n. They implemented integer linear programming. They did not\nhandle images of the input."}, {"heading": "3. DOCUMENT DECOMPOSITION and RECOGNITION", "text": "Document decomposition is a process where various regions of any scanned image or scanned text document are identified and categorized. Its main aim is to distinguish the textual zone from non-textual zone. Documents\u201f layout may vary depending on the sources (newspaper, text books, magazines etc.) they have been taken from. The key parts of any document are identified through document decomposition. Like for a newspaper key parts are images, headlines, columns, sub-headlines.\nFig.1 Various regions of a document\nFor document decomposition, at first we analysed the layout of taken input. We all know that each page maintains a certain gap between the margins and the textual zone. We used this characteristic of pages to find out the area from where the main document is started. Then we detected the edges of each element. Next,\naccording to some threshold values we converted the edge detected image into an image which contains only white and black pixels. Then we distinguished elements separators. Later, we recognized the elements.\nThe images of any document occupy a certain ratio of height and width, a specific pixel ratio, as well as a particular ratio of the sum of the total length and total width. We specified these characteristics to determine images. When it comes to text zones, we know that the descending order of the height of the letters of any document are headline, sub headline, column. We implemented methodology to calculate the letter height and then recognized the textual zones. Next we worked on de-skewing and de-rotating. A document may start with textual zones or image. Our process worked properly to de-skew and de-rotate whether they start with text or image.\nAll our methods are abridged below."}, {"heading": "3.1 Edge Detection", "text": "At first, we took an input image in form of matrix. Then we found out the edges in the input image and mark them in the output map using the \u201eCanny\u201f algorithm. Before applying this algorithm, it was ensured that, the image and the edges of various portions of the image remains noiseless and fairly sharp respectively. Fig.2 shows the input image on left side and the edge detected image on right side.\nFig.2 Input image and edge detected image"}, {"heading": "3.2 Conversion of Input Image", "text": "In this step, we pick every pixel of the input and assigned them a value according to the length of the vertical and horizontal line it belongs to. That is every pixel of same line will be given a same value. Here we assigned 0 (black) for pixel to the array if the pixel color is not black (>0). Thus we will get an image where, the textual area will be black and the not textual area will be white. The converted image for the Fig.2 is shown in Fig.3.\nFig.3 Converted image"}, {"heading": "3.3 Discerning the Element Separators", "text": "Now, we stored the information of every pixel from the converted image. Like 1 for white colour and 0 for black colour. We picked pixels from the converted image and compared the value of height and width, they are assigned with, with some threshold values to determine whether they belong to non-textual area or not."}, {"heading": "3.4 Segmenting Black Boxes", "text": "In this stage, we stored the top-left and bottom-right coordinates of each black box, neighbouring the horizontal or vertical element separators of the converted image. Then we cut our input according to the coordinates of the black boxes. This cut blocks are send to detect their category."}, {"heading": "3.5 Distinguishing the Elements", "text": ""}, {"heading": "3.5.1 Recognizing Images", "text": "We proposed three filters to define a region as image. First filter checks, whether the length and width of each block is greater than a threshold value or not. Second one finds the pixel ratio of the block and compares them with a threshold value. Third filter determines the ratio of the sum of the total length of each block and total width of each block. The detected images of Fig 2 are shown here.\nFig.4 Detected images"}, {"heading": "3.5.2 Labelling the Text Zones", "text": "To categorise the regions of text zones into headline, sub headline and columns we moved ahead with the methodology of dominant line/letter height. We checked out the line heights of inputs from several \u201eepapers\u201f. We found out the gap between height of dominant letters, height of headlines, and sub headlines. To detect the letter heights perfectly, we removed the image portion from the input after detection. Through those results, we decided some threshold value \u201egap1\u201f, \u201egap2\u201f, \u201ex1\u201f, \u201ex2\u201f, \u201ex3\u201f to label the text regions.\nIf, Line height difference> gap1 && line height>=x3, Then it is labelled as headline. If, Line height difference> gap2 && x1<line height<x2, Then it is decided to be sub headline. Other text zones are columns. In Fig.5, Fig.6 the detected headlines and columns are shown.\nFig.5 Detected headline\nFig.6 Detected colums"}, {"heading": "3.6 Image Auto Rotation and De-skew", "text": "Input may be both skewed and rotated. To solve this problem, we computed the skew angle of the input. Input may contain an image portion at the beginning. For such cases to compute the skew angle perfectly the input was rotated to 90\u00ba, 180\u00ba and -90\u00ba. Then skew angles of all these rotated inputs were calculated. We picked the highest absolute value of skew angles and de-skewed the input according to this value. After this the deskewed image was auto rotated. For this we calculated the line height, matra line, index point, total black pixel on matra line for both the input image and 90\u00ba rotated image. Then we compared the data computed from previous step. We compared the pixel ratio (pixel/line height), and took the bigger value. Now, we checked whether \u201ethe half of the line height\u201f is greater than \u201ethe matra line index\u201f or not. In addition, depending on this we decided whether we have to rotate the image or not. If rotation is needed, we will return 180\u00ba or -90\u00ba, depending on which pixel ratio value we chose.\nWhen the input contains a large image portion at the beginning then, in order to ensure the flawless performance of de-skew and rotation, the line height, matra line, index point, total black pixel on matra line is also calculated for the last line of the input, and 90\u00ba rotated image. In Fig 7, you can see a skewed and rotated input containing image at the beginning, and the de-skewed and de rotated output of it.\nFig.7 Skewed and rotated input, de-skewed and de-rotated output"}, {"heading": "4. RESULT ANALYSIS", "text": "We checked about 70 skewed images. The accuracy of our de- skew system is approximately 100%. We checked 70 skewed as well as rotated images. Then the accuracy was 97.83%. We decomposed about 300 scanned newspaper pages. The precision, accuracy and recall of our methodology are given below."}, {"heading": "5. CONCLUSION", "text": "Documents may also contain table. Still we did not find out the features for tables. So, at this moment if any document contains table, then it is detected as image. So, in future, tables should be detected. And if the noise and images can be detected and removed before de-skew operation then the output will be 100% accurate. For 100% flawless decomposition and recognition, the noise from the input image must be removed."}], "references": [{"title": "Integrated algorithm for newspaper page decomposition and article tracking", "author": ["B. Gatos", "S.L. Mantzaris", "K.V. Chandrinos", "A. Tsigris", "S.J Perantonis"], "venue": "th International Conf. Document Analysis and Recognition,", "citeRegEx": "Gatos et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gatos et al\\.", "year": 2001}, {"title": "An algorithm for headline and column separation in Bangla documents", "author": ["Farjana Yeasmin Omee", "Md. Shiam Shabbir Himel", "Md. Abu Naser Bikas"], "venue": "in Proc. International Symposium on Intelligent Informatics ISI,", "citeRegEx": "Omee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Omee et al\\.", "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "skew, Auto rotation. Abstract: Today all kind of information is getting digitized and along with all this digitization, the huge archive of various kinds of documents is being digitized too. We know that, Optical Character Recognition is the method through which, newspapers and other paper documents convert into digital resources. But, it is a fact that this method works on texts only. As a result, if we try to process any document which contains non-textual zones, then we will get garbage texts as output. That is why; in order to digitize documents properly they should be preprocessed carefully. And while preprocessing, segmenting document in different regions according to the category properly is most important. But, the Optical Character Recognition processes available for Bangla language have no such algorithm that can categorize a newspaper/book page fully. So we worked to decompose a document into its several parts like headlines, sub headlines, columns, images etc. And if the input is skewed and rotated, then the input was also deskewed and de-rotated. To decompose any Bangla document we found out the edges of the input image. Then we find out the horizontal and vertical area of every pixel where it lies in. Later on the input image was cut according to these areas. Then we pick each and every sub image and found out their height-width ratio, line height. Then according to these values the sub images were categorized. To de skew the image we found out the skew angle and de skewed the image according to this angle. To de-rotate the image we used the line height, matra line, pixel ratio of matra line.", "creator": "Microsoft\u00ae Word 2010"}}}