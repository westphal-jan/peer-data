{"id": "1701.08886", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "SenseGen: A Deep Learning Architecture for Synthetic Sensor Data Generation", "abstract": "eldercare Our gubernatorial ability to frescobaldi synthesize sensory data lesja that palestinian preserves specific statistical seventh-century properties malyshev of 452,000 the samity real froglet data sudhir has had tremendous implications konstfack on data oscillatory privacy pintel and starling big mulay data ahsha analytics. The synthetic capadocia data can be used kutina as seated a substitute for evangelica selective real data segments, tokcan that are haeng sensitive coitus to the 103.6 user, kolber thus kalari protecting noades privacy and resulting 70-point in improved bespalov analytics. However, increasingly paddon adversarial kuty roles taken 59.85 by data recipients nyssa such infest as santas mobile camerawoman apps, or tabankin other ocotillo cloud - based analytics services, mandate that fgh the synthetic data, hopping in sealings addition temane to preserving freixenet statistical parlacen properties, should huselius also be difficult 331,000 to distinguish from the handbuilt real data. crimmel Typically, abdennour visual krystof inspection has ki been used 15-16 as a jeron test embroils to distinguish prufrock between pallam datasets. ghadiya But maternelle more recently, joannes sophisticated quercifolia classifier models (discriminators ), dories corresponding white-spotted to hayase a guidoni set of events, opper have also been rath employed pundalik to ghostley distinguish minibar between bioproducts synthesized and coulton real kovanda data. The imjin model operates 5dn on poujade both datasets and armillaria the respective event adeola outputs interclub are hounding compared luxembourg for consistency. yuke In extendicare this paper, we estrangements take ptolemies a 1,921 step geomagnetic towards generating magn\u00fas sensory simpfendorfer data that can pass a deep nashim learning based discriminator model test, 27-billion and ciganlija make angarita two yoshihiko specific contributions: first, ibm-pc we 311,000 present 8-member a 1992-95 deep al-radi learning loveman based architecture http://www.nyc.gov for synthesizing macraes sensory eliza data. This concubines architecture urba comprises of advocaat a generator retailed model, which lightburn is 119.11 a zhongjian stack trique of killswitch multiple Long - margined Short - Term - fluxions Memory (LSTM) borgo networks chelule and p\u00e1ez a bilger Mixture Density Network. second, we clearers use another LSTM network based backed-up discriminator ch\u014dkai model for kerrisdale distinguishing qualms between garroted the true cendon and tomi the synthesized nurse data. shuwen Using a 2,871 dataset sa\u00efda of 1332 accelerometer traces, fiorenza collected soferim using smartphones euro297 of users doing gordie their gastropods daily stamens activities, we 2,317 show albritton that the relief deep padrino learning bola\u00f1os based boorman discriminator rabbet model 45.75 can ulundi only negrito distinguish between the real mangu and synthesized chaid traces with an thaskin accuracy dsm-iv-tr in the neighborhood of rice-growing 50%.", "histories": [["v1", "Tue, 31 Jan 2017 01:59:58 GMT  (483kb,D)", "http://arxiv.org/abs/1701.08886v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["moustafa alzantot", "supriyo chakraborty", "mani b srivastava"], "accepted": false, "id": "1701.08886"}, "pdf": {"name": "1701.08886.pdf", "metadata": {"source": "CRF", "title": "SenseGen: A Deep Learning Architecture for Synthetic Sensor Data Generation", "authors": ["Moustafa Alzantot", "Supriyo Chakraborty", "Mani Srivastava"], "emails": ["malzantot@ucla.edu", "supriyo@us.ibm.com", "mbs@ucla.edu"], "sections": [{"heading": null, "text": "Prior work on data synthesis have often focussed on classifiers that are built for features explicitly preserved by the synthetic data. This suggests that an adversary can build classifiers that can exploit a potentially disjoint set of features for differentiating between the two datasets. In this paper, we take a step towards generating sensory data that can pass a deep learning based discriminator model test, and make two specific contributions: first, we present a deep learning based architecture for synthesizing sensory data. This architecture comprises of a generator model, which is a stack of multiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network (MDN); second, we use another LSTM network based discriminator model for distinguishing between the true and the synthesized data. Using a dataset of accelerometer traces, collected using smartphones of users doing their daily activities, we show that the deep learning based discriminator model can only distinguish between the real and synthesized traces with an accuracy in the neighborhood of 50%.\nI. INTRODUCTION\nA large number of data recipients (e.g., mobile apps, and other cloud-based big-data analytics) rely on the collection of personal sensory data from devices such as smartphones, wearables, and home IoT devices to provide services such as remote health monitoring [1], location tracking [2], automatic indoor map construction and navigation [3] and so on. However, the prospect of sharing sensitive personal data, often prohibits large-scale user adoption and therefore the success of such systems. To circumvent these issues and increase data sharing, synthetic data generation has been used as an alternative to real data sharing. The generated data preserves only the required statistics of the real data (used by the apps to provide service) and nothing else and are used as a substitute for selective real\ndata segments that are sensitive to the user thus protecting privacy and resulting in improved analytics.\nHowever, increasingly adversarial roles taken by the data recipients mandate that the synthetic data, in addition to preserving statistical properties, should also be \u201cdifficult\u201d to distinguish from the real data. Even in non-adversarial settings, analytics services can behave in unexpected ways if the input data is different from the expected data, thereby requiring the synthesized and real datasets to exhibit \u201csimilarity\u201d. Typically, visual inspection has been used as a test to distinguish between datasets. But more recently, sophisticated classifier models (discriminators), corresponding to a set of events, have also been employed to distinguish between synthesized and real data. The model operates on both datasets and the respective event outputs are compared for consistency. In fact, prior work on data synthesis have often focussed on classifiers that are built for features explicitly preserved by the synthetic data. This suggests that an adversary can build classifiers that can exploit a potentially disjoint set of features for differentiating between the two datasets.\nIn this paper, we present SenseGen \u2013 a deep learning based generative model for synthesizing sensory data. While deep learning methods are known to be capable of generating realistic data samples, training them was considered to be difficult requiring large amounts of data. However, recent work on generative models such as Generative Adversarial Networks [4], [5] (GAN) and variational auto-encoders [6], [7] have shown that it is possible to train these models with moderate sized datasets. GANs have proven successful in generating different types of data including photo-realistic high resolution images [8], realistic images from text description [9], and even for new text and music composition [10], [11]. Furthermore, inspired by the architecture of GANs, we also use a deep learning based discriminator model. The goal of the generator model is to synthesize data that can pass the discriminator test that is designed to distinguish between synthesized and real data. Note, unlike prior work on data synthesis, a deep learning based discriminator is not trained on a pre-determined set of features. Instead, it continuously learns the best set of features that can be used to differentiate between the real and synthesized data making it hard for the generator to pass the discriminator test.\nTo summarize, we make two contributions. First, we present a deep learning based architecture for synthesizing sensory\nar X\niv :1\n70 1.\n08 88\n6v 1\n[ cs\n.L G\n] 3\n1 Ja\nn 20\n17\ndata. This architecture comprises of a generator model, which is a stack of multiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network (MDN). Second, we use another LSTM network based discriminator model for distinguishing between the true and the synthesized data. Using a dataset of accelerometer traces, collected using smartphones of users doing their daily activities, we show that the deep learning based discriminator model can only distinguish between the real and synthesized traces with an accuracy in the neighborhood of 50%.\nThe rest of this paper is organized as follows: Section II provides a description for our model architecture and the training algorithm used. This is followed by Section III that describes our experimental design and initial results. Finally, Section IV concludes the paper."}, {"heading": "II. MODEL DESIGN", "text": "Sensors data, e.g. accelerometer, gyroscope, barometer, etc., are represented as a sequence of values x = (x1,x2, ...,xT ) where xi \u2208 Rd, for i = 1, . . . , |T | where d is the dimensionality of the time series (i.e. d = 3 in case of 3-axis accelerometer ) and T is the number of time steps for which the data has been collected.\nSenseGen consists of two deep learning models: \u2022 Generator (G): The generator G is capable of generating\nnew synthetic time series data from random noise input. \u2022 Discriminator (D): The goal of the discriminator D is\nto assess the quality of the examples generated by the generator G.\nBoth G and D are based on recurrent neural network models which have shown a lot of success in sequential data modeling. We describe the model details below.\nAlgorithm 1 Training algorithm 1: for t = 1, 2, . . . , T do 2: Sample Xtrue minibatch from true data 3: Sample Xgen minibatch from the generative model G 4: Train the discriminative model D on the training set\n(Xtrue,Xgen) for 200 epochs 5: Sample another Xtrue minibatch from true data 6: Sample another Xgen minibatch from the generative model G 7: Train the generative model G on the training set (Xtrue) for 100 epochs 8: end for"}, {"heading": "A. Generative Model", "text": "Recurrent neural networks (RNN) are a class of neural networks which are distinguished by having units with feedback cycles which allows the units to maintain a memory of state about the previous inputs. This makes them suitable for handling tasks dealing with sequential time-series inputs. The input time-series is applied to the neural network units one step at time. Each RNN artificial neuron (often called RNN unit or RNN cell) maintains a hidden internal state memory\nht which is updated at each time-step according to the new input xt and previous internal state memory value ht\u22121\nht = \u03c3(Whhht\u22121 +Wxhxt + bh)\nwhere \u03c3(x) is the sigmoid activation function\n\u03c3(x) = 1\n1 + e\u2212x\nAlso each unit generates another time-series of outputs ot as a function of the internal memory state which is computed according to the following equation:\not = tanh(Woht + bo)\nThe set \u03b8 = {Whh,Wxh, bh,Wo, bo} represents the RNN cell parameters. The RNN training algorithm picks the values of \u03b8 that minimizes the defined loss function.\nIn order to handle complex time-series sequences, Multiple RNN units can be used at the same layer and also multiple RNN units can be stacked on top of each other such that the time series of outputs from the RNN units at one layer are used as inputs to the RNN units on top of them. This way, we can design more powerful recurrent neural networks which are both deep and wide. Like other neural networks, we train a recurrent neural networks possible by using a modified version of back-propagation known as back-propagation through time (BPTT) algorithm [12]. However, RNN units suffer from two major problems during training deep models over long timeseries inputs. First, it is the vanishing gradient problem, where the error gradient goes to zero during propagation presenting difficulty while learning the weights of early layers or capturing long-term depedencies. Second, it is the exploding gradient problem, where the gradient value might grow exponentially causing numerical errors in the training algorithm. These two problems present a major hurdle in training RNNs. To solve the exploding gradient problem, the gradient value is clipped at each unit, while modified architectures of RNN units such as the Long Short Term Memory (LSTM) [13] and Gated Recurrent Units (GRU) [14] have been introduced to come over the vanishing gradient problem.\nLSTM units are modified version of the standard RNN units that add three additional gates inside the RNN unit : input gate (it), forget gate (ft) and output gate (ot). The values of these gates are computed as functions of the unit\u2019s internal cell state ct and current input xt. These gates are used to control what information being stored in the unit\u2019s internal memory ht to avoid vanishing gradient problem and become better in remembering sequence dependencies for longer range. The gates, internal memory and LSTM unit output at each time step are computed according to the following equations:\nft = \u03c3(Wxfxt +Whfht\u22121 + bf )\nit = \u03c3(Wxixt +Whiht\u22121 + bi)\not = \u03c3(Wxoxt +Whoht\u22121 + bo)\nct = ft ct\u22121 + it tanh(Whcht\u22121 +Wxcxt + bc)\nht = ot tanh(ct)\nwhere is the elementwise multiplication. In the rest of the paper, we define the function LSTM that maps the current input xt and current LSTM unit output ht to new output as an abstraction of the previous LSTM update equations.\nht = LSTM(xt, ht\u22121)\nLike the standard RNN units, LSTM units can also be stacked on top of each other in order to model complex time-series data. We use LSTMs in our model because they are successful in modeling sequences with long-term dependencies.\nRecurrent Neural networks can be used for the generation of a sequence with any length by predicting the sequence one step at a time. At each time step, the network output yt is used to define a probability distribution for the next step xt+1 value.\nxt+1 \u223c pr(xt+1|yt)\nThe value xt+1 is then fed back into the model as a new input to predict another time step. By repeatedly doing this, it is theoretically possible to generate a sequence of any length. However, the choice of output distribution becomes critical and must be chosen carefully to represent the type of data we are generating. The simplest choice that we consider the output yt as the next step sample xt+1 = yt. and then we define the loss as the root mean squared difference between the sequence of inputs and the sequence of predictions.\nLG(\u03b8G) = T\u2211\nt=1\n(xt \u2212 yt)2\nThen we train the whole model by using gradient descent to minimize the loss value. However, we find this setup to be incapable of generating good sensory data sequences for the following reasons: \u2022 Since all RNN update equations are deterministic, this\nmeans that if you try generating sequences from a given start input value x0 (usually starting by zero) the model will generate the same sequence again at every-time. \u2022 Assigning the model output as the next sample means that the next sample distribution is a uni-model distribution with zero variance. Because for sensory data at a given step more than one value can be a good choice for the next step a uni-modal prediction is not enough. A more flexible generation of sensory data requires probabilistic sampling from a multi-modal distribution on top of the RNN.\nAs a solution for these issues, we use Mixture Density Network (MDN) [15]. Mixture density network is a combination of a neural network and mixture distribution. The outputs of neural network are used to specify the weights of the mixtures and the parameters of each distribution. [16] shows how MDN with Gaussian mixture model (GMM) defined on top of a recurrent neural network is successful in learning how to generate highly realistic handwriting by predicting the pen location one point at a time.\nOur generative model architecture is shown in Figure II-A. At the bottom we have a stack of 3 layers (l(1)t , l (2) t , l (3) t ) of LSTM units. Each layer has 256 units.\nl (1) t = LSTM(l (1) t\u22121, xt)\nl (2) t = LSTM(l (2) t\u22121, l (1) t )\nl (3) t = LSTM(l (2) t\u22121, l (1) t )\nThe output from the last LSTM layer is feed into a fully connected layer with 128 units with sigmoid activations.\nl (4) t = \u03c3(W4l (3) t + b4)\nwhere W4 \u2208 R256x128, b4 \u2208 R128 The final layer is another fully connected layer with 72 output units.\nl (5) t = \u03c3(W5l (4) t + b5)\nwhere W5 \u2208 R128x72, b5 \u2208 R72. The outputs from the last layer l(5)t are used as the weights and parameters of the output Gaussian mixture model (GMM).\n\u03c0t(x1..t) = softmax(l (5) t [1...24])\n\u00b5t(x1..t) = l (5) t [25...48]\n\u03c3t(x1..t) = e (l\n(5) t [49...72])\nwhere the softmax function:\nsoftmax(xk) = exk\u221124 j=1 e xj\nis used to ensure that weights defined by pit are normalized (i.e\u221124 k=1 \u03c0k = 1), and the exponential function while computing the standard deviation of the Gaussians \u03c3t is meant to ensure that the \u03c3t is positive. The mixture weights \u03c0t, guassian means \u00b5t, and standard deviations \u03c3t are used to define to a probability distribution for the next output\npr(xt+1|\u03c0t, \u00b5t, \u03c3t) = 24\u2211 k=1 \u03c0kt (x1..t) \u2217 N (xt+1;\u00b5kt (x1..t), \u03c3kt (x1..t))\nfrom which we can sample the predicted next step value. xt+1 \u223c pr ( xt+1|\u03c0t(x1..t), \u00b5t(x1..t), \u03c3t(x1..t) ) The whole model is trained end-to-end by RMSProp [17] and truncated back-propagation through time with a cost function L(\u03b8G) defined to increase the likelihood of generating the next timestep value. This is the equivalent to minimizing the negative log likelihood L(\u03b8G) with respect to the set of generative model parameters \u03b8G .\nLG(\u03b8G) = \u2212 T\u2211\nt=1\nlog (pr(xt+1|\u03c0t(x1..t), \u00b5t(x1..t), \u03c3t(x1..t)))"}, {"heading": "B. Discriminative Model", "text": "In order to quantify the similarity between the generated time-series and the real sensor timeseries collected from users. We build another model D whose goal is to distinguish between samples generated by G. The discriminative model D is trained to distinguish between the samples coming from the dataset for real sensor traces Xtrue and others samples from the dataset Xgen which is generated by the model G.\nThe architecture of model D consists of a layer of 64 LSTM units followed by a fully connected layers with 16 hidden units using sigmoid activation function and an output layer with a single unit with sigmoid activation function. The output value this discriminative model when a given an input sensor values timeseries xtest is interpreted as the probability that the given input timeseries is coming from the real dataset Xtrue.\nD(xtest) = Pr(xtest \u2208 Xtrue)\nWe train the model D in a supervised way by using a training data consists from mini-batchs of m samples from the real data dataset Xtrue with their target output = 1, and other mini-batchs of m samples generated from the the model G with their target output = 0. Each samples is a time series of 400 steps. The training aims to minimize the cross-entropy loss LD with respect to the set of discriminitive model parameters.\nLD(\u03b8D) = \u2212 ( m\u2211 i=1 log (D(X (i)true)) + log (1\u2212D(X (i)gen)) )"}, {"heading": "III. RESULTS AND ANALYSIS", "text": "For our experiments and evaluation studies, We use the Human Activity Recognition database [18] as our training data. The HAR database contains accelerometer and gyroscope recordings of 30 individuals while performing activities of daily living (ADL) (Walking, walking upstairs, walking downstairs, sitting, standing, and laying). Accelerometer and gyroscope were collected at 50Hz from a Samsung Galaxy SII phone attached to the user\u2019s the waist. The accelerometer and gyroscope values were pre-processed to compute the linear acceleration (by removing the gravity component). We train the deep learning model using Google TensorFlow [19] deep learning framework r0.11 on Nvidia GTX Titan X GPU with 3,584 CUDA cores running 11 TFLOPS with 12 GB Memory. The training takes about 5 hours until the generative model converges after 20,000 epochs when trained on a time-series of 7000 time steps.\nEvaluating a generative model is challenging because it is hard to find one metric that quantifies how realistic the output looks and also how novel is it compared to the training data (to avoid the trap of having a model that just remembers the input training data and outputs it again). These metrics should be specific according to the type of the data the model is trained on. Prior work on generative models for images resort to human judgment of output samples quality. In our work, we use the following methods for qualification:\nGenerative loss during training We show how the loss of the generative model goes down while training. Figure III\nshows the negative log likelihood cost of the generative model LG during training. This means that the model is becoming better in assigning higher probability for the true next-step values during prediction.\nGenerative loss during training Figure 2 shows a visual comparison between 4 random samples generated by generative model G and 4 random subset of real accelerometer timeseries values from the HAR dataset.\nIndistinguishability between synthesized and real data samples We use another deep learning model D whose is training to quantify the differences between the real samples and the synthesized samples. Figure 4 shows how the accuracy of this model goes down as training continues. At the beginning the accuracy in deciding whether input samples are synthesized is almost 100% However, as we train the models for more epochs, the accuracy of model D in identifying the synthetic samples reduces to around 50%."}, {"heading": "IV. CONCLUSION", "text": "In this paper, we outlined our initial experiences of using a deep learning based architecture for synthesizing time series of sensory data. We identified that the synthesized data should be able to pass a deep learning based discriminator test designed to distinguish between the synthesized and true data. We then demonstrated that our generator can be successfully used to beat such a discriminator by restricting its accuracy to around 50%.\nOur generator-discriminator model pair is a GAN-similar architecture. However, due to the difficulties of doing backpropagation through the MDN-based stochastic network, we do not yet incorporate adversarial training by feeding back the discriminator output into the generator training. we hope to close the feedback loop between the discriminator and the generator model for synthesizing even more effective data samples."}, {"heading": "ACKNOWLEDGEMENT", "text": "This research was sponsored by the U.S. Army Research Laboratory and the U.K. Ministry of Defence under Agreement Number W911NF-16-3-0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army Research Laboratory, the U.S. Government, the U.K. Ministry of Defence or the U.K. Government. The U.S. and U.K. Governments are authorized to reproduce and distribute reprints for Government purposes notwithstanding any copy-right notation hereon."}], "references": [{"title": "Fitbit R  \u00a9: An accurate and reliable device for wireless physical activity tracking.", "author": ["K.M. Diaz", "D.J. Krupka", "M.J. Chang", "J. Peacock", "Y. Ma", "J. Goldsmith", "J.E. Schwartz", "K.W. Davidson"], "venue": "International journal of cardiology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "No need to war-drive: unsupervised indoor localization", "author": ["H. Wang", "S. Sen", "A. Elgohary", "M. Farid", "M. Youssef", "R.R. Choudhury"], "venue": "Proceedings of the 10th international conference on Mobile systems, applications, and services. ACM, 2012, pp. 197\u2013210.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Crowdinside: automatic construction of indoor floorplans", "author": ["M. Alzantot", "M. Youssef"], "venue": "Proceedings of the 20th International Conference on Advances in Geographic Information Systems. ACM, 2012, pp. 99\u2013108.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2672\u2013 2680.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Nips 2016 tutorial: Generative adversarial networks", "author": ["I. Goodfellow"], "venue": "arXiv preprint arXiv:1701.00160, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 2226\u20132234.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Photo-realistic single image super-resolution using a generative adversarial network", "author": ["C. Ledig", "L. Theis", "F. Husz\u00e1r", "J. Caballero", "A. Cunningham", "A. Acosta", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang"], "venue": "arXiv preprint arXiv:1609.04802, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial text to image synthesis", "author": ["S. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"], "venue": "arXiv preprint arXiv:1605.05396, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "arXiv preprint arXiv:1609.05473, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating sentences from a continuous space", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": "arXiv preprint arXiv:1511.06349, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Gated feedback recurrent neural networks", "author": ["J. Chung", "C. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1502.02367, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": "1994.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, no. 2, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine", "author": ["D. Anguita", "A. Ghio", "L. Oneto", "X. Parra", "J.L. Reyes-Ortiz"], "venue": "International Workshop on Ambient Assisted Living. Springer, 2012, pp. 216\u2013223.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": ", mobile apps, and other cloud-based big-data analytics) rely on the collection of personal sensory data from devices such as smartphones, wearables, and home IoT devices to provide services such as remote health monitoring [1], location tracking [2], automatic indoor map construction and navigation [3] and so on.", "startOffset": 224, "endOffset": 227}, {"referenceID": 1, "context": ", mobile apps, and other cloud-based big-data analytics) rely on the collection of personal sensory data from devices such as smartphones, wearables, and home IoT devices to provide services such as remote health monitoring [1], location tracking [2], automatic indoor map construction and navigation [3] and so on.", "startOffset": 247, "endOffset": 250}, {"referenceID": 2, "context": ", mobile apps, and other cloud-based big-data analytics) rely on the collection of personal sensory data from devices such as smartphones, wearables, and home IoT devices to provide services such as remote health monitoring [1], location tracking [2], automatic indoor map construction and navigation [3] and so on.", "startOffset": 301, "endOffset": 304}, {"referenceID": 3, "context": "However, recent work on generative models such as Generative Adversarial Networks [4], [5] (GAN) and variational auto-encoders [6], [7] have shown that it is possible to train these models with moderate sized datasets.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "However, recent work on generative models such as Generative Adversarial Networks [4], [5] (GAN) and variational auto-encoders [6], [7] have shown that it is possible to train these models with moderate sized datasets.", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "However, recent work on generative models such as Generative Adversarial Networks [4], [5] (GAN) and variational auto-encoders [6], [7] have shown that it is possible to train these models with moderate sized datasets.", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "However, recent work on generative models such as Generative Adversarial Networks [4], [5] (GAN) and variational auto-encoders [6], [7] have shown that it is possible to train these models with moderate sized datasets.", "startOffset": 132, "endOffset": 135}, {"referenceID": 7, "context": "GANs have proven successful in generating different types of data including photo-realistic high resolution images [8], realistic images from text description [9], and even for new text and music composition [10], [11].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "GANs have proven successful in generating different types of data including photo-realistic high resolution images [8], realistic images from text description [9], and even for new text and music composition [10], [11].", "startOffset": 159, "endOffset": 162}, {"referenceID": 9, "context": "GANs have proven successful in generating different types of data including photo-realistic high resolution images [8], realistic images from text description [9], and even for new text and music composition [10], [11].", "startOffset": 208, "endOffset": 212}, {"referenceID": 10, "context": "GANs have proven successful in generating different types of data including photo-realistic high resolution images [8], realistic images from text description [9], and even for new text and music composition [10], [11].", "startOffset": 214, "endOffset": 218}, {"referenceID": 11, "context": "Like other neural networks, we train a recurrent neural networks possible by using a modified version of back-propagation known as back-propagation through time (BPTT) algorithm [12].", "startOffset": 178, "endOffset": 182}, {"referenceID": 12, "context": "To solve the exploding gradient problem, the gradient value is clipped at each unit, while modified architectures of RNN units such as the Long Short Term Memory (LSTM) [13] and Gated Recurrent Units (GRU) [14] have been introduced to come over the vanishing gradient problem.", "startOffset": 169, "endOffset": 173}, {"referenceID": 13, "context": "To solve the exploding gradient problem, the gradient value is clipped at each unit, while modified architectures of RNN units such as the Long Short Term Memory (LSTM) [13] and Gated Recurrent Units (GRU) [14] have been introduced to come over the vanishing gradient problem.", "startOffset": 206, "endOffset": 210}, {"referenceID": 14, "context": "As a solution for these issues, we use Mixture Density Network (MDN) [15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "[16] shows how MDN with Gaussian mixture model (GMM) defined on top of a recurrent neural network is successful in learning how to generate highly realistic handwriting by predicting the pen location one point at a time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The whole model is trained end-to-end by RMSProp [17] and truncated back-propagation through time with a cost function L(\u03b8G) defined to increase the likelihood of generating the next timestep value.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "For our experiments and evaluation studies, We use the Human Activity Recognition database [18] as our training data.", "startOffset": 91, "endOffset": 95}], "year": 2017, "abstractText": "Our ability to synthesize sensory data that preserves specific statistical properties of the real data has had tremendous implications on data privacy and big data analytics. The synthetic data can be used as a substitute for selective real data segments \u2013 that are sensitive to the user \u2013 thus protecting privacy and resulting in improved analytics. However, increasingly adversarial roles taken by data recipients such as mobile apps, or other cloud-based analytics services, mandate that the synthetic data, in addition to preserving statistical properties, should also be \u201cdifficult to distinguish from the real data. Typically, visual inspection has been used as a test to distinguish between datasets. But more recently, sophisticated classifier models (discriminators), corresponding to a set of events, have also been employed to distinguish between synthesized and real data. The model operates on both datasets and the respective event outputs are compared for consistency. Prior work on data synthesis have often focussed on classifiers that are built for features explicitly preserved by the synthetic data. This suggests that an adversary can build classifiers that can exploit a potentially disjoint set of features for differentiating between the two datasets. In this paper, we take a step towards generating sensory data that can pass a deep learning based discriminator model test, and make two specific contributions: first, we present a deep learning based architecture for synthesizing sensory data. This architecture comprises of a generator model, which is a stack of multiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network (MDN); second, we use another LSTM network based discriminator model for distinguishing between the true and the synthesized data. Using a dataset of accelerometer traces, collected using smartphones of users doing their daily activities, we show that the deep learning based discriminator model can only distinguish between the real and synthesized traces with an accuracy in the neighborhood of 50%.", "creator": "TeX"}}}