{"id": "1701.08936", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "Deep Reinforcement Learning for Visual Object Tracking in Videos", "abstract": "Convolutional neural p500 network (e911 CNN) ikard models have achieved ateneo tremendous indexicality success 6,001 in many marantaceae visual satins detection a-month and recognition whitsuntide tasks. yelm Unfortunately, sodam visual onomatopoeic tracking, 100-calorie a railwaymen fundamental blase computer shraddha vision suzerains problem, is not twentieth-century handled kandi well http://nytsyn.com using 33-minute the existing CNN pud models, televisions because chitty most 24-member object prinsengracht trackers cedric implemented with chinookan CNN do jankelowitz not mooing effectively qureshi leverage 35,300 temporal unscripted and ryokans contextual information ulfilas among consecutive frames. piously Recurrent t-bag neural frimont network (h9n2 RNN) riepe models, copyists on the other hand, are pythagoreans often pilecon used janakantha to process aish text suppletive and voice vojin data due pre-reformation to 2-45 their stremme ability residentially to learn intrinsic representations novelists of sequential and herbivory temporal data. indecisively Here, we propose kozeny a cartwheeled novel kendricks neural reliever network tracking todar model that 6,400 is capable of integrating information over time easingwold and tracking stiner a selected target in seron video. It comprises hoque three components: a glickman CNN territoires extracting best tracking revati features in veer each video cornett frame, an RNN constructing 3-of-4 video memory 2002/3 state, franco-prussian and a reinforcement learning (red-eared RL) mwinyi agent isikoff making selectnet target location onuma decisions. The tracking synchronicity problem is formulated cassandre as killshot a decision - col101 making harlots process, sill and our castlebar model slatington can tarkanian be alona trained 92.93 with 85.72 RL rhoemetalces algorithms eyaktek to learn bona-fide good tracking policies that pay attention hyperpigmentation to luman continuous, inter - ebz frame 54.46 correlation peridium and 1-64 maximize tracking lolled performance marse in siefer the korkmaz long run. acla We compare vinayagamoorthy our kabary model xanthos with mazzara an shrinathji existing neural - network badonkadonk based tracking soldan method 4,058 and 24,583 show hirado that the animators proposed tracking zoisite approach works well brighstone in chouhan various ranchland scenarios presbyterians by performing rigorous godward validation experiments on artificial video dando sequences with ground truth. To the best of our knowledge, geodesist our tracker is sertanejo the hfp first boneshaker neural - network tracker c\u00e9spedes that combines mitha convolutional then-no and broward recurrent networks self-produced with bbfc RL quelling algorithms.", "histories": [["v1", "Tue, 31 Jan 2017 07:48:56 GMT  (5251kb,D)", "http://arxiv.org/abs/1701.08936v1", null], ["v2", "Mon, 10 Apr 2017 20:34:43 GMT  (3619kb,D)", "http://arxiv.org/abs/1701.08936v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["da zhang", "hamid maei", "xin wang", "yuan-fang wang"], "accepted": false, "id": "1701.08936"}, "pdf": {"name": "1701.08936.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning for Visual Object Tracking in Videos", "authors": ["Da Zhang", "Hamid Maei", "Xin Wang", "Yuan-Fang Wang"], "emails": ["yfwang}@cs.ucsb.edu", "hamid.maei@samsung.com"], "sections": [{"heading": "1. Introduction", "text": "Given some object of interest marked in one frame of a video, the goal of single-object tracking is to locate this object in subsequent video frames, despite object motion, changes in the camera\u2019s viewpoint, and other incidental environmental variations such as lighting and shadows.\nSingle-object tracking finds applications in many important scenarios. For autonomous driving, a tracker must follow dynamic objects in order to estimate how they are moving and predict where they will end up in the future. For security applications, a surveillance camera must track numerous people as they move through the environments.\nCNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19]. Such great success of CNNs is mostly attributed to their outstanding performance in representing spatial information, often identified from collections of unrelated, discrete image frames.\nVisual tracking, however, has seen less success using this line of attack since it is often difficult to learn a robust spatial and temporal representation for continuous video data. Recently, some initial works have been done in using neural networks for visual tracking, and these efforts have produced some impact and improved state-of-the-art accuracy [29, 41, 40]. Although these methods may be sufficient to predict a target\u2019s bounding box locations, these classification-based approaches could be limited in learning robust tracking features and maximizing long-term tracking performance without taking temporal coherency and correlation into account. That is, training algorithms specialized for long-term visual tracking is still not fully explored.\nTo fully exploit the spatial and temporal information in videos for visual tracking, it is then desirable to add continuous, recurrent links to augment discrete, convolutional links and use a reinforcement algorithm to maximize tracking performance in the long run. Here, we propose a novel framework which processes sequential video frames as a whole and directly outputs location predictions of the target object in each frame. Our model integrates convolutional network with recurrent network (Figure 1), and builds up an internal representation of the video. It fuses past recurrent memories with current visual features to make\n1\nar X\niv :1\n70 1.\n08 93\n6v 1\n[ cs\n.C V\n] 3\n1 Ja\nn 20\npredictions of the target object\u2019s location over time. We describe an end-to-end optimization procedure that allows the model to be trained to maximize tracking performance in the long run. This procedure uses backpropagation to train the neural-network components and REINFORCE algorithm [42] to train the policy network.\nOur algorithm augments traditional CNN with a recurrent convolutional model learning temporal representations and an RL unit making optimal tracking decisions. The main contributions of our work are:\n\u2022 We propose a convolutional recurrent neural network model, which learns robust spatial representations in each single frame and temporal representations cross multiple frames. The learned features better capture temporal information in video and can be directly applied in a tracking problem.\n\u2022 Our framework is trained end-to-end with deep RL algorithms, in which the model is optimized to maximize a tracking performance measure which depends on the entire video sequence.\n\u2022 Our model is trained fully off-line. This off-line training strategy is advantageous in test time, as there is no need to fine-tune the system online to gain efficiency.\n\u2022 We have validated our algorithms by comparing with an existing neural-network based method and perform-\ning rigorous experiments on artificial video sequences to demonstrate both the validity and applicability.\nThe rest of the paper is organized as follows. We first review related work in Section 2, and discuss our RL approach for visual tracking in Section 3. Section 4 describes our end-to-end optimization algorithm, and Section 5 demonstrates the experimental results."}, {"heading": "2. Related Work", "text": ""}, {"heading": "2.1. Visual Tracking Algorithms", "text": "Visual Tracking is a fundamental problem in computer vision and has been actively studied for decades. Many methods have been proposed for single-object tracking. For a systematic review and comparison, we refer the readers to a recent benchmark and a tracking challenge report [43, 21]. Generally speaking, most existing trackers belong to either one of two categories: generative trackers and discriminative trackers. Generative methods describe the target appearances using generative models and search for candidate image regions that fit the model best. Some representative methods are based on principal component analysis [32], and sparse representation [26, 44]. On the other hand, discriminative trackers learn to separate the foreground from the background using a classifier. Many advanced machine learning algorithms have been used, including online boosting [11, 12], structured output SVM [14], and multiple-\ninstance learning [2]. However, almost all existing appearance-based tracking methods rely on low-level, hand-crafted features which are incapable of capturing semantic information of the targets, not robust to significant appearance changes, and only have limited discriminative power."}, {"heading": "2.2. Convolutional Neural Networks", "text": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19]. Krizhevsky et al. [22] brought significant performance improvement in image classification by training a deep CNN on ImageNet dataset [6]. Region CNN [10] applied a CNN to an object detection task, which can accurately locate and classify objects in natural images.\nDespite the impressive success of CNNs applied to still images, the application of deep neural network (DNN) models in visual tracking is not fully explored. Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29]. An early tracking algorithm transferred CNNs pretrained on a large-scale dataset constructed for image classification [41]. Although [24] proposed an online learning method based on a pool of CNNs, its accuracy is not particularly better than that of the methods based on hand-crafted features. A recent approach [29] trained a multi-domain neural network and achieved great performance improvement. Unfortunately, existing DNN-based trackers are either very slow at test time due to intensive computational overhead, or they don\u2019t effectively leverage cross-frame information in training videos and thus result in bad performance. Our tracker explores temporal information through an RNN and can run very efficiently by performing only a forward pass at run time."}, {"heading": "2.3. Recurrent Neural Networks", "text": "RNNs are powerful learning models that are often used for learning sequential tasks. Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27]. The standard RNN cell consists of an input, a hidden and an output layer as illustrated in Figure 2.\nBesides traditional RNN, the application of recurrent networks with sophisticated hidden units, such as Long Short-Term Memory (LSTM) [17] or Gated Recurrent Unit (GRU) [5], has become common in recent years. Mnih et al. [27] applied LSTM in visual attention problems by training a decision making network, which sufficiently improves computational efficiency for computer vision tasks. In recent works, Donahue et al. [8] proposed a class of recurrent convolutional architectures which are suitable for\nlarge-scale visual understanding tasks, and demonstrated the value of these models for activity recognition, image captioning, and video description. The LSTM cell used in this paper is shown in Figure 2."}, {"heading": "2.4. Deep Reinforcement Learning", "text": "RL is a learning method based on trail and error, where an agent does not necessarily have a prior knowledge about which is the correct action to take. The underlying model that RL learns is a Markov Decision Process (MDP): An agent interacts with the environment and selects an action. Applying the action at a single state, and the environment emits a new state and a reward signal. In order to maximize the expected rewards in long term, the agent learns the best policy to take actions [36].\nSome works in computer vision literature and elsewhere e.g., [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task. RL is capable of solving sequential decision making tasks especially with the help of good environment representations learned by DNN models. Some recent works included training an RL agent to play Atari games from raw input pixels [28], and a recurrent visual attention model [27] to reduce computation for computer vision tasks. Our work is similar to the attention model described in [27], but we designed our own network architecture especially for solving the visual tracking problem by combining CNN, RNN and RL algorithms.\nOur work formulates visual tracking as a sequential decision making process and leverages RL to solve this problem. We describe an end-to-end optimization procedure that allows the model to be trained directly with respect to a visual tracking task and to maximize a long-term tracking performance measure which depends on the entire sequence of predictions made by the model. We will describe our framework in detail in next section."}, {"heading": "3. Deep RL Agent", "text": "In this paper we formulate the tracking problem as the sequential decision process of a goal-directed agent interacting with the visual environment: As shown in Figure 3, the visual environment is a sequence of video frames, the agent is modeled as our neural network, and the decision or action is to decide the location of target object. At each point in time, the agent observes the environment and extracts representative features from observations. Since the agent only observes one image frame at a time, it needs to integrate information over time in order to determine how to take actions most effectively. At each step, the agent receives a scalar reward which depends on its actions, and the goal of the agent is to maximize the total long-term rewards. In our system, the agent is built around an RNN as shown in Figure 1. At each time step, it processes the input frame,\nintegrates information over time, and chooses how to act correspondingly.\nIn more detail, the proposed deep RL agent consists of three components: the feature extractor (processes input frame and generates feature representations), the memory component (integrates information overtime and formulates and updates the internal state) and the RL unit (chooses how to act and maximizes the overall reward). These are described blow."}, {"heading": "3.1. Feature Extractor", "text": "At each time step t the agent observes the environment in the form of an image xt, on which the feature extractor computes a representation. This representation can be as simple as a linear transformation or a more sophisticated feature computed by a CNN, and is used by other components to integrate sequential information and take appropriate actions. The extracted features are denoted as:\nit = fi(xt;Wi) (1)\nwhere it is a function of xt given parameters Wi, and the functional relationship is defined by fi.\nThe same feature extractor will be applied to every single frame in an video, thus the feature extractor has to be generic enough to capture representative features for different objects in different contexts. Given varying tracking tasks, the feature extractor can either be pre-trained, transferred from other models, or initialized and trained from scratch. To make the generic extractor adaptive over time, its parametersWi will continue to be updated during end-toend training which provides better descriptors to maximize the overall rewards."}, {"heading": "3.2. Memory Component", "text": "The memory component maintains an internal memory state which summarizes information extracted from the history of past observations. This hidden state encodes the knowledge of the environment and is fundamental for the agent to take actions.\nAt one single time step t, the feature vector of the input frame it is fed into an RNN. The recurrent neural network updates its internal memory vector ht based on the previous memory vector ht\u22121 and the current input feature it provided by the feature extractor:\nht = fh(ht\u22121, it;Wh) (2)\nwhere fh is a recurrent transformation function such as GRU, LSTM or a simple logistic function, parameterized by the parameter Wh.\nWe use the LSTM cell in Figure 2 which has proven to be better at handling long-term and short-term connections, and robust for processing different sequential data."}, {"heading": "3.3. RL Unit", "text": "The instrumental component is an RL unit taking actions based on the internal memory state and receiving rewards from the environment. At each time step t, the agent performs an action to locate the target object lt. In this work, the location actions are chosen stochastically from a distribution parameterized by RL unit fl(ht;Wl) at time t:\nlt \u223c p(\u00b7|fl(ht;Wl)) (3)\nwhere p is the probability distribution function determined by fl(ht;Wl), the external input is the internal memory state ht and fl is the functional relationship defined by the RL unit.\nMore specifically, the distribution p(\u00b7|fl(ht;Wl)) is defined as a multi-variate Gaussian. The variance of the Gaussian is fixed and decided by our experiments, while the mean value \u00b5 = fl(ht;Wl) is the output of the RL unit. Given different tracking tasks, the output of the RL unit can either be the center of a target object (xt, yt), a circle (xt, yt, rt) centered at (xt, yt) with radius rt or a rectangular bounding box (xt, yt, wt, ht) whose top left corner is (xt, yt) and width and height are wt and ht respectively.\nDuring training, the agent will receive a reward signal rt from the environment after executing an action at time t. In this work, the scalar reward signal is defined as follows:\nrt = \u2212avg(|lt \u2212 gt|)\u2212max(|lt \u2212 gt|) (4)\nwhere lt is the location outputted by RL unit, gt is the target ground truth at time t, avg(\u00b7) computes the mean value of a given matrix and max(\u00b7) computes the maximum value.\nThe training objective is to maximize the sum of the reward signal: R = \u2211T t=1 rt. By definition, the reward in Equation 4 measures the closeness between predicted location lt and ground-truth location gt. Our training algorithm is designed to maximize R towards 0 such that both average and maximum errors made by the deep RL agent is minimized. We will discuss training details in the next section."}, {"heading": "4. Training the Deep RL Agent", "text": "Training this network to maximize the overall tracking performance is a non-trivial task, and we leverage the REINFORCE algorithm [42] from the RL community to solve this problem."}, {"heading": "4.1. Gradient Approximation", "text": "Our deep RL agent is parameterized by W = {Wi,Wh,Wl} and we aim to learn these parameters to maximize the total tracking reward the agent can expect when taking different actions. More specifically, the objective of the agent is to learn a policy function \u03c0(lt|z1:t;W ) with parameters W that, at each step t, maps the history of past interactions with the environment z1:t =\nx1, l1, ...lt\u22121, xt (a sequence of past observations and actions taken by the agent) to a distribution over actions for the current time step. Here, the policy \u03c0 is defined by our neural network architecture, and the history of interactions z1:t is summarized in the internal memory state ht. For simplicity, we will use Zt = z1:t to indicate all histories up to time t, thus, the policy function can be written as \u03c0(lt|Zt;W ).\nTo put it in a formal way, the policy of deep RL agent \u03c0(lt|Zt;W ) induces a distribution over possible interactions Zt and we aim to maximize the total reward R under this distribution, thus, the objective is defined as:\nG(W ) = Ep(z1:T ;W )[ T\u2211 t=1 rt] = Ep(ZT ;W )[R] (5)\nwhere p(z1:T ;W ) is the distribution over possible interactions parameterized by W .\nThis formulation involves an expectation over highdimensional interactions which is hard to solve in traditional supervised manner. Here, we bring techniques from the RL community to solve this problem, as shown in [42], the gradient can be first simplified by taking the derivative over log-probability of the policy function \u03c0:\n\u2207WG = T\u2211 t=1 Ep(ZT ;W )[\u2207W ln\u03c0(lt|Zt;W )R] (6)\nand the expectation can be further approximated by an episodic algorithm: since the action is drawn from probabilistic distributions, one can execute the same policy for many episodes and approximate expectation by taking the average, thus\n\u2207WG \u2248 1\nN N\u2211 i=1 T\u2211 t=1 \u2207W ln\u03c0(lit|Zit ;W )Ri (7)\nwhere Ris are cumulative rewards obtained by running the current policy \u03c0 for N episodes, i = 1...N .\nThe above training rule is known as the episodic REINFORCE [42] algorithm, and it involves running the deep RL agent with its current policy to obtain samples of interactions and then updating parameters W of the agent such that the log-probability of chosen actions that have led to high overall rewards is increased.\nIn practice, although Equation 7 computes a good estimation of the gradient \u2207WG, when applied to train the deep RL agent, the training process is hard to converge due to the high variance of this gradient estimation. Thus, in order to obtain an unbiased low-variance gradient estimation, a common method is to subtract a reinforcement baseline from the cumulative rewards R:\n\u2207WG \u2248 1\nN N\u2211 i=1 T\u2211 t=1 \u2207W ln\u03c0(lit|Zit ;W )(Rit \u2212 bt) (8)\nwhere bt is called reinforcement baseline in the RL literature, it is natural to select bt = E\u03c0[Rt], and this form of baseline is known as the value function [37]. This estimation maintains the same expectation with Equation 7 while sufficiently reduces the variance."}, {"heading": "4.2. Training with Backpropagation", "text": "The only remaining part to compute the gradient in Equation 8 is to compute the gradient over log-probability of the policy function \u2207W ln\u03c0(lt|Zt;W ). To simplify notation, we focus on one single time step and omit usual unit index subscript throughout. In our network design, the policy function \u03c0 outputs the target location l which is drawn from a Gaussian distribution centered at \u00b5 with fixed variance \u03c3, and \u00b5 is the output of the deep RL agent parameterized by W . The density function g determining the output l on any single trial is given by:\ng(l, \u00b5, \u03b4) = 1\n(2\u03c0) 1 2\u03c3 e\u2212\n(l\u2212\u00b5)2\n2\u03c32 (9)\nBased on REINFORCE algorithm [42], the gradient of the policy function with respect to \u00b5 is given by the gradient of the density function:\n\u2207\u00b5 ln\u03c0 = \u2202 ln g \u2202\u00b5 = l \u2212 \u00b5 \u03c32\n(10)\nsince \u00b5 is the output of deep RL agent parameterized by W , the gradients with respect to network weights W can be easily computed by standard backpropagation."}, {"heading": "4.3. Overall Procedure", "text": "The overall procedure of our training algorithm is presented in Algorithm 1. the network parameters W are first randomly initialized to define our initial policy. Then, we take first T frames from one training video to be the input of our network. We execute current policy N times, compute gradients and update network parameters. Next, we take consecutive T frames from the same video and apply the same training procedure. We repeat this for all training videos in our dataset, and we stop when we reach the maximum number of epochs or the cumulative reward ceases to increase.\nDuring testing, the network parameters W are fixed and no online fine-tuning is needed. The procedure at test time is as simple as computing one forward pass of our deep RL agent, i.e., given a test video, the deep RL agent predicts the location of target object in every single frame by sequentially processing the video data."}, {"heading": "5. Experiments", "text": "We evaluated the proposed approach of visual object tracking on artificially generated datasets. We varied the\nAlgorithm 1 Deep RL agent training algorithm Input: Training videos {v1, ..., vM} with ground-truth Output: Network weights W\n1: Randomly initialize weights Wi, Wh and Wl 2: Start from the first frame in training dataset 3: repeat 4: Sequentially select T frames {x1, ..., xT } 5: Extract features {i1, ..., iT } 6: Generate internal memory states {h1, ..., hT } 7: Compute network output {\u00b51, ..., \u00b5T } 8: Randomly sample predictions for N episodes {l1:N1 , ..., l1:NT } according to Equation 9 9: Calculates rewards {r1:N1 , ..., r1:NT } based on Equation 4\n10: bt\u2190 1N \u2211N i=1 r i t, t = 1, ..., T 11: Computes gradient according to Equation 8 12: Update W using backpropagation 13: Move on to next T frames 14: until reward doesn\u2019t increase\nconfigurations of generating these datasets to validate the performance of our proposed tracker in different scenarios."}, {"heading": "5.1. Evaluation Metrics", "text": "We used two performance measures to quantitatively evaluate our tracking model on test data. The first one was the average central pixel error\ne = \u221a (xpred \u2212 xgt)2 + (ypred \u2212 ygt)2 (11)\nwhere (xgt, ygt) and (xpred, ypred) were the ground truth and predicted central pixel coordinate in a 2D image, and the error was computed as the pixel distance between these two points. The error was then averaged among all predictions in test data.\nAverage central pixel error measured how far away the center of prediction deviated from the center of ground truth, but it didn\u2019t measure the scale difference between the prediction and the ground truth. Here, we used the average Intersection-over-Union (IoU)\nIoU = |bpred \u2229 bgt| |bpred \u222a bgt|\n(12)\nwhere bgt and bpred were the ground truth and predicted bounding boxes, and IoU was computed as the intersection area divided by union area between bgt and bpred. Commonly, bgt and bpred were defined as rectangles."}, {"heading": "5.2. Implementation Details", "text": "We generated videos from MNIST images of handwritten digits [23] by placing randomly-drawn digits in a larger\n100 \u00d7 100 canvas and moving the digits from one frame to the next. We respected the same data distribution for training and testing as in the original MNIST dataset, i.e., digits were drawn from the training split to generate the training sequences and from the test split to generate the test sequences. Our tracker demonstrated good performance in challenging scenarios and achieved better results comparing with [20] in the same experimental settings.\nThe CNN, RNN and RL parts were all implemented using the TensorFlow toolbox1. We first described the design choices that were common to all our experiments:\nFeature extractor: In order to capture a robust representation against small variations in the MNIST dataset, a shallow hierarchical CNN was used as the feature extractor. The CNN structure had two convolutional layers with filter sizes of 8\u00d7 5\u00d7 5 and 16\u00d7 5\u00d7 5, each followed by a 2\u00d7 2 maxpooling layer and the ReLU activation function.\nMemory component: An LSTM was used here as the memory component. The LSTM structure was similar to the one proposed in [17] with all gated controls except that\n1https://www.tensorflow.org/\nthe activation function was replaced with ReLU. The LSTM used in our network is shown in Figure 2. The LSTM state vector h had dimensionality 256.\nRL unit: This component was implemented as a 256- neuron hidden layer with ReLU activation function which took state vector h as input and generated the mean value \u00b5 of the location policy l. In different experimental settings, the network output \u00b5 could either be the central target location (x, y) or the target square (x, y, w). The location policy was sampled from a Gaussian distribution with mean \u00b5 and variance \u03c3 during training phase, and we found that \u03c3 = 1 was good for both randomness and certainty in our experiment. During test phase, we directly used the output mean value \u00b5 as prediction which was the same as setting \u03c3 = 0."}, {"heading": "5.3. Evaluation on Moving MNIST", "text": "Fixed-size moving MNIST against a black background: We first tested the ability of our deep RL agent to find good tracking policies for fixed-size MNIST digits moving in black background. In this experiment, we generated videos, each with a single 28 \u00d7 28 MNIST digit\nmoving in a random walk manner with momentum. The dataset consisted of 500 training sequences and 100 test sequences. Each testing video sequence contained 100 frames while each training sequence only contained 20 frames. The training algorithm was the same as Algorithm 1, we used T = 10 and N = 5 as these hyper-parameters provided the best tracking performance. The initial learning rate was 0.00001 and we annealed the learning rate linearly from its initial value to 0 over the course of training. We trained the model up to 2000 epochs, or until the cumulative tracking reward R stopped increasing. The trained model was directly applied to the test sequences with no online finetuning. Since the scale of MNIST digits didn\u2019t change in this experiment, the output of deep RL agent had two neurons directly encoding the center location (x, y) of target object. The qualitative result is shown in Figure 4a, and the second row of Table 1 shows the evaluation result.\nScalable moving MNIST against a black background: The second problem we considered was tracking a scalable MNIST digit moving in a black background, and we wanted to see whether our model was capable of predicting scale changes. In this experiment, we generated new sequences for training and testing. We used the same script as in the previous experiment to generate this dataset except that the 28 \u00d7 28 MNIST digits were replaced by scalable MNIST digits: the side length of a MNIST digit was defined as st = kst\u22121 where k was a random number uniformly sampled from (\u22120.5, 2), and we limited the side length s in range (14, 56) to make sure that the digit did not become either too large or too small. The network architectures were slightly changed such that there were 3 output neurons which encoded not only the center location (x, y) but also the side length w, and we didn\u2019t modify the CNN and RNN architecture. The network was randomly initialized and trained from scratch following the same training and testing procedures as discussed in the previous experiment. Figure 4b shows the tracking result, and the third row of Table 1 presents the quantitative evaluation metrics.\nScalable moving MNIST against a noisy background: One of the most challenging aspects of object tracking is the presence of cluttered and noisy background. It is interesting to investigate how robust our tracker is in this scenario. In this experiment, we leveraged the same dataset in the second experiment, but added random background noise for each training and testing video. Technically, we added white noise dots by randomly selecting 500 points in the 100 \u00d7 100 canvas, and we also added one MNIST digit as distraction in the background. We used the same network design as in the second experiment and followed the same training procedure to train our network from scratch. Our testing results showed that our tracker was robust enough to not only predict center locations but also kept track of scale changes. The qualitative and quantitative results are shown\nin Figure 4c and fourth row of Table 1 respectively."}, {"heading": "5.4. Discussion", "text": "We show that the proposed tracking approach works well in three different scenarios by performing rigorous validation experiments on artificial video sequences with ground truth. We have found in our experiments that the proposed framework possesses:\n\u2022 Generalizability: In all experiments, we evaluate a trained model on test sequences that are longer than the training sequences, and our model has proven to be able to generalize to longer test sequences. This is not only because of we apply RNN in our design, but also due to the RL algorithm that allows us to take longterm rewards into consideration.\n\u2022 Adaptability: By formulating tracking as a decision making process, the number of neurons in the output layer entirely depends on tracking complexity. We have shown that our model can achieve reasonable performance by changing only the last-layer architecture and keeping other components untouched. This is a desirable feature for machine learning models since a general model can be applied to different tasks with minimal alteration.\n\u2022 Robustness: The features are captured by the CNN and RNN in our tracking model, and our experiments have shown that by leveraging the same CNN and RNN architectures, our model is not only able to generate more complex predictions but is also robust to background noise. This shows that our network design is robust and informative in terms of learning good tracking features.\nWe will extend our framework for real-world tracking datasets in our future work."}, {"heading": "6. Conclusion", "text": "In this paper, we proposed a novel neural network tracking model based on a recurrent convolutional network trained with deep RL algorithm. To the best of our knowledge, we are the first to bring RL into CNN and RNN to solve visual tracking problems. The entire network is endto-end trainable offline, and the full pipeline is jointly tuned to maximize the tracking quality. The deep RL algorithm directly optimizes a long-term tracking performance measure which depends on the whole tracking video sequence. The proposed deep RL agent maps raw image pixels to target location decisions, and the spatial temporal representations learned by our model are general and robust to accommodate different tracking scenarios. Extensive experiments have validated the applicability of our proposed tracker."}], "references": [{"title": "Searching for objects driven by context", "author": ["B. Alexe", "N. Heess", "Y.W. Teh", "V. Ferrari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Robust object tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Optimal scanning for faster object detection", "author": ["N.J. Butko", "J.R. Movellan"], "venue": "In Computer vision and pattern recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3531,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["M. Denil", "L. Bazzani", "H. Larochelle", "N. de Freitas"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Human tracking using convolutional neural networks", "author": ["J. Fan", "W. Xu", "Y. Wu", "Y. Gong"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Real-time tracking via on-line boosting", "author": ["H. Grabner", "M. Grabner", "H. Bischof"], "venue": "In BMVC,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Semi-supervised on-line boosting for robust tracking", "author": ["H. Grabner", "C. Leistner", "H. Bischof"], "venue": "In European conference on computer vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P.H. Torr"], "venue": "In 2011 International Conference on Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Learning to track at 100 fps with deep regression networks", "author": ["D. Held", "S. Thrun", "S. Savarese"], "venue": "arXiv preprint arXiv:1604.01802,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Online tracking by learning discriminative saliency map with convolutional neural network", "author": ["S. Hong", "T. You", "S. Kwak", "B. Han"], "venue": "arXiv preprint arXiv:1502.06796,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["J. Johnson", "A. Alahi", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1603.08155,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Ratm: Recurrent attentive tracking model", "author": ["S.E. Kahou", "V. Michalski", "R. Memisevic"], "venue": "arXiv preprint arXiv:1510.08660,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "The visual object tracking vot2015 challenge results", "author": ["M. Kristan", "J. Matas", "A. Leonardis", "M. Felsberg", "L. Cehovin", "G. Fernandez", "T. Vojir", "G. Hager", "G. Nebehay", "R. Pflugfelder"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Robust visual tracking", "author": ["X. Mei", "H. Ling"], "venue": "IEEE 12th International Conference on Computer Vision,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Learning multi-domain convolutional neural networks for visual tracking", "author": ["H. Nam", "B. Han"], "venue": "arXiv preprint arXiv:1510.07945,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML (3),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "On learning where to look", "author": ["M. Ranzato"], "venue": "arXiv preprint arXiv:1405.5488,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Incremental learning for robust visual tracking", "author": ["D.A. Ross", "J. Lim", "R.-S. Lin", "M.-H. Yang"], "venue": "International Journal of Computer Vision,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "CoRR, abs/1502.04681,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Visual tracking with fully convolutional networks", "author": ["L. Wang", "W. Ouyang", "X. Wang", "H. Lu"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Transferring rich feature hierarchies for robust visual tracking", "author": ["N. Wang", "S. Li", "A. Gupta", "D.-Y. Yeung"], "venue": "arXiv preprint arXiv:1501.04587,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1992}, {"title": "Online object tracking: A benchmark", "author": ["Y. Wu", "J. Lim", "M.-H. Yang"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Robust visual tracking via multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 151, "endOffset": 163}, {"referenceID": 14, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 151, "endOffset": 163}, {"referenceID": 31, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 151, "endOffset": 163}, {"referenceID": 9, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 182, "endOffset": 186}, {"referenceID": 23, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 210, "endOffset": 214}, {"referenceID": 3, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 232, "endOffset": 247}, {"referenceID": 34, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 232, "endOffset": 247}, {"referenceID": 35, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 232, "endOffset": 247}, {"referenceID": 18, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 232, "endOffset": 247}, {"referenceID": 27, "context": "Recently, some initial works have been done in using neural networks for visual tracking, and these efforts have produced some impact and improved state-of-the-art accuracy [29, 41, 40].", "startOffset": 173, "endOffset": 185}, {"referenceID": 37, "context": "Recently, some initial works have been done in using neural networks for visual tracking, and these efforts have produced some impact and improved state-of-the-art accuracy [29, 41, 40].", "startOffset": 173, "endOffset": 185}, {"referenceID": 36, "context": "Recently, some initial works have been done in using neural networks for visual tracking, and these efforts have produced some impact and improved state-of-the-art accuracy [29, 41, 40].", "startOffset": 173, "endOffset": 185}, {"referenceID": 38, "context": "This procedure uses backpropagation to train the neural-network components and REINFORCE algorithm [42] to train the policy network.", "startOffset": 99, "endOffset": 103}, {"referenceID": 39, "context": "For a systematic review and comparison, we refer the readers to a recent benchmark and a tracking challenge report [43, 21].", "startOffset": 115, "endOffset": 123}, {"referenceID": 20, "context": "For a systematic review and comparison, we refer the readers to a recent benchmark and a tracking challenge report [43, 21].", "startOffset": 115, "endOffset": 123}, {"referenceID": 30, "context": "Some representative methods are based on principal component analysis [32], and sparse representation [26, 44].", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "Some representative methods are based on principal component analysis [32], and sparse representation [26, 44].", "startOffset": 102, "endOffset": 110}, {"referenceID": 40, "context": "Some representative methods are based on principal component analysis [32], and sparse representation [26, 44].", "startOffset": 102, "endOffset": 110}, {"referenceID": 10, "context": "Many advanced machine learning algorithms have been used, including online boosting [11, 12], structured output SVM [14], and multiple-", "startOffset": 84, "endOffset": 92}, {"referenceID": 11, "context": "Many advanced machine learning algorithms have been used, including online boosting [11, 12], structured output SVM [14], and multiple-", "startOffset": 84, "endOffset": 92}, {"referenceID": 13, "context": "Many advanced machine learning algorithms have been used, including online boosting [11, 12], structured output SVM [14], and multiple-", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "instance learning [2].", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 9, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 21, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 23, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 31, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 34, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 35, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 14, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 27, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 18, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 21, "context": "[22] brought significant performance improvement in image classification by training a deep CNN on ImageNet dataset [6].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[22] brought significant performance improvement in image classification by training a deep CNN on ImageNet dataset [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 9, "context": "Region CNN [10] applied a CNN to an object detection task, which can accurately locate and classify objects in natural images.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 17, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 37, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 36, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 15, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 27, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 37, "context": "An early tracking algorithm transferred CNNs pretrained on a large-scale dataset constructed for image classification [41].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "A recent approach [29] trained a multi-domain neural network and achieved great performance improvement.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 28, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 12, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 33, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 4, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 32, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 19, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 7, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 25, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 16, "context": "Besides traditional RNN, the application of recurrent networks with sophisticated hidden units, such as Long Short-Term Memory (LSTM) [17] or Gated Recurrent Unit (GRU) [5], has become common in recent years.", "startOffset": 134, "endOffset": 138}, {"referenceID": 4, "context": "Besides traditional RNN, the application of recurrent networks with sophisticated hidden units, such as Long Short-Term Memory (LSTM) [17] or Gated Recurrent Unit (GRU) [5], has become common in recent years.", "startOffset": 169, "endOffset": 172}, {"referenceID": 25, "context": "[27] applied LSTM in visual attention problems by training a decision making network, which sufficiently improves computational efficiency for computer vision tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] proposed a class of recurrent convolutional architectures which are suitable for ReLU RNN Cell", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 2, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 29, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 26, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 25, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 26, "context": "Some recent works included training an RL agent to play Atari games from raw input pixels [28], and a recurrent visual attention model [27] to reduce computation for computer vision tasks.", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "Some recent works included training an RL agent to play Atari games from raw input pixels [28], and a recurrent visual attention model [27] to reduce computation for computer vision tasks.", "startOffset": 135, "endOffset": 139}, {"referenceID": 25, "context": "Our work is similar to the attention model described in [27], but we designed our own network architecture especially for solving the visual tracking problem by combining CNN, RNN and RL algorithms.", "startOffset": 56, "endOffset": 60}, {"referenceID": 38, "context": "Training this network to maximize the overall tracking performance is a non-trivial task, and we leverage the REINFORCE algorithm [42] from the RL community to solve this problem.", "startOffset": 130, "endOffset": 134}, {"referenceID": 38, "context": "Here, we bring techniques from the RL community to solve this problem, as shown in [42], the gradient can be first simplified by taking the derivative over log-probability of the policy function \u03c0:", "startOffset": 83, "endOffset": 87}, {"referenceID": 38, "context": "The above training rule is known as the episodic REINFORCE [42] algorithm, and it involves running the deep RL agent with its current policy to obtain samples of interactions and then updating parameters W of the agent such that the log-probability of chosen actions that have led to high overall rewards is increased.", "startOffset": 59, "endOffset": 63}, {"referenceID": 38, "context": "Based on REINFORCE algorithm [42], the gradient of the policy function with respect to \u03bc is given by the gradient of the density function:", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "We generated videos from MNIST images of handwritten digits [23] by placing randomly-drawn digits in a larger", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Single-digit in RATM [20] 63.", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "Our tracker demonstrated good performance in challenging scenarios and achieved better results comparing with [20] in the same experimental settings.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "The LSTM structure was similar to the one proposed in [17] with all gated controls except that", "startOffset": 54, "endOffset": 58}], "year": 2017, "abstractText": "Convolutional neural network (CNN) models have achieved tremendous success in many visual detection and recognition tasks. Unfortunately, visual tracking, a fundamental computer vision problem, is not handled well using the existing CNN models, because most object trackers implemented with CNN do not effectively leverage temporal and contextual information among consecutive frames. Recurrent neural network (RNN) models, on the other hand, are often used to process text and voice data due to their ability to learn intrinsic representations of sequential and temporal data. Here, we propose a novel neural network tracking model that is capable of integrating information over time and tracking a selected target in video. It comprises three components: a CNN extracting best tracking features in each video frame, an RNN constructing video memory state, and a reinforcement learning (RL) agent making target location decisions. The tracking problem is formulated as a decision-making process, and our model can be trained with RL algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. We compare our model with an existing neural-network based tracking method and show that the proposed tracking approach works well in various scenarios by performing rigorous validation experiments on artificial video sequences with ground truth. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.", "creator": "LaTeX with hyperref package"}}}