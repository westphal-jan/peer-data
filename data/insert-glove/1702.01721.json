{"id": "1702.01721", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2017", "title": "View Independent Vehicle Make, Model and Color Recognition Using Convolutional Neural Network", "abstract": "marwah This paper describes pre the 46-run details liberal-leaning of camisa Sighthound ' feile s hongying fully automated prescribes vehicle labiche make, model simples and tellis color nasrin recognition rubato system. windspeeds The howaldtswerke-deutsche backbone cuza of our self-funded system is a deep oriole convolutional ehd neural boretti network that 83 is not only rhenen computationally inexpensive, but cascos also provides state - eoc of - kleptoparasitism the - art results tannadice on several competitive benchmarks. matronymic Additionally, ch-47 our deep network is whiplike trained on americal a large gr\u00e4tz dataset astier of self-publishing several 5-ounce million hallucis images which are labeled through a semi - sumgait automated rire process. alternatively Finally childrens we waman test bardera our system on virac several accompli public datasets 11-month as msk well bosingwa as our adderall own internal test ervins dataset. Our results show anzani that grandeau we outperform 540,000 other kogel methods on long-tongued all kilmory benchmarks by significant exams margins. Our eagling model is available myllylae to a-330 developers through bgan the amon Sighthound guesting Cloud soderling API at", "histories": [["v1", "Mon, 6 Feb 2017 17:47:08 GMT  (3772kb,D)", "http://arxiv.org/abs/1702.01721v1", "7 Pages"]], "COMMENTS": "7 Pages", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["afshin dehghan", "syed zain masood", "guang shu", "enrique g ortiz"], "accepted": false, "id": "1702.01721"}, "pdf": {"name": "1702.01721.pdf", "metadata": {"source": "CRF", "title": "View Independent Vehicle Make, Model and Color Recognition Using Convolutional Neural Network", "authors": ["Afshin Dehghan", "Syed Zain Masood", "Guang Shu", "Enrique G. Ortiz"], "emails": ["egortiz}@sighthound.com"], "sections": [{"heading": "1 Introduction", "text": "Make, model and color recognition (MMCR) of vehicles [1,2,3] is of great interest in several applications such as law-enforcement, driver assistance, surveillance and traffic monitoring. This fine-grained visual classification task [4,5,6,7,8,9] has been traditionally a difficult task for computers. The main challenge is the subtle differences between classes (e.g BMW 3 series and BMW 5 series) compared to some traditional classification tasks such as ImageNet. Recently, there have been efforts to design more accurate algorithms for MMCR such as those in the works of Sochor et al in [1] and Hsieh et al [2]. Moreover, many researchers have focused on collecting large datasets to facilitate research in this area [4]. However, the complexity of current methods and/or the small size of current datasets lead to sub-optimal performance in real world use cases. Thus, there are still considerable short-comings for agencies or commercial entities looking to deploy reliable software for the task of MMCR. In this paper, we present a system that is capable of detecting and tagging the make, model and color of vehicles irrespective of viewing angle with high accuracy. Our model is trained to recognize 59 different vehicle makes as well as 818 different models in what we believe is the largest set available for commercial or non commercial use. 1 The contributions of Sighthound\u2019s vehicle MMCR system are listed as follows:\n1 Our system covers almost all popular models in North America.\nar X\niv :1\n70 2.\n01 72\n1v 1\n[ cs\n.C V\n] 6\nF eb\n2 01\n7\nTo date, we have collected what we believe to be the largest vehicle dataset, consisting of more than 3 million images labeled with corresponding vehicle make and model. Additionally, we labeled part of this data with corresponding labels for vehicle color.\nWe propose a semi-automated method for annotating several million vehicle images.\nWe present an end-to-end pipeline, along with a novel deep network, that not only is computationally inexpensive, but also outperforms competitive methods on several benchmarks.\nWe conducted a number of experiments on existing benchmarks and obtained state-of-the-art results on all of them."}, {"heading": "2 System Overview", "text": "The overview of our system is shown in Figure 1. Our training consists of a 3-stage processing pipeline including data collection, data pre-processing and deep training. Data collection plays an important role in our final results, thus collecting data, which requires the least effort in labeling, is of great importance. We collected a large dataset with two different sets of annotations. All the images are annotated with their corresponding vehicle make and model and part of the data is annotated with vehicle colors 2. In order to prepare the final training data we further process the images to eliminate the effect of background. Finally these images are fed into two separate deep neural networks to train the final model."}, {"heading": "3 Training", "text": "Below we describe in more detail different components of our 3-stage training procedure.\nData Collection: Data collection plays an important role in training any deep neural network, especially when it comes to fine-grained classification tasks. To address this issue we collected the largest vehicle dataset known to date, where each image is labeled with corresponding make and model of the vehicle. We initially collected over 5 million images from various sources. We developed a semi-automated process to partially prune the data and remove the undesired images. We finally used a team of human annotators to remove any remaining errors from the dataset. The final set of data contains over 3 million images of vehicles with their corresponding make and model tags. Additionally, we labeled part of this data with the corresponding color of the vehicle, chosen from a set of 10 colors; blue, black, beige, red, white, yellow, orange, purple, green and gray.\n2 Please note the number of color categories is far less than number of vehicle models\nData Pre-processing: An important step in our training is alignment. In order to align images such that all the labeled vehicles are centered in the image, we used Sighthound\u2019s vehicle detection model available through the Sighthound Cloud API 3. Vehicle detection not only helps us align images based on vehicle bounding boxes but also reduces the impact of the background. This is especially important when there is more than one vehicle in the image. Finally we consider a 10% margin around the vehicle box to compensate for inaccurate (or very tight) bounding boxes. For the task of color recognition, we took pains to further eliminate any influence the background may have on the outcome. To achieve this, we further mask the images with an elliptical mask as shown in Figure 1. Note that in certain cases the elliptical mask removes some boundary information of the vehicle. However, this had little effect on the color classification accuracy.\nDeep training: The final stage of our pipeline in Figure 1 involves training two deep neural networks. One is trained to classify vehicles based on their make and model and the other is trained to classify vehicles based on their color. Our networks are designed such that they achieve high accuracy while remaining computationally inexpensive. We trained our networks for four days on four GTX TITAN X PASCAL GPUs. Once the model is trained, we can label images at 150 fps in batch processing mode.\n3 https://www.sighthound.com/products/cloud"}, {"heading": "4 Experiments on SIN 2014 Test set", "text": "In this section, we report experimental results on two publicly available datasets; the Stanford Cars dataset [10] and the Comprehensive Car (compCar) dataset [4]. The Stanford Cars dataset consists of 196 classes of cars with a total of 16, 185 images. The data is divided into almost a 50-50 train/test split with 8, 144 training images and 8, 041 testing images. Categories are typically at the level of Make, Model, Year. This means that several categories contain the same model of a make, and the only difference is the year that the car is made. Our original model is not trained to classify vehicle models based on the year of their production. However, after fine-tuning our model on the Stanford Cars training data, we observe that we can achieve better results compared to previously published methods. This is mainly due to the sophistication in the design of our proprietary deep neural network as well as the sizable amount of data used to train this network. The quantitative results are shown in Table 1.\nWe also report results on the Comprehensive Car dataset which has recently been published. The task here is to classify data into 431 different classes based on vehicle make and model. The data is divided into 70% training and 30% testing. There are a total of 36, 456 training images and 15, 627 test images. The top-1 and top-5 accuracy are reported in Table 2. We compare our results with the popular deep network architectures reported in [4]. We can clearly see that our fine-tuned model outperforms the existing methods by 4.68% in top-1 accuracy. It is also worth noting that the our model is an order of magnitude faster than GoogLeNet.\nLastly we test the verification accuracy of the proposed method on compCar dataset. The compCar dataset includes three sets of data for verification experiments, sorted by their difficulties. Each set contains 20,000 pairs of images. The likelihood ratio of each image pair is obtained by computing the euclidean distant between features computed using our the deep network. The likelihood ratio is then compared against a threshold to make the final decision. The results are shown in Table 3. As can be seen our model, fine-tuned on the verification training data of compCar dataset, outperforms other methods. It is worth to\nmention that , even without fine-tuning our features can achieve a high verification accuracy of 92.03%, 86.52%, 80.17% on different sets of easy, medium and hard respectively."}, {"heading": "5 Quantitative Results", "text": "We demonstrate some quantitative results in Figures 2 and 3, capturing different scenarios. Figure 2 shows results for images mostly taken by people. Figure 3 shows a surveillance-like scenario where the camera is mounted at a higher distance from the ground. These images are illustrative of the robustness of our large training dataset, captured from different sources, to real world scenarios."}, {"heading": "6 Conclusions", "text": "In this paper we presented an end to end system for vehicle make, model and color recognition. The combination of Sighthound\u2019s novel approach to the design and implementation of deep neural networks and a sizable dataset for training allow us to label vehicles in real time with high degrees of accuracy.We conducted several experiments for both classification and verification tasks on public benchmarks and showed significant improvement over previous methods."}], "references": [{"title": "Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition", "author": ["J. Sochor", "A. Herout", "J. Havel."], "venue": "CVPR.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Symmetrical surf and its applications to vehicle detection and vehicle make and model recognition", "author": ["J.W. Hsieh", "L.C. Chen", "D.Y. Chen"], "venue": "IEEE Transactions on intelligent transportation systems.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Car model recognition by utilizing symmetric property to overcome severe pose variation", "author": ["H.Z. Gu", "S.Y. Lee."], "venue": "Machine vision and applications.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A large-scale car dataset for fine-grained categorization and verification", "author": ["Yang", "Linjie", "e.a."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilinear cnn models for fine-grained visual recognition", "author": ["T.Y. Lin", "A. RoyChowdhury", "S. Maji."], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Hyper-class augmented and regularized deep learning for finegrained image classification", "author": ["Xie", "Saining", "e.a."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "3d object representations for finegrained categorization", "author": ["J. Krause", "M. Stark", "J. Deng", "L. Fei-Fei"], "venue": "4th IEEE Workshop on 3D Representation and Recognition, ICCV.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning features and parts for fine-grained recognition", "author": ["Jonathan Krause", "e.a."], "venue": "Pattern Recognition (ICPR), 2014 22nd International Conference on.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Jointly optimizing 3d model fitting and fine-grained classification", "author": ["Yen-Liang Lin", "e.a."], "venue": "ECCV.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Fine-grained recognition without part annotations", "author": ["Krause", "Jonathan", "e.a."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Embedding label structures for fine-grained feature representation", "author": ["X.Z. et al."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Re-visiting the fisher vector for fine-grained classification", "author": ["P.H. Gosselin", "N. Murray", "H. Jegou", "F. Perronnin."], "venue": "Pattern Recognition Letters.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Make, model and color recognition (MMCR) of vehicles [1,2,3] is of great interest in several applications such as law-enforcement, driver assistance, surveillance and traffic monitoring.", "startOffset": 53, "endOffset": 60}, {"referenceID": 1, "context": "Make, model and color recognition (MMCR) of vehicles [1,2,3] is of great interest in several applications such as law-enforcement, driver assistance, surveillance and traffic monitoring.", "startOffset": 53, "endOffset": 60}, {"referenceID": 2, "context": "Make, model and color recognition (MMCR) of vehicles [1,2,3] is of great interest in several applications such as law-enforcement, driver assistance, surveillance and traffic monitoring.", "startOffset": 53, "endOffset": 60}, {"referenceID": 3, "context": "This fine-grained visual classification task [4,5,6,7,8,9] has been traditionally a difficult task for computers.", "startOffset": 45, "endOffset": 58}, {"referenceID": 4, "context": "This fine-grained visual classification task [4,5,6,7,8,9] has been traditionally a difficult task for computers.", "startOffset": 45, "endOffset": 58}, {"referenceID": 5, "context": "This fine-grained visual classification task [4,5,6,7,8,9] has been traditionally a difficult task for computers.", "startOffset": 45, "endOffset": 58}, {"referenceID": 6, "context": "This fine-grained visual classification task [4,5,6,7,8,9] has been traditionally a difficult task for computers.", "startOffset": 45, "endOffset": 58}, {"referenceID": 7, "context": "This fine-grained visual classification task [4,5,6,7,8,9] has been traditionally a difficult task for computers.", "startOffset": 45, "endOffset": 58}, {"referenceID": 8, "context": "This fine-grained visual classification task [4,5,6,7,8,9] has been traditionally a difficult task for computers.", "startOffset": 45, "endOffset": 58}, {"referenceID": 0, "context": "Recently, there have been efforts to design more accurate algorithms for MMCR such as those in the works of Sochor et al in [1] and Hsieh et al [2].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Recently, there have been efforts to design more accurate algorithms for MMCR such as those in the works of Sochor et al in [1] and Hsieh et al [2].", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "Moreover, many researchers have focused on collecting large datasets to facilitate research in this area [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "In this section, we report experimental results on two publicly available datasets; the Stanford Cars dataset [10] and the Comprehensive Car (compCar) dataset [4].", "startOffset": 110, "endOffset": 114}, {"referenceID": 3, "context": "In this section, we report experimental results on two publicly available datasets; the Stanford Cars dataset [10] and the Comprehensive Car (compCar) dataset [4].", "startOffset": 159, "endOffset": 162}, {"referenceID": 9, "context": "[10] 92.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] 91.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] 88.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] 86.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] 82.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "We compare our results with the popular deep network architectures reported in [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "We compare our results with popular deep networks of GoogLeNet, Overfeat and AlexNet reported in [4]", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "GoogLeNet [4] 91.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "1% Overfeat [4] 87.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "9% AlexNet [4] 81.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "Verification accuracy of three different sets, easy, medium and hard in [4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "[4] 83.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] 85.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": null, "creator": "LaTeX with hyperref package"}}}