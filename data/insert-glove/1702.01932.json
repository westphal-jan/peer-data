{"id": "1702.01932", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "A Knowledge-Grounded Neural Conversation Model", "abstract": "dens Neural expensed network espa\u00f1oles models khurana are capable collantes of islah generating extremely 1946-48 natural ros\u00e9n sounding \u0f44 conversational interactions. Nevertheless, samajtantrik these models caccioppoli have bellingham yet hilary to stage demonstrate that oruro they khaldei can yomo incorporate content in the stressor form of factual information or ginetta entity - grounded opinion that hossack would enable principato them xiaobo to mayorga serve in 22,000 more 84.16 task - oriented conversational shawne applications. 12-second This borophaginae paper presents a 22 novel, fully mineralogy data - oliseh driven, imporant and 31-foot knowledge - grounded 951 neural conversation model socle aimed at producing fueled more contentful responses dehua without derivable slot fritter filling. georgian-style We cupido generalize the tributes widely - used sharry Seq2Seq approach by koshkuiyeh conditioning responses gonnet on both bruggisser conversation history chagny and external \" amburn facts \", allowing reuter the aridi model jamerson to season-opening be jqc versatile veber and otsuka applicable 26-hour in an microcell open - domain setting. nerz Our lindenhof approach yields bologoye significant improvements avhrr over akkineni a 22-22 competitive Seq2Seq bogdana baseline. Human judges bilboa found descriptions that our boardgames outputs are cosa significantly more informative.", "histories": [["v1", "Tue, 7 Feb 2017 09:16:46 GMT  (2637kb,D)", "http://arxiv.org/abs/1702.01932v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marjan ghazvininejad", "chris brockett", "ming-wei chang", "bill dolan", "jianfeng gao", "wen-tau yih", "michel galley"], "accepted": false, "id": "1702.01932"}, "pdf": {"name": "1702.01932.pdf", "metadata": {"source": "CRF", "title": "A Knowledge-Grounded Neural Conversation Model", "authors": ["Marjan Ghazvininejad", "Chris Brockett", "Ming-Wei Chang", "Bill Dolan", "Jianfeng Gao", "Wen-tau Yih", "Michel Galley"], "emails": ["ghazvini@isi.edu,", "mgalley@microsoft.com", "@pizzalibretto"], "sections": [{"heading": null, "text": "Neural network models are capable of generating extremely natural sounding conversational interactions. Nevertheless, these models have yet to demonstrate that they can incorporate content in the form of factual information or entity-grounded opinion that would enable them to serve in more task-oriented conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. We generalize the widely-used SEQ2SEQ approach by conditioning responses on both conversation history and external \u201cfacts\u201d, allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive SEQ2SEQ baseline. Human judges found that our outputs are significantly more informative."}, {"heading": "1 Introduction", "text": "Conversational agents such as Alexa, Siri, and Cortana have been increasingly popular, as they facilitate interaction between people and their devices. There is thus a growing need to build systems that can respond seamlessly and appropriately, and the task of conversational response generation has recently become an active area of research in natural language processing.\nRecent work (Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015) has shown that it is possible to train conversational models in an end-to-end and completely\n* This work was conducted at Microsoft.\ndata-driven fashion, without hand-coding. However, these fully data-driven systems lack grounding in the real world and do not have access to any external knowledge (textual or structured), which makes it difficult to respond substantively. Fig. 1 illustrates the difficulty: while an ideal response would directly reflect on the entities mentioned in the query (user input), neural models produce responses that, while conversationally appropriate, seldom include factual content. This contrasts with traditional dialog systems, which can easily inject entities and facts into responses using slotfilling, but often at the cost of significant handcoding, making such systems difficult to scale to new domains or tasks.\nThe goal of this paper is to benefit from both lines of research\u2014fully data-driven and grounded in external knowledge. The tie to external data is critical, as the knowledge that is needed to make the conversation useful is often stored in nonconversational data, such as Wikipedia, books reviews on Goodreads, and restaurant reviews on Foursquare. While conversational agents can learn\nar X\niv :1\n70 2.\n01 93\n2v 1\n[ cs\n.C L\n] 7\nF eb\n2 01\n7\nthe backbone of human conversations from millions of conversations (Twitter, Reddit, etc.), we rely on non-conversational data to infuse relevant knowledge in conversation with users based on the context. More fundamentally, this line of research also targets more useful conversations. While prior data-driven conversational models have been essentially used as chitchat bots, access to external data can help users make better decisions (e.g., recommendation or QA systems) or accomplish specific tasks (e.g., task-completion agents).\nThis paper presents a novel, fully datadriven, and knowledge-grounded neural conversation model aimed at producing more contentful responses. It offers a framework that generalizes the SEQ2SEQ approach (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014) of most previous neural conversation models, as it naturally combines conversational and non-conversational data via multi-task learning (Caruana, 1998; Liu et al., 2015). The key idea of this approach is that it not only conditions responses based on conversation history (Sordoni et al., 2015), but also on external \u201cfacts\u201d that are relevant to the current context (for example, Foursquare entries as in Fig. 1). Our approach only requires a way to ground external information based on conversation context (e.g., via simple entity name matching), which makes it highly versatile and applicable in an open-domain setting. This allowed us to train our system at a very large scale using 23M social media conversations and 1.1M Foursquare tips. The trained system showed significant improvements over a competitive large-scale SEQ2SEQ baseline. To the best of our knowledge, this is the first large-scale, fully data-driven neural conversation model that effectively exploits external knowledge, and it does so\nwithout explicit slot filling."}, {"heading": "2 Grounded Response Generation", "text": "A primary challenge in building fully data-driven conversation models is that most of the world\u2019s knowledge is not represented in any existing conversational datasets. While these datasets (Serban et al., 2015) have grown dramatically in size thanks in particular to social media (Ritter et al., 2011), this data is still very far from containing discussions of every entry in Wikipedia, Foursquare, Goodreads, or IMDB. This problem considerably limits the appeal of existing datadriven conversation models, as they are bound to respond evasively or deflectively as in Fig. 1, especially with regard to those entities that are poorly represented in the conversational training data. On the other hand, even when such conversational data representing most entities of interest did exist, we would still face challenges as such huge dataset would be difficult to be used for model training, and many conversational patterns exhibited in the data (e.g., for similar entities) would be redundant.\nOur approach aims to avoid redundancy and attempts to better generalize from existing conversational data, as illustrated in Fig. 2. While the conversations in the figure are about specific venues, products, and services, conversational patterns are general and equally applicable to other entities. The learned conversational behaviors could be used to, e.g., recommend other products and services. A traditional dialog system would use predefined slots to fill conversational backbone (bold text) with content; here, we present a more robust and scalable approach.\nIn order to infuse the response with factual information relevant to the conversational context,\nwe propose a knowledge-grounded model architecture depicted in Fig. 3. First, we have available a large collection of world facts,1 which is a large collection of raw text entries (e.g., Foursquare, Wikipedia, or Amazon reviews) indexed by named entities as keys. Then, given a conversational history or source sequence S, we identify the \u201cfocus\u201d in S, which is the text span (one or more entities) based on which we form a query to link to the facts. This focus can either be identified using keyword matching (e.g., a venue, city, or product name), or detected using more advanced methods such as entity linking or named entity recognition. The query is then used to retrieve all contextually relevant facts: F = {f1, ..., fk}.2 Finally, both conversation history and relevant facts are fed into a neural architecture that features distinct encoders for conversation history and facts. We will detail this architecture in the subsections below.\nThis knowledge-grounded approach is more general than SEQ2SEQ response generation, as it avoids the need to learn the same conversational pattern for each distinct entity that we care about. In fact, even if a given entity (e.g., @pizzalibretto in Fig. 2) is not part of our conversational training data and therefore out-of-vocabulary, our approach is still able to rely on retrieved facts to generate an appropriate response. This also implies that we can enrich our system with new facts without the need to retrain the full system.\nWe train our system using multi-task learning (Luong et al., 2015) as a way of combining conversational data that is naturally associated with external data (e.g., discussions about restaurants\n1For presentation purposes, we refer to these items as \u201cfacts\u201d, but a \u201cfact\u201d here is simply any snippet of authored text, which may contain subjective or inaccurate information.\n2In our work, we use a simple keyword-based IR engine to retrieve relevant facts from the full collection; more details are provided in Sec. 3.\nand other businesses as in Fig. 2), and less informal exchanges (e.g., a response to hi, how are you). More specifically, our multi-task setup contains two types of tasks:\n(1) one purely conversational, where we expose the model without fact encoder with (S,R) training examples, where S represents the conversation history and R is the response; (2) the other task exposes the full model with ({f1, . . . , fk, S}, R) training examples.\nThis decoupling of the two training conditions offer several advantages, including: First, it allows us to pre-train the conversation-only dataset separately, and start multi-task training (warm start) with a dialog encoder and decoder that already learned the backbone of conversations. Second, it gives us the flexibility to expose different kind of conversational data in the two tasks. Finally, one interesting option is to replace the response in task (2) with one of the facts (R = fi), which makes task (2) similar to an autoencoder and helps produce responses that are even more contentful. We will discuss the different ways we apply multi-task learning in practice in greater detail in Sec. 4."}, {"heading": "2.1 Dialog Encoder and Decoder", "text": "The dialog encoder and response decoder form together a sequence-to-sequence (SEQ2SEQ model (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a). Both encoder and decoder are recurrent neural network (RNN) models: an RNN that encodes a variable-length input string into a fixed-length vector representation and an RNN that decodes the vector representation into a variable-length output string. This part of our model is almost identical to prior conversational SEQ2SEQ models, except that we use gated recurrent units (GRU) (Chung et al., 2014) instead of LSTM (Hochreiter and Schmidhuber, 1997) cells. Encoders and decoders in sequence-to-sequence models sometimes share weights in monolingual tasks, but do not do so in the present model, nor do they share word embeddings."}, {"heading": "2.2 Facts Encoder", "text": "The Facts Encoder of Fig. 3 is similar to the Memory Network model first proposed by (Weston et al., 2014; Sukhbaatar et al., 2015). It uses\nan associative memory for modeling the facts relevant to a particular problem\u2014in our case, an entity mentioned in a conversation\u2013then retrieves and weights these facts based on the user input and conversation history to generate an answer. Memory network models are widely used in Question Answering to make inferences based on the facts saved in the memory (Weston et al., 2015).\nIn our adaptation of memory networks, we use an RNN encoder to turn the input sequence (conversation history) into a vector, instead of a bag of words representation as used in the original memory network models. This enables us to better exploit interlexical dependencies between different parts of the input, and makes this memory network model (facts encoder) more directly comparable to a SEQ2SEQ model.\nMore formally, we are given an input sentence S = {s1, s2, ..., sn}, and a fact set F = {f1, f2, ..., fk} that are relevant to the conversation history. The RNN encoder reads the input string word by word and updates its hidden state. After reading the whole input sentence the hidden state of the RNN encoder, u is the summary of the input sentence. By using an RNN encoder, we have a rich representation for a source sentence.\nLet us assume u is a d dimensional vector and ri is the bag of words representation of fi with dimension v. Based on (Sukhbaatar et al., 2015) we have:\nmi = Ari (1) ci = Cri (2)\npi = softmax(uTmi) (3)\no = k\u2211\ni=1\npici (4)\nu\u0302 = o+ u (5)\nWhere A,C \u2208 Rd\u00d7v are the parameters of the memory network. Then, unlike the original version of the memory network, we use an RNN decoder that is good for generating the response. The hidden state of the RNN is initialized with u\u0302which is a symmetrization of input sentence and the external facts, to predict the response sentence R word by word.\nAs alternatives to summing up facts and dialog encodings in equation 5, we also experimented with other operations such as concatenation, but summation seemed to yield the best results. The memory network model of (Weston et al., 2014)\ncan be defined as a multi-layer structure. In this task, however, 1-layer memory network was used, since multi-hop induction was not needed."}, {"heading": "3 Datasets", "text": "The approach we describe above is quite general, and is applicable to any dataset that allows us to map named entities to free-form text (e.g., Wikipedia, IMDB, TripAdvisor, etc.). For experimental purposes, we utilize datasets derived from two popular social media services: Twitter (conversational data) and Foursquare (nonconversational data). Note that none of the processing applied to these datasets is specific to any underlying task or domain."}, {"heading": "3.1 Foursquare", "text": "Foursquare tips are comments left by customers about restaurants and other, usually commercial, establishments. A large proportion of these describe aspects of the establishment, and provide recommendations about what the customer enjoyed (or otherwise) We extracted from the web 1.1M tips relating to establishments in North America. This was achieved by identifying a set of 11 likely \u201cfoodie\u201d cities and then collecting tip data associated with zipcodes near the city centers. While we targeted foodie cities, the dataset is very general and contains tips many types of local businesses (restaurants, theaters, museums, shopping, etc.) In the interests of manageability for experimental purposes, we ignored establishments associated with fewer than 10 tips, but other experiments with up to 50 tips per venue yield comparable results. We further limited the tips to those that for which Twitter handles were found in the Twitter conversation data."}, {"heading": "3.2 Twitter", "text": "We collected a 23M general dataset of 3-turn conversations. This serves as a background dataset not associated with facts, and its massive size is key to learning the conversational structure or backbone.\nSeparately, on the basis of Twitter handles found in the Foursquare tip data, we collected approximately 1 million two-turn conversations that contain entities that tie to Foursquare. We refer to this as the 1M grounded dataset. Specifically, we identify conversation pairs in which the first turn contained either a handle of the business name (preceded by the \u201c@\u201d symbol) or a hashtag that\nmatched a handle.3 Because we are interested in conversations among real users (as opposed to customer service agents), we removed conversations where the response was generated by a user with a handle found in the Foursquare data."}, {"heading": "3.3 Grounded Conversation Datasets", "text": "We augment the 1M grounded dataset with facts (here Foursquare tips) relevant to each conversation history. The number of contextually relevant tips for some handles can sometimes be enormous, up to 10k. To filter them based on relevance to the input, the system uses tf-idf similarity between the input sentence and all of these tips and retains 10 tips with the highest score.\nFurthermore, for a significant portion of the 1M Twitter conversations collected using handles found on Foursquare, the last turn was not particularly informative, e.g., when it provides a purely socializing response (e.g., \u201chave fun there\u201d) rather than a contentful one. As one of our goals is to evaluate conversational systems on their ability to produce contentful responses, we select a dev and test set (4k conversations in total) designed to contain responses that are informative and useful.\nFor each handle in our dataset we created two scoring functions:\n\u2022 Perplexity according to a 1-gram LM trained on all the tips containing that handle. \u2022 \u03c7-square score, which measures how much\ncontent each token bears in relation to the handle. Each tweet is then scored on the basis of the average content score of its terms.\nIn this manner, we selected 15k top-ranked conversations using the LM score and 15k using the chi-square score. A further 15k conversations were randomly sampled. We then randomly sampled 10k conversations these data to be evaluated by crowdsourced annotators. Human judges were presented with the conversations and asked to determine whether the response contained actionable information, i.e., did they contain information that would permit the respondents to decide, e.g., whether or not they should patronize an establishment. From this, we selected the top-ranked 4k conversations to be held out validation set and test set; these were removed from our training data.\n3This mechanism of linking conversations to facts using exact match on the handle is high precision but low recall, but low recall seems reasonable as we are far from exhausting all available Twitter and Foursquare data."}, {"heading": "4 Experimental Setup", "text": ""}, {"heading": "4.1 Multi-Task Learning", "text": "We use multi-task learning with these tasks:\n\u2022 FACTS task: We expose the full model with ({f1, ..., fn, S}, R) training examples. \u2022 NOFACTS task: We expose the model with-\nout fact encoder with (S,R) examples. \u2022 AUTOENCODER task: It is similar to the\nFACTS task, except that we replace the response with each of the facts, i.e., this model is trained on ({f1, ..., fn, S}, fi) examples. There are n times many samples for this task than for the FACTS task.4\nThe tasks FACTS and NOFACTS are representative of how our model is intended to work, but we found that the AUTOENCODER tasks helps inject more factual content into the response. Then, the different variants of our multi-task learned system exploits these tasks as follows:\n\u2022 SEQ2SEQ: This system is trained on task NOFACTS with the 23M general conversation dataset. Since there is only one task, it is not per se a multi-task setting. \u2022 MTASK: This system is trained on two\ninstances of the NOFACTS task, respectively with the 23M general dataset and 1M grounded dataset (but without the facts). While not an interesting system in itself, we include it to assess the effect of multi-task learning separately from facts. \u2022 MTASK-R: This system is trained on the NO-\nFACTS task with the 23M dataset, and the FACTS task with the 1M grounded dataset. \u2022 MTASK-F: This system is trained on the NO-\nFACTS task with the 23M dataset, and the AUTOENCODER task with the 1M dataset. \u2022 MTASK-RF: This system blends MTASK-F\nand MTASK-R, as it incorporates 3 tasks: NOFACTS with the 23M general dataset, FACTS with the 1M grounded dataset, and AUTOENCODER again with the 1M dataset.\nWe trained a one-layer memory network structure with two-layer SEQ2SEQ models. More specifically, we use 2-layer GRU models with 512 hidden cells for each layer is used for encoder and decoder, the dimensionality of word embeddings\n4This is akin to an autoencoder (hence the name) as the fact fi is represented both in the input and output, but of course not strictly an autoencoder.\nis set to 512, and the size of input/output memory representation is 1024. We used the Adam optimizer with a fixed learning rate of 0.1, with a batch size is set to 128. All parameters are initialized from a uniform distribution in [\u2212 \u221a 3/d, \u221a 3/d], where d is the dimension of the parameter. Gradients are clipped at 5 to avoid gradient explosion.\nEncoder and decoder use different sets of parameters. The top 50k frequent types from conversation data is used as vocabulary which is shared between both conversation and non-conversation data. We use the same learning technique as (Luong et al., 2015) for multi-task learning. In each batch, all training data is sampled from one task only. For task i we define its mixing ratio value of \u03b1i, and for each batch we select randomly a new task i with probability of \u03b1i/ \u2211 j \u03b1j and train the system by its training data."}, {"heading": "4.2 Decoding and Reranking", "text": "We use a beam-search decoder similar to (Sutskever et al., 2014) with beam size of 200, and maximum response length of 30. Following (Li et al., 2016a), we generate N -best lists containing three features: (1) the log-likelihood logP (R|S, F ) according to the decoder; (2) word count; (3) the log-likelihood logP (S|R) of the source given the response. The third feature is added to deal with the issue of generating commonplace and generic responses such as \u201cI don\u2019t know\u201d, which is discussed in details in (Li et al., 2016a). Our models often do not need the third feature to be effective, but\u2014since our baseline needs it to avoid commonplace responses\u2014we include this feature in all systems. This yields the following reranking score:\nlogP (R|S, F ) + \u03bb logP (S|R) + \u03b3|R| \u03bb and \u03b3 are free parameters, which we tune on our development N -best lists using MERT (Och, 2003) by optimizing BLEU (Papineni et al., 2002a). To estimate P (S|R) we train a Sequenceto-sequence model by swapping messages and responses. In this model we do not use any facts."}, {"heading": "4.3 Evaluation Metrics", "text": "Following (Sordoni et al., 2015; Wen et al., 2016; Li et al., 2016a), we use BLEU automatic evaluation. While (Liu et al., 2016) suggest that BLEU correlates poorly with human judgment at the sentence-level,5 we use instead corpus-level\n5This corroborates earlier findings that accurate sentencelevel automatic evaluation is indeed difficult, even for Ma-\nBLEU, which is known to better correlate with human judgments (Przybocki et al., 2008) including for response generation (Galley et al., 2015). We also report perplexity and lexical diversity, the latter as a raw yet automatic measure of informativeness and diversity. Automatic evaluation is augmented with human judgments of appropriateness and informativeness."}, {"heading": "5 Results", "text": "Automatic Evaluation: We computed perplexity and BLEU (Papineni et al., 2002b) for each system. These are shown in Tables 1 and 2 respectively. We observe that the perplexity of MTASK and MTASK-R models on both general and grounded data is as low as the SEQ2SEQ models that are trained specifically on general and grounded data respectively. As expected, injecting more factual content into the response in MTASKF and MTASK-RF increased the perplexity especially on grounded data.\nBLEU scores are low, but this is not untypical of conversational systems (e.g., (Li et al., 2016a,b)). Table 2 shows that the MTASK-R model yields a significant performance boost, with a BLEU score increase of 96% and 71% jump in 1-gram diversity compared to the competitive SEQ2SEQ baseline. In terms of BLEU scores, MTASK-RF improvements is not significant, but it generates the highest\nchine Translation (Graham et al., 2015), as BLEU and related metrics were originally designed as corpus-level metrics.\n1-gram and 2-gram diversity among all models.\nHuman Evaluation: We conducted human evaluations using a crowdsourcing service. We had annotators judge 500 paired conversations, asking which is better on two parameters: appropriateness to the topic, and informativeness. Seven judges were assigned to each pair. Annotators whose variance fell greater than two standard deviations from the mean variance were dropped. Ties were permitted.\nThe results of annotation are show in Table 3. On Appropriateness, no system performed significantly better than baseline, and in two cases, MTASK and MTASK-F, the baseline system was significantly better. MTASK-R appears to be slightly better than baseline in the table, but the difference is small and not statistically significant by conventional standards of \u03b1 = 0.05. On Informativeness, MTASK-F and MTASK-R perform significantly better than Baseline (p = 0.005 and p = 0.003 respectively). Since the baseline system outperforms MTASK-F with respect to Appropriateness, this may mean that MTASK-F encounters difficulty representing the social dimensions of conversation, but is strong on informational content. MTASK-R, on the other hand, appears to hold its own on Appropriateness while improving with respect to Informativeness.\nThe narrow differences in averages in Table 3 tend to obfuscate the judges\u2019 voting trends. Accordingly, we translated the scores for each output into the ratio of judges who preferred that system and binned their counts. The results are shown in Figs. 4 and 5 where we compare MTASK-R with the SEQ2SEQ baseline. Bin 7 on the left corresponds to the case where all 7 judges \u201cvoted\u201d for the system, bin 6 to that where 6 out of 7 judges \u201cvoted\u201d for the system, and so on.6 Other bins are not shown since these are a mirror image of bins 7 through 4. The distributions in Fig. 4 are more similar to each other than in Fig. 5. indicating that judge preference for the MTASK-R model is relatively stronger with regard to informativeness."}, {"heading": "6 Discussion", "text": "Figure 6 presents examples of outputs generated by MTASK-RF model. It illustrates that responses of our model are generally not only generally adequate, but also more informative and useful.\n6Partial scores were rounded up. This affects both systems equally.\nFor example the first response combines \u201chave a safe flight\u201d, which is safe and appropriate and as such typical of existing neural conversational systems, but also \u201cnice airport terminal\u201d, which is grounded in knowledge about the specific airport. While our model sometimes regurgitates fragments of tips in its responses, it often blends together information from various tips and the conversational in order to produce a response, such as in the the 5th and last responses of the figure. The 5th is mainly influenced by two Foursquare tips7 and the model creates a fusion of the two, a kind of text manipulation that would be difficult with slot filling."}, {"heading": "7 Related Work", "text": "The present work extends the data-driven paradigm of conversation generation by injecting knowledge from textual data into models derived from conversational data. This paradigm was introduced by Ritter et al. (2011) who first proposed using statistical Machine Translation models to generate conversational responses from social media data. It has been was further advanced by the introduction of neural network\n7(\u201csit with and take a picture of the Simpsons on the 3rd floor\u201d) and (\u201cCheck out the video feed on 6 and Simpsons/billiards on 3!\u201d).\nmodels (Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,b). The introduction of contextual models by (Sordoni et al., 2015) is an important advance; we build on this by incorporating context from outside the conversation.\nThis work distinguishes itself from a second paradigm of neural dialog modeling in which question answer slots are explicitly learned from small amounts of crowd-sourced data or customer support logs (Wen et al., 2015, 2016). In many respects, this second paradigm can be characterized as an extension of conventional dialog models with or without statistical modelling, e.g., (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).\nRelevant to the current work is (Bordes and Weston, 2016), who employ memory networks to handle restaurant reservations, using a small number of keywords to handle entity types in a knowledge base (cuisine type, location, price range, party size, rating, phone number and address). That approach requires a highly structured knowledge base, whereas we are attempting to leverage free-form text using a highly scalable approach in order to learning implicit slots."}, {"heading": "8 Conclusions", "text": "We have presented a novel knowledge-grounded conversation engine that could serve as the core component of a multi-turn recommendation or conversational QA system. The model is a largescale, scalable, fully data-driven neural conversation model that effectively exploits external knowledge, and does so without explicit slot filling. It generalizes the SEQ2SEQ approach to neural conversation models by naturally combining conversational and non-conversational data through multi-task learning. Our simple entity matching approach to grounding external information based on conversation context makes for a model that is informative, versatile and applicable in open-domain systems."}, {"heading": "Acknowledgments", "text": "We thank Xuetao Yin and Leon Xu for helping us extract Foursquare data. We also thank Kevin Knight, Chris Quirk, Nebojsa Jojic, Lucy Vanderwende, Vighnesh Shiv, Yi Luan, John Wieting, Alan Ritter, Donald Brinkman, and Puneet Agrawal for helpful suggestions and discussions."}], "references": [{"title": "Luke, i am your father: dealing with out-of-domain requests by using movies subtitles", "author": ["David Ameixa", "Luisa Coheur", "Pedro Fialho", "Paulo Quaresma."], "venue": "Intelligent Virtual Agents. Springer, pages 13\u201321.", "citeRegEx": "Ameixa et al\\.,? 2014", "shortCiteRegEx": "Ameixa et al\\.", "year": 2014}, {"title": "Iris: a chat-oriented dialogue system based on the vector space model", "author": ["Rafael E Banchs", "Haizhou Li."], "venue": "Proceedings of the ACL 2012 System Demonstrations. Association for Computational Linguistics, pages 37\u201342.", "citeRegEx": "Banchs and Li.,? 2012", "shortCiteRegEx": "Banchs and Li.", "year": 2012}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston."], "venue": "CoRR abs/1605.07683.", "citeRegEx": "Bordes and Weston.,? 2016", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Learning to learn, Springer, pages 95\u2013133.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Accurate evaluation of segment-level machine translation metrics", "author": ["Yvette Graham", "Timothy Baldwin", "Nitika Mathur."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Graham et al\\.,? 2015", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of NAACL-HLT .", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of ACL.", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian Vlad Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "CoRR", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Developing non-goal dialog system based on examples of drama television", "author": ["Lasguido Nio", "Sakriani Sakti", "Graham Neubig", "Tomoki Toda", "Mirna Adriani", "Satoshi Nakamura."], "venue": "Natural Interaction with Robots, Knowbots and Smartphones, Springer,", "citeRegEx": "Nio et al\\.,? 2014", "shortCiteRegEx": "Nio et al\\.", "year": 2014}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1. Association for Computational Linguistics, pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["Alice H Oh", "Alexander I Rudnicky."], "venue": "Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems-Volume 3. Association for Computational Linguistics, pages 27\u2013", "citeRegEx": "Oh and Rudnicky.,? 2000", "shortCiteRegEx": "Oh and Rudnicky.", "year": 2000}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002a", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proc. of ACL. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002b", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Official results of the nist 2008 metrics for machine translation challenge", "author": ["M. Przybocki", "K. Peterson", "S. Bronsart"], "venue": null, "citeRegEx": "Przybocki et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Przybocki et al\\.", "year": 2008}, {"title": "Trainable approaches to surface natural language generation and their application to conversational dialog systems", "author": ["Adwait Ratnaparkhi."], "venue": "Computer Speech & Language 16(3):435\u2013455.", "citeRegEx": "Ratnaparkhi.,? 2002", "shortCiteRegEx": "Ratnaparkhi.", "year": 2002}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 583\u2013 593.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proc. of AAAI.", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A survey of available corpora for building data-driven dialogue systems", "author": ["Iulian Vlad Serban", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau."], "venue": "CoRR abs/1512.05742.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "ACL-IJCNLP. pages 1577\u20131586.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "End-to-end memory networks. In Advances in neural information processing systems", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems (NIPS). pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Proc. of ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Multi-domain neural network language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina Maria Rojas-Barahona", "Pei-hao Su", "David Vandyke", "Steve J. Young."], "venue": "CoRR abs/1603.01232.", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrk\u0161i\u0107", "PeiHao Su", "David Vandyke", "Steve Young."], "venue": "Proc. of EMNLP. Association for Computational", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1502.05698 .", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "arXiv preprint arXiv:1410.3916 .", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "Recent work (Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015) has shown that it is possible to train conversational models in an end-to-end and completely", "startOffset": 12, "endOffset": 97}, {"referenceID": 24, "context": "Recent work (Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015) has shown that it is possible to train conversational models in an end-to-end and completely", "startOffset": 12, "endOffset": 97}, {"referenceID": 23, "context": "Recent work (Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015) has shown that it is possible to train conversational models in an end-to-end and completely", "startOffset": 12, "endOffset": 97}, {"referenceID": 27, "context": "Recent work (Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015) has shown that it is possible to train conversational models in an end-to-end and completely", "startOffset": 12, "endOffset": 97}, {"referenceID": 7, "context": "It offers a framework that generalizes the SEQ2SEQ approach (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014) of most previous neural conversation models, as it naturally combines conversational and non-conversational data via multi-task learning (Caruana, 1998; Liu et al.", "startOffset": 60, "endOffset": 118}, {"referenceID": 26, "context": "It offers a framework that generalizes the SEQ2SEQ approach (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014) of most previous neural conversation models, as it naturally combines conversational and non-conversational data via multi-task learning (Caruana, 1998; Liu et al.", "startOffset": 60, "endOffset": 118}, {"referenceID": 3, "context": ", 2014) of most previous neural conversation models, as it naturally combines conversational and non-conversational data via multi-task learning (Caruana, 1998; Liu et al., 2015).", "startOffset": 145, "endOffset": 178}, {"referenceID": 11, "context": ", 2014) of most previous neural conversation models, as it naturally combines conversational and non-conversational data via multi-task learning (Caruana, 1998; Liu et al., 2015).", "startOffset": 145, "endOffset": 178}, {"referenceID": 24, "context": "The key idea of this approach is that it not only conditions responses based on conversation history (Sordoni et al., 2015), but also on external \u201cfacts\u201d that are relevant to the current context (for example, Foursquare entries as in Fig.", "startOffset": 101, "endOffset": 123}, {"referenceID": 22, "context": "While these datasets (Serban et al., 2015) have grown dramatically in size thanks in particular to social media (Ritter et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 20, "context": ", 2015) have grown dramatically in size thanks in particular to social media (Ritter et al., 2011), this data is still very far from containing discussions of every entry in Wikipedia, Foursquare, Goodreads, or IMDB.", "startOffset": 77, "endOffset": 98}, {"referenceID": 12, "context": "We train our system using multi-task learning (Luong et al., 2015) as a way of combining conversational data that is naturally associated with external data (e.", "startOffset": 46, "endOffset": 66}, {"referenceID": 7, "context": "The dialog encoder and response decoder form together a sequence-to-sequence (SEQ2SEQ model (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al.", "startOffset": 92, "endOffset": 150}, {"referenceID": 26, "context": "The dialog encoder and response decoder form together a sequence-to-sequence (SEQ2SEQ model (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al.", "startOffset": 92, "endOffset": 150}, {"referenceID": 24, "context": ", 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a).", "startOffset": 87, "endOffset": 149}, {"referenceID": 27, "context": ", 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a).", "startOffset": 87, "endOffset": 149}, {"referenceID": 8, "context": ", 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a).", "startOffset": 87, "endOffset": 149}, {"referenceID": 4, "context": "This part of our model is almost identical to prior conversational SEQ2SEQ models, except that we use gated recurrent units (GRU) (Chung et al., 2014) instead of LSTM (Hochreiter and Schmidhuber, 1997) cells.", "startOffset": 130, "endOffset": 150}, {"referenceID": 7, "context": ", 2014) instead of LSTM (Hochreiter and Schmidhuber, 1997) cells.", "startOffset": 24, "endOffset": 58}, {"referenceID": 31, "context": "3 is similar to the Memory Network model first proposed by (Weston et al., 2014; Sukhbaatar et al., 2015).", "startOffset": 59, "endOffset": 105}, {"referenceID": 25, "context": "3 is similar to the Memory Network model first proposed by (Weston et al., 2014; Sukhbaatar et al., 2015).", "startOffset": 59, "endOffset": 105}, {"referenceID": 30, "context": "Memory network models are widely used in Question Answering to make inferences based on the facts saved in the memory (Weston et al., 2015).", "startOffset": 118, "endOffset": 139}, {"referenceID": 25, "context": "Based on (Sukhbaatar et al., 2015) we have:", "startOffset": 9, "endOffset": 34}, {"referenceID": 31, "context": "The memory network model of (Weston et al., 2014) can be defined as a multi-layer structure.", "startOffset": 28, "endOffset": 49}, {"referenceID": 12, "context": "We use the same learning technique as (Luong et al., 2015) for multi-task learning.", "startOffset": 38, "endOffset": 58}, {"referenceID": 26, "context": "We use a beam-search decoder similar to (Sutskever et al., 2014) with beam size of 200, and maximum response length of 30.", "startOffset": 40, "endOffset": 64}, {"referenceID": 8, "context": "Following (Li et al., 2016a), we generate N -best lists containing three features: (1) the log-likelihood logP (R|S, F ) according to the decoder; (2) word count; (3) the log-likelihood logP (S|R) of the source given the response.", "startOffset": 10, "endOffset": 28}, {"referenceID": 8, "context": "The third feature is added to deal with the issue of generating commonplace and generic responses such as \u201cI don\u2019t know\u201d, which is discussed in details in (Li et al., 2016a).", "startOffset": 155, "endOffset": 173}, {"referenceID": 14, "context": "\u03bb and \u03b3 are free parameters, which we tune on our development N -best lists using MERT (Och, 2003) by optimizing BLEU (Papineni et al.", "startOffset": 87, "endOffset": 98}, {"referenceID": 16, "context": "\u03bb and \u03b3 are free parameters, which we tune on our development N -best lists using MERT (Och, 2003) by optimizing BLEU (Papineni et al., 2002a).", "startOffset": 118, "endOffset": 142}, {"referenceID": 24, "context": "Following (Sordoni et al., 2015; Wen et al., 2016; Li et al., 2016a), we use BLEU automatic evaluation.", "startOffset": 10, "endOffset": 68}, {"referenceID": 28, "context": "Following (Sordoni et al., 2015; Wen et al., 2016; Li et al., 2016a), we use BLEU automatic evaluation.", "startOffset": 10, "endOffset": 68}, {"referenceID": 8, "context": "Following (Sordoni et al., 2015; Wen et al., 2016; Li et al., 2016a), we use BLEU automatic evaluation.", "startOffset": 10, "endOffset": 68}, {"referenceID": 10, "context": "While (Liu et al., 2016) suggest that BLEU correlates poorly with human judgment at the sentence-level,5 we use instead corpus-level", "startOffset": 6, "endOffset": 24}, {"referenceID": 18, "context": "BLEU, which is known to better correlate with human judgments (Przybocki et al., 2008) including for response generation (Galley et al.", "startOffset": 62, "endOffset": 86}, {"referenceID": 5, "context": ", 2008) including for response generation (Galley et al., 2015).", "startOffset": 42, "endOffset": 63}, {"referenceID": 17, "context": "Automatic Evaluation: We computed perplexity and BLEU (Papineni et al., 2002b) for each system.", "startOffset": 54, "endOffset": 78}, {"referenceID": 6, "context": "chine Translation (Graham et al., 2015), as BLEU and related metrics were originally designed as corpus-level metrics.", "startOffset": 18, "endOffset": 39}, {"referenceID": 20, "context": "This paradigm was introduced by Ritter et al. (2011) who first proposed using statistical Machine Translation models to generate conversational responses from social media data.", "startOffset": 32, "endOffset": 53}, {"referenceID": 24, "context": "The introduction of contextual models by (Sordoni et al., 2015) is an important advance; we build on this by incorporating context from outside the conversation.", "startOffset": 41, "endOffset": 63}, {"referenceID": 15, "context": ", (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 2, "endOffset": 104}, {"referenceID": 19, "context": ", (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 2, "endOffset": 104}, {"referenceID": 1, "context": ", (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 2, "endOffset": 104}, {"referenceID": 0, "context": ", (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 2, "endOffset": 104}, {"referenceID": 13, "context": ", (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 2, "endOffset": 104}, {"referenceID": 2, "context": "Relevant to the current work is (Bordes and Weston, 2016), who employ memory networks to handle restaurant reservations, using a small number of keywords to handle entity types in a knowledge base (cuisine type, location, price range, party size, rating, phone number and address).", "startOffset": 32, "endOffset": 57}], "year": 2017, "abstractText": "Neural network models are capable of generating extremely natural sounding conversational interactions. Nevertheless, these models have yet to demonstrate that they can incorporate content in the form of factual information or entity-grounded opinion that would enable them to serve in more task-oriented conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. We generalize the widely-used SEQ2SEQ approach by conditioning responses on both conversation history and external \u201cfacts\u201d, allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive SEQ2SEQ baseline. Human judges found that our outputs are significantly more informative.", "creator": "LaTeX with hyperref package"}}}