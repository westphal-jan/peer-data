{"id": "1702.02640", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Character-level Deep Conflation for Business Data Analytics", "abstract": "Connecting different admitted text gianotti attributes associated pickup6thgraf with the qurnah same entity (sashaying conflation) expletives is sokollu important in 144 business hichilema data onnie analytics since atmatzidis it could help sevrin merge melikyan two privee different tables eccentrically in a database \u0f44 to provide skibbe a more vecchione comprehensive dixmude profile of aristocats an rosebuds entity. corradini However, prempeh the conflation 134.30 task natica is tranquebar challenging because tabuk two text pasukan strings 1965-68 that cziffra describe manhunt the same glycine entity hongnong could be massmutual quite 0510 different sablons from 100-mark each other woolwich for trostberg reasons such stabbings as thx misspelling. altina It tallangatta is p\u00fablica therefore portlethen critical wereld to develop m1911a1 a conflation ronquillo model that is aisl able to truly lentol understand boussac the brandauer semantic 3,202 meaning giallorossi of the strings carmon and match them .401 at the semantic level. To detest this dialectically end, we develop a sanjika character - level deep m\u00e1scara conflation kolnhofer model jogos that nasionale encodes 43-minute the 1639 input fordice text strings from character i-390 level donghe into bookfair finite 4 dimension grbs feature vectors, which astronomico are then erd used to houfei compute the 117.43 cosine xuanhua similarity between ereshkigal the text formula strings. The model is visas trained shabdrung in narcisco an gawler end - euro-mediterranean to - liechtenauer end manner using back arbi propagation kdz and stochastic gradient radiothon descent wodan to fassel maximize the likelihood of l\u00e9onie the correct association. Specifically, we cassara propose nature-based two 134-nation variants restrepo of the lv deep scrapbooks conflation model, based on 253.5 long - summerskill short - furphy term memory (LSTM) secrete recurrent tsushko neural network (mundania RNN) wootten and convolutional late-stage neural network (CNN ), respectively. Both load models perform sacharow well on otri a real - clewiston world business self-promotion analytics zipline dataset and materialists significantly ravix outperform the hippolytus baseline bag - al-malik of - contrite character (wholistic BoC) model.", "histories": [["v1", "Wed, 8 Feb 2017 22:24:14 GMT  (350kb,D)", "http://arxiv.org/abs/1702.02640v1", "Accepted for publication, at ICASSP 2017"]], "COMMENTS": "Accepted for publication, at ICASSP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhe gan", "p d singh", "ameet joshi", "xiaodong he", "jianshu chen", "jianfeng gao", "li deng"], "accepted": false, "id": "1702.02640"}, "pdf": {"name": "1702.02640.pdf", "metadata": {"source": "CRF", "title": "CHARACTER-LEVEL DEEP CONFLATION FOR BUSINESS DATA ANALYTICS", "authors": ["Zhe Gan", "P. D. Singh", "Ameet Joshi", "Xiaodong He", "Jianshu Chen", "Jianfeng Gao", "Li Deng"], "emails": ["zhe.gan@duke.edu,", "deng}@microsoft.com"], "sections": [{"heading": null, "text": "Index Terms\u2014 Deep conflation, character-level model, convolutional neural network, long-short-term memory"}, {"heading": "1. INTRODUCTION", "text": "In business data analytics, different fields and attributes related to the same entities (e.g., same person) are stored in different tables in a database or across different databases. It is important to connect these attributes so that we can get a more comprehensive and richer profile of the entity. This is important because exploiting a more comprehensive profile could lead to better prediction in business data analytics.\nSpecifically, the conflation of data aims to connect two rows from the same or different datasets that contain one or more common fields, when the values of the common fields match within a predefined threshold. For example, in the business data considered in this paper, we aim to detect whether two names refer to the same person or not \u2014 see the example in Table 1. Row A and row B represent two name fields from different tables in a database, which is a text string consisting of characters. The strings in the same column of Table 1 represent the names of a same person. There are several reasons for the strings in A and B being different: (i) possible mis-spelling typos; (ii) the lack of suffix; (iii) the reverse of family names and given names. Due to these variations and imperfection in\nEmails: zhe.gan@duke.edu, {prabhs, ameetj, xiaohe, jianshuc, jfgao, deng}@microsoft.com\ndata entries, plain keyword matching does not work well [1, 2], and we need a data conflation model in the semantic level; that is, the model should be able to identify two different character strings to be associated with a same entity.\nTo address the aforementioned challenges, we propose characterlevel deep conflation models that take the raw text strings as the input and predict whether two data entries refer to the same entity. The proposed model consists of two parts: (i) a deep feature extractor, and (ii) a ranker. The feature extractor takes the raw text string at the character level and produce a finite dimension representation of the text. In particular, we constructed two different deep architectures of feature extractors: (i) long-short-term-memory (LSTM) recurrent neural network (RNN) [3, 4] and (ii) deep convolutional neural network (CNN) [5, 6]. Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14]. Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19]. As we will show later, our proposed deep conflation model achieves high prediction accuracy in the conflation task for business data, and greatly outperform strong baselines."}, {"heading": "2. CHARACTER-LEVEL DEEP CONFLATION MODELS", "text": "We formulate the deep conflation problem as a ranking problem. That is, given a query string from field A, we rank all the target strings in field B, with the hope that the most similar string in B is ranked on the top of the list. The proposed deep conflation model consists of two parts: (i) a deep feature extractor; (ii) a ranker. Fig. 1 shows the architecture of the deep conflation model. The deep feature extractors transform the input text strings from character-level into finite dimension feature vectors. Then, the cosine similarity is computed between the query string from field A and all the target strings from field B. The cosine similarity value for each pair of text strings measures the semantic relevance between each pair of the text strings, according to which the target strings are ranked. The entire model will be trained in an end-to-end manner so that the deep feature extractors are encouraged to learn the proper feature vectors\nar X\niv :1\n70 2.\n02 64\n0v 1\n[ cs\n.C L\n] 8\nF eb\n2 01\n7\nDeep Feature Extractor\nDeep Feature Extractor\nDeep Feature Extractor\n\u2026\u2026\nQuery string Target string 1 Target string J+1 Strings\nDeep features\n\u2026\u2026 Cosine similarity\nFig. 1: Character-level deep conflation model.\nLSTM LSTM LSTM\nWe\n\u2026\nWe We\u2026\ne m \u2026 hString:\nFig. 2: LSTM based deep feature extractor.\nthat are measurable by cosine similarity. In the rest of this section, we will explain these two components of the deep conflation model with detail."}, {"heading": "2.1. Deep Feature Extractors", "text": "The inputs into the system are text strings, which are sequences of characters. Note that the order of the input characters and words is critical to understand the text correctly. For this reason, we propose to use two deep learning models that are able to retain the order information to extract features from the raw input character sequences. The two deep models we use are: (i) Recurrent Neural Networks (RNNs); (ii) Convolutional Neural Networks (CNNs).\nRNN is a nonlinear dynamic system that can be used for sequence modeling. However, during the training of a regular RNN, the components of the gradient vector can grow or decay exponentially over long sequences. This problem with exploding or vanishing gradients makes it difficult for the regular RNN model to learn long-range dependencies in a sequence [20]. A useful architecture of RNN that overcomes this problem is the Long Short-Term Memory (LSTM) structure. On the other hand, CNN is a deep feedforward neural network that first uses convolutional and max-pooling layers to capture the local and global contextual information of the input sequence, and then uses a fully-connected layer to produce a fixed-length encoding of the sequence. In sequel, we first introduce LSTM, and then CNN."}, {"heading": "2.1.1. LSTM feature extractor", "text": "The LSTM architecture [3] addresses the problem of learning longterm dependencies by introducing a memory cell, that is able to preserve the state over long periods of time. Specifically, each LSTM unit has a cell containing a state ct at time t. This cell can be viewed as a memory unit. Reading or writing the memory unit is controlled through sigmoid gates: input gate it, forget gate f t, and output gate\not. The hidden units ht are updated as follows:\nit = \u03c3(Wixt +Uiht\u22121 + bi) , (1) f t = \u03c3(Wfxt +Ufht\u22121 + bf ) , (2) ot = \u03c3(Woxt +Uoht\u22121 + bo) , (3) c\u0303t = tanh(Wcxt +Ucht\u22121 + bc) , (4) ct = f t ct\u22121 + it c\u0303t , (5) ht = ot tanh(ct) , (6)\nwhere \u03c3(\u00b7) denotes the logistic sigmoid function, and represents the element-wise multiply operator. Wi Wf , Wo, Wc, Ui, Uf , Uo, Uc, bi, bf , bo and bc are the free model parameters to be learned from training data.\nGiven the text string q = [q1, . . . , qT ], where qt is the one-hot vector representation of character at position t and T is the number of characters, we first embed the characters into a vector space via a linear transform xt = Weqt, where We is the embedding matrix. Then for every time step, we feed the embedding vector of characters in the text string to LSTM:\nxt = Weqt, t \u2208 {1, . . . , T} , (7) ht = LSTM(xt), t \u2208 {1, . . . , T} , (8)\nwhere the operator LSTM(\u00b7) denotes the operations defined in (1)- (6). For example, in Fig. 2, the string emilio yentsch is fed into the LSTM. The final hidden vector is taken as the feature vector for the string, i.e., y = hT . We repeat this process for the query text and all the target texts so that we will have yQ and yDj (j = 1, . . . J + 1), which will be fed into the ranker to compute cosine similarity (see Sec. 2.2).\nIn the experiments, we use a bidirectional LSTM to extract sequence features, which consists of two LSTMs that are run in parallel: one on the input sequence and the other on the reverse of the input sequence. At each time step, the hidden state of the bidirectional LSTM is the concatenation of the forward and backward hidden states."}, {"heading": "2.1.2. CNN feature extractor", "text": "Next, we consider the CNN for string feature extraction. Similar to the LSTM-based model, we first embed characters to vectors xt = Weqt and then concatenating these vectors:\nx1:T = [x1, . . . ,xT ] . (9)\nThen we apply convolution operation on the character embedding vectors. We use three different convolution filters, which have the size of two (bigram), three (trigram) and four (4-gram), respectively. These different convolution filters capture the context information of different lengths. The t-th convolution output using window size c is given by\nhc,t = tanh(Wcxt:t+c\u22121 + bc) , (10)\nwhere the notation xt:t+c\u22121 denotes the vector that is constructed by concatenating xt to xt+c\u22121. That is, the filter is applied only to window t : t + c \u2212 1 of size c. Wc is the convolution weight and bc is the bias. The feature map of the filter with convolution size c is defined as\nhc = [hc,1,hc,2, . . . ,hc,T\u2212c+1] . (11)\nWe apply max-pooling over the feature maps of the convolution size c and denote it as\nh\u0302c = max{hc,1,hc,2, . . . ,hc,T\u2212c+1} , (12)\nwhere the max is a coordinate-wise max operation. For convolution feature maps of different sizes c = 2, 3, 4, we concatenate them to form the feature representation vector of the whole character sequence: h = [h\u03022, h\u03023, h\u03024] . Observe that the convolution operations explicitly capture the local (short-term) context information in the character strings, while the max-pooling operation aggregates the information from different local filters into a global representation of the input sequence. These local and global operations enable the model to encode different levels of dependency in the input sequence.\nThe above vector h is the final feature vector extracted by CNN and will be fed into the ranker, i.e., y = h. We repeat this process for the query text and all the target texts so that we will have yQ and yDj (j = 1, . . . J + 1). The above feature extraction process using CNN is illustrated in Fig. 3.\nThere exist other CNN architectures in the literature [6, 21, 22]. We adopt the CNN model in [5, 23] due to its simplicity and excellent performance on classification. Empirically, we found that it can extract high-quality text string representations for ranking."}, {"heading": "2.1.3. Comparison between the two deep feature extractors", "text": "Compared with the LSTM feature extractor, a CNN feature extractor may have the following advantages [24]. First, the sparse connectivity of a CNN, which indicates fewer parameters are required, typically improves its statistical efficiency as well as reduces memory requirements. Second, a CNN is able to encode regional (ngram) information containing rich linguistic patterns. Furthermore, an LSTM encoder might be disproportionately influenced by characters appearing later in the sequence, while the CNN gives largely uniform importance to the signal coming from each of the characters in the sequence. This makes the LSTM excellent at language modeling, but potentially suboptimal at encoding n-gram information placed further back into the sequence."}, {"heading": "2.2. Ranker", "text": "Now that we have extracted deep feature vectors yQ, yD1 ,..., yDJ+1 from the query and candidate strings, we can proceed to compute their semantic relevance scores by computing their corresponding\ncosine similarity between query Q and each j-th target string Dj . More formally, it is defined as\nR(Q,Dj) = y>QyDj\n||yQ|| \u00b7 ||yDj || , (13)\nwhere Dj denotes the j-th target string. At test time, given a query, the candidates are ranked by this relevance scores."}, {"heading": "2.3. Training of the deep conflation model", "text": "We now explain how the deep conflation model could be trained in an end-to-end manner. Given that we have the relevance scores between the query string and each of the target stringDj : R(Q,Dj), we define the posterior probability of the correct candidate given the query by the following softmax function\nP (D+|Q) = exp(\u03b3R(Q,D +))\u2211\nD\u2032\u2208D exp(\u03b3R(Q,D \u2032))\n, (14)\nwhereD+ denotes the correct target string (the positive sign denotes that it is a positive sample), \u03b3 is a tuning hyper-parameter in the softmax function (to be tuned empirically on a validation set). D denotes the set of candidate strings to be ranked, which includes the positive sample D+ and J randomly selected incorrect (negative) candidates {D\u2212j ; j = 1, . . . , J}. The model parameters are learned to maximize the likelihood of the correct candidates given the queries across the training set. That is, we minimize the following loss function\nL(\u03b8) = \u2212 log \u220f\n(Q,D+)\nP (D+|Q) , (15)\nwhere the product is over all training samples, and \u03b8 denotes the parameters (to be learned), including all the model parameters in the deep feature extractors. The above cost function is minimized by back propagation and (mini-batch) stochastic gradient descent."}, {"heading": "3. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "3.1. Dataset", "text": "We evaluate the performance of our proposed deep conflation model on a corporate proprietary business dataset. Since each string can be considered as a sequence of characters, the vocabulary size is 32 (including one period symbol and one space symbol), which includes the following elements:\nDMPSabcdefghijklmnopqrstuvwxyz.\nSpecifically, the dataset contains 10, 000 pairs of query and the associated correct target string (manually annotated). The average length of the string is 14.47 with standard deviation 2.89. The maximum length of the strings is 26 and the minimum length is 6."}, {"heading": "3.2. Setup", "text": "We provide the deep conflation results using LSTM and CNN for feature extraction, respectively. Furthermore, we also implement a baseline using Bag-of-Characters (BoC) representation of input text string. This BoC vector is then sent into a two-hidden-layer (fullyconnected) feed-forward neural networks. In our experiment, we implement 10-fold cross validation, and in each fold, we randomly select 80% of the samples as training, 10% as validation, and the\nrest 10% as testing dataset. No specific hyper-parameter tuning is implemented, other than early stopping on the validation set.\nFor the feed-forward neural network encoder based on the BoC representation, we use two hidden layers, each layer contains 300 hidden units, hence each string is embedded as a 300-dimensional vector. For LSTM and CNN encoder, we first embed each character into a 128-dimensional vector. Based on this, for the bidirectional LSTM encoder, we further use one hidden layer of 128 units for sequence embedding, hence each text string is represented as a 256- dimensional vector. For the CNN encoder, we employ filter windows of sizes {2,3,4} with 100 feature maps each, hence each text string is represented as a 300-dimensional vector.\nFor training, all weights in the CNN and non-recurrent weights in the LSTM are initialized from a uniform distribution in [- 0.01,0.01]. Orthogonal initialization is employed on the recurrent matrices in the LSTM [25]. All bias terms are initialized to zero. It is observed that setting a high initial forget gate bias for LSTMs can give slightly better results [26]. Hence, the initial forget gate bias is set to 3 throughout the experiments. Gradients are clipped if the norm of the parameter vector exceeds 5 [10]. The Adam algorithm [27] with learning rate 2 \u00d7 10\u22124 is utilized for optimization. For both the LSTM and CNN models, we use mini-batches of size 100. The hyper-parameter \u03b3 is set to 10. The number of negative candidates J is set to 50, which are randomly sampled from the rest of the candidate strings excluding the correct one. All experiments are implemented in Theano [28] on a NVIDIA Tesla K40 GPU. For reference, the training of a CNN model takes around 45 minutes to go through the dataset 20 times."}, {"heading": "3.3. Results", "text": "Performance is evaluated using Recall@K, which measures the average times a correct item is found within the top-K retrieved results. Results are summarized in Table 2. As can be seen, both of the proposed deep conflation models with LSTM and CNN feature extractors achieve superior performance compared to the BoC baseline. This is not surprising, since sequential order information is utilized in LSTM and CNN. Furthermore, we observe that CNN significantly outperforms LSTM on this task. We hypothesize that\nthis observation is due to the fact that the local (regional) sequential order information (captured by CNN) is more important than the gloabl sequential order information (captured by LSTM) in matching two names. For example, if we reverse the family name and given name of a given query name, LSTM might be more prone to mistakenly classifying these two names to be different, while in fact they refer to the same person.\nFor further analysis, we checked the CNN results on one predefined train/validation/test splits of the dataset. When CNN is used, for Recall@1, out of 1,000 test samples, only 5 samples are mistakenly retrieved. In Table 4, we show an example of the mistaken case. We can see that the mistakenly retrieved case is quite reasonable. Even humans will make mistakes on these cases. Other four mistakenly retrieved cases are similar and are omitted due to space limit. The average scores for each of the top four retrieved items are given in Table 3. This suggests that, when judging whether two text strings have the same meaning, we can empirically set the threshold to be (0.792 + 0.448)/2 = 0.62. That is, when the similarity score between two strings is higher than 0.62, we can safely conclude that they refer to the same entity, and we can then conflate the corresponding two rows accordingly."}, {"heading": "4. CONCLUSION", "text": "We have proposed a deep conflation model for matching two text fields in business data analytics, with two different variants of feature extractors, namely, long-short-term memory (LSTM) and convolutional neural networks (CNN). The model encodes the input text from raw character-level into finite dimensional feature vectors, which are used for computing the corresponding relevance scores. The model is learned in an end-to-end manner by back propagation and stochastic gradient descent. Since both LSTM and CNN feature extractors retain the order information in the text, the deep conflation model achieve superior performance compared to the bag-ofcharacter (BoC) baseline."}, {"heading": "5. REFERENCES", "text": "[1] Vetle I Torvik and Neil R Smalheiser, \u201cAuthor name disambiguation in medline,\u201d TKDD, 2009.\n[2] Stas\u030ca Milojevic\u0301, \u201cAccuracy of simple, initials-based methods for author name disambiguation,\u201d Journal of Informetrics, 2013.\n[3] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d in Neural computation, 1997.\n[4] T. Mikolov, M. Karafia\u0301t, L. Burget, J. Cernocky\u0300, and S. Khudanpur, \u201cRecurrent neural network based language model,\u201d in INTERSPEECH, 2010.\n[5] Y. Kim, \u201cConvolutional neural networks for sentence classification,\u201d in EMNLP, 2014.\n[6] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, \u201cA convolutional neural network for modelling sentences,\u201d in ACL, 2014.\n[7] Andrew M Dai and Quoc V Le, \u201cSemi-supervised sequence learning,\u201d in Advances in Neural Information Processing Systems, 2015.\n[8] N. Kalchbrenner and P. Blunsom, \u201cRecurrent continuous translation models.,\u201d in EMNLP, 2013.\n[9] K. Cho, B. Van Merrie\u0308nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn encoder-decoder for statistical machine translation,\u201d in EMNLP, 2014.\n[10] I. Sutskever, O. Vinyals, and Q. Le, \u201cSequence to sequence learning with neural networks,\u201d in NIPS, 2014.\n[11] F. Meng, Z. Lu, M. Wang, H. Li, W. Jiang, and Q. Liu, \u201cEncoding source language with convolutional neural network for machine translation,\u201d in ACL, 2015.\n[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck, \u201cLearning deep structured semantic models for web search using clickthrough data,\u201d in CIKM, 2013.\n[13] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gre\u0301goire Mesnil, \u201cA latent semantic model with convolutionalpooling structure for information retrieval,\u201d in CIKM, 2014.\n[14] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward, \u201cDeep sentence embedding using long short-term memory networks: Analysis and application to information retrieval,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.\n[15] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush, \u201cCharacter-aware neural language models,\u201d AAAI, 2016.\n[16] Wang Ling, Tiago Lu\u0131\u0301s, Lu\u0131\u0301s Marujo, Ramo\u0301n Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso, \u201cFinding function in form: Compositional character models for open vocabulary word representation,\u201d arXiv:1508.02096, 2015.\n[17] Xiang Zhang, Junbo Zhao, and Yann LeCun, \u201cCharacter-level convolutional networks for text classification,\u201d in NIPS, 2015.\n[18] David Golub and Xiaodong He, \u201cCharacter-level question answering with attention,\u201d EMNLP, 2016.\n[19] Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio, \u201cA character-level decoder without explicit segmentation for neural machine translation,\u201d arXiv:1603.06147, 2016.\n[20] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio, \u201cOn the difficulty of training recurrent neural networks.,\u201d in ICML, 2013.\n[21] B. Hu, Z. Lu, H. Li, and Q. Chen, \u201cConvolutional neural network architectures for matching natural language sentences,\u201d in NIPS, 2014.\n[22] R. Johnson and T. Zhang, \u201cEffective use of word order for text categorization with convolutional neural networks,\u201d in NAACL HLT, 2015.\n[23] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, \u201cNatural language processing (almost) from scratch,\u201d in JMLR, 2011.\n[24] Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin, \u201cUnsupervised learning of sentence representations using convolutional neural networks,\u201d arXiv:1611.07897, 2016.\n[25] A. M. Saxe, J. L. McClelland, and S. Ganguli, \u201cExact solutions to the nonlinear dynamics of learning in deep linear neural networks,\u201d in ICLR, 2014.\n[26] Q. V. Le, N. Jaitly, and G. E. Hinton, \u201cA simple way to initialize recurrent networks of rectified linear units,\u201d arXiv:1504.00941, 2015.\n[27] Diederik Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization,\u201d in ICLR, 2015.\n[28] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. Warde-Farley, and Y. Bengio, \u201cTheano: new features and speed improvements,\u201d arXiv:1211.5590, 2012."}], "references": [{"title": "Author name disambiguation in medline", "author": ["Vetle I Torvik", "Neil R Smalheiser"], "venue": "TKDD, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Accuracy of simple, initials-based methods for author name disambiguation", "author": ["Sta\u0161a Milojevi\u0107"], "venue": "Journal of Informetrics, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "ACL, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le"], "venue": "Advances in Neural Information Processing Systems, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "NIPS, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["F. Meng", "Z. Lu", "M. Wang", "H. Li", "W. Jiang", "Q. Liu"], "venue": "ACL, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "CIKM, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A latent semantic model with convolutionalpooling structure for information retrieval", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "CIKM, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval", "author": ["Hamid Palangi", "Li Deng", "Yelong Shen", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Xinying Song", "Rabab Ward"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "AAAI, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "arXiv:1508.02096, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "NIPS, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Character-level question answering with attention", "author": ["David Golub", "Xiaodong He"], "venue": "EMNLP, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv:1603.06147, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "NIPS, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "NAACL HLT, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised learning of sentence representations using convolutional neural networks", "author": ["Zhe Gan", "Yunchen Pu", "Ricardo Henao", "Chunyuan Li", "Xiaodong He", "Lawrence Carin"], "venue": "arXiv:1611.07897, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "ICLR, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv:1504.00941, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "ICLR, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "arXiv:1211.5590, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "data entries, plain keyword matching does not work well [1, 2], and we need a data conflation model in the semantic level; that is, the model should be able to identify two different character strings to be associated with a same entity.", "startOffset": 56, "endOffset": 62}, {"referenceID": 1, "context": "data entries, plain keyword matching does not work well [1, 2], and we need a data conflation model in the semantic level; that is, the model should be able to identify two different character strings to be associated with a same entity.", "startOffset": 56, "endOffset": 62}, {"referenceID": 2, "context": "In particular, we constructed two different deep architectures of feature extractors: (i) long-short-term-memory (LSTM) recurrent neural network (RNN) [3, 4] and (ii) deep convolutional neural network (CNN) [5, 6].", "startOffset": 151, "endOffset": 157}, {"referenceID": 3, "context": "In particular, we constructed two different deep architectures of feature extractors: (i) long-short-term-memory (LSTM) recurrent neural network (RNN) [3, 4] and (ii) deep convolutional neural network (CNN) [5, 6].", "startOffset": 151, "endOffset": 157}, {"referenceID": 4, "context": "In particular, we constructed two different deep architectures of feature extractors: (i) long-short-term-memory (LSTM) recurrent neural network (RNN) [3, 4] and (ii) deep convolutional neural network (CNN) [5, 6].", "startOffset": 207, "endOffset": 213}, {"referenceID": 5, "context": "In particular, we constructed two different deep architectures of feature extractors: (i) long-short-term-memory (LSTM) recurrent neural network (RNN) [3, 4] and (ii) deep convolutional neural network (CNN) [5, 6].", "startOffset": 207, "endOffset": 213}, {"referenceID": 4, "context": "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].", "startOffset": 226, "endOffset": 232}, {"referenceID": 6, "context": "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].", "startOffset": 226, "endOffset": 232}, {"referenceID": 7, "context": "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].", "startOffset": 254, "endOffset": 268}, {"referenceID": 8, "context": "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].", "startOffset": 254, "endOffset": 268}, {"referenceID": 9, "context": "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].", "startOffset": 254, "endOffset": 268}, {"referenceID": 10, "context": "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].", "startOffset": 254, "endOffset": 268}, {"referenceID": 11, "context": "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].", "startOffset": 295, "endOffset": 307}, {"referenceID": 12, "context": "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].", "startOffset": 295, "endOffset": 307}, {"referenceID": 13, "context": "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].", "startOffset": 295, "endOffset": 307}, {"referenceID": 14, "context": "Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19].", "startOffset": 161, "endOffset": 181}, {"referenceID": 15, "context": "Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19].", "startOffset": 161, "endOffset": 181}, {"referenceID": 16, "context": "Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19].", "startOffset": 161, "endOffset": 181}, {"referenceID": 17, "context": "Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19].", "startOffset": 161, "endOffset": 181}, {"referenceID": 18, "context": "Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19].", "startOffset": 161, "endOffset": 181}, {"referenceID": 19, "context": "This problem with exploding or vanishing gradients makes it difficult for the regular RNN model to learn long-range dependencies in a sequence [20].", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "The LSTM architecture [3] addresses the problem of learning longterm dependencies by introducing a memory cell, that is able to preserve the state over long periods of time.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "There exist other CNN architectures in the literature [6, 21, 22].", "startOffset": 54, "endOffset": 65}, {"referenceID": 20, "context": "There exist other CNN architectures in the literature [6, 21, 22].", "startOffset": 54, "endOffset": 65}, {"referenceID": 21, "context": "There exist other CNN architectures in the literature [6, 21, 22].", "startOffset": 54, "endOffset": 65}, {"referenceID": 4, "context": "We adopt the CNN model in [5, 23] due to its simplicity and excellent performance on classification.", "startOffset": 26, "endOffset": 33}, {"referenceID": 22, "context": "We adopt the CNN model in [5, 23] due to its simplicity and excellent performance on classification.", "startOffset": 26, "endOffset": 33}, {"referenceID": 23, "context": "Compared with the LSTM feature extractor, a CNN feature extractor may have the following advantages [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "Orthogonal initialization is employed on the recurrent matrices in the LSTM [25].", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "It is observed that setting a high initial forget gate bias for LSTMs can give slightly better results [26].", "startOffset": 103, "endOffset": 107}, {"referenceID": 9, "context": "Gradients are clipped if the norm of the parameter vector exceeds 5 [10].", "startOffset": 68, "endOffset": 72}, {"referenceID": 26, "context": "The Adam algorithm [27] with learning rate 2 \u00d7 10\u22124 is utilized for optimization.", "startOffset": 19, "endOffset": 23}, {"referenceID": 27, "context": "All experiments are implemented in Theano [28] on a NVIDIA Tesla K40 GPU.", "startOffset": 42, "endOffset": 46}], "year": 2017, "abstractText": "Connecting different text attributes associated with the same entity (conflation) is important in business data analytics since it could help merge two different tables in a database to provide a more comprehensive profile of an entity. However, the conflation task is challenging because two text strings that describe the same entity could be quite different from each other for reasons such as misspelling. It is therefore critical to develop a conflation model that is able to truly understand the semantic meaning of the strings and match them at the semantic level. To this end, we develop a character-level deep conflation model that encodes the input text strings from character level into finite dimension feature vectors, which are then used to compute the cosine similarity between the text strings. The model is trained in an end-to-end manner using back propagation and stochastic gradient descent to maximize the likelihood of the correct association. Specifically, we propose two variants of the deep conflation model, based on long-short-term memory (LSTM) recurrent neural network (RNN) and convolutional neural network (CNN), respectively. Both models perform well on a real-world business analytics dataset and significantly outperform the baseline bag-of-character (BoC) model.", "creator": "LaTeX with hyperref package"}}}