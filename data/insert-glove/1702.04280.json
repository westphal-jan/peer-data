{"id": "1702.04280", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2017", "title": "DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional Neural Network", "abstract": "This plass paper tregony describes steibel the details optimizers of tatishvili Sighthound ' compulsivity s c-141 fully uytdehaage automated khunn age, gender and hornbills emotion recognition system. The licencee backbone claycomo of tyagachev our olick system consists of several posovalyuk deep convolutional sottile neural networks that gamber are not only comer computationally inexpensive, kontinental but 1932/33 also provide trkb state - 248.4 of - kochanski the - edgeware art bryologist results intensive-care on several josemi competitive benchmarks. To hosen power ricard our novel deep networks, milak we scutage collected large labeled dehl datasets cornmeal through a bioturbation semi - fendalton supervised pipeline to reduce the zung annotation effort / benthem time. We soundex tested fallback our system on several anti-slavery public quattlebaum benchmarks centers and hortefeux report 2.6-percent outstanding earning results. Our kerckhoven age, poppea gender and emotion 51-man recognition nitwits models iht are available phenomenological to developers non-executive through the as-salih Sighthound 6/9 Cloud hatzair API at", "histories": [["v1", "Tue, 14 Feb 2017 16:34:05 GMT  (9242kb,D)", "http://arxiv.org/abs/1702.04280v1", "10 Pages"], ["v2", "Sat, 4 Mar 2017 01:43:04 GMT  (9242kb,D)", "http://arxiv.org/abs/1702.04280v2", "10 Pages"]], "COMMENTS": "10 Pages", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["afshin dehghan", "enrique g ortiz", "guang shu", "syed zain masood"], "accepted": false, "id": "1702.04280"}, "pdf": {"name": "1702.04280.pdf", "metadata": {"source": "CRF", "title": "DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional Neural Networks", "authors": ["Afshin Dehghan", "Enrique G. Ortiz", "Guang Shu", "Syed Zain Masood"], "emails": ["zainmasood}@sighthound.com"], "sections": [{"heading": "1 Introduction", "text": "Facial attribute recognition, including age, gender and emotion, [1,2,3,4,5,6,7] has been a topic of interest among computer vision researchers for over a decade. One of the key reasons is the numerous applications of this challenging problem which range from security control, to person identification, to human-computer interaction. Due to the release of large labeled datasets, as well as the advances made in the design of convolutional neural networks, error rates have dropped significantly. In many cases, these systems are able to outperform humans [5]. However, this still remains a difficult problem and existing commercial systems fall short when dealing with real world scenarios. In this work, we present an end-to-end system capable of estimating facial attributes including age, gender and emotion with low error rates. In order to support our claims, we tested our system on several benchmarks and achieved results better than the previous state-of-the-art. The contributions of this work are summarized below.\nWe present an end-to-end pipeline, along with novel deep networks, that not only are computationally inexpensive, but also outperform competitive methods on several benchmarks. We present large datasets for age, emotion and gender recognition that are used to train state-of-the-art deep neural networks. We conducted a number of experiments on existing benchmarks and obtained leading results on all of them. ar X\niv :1\n70 2.\n04 28\n0v 1\n[ cs\n.C V\n] 1\n4 Fe\nb 20\n17"}, {"heading": "2 System Overview", "text": "The pipeline of our system is shown in Figure 1. Our first deep model is trained on a large dataset of four million images for the task of face recognition. This model serves as the backbone to our facial attribute recognizers and is used to fine-tune networks for four tasks: real age estimation, apparent age estimation, gender recognition and emotion recognition. What follows explains each of the steps in more detail."}, {"heading": "2.1 Training", "text": "Below we describe different steps involved in training our models in more detail.\nData collection: Data collection plays an important role in training any deep neural network (DNN). In this paper, we aim to label data for three\nseparate tasks: age, gender and emotion recognition. Collecting labeled data for some tasks, such as real age estimation, is much more challenging compared to popular classification [8,9] or detection [10] problems. This disparity is due to the fact that human error in estimating real age is large (sometimes greater than the computer vision estimations) and one cannot rely on human annotators to label faces with their corresponding real age. However, at Sighthound we have collected a large dataset of faces with their corresponding age, gender and emotion labels. To our knowledge, our datasets are the largest or among the largest in either the academic or commercial world. Below we provide some statistics on the data used for training our models. Face recognition: The base model for our facial recognition is trained on over four million images of more than 40, 000 individuals. The large variation in images of each identity make our deep model robust to common challenges in face recognition. Our face recognition model is available to developers through the Sighthound Cloud API 1. Age estimation: Recently there have been some efforts in collecting data with corresponding age labels [5,11,4]. Among those, the dataset proposed by Rothe et al. in [4] is the largest dataset that contains 523, 051 images and is available for research purposes. However, the dataset is not carefully annotated and contains many mistakes. Additionally the distribution of the data across different ages is highly unbalanced. This led to the authors using only half of the data for training in the original paper [4]. To better address this problem we collected a large dataset of \u223c 600, 000 images with corresponding age labels. In contrast to previous works, our dataset has a more balanced distribution across different ages. For example we have over 120, 000 people in our dataset with labeled ages over 70 or younger than 20 years of age. We used a team of human annotators to further clean our dataset through a semi-supervised procedure. Gender and emotion recognition: Our four million faces labeled for the task of face recognition are also labeled with their corresponding gender. To better improve our model, we added tens of thousands of images of different ethnicities as well as age groups. Additionally, we also annotated part of our data with emotion labels for the task of emotion recognition.\nData pre-processing: We pre-process each image before feeding them to our DNNs. These pre-processing steps include face detection, facial landmark detection and alignment. We used Sighthound\u2019s face detection which is available through our cloud API. If more than one face is detected in an image, we choose the most centered one. (This is especially the case in the ChaLearn v2 dataset which contains multiple faces). In the Chalearn dataset we were able to detect all faces using a combination of techniques, but all using the Sighthound Cloud APIs.2 Given the face bounding boxes, we detect\n1 https://www.sighthound.com/products/cloud 2 Not all of the face detection bounding boxes generated by the cloud API were perfectly accurate, mostly due to occlusion or low resolution of some images. However,\n68 facial landmarks and use those for alignment. Finally the aligned faces are all cropped and resized to a fixed size. In contrast to some previous works, which do not use any face alignment [4], we found this to be important in our final accuracy numbers.\nDeep training: As shown in Figure 1, we start by training a deep neural network for the task of face recognition using four million images of over 40, 000 identities. Our face recognition model is not only computationally inexpensive (with feature extraction time of 70ms using just the CPU), but also achieves outstanding results on the LFW dataset [2]. This model serves as the backbone of our facial attribute recognition engine. We designed a highly optimized deep network architecture for accuracy and speed for each task. In some recent works [12], researchers try to design a network which performs all tasks at the same time, and they have shown marginal improvements. However, having separate networks for each task allowed us to design faster and more portable models for each task. Additionally running all models combined takes less time compared to the all-in-one model of [12] and we achieve better results. We should add that even though the network architecture is not the same for each task, all networks are trained first for the task of facial recognition using the four million image set.\nwhen comparing with other methods on pubic datasets, we directly used the output of our face detection for age estimation without further adjustment."}, {"heading": "3 Experiments", "text": "In this section, we report experimental results on several publicly available datasets as well as our internal datasets."}, {"heading": "3.1 Real Age Estimation", "text": "For real age estimation, we report results on two publicly available datasets, the Group dataset [5] and the Adience dataset [11]. The images in these datasets are labeled with their corresponding age groups. In order to further evaluate our system on estimating the actual age and not only the age group, we collected an internal dataset of 3, 800 images, on which we report the results of the proposed method in addition to other competitive algorithms.\nIn Table 1, we present quantitative results on our Sighthound dataset, containing 3, 800 test images. Each image is labeled with its corresponding age ranging from 10 to 90 years old. Unlike the Adience and Group datasets, our dataset includes the exact age labels for each image and not the age groups. We compare our results with [4], whose model is trained on the IMDB-Wiki dataset. Additionally, we compare our system with available commercial APIs: Microsoft [13], Face++ [15] and Kairos [14]. For quantitative comparison, we used the Mean Absolute Error (MAE) which is commonly used in the literature [4]. As can be seen, our method outperforms competitive approaches by a significant margin. 3\nNext, we provide results on the Group dataset, which contains 28, 231 faces collected from Flickr and labeled with one of seven age categories roughly correspond to different life stages. Most faces are low-resolution making it more challenging for accurate age estimation. The median of faces are reported to have only 18.5 pixels between the eye centers. We followed the setup in [5] where 3 We compute the error rate for Microsoft and Face++ using the versions of their cloud API available in October 2016.\n3, 500 images are used for training and 1, 050 images are used for testing. Both training and testing images are equally distributed across seven age groups. The age classification results in terms of accuracy along with a confusion table are reported in Table 2 and Figure 3. We can observe that Sighthound\u2019s age estimation outperforms the latest research results.\nFinally we report results on the Adience benchmark. The entire Adience benchmark includes roughly 26, 000 images of 2, 284 subjects. However, some images are not annotated with corresponding age groups. Therefore the total number of images used for final testing is smaller than 26K. We used the standard 5-fold cross validation experiment defined for this set. When testing on each fold, the rest of the folds are used to fine-tune our model for the eight age groups defined in the dataset. The results of our method along with competitive approaches are shown in Table 3. Once again, our method improves on the best reported results on this dataset."}, {"heading": "3.2 Apparent Age Estimation", "text": "The apparent age of a person could be very different from the real age of a person. Recently, thanks to the availability of the Chalearn LAP Apparent Age Estimation dataset and challenge [19], several researchers have focused on designing models that are focused on predicting the apparent age, rather than the\nactual age. In the most recent version of the competition, the size of the dataset was extended to 7, 591 images where 4, 113 of them are used for training and 1, 500 and 1, 978 are used for validation and testing respectively. Each image in the dataset is annotated using at least 10 human votes and the mean (\u00b5) and standard devision (\u03c3) of the votes are recorded and released with the dataset. Given the prediction for each image (x), the error for each image is computed using = 1\u2212 exp\u2212 (x\u0302\u2212\u00b5) 2\n2\u03c32 . This means the apparent age on an image with small standard deviation gets penalized more compared to one with larger standard deviation. The winner of the competition [6] used a multi CNN framework and achieved a test error of 0.2411. However, this method [6] has several limitations. Their network architecture is an order of magnitude slower in speed and an order of magnitude larger in size compared to ours. Additionally, the multiple CNNs (minimum of 88 forward passes for each image) in their pipeline makes it impossible to use their system in a real-time or even close-to real-time application. We should also mention that all the runner up approaches suffer from the same limitations. Even though our goal is to keep the computational complexity low, we still achieve the outstanding error rate of 0.319, which places our approach second."}, {"heading": "3.3 Emotion Recognition", "text": "There are several public datasets for emotion recognition. FER-2013 [22] and EmotiW are among the popular ones. The FER dataset contains low-quality gray scale images of size 48 \u00d7 48 which is not very representative of real world scenarios. Access to the EmotiW dataset was not granted to us. Thus we collected our own dataset of 2, 156 images. Each image is labeled with one of the 7 labels of \"happy\", \"sad\", \"neutral\", \"disgusted\", \"surprised\", \"fearful\" and \"angry\". The data has a relatively equal distribution across the 7 emotions. We compared our method with the Microsoft Face API [13]. The results as well as the confusion\ntables are shown in Table 5 and Figure 4 respectively. As shown, Sighthound\u2019s emotion recognition system outperforms Microsoft by a 15% margin. 4"}, {"heading": "3.4 Gender Recognition", "text": "We compared our gender recognition model on the Adience benchmark with other leading methods. The Adience benchmark contains 17, 492 faces labeled with their corresponding gender. The faces are divided into 5 folds. However, we used the same model across all folds without further fine-tuning. Along with published state-of-the-art results, we compare our method with a couple of commercial APIs such as [15] and [14]. The results reported in Table 6 clearly 4 Microsoft\u2019s API failed detecting a face in 193 images. To be fair to Microsoft we removed these images while evaluating their method.\nshow Sighthound\u2019s enhanced gender recognition capability compared to recent research publications and commercial products."}, {"heading": "4 Conclusions", "text": "In this paper, we present an end to end system for age, gender and emotion recognition. We show that our novel deep architecture, along with our large, in-house collected data, can outperform competitive commercial and academic algorithms on several benchmarks."}], "references": [{"title": "Analysis of emotion recognition using facial expressions, speech and multimodal information", "author": ["Busso", "Carlos", "e.a."], "venue": "Proceedings of the 6th international conference on Multimodal interfaces.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Emotion recognition in the wild via convolutional neural networks and mapped binary patterns", "author": ["G. Levi", "T. Hassner."], "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. ACM.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Mutlimodal learning with deep boltzmann machine for emotion prediction in user generated videos", "author": ["L. Pang", "C.W. Ngo."], "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Retrieval. ACM.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Dex: Deep expectation of apparent age from a single image", "author": ["R. Rothe", "L.V.G. Radu Timofte"], "venue": "International Conference on Computer Vision (ICCV),.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding images of groups of people", "author": ["A.C. Gallagher", "T. Chen."], "venue": "CVPR.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Apparent age estimation from face images combining general and children-specialized deep learning models", "author": ["Antipov", "Grigory", "e.a."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Deeply-learned feature for age estimation", "author": ["X. Wang", "R. Guo", "C. Kambhamettu"], "venue": "WACV.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "View independent vehicle make, model and color recognition using convolutional neural network", "author": ["A. Dehghan", "S.Z. Masood", "G. Shu", "E.G. Ortiz."], "venue": "arXiv:1702.01721.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "e.a."], "venue": "Computer Vision and Pattern Recognition.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "E.S.M.A.V.G.L.W.C.K.I.W.J.", "A. Zisserman"], "venue": "International Journal of Computer Vision.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Age and gender estimation of unfiltered faces", "author": ["E. Eidinger", "R. Enbar", "T. Hassner."], "venue": "IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "An all-in-one convolutional neural network for face analysis", "author": ["R. Ranjan", "S. Sankaranarayanan", "C.D. Castillo", "R. Chellappa"], "venue": "arXiv:1611.00851.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Squared earth mover\u2019s distance-based loss for training deep neural networks", "author": ["L. Hou", "C.P. Yu", "D. Samaras."], "venue": "arXiv.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic age estimation based on deep learning algorithm", "author": ["D. Yuan", "Y. Liu", "S. Lian."], "venue": "Neurocomputing.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Age and gender classification using convolutional neural networks", "author": ["G. Levi", "T. Hassner."], "venue": "CVPRW.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Chalearn looking at people and faces of the world: Face analysis workshop and challenge 2016", "author": ["Escalera", "Sergio", "e.a."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep age distribution learning for apparent age estimation", "author": ["Z. Huo", "X. Yang", "C. Xing", "Y. Zhou", "P. Hou", "J. Lv", "X. Geng"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Structured output SVM prediction of apparent age, gender and smile from deep features", "author": ["M. U\u0159i\u010d\u00e1\u0159", "R. Timofte", "R. Rothe", "J. Matas", "L.V. Gool"], "venue": "Proceedings of IEEE conference on Computer Vision and Pattern Recognition Workshops, Las Vegas, USA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Challenges in representation learning: A report on three machine learning contests", "author": ["I.J. Goodfellow"], "venue": "International Conference on Neural Information Processing.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Facial attribute recognition, including age, gender and emotion, [1,2,3,4,5,6,7] has been a topic of interest among computer vision researchers for over a decade.", "startOffset": 65, "endOffset": 80}, {"referenceID": 1, "context": "Facial attribute recognition, including age, gender and emotion, [1,2,3,4,5,6,7] has been a topic of interest among computer vision researchers for over a decade.", "startOffset": 65, "endOffset": 80}, {"referenceID": 2, "context": "Facial attribute recognition, including age, gender and emotion, [1,2,3,4,5,6,7] has been a topic of interest among computer vision researchers for over a decade.", "startOffset": 65, "endOffset": 80}, {"referenceID": 3, "context": "Facial attribute recognition, including age, gender and emotion, [1,2,3,4,5,6,7] has been a topic of interest among computer vision researchers for over a decade.", "startOffset": 65, "endOffset": 80}, {"referenceID": 4, "context": "Facial attribute recognition, including age, gender and emotion, [1,2,3,4,5,6,7] has been a topic of interest among computer vision researchers for over a decade.", "startOffset": 65, "endOffset": 80}, {"referenceID": 5, "context": "Facial attribute recognition, including age, gender and emotion, [1,2,3,4,5,6,7] has been a topic of interest among computer vision researchers for over a decade.", "startOffset": 65, "endOffset": 80}, {"referenceID": 6, "context": "Facial attribute recognition, including age, gender and emotion, [1,2,3,4,5,6,7] has been a topic of interest among computer vision researchers for over a decade.", "startOffset": 65, "endOffset": 80}, {"referenceID": 4, "context": "In many cases, these systems are able to outperform humans [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "Collecting labeled data for some tasks, such as real age estimation, is much more challenging compared to popular classification [8,9] or detection [10] problems.", "startOffset": 129, "endOffset": 134}, {"referenceID": 8, "context": "Collecting labeled data for some tasks, such as real age estimation, is much more challenging compared to popular classification [8,9] or detection [10] problems.", "startOffset": 129, "endOffset": 134}, {"referenceID": 9, "context": "Collecting labeled data for some tasks, such as real age estimation, is much more challenging compared to popular classification [8,9] or detection [10] problems.", "startOffset": 148, "endOffset": 152}, {"referenceID": 4, "context": "Age estimation: Recently there have been some efforts in collecting data with corresponding age labels [5,11,4].", "startOffset": 103, "endOffset": 111}, {"referenceID": 10, "context": "Age estimation: Recently there have been some efforts in collecting data with corresponding age labels [5,11,4].", "startOffset": 103, "endOffset": 111}, {"referenceID": 3, "context": "Age estimation: Recently there have been some efforts in collecting data with corresponding age labels [5,11,4].", "startOffset": 103, "endOffset": 111}, {"referenceID": 3, "context": "in [4] is the largest dataset that contains 523, 051 images and is available for research purposes.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "This led to the authors using only half of the data for training in the original paper [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "In contrast to some previous works, which do not use any face alignment [4], we found this to be important in our final accuracy numbers.", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "Our face recognition model is not only computationally inexpensive (with feature extraction time of 70ms using just the CPU), but also achieves outstanding results on the LFW dataset [2].", "startOffset": 183, "endOffset": 186}, {"referenceID": 11, "context": "In some recent works [12], researchers try to design a network which performs all tasks at the same time, and they have shown marginal improvements.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "Additionally running all models combined takes less time compared to the all-in-one model of [12] and we achieve better results.", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "For real age estimation, we report results on two publicly available datasets, the Group dataset [5] and the Adience dataset [11].", "startOffset": 97, "endOffset": 100}, {"referenceID": 10, "context": "For real age estimation, we report results on two publicly available datasets, the Group dataset [5] and the Adience dataset [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 3, "context": "[4] 7.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "We compare our results with [4], whose model is trained on the IMDB-Wiki dataset.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "For quantitative comparison, we used the Mean Absolute Error (MAE) which is commonly used in the literature [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "We followed the setup in [5] where", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "[16] 65.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] 62.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[17] 56.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] 42.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[16] 61.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] 45.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "6% Levi and Hassner [18] 50.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "Recently, thanks to the availability of the Chalearn LAP Apparent Age Estimation dataset and challenge [19], several researchers have focused on designing models that are focused on predicting the apparent age, rather than the", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "The winner of the competition [6] used a multi CNN framework and achieved a test error of 0.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "However, this method [6] has several limitations.", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "319 No OrangeLabs [6] 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 16, "context": "2411 Yes Palm-seu [20] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "3214 Yes CMP+ETH [21] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "Table 4: Results for ChaLearn [19] apparent age estimation 2016 challenge.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "FER-2013 [22] and EmotiW are among the popular ones.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "[4] 88.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Levi and Hassner [18] 86.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "Table 6: Results for gender recognition on the Adience benchmark [11].", "startOffset": 65, "endOffset": 69}], "year": 2017, "abstractText": "This paper describes the details of Sighthound\u2019s fully automated age, gender and emotion recognition system. The backbone of our system consists of several deep convolutional neural networks that are not only computationally inexpensive, but also provide state-of-theart results on several competitive benchmarks. To power our novel deep networks, we collected large labeled datasets through a semi-supervised pipeline to reduce the annotation effort/time. We tested our system on several public benchmarks and report outstanding results. Our age, gender and emotion recognition models are available to developers through the Sighthound Cloud API at https://www.sighthound.com/products/cloud", "creator": "LaTeX with hyperref package"}}}