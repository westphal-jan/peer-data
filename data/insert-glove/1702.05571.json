{"id": "1702.05571", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2017", "title": "Thresholding based Efficient Outlier Robust PCA", "abstract": "We consider 37-24 the sailendra problem of outlier churnet robust rubinoos PCA (OR - PCA) where ordinariate the mac goal d-xylose is cordele to valentine recover schwarzmann principal directions supercharged despite dominionism the 22.68 presence of pernem outlier contemporain data points. post-merger That 47-0 is, given bauen a reasserting data dismounts matrix $ M ^ * $, where $ (1 - \\ alpha) $ independency fraction sch\u00f6nborn of sts-1 the points mukha are priamo noisy championships samples from sabmiller a 58.88 low - egil dimensional mechatronics subspace mendieta while $ \\ mischief alpha $ fraction hypnotics of the points can be formula arbitrary north/south outliers, the arctiidae goal is suspected to itis recover citerior the offaly subspace cbg accurately. retailed Existing results staunton for \\ felica OR - PCA 13.18 have serious drawbacks: while opande some yanping results sunbathers are bl\u00e9 quite margarine weak recited in the anthracycline presence wrongness of kanembu noise, other doubtless results have runtime lavizan quadratic angping in dimension, downturn rendering holbein them engr impractical 14g for atriss large scale vba applications.", "histories": [["v1", "Sat, 18 Feb 2017 05:00:04 GMT  (106kb)", "http://arxiv.org/abs/1702.05571v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yeshwanth cherapanamjeri", "prateek jain", "praneeth netrapalli"], "accepted": false, "id": "1702.05571"}, "pdf": {"name": "1702.05571.pdf", "metadata": {"source": "CRF", "title": "Thresholding based Efficient Outlier Robust PCA", "authors": ["Yeshwanth Cherapanamjeri", "Prateek Jain Praneeth Netrapalli"], "emails": ["t-yecher@microsoft.com", "prajain@microsoft.com", "praneeth@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n05 57\n1v 1\n[ cs\n.L G\n] 1\n8 Fe\nb 20\nIn this work, we provide a novel thresholding based iterative algorithm with per-iteration complexity at most linear in the data size. Moreover, the fraction of outliers, \u03b1, that our method can handle is tight up to constants while providing nearly optimal computational complexity for a general noise setting. For the special case where the inliers are obtained from a low-dimensional subspace with additive Gaussian noise, we show that a modification of our thresholding based method leads to significant improvement in recovery error (of the subspace) even in the presence of a large fraction of outliers."}, {"heading": "1 Introduction", "text": "Principal Component Analysis (PCA) is a critical first step for any typical data exploration/analysis effort and is widely used in a variety of applications. A key reason for the success of PCA is that it can be performed efficiently using Singular Value Decomposition (SVD).\nHowever, due to various practical reasons like measurement error, presence of anomalies etc., a large fraction of data points can be corrupted in a somewhat correlated and even adversarial manner. Unfortunately, SVD is fragile with respect to outliers and can lead to arbitrarily inaccurate principal directions in the presence of even a small number of outliers. So, designing an outlier robust PCA (OR-PCA) algorithm is critical for several application domains.\nFormally, the setting of OR-PCA is as follows: given a data matrix M\u2217 = D\u2217 + C\u2217 \u2208 Rd\u00d7n where D\u2217 = [x1, . . . , xn] corresponds to n clean \u201cinlier\u201d data points and C\u2217 has at most \u03b1-fraction of non-zero columns that can corrupt the corresponding clean data points arbitrarily, the goal of OR-PCA is to estimate the principal components of D\u2217 accurately, i.e., recover U\u2217 \u2208 Rd\u00d7r, the top-r left singular vectors of D\u2217.\nVanilla SVD does not do the job since the top singular vectors of M\u2217 can be arbitrarily far from U\u2217 if the operator norm of C\u2217 (\u2016C\u2217\u20162) is large, as can be the case when \u03b1 = \u2126(1). Any algorithm trying to solve OR-PCA needs to exploit the column sparsity of C\u2217 to obtain a better estimate of U\u2217. In particular, they need to find S\u2217 = Supp (C\u2217)\u2014Supp (A) is the index of non-zero columns of A\u2014so that U\u2217 can be estimated using top singular directions of M\u2217\\S\u2217 , i.e., columns of M \u2217 restricted to complement set of S\u2217.\nExisting results for OR-PCA fall into two categories based on: a) Nuclear norm [Xu et al., 2012a, Zhang et al., 2016], b) iterative PCA [Xu et al., 2013]. Nuclear norm based approaches work with exactly same setting as ours, but require O(nd2) computational time which is prohibitive for typical applications. Iterative PCA based techniques require O(n2d) computation which in general is significantly higher than our algorithms. Moreover, these results do not recover the exact principal directions even if the inliers are restricted to a low-dimensional subspace and just a constant number of outliers are present.\nOur approach is based on solving the following natural optimization problem:\nOR-PCA : min D,C\u2208Rd\u00d7n\n\u2016M\u2217 \u2212D \u2212 C\u20162F s.t. rank(D) \u2264 r, |Supp (C) | \u2264 \u03b1n.\nTechnical Challenges: The main challenges with OR-PCA are its non-convexity and combinatorial structure which rules out standard tools from convex optimization. Furthermore, due to the column-sparsity constraint standard SVD based techniques also do not apply. Instead, we propose a simple iterative method that constructs an estimate of outliers C and using that, an estimate of the inliers M\u2217\u2212C. SVD of the estimated inliers is then used to obtain an estimate of the principal directions. Now, a significant challenge is to use these principal directions to re-estimate outliers. There are two different scenarios here:\n\u2022 Length of all the outliers is smaller than the smallest singular value of the inliers and hence do not stand out : In this case however, the principal directions are not much affected by outliers. So, outliers can be recognized by taking a projection on to the orthogonal space to the estimated principal directions. As we get better estimate of principal directions, we can be more aggressive in determining the outliers.\n\u2022 Length of at least one of the outliers is larger than the small singular values of inliers : In this case again, length based thresholding fails since the lengths of the inliers are dominated by the larger singular values. Similarly, the above mentioned thresholding scheme also fails as some of the estimated principal directions will be heavily biased towards those outliers. A key and somewhat surprising algorithmic insight of our work is that: as some of the estimated singular vectors are heavily affected by outliers, projection of such outliers on these spurious singular vectors will be inordinately high. So, in contrast to the above thresholding operator, we can use the length of projection of points along estimated principal directions as well to detect and threshold outliers.\nIn both the scenarios, we can identify more outliers and repeat the procedure till convergence. A key assumption we make, in order to ensure that inliers are not thresholded, is that most of the inliers have \u201climited\u201d influence on the principal directions, i.e., the data matrix is incoherent (see Assumption 1). Such an assumption holds for various typical settings, for example when inliers are noisy and uniform samples from a low-dimensional subspace. Contributions: The main contribution of our work is to show that, under some regularity conditions, it is indeed possible to solve OR-PCA near optimally, in essentially the same time as that taken by vanilla PCA. In particular, we propose a thresholding based approach that iteratively estimates inliers using two different thresholding operators and then use SVD to estimate the principal directions. We also show that our method recovers U\u2217 nearly optimally and efficiently as long as the fraction of corrupted data points (column-support of C\u2217) is less than O(1/r) where r is the dimensionality of principal subspace U\u2217. More concretely, we study the problem in three settings:\n1. Noiseless setting: M\u2217 = D\u2217+C\u2217 where the clean data matrix D\u2217 is a rank-r matrix with \u00b5-incoherent right singular vectors (see Assumption 1) and C\u2217 has at most \u03b1 \u00b7n non-zero columns. For this setting, we design a novel Thresholding based Ouliter Robust PCA algorithm (TORP) that recovers U\u2217 up\nto an error of \u01eb in time O ( ndr log\nn\u2016M\u2217\u20162 \u01eb ) , if \u03b1 \u2264 1128\u00b52r . Note that this is essentially the time taken\nfor vanilla PCA as well. In contrast, existing results for the same setting require O(nd 2\n\u01eb2 ) 1 computation\nto recover U\u2217. Note that the number of outliers our results can handle (i.e., \u03b1 \u2264 1/128\u00b52r) is optimal up to constant factors in the sense that if \u03b1 > 1/\u00b52r, then there exists a matrix M\u2217 which has more than one decomposition satisfying the above conditions.\n2. Arbitrary noise: In the second setting, M\u2217 = D\u2217 +C\u2217 where the clean data matrix D\u2217 can be written as L\u2217 + N\u2217 and L\u2217, the rank-r projection of D\u2217 has \u00b5 incoherent right singular vectors. C\u2217 on the other hand, again has at most \u03b1 \u00b7 n non-zero columns. If \u03b1 \u2264 1128\u00b52r , our proposed algorithm\n1Dependence on \u01eb is due to the standard rates of gradient descent when applied to the non-smooth non-strongly convex optimization problem given in (1); however, using more refined RSC-style analysis, \u01eb dependency might be improved but we are not aware of such an existing result.\nTORP-N guarantees recovery of U\u2217 (left singular vectors of L\u2217) up to O ( \u221a r \u2016N\u2217\u2016F + \u01eb) error, in\nO ( ndr log\nn\u2016M\u2217\u20162\u221a r\u2016N\u2217\u2016F+\u01eb\n) time. Again this is essentially the same as the time taken for vanilla PCA up\nto log factors. In contrast, existing results for this problem get stuck at a significantly larger error of ( \u221a n \u2016N\u2217\u2016F ), with a runtime of O(nd 2 \u01eb2 ), which is slower than ours by a factor of d.\n3. Gaussian noise: In this setting, we again have M\u2217 = D\u2217 + C\u2217 where the clean data matrix D\u2217 is a sum of low-rank matrix L\u2217 and a Gaussian noise matrix N\u2217, i.e., each element of N\u2217 is sampled independently and identically (iid) from N (0, \u03c32). TORP-G, which is our proposed algorithm for this special case, recovers U\u2217 up to an error of O (\u221a r log d \u2016N\u2217\u20162 ) . This not only improves upon the result\nwe obtained for the arbitrary noise case above, which is O ( \u221a r \u2016N\u2217\u2016F ), it also improves significantly upon the existing results. However, in order to achieve this improvement in error, we require n > d2 and the algorithm has a runtime of O ( n2d ) .\nTo summarize, our results obtain stronger guarantees in terms of both final error and runtime, while being able to handle a large number of outliers, for three different settings of OR-PCA. Moreover, in the first two settings, our run time matches that of standard PCA up to log factors. Please refer Tables 1 and 2 for comparison of our results with existing results.\nPaper Outline: The paper is organized as follows. We will review related work in Section 1.1. We then present a formal definition of the problem in Section 2 and our main results in Section 3. We then present our algorithm for each of the three settings: a) noise-less setting, b) arbitrary data, c) Gaussian noise, in Section 4, 5, 6, respectively. We provide a brief overview of our proofs in Section 7. Finally, we conclude with a few open problems and promising future directions in Section 8."}, {"heading": "1.1 Related Works", "text": "In this section, we will discuss related work and compare existing results with ours. Existing theoretical results for OR-PCA fall into two categories:\na) The first category of approaches, more in-line with our own work, are based on Outlier-Pursuit (Xu et al. [2012a]) which optimizes a convex relaxation of the OR-PCA problem where the rank and column sparsity constraints are replaced by the trace-norm (sum of singular values) and \u2016.\u20162,1 (sum of the column lengths) penalties. That is, they solve the following optimization problem:\n(Outlier Pursuit): min \u2016L\u2016\u2217 + \u03bb \u2016C\u20162,1 s.t \u2016M \u2212 L+ C\u2016F \u2264 \u2016N\u2217\u2016F . (1)\nWhile outlier pursuit obtains optimal recovery guarantees in absence of any noise, a main drawback is that its computational complexity is quadratic in d, i.e., O(nd2). Moreover, in presence of noise the bounds given in Xu et al. [2012a] are O( \u221a n) worse than our result. Extensions of Outlier-Pursuit to the partially observed setting (Chen et al. [2016]) and online setting Feng et al. [2013] have also been proposed but share the drawback of high computational complexity. Recently, Zhang et al. [2016] showed that Outlier Pursuit achieves recovery even with the fraction of outliers larger than the information theoretic lower bound. However, this requires the outliers to be \u201cwell-spread\u201d which in practice is restrictive; our results allow the corruption matrix to be constructed in adversarial manner although the corruptions cannot depend on the Gaussian noise in the setting (3) described in previous section.\nb) The second line of approaches based on HR-PCA (Xu et al. [2013]) iteratively prune or reweigh data points which have a large influence on the singular vectors and select an estimate with the Robust Variance Estimator metric. When applied to our finite sample setting, these results cannot achieve exact recovery even in the noiseless case with a single outlier. Moreover, their running time in this setting is O ( n2dr ) while ours is nearly linear in the input size. In the special case of Gaussian noise, these results incur at least a constant error O (\u03c31(L\n\u2217)) whereas our recovery guarantee scales linearly with the standard deviation of the noise O ( \u221a r\u03c3 log d), achieving exact recovery when \u03c3 = 0. Feng et al. [2012] propose a deterministic variant of HR-PCA and Yang and Xu [2015] extend HR-PCA to PCA-like algorithms like Sparse PCA and Non-Negative PCA.\nThere has been much recent work on the related problem of Robust PCA (Cande\u0300s et al. [2011], Netrapalli et al. [2014], Yi et al. [2016], Cherapanamjeri et al. [2016]). In contrast to the setting considered here, the corruptions are assumed to be both row and column sparse i.e., unlike our setting no data can be corrupted in all its dimensions. This restriction allows stronger recovery guarantees but makes the results inapplicable to the setting of outlier robust PCA."}, {"heading": "1.2 Notations", "text": "We use the following notations in this paper. For a vector v, \u2016v\u2016 and \u2016v\u20162 denote the \u21132 norm of v. For a matrix M , \u2016M\u2016 and \u2016M\u20162 denote the operator norm of M while \u2016M\u2016F denotes the Frobenius norm of M . \u03c3k(M) denotes the k\nth largest singular value of M . SVD refers to singular value decomposition of a matrix. SVDr(M) refers to the rank-r SVD of M . Given a matrix M , Mi denotes the ith column of M while Mi,: denotes the ith row of M . Given a matrix M \u2208 Rd\u00d7n and a set S \u2286 [n], MS is defined as\n(MS)i = { Mi for i \u2208 S, 0 otherwise.\nM\\S denotes M[n]\\S. Supp (M) denotes column support of M , i.e., the set of indices of non-zero columns of M .\nWe use two hard -thresholding operators in this paper. Given a matrix R, the first hard-thresholding operator, HT \u03c1 (R) denotes the set of indices j of the top \u03c1 fraction of largest columns (in \u21132 norm) in R. Given a matrix R, the second hard-thresholding operator H\u0303T \u03b6(R) is defined as,\nH\u0303T \u03b6(N) = {i : s.t. \u2016Ni\u20162 \u2265 \u03b6}. (2)\nPU\u22a5 (M) denotes (I \u2212 UU\u22a4)M . For any set S \u2208 Rd, we will use PS to denote the projection onto the set S. We will also use PU to denote the projection onto the column space of U for U \u2208 Rd\u00d7r."}, {"heading": "2 Problem Formulation", "text": "In this section, we will formally present the setting of the paper. We are given M\u2217 = D\u2217 + C\u2217 \u2208 Rd\u00d7n, where columns of D\u2217 are inliers and C\u2217 are outliers. Only an \u03b1 fraction of the points are outliers, i.e., only alpha fraction of the columns of C\u2217 are non-zero. Broadly, we consider three scenarios:\n\u2022 OR-PCA (Noiseless setting): The points inD\u2217 lie entirely in a low-dimensional subspace i.e., D\u2217 = L\u2217 is a rank-r matrix.\n\u2022 OR-PCAN (Noisy setting): The points in D\u2217 lie approximately in a low-dimensional subspace i.e., D\u2217 = L\u2217 +N\u2217 where L\u2217 is the rank-r projection of D\u2217 and N\u2217 is the noise matrix.\n\u2022 OR-PCAG (Gaussian noise setting): The points in D\u2217 come from a low-dimensional subspace with additive Gaussian noise i.e., D\u2217 = L\u2217 + N\u2217 where L\u2217 is a rank-r matrix and N\u2217 is a Gaussian noise matrix i.e., each element of N\u2217 is sampled iid from N (0, \u03c32).\nIn all the above settings, the goal is to find the low dimensional subspace spanned by the columns of L\u2217. This problem is in general ill-posed. Consider for instance the case, when most of the true data points L\u2217 are zero and only an \u03b1 fraction of them are non-zero. These points can either be considered inliers or outliers. In order to overcome this issue, standard assumption used in literature [Xu et al., 2012a, Feng et al., 2013] is that of incoherence. Also, incoherence is satisfied in several standard settings; for example, when the inliers are noise and uniform samples from a low-dimensional subspace.\nAssumption 1. Rank and incoherence of L\u2217: L\u2217 \u2208 Rd\u00d7n is a rank-r incoherent matrix, i.e.,\u2225\u2225e\u22a4i V \u2217 \u2225\u2225 2 \u2264 \u00b5 \u221a r n \u2200i \u2208 [n], where L\u2217 = U\u2217\u03a3\u2217(V \u2217)\u22a4 is the SVD of L\u2217."}, {"heading": "3 Our Results", "text": "In this section, we will present our results for the three settings mentioned above."}, {"heading": "3.1 OR-PCA \u2013 Noiseless Setting", "text": "Recall that in the noiseless setting, we observe M\u2217 = D\u2217 + C\u2217 where D\u2217 is a rank-r, \u00b5-incoherent matrix corresponding to clean data points and the column-support of C\u2217 is at most \u03b1n. The following theorem is our main result for this setting.\nTheorem 1 (Noise-less Setting). Let M\u2217, D\u2217 and C\u2217 be as described above. If \u03b1 \u2264 1128\u00b52r , then Algorithm 1 run with parameters \u03c1 = 1128\u00b52r and T = log 10n\u2016M\u2217\u20162 \u01eb , returns a subspace U such that,\n\u2225\u2225(I \u2212 UU\u22a4)D\u2217 \u2225\u2225 F \u2264 \u01eb.\nRemarks:\n\u2022 Note that the guarantee of Theorem 1 can be right away converted to a bound on the subspace distance between U and that spanned by the columns of D\u2217. In particular, we obtain \u2225\u2225(I \u2212 UU\u22a4)U\u2217 \u2225\u2225 F\n\u2264 \u01eb/\u03c3r(D\n\u2217), where \u03c3r(D\u2217) denotes the smallest singular value of D\u2217 and U\u2217 contains the singular vectors of D\u2217.\n\u2022 Since the most time consuming step in each iteration is computing the top-r SVD of an n\u00d7 d matrix, the total runtime of the algorithm is O ( ndr log 10n\u2016M\n\u2217\u20162 \u01eb\n) .\n\u2022 The above assumption on the column sparsity of C\u2217 is tight up to constant factors i.e., we may construct an incoherent matrix L\u2217 and column sparse matrix C\u2217 such that it is not possible to recover the true column space of L\u2217 when the column sparsity of C\u2217 is larger than 1\u00b52r ."}, {"heading": "3.2 OR-PCAN \u2013 Arbitrary Noise", "text": "We now consider the noisy setting. Here we observe M\u2217 = D\u2217 + C\u2217, where D\u2217 is a near low rank matrix i.e., D\u2217 = L\u2217+N\u2217 where L\u2217 is the best rank r approximation to D\u2217 and is a \u00b5 incoherent matrix, while N\u2217 is a noise matrix. C\u2217 is again column sparse with at most an \u03b1 fraction of the columns being non-zero.\nTheorem 2 (Arbitrary Noise). Consider the setting above. If \u03b1 \u2264 1128\u00b52r , then Algorithm 2 when run with parameters \u03c1 = 1128\u00b52r , \u03b7 = 2\u00b5 \u221a r n and T = log 20\u2016M\u2217\u20162\u00b7n \u01eb iterations, returns a subspace U such that:\n\u2225\u2225(I \u2212 UU\u22a4)L\u2217 \u2225\u2225 F \u2264 60\u221ar \u2016N\u2217\u2016F + \u01eb.\nRemarks: \u2022 The theorem shows that up to \u221ar \u2016N\u2217\u2016F error, recovered directions U contains top r principle directions of inliers. We do not optimize the constants in our proof. In fact, we obtain a stronger result in Theorem 4 which for certain regime of noise N\u2217 can lead to significantly better error bound. Note that when there is no noise i.e., N\u2217 = 0, we recover Theorem 1.\n\u2022 The guarantee here can again be converted to a bound on subspace distance. For instance, for any k \u2264 r, we have \u2225\u2225\u2225(I \u2212 UU\u22a4)U\u2217[k] \u2225\u2225\u2225 F\n\u2264 (60\u221ar \u2016N\u2217\u2016F + \u01eb) /\u03c3k(L\u2217), where U\u2217k denotes the top-k left singular subspace of L\u2217 and \u03c3k(L\u2217) denotes the kth largest singular value of L\u2217.\n\u2022 The total runtime of the algorithm is O ( ndr2 log 10\u2016M\n\u2217\u20162\u00b7n \u01eb\n) . However, the outer loop over k in\nAlgorithm 2 can be replaced by a binary search for values of k between 1 and r. This reduces the runtime to O ( ndr log r log 10\u2016M\n\u2217\u20162\u00b7n \u01eb\n) . See Algorithm 4 for more details."}, {"heading": "3.3 OR-PCAG\u2013 Gaussian Noise", "text": "We now consider the Gaussian noise setting. Here we observe M\u2217 = D\u2217 + C\u2217, where D\u2217 is a near low rank matrix i.e., D\u2217 = L\u2217 + N\u2217 where L\u2217 is a rank-r, \u00b5 incoherent matrix, while N\u2217 is a Gaussian matrix with each entry sampled iid from N (0, \u03c32). C\u2217 is again column sparse with at most an \u03b1 fraction of the columns being non-zero.\nTheorem 3 (Gaussian Data). Consider the setting mentioned above. Suppose \u03b1 \u2264 11024\u00b52r . Then, Algorithm 3 stops after at most T = \u03b1n iterations and returns a subspace U such that:\n\u2016(I \u2212 UU\u22a4)L\u2217\u20162 \u2264 4 \u221a log d\u2016N\u2217\u20162.\nwith probability at least 1 \u2212 \u03b4 as long as n \u2265 16\u00b52r2dc1 [ log ( 1 3\u03b4 ) + d log(80d) ] for some absolute constants c1 and c2.\nRemarks:\n\u2022 Data points coming from a low-dimensional subspace with additive Gaussian noise is a standard statistical model that is used to justify PCA. Though this can be seen as a special case of arbitrary noise model, we get a much tighter bound than that obtained from Theorem 2. \u2022 While Theorem 2 gives an asymptotic error bound of \u2225\u2225(I \u2212 UU\u22a4)L\u2217 \u2225\u2225 F\n\u2264 60\u221ar \u2016N\u2217\u2016F , Theorem 3 gives an asymptotic error bound of \u2225\u2225(I \u2212 UU\u22a4)L\u2217 \u2225\u2225 F \u2264 4 \u2016N\u2217\u20162. Note that the right hand sides above refer to Frobenius and operator norms respectively.\n\u2022 The improvement mentioned above is obtained by carefully leveraging the fact that Gaussian random vectors are spread uniformly in all directions and that there is a small fraction of vectors which is correlated. However, in order to make this argument, we need n = O ( d2 ) . It is an open problem to\nget rid of this assumption.\n\u2022 Note also that our result is tight in the sense that as \u03c3 \u2192 0, we recover the result of Theorem 1. However, the running time of the algorithm is O(n2d) which is significantly worse than that of TORP. We leave design/analysis of a more efficient algorithm that achieves similar error bounds as Theorem 3 as an open problem.\n\u2022 We can obtain the above result even when each column inN\u2217 is drawn from a sub-Gaussian distribution rather than each entry being iid N (0, \u03c32)."}, {"heading": "4 Outlier Robust PCA: Noiseless Setting", "text": "In this section, we present our algorithm TORP(Algorithm 1) that applies to the special case of noise-less data, i.e., when M\u2217 = L\u2217+C\u2217, L\u2217 is rank-r, \u00b5-incoherent matrix. While restrictive, this setting allows us to illustrate the main ideas behind our algorithm approach and the analysis techniques in a relatively simpler fashion.\nRecall that the goal is to estimate U\u2217, the left singular vectors of L\u2217. However, SVD of M\u2217 can lead to singular vectors arbitrary far from U\u2217, because a few column of C\u2217 can be so large that they can bias entire singular vectors in their direction.\nOur algorithm instead tries to exploit two key structural properties of the problem: sparsity of C\u2217 and incoherence of L\u2217. Our algorithm maintains a column-sparse estimate C(t) of C\u2217. Each iteration of the algorithm computes a low-rank approximation of an estimate of the inliers M\u2217 \u2212 C(t) = L\u2217 + C\u2217 \u2212 C(t). Note that if (I \u2212 U\u2217(U\u2217)\u22a4)C\u2217 = (I \u2212 U\u2217(U\u2217)\u22a4)C(t), then left singular vectors of M\u2217 \u2212 C(t) will be U\u2217.\nOur next step finds residual length of each column M\u2217i when projected on to the orthogonal subspace to U (t). If length of each outlier is smaller compared to the smallest singular value of L\u2217, then using sparsity of C(t) and C\u2217, we can show that U (t) is \u201dclose\u201d to U\u2217 in all directions. So, the residual of some of the outliers will stand out and those columns can be removed. This is achieved by the hard-thresholding step 5, 8 of Algorithm 1.\nA big challenge in this scheme is that if a column of the perturbation matrix C\u2217 \u2212 C(t) is \u201dvery\u201d long compared to smaller singular values of L\u2217, then they can perturb some directions of U\u2217 significantly. This will lead to a failure of the above thresholding approach. However, in such a case, some of the columns of C\u2217 \u2212 C(t) will be close to a few spurious singular vectors in U (t) (our current estimate of U\u2217). Hence, projection of such outliers along U (t) will be inordinately long. On the other hand, due to incoherence of L\u2217, inliers\u2019 projection along U (t) can be bounded in magnitude. So, we can safely threshold out certain outliers. Steps 6, 8 of Algorithm 1 perform this thresholding operation.\nIn summary, our algorithm computes low-rank approximation ofM\u2217\u2212C(t) and uses the obtained singular vectors U (t) to threshold out a few columns of C(t) to obtain next estimate C(t+1) of C\u2217. See Algorithm 1 for a pseudo-code of our approach.\nTime Complexity: Note that the computationally most expensive operation in each iteration is that of SVD which requires O(ndr) time. So, the overall time complexity of the algorithm is O(ndr \u00b7 T ). As we show in Section 7, as long as C\u2217 is column-sparse, T \u2248 log 1\u01eb suffices to obtain an \u01eb approximation to U\u2217. So, the overall complexity of the algorithm is O(ndr \u00b7 log 20\u2016M\n\u2217\u20162 \u01eb ). Note that typically SVD computation\nis approximate, while all three of our algorithms and analyses assumes exact SVD. However, extension of our analysis to allow for small additive error is straightforward and we ignore it in favor of simplicity and readability.\nParameters: The algorithm requires an estimate of rank r and threshold parameter \u03c1 which in turn depends on estimate of incoherence \u00b5 of L\u2217. We propose to set these parameters via cross-validation. Note that setting rank to be any value larger than rank of L\u2217 will lead to recovery of U\u2217, as long as C\u2217 is sparse enough. Similarly, if estimation of \u00b5 is larger than incoherence of L\u2217, then it only effects number of corrupted columns in C\u2217 that can be allowed. So, a simple cross-validation approach with appropriately chosen grid-size leads to recovery of U\u2217 as long as C\u2217 is sparse enough (as specified in Theorem 1).\nAlgorithm 1 Thresholding based Outlier Robust PCA (TORP)\n1: Input: Data M\u2217 \u2208 Rd\u00d7n, Target rank r, Threshold fraction \u03c1, Number of iterations T 2: C(0) \u2190 0 3: for Iteration t = 0 to t = T do 4: [U (t),\u03a3(t), V (t)] \u2190 SVDr ( M\u2217 \u2212 C(t) ) ; L(t) \u2190 U (t)\u03a3(t)(V (t))\u22a4 } Projection onto space of low rank matrices5: R \u2190 (I \u2212 U (t)(U (t))\u22a4)M\u2217 /* Compute residual */ 6: E \u2190 (\u03a3(t))\u22121(U (t))\u22a4M\u2217/* Compute incoherence */ 7: CS(t+1) \u2190 HT \u03c1 (R) \u222aHT \u03c1 (E)\n   Projection onto space of\ncolumn sparse matrices8: C (t+1) \u2190 M\u2217S /* Threshold points with high coherence or high residual*/\n9: end for 10: [U,\u03a3, V ] \u2190 SVDr ( M\u2217 \u2212 C(T+1) ) 11: Return: U"}, {"heading": "5 Outlier Robust PCA: General Noise", "text": "In this section, we introduce our algorithm for the general case of Outlier Robust PCA with arbitrary inlier data D\u2217 = L\u2217 + N\u2217, i.e., the noise matrix N\u2217 is arbitrary. Recall that the goal is to recover left singular vectors of L\u2217.\nOur algorithm for the general OR-PCA problem builds upon the TORP algorithm but with added complexity due to the presence of noise matrix N\u2217. That is the algorithm alternately updates estimate of the outliers C(t) and the principal direction U (t) using two thresholding operators along with SVD. However due to noise N\u2217, our estimate of U (t) gets perturbed furthermore leading to arbitrary perturbation of the singular vectors of U\u2217 corresponding to smaller eigenvalues of L\u2217 and hence cannot be recovered. To alleviate this concern, our TORP-N algorithm proceeds via a pair of nested loops:\nOuter Iteration on k: The outer loop iterates over the rank-variable k which represents the rank of the principal subspace we wish to estimate.\nInner Iteration on t: The inner loop iteratively revises estimates of the principal subspace and a set of outliers until a stopping criteria is triggered.\nIntuitively, as in Algorithm 1, each inner iteration of Algorithm 2 obtains a better estimate of C\u2217 and top-k singular components of L\u2217. That is, the k-th outer iteration after several of such inner iterations estimates U\u2217 up to \u2248 \u03c3k(L\u2217). But when the noise \u2016N\u2217\u2016F becomes comparable to the kth singular value of L\u2217, then the algorithm terminates (Line 14, Algorithm 2) as at that point it may not be possible to estimate the remaining singular vectors of L\u2217. As we don\u2019t know \u2016N\u2217\u2016F explicitly, we detect this event based on the number of data points which have a large influence on the estimated singular vectors (see lines 11, 12 of Algorithm 2).\nRoughly, our stopping criterion allows us to make two statements regarding the termination of the algorithm:\n1. When the algorithm terminates, the outlier columns that we have not thresholded will only have small influence on the estimated principal vectors. This is because all points with large influence will be thresholded before the estimate is computed.\n2. The algorithm will not terminate if \u03c3\u2217k >> \u2016N\u2217\u2016F : The bound on \u2016N\u2217\u2016F ensures that not many inlier points can have large influence on estimate of the kth singular vector.\nBy using the above two claims, our analysis shows that TORP-N recovers U\u2217 up to \u223c \u2016N\u2217\u2016F error. Time Complexity: Time complexity of each inner iteration of TORP-N is O(ndk). Hence, overall time complexity is O(ndr2), as k can be as large as r. However, using a slightly more complicated algorithm and\nanalysis (see Algorithm 4), we can search for appropriate k using binary search, so the time complexity of the algorithm can be improved to O(ndr log r).\nParameter Estimation: The algorithm requires 3 parameters: rank r, threshold \u03c1 which depends on incoherence \u00b5 of L\u2217 and expressivity parameter \u03b7. We can search for these parameters using a coarse-grid search as estimates of these parameters up to constants are enough for our algorithm to succeed albeit with a slightly stricter restriction (by constant factors) on the number of corrupted data points.\nAlgorithm 2 Thresholding based Noisy Outlier Robust PCA (TORP-N)\n1: Input: Corrupted matrix M\u2217 \u2208 Rd\u00d7n, Target rank r, Expressivity parameter \u03b7, Threshold fraction \u03c1, Inner iterations T 2: for k = 1 to k = r do 3: C(0) \u2190 0, \u03c4 \u2190 false 4: for t = 0 to t = T do 5: [U (t),\u03a3(t), V (t)] \u2190 SVDk ( M\u2217 \u2212 C(t) ) , L(t) \u2190 U (t)\u03a3(t)(V (t))\u22a4 } Projection onto space of\nlow rank matrices\n6: E \u2190 (\u03a3(t))\u22121(U (t))\u22a4M\u2217 /* Compute Incoherence */ 7: R \u2190 (I \u2212 U (t)(U (t))\u22a4)M\u2217 /* Compute residual */\n   Projection onto space of column sparse matrices\n8: CS(t+1) \u2190 HT 2\u03c1 (M\u2217, E) \u222aHT \u03c1 (M\u2217, R) 9: C(t+1) \u2190 M\u2217CS(t+1)\n10: nthres \u2190 |{i : \u2016Ei\u2016 \u2265 \u03b7}| /* Compute high incoherence points */ 11: \u03c4 \u2190 \u03c4 \u2228 (nthres \u2265 2\u03c1n) /* Check termination conditions */ 12: end for 13: if \u03c4 then 14: break 15: end if 16: [U,\u03a3, V ] \u2190 SVDk ( M\u2217 \u2212 C(T+1) ) 17: end for 18: Return: U"}, {"heading": "6 Outlier Robust PCA: Gaussian Noise", "text": "In this section, we present our algorithm for the special case of the Outlier Robust PCA problem when inlier points are generated using a standard Gaussian noise model. That is, when D\u2217 = L\u2217 + N\u2217 \u2208 Rd\u00d7n where each entry of the noise matrix N\u2217 is sampled i.i.d. from N (0, \u03c32). Our result for arbitrary N\u2217 (Theorem 2) estimates U\u2217 up to \u223c \u2016N\u2217\u2016F error, which is \u2126(\u03c3 \u221a dn) for Gaussian noise. However, using a slight variant\nof Algorithm 2 and exploiting the noise structure, Algorithm 3 is able to estimate U\u2217 up to \u03c3 \u221a n log d error, which is better than the previous one by a factor of O (d/ log d). At a high level the philosophy of our TORP-G algorithm is similar to TORP, i.e., we iteratively revise estimate of C\u2217 and the top singular vectors U\u2217 using SVD and thresholding. That is, we iteratively threshold columns of M\u2217, that we estimate are corrupted. However, due to Gaussian noise structure our thresholding step is significantly different than that of TORP or TORP-N.\nIn particular, the choice of our thresholding criteria (see lines 10, 12 of Algorithm 3) uses the following two insights:\n1. Length based thresholding (with respect to \u03b61\u2014line 10 of Algorithm 3): This thresholding step is\nused to ensure that the noise in each data point is at most O ( \u03c3 \u221a d ) . As the length of random Gaussian vector is at most O ( \u03c3 \u221a d ) with high probability, only a small number of inliers are\nthresholded in this step (Lemma 9).\nAlgorithm 3 Thresholding based Outlier Robust PCA with Gaussian Noise (TORP-G)\n1: Input: Corrupted matrix M\u2217 \u2208 Rd\u00d7n, Target rank r, Incoherence Parameter \u00b5, Noise Level \u03c3 2: M \u2190 M\u2217, \u03c4 \u2190 true 3: \u03b61 \u2190 \u03c3 ( 5 4\u00b5 \u221a r + d 1 2 + 2d 1 4 \u221a log ( \u00b52r c2 )) , \u03b62 \u2190 \u03c3 \u221a 2r ( 5 4\u00b5+ 2 \u221a log ( \u00b52r2d c1 )) 4: C(0) \u2190 0, CS(0) \u2190 {}, CS(\u22121) \u2190 {0}, t \u2190 0 5: while CS(t) 6= CS(t\u22121) do 6: [U (t),\u03a3(t), V (t)] \u2190 SVDr+1(M\u2217 \u2212 C(t)), L(t) \u2190 U (t)\u03a3(t)(V (t))\u22a4 } Projection onto space of low rank matrices 7: E(t) \u2190 {x : x = U (t)\u03a3(t)y for some \u2016y\u2016 \u2264 2\u00b5 \u221a r/n} 8: L\u0302(t) \u2190 PE(t)(L(t)) /* Projection onto incoherent matrices */ 9: I \u2190 { i : \u2225\u2225\u2225L(t)i \u2212 L\u0302 (t) i \u2225\u2225\u2225 > \u03b62 } /* Points with large influence */\n10: CS(t+1) \u2190 CS(t) \u222a H\u0303T \u03b61 ( L(t) \u2212 L\u0302(t) )\n/* Updating support of outliers */ 11: if |I| \u2265 24nc1\u00b52dr then 12: CS(t+1) \u2190 CS(t+1) \u222a H\u0303T \u03b62 ( L(t) \u2212 L\u0302(t) )\n   Projection onto space of column sparse matrices /* Update support of outliers */ 13: end if 14: C(t+1) \u2190 M\u2217CS(t+1) /* Compute Sparse Projection */ 15: t \u2190 t+ 1 16: end while 17: Return: U\n2. Projection based thresholding (with respect to \u03b62\u2014line 12 of Algorithm 3): In this step, we threshold points that have large projection along the estimated principal subspace. Note that out of n columns\nof N\u2217, at most O (\n1 \u00b52rd\n) fraction of points have projected lengths greater than O ( \u03c3 \u221a log(d) ) along\nany direction (Lemma 12). Thus, chances of a inliers being thresholded in this step is low. On the other hand, any outlier that heavily influences a principal direction will be thresholded by this step.\nAlgorithm 3 provides a detailed pseudo-code of TORP-G. Step 6 of the algorithm computes rank-(r + 1) SVD of the estimate of inlier matrix M\u2217\u2212C\u2217. Step 7 defines a set of vectors, whose projection onto singular vectors of L(t) is \u201ctypical\u201d for an inlier which is composed of a low-dimensional point perturbed by Gaussian noise vector of length O(\u03c3 \u221a d).\nThis set is used in step 10 to threshold outliers using the hard-thresholding operator H\u0303T \u03b6 as defined in (2). Next, the set I consists of points which have a large influence on the estimated principal components. In the absence of outliers, the size of this set is bounded by 12nc1\u00b52dr with high probability. A large deviation in the size of this set indicates the presence of of outliers and the entire set is thresholded.\nNote on Approximate Computation: We would like to note that the projection operator defined in step 8 of the algorithm can be computed efficiently to arbitrary accuracy. A pseudo-code for computing the required projection can be found in Algorithm 5. Algorithm 5 reduces the problem to the univariate problem of finding the root of a monotonically decreasing function in a bounded interval which can be found efficiently via binary search. For the sake of simplicity, we assume that the projection step and the SVD are computed exactly. Our analysis can be extended to the case where the projection and SVD are computed approximately with some added technical difficulty."}, {"heading": "7 Proof Overview", "text": "In this section, we provide a brief overview of our analysis for the three main results."}, {"heading": "7.1 Noiseless Setting\u2014Theorem 1", "text": "In this section, we present the proof of Theorem 1. Recall that we are given M\u2217 = D\u2217 +C\u2217, where D\u2217 = L\u2217 is a rank-r, \u00b5-incoherent matrix and C\u2217 has at most a fraction of \u03c1 non-zero columns. We can assume with out loss of generality that D\u2217 and C\u2217 have disjoint column supports as we can rewrite M\u2217i , for i \u2208 Supp (C\u2217), as M\u2217i = D \u2217 i + C \u2217 i = 0 + (C \u2217 i +D \u2217 i ) thus absorbing D \u2217 i in C \u2217 i itself.\nOur proof consists of three main steps. Given any set of columns S and letting [U\\S,\u03a3\\S , V\\S ] be the top-r SVD of M\u2217\\S , we establish the following:\nStep 1: Every non-zero column of D\u2217 has significantly smaller residual when projected onto subspace orthogonal to U\\S than the norm of corrupted columns of M \u2217 \\S (Lemma 1), so its likelihood of being\nthresholded (Line 6, 8 of Algorithm 1) is small,\nStep 2: Every non-zero column of D\u2217 has small incoherence with respect to [U\\S ,\u03a3\\S , V\\S ] (Lemma 2), i.e., its projection onto U\\S cannot be \u201ctoo large\u201d. Hence, its likelihood of being thresholded (Line 7,8 of Algorithm 1) is also small,\nStep 3: Any non-zero column of C\u2217 which has small residual and incoherence compared to those of a non-zero column of D\u2217 and hence won\u2019t be thresholded by Algorithm 1, has small residual when projected onto U\u2217 . That is, the column itself is close to subspace spanned by U\u2217 and hence does not effect estimation of U\u2217 (Proof of Theorem 1).\nThat is, either a corrupted column will be thresholded or it is close to U\u2217 while inliers (D\u2217) have little likelihood of being thresholded (step 1,2 above). We now present the formal statements and their proofs. We start with two lemmata establishing Steps 1,2 above. Detailed proofs of the lemmata are given in Appendix B.1 and B.2, respectively.\nLemma 1. Consider the setting of Theorem 1. Let S \u2282 [n] denote a subset of columns of M\u2217 such that |S| \u2264 2\u03c1n. Let M\u2217\\S (L\u2217\\S) be obtained from M\u2217 (L\u2217) by setting the columns corresponding to indices specified in S to 0. Let U\\S\u03a3\\S(V\\S) \u22a4 (U\u2217\\S\u03a3 \u2217 \\S(V \u2217 \\S) \u22a4) be the rank-r SVD of M\u2217\\S (L\u2217\\S), then \u2200i:\n\u2225\u2225(I \u2212 U\\S(U\\S)\u22a4)L\u2217i \u2225\u2225 \u2264 33\n32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225(I \u2212 U\u2217(U\u2217)\u22a4)M\u2217\\S \u2225\u2225\u2225\nLemma 2. Under the setting of Lemma 1, we have for every i:\n\u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SL \u2217 i \u2225\u2225\u2225 \u2264 33 32 \u00b5\n\u221a r\nn .\nWe now present the proof of Theorem 1 where we illustrate Step 3:\nProof. We will start by showing the quantity \u2225\u2225(I \u2212 U\u2217(U\u2217)\u22a4)M (t+1) \u2225\u2225 F decreases at a geometric rate, where M (t+1) = M\u2217 \u2212 C(t+1). Let Q(t) denote the columns of C\u2217 that are not thresholded in iteration t. Also let S(t) denote the columns of L\u2217 that are thresholded in iteration t. Let L\u0303(t+1) := L\u2217\\S(t) , C\u0303 (t+1) := C\u2217 Q(t) , and PU\u22a5 (M) = (I \u2212 U(U)\u22a4)M . Then, we have: \u2225\u2225\u2225PU \u2217 \u22a5 (M (t+1)) \u2225\u2225\u2225 2\nF =\n\u2225\u2225\u2225PU \u2217 \u22a5 (L\u0303 (t+1) + C\u0303(t+1)) \u2225\u2225\u2225 2\nF =\n\u2225\u2225\u2225PU \u2217 \u22a5 (C\u0303 (t+1)) \u2225\u2225\u2225 2\nF\n= \u2211\nj\u2208Q(t)\n\u2225\u2225\u2225PU \u2217 \u22a5 (U (t)\u03a3(t)W (t) j +R (t) j ) \u2225\u2225\u2225 2 \u2264 2 \u2211\nj\u2208Q(t)\n\u2225\u2225\u2225PU \u2217 \u22a5 (U (t))\u03a3(t)W (t) j \u2225\u2225\u2225 2 + \u2225\u2225\u2225R(t)j \u2225\u2225\u2225 2 , (3)\nwhere W (t) j = (\u03a3 (t))\u22121(U (t))TC\u2217j and R (t) j = P U(t) \u22a5 (C \u2217 j ), \u2200j \u2208 Q(t). The last inequality follows from triangle inequality and the fact that (a+ b)2 \u2264 2(a2 + b2). Recall, that we threshold a particular column l in iteration t based on \u2225\u2225\u2225PU(t)\u22a5 (M\u2217l ) \u2225\u2225\u2225 and \u2225\u2225(\u03a3(t))\u22121(U (t))\u22a4M\u2217l \u2225\u2225. For a particular j \u2208 S(t) that wasn\u2019t thresholded in iteration t, we know that there exists a column ij such that \u2225\u2225\u2225(\u03a3(t))\u22121(U (t))\u22a4L\u2217ij \u2225\u2225\u2225 \u2265\n\u2225\u2225(\u03a3(t))\u22121(U (t))\u22a4C\u2217j \u2225\u2225. Similarly, there exists a column kj such that\u2225\u2225\u2225PU(t)\u22a5 (L\u2217kj ) \u2225\u2225\u2225 \u2265 \u2225\u2225\u2225PU(t)\u22a5 (C\u2217j ) \u2225\u2225\u2225. From Lemmas 2 and 1, we have:\n\u2225\u2225\u2225W (t)j \u2225\u2225\u2225 \u2264 33\n32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225R(t)j \u2225\u2225\u2225 \u2264 33\n32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225PU \u2217\n\u22a5 (M (t))\n\u2225\u2225\u2225 (4)\nUsing (3) and (4), we have:\n\u2225\u2225\u2225PU \u2217 \u22a5 (M (t+1)) \u2225\u2225\u2225 2\nF \u2264 2\n\u2211\nj\u2208S(t)\n( 33\n32\n)2 \u00b52 r\nn\n\u2225\u2225\u2225PU \u2217 \u22a5 (U (t))\u03a3(t) \u2225\u2225\u2225 2 + ( 33\n32\n)2 \u00b52 r\nn\n\u2225\u2225\u2225PU \u2217 \u22a5 (M (t)) \u2225\u2225\u2225 2\n\u2264 4 \u00b7 9 8 \u00b7 \u00b5\n2r n \u00b7 \u03c1n \u00b7 \u2016PU\u2217\u22a5 (M (t))\u20162 \u2264 1 4\n\u2225\u2225\u2225PU \u2217 \u22a5 (M (t)) \u2225\u2225\u2225 2 ,\nwhere second last inequality follows from |S(t)| \u2264 \u03c1n and the last inequality follows from \u03c1 \u2264 \u03b1 \u2264 1128\u00b52r . By recursively applying the above inequality, we obtain:\n\u2225\u2225\u2225PU \u2217 \u22a5 (M (T+1)) \u2225\u2225\u2225 F \u2264 \u01eb 20n . (5)\nAlso, note that using variational characterization of SVD, we have \u2016PU\u22a5 (M (T+1))\u2016F \u2264 \u2225\u2225PU\u2217\u22a5 (M (T+1)) \u2225\u2225 F . Theorem now follows from the following argument:\n\u2225\u2225PU\u22a5 (L\u2217) \u2225\u22252 F = \u2225\u2225\u2225PU\u22a5 (L\u0303(T+1)) \u2225\u2225\u2225 2\nF +\n\u2211\ni\u2208S(T )\n\u2225\u2225PU\u22a5 (L\u2217i ) \u2225\u22252 \u2264 \u2225\u2225\u2225PU\u22a5 (M (T+1)) \u2225\u2225\u2225 2\nF +\n\u2211\ni\u2208S(T )\n332 322 \u00b52 r n\n\u2225\u2225\u2225PU\u22a5 (L\u0303(T+1)) \u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225PU\u22a5 (M (T+1)) \u2225\u2225\u2225 2\nF + 2\u03c1n\n( 33\n32\n)2 \u00b52 r\nn\n\u2225\u2225\u2225PU\u22a5 (M (T+1)) \u2225\u2225\u2225 2\nF \u2264 \u01eb 10n ,\nwhere the first inequality follows from Lemma 6 and using M (T+1) = L\u0303(T+1) + C\u0303(T+1), and the fact that L\u0303(T+1) and C\u0303(T+1) have different support. The second inequality follows from the fact that at most 2\u03c1 \u00b7 n points can be thresholded and then using (5)."}, {"heading": "7.2 Arbitrary Noise\u2014Theorem 2", "text": "We now briefly discuss the proof of Theorem 2. In fact, we prove a stronger result:\nTheorem 4. Let M\u2217 = L\u2217 + C\u2217 + N\u2217 such that L\u2217 satisfies Assumption 1 and C\u2217 has column sparsity \u03b1 \u2264 1128\u00b52r . Furthermore, suppose that \u2016N\u2217\u2016F \u2264 \u03c3\u2217k 16 for some k \u2208 [r]. Then, Algorithm 1 run with \u03c1 = 1128\u00b52r and \u03b7 set to 2\u00b5 \u221a\nr n with T = log 20\u2016M\u2217\u20162\u00b7n \u01eb , returns a subspace U such that:\n\u2225\u2225(I \u2212 UU\u22a4)L\u2217 \u2225\u2225 F \u2264 3 \u2225\u2225(I \u2212 U\u2217k (U\u2217k )\u22a4)L\u2217 \u2225\u2225 F + 9 \u2016N\u2217\u2016F + \u01eb\n10n .\nIntuitively, the proof of Theorem 4 proceeds along the same lines as that of Theorem 1 but requires significantly more careful analysis due to presence of noise and due to the outer loop. For example, due to the presence of noise, we cannot guarantee that Lemma 2, that was critical to proof of Theorem 1, holds for all columns i. We show instead that the number of data points which have a large influence on the top-k\nsingular vectors is bounded (see Lemma 10). This ensures that the algorithm at least reaches the kth stage of the outer iteration before terminating. Similarly, we generalize Lemma 1 to handle N\u2217 (see Lemma 11). Finally, we present the key lemma that shows that if the algorithm does not terminate in the kth outer iteration, then it would have obtained a good approximation to the top-k principal subspace of L\u2217.\nLemma 3. Asume the conditions of Theorem 2. Furthermore, assume that Algorithm 2 has not terminated during the kth outer iteration. Then, the iterate U at the end of the kth outer iteration satisfies:\n\u2225\u2225(I \u2212 UU\u22a4)L\u2217 \u2225\u2225 F \u2264 3 \u2225\u2225(I \u2212 U\u22171:k(U\u22171:k)\u22a4)L\u2217 \u2225\u2225 F + 9 \u2016N\u2217\u2016F + \u01eb\n10n ,\nwhen Algorithm 2 has been run with parameters \u03c1 = 1128\u00b52r and \u03b7 = 2\u00b5 \u221a r n .\nSee Appendix B.5 for a detailed proof. We can now prove Theorem 4 as follows:\nProof. Note that by Lemma 10, the algorithm does not terminate before the completion of kth outer iteration. Now, suppose that the algorithm terminates at some iteration k\u2032 > k. Then, by Lemma 3, we have:\n\u2225\u2225PU\u22a5 (L\u2217) \u2225\u2225 \u2264 3 \u2225\u2225\u2225PU \u2217 1:k\u2032\u22121 \u22a5 (L \u2217) \u2225\u2225\u2225 F + 9 \u2016N\u2217\u2016F + \u01eb 10n \u2264 3 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (L \u2217) \u2225\u2225\u2225 F + 9 \u2016N\u2217\u2016F + \u01eb 10n .\nThis concludes the proof of the Theorem."}, {"heading": "7.3 Gaussian Noise\u2014Theorem 3", "text": "Our analysis of TORP-G show that the algorithm maintains the following critical invariant with high probability:\nInvariant 1. We assume that the following hold with respect to the two thresholding steps used in Algorithm 3.\n1. With respect to \u03b61: If a column i 6\u2208 Supp (C\u2217) is thresholded, then the following condition holds:\n\u2016N\u2217i \u2016 \u2265 \u03c3 ( \u221a d+ 2d 1 4 \u221a log ( \u00b52r\nc2\n)) .\nand consequently only 3nc22\u00b52r points are removed in this step.\n2. With respect to \u03b62: If a thresholding step occurs due to the second thresholding step with \u03b62, then at least half the points thresholded in this step are corrupted points.\nLemma 4. Assume the conditions of Theorem 3. Then, Invariant 1 holds at any point in the running of Algorithm 3 with probability at least 1\u2212 \u03b4.\nSee Appendix B.8 for a detailed proof. Our proof then uses the above invariant along with a careful analysis of each of the two thresholding\nsteps (Line 10, 12) to obtain the desired result. See Appendix C for a detailed proof."}, {"heading": "8 Conclusions and Future Works", "text": "In this paper, we studied the outlier robust PCA problem. We proposed a novel thresholding based approach that, under standard regularity conditions, can accurately recover the top principal directions of the clean data points, as long as the number of outliers is less than O(1/r) which is information theoretically tight up to constant factors. For noiseless or arbitrary noise case, our algorithms are based on two thresholding operators to detect outliers and leads to better recovery compared to existing methods in essentially the\nsame time as that taken by vanilla PCA. For Gaussian noise, we obtain improved recovery guarantees but at a cost of higher run time.\nThough our bounds have significant improvement over existing ones, they are still weaker than guarantees obtained by vanilla PCA (with out outliers). For instance, for arbitrary noise, our errors are bounded in the Frobenius norm. In contrast, in absence of outliers, SVD can estimate the principal directions in operator norm. A challenging and important open problem is if the principal directions can be estimated in operator norm even in the presence of outliers.\nSimilarly, for Gaussian noise, where each entry has variance \u03c32, our result obtains an error bound of O(\u03c3 \u221a n) which is significantly better than the Frobenius norm bound we get for arbitrary noise. But again in absence of outliers, SVD can estimate the principal directions exactly asymptotically. So, another open problem is if it is possible to do asymptotically consistent estimation of the principal directions with Gaussian noise in the presence of outliers. Moreover, our algorithm for the Gaussian setting is nearly a factor of n slower than that for vanilla PCA. In order for this to be practical, it is very important to design an algorithm for this setting with nearly the same runtime as that of vanilla PCA."}, {"heading": "A Supplementary Results and Preliminaries", "text": "Here, we will state and prove a few results useful in proving our main theorems. We will start by restating Weyl\u2019s perturbation inequality from Bhatia [1997].\nTheorem 5. Let A \u2208 Rd\u00d7n. Furthermore, let B = A+ E for some matrix E. Then, we have that:\n|\u03c3(A)i \u2212 \u03c3(B)i| \u2264 \u2016E\u2016 \u2200i \u2208 min(d, n)\nIn the next lemma, we show that the singular values of the sum of two matrices with disjoint column supports are greater than either of the two matrices individually.\nLemma 5. Let A \u2208 Rd\u00d7n and B \u2208 Rd\u00d7n be two matrices with disjoint column support. Then, we have \u2200i \u2208 min(d, n):\nmax(\u03c3i(A), \u03c3i(B)) \u2264 \u03c3i(A+B)\nProof. Let the SVD of A and B be UA\u03a3AV \u22a4 A and UB\u03a3BV \u22a4 B respectively. The lemma holds for i = 0 as\u2225\u2225v\u22a4(A+B) \u2225\u2225 \u2265 max( \u2225\u2225v\u22a4A \u2225\u2225 , \u2225\u2225v\u22a4B\n\u2225\u2225). For any matrix M , \u03c3i(M) = min U\u2208Rd\u00d7(i\u22121) \u2225\u2225(I \u2212 UU\u22a4)M \u2225\u2225 \u2200i > 1. For\nany U , there exist v1 and v2 in Span((UA)[i]) and Span((UB)[i]) respectively and v \u22a4 1 U = v \u22a4 2 U = 0. This is because the rank of Span(U) is at most (i \u2212 1) and Span((UA)[i]) and Span((UB)[i]) are both rank-i subspaces. Now, we have \u2225\u2225v\u22a41 (A+B) \u2225\u2225 \u2265 \u2225\u2225v\u22a41 A \u2225\u2225 \u2265 \u03c3iA and \u2225\u2225v\u22a41 (A+B) \u2225\u2225 \u2265 \u2225\u2225v\u22a41 B \u2225\u2225 \u2265 \u03c3i(B). The lemma by using either v1 or v2 for any U .\nThe next lemma shows that an incoherent matrix remains incoherent even if a small number of columns have been set to 0.\nLemma 6. Let L \u2208 Rd\u00d7n be a \u00b5-column-incoherent, rank-r matrix. Let S \u2282 [n] such that |S| \u2264 132\u00b52r . Let [U, S, V ] and [U\\S ,\u03a3\\S , V\\S ] denote the SVDs of L and L\\S respectively. Then, the following hold \u2200i \u2208 [n]:\nClaim 1: \u2225\u2225e\u22a4i V\\S \u2225\u2225 \u2264 33 32 \u00b5\n\u221a r\nn Claim 2:\n31 32 \u03c3i(L) \u2264 \u03c3i(L\\S) \u2264 \u03c3i(L)\nFurthermore, each column Li\u2200i \u2208 [n] can be expressed as:\nClaim 3: Li = U\\S\u03a3\\Swi with \u2016wi\u2016 \u2264 33\n32 \u00b5\n\u221a r\nn\nProof. Let T be defined as the matrix V with the rows in set S set to 0. We will first begin by proving that T is full rank. Let u \u2208 Rr and \u2016u\u2016 = 1:\n1 = \u2016V u\u2016 \u2265 \u2016Tu\u2016 =\n  n\u2211\ni=1\n\u3008u, Vi,:\u30092 \u2212 \u2211\nj\u2208S \u3008u, Vj,:\u30092\n  1 2\n\u2265  1\u2212 \u2211\nj\u2208S \u2016Vj,:\u20162\n  1 2 \u2265 ( 1\u2212 1\n32\n) 1 2\nwhere the second inequality is obtained from the bound on |S| and \u2016Vj,:\u2016. Since T and V\\S have the same column space, there exists a matrix R \u2208 Rr\u00d7r such that TR = V\\S . We know that R is full rank. We will now prove bounds on the singular values of R. For any u \u2208 Rr and \u2016u\u2016 = 1\n\u2016Ru\u2016 = \u2016V Ru\u2016 \u2265 \u2016TRu\u2016 = \u2225\u2225V\\Su \u2225\u2225 = 1 = \u2225\u2225V\\Su \u2225\u2225 = \u2016TRu\u2016 \u2265 ( 1\u2212 1\n32\n) 1 2\n\u2016Ru\u2016\nFrom this, we obtain the following inequality:\n1 \u2264 \u2016Ru\u2016 \u2264 ( 1\u2212 1\n32\n)\u2212 12\nFrom this, we have the first claim of the lemma as TR = V\\S . We also know that U\\S\u03a3\\SV \u22a4 \\S = U\u03a3T \u22a4. Writing V\\S as TR, we have U\\S\u03a3\\SR \u22a4T\u22a4 = U\u03a3T\u22a4. Using the fact that T\u22a4 is full rank, we have U\\S\u03a3\\SR \u22a4 = U\u03a3. From this we have that L = U\u03a3V \u22a4 = U\\S\u03a3\\SR\n\u22a4V \u22a4. Choosing wi = (R\u22a4V \u22a4)i, the second claim of the lemma follows.\nFor the final claim of the lemma, note that the singular values of L\\S are the same as the singular values of U\u03a3(R\u22a4)\u22121. We know that \u03c3k+1(L\\S) = min\nQ\u2208Rd\u00d7k\n\u2225\u2225(I \u2212QQ\u22a4)U\u03a3(R\u22a4)\u22121 \u2225\u2225. The upper bound follows\nfrom setting Q to be the first k singular vectors of L and our bound on the singular values of R. For the lower bound, consider any Q \u2208 Rd\u00d7k. Span(Q) is a subspace of rank at most k. Therefore, there exists v \u2208 Span(U1) such that \u2016v\u2016 = 1 and v\u22a4Q = 0. We now have\n\u2225\u2225v\u22a4(I \u2212QQ\u22a4)U\u03a3(R\u22a4)\u22121 \u2225\u2225 = \u2225\u2225v\u22a4U\u03a3(R\u22a4)\u22121 \u2225\u2225 \u2265 \u03c3k+1(L)\u2016R\u2016 \u2265 31 32 \u03c3k+1(M)\nWhere the last inequality follows from our bounds on the singular values of R and noting that the singular values of R\u22121 are the inverses of the singular values of R. This proves the third claim of the lemma.\nWe begin by stating a lemma used for bounding the length of Gaussian random vectors from Laurent and Massart [2000]:\nLemma 7. Let Y1, Y2, \u00b7 \u00b7 \u00b7 , Yd be i.i.d Gaussian random variables with mean 0 and variance 1. Let Z = d\u2211\ni=1\n( Y 2i \u2212 1 ) . Then the following inequality holds for any positive x:\nP ( Z \u2265 2 \u221a dx+ 2x ) \u2264 exp(\u2212x)\nWe will now state the famous Bernstein\u2019s Inequality from Boucheron et al. [2013].\nTheorem 6. Let X1, . . . , Xn be independent real-valued random variables. Assume that there exist positive real numbers \u03bd and c such that n\u2211\ni=1\nE [ X2i ] \u2264 \u03bd and\nn\u2211\ni=1\nE [ (Xi) q + ] \u2264 q!\n2 \u03bdcq\u22122 \u2200 q \u2265 3,\nwhere x+ = max(x, 0).\nIf S = n\u2211\ni=1\n(Xi \u2212 E[Xi]), then \u2200t \u2265 0, we have:\nP ( S \u2265 \u221a 2\u03bdt+ ct ) \u2264 exp(\u2212t)\nWe will now restate a lemma for controlling the singular values of a matrix with Gaussian random entries from Vershynin [2010].\nLemma 8. Let A \u2208 Rd\u00d7n be a random matrix whose entries are independent standard normal random variables. Then, for every t \u2265 0, with probability at least 1\u2212 2 exp ( \u2212t2/2 ) , we have:\n\u221a n\u2212 \u221a d\u2212 t \u2264 \u03c3min(A) \u2264 \u03c3max(A) \u2264 \u221a n+ \u221a d+ t\nCorollary 1. Let A \u2208 Rd\u00d7n be a random matrix whose entries are independent standard normal random variables. For n \u2265 200 ( d+ 2 log ( 2 \u03b4 )) , we have:\n0.9 \u221a n \u2264 \u03c3min(A) \u2264 \u03c3max(A) \u2264 1.1 \u221a n\nwith probability at least 1\u2212 \u03b4 Lemma 9. Let Y1, Y2, . . . , Yn be iid d-dimensional random vectors such that Yi \u223c N (0, I)\u2200i \u2208 [n]. Then, we have for any c2 \u2264 1:\nP (\u2223\u2223\u2223\u2223\u2223 { i : \u2016Yi\u2016 \u2265 d 1 2 + 2d 1 4 ( log ( 1\nc2\n) + log ( \u00b52r )) 12 }\u2223\u2223\u2223\u2223\u2223 \u2265 3c2n 2\u00b52r ) \u2264 \u03b2\nwhen n \u2265 16\u00b52rc2 log ( 1 \u03b2 ) .\nProof. Let Y1, . . . , Yn be iid random vectors such that Yi \u223c N (0, I)\u2200i \u2208 [n]. From Lemma 7, we have that:\nP ( \u2016Yi\u2016 \u2265 d1/2 + 2d 1 4 ( log ( 1\nc2\n) + log ( \u00b52r ))1/2 )\n\u2264 c2 \u00b52r\nLet p := P ( \u2016Yi\u2016 \u2265 d1/2 + 2d 1 4 ( log ( 1 c2 ) + log ( \u00b52r ))1/2) . Consider random variables Zi\u2200i \u2208 [n] be defined\nsuch that Zi = I [ \u2016Yi\u2016 \u2265 d1/2 + 2d 14 ( log ( 1 c2 ) + log ( \u00b52r ))1/2] . Note that Zi satisfy the conditions of Theorem 6 with \u03bd = np and c = 1. We can now bound the probability that n\u2211\ni=1\nZi is large by setting\nt = nc216\u00b52r :\nP ( n\u2211\ni=1\nZi \u2264 3nc2 2\u00b52r\n) \u2264 P ( n\u2211\ni=1\nZi \u2264 n\u2211\ni=1\nE [Zi] + nc2 2\u00b52r\n) \u2264 P ( n\u2211\ni=1\nZi \u2264 np+ \u221a 2npt+ t ) \u2264 exp (\u2212t)\nFor our choice of n, the above inequality implies the lemma."}, {"heading": "B Proof of Technical Lemmas", "text": ""}, {"heading": "B.1 Proof of Lemma 1", "text": "Proof. We prove the lemma through a series of inequalities:\n\u2225\u2225\u2225(I \u2212 U\\SU\u22a4\\S)L\u2217i \u2225\u2225\u2225 (\u03b61)\n\u2264 33 32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225(I \u2212 U\\SU\u22a4\\S)L\u2217\\S \u2225\u2225\u2225 (\u03b62)\n\u2264 33 32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225(I \u2212 U\\S(U\\S)\u22a4)M\u2217\\S \u2225\u2225\u2225\n\u2264 33 32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225(I \u2212 U\u2217(U\u2217)\u22a4)M\u2217\\S \u2225\u2225\u2225 ,\nwhere (\u03b61) holds from Lemma 6 and (\u03b62) follows by using the fact that L \u2217 \\S can be obtained by setting a few columns of M\u2217\\S to 0. The last inequality follows from the fact that U\\S contains the top-r singular vectors of M\u2217\\S."}, {"heading": "B.2 Proof of Lemma 2", "text": "Proof. The lemma can be proved through the following set of inequalities:\n\u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SL \u2217 i \u2225\u2225\u2225 (\u03b61) \u2264 \u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SU \u2217 \\S\u03a3 \u2217 \\Sw \u2225\u2225\u2225 \u2264 33 32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SL \u2217 \\S \u2225\u2225\u2225\n(\u03b62)\n\u2264 33 32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SM \u2217 \\S \u2225\u2225\u2225 \u2264 33 32 \u00b5\n\u221a r\nn ,\nwhere (\u03b61) holds with \u2016w\u2016 \u2264 3332\u00b5 \u221a r n from Lemma 6 and (\u03b62) follows from the fact that L \u2217 \\S can be obtained from M\u2217\\S by setting some columns in M \u2217 \\S to 0."}, {"heading": "B.3 Lemma 10", "text": "Lemma 10. Consider the setting of Theorem 2. Let S \u2282 [n] denote any subset of the columns of M\u2217 such that |S| \u2264 3\u03c1n. Furthermore, suppose that \u2016N\u2217\u2016F \u2264 \u03c3k(L \u2217)\n16 for some k \u2208 [r]. Let M\u2217\\S(L\u2217\\S, N\u2217\\S, C\u2217\\S) denote the matrix M\u2217(L\u2217, N\u2217, C\u2217) projected onto the columns not in S. Let U\\S\u03a3\\SV \u22a4 \\S denote the rank-k \u2032"}, {"heading": "SVD of M\u2217\\S for some k", "text": "\u2032 \u2264 k. Then, we have:\n# ( i : \u2016Ei\u2016 \u2265 2\u00b5 \u221a r\nn\n) \u2264 2\u03c1n,\nwhere E = \u03a3\u22121\\SU \u22a4 \\SM \u2217\nProof. From Lemma 6, we get that \u03c3k\u2032(L \u2217 \\S) \u2265 3132\u03c3k\u2032 (L\u2217). Along with Theorem 5, we conclude that \u03c3k\u2032 (L\u2217\\S+ N\u2217\\S) \u2265 78\u03c3k\u2032 (L\u2217). Since the column supports of L\u2217+N\u2217 and C\u2217 are disjoint, we have that \u03c3k\u2032(L\u2217\\S+N\u2217\\S) \u2264 \u03c3k\u2032 (M \u2217 \\S). That is,\n\u03c3k\u2032 (M \u2217 \\S) \u2265\n7 8 \u03c3k\u2032(L \u2217). (6)\nWe first bound the quantity \u2225\u2225\u2225\u03a3\u22121\\SU\u22a4\\SL\u2217i \u2225\u2225\u2225 \u2200i \u2208 [n]:\n\u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SL \u2217 i \u2225\u2225\u2225 (\u03b61) \u2264 33 32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SL \u2217 \\S \u2225\u2225\u2225 (\u03b62) \u2264 33 32 \u00b5\n\u221a r\nn\n(\u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\S(L \u2217 \\S +N \u2217 \\S) \u2225\u2225\u2225+ \u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SN \u2217 \\S \u2225\u2225\u2225 )\n(\u03b63)\n\u2264 33 32 \u00b5\n\u221a r\nn\n(\u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SM \u2217 \\S \u2225\u2225\u2225+ \u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SN \u2217 \\S \u2225\u2225\u2225 ) (\u03b64) \u2264 33 32 \u00b5\n\u221a r\nn\n( 1 + 1\n14\n) \u2264 9\n8 \u00b5\n\u221a r\nn , (7)\nwhere \u03b61 follows from Lemma 6, \u03b62 using triangle inequality, \u03b63 using the above given fact that SVDr(M\u2217\\S) = U\\S\u03a3\\S(V\\S)\n\u22a4, \u03b64 follows from using (6) with bound on \u2016N\u2217\u2016F . Suppose \u2225\u2225\u2225\u03a3\u22121\\SU\u22a4\\S(L\u2217i +N\u2217i ) \u2225\u2225\u2225 \u2265 2\u00b5 \u221a r n for some i. We now have:\n\u2225\u2225\u2225\u03a3\u22121\\SU \u22a4 \\SN \u2217 i \u2225\u2225\u2225 \u2265 7 8 \u00b5\n\u221a r\nn .\nSimilarly, using (6), we get that:\n\u2016N\u2217i \u2016 \u2265 3\n4 \u00b5\n\u221a r\nn \u03c3k\u2032 (L\n\u2217).\nLet \u0393 := {i : \u2225\u2225\u2225\u03a3\u22121\\SU\u22a4\\S(L\u2217i +N\u2217i ) \u2225\u2225\u2225 \u2265 2\u00b5 \u221a r n}. Let N\u2217\u0393 denote the matrix N\u2217 restricted to the set \u0393. Then\nwe have, \u221a |\u0393|3\n4 \u00b5\n\u221a r\nn \u03c3k\u2032(L\n\u2217) \u2264 \u2016N\u2217\u0393\u2016 \u2264 \u2016N\u2217\u2016 \u2264 1\n16 \u03c3k\u2032(L\n\u2217).\nThis implies that |\u0393| \u2264 n144\u00b52r \u2264 \u03c1n. Also, by our assumption, \u03b1 \u2264 \u03c1, i.e., number of non-zero C\u2217i is less than \u03c1n. That is, the set {i : \u2225\u2225\u2225\u03a3\u22121\\SU\u22a4\\SC\u2217i \u2225\u2225\u2225 \u2265 2\u00b5 \u221a r n} is of size at most \u03c1n. Using the fact that support of C\u2217 and L\u2217 +N\u2217 is disjoint, we have that the set {i : \u2225\u2225\u2225\u03a3\u22121\\SU\u22a4\\SM\u2217i \u2225\u2225\u2225 \u2265 2\u00b5 \u221a r n} is of size at most 2\u03c1n."}, {"heading": "B.4 Proof of Lemma 11", "text": "Lemma 11. Assume the setting of Lemma 10. Let U\\S\u03a3\\SV \u22a4 \\S (U \u2217 \\S\u03a3 \u2217 \\S(V \u2217 \\S) \u22a4) be the rank-k SVD of M\u2217\\S (L\u2217\\S), then the following holds \u2200i, 1 \u2264 i \u2264 n:\n\u2225\u2225\u2225(I \u2212 U\\SU\u22a4\\S)(L\u2217i +N\u2217i ) \u2225\u2225\u2225 \u2264 33\n32 \u00b5\n\u221a r\nn\n(\u2225\u2225\u2225(I \u2212 U\u22171:k(U\u22171:k)\u22a4)M\u2217\\S \u2225\u2225\u2225+ \u2016N\u2217\u2016 ) + \u2016N\u2217i \u2016 ."}, {"heading": "Proof.", "text": "\u2225\u2225\u2225(I \u2212 U\\SU\u22a4\\S)(L\u2217i +N\u2217i ) \u2225\u2225\u2225 \u2264 \u2016N\u2217i \u2016+ \u2225\u2225\u2225(I \u2212 U\\SU\u22a4\\S)L\u2217i \u2225\u2225\u2225 (\u03b61) \u2264 \u2016N\u2217i \u2016+ 33\n32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225(I \u2212 U\\SU\u22a4\\S)L\u2217\\S \u2225\u2225\u2225\n\u2264 \u2016N\u2217i \u2016+ 33\n32 \u00b5\n\u221a r\nn\n(\u2225\u2225\u2225(I \u2212 U\\SU\u22a4\\S)(L\u2217\\S +N\u2217\\S) \u2225\u2225\u2225+ \u2225\u2225\u2225N\u2217\\S \u2225\u2225\u2225 )\n\u2264 \u2016N\u2217i \u2016+ 33\n32 \u00b5\n\u221a r\nn\n(\u2225\u2225\u2225(I \u2212 U\\SU\u22a4\\S)M\u2217\\S \u2225\u2225\u2225+ \u2016N\u2217\u2016 )\n\u2264 \u2016N\u2217i \u2016+ 33\n32 \u00b5\n\u221a r\nn\n(\u2225\u2225\u2225(I \u2212 U\u22171:k(U\u22171:k)\u22a4)M\u2217\\S \u2225\u2225\u2225+ \u2016N\u2217\u2016 )\nwhere (\u03b61) follows from Lemma 6 and the fact that only 3\u03c1n columns are ever thresholded at any stage of the algorithm. The remaining inequalities follow using triangle inequality and M\u2217\\S = L \u2217 \\S +N \u2217 \\S along with SVDr(M\u2217\\S) = U\\S\u03a3\\S(V\\S)\u22a4."}, {"heading": "B.5 Proof of Lemma 3", "text": "Proof. Let S(t) denote the columns of C\u2217 that are not thresholded in the tth inner iteration. For each j \u2208 S(t), we know that \u2225\u2225(\u03a3(t))\u22121(U (t))\u22a4C\u2217j \u2225\u2225 \u2264 2\u00b5 \u221a r n from our assumption on the termination of the algorithm. Furthermore, since C\u2217j is not thresholded, we can associate a unique column ij which is thresholded and\nij 6\u2208 Supp (C\u2217) such that \u2225\u2225\u2225I \u2212 U (t)(U (t))\u22a4M\u2217ij \u2225\u2225\u2225 \u2265 \u2225\u2225I \u2212 U (t)(U (t))\u22a4M\u2217j \u2225\u2225. Let yi,t := (U (t))\u22121(\u03a3(t))\u22121M\u2217i and ri,t := (I \u2212 U (t)(U (t))\u22a4)M\u2217i , \u2200i. Thus we have:\n\u2225\u2225yj,t \u2225\u2225 \u2264 2\u00b5\n\u221a r\nn ,\n\u2225\u2225rj,t \u2225\u2225 \u2264 \u2225\u2225rij ,t \u2225\u2225 .\nLet Q(t) denote the columns of L\u2217 that have been thresholded in the tth iteration. Furthermore, we definite the matrices L\u0303(t+1) := L\u2217\\Q(t) , N\u0303 (t+1) := N\u2217\\Q(t) and C\u0303 (t+1) := C\u2217 S(t) . Recall the notation, PU\u22a5 (M) = (I \u2212 UUT )M . We now have for any t \u2265 0: \u2225\u2225\u2225PU (t+1)\n\u22a5 (L \u2217) \u2225\u2225\u2225 F = \u2225\u2225\u2225PU (t+1) \u22a5 (L\u0303 (t+1) + (L\u2217 \u2212 L\u0303(t+1))) \u2225\u2225\u2225 F \u2264 \u2225\u2225\u2225PU (t+1) \u22a5 (L\u0303 (t+1)) \u2225\u2225\u2225 F + \u2225\u2225\u2225PU (t+1) \u22a5 (L \u2217 \u2212 L\u0303(t+1)) \u2225\u2225\u2225 F\n\u2264 \u2225\u2225\u2225PU (t+1) \u22a5 (L\u0303 (t+1)) \u2225\u2225\u2225 F +\n  \u2211\ni\u2208Q(t)\n\u2225\u2225\u2225PU (t+1) \u22a5 (L \u2217 i ) \u2225\u2225\u2225 2\n  1 2 (\u03b61) \u2264 \u2225\u2225\u2225PU (t+1) \u22a5 (L\u0303 (t+1)) \u2225\u2225\u2225 F ( 1 + \u221a 3\u03c1n 33 32 \u00b5 \u221a r n )\n(\u03b62)\n\u2264 5 4\n(\u2225\u2225\u2225PU (t+1)\n\u22a5 (L\u0303 (t+1) + N\u0303 (t+1)) \u2225\u2225\u2225 F + \u2225\u2225\u2225PU (t+1) \u22a5 (N\u0303 (t+1)) \u2225\u2225\u2225 F )\n(\u03b63)\n\u2264 5 4\n(\u2225\u2225\u2225PU (t+1) \u22a5 (L\u0303 (t+1) + N\u0303 (t+1) + C\u0303(t+1)) \u2225\u2225\u2225 F + \u2225\u2225\u2225N\u0303 (t+1) \u2225\u2225\u2225 F )\n(\u03b64)\n\u2264 5 4\n( P\nU\u22171:k \u22a5 (L\u0303 (t+1) + N\u0303 (t+1) + C\u0303(t+1)) ) + \u2225\u2225\u2225N\u0303 (t+1) \u2225\u2225\u2225 F (\u03b65) \u2264 5 4 (\u2225\u2225\u2225PU \u2217 1:k \u22a5 (M (t+1)) \u2225\u2225\u2225 F + \u2016N\u2217\u2016F ) , (8)\nwhere (\u03b61) follows from Lemma 6, (\u03b62) from triangle inequality and bound over \u03c1, (\u03b63) from Lemma 5, (\u03b64) from the properties of the SVD and (\u03b65) from Lemma 5.\nWe will now show that \u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (M (t+1)) \u2225\u2225\u2225 F decreases at a geometric rate:\n\u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (M (t+1) \u2225\u2225\u2225 F (\u03b66) \u2264 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (L\u0303 (t+1)) \u2225\u2225\u2225 F + \u2016N\u2217\u2016F + (\u2225\u2225\u2225PU \u2217 1:k \u22a5 (C\u0303 (t+1)) \u2225\u2225\u2225 F )\n(\u03b67) \u2264 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (L \u2217) \u2225\u2225\u2225 F + \u2016N\u2217\u2016F +\n  \u2211\nj\u2208S(t\u22121)\n\u2225\u2225\u2225PU \u2217 1:k\n\u22a5 ((C\u0303 (t+1))j)\n\u2225\u2225\u2225 2  \n(\u03b68) \u2264 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (L \u2217) \u2225\u2225\u2225 F + \u2016N\u2217\u2016F +\n 2 \u2211\nj\u2208S(t)\n\u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (U (t)\u03a3(t)yj,t\n\u2225\u2225\u2225 2 + \u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (r j,t)\n\u2225\u2225\u2225 2   1 2\n\u2264 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (L \u2217) \u2225\u2225\u2225 F + \u2016N\u2217\u2016F +\n 8\u03c1n\u00b5 2r\nn\n\u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (U (t)\u03a3(t))\n\u2225\u2225\u2225 2 + 2 \u2211\nj\u2208S(t)\n\u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (r j,t)\n\u2225\u2225\u2225 2   1 2\n(\u03b69) \u2264 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (L \u2217) \u2225\u2225\u2225 F + \u2016N\u2217\u2016F +  1 8 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (U (t)\u03a3(t)) \u2225\u2225\u2225 2 + 2 \u2211\nj\u2208S(t)\n\u2225\u2225rij ,t \u2225\u22252   1 2\n(\u03b610) \u2264 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (L \u2217) \u2225\u2225\u2225 F + \u2016N\u2217\u2016F +   1 8 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (U (t)\u03a3(t)) \u2225\u2225\u2225 2\n\ufe38 \ufe37\ufe37 \ufe38 Term 1\n+4 \u2211\nj\u2208S(t)\n\u2225\u2225\u2225N\u2217ij \u2225\u2225\u2225 2 + ( 33\n32\n)2 \u00b52 r\nn\n(\u2225\u2225\u2225PU (t) \u22a5 (L\u0303 (t) + N\u0303 (t)) \u2225\u2225\u2225 )2\n\ufe38 \ufe37\ufe37 \ufe38 Term 2\n  1 2 , (9)\nwhere (\u03b66) follows from triangle inequality, (\u03b67) follows from Lemma 5, (\u03b68) follows from triangle inequality and the fact that (a+ b)2 \u2264 2a2+2b2, (\u03b69) from our previous observations about yj,r and rj,t and (\u03b610) from Lemma 10.\nWe will now proceed to bound Term 1 as follows:\n\u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (U (t)\u03a3(t))\n\u2225\u2225\u2225 2\nF\n(\u03b611) \u2264 \u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (M (t))\n\u2225\u2225\u2225 2\nF , (10)\nwhere (\u03b611) follows from considering the full SVD of M (t) and Lemma 5.\nWe now proceed to bound Term 2 as:\n\u2211\nj\u2208S(t)\n\u2225\u2225\u2225N\u2217ij \u2225\u2225\u2225 2 + ( 33\n32\n)2 \u00b52 r\nn\n(\u2225\u2225\u2225PU (t) \u22a5 (L\u0303 (t) + N\u0303 (t)) \u2225\u2225\u2225 )2 \u2264 \u2016N\u2217\u20162F + 9\n8 \u03c1n\n\u00b52r\nn\n\u2225\u2225\u2225PU (t) \u22a5 (M (t)) \u2225\u2225\u2225 2\nF\n\u2264 \u2016N\u2217\u20162F + 1\n32\n\u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (M (t))\n\u2225\u2225\u2225 2\nF , (11)\nwhere the first inequality follows from Lemma 5 and the second inequality from the fact that U (t+1) are top-k left singular vectors of M (t).\nUsing (9), (10), (11), we have:\n\u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (M (t+1)) \u2225\u2225\u2225 F \u2264 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (L \u2217) \u2225\u2225\u2225 F + \u2016N\u2217\u2016F + ( 1 8 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (M (t)) \u2225\u2225\u2225 2 F + 4 \u2016N\u2217\u20162F + 1 8 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (M (t)) \u2225\u2225\u2225 2 F ) 1 2\n\u2264 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (L \u2217) \u2225\u2225\u2225 F + 3 \u2016N\u2217\u2016F + 1 2 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (M (t)) \u2225\u2225\u2225 F ,\nwhere the first inequality follows from Lemma 5 and considering the full SVD of M (t) and the last inequality follows from the fact that \u221a a+ b+ c \u2264 \u221aa+ \u221a b + \u221a c.\nBy recursively applying the above inequality, we have: \u2225\u2225\u2225PU \u2217 1:k\n\u22a5 (M (T+1)) \u2225\u2225\u2225 F \u2264 2 \u2225\u2225\u2225PU \u2217 1:k \u22a5 (L \u2217) \u2225\u2225\u2225 F + 6 \u2016N\u2217\u2016F + \u01eb 20n .\nLemma now follows using (8) with the above equation."}, {"heading": "B.6 Lemma 12", "text": "Lemma 12. Let Y1, Y2, . . . , Yn be iid d-dimensional random vectors such that Yi \u223c N (0, I)\u2200i \u2208 [n]. Then, we have for any c1 \u2264 1:\nP ( \u2203v \u2208 Rd, \u2016v\u2016 = 1 s.t \u2223\u2223\u2223\u2223\u2223 { i : \u3008v, Yi\u3009 \u2265 2 ( log ( \u00b52r ) + log (d) + log ( 1\nc1\n)) 1 2 }\u2223\u2223\u2223\u2223\u2223 \u2265 3c1n \u00b52rd ) \u2264 \u03b2,\nwhen n \u2265 16\u00b52rdc1 [ log ( 1 \u03b2 ) + d log (80d) ] .\nProof. Let v \u2208 Rd s.t \u2016v\u2016 = 1. We define the set Sv,\u03b8 as follows:\nSv,\u03b8 = {u : u \u2208 Rd \u2227 \u2016u\u2016 = 1 \u2227 \u3008u, v\u3009 \u2265 cos(\u03b8)}.\nWe now define the set T (v, \u03b8, \u03b4) as: T (v, \u03b8, \u03b4) = {x : x \u2208 Rd \u2227 \u2203u \u2208 Sv,\u03b8 s.t \u3008u, x\u3009 \u2265 \u03b4}.\nNow, let y \u223c N (0, I). Using spherical symmetry of the Gaussian, w.l.o.g. v = e1. We now define the complementary sets Q (\u03bd) and R (\u03bd) as:\nQ (\u03bd) = {x : x \u2208 Rd \u2227 x1 < \u03bd}, R (\u03bd) = {x : x \u2208 Rd \u2227 x1 \u2265 \u03bd}.\nWe will now bound the probability that y \u2208 T (v, \u03b8, \u03b4) for \u03b4 = 2 ( log ( \u00b52r ) + log (d) + log ( 1 c1 ))1/2 and\n\u03b8 = csc\u22121 ( 10(d\u2212 1)1/2 ) .\nP (y \u2208 T (e1, \u03b8, \u03b4)) = \u222b\nT (e1,\u03b8,\u03b4)\n1 (\u221a\n2\u03c0 )d exp\n( \u2212\u2016y\u2016 2\n2\n) dy\n=\n\u222b\nT (e1,\u03b8,\u03b4)\u2229Q(\u03b4/ \u221a 2)\n1 (\u221a\n2\u03c0 )d exp\n( \u2212\u2016y\u2016 2\n2\n) dy +\n\u222b\nT (e1,\u03b8,\u03b4)\u2229R(\u03b4/ \u221a 2)\n1 (\u221a\n2\u03c0 )d exp\n( \u2212\u2016y\u2016 2\n2\n) dy\n\u2264 \u222b\nR(\u03b4/ \u221a 2)\n1 (\u221a\n2\u03c0 )d exp\n( \u2212\u2016y\u2016 2\n2\n) dy +\n\u222b\nT (e1,\u03b8,\u03b4)\u2229Q(\u03b4/ \u221a 2)\n1 (\u221a\n2\u03c0 )d exp\n( \u2212\u2016y\u2016 2\n2\n) dy\n(\u03b61)\n\u2264 c1 \u00b52rd +\n\u222b\nT (e1,\u03b8,\u03b4)\u2229Q(\u03b4/ \u221a 2)\n1 (\u221a\n2\u03c0 )d exp\n( \u2212\u2016y\u2016 2\n2\n) dy,\nwhere (\u03b61) follows from the fact that for t \u2265 1, \u221e\u222b t 1\u221a 2\u03c0 exp ( \u2212x22 ) dx \u2264 exp ( \u2212 t22 ) .\nWe will use M (\u03b8, \u03b4, \u03b3) to denote the set {z : z \u2208 Rd\u22121 \u2227 (\u03b3, z) \u2208 T (e1, \u03b8, \u03b4)\u2229Q ( \u03b4/ \u221a 2 ) }. We can bound\nthe second term as follows:\n\u222b\nT (e1,\u03b8,\u03b4)\u2229Q(\u03b4/ \u221a 2)\n1 (\u221a\n2\u03c0 )d exp\n( \u2212\u2016y\u2016 2\n2\n) dy\n\u2264 \u03b4/\n\u221a 2\u222b\n\u2212\u221e\n1\u221a 2\u03c0 exp\n( \u2212y 2 1\n2\n)   \u222b\nM(\u03b8,\u03b4,y1)\n1 (\u221a\n2\u03c0 )d\u22121 exp\n( \u2212\u2016z\u2016 2\n2\n) dz   dy1. (12)\nNow, let z \u2208 Rd be such that z1 = y1 \u2227 z2:d \u2208 M (\u03b8, \u03b4, y1) for some y1 \u2208 [\u2212\u221e, \u03b4/ \u221a 2]. Therefore, \u2203w \u2208 Sv,\u03b8 such that \u3008w, z\u3009 \u2265 \u03b4. We can decompose w into its components along v and orthogonal to it, w = cos(\u03b8\u2032)v+ sin(\u03b8\u2032)v\u22a5 for some unit vector v\u22a5 orthogonal to v and some \u03b8\u2032 \u2208 [0, \u03b8]. We know that \u3008w, z\u3009 \u2265 \u03b4 and that \u3008w, v\u3009 \u2264 \u03b4/ \u221a 2. From these two inequalities and using the fact that v = e1, we get:\nsin(\u03b8\u2032) \u2016z2:d\u2016 \u2265 sin(\u03b8\u2032) \u2329 v\u22a5, z \u232a \u2265 \u03b4 \u2212 cos(\u03b8\u2032) \u3008v, z\u3009 \u2265 \u03b4 \u2212 cos(\u03b8\u2032) \u03b4\u221a\n2 \u2265\n( 1\u2212 1\u221a\n2\n) \u03b4.\nThis allows us to lower bound the length of z2:d by 10(d\u2212 1)1/2 ( 1\u2212 1\u221a\n2\n) \u03b4. For our choice of \u03b4 and \u03b8 and\nusing Lemma 7, we now get that the inner integration in equation 12 is atmost c1\u00b52rd . Thus, we have the following bound on P (y \u2208 T (e1, \u03b8, \u03b4)):\nP (y \u2208 T (e1, \u03b8, \u03b4)) \u2264 2c1 \u00b52rd . (13)\nLet p be used to denote the value P (y \u2208 T (e1, \u03b8, \u03b4)). Now, assume Y1, . . . , Yn are iid random vectors with Yi \u223c N (0, I) \u2200i \u2208 [n]. Now let Zi be defined such that Zi = I [Yi \u2208 T (ei, \u03b8, \u03b4)] \u2200i \u2208 [n]. Note that Zi is a Bernoulli random variable which is 1 with probability p. It can be seen that Zi satisfy satisfy the conditions of 6 with \u03bd = np and c = 1. Therefore, setting t = nc116\u00b52rd in Theorem 6, we get:\nP ( n\u2211\ni=1\nZi \u2265 3nc1 \u00b52rd\n) \u2264 P ( n\u2211\ni=1\nZi \u2265 np+ nc1 \u00b52rd\n) \u2264 P ( n\u2211\ni=1\nZi \u2265 np+ \u221a 2\u03bdt+ t ) \u2264 exp (\u2212t) . (14)\nNow, consider the subset K := {x : x \u2208 Rd \u2227 |xi| \u2264 1\u2200i \u2208 [d]}. Consider a partitioning of K into subsets K (\u01eb, j) = {x : x \u2208 K\u2227\u2200i \u2208 [d]ji\u01eb\u2212 1 \u2264 xi \u2264 (ji +1)\u01eb\u2212 1} where j \u2208 J is an index for each of these subsets. Note that for any \u01eb, at most (\u2308 2 \u01eb \u2309)d such indices are required to ensure that K \u2286 \u22c3j\u2208J K (\u01eb, j). Setting \u01eb = 140d , we have for any two unit vectors v1 and v2 such that v1, v2 \u2208 K (\u01eb, j) for some j, \u2016v1 \u2212 v2\u2016 \u2264 140d1/2 . From this fact, it can be seen that Equation 14 holds for all unit vectors in K (\u01eb, j) with any unit vector v \u2208 K (\u01eb, j). Therefore, we choose for each subset K (\u01eb, j), which contains a unit vector, a unit vector v and take an union bound over all such subsets K (\u01eb, j). After doing so we get the following bound:\nP ( \u2203v \u2208 Rd \u2227 \u2016v\u2016 = 1 s.t n\u2211\ni=1\nZi \u2265 3nc1 \u00b52rd\n) \u2264 (80d)d exp (\u2212t) (\u03b62) \u2264 \u03b2, (15)\nwhere (\u03b62) follows from the conditions of the theorem. Thus, we have proved the theorem."}, {"heading": "B.7 Lemma 13", "text": "Lemma 13. Assume the conditions of Theorem 3. Let S \u2282 [n] denote any subset such that |S| \u2264 164\u00b52r . Let M\u2217\\S(L \u2217 \\S, N \u2217 \\S, C \u2217 \\S) denote the matrices M \u2217 (L\u2217, N\u2217, C\u2217) restricted to the columns not in S. Let [U\\S ,\u03a3\\S , V\\S ] = SVDr+1(M\u2217\\S). Furthermore, let E = {x : x = U\\S\u03a3\\Sy for some \u2016y\u2016 \u2264 2\u00b5 \u221a r/n}. Then \u2200i, we have: \u2225\u2225PU\\S (L\u2217i +N\u2217i )\u2212 PE (L\u2217i +N\u2217i ) \u2225\u2225 \u2264 33 32 \u00b5 \u221a r n \u2016N\u2217\u2016+ \u2225\u2225PU\\S (N\u2217i ) \u2225\u2225 ,\nwhere PU\\S (M) = U\\SU\u22a4\\SM .\nProof. Let [U\u2217\\S,\u03a3 \u2217 \\S , V \u2217 \\S ] = SVDr(L\u2217\\S). Using Lemma 6, L\u2217i = L\u2217\\SV \u2217\\Swi for some \u2016wi\u2016 \u2264 3332\u00b5 \u221a r n . Now consider the vector yi := U\\S\u03a3\\SV \u22a4 \\SV \u2217 \\Swi. Note that \u2225\u2225\u2225(\u03a3\\S)\u22121U\u22a4\\Syi \u2225\u2225\u2225 \u2264 3332\u00b5 \u221a r n . Now, using definition of PE : \u2225\u2225PU\\S (L\u2217i +N\u2217i )\u2212 PE (L\u2217i +N\u2217i ) \u2225\u2225 \u2264 \u2225\u2225PU\\S (L\u2217i +N\u2217i )\u2212 yi \u2225\u2225\n\u2264 \u2225\u2225PU\\S (N\u2217i ) \u2225\u2225+ \u2225\u2225\u2225PU\\S ( L\u2217\\SV \u2217 \\Swi ) \u2212 U\\S\u03a3\\SV \u22a4\\SV \u2217\\Swi \u2225\u2225\u2225 (\u03b61)\n\u2264 \u2225\u2225PU\\S (N\u2217i ) \u2225\u2225+ 33 32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225PU\\S ( L\u2217\\S ) V \u2217\\S \u2212 PU\\S ( L\u2217\\S +N \u2217 \\S ) V \u2217\\S \u2225\u2225\u2225\n\u2264 \u2225\u2225PU\\S (N\u2217i ) \u2225\u2225+ 33 32 \u00b5\n\u221a r\nn\n\u2225\u2225\u2225PU\\S ( N\u2217\\S )\u2225\u2225\u2225 ,\nwhere (\u03b61) holds from Lemma 6 and the fact that V \u2217 \\S has zeros in the rows corresponding to the support of C\u2217."}, {"heading": "B.8 Proof of Lemma 4", "text": "Proof. Note that under the conditions of Theorem 3, the following three conditions hold with probability at least 1\u2212 \u03b4.\n#i ( \u2016N\u2217i \u2016 \u2265 \u03c3 ( \u221a d+ 2d 1 4 \u221a log ( \u00b52r\nc2\n))) \u2264 3c2n\n\u00b52r\n\u2200 \u2016v\u2016 = 1 #i ( |\u3008v,N\u2217i \u3009| \u2265 2\u03c3 \u221a log ( \u00b52r2d\nc1\n)) \u2264 6c1n\n\u00b52r2d\n0.9\u03c3 \u221a n \u2264 \u2016N\u2217\u2016 \u2264 1.1\u03c3\u221an\nAlso, for c1 = 1 12288 and c2 = 1 1536 , the invariant implies at most n\n512\u00b52r clean data points are thresholded at any stage. This allows us to apply Lemma 13 in the subsequent steps.\nWe will prove the lemma by induction on the number of thresholding steps completed so far. Let t denote the number of thresholding steps executed so far.\nBase Case (t = 0): The invariant trivially holds before any data points have been thresholded. Induction Step (t = k + 1): Assuming that the invariant holds after the kth thresholding step, we\nnow have two cases for the (k + 1)th thresholding step:\nCase 1: Thresholding with respect to \u03b61. For a column i 6\u2208 Supp (C\u2217) thresholded with respect to \u03b61, we have from Lemma 13 that:\n\u2016N\u2217i \u2016 \u2265 \u03b61 \u2212 33\n32 \u00b5\n\u221a r\nn \u2016N\u2217\u2016 = \u03c3\n( 5\n4 \u00b5 \u221a r + d 1 2 + 2d 1 4\n\u221a log ( \u00b52r\nc2\n)) \u2212 5\n4 \u03c3\u00b5\n\u221a r\n\u2265 \u03c3 ( d 1 2 + 2d 1 4 \u221a log ( \u00b52r\nc2\n)) . (16)\nFrom our choice of \u03b61, there are only n\n1024\u00b52r clean points which satisfy 16.\nCase 2: Thresholding with respect to \u03b62. For a column i 6\u2208 Supp (C\u2217), it can only be thresholded if the number of columns to be thresholded exceeds 24c1n\u00b52rd . Note that:\n\u2016PU(k+1)(N\u2217i )\u2016 \u2265 \u03b62 \u2212 5\n4 \u03c3\u00b5\n\u221a r = 2\u03c3 \u221a 2r \u221a log ( \u00b52r2d\nc1\n) .\nNote that U (k+1) is at most a rank 2r subspace. Therefore, \u2203j such that: \u2223\u2223\u2223 \u2329 N\u2217i , U (k+1) j \u232a\u2223\u2223\u2223 \u2265 2\u03c3 \u221a log ( \u00b52r2d\nc1\n) . (17)\nTaking a union bound over all j \u2208 [r + 1] and using Lemma 12 for both positive and negative inner product values, we get that at most 12c1n\u00b52rd clean points satisfy 17. Since, we threshold at least 24c1n \u00b52rd points, at least half of them must be outliers and hence the invariant holds in the next iteration."}, {"heading": "C Gaussian Noise: Proof of Theorem 3", "text": "Proof. We will prove the theorem for c1 = 1 12288 and c2 = 1\n1536 . For our choices of c1 and c2 and n, we have that:\n#i ( \u2016N\u2217i \u2016 \u2265 \u03c3 ( \u221a d+ 2d 1 4 \u221a log ( \u00b52r\nc2\n))) \u2264 3c2n\n\u00b52r\n\u2200 \u2016v\u2016 = 1 #i ( |\u3008v,N\u2217i \u3009| \u2265 2\u03c3 \u221a log ( \u00b52r2d\nc1\n)) \u2264 6c1n\n\u00b52r2d\n0.9\u03c3 \u221a n \u2264 \u2016N\u2217\u2016 \u2264 1.1\u03c3\u221an\nwith probability at least 1\u2212 \u03b4 from Lemmas 12, 9 and Corollary 1 along with our choice of n. From Lemma 4, we know that Invariant 1 holds at the termination of the algorithm. Therefore, at most\nn 512\u00b52r inliers are removed (The number of inliers removed is at most \u03b1n+ n 1024\u00b52r ).\nSuppose the algorithm terminated in the T th iteration. Let M := M\u2217 \u2212C(T ). We will start by making a few observations. The algorithm terminates when no data point is thresholded. Let [U,\u03a3, V ] = SVDr+1(M). Furthermore, define E = {x : x = U\u03a3y for some \u2016y\u2016 \u2264 2\u00b5 \u221a r/n}. Now, define set A as:\nA := { i : \u2016PU ((M)i)\u2212 PE((M)i)\u2016 \u2265 \u03c3 \u221a 2r ( 5\n4 \u00b5+ 2 log\n1 2\n( \u00b52r2d\nc1\n))} ,\nand B as:\nB := { i : \u2016PU (Mi)\u2212 PE(Mi)\u2016 \u2265 \u03c3 ( d 1 2 + 2d 1 4 ( log 1 2 ( \u00b52r\nc2\n))) + \u03c3 5\n4 \u00b5 \u221a r\n} .\nRecall that we will threshold the columns in A if |A| \u2265 24c1n\u00b52rd and the columns in B if B is not empty. Therefore, we know that:\n\u2200i \u2208 [n] \u2016PU (Mi)\u2212 PE(Mi)\u2016 \u2264 \u03c3 ( d 1 2 + 2d 1 4 ( log 1 2 ( \u00b52r\nc2\n))) + \u03c3 5\n4 \u00b5 \u221a r, |A| \u2264 24c1n \u00b52rd . (18)\nLet S denote the set of data points that have been thresholded when the algorithm terminated, i.e S = CS(T ). Let L = L\u2217\\S, N = N\u2217\\S and C = C\u2217\\S . Additionally, let [U\u2217\\S ,\u03a3\u2217\\S , V \u2217\\S ] = SVD(L). Similar to the proofs of Theorems 1 and 2, we start as follows:\n\u2225\u2225\u2225PU1:r\u22a5 (L\u2217) \u2225\u2225\u2225 \u2264 (\u2225\u2225\u2225PU1:r\u22a5 (L) \u2225\u2225\u2225 2 + \u2211\ni\u2208S\n\u2225\u2225\u2225PU1:r\u22a5 (U\u2217\\S\u03a3\u2217\\Swi) \u2225\u2225\u2225 2 ) 1 2\n\u03b61 \u2264 (\u2225\u2225\u2225PU1:r\u22a5 (L) \u2225\u2225\u2225 2 + \u2211\ni\u2208S\n9 8 \u00b52 r n\n\u2225\u2225\u2225PU1:r\u22a5 (U\u2217\\S\u03a3\u2217\\S) \u2225\u2225\u2225 2 ) 1 2 \u03b62 \u2264 (\u2225\u2225\u2225PU1:r\u22a5 (L) \u2225\u2225\u2225 2 + 3\u03c1n 9\n8 \u00b52\nr\nn\n\u2225\u2225\u2225PU1:r\u22a5 (L) \u2225\u2225\u2225 2 ) 1 2\n\u2264 5 4\n\u2225\u2225\u2225PU1:r\u22a5 (L) \u2225\u2225\u2225 \u2264 5\n4\n( \u2016N\u2016+ \u2225\u2225\u2225PU1:r\u22a5 (L+N) \u2225\u2225\u2225 )\n\u03b63 \u2264 5\n4\n( \u2016N\u2217\u2016+ \u2225\u2225\u2225PU1:r\u22a5 (M) \u2225\u2225\u2225 ) = 5\n4\n( \u2016N\u2217\u2016+ \u2225\u2225\u2225PU1:r\u22a5 (PU (M)) \u2225\u2225\u2225 ) , (19)\n\u03b61 follows using Lemma 6, \u03b62 follows using |S| \u2264 2\u03c1n, \u03b63 follows using Lemma 5 where the last equality follows from the fact that U consists of the top r + 1 singular vectors of M .\nNow, let Y be an orthogonal basis of the subspace spanned by PU (L). Note that the subspace spanned by Y is at most rank-r. Let O denote the set of corrupted columns that haven\u2019t been thresholded at the\ntermination of the algorithm and let Ol := O \u2229A and Os := O\\Ol. We can now bound \u2225\u2225\u2225PU1:r\u22a5 (PU (M)) \u2225\u2225\u2225 as follows:\n\u2225\u2225\u2225PU1:r\u22a5 (PU (M)) \u2225\u2225\u2225 \u2264 \u2225\u2225P Y\u22a5 (PU (L +N + C)) \u2225\u2225 \u2264 \u2225\u2225P Y\u22a5 (PU (N + C)) \u2225\u2225 \u2264 \u2016N\u2016+ \u2225\u2225P Y\u22a5 (PU (C)) \u2225\u2225\n\u2264 \u2016N\u2217\u2016+   \u2211\ni\u2208Ol\n\u2225\u2225P Y\u22a5 (PU (Ci)) \u2225\u22252\n\ufe38 \ufe37\ufe37 \ufe38 Term 1\n+ \u2211\nj\u2208Os\n\u2225\u2225P Y\u22a5 (PU (Cj)) \u2225\u22252\n\ufe38 \ufe37\ufe37 \ufe38 Term 2\n  1 2 , (20)\nwhere first inequality follows from the fact that U1:r are top singular vectors of PU (M) and second inequality follows from definition of Y .\nWe can now bound Term 1 as follows:\n\u2211\ni\u2208Ol\n\u2225\u2225P Y\u22a5 (PU (Ci)) \u2225\u22252 (\u03b61)\u2264 2 \u2211\ni\u2208Ol\n\u2225\u2225P Y\u22a5 (PE (Ci)) \u2225\u22252 + \u2225\u2225P Y\u22a5 ((PU (Ci)\u2212 PE (Ci))) \u2225\u22252\n\u2264 2 \u2211\ni\u2208Ol\n4\u00b52r\nn\n\u2225\u2225P Y\u22a5 (U\u03a3) \u2225\u22252 + \u2016PU (Ci)\u2212 PE (Ci)\u20162 (From Definition of E)\n(\u03b62)\n\u2264 48c1n \u00b52rd\n 4\u00b5 2r\nn\n\u2225\u2225P Y\u22a5 (U\u03a3) \u2225\u22252 + \u03c32  4\u00b52r + 2 ( \u221a d+ 2d 1 4 \u221a log ( \u00b52r\nc2\n))2   \n(\u03b63)\n\u2264 48c1n \u00b52rd\n( 4\u00b52r\nn\n\u2225\u2225P Y\u22a5 (U\u03a3) \u2225\u22252 + \u03c32 ( 4\u00b52r + 2 ( 2d+ 8d 1 2 log ( \u00b52r\nc2\n))))\n\u2264 48c1n \u00b52rd\n( 4\u00b52r\nn\n\u2225\u2225P Y\u22a5 (U\u03a3) \u2225\u22252 + \u03c32 ( 4\u00b52r + 4d+ 16d 1 2 log ( \u00b52r\nc2\n)))\n(\u03b64)\n\u2264 48c1n \u00b52rd\n( 4\u00b52r\nn\n\u2225\u2225P Y\u22a5 (U\u03a3) \u2225\u22252 + \u03c32 ( 4\u00b52r + 4d+ 16d 1 2 log(\u00b52r) + 16d 1 2 log(1536)\n))\n(\u03b65)\n\u2264 48c1n \u00b52rd\n( 4\u00b52r\nn\n\u2225\u2225P Y\u22a5 (U\u03a3) \u2225\u22252 + \u03c32 ( 24\u00b52rd+ 120d\n))\n\u2264 \u2225\u2225P Y\u22a5 (M) \u2225\u22252\n32 +\n48c1n\n\u00b52rd \u03c32\n( 144\u00b52rd ) (From Lemma 5)\n\u2264 \u2225\u2225P Y\u22a5 (M) \u2225\u22252\n32 + 0.563\u03c32n\nwhere (\u03b61) and (\u03b63) follow from (a+b) 2 \u2264 2a2+2b2, \u03b62 follows from (18). (\u03b64) and (\u03b65) follow from log(x) \u2264 x\nand \u221a x \u2264 x for all x \u2265 1.\nWe now bound Term 2 as:\n\u2211\nj\u2208Os\n\u2225\u2225P Y\u22a5 (PU (Cj)) \u2225\u22252 (\u03b66)\u2264 2 \u2211\nj\u2208Os\n\u2225\u2225P Y\u22a5 (PE (Cj)) \u2225\u22252 + \u2225\u2225P Y\u22a5 ((PU (Cj)\u2212 PE (Cj))) \u2225\u22252\n\u2264 2 \u2211\ni\u2208Os\n4\u00b52r\nn\n\u2225\u2225P Y\u22a5 (U\u03a3) \u2225\u22252 + \u2016PU (Cj)\u2212 PE (Cj)\u20162 (Definition of E)\n(\u03b67) \u2264 2\u03b1n ( 4\u00b52r\nn\n\u2225\u2225P Y\u22a5 (U\u03a3) \u2225\u22252 + 2\u03c32 ( 4\u00b52r + 8r log ( \u00b52r2d\nc1\n)))\n(\u03b68) \u2264 \u2225\u2225P Y\u22a5 (M) \u2225\u22252\n32 + 4\u03b1n\u03c32\n( 4\u00b52r + 8r log(\u00b52) + 16r log(r) + 8r log(d) + 8r log(12288) )\n(\u03b69) \u2264 \u2225\u2225P Y\u22a5 (M) \u2225\u22252\n32 + 4\u03b1n\u03c32\n( 12\u00b52r + 24r log(d) + 8r log(12288) ) (\u03b610) \u2264\n\u2225\u2225P Y\u22a5 (M) \u2225\u22252\n32 + 0.435\u03c32n log(d),\nwhere (\u03b66) and (\u03b67) follow from (a+ b) 2 \u2264 2a2+2b2 along with (18). (\u03b68) follows from Lemma 5, (\u03b69) follows from the fact that r \u2264 d and log(x) \u2264 x and (\u03b610) follows from assuming log(d) \u2265 1 and \u00b5 \u2265 1. From our bounds on Term 1 and Term 2 and (20), we have:\n\u2225\u2225\u2225PU1:r\u22a5 (PU (M)) \u2225\u2225\u2225 \u2264 \u2225\u2225P Y\u22a5 (PU (M)) \u2225\u2225 \u2264 4 3 \u03c3 \u221a n log(d).\nTheorem now follows by using the above observation with (19): \u2225\u2225\u2225PU1:r\u22a5 (L\u2217) \u2225\u2225\u2225 \u2264 6 4 \u03c3 \u221a n+ 5 3 \u03c3 \u221a n log(d) \u2264 4\u03c3 \u221a n log(d)."}, {"heading": "D TORP-BIN", "text": "In this section, we propose an improvement to Algorithm 2 which uses binary search instead of a linear scan in the outer iteration. This improves the running time on Algorithm 2 by almost a factor of r."}, {"heading": "D.1 Algorithm", "text": "In this section, we present our algorithm (See Algorithm 4) for OR-PCAN which improves the running time of Algorithm 2 by almost a factor of r. The main insight is that inner iteration of Algorithm 2 is independent of the value of k in the outer iteration save for the rank of the projection. In Algorithm 4, we use binary search on k instead of a linear scan which reduces the number of outer iterations from O (r) to O (log r).\nAlgorithm 4 Binary search based TORP (TORP-BIN)\n1: Input: Corrupted matrix M\u2217 \u2208 Rd\u00d7n, Target rank r, Expressivity parameter \u03b7, Threshold fraction \u03c1, Number of inner iterations T 2: minK \u2190 1, maxK \u2190 r 3: while minK \u2264 maxK do 4: k \u2190 \u230aminK+maxK2 \u230b 5: C(0) \u2190 0, \u03c4 \u2190 false 6: for t = 0 to t = T do 7: [U (t),\u03a3(t), V (t)] \u2190 SVDk ( M\u2217 \u2212 C(t) ) , L(t) \u2190 U (t)\u03a3(t)(V (t))\u22a4 } Projection onto space of\nlow rank matrices\n8: E \u2190 (\u03a3(t))\u22121(U (t))\u22a4M\u2217 /* Compute Incoherence */ 9: R \u2190 (I \u2212 U (t)(U (t))\u22a4)M\u2217 /* Compute residual */\n   Projection onto space of column sparse matrices\n10: CS(t+1) \u2190 HT 2\u03c1 (M\u2217, E) \u222aHT \u03c1 (M\u2217, R) 11: C(t+1) \u2190 M\u2217CS(t+1) 12: nthres \u2190 |{i : \u2016Ei\u2016 \u2265 \u03b7}| /* Compute high incoherence points */ 13: \u03c4 \u2190 \u03c4 \u2228 (nthres \u2265 2\u03c1n) /* Check termination conditions */ 14: end for 15: if \u03c4 then 16: maxK \u2190 k \u2212 1 17: else 18: minK \u2190 k + 1 19: [U,\u03a3, V ] \u2190 SVDk ( M\u2217 \u2212 C(T+1) ) 20: end if 21: end while 22: Return: U"}, {"heading": "D.2 Analysis", "text": "In this section, we will state and prove a theoretical guarantee for Algorithm 4.\nTheorem 7. Assume the conditions of Theorem 2. Furthermore, suppose that \u2016N\u2217\u2016F \u2264 \u03c3k(L \u2217) 16 for some\nk. Then, for all \u03b1 \u2264 1128\u00b52r , Algorithm 4 when run with parameters \u03c1 = 1128\u00b52r , \u03b7 = 2\u00b5 \u221a r n and T = log 20n\u03c31(M)\u01eb , returns a subspace U which satisfies:\n\u2225\u2225(I \u2212 UU\u22a4)L\u2217 \u2225\u2225 F \u2264 3 \u2225\u2225(I \u2212 U\u22171:k(U\u22171:k)\u22a4)L\u2217 \u2225\u2225 F + 9 \u2016N\u2217\u2016F + \u01eb\n10n .\nProof. We will begin by bounding the running time of the algorithm. Note that because of the binary search, the algorithm will run for at most O (log r) outer iterations.\nLet t denote the number of outer iterations of the algorithm. Let the value of k(maxK, minK) in iteration t be denoted by k(t) (maxK(t), minK(t)). We will first prove the claim that at any point in the running of the algorithm, maxK \u2265 k. We will prove the claim via induction on the number of iterations t:\nBase Case: t = 0 The base case is trivially true as maxK = r.\nInduction Step: t = l + 1 Assume that the claim remains true at iteration t = l. In the (l + 1)th iteration, we assume two cases:\nCase 1: The inner iteration finishes with \u03c4 = false. In this case, maxK is not updated. So, the claim remains true for t = (l + 1)\nCase 2: The inner iteration finishes with \u03c4 = true. In this case, k(t) > k (From Lemma 10 and the termination condition of the inner iteration.). In this iteration, maxK is updated to k(t) \u2212 1 \u2265 k. Thus, the claim remains true.\nTherefore, at termination of the algorithm, we have maxK \u2265 k. Suppose that the algorithm terminated after iteration T . Note that minK(t) \u2264 k \u2264 maxK(t)\u22000 \u2264 t \u2264 T . Therefore, we have minK(T+1) = maxK(T+1) + 1. For this to happen, the inner iteration must have run with k(T\n\u2032) = maxK(T+1) with \u03c4 = false for some iteration T \u2032 and also that this is the last such successful iteration as minK is not updated after iteration T \u2032. Therefore, the algorithm returns the subspace corresponding to k(T\n\u2032) = maxK(T+1) \u2265 k. Since the inner iteration is successful for iteration T \u2032, the Theorem is true from the application of Lemma 3 and noting that kT \u2032 \u2265 k."}, {"heading": "E Fast Projection Operator", "text": "In this section, we will describe a fast algorithm to compute the projection operator onto the ellipsoid in Algorithm 2. Formally, we are provided an orthogonal basis U \u2208 Rd\u00d7r, a positive diagonal matrix \u03a3, a bound b and a vector x. Let E = {y : y = U\u03a3z for some \u2016z\u2016 \u2264 b}. The goal is to compute the projection of the vector x onto the set E ."}, {"heading": "E.1 Algorithm", "text": "In this section, we present our algorithm (Algorithm 5) to compute the projection onto the set E . We show that the projection operation boils down to an univariate optimization problem on a monotone function. We then perform binary search on an interval in which the solution is guaranteed to lie."}, {"heading": "E.2 Analysis", "text": "Theorem 8. Let U \u2208 Rd\u00d7r be an orthonormal matrix and \u03a3 \u2208 Rr\u00d7r be a positive diagonal matrix. Then, for any b \u2265 0, x \u2208 Rd and \u01eb, the vector w returned by Algorithm 5 satisfies:\n\u2016w \u2212 PE(x)\u2016 \u2264 \u01eb\nwhere E := {y : y = U\u03a3z for some \u2016z\u2016 \u2264 b}\nProof. We first define the convex optimization problem corresponding to the projection operator PE . Then, we have:\nPE(x) = argmin y \u2016x\u2212 y\u2016 s.t y \u2208 E\nSince, y \u2208 E , a solution to the above optimization problem is equivalent to:\nAlgorithm 5 w = FAST-PR(U,\u03a3, b, x, \u01eb)\n1: Input: Orthogonal Basis U \u2208 Rd\u00d7r, Positive Diagonal Matrix \u03a3, Bound b, Projection Vector x, Accuracy Parameter \u01eb 2: \u03c3min = min i\u2208[r] (\u03a3i,i), \u03c3max = max i\u2208[r] (\u03a3i,i) 3: y \u2190 \u03a3U\u22a4x 4: \u03bbmin = 0, \u03bbmax = \u2016y\u2016 b\n5: T \u2190 log ( \u03bbmax \u221a r\u2016x\u2016\n\u03c32min\u01eb\n)\n6: for Iteration t = 0 to t = T do 7: \u03bb(t) \u2190 \u03bbmin+\u03bbmax2 8: z(t) \u2190 (\u03bbI +\u03a32)\u22121y 9: if \u2225\u2225z(t) \u2225\u2225 \u2264 b then\n10: \u03bbmax \u2190 \u03bb(t) 11: else 12: \u03bbmin \u2190 \u03bb(t) 13: end if 14: end for 15: Return: U\u03a3z(T )\nPE(x) = argmin z \u2016x\u2212 U\u03a3z\u20162 s.t \u2016z\u20162 \u2264 b2 (21)\nNote that both the constraint and the objective function are convex. Therefore, we can introduce a KKT multiplier \u03bb \u2265 0 and writing down the stationarity conditions of 21, we get:\n2\u03a32z + 2\u03bbz = 2\u03a3U\u22a4x =\u21d2 z = ( \u03a32 + \u03bbI )\u22121 \u03a3U\u22a4x\nNow, we just need to ensure that \u2225\u2225\u2225 ( \u03a32 + \u03bbI )\u22121 \u03a3U\u22a4x \u2225\u2225\u2225 \u2264 b. Let f(\u03bb) = \u2225\u2225\u2225 ( \u03a32 + \u03bbI )\u22121 \u03a3U\u22a4x \u2225\u2225\u2225 for \u03bb \u2265 0. Let \u03bb\u2217 be the solution to f(\u03bb) = min( \u2225\u2225\u03a3\u22121U\u22a4x \u2225\u2225 , b). We will first prove that at any point in the running of the algorithm \u03bbmax \u2265 \u03bb\u2217 and \u03bbmin \u2264 \u03bb\u2217. We prove the claim by induction on the number of iterations t:\nBase Case t = 0: Since \u03bbmin = 0, the lower bound holds trivially. That \u03bbmax \u2265 \u03bb\u2217 can be proved as follows:\nf(\u03bbmax) = \u2225\u2225\u2225 ( \u03a32 + \u03bbmaxI )\u22121 \u03a3U\u22a4x \u2225\u2225\u2225 \u2264 min ( \u2225\u2225\u03a3\u22121U\u22a4x \u2225\u2225 , \u2225\u2225\u03a3U\u22a4x \u2225\u2225 \u03bbmax ) \u2264 min (\u2225\u2225\u03a3\u22121U\u22a4x \u2225\u2225 , b ) .\nSince, f is a monotonically decreasing function, the claim holds true in the base case.\nInduction Step t = (k + 1): Assume that the claim holds till t = k. We have two cases for iteration k + 1:\nCase 1: \u03bbmax \u2190 \u03bb(t+1). In this case, \u03bbmin \u2264 \u03bb\u2217 still holds from the inductive hypothesis. For \u03bbmax, we have:\nf(\u03bb(t+1)) = \u2225\u2225\u2225\u2225 ( \u03a32 + \u03bb(t+1)I )\u22121 \u03a3U\u22a4x \u2225\u2225\u2225\u2225 \u2264 min (\u2225\u2225\u03a3\u22121U\u22a4x \u2225\u2225 , f(\u03bb(t+1)) ) \u2264 min (\u2225\u2225\u03a3\u22121U\u22a4x \u2225\u2225 , b ) ,\nwhere the last inequality holds from the fact that \u03bbmax was updated in this iteration. From the monotonicity of f , the induction hypothesis holds in this iteration.\nCase 2: \u03bbmin \u2190 \u03bb(t+1). In this case, \u03bbmax \u2265 \u03bb\u2217 by the inductive hypothesis. In this case, we have: f(\u03bb(t+1)) \u2265 b \u2265 f(\u03bb\u2217). From the monotonicity of f , the induction hypothesis holds in this iteration.\nNote that (\u03bbmax \u2212 \u03bbmin) is halved at each iteration. Therefore, at the termination of the algorithm, we have (\u03bbmax \u2212 \u03bbmin) \u2264 \u03c3 2 min\u01eb\u221a r\u2016x\u2016 . From our claim, this implies that \u2223\u2223\u03bb\u2217 \u2212 \u03bb(T ) \u2223\u2223 \u2264 \u03c3 2 min\u01eb\u221a r\u2016x\u2016 . Note that we can\nwrite PE(x) = U\u03a32(\u03bb\u2217I +\u03a32)\u22121U\u22a4x. Note that \u2016PE(x) \u2212 w\u2016 = \u2225\u2225U\u22a4(PE(x) \u2212 w) \u2225\u2225. We will now bound the element-wise difference between U\u22a4PE(x) and U\u22a4w:\n\u2223\u2223\u2223e\u22a4i \u03a32((\u03bb\u2217I +\u03a32)\u22121 \u2212 (\u03bb(T )I +\u03a32)\u22121)U\u22a4x \u2223\u2223\u2223 \u2264 \u2016x\u2016 \u2223\u2223\u2223\u2223\u03c3 2 i ( 1\n\u03bb\u2217 + \u03c32i \u2212 1 \u03bb(T ) + \u03c32i\n)\u2223\u2223\u2223\u2223\n\u2264 \u2016x\u2016 \u03c32i \u2223\u2223\u2223\u2223\n\u03bb(T ) \u2212 \u03bb\u2217 (\u03bb\u2217 + \u03c32i )(\u03bb (T ) + \u03c32i )\n\u2223\u2223\u2223\u2223 \u2264 \u2016x\u2016 \u2223\u2223\u2223\u2223 \u03bb(T ) \u2212 \u03bb\u2217\n\u03c32i\n\u2223\u2223\u2223\u2223 \u2264 \u01eb\u221a r .\nBy applying the element-wise bound to \u03a32((\u03bb\u2217I +\u03a32)\u22121 \u2212 (\u03bb(T )I +\u03a32)\u22121)U\u22a4x, we have:\n\u2016PE(x)\u2212 w\u2016 \u2264 \u221a r\n\u01eb\u221a r \u2264 \u01eb."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We consider the problem of outlier robust PCA (OR-PCA) where the goal is to recover principal<lb>directions despite the presence of outlier data points. That is, given a data matrix M\u2217, where (1 \u2212 \u03b1)<lb>fraction of the points are noisy samples from a low-dimensional subspace while \u03b1 fraction of the points can<lb>be arbitrary outliers, the goal is to recover the subspace accurately. Existing results for OR-PCA have<lb>serious drawbacks: while some results are quite weak in the presence of noise, other results have runtime<lb>quadratic in dimension, rendering them impractical for large scale applications.<lb>In this work, we provide a novel thresholding based iterative algorithm with per-iteration complexity<lb>at most linear in the data size. Moreover, the fraction of outliers, \u03b1, that our method can handle is tight<lb>up to constants while providing nearly optimal computational complexity for a general noise setting. For<lb>the special case where the inliers are obtained from a low-dimensional subspace with additive Gaussian<lb>noise, we show that a modification of our thresholding based method leads to significant improvement<lb>in recovery error (of the subspace) even in the presence of a large fraction of outliers.", "creator": "LaTeX with hyperref package"}}}