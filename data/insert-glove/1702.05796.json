{"id": "1702.05796", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2017", "title": "Collaborative Deep Reinforcement Learning", "abstract": "demir Besides independent learning, slovensko human learning process palace is gullane highly improved netware by apprentices summarizing what teerth has ibibio been learned, communicating danys it with krensavage peers, nonfactor and mannerisms subsequently fusing harlon knowledge from different lennikov sources flanker to covariation assist the current smirky learning wenzao goal. This collaborative zax learning exonerates procedure karnaphuli ensures ti that toutle the scanlan knowledge is shared, continuously 2,926 refined, comal and ecer concluded hogwood from different perspectives to thevenet construct postlude a more shawm profound understanding. The ashden idea of knowledge rensing transfer has led alyx to 107.07 many elective advances heimburg in machine learning aicraft and data elya mining, but significant barbaresco challenges 2,200-square remain, bloomington especially al-ashraf when it comes to reinforcement learning, astemirov heterogeneous model structures, and different margaritis learning wcw tasks. bli Motivated sub-county by human 2236 collaborative satsu learning, 109.39 in this paper psychiatrie we annik propose a collaborative non-ideological deep reinforcement khitan learning (CDRL) belfort framework patmore that steadily performs doa adaptive 8.2770 knowledge intubate transfer wich among shoup heterogeneous chechi learning pondimin agents. questionable Specifically, ust-kamenogorsk the amia proposed CDRL 210.9 conducts mikulas a ptdc novel deep sumps knowledge sevel distillation method darryn to address the heterogeneity among grimwood different millz learning tasks with a treleaven deep a-20 alignment vermessung network. 105.38 Furthermore, pruszk\u00f3w we tsui present ciclo an phrasebook efficient brainard collaborative Asynchronous Advantage staat Actor - Critic (minehunters cA3C) algorithm manilow to ritzau incorporate hubbells deep knowledge distillation roves into the online levin training of agents, sdrc and demonstrate baccalaureus the effectiveness of the CDRL framework mccurdy using havasu extensive homophonous empirical sardar evaluation on smooth-bore OpenAI six-dimensional gym.", "histories": [["v1", "Sun, 19 Feb 2017 21:13:45 GMT  (1606kb,D)", "http://arxiv.org/abs/1702.05796v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kaixiang lin", "shu wang", "jiayu zhou"], "accepted": false, "id": "1702.05796"}, "pdf": {"name": "1702.05796.pdf", "metadata": {"source": "META", "title": "Collaborative Deep Reinforcement Learning", "authors": ["Kaixiang Lin", "Shu Wang", "Jiayu Zhou"], "emails": ["linkaixi@msu.edu", "sw498@cs.rutgers.edu", "jiayuz@msu.edu"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022Computingmethodologies \u2192Machine learning; Reinforcement learning; Transfer learning;\nKEYWORDS Knowledge distillation; Transfer learning; Deep reinforcement learning ACM Reference format: Kaixiang Lin, Shu Wang, and Jiayu Zhou. 1997. Collaborative Deep Reinforcement Learning. In Proceedings of ACM Woodstock conference, El Paso, Texas USA, July 1997 (WOODSTOCK\u201997), 9 pages. DOI: 10.475/123 4"}, {"heading": "1 INTRODUCTION", "text": "It is the development of cognitive abilities including learning, remembering, communicating that enables human to conduct social\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WOODSTOCK\u201997, El Paso, Texas USA \u00a9 2016 Copyright held by the owner/author(s). 123-4567-24-567/08/06. . .$15.00 DOI: 10.475/123 4\ncooperation, which is the key to the rise of humankind. As a social animal, the ability to collaborate awoke the cognitive revolution and reveals the prosperous history of human [14]. In disciplines of cognitive science, education and psychology, collaborative learning, a situation in which a group of people learn to achieve a set of tasks together, has been advocated throughout previous studies [9]. It is intuitive to illustrate the concept of collaborative learning in the example of group study. A group of students are studying together to master some challenging course materials. As each student may understand the materials from a distinctive perspective, e ective communication would greatly help the entire group achieve a better understanding than those from independent study, and could signi cantly improve the e ciency and e ectiveness of learning process, as well [12].\nOn the other hand, the study of human learning has largely advanced the design of machine learning and data mining algorithms, especially in reinforcement learning and transfer learning. e recent success of deep reinforcement learning (DRL) has a racted increasing a ention from the community, as DRL can discover very competitive strategies by having learning agents interacting with a given environment and using rewards from the environment as the supervision (e.g., [16, 18, 20, 28]). Even though most of current research on DRL has focused on learning from games, it possesses great transformative power to impact many industries with data mining and machine learning techniques such as clinical decision support [32], marketing [2], nance [1], visual navigation [37], and autonomous driving [8]. Although there are many existing e orts towards e ective algorithms for DRL [19, 21], the computational cost still imposes signi cant challenges as training DRL for even a\nar X\niv :1\n70 2.\n05 79\n6v 1\n[ cs\n.L G\n] 1\n9 Fe\nb 20\n17\nsimple game such as Pong [5] remains very expensive. e underlying reasons for the obstacle of e cient training mainly lie in two aspects: First, the supervision (rewards) from the environment is very sparse and implicit during training. It may take an agent hundreds or even thousands actions to get a single reward, and which actions that actually lead to this reward are ambiguous. Besides the insu cient supervision, training deep neural network itself takes lots of computational resources.\nDue to the aforementioned di culties, performing knowledge transfer from other related tasks or well-trained deep models to facilitate training has drawn lots of a ention in the community [16, 24\u2013 26, 31]. Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35]. Model transfer methods implement knowledge transfer from introducing inductive bias during the learning, and has been extensively studied in both transfer learning/multi-task learning (MTL) community and deep learning community. For example, in the regularized MTL models such as [11, 36], tasks with the same feature space are related through some structured regularization. Another example is the multi-task deep neural network, where di erent tasks share parts of the network structures [35]. One obvious disadvantage of model transfer is the lack of exibility: usually the feasibility of inductive transfer has largely restricted the model structure of learning task, which makes it not practical in DRL because for di erent tasks the optimal model structures may be radically di erent. On the other hand, the recently developed data transfer (also known as knowledge distillation or mimic learning) [15, 24, 26] embeds the source model knowledge into data points. en they are used as knowledge bridge to train target models, which can have di erent structures as compared to the source model [6, 15]. Because of the structural exibility, the data transfer is especially suitable to deal with structure variant models.\nere are two situations that transfer learning methods are essential in DRL: Certi cated heterogeneous transfer. Training a DRL agent is computational expensive. If we have a well-trained model, it will be bene cial to assist the learning of other tasks by transferring knowledge from this model. erefore we consider following research question: Given one certi cated task (i.e. the model is well-designed, extensively trained and performs very well), how can we maximize the information that can be used in the training of other related tasks? Some model transfer approaches directly use the weights from the trained model to initialize the new task [24], which can only be done when the model structures are the same. us, this strict requirement has largely limited its general applicability on DRL. On the other hand, the initialization may not work well if the tasks are signi cantly di erent from each other in nature [24]. is challenge could be partially solved by generating an intermediate dataset (logits) from the existing model to help learning the new task. However, new problems would arise when we are transferring knowledge between heterogeneous tasks. Not only the action spaces are di erent in dimension, the intrinsic action probability distributions and semantic meanings of two tasks could di er a lot. Speci cally, one action in Pong may refer to move the paddle upwards while the same action index in Riverraid [5] would correspond to re. erefore, the distilled dataset generated from\nthe trained source task cannot be directly used to train the heterogeneous target task. In this scenario, the rst key challenge we identi ed in this work is that how to conduct data transfer among heterogeneous tasks so that we can maximally utilize the information from a certi cated model while still maintain the exibility of model design for new tasks. During the transfer, the transferred knowledge from other tasks may contradict to the knowledge that agents learned from its environment. One recently work [25] use an a ention network selective eliminate transfer if the contradiction presents, which is not suitable in this se ing since we are given a certi cated task to transfer. Hence, the second challenge is how to resolve the con ict and perform a meaningful transfer. Lack of expertise. A more general desired but also more challenging scenario is that DRL agents are trained for multiple heterogeneous tasks without any pre-trained models available. One feasible way to conduct transfer under this scenario is that agents of multiple tasks share part of their network parameters [26, 35]. However, an inevitable drawback is, multiple models lose their task-speci c designs since the shared part needs to be the same. Another solution is to learn a domain invariant feature space shared by all tasks [3]. However, some task-speci c information is o en lost while converting the original state to a new feature subspace. In this case, an intriguing questions is that: can we design a framework that fully utilizes the original environment information and meanwhile leverages the knowledge transferred from other tasks?\nis paper investigates the aforementioned problems systematically and proposes a novel Collaborative Deep Reinforcement Learning (CDRL) framework (illustrated in Figure 1) to resolve them. Our major contribution is threefold:\n\u2022 First, in order to transfer knowledge among heterogeneous tasks while remaining the task-speci c design of model structure, a novel deep knowledge distillation is proposed to address the heterogeneity among tasks, with the utilization of deep alignment network designed for the domain adaptation. \u2022 Second, in order to incorporate the transferred knowledge from heterogeneous tasks into the online training of current learning agents, similar to human collaborative learning, an e cient collaborative asynchronously advantage actor-critic learning (cA3C) algorithm is developed under the CDRL framework. In cA3C, the target agents are able to learn from environments and its peers simultaneously, which also ensure the information from original environment is su ciently utilized. Further, the knowledge con ict among di erent tasks is resolved by adding an extra distillation layer to the policy network under CDRL framework, as well. \u2022 Last but not least we present extensive empirical studies on OpenAI gym to evaluate the proposed CDRL framework and demonstrate its e ectiveness by achieving more than 10% performance improvement compared to the current state-of-theart.\nNotations: In this paper, we use teacher network/source task denotes the network/task contained the knowledge to be transferred to others. Similarly, the student network/target task is referred to those tasks utilizing the knowledge transferred from others to facilitate its own training. e expert network denotes the network that has already reached a relative high averaged reward in its own\nenvironment. In DRL, an agent is represented by a policy network and a value network that share a set of parameters. Homogeneous agents denotes agents that perform and learn under independent copies of same environment. Heterogeneous agents refer to those agents that are trained in di erent environments."}, {"heading": "2 RELATEDWORK", "text": "Multi-agent learning. One closely related area to our work is multi-agent reinforcement learning. A multi-agent system includes a set of agents interacting in one environment. Meanwhile they could potentially interact with each other [7, 13, 17, 30]. In collaborative multi-agent reinforcement learning, agents work together to maximize a shared reward measurement [13, 17]. ere is a clear distinction between the proposed CDRL framework and multi-agent reinforcement learning. In CDRL, each agent interacts with its own environment copy and the goal is to maximize the reward of the target agents. e formal de nition of the proposed framework is given in Section 4.1. Transfer learning. Another relevant research topic is domain adaption in the eld of transfer learning [23, 29, 33]. e authors in [29] proposed a two-stage domain adaptation framework that considers the di erences among marginal probability distributions of domains, as well as conditional probability distributions of tasks. e method rst re-weights the data from the source domain using Maximum Mean Discrepancy and then re-weights the predictive function in the source domain to reduce te di erence on conditional probabilities. In [33], the marginal distributions of the source and the target domain are aligned by training a network, which maps inputs into a domain invariant representation. Also, knowledge distillation was directly utilized to align the source and target class distribution. One clear limitation here is that the source domain and the target domain are required to have the same dimensionality (i.e. number of classes) with same semantics meanings, which is not the case in our deep knowledge distillation.\nIn [3], an invariant feature space is learned to transfer skills between two agents. However, projecting the state into a feature space would lose information contained in the original state. ere is a trade-o between learning the common feature space and preserving the maximum information from the original state. In our work, we use data generated by intermediate outputs in the knowledge transfer instead of a shared space. Our approach thus retains complete information from the environment and ensures high quality transfer. e recently proposed A2T approach [25] can avoid negative transfer among di erent tasks. However, it is possible that some negative transfer cases may because of the inappropriate design of transfer algorithms. In our work, we show that we can perform successful transfer among tasks that seemingly cause negative transfer. Knowledge transfer in deep learning. Since the training of each agent in an environment can be considered as a learning task, and the knowledge transfer among multiple tasks belongs to the study of multi-task learning. e multi-task deep neural network (MTDNN) [35] transfers knowledge among tasks by sharing parameters of several low-level layers. Since the low-level layers can be considered to perform representation learning, the MTDNN is learning a shared representation for inputs, which is then used\nby high-level layers in the network. Di erent learning tasks are related to each other via this shared feature representation. In the proposed CDRL, we do not use the share representation due to the inevitable information loss when we project the inputs into a shared representation. We instead perform explicitly knowledge transfer among tasks by distilling knowledge that are independent of model structures. In [15], the authors proposed to compress cumbersome models (teachers) to more simple models (students), where the simple models are trained by a dataset (knowledge) distilled from the teachers. However, this approach cannot handle the transfer among heterogeneous tasks, which is one key challenge we addressed in this paper. Knowledge transfer in deep reinforcement learning. Knowledge transfer is also studied in deep reinforcement learning. [19] proposed multi-threaded asynchronous variants of several most advanced deep reinforcement learning methods including Sarsa, Q-learning, Q-learning and advantage actor-critic. Among all those methods, asynchronous advantage actor-critic (A3C) achieves the best performance. Instead of using experience replay as in previous work, A3C stabilizes the training procedure by training di erent agents in parallel using di erent exploration strategies. is was shown to converge much faster than previous methods and use less computational resources. We show in Section 4.1 that the A3C is subsumed to the proposed CDRL as a special case. In [24], a single multi-task policy network is trained by utilizing a set of expert Deep Q-Network (DQN) of source games. At this stage, the goal is to obtain a policy network that can play source games as close to experts as possible. e second step is to transfer the knowledge from source tasks to a new but related target task. e knowledge is transferred by using the DQN in last step as the initialization of the DQN for the new task. As such, the training time of the new task can be signi cantly reduced. Di erent from their approach, the proposed transfer strategy is not to directly mimic experts\u2019 actions or initialize by a pre-trained model. In [26], knowledge distillation was adopted to train a multi-task model that outperforms single task models of some tasks. e experts for all tasks are rstly acquired by single task learning. e intermediate outputs from each expert are then distilled to a similar multi-task network with an extra controller layer to coordinate di erent action sets. One clear limitation is that major components of the model are exactly the same for di erent tasks, which may lead to degraded performance on some tasks. In our work, transfer can happen even when there are no experts available. Also, our method allow each task to have their own model structures. Furthermore, even the model structures are the same for multiple tasks, the tasks are not trained to improve the performance of other tasks (i.e. it does not mimic experts from other tasks directly). erefore our model can focus on maximizing its own reward, instead of being distracted by others."}, {"heading": "3 BACKGROUND", "text": ""}, {"heading": "3.1 Reinforcement Learning", "text": "In this work, we consider the standard reinforcement learning se ing where each agent interacts with it\u2019s own environment over a number of discrete time steps. Given the current state st \u2208 S at step t , agent \u0434i selects an action at \u2208 A according\nto its policy \u03c0 (at |st ), and receives a reward rt+1 from the environment. e goal of the agent is to choose an action at at step t that maximize the sum of future rewards {rt } in a decaying manner: Rt = \u2211\u221e i=0 \u03b3\nirt+i , where scalar \u03b3 \u2208 (0, 1] is a discount rate. Based on the policy \u03c0 of this agent, we can further de ne a state value function V (st ) = E[Rt |s = st ], which estimates the expected discounted return starting from state st , taking actions following policy \u03c0 until the game ends. e goal in reinforcement learning algorithm is to maximize the expected return. Since we are mainly discussing one speci c agent\u2019s design and behavior throughout the paper, we leave out the notation of the agent index for conciseness."}, {"heading": "3.2 Asynchronous Advantage actor-critic algorithm (A3C)", "text": "e asynchronous advantage actor-critic (A3C) algorithm [19] launches multiple agents in parallel and asynchronously updates a global shared target policy network \u03c0 (a |s,\u03b8p ) as well as a value network V (s,\u03b8v ). parametrized by \u03b8p and \u03b8v , respectively. Each agent interacts with the environment, independently. At each step t the agent takes an action based on the probability distribution generated by policy network. A er playing a n-step rollout or reaching the terminal state, the rewards are used to compute the advantage with the output of value function. e updates of policy network is conducted by applying the gradient:\n\u2207\u03b8p log\u03c0 (at |st ;\u03b8p )A(st ,at ;\u03b8v ),\nwhere the advantage function A(st ,at ;\u03b8v ) is given by:\u2211T\u2212t\u22121 i=0 \u03b3 irt+i + \u03b3 T\u2212tV (sT ;\u03b8v ) \u2212V (st ;\u03b8v ).\nTerm T represents the step number for the last step of this rollout, it is either the max number of rollout steps or the number of steps from t to the terminal state. e update of value network is to minimize the squared di erence between the environment rewards and value function outputs, i.e.,\nmin \u03b8v ( \u2211T\u2212t\u22121 i=0 \u03b3 irt+i + \u03b3 T\u2212tV (sT ;\u03b8v ) \u2212V (st ;\u03b8v ))2.\ne policy network and the value network share the same layers except for the last output layer. An entropy regularization of policy \u03c0 is added to improve exploration, as well."}, {"heading": "3.3 Knowledge distillation", "text": "Knowledge distillation [15] is a transfer learning approach that distills the knowledge from a teacher network to a student network using a temperature parameterized \u201dso targets\u201d (i.e. a probability distribution over a set of classes). It has been shown that it can accelerate the training with less data since the gradient from \u201dso targets\u201d contains much more information than the gradient obtained from \u201dhard targets\u201d (e.g. 0, 1 supervision).\nTo be more speci c, logits vector z \u2208 Rd for d actions can be converted to a probability distribution h \u2208 (0, 1)d by a so max function, raised with temperature \u03c4 :\nh(i) = so max(z/\u03c4 )i = exp(z(i)/\u03c4 )\u2211 j exp(z(j)/\u03c4 ) , (1)\nwhere h(i) and z(i) denotes the i-th entry of h and z, respectively.\nen the knowledge distillation can be completed by optimize the following Kullback-Leibler divergence (KL) with temperature \u03c4 [15, 26].\nLKL(D,\u03b8 \u03b2 p ) = \u2211 t=1 so max(z\u03b1t /\u03c4 ) ln so max(z\u03b1t /\u03c4 ) so max(z\u03b2t ) (2)\nwhere z\u03b1t is the logits vector from teacher network (notation \u03b1 represents teacher) at step t , while z\u03b2t is the logits vector from student network (notation \u03b2 represents student) of this step. \u03b8 \u03b2p denotes the parameters of the student policy network. D is a set of logits from teacher network."}, {"heading": "4 COLLABORATIVE DEEP REINFORCEMENT LEARNING FRAMEWORK", "text": "In this section, we introduce the proposed collaborative deep reinforcement learning (CDRL) framework. Under this framework, a collaborative Asynchronous Advantage Actor-Critic (cA3C) algorithm is proposed to con rm the e ectiveness of the collaborative approach. Before we introduce our method in details, one underlying assumption we used is as follows:\nAssumption 1. If there is a universe that contains all the tasks E = {e1, e2, ..., e\u221e} and ki represents the corresponding knowledge to master each task ei , then \u2200i, j,ki \u2229 kj , \u2205.\nis is a formal description of our common sense that any pair of tasks are not absolutely isolated from each other, which has been implicitly used as a fundamental assumption by most prior transfer learning studies [11, 24, 26]. erefore, we focus on mining the shared knowledge across multiple tasks instead of providing strategy selecting tasks that share knowledge as much as possible, which remains to be unsolved and may lead to our future work. e goal here is to utilize the existing knowledge as well as possible. For example, we may only have a well-trained expert on playing Pong game, and we want to utilize its expertise to help us perform be er on other games. is is one of the situations that can be solved by our collaborative deep reinforcement learning framework."}, {"heading": "4.1 Collaborative deep reinforcement learning", "text": "In deep reinforcement learning, since the training of agents are computational expensive, the well-trained agents should be further utilized as source agents (agents where we transferred knowledge from) to facilitate the training of target agents (agents that are provided with the extra knowledge from source). In order to incorporate this type of collaboration to the training of DRL agents, we formally de ne the collaborative deep reinforcement learning (CDRL) framework as follows:\nDe nition 4.1. Givenm independent environments {\u03b51, \u03b52, ..., \u03b5m } ofm tasks {e1, e2, ..., em } , the correspondingm agents {\u04341,\u04342, ...,\u0434m } are collaboratively trained in parallel to maximize the rewards (master each task) with respect to target agents.\n\u2022 Environments. ere is no restriction on the environments: e m environments can be totally di erent or with some duplications.\n\u2022 In parallel. Each environment \u03b5i only interacts with the one corresponding agent \u0434i , i.e., the action a j t from agent \u0434j at step\nt has no in uence on the state sit+1 in \u03b5i ,\u2200i , j. \u2022 Collaboratively. e training procedure of agent \u0434i consists of in-\nteracting with environment \u03b5i and interacting with other agents as well. e agent \u0434i is not necessary to be at same level as \u201dcollaborative\u201d de ned in cognitive science [9]. E.g., \u04341 can be an expert for task e1 (environment \u03b51) while he is helping agent \u04342 which is a student agent in task e2. \u2022 Target agents. e goal of CDRL can be set as maximizing the\nrewards that agent \u0434i obtains in environment \u03b5i with the help of interacting with other agents, similar to inductive transfer learning where \u0434i is the target agent for target task and others are source tasks. e knowledge is transfered from source to target \u0434i by interaction. When we set the goal to maximize the rewards of multiple agents jointly, it is similar to multi-task learning where all tasks are source tasks and target tasks at the same time.\nNotice that our de nition is very di erent from the previously de ned collaborative multiagent Markow Decision Process (collaborative multiagent MDP) [13, 17] where a set of agents select a global joint action to maximize the sum of their individual rewards and the environment is transi ed to a new state based on that joint action. First, MDP is not a requirement in CDRL framework. Second, in CDRL, each agent has its own copy of environment and maximizes its own cumulative rewards. e goal of collaboration is to improve the performance of collaborative agents, compared with isolated ones, which is di erent from maximizing the sum of global rewards in collaborative multiagent MDP. ird, CDRL focuses on how agents collaborate among heterogeneous environments, instead of how joint action a ects the rewards. In CDRL, di erent agents are acting in parallel, the actions taken by other agents won\u2019t directly in uence current agent\u2019s rewards. While in collaborative multiagent MDP, the agents must coordinate their action choices since the rewards will be directly a ected by the action choices of other agents.\nFurthermore, CDRL includes di erent types of interaction, which makes this a general framework. For example, the current state-ofthe-art is A3C [19] can be categorized as one homogeneous CDRL method with advantage actor-critic interaction. Speci cally, multiple agents in A3C are trained in parallel with the same environment. All agents rst synchronize parameters from a global network, and then update the global network with their individual gradients. is procedure can be seen as each agent maintains its own model (a\ndi erent version of global network) and interacts with other agents by sending and receiving gradients.\nIn this paper, we propose a novel interaction method named deep knowledge distillation under the CDRL framework. It is worth noting that the interaction in A3C only deals with the homogeneous tasks, i.e. all agents have the same environment and the same model structure so that their gradients can be accumulated and interacted. By deep knowledge distillation, the interaction can be conducted among heterogeneous tasks."}, {"heading": "4.2 Deep knowledge distillation", "text": "As we introduced before, knowledge distillation [15] is trying to train a student network that can behave similarly to the teacher network by utilizing the logits from the teacher as supervision. However, transferring the knowledge among heterogeneous tasks faces several di culties. First, the action spaces of di erent tasks may have di erent dimensions. Second, even if the dimensionality of action space is same among tasks, the action probability distributions for di erent tasks could vary a lot, as we illustrated in Figure 5 (a) and (b). us, the action pa erns represented by the logits of di erent policy networks are usually di erent from task to task. If we directly force a student network to mimic the action pa ern of a teacher network for a di erent task, it could be trained in a wrong direction, and nally ends up with worse performance than isolated training. In fact, this suspect has been empirically veri ed in our experiments.\nBased on the above observation, we propose deep knowledge distillation to transfer knowledge between heterogeneous tasks. As illustrated in Figure 2 (a), the approach for deep knowledge distillation is straightforward. We use a deep alignment network to map the logits of the teacher network from a heterogeneous source task e\u03b1 (environment \u03b5\u03b1 ), then the logits is used as our supervision to update the student network of target task e\u03b2 (environment \u03b5\u03b2 ). is procedure is performed by minimizing following objective function over student policy network parameters \u03b8 \u03b2p \u2032 :\nLKL(D,\u03b8 \u03b2 p \u2032 ,\u03c4 ) = \u2211 t lKL(F\u03b8\u03c9 (z\u03b1t ), z \u03b2 t \u2032 ,\u03c4 ), (3)\nwhere\nlKL(F\u03b8\u03c9 (z\u03b1t ), z \u03b2 t \u2032 ,\u03c4 ) = so max(F\u03b8\u03c9 (z\u03b1t )/\u03c4 ) ln\nso max(F\u03b8\u03c9 (z\u03b1t )/\u03c4 )\nso max(z\u03b2t \u2032 )\n.\nHere \u03b8\u03c9 denotes the parameters of the deep alignment network, which transfers the logits z\u03b1t from the teacher policy network for\nknowledge distillation by function F\u03b8\u03c9 (z\u03b1t ) at step t . As we show in Figure 2 (b), \u03b8 \u03b2p is the student policy network parameters (including parameters of CNN, LSTM and policy layer) for task e\u03b2 , while \u03b8 \u03b2p \u2032 denotes student network parameters of CNN, LSTM and distillation layer. It is clear that the distillation logits z\u03b2t \u2032 from the student network does not determine the action probability distribution directly, which is established by the policy logits z\u03b2t , as illustrated in Figure 2 (b). We add another fully connected distillation layer to deal with the mismatch of action space dimensionality and the contradiction of the transferred knowledge from source domain and the learned knowledge from target domain. e input to both of the teacher and the student network is the state of environment \u03b5\u03b2 of target task e\u03b2 . It means that we want to transfer the expertise from an expert of task e\u03b1 towards the current state. Symbol D is a set of logits from the teacher network in one batch and \u03c4 is the temperature same as described in Eq (1). In a trivial case that the teacher network and the student network are trained for same task (e\u03b1 equals e\u03b2 ), then the deep alignment network F\u03b8\u03c9 would reduce to an identity mapping, and the problem is also reduced to a single task policy distillation, which has been proved to be e ective in [26]. Before we can apply the deep knowledge distillation, we need to rst train a good deep alignment network. In this work, we provide two types of training protocols for di erent situations: O line training: is protocol rst trains two teacher networks in both environment \u03b5\u03b1 and \u03b5\u03b2 . en we use the logits of both two teacher networks to train a deep alignment network F\u03b8\u03c9 . A er acquiring a pre-trained F\u03b8\u03c9 , we train a student network of task e\u03b2 from scratch, in the meanwhile the teacher network of task e\u03b1 and F\u03b8\u03c9 are used for deep knowledge distillation. Online training: Suppose we only have a teacher network of task e\u03b1 , and we want to use the knowledge from task e\u03b1 to train the student network for task e\u03b2 to get higher performance from scratch. e pipeline of this method is that, we rstly train the student network by interacting with the environment \u03b5\u03b2 for a certain amount of steps T1, and then start to train the alignment network F\u03b8\u03c9 , using the logits from the teacher network and the student network. A erwards, at step T2, we start performing deep knowledge distillation. ObviouslyT2 is larger thanT1, and the value of them are task-speci c, which is decided empirically in this work.\ne o ine training could be useful if we have already had a reasonably good model for task e\u03b2 , while we want to further improve the performance using the knowledge from task e\u03b1 . e online training method is used when we need to learn the student network from scratch. Both types of training protocol can be extended to multiple heterogeneous tasks."}, {"heading": "4.3 Collaborative Asynchronous Advantage Actor-Critic", "text": "In this section, we introduce the proposed collaborative asynchronous advantage actor-critic (cA3C) algorithm. As we described in section 4.1, the agents are running in parallel. Each agent goes through the same training procedure as described in Algorithm 1. As it shows, the training of agent \u04341 can be separated into two parts: e rst part is to interact with the environment, get the reward and compute the gradients to minimize the value loss and policy\nloss based on Generalized Advantage Estimation (GAE) [27]. e second part is to interact with source agent \u04342 so that the logits distilled from agent \u04342 can be transferred by the deep alignment network and used as supervision to bias the training of agent \u04341.\nTo be more concrete, the pseudo code in Algorithm 1 is an envolved version of A3C based on online training of deep knowledge distillation. At T -th iteration, the agent interacts with the environment for tmax steps or until the terminal state is reached (Line 6 to Line 15). en the updating of value network and policy network is conducted by GAE. is variation of A3C is rstly implemented in OpenAI universe starter agent [22]. Since the main asynchronous framework is the same as A3C, we still use the A3C to denote this algorithm although the updating is the not the same as advantage actor-critic algorithm used in original A3C paper [19].\ne online training of deep knowledge distillation is mainly completed from Line 25 to Line 32 in Algorithm 1. e training of the deep alignment network starts from T1 steps (Line 25 - 28). A er T1 steps, the student network is able to generate a representative action probability distribution, and we have suitable supervision to train the deep alignment network as well, parameterized by \u03b8\u03c9 . A erT2 steps, \u03b8\u03c9 will gradually converge to a local optimal, and we start the deep knowledge distillation. As illustrated in Figure 2 (b), we use symbol \u03b8 \u03b2p \u2032 to represent the parameters of CNN, LSTM and the fully connected distillation layer, since we don\u2019t want the logits from heterogeneous directly a ect the action pa ern of target task. To simplify the discussion, the above algorithm is described based on interacting with a single agent from a heterogeneous task. In algorithm 1, logits z\u03b1t can be acquired from multiple teacher networks of di erent tasks, each task will train its own deep alignment network \u03b8\u03c9 and distill the aligned logits to the student network.\nAs we described in previous section 4.1, there are two types of interactions in this algorithm: 1). GAE interaction uses the gradients shared by all homogeneous agents. 2) Distillation interaction is the deep knowledge distillation from teacher network. e GAE interaction is performed only among homogeneous tasks. By synchronizing the parameters from a global student network in Algorithm 1 (line 3), the current agent receives the GAE updates from all the other agents who interactes with the same environment. In line 21 and 22, the current agent sends his gradients to the global student network, which will be synchronized with other homogeneous agents. e distillation interaction is then conducted in line 31, where we have the aligned logits F\u03b8\u03c9 (z\u03b1t ) and the distillation logits z\u03b2t \u2032 to compute the gradients for minimizing the distillation loss. e gradients of distillation are also sent to the global student network. e role of global student network can be regarded as a parameter server that helps sending interactions among the homogeneous agents. From a di erent angle, each homogeneous agent maintains an instinct version of global student network. erefore, both two types of interactions a ect all homogeneous agents, which means that the distillation interactions from agent \u04342 and agent \u04341 would a ect all homogeneous agents of agent \u043411.\n1Code is publicly available at h ps://github.com/illidanlab/cdrl\nAlgorithm 1 online cA3C Require: Global shared parameter vectors \u0398p and \u0398v and global shared\ncounter T = 0; Agent-speci c parameter vectors \u0398\u2032p and \u0398\u2032v , GAE [27] parameters \u03b3 and \u03bb. Time step to start training deep alignment network and deep knowledge distillation T1, T2.\n1: while T < Tmax do 2: Reset gradients: d\u03b8p = 0 and d\u03b8v = 0 3: Synchronize agent-speci c parameters \u03b8 \u2032p = \u03b8p and \u03b8 \u2032v = \u03b8v 4: tstar t = t , Get state st 5: Receive reward rt and new state st+1 6: repeat 7: Perform at according to policy 8: Receive reward rt and new state st+1 9: Compute value of state vt = V (st ; \u03b8 \u2032v )\n10: if T \u2265 T1 then 11: Compute the logits z\u03b1t from teacher network. 12: Compute the policy logits z\u03b2t and distillation logits z \u03b2 t \u2032\nfrom student network.\n13: end if 14: t = t + 1, T = T + 1 15: until terminal st or t \u2212 tstar t >= tmax 16:\nR = vt = { 0 for terminal st V (st , \u03b8 \u2032v ) for non-terminal st\n17: for i \u2208 {t \u2212 1, ..., tstar t } do 18: \u03b4i = ri + \u03b3vi+1 \u2212 vi 19: A = \u03b4i + (\u03b3 \u03bb)A 20: R = ri + \u03b3R 21: d\u03b8p \u2190 d\u03b8p + \u2207 log \u03c0 (ai |si ; \u03b8 \u2032)A 22: d\u03b8v \u2190 d\u03b8v + \u2202(R \u2212 vi )2/\u2202\u03b8 \u2032v 23: end for 24: Perform asynchronous update of \u03b8p using d\u03b8p and of \u03b8v using d\u03b8v . 25: if T \u2265 T1 then 26: // Training deep alignment network. 27: min\u03b8\u03c9 \u2211 t lKL (z \u03b2 t , z \u03b1 t , \u03c4 ), lKL is de ned in Eq (3). 28: end if 29: if T \u2265 T2 then 30: // online deep knowledge distillation. 31: min\n\u03b8 \u03b2p \u2032 \u2211 t lKL (F\u03b8\u03c9 (z\u03b1t ), z \u03b2 t \u2032 )\n32: end if 33: end while"}, {"heading": "5 EXPERIMENTS", "text": ""}, {"heading": "5.1 Training and Evaluation", "text": "In this work, training and evaluation are conducted in OpenAI Gym [5], a toolkit that includes a collection of benchmark problems such as classic Atari games using Arcade Learning Environment (ALE) [4], classic control games, etc. Same as the standard RL se ing, an agent is stimulated in an environment, taking an action and receiving rewards and observations at each time step. e training of the agent is divided into episodes, and the goal is to maximize the expectation of the total reward per episode or to reach higher performance using as few episodes as possible."}, {"heading": "5.2 Certi cated Homogeneous transfer", "text": "In this subsection, we verify the e ectiveness of knowledge distillation as a type of interaction in collaborative deep reinforcement\nlearning for homogeneous tasks. is is also to verify the e ectiveness of the simplest case for deep knowledge distillation. Although the e ectiveness of policy distillation in deep reinforcement learning has been veri ed in [26] based on DQN, there is no prior studies on asynchronous online distillation. erefore, our rst experiment is to demonstrate that the knowledge distilled from a certi cated task can be used to train a decent student network for a homogeneous task. Otherwise, the even more challenging task of transferring among heterogeneous sources may not work. We note that in this case, the Assumption 1 is fully satis ed given k1 = k2, where k1 and k2 are the knowledge needed to master task e1 and e2, respectively. In this experiment, we conduct experiments in a gym environment named Pong. It is a classic Atari game that an agent controls a paddle to bounce a ball pass another player agent. e maximum reward that each episode can reach is 21.\nFirst, we train a teacher network that learns from its own environment by asynchronously performing GAE updates. We then train a student network using only online knowledge distillation from the teacher network. For fair comparisons, we use 8 agents for all environments in the experiments. Speci cally, both the student and the teacher are training in Pong with 8 agents. e 8 agents of the teacher network are trained using the A3C algorithm (equivalent to CDRL with GAE updates in one task). e 8 agents of student network are trained using normal policy distillation, which uses the logits generated from the teacher network as supervision to train the policy network directly. From the results in Figure 3 (a) we see that the student network can achieve a very competitive performance that is is almost same as the state-of-arts, using online knowledge distillation from a homogeneous task. It also suggests that the teacher doesn\u2019t necessarily need to be an expert, before it can guide the training of a student in the homogeneous case. Before 2 million steps, the teacher itself is still learning from the environment, while the knowledge distilled from teacher can already be used to train a reasonable student network. Moreover, we see that the hybrid of two types of interactions in CDRL has a positive e ect on the training, instead of causing performance deterioration.\nIn the second experiment, the student network is learning from both the online knowledge distillation and the GAE updates from the environment. We nd that the convergence is much faster than the state-of-art, as shown in Figure 3 (b). In this experiment, the knowledge is distilled from the teacher to student in the rst one million steps and the distillation is stopped a er that. We note that in homogeneous CDRL, knowledge distillation is used directly with policy logits other than distillation logits. e knowledge transfer se ing in this experiment is not a practical one because we already have a well-trained model of Pong, but it shows that when knowledge is correctly transferred, the combination of online knowledge distillation and the GAE updates is an e ective training procedure."}, {"heading": "5.3 Certi cated Heterogeneous Transfer", "text": "In this subsection, we design experiments to illustrate the e ectiveness of CDRL in certi cated heterogeneous transfer, with the proposed deep knowledge distillation. Given a certi cated task Pong, we want to utilize the existing expertise and apply it to facilitate the training of a new task Bowling. In the following experiments, we do not tune any model-speci c parameters such\nas number of layers, size of lter or network structure for Bowling. We rst directly perform transfer learning from Pong to Bowling by knowledge distillation. Since the two tasks has di erent action pa erns and action probability distributions, directly knowledge distillation with a policy layer is not successful, as shown in Figure 4 (a). In fact, the knowledge distilled from Pong contradicts to the knowledge learned from Bowling, which leads to the much worse performance than the baseline. We show in Figure 5 (a) and (b) that the action distributions of Pong and Bowling are very di erent. To resolve this, we distill the knowledge through an extra distillation layer as illustrated in Figure 2 (b). As such, the knowledge distilled from the certi cated heterogeneous task can be successfully transferred to the student network with improved performance a er the learning is complete. However, this leads to a much slower convergence than the baseline as shown in Figure 4 (b), because that it takes time to learn a good distillation layer to align the knowledge distilled from Pong to the current learning task. An interesting question is that, is it possible to have both improved performance and faster convergence?\nDeep knowledge distillation \u2013 O line training. To handle the heterogeneity between Pong and Bowling, we rst verify the effectiveness of deep knowledge distillation with an o ine training procedure. e o ine training is split into two stages. In the rst stage, we train a deep alignment network with four fully connected layers using the Relu activation function. e training data are logits generated from an expert Pong network and Bowling network. e rewards of the networks at convergence are 20 and 60 respectively. In stage 2, with the Pong teacher network and trained deep alignment network, we train a Bowling student network from scratch. e student network is trained with both GAE interactions with its environment, and the distillation interactions from the teacher network and the deep alignment network. e results in Figure 6 (a) show that deep knowledge distillation can transfer knowledge from Pong to Bowling both e ciently and e ectively. Deep knowledge distillation \u2013 Online training. A more practical se ing of CDRL is the online training, where we simultaneously train deep alignment network and conduct the online deep knowledge distillation. We use two online training strategies: 1) e training of deep alignment network starts a er 4 million steps, when the student Bowling network can perform reasonably well, and the knowledge distillation starts a er 6 million steps. 2) e training of deep alignment network starts a er 0.1 million steps, and the knowledge distillation starts a er 1 million steps. Results are shown in Figure 6 (b) and (c) respectively. e results show that both strategies reach higher performance than the baseline. Moreover, the results suggest that we do not have to wait until the student network reaches a reasonable performance before we start to train the deep alignment network. is is because the deep alignment network is train to align two distributions of Pong and Bowling, instead of transferring the actual knowledge. Recall that the action probability distribution of Pong and Bowling are quite di erent as shown in Figure 5 (a) and (b). A er we projecting the logits of Pong using the deep alignment network, the distribution is very similar to Bowling, as shown in Figure 5 (c)."}, {"heading": "5.4 Collaborative Deep Reinforcement Learning", "text": "In previous experiments, we assume that there is a well-trained Pong expert, and we transfer knowledge from the Pong expert to the Bowling student via deep knowledge distillation. A more challenging se ings that both of Bowling and Pong are trained from scratch. In this experiment, we we show that the CDRL framework can still be e ective in this se ing. In this experiment, we train a Bowling network and a Pong network from scratch using the proposed cA3C algorithm. e Pong agents are trained with GAE interactions only, and the target Bowling receive supervision from both GAE interactions and distilled knowledge from Pong via a deep alignment network. We start to train the deep alignment network a er 3 million steps, and perform deep knowledge distillation a er 4 million steps, where the Pong agents are still updating from the environment. We note that in this se ing, the teacher network is constantly being updated, as knowledge is distilled from the teacher until 15 million steps. Results in Figure 6 (d) show that the proposed cA3C is able to converge to a higher performance than the current state-of-art. e reward of last one hundred episodes\nof A3C is 61.48 \u00b1 1.48, while cA3C achieves 68.35 \u00b1 1.32, with a signi cant reward improvement of 11.2%."}, {"heading": "6 CONCLUSION", "text": "In conclusion, we propose a collaborative deep reinforcement learning framework that can address the knowledge transfer among heterogeneous tasks. Under this framework, we propose deep knowledge distillation to adaptively align the domain of di erent tasks with the utilization of deep alignement network. Furthermore, we develeop an e cient cA3C algorithm and demonstrate its e ectiveness by extensive evaluation on OpenAI gym."}, {"heading": "ACKNOWLEDGMENTS", "text": "is research is supported in part by the O ce of Naval Research (ONR) under grant number N00014-14-1-0631 and National Science Foundation under Grant IIS-1565596, IIS-1615597."}], "references": [{"title": "Cross channel optimized marketing by reinforcement learning", "author": ["Naoki Abe", "Naval Verma", "Chid Apte", "Robert Schroko"], "venue": "In SIGKDD", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning. In Under review as a conference paper at ICLR 2017", "author": ["Coline Devin"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "\u008ce Arcade Learning Environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "J. Artif. Intell. Res.(JAIR)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Multi-agent reinforcement learning: An overview", "author": ["Lucian Bu\u015foniu", "Robert Babu\u0161ka", "Bart De Schu\u008aer"], "venue": "In Innovations in multi-agent systems and applications-1", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "A behavior-based scheme using reinforcement learning for autonomous underwater vehicles", "author": ["Marc Carreras", "Junku Yuh", "Joan Batlle", "Pere Ridao"], "venue": "IEEE Journal of Oceanic Engineering 30,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Collaborative Learning: Cognitive and Computational Approaches. Advances in Learning and Instruction Series. ERIC", "author": ["Pierre Dillenbourg"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Multi-task feature learning", "author": ["A Evgeniou", "Massimiliano Pontil"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Regularized multi\u2013task learning", "author": ["\u008ceodoros Evgeniou", "Massimiliano Pontil"], "venue": "In SIGKDD", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Collaborative learning enhances critical thinking", "author": ["Anuradha A Gokhale"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Coordinated reinforcement learning", "author": ["Carlos Guestrin", "Michail Lagoudakis", "Ronald Parr"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Sapiens: a brief history of humankind", "author": ["Yuval N. Harari"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Geo\u0082rey Hinton", "Oriol Vinyals", "Je\u0082 Dean"], "venue": "arXiv preprint arXiv:1503.02531", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Max Jaderberg", "Volodymyr Mnih", "Wojciech Marian Czarnecki", "Tom Schaul", "Joel Z Leibo", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Collaborative multiagent reinforcement learning by payo\u0082 propagation", "author": ["Jelle R Kok", "Nikos Vlassis"], "venue": "JMLR 7,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Playing FPS games with deep reinforcement learning", "author": ["Guillaume Lample", "Devendra Singh Chaplot"], "venue": "arXiv preprint arXiv:1609.05521", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "TKDE 22,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Actormimic: Deep multitask and transfer reinforcement learning", "author": ["Emilio Pariso\u008ao", "Jimmy Lei Ba", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.06342", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "A two-stage weighting framework for multi-source domain adaptation", "author": ["Qian Sun", "Rita Cha\u008aopadhyay", "Sethuraman Panchanathan", "Jieping Ye"], "venue": "In NIPS", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Ming Tan"], "venue": "In ICML", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1993}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Ma\u008ahew E Taylor", "Peter Stone"], "venue": "JMLR 10,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Agent based decision support system using reinforcement learning under emergency circumstances", "author": ["Devinder \u008capa", "In-Sung Jung", "Gi-Nam Wang"], "venue": "In International Conference on Natural Computation", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["Eric Tzeng", "Judy Ho\u0082man", "Trevor Darrell", "Kate Saenko"], "venue": "In ICCV", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Yu Zhang", "Dit-Yan Yeung"], "venue": "arXiv preprint arXiv:1203.3536", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Zhanpeng Zhang", "Ping Luo", "Chen Change Loy", "Xiaoou Tang"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "MALSAR: Multi-task learning via structural regularization", "author": ["Jiayu Zhou", "Jianhui Chen", "Jieping Ye"], "venue": "Arizona State University", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Yuke Zhu", "Roozbeh Mo\u008aaghi", "Eric Kolve", "Joseph J Lim", "Abhinav Gupta", "Li Fei-Fei", "Ali Farhadi"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "As a social animal, the ability to collaborate awoke the cognitive revolution and reveals the prosperous history of human [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 5, "context": "In disciplines of cognitive science, education and psychology, collaborative learning, a situation in which a group of people learn to achieve a set of tasks together, has been advocated throughout previous studies [9].", "startOffset": 215, "endOffset": 218}, {"referenceID": 8, "context": "process, as well [12].", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": ", [16, 18, 20, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 14, "context": ", [16, 18, 20, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 22, "context": "great transformative power to impact many industries with data mining and machine learning techniques such as clinical decision support [32], marketing [2], \u0080nance [1], visual navigation [37], and", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "great transformative power to impact many industries with data mining and machine learning techniques such as clinical decision support [32], marketing [2], \u0080nance [1], visual navigation [37], and", "startOffset": 152, "endOffset": 155}, {"referenceID": 27, "context": "great transformative power to impact many industries with data mining and machine learning techniques such as clinical decision support [32], marketing [2], \u0080nance [1], visual navigation [37], and", "startOffset": 187, "endOffset": 191}, {"referenceID": 4, "context": "autonomous driving [8].", "startOffset": 19, "endOffset": 22}, {"referenceID": 15, "context": "Although there are many existing e\u0082orts towards e\u0082ective algorithms for DRL [19, 21], the computational cost still imposes signi\u0080cant challenges as training DRL for even a ar X iv :1 70 2.", "startOffset": 76, "endOffset": 84}, {"referenceID": 12, "context": "transfer from other related tasks or well-trained deep models to facilitate training has drawn lots of a\u008aention in the community [16, 24\u2013 26, 31].", "startOffset": 129, "endOffset": 145}, {"referenceID": 21, "context": "transfer from other related tasks or well-trained deep models to facilitate training has drawn lots of a\u008aention in the community [16, 24\u2013 26, 31].", "startOffset": 129, "endOffset": 145}, {"referenceID": 11, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 131, "endOffset": 143}, {"referenceID": 17, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 131, "endOffset": 143}, {"referenceID": 6, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 163, "endOffset": 179}, {"referenceID": 17, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 163, "endOffset": 179}, {"referenceID": 24, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 163, "endOffset": 179}, {"referenceID": 25, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 163, "endOffset": 179}, {"referenceID": 7, "context": "For example, in the regularized MTL models such as [11, 36], tasks with the same feature space are related through some structured regularization.", "startOffset": 51, "endOffset": 59}, {"referenceID": 26, "context": "For example, in the regularized MTL models such as [11, 36], tasks with the same feature space are related through some structured regularization.", "startOffset": 51, "endOffset": 59}, {"referenceID": 25, "context": "Another example is the multi-task deep neural network, where di\u0082erent tasks share parts of the network structures [35].", "startOffset": 114, "endOffset": 118}, {"referenceID": 11, "context": "On the other hand, the recently developed data transfer (also known as knowledge distillation or mimic learning) [15, 24, 26] embeds the source model knowledge into data points.", "startOffset": 113, "endOffset": 125}, {"referenceID": 17, "context": "On the other hand, the recently developed data transfer (also known as knowledge distillation or mimic learning) [15, 24, 26] embeds the source model knowledge into data points.", "startOffset": 113, "endOffset": 125}, {"referenceID": 11, "context": "\u008cen they are used as knowledge bridge to train target models, which can have di\u0082erent structures as compared to the source model [6, 15].", "startOffset": 129, "endOffset": 136}, {"referenceID": 17, "context": "extensively trained and performs very well), how can we maximize the information that can be used in the training of other related tasks? Some model transfer approaches directly use the weights from the trained model to initialize the new task [24], which can only be done when the model structures are the same.", "startOffset": 244, "endOffset": 248}, {"referenceID": 17, "context": "tasks are signi\u0080cantly di\u0082erent from each other in nature [24].", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "One feasible way to conduct transfer under this scenario is that agents of multiple tasks share part of their network parameters [26, 35].", "startOffset": 129, "endOffset": 137}, {"referenceID": 1, "context": "Another solution is to learn a domain invariant feature space shared by all tasks [3].", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "Meanwhile they could potentially interact with each other [7, 13, 17, 30].", "startOffset": 58, "endOffset": 73}, {"referenceID": 9, "context": "Meanwhile they could potentially interact with each other [7, 13, 17, 30].", "startOffset": 58, "endOffset": 73}, {"referenceID": 13, "context": "Meanwhile they could potentially interact with each other [7, 13, 17, 30].", "startOffset": 58, "endOffset": 73}, {"referenceID": 20, "context": "Meanwhile they could potentially interact with each other [7, 13, 17, 30].", "startOffset": 58, "endOffset": 73}, {"referenceID": 9, "context": "maximize a shared reward measurement [13, 17].", "startOffset": 37, "endOffset": 45}, {"referenceID": 13, "context": "maximize a shared reward measurement [13, 17].", "startOffset": 37, "endOffset": 45}, {"referenceID": 16, "context": "Another relevant research topic is domain adaption in the \u0080eld of transfer learning [23, 29, 33].", "startOffset": 84, "endOffset": 96}, {"referenceID": 19, "context": "Another relevant research topic is domain adaption in the \u0080eld of transfer learning [23, 29, 33].", "startOffset": 84, "endOffset": 96}, {"referenceID": 23, "context": "Another relevant research topic is domain adaption in the \u0080eld of transfer learning [23, 29, 33].", "startOffset": 84, "endOffset": 96}, {"referenceID": 19, "context": "\u008ce authors in [29] proposed a two-stage domain adaptation framework that considers the di\u0082erences among marginal probability distributions of domains, as well as conditional probability distributions of tasks.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "In [33], the marginal distributions of the source and the target domain are aligned by training a network, which maps inputs into a domain invariant representation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "In [3], an invariant feature space is learned to transfer skills between two agents.", "startOffset": 3, "endOffset": 6}, {"referenceID": 25, "context": "\u008ce multi-task deep neural network (MTDNN) [35] transfers knowledge among tasks by sharing pa-", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "In [15], the authors proposed to compress cumbersome models (teachers) to more simple models (students), where the simple models are trained by a dataset (knowledge) dis-", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "[19]", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In [24], a single multi-task policy network is trained by utilizing a set of expert Deep Q-Network (DQN) of source games.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "\u008ce asynchronous advantage actor-critic (A3C) algorithm [19] launches multiple agents in parallel and asynchronously updates a global shared target policy network \u03c0 (a |s,\u03b8p ) as well as a value network V (s,\u03b8v ).", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "Knowledge distillation [15] is a transfer learning approach that distills the knowledge from a teacher network to a student network", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "\u008cen the knowledge distillation can be completed by optimize the following Kullback-Leibler divergence (KL) with temperature \u03c4 [15, 26].", "startOffset": 126, "endOffset": 134}, {"referenceID": 7, "context": "\u008cis is a formal description of our common sense that any pair of tasks are not absolutely isolated from each other, which has been implicitly used as a fundamental assumption by most prior transfer learning studies [11, 24, 26].", "startOffset": 215, "endOffset": 227}, {"referenceID": 17, "context": "\u008cis is a formal description of our common sense that any pair of tasks are not absolutely isolated from each other, which has been implicitly used as a fundamental assumption by most prior transfer learning studies [11, 24, 26].", "startOffset": 215, "endOffset": 227}, {"referenceID": 5, "context": "\u008ce agent \u0434i is not necessary to be at same level as \u201dcollaborative\u201d de\u0080ned in cognitive science [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "Notice that our de\u0080nition is very di\u0082erent from the previously de\u0080ned collaborative multiagent Markow Decision Process (collaborative multiagent MDP) [13, 17] where a set of agents select a global joint action to maximize the sum of their individual rewards and the environment is transi\u008aed to a new state based on that joint action.", "startOffset": 150, "endOffset": 158}, {"referenceID": 13, "context": "Notice that our de\u0080nition is very di\u0082erent from the previously de\u0080ned collaborative multiagent Markow Decision Process (collaborative multiagent MDP) [13, 17] where a set of agents select a global joint action to maximize the sum of their individual rewards and the environment is transi\u008aed to a new state based on that joint action.", "startOffset": 150, "endOffset": 158}, {"referenceID": 15, "context": "the-art is A3C [19] can be categorized as one homogeneous CDRL method with advantage actor-critic interaction.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "As we introduced before, knowledge distillation [15] is trying to train a student network that can behave similarly to the teacher network by utilizing the logits from the teacher as supervision.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "As it shows, the training of agent \u04341 can be separated into two parts: \u008ce \u0080rst part is to interact with the environment, get the reward and compute the gradients to minimize the value loss and policy loss based on Generalized Advantage Estimation (GAE) [27].", "startOffset": 253, "endOffset": 257}, {"referenceID": 15, "context": "Since the main asynchronous framework is the same as A3C, we still use the A3C to denote this algorithm although the updating is the not the same as advantage actor-critic algorithm used in original A3C paper [19].", "startOffset": 209, "endOffset": 213}, {"referenceID": 18, "context": "Require: Global shared parameter vectors \u0398p and \u0398v and global shared counter T = 0; Agent-speci\u0080c parameter vectors \u0398p and \u0398\u2032 v , GAE [27] parameters \u03b3 and \u03bb.", "startOffset": 134, "endOffset": 138}, {"referenceID": 2, "context": "(ALE) [4], classic control games, etc.", "startOffset": 6, "endOffset": 9}], "year": 2017, "abstractText": "Besides independent learning, human learning process is highly improved by summarizing what has been learned, communicating it with peers, and subsequently fusing knowledge from di\u0082erent sources to assist the current learning goal. \u008cis collaborative learning procedure ensures that the knowledge is shared, continuously re\u0080ned, and concluded from di\u0082erent perspectives to construct a more profound understanding. \u008ce idea of knowledge transfer has led to many advances in machine learning and data mining, but signi\u0080cant challenges remain, especially when it comes to reinforcement learning, heterogeneous model structures, and di\u0082erent learning tasks. Motivated by human collaborative learning, in this paper we propose a collaborative deep reinforcement learning (CDRL) framework that performs adaptive knowledge transfer among heterogeneous learning agents. Speci\u0080cally, the proposed CDRL conducts a novel deep knowledge distillation method to address the heterogeneity among di\u0082erent learning tasks with a deep alignment network. Furthermore, we present an e\u0081cient collaborative Asynchronous Advantage Actor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into the online training of agents, and demonstrate the e\u0082ectiveness of the CDRL framework using extensive empirical evaluation on OpenAI gym.", "creator": "LaTeX with hyperref package"}}}