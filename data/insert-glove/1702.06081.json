{"id": "1702.06081", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Learning Non-Discriminatory Predictors", "abstract": "We off-the-cuff consider ccap learning nasb a predictor which mid-1970 is ze'ev non - redang discriminatory with respect to a \" 33,750 protected 586 attribute \" fmcg according kikin to the pre-monsoon notion 80.65 of \" catasauqua equalized odds \" bhlh proposed demurs by .406 Hardt beatbox et english-canadian al. [checo 2016 ]. We oleracea study the zipaquira problem of otechestvennye learning qian such a non - chantries discriminatory epidemia predictor from a ivl finite reincarnating training kosmos-2i set, both statistically amsterdammer and mahinmi computationally. We show that a brightness post - hoc correction lahoti approach, eastone as 1977-2010 suggested by Hardt ebby et ponnampalam al, woodforest can be doenjang highly neds suboptimal, present a szymczyk nearly - optimal belonging statistical atochem procedure, lankhmar argue that ignatieff the braugher associated powersoft computational nankin problem is atlacatl intractable, 190-pound and suggest craigavon a skating second moment relaxation of musovic the 3,560 non - schoendoerffer discrimination definition for mcsa which learning gottscheerish is maithripala tractable.", "histories": [["v1", "Mon, 20 Feb 2017 17:52:14 GMT  (99kb,D)", "http://arxiv.org/abs/1702.06081v1", null], ["v2", "Fri, 29 Sep 2017 14:52:10 GMT  (143kb,D)", "http://arxiv.org/abs/1702.06081v2", "28 pages"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["blake woodworth", "suriya gunasekar", "mesrob i ohannessian", "nathan srebro"], "accepted": false, "id": "1702.06081"}, "pdf": {"name": "1702.06081.pdf", "metadata": {"source": "CRF", "title": "Learning Non-Discriminatory Predictors", "authors": ["Blake Woodworth", "Suriya Gunasekar", "Mesrob I. Ohannessian"], "emails": ["blake@ttic.edu", "suriya@ttic.edu", "mesrob@ttic.edu", "nati@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "Machine learning algorithms are increasingly deployed in important decision making tasks that affect people\u2019s lives significantly. These tools already appear in domains such as lending, policing, criminal sentencing, and targeted service offerings. In many of these domains, it is morally and legally undesirable to discriminate based on certain \u201cprotected attributes\u201d such as race and gender. Even in seemingly innocent applications, such as ad placement and product recommendations, such discrimination might be illegal or detrimental. Consequently, there has been abundant public, academic and technical interest in notions of non-discrimination and fairness, and achieving \u201cequal opportunity by design\u201d is a major United States national Big Data challenge, [White House, 2016].\nWe consider non-discrimination in supervised learning where the goal is to learn a (potentially randomized) predictor h(X) or1 h(X,A) for a target quantity Y \u2208 Y using features X \u2208 X and a protected attribute A \u2208 A, while ensuring non-discrimination with respect to A. As an illustrative example, consider a financial institution that wants to predict whether a particular individual will pay back a loan or not, corresponding to Y = 1 or Y = 0. The features X could include financial as well as other information, e.g. about education, driving, and housing history, languages spoken, and the number of members in the household, all of which have a potential of being used inappropriately as a surrogate for the protected attribute, A, such as gender or race. It is important that the predictor for loan repayment not be even implicitly \u201cdiscriminatory\u201d with respect to A.\nThe particular notion of non-discrimination we consider here is \u201cequalized odds\u201d, recently presented and studied by Hardt et al. [2016]:\nDefinition 1 (Equalized Odds). A possibly randomized predictor Y\u0302 = h(X,A) for target quantity Y is non-discriminatory with respect to a protected attribute A if Y\u0302 is independent of A conditioned on Y .\nInformally, we require that even if the correct label Y provides information about the protected attribute A, if we already know Y , the prediction does not provide any additional information about\n1See Hardt et al. [2016] for a discussion on why it might be necessary for a non-discriminatory predictor to use A\nar X\niv :1\n70 2.\n06 08\n1v 1\n[ cs\n.L G\n] 2\n0 Fe\nb 20\n17\nA. The definition can also be motivated in terms incentive structure and of moving the burden of uncertainty from the protected population to the decision maker. See Hardt et al. [2016] for further discussion of the definition, its implications, and comparisons to alternative notions.\nIn a binary prediction task when Y\u0302 , A, Y \u2208 {0, 1}, Definition 1 can be interpreted in terms of true and false positive rates. Denote the group-conditional true and false positive rates as,\n\u03b3ya(Y\u0302 ) := P(Y\u0302 = 1 | Y = y,A = a), (1)\nThen Definition 1 is equivalent to requiring that the class conditional true and false positive rates agree on different \u201cgroups\u201d (different values of A):\n\u03b300(Y\u0302 ) = \u03b301(Y\u0302 ) and \u03b310(Y\u0302 ) = \u03b311(Y\u0302 ) (2)\nReturning to the loan example, this definition requires that the percentage of men who are wrongly denied loans even though they would have paid it back must be the same as the corresponding percentage for women, and that the percentage of men who are wrongly given loans that they will not pay back must match the percentage of women. This does not however require that the same percentage of male and female applicants will receive loans. For instance, if women pay back loans with truly higher frequency than men, then the predictor would be allowed to deny loans to men more often than women.\nWhile Hardt et al. focused on the notion itself and how it behaves on the population, here we tackled the problem of how to learn a good non-discriminatory predictor (i.e. satisfying the equalized odds Definition 1) from a finite training set. We examine this both from a statistical perspective, of how to best obtain a predictor from finite data that would be as good and non-discriminatory as possible on the population, and from a computational perspective.\nOne possible approach to learning a non-discriminative predictor is post hoc correction [Hardt et al., 2016]: first learn a good predictor ignoring non-discrimination, i.e. a possibly discriminatory predictor. Afterwards, this predictor is \u201ccorrected\u201d by taking into account A in order to make the predictor non-discriminatory. When Y is binary and the predictors Y\u0302 are real-valued, they show that the unconstrained Bayes optimal least-square regressor can be post hoc corrected to the optimal predictor with respect to the 0-1 loss. In Section 2, we consider more carefully the limitations of such a post hoc procedure. In particular, we show that this approach can fail for the 0-1 and hinge losses, even if the Bayes optimal predictor with respect to those losses is learned in the first step. We also show that even when minimizing the squared loss, the approach can fail once we limit ourselves to a specific hypothesis class, as is essential when learning from finite data. From this, we conclude that post hoc correction is not sufficient, and that it is necessary to directly incorporate non-discrimination into the learning process.\nTurning to learning from finite data, we cannot hope to ensure exact non-discrimination on the population. To this end, in Section 3 we define a notion of approximate non-discrimination, motivate it, and explore its limits by analyzing the statistical problem of detecting whether or not a predictor is at least \u03b1-discriminatory.\nWe then turn to the main statistical question: given a finite training set, how can we best learn a predictor that is ensured to be as non-discriminatory as possible (on the population) and competes (in terms of its population loss) with the best non-discriminatory predictor in some given hypothesis class (this is essentially an extension of the notion of agnostic PAC learning with a non-discrimination constraint). In Section 4 we show that an ERM-type procedure, minimizing the training error subject to an empirical non-discrimination constraint, is statistically sub-optimal, and instead we present a statistically optimal (up to constant factors) two-step learning procedure for non-discriminatory binary classification.\nUnfortunately, learning a non-discriminatory binary classifier is computationally hard, which we prove in Section 5. In order to allow tractable training, in Section 6, we present a relaxation of equalized odds, based only on a second-moment condition instead of full conditional independence. We show that under this second moment notion of non-discrimination it is computationally tractable to learn a nearly optimal non-discriminatory linear predictor with respect to a convex loss."}, {"heading": "2 Sub-optimality of Post-training Correction", "text": "When the protected attribute A and the target Y are both binary, the post hoc correction algorithm proposed by Hardt et al. [2016] can be applied to a binary or real-valued predictor Y\u0302 \u2208 H, deriving a randomized binary predictor that is non-discriminatory. The algorithm is convenient because it requires access only to the joint distribution over (Y\u0302 , A, Y ) and does not use the features X, thus it can be applied retroactively to an already trained predictor. Such predictors are formulated using the notion of a derived predictor:\nDefinition 2. [Definition 4.1 Hardt et al. [2016]] A predictor Y\u0303 is derived from a random variable R and protected attribute A if it is a possibly randomized function of (R,A) alone. In particular, Y\u0303 is independent of X conditioned on (R,A).\nFor binary classification problems, the optimal post hoc correction Y\u0303 for a binary or real valued predictor Y\u0302 \u2208 R is a straightforward ternary optimization problem: it is simply the nondiscriminatory, derived, binary predictor that minimizes the expectation of loss ` (Hardt et al. [2016]):\nY\u0303 = argmin f :R\u00d7{0,1}7\u2192{0,1}\nE ` ( f(Y\u0302 , A), Y ) s.t. \u03b3y0(f) = \u03b3y1(f) \u2200y = {0, 1}\n(3)\nTwo notable features of the corrected predictor Y\u0303 are that a) it is not constrained to any particular hypothesis class, and b) it may be a random function of Y\u0302 and A; indeed for many distributions and hypothesis classes there may not even exist a non-constant, deterministic, non-discriminatory predictor. Nevertheless, Y\u0303 does indirectly depend on the hypothesis class from which Y\u0302 was learned and the loss function used to train Y\u0302 .\nWe are interested in comparing the optimality of Y\u0303 from post hoc correction to the following Y \u2217\nwhich is the optimal non-discriminatory predictor in the hypothesis class H under consideration:\nY \u2217 = argmin h\u2208H E ` (h(X,A), Y ) s.t. \u03b3y0(h) = \u03b3y1(h) \u2200y = {0, 1} (4)\nIdeally, the expected loss of Y\u0303 would compare favorably against that of Y \u2217. Indeed, Hardt et al. [2016] show that when the target Y is binary, if we can first find a predictor R that is exactly or nearly Bayes optimal for the squared loss over an unconstrained hypothesis class, then applying the post hoc correction (3) using the 0-1 loss (i.e. with ` = `01 in (3)) to R will yield a predictor Y\u0303 that is non-discriminatory and has loss no worse than Y \u2217. This statement can be extended to the case of first finding the optimal unconstrained predictor with respect to any strictly convex loss, and then using the post hoc correction with the 0-1 loss.\nNevertheless, from a practical perspective this approach is very unsatisfying. First, for general distributions, it is impossible to learn the Bayes optimal predictor from finite samples of data. Also, as we will show, the post hoc correction of the optimal predictor with respect to the 0-1 or hinge losses can have much worse performance than the best non-discriminatory predictor, even when\nthe hypothesis class in unconstrained. Moreover, if the hypothesis class is restricted there can also be a gap between the post hoc correction of the optimal predictor in the hypothesis class and the best non-discriminatory predictor, even when optimizing a strictly convex loss function.\nIn the following example, we see that when the loss function is not strictly convex, the post hoc correction of even the unconstrained Bayes optimal predictor can have poor accuracy:\nExample 1. When the hypothesis class is unconstrained, for any \u2208 (0, 1/4) there exists a distribution D such that a) the optimal non-discriminatory predictor with respect to the 0-1 loss has loss at most 2 but b) the post hoc correction of the unrestricted Bayes optimal predictor has loss at least 0.5.\nA similar statement can also be made about hinge loss. For an unconstrained hypothesis class, for any \u2208 (0, 1/4) and the same distribution D , a) the optimal non-discriminatory predictor with respect to the hinge loss has loss at most 4 but b) the post hoc correction of the Bayes optimal unrestricted predictor has loss at least 1.\nWe construct D as follows:\nX\nY\nA X,A, Y \u2208 {0, 1} PD (Y = 1) = 1\n2 PD (A = y | Y = y) = 1\u2212 PD (X = y | Y = y) = 1\u2212 2\n(5)\nBoth X and A are highly predictive of Y , but A is slightly more so. Therefore, minimizing either the 0-1 or the hinge loss requires returning A and ignoring X entirely. Consequently, \u03b3y1 = 1 = 1\u2212 \u03b3y0 so the optimal predictor is 1-discriminatory, and the post hoc correction is forced to return a terrible predictor even though returning X would be accurate and non-discriminatory. A more detailed proof is included in Appendix A.1\nIn the second example, we show that when the hypothesis class is restricted, the correction of the optimal regressor in the class can yield a suboptimal classifier, even with squared loss.\nExample 2. Let H be the class of linear predictors with L1 norm at most 12 \u2212 2 , for some \u2208 (2/25, 1/4). Then there exists a distribution D such that a) the optimal non-discriminatory predictor in H with respect to the squared loss has square loss at most 116 + 3 2 + 3\n2, but b) the post hoc correction of the Bayes optimal square loss regressor in H returns a constant predictor which has (trivial) square loss of 1/4.\nSimilarly, for the class H of sparse linear predictors, for any \u2208 (0, 1/4), there exists a distribution D such that a) the optimal non-discriminatory predictor in H with respect to the squared loss has square loss at most 2 \u2212 4 2, but b) the post hoc correction of the Bayes optimal squared loss regressor in H again returns a constant predictor which has (trivial) square loss of 1/4.\nThe distribution D is the same as was defined in (5). Again, A is slightly more predictive of Y than X, and since the sparsity or the sparsity surrogate L1 norm of the predictor is constrained by the hypothesis class, the Bayes optimal predictor chooses to use just the feature A and ignore X. Consequently, the predictor is extremely discriminatory, and the post hoc correction algorithm will return a highly sub-optimal constant predictor which performs no better than chance. Details of the proof are deferred to Appendix A.2\nFrom these examples, it is clear that simply finding the optimal predictor with respect to a particular loss function and hypothesis class and correcting it post hoc can perfom very poorly. We conclude that in order to learn a predictor that is simultaneously accurate and non-discriminatory in the general case, it is essential to account for non-discrimination during the learning process."}, {"heading": "3 Detecting Discrimination in Binary Predictors", "text": "In the following sections, we look at tools for integrating non-discrimination into the supervised learning framework. In formulating algorithms for learning accurate and non-discriminatory predictors, it is important to address the issues that arise from dealing with finite data.\nTo begin with, using finite samples, we can never hope to ensure or even verify if a predictor Y\u0302 satisfies the non-discrimination criterion in Definition 1. This necessitates defining a notion of approximate non-discrimination which generalizes to the equalized odds criteria in the population, but can also be computed using finite samples.\nLet us consider the task of binary classification, where both A, Y \u2208 {0, 1} and the predictors Y\u0302 output values in {0, 1}. Recall the definition of the population class-conditional true and false positive rates \u03b3ya(Y\u0302 ) = P(Y\u0302 = 1 | Y = y,A = a) and the fact that non-discrimination is equivalent to the condition that \u03b300 = \u03b301 and \u03b310 = \u03b311. For a set of of n i.i.d. samples, S = {(xi, ai, yi)}ni=1 \u223c Pn(X,A, Y ), the sample analogue of \u03b3ya is defined as follows,\n\u03b3Sya(Y\u0302 ) = 1\nnSya n\u2211 i=1 Y\u0302 (xi, ai)1(yi = y, ai = a), , where n S ya = n\u2211 i=1 1(yi = y, ai = a). (6)\nTo ensuring non-discrimination, we could possibly require \u03b3Sy0 = \u03b3 S y1 on a large enough sample S, however this not sufficient for two reasons. First, in general it is not feasible to match \u03b3Sy0 = \u03b3 S y1\nexactly for nSy0 6= nSy1, e.g. if nSy0 = 2 and nSy1 = 3, then \u03b3Sy0 \u2208 { 0, 12 , 1 } but \u03b3Sy1 \u2208 { 0, 13 , 2 3 , 1 }\n, thus only predictors which are constant conditioned on Y = y, i.e. either the constant or the perfect predictor, would be non-discriminatory. Second, even when \u03b3Sy0 = \u03b3 S y1 on S, this almost certainly does not ensure that \u03b3y0 = \u03b3y1 on the population. For this same reason, it is impossible to be certain that a given predictor is non-discriminatory on the population.\nFor these reasons, we define a notion of approximate non-discrimination, which is possible to ensure on a sample and, when it holds on a sample, generalizes to the population.\nDefinition 3. A possibly randomized binary predictor Y\u0302 is said to be \u03b1-discriminatory with respect to a binary protected attribute A on the population or a sample S if\n\u0393(Y\u0302 ) := max y\u2208{0,1} \u2223\u2223\u2223\u03b3y0(Y\u0302 )\u2212 \u03b3y1(Y\u0302 )\u2223\u2223\u2223 \u2264 \u03b1 or \u0393S(Y\u0302 ) := max y\u2208{0,1} \u2223\u2223\u2223\u03b3Sy0(Y\u0302 )\u2212 \u03b3Sy1(Y\u0302 )\u2223\u2223\u2223 \u2264 \u03b1. (7) The decision to define approximate non-discrimination in terms of conditional rather than joint probabilities is important, particularly in the case that the a, y pairs occur with widely varying frequencies. For example, if approximate non-discrimination were defined in terms of the joint probabilities P (Y\u0302 = y\u0302, A = a, Y = y) and if P (A = 0, Y = 1) = \u03b1/10, then a predictor could be \u201c\u03b1-discriminatory\u201d all while being arbitrarily unfair towards the A = 0, Y = 1 population. This issue does not arise when using Definition 3. Nevertheless, with this definition, approximate non-discrimination becomes more difficult to ensure as the least frequent group becomes rarer.\nGiven this definition, we propose a simple statistical test to test the hypothesis that a given predictor Y\u0302 is at most \u03b1-discriminatory on the population for some \u03b1 > 0. Let S = {(xi, ai, yi)}ni=1 \u223c Pn(X,Y,A) denote a set of n i.i.d. samples, and for y, a \u2208 {0, 1}, let Pya = P(Y = y,A = a). We propose the following test for detecting \u03b1-discrimination:\nT ( Y\u0302 , S, \u03b1 ) = 1 ( \u0393S(Y\u0302 ) > \u03b1\n2\n) (8)\nLemma 1. Given n i.i.d. samples S, \u2200\u03b1, \u03b4 > 0, if n > 16 maxya log 16/\u03b4Pya\u03b12 , then with probability greater than 1\u2212 \u03b4, T satisfies,\nT ( Y\u0302 , S, \u03b1 ) =\n{ 0 if Y\u0302 is 0-discriminatory on population\n1 if Y\u0302 is at least \u03b1-discriminatory on population.\nThe proof is based on concentration of \u0393S , and is provided in Appendix B.1."}, {"heading": "4 Integrated Learning of Non-Discriminatory Binary Predictors", "text": "In Example 1, we saw that even though an almost perfect non-discriminatory predictor exists within the hypothesis class, if we ignore non-discrimination in training with 0-1 loss, then optimal post hoc correction using (3) yields a poor predictor with no better than chance accuracy. Thus, to find a predictor that both is nearly non-discriminatory and has nearly optimal loss for general hypothesis classes, it is necessary to incorporate non-discrimination criteria into the learning process. For a hypothesis class H, we would ideally find the optimal non-discriminatory predictor:\nY \u2217 = argmin h\u2208H E` (h(X,A), Y ) s.t. \u03b3y0(h) = \u03b3y1(h) \u2200y = {0, 1} . (9)\nHowever, as motivated in Section 3, it is impossible to learn 0-discriminatory predictors from finite samples. In this section, we address following question: given access to n i.i.d samples, what level of approximate non-discrimination and accuracy is it possible to ensure in a learned predictor?\nWe propose a two step framework for learning a non-discriminatory binary predictor that minimizes the expected 0-1 loss L(Y\u0302 ) = E `01(Y\u0302 , Y ) = E1(Y\u0302 6= Y ) over a binary hypothesis class H = {h : X \u2192 {0, 1}}. Broadly, the two-step framework is as follows:\n1. Non-discrimination in training: Estimate an almost non-discriminatory empirical risk minimizer Y\u0302 by incorporating approximate non-discrimination constraints on the samples. 2. Post-training correction: With additional samples from (Y\u0302 , Y, A), derive a randomized predictor Y\u0303 to further reduce discrimination."}, {"heading": "4.1 Two step framework for binary predictors", "text": "We partition the training data consisting of n independent samples S = {(xi, ai, yi) \u223c P(X,A, Y )}Ni=1 into two subsets S1 and S2 to be used in Step 1 and Step 2, respectively\n2. For a predictor h \u2208 H and y, a \u2208 {0, 1}, recall notation for the population and sample groupconditional true and false positive rates\u03b3ya(h) and \u03b3 S ya(h) from (1) and (6), respectively. Additionally, for S1 and S2, let n Sk ya = \u2211 i\u2208Sk 1(yi = y, ai = a). In general the subsets need not be of equal size, but for simplicity, let |S1| = |S2| = n/2.\nStep 1: Non-discrimination in training\nFor the first step, we estimate an empirical risk minimizing predictor Y\u0302 \u2208 H, subject to the constraint that Y\u0302 be \u03b1n-discriminatory on S1, where \u03b1n is a tunable hyperparameter:\nY\u0302 = argmin h\u2208H\n2\nn \u2211 i\u2208S1 `01(h(xi), yi)\ns.t. \u0393S1(h) = max y\u2208{0,1} \u2223\u2223\u2223\u03b3S1y0 (h)\u2212 \u03b3S1y1 (h)\u2223\u2223\u2223 < \u03b1n. (10) 2We occasionally overload the S to also denote the indices [n], e.g. i \u2208 S to denote the ith sample (xi, ai, yi) \u2208 S.\nStep 2: Post-training correction\nAs Step 2, we propose a post hoc correction to Y\u0302 estimated in Step 1 for improved non-discrimination. Using samples in S2, which are independent of S1, we estimate the best randomized predictor Y\u0303 derived from (Y\u0302 , A) (Definition 2). Let P(Y\u0302 ) denote the set of randomized binary predictors that can be derived solely from P(Y\u0302 , A, Y ) and let \u03b1\u0303n be a tunable hyperparameter, then Y\u0303 is given by,\nY\u0303 = argmin Y\u0303 \u2208P(Y\u0302 )\n2\nn \u2211 i\u2208S2 E Y\u0303 `01(Y\u0303 (xi), yi)\ns.t. \u0393S2(Y\u0303 ) = max y\u2208{0,1} \u2223\u2223\u2223\u03b3S2y0 (Y\u0303 )\u2212 \u03b3S2y1 (Y\u0303 )\u2223\u2223\u2223 < \u03b1\u0303n, (11) where for a randomized predictor Y\u0303 , the group-conditional probabilities on a sample is defined to be \u03b3S2ya (Y\u0303 ) = 1\nn S2 ya\n\u2211 i\u2208S2 EY\u0303 1(Y\u0303i = 1, Yi = y,Ai = a). The above optimization problem is a finite\nsample adaptation of the post hoc correction in (3) proposed by Hardt et al. [2016]. As with the post hoc correction on the population (3), estimating a predictor Y\u0303 \u2208 P(Y\u0302 ) derived from (Y\u0302 , A) is simply optimization over the following four parameters that completely specify Y\u0303 ,\np\u0303y\u0302a := p\u0303y\u0302a(Y\u0303 ) = P(Y\u0303 = 1 | Y\u0302 = y\u0302, A = a) for y\u0302, a \u2208 {0, 1}. (12)\nIn Section 4.2, we discuss how the post hoc correction step offers statistical advantages over the one-shot approach of using all of the training data for Step 1. Besides these statistical advantages, the addition of the post hoc correction is also motivated from other practical considerations: (a) The derived predictors only need access to samples from P(Y\u0302 , Y, A) and can be deployed without explicit access to predictive features X, and (b) the proposed correction step (11) can be easily optimized using ternary search and can be repeated multiple times as more and more samples from P(Y\u0302 , Y, A) are seen by the system, without having to retrain the classifier from scratch."}, {"heading": "4.2 Statistical guarantees", "text": "In this section, we discuss the statistical properties of the estimators Y\u0302 and Y\u0303 from Step 1 and Step 2, respectively. We define the following notation for succinctly describing the quality of a predictor:\nDefinition 4. Q(L, ) = {h : X \u2192 {0, 1} : \u0393(h) \u2264 ,L(h) \u2264 L} denotes the set of -discriminatory binary predictors with loss L.\nThe following theorem shows the statistical learnability of hypothesis classes H with respect to the best non-discriminatory predictor in H using the two-step framework. In the following results, recall the notation for \u0393(Y\u0302 ) from Definition 3.\nFor y, a \u2208 {0, 1}, let Pya = P(Y = y,A = a) denote the group-outcome probabilities, and let V C(H) denote the Vapnik-Chervonenkis dimension of a hypothesis class H.\nTheorem 1. Let n=\u2126 ( maxya\nlog 1/\u03b4 Pya\n) and the hyperparameters satisfy \u03b1n, \u03b1\u0303n=\u0398 ( maxya \u221a log 1/\u03b4 nPya ) .\nFor a binary hypothesis class H, any distribution P(X,Y,A), and any \u03b4 \u2208 (0, 1/2), if Y \u2217 \u2208 H is a non-discriminatory predictor, then with probability greater than 1\u2212 \u03b4, the output of the two step procedure Y\u0303 satisfies the following for absolute constants C1 and C2,\nL(Y\u0303 ) \u2264 L(Y \u2217) + C1 max ya\n\u221a V C(H) + log 1/\u03b4\nnPya , and \u0393(Y\u0303 ) \u2264 C2 max ya\n\u221a log 1/\u03b4\nnPya .\nThus, with n = \u2126 (\nmaxya V C(H) Pya 2 + maxya 1 Pya\u03b12\n) samples, and an appropriate choice of \u03b1n, \u03b1\u0303n,\nthe two step framework returns Y\u0303 \u2208 Q(L(Y \u2217) + , \u03b1) with high probability.\nThe proof is based on the following two Lemmas. The first is a statistical guarantee on the the loss and non-discrimination after training in Step 1:\nLemma 2. Under the conditions in Theorem 1, w.p. greater than 1\u2212 \u03b4, Y\u0302 from Step 1 satisfies\nL(Y\u0302 ) \u2264 L(Y \u2217) + C1\n\u221a V C(H) + log 1/\u03b4\nn , and \u0393(Y\u0302 ) \u2264 C2 max ya\n\u221a V C(H) + log 1/\u03b4\nnPya .\nNotice that the non-discrimination we can guarantee for Y\u0302 also scales with the complexity of the hypothesis class V C(H). In the second step, we search over a much more restricted space of derived predictors, essentially the convex hull of |A| conditional predictors of Y\u0302 , which allows us to obtain a guarantee on non-discrimination that does not scale with V C(H). The following lemma ensures that if Y\u0302 from Step 1 is approximately non-discriminatory, the correction in the second step does not incur significant additional loss:\nLemma 3. If h is an \u03b1-discriminatory binary predictor h \u2208 Q(L(h), \u03b1), then the optimal 0- discriminatory derived predictor Y\u0303 \u2217(h) from (3) using 0-1 loss satisfies Y\u0303 \u2217(h) \u2208 Q(L(h) + \u03b1, 0).\nThis lemma, along with the examples in Section 2 further motivates an integrated leaning step, such as Step 1, where non-discrimination is explicitly encouraged in training.\nThe following theorem shows that the level of non-discrimination that it is possible to ensure using just the first step of the procedure can grow with the complexity of the hypothesis class.\nTheorem 2. There exists a finite, binary hypothesis class H and a data distribution D such that with probability at least 1/2, the classifier Y\u0302 learned from Step 1 using n samples from D is at least maxy,a 3 log |H|\u22121 5\n4nPya -discriminatory on the population.\nTheorem 2 suggests that the non-discrimination guarantee provided by Lemma 2 has the correct dependence on the problem parameters. The intermediate predictor Y\u0302 from Step 1, without the post hoc correction, can only be guaranteed to be non-discriminatory up to a tolerance term that grows with the complexity of the hypothesis class. On the other hand, when using the two step procedure, the sample complexity of ensuring that the final predictor Y\u0303 is at most \u03b1-discriminatory is \u2126(maxya \u03b1\n\u22122P\u22121ya ) (Theorem 1) which does not depend on the complexity of the hypothesis class, and also matches the sample complexity of merely detecting \u03b1-discrimination from Lemma 1.\nFinally, we note that the sample complexity\u2019s dependence on minya Pya in Theorem 1 is unavoidable for our definition of approximate non-discrimination. If there is a rare group or group-outcome combination, we still need enough samples from that group to ensure that the loss and \u03b3Sya generalize to the popluation for every A = a, Y = y in order to be non-discriminatory. This is the same reason why the dependence on Pya arises in Lemma 1. On the positive side, this bottleneck provides further incentive to actively seek samples and target labels for minority populations, which might otherwise be disregarded if non-discrimination were not a consideration."}, {"heading": "5 Computational Intractability", "text": "The proposed procedure for learning non-discriminatory predictors from a sample is statistically optimal, but it is clearly computationally intractable for almost any interesting hypothesis class\nsince the first step (10) involves minimizing the 0-1 loss. As is typically done with intractable learning problems, we therefore look to alternative loss functions and hypothesis classes in order to find a computationally feasible procedure.\nA natural choice is the hypothesis class of linear predictors with a convex loss function. In this case, we would like to have an efficient algorithm for finding a non-discriminatory predictor that has convex loss that is approximately as good as the loss of the best non-discriminatory linear predictor. However, even in the case of binary A and Y and without the difficulty of optimizing the 0-1 loss, the non-discrimination constraint is extremely strong for real-valued predictors, requiring that the cumulative distribution functions of the predictor conditioned on A = 0 and A = 1 match at every threshold. In fact, the mere existence of a non-trivial (i.e. non-constant) linear predictor that is non-discriminatory requires a relatively special distribution. This is the case even when considering a real-valued analogue of \u03b1-approximate non-discrimination.\nOne could relax the problem one step further with a less restrictive non-discrimination requirement. Consider the class of linear predictors with a convex loss where only the sign of the predictor need be non-discriminatory. Unfortunately, using a result by Daniely [2015] even this is computationally intractable:\nTheorem 3. Let L\u2217 be the loss of the optimal linear predictor whose sign is non-discriminatory. Subject to the assumption that refuting random K-XOR formulas is computationally hard,3 the learning problem of finding a possibly randomized function f such that Lhinge(f) \u2264 L\u2217+ such that sign(f) is \u03b1-discriminatory requires exponential time in the worst case for < 18 and \u03b1 < 1 8 .\nThe proof goes through a reduction from the hardness of improper, agnostic PAC learning of Halfspaces. Given a distribution D over (X,Y ) and the knowledge that there is a linear predictor which achieves 0-1 loss L\u2217 on D, we construct a new distribution D\u0303 over (X\u0303, A\u0303, Y\u0303 ) such that an approximately non-discriminatory predictor with small hinge loss can be used to make accurate predictions on D, even if it is not a linear function. The distribution D\u0303 is identical to the original distribution D when conditioned on A\u0303 = 1, and is supported on only two points conditioned on A\u0303 = 0. The probabilities of the two points are constructed so that satisfying non-discrimination requires making accurate predictions on the A\u0303 = 1 population, and thus on D. In particular, for parameters , \u03b1 < 18 , the predictor will have 0-1 loss at most 15 16L\n\u2217 + 47128 on D, which is bounded away from 12 when L\n\u2217 < 110 . Since Daniely [2015] prove that finding a predictor with accuracy bounded away from 12 is hard in general, we conclude that the learning problem is computationally hard. See Appendix D for a complete proof.\nTo summarize, learning a non-discriminatory binary predictor with the 0-1 loss is hard; learning a real-valued linear predictor with respect to a convex loss function is problematic due to the potential inexistence of a non-trivial non-discriminatory linear predictor; and even requiring only that the sign of the linear predictor be non-discriminatory is computationally hard. A more significant relaxation of non-discrimination is therefore required to arrive at a computationally tractable learning problem."}, {"heading": "6 Relaxing non-discrimination", "text": "Motivated by the hardness result in Section 5, we now proceed to relax the criterion of equalized odds. Previous work by Zafar et al. [2016] addresses a notion of non-discrimination which amounts to relaxing the equalized odds constraint that P(Y\u0302 = y\u0302 | Y = y,A = 0) = P(Y\u0302 = y\u0302 | Y = y,A = 1) to the constraint that E[Y\u0302 | Y = y,A = 0] = E[Y\u0302 | Y = y,A = 1], i.e. the first moments of Y\u0302 must\n3See Daniely [2015] for a description of the problem.\nmatch. They propose optimizing the hinge loss subject to an approximation of this first moment constraint. Their work is primarily applied and gives no learning or non-discrimination guarantees for their learning rule, both of which we address in this section for a different relaxation.\nHere, we seek to identify a more tractable notion of non-discrimination based on second order moments. In particular, we propose the following condition, which under minor conditions is implied by equalized odds. Equalized correlations is generally a weaker condition than equalized odds, but when considering the squared loss and X,A, Y are jointly Gaussian, it is in fact equivalent.\nDefinition 5 (Equalized correlations). We say that a score R satisfies equalized correlations or is second-moment non-discriminatory with respect to the protected attribute A and target Y , if R, A, and Y satisfy the following:\n\u03c3RA\u03c3 2 Y = \u03c3RY \u03c3Y A, (13)\nwhere \u03c3\u03b1\u03b2 = E [(\u03b1\u2212 E[\u03b1])(\u03b2 \u2212 E[\u03b2])] is the covariance between two random variables \u03b1 and \u03b2.\nTheorem 4 (Gaussian non-discrimination). Let X, A, and Y be jointly Gaussian. Then the squared loss optimal (equalized odds) non-discriminatory predictor is linear. Furthermore, for all linear predictors in this setting, non-discrimination is equivalent to satisfying Equation (13).\nTheorem 4 means in particular that in the Gaussian setting under the squared loss, the optimal predictor that is non-discriminatory in the sense of Definition 1 is the same as the optimal predictor that is non-discriminatory in the more relaxed sense of Definition 5. We stress that this is generally not the case for non-Gaussian scenarios and with losses other than the squared loss that may result in non-linear optimal predictors.\nOne may also consider intermediate notions of non-discrimination between equalized odds and equalized correlations. One such option is to require that the conditional covariance \u03c3RA|Y vanishes. We do not elaborate further, except to note that it is immediate that equalized odds implies this to be the case and this in turn generally implies equalized correlations.\nSince linear prediction can be thought of as the hallmark of Gaussian processes, one could therefore justify the relaxation of Definition 5 as being the appropriate notion of non-discrimination when we restrict our predictors to be linear. Linear predictors, especially under kernel transformations, are used in a wide array of applications. They thus form a practically relevant family of predictors where one would like to achieve non-discrimination. Therefore, in the remainder of this section, we develop a theory for non-discriminating linear predictors.\nA particularly attractive feature of equalized correlations is that, for linear predictors, Equation (13) amounts to a linear constraint. With any convex loss `(\u00b7, \u00b7), finding the optimal second-moment non-discriminatory predictor can be written as:\nmin w\nE [ ` ( Y,wT [ X A ])] s.t. wT ( \u03a3[X\nA\n] ,A \u03c32Y \u2212 \u03a3[X\nA\n] ,Y \u03c3Y A\n) = 0\nThis is a convex optimization problem with a single linear constraint, and is thus generally tractable. In what follows, we take X to be a real-valued vector, A to be a scalar, and the target to be binary Y \u2208 {\u00b11}, unless otherwise noted. We also commit to the squared loss (R\u2212Y )2 throughout. Without loss of generality, we assume that X, A, and Y all have zero mean. Recall the definition of the optimal linear predictor :\nR\u0302 = arg min r(x,a)=wT [xa ]\nE[(Y \u2212 r(X,A))2] = r\u0302(X,A), (14)\nwhere one can determine that r\u0302(x, a) = w\u0302T [ xa ] with w\u0302 = E [[ X A ] [XT AT ] ]\u22121 E [[ X A ] Y ]\n= \u03a3\u22121[ X A ] , [ X A ] \u03a3[X A ] ,Y .\nThe optimal linear predictor may, in general, be discriminatory, so we define the optimal secondmoment non-discriminatory linear predictor as follows:\nR? = arg min r(x,a)=wT [xa ] E[(Y \u2212 r(X,A))2] = r?(X,A) s.t. \u03c3r(X,A),A\u03c32Y = \u03c3r(X,A),Y \u03c3Y A (15)\nthus the predictor is constrained to be linear and to satisfy (13), which is a single linear constraint. We can give a closed-form expression for R? (see Section E.2 in Appendix E for details). We have that r?(x, a) = w?T [ xa ], where\nw? = \u03a3\u22121[ X A ] , [ X A ] (\u03a3[X A ] ,Y \u2212 \u03b1v ) where v is a vector encoding the non-discrimination constraint and \u03b1 is a scalar defined as\nv = \u03a3[X A ] ,A \u2212 \u03a3[X A ] ,Y \u03c3Y A/\u03c3 2 Y and \u03b1 =\nvT\u03a3\u22121[ X A ] , [ X A ]\u03a3[X A ] ,Y\nvT\u03a3\u22121[ X A ] , [ X A ]v Written as such, R? is a function of X and A. Nevertheless, it turns out that this optimal non-discriminatory linear predictor can be derived, in the sense of Definition 2, from the optimal (possibly discriminatory) linear predictor R\u0302 of Equation 14 and without access to X individually.\nTheorem 5 (Derived). The second-moment non-discriminatory linear predictor minimizing the squared loss can be derived from the optimal least squares linear predictor R? and the joint (second moment) statistics of (R?, A, Y ). Specifically,\nR? = R\u0302\u2212 \u03b1 ( A+ R\u0302\u03c3Y A/\u03c3 2 Y ) ,\nwith\n\u03b1 = \u03c3Y A + \u03c3R\u0302,Y \u03c3Y A/\u03c3\n2 Y \u03c32A \u2212 (\u03c3Y A)2/\u03c32Y + ( \u03c3 R\u0302,A \u2212 \u03c3 R\u0302,Y \u03c3Y A/\u03c32Y ) \u03c3Y A/\u03c32Y .\nTheorem 5 shows that, as far as the equalized correlation criterion is concerned, there is no penalty for first finding an optimal linear predictor and then correcting it. Consequently, this criterion easily enforceable on existing predictors. Intuitively, one must simply \u201csubtract\u201d any potential correlation one could derive about protected attributes from the prediction score and the outcome, and this can be entirely determined from the statistics of the optimal linear predictor R\u0302, the protected attribute A, and the outcome Y , without the need to know the extended set of attributes X. We emphasize that this result does not rely on any Gaussian assumptions, but simply on the fact that we have limited ourselves to linear predictors and the relaxed notion of non-discrimination. Finally, it is worth mentioning that a two-step procedure, as in Section 4, could be developed also for learning second-moment (approximately) non-discriminatory linear predictors from samples."}, {"heading": "7 Conclusion", "text": "In this work we took the first steps toward a statistical and computational theory of learning non-discriminatory (equalized odds) predictors. We saw that post hoc correction might not be optimal and devised a statistically optimal two-step procedure, after observing that a straightforward\nERM-type approach is not sufficient. Computationally, working with binary non-discrimination is essentially has hard as agnostically learning binary predictors, and so we should expect to have to resort to relaxations. We took the first step to this end in Section 6 where we considered a second moment relaxation of non-discrimination which leads to tractable learning. We hope this will not be the final word on learning non-discriminatory predictors and that this work will spur interest in further understanding our relaxation, suggesting other relaxations, and studying other computationally efficient procedures with provable guarantees."}, {"heading": "A Deferred Proofs From Section 2", "text": ""}, {"heading": "A.1 Proof of Example 1", "text": "We restate the example for convenience:\nExample 1. When the hypothesis class is unconstrained, for any \u2208 (0, 1/4) there exists a distribution D such that a) the optimal non-discriminatory predictor with respect to the 0-1 loss has loss at most 2 but b) the post hoc correction of the unrestricted Bayes optimal predictor has loss at least 0.5.\nA similar statement can also be made about hinge loss. For an unconstrained hypothesis class, for any \u2208 (0, 1/4) and the same distribution D , a) the optimal non-discriminatory predictor with respect to the hinge loss has loss at most 4 but b) the post hoc correction of the Bayes optimal unrestricted predictor has loss at least 1.\nConsider the unconstrained hypothesis class of all (possibly randomized) functions from (X,A) to {0, 1}. Let D be the following distribution over (X,A, Y ), with X,A, Y \u2208 {0, 1}:\nP(Y = 1) = 0.5 P(A = y | Y = y) = 1\u2212 P(X = y | Y = y) = 1\u2212 2 (16)\nThe graphical model representing this distribution is\nX Y A\nClearly, X \u22a5 A | Y , so Y \u2217 = X is non-discriminatory and achieves a 0-1 loss of 2 . This same predictor achieves hinge loss 4 . This predictor, being non-discriminatory, upper bounds the loss of the optimal non-discriminatory predictor with respect to the 0-1 and hinge losses.\nThe optimal predictor with respect to the 0-1 loss, which might be discriminatory, is in the convex hull (i.e. it might be randomized combination) of the sized mappings from {0, 1}\u00d7{0, 1} 7\u2192 {0, 1}. The Bayes optimal predictor with respect to the 0-1 loss is the hypothesis\nh\u0302(x, a) = argmax y\u2208{0,1}\nP(Y = y | X = x,A = a) (17)\nGiven a \u2208 {0, 1}, note that since < 1/4\nP(Y = a | X = a,A = a) = 1 1 + P(A=a | Y=1\u2212a)P(X=a | Y=1\u2212a)P(A=a | Y=a)P(X=a | Y=a)\n= 1\n1 + ( )(2 )(1\u2212 )(1\u22122 )\n> 1\n2 .\n(18)\nSimilarly\nP(Y = a | X = 1\u2212 a,A = a) = 1 1 + P(A=a | Y=1\u2212a)P(X=1\u2212a | Y=1\u2212a)P(A=a | Y=a)P(X=1\u2212a | Y=a)\n= 1\n1 + (1\u22122 )(1\u2212 )(2 )\n> 1\n2 .\n(19)\nTherefore, the Bayes optimal predictor is h\u0302(X,A) = A, which is 1-discriminatory, as\nP(h\u0302(X,A) = 1 | Y = y,A = 0) = 0 but P(h\u0302(X,A) = 1 | Y = y,A = 1) = 1 (20)\nConsider now the post-hoc correction h\u0303 of h\u0302. The best non-discriminatory predictor Y\u0303 derived from the joint distribution (h\u0302, A, Y ) \u2261 (A,A, Y ) is given by the following optimization problem\nY\u0303 = argmin h\nL(h)\ns.t. \u03b3y0(h) = \u03b3y1(h) for y = 0, 1[ \u03b30a(h) \u03b31a(h) ] \u2208 ConvHull ([ 0 0 ] , [ \u03b30a(h\u0302) \u03b31a(h\u0302) ] , [ \u03b30a(1\u2212 h\u0302) \u03b31a(1\u2212 h\u0302) ] , [ 1 1 ]) for a = 0, 1.\n(21)\nThe first constraint requires that the resultant predictor be non-discriminatory. The second requires that the class conditional true positive and false positive rates of the predictor be in the convex hull of the constant 0 predictor, the constant 1 predictor, the predictor h\u0302 and its negative. This constraint is equivalent to requiring that h\u0303 be derived from h\u0302. Because \u03b3y0(h\u0302) = 0 and \u03b3y1(h\u0302) = 1, the second constraint requires the predictor have equal true and false positive rates. As P(Y = 1) = 0.5, 0.5 is optimal true and false positive rate and L(h\u0303) = 0.5.\nUsing the same distribution, but with X,A, Y \u2208 {\u22121, 1} instead of {0, 1}, the optimal nondiscriminatory predictor with respect to the hinge loss is no worse than the predictor which returns X, achieving hinge loss 4 . However, the Bayes optimal predictor with respect to the hinge loss is again that predictor which returns\nh\u0302(x, a) = argmax y\u2208{0,1}\nP(Y = y | X = x,A = a) = a.\nBy the same line of reasoning, the post hoc correction of h\u0302, which must have identical statistics for A = 0 and A = 1 is forced to have equal true and false positive rates, and thus the best derived predictor is identically 0 which has hinge loss 1."}, {"heading": "A.2 Proof of Example 2", "text": "We restate the example for convenience:\nExample 2. Let H be the class of linear predictors with L1 norm at most 12 \u2212 2 , for some \u2208 (2/25, 1/4). Then there exists a distribution D such that a) the optimal non-discriminatory predictor in H with respect to the squared loss has square loss at most 116 + 3 2 + 3\n2, but b) the post hoc correction of the Bayes optimal square loss regressor in H returns a constant predictor which has (trivial) square loss of 1/4.\nSimilarly, for the class H of sparse linear predictors, for any \u2208 (0, 1/4), there exists a distribution D such that a) the optimal non-discriminatory predictor in H with respect to the squared loss has square loss at most 2 \u2212 4 2, but b) the post hoc correction of the Bayes optimal squared loss regressor in H again returns a constant predictor which has (trivial) square loss of 1/4.\nIn this example, we consider the squared loss and the hypothesis class of linear predictors with L1 norm at most 12 \u2212 2 for some \u2208 (2/25, 1/4):\nH = { w1X + w2A+ b : |w1|+ |w2| \u2264 1\n2 \u2212 2\n} .\nWith this parameter , we use the same distribution as in the proof of Theorem 1:\nP(Y = 1) = 0.5 P(A = y | Y = y) = 1\u2212 P(X = y | Y = y) = 1\u2212 2\nSince X \u22a5 A | Y , any linear function of X only will be non-discriminatory and h(X) = (\n1 2 \u2212 2\n) X+\n1 4 + has the required L 1 norm and achieves squared loss 116 + 3 2 + 3 2. This predictor, being nondiscriminatory, upper bounds the squared loss of the optimal non-discriminatory predictor.\nTo derive the optimal potentially discriminatory predictor, it will be useful to begin by calculating the covariances between each of the variables:\nE [X] = E [A] = E [Y ] = 1\n2 (22) E [ X2 ] = E [ A2 ] = E [ Y 2 ] = 1\n2 (23)\nE [XA] = 1 2 \u2212 3 2 + 2 2 (24) E [XY ] = 1\u2212 2\n2 (25)\nE [AY ] = 1\u2212\n2 (26)\nThe optimal predictor optimizes\nh\u0302 = argmin w1,w2,b\nE [ (w1X + w2A+ b\u2212 y)2 ] s.t. |w1|+ |w2| \u2264 1\n2 \u2212 2 .\n(27)\nForming the Lagrangian:\nL(w1, w2, b, \u03bb) = E [ (w1X + w2A+ b\u2212 y)2 ] + \u03bb ( |w1|+ |w2| \u2212 1\n2 \u2212 2 ) = w21E [ X2 ] + w22E [ A2 ] + b2 + E [ Y 2 ] + 2w1w2E [XA] + 2w1bE [X]\n\u2212 2w1E [XY ] + 2w2bE [A]\u2212 2w2E [AY ]\u2212 2bE [Y ] + \u03bb ( |w1|+ |w2| \u2212 1\n2 \u2212 2 ) = w21 + w 2 2 + 1\n2 + b2 + w1w2(1\u2212 3 + 4 2) + w1b\u2212 w1(1\u2212 2 ) + w2b \u2212 w2(1\u2212 )\u2212 b+ \u03bb ( |w1|+ |w2| \u2212 1\n2 \u2212 2\n) . (28)\nAt the following values:\nw1 = 0 w2 = 1 2 \u2212 2 b = 1 4 + \u03bb = 1 4 (29)\nthe subdifferential of L contains 0 for any \u2208 (2/25, 1/4). Looking term by term we have:\n\u2202`\n\u2202w1 = w1 + w2(1\u2212 3 + 4 2) + b\u2212 (1\u2212 2 ) + \u03bbsign(w1) (30)\n=\n( 1\n2 \u2212 2\n) (1\u2212 3 + 4 2) + 1\n4 + \u2212 (1\u2212 2 ) + 1 4 sign(0) (31)\n= 1\n4 sign(0)\u2212 8 3 + 8 2 \u2212 2 \u2212 1 4 , (32)\nwhere sign(0) is an arbitrary value in [\u22121, 1] which is the subdifferential of f(z) = |z| at 0. For any \u2208 (2/25, 1/4) \u2223\u2223\u2223\u2223\u22128 3 + 8 2 \u2212 2 \u2212 14 \u2223\u2223\u2223\u2223 < 14 =\u21d2 0 \u2208 \u2202`\u2202w1 (33)\nFurthermore,\n\u2202`\n\u2202w2 = w1(1\u2212 3 + 4 2) + w2 + b\u2212 (1\u2212 ) + \u03bbsign(w2)\n= 1 2 \u2212 2 + 1 4 + \u2212 (1\u2212 ) + 1 4 = 0,\n(34)\nand\n\u2202` \u2202b = 2b+ w1 + w2 \u2212 1\n= 1\n2 + 2 +\n1 2 \u2212 2 \u2212 1 = 0.\n(35)\nThis proves that h\u0302(X,A) = (\n1 2 \u2212 2 ) A+ 14 + is the Bayes optimal predictor in H with respect\nto the squared loss. Furthermore, the random variable is supported on only two points: 14 + when A = 0 and 34 \u2212 when A = 1. It is clear that h\u0302 is not independent of A conditioned on Y . Since h\u0302 is a deterministic function of A, the post hoc correction h\u0303, which must be independent of A conditioned on Y , is forced to be independent of A, and consequently Y . Thus, h\u0303 \u2261 0.5 is the best possible derived predictor, achieving square loss 1/4.\nConsidering the class of 1-sparse linear predictors, the predictor h(X,A) = (1 \u2212 4 )X + 2 , being conditionally independent of A is non-discriminatory and achieves squared loss 2 \u2212 4 2, upper bounding the loss of the optimal non-discriminatory 1-sparse linear predictor. Without regard for non-discrimination, the optimal hypothesis in the class with respect to the squared loss is h\u0302(X,A) = (1\u2212 2 )A+ . This post hoc correction of this predictor suffers from the same issue as in the bounded norm case, resulting in a predictor that can have squared loss no better than 1/4."}, {"heading": "B Deferred Proofs From Section 3", "text": "Recall the notation \u0393ya(h) = maxy |\u03b3y0(h) \u2212 \u03b3y1(h)| and \u0393Sya(h) = maxy |\u03b3Sy0(h) \u2212 \u03b3Sy1(h)| from Definition 3. To avoid clutter, we sometimes drop the dependence on h for \u03b3ya when h is evident from the context. Also, recall that S = {(xi, yi, ai) : i \u2208 [n]} \u223c Pn(X,Y,A) and Pya = P(Y = y,A = a).\nThe following lemma on the concentration for \u0393S is used in multiple proofs. The lemma is proved in B.2.\nLemma 4. For \u03b4 \u2208 (0, 1/2) and a binary predictor h, if nPya > 2 log 16/\u03b4, the following hold,\nP ( |\u0393(h)\u2212 \u0393S(h)| > 2 max\nya\n\u221a log 16/\u03b4\nnPya\n) \u2264 \u03b4. (36)"}, {"heading": "B.1 Proof of Lemma 1", "text": "Lemma 1 Given n i.i.d. samples S, \u2200\u03b1, \u03b4 > 0, if n > 16 maxya log 16/\u03b4Pya\u03b12 , then with probability greater than 1\u2212 \u03b4, T satisfies,\nT ( Y\u0302 , S, \u03b1 ) =\n{ 0 if Y\u0302 is 0-discriminatory on population\n1 if Y\u0302 is at least \u03b1-discriminatory on population.\nProof. Recall that T (Y\u0302 , S, \u03b1n) = 1 ( \u0393S(Y\u0302 ) > \u03b1n ) . Let \u03b1n be chosen to satisfy,\n2 max ya\n\u221a log 16/\u03b4\nnPya < \u03b1n < \u03b1\u2212 2 max ya\n\u221a log 16/\u03b4\nnPya .\nThen the following results readily follow from Lemma 4, 1. If Y\u0302 is non-discriminatory, i.e. \u0393(Y\u0302 ) = 0 .\nP ( T (Y\u0302 , S, \u03b1n) = 1 ) = P(\u0393S(Y\u0302 ) > \u03b1n) \u2264 P ( \u0393S(Y\u0302 ) > \u0393(Y\u0302 ) + 2 max\nya\n\u221a log 16/\u03b4\nnPya\n) \u2264 \u03b4.\n2. Similarly, suppose a binary predictor is Y\u0302 is at least \u03b1-discriminatory, i.e. \u0393(Y\u0302 ) \u2265 \u03b1, then\nP ( T (Y\u0302 , S, \u03b1n) = 0 ) = P ( \u0393S(Y\u0302 ) \u2264 \u03b1n ) \u2264 P ( \u0393S(Y\u0302 ) \u2264 \u0393(Y\u0302 )\u2212 2 max\nya\n\u221a log 16/\u03b4\nnPya\n) \u2264 \u03b4.\nThus, if n > 16 maxya log 16/\u03b4 Pya\u03b12 , then T (Y\u0302 , S, \u03b12 ) satisfies the conditions of the test in Lemma 1."}, {"heading": "B.2 Proof of Lemma 4", "text": "Proof of Lemma 4. Recall that Pya = P(Y = y,A = a), S = {(Xi, Yi, Ai) : i \u2208 [n]} \u223c Pn(X,Y,A) and nSya = \u2211 i 1(Yi = y,Ai = a). With slight abuse of notation, we define random variables Sya = {i : Yi = y,Ai = a}. We then have \u03b3Sya(h)|Sya = \u2211 j\u2208Sya h(xj)\nnSya \u223c 1 nSya Binomial(\u03b3ya, n S ya) with E[\u03b3Sya|Sya] = \u03b3ya.\nP(|\u03b3Sya \u2212 \u03b3ya| > t) = \u2211 Sya P(|\u03b3Sya \u2212 \u03b3ya| > t|Sya)P(Sya) (a) \u2264 \u2211 Sya 2 exp (\u22122t2nSya)P(Sya)\n\u2264 2P(nSya \u2264 nPya\n2 ) + 2 exp (\u2212t2nPya)P(nSya \u2265 nPya 2 )\n(b) \u2264 2 exp (\u2212nPya/2) + 2 exp (\u2212t2nPya) (c) \u2264 \u03b4 8 + 2 exp (\u2212t2nPya) (37)\nwhere (a) and (b) follow from Hoeffding\u2019s inequality on \u03b3Sya|Sya \u223c Binomial(\u03b3ya, nSya) and nSya \u223c Binomial(n,Pya), respectively, and (c) follows from conditions on nPya in Lemma 4.\nFinally, for y \u2208 {0, 1}, using a series of triangle inequality,\u2223\u2223|\u03b3Sy0 \u2212 \u03b3Sy1| \u2212 |\u03b3y0 \u2212 \u03b3y1|\u2223\u2223 \u2264 |\u03b3Sy0 \u2212 \u03b3Sy1 \u2212 \u03b3y0 + \u03b3y1| \u2264 |\u03b3Sy0 \u2212 \u03b3Sy0|+ |\u03b3Sy1 \u2212 \u03b3y1|, and PS (\u2223\u2223|\u03b3Sy0 \u2212 \u03b3Sy1| \u2212 |\u03b3y0 \u2212 \u03b3y1|\u2223\u2223 > 2t) \u2264 PS(|\u03b3Sy0 \u2212 \u03b3y0|+ |\u03b3Sy1 \u2212 \u03b3y1| > 2t)\n(a) \u2264 PS ( |\u03b3Sy0 \u2212 \u03b3y0| > t ) + PS ( |\u03b3Sy1 \u2212 \u03b3y1| > t ) (b) \u2264 \u03b4\n2 , (38)\nwhere (a) follows from union bound, and (b) follows from (37) using t = maxya \u221a log 16/\u03b4 nPya . The lemma follows from collecting the failure probabilities for y = 0, 1."}, {"heading": "C Deferred Proofs From Section 4", "text": "We use the notation A \u2264\u03b4 B to denote that A \u2264 B holds with probability greater than 1 \u2212 \u03b4. Recall the notation \u0393ya(h) = maxy |\u03b3y0(h) \u2212 \u03b3y1(h)| and \u0393Sya(h) = maxy |\u03b3Sy0(h) \u2212 \u03b3Sy1(h)| from Definition 3. To avoid clutter, we sometimes drop the dependence on h for \u03b3ya when h is evident from the context. Finally, in this section C,C1 and C2 denote absolute constants that are not necessarily the same at each occurrence."}, {"heading": "C.1 Proof of Theorem 1", "text": "Theorem 1. Let n=\u2126 ( maxya\nlog 1/\u03b4 Pya\n) and the hyperparameters satisfy \u03b1n, \u03b1\u0303n=\u0398 ( maxya \u221a log 1/\u03b4 nPya ) .\nFor a binary hypothesis class H, any distribution P(X,Y,A), and any \u03b4 \u2208 (0, 1/2), if Y \u2217 \u2208 H is a non-discriminatory predictor, then with probability greater than 1\u2212 \u03b4, the output of the two step procedure Y\u0303 satisfies the following for absolute constants C1 and C2,\nL(Y\u0303 ) \u2264 L(Y \u2217) + C1 max ya\n\u221a V C(H) + log 1/\u03b4\nnPya , and \u0393(Y\u0303 ) \u2264 C2 max ya\n\u221a log 1/\u03b4\nnPya .\nThus, with n = \u2126 (\nmaxya V C(H) Pya 2 + maxya 1 Pya\u03b12\n) samples, and an appropriate choice of \u03b1n, \u03b1\u0303n,\nthe two step framework returns Y\u0303 \u2208 Q(L(Y \u2217) + , \u03b1) with high probability.\nProof. We begin by stating the following result from Lemma 6 (stated and proved in C.3) which shows the concentration of loss and discrimination in derived predictors: if nPya > 4 log 32/\u03b4, for any randomized predictor h\u0303 derived from Y\u0302 , A, i.e. h\u0303 \u2208 P(Y\u0302 ), satisfies the following:\n\u2223\u2223L(h\u0303)\u2212 LS2(h\u0303)\u2223\u2223 \u2264\u03b4/4 \u221a log 8/\u03b4n , and |\u0393(h\u0303)\u2212 \u0393S2(h\u0303)| \u2264\u03b4/2 2 maxya \u221a 2 log 32/\u03b4 nPya . (39)\nWe also recall from Lemma 3 that if h is an \u03b1-discriminatory binary predictor, the optimum nondiscriminatory derived predictor Y\u0303 \u2217(h) given by (3) satisfies, Y\u0303 \u2217(h) \u2208 Q(L(h)+\u03b1, 0). Additionally, using Lemma 2, we have that \u0393(Y\u0302 ) \u2264\u03b4 C maxya \u221a V C(H)+log 1/\u03b4\nnPya . Thus, using Lemma 2 and Lemma 3, \u2203Y\u0303 \u2217 \u2208 P(Y\u0302 ), such that Y\u0303 \u2217 \u2208 Q ( C maxya \u221a V C(H)+log 1/\u03b4 nPya , 0 ) , and using using the concentration in (39), with high probability\nLS2(Y\u0303 ) \u2264 LS2(Y\u0303 \u2217) \u2264 L(Y\u0303 \u2217) + C \u221a log 8/\u03b4\n2n \u2264 C max ya\n\u221a V C(H) + log 1/\u03b4\nnPya . (40)\nFinally, consider a finite hypothesis class that includes all mappings from (Y\u0302 , A) to binary values, if Y\u0302 and A are both binary. There are 42 = 16 such mappings denoted by Hderived = {h : {0, 1} \u00d7 {0, 1} \u2192 {0, 1}}. Also, recall that the feasible set of (randomized) binary predictors derived solely from P(h, Y,A) is denoted by P(h) and any Y\u0303 \u2208 P(h) is completely specified by {p\u0303 h\u0302,a\n(Y\u0303 ) = P(Y\u0303 = 1|h(X) = h\u0302, A = a) : h\u0302 \u2208 {0, 1}, a \u2208 {0, 1}}. Any such randomized derived predictor Y\u0303 \u2208 P(Y\u0302 ) derived from (Y\u0302 ,A) is in the convex hull of Hderived.\nThus, V C(P(Y\u0302 )) = V C(conv(Hderived)) = log 16 which is a constant. Using this in standard VC dimension uniform bound [Bousquet et al., 2004], we have the following with high probability,\nL(Y\u0303 ) \u2264 LS2(Y\u0303 ) + C1\n\u221a log 1/\u03b4\nn\n(a) \u2264 C max ya\n\u221a V C(H) + log 1/\u03b4\nnPya ,\n\u0393(Y\u0303 ) (b)\n\u2264 \u03b1\u0303n + C2 max ya\n\u221a log 1/\u03b4\nnPya , (41)\nwhere (a) follows from (40), and (b) from (39) and using the fact that V C(P(Y\u0302 ) is constant."}, {"heading": "C.2 Proof of Lemma 2 and Lemma 3", "text": "Lemma 2 Under the conditions in Theorem 1, w.p. greater than 1\u2212 \u03b4, Y\u0302 from Step 1 satisfies\nL(Y\u0302 ) \u2264 L(Y \u2217) + C1\n\u221a V C(H) + log 1/\u03b4\nn , and \u0393(Y\u0302 ) \u2264 C2 max ya\n\u221a V C(H) + log 1/\u03b4\nnPya .\nProof of Lemma 2. Recall that the training data for Step 1 are denoted by S1 = {(xi, ai, yi) : i \u2208 [n/2]} \u223c Pn/2(X,A, Y ) and Pya = P(Y = y,A = a). From Using Hoeffding\u2019s inequality on the empirical 0-1 loss LS1(h), and using concentration results for \u0393S1 from Lemma 4, the following holds for \u03b4 \u2208 (0, 1/2) and \u2200(y, a), nPya > 4 log 32/\u03b4.\n\u2223\u2223L(h)\u2212 LS1(h)\u2223\u2223 \u2264\u03b4/4 \u221a log 8/\u03b4n , and |\u0393(h)\u2212 \u0393S1(h)| \u2264\u03b4/2 2 maxya \u221a 2 log 32/\u03b4 nPya . (42)\nUsing (42) and the standard VC dimension uniform bound [Bousquet et al., 2004], the following holds with high probability for absolute constants C1 and C2,\n|L(Y\u0302 )\u2212 LS1(Y\u0302 )| \u2264\u03b4/4 C1\n\u221a V C(H) + log 1/\u03b4\nn , and\n|\u0393(Y\u0302 )\u2212 \u0393S1(Y\u0302 )| \u2264\u03b4/2 C2 max ya\n\u221a V C(H) + log 1/\u03b4\nnPya .\n(43)\nFinally, from Lemma 5 (stated and proved in C.3), with probability greater than 1 \u2212 \u03b4/4, any 0-discriminatory Y \u2217 \u2208 H is in the feasible set for Step 1 in (10), and thus by optimality of Y\u0302 , LS1(Y\u0302 ) \u2264 LS1(Y \u2217).\nL(Y\u0302 ) \u2264\u03b4/4 LS1(Y\u0302 ) + C1\n\u221a V C(H) + log 1/\u03b4\nn \u2264\u03b4/4 L(Y \u2217) + 2C1\n\u221a V C(H) + log 1/\u03b4\nn ,\n\u0393(Y\u0302 ) \u2264\u03b4/2 \u03b1n + C2 max ya\n\u221a V C(H) + log 1/\u03b4\nnPya . (44)\nThe lemma follows from combining the failure probabilities.\nLemma 3 If h is an \u03b1-discriminatory binary predictor h \u2208 Q(L(h), \u03b1), then the optimal 0- discriminatory derived predictor Y\u0303 \u2217(h) from (3) using 0-1 loss satisfies Y\u0303 \u2217(h) \u2208 Q(L(h) + \u03b1, 0).\nProof of Lemma 3. The intuition is to conservatively bound the true and false positive rates of the non-discriminatory derived predictor using the class conditional rates for h. In the case of binary predictors, Y\u0303 being derived from h is equivalent to requiring that\n] ( \u03b30a(Y\u0303 (h)), \u03b31a(Y\u0303 (h)) ) \u2208 Conv ((0, 0), (1, 1), (\u03b30a(h), \u03b31a(h)), (1\u2212 \u03b30a(h), 1\u2212 \u03b31a(h)))\nIn the figure below, Y\u0303 \u2217(h) is the actual optimal derived non-discriminatory predictor, but we estimate it conservatively using the worse of the class conditional true and false positive rates of h:\nWithout loss of generality, assume \u03b31a(h) \u2265 0.5 and \u03b30a(h) \u2264 0.5 for all a (the hypothesis class is at least as good as chance). Consider the predictor Y\u0303 such that \u2200a \u2208 {0, 1},(\n\u03b30a(Y\u0303 ), \u03b31a(Y\u0303 ) ) = (max(\u03b300, \u03b301),min(\u03b310, \u03b311)) \u2208 Conv ((0, 0), (1, 1), (\u03b30a(h), \u03b31a(h)))\nthat is, Y\u0303 has the greater of the two false positive rates and lesser of the two true positive rates for both classes A = 1 and A = 0. Clearly, this choice of Y\u0303 is both non-discriminatory and derived, thus it is a feasible point for (3). Let \u03b3\u0303y := \u03b3y1(Y\u0303 ) = \u03b3y0(Y\u0303 ), we then have\nE[`01(Y\u0303 \u2217(h))] \u2264 E[`01(Y\u0303 )] = P(Y = 0)\u03b3\u03030 + P(Y = 1)(1\u2212 \u03b3\u03031) = \u2211\na\u2208{0,1}\nP(Y = 0, A = a)\u03b30a(Y\u0303 ) + \u2211\na\u2208{0,1}\nP(Y = 1, A = a)(1\u2212 \u03b31a(Y\u0303 )) (45)\nSince h is \u03b1-discriminatory, for each a \u03b31a(h)\u2212mina\u2032 \u03b31a\u2032(h) = \u03b31a(h)\u2212\u03b31a(Y\u0303 ) < \u03b1 and maxa\u2032 \u03b30a\u2032(h)\u2212 \u03b30a(h) = \u03b30a(Y\u0303 )\u2212 \u03b30a(h) < \u03b1, thus (45) can be upper bounded with\u2211\na\u2208{0,1}\nP(Y = 0, A = a) (\u03b30a(h) + \u03b1) + \u2211\na\u2208{0,1}\nP(Y = 1, A = a)(1\u2212 \u03b31a(h) + \u03b1)\n\u2264 \u03b1+ \u2211\na\u2208{0,1}\nP(Y = 0, A = a)\u03b30a(h) + \u2211\na\u2208{0,1}\nP(Y = 1, A = a)(1\u2212 \u03b31a(h))\n= \u03b1+ E[`01(h)]\n(46)"}, {"heading": "C.3 Supporting Lemmas for Proof of Theorem 1", "text": "Lemma 5. Let HS1\u03b1n = {h \u2208 H : \u0393 S1(h) \u2264 \u03b1n}. If \u2200(y, a), nPya > 4 log 32/\u03b4 and \u03b1n in (10) satisfies \u03b1n \u2265 2 maxya \u221a\n2 log 64/\u03b4 nPya , then with probability greater than 1 \u2212 \u03b44 , for any Y \u2217 \u2208 H \u2229 Q(L(Y \u2217), 0),\nY \u2217 \u2208 HS\u03b1n.\nProof of Lemma 5. Given Y \u2217 \u2208 H \u2229Q(L\u2217, 0),\nP(Y \u2217 /\u2208 HS1\u03b1n) = P(\u0393 S1(Y \u2217) > \u03b1n)\n(a) \u2264 P (\n\u0393S1(Y \u2217) > 2 max ya\n\u221a 2 log 64/\u03b4\nnPya ) \u2264 P ( \u0393S1(Y \u2217) > \u0393(Y \u2217) + 2 max\nya\n\u221a 2 log 64/\u03b4\nnPya\n) (47)\n(b) \u2264 \u03b4 4 , (48)\nwhere (a) follows from the condition on \u03b1n and (b) from Lemma 4.\nLemma 6. For \u03b4 \u2208 (0, 1/2) and h \u2208 H, if n2Pya > 4 log 32/\u03b4, For any randomized predictor Y\u0303 derived from Y\u0302 , A, i.e.Y\u0303 \u2208 P(Y\u0302 ), satisfies the following:\n\u2223\u2223L(Y\u0303 )\u2212 LS2(Y\u0303 )\u2223\u2223 \u2264\u03b4/4 \u221a log 8/\u03b4n , and |\u0393(Y\u0303 )\u2212 \u0393S2(Y\u0303 )| \u2264\u03b4/2 2 maxya \u221a 2 log 32/\u03b4 nPya . (49)\nProof of Lemma 6. The proof essentially follows the same arguments that were used for (42) and Lemma 4.\nFor a randomized predictor Y\u0303 ,\nLS(Y\u0303 )\u2212 L(Y\u0303 ) = 2 n \u2211 i\u2208S2 E Y\u0303 `(Y\u0303 (y\u0302i, ai), yi)\u2212 EX,A,Y EY\u0303 `(Y\u0303 , Y )\nHere LS(Y\u0303 )\u2212L(Y\u0303 ) is merely a sum of n/2, [0, 1] bounded random variables and Hoeffdings bound can be applied to get the required concentration in (39). Similarly, for any randomized predictor Y\u0303 , the conditional random variable \u03b3S2ya (Y\u0303 )|S2ya =\u2211 j\u2208S2ya E Y\u0303 Y\u0303 (y\u0302j ,aj)\nnSya is a sum of [0, 1] bounded random variables with mean E[\u03b3S2ya (Y\u0303 )|S2ya ] = \u03b3ya(Y\u0303 ),\nthe proof of Lemma 4 can be repeated verbatim for the randomized prediction where instead of the Hoeffdings\u2019 bound on Binomial random variables, we use the identical Hoeffdings\u2019 bound for [0, 1] bounded random variables."}, {"heading": "C.4 Proof of Theorem 2", "text": "Let the marginal distribution over (A, Y ) be given with p = mina,y P(A = a, Y = y). Since the definiton of fairness is invariant to re-labelling of A, Y , assume without loss of generality that p corresponds to A = 1, Y = 1. For \u03b1 \u2208 (0, 1/2), the distribution D over (X,A, Y ) \u2208 {0, 1}n \u00d7\n{0, 1} \u00d7 {0, 1} is described by\nP(X1 = y | Y = y) = 1\u2212 \u03b1 P(Xi = 0 | Y = 0, A = 0) = 1 for i = 2, 3, ..., n P(Xi = 1 | Y = 1, A = 0) = 1 for i = 2, 3, ..., n P(Xi = 0 | Y = 0, A = 1) = 1 for i = 2, 3, ..., n P(Xi = 1 | Y = 1, A = 1) = 1\u2212 \u03b1 for i = 2, 3, ..., n\n(50)\nConsider the following hypothesis class H = {hi}ni=1 with hi(X,A) = Xi. The hypothesis h1 has 0-1 loss L01(h1) = P(X1 6= Y ) = \u03b1 and is exactly non-discriminatory since X1 \u22a5 A | Y by construction. For every other i = 2, 3, ..., n, the 0-1 loss of hi is the same:\nL01(hi) = \u2211 y \u2211 a P(Xi = 1\u2212 y | Y = y,A = a)P(Y = y,A = a) = p\u03b1 (51)\nhowever, for these hypotheses |P(hi = 1|Y = 1, A = 1)\u2212 P(hi = 1|Y = 1, A = 0)| = \u03b1 so hi is \u03b1discriminatory.\nWe will now show that on a sample S of size m, the empirical risk minimizer subject to an approximate non-discrimination constraint, h\u0302, will be hi for i 6= 1 with probability 0.5. Hence, the first step alone cannot assure with probability better than 0.5 a classifier that is better than \u03b1-discriminatory.\nFirst, we note that the predictions of hi and hj are independent for i 6= j since Xi and Xj are independent. Therefore, the number of errors made by each classifier hi on the sample S are independent and\nP(LS01(h1) = 0) = (1\u2212 \u03b1)m P(LS01(hi) = 0) = (1\u2212 p\u03b1)m for i = 2, 3, ..., n (52)\nSince a classifier that makes zero errors on S is automatically non-discriminatory on S, if h1 makes at least 1 mistake on S and some hi does not make any errors, then h1 will be the optimum of (10). This event occurs with probability:\nP ( LS01(h1) > 0 \u2227 \u2203i LS01(hi) = 0 ) = (1\u2212 (1\u2212 \u03b1)m) ( 1\u2212 P ( \u2200i > 1 LS01(hi) > 0 )) (53)\n= (1\u2212 (1\u2212 \u03b1)m) ( 1\u2212\nn\u220f i=2 (1\u2212 (1\u2212 p\u03b1)m)\n) (54)\n= (1\u2212 (1\u2212 \u03b1)m) ( 1\u2212 (1\u2212 (1\u2212 p\u03b1)m)n\u22121 )\n(55)\nFrom here, we use that\n\u2200k \u2208 N \u2200x \u2208 [0, 1] (1\u2212 x)k \u2264 1 1 + kx\n(56)\nThus, since 1\u2212 p\u03b1, \u03b1 \u2208 [0, 1]:\nP ( LS01(h1) > 0 \u2227 \u2203i LS01(hi) = 0 ) = (1\u2212 (1\u2212 \u03b1)m) ( 1\u2212 (1\u2212 (1\u2212 p\u03b1)m)n\u22121 ) (57)\n\u2265 (\n1\u2212 1 1 +m\u03b1\n)( 1\u2212 1\n1 + (n\u2212 1)(1\u2212 p\u03b1)m\n) (58)\n= m\u03b1 1 +m\u03b1 \u2212 ( 1\u2212 1 1 +m\u03b1 ) 1 1 + (n\u2212 1)(1\u2212 p\u03b1)m (59)\nThis expression is is greater than 1/2 if the first term is at least 2/3 and the second term is at most 1/6. Thus\nm\u03b1 1 +m\u03b1 \u2265 2 3 \u21d0\u21d2 \u03b1 \u2265 2 m (60)\nand ( 1\u2212 1\n1 +m\u03b1\n) 1\n1 + (n\u2212 1)(1\u2212 p\u03b1)m \u2264 1 6\n\u21d0= 1 1 + (n\u2212 1)(1\u2212 p\u03b1)m \u2264 1 6 \u21d0\u21d2 log 1 1\u2212 p\u03b1 \u2264 log n\u221215 m\n(61)\nSince \u2212 log(1\u2212 x) < x1\u2212x for x \u2208 (0, 1), and p \u2264 1 4 , the expression (59) is at least 1/2 when\n2\nm \u2264 \u03b1 \u2264 3 log n5 4pm\n(62)\nTherefore, when \u03b1 = 3 log n\n5 4pm , with probability 0.5 h1 has non-zero error on S and a different\npredictor has zero error. We conclude that there exists a distribution and hypothesis class such that with probability 0.5, the hypothesis returned by the first step is 3 log n\n5 4pm -discriminatory."}, {"heading": "D Proof of Theorem 3", "text": "Let A be an algorithm that takes as inputs a hypothesis class H, a distribution D\u0303 over (X\u0303, A\u0303, Y\u0303 ) with A\u0303 \u2208 {0, 1} and Y\u0303 \u2208 {\u22121,+1}, an accuracy parameter > 0, and a non-discrimination parameter \u03b1 > 0 and returns a predictor f = A(D\u0303, , \u03b1) such that with probability 1\u2212 \u03b6\nLhinge D\u0303 (f) \u2264 min h\u2208H0-disc(D\u0303) Lhinge D\u0303 (h) + (63)\u2223\u2223\u2223PD\u0303 (f \u2265 0 \u2223\u2223\u2223 Y\u0303 = y, A\u0303 = 0)\u2212 PD\u0303 (f \u2265 0 \u2223\u2223\u2223 Y\u0303 = y, A\u0303 = 1)\u2223\u2223\u2223 \u2264 \u03b1 for y = \u22121,+1 The possibly randomized predictor f need not be in the hypothesis classH, but it is being compared against the best predictor in H whose sign is non-discriminatory.\nWe will show that such an algorithm can be used to improperly weakly learn Halfspace which, subject to the complexity assumption that refuting random K-XOR formulas is hard, was shown to be computationally hard by Daniely [2015]. We conclude that A must be computationally hard to compute.\nThe Halfspace problem is to take a distribution D over (X,Y ) withX \u2208 Rd and Y \u2208 {\u22121,+1}, and find the linear predictor\nh\u2217(x) = sign(w\u2217Tx) where w\u2217 = argmin w\u2208Rd E x,y\u223cD\n[ sign(wTx) 6= y ] (64)\nThe proof of hardenss of the Halfspace problem was shown using a distribution over the unit hypercube in d-dimensions, thus we will assume that D is a bounded distribution. We assume access to the distribution D, knowledge of L01D (h\u2217), and for now, access to the joint distribution of (h\u2217(X), Y ):\n\u03b7\u2212\u2212 = PD(h\u2217(X) = \u22121, Y = \u22121) \u03b7\u2212+ = PD(h\u2217(X) = \u22121, Y = +1) \u03b7+\u2212 = PD(h\u2217(X) = +1, Y = \u22121) \u03b7++ = PD(h\u2217(X) = +1, Y = +1)\n(65)\nhowever, we will show later that it is not necessary to know the \u03b7\u2019s. Since it is always possible to get 0-1 loss at most 12 with a Halfspace predictor, we assume that \u03b7++ + \u03b7\u2212\u2212 \u2265 \u03b7+\u2212 + \u03b7\u2212+ = L01D (h\u2217). From the distribution D we construct a new distribution D\u0303 over (X\u0303, A\u0303, Y\u0303 ) with X\u0303 \u2208 Rd+1, A\u0303 \u2208 {0, 1}, and Y\u0303 \u2208 {\u22121,+1} in the following manner:\nPD\u0303(A\u0303 = 0) = 1\u2212 \u03b4 PD\u0303(A\u0303 = 1) = \u03b4 PD\u0303(X\u0303 = \u2212e1, Y\u0303 = \u22121 | A\u0303 = 0) = \u03b7\u2212\u2212 PD\u0303(X\u0303 = \u2212e1, Y\u0303 = +1 | A\u0303 = 0) = \u03b7\u2212+ PD\u0303(X\u0303 = e1, Y\u0303 = \u22121 | A\u0303 = 0) = \u03b7+\u2212 PD\u0303(X\u0303 = e1, Y\u0303 = +1 | A\u0303 = 0) = \u03b7++ PD\u0303(X\u0303 = [0, x], Y\u0303 = y | A\u0303 = 1) = PD(X = x, Y = y) \u2200x, y\n(66)\nwhere e1 is the first standard basis vector in Rd+1. In other words, when A\u0303 = 1 the distribution D\u0303 is identical to D besides a zero appended to the beginning of X. When A\u0303 = 0, D\u0303 is supported on two points \u2212e1 and e1.\nWe will apply the algorithm A to the distribution D\u0303 with parameters and \u03b1 to be determined later. In this case H is the class of linear predictors so the hinge loss of f on D\u0303 must be competitve with the hinge loss of the best linear predictor whose sign is non-discriminatory.\nUsing the following lemmas, we show that the output of A must have small hinge loss on D\u0303, that this output can then be modified so that it has small 0-1 loss on D\u0303, and finally that it can be further modified to achieve small 0-1 loss on D. The proofs are deferred to the end of this discussion.\nLemma 7. There exists a linear predictor h whose sign is 0-discrminatory such that\nLhinge D\u0303 (h) = 2(1\u2212 \u03b4)L01D (h\u2217) + 2\u03b4\nBy Lemma 7 and the defintion of f from (63),\nLhinge D\u0303 (f) \u2264 min h\u2208H0-disc(D\u0303) Lhinge D\u0303 (h) + \u2264 2(1\u2212 \u03b4)L01D (h\u2217) + 2\u03b4 + (67)\nand the sign of f is \u03b1-discriminatory. Next,\nLemma 8. The predictor f can be efficiently modified to yield a new predictor f \u2032 whose sign is is \u03b1-discriminatory such that\nL01D\u0303 (f \u2032) \u2264 (1\u2212 \u03b4)L01D (h\u2217) + 2\u03b4 +\nFinally,\nLemma 9. The predictor f \u2032\u2032(x) = f \u2032([0, x], 1) achieves L01D (f \u2032\u2032) \u2264 L01D\u0303 (f \u2032) + \u03b1(1\u2212 \u03b4).\nThe predictor f \u2032\u2032 described in Lemma 9 thus has 0-1 loss on D\nL01D (f \u2032\u2032) \u2264 (1\u2212 \u03b4)(L01D (h\u2217) + \u03b1) + 2\u03b4 + (68)\nTheorem 1.3 from Daniely [2015] proves that there is no algorithm running in time polynomial in the dimension d that can return a predictor achieving 0-1 error \u2264 12 \u2212 d \u2212c with high probability\nfor a constant c > 0 for an arbitrary distribution, even with the knowledge that L01D (h\u2217) \u2264 L\u2217 for L\u2217 < 1/2. Thus, A(D\u0303, , \u03b1) cannot run in time polynomial in the dimension d for any parameters L01D (h\u2217), , \u03b1, and \u03b4 such that (68) is greater than 1 2 \u2212 d\n\u2212c for any c > 0. For , \u03b1 < 18 , and setting \u03b4 = 116 , (68) shows that\nL01D (f \u2032\u2032) \u2264 15 16 L01D (h\u2217) + 47 128 (69)\nFor any L\u2217 < 110 this is at most 1 2 \u2212 1 40 and does not depend on the dimension.\nIn this proof we assumed knowledge of the parameters \u03b7 which describe the conditional error rates of h\u2217. If a polynomial time algorithm for A existed, then it would be possible to perform two-dimensional grid search over the \u03b7. Calls made to A with the incorrect values of \u03b7 might result in very inaccurate or discriminatory predictors, but using an estimate of \u03b7 up to O(\u03b1) accuracy is sufficient to approximate the Halfspaces solution using A. Thus at most O(log2(1/\u03b1)) calls to the polynomial time algorithm would be needed. Therefore, in order for A to guarantee for an arbitrary D\u0303 that its output would have excess hinge loss at most 18 and its sign would be at most 1 8 -discriminatory, it must run in time super-polynomial in the dimension in the worst case."}, {"heading": "D.1 Deferred proofs", "text": "Proof of Lemma 7. Define h(X,A) = sign ([ 1 w\u2217 ]T X ) . Then h(\u2212e1, 0) = \u22121, h(e1, 0) = 1, and h([0, x], 1) = h\u2217(x), where h\u2217 is as defined in (64). Since the sign function is invariant to scaling, L01D (h\u2217) = L01D (ch\u2217) for any c > 0.\nTheorem 1.3 in Daniely [2015] involves a distribution D that is supported on the unit hypercube in Rd, thus the predictor h\u2217\u2016w\u2217\u20162 \u221a d \u2208 [\u22121, 1] with probability 1, and has the same 0-1 loss as h\u2217.\nBy the definition of D\u0303, \u03b7+\u2212, and \u03b7\u2212+:\nPD\u0303(h \u2265 0 | Y\u0303 = \u22121, A\u0303 = 0) = \u03b7+\u2212\nPD\u0303(Y\u0303 = \u22121 | A\u0303 = 0)\n= \u03b7+\u2212\nPD(Y = \u22121) = PD(h\u2217 \u2265 0 | Y = \u22121)\n= PD\u0303(h \u2265 0 | Y\u0303 = \u22121, A\u0303 = 1)\n(70)\nPD\u0303(h < 0 | Y\u0303 = 1, A\u0303 = 0) = \u03b7\u2212+\nPD\u0303(Y\u0303 = 1 | A\u0303 = 0)\n= \u03b7\u2212+\nPD(Y = 1) = PD(h\u2217 < 0 | Y = 1)\n= PD\u0303(h < 0 | Y\u0303 = 1, A\u0303 = 1)\n(71)\nTherefore, h is 0-discriminatory at threshold 0. Also\nLhinge D\u0303 (h) = [1 + h(x1)]+PD\u0303(X\u0303 = x1, A\u0303 = 0, Y\u0303 = \u22121) (72)\n+[1\u2212 h(x0)]+PD\u0303(X\u0303 = x0, A\u0303 = 0, Y\u0303 = 1)\n+ \u222b X [1 + h(x)]+PD\u0303(X\u0303 = x, A\u0303 = 1, Y\u0303 = \u22121)dx\n+ \u222b X [1\u2212 h(x)]+PD\u0303(X\u0303 = x, A\u0303 = 1, Y\u0303 = 1)dx\n\u2264 2\u03b7+\u2212PD\u0303(A\u0303 = 0) + 2\u03b7\u2212+PD\u0303(A\u0303 = 0) (73)\n+2 \u222b X ( PD\u0303(X\u0303 = x, A\u0303 = 1, Y\u0303 = \u22121) + PD\u0303(X\u0303 = x, A\u0303 = 1, Y\u0303 = 1) ) dx\n= 2PD\u0303(A\u0303 = 0)(\u03b7+\u2212 + \u03b7\u2212+) + 2PD\u0303(A\u0303 = 1) (74) = 2(1\u2212 \u03b4)(\u03b7\u2212+ + \u03b7+\u2212) + 2\u03b4 (75)\nProof of Lemma 8. Since only the sign of f is required to be \u03b1-discriminatory, we can modify the magnitude of its predictions without affecting its level of non-discrimination. Therefore, we first truncate the output of f to lie in the range [\u22121, 1], which can only reduce the hinge loss.\nIgnoring for the moment that the sign of f must be \u03b1-discriminatory, we would like to define f \u2032 so that f \u2032(\u2212e1, 0) = \u22121 and f \u2032(e1, 0) = 1 with probability 1. In this case, the hinge loss when A\u0303 = 0 is exactly 2(\u03b7+\u2212 + \u03b7\u2212+). With that being said, any modification to f that changes the distribution of the sign of the predictor risks rendering it more than \u03b1-discriminatory. With this in mind, we construct f \u2032 such that\nf \u2032(\u2212e1, 0) = { \u22121 w.p. P(f(\u2212e1, 0) < 0) 0 w.p. P(f(\u2212e1, 0) \u2265 0)\nf \u2032(e1, 0) = { 1 w.p. P(f(e1, 0) \u2265 0) \u22120 w.p. P(f(e1, 0) < 0)\nf \u2032([0, x], 1) = f([0, x], 1)\n(76)\nwhere \u22120 is a negative number of arbitrarily small magnitude. Constructed this way, the distribution of the sign of f \u2032 conditioned on A\u0303 is identical to that of f , meaning that the sign of f \u2032 is \u03b1-discriminatory.\nThe hinge loss of f \u2032 is an upper bound on the 0-1 loss, and in order to show that f \u2032 achieves small 0-1 loss, we will show that the hinge loss is a loose upper bound. The construction of D\u0303 and the predictions of f \u2032 conditioned on A\u0303 = 0 creates a substantial gap between the losses.\nNotice that when f \u2032 makes a prediction of magnitude 1 that has the correct sign, both the hinge loss and the 0-1 loss evaluate to 0. Similarly, when f \u2032 makes a prediction of magnitude 0 with the incorrect sign, both losses are 1. Thus in each of these cases, the hinge loss is equivalent to the 0-1 loss.\nHowever, if f \u2032 makes a prediction of magnitude 1 with the incorrect sign, the hinge loss is 2 but the 0-1 loss is only 1, and when f \u2032 makes a prediction of magnitude 0 with the correct sign, the hinge loss is 1 but the 0-1 loss is 0. Consequently, in each of these cases there is a gap of 1 between the hinge and 0-1 losses. Thus,\nE [ `hinge(f \u2032)\u2212 `01(f \u2032) \u2223\u2223\u2223 A\u0303 = 0] = P(|f \u2032| = 1, sign(f \u2032) 6= Y\u0303 \u2223\u2223\u2223 A\u0303 = 0) (77) + P ( |f \u2032| = 0, sign(f \u2032) = Y\u0303\n\u2223\u2223\u2223 A\u0303 = 0) Considering each term separately:\nP ( |f \u2032| = 1, sign(f \u2032) 6= Y\u0303 \u2223\u2223\u2223 A\u0303 = 0) = P ( f \u2032(\u2212e1, 0) = \u22121, Y\u0303 = 1\n\u2223\u2223\u2223 X\u0303 = \u2212e1, A\u0303 = 0)P(X\u0303 = \u2212e1 \u2223\u2223\u2223 A\u0303 = 0) (78) + P ( f \u2032(e1, 0) = 1, Y\u0303 = \u22121\n\u2223\u2223\u2223 X\u0303 = e1, A\u0303 = 0)P(X\u0303 = e1 \u2223\u2223\u2223 A\u0303 = 0) = P ( f \u2032(\u2212e1, 0) = \u22121 ) P ( X\u0303 = \u2212e1, Y\u0303 = 1\n\u2223\u2223\u2223 A\u0303 = 0) (79) + P ( f \u2032(e1, 0) = 1 ) P ( X\u0303 = e1, Y\u0303 = \u22121\n\u2223\u2223\u2223 A\u0303 = 0) = P (f(\u2212e1, 0) < 0) \u03b7\u2212+ + P (f(e1, 0) \u2265 0) \u03b7+\u2212 (80)\nWith (79) following from the fact that conditioned on X\u0303 and A\u0303 = 0, f \u2032(X\u0303, 0) is a random variable\nthat is independent of the value of Y\u0303 . Similarly, P ( |f \u2032(X\u0303, 0)| = 0, sign(f \u2032(X\u0303, 0)) = Y\u0303 \u2223\u2223\u2223 A\u0303 = 0) = P ( f \u2032(\u2212e1, 0) = 0, Y\u0303 = 1\n\u2223\u2223\u2223 X\u0303 = \u2212e1, A\u0303 = 0)P(X\u0303 = \u2212e1 \u2223\u2223\u2223 A\u0303 = 0) (81) + P ( f \u2032(e1, 0) = \u22120, Y\u0303 = \u22121\n\u2223\u2223\u2223 X\u0303 = e1, A\u0303 = 0)P(X\u0303 = e1 \u2223\u2223\u2223 A\u0303 = 0) = P ( f \u2032(\u2212e1, 0) = 0 ) P ( X\u0303 = \u2212e1, Y\u0303 = 1\n\u2223\u2223\u2223 A\u0303 = 0) (82) + P ( f \u2032(e1, 0) = \u22120 ) P ( X\u0303 = e1, Y\u0303 = \u22121\n\u2223\u2223\u2223 A\u0303 = 0) = P (f(\u2212e1, 0) \u2265 0) \u03b7\u2212+ + P (f(e1, 0) < 0) \u03b7+\u2212 (83)\nCombining (77) with (80) and (83), we see that E [ `hinge(f \u2032)\u2212 `01(f \u2032) \u2223\u2223\u2223 A\u0303 = 0] = P (f(\u2212e1, 0) < 0) \u03b7\u2212+ + P (f(e1, 0) \u2265 0) \u03b7+\u2212 (84) + P (f(\u2212e1, 0) \u2265 0) \u03b7\u2212+ + P (f(e1, 0) < 0) \u03b7+\u2212\n= \u03b7\u2212+ + \u03b7+\u2212 (85)\nThe 0-1 loss of f \u2032 can be decomposed as\nL01D\u0303 (f \u2032) = P(A\u0303 = 0)E\n[ `01(f) \u2223\u2223\u2223 A\u0303 = 0]+ P(A\u0303 = 1)E [`01(f) \u2223\u2223\u2223 A\u0303 = 1] (86) By (85),\nP(A\u0303 = 0)E [ `01(f) \u2223\u2223\u2223 A\u0303 = 0] = (1\u2212 \u03b4)(E [`hinge(f) \u2223\u2223\u2223 A\u0303 = 0]\u2212 \u03b7\u2212+ \u2212 \u03b7+\u2212) (87) and since the hinge loss is always an upper bound on the 0-1 loss\nP(A\u0303 = 1)E [ `01(f) \u2223\u2223\u2223 A\u0303 = 1] \u2264 \u03b4E [`hinge(f) \u2223\u2223\u2223 A\u0303 = 1] (88) From Lemma 7, the hinge loss of f \u2032 (which is at most the hinge loss of f) is upper bounded by 2(1\u2212 \u03b4)(\u03b7\u2212+ + \u03b7+\u2212) + 2\u03b4 + . Thus,\nL01D\u0303 (f \u2032) \u2264 (1\u2212 \u03b4)\n( E [ `hinge(f) \u2223\u2223\u2223 A\u0303 = 0]\u2212 \u03b7\u2212+ \u2212 \u03b7+\u2212)+ \u03b4E [`hinge(f) \u2223\u2223\u2223 A\u0303 = 1] (89) = Lhinge\nD\u0303 (f \u2032)\u2212 (1\u2212 \u03b4)(\u03b7\u2212+ + \u03b7+\u2212) (90) \u2264 (1\u2212 \u03b4)(\u03b7\u2212+ + \u03b7+\u2212) + 2\u03b4 + (91)\nProof of Lemma 9. Because f \u2032 is \u03b1-discriminatory at threshold 0\u2223\u2223\u2223PD\u0303(f \u2032 \u2265 0 | Y\u0303 = \u22121, A\u0303 = 0)\u2212 PD\u0303(f \u2032 \u2265 0 | Y\u0303 = \u22121, A\u0303 = 1)\u2223\u2223\u2223 \u2264 \u03b1\u2223\u2223\u2223PD\u0303(f \u2032 < 0 | Y\u0303 = 1, A\u0303 = 0)\u2212 PD\u0303(f \u2032 < 0 | Y\u0303 = 1, A\u0303 = 1)\u2223\u2223\u2223 \u2264 \u03b1 (92)\nLet f \u2032\u2032(x) = f \u2032([0, x], 1), then\nL01D\u0303 (h) = PD\u0303(Y\u0303 = \u22121, A\u0303 = 0)PD\u0303(f \u2032 \u2265 0 | Y\u0303 = \u22121, A\u0303 = 0) (93)\n+ PD\u0303(Y\u0303 = 1, A\u0303 = 0)PD\u0303(f \u2032 < 0 | Y\u0303 = 1, A\u0303 = 0) + PD\u0303(Y\u0303 = \u22121, A\u0303 = 1)PD\u0303(f \u2032 \u2265 0 | Y\u0303 = \u22121, A\u0303 = 1) + PD\u0303(Y\u0303 = 1, A\u0303 = 1)PD\u0303(f \u2032 < 0 | Y\u0303 = 1, A\u0303 = 1)\n\u2265 PD\u0303(Y\u0303 = \u22121, A\u0303 = 0) ( PD\u0303(f \u2032 \u2265 0 | Y\u0303 = \u22121, A\u0303 = 1)\u2212 \u03b1 )\n(94) + PD\u0303(Y\u0303 = 1, A\u0303 = 0) ( PD\u0303(f \u2032 < 0 | Y\u0303 = 1, A\u0303 = 1)\u2212 \u03b1 ) + PD\u0303(Y\u0303 = \u22121, A\u0303 = 1)PD\u0303(f \u2032 \u2265 0 | Y\u0303 = \u22121, A\u0303 = 1) + PD\u0303(Y\u0303 = 1, A\u0303 = 1)PD\u0303(f \u2032 < 0 | Y\u0303 = 1, A\u0303 = 1)\n= ( PD\u0303(Y\u0303 = \u22121, A\u0303 = 0) + PD\u0303(Y\u0303 = \u22121, A\u0303 = 1) ) PD\u0303(f \u2032 \u2265 0 | Y\u0303 = \u22121, A\u0303 = 1) (95)\n+ ( PD\u0303(Y\u0303 = 1, A\u0303 = 0) + PD\u0303(Y\u0303 = 1, A\u0303 = 1) ) PD\u0303(f \u2032 < 0 | Y\u0303 = 1, A\u0303 = 1)\n+ ( PD\u0303(Y\u0303 = \u22121, A\u0303 = 0) + PD\u0303(Y\u0303 = 1, A\u0303 = 0) ) (\u2212\u03b1)\n= PD\u0303(Y\u0303 = \u22121)PD\u0303(f \u2032 \u2265 0 | Y\u0303 = \u22121, A\u0303 = 1) (96)\n+ PD\u0303(Y\u0303 = 1) PD\u0303(f \u2032 < 0 | Y\u0303 = 1, A\u0303 = 1)\u2212 \u03b1PD\u0303(A\u0303 = 0) (97) = PD(Y = \u22121)PD(f \u2032\u2032 \u2265 0 | Y = \u22121) (98) + PD(Y = 1)PD(f \u2032\u2032 < 0 | Y = 1)\u2212 \u03b1(1\u2212 \u03b4) = L01D (f \u2032\u2032)\u2212 \u03b1(1\u2212 \u03b4) (99)\nThe lemma follows immediately."}, {"heading": "E Proofs for Section 6 - Relaxing non-discrimination", "text": "We use \u03a3\u00b7 to denote covariances involving a vector and we reserve \u03c3\u00b7 for scalar covariances."}, {"heading": "E.1 Proof of Theorem 4", "text": "First, let us show the second claim. Let R be any linear predictor. By linearity, it follows that (R,A, Y ) are jointly Gaussian. By the conditional covariance formula, we have:\n\u03c3RA|Y = \u03c3RA \u2212 \u03c3RY \u03c3Y A/\u03c32Y .\nIf R satisfies equalized correlations, then the right-hand side here is 0. It follows that R and A are uncorrelated conditionally on Y . But since they are jointly Gaussian, they are also independent conditionally on Y . Therefore R also satisfies equalized odds non-discrimination. The converse also holds: if R satisfies equalized odds, then R and A are uncorrelated given Y , and therefore equalized correlations is satified.\nNow let us move back to the main claim. Assume, without loss of generality, that all variables are centered. Let us first find the optimal (a priori not necessarily linear) predictor that satisfies the relaxed second-moment non-discrimination criterion. In particular, the Lagrangian to minimize may be written as:\nE[(R\u2212 Y )2]\u2212 \u03bb ( \u03c32Y E[RA]\u2212 \u03c3AY E[RY ] ) ,\nBut just like in the unconstrained least squares problem, we may apply the law of total expectatons to condition the loss and the R- terms in the second-moment non-discrimination constraint to be conditioned on X and A. Thus the optimum is achieved for each X,A by minimizing the following Lagrangian:\nE[(R\u2212 Y )2|X,A]\u2212 \u03bb ( \u03c32YAR\u2212 \u03c3AY E[Y |X,A]R ) ,\nor equivalently R2 \u2212 2E[Y |X,A]R\u2212 \u03bb ( \u03c32YAR\u2212 \u03c3AY E[Y |X,A]R ) . It follows that the optimal R is a linear function of A, E[Y |X,A] and \u03bb. \u03bb is determined over the statistics of the problem, and therefore it is a constant that does not depend on specific values of X and A, and E[Y |X,A] in the Gaussian setting is linear. It thus follows that the optimum, let\u2019s call it R\u25e6, is a linear function of X and A. It minimizes the expected square loss subject to a relaxed non-discrimination criterion, therefore it is not larger than the optimizer under the stricter constraint. Conversely, by linearity it does also satisfy the stricter constraint, and is thus no smaller than the optimizer under that constraint (recall that we didn\u2019t start out by imposing linearity). Therefore R\u25e6 is precisely the squared loss optimal equalized odds non-discriminatory predictor.\nE.2 Optimal equalized correlations linear predictor\nWe write the proofs more generally for a vector-valued protected attribute A, and the scalar case follows directly. In this case v is a matrix and \u03b1 is a vector mixing the columns of v. First note that condition (13) translates into a linear constraint on w in the least-squares problem of Equation (15). Using the bilinearity of the covariance, this constraint is equivalent to:\nwT\u03a3[X A ] ,A \u2212 wT\u03a3[X A ] ,Y \u03a3Y,A/\u03c3 2 Y \u2261 wTv = 0.\nWe can now write the cost function with a vector of Lagrange multipliers: J(w, \u03bb) = E[(Y \u2212 wT [ X A ] )2] + wTv\u03bb,\nwhose gradient is \u2207wJ = 2\u03a3[X\nA ] , [ X A ]w \u2212 2\u03a3[X A ] ,Y + v\u03bb.\nSetting this to zero, the optimality conditions give us the claimed functional form. The vector \u03b1 can then be obtained by enforcing the constraint:\nwTv = ( \u03a3 Y, [ X A ] \u2212 \u03b1T vT ) \u03a3\u22121[ X A ] , [ X A\n]v = 0 and thus\nvT\u03a3\u22121[ X A ] , [ X A ]v\u03b1 = vT\u03a3\u22121[ X A ] , [ X A ]\u03a3[X A ] ,Y ."}, {"heading": "E.3 Proof of Theorem 5", "text": "First, let us rewrite R? as: R? = w?T [ X A ] = w\u0302T [ X A ] \u2212 \u03b1TvT\u03a3\u22121[\nX A ] , [ X A\n] [X A ] = R\u0302\u2212 \u03b1TvT\u03a3\u22121[\nX A ] , [ X A\n] [X A ]\nNext, recall that v = \u03a3[X\nA\n] ,A \u2212 \u03a3[X\nA\n] ,Y \u03a3Y,A/\u03c3 2 Y ,\nand since\n\u03a3[X A ] , [ X A\n] = [\u03a3 X, [ X A\n] \u03a3 A, [ X A\n] ] \u21d2 \u03a3\nA, [ X A ]\u03a3\u22121[ X A ] , [ X A ] = [ 0dim(A)\u00d7dim(X) Idim(A) ] , we have\nvT\u03a3\u22121[ X A ] , [ X A ] = [ 0 I ] + \u03a3A,Y w\u0302T /\u03c32Y . Therefore\nvT\u03a3\u22121[ X A ] , [ X A\n] [X A ] = A+ R\u0302\n\u03c32Y \u03a3A,Y\nand can be derived from (R\u0302, A, Y ). Then multiplying from the right by v and using the bilinearity of the covariance, we get the terms in \u03b1:\nvT\u03a3\u22121[ X A ] , [ X A ]v = \u03a3A,A \u2212 1\u03c32Y \u03a3A,Y \u03a3Y,A + 1\u03c32Y \u03a3A,Y ( \u03a3 R\u0302,A \u2212 1 \u03c32Y \u03a3 R\u0302,Y \u03a3Y,A ) ,\nand vT\u03a3\u22121[ X A ] , [ X A ]\u03a3[X A ] ,Y = \u03a3A,Y + 1 \u03c32Y \u03a3A,Y \u03a3R\u0302,Y .\nThis shows that \u03b1 also derives from (R\u0302, A, Y ) as stated, completing the proof."}], "references": [{"title": "Introduction to statistical learning theory", "author": ["Olivier Bousquet", "St\u00e9phane Boucheron", "G\u00e1bor Lugosi"], "venue": "In Advanced lectures on machine learning,", "citeRegEx": "Bousquet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2004}, {"title": "Complexity theoretic limitations on learning halfspaces", "author": ["Amit Daniely"], "venue": "arXiv preprint arXiv:1505.05800,", "citeRegEx": "Daniely.,? \\Q2015\\E", "shortCiteRegEx": "Daniely.", "year": 2015}, {"title": "From average case complexity to improper learning complexity", "author": ["Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Daniely et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2014}, {"title": "Equality of opportunity in supervised learning", "author": ["Moritz Hardt", "Eric Price", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hardt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2016}, {"title": "Big data: A report on algorithmic systems, opportunity, and civil rights. 2016", "author": ["White House"], "venue": "URL https://obamawhitehouse.archives.gov/sites/default/files/microsites/", "citeRegEx": "House.,? \\Q2016\\E", "shortCiteRegEx": "House.", "year": 2016}, {"title": "Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment", "author": ["Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez-Rodriguez", "Krishna P. Gummadi"], "venue": "CoRR, abs/1610.08452,", "citeRegEx": "Zafar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zafar et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "We consider learning a predictor which is non-discriminatory with respect to a \u201cprotected attribute\u201d according to the notion of \u201cequalized odds\u201d proposed by Hardt et al. [2016]. We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally.", "startOffset": 157, "endOffset": 177}, {"referenceID": 3, "context": "The particular notion of non-discrimination we consider here is \u201cequalized odds\u201d, recently presented and studied by Hardt et al. [2016]:", "startOffset": 116, "endOffset": 136}, {"referenceID": 3, "context": "Informally, we require that even if the correct label Y provides information about the protected attribute A, if we already know Y , the prediction does not provide any additional information about See Hardt et al. [2016] for a discussion on why it might be necessary for a non-discriminatory predictor to use A", "startOffset": 202, "endOffset": 222}, {"referenceID": 3, "context": "See Hardt et al. [2016] for further discussion of the definition, its implications, and comparisons to alternative notions.", "startOffset": 4, "endOffset": 24}, {"referenceID": 3, "context": "One possible approach to learning a non-discriminative predictor is post hoc correction [Hardt et al., 2016]: first learn a good predictor ignoring non-discrimination, i.", "startOffset": 88, "endOffset": 108}, {"referenceID": 3, "context": "When the protected attribute A and the target Y are both binary, the post hoc correction algorithm proposed by Hardt et al. [2016] can be applied to a binary or real-valued predictor \u0176 \u2208 H, deriving a randomized binary predictor that is non-discriminatory.", "startOffset": 111, "endOffset": 131}, {"referenceID": 3, "context": "1 Hardt et al. [2016]] A predictor \u1ef8 is derived from a random variable R and protected attribute A if it is a possibly randomized function of (R,A) alone.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "For binary classification problems, the optimal post hoc correction \u1ef8 for a binary or real valued predictor \u0176 \u2208 R is a straightforward ternary optimization problem: it is simply the nondiscriminatory, derived, binary predictor that minimizes the expectation of loss ` (Hardt et al. [2016]): \u1ef8 = argmin f :R\u00d7{0,1}7\u2192{0,1} E ` ( f(\u0176 , A), Y )", "startOffset": 269, "endOffset": 289}, {"referenceID": 3, "context": "Indeed, Hardt et al. [2016] show that when the target Y is binary, if we can first find a predictor R that is exactly or nearly Bayes optimal for the squared loss over an unconstrained hypothesis class, then applying the post hoc correction (3) using the 0-1 loss (i.", "startOffset": 8, "endOffset": 28}, {"referenceID": 3, "context": "The above optimization problem is a finite sample adaptation of the post hoc correction in (3) proposed by Hardt et al. [2016]. As with the post hoc correction on the population (3), estimating a predictor \u1ef8 \u2208 P(\u0176 ) derived from (\u0176 , A) is simply optimization over the following four parameters that completely specify \u1ef8 ,", "startOffset": 107, "endOffset": 127}, {"referenceID": 1, "context": "Unfortunately, using a result by Daniely [2015] even this is computationally intractable: Theorem 3.", "startOffset": 33, "endOffset": 48}, {"referenceID": 1, "context": "Unfortunately, using a result by Daniely [2015] even this is computationally intractable: Theorem 3. Let L\u2217 be the loss of the optimal linear predictor whose sign is non-discriminatory. Subject to the assumption that refuting random K-XOR formulas is computationally hard,3 the learning problem of finding a possibly randomized function f such that Lhinge(f) \u2264 L\u2217+ such that sign(f) is \u03b1-discriminatory requires exponential time in the worst case for < 1 8 and \u03b1 < 1 8 . The proof goes through a reduction from the hardness of improper, agnostic PAC learning of Halfspaces. Given a distribution D over (X,Y ) and the knowledge that there is a linear predictor which achieves 0-1 loss L\u2217 on D, we construct a new distribution D\u0303 over (X\u0303, \u00c3, \u1ef8 ) such that an approximately non-discriminatory predictor with small hinge loss can be used to make accurate predictions on D, even if it is not a linear function. The distribution D\u0303 is identical to the original distribution D when conditioned on \u00c3 = 1, and is supported on only two points conditioned on \u00c3 = 0. The probabilities of the two points are constructed so that satisfying non-discrimination requires making accurate predictions on the \u00c3 = 1 population, and thus on D. In particular, for parameters , \u03b1 < 1 8 , the predictor will have 0-1 loss at most 15 16L \u2217 + 47 128 on D, which is bounded away from 12 when L \u2217 < 1 10 . Since Daniely [2015] prove that finding a predictor with accuracy bounded away from 1 2 is hard in general, we conclude that the learning problem is computationally hard.", "startOffset": 33, "endOffset": 1399}, {"referenceID": 4, "context": "Previous work by Zafar et al. [2016] addresses a notion of non-discrimination which amounts to relaxing the equalized odds constraint that P(\u0176 = \u0177 | Y = y,A = 0) = P(\u0176 = \u0177 | Y = y,A = 1) to the constraint that E[\u0176 | Y = y,A = 0] = E[\u0176 | Y = y,A = 1], i.", "startOffset": 17, "endOffset": 37}, {"referenceID": 1, "context": "the first moments of \u0176 must See Daniely [2015] for a description of the problem.", "startOffset": 32, "endOffset": 47}, {"referenceID": 0, "context": "Using this in standard VC dimension uniform bound [Bousquet et al., 2004], we have the following with high probability,", "startOffset": 50, "endOffset": 73}, {"referenceID": 0, "context": "Using (42) and the standard VC dimension uniform bound [Bousquet et al., 2004], the following holds with high probability for absolute constants C1 and C2,", "startOffset": 55, "endOffset": 78}, {"referenceID": 1, "context": "We will show that such an algorithm can be used to improperly weakly learn Halfspace which, subject to the complexity assumption that refuting random K-XOR formulas is hard, was shown to be computationally hard by Daniely [2015]. We conclude that A must be computationally hard to compute.", "startOffset": 214, "endOffset": 229}, {"referenceID": 1, "context": "3 from Daniely [2015] proves that there is no algorithm running in time polynomial in the dimension d that can return a predictor achieving 0-1 error \u2264 12 \u2212 d \u2212c with high probability", "startOffset": 7, "endOffset": 22}, {"referenceID": 1, "context": "3 in Daniely [2015] involves a distribution D that is supported on the unit hypercube in Rd, thus the predictor h\u2217 \u2016w\u2217\u20162 \u221a d \u2208 [\u22121, 1] with probability 1, and has the same 0-1 loss as h\u2217.", "startOffset": 5, "endOffset": 20}], "year": 2017, "abstractText": "We consider learning a predictor which is non-discriminatory with respect to a \u201cprotected attribute\u201d according to the notion of \u201cequalized odds\u201d proposed by Hardt et al. [2016]. We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally. We show that a post-hoc correction approach, as suggested by Hardt et al, can be highly suboptimal, present a nearly-optimal statistical procedure, argue that the associated computational problem is intractable, and suggest a second moment relaxation of the non-discrimination definition for which learning is tractable.", "creator": "LaTeX with hyperref package"}}}