{"id": "1702.06186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2017", "title": "Survey of reasoning using Neural networks", "abstract": "dhoj Reason and inference require process homesteads as portend well zaw as jahvid memory skills cuties by liebe humans. Neural networks three-sport are able yk to giustizia process pressbox tasks like 119.11 image kanchan recognition (better than humans) dziady but cd-i in magan memory aspects are still limited (by mineralization attention 98.68 mechanism, hajredin size ). kemoko Recurrent chevaline Neural Network (RNN) turn-over and detest it ' s crawdad modified 12.76 version LSTM future-oriented are able to 2,762 solve nase small belva memory 26-hour contexts, but oenothera as context sherkat-e becomes larger than gunnison a drive-ins threshold, adriaenssens it is electing difficult pu\u0142tusk to use eubie them. The osuga Solution is to bolt use mufti large penaltiesnone external memory. kampman Still, 108.14 it poses many bussan challenges a320neo like, how 1949-1950 to train all-electronic neural mcnicol networks memmo for legi discrete 3,007 memory representation, how ahlert to collingtree describe sayeed long term dependencies 86.59 in sheamus sequential 245th data antonette etc. Most prominent virbhadra neural airlifter architectures uspensky for such tasks demat are d-mi Memory networks: 24.87 inference components combined natasa with long promicin term tangut memory and Neural Turing telecomunicacoes Machines: neural asrm networks using kuli external intersect memory darke resources. estimating Also, sherbet additional capitata techniques silicon like figured attention mechanism, 248.2 end jill to euro236 end nanhua gradient descent on rbr discrete mannargudi memory representation 1935/36 are needed webstock to 693 support now-familiar these jackhammers solutions. raistlin Preliminary results jiken of d'aubigny above neural architectures britpop on simple algorithms (dobara sorting, reversibly copying) eshoo and Question Answering (based on story, marich dialogs) r.j. application okin are comparable dilophosaurus with phosphoryl the signage state dannecker of the seraing art. In scalpers this catal\u00e1n paper, I explain these katiyar architectures (2-93 in 139.89 general ), delineates the 5,511 additional wlan techniques used and the finnegan results of wihl their kola application.", "histories": [["v1", "Tue, 14 Feb 2017 17:24:04 GMT  (419kb,D)", "http://arxiv.org/abs/1702.06186v1", "12 pages"], ["v2", "Thu, 2 Mar 2017 09:36:24 GMT  (419kb,D)", "http://arxiv.org/abs/1702.06186v2", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["amit sahu"], "accepted": false, "id": "1702.06186"}, "pdf": {"name": "1702.06186.pdf", "metadata": {"source": "CRF", "title": "Seminar in Collaborative Intelligence: Reasoning using Neural Networks", "authors": ["Amit Sahu"], "emails": ["asahu@rhrk.uni-kl.de"], "sections": [{"heading": null, "text": "Keywords: Neural Networks, Turing Machine, RNN, LSTM, gradient descent, sorting, discrete memory, external memory, long term memory"}, {"heading": "1 Introduction", "text": "Artificial Intelligence has two grand challenges: build models that can make multiple computational steps in answering a question and models that can describe long term dependence in sequential data. Most machine learning models lack in the ability to easily read and write to a memory (large) component and infer using a small part of this large memory. For example, tasks to answer questions from a set of facts in a story, to watch a movie and answer questions about it or to conduct dialogues cannot be solved by these models. In principle they can be solved by a language modeller such as a recurrent neural network (RNN) ([1]; [2]), but their memory is too small and not compartmentalized enough to remember the required facts accurately. Even in simple memorization task like copying a just seen sequence RNNs are known to have problems [3]. RNNs are turing complete [4] and therefore have the capacity to simulate arbitrary procedures but in practice they are not able to. ar X iv :1\n70 2.\n06 18\n6v 1\n[ cs\n.L G\n] 1\n4 Fe\nb 20\n17\nIn this survey, I discuss and compare some of the proposed solutions to these problems. In Neural Turing Machine (NTM) [5], these problems are attempted in analogy to Turing\u2019s enrichment of finite-state machine by an infinte memory tape. NTM work like a working memory by solving tasks that require the application of approximate rules to \u201crapidly-created variables \u201d[6] and by using an attentional process to read from and write to memory selectively. In Memory Networks [7], the idea is to combine successful machine learning strategies with memory component that can be read and written to. End-to-end memory networks (MemN2N) [8] extends on memory networks by removing problem in backpropagation and requirement of strong supervision at each layer. It is a continuous model that only require inputoutput pairs in comparison to memory network which require supporting facts in memory (only during training) as well.\nPaper Structure The organization of this survey paper begins with a brief background about basic Neural architectures that use some kind of memory for reasoning and inference (section 2). It follows with explanation of some of the prominent approaches in using large memory (section 3). Some of the additional techniques like memory focus and their continuous representation are discussed along with the approaches. After approaches, the experiments done using them, their results and conclusions are discussed (section 4). Finally, conclusion on comparison is drawn from experiments about these approaches (section 5)."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 RNN", "text": "Recurrent Neural Networks are neural networks with loop at a hidden node i.e. output of the hidden node is put back into the hidden node alongwith the input at next timestamp. Thus, the output of hidden node acts like a dynamic state whose evolution depends on both the input and current state (output of hidden node at previous timestamp). By unfolding RNN through time one can perceive that the context (dynamic state) from an earlier timestamp could affect the behaviour of the network at later timestamps. RNN give way to \u201cvanishing and exploding gradient \u201dproblem. As the gradient moves across timestamps in backpropagation it\u2019s multiplied with wlh(t) (weight of loop link) depending on it\u2019s value it vanishes (wlh(t) < 1) or explodes (wlh(t) > 1). Thus, RNN can unflod into a limited number of timetamps, as increasing after won\u2019t have any effect because of this problem."}, {"heading": "2.2 LSTM", "text": "To solve the problem of \u201cvanishing and exploding gradient \u201d, another architecture called Long Short-Term Memory (LSTM) [2] was developed. It solves the problem by embedding perfect integrators[9] for memory storage in the network. This is implemented with a complex structure of gates as shown in Fig. 1. For understanding\npurpose, take a simple perfect integrator x(t+1) = x(t)+i(t), where i(t) is input and x(t) is memory storage. As weight on x(t) here is identity, gradient does not vanish or explode. If we now attach a mechanism to choose when integrator takes the input i.e. a programmable gate depending on context: x(t+ 1) = x(t) + g(context)i(t), we can now selectively store information for indefinite length of time. Gates in similar sense are used in LSTM to make this possible."}, {"heading": "3 Approaches", "text": ""}, {"heading": "3.1 Neural Turing Machines", "text": "Architecture of Neural Turing Machine (NTM)[5] contains mainly: a neural network controller and a memory bank. The controller interacts with outside environment using input vector and output vector. Unlike other neural networks, NTM can also interact with a memory matrix using selective read and write operations (called heads). Every component of this architecture is differentiable, so gradient descent can be applied to train the network.\nIn NTM an attention mechanism uses \u2018degree of blurriness\u2019 which defines the degree to which the head reads or writes at each memory location. In other words, the head can read or write completely at one location or distributed on many locations. The components of NTM are defined as follows.\nReading Mt is the contents of N \u00d7M memory matrix at time t, where N is the number of memory locations and M the size of memory vector. The model defines wt, a normalized vector over N locations. Normalization of weight vector implies:\nN\u2211 i=1 wt(i) = 1, 0 \u2264 wt(i) \u2264 1,\u2200i (1)\nThe length M read vector rt is defined by following equation:\nrt = N\u2211 i=1 wt(i)Mt(i) (2)\nWriting Write operation has two parts: an erase followed by an add operation. For erase operation, the model defines an erase vector et whose M elements lie in [0,1]. The old memory Mt\u22121 is erased using the following equation:\nM\u0303t(i) = Mt\u22121(i)[1\u2212 wt(i)et], (3)\nwhere 1 is a row-vector of all 1-s, and the multiplication against the memory locations acts point-wise. Memory is erased only when both weighting and erase element at that location are 0. For add operation a length M add vector at is defined. It is performed after erase as follows:\nMt(i) = M\u0303t(i) + wt(i)at (4)\nSince both multiplication in erase and addition in add operations are commutative, the order in which multiple heads write is irrelevant. The final memory content Mt is obtained when all heads have done their write operation.\nAddressing Mechanism Weightings wt defined in Reading and Writing operations are produced using the addressing mechanism. Two types of addressing mechanism that complement each other are used:\n\u2013 Content-based addressing: focusses attention on memory locations related to values emitted by controller [11]. \u2013 Location-based addressing: For mathematical functions like f(x, y) = x \u00d7 y, location of the variable is more important than content of the variable. To convey this information location-based addressing is used.\nFocusing by content The model uses a length M key vector kt produced from head (read or write), a positive key strength \u03b2t, which can amplify or attenuate the precision of the focus, and a similarity measure K[., .] (e.g. cosine similarity) between kt and memory vector Mt(i). These are combined according to following equation to give normalized (using softmax) content based weighting:\nwct (i) = exp (\u03b2tK[kt,Mt(i)])\n\u03a3j exp (\u03b2tK[kt,Mt(j)]) (5)\nFocusing by Location The location-based addressing mechanism facilitates both simple iteration across the memory locations and random access jumps. First,\na scalar interpolation gate gt (gt \u2208 [0, 1]) is used to have weighted focus on the content weighting wct and/or the weighting from previous timestep wt\u22121:\nwgt = gtwt + (1\u2212 gt)wt\u22121 (6)\nSecond, shift weighting st is defined as a normalized distribution over the allowed integer shifts (0 to N-1 memory locations). Then, rotation is applied to wgt by st by following convolution:\nw\u0303t(i) = N\u22121\u2211 j=0 wgt (j)st(i\u2212 j) (7)\nThe combined addressing system can operate in three complementary modes: \u2013 weighting chosen only by content system \u2013 wighting chosen by content system and then shifted by location system. This\nallows head to find a contiguous block of data and then, access a particular element within that block. \u2013 weighting from previous timestep is rotated by location system. This allows weighting to iterate through a sequence of addresses by advancing same distance at each timestep.\nController Network Two choices for the neural network to be used as controller network are discussed: \u2013 Recurrent controller such as LSTM: Internal memory in this network can be\nconsidered as RAM and hidden activations as registers if controller is taken as a CPU. This allows controller to mix information (by unfolding RNN) accross multiple time seps of operations. \u2013 FeedForward controller: It can mimic recurrent network by reading and writing from the same location in memory at each step. Additionally, these \u2019read and write operations\u2019 on memory matrix are easier to interpret than internal state \u2019read and write operations\u2019 in RNN However, the number of concurrent read and write heads in feedforward controller imposes limitations on type of computation by NTM: with one single read head only unary operations can be performed on memory at each timestep, with two - binary operations, and follows. In RNN, it\u2019s taken care of by storing read vectors internally, from previous time steps."}, {"heading": "3.2 Memory Networks", "text": "A memory network[7] consists of a memory m and four components: I: input feature map - converts input to internal features G: generalization - updates old memories (state) according to the new input O: output feature map - produces output in feature representation space based on the new input and the current memory state R: response - converts output into desired format\nFlow of the model:\n1. Convert input x to internal input representation I(x) 2. Update memories m using G 3. Compute output features o using O 4. Decode output features o to give the final response\nA MemNN implementation for text When neural networks are used as components of a memory network (defined above), it is called memory neural network (MemNN). Basic model Four components of MemNN are defined as follows: I: set of sentences x(question or statement of a fact) transformed as embedding vectors I(x) G: New memories are just stored (no updates) mi = I(x). Let their number be N memories. O: output features are produced by finding k (taken as 2) supporting memories given x. First memory o1 (k = 1) is retrieved using the following equation:\no1 = O1(I(x),m) = argi=1,\u00b7\u00b7\u00b7,NsO(I(x),mi) (8)\nwhere sO is a scoring function on match between I(x) and mi. For k = 2, second memory o2 is found given the first found in previous iteration:\no2 = O2(I(x),m) = argi=1,\u00b7\u00b7\u00b7,NsO([I(x),mo1 ],mi) (9)\nwhere mi is scored with respect to both the original input and o1, square brackets denote a list. R: It produces a textual response r. Limiting textual response to a single word (out of all words), response is produced by ranking them:\nr = argmaxw\u2208W sR([I(x),mo1 ,mo2 ], w) (10)\nwhere W is the set of all words in the dictionary, and sR is a function that scores the match.\nTraining: It is done in fully supervised setting where desired inputs and responses, and the supporting sentences are labeled as such in the training data (but not in the test data, where only inputs are given). Thus, both o1 and o2 are known at training time. Training is performed with margin ranking loss and stochastic gradient descent (SGD)."}, {"heading": "3.3 End-To-End Memory Networks", "text": "This model[8] takes discrete input representations x1, . . . , xn to store them in memory, a query q, and outputs an answer a. Each of the xi, q and a contains symbols from a dictionary with vocabulary V. The model converts x (upto a fixed buffer size) and q to a continuous representation. This representation is processed via multiple hops to output a. As all these representations are continuous we can use backpropagation for training.\nSingle Layer In single layer case, the model implements a single memory hop operation. Structure and flow of single layer model is as follows: Input memory representation: Using embedding matrices A, B (of size d \u00d7 V) we convert input x and query q respectively to same continuous space of dimension d. Transformed input is memory vectors {mi} and transformed query is u. In the embedding space we compute the similarity between u and mi by taking the inner product followed by a softmax:\npi = Softmax(u Tmi) (11)\npi as defined above is probability vector over the inputs. Output memory representation: Using emedding matrix C, each input xi is transformed to output vector ci. The output response vector o is computed by the following equation:\no = \u2211 i pici (12)\nGenerating the final prediction: The predicted label is computed using the final weight matrix W (of size V \u00d7 d) by following equation:\na\u0302 = Softmax(W (o+ u)) (13)\nAll three embedding matrices A, B and C, and W are jointly learned during training (Stochastic Gradient Descent) by minimizing a standard cross entropy loss between a\u0302 and the true label a."}, {"heading": "4 Experiment and Results", "text": ""}, {"heading": "4.1 Neural Turing Machine", "text": "Experiments were done on a set of simple algorithms tasks like copying and sorting data sequences. The goal of the experiments was to observe problem solving and learning of compact internal programs by the NTM architecture. Such solution programs could generalize well beyond training data. For example network trained to copy sequences of length 20 was tested on sequences of length 100. Three architectures are compared in experiments:\n\u2013 NTM with a feedforward controller\n\u2013 NTM with a LSTM controller\n\u2013 Standard LSTM network\nTasks were episodic and thus, the dynamic state (previous hidden state) was reset (to learned bias vector) at the start of each input sequence. All the tasks were supervised learning problems; all network had logistic sigmoid output layers and were trained with cross-entropy objective function. Sequence prediction errors are reported in bits-per-sequence.\nCopy The task was to copy a sequence of random binary vectors followed by a delimiter (to fix length). Thus, input was a sequence with delimiter and output was the same sequence without delimiter. It was done to compare effect of longer time delays on NTM with LSTM. As can be seen from Fig. 2 NTM learned much faster than LSTM alone, and converged to a lower cost. NTM continues to copy as the length increases while LSTM rapidly degrades beyond length 20. These disparities suggest a qualitative rather than a quantitative difference in problem solving by the two architectures.\nPriority Sort Sorting capacity of NTM was tested in this task. Input was a collection of random binary vectors with priority from the range [-1,1]. Hypothesis for NTM was that it uses the priorities to determine the relative location of each write. To test the hypothesis a linear function of the priority was fitted on the write locations. Fig. 3 shows the results, locations returned by the linear function closely\nmatch write locations of NTM and reads from the memory locations are in increasing order, thereby sequences were traversed in sorted manner. The learning curves in Fig. 4 show that NTM outperforms LSTM."}, {"heading": "4.2 Memory Networks", "text": "Large-scale QA Experiments were performed on QA dataset introduced in [13]. It consists of 14M statements, stored as (subject, relation, object) triples. It was combination of pseudo-labeled QA pairs, made of a question and an associated triple, and 35M pairs of paraphrased questions.\nIn experiment framework, top returned candidate answers were re-ranked and results were measured using F1 score over the test set. Systems developed following architecture given in [13] and [14], were tested on the same and compared as shown in Table 1. The results show viability of MemNNs in large scale QA answering but the lookup is slow for which method extension like word hashing and cluster hashing were used.\nSimulated World QA A simple simulation of 4 characters, 3 objects and 5 rooms - where characters move around, pick up and drop objects, based on the approach of [15] was built. This simulation was converted into text to form statements and QA.\nA sample QA is shown in Fig. 5. 7K statements and 3K questions were generated for training and the same for testing. MemNNs are compared with RNNs and LSTMs on this task. Difficulty of the task was set based on the limit in the number of time steps (statements), before the entity (in question) was last mentioned. Three kinds of questions were presented to the system separately: about the actor only (actor), about both actor and object, and about actors but without before i.e. not about previous location of actor (actor w/o before).\nResults for single word answers are given in Table 2. Following observations were made: \u2013 RNN and LSTM solved difficulty 1 task w/o before but performed worse with\nbefore questions and even worse with difficulty 5. This poor performance was attributed to failure in encoding long term memory in RNN, and failure to remember too far sentences in LSTM. \u2013 MemNNs did not have above limitations and error was due to wrong statement pick by sO. \u2013 Extension of MemNN with time features, based on when a memory slot is written, was essential for such story tasks."}, {"heading": "4.3 End-To-End Memory Networks", "text": "Synthetic Question and Answering Experiments Experiemnts were performed on synthetic QA tasks defined in [12]. A QA task consists of a set of statements, a question and a corresponding answer. The answers is available at training time and is predicted at testing time. There are 20 different types of tasks that require different forms of reasoning and deduction. Only a subset of provided\nstatements in the task are relevant for answering. This information is not provided to the model at both training and testing time. Following three models (baselines) are compared with this approach (abbreviated MemN2N):\n\u2013 MemNN: The strongly supervised AM+NG+NL Memory Network approach, proposed in [12]. It uses supporting facts (strong supervision), n-gram modelling NG, nonlinear layers NL and an adaptive number of hops AM per query.\n\u2013 MemNN-WSH: A weakly supervised version of MemNN\n\u2013 LSTM: A standard LSTM model, trained using question / answer pairs only (weakly supervised)\nResults: The results across all 20 tasks are given in Table 3 for the 1K training set, along with mean performance for 10k training set. Following observations are made:\n\u2013 The best MemN2N models are reasonably close (mean error) to the supervised models.\n\u2013 All variants of MemN2N comfortably beat the weakly supervised baseline methods.\n\u2013 Joint training on all tasks help\n\u2013 More computational hops give improved performance."}, {"heading": "5 Conclusion", "text": "NTMs enrich the capabilities of recurrent networks most profoundly by using attention mechanism, memory write and a large addressable memory. However, the results of NTMs are only shown on simple tasks of copying and sorting as discussed in section 4.1. Results of MemNN and MemN2N are compared in Table 3. These suggest, for strong supervision (when supporting facts are known during training) MemNN work the best with least error percentage. But, in case of weak supervision MemN2N are better. It has been consistenly observed in all the experiments (Tables 2, 3 , Fig. 2, 4 ) that these new architectures are better in performance than RNN, LSTM for tasks that require large memory lookup for inference. MemN2N have further been applied in many situations like dialogs in a restaurant setting, QA based on a story, goal oriented dialogs etc. These research suggest the prominence of Neural networks in reasoning tasks."}], "references": [{"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "J. Interspeech. 1045\u20131048", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "Schmidhuber J."], "venue": "J. Neural Computation. 9(8), 1735\u20131780", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint:1410.4615", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "On the computational power of neural nets", "author": ["H.T. Siegelmann", "E.D. Sontag"], "venue": "Journal of computer and system sciences. 50(1), 132\u2013150", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Neural Turing Machine", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint:1410.5401v2", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The problem of rapid variable creation", "author": ["R.F. Hadley"], "venue": "J. Neural computation. 21(2), 510\u2013532", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Memory Networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "C. International Conference on Learning Representations. arXiv:1410.3916", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "End-To-End Memory Networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "J. Advances in Neural Information Processing Systems. 28, 2440\u20132448", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous attractors and oculomotor control", "author": ["H.S. Seung"], "venue": "J. Neural Networks. 11(7), 1253\u20131258", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["A. Graves"], "venue": "Springer, vol.385", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural Networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "J. Proceedings of the national academy of sciences. 79(8), 2554\u20132558", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1982}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv:1502.05698", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Paraphrase-driven learning for open question answering", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "J. Association for Computational linguistics. 1608\u20131618", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Open question answering with weakly supervised embedding models", "author": ["A. Bordes", "J. Weston", "N. Usunier"], "venue": "C. ECML-PKDD.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards understanding situated natural language", "author": ["A. Bordes", "N. Usunier", "R. Collobert", "J. Weston"], "venue": "C. AISTATS.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "In principle they can be solved by a language modeller such as a recurrent neural network (RNN) ([1]; [2]), but their memory is too small and not compartmentalized enough to remember the required facts accurately.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "In principle they can be solved by a language modeller such as a recurrent neural network (RNN) ([1]; [2]), but their memory is too small and not compartmentalized enough to remember the required facts accurately.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "Even in simple memorization task like copying a just seen sequence RNNs are known to have problems [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "RNNs are turing complete [4] and therefore have the capacity to simulate arbitrary procedures but in practice they are not able to.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "In Neural Turing Machine (NTM) [5], these problems are attempted in analogy to Turing\u2019s enrichment of finite-state machine by an infinte memory tape.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "NTM work like a working memory by solving tasks that require the application of approximate rules to \u201crapidly-created variables \u201d[6] and by using an attentional process to read from and write to memory selectively.", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "In Memory Networks [7], the idea is to combine successful machine learning strategies with memory component that can be read and written to.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "End-to-end memory networks (MemN2N) [8] extends on memory networks by removing problem in backpropagation and requirement of strong supervision at each layer.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "To solve the problem of \u201cvanishing and exploding gradient \u201d, another architecture called Long Short-Term Memory (LSTM) [2] was developed.", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "It solves the problem by embedding perfect integrators[9] for memory storage in the network.", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "A LSTM block (adapted from [10])", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "Architecture of Neural Turing Machine (NTM)[5] contains mainly: a neural network controller and a memory bank.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "For erase operation, the model defines an erase vector et whose M elements lie in [0,1].", "startOffset": 82, "endOffset": 87}, {"referenceID": 10, "context": "Two types of addressing mechanism that complement each other are used: \u2013 Content-based addressing: focusses attention on memory locations related to values emitted by controller [11].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "a scalar interpolation gate gt (gt \u2208 [0, 1]) is used to have weighted focus on the content weighting w t and/or the weighting from previous timestep wt\u22121: w t = gtwt + (1\u2212 gt)wt\u22121 (6)", "startOffset": 37, "endOffset": 43}, {"referenceID": 6, "context": "A memory network[7] consists of a memory m and four components: I: input feature map - converts input to internal features G: generalization - updates old memories (state) according to the new input O: output feature map - produces output in feature representation space based on the new input and the current memory state R: response - converts output into desired format", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "This model[8] takes discrete input representations x1, .", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "Copy Learning Curves (adapted from [5])", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "(adapted from [5])", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "Input was a collection of random binary vectors with priority from the range [-1,1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 4, "context": "(adapted from [5])", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "Results on the large-scale QA task of [13](adapted from [7]).", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "Results on the large-scale QA task of [13](adapted from [7]).", "startOffset": 56, "endOffset": 59}, {"referenceID": 12, "context": "Method F1 Adapted from [13] 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "54 Adapted from [14] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "Large-scale QA Experiments were performed on QA dataset introduced in [13].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "Systems developed following architecture given in [13] and [14], were tested on the same and compared as shown in Table 1.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "Systems developed following architecture given in [13] and [14], were tested on the same and compared as shown in Table 1.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "Simulated World QA A simple simulation of 4 characters, 3 objects and 5 rooms - where characters move around, pick up and drop objects, based on the approach of [15] was built.", "startOffset": 161, "endOffset": 165}, {"referenceID": 6, "context": "Test accuracy on the simulation QA task (adapted from [7]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 11, "context": "Synthetic Question and Answering Experiments Experiemnts were performed on synthetic QA tasks defined in [12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "\u2013 MemNN: The strongly supervised AM+NG+NL Memory Network approach, proposed in [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "(adapted from [8])", "startOffset": 14, "endOffset": 17}], "year": 2017, "abstractText": "Reason and inference require process as well as memory skills by humans. Neural networks are able to process tasks like image recognition (better than humans) but in memory aspects are still limited (by attention mechanism, size). Recurrent Neural Network (RNN) and it\u2019s modified version LSTM are able to solve small memory contexts, but as context becomes larger than a threshold, it is difficult to use them. The Solution is to use large external memory. Still, it poses many challenges like, how to train neural networks for discrete memory representation, how to describe long term dependencies in sequential data etc. Most prominent neural architectures for such tasks are Memory networks: inference components combined with long term memory and Neural Turing Machines: neural networks using external memory resources. Also, additional techniques like attention mechanism, end to end gradient descent on discrete memory representation are needed to support these solutions. Preliminary results of above neural architectures on simple algorithms (sorting, copying) and Question Answering (based on story, dialogs) application are comparable with the state of the art. In this paper, I explain these architectures (in general), the additional techniques used and the results of their application.", "creator": "LaTeX with hyperref package"}}}