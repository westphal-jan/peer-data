{"id": "1702.06696", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "One Representation per Word - Does it make Sense for Composition?", "abstract": "In this paper, wtvw we youngdahl investigate whether magnetotail an obsolescent a priori rosey disambiguation mongolia of padam word pecnik senses toros is leghorn strictly brouhaha necessary or phaulkon whether externalities the meaning 14a of a word in 557,000 context can be disambiguated zachar through hallamshire composition alone. premeditation We evaluate the ldds performance of off - 108.29 the - shelf single - daliang vector tucanae and mcvaney multi - ibara sense vector models deschamps on a vm benchmark uthai phrase similarity varos task and readme a schlepping novel turcotte task 263 for word - vane sense discrimination. We toarcian find emes that resurrections single - saab sense windrush vector models perform mitchel as 11/22 well antics or better jatayu than multi - untroubled sense valida vector models despite arguably less ujfalusi clean magdy elementary emceed representations. daqin Our findings furthermore 504th show baube that simple agresto composition functions such vernon as pointwise addition are able bruskin to volponi recover vettori sense benegas specific information slalom from a bustamente single - szepes sense pyr\u00e9n\u00e9es-orientales vector model remarkably portim\u00e3o well.", "histories": [["v1", "Wed, 22 Feb 2017 07:41:08 GMT  (363kb,D)", "http://arxiv.org/abs/1702.06696v1", "to appear at the EACL 2017 workshop on Sense, Concept and Entity Representations and their Applications"]], "COMMENTS": "to appear at the EACL 2017 workshop on Sense, Concept and Entity Representations and their Applications", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thomas kober", "julie weeds", "john wilkie", "jeremy reffin", "david weir"], "accepted": false, "id": "1702.06696"}, "pdf": {"name": "1702.06696.pdf", "metadata": {"source": "CRF", "title": "One Representation per Word \u2014 Does it make Sense for Composition?", "authors": ["Thomas Kober", "Julie Weeds", "John Wilkie", "Jeremy Reffin", "David Weir"], "emails": ["d.j.weir}@sussex.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Distributional word representations based on counting co-occurrences have a long history in natural language processing and have successfully been applied to numerous tasks such as sentiment analysis, recognising textual entailment, wordsense disambiguation and many other important problems. More recently low-dimensional and dense neural word embeddings have received a considerable amount of attention in the research community and have become ubiquitous in numerous NLP pipelines in academia and industry. One fundamental simplifying assumption commonly made in distributional semantic models, however, is that every word can be encoded by a single representation. Combining polysemous lexemes into a single vector has the consequence of essentially creating a weighted average of all observed meanings of a lexeme in a given text corpus.\nTherefore a number of proposals have been made to overcome the issue of conflating several different senses of an individual lexeme into a single representation. One approach (Reisinger and Mooney, 2010; Huang et al., 2012) is to try directly inferring a predefined number of senses from data and subsequently label any occurrences of a polysemous lexeme with the inferred inventory. Similar approaches are proposed by Reddy et al. (2011) and Kartsaklis et al. (2013) who show that appropriate sense selection or disambiguation typically improves performance for composition of noun phrases (Reddy et al., 2011) and verb phrases (Kartsaklis et al., 2013). Dinu and Lapata (2010) proposed a model that represents the meaning of a word as a probability distribution over latent senses which is modulated based on contextualisation, and report improved performance on a word similarity task and the lexical substitution task. Other approaches leverage an existing lexical resource such as BabelNet or WordNet to obtain sense labels a priori to creating word representations (Iacobacci et al., 2015), or as a postprocessing step after obtaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016). While these approaches have exhibited strong performance on benchmark word similarity tasks (Huang et al., 2012; Iacobacci et al., 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al., 2015), and at tasks which require the meaning of a word to be considered in context; e.g, word sense disambiguation (Iacobacci et al., 2016) and word similarity in context (Iacobacci et al., 2015).\nIn this paper we consider what happens when distributional representations are composed to\nar X\niv :1\n70 2.\n06 69\n6v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\n17\nform representations for larger units of meaning. In a compositional phrase, the meaning of the whole can be inferred from the meaning of its parts. Thus, assuming compositionality, the representation of a phrase such as black mood, should be directly inferable from the representations for black and for mood. Further, one might suppose that composing the correct senses of the individual lexemes would result in a more accurate representation of that phrase. However, our counterhypothesis is that the act of composition contextualises or disambiguates each of the lexemes thereby making the representations of individual senses redundant. We investigate this hypothesis by evaluating the performance of single-vector representations and multi-sense representations at both a benchmark phrase similarity task and at a novel word-sense discrimination task.\nOur contributions in this work are thus as follows. First, we provide quantitative and qualitative evidence that even simple composition functions have the ability to recover sense-specific information from a single-vector representation of a polysemous lexeme in context. Second, we introduce a novel word-sense discrimination task1, which can be seen as the first stage of word-sense disambiguation. The goal is to find whether the occurrences of a lexeme in two or more sentential contexts belong to the same sense or not, without necessarily labelling the senses. While it has received relatively little attention in recent years, it is an important natural language understanding problem and can provide important insights into the process of semantic composition."}, {"heading": "2 Evaluating Distributional Models of Composition", "text": "For evaluation we use several readily available off-the-shelf word embeddings, that have already been shown to work well for a number of different NLP applications. We compare the 300-dimensional skip-gram word2vec (Mikolov et al., 2013) word embeddings2 to the dependency based version of word2vec \u2014 henceforth dep2vec3 (Levy and Goldberg, 2014) \u2014 and the\n1Our task is available from https://github.com/ tttthomasssss/sense2017\n2Available from: https://code.google.com/p/ word2vec/\n3Available from: https://levyomer. wordpress.com/2014/04/25/ dependency-based-word-embeddings/\nSENSEMBED model4 by Iacobacci et al. (2015), which creates word-sense embeddings by performing word-sense disambiguation prior to running word2vec.\nWe note that word2vec and dep2vec use a single vector per word approach and therefore conflate the different senses of a polysemous lexeme. On the other hand, SENSEMBED utilises Babelfy (Moro et al., 2014) as an external knowledge source to perform word-sense disambiguation and subsequently creates one vector representation per word sense.\nFor composition we use pointwise addition for all models as this has been shown to be a strong baseline in a number of studies (Hashimoto et al., 2014; Hill et al., 2016). We also experimented with pointwise multiplication as composition function but, similar to Hill et al. (2016), found its performance to be very poor (results not reported). We model any out-of-vocabulary items as a vector consisting of all zeros and determine proximity of composed meaning representations in terms of cosine similarity. We lowercase and lemmatise the data in our task but do not perform number or date normalisation, or removal of rare words."}, {"heading": "3 Phrase Similarity", "text": "Our first evaluation task is the benchmark phrase similarity task of Mitchell and Lapata (2010). This dataset consists of 108 adjective-noun (AN), 108 noun-noun (NN) and 108 verb-object (VO) pairs. The task is to compare a compositional model\u2019s similarity estimates with human judgements by computing Spearman\u2019s \u03c1. An average \u03c1 of 0.47- 0.48 represents the current state-of-the-art performance on this task (Hashimoto et al., 2014; Kober et al., 2016; Wieting et al., 2015).\nFor single-sense representations, the strategy for carrying out this task is simple. For each phrase in each pair, we compose the constituent representations and then compute the similarity of each pair of phrases using the cosine similarity. For multi-sense representations, we adapted the strategy which has been used successfully in various word similarity experiments (Huang et al., 2012; Iacobacci et al., 2015). Typically, for each word pair, all pairs of senses are considered and the similarity of the word pair is considered to be\n4Available from: http://lcl.uniroma1.it/ sensembed/\nthe similarity of the closest pair of senses. The fact that this strategy works well suggests that when humans are asked to judge word similarity, the pairing automatically primes them to select the closest senses. Extending this to phrase similarity requires us to compose each possible pair of senses for each phrase and then select the sense configuration which results in maximal phrase similarity. For comparison, we also give results for the configuration which results in minimal phrase similarity and the arithmetic mean5 of all sense configurations."}, {"heading": "3.1 Results", "text": "Table 1 shows that the simple strategy of adding high quality single-vector representations is very competitive with the state-of-the-art for this task. None of the strategies for selecting a sense configuration for the multi-sense representations could compete with the single sense representations on this task. One possible explanation is that the commonly adopted closest sense strategy is not effective for composition since the composition of incorrect senses may lead to spuriously high similarities (for two \u201cimplausible\u201d sense configurations).\nTable 2 lists a number of example phrase pairs with low average human similarity scores in the Mitchell and Lapata (2010) test set. The results show the tendency of the closest sense strategy with SENSEMBED (SE) to overestimate the similarity of dissimilar phrase pairs. For a comparison we manually labelled the lexemes in the sample phrases with the appropriate BabelNet senses prior to composition (SE*). Human (H) similarity scores are normalised and averaged for an easier comparison, model estimates represent cosine similarities."}, {"heading": "4 Word Sense Discrimination", "text": "Word-sense discrimination can be seen as the first stage of word-sense disambiguation, where the\n5We also tried the geometric mean and the median but these performed comparably with the arithmetic mean.\ngoal is to find whether two or more occurrences of the same lexeme express identical senses, without necessarily labelling the senses yet. It has received relatively little attention despite its potential for providing important insights into semantic composition, focusing in particular on to the ability of compositional distributional semantic models to appropriately contextualise a polysemous lexeme.\nWork on word-sense discrimination has suffered from the absence of a benchmark task as well as a clear evaluation methodology. For example Schu\u0308tze (1998) evaluated his model on a dataset consisting of 20 polysemous words (10 naturally ambiguous lexemes and 10 artificially ambiguous \u201cpseudo-lexemes\u201d) in terms of accuracy for coarse grained sense distinctions, and an information retrieval task. Pantel and Lin (2002), and Van de Cruys (2008) used automatically extracted words from various newswire sources and evaluated the output of their models in comparison to WordNet and EuroWordNet, respectively. Purandare and Pedersen (2004) used a subset of the words from the SENSEVAL-2 task and evaluated their models in terms of precision, recall and F1-score of how well available sense tags match with clusters discovered by their algorithms. Akkaya et al. (2012) used the concatenation of the SENSEVAL-2 and SENSEVAL-3 tasks and evaluated their models in terms of cluster purity and accuracy. Finally, Moen et al. (2013) used the semantic textual similarity (STS) 2012 task, which is based on human judgements of the similarity between two sentences.\nOne contribution of our work is a novel wordsense discrimination task, evaluated on a number of robust baselines in order to facilitate future research in that area. In particular, our task offers a testbed for assessing the contextualisation ability of compositional distributional semantic models. The goal is, for a given polysemous lexeme in context, to identify the sentence from a list of options\nthat is expressing the same sense of that lexeme as the given target sentence. These two sentences \u2014 the target and the \u201ccorrect answer\u201d \u2014 can exhibit any degree of semantic similarity as long as they convey the same sense of the target lexeme. Table 3 shows an example of the polysemous adjective black in our task. The goal of any model would be to determine that the expressed sense of black in the sentence She was going to set him free from all of the evil and black hatred he had brought to the world is identical to the expressed sense of black in the target sentence Or should they rebut the Democrats\u2019 black smear campaign with the evidence at hand.\nOur task assesses the ability of a model to discriminate a particular sense in a sentential context from any other senses and thus provides an excellent testbed for evaluating multi-sense word vector models as well as compositional distributional semantic models. By composing the representation of a target lexeme with its surrounding context, it should be possible to determine its sense. For example, composing black smear campaign should lead to a compositional representation that is closer to the composed representation of black hatred than to black mood, black sense of humour or black coffee. This essentially uses the similarity of the compositional representation of a lexeme\u2019s context to determine its sense. Similar approaches to word-sense disambiguation have already been successfully used in past works (Akkaya et al., 2012; Basile et al., 2014)."}, {"heading": "4.1 Task Construction", "text": "For the construction of our dataset we made use of data from two english dictionaries (Oxford Dictionary and Collins Dictionary), accessible via their respective web APIs6, as well as examples from the sense annotated corpus SemCor (Miller et al., 1993). Our use of dictionary data is motivated by a number of favourable properties which make it a very suitable data source for our proposed task:\n\u2022 The content is of very high-quality and curated by expert lexicographers.\n\u2022 All example sentences are carefully crafted in order to unambiguously illustrate the usage\n6https://developer. oxforddictionaries.com for the Oxford Dictionary, https://www.collinsdictionary.com/api/ for the Collins Dictionary. We use NLTK 3.2 to access SemCor.\nof a particular sense for a given polysemous lexeme.\n\u2022 The granularity of the sense inventory reflects common language use7.\n\u2022 The example sentences are typically free of any domain bias wherever possible.\n\u2022 The data is easily accessible via a web API.\nBy using the data from curated resources we were able to avoid a setup as a sentence similarity task and any potentially noisy crowd-sourced human similarity judgements.\nWe were furthermore able to collect data from varying frequency bands, enabling an assessment of the impact of frequency on any model. Figure 1 shows the number of target lexemes per frequency band. While the majority of lexemes, with reference to a cleaned October 2013 Wikipedia dump8, is in the middle band, there is a considerable amount of less frequent lexemes. The most frequent target lexeme in our task is the verb be with \u22481.8m occurrences in Wikipedia, and the least frequent lexeme is the verb ruffle with only 57 occurrences. The average target lexeme frequency is \u224895k for adjectives, and \u224845k\u221246k for nouns and verbs9.\n7The Oxford dictionary lists 5 different senses for the noun \u201cbank\u201d, whereas WordNet 3.0 lists 10 synsets, for example distinguishing \u201cbank\u201d as the concept for a financial institution and \u201cbank\u201d as a reference to the building where financial transactions take place.\n8We removed any articles with fewer than 20 page views. 9The overall number of unique word types is smaller than the number of examples in our task as there are a number of lexemes that can occur with more than one part-of-speech."}, {"heading": "4.2 Task Setup Details", "text": "We collected data for 3 different parts-of-speech: adjectives, nouns and verbs. We furthermore created task setups with varying numbers of senses to distinguish (2-5 senses) for a given target lexeme. This is to evaluate how well a model is able to discriminate different degrees of polysemy of any lexeme. For any task setup evaluating for n senses, we included all lexemes with > n senses and randomly sampled n senses from its inventory. For each lexeme, we furthermore ensured that it had at least 2 example sentences per sense. For the available senses of any given lexeme, we randomly chose a sense as the target sense, and from its list of example sentences randomly sampled 2 sentences, one as the target example and one as the \u201ccorrect answer\u201d for the list of candidate sentences. Finally we once again randomly sampled the required number of other senses and example sentences to complete the task setup. Using random sampling of word senses and targets aims to avoid a predominant sense bias.\nFor each part-of-speech we created a development split for parameter tuning and a test split for the final evaluation. Table 4 shows the number of examples for each setup variant of our task. The biggest category are polysemous nouns, representing roughly half of the data, followed by verbs representing another third, and the smallest category are adjectives taking up the remaining\u224817%. We measure performance in terms of accuracy of\ncorrectly predicting which two sentences share the\nsame sense of a given target lexeme. Accuracy has the advantage of being much easier to interpret \u2014 in absolute terms as well as in the relative difference between models \u2014 in comparison to other commonly used evaluation metrics such as cluster purity measures or correlation metrics such as Spearman \u03c1 and Pearson r."}, {"heading": "4.3 Experimental Setup", "text": "In this paper we compare the compositional models outlined earlier with two baselines, a random baseline and a word-overlap baseline of the extracted contexts. For the single-vector representations, we composed the target lexeme with all of the words in the context window and compared it with the equivalent representation of each of the options (lexeme plus context words). The option with the highest cosine similarity was deemed to be the selected sense. For SENSEMBED, we composed all sense vectors of a target lexeme with the given context and then used the closest sense strategy (Iacobacci et al., 2015) on composed representations to choose the predicted sense10. The wordoverlap baseline is simply the number of words in common between the context window for the target and each of the options.\nWe experimented with symmetric linear bagof-words contexts of size 1, 2 and 4 around the target lexeme. We also experimented with dependency contexts, where first-order dependency contexts performed almost identical to using a 2- word bag-of-words context window (results not reported). We excluded stop words prior to extracting the context window in order to maximise the number of content words. We break ties for any of the methods \u2014 including the baselines \u2014 by randomly picking one of the options with the highest similarity to the composed representation\n10We also tried an all-by-all senses composition, however found this to be computationally not tractable.\nof the target lexeme with its context. Statistical significance between the best performing model and the word overlap baseline is computed by using a randomised pairwise permutation test (Efron and Tibshirani, 1994)."}, {"heading": "4.4 Results", "text": "Table 5 shows the results for all context window sizes across all parts-of-speech and number of senses. All models substantially outperform the random baseline for any number of senses. Interestingly the word overlap baseline is competitive for all context window sizes. While it is a very simple method, it has already been found to be a strong baseline for paraphrase detection and semantic textual similarity (Dinu and Thater, 2012). One possible explanation for its robust performance on our task is an occurrence of the one-sense-per-collocation hypothesis (Yarowsky, 1993). The performance of all other models is roughly in the same ballpark for all parts-ofspeech and number of senses, suggesting that they form robust baselines for future models. While the results are relatively mixed for adjectives, word2vec appears to be the strongest model for polysemous nouns and verbs.\nThe perhaps most interesting observation in Table 5 is that word2vec and dep2vec are performing as well or better than SENSEMBED despite the fact that the former conflate the senses of a polysemous lexeme in a single vector representation. Figure 2 shows the average performance of all models across parts-of-speech per number of senses and for all context window sizes.\nSENSEMBED and Babelfy\nOne possible explanation for SENSEMBED not outperforming the other methods despite its cleaner encoding of different word senses in the above experiments is that at train time, it had access to sense labels from Babelfy. At test time on our task however, it did not have any sense labels available. We therefore sense tagged the 5-sense noun subtask with Babelfy and re-ran SENSEMBED. As Table 6 shows, access to sense labels at test time did not give a substantive performance boost, representing further support for our hypothesis that composition in single-sense vector models might be sufficient to recover sense specific information.\nFrequency Range\nWe chose the 2-sense noun subtask to estimate the degree sensitivity of target lexeme frequency on our task we merged the [1, 1k) and [1k, 10k), and [50k, 100k) and [100k,\u221e) frequency bands from Figure 1, and sampled an equal number of target words from each band. Table 7 reports the results for this experiment. All methods outperform the random and word overlap baseline and appear to be working better for less frequent lexemes. One possible explanation for this behaviour is that less frequent lexemes have fewer senses and potentially less subtle sense differences than more frequent lexemes, which would make them easier to discriminate by distributional semantic methods."}, {"heading": "5 Discussion", "text": "Our results suggest that pointwise addition in a single-sense vector model such as word2vec is able to discriminate the sense of a polysemous lexeme in context in a surprisingly effective way and represents a strong baseline for future work. Distributional composition can therefore be interpreted as a process of contextualising the meaning of a lexeme. This way, composition does not only act as a way to represent the meaning of a phrase as a whole, but also as a local discriminator for any lexemes in the phrase. For example the composed representation of dry clothes should only keep contexts that dry shares with clothes while suppressing contexts it shares with weather or wine. Hence, one would expect the same to happen with a polysemous lexeme such as bank in the context of river and account, respectively.\nRecent work by Arora et al. (2016) has shown that the different senses of a polysemous lexeme reside in a linear substructure within a single vector and are recoverable by sparse coding. There is furthermore evidence that additive composition in low-dimensional word embeddings approximates an intersection of the contexts of two distributional word vectors (Tian et al., 2015). It therefore seems plausible that an intersective composition function should be able to recover sense specific information.\nTo qualitatively analyse this hypothesis we used the word2vec and SENSEMBED vectors to compose a small number of example phrases by pointwise addition and calculated their top 5 nearest neighbours in terms of cosine similarity. For SENSEMBED we manually sense tagged the\nphrases with the appropriate BabelNet sense labels prior to composition. We omitted the BabelNet sense labels in the neighbour list for brevity, however they were consistent with the intended\nsense in all cases. Table 8 supports the view of composition as a way of contextualising the meaning of a lexeme. In all cases in our example the word2vec neighbours reflect the intended sense\nof the polysemous lexeme, providing evidence for the linear substructure of word senses in a single vector as discovered by Arora et al. (2016), and suggesting that distributional composition is able to recover sense specific information from a polysemous lexeme. The very fine-grained sense-level vector space of SENSEMBED is giving rise to a very focused neighbourhood, however there does not seem to be any advantage over word2vec from a qualitative point of view when using simple additive composition."}, {"heading": "6 Related Work", "text": "The perhaps most popular tasks for evaluating the ability of a model to capture or encode the different senses of a polysemous lexeme in a given context are the english lexical substitution task (McCarthy and Navigli, 2007) and the Microsoft sentence completion challenge (Zweig and Burges, 2011). Both tasks require any model to fill an appropriate word into a pre-defined slot in a given sentential context. The sentence completion challenge provides a list of candidate words while the english lexical substitution task does not. However, neither task focuses on polysemy and the english lexical substitution task conflates the problems of discriminating word senses and finding meaning preserving substitutes.\nDictionary definitions have previously been used to evaluate compositional distributional semantic models where the goal is to match a dictionary entry with its corresponding definition (Kartsaklis et al., 2012; Polajnar and Clark, 2014). These datasets are commonly set up as retrieval tasks, but generally do not test the ability of a model to disambiguate a polysemous word in con-\ntext, or discriminate multiple definitions of the same word.\nOur task also provides a novel evaluation for compositional distributional semantic models, where the predominant strategy is to estimate the similarity of two short phrases (Bernardi et al., 2013; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) or sentences (Agirre et al., 2016; Huang et al., 2012; Marelli et al., 2014) in comparison to human provided gold-standard judgements. One problem with these similarity tasks is that the similarity or relatedness of two sentences is very difficult to judge \u2014 especially on a fine-grained scale \u2014 even for humans. This frequently results in a relatively high variance of judgements and low interannotator agreement (Batchkarov et al., 2016). The short phrase datasets typically have a fixed structure that only test a very small fraction of the possible grammatical constructions in which a lexeme can occur, and furthermore provide very little context. The use of full sentences remedies the lack of context and grammatical variation, however can still contain a significant level of noise due to the automatic construction of the dataset or the variance in human ratings. In contrast, our task is not set up as a sentence similarity task and therefore avoids the use of human similarity judgements.\nOur task is similar to word-sense induction (WSI), however we only focus on discriminating the sense of a polysemous lexeme in context rather than inducing a set of senses from raw data and appropriately tagging subsequent occurrences of polysemous instances with the inferred inventory.\nSeparating the sense discrimination task from the problem of sense induction has the advantage of making our task applicable to evaluating compositional distributional semantic models in order to test their ability to appropriately contextualise a polysemous lexeme. Due to not requiring any models to perform an extra step for sense induction, our task is easier to evaluate as no matching between sense clusters identified by a model and some gold standard sense classes needs to be performed, as typically proposed in the WSI literature (Agirre and Soroa, 2007; Manandhar et al., 2010).\nMost closely related to our task are the Stanford Contextual Word Similarity (SCWS) dataset by Huang et al. (2012) and the Usage Similarity (USim) task by Erk et al. (2009). The goal in both tasks is to estimate the similarity of two polysemous words in context in comparison to human provided gold standard judgements. In the SCWS dataset typically two different lexemes are considered whereas in USim and our task the same lexemes with different contexts are compared. Instead of relying on crowd-sourced human gold-standard similarity judgements, which can be prone to a considerable amount of noise11, we leverage the high-quality content of available english dictionaries. Furthermore, our task is not formulated as estimating the similarity between two lexemes in context, but identifying the sentences that use the same sense of a given polysemous lexeme."}, {"heading": "7 Conclusion", "text": "While elementary multi-sense representations of words might capture a more fine grained semantic picture of a polysemous word, that advantage does not appear to transfer to distributional composition in a straightforward way. Our experiments on a popular phrase similarity benchmark and our novel word-sense discrimination task have demonstrated that semantic composition does not appear to benefit from a fine grained sense inventory, but that the ability to contextualise a polysemous lexeme in single-sense vector models is sufficient for superior performance. We furthermore have provided qualitative and quantitative evidence that an intersective composition function such as point-\n11For example the average standard deviation of human ratings in the SCWS dataset is \u22483 on a 10-point scale, and can be up to 4\u20135 in some cases.\nwise addition for neural word embeddings is able to discriminate the meaning of a word in context, and is able to recover sense specific information remarkably well.\nLastly, our experiments have uncovered an important question for multi-sense vector models, namely how to exploit the fine-grained sense level representations for distributional composition. Our novel word-sense discrimination task provides an excellent testbed for compositional distributional semantic models, both following a single-sense or multi-sense vector modelling paradigm, due to its focus on assessing the ability of a model to appropriately contextualise the meaning of a word. Our task furthermore provides another evaluation option away from intrinsic evaluations which are based on often noisy human similarity judgements, while also not being embedded in a downstream task.\nIn future work we aim to extend our evaluation to more complex compositional distributional semantic models such as the lexical function model (Paperno et al., 2014) or the Anchored Packed Dependency Tree framework (Weir et al., 2016). We furthermore want to investigate how far the sense-discriminating ability of composition can be leveraged for other tasks."}, {"heading": "Acknowledgments", "text": "We would like to thank our anonymous reviewers for their helpful comments."}], "references": [{"title": "Semeval-2007 task 02: Evaluating word sense induction and discrimination systems", "author": ["Eneko Agirre", "Aitor Soroa"], "venue": "In Proceedings of SemEval,", "citeRegEx": "Agirre and Soroa.,? \\Q2007\\E", "shortCiteRegEx": "Agirre and Soroa.", "year": 2007}, {"title": "Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation", "author": ["Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "German Rigau", "Janyce Wiebe"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "Utilizing semantic composition in distributional semantic models for word sense discrimination and word sense disambiguation", "author": ["Cem Akkaya", "Janyce Wiebe", "Rada Mihalcea"], "venue": "In Proceedings of ICSC,", "citeRegEx": "Akkaya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Akkaya et al\\.", "year": 2012}, {"title": "An enhanced lesk word sense disambiguation algorithm through a distributional semantic model", "author": ["Pierpaolo Basile", "Annalina Caputo", "Giovanni Semeraro"], "venue": "In Proceedings of Coling,", "citeRegEx": "Basile et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Basile et al\\.", "year": 2014}, {"title": "A critique of word similarity as a method of evaluating distributional semantic models", "author": ["Miroslav Batchkarov", "Thomas Kober", "Jeremy Reffin", "Julie Weeds", "David Weir"], "venue": "In Proceedings of the 1st Workshop on Evaluating Vector-Space Representa-", "citeRegEx": "Batchkarov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Batchkarov et al\\.", "year": 2016}, {"title": "A relatedness benchmark to test the role of determiners in compositional distributional semantics", "author": ["Raffaella Bernardi", "Georgiana Dinu", "Marco Marelli", "Marco Baroni"], "venue": "In Proceedings of ACL,", "citeRegEx": "Bernardi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bernardi et al\\.", "year": 2013}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Measuring distributional similarity in context", "author": ["Georgiana Dinu", "Mirella Lapata"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Dinu and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Dinu and Lapata.", "year": 2010}, {"title": "Saarland: Vector-based models of semantic textual similarity", "author": ["Georgiana Dinu", "Stefan Thater"], "venue": "In Proceedings of *SEM/SemEval,", "citeRegEx": "Dinu and Thater.,? \\Q2012\\E", "shortCiteRegEx": "Dinu and Thater.", "year": 2012}, {"title": "An Introduction to the Bootstrap", "author": ["Bradley Efron", "Robert Tibshirani"], "venue": "CRC press", "citeRegEx": "Efron and Tibshirani.,? \\Q1994\\E", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "Investigations on word senses and word usages", "author": ["Katrin Erk", "Diana McCarthy", "Nicholas Gaylord"], "venue": "In Proceedings of ACL/AFNLP,", "citeRegEx": "Erk et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erk et al\\.", "year": 2009}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Grefenstette and Sadrzadeh.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Jointly learning word representations and composition functions using predicate-argument structures", "author": ["Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Hashimoto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "Learning to understand phrases by embedding the dictionary", "author": ["Felix Hill", "KyungHyun Cho", "Anna Korhonen", "Yoshua Bengio"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of ACL,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Sensembed: Learning sense embeddings for word and relational similarity", "author": ["Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In Proceedings of ACL,", "citeRegEx": "Iacobacci et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2015}, {"title": "Embeddings for word sense disambiguation: An evaluation study", "author": ["Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In Proceedings of ACL,", "citeRegEx": "Iacobacci et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2016}, {"title": "A study of entanglement in a categorical framework of natural language", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL)", "citeRegEx": "Kartsaklis and Sadrzadeh.,? \\Q2014\\E", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2014}, {"title": "A unified sentence space for categorical distributional-compositional semantics: Theory and experiments", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman"], "venue": "In Proceedings of Coling,", "citeRegEx": "Kartsaklis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "Separating disambiguation from composition in distributional semantics", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Kartsaklis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2013}, {"title": "Improving sparse word representations with distributional inference for semantic composition", "author": ["Thomas Kober", "Julie Weeds", "Jeremy Reffin", "David Weir"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kober et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2016}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of ACL,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Jiwei Li", "Dan Jurafsky"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Li and Jurafsky.,? \\Q2015\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2015}, {"title": "Semeval-2010 task 14: Word sense induction & disambiguation", "author": ["Suresh Manandhar", "Ioannis Klapaftis", "Dmitriy Dligach", "Sameer Pradhan"], "venue": "In Proceedings of SemEval,", "citeRegEx": "Manandhar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Manandhar et al\\.", "year": 2010}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella bernardi", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Semeval2007 task 10: English lexical substitution task", "author": ["Diana McCarthy", "Robert Navigli"], "venue": "In Proceedings of SemEval,", "citeRegEx": "McCarthy and Navigli.,? \\Q2007\\E", "shortCiteRegEx": "McCarthy and Navigli.", "year": 2007}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A semantic concordance", "author": ["George A. Miller", "Claudia Leacock", "Randee Tengi", "Ross T. Bunker"], "venue": "In Proceedings of the Arpa Workshop on Human Language Technology,", "citeRegEx": "Miller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1993}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of ACL,", "citeRegEx": "Mitchell and Lapata.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Towards dynamic word sense discrimination with random indexing", "author": ["Hans Moen", "Erwin Marsi", "Bj\u00f6rn Gamb\u00e4ck"], "venue": "In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,", "citeRegEx": "Moen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Moen et al\\.", "year": 2013}, {"title": "Entity linking meets word sense disambiguation: A unified approach", "author": ["Andrea Moro", "Alessandro Raganato", "Roberto Navigli"], "venue": null, "citeRegEx": "Moro et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Moro et al\\.", "year": 2014}, {"title": "Discovering word senses from text", "author": ["Patrick Pantel", "Dekang Lin"], "venue": "In Proceedings of SIGKDD,", "citeRegEx": "Pantel and Lin.,? \\Q2002\\E", "shortCiteRegEx": "Pantel and Lin.", "year": 2002}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of ACL,", "citeRegEx": "Paperno et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "De-conflated semantic representations", "author": ["Mohammad Taher Pilehvar", "Nigel Collier"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pilehvar and Collier.,? \\Q2016\\E", "shortCiteRegEx": "Pilehvar and Collier.", "year": 2016}, {"title": "Improving distributional semantic vectors through context selection and normalisation", "author": ["Tamara Polajnar", "Stephen Clark"], "venue": "In Proceedings of EACL,", "citeRegEx": "Polajnar and Clark.,? \\Q2014\\E", "shortCiteRegEx": "Polajnar and Clark.", "year": 2014}, {"title": "Word sense discrimination by clustering contexts in vector and similarity spaces", "author": ["Amruta Purandare", "Ted Pedersen"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Purandare and Pedersen.,? \\Q2004\\E", "shortCiteRegEx": "Purandare and Pedersen.", "year": 2004}, {"title": "Dynamic and static prototype vectors for semantic composition", "author": ["Siva Reddy", "Ioannis Klapaftis", "Diana McCarthy", "Suresh Manandhar"], "venue": "In Proceedings of IJCNLP,", "citeRegEx": "Reddy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2011}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Reisinger and Mooney.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "A word embedding approach to predicting the compositionality of multiword expressions", "author": ["Bahar Salehi", "Paul Cook", "Timothy Baldwin"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Salehi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salehi et al\\.", "year": 2015}, {"title": "Automatic word sense discrimination", "author": ["Hinrich Sch\u00fctze"], "venue": "Computational Linguistics,", "citeRegEx": "Sch\u00fctze.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00fctze.", "year": 1998}, {"title": "The mechanism of additive composition", "author": ["Ran Tian", "Naoaki Okazaki", "Kentaro Inui"], "venue": "CoRR, abs/1511.08407", "citeRegEx": "Tian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2015}, {"title": "Using three way data for word sense discrimination", "author": ["Tim Van de Cruys"], "venue": "In Proceedings of Coling,", "citeRegEx": "Cruys.,? \\Q2008\\E", "shortCiteRegEx": "Cruys.", "year": 2008}, {"title": "Aligning packed dependency trees: a theory of composition for distributional semantics", "author": ["David Weir", "Julie Weeds", "Jeremy Reffin", "Thomas Kober"], "venue": "Computational Linguistics, special issue on Formal Distributional Semantics,", "citeRegEx": "Weir et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Weir et al\\.", "year": 2016}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": null, "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "One sense per collocation", "author": ["David Yarowsky"], "venue": "In Proceedings of the Workshop on Human Language Technology,", "citeRegEx": "Yarowsky.,? \\Q1993\\E", "shortCiteRegEx": "Yarowsky.", "year": 1993}, {"title": "The microsoft research sentence completion challenge", "author": ["Geoffrey Zweig", "J.C. Chris Burges"], "venue": "Technical report, Microsoft Research", "citeRegEx": "Zweig and Burges.,? \\Q2011\\E", "shortCiteRegEx": "Zweig and Burges.", "year": 2011}], "referenceMentions": [{"referenceID": 38, "context": "One approach (Reisinger and Mooney, 2010; Huang et al., 2012) is to try", "startOffset": 13, "endOffset": 61}, {"referenceID": 14, "context": "One approach (Reisinger and Mooney, 2010; Huang et al., 2012) is to try", "startOffset": 13, "endOffset": 61}, {"referenceID": 35, "context": "Similar approaches are proposed by Reddy et al. (2011) and Kartsaklis et al.", "startOffset": 35, "endOffset": 55}, {"referenceID": 18, "context": "(2011) and Kartsaklis et al. (2013) who show that appropriate sense selection or disambigua-", "startOffset": 11, "endOffset": 36}, {"referenceID": 37, "context": "tion typically improves performance for composition of noun phrases (Reddy et al., 2011) and verb phrases (Kartsaklis et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 19, "context": ", 2011) and verb phrases (Kartsaklis et al., 2013).", "startOffset": 25, "endOffset": 50}, {"referenceID": 7, "context": "Dinu and Lapata (2010) proposed a model that represents the meaning of a word as a probability distribu-", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "WordNet to obtain sense labels a priori to creating word representations (Iacobacci et al., 2015), or as a postprocessing step after obtaining initial word representations (Chen et al.", "startOffset": 73, "endOffset": 97}, {"referenceID": 6, "context": ", 2015), or as a postprocessing step after obtaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016).", "startOffset": 82, "endOffset": 129}, {"referenceID": 34, "context": ", 2015), or as a postprocessing step after obtaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016).", "startOffset": 82, "endOffset": 129}, {"referenceID": 14, "context": "While these approaches have exhibited strong performance on benchmark word similarity tasks (Huang et al., 2012; Iacobacci et al., 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al.", "startOffset": 92, "endOffset": 136}, {"referenceID": 15, "context": "While these approaches have exhibited strong performance on benchmark word similarity tasks (Huang et al., 2012; Iacobacci et al., 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al.", "startOffset": 92, "endOffset": 136}, {"referenceID": 22, "context": ", 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al.", "startOffset": 104, "endOffset": 127}, {"referenceID": 39, "context": ", 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al., 2015), and at tasks which require the meaning of a word to be considered in context; e.", "startOffset": 251, "endOffset": 272}, {"referenceID": 16, "context": "g, word sense disambiguation (Iacobacci et al., 2016) and word similarity in context (Iacobacci et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 15, "context": ", 2016) and word similarity in context (Iacobacci et al., 2015).", "startOffset": 39, "endOffset": 63}, {"referenceID": 26, "context": "We compare the 300-dimensional skip-gram word2vec (Mikolov et al., 2013) word embeddings2 to the dependency based version of word2vec \u2014 henceforth dep2vec3 (Levy and Goldberg, 2014) \u2014 and the", "startOffset": 50, "endOffset": 72}, {"referenceID": 21, "context": ", 2013) word embeddings2 to the dependency based version of word2vec \u2014 henceforth dep2vec3 (Levy and Goldberg, 2014) \u2014 and the", "startOffset": 91, "endOffset": 116}, {"referenceID": 15, "context": "dependency-based-word-embeddings/ SENSEMBED model4 by Iacobacci et al. (2015), which creates word-sense embeddings by per-", "startOffset": 54, "endOffset": 78}, {"referenceID": 31, "context": "On the other hand, SENSEMBED utilises Babelfy (Moro et al., 2014) as an external knowledge source to perform word-sense disambiguation and subsequently creates one vector representation per word sense.", "startOffset": 46, "endOffset": 65}, {"referenceID": 12, "context": "For composition we use pointwise addition for all models as this has been shown to be a strong baseline in a number of studies (Hashimoto et al., 2014; Hill et al., 2016).", "startOffset": 127, "endOffset": 170}, {"referenceID": 13, "context": "For composition we use pointwise addition for all models as this has been shown to be a strong baseline in a number of studies (Hashimoto et al., 2014; Hill et al., 2016).", "startOffset": 127, "endOffset": 170}, {"referenceID": 13, "context": "sition function but, similar to Hill et al. (2016), found its performance to be very poor (results not reported).", "startOffset": 32, "endOffset": 51}, {"referenceID": 12, "context": "48 represents the current state-of-the-art performance on this task (Hashimoto et al., 2014; Kober et al., 2016; Wieting et al., 2015).", "startOffset": 68, "endOffset": 134}, {"referenceID": 20, "context": "48 represents the current state-of-the-art performance on this task (Hashimoto et al., 2014; Kober et al., 2016; Wieting et al., 2015).", "startOffset": 68, "endOffset": 134}, {"referenceID": 44, "context": "48 represents the current state-of-the-art performance on this task (Hashimoto et al., 2014; Kober et al., 2016; Wieting et al., 2015).", "startOffset": 68, "endOffset": 134}, {"referenceID": 26, "context": "similarity task of Mitchell and Lapata (2010). This dataset consists of 108 adjective-noun (AN), 108 noun-noun (NN) and 108 verb-object (VO) pairs.", "startOffset": 19, "endOffset": 46}, {"referenceID": 14, "context": "For multi-sense representations, we adapted the strategy which has been used successfully in various word similarity experiments (Huang et al., 2012; Iacobacci et al., 2015).", "startOffset": 129, "endOffset": 173}, {"referenceID": 15, "context": "For multi-sense representations, we adapted the strategy which has been used successfully in various word similarity experiments (Huang et al., 2012; Iacobacci et al., 2015).", "startOffset": 129, "endOffset": 173}, {"referenceID": 28, "context": "Table 2 lists a number of example phrase pairs with low average human similarity scores in the Mitchell and Lapata (2010) test set.", "startOffset": 95, "endOffset": 122}, {"referenceID": 32, "context": "Pantel and Lin (2002), and Van de Cruys (2008) used automatically extracted words from various newswire sources and evaluated the output of their models in comparison to WordNet and EuroWordNet, respectively.", "startOffset": 0, "endOffset": 22}, {"referenceID": 32, "context": "Pantel and Lin (2002), and Van de Cruys (2008) used automatically extracted words from various newswire sources and evaluated the output of their models in comparison to WordNet and EuroWordNet, respectively.", "startOffset": 0, "endOffset": 47}, {"referenceID": 2, "context": "Akkaya et al. (2012) used the concatenation of the SENSEVAL-2 and SENSEVAL-3 tasks and evaluated their models in terms of cluster purity and accuracy.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Akkaya et al. (2012) used the concatenation of the SENSEVAL-2 and SENSEVAL-3 tasks and evaluated their models in terms of cluster purity and accuracy. Finally, Moen et al. (2013) used the semantic textual similarity (STS) 2012 task, which is based on human judgements of the similarity between two sentences.", "startOffset": 0, "endOffset": 179}, {"referenceID": 2, "context": "successfully used in past works (Akkaya et al., 2012; Basile et al., 2014).", "startOffset": 32, "endOffset": 74}, {"referenceID": 3, "context": "successfully used in past works (Akkaya et al., 2012; Basile et al., 2014).", "startOffset": 32, "endOffset": 74}, {"referenceID": 27, "context": "For the construction of our dataset we made use of data from two english dictionaries (Oxford Dictionary and Collins Dictionary), accessible via their respective web APIs6, as well as examples from the sense annotated corpus SemCor (Miller et al., 1993).", "startOffset": 232, "endOffset": 253}, {"referenceID": 15, "context": "egy (Iacobacci et al., 2015) on composed representations to choose the predicted sense10.", "startOffset": 4, "endOffset": 28}, {"referenceID": 9, "context": "significance between the best performing model and the word overlap baseline is computed by using a randomised pairwise permutation test (Efron and Tibshirani, 1994).", "startOffset": 137, "endOffset": 165}, {"referenceID": 45, "context": "One possible explanation for its robust performance on our task is an occurrence of the one-sense-per-collocation hypothesis (Yarowsky, 1993).", "startOffset": 125, "endOffset": 141}, {"referenceID": 41, "context": "There is furthermore evidence that additive composition in low-dimensional word embeddings approximates an intersection of the contexts of two distributional word vectors (Tian et al., 2015).", "startOffset": 171, "endOffset": 190}, {"referenceID": 25, "context": "text are the english lexical substitution task (McCarthy and Navigli, 2007) and the Microsoft sentence completion challenge (Zweig and Burges, 2011).", "startOffset": 47, "endOffset": 75}, {"referenceID": 46, "context": "text are the english lexical substitution task (McCarthy and Navigli, 2007) and the Microsoft sentence completion challenge (Zweig and Burges, 2011).", "startOffset": 124, "endOffset": 148}, {"referenceID": 18, "context": "Dictionary definitions have previously been used to evaluate compositional distributional semantic models where the goal is to match a dictionary entry with its corresponding definition (Kartsaklis et al., 2012; Polajnar and Clark, 2014).", "startOffset": 186, "endOffset": 237}, {"referenceID": 35, "context": "Dictionary definitions have previously been used to evaluate compositional distributional semantic models where the goal is to match a dictionary entry with its corresponding definition (Kartsaklis et al., 2012; Polajnar and Clark, 2014).", "startOffset": 186, "endOffset": 237}, {"referenceID": 1, "context": "2013; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) or sentences (Agirre et al., 2016; Huang et al., 2012; Marelli et al., 2014) in comparison to human pro-", "startOffset": 139, "endOffset": 202}, {"referenceID": 14, "context": "2013; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) or sentences (Agirre et al., 2016; Huang et al., 2012; Marelli et al., 2014) in comparison to human pro-", "startOffset": 139, "endOffset": 202}, {"referenceID": 24, "context": "2013; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) or sentences (Agirre et al., 2016; Huang et al., 2012; Marelli et al., 2014) in comparison to human pro-", "startOffset": 139, "endOffset": 202}, {"referenceID": 4, "context": "atively high variance of judgements and low interannotator agreement (Batchkarov et al., 2016).", "startOffset": 69, "endOffset": 94}, {"referenceID": 0, "context": "Due to not requiring any models to perform an extra step for sense induction, our task is easier to evaluate as no matching between sense clusters identified by a model and some gold standard sense classes needs to be performed, as typically proposed in the WSI literature (Agirre and Soroa, 2007; Manandhar et al., 2010).", "startOffset": 273, "endOffset": 321}, {"referenceID": 23, "context": "Due to not requiring any models to perform an extra step for sense induction, our task is easier to evaluate as no matching between sense clusters identified by a model and some gold standard sense classes needs to be performed, as typically proposed in the WSI literature (Agirre and Soroa, 2007; Manandhar et al., 2010).", "startOffset": 273, "endOffset": 321}, {"referenceID": 14, "context": "Most closely related to our task are the Stanford Contextual Word Similarity (SCWS) dataset by Huang et al. (2012) and the Usage Similar-", "startOffset": 95, "endOffset": 115}, {"referenceID": 10, "context": "ity (USim) task by Erk et al. (2009). The goal in both tasks is to estimate the similarity of two polysemous words in context in comparison to human provided gold standard judgements.", "startOffset": 19, "endOffset": 37}, {"referenceID": 33, "context": "tional semantic models such as the lexical function model (Paperno et al., 2014) or the Anchored Packed Dependency Tree framework (Weir et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 43, "context": ", 2014) or the Anchored Packed Dependency Tree framework (Weir et al., 2016).", "startOffset": 57, "endOffset": 76}], "year": 2017, "abstractText": "In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated through composition alone. We evaluate the performance of off-the-shelf singlevector and multi-sense vector models on a benchmark phrase similarity task and a novel task for word-sense discrimination. We find that single-sense vector models perform as well or better than multi-sense vector models despite arguably less clean elementary representations. Our findings furthermore show that simple composition functions such as pointwise addition are able to recover sense specific information from a single-sense vector model remark-", "creator": "TeX"}}}