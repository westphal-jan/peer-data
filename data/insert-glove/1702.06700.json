{"id": "1702.06700", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Task-driven Visual Saliency and Attention-based Visual Question Answering", "abstract": "stoy Visual question medicean answering (VQA) mezzalama has witnessed stephania great agma progress gadjo since jim May, 50-47 2015 as a classic problem submicroscopic unifying afran visual and textual snoods data into a meanings system. Many 180.3 enlightening VQA works stepin explore 465th deep into the image and recasting question encodings 29,028 and fusing 32.49 methods, discordian of oncidium which attention is versace the kalejs most voyeurism effective hard-drive and infusive mechanism. Current stoppage-time attention nordrhein-westfalen based ajhl methods taare focus on 2507 adequate gdr fusion rufa of visual macgregors and 3,538 textual eshowe features, but lack the 3,000-member attention to enemas where people self-reflexive focus to 6.9-magnitude ask match-winning questions about obote the image. copo Traditional mcfaddin attention khannouchi based methods attach a single value longton to kh\u00e1n the feature at hujar each fanon spatial location, tharp which mistrusts losses slu many useful 2-17 information. in-phase To remedy these problems, dollis we okoth propose a general upu method to say perform huckleberry saliency - like deac pre - selection grenander on overlapped spits region validity features stiff by speakerphone the merkley interrelation prida of bidirectional bastl LSTM (177.00 BiLSTM ), and use romancing a novel element - awoniyi wise haberfeld multiplication pharming based lembede attention method to zarinah capture hajto more decertifying competent kaemmerer correlation serjeants information between visual phytochemistry and satsu textual features. We conduct dillingen experiments bidston on donya the modaf large - scale e-bikes COCO - VQA 255.7 dataset 24-30 and analyze 2:55 the disconnections effectiveness of b.sc our adimmune model demonstrated tietjen by strong empirical 47-year results.", "histories": [["v1", "Wed, 22 Feb 2017 08:19:38 GMT  (907kb,D)", "http://arxiv.org/abs/1702.06700v1", "8 pages, 3 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.NE", "authors": ["yuetan lin", "zhangyang pang", "donghui wang", "yueting zhuang"], "accepted": false, "id": "1702.06700"}, "pdf": {"name": "1702.06700.pdf", "metadata": {"source": "CRF", "title": "Task-driven Visual Saliency and Attention-based Visual Question Answering", "authors": ["Yuetan Lin", "Zhangyang Pang", "Donghui Wang", "Yueting Zhuang"], "emails": ["linyuetan@zju.edu.cn", "pzy@zju.edu.cn", "dhwang@zju.edu.cn", "yzhuang@zju.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "Visual question answering (VQA) comes as a classic task which combines visual and textual modal data into a unified system. Taking an image and a natural language question about it as input, a VQA system is supposed to output the corresponding natural language answer. VQA problem requires image and text understanding, common sense and knowledge inference. The solution of VQA problem will be a great progress in approaching the goal of Visual Turing Test, and is also conducive to tasks such as multi-modal retrieval, image captioning and accessibility facilities.\nAfter the first attempt and introduction of VQA [21], more than thirty works on VQA have sprung up over the past one year from May, 2015. Over ten VQA datasets\n\u2217Corresponding author\nR e\nand a big VQA challenge [3] have been proposed so far. Four commonly used datasets (i.e. DAQUAR [21], COCOQA [24], COCO-VQA [3] and Visual7W [36]) feature different aspects. The common practice to tackle VQA problem is to translate the words as word embeddings and encode the questions using bag-of-word (BoW) or Long Short Term Memory (LSTM) network, and encode the images using deep convolutional neural networks (CNN). The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc. Current VQA works focus on the fusion of these two features, yet no one cares about \u201cwhere we focus\u201d to ask questions on the image. It is a common practice to treat the VQA problem as either a generation, classification or a scoring task, and classification gains more popularity due to its simplicity and easiness for comparison.\nThese works treat VQA as a discriminative model, learning the conditional probability of answer given the image and question. From the generative view, we emulate the behavior that before people ask questions about the given image they first glance at it and find some interesting regions.\n1\nar X\niv :1\n70 2.\n06 70\n0v 1\n[ cs\n.C V\n] 2\n2 Fe\nb 20\nIn terms of a single person, he has unique taste for choosing image regions that interest him. For a large amount of people, there are statistical region-of-interest (RoI) distributions. These region patterns are task-driven, e.g. the picture in Figure. 1, for VQA task people may focus mostly on the beds, the chairs, the laptop and the notebook regions (namely the RoI patterns) as captured in the weighted image, but for image captioning task they pay attention to more areas including the striped floor. It is very valuable to intensify the interesting region features and suppress others, and this image preprocessing step provides more accurate visual features to the follow-up steps and is missing in current VQA works. By analogy with visual saliency which captures the standing out regions or objects of an image, we propose a region pre-selection mechanism named task-driven visual saliency which attaches interesting regions (more possibly questioned on) with higher weights. Taking advantage of the bidirectional LSTM (BiLSTM) [7] that the output at an arbitrary time step has complete and sequential information about all time steps before and after it, we compute the weight of interest for each region feature which is relative to all of them. To the best of our knowledge, this is the first work that employs and analyzes BiLSTM in VQA models for task-driven saliency detection, and this is the first contribution of our work.\nAs a simple and effective VQA baseline method, [35] shows that question feature always contributes more to predict the answer than image feature. But image is as equally important as question for answer generation. It is necessary to further explore finer-grained image features to achieve better VQA performance, e.g. attention mechanism [32]. Current attention based models generally use the correlation scores between question and image representations as weights to perform weighted sum of region features, the resulting visual vector is concatenated to the question vector for final answer generation. The recent \u201cmulti-step\u201d attention models (i.e. containing multiple attention layers) [34, 19] dig deeper into the image understanding and help achieve better VQA performance than the \u201cregular\u201d attention models. However, the correlation score obtained by inner product between visual and textual features is essentially the sum of the correlation vector obtained by elementwise multiplication of the two features. Besides, [3] shows that element-wise multiplication of these features achieves more accurate results than concatenation of them in the baseline model. Hence we propose to employ element-wise multiplication way in the attention mechanism, the fused features are directly feed forward to a max pooling layer to get the final fused feature. Together with the saliency-like region pre-selection operation, this novel attention method effectively improves VQA performance and is the second contribution of this work.\nThe remainder of the paper is organized as follows. We\nfirst briefly review saliency and the attention mechanism. Then, we elaborate our proposed method. We present experiments of some baseline models and compare with stateof-the-art models and visualize the pre-selection saliency and attention maps. Finally we summarize our work."}, {"heading": "2. Related Work", "text": ""}, {"heading": "2.1. Saliency Detection Modeling", "text": "Saliency generally comes from contrasts between a pixel or an object and its surroundings, describing how outstanding it is. It could facilitate learning by focusing the most pertinent regions. Saliency detection methods mimic the human attention in psychology, including both bottomup and top-down manners [29]. Typical saliency methods [10, 18] are pixel- or object-oriented, which are not appropriate for VQA due to center bias and are difficulty in collecting large scale eye tracking data.\nWe think task-driven saliency on image features could be conductive to solving VQA problem. What inspires us is that BiLSTM used in saliency detection has achieved good results on text and video tasks. In sentiment classification tasks, [16] assigns saliency scores to words related to sentiment for visualizing and understanding the effects of BiLSTM in textual sentence. While in video highlight detection, [33] uses a recurrent auto-encoder configured with BiLSTM cells and extracts video highlight segments effectively. BiLSTM has demonstrated its effectiveness in saliency detection, but to the best of our knowledge it has not been used in visual saliency for VQA task."}, {"heading": "2.2. Attention in VQA Models", "text": "Visual attention mechanism has drawn great interest in VQA [34, 36, 26] and gained performance improvement from traditional methods using holistic image features. Attention mechanism is typically the weighted sum of the image region features at each spatial location, where the weights describe the correlation and are implemented as the inner products of the question and image features. It explores finer-grained visual features and mimics the behavior that people attend to different areas according to the questions. Focusing on \u201cknowing where to look\u201d for multiplechoice VQA tasks, [26] uses 99 detected object regions plus a holistic image feature to make correlation with the question encoding, and uses the correlation scores as weights to fuse the features. [34] uses the last pooling layer features (512 \u00d7 14 \u00d7 14) of VGG-19 [27] as image region partitions, and adopts two-layer attention to obtain more effective fused features for complex questions. [2] proposes an ingenious idea to use assembled network modules according to the parsed questions, and achieves multi-step transforming attention by specific rules.\nHowever, these attention methods use correlation score\n(i.e. inner product between visual and textual feature) for each location, which is the sum of the correlation vector representation (i.e. element-wise multiplication between them). Besides, the concatenation of image and question features is less accurate than the element-wise multiplication vector of them shown in the baseline model [3]. Moreover, there are many answers derived from non-object and background regions, e.g. questions about scenes, hence it is not fit for the object detection based attention methods."}, {"heading": "3. Proposed Method", "text": "Compared with image captioning which generates general descriptions about an image, VQA focuses on specific image regions depending on the question. On the one hand, these regions include non-object and background contents which are hard for object detection based VQA methods. On the other hand, although people may ask questions at any areas of a given image, there are always some region patterns that attract more questions. On the whole, there are statistical region-of-interest (RoI) patterns which represent human-interested areas that are important for later VQA task. We propose a saliency-like region pre-selection and attention-based VQA framework illustrated in Figure. 2. The VQA is regarded as a classification task, which is simple and easy to transform to a generating or scoring model."}, {"heading": "3.1. Model", "text": "In this section, we elaborate our model consisting of four parts: (a) image feature pre-selection part which models the tendency where people focus to ask questions, (b) question encoding part which encodes the question words as a condensed semantic embedding, (c) attention-based feature fusion part performs second selection on image features and (d) answer generation part which gives the answer output."}, {"heading": "3.1.1 Image Feature Pre-selection", "text": "As described above, current object detection based VQA methods may not be qualified and the answers may not be derived from these specific object regions in images, for example, when asked \u201cWhere is the bird/cat?\u201d, the answers \u201cfence/sink\u201d are not contained in ILSVRC [25] (200 categories) and Pascal VOC [5] (20 categories) detection classes. Thus we use a more general pattern detector.\nIn addition, from the generative perspective, we pay attention to where people focus to ask questions. General visual saliency provides analogous useful information of noticeable objects or areas which outstand the surroundings, but it is not the only case for VQA task. Current attention mechanism relates the question to the focusing location. As more samples are available, we could yield the region patterns that attract more questions by statistics. From the statistical behavior of large amounts of workers on Amazon\nMechanical Turk (AMT) who have labeled the questions, we model the region-of-interest patterns that could attract more questions.\nWe propose to perform saliency-like pre-selection operation to alleviate the problems and model the RoI patterns. The image is first divided into g \u00d7 g grids as illustrated in Figure. 2. Takingm\u00d7m grids as a region, with s grids as the stride, we obtain n\u00d7n regions, where n = \u230a g\u2212m s \u230b +1. We then feed the regions to a pre-trained ResNet [8] deep convolutional neural network to produce n\u00d7n\u00d7dI -dimensional region features, where dI is the dimension of feature from the layer before the last fully-connected layer.\nSince the neighboring overlapped regions share some visual contents, the corresponding features are related but focusing on different semantic information. We regard the sequence of regions as the result of eye movement when glancing at the image, and these regions are selectively allocated different degrees of interest. Specifically, the LSTM is a special kind of recurrent neural network (RNN), capable of learning long-term dependencies via the memory cell and the update gates, which endows itself with the ability to retain information of previous time-steps (i.e. the previous region sequence in this case). The update rules of the LSTM at time step t are as follows:\nit = \u03c3(W (i)xt + U (i)ht\u22121 + b (i)), (1)\nft = \u03c3(W (f)xt + U (f)ht\u22121 + b (f)), (2)\not = \u03c3(W (o)xt + U (o)ht\u22121 + b (o)), (3)\nut = tanh(W (u)xt + U (u)ht\u22121 + b (u)), (4)\nct = ut it + ct\u22121 ft, (5) ht = ot tanh(ct), (6)\nwhere i, f, o denote the input, forget and output gates, x, c, h are the input region feature, memory cell and hidden unit output, and W,U, b are the parameters to be trained. We activate the gates by the sigmoid nonlinearity \u03c3(x) = 1/(1+ e\u2212x) and the cell contents by the hyperbolic tangent tanh(x) = (ex \u2212 e\u2212x)/(ex + e\u2212x). The gates control the information in the memory cell to be retained or forgotten through element-wise multiplication .\nInspired by the information completeness and high performance of BiLSTM, we encode the region features in two directions using BiLSTM and obtain a scalar output per region. The output of the BiLSTM is the summation of the forward and backward LSTM outputs at this region location: ht = h (f) t + h (b) n\u2212t+1, where n is the number of regions, h(f)t , h (b) n\u2212t+1 are computed using Eq. 6. Hence, the output at each location is influenced by the region features before and after it, which embodies the correlation among these regions. Note that, although the DMN+ work [31] uses similar bi-directional gated recurrent units (BiGRU) in the visual input module, their purpose is to produce input\nfacts which contain global information. Besides, their BiGRU takes the features embedded to the textual space as inputs. In contrast, the BiLSTM used in our model takes directly visual CNN features as input, and the main purpose is to output weights for region feature selection.\nThe output values of the BiLSTM are normalized through a softmax layer, and the resulting weights are multiplied by the region features. We treat the weights as degree of interest which are trained by error back-propagation of the final class cross entropy losses, and higher weights embody that the corresponding region patterns will attract more questions, in other words, these region patterns may get higher attention values in the latter interaction with question embeddings in a statistical way."}, {"heading": "3.1.2 Question Encoding", "text": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc. Since question BoW encodings already dominate the contribution to answer generation compared with the image features [35], we simply encode the question word as word2vec embedding, and use LSTM to encode the questions to match the pre-selected region features. To encode more abstract and higher-level information and achieve better performance, a deeper LSTM [3, 12] for question encoding is adopted in our model.\nThe question encoding LSTM in our model has l hidden layers with r hidden units per layer, and the question representation is the last output and the cell units of the LSTM,\nand the dimension is dQ = 2 \u00d7 l \u00d7 r. The resulting condensed feature vector encodes the semantic and syntactic information of the question."}, {"heading": "3.1.3 Attention-based Feature Fusion", "text": "According to the statistic image-question-answer (IQA) training triples, the image feature pre-selection has attached the regions with different prior weights, generating more meaningful region features. But different questions may focus on different aspects of the visual content. It is necessary to use attention mechanism to second select regions by the question for more effective features.\nWe propose a novel attention method, which takes the element-wise multiplication vector as correlation between image and question features at each spatial location. Specifically, given the pre-selected region features and question embedding, we map the visual and textual features into a common space of dC dimension and perform element-wise multiplication between them. The n\u00d7 n\u00d7 dC-dimensional fused features contain visual and textual information, and higher responses indicate more correlative features. In traditional attention models, the correlation score (scalar) achieved by inner product between the mapped visual and textual features per region, is essentially the sum of elements in our fused feature. This novel attention method has two noticeable advantages against traditional attention, i.e. information richer correlation vector versus correlation scalar, more effective element-wise multiplication vector versus the concatenated vector of the visual and textual features.\nSince higher responses in the fused features indicate\nmore correlative visual and textual features, and the question may only focus on one or two regions. We choose to apply max pooling operation on the intermediate fused features to pick out the maximum responses. The produced dC-dimensional fused feature is then fed to the final answer generation part. Compared to the sum/average operation in traditional attention models, the max operation highlights the responses of the final fused feature from every spatial location."}, {"heading": "3.1.4 Answer Generation", "text": "Taking the VQA problem as a classification task is simple to be implemented and evaluated, and it is easy to be extended to generation or multiple choice tasks through a network surgery using the fused feature in the previous step. We use a linear layer and a softmax layer to map from the fused feature to the answer candidates, of which the entries are the top-1000 answers from the training data.\nConsidering multiple choice VQA problems, e.g. Visual7W [36] telling questions and COCO-VQA [3] multiple choice tasks, our model is adaptive to be extended by concatenating the question and answer vectors before fusion with visual features or by using bilinear model between the final fused feature and answer feature [26, 11], which is a possible future work. Meanwhile, in view of generation VQA problem, we can train an LSTM taking the fused feature as input to obtain answer word lists, phrases or sentences [22, 6]."}, {"heading": "3.2. Training", "text": "Our framework is trained end-to-end using backpropagation, while the feature extraction part using ResNet is kept fixed to speed up training and avoid the noisy gradients back-propagated from the LSTM as elaborated in [6]. RMSprop algorithm is employed with low initial learning rate of 3e-4 which is proved important to prevent the softmax from spiking too early and prevent the visual features from dominating too early [26]. Due to simplicity and proved similar performance as pre-trained word embedding parameters, we initialize the parameters of the network with random numbers. We randomly sample 500 IQA triples per iteration."}, {"heading": "4. Experiments", "text": "In this section, we describe the implementation details and evaluate our model (SalAtt) on the large-scale COCOVQA dataset. Besides, we visualize and analyze the role of pre-selection and the novel attention method."}, {"heading": "4.1. Implementation Details", "text": "In our experiment, the input images are first scaled to 448\u00d7 448\u00d7 3 pixels before we apply 4\u00d7 4 grids on them.\nWe obtain 3 \u00d7 3 regions by employing 2 \u00d7 2 grids (i.e. 224\u00d7224\u00d73 pixels) as a region with stride 1 grid. Then we extract the 2048-D feature per region from the layer before the last fully-connected layer of ResNet. The dimension of word embedding is 200, and the weights of the embedding are initialized randomly from a uniform distribution on [\u22120.08, 0.08) due to similar performance as the pre-trained one. The pre-selection BiLSTM for region features has 1 layer and the size is 1, and the LSTM for question uses 2 layers and 512 hidden units per layer. The common space of visual and textual features is 1024-dimensional. We use dropout [28] after all convolutional and linear layers. The non-linear function is hyperbolic tangent.\nThe training procedure is early stopped when there is no accuracy increase in validation set for 5, 000 iterations where we evaluate every 1, 000 iterations. It takes around 18 hours to train our model on a single NVIDIA Tesla K40 GPU for about 91, 000 iterations. And for evaluation, each sample needs less than 0.5 millisecond."}, {"heading": "4.2. Datasets", "text": "The COCO-VQA dataset [3] is the largest among the commonly used VQA datasets, which contains two tasks (i.e. multiple-choice task and open-ended task) on two image datasets (i.e. real image MSCOCO dataset [17] and abstract scene dataset). We follow the common practice to evaluate models on two tasks on the real image dataset, which includes 248,349 training questions, 121,512 validation questions and 244,302 testing questions. There are many types of questions which require image and question understanding, commonsense knowledge, knowledge inference and even external knowledge. The answers are roughly divided into 3 types, i.e. \u201cyes/no\u201d, \u201cnumber\u201d and \u201cother\u201d.\nTo evaluate the results, each answer is compared with 10 human-labeled answers, the accuracy is computed via this metric: min(#consistent human\u2212labeled answers3 , 1), i.e. the accuracy is 100% if the predicted answer is consistent with at least 3 human-labeled answers. The COCOVQA dataset provide human-labeled answers for the training and validation sets, and the results of testing set can only be tested on the evaluation server. The whole testing set is named test-standard and can be evaluated once per day and 5 times in total, and a smaller development set named testdev can be tested 10 times per day and 9999 times in total. In short, the COCO-VQA dataset is large and hard enough for evaluating models and hence we choose to evaluate our model on it."}, {"heading": "4.3. Compared Models", "text": "We compare our propose model (SalAtt) with some function-disabled models listed below, to prove the effectiveness of the region pre-selection via BiLSTM and the novel attention method.\n\u2022 holistic: The baseline model which maps the holistic image feature and LSTM-encoded question feature to a common space and perform element-wise multiplication between them.\n\u2022 TraAtt: The traditional attention model, implementation of WTL model [26] using the same 3\u00d7 3 regions in SalAtt model.\n\u2022 RegAtt: The region attention model which employs our novel attention method, same as the SalAtt model but without region pre-selection.\n\u2022 ConAtt: The convolutional region pre-selection attention model which replaces the BiLSTM in SalAtt model with a weight-sharing linear mapping, implemented by a convolutional layer.\nBesides, we also compare our SalAtt model with the popular baseline models i.e. iBOWIMG [35], VQA [3], and the state-of-the-art attention-based models i.e. WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA."}, {"heading": "4.4. Results and Analysis", "text": ""}, {"heading": "4.4.1 Effectiveness of Proposed Functions", "text": "We train the function-disabled models on COCO-VQA training set and show the accuracies on validation set in Table. 1. From the columns, we can see that: (1) holistic is better than TraAtt, proving the effectiveness of elementwise multiplication feature fusion compared with concatenation of features. (2) RegAtt is better than holistic, indicating our novel attention method indeed enriches the visual features and improves the performance. (3) SalAtt is better than RegAtt, demonstrating the strength of our region pre-selection mechanism. (4) ConAtt is worse than SalAtt, showing that BiLSTM is important for the region pre-selection part. From each row, we find the consistent improvement by the ResNet features, showing the importance of good CNN features to VQA."}, {"heading": "4.4.2 Quantitative Results on Evaluation Server", "text": "We summarize the accuracies on test-dev in Table. 2 and the test-standard results in Table. 3. Our results are comparative or higher than the attention based methods, especially on multiple-choice tasks. The results on answer type\n\u201cother\u201d, which includes object and scene type questions, demonstrate the competence of our model in RoI detection.\nNote that, we only apply the proposed region preselection mechanism to the basic VQA model [3], it can be embedded into any other attention-based models to improve their performance. Due to computation and training time, we use only 3 \u00d7 3 regions compared with other attention-based methods (e.g. 100 or 14 \u00d7 14 region features). Through observation, we find that many small objects could not be split by the 3\u00d73 regions, which is adverse to the counting questions and could be further improved and is a possible future work."}, {"heading": "4.4.3 Visualization of Intermediate Results", "text": "We illustrate three groups of samples produced by our model in Figure. 3. Each group contains four figures, from left to right and from top to bottom, they are respectively the original image, pre-selection weights on the image, and two attention maps for different questions with the corresponding questions (Q), ground truth answers (A) and the predicted answers (P) shown below them. And the number in the parentheses means the amount for this human-labeled answer entry. The weights are normalized to have minimum 0 and maximum 1 for visualization enhancement, i.e. the weight in the dark region may not necessarily be 0.\nTake the first sample for example, the pre-selection operation gives high weight to the boy\u2019s head region which may be interesting to people and attract more questions (e.g. questions containing the word \u201cboy\u201d). For the question \u201cIs the boy dressed for the weather?\u201d, the attention map focuses on the boy, his clothes and the surrounding regions to get a positive answer. While for question \u201cWhat is the boy doing?\u201d, it attends the boy and the snowboard, thus giving answer \u201csnowboarding\u201d. The third sample gives inaccurate but explainable answers, i.e. the birds may live in the park/zoo and come for food provided by the tourist so it may not be classified into pets, and the left hand of the woman holds indeed a phone while the human-labeled answers focus on the right hand."}, {"heading": "5. Conclusion", "text": "In this work, we propose a general VQA solution which integrates region pre-selection and a novel attention method to capture generic class region and richer fused feature representation. These two procedures are independent, meanwhile they both contribute to better VQA performance. Although the model is simple, it achieves comparative or higher empirical results than state-of-the-art models.\nPossible future works include adopting finer-grained grids which capture more precise regions, employing stacked attention layers for multi-step reasoning and more accurate answer location, and applying the general pre-\nselection method to other attention-based VQA models. The pre-selection mechanism is valuable and applicable to similar task, such as image captioning."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "NAACL,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "CVPR, pages 39\u201348,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV, pages 2425\u20132433,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "On the properties of neural machine translation: Encoderdecoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 88(2):303\u2013338,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS, pages 2296\u20132304,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, 18(5):602\u2013610,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR, June", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["I. Ilievski", "S. Yan", "J. Feng"], "venue": "ECCV,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "A model of saliencybased visual attention for rapid scene analysis", "author": ["L. Itti", "C. Koch", "E. Niebur"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Revisiting visual question answering baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "arXiv preprint arXiv:1606.08390,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "L. Fei-Fei"], "venue": "ICLR Workshop,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal residual learning for visual qa", "author": ["J.-H. Kim", "S.-W. Lee", "D.-H. Kwak", "M.-O. Heo", "J. Kim", "J.- W. Ha", "B.-T. Zhang"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "NIPS, pages 3294\u20133302,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "ACL, pages 423\u2013430. Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Visualizing and understanding neural models in nlp", "author": ["J. Li", "X. Chen", "E. Hovy", "D. Jurafsky"], "venue": "NAACL,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, pages 740\u2013755. Springer,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to detect a salient object", "author": ["T. Liu", "Z. Yuan", "J. Sun", "J. Wang", "N. Zheng", "X. Tang", "H.-Y. Shum"], "venue": "PAMI, 33(2):353\u2013367,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["L. Ma", "Z. Lu", "H. Li"], "venue": "AAAI,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS, pages 1682\u20131690,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV, pages 1\u20139,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P.H. Seo", "B. Han"], "venue": "CVPR, June", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS, pages 2953\u20132961,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "CVPR, June", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Task-dependent learning of attention", "author": ["P. Van De Laar", "T. Heskes", "S. Gielen"], "venue": "Neural Networks,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1997}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. v. d. Hengel", "A. Dick"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "ICML,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "ICML, volume 37, pages 2048\u20132057,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised extraction of video highlights via robust recurrent auto-encoders", "author": ["H. Yang", "B. Wang", "S. Lin", "D. Wipf", "M. Guo", "B. Guo"], "venue": "ICCV, pages 4633\u20134641,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "CVPR, June", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR, June", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 20, "context": "After the first attempt and introduction of VQA [21], more than thirty works on VQA have sprung up over the past one year from May, 2015.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "and a big VQA challenge [3] have been proposed so far.", "startOffset": 24, "endOffset": 27}, {"referenceID": 20, "context": "DAQUAR [21], COCOQA [24], COCO-VQA [3] and Visual7W [36]) feature different aspects.", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "DAQUAR [21], COCOQA [24], COCO-VQA [3] and Visual7W [36]) feature different aspects.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "DAQUAR [21], COCOQA [24], COCO-VQA [3] and Visual7W [36]) feature different aspects.", "startOffset": 35, "endOffset": 38}, {"referenceID": 35, "context": "DAQUAR [21], COCOQA [24], COCO-VQA [3] and Visual7W [36]) feature different aspects.", "startOffset": 52, "endOffset": 56}, {"referenceID": 34, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 163, "endOffset": 174}, {"referenceID": 21, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 163, "endOffset": 174}, {"referenceID": 5, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 163, "endOffset": 174}, {"referenceID": 2, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 203, "endOffset": 206}, {"referenceID": 22, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 235, "endOffset": 239}, {"referenceID": 30, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 256, "endOffset": 260}, {"referenceID": 25, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 282, "endOffset": 294}, {"referenceID": 12, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 282, "endOffset": 294}, {"referenceID": 18, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 282, "endOffset": 294}, {"referenceID": 6, "context": "Taking advantage of the bidirectional LSTM (BiLSTM) [7] that the output at an arbitrary time step has complete and sequential information about all time steps before and after it, we compute the weight of interest for each region feature which is relative to all of them.", "startOffset": 52, "endOffset": 55}, {"referenceID": 34, "context": "As a simple and effective VQA baseline method, [35] shows that question feature always contributes more to predict the answer than image feature.", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "attention mechanism [32].", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": "containing multiple attention layers) [34, 19] dig deeper into the image understanding and help achieve better VQA performance than the \u201cregular\u201d attention models.", "startOffset": 38, "endOffset": 46}, {"referenceID": 18, "context": "containing multiple attention layers) [34, 19] dig deeper into the image understanding and help achieve better VQA performance than the \u201cregular\u201d attention models.", "startOffset": 38, "endOffset": 46}, {"referenceID": 2, "context": "Besides, [3] shows that element-wise multiplication of these features achieves more accurate results than concatenation of them in the baseline model.", "startOffset": 9, "endOffset": 12}, {"referenceID": 28, "context": "Saliency detection methods mimic the human attention in psychology, including both bottomup and top-down manners [29].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "Typical saliency methods [10, 18] are pixel- or object-oriented, which are not appropriate for VQA due to center bias and are difficulty in collecting large scale eye tracking data.", "startOffset": 25, "endOffset": 33}, {"referenceID": 17, "context": "Typical saliency methods [10, 18] are pixel- or object-oriented, which are not appropriate for VQA due to center bias and are difficulty in collecting large scale eye tracking data.", "startOffset": 25, "endOffset": 33}, {"referenceID": 15, "context": "In sentiment classification tasks, [16] assigns saliency scores to words related to sentiment for visualizing and understanding the effects of BiLSTM in textual sentence.", "startOffset": 35, "endOffset": 39}, {"referenceID": 32, "context": "While in video highlight detection, [33] uses a recurrent auto-encoder configured with BiLSTM cells and extracts video highlight segments effectively.", "startOffset": 36, "endOffset": 40}, {"referenceID": 33, "context": "Visual attention mechanism has drawn great interest in VQA [34, 36, 26] and gained performance improvement from traditional methods using holistic image features.", "startOffset": 59, "endOffset": 71}, {"referenceID": 35, "context": "Visual attention mechanism has drawn great interest in VQA [34, 36, 26] and gained performance improvement from traditional methods using holistic image features.", "startOffset": 59, "endOffset": 71}, {"referenceID": 25, "context": "Visual attention mechanism has drawn great interest in VQA [34, 36, 26] and gained performance improvement from traditional methods using holistic image features.", "startOffset": 59, "endOffset": 71}, {"referenceID": 25, "context": "Focusing on \u201cknowing where to look\u201d for multiplechoice VQA tasks, [26] uses 99 detected object regions plus a holistic image feature to make correlation with the question encoding, and uses the correlation scores as weights to fuse the features.", "startOffset": 66, "endOffset": 70}, {"referenceID": 33, "context": "[34] uses the last pooling layer features (512 \u00d7 14 \u00d7 14) of VGG-19 [27] as image region partitions, and adopts two-layer attention to obtain more effective fused features for complex questions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[34] uses the last pooling layer features (512 \u00d7 14 \u00d7 14) of VGG-19 [27] as image region partitions, and adopts two-layer attention to obtain more effective fused features for complex questions.", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "[2] proposes an ingenious idea to use assembled network modules according to the parsed questions, and achieves multi-step transforming attention by specific rules.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Besides, the concatenation of image and question features is less accurate than the element-wise multiplication vector of them shown in the baseline model [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 24, "context": "As described above, current object detection based VQA methods may not be qualified and the answers may not be derived from these specific object regions in images, for example, when asked \u201cWhere is the bird/cat?\u201d, the answers \u201cfence/sink\u201d are not contained in ILSVRC [25] (200 categories) and Pascal VOC [5] (20 categories) detection classes.", "startOffset": 268, "endOffset": 272}, {"referenceID": 4, "context": "As described above, current object detection based VQA methods may not be qualified and the answers may not be derived from these specific object regions in images, for example, when asked \u201cWhere is the bird/cat?\u201d, the answers \u201cfence/sink\u201d are not contained in ILSVRC [25] (200 categories) and Pascal VOC [5] (20 categories) detection classes.", "startOffset": 305, "endOffset": 308}, {"referenceID": 7, "context": "We then feed the regions to a pre-trained ResNet [8] deep convolutional neural network to produce n\u00d7n\u00d7dI -dimensional region features, where dI is the dimension of feature from the layer before the last fully-connected layer.", "startOffset": 49, "endOffset": 52}, {"referenceID": 30, "context": "Note that, although the DMN+ work [31] uses similar bi-directional gated recurrent units (BiGRU) in the visual input module, their purpose is to produce input", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc.", "startOffset": 113, "endOffset": 121}, {"referenceID": 33, "context": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc.", "startOffset": 113, "endOffset": 121}, {"referenceID": 3, "context": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc.", "startOffset": 151, "endOffset": 154}, {"referenceID": 13, "context": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc.", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc.", "startOffset": 221, "endOffset": 225}, {"referenceID": 34, "context": "Since question BoW encodings already dominate the contribution to answer generation compared with the image features [35], we simply encode the question word as word2vec embedding, and use LSTM to encode the questions to match the pre-selected region features.", "startOffset": 117, "endOffset": 121}, {"referenceID": 2, "context": "To encode more abstract and higher-level information and achieve better performance, a deeper LSTM [3, 12] for question encoding is adopted in our model.", "startOffset": 99, "endOffset": 106}, {"referenceID": 11, "context": "To encode more abstract and higher-level information and achieve better performance, a deeper LSTM [3, 12] for question encoding is adopted in our model.", "startOffset": 99, "endOffset": 106}, {"referenceID": 35, "context": "Visual7W [36] telling questions and COCO-VQA [3] multiple choice tasks, our model is adaptive to be extended by concatenating the question and answer vectors before fusion with visual features or by using bilinear model between the final fused feature and answer feature [26, 11], which is a possible future work.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "Visual7W [36] telling questions and COCO-VQA [3] multiple choice tasks, our model is adaptive to be extended by concatenating the question and answer vectors before fusion with visual features or by using bilinear model between the final fused feature and answer feature [26, 11], which is a possible future work.", "startOffset": 45, "endOffset": 48}, {"referenceID": 25, "context": "Visual7W [36] telling questions and COCO-VQA [3] multiple choice tasks, our model is adaptive to be extended by concatenating the question and answer vectors before fusion with visual features or by using bilinear model between the final fused feature and answer feature [26, 11], which is a possible future work.", "startOffset": 271, "endOffset": 279}, {"referenceID": 10, "context": "Visual7W [36] telling questions and COCO-VQA [3] multiple choice tasks, our model is adaptive to be extended by concatenating the question and answer vectors before fusion with visual features or by using bilinear model between the final fused feature and answer feature [26, 11], which is a possible future work.", "startOffset": 271, "endOffset": 279}, {"referenceID": 21, "context": "Meanwhile, in view of generation VQA problem, we can train an LSTM taking the fused feature as input to obtain answer word lists, phrases or sentences [22, 6].", "startOffset": 151, "endOffset": 158}, {"referenceID": 5, "context": "Meanwhile, in view of generation VQA problem, we can train an LSTM taking the fused feature as input to obtain answer word lists, phrases or sentences [22, 6].", "startOffset": 151, "endOffset": 158}, {"referenceID": 5, "context": "Our framework is trained end-to-end using backpropagation, while the feature extraction part using ResNet is kept fixed to speed up training and avoid the noisy gradients back-propagated from the LSTM as elaborated in [6].", "startOffset": 218, "endOffset": 221}, {"referenceID": 25, "context": "RMSprop algorithm is employed with low initial learning rate of 3e-4 which is proved important to prevent the softmax from spiking too early and prevent the visual features from dominating too early [26].", "startOffset": 199, "endOffset": 203}, {"referenceID": 27, "context": "We use dropout [28] after all convolutional and linear layers.", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "The COCO-VQA dataset [3] is the largest among the commonly used VQA datasets, which contains two tasks (i.", "startOffset": 21, "endOffset": 24}, {"referenceID": 16, "context": "real image MSCOCO dataset [17] and abstract scene dataset).", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "\u2022 TraAtt: The traditional attention model, implementation of WTL model [26] using the same 3\u00d7 3 regions in SalAtt model.", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "iBOWIMG [35], VQA [3], and the state-of-the-art attention-based models i.", "startOffset": 8, "endOffset": 12}, {"referenceID": 2, "context": "iBOWIMG [35], VQA [3], and the state-of-the-art attention-based models i.", "startOffset": 18, "endOffset": 21}, {"referenceID": 25, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 4, "endOffset": 8}, {"referenceID": 1, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 14, "endOffset": 17}, {"referenceID": 33, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 23, "endOffset": 27}, {"referenceID": 29, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 54, "endOffset": 57}, {"referenceID": 30, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "Note that, we only apply the proposed region preselection mechanism to the basic VQA model [3], it can be embedded into any other attention-based models to improve their performance.", "startOffset": 91, "endOffset": 94}], "year": 2017, "abstractText": "Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.", "creator": "LaTeX with hyperref package"}}}