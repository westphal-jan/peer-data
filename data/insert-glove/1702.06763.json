{"id": "1702.06763", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples", "abstract": "Recent mtct studies archdiocesan have parlays shown aftereffects that uchinoura deep neural grabbi networks (DNN) are evite vulnerable to adversarial samples: margiela maliciously - reinsurance perturbed k\u0131z\u0131lcahamam samples hiroo crafted solicits to yield incorrect model schepers outputs. Such 74.99 attacks can severely bodor undermine DNN systems, pre-olympic particularly tabb in cki security - gallimore sensitive 1680 settings. god-daughter It contrabando was chauth observed saldate that an slavic-speaking adversary could succesful easily generate adversarial garraf samples g\u00f6tterd\u00e4mmerung by making nohl a benatar small perturbation anticipation on irrelevant 2,353 feature dimensions siebels that are homebuilt unnecessary razorbacks for the current galaxie classification waff task. To overcome navua this barbery problem, cobitis we introduce spolia a melee defensive mechanism called DeepMask. By taitz identifying deniau and self-consistent removing unnecessary 68.70 features replicants in paisano a metrostudy DNN illustrates model, rigdon DeepMask limits the pardalis capacity an attacker can use generating adversarial mckale samples and hydroxymethyl therefore elmdon increase pow\u0105zki the 7:52 robustness against such inputs. goalies Comparing with jevon other aest defensive maffra approaches, DeepMask is easy codice to implement and saint-s\u00e9verin computationally nidwalden efficient. Experimental chengshan results khristina show that binoculars DeepMask 1894-1896 can kalaje increase the thurstone performance softens of sremski state - mutasim of - the - wachs art sudipta DNN depositories models against adversarial rebeccac samples.", "histories": [["v1", "Wed, 22 Feb 2017 11:48:35 GMT  (209kb,D)", "http://arxiv.org/abs/1702.06763v1", "adversarial samples, deep neural network"], ["v2", "Wed, 1 Mar 2017 05:55:00 GMT  (216kb,D)", "http://arxiv.org/abs/1702.06763v2", "adversarial samples, deep neural network"], ["v3", "Thu, 2 Mar 2017 16:42:12 GMT  (216kb,D)", "http://arxiv.org/abs/1702.06763v3", "adversarial samples, deep neural network"], ["v4", "Wed, 8 Mar 2017 17:18:19 GMT  (289kb,D)", "http://arxiv.org/abs/1702.06763v4", "adversarial samples, deep neural network"], ["v5", "Fri, 10 Mar 2017 00:02:04 GMT  (329kb,D)", "http://arxiv.org/abs/1702.06763v5", "adversarial samples, deep neural network"], ["v6", "Tue, 21 Mar 2017 16:26:33 GMT  (388kb,D)", "http://arxiv.org/abs/1702.06763v6", "adversarial samples, deep neural network"], ["v7", "Sat, 25 Mar 2017 22:16:05 GMT  (394kb,D)", "http://arxiv.org/abs/1702.06763v7", "adversarial samples, deep neural network"], ["v8", "Mon, 17 Apr 2017 21:54:30 GMT  (395kb,D)", "http://arxiv.org/abs/1702.06763v8", "adversarial samples, deep neural network"]], "COMMENTS": "adversarial samples, deep neural network", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CR", "authors": ["ji gao", "beilun wang", "zeming lin", "weilin xu", "yanjun qi"], "accepted": false, "id": "1702.06763"}, "pdf": {"name": "1702.06763.pdf", "metadata": {"source": "CRF", "title": "DEEPMASK: MASKING DNN MODELS FOR ROBUST- NESS AGAINST ADVERSARIAL SAMPLES", "authors": ["Ji Gao", "Beilun Wang", "Yanjun Qi"], "emails": ["jg6yd@virginia.edu", "bw4mw@virginia.edu", "yanjun@virginia.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep Neural Networks (DNNs) have achieved great success in a variety of applications. Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples.\nMany recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value. A recent paper Papernot et al. (2016a) proposed a black-box attack, which first approximates the target classifier and then generate adversarial samples to the approximated model.\nResearchers also studied how to defend attacks enforced by adversarial samples. Goodfellow et al. (2014) showed retraining the model using adversarial samples can improve the adversarial robustness of the model. Papernot et al. (2016b) proposed defensive distillation to make the model less sensible to gradient change. Wang et al. (2016) showed that one cause of adversarial sample is those redundant features that are irrelevant to classification. A perturbation along an irrelevant feature dimension can easily fool a classifier.\nIn this paper, we introduce a new defensive approach for DNN models: DeepMask. The motivation of DeepMask is to increase model robustness by removing irrelevant features. Comparing to previous defensive approach, DeepMask have the following benefits: 1. DeepMask doesn\u2019t need extra\nar X\niv :1\n70 2.\n06 76\n3v 1\n[ cs\n.L G\n] 2\n2 Fe\nb 20\n17\ntraining process, therefore it is computationally efficient. 2. DeepMask can be easily applied to any base DNN models.\nIn the rest part, Section 2 briefly introduces background of adversarial samples and its relationship to irrelevant features. Section 3 introduces a new defensive approach DeepMask. It removes irrelevant features in a trained DNN, and thus increase the adversarial robustness, which is simple and powerful.In Section 4, experiment results show that DeepMask works effectively in different DNN settings."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 ADVERSARIAL SAMPLES", "text": "For the following analysis, we denote a base DNN classifier as F (\u00b7) : X \u2192 Y. X represents input space. Y denotes the output space. x \u2208 X denotes a sample. Adversarial samples are deliberately created samples. We define of an adversarial sample x\u2032 as:\nx\u2032 = x+ \u2206x, |\u2206x| < F (x) 6= F (x\u2032) (2.1)\n\u2206x needs to be small, so that x and x\u2032 are imperceptible to human eyes for image classification. However, for the machine classifier F , it classifies x and x\u2032 into two different classes. Therefore, such sample x\u2032 can successfully fool a machine classifier."}, {"heading": "2.2 ADVERSARIAL SAMPLES AND IRRELEVANT FEATURES", "text": "Wang et al. (2016) show that the effectiveness of adversarial samples is related to extra irrelevant features extracted by the machine classifier. If a classifier extracts many irrelevant features in the training process, it will be vulnerable to adversarial attacks. An example of this vulerability is shown in Appendix Section 6.2.\nTo solve this problem, we then propose an approach that can reduce the number of irrelevant features. Presented in Figure 6.2, the basic motivation is that the distance between an adversarial sample and its seed example will be small along relevant feature dimension (e.g. hardly visible by humans eyes) and relatively large on the irrelevant dimensions (i.e. to misclassify) for the current task. DNN combines feature extraction and classification in one model. Therefore, we propose to remove irrelevant features for a DNN model by directly modifying its network structure."}, {"heading": "3 METHOD: DEEPMASK", "text": "The basic idea is to remove those irrelevant features used by adversarial samples. To identify which feature is irrelevant, we test pairs of adversarial samples x\u2032 and its normal seed x, and compare the difference between the extracted features in DNN. Those features changed rapidly are utilized by the adversary, and thus should be removed to improve the robustness of the model.\nTo remove those features, we insert a mask layer in DNN model, right before the linear layer handling classification. The proposed structure is shown in Figure 1.\nThe input of the mask layer is feature vector extracted by layers of DNN model. The weight of the mask layer is either 0 or 1. An element-wise multiplication is done in the mask layer, therefore the output is either the input feature or 0. Features are sorted by its sensitivity to adversarial samples, which can be measured by the distance between pairs of adversarial samples and normal samples.\nThe process is summarized in Algorithm 1. X can be a set of the full training set. Therefore the process of learning the mask can be really fast. No retraining of the model is needed.\nAlgorithm 1 DeepMask algorithm Input:Training set X, DNN classifier F , adversarial power .\n1: Initialize a vector v with all 0. 2: for \u2200x \u2208 X do 3: Generate an adversarial sample x\u2032 using sample x with power . 4: Forward x into the network, get the output feature vector O1 5: Forward x\u2032 into the network, get the output feature vector O2 6: Add |O1\u2212O2| into v. 7: end for 8: Set v\u2019s top m entries to 0 and the rest to 1. 9: Insert the mask layer v into DNN F after the inspected feature layer."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 EXPERIMENT SETTING", "text": "\u2022 Dataset: We choose CIFAR-10 (Krizhevsky & Hinton, 2009) as the dataset of our experiment. CIFAR-10 is a tiny image dataset with 50,000 32x32 training images and 10,000 testing images.\n\u2022 We choose a Residual network with 164 layers(He et al., 2016) as our target DNN model. The model is pre-trained and achieve state-of-the-art performance on corresponding dataset.\n\u2022 Metric: We generate adversarial samples for every sample in the test set and test all adversarial samples on each DNN model. The accuracy on the adversarial sample set is reported as \u201cadversarial accuracy\u201d to measure the adversarial robustness of a DNN model."}, {"heading": "4.2 EXPERIMENT RESULT", "text": "The result of DeepMask against adversarial samples is displayed in Table 1. Generally DeepMask can reduce the effectiveness of such adversarial samples by 10%. The adversarial perturbation is generated using fast gradient sign method (Appendix Section 6.1).\nTable 1 also indicates that only a small percentage of features are important for adversarial samples. In the figure, masking 1% of features can increase the adversarial performance by 5%. Therefore, we only need to remove a small percent of features to improve the adversarial robustness a lot and the model still keeps high accuracy on test samples."}, {"heading": "5 CONCLUSION", "text": "In this study, we present DeepMask, a simple and cost-efficient strategy to reduce the effectiveness of adversarial samples on DNN classifiers. Many other possible strategies can be designed, for example, by adding masks that consider the difference among output labels."}, {"heading": "6 APPENDIX", "text": ""}, {"heading": "6.1 FAST GRADIENT SIGN ALGORITHM", "text": "As introduced in Section 1, multiple algorithms have been proposed to generate x among them. Fast Gradient Sign Algorithm proposed by Goodfellow et al. (2014) is one efficient algorithm in generating adversarial samples. The perturbation is calculated through the following equation:\n\u2206x = sign(\u2207zLoss(F (z), F (x))) (6.1) This is motivated by controlling the L\u221e norm of \u2206x, which results in the size of perturbation in each feature dimension are the same size. Maximizing the difference between x and x\u2032 while limit the L\u221e norm of \u2206x is to follow the gradient direction on x on every dimension, which is exactly sign of the gradient function."}, {"heading": "6.2 EXAMPLES OF AN IRRELEVANT FEATURE CAN BE UTILIZED BY ADVERSARIAL SAMPLES", "text": "Figure 6.2 shows a situation when a linear classification model faces irrelevant feature. In this case, human annotators only use one feature to do the separation. But since the machine classifier use one extra feature, the attacker can push the original sample (blue cross) along that extra feature dimension(y axis) to generate an adversarial samples (green cross), which is misclassified by the classification model."}], "references": [{"title": "Large-scale malware classification using random projections and neural networks", "author": ["George E Dahl", "Jack W Stokes", "Li Deng", "Dong Yu"], "venue": "In ICASSP,", "citeRegEx": "Dahl et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Technique report, University of Toronto,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1511.07528,", "citeRegEx": "Papernot et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2015}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "In Security and Privacy (SP),", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "A theoretical framework for robustness of (deep) classifiers under adversarial noise", "author": ["Beilun Wang", "Ji Gao", "Yanjun Qi"], "venue": "arXiv preprint arXiv:1612.00334,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013).", "startOffset": 101, "endOffset": 149}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al.", "startOffset": 131, "endOffset": 699}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al.", "startOffset": 131, "endOffset": 723}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al.", "startOffset": 131, "endOffset": 749}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al.", "startOffset": 131, "endOffset": 769}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method.", "startOffset": 131, "endOffset": 895}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm.", "startOffset": 131, "endOffset": 1099}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value.", "startOffset": 131, "endOffset": 1235}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value. A recent paper Papernot et al. (2016a) proposed a black-box attack, which first approximates the target classifier and then generate adversarial samples to the approximated model.", "startOffset": 131, "endOffset": 1366}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value. A recent paper Papernot et al. (2016a) proposed a black-box attack, which first approximates the target classifier and then generate adversarial samples to the approximated model. Researchers also studied how to defend attacks enforced by adversarial samples. Goodfellow et al. (2014) showed retraining the model using adversarial samples can improve the adversarial robustness of the model.", "startOffset": 131, "endOffset": 1612}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value. A recent paper Papernot et al. (2016a) proposed a black-box attack, which first approximates the target classifier and then generate adversarial samples to the approximated model. Researchers also studied how to defend attacks enforced by adversarial samples. Goodfellow et al. (2014) showed retraining the model using adversarial samples can improve the adversarial robustness of the model. Papernot et al. (2016b) proposed defensive distillation to make the model less sensible to gradient change.", "startOffset": 131, "endOffset": 1743}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value. A recent paper Papernot et al. (2016a) proposed a black-box attack, which first approximates the target classifier and then generate adversarial samples to the approximated model. Researchers also studied how to defend attacks enforced by adversarial samples. Goodfellow et al. (2014) showed retraining the model using adversarial samples can improve the adversarial robustness of the model. Papernot et al. (2016b) proposed defensive distillation to make the model less sensible to gradient change. Wang et al. (2016) showed that one cause of adversarial sample is those redundant features that are irrelevant to classification.", "startOffset": 131, "endOffset": 1846}, {"referenceID": 2, "context": "\u2022 We choose a Residual network with 164 layers(He et al., 2016) as our target DNN model.", "startOffset": 46, "endOffset": 63}], "year": 2017, "abstractText": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task. To overcome this problem, we introduce a defensive mechanism called DeepMask. By identifying and removing unnecessary features in a DNN model, DeepMask limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepMask is easy to implement and computationally efficient. Experimental results show that DeepMask can increase the performance of state-of-the-art DNN models against adversarial samples.", "creator": "LaTeX with hyperref package"}}}