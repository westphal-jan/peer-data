{"id": "1702.06925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Regularizing Face Verification Nets For Pain Intensity Regression", "abstract": "graubuenden Limited augie annotated nige data brandl is available for the research turpal of songpa estimating facial cottars expression geografica intensities, which makes emitted the 1978-1984 training al-marri of subnet deep repower networks for a.j.p. automated selex expression cyprinodon assessment atsunori very epl challenging. centinel Fortunately, stutman fine - tuning from a keepsakes data - enga\u00f1o extensive pre - trained domain such 11.40 as face enad verification mecano can balam alleviate outlandishly the problem. In bisotun this gangopadhyay paper, mussaf we propose crossers a interlocutors transferred network that digne fine - tunes 51.60 a laxity state - fennec of - 18.54 the - drd art face donalds verification network using expression - intensity labeled trailbreaker data mbf with roam a noyon regression n70 layer. In this strigulae way, fajr-3 the j.r.r. expression regression task 1905-1906 can benefit from the rich finstrom feature cortisol representations apiku trained carcinogen on a mzee huge amount khoy of dauman data for face flooring verification. The proposed transferred tulpehocken deep aracely regressor mogu is applied religa in gammage estimating landour the liberalizations intensity 531,000 of facial action units (webbed 2017 39.40 EmotionNet chastened Challenge) sinfulness and in apnewsalert particular hathcock pain century-long intensity estimation (tvsurfer UNBS - McMaster Shoulder - zygons Pain dataset ). illustribus It wins 94-93 the second place sch\u00e4ffer in porro the 228.6 challenge and hujara achieves 6-yard the beny state - of - the - art atrium performance speedmaster on Shoulder - Pain armands dataset. algonquians Particularly britsch for kasongo Shoulder - trebly Pain with 85-90 the imbalance issue bindweed of thromboxane different pain krings levels, 20:05 a cimbalom new weighted ninlil evaluation metric inflammation is o'bannon proposed.", "histories": [["v1", "Wed, 22 Feb 2017 18:15:42 GMT  (1234kb,D)", "http://arxiv.org/abs/1702.06925v1", "5 pages, 3 figure; submitted to IEEE ICIP 2017 which does not perform blind reviews"], ["v2", "Tue, 9 May 2017 17:58:58 GMT  (1235kb,D)", "http://arxiv.org/abs/1702.06925v2", "5 pages, 3 figure; To appear in IEEE ICIP 2017"], ["v3", "Thu, 1 Jun 2017 17:49:56 GMT  (1228kb,D)", "http://arxiv.org/abs/1702.06925v3", "5 pages, 3 figure; Camera-ready version to appear at IEEE ICIP 2017"]], "COMMENTS": "5 pages, 3 figure; submitted to IEEE ICIP 2017 which does not perform blind reviews", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.MM", "authors": ["feng wang", "xiang xiang", "chang liu", "trac d tran", "austin reiter", "gregory d hager", "harry quon", "jian cheng", "alan l yuille"], "accepted": false, "id": "1702.06925"}, "pdf": {"name": "1702.06925.pdf", "metadata": {"source": "CRF", "title": "TRANSFERRING FACE VERIFICATION NETS TO PAIN AND EXPRESSION REGRESSION", "authors": ["Feng Wang", "Xiang Xiang", "Chang Liu", "Trac D. Tran", "Austin Reiter", "Gregory D. Hager", "Harry Quon", "Jian Cheng", "Alan L. Yuille"], "emails": ["xxiang@cs.jhu.edu)."], "sections": [{"heading": null, "text": "Index Terms\u2014 CNN, regression, expression intensity"}, {"heading": "1. INTRODUCTION", "text": "Obtaining accurate patient-reported pain assessments is important to effectively manage pain. Hence, it is highly desired to develop an automated approach. Firstly, it simplifies the pain reporting process and reduce the strain on manual efforts. Secondly, it standardizes the feedback mechanism by ensuring a single automated metric that performs all assessments and thus reduces bias. Thirdly, it enables giving continuousvalued pain levels. There exist efforts to measure pain using the observational or behavioral effect caused by pain such as physiological data. Medasense\u00a9has developed medical devices for objective pain monitoring. Their basic premise is that pain may cause the vital signs such as blood pressure, pulse rate, respiration rate, SpO2 from EMG, ECG or EEG, alone or in combination to change and often to increase. However, it takes much more efforts to obtain physiological data than face images and videos from unobtrusive cameras.\n* Corresponding author (e-mail: xxiang@cs.jhu.edu).\nComputer vision and supervised learning has come a long way in recent years, redefining the state-of-the-art using deep Convolutional Neural Networks (CNNs). However, the ability to train deep CNNs for automated pain assessment is limited by small datasets associated with labels of patientreported pain levels, i.e., annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3]. In particular we perform experiments on the Shoulder-Pain dataset which is designed for visual analysis only. The dataset contains 200 videos of 25 patients who repeatedly raise their arms (feeling pain) and then put them down (pain released). All frames per video are labeled with discrete-valued ground-truth pain levels, as illustrated in Fig. 1. Note that 91.35% of all images are labeled as pain level 0 which may induce trivial solutions.\nTwo pieces of recent works make progress in estimating pain intensity visually using the Shoulder-Pain dataset only: Ordinal Support Vector Regression (OSVR) [4] and Recurrent Convolutional Regression (RCR) [5]. Notably, RCR\nar X\niv :1\n70 2.\n06 92\n5v 1\n[ cs\n.C V\n] 2\n2 Fe\nb 20\n[5] is trained end-to-end achieving sub-optimal performance. Please see reference therein for other existing works.\nAlthough the limited labeled data prevents us from directly training a deep pain intensity regressor, we show that fine-tuning from a data-extensive pre-trained domain such as face verification can alleviate this problem, While our work is not the first attempt of this transferring idea, to our knowledge we are the first to apply it in pain intensity estimation.\nTo summarize, the main contributions of this work are: \u2022We address limited data with expression intensity labels by transferring face verification models to new tasks. \u2022We push the bound of the performance of the expression intensity estimation by a large margin. \u2022We propose to add center loss regularization to make the predicted values more close to discrete labels. \u2022A novel evaluation metric is proposed to fairly judge the performance on imbalanced dataset, such as the video-based Shoulder-Pain [1] where mostly painless expression occurs."}, {"heading": "2. RELATED WORKS", "text": "For facial expression recognition in general, there is a tradeoff between method simplicity and performance, i.e., imagebased [6, 7] vs. video-based [8, 9, 10] methods. Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14]. Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17]. Similar tradeoff also lies in linear model vs. non-linear models. Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21]. By introducing more information, one approach is 3D models [22] while another is multi-modal models [23].\nAs regards transfer learning with deep networks, there exist recent works that regularize deep face recognition nets for expression classification - FaceNet2ExpNet [6]. During pretraining, they train convolutional layers of the expression net, regularized by the deep face recognition net. In the refining stage, they append fully-connected layers to the pre-trained convolutional layers and train the whole network jointly."}, {"heading": "3. TRANSFERRED DEEP REGRESSIOR", "text": "A face f is visually generated by confounding factors the primary of which are the identity i, the facial expression e and the head pose p. Normally, given a set of rich training examples (f , i), deep face verification algorithms [24] seek a function g : F \u2192 I where F is the input space spanned by all possible face appearances and I is the output space formed by all possible identities. Now, given g and limited training samples (f , e), the problem that we are addressing is learning a function h : F\u2192 E where E is the output space formed by all possible face expressions. A natural question is if we are able to transfer g to h. In this section, we investigate how g and h can be related, since they are mappings from the same\ninput space to different output space. Namely, given g, we design a network to learn h by refining g with a few additional training examples of (f , e).\nOur network is transferred from a state-of-the-art face feature learning and verification network [24]1 which is trained using the CASIA-WebFace dataset contaning 0.5 million face images with identity labels. Now, we transfer the network to learn features for pain intensity regression with limited face images with pain labels. In detail, we remove all the fullyconnected (FC) layers in the original network and then add two new FC layers. Since over-fitting is a severe problem when training with limited data, the number of neurons in our hidden FC layer is relatively smaller than those in the original network (50 vs 512), and Dropout[25] operation is applied before the two FC layers. We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 51+e\u2212x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].\nThe architecture of the new added layers is shown in Fig. 2, details of which will be explained in the following sections."}, {"heading": "3.1. Regression Loss", "text": "At the end of the network, we leverage both `1 and `2 norm distance to do the regression task.\nLR = \u2016y \u2212 y\u0303\u2016pp (1)\nwhere I is the input image; y \u2208 R1 is the output of the activated output of the last fully connected layer; y\u0303 is the ground truth label and p denotes which norm should be used.\nIn practice, whether to choose `1-norm or `2-norm depends on the evaluation metrics, e.g. `2-norm performs better on Mean Square Error (MSE) and `1-norm performs better on Mean Abosolute Error (MAE). However, we found that\n1Model available at https://github.com/ydwen/caffe-face\ndirectly applying the `2-norm distance layer would cause the gradient exploding problem because of the big gradient magnitude in initial iterations. This phenomenon is also described in [26]. To solve this problem, we follow [26] to use a smooth `1 loss instead of `2 loss, to make the gradients smaller when the error |y \u2212 y\u0303| is very large. The smooth `1 is defined as\nLR = {\n0.5|y \u2212 y\u0303|2 if |y \u2212 y\u0303| < t |y \u2212 y\u0303| \u2212 t+ 0.5t2 otherwise (2)\nwhere x is the output of the last layer and t is the turning point between `2 distance and `1 distance. When t = 1, it works similar with `2-norm loss since the final error is usually below 1. When t = 0, it just equals with `1-norm loss. In this way, we can implement our regression loss in one layer."}, {"heading": "3.2. Regularizing by Reducing Inter-Class Variance", "text": "Since the pain intensity level is discretely labeled specifically in the Shoulder-Pain dataset, it is natural to add some kinds classification signals on the loss function to make the regressed values to be more \u2018discrete\u2019. In this work, we try two kinds of classification loss functions, one is the Softmax,\nLS = \u2212log WTy\u0303 x+ by\u0303\u2211n i=1 W T i x+ bi , (3)\nanother is center loss [24],\nLC = \u2212\u2016x\u2212 cy\u0303\u2016pp, (4)\nwhere x is the output of the second last layer, y\u0303 is the corresponding label, cy\u0303 represent the center for class y\u0303 and p denote which norm should be selected. The classification losses are added after the first fully connected layer, acting on training\nthe neural network simultaneously with the regression loss. Different from the description of center loss in [24], we jointly learn the centers and minimize the distances within classes by gradient descending, while [24]\u2019s centers are learned in a moving-average way.\nThe main difference between the two kinds of classification loss is that the Softmax loss optimizes both distances between and within classes, while the center loss only concerns about distances within class (Fig. 3). From the Figure, it can be inferred that there is some degree of conflict between Softmax loss and the regression loss. We hope that the calibration is determined by the regression loss only to get the minimal error, but Softmax still tries to enlarge the distances between classes, which makes the calibrated scale on the coordinate axis invalid. The center loss, which tries to shrink the distances within classes only, would have far less influence on the calibration of the regression loss."}, {"heading": "4. EXPERIMENTS", "text": "In this section, we present implementations and experiments. The project page2 has been set up with programs and data."}, {"heading": "4.1. Dataset and Training Details", "text": "We test our network on the UNBC-McMaster Shoulder-Pain dataset [1] that is widely used for benchmarking intensity estimations of the pain expression in particular and facial action units in general. The dataset comes with four types of labels. The three annotated online during the video collection are the sensory scale (SEN), affective scale (AFF) and visual analog scale (VAS) ranging from 0 (no pain) to 15 (severe pain). Additionally, observers rated pain intensity (OPI) offline from recorded videos ranging from 0 (no pain) to 5 (severe pain). In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].\nThe face verification network [24] is trained on CASIAWebface dataset [28], which contains 494,414 training images from 10,575 identities. To be consistent with face verification, we perform the same pre-processing on the images of Shoulder-Pain dataset. To be specific, we leverage MTCNN model [29] to detect faces and facial landmarks. Then the faces are aligned according to the detected landmarks.\nThe learning rate is set to 0.0001 to avoid huge modification on the convolution layers. The network is trained over 5,000 iterations, which is reasonable for the networks to converge observed in a few cross validation folds. We set the weight of the regression loss to be 1 and the weights of Softmax loss and center loss to be 1 and 0.01 respectively."}, {"heading": "4.2. Pain Intensity Regression", "text": "We run leave-one-out cross validation 25 times on the Shoulder-Pain dataset. Each time, the videos of one patient\n2https://github.com/happynear/PainRegression.\nare reserved for testing. All the other videos are used to train the deep regression network. The performance is summarized in Table 1. It can be concluded from the table that our algorithm performs best or equally best on various evaluation metrics, especially the combination of smooth `1 loss and `1 center loss. Even though the MAE of using Softmax loss as regularization is also competitive, we find that it just learns to predict more zeros by observing the predicted curve."}, {"heading": "4.3. Class Imbalance Problem", "text": "In Table 1, we provide the performance of predicting all zeros as a baseline. Interestingly, on the metrics MAE and MSE, zero prediction performs much better than several state-ofthe-art algorithms. This is because the Shoulder-Pain dataset is highly imbalanced. 91.35% frames are labeled as pain level 0. Thus, for the regression algorithms, it is relatively safe to predict the pain level to be zero.\nTo fairly evaluate the performance, we propose the weighted version of evaluation metrics i.e. weighted MAE (wMAE) and MSE (wMSE) to address the dataset imbalance issue. For example, the wMAE is simply the mean of MAE on each pain level. In this way, the MAE is weighted by the population of each pain level.\nWe apply two techniques to sample the training data to make our training set more consistent with the new metrics. First, we eliminate the redundant frames on the sequences following [4]. If the intensity level remains the same for more than 5 consecutive frames, we choose the first one as representative frame. Second, during training, we uniformly sample images from the 6 classes to feed into the network. In this way, what the neural network \u2018see\u2019 is a total balanced dataset.\nUsing the new proposed metrics, the performance is summarized in Table 2. It can be drawn that the uniform class sampling strategy performs much better on the new evalu-\nation metrics. Simply predicting all zeros no longer yields good performance. We have not compared with other works using the proposed new metrics because we find it challenging to obtain the predicted values of other works. However, we will provide the evaluation program in our project page and encourage future works to report their performance with the new evaluation metrics."}, {"heading": "4.4. Facial Action Unit Recognition in General", "text": "Replacing the regression layer with a Softmax layer, we apply our proposed method on the EmotionNet Challenge3, a facial expression recognition in the wild challenge organized by the CBCSL of Ohio State University. The competition has two tracks. The first one requires the detection of 11 action units (AUs), and the second one is an emotion recognition task. 26,117 and 2,477 labeled images are provided by the organizer for training the two tracks respectively. The predictions generated by our program are submitted to the organizer and they further evaluate our program\u2019s output according to the metrics they previously defined. For the AU detection track, we got second place with averaged metrics of 0.7101, while the best is 0.7290. Our individual F-scores are all the top (F1 is 0.6405, F2 is 0.6354 and F.5 is 0.6380). For the emotion recognition task, we also got the second place. Our final score is 0.4799 while the top is 0.5968. It is noteworthy that the winning team comes from an industrial face verification company, who is capable of additional training data, and we only use the data provided by the organizer."}, {"heading": "5. SUMMARY", "text": "Given the restriction of labeled data which prevents us to directly train a deep pain intensity regressor, fine-tuning from a data-extensive pre-trained domain such as identities can alleviate the problem. In this paper, we transfer a face verification network for pain intensity regression. The fine-tuned transferred network with a regression layer is tested on the UNBC-McMaster Shoulder-Pain dataset and achieves stateof-the-art performance on pain intensity estimation.\n3For challenge details and results, please see http://cbcsl.ece. ohio-state.edu/EmotionNetChallenge/."}, {"heading": "6. REFERENCES", "text": "[1] Patrick Lucey, Jeffrey F Cohn, Kenneth M Prkachin, Patricia E Solomon, and Iain Matthews, \u201cPainful data: The unbcmcmaster shoulder pain expression archive database,\u201d in IEEE International Conference on Automatic Face & Gesture Recognition, 2011, pp. 57\u201364.\n[2] Min Aung, Sebastian Kaltwang, Bernardino Romera-Paredes, Brais Martinez, Aneesha Singh, Matteo Cella, Michel Valstar, Hongying Meng, Andrew Kemp, Aaron Elkins, et al., \u201cThe automatic detection of chronic pain-related expression: requirements, challenges and a multimodal dataset,\u201d IEEE Trans. Affective Computing.\n[3] Philipp Werner, Ayoub Al-Hamadi, Robert Niese, Steffen Walter, Sascha Gruss, and Harald C Traue, \u201cTowards pain monitoring: Facial expression, head pose, a new database, an automatic system and remaining challenges,\u201d in BMVC, 2013.\n[4] Rui Zhao, Quan Gan, Shangfei Wang, and Qiang Ji, \u201cFacial expression intensity estimation using ordinal information,\u201d in CVPR, 2016.\n[5] Jing Zhou, Xiaopeng Hong, Fei Su, and Guoying Zhao, \u201cRecurrent convolutional neural network regression for continuous pain intensity estimation in video,\u201d CVPR workshops, 2016.\n[6] Hui Ding, Shaohua Kevin Zhou, and Rama Chellappa, \u201cFacenet2expnet: Regularizing a deep face recognition net for expression recognition,\u201d in FG, 2017.\n[7] C Fabian Benitez-Quiroz, Ramprakash Srinivasan, and Aleix M Martinez, \u201cEmotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild,\u201d in CVPR, 2016, pp. 5562\u20135570.\n[8] Kaili Zhao, Wen-Sheng Chu, Fernando De la Torre, Jeffrey F. Cohn, and Honggang Zhang, \u201cJoint patch and multi-label learning for facial action unit detection,\u201d in CVPR, 2015.\n[9] Ping Liu, Joey Tianyi Zhou, Ivor Wai-Hung Tsang, Zibo Meng, Shizhong Han, and Yan Tong, \u201cFeature disentangling machinea novel approach of feature selection and disentangling in facial expression analysis,\u201d in ECCV, 2014, pp. 151\u2013166.\n[10] Salah Rifai, Yoshua Bengio, Aaron Courville, Pascal Vincent, and Mehdi Mirza, \u201cDisentangling factors of variation for facial expression recognition,\u201d in ECCV, 2012, pp. 808\u2013822.\n[11] Arnaud Dapogny, Kevin Bailly, and Severine Dubuisson, \u201cPairwise conditional random forests for facial expression recognition,\u201d in ICCV, 2015.\n[12] Mengyi Liu, Shiguang Shan, Ruiping Wang, and Xilin Chen, \u201cLearning expressionlets on spatio-temporal manifold for dynamic facial expression recognition,\u201d in CVPR, 2014, pp. 1749\u20131756.\n[13] Ziheng Wang, Shangfei Wang, and Qiang Ji, \u201cCapturing complex spatio-temporal relations among facial muscles for facial expression recognition,\u201d in CVPR, 2013, pp. 3422\u20133429.\n[14] Yimo Guo, Guoying Zhao, and Matti Pietika\u0308inen, \u201cDynamic facial expression recognition using longitudinal facial expression atlases,\u201d in ECCV, pp. 631\u2013644. 2012.\n[15] Ognjen Rudovic, Vladimir Pavlovic, and Maja Pantic, \u201cMultioutput laplacian dynamic ordinal regression for facial expression recognition and intensity estimation,\u201d in CVPR, 2012, pp. 2634\u20132641.\n[16] Minyoung Kim and Vladimir Pavlovic, \u201cStructured output ordinal regression for dynamic facial emotion intensity prediction,\u201d in ECCV, 2010, pp. 649\u2013662.\n[17] Peng Yang, Qingshan Liu, and Dimitris N Metaxas, \u201cExploring facial expressions with compositional features,\u201d in CVPR, 2010, pp. 2638\u20132644.\n[18] C. Fabian Benitez-Quiroz, Ramprakash Srinivasan, and Aleix M. Martinez, \u201cEmotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild,\u201d in CVPR, 2016.\n[19] Xiangyun Zhao, Xiaodan Liang, Luoqi Liu, Teng Li, Yugang Han, Nuno Vasconcelos, and Shuicheng Yan, \u201cPeak-piloted deep network for facial expression recognition,\u201d in ECCV, 2016, pp. 425\u2013442.\n[20] Heechul Jung, Sihaeng Lee, Junho Yim, Sunjeong Park, and Junmo Kim, \u201cJoint fine-tuning in deep neural networks for facial expression recognition,\u201d in ICCV, 2015.\n[21] Ping Liu, Shizhong Han, Zibo Meng, and Yan Tong, \u201cFacial expression recognition via a boosted deep belief network,\u201d in CVPR, 2014, pp. 1805\u20131812.\n[22] Hui Chen, Jiangdong Li, Fengjun Zhang, Yang Li, and Hongan Wang, \u201c3d model-based continuous emotion recognition,\u201d in CVPR, 2015.\n[23] Zheng Zhang, Jeff M. Girard, Yue Wu, Xing Zhang, Peng Liu, Umur Ciftci, Shaun Canavan, Michael Reale, Andy Horowitz, Huiyuan Yang, Jeffrey F. Cohn, Qiang Ji, and Lijun Yin, \u201cMultimodal spontaneous emotion corpus for human behavior analysis,\u201d in CVPR, 2016.\n[24] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao, \u201cA discriminative feature learning approach for deep face recognition,\u201d in ECCV, 2016, pp. 499\u2013515.\n[25] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, \u201cDropout: a simple way to prevent neural networks from overfitting.,\u201d Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.\n[26] Ross Girshick, \u201cFast r-cnn,\u201d in ICCV, 2015.\n[27] Ognjen Rudovic, Vladimir Pavlovic, and Maja Pantic, \u201cAutomatic pain intensity estimation with heteroscedastic conditional ordinal random fields,\u201d in International Symposium on Visual Computing, 2013, pp. 234\u2013243.\n[28] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li, \u201cLearning face representation from scratch,\u201d arXiv preprint arXiv:1411.7923, 2014.\n[29] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, \u201cJoint face detection and alignment using multitask cascaded convolutional networks,\u201d IEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499\u20131503, 2016."}], "references": [{"title": "Painful data: The unbcmcmaster shoulder pain expression archive database", "author": ["Patrick Lucey", "Jeffrey F Cohn", "Kenneth M Prkachin", "Patricia E Solomon", "Iain Matthews"], "venue": "IEEE International Conference on Automatic Face & Gesture Recognition, 2011, pp. 57\u201364.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "The automatic detection of chronic pain-related expression: requirements, challenges and a multimodal dataset", "author": ["Min Aung", "Sebastian Kaltwang", "Bernardino Romera-Paredes", "Brais Martinez", "Aneesha Singh", "Matteo Cella", "Michel Valstar", "Hongying Meng", "Andrew Kemp", "Aaron Elkins"], "venue": "IEEE Trans. Affective Computing.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 0}, {"title": "Towards pain monitoring: Facial expression, head pose, a new database, an automatic system and remaining challenges", "author": ["Philipp Werner", "Ayoub Al-Hamadi", "Robert Niese", "Steffen Walter", "Sascha Gruss", "Harald C Traue"], "venue": "BMVC, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Facial expression intensity estimation using ordinal information", "author": ["Rui Zhao", "Quan Gan", "Shangfei Wang", "Qiang Ji"], "venue": "CVPR, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent convolutional neural network regression for continuous pain intensity estimation in video", "author": ["Jing Zhou", "Xiaopeng Hong", "Fei Su", "Guoying Zhao"], "venue": "CVPR workshops, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Facenet2expnet: Regularizing a deep face recognition net for expression recognition", "author": ["Hui Ding", "Shaohua Kevin Zhou", "Rama Chellappa"], "venue": "FG, 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild", "author": ["C Fabian Benitez-Quiroz", "Ramprakash Srinivasan", "Aleix M Martinez"], "venue": "CVPR, 2016, pp. 5562\u20135570.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint patch and multi-label learning for facial action unit detection", "author": ["Kaili Zhao", "Wen-Sheng Chu", "Fernando De la Torre", "Jeffrey F. Cohn", "Honggang Zhang"], "venue": "CVPR, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature disentangling machinea novel approach of feature selection and disentangling in facial expression analysis", "author": ["Ping Liu", "Joey Tianyi Zhou", "Ivor Wai-Hung Tsang", "Zibo Meng", "Shizhong Han", "Yan Tong"], "venue": "ECCV, 2014, pp. 151\u2013166.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Disentangling factors of variation for facial expression recognition", "author": ["Salah Rifai", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent", "Mehdi Mirza"], "venue": "ECCV, 2012, pp. 808\u2013822.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Pairwise conditional random forests for facial expression recognition", "author": ["Arnaud Dapogny", "Kevin Bailly", "Severine Dubuisson"], "venue": "ICCV, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition", "author": ["Mengyi Liu", "Shiguang Shan", "Ruiping Wang", "Xilin Chen"], "venue": "CVPR, 2014, pp. 1749\u20131756.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Capturing complex spatio-temporal relations among facial muscles for facial expression recognition", "author": ["Ziheng Wang", "Shangfei Wang", "Qiang Ji"], "venue": "CVPR, 2013, pp. 3422\u20133429.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic facial expression recognition using longitudinal facial expression atlases", "author": ["Yimo Guo", "Guoying Zhao", "Matti Pietik\u00e4inen"], "venue": "ECCV, pp. 631\u2013644. 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Multioutput laplacian dynamic ordinal regression for facial expression recognition and intensity estimation", "author": ["Ognjen Rudovic", "Vladimir Pavlovic", "Maja Pantic"], "venue": "CVPR, 2012, pp. 2634\u20132641.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Structured output ordinal regression for dynamic facial emotion intensity prediction", "author": ["Minyoung Kim", "Vladimir Pavlovic"], "venue": "ECCV, 2010, pp. 649\u2013662.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploring facial expressions with compositional features", "author": ["Peng Yang", "Qingshan Liu", "Dimitris N Metaxas"], "venue": "CVPR, 2010, pp. 2638\u20132644.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild", "author": ["C. Fabian Benitez-Quiroz", "Ramprakash Srinivasan", "Aleix M. Martinez"], "venue": "CVPR, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Peak-piloted deep network for facial expression recognition", "author": ["Xiangyun Zhao", "Xiaodan Liang", "Luoqi Liu", "Teng Li", "Yugang Han", "Nuno Vasconcelos", "Shuicheng Yan"], "venue": "ECCV, 2016, pp. 425\u2013442.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint fine-tuning in deep neural networks for facial expression recognition", "author": ["Heechul Jung", "Sihaeng Lee", "Junho Yim", "Sunjeong Park", "Junmo Kim"], "venue": "ICCV, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Facial expression recognition via a boosted deep belief network", "author": ["Ping Liu", "Shizhong Han", "Zibo Meng", "Yan Tong"], "venue": "CVPR, 2014, pp. 1805\u20131812.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "3d model-based continuous emotion recognition", "author": ["Hui Chen", "Jiangdong Li", "Fengjun Zhang", "Yang Li", "Hongan Wang"], "venue": "CVPR, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal spontaneous emotion corpus for human behavior analysis", "author": ["Zheng Zhang", "Jeff M. Girard", "Yue Wu", "Xing Zhang", "Peng Liu", "Umur Ciftci", "Shaun Canavan", "Michael Reale", "Andy Horowitz", "Huiyuan Yang", "Jeffrey F. Cohn", "Qiang Ji", "Lijun Yin"], "venue": "CVPR, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "A discriminative feature learning approach for deep face recognition", "author": ["Yandong Wen", "Kaipeng Zhang", "Zhifeng Li", "Yu Qiao"], "venue": "ECCV, 2016, pp. 499\u2013515.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1929}, {"title": "Fast r-cnn", "author": ["Ross Girshick"], "venue": "ICCV, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic pain intensity estimation with heteroscedastic conditional ordinal random fields", "author": ["Ognjen Rudovic", "Vladimir Pavlovic", "Maja Pantic"], "venue": "International Symposium on Visual Computing, 2013, pp. 234\u2013243.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning face representation from scratch", "author": ["Dong Yi", "Zhen Lei", "Shengcai Liao", "Stan Z Li"], "venue": "arXiv preprint arXiv:1411.7923, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint face detection and alignment using multitask cascaded convolutional networks", "author": ["K. Zhang", "Z. Zhang", "Z. Li", "Y. Qiao"], "venue": "IEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499\u20131503, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Face Samples are also provided to give an intuitive feeling about the Shoulder-Pain dataset[1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": ", annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3].", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": ", annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3].", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": ", annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "Two pieces of recent works make progress in estimating pain intensity visually using the Shoulder-Pain dataset only: Ordinal Support Vector Regression (OSVR) [4] and Recurrent Convolutional Regression (RCR) [5].", "startOffset": 158, "endOffset": 161}, {"referenceID": 4, "context": "Two pieces of recent works make progress in estimating pain intensity visually using the Shoulder-Pain dataset only: Ordinal Support Vector Regression (OSVR) [4] and Recurrent Convolutional Regression (RCR) [5].", "startOffset": 207, "endOffset": 210}, {"referenceID": 4, "context": "[5] is trained end-to-end achieving sub-optimal performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "\u2022A novel evaluation metric is proposed to fairly judge the performance on imbalanced dataset, such as the video-based Shoulder-Pain [1] where mostly painless expression occurs.", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": ", imagebased [6, 7] vs.", "startOffset": 13, "endOffset": 19}, {"referenceID": 6, "context": ", imagebased [6, 7] vs.", "startOffset": 13, "endOffset": 19}, {"referenceID": 7, "context": "video-based [8, 9, 10] methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 8, "context": "video-based [8, 9, 10] methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 9, "context": "video-based [8, 9, 10] methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 10, "context": "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 11, "context": "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].", "startOffset": 170, "endOffset": 182}, {"referenceID": 12, "context": "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].", "startOffset": 170, "endOffset": 182}, {"referenceID": 13, "context": "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].", "startOffset": 170, "endOffset": 182}, {"referenceID": 3, "context": "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].", "startOffset": 77, "endOffset": 88}, {"referenceID": 14, "context": "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].", "startOffset": 77, "endOffset": 88}, {"referenceID": 15, "context": "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].", "startOffset": 77, "endOffset": 88}, {"referenceID": 16, "context": "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 98, "endOffset": 117}, {"referenceID": 9, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 98, "endOffset": 117}, {"referenceID": 18, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 98, "endOffset": 117}, {"referenceID": 19, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 98, "endOffset": 117}, {"referenceID": 20, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 98, "endOffset": 117}, {"referenceID": 21, "context": "By introducing more information, one approach is 3D models [22] while another is multi-modal models [23].", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "By introducing more information, one approach is 3D models [22] while another is multi-modal models [23].", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "As regards transfer learning with deep networks, there exist recent works that regularize deep face recognition nets for expression classification - FaceNet2ExpNet [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 23, "context": "Normally, given a set of rich training examples (f , i), deep face verification algorithms [24] seek a function g : F \u2192 I where F is the input space spanned by all possible face appearances and I is the output space formed by all possible identities.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "Our network is transferred from a state-of-the-art face feature learning and verification network [24]1 which is trained using the CASIA-WebFace dataset contaning 0.", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "Since over-fitting is a severe problem when training with limited data, the number of neurons in our hidden FC layer is relatively smaller than those in the original network (50 vs 512), and Dropout[25] operation is applied before the two FC layers.", "startOffset": 198, "endOffset": 202}, {"referenceID": 4, "context": "We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 5 1+e\u2212x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].", "startOffset": 65, "endOffset": 71}, {"referenceID": 0, "context": "We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 5 1+e\u2212x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 5 1+e\u2212x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].", "startOffset": 212, "endOffset": 218}, {"referenceID": 25, "context": "This phenomenon is also described in [26].", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "To solve this problem, we follow [26] to use a smooth `1 loss instead of `2 loss, to make the gradients smaller when the error |y \u2212 \u1ef9| is very large.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "another is center loss [24],", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "Different from the description of center loss in [24], we jointly learn the centers and minimize the distances within classes by gradient descending, while [24]\u2019s centers are learned in a moving-average way.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Different from the description of center loss in [24], we jointly learn the centers and minimize the distances within classes by gradient descending, while [24]\u2019s centers are learned in a moving-average way.", "startOffset": 156, "endOffset": 160}, {"referenceID": 0, "context": "We test our network on the UNBC-McMaster Shoulder-Pain dataset [1] that is widely used for benchmarking intensity estimations of the pain expression in particular and facial action units in general.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].", "startOffset": 34, "endOffset": 44}, {"referenceID": 4, "context": "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].", "startOffset": 34, "endOffset": 44}, {"referenceID": 26, "context": "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].", "startOffset": 34, "endOffset": 44}, {"referenceID": 14, "context": "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].", "startOffset": 129, "endOffset": 136}, {"referenceID": 4, "context": "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].", "startOffset": 152, "endOffset": 158}, {"referenceID": 23, "context": "The face verification network [24] is trained on CASIAWebface dataset [28], which contains 494,414 training images from 10,575 identities.", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "The face verification network [24] is trained on CASIAWebface dataset [28], which contains 494,414 training images from 10,575 identities.", "startOffset": 70, "endOffset": 74}, {"referenceID": 28, "context": "To be specific, we leverage MTCNN model [29] to detect faces and facial landmarks.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "OSVR-L1 (CVPR16) [4] 1.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "OSVR-L2 (CVPR16) [4] 0.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "601 RCNN (CVPR16w) [5] N/A 1.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "First, we eliminate the redundant frames on the sequences following [4].", "startOffset": 68, "endOffset": 71}], "year": 2017, "abstractText": "Limited annotated data is available for the research of estimating facial expression intensities, which makes the training of deep networks for automated expression assessment very challenging. Fortunately, fine-tuning from a data-extensive pre-trained domain such as face verification can alleviate the problem. In this paper, we propose a transferred network that fine-tunes a state-of-the-art face verification network using expression-intensity labeled data with a regression layer. In this way, the expression regression task can benefit from the rich feature representations trained on a huge amount of data for face verification. The proposed transferred deep regressor is applied in estimating the intensity of facial action units (2017 EmotionNet Challenge) and in particular pain intensity estimation (UNBS-McMaster Shoulder-Pain dataset). It wins the second place in the challenge and achieves the stateof-the-art performance on Shoulder-Pain dataset. Particularly for Shoulder-Pain with the imbalance issue of different pain levels, a novel weighted evaluation metric is proposed.", "creator": "LaTeX with hyperref package"}}}