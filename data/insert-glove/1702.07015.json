{"id": "1702.07015", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Unsupervised Learning of Morphological Forests", "abstract": "This paper focuses on 6th unsupervised jackdaw modeling vautrin of theoderic morphological families, system-on-chip collectively hawkers comprising ten-sided a rockin forest kneeled over the language lajolo vocabulary. This formulation 45.70 enables us goan to transgressive capture ahmadi edgewise xenophobic properties instructional reflecting 6,910 single - step altruistically morphological derivations, along cranopsis with sofie global torriente distributional wenli properties of the allstate entire sea-land forest. 167 These transaero global properties expressible constrain the kosolapov size vineyards of missense the 58.55 affix dybbuk set gentilis and 546 encourage formation wiktorin of tight paramour morphological families. lindup The autry resulting objective is cam\u00e9ra solved wbns using dion Integer Linear afwerki Programming (flitcroft ILP) paired randee with thabit contrastive estimation. We train 400-person the s\u00f6rensen model by sanguinea alternating between belize optimizing the over-enthusiastic local log - linear skippers model weinrich and sanguinetti the global ILP objective. sonatas We evaluate our system brashares on three tasks: shouk root detection, clustering of chacun morphological misstate families executable and national-socialist segmentation. Our experiments 105.24 demonstrate that our model bantock yields 45-18 consistent walk-off gains in 172.3 all three tropicalia tasks compared with the best stem published weitling results.", "histories": [["v1", "Wed, 22 Feb 2017 21:44:02 GMT  (3595kb,D)", "http://arxiv.org/abs/1702.07015v1", "12 pages, 5 figures, accepted by TACL 2017"]], "COMMENTS": "12 pages, 5 figures, accepted by TACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiaming luo", "karthik narasimhan", "regina barzilay"], "accepted": true, "id": "1702.07015"}, "pdf": {"name": "1702.07015.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning of Morphological Forests", "authors": ["Jiaming Luo", "Karthik Narasimhan", "Regina Barzilay"], "emails": ["j_luo@mit.edu", "karthikn@mit.edu", "regina@csail.mit.edu"], "sections": [{"heading": null, "text": "This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edgewise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results.1"}, {"heading": "1 Introduction", "text": "The morphological study of a language inherently draws upon the existence of families of related words. All words within a family can be derived from a common root via a series of transformations, whether inflectional or derivational. Figure 1 depicts one such family, originating from the word faith. This representation can benefit a range of applications, including segmentation, root detection and clustering of morphological families.\n1Code is available at https://github.com/ j-luo93/MorphForest.\nUsing graph terminology, a full morphological assignment of the words in a language can be represented as a forest.2 Valid forests of morphological families exhibit a number of well-known regularities. At the global level, the number of roots is limited, and only constitute a small fraction of the vocabulary. A similar constraint applies to the number of possible affixes, shared across families. At the local edge level, we prefer derivations that follow regular orthographic patterns and preserve semantic relatedness. We hypothesize that enforcing these constraints as part of the forest induction pro-\n2The correct mathematical term for the structure in Figure 1 is a directed 1-forest or functional graph. For simplicity, we shall use the terms forest and tree to refer to a directed 1-forest or a directed 1-tree because of the cycle at the root.\nar X\niv :1\n70 2.\n07 01\n5v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\ncess will allow us to accurately learn morphological structures in an unsupervised fashion.\nTo test this hypothesis, we define an objective over the entire forest representation. The proposed objective is designed to maximize the likelihood of local derivations, while constraining the overall number of affixes and encouraging tighter morphological families. We optimize this objective using integer linear programming (ILP), which is commonly employed to handle global constraints. While in prior work, ILP has often been employed in supervised settings, we explore its effectiveness in unsupervised learning. We induce a forest by alternating between learning local edge probabilities using a log-linear model, and enforcing global constraints with the ILP-based decoder. With each iteration, the model progresses towards more consistent forests.\nWe evaluate our model on three tasks: root detection, clustering of morphologically related families and segmentation. The last task has been extensively studied in the recent literature, providing us with the opportunity to compare the model with multiple unsupervised techniques. On benchmark datasets representing four languages, our model outperforms the baselines, yielding new state-of-the-art results. For instance, we improve segmentation performance on Turkish by 4.4% and on English by 3.7%, relative to the best published results (Narasimhan et al., 2015). Similarly, our model exhibits superior performance on the other two tasks. We also provide analysis of the model behavior which reveals that most of the gain comes from enforcing global constraints on the number of unique affixes."}, {"heading": "2 Related Work", "text": "Unsupervised morphological segmentation Most top performing algorithms for unsupervised segmentation today center around modeling singlestep derivations (Poon et al., 2009; Naradowsky and Toutanova, 2011; Narasimhan et al., 2015). A commonly used log-linear formulation enables these models to consider a rich set of features ranging from orthographic patterns to semantic relatedness. However, these models generally bypass global constraints (Narasimhan et al., 2015) or require performing inference over very large spaces (Poon et al., 2009). As we show in our\nanalysis (Section 5), this omission negatively affects model performance.\nIn contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013). These models are inherently limited in their ability to incorporate diverse features that are effectively utilized by local discriminative models.\nOur proposed approach attempts to combine the advantages of both approaches, by defining an objective that incorporates both levels of linguistic properties over the entire forest representation, and adopting an alternating training regime for optimization.\nGraph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016). The graph induction methods vary widely depending on the task and the available supervision. The distinctive feature of our work is the use of global constraints to guide the learning of local, edge-level derivations.\nILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al., 2011). In all of these applications, the ILP formulation is used with a supervised classifier. Our work demonstrates that this framework continues to be effective in an unsupervised setting, providing strong guidance for a local, unsupervised classifier."}, {"heading": "3 Model", "text": "Our model considers a full morphological assignment for all the words in a language, representing it as a forest. Let F = (V,E) be a directed graph where each word corresponds to a node v \u2208 V . A directed edge e = (vc, vp) \u2208 E encodes a single morphological derivation from a parent word vp to a child word vc. Edges also reflect the type of the\nunderlying derivation (e.g., prefixation), and an associated probability Pr(e). Note that the root of a tree is always marked with a self-directed (i.e. vc = vp) edge associated with the label stop. Figure 1 illustrates a single tree in the forest."}, {"heading": "3.1 Inducing morphological forests", "text": "We postulate that a valid assignment yields forests with the following properties:\n1. Increased edge weights Edge weights reflect probabilities of single-step derivations based on the local features including orthographic patterns and semantic relatedness. This local information helps identify that the edge (painter, paint) should be preferred over (painter, pain), because \u2212er is a valid suffix and paint is semantically closer to painter.\n2. Minimized number of affixes Prior research has shown that local models tend to greatly overestimate the number of suffixes. For instance, the model of Narasimhan et al. (2015) produces 617 unique affixes when segmenting 10000 English words. Thus, we explicitly encourage the model towards assignments with the least number of affixes.\n3. Minimized number of roots relatively to vocabulary size Similarly, the number of roots, and consequently the number of morphological families is markedly smaller than the size of the vocabulary.\nThe first property is local in nature, while the last two are global and embody the principle of Minimum Description Length (MDL). Based on these properties, we formulate an objective function S(F ) over a forest F :\nS(F ) = \u2212 \u2211 e\u2208E log Pr(e)\n|E| +\u03b1|Affix|+\u03b2 |F | |V | , (1)\nwhere |\u00b7| denotes set cardinality, Affix = {ak}Kk=1 is the set of all affixes, and |F | is the number of trees in F . |E| and |V | are the size of the edge set and vocabulary, respectively. The hyperparameters \u03b1 and \u03b2 capture the relative importance of the three terms.\nBy minimizing this objective, we encourage assignments with high edge probabilities (first term),\nwhile limiting the number of affixes and morphological families (second and third terms, respectively). This objective can also be viewed as a simple log-likelihood objective regularized by the last two terms in Equation (1).\nTo illustrate the interaction between local and global constraints in this objective, consider an example in Figure 2. If the model selects a different edge \u2013 e.g. (paint, pain) instead, all the terms in Equation (1) will be affected."}, {"heading": "3.2 Computing local probabilities", "text": "We now describe how to parameterize Pr(e), which captures the likelihood of a single-step morphological derivation between two words. Following prior\nwork (Narasimhan et al., 2015), we model this probability using a log-linear model:\nPr(w, z) \u221d exp(\u03b8 \u00b7 \u03c6(w, z)), (2)\nwhere \u03b8 is the set of parameters to be learned, and \u03c6(w, z) is the feature vector extracted from w and z. Each candidate z is a tuple (string, label), where label refers to the label of the potential edge.\nAs a result, the marginal probability is Pr(w) = \u2211\nz\u2208C(w)\nPr(w, z)\n= \u2211 z\u2208C(w) exp(\u03b8 \u00b7 \u03c6(w, z))\u2211\nw\u2032\u2208\u03a3\u2217 \u2211 z\u2032\u2208C(w\u2032) exp(\u03b8 \u00b7 \u03c6(w\u2032, z\u2032)) ,\nwhere \u03a3\u2217 is the set of all possible strings. Computing the sum in the denominator is infeasible. Instead, we make use of contrastive estimation (Smith and Eisner, 2005), substituting \u03a3\u2217 with a (limited) set of neighbor strings N(w) that are orthographically close to w. This technique distributes the probability mass among neighboring words and forces the model to identify meaningful discriminative features. We obtain N(w) by transposing characters in w, following the method described in Narasimhan et al. (2015).\nNow for the forest over the set of nodes V , the log-likelihood loss function is defined as:\nL(V ; \u03b8) = \u2212 \u2211 v\u2208V log Pr(v)\n= \u2212 \u2211 v\u2208V [ log \u2211 z\u2208C(v) exp(\u03b8 \u00b7 \u03c6(v, z))\n\u2212 log \u2211\nv\u2032\u2208N(v) \u2211 z\u2032\u2208C(v\u2032) exp(\u03b8 \u00b7 \u03c6(v\u2032, z\u2032)) ] ,\n(3)\nThis objective can be minimized by gradient descent.\nSpace of Possible Candidates We only consider assignments where the parent word is strictly shorter than the child to prevent cycles of length two or more. In addition to suffixation and prefixation, we also consider three types of transformations introduced in Goldwater and Johnson (2004): repetition, deletion, and modification. We also handle compounding, where two stems are combined to form a\nnew word (e.g., football). One of these stems carries the main semantic meaning of the compound and is considered to be the parent of the word. Note that stems are not considered affixes, so this does not affect the affix list.\nWe allow parents to be words outside V , since many legitimate word forms might never appear in the corpus. For instance, if we have V = {painter, paints}, the optimal solution would add an unseen word paint to the forest, and choose edges (painter, paint) and (paints, paint).\nFeatures We use the same set of features shown to be effective in prior work (Narasimhan et al., 2015), including word vector similarity, beginning and ending character bigrams, word frequencies and affixes. Affix features are automatically extracted from the corpus based on string difference and are thresholded based on frequency. We also include an additional sibling feature that counts how many words are siblings of word w in its tree. Siblings are words that are derived from the same parent, e.g., faithful and faithless, both from the word faith."}, {"heading": "3.3 ILP formulation", "text": "Minimizing the objective in Equation (1) is challenging because the second and third terms capture discrete global properties of the forest, which prevents us from performing gradient descent directly. Instead, we formulate this optimization problem as Integer Linear Programming (ILP), where these two terms can be cast as constraints.3\nFor each child word vi \u2208 V , we have a bounded set of its candidate outgoing edges C(vi) = {zji }, where zji is the j-th candidate for vi. C(vi) is the same set as defined in Section 3.2. Each edge is associated with pij , which is computed as log Pr(zji |vi). Let xij be a binary variable that has value 1 if and only if zji is chosen to be in the forest. Without loss of generality, we assume the first candidate edge is always the self-edge (or stop case), i.e., z1i = (vi, stop). We also use a set of binary variables {yk} to indicate whether affix ak is used at\n3If we had prior knowledge of words belonging to the same family, we can frame the problem as growing a Minimum Spanning Tree (MST), and use Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to solve it. However, this information is not available to us.\nleast once in F (i.e. required to explain a morphological change).\nNow let us consider how to derive our ILP formulation using the notations above. Note that |F | is equal to the number of self-edges \u2211 i xi1, and also a valid forest will satisfy |V | = |E|. Combining these pieces, we can rewrite the objective in equation (1) and arrive at the following ILP formulation:\nminimize xij ,yk\n\u2212 1 |V | \u2211 ij xijpij + \u03b1 \u2211 k yk + \u03b2 |V | \u2211 i xi1\nsubject to xij , yk \u2208 {0, 1},\u2211 j xij = 1,\u2200i, (4)\nxij \u2264 yk, if ak is involved in zji . (5)\nConstraint 4 states that exactly one of the candidate edges should be chosen for each word. The last constraint implies that we can only consider this candidate (and construct the corresponding edge) when the involved affix4 is used at least once in the forest representation."}, {"heading": "3.4 Alternating training", "text": "The objective function contains two sets of parameters: a continuous weight vector \u03b8 that parameterizes edge probabilities, and binary variables {xij} and {yk} in ILP. Due to the discordance between continuous and discrete variables, we need to optimize the objective in an alternating manner. Algorithm 1 details the training procedure. After automatically extracting affixes from the corpus, we alternate between learning the local edge probabilities (line 3) and solving ILP (line 4).\nThe feedback from solving ILP with the global constraints can help us refine the learning of local probabilities by removing incorrect affixes (line 5). For instance, automatic extraction based on frequencies can include -ers as an English suffix. This is likely to be eliminated by ILP, since all occurrences of -ers can be explained away without adding a new affix by concatenating -er and -s, two very common suffixes. After refining the affix set, we remove all candidates that involve any affix discarded by ILP. This corresponds to reducing the size of C(w) in equation (3). We then train the log-linear model\n4For English and German, where non-concatenative transformations are possible such as deletion of ending e (taking \u2192 take), we also include them in Affix.\nagain using the newly-pruned candidate set. By doing so, we force the model to learn from better contrastive signals, and focus on affixes of higher quality, resulting in a new set of probabilities {pij}. This procedure is repeated until no more affixes are rejected.5"}, {"heading": "4 Experiments", "text": "We evaluate our model on three tasks: segmentation, morphological family clustering, and root detection. While the first task has been extensively studied in the prior literature, we consider two additional tasks to assess the flexibility of the derived representation."}, {"heading": "4.1 Morphological segmentation", "text": "Data We choose four languages with distinct morphological properties: English, Turkish, Arabic, and German. Our training data consists of standard datasets used in prior work. Statistics for all datasets are summarized in Table 1. Note that for the Arabic test set, we filtered out duplicate words, and we reran the baselines to obtain comparable results.\nFollowing Narasimhan et al. (2015), we reduce the noise by truncating the training word list to the top K frequent words. In addition, we train word vectors (Mikolov et al., 2013) to obtain cosine similarity features. Statistics for all datasets are summarized in Table 1.\nBaselines We compare our approach against the state-of-the-art unsupervised method of Narasimhan et al. (2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009). For this baseline, we report the results of the publicly available implementation of the technique (NBJ\u201915), as well as our own improved reimplementation (NBJ-Imp). Specifically in NBJ-Imp, we expanded the original algorithm to handle compounding, along with sibling features as described in Section 3.2, making it essentially an ablation of our model without ILP and alternating training. We employ grid search to find the optimal hyperparameter setting.6\n5Typically the model converges after 5 rounds 6K \u2208 {2500, 5000, 10000}, number of automatically ex-\ntracted affixes \u2208 {100, 200, 300, 400, 500}\nAlgorithm 1 Morphological Forest Induction Input: wordlist V Output: Forest representation of V\n1: Affix\u2190 ExtractAffixes(W ) . Extract common patterns as affixes from the wordlist 2: for t\u2190 1 to T do . Alternating training for T iterations 3: ptij \u2190 ContrastiveEstimation(W,Affix) . Compute local probabilities, cf. Section 3.2 4: y\u2217t, F t \u2190 ILP(ptij) . Get indicators for affixes, and the forest, cf. Section 3.3 5: PruneAffixSet(Affix, y\u2217t) . Prune affix set using the output from ILP, cf. Section 3.4\nreturn F T\nLanguage Train Test WordVec#Words #Words #Words\nEnglish MC-10 MC-05:10 Wikipedia 878K 2212 129M\nTurkish MC-10 MC-05:10 BOUN 617K 2531 361M\nArabic Gigaword ATB Gigaword\n3.83M 21085 1.22G\nGerman MC-10 Dsolve Wikipedia 2.34M 15522 589M\nTable 1: Data statistics: MC-10 = MorphoChallenge 2010 , MC:05-10 = aggregated from MorphoChallenge 2005-2010, BOUN = BOUN corpus (Sak et al., 2008), Gigaword = Arabic Gigaword corpus (Parker et al., 2011), ATB = Arabic Treebank (Maamouri et al., 2003). Duplicates in Arabic test set are filtered. Dsolve is the dataset released by Wu\u0308rzner and Jurish (2015), and for training German vectors, we use the pre-processed Wikipedia dump from (Al-Rfou et al., 2013).\nWe also include a supervised counterpart, which uses the same set of features as NBJ-Imp but has access to gold segmentation during training (we perform 5-fold cross-validation using the same data). We obtain the gold standard parent-child pairs required for training from the segmented words in a straightforward fashion.\nEvaluation metric Following prior work (Virpioja et al., 2011), we evaluate all models using the standard boundary precision and recall (BPR). This measure assesses the accuracy of individual segmentation points, producing IR-style Precision, Recall and F1 scores.\nLanguage #Words #Clusters #Wordsper Cluster English 75,416 20,249 3.72 German 367,967 28,198 13.05\nTable 2: Data statistics for the family clustering task (CELEX). We only evaluate on English and German, since these are the languages MorphoChallenge has segmentations for.\nTraining For unsupervised training, we use the gradient descent method ADAM (Kingma and Ba, 2014) and optimize over the whole batch of training words. We use a Gurobi7 solver for the ILP."}, {"heading": "4.2 Morphological family clustering", "text": "Morphological family clustering is the task of clustering morphologically related word forms. For instance, we want to group paint, paints and pain into two clusters: {paint, paints} and {pain}. To derive clusters from the forest representation, we assume that all the words in the same tree form a cluster.\nData To obtain gold information about morphological clusters, we use CELEX (Baayen et al., 1993). Data statistics are summarized in Table 2. We remove words without stems from CELEX.8\nBaseline We compare our model against NBJ-Imp described above. We select the best variant of our model and the base model based on their respective performance on the segmentation task.\nEvaluation We use the metrics proposed by Schone and Jurafsky (2000). Specifically, let Xw\n7http://www.gurobi.com/ 8An example is aerodrome, where both aero- and drome are\naffixes.\nand Yw be the clusters for word w in our predictions and gold standard respectively. We compute the number of correct (C), inserted (I) and deleted (D) words for the clusters as follows:\nC = \u2211 w\u2208W |Xw \u2229 Yw| |Yw|\nI = \u2211 w\u2208W |Xw \\ Yw| |Yw|\nD = \u2211 w\u2208W |Yw \\Xw| |Yw|\nThen we compute precision = CC+I , recall = C C+D , F1 = 2 precision\u00b7recall precision+recall ."}, {"heading": "4.3 Root detection", "text": "In addition, we evaluate how accurately our model can predict the root of any given word.\nData We report the results on the Chipmunk dataset (Cotterell et al., 2015) which has been used for evaluating supervised models for root detection. Since our model is unsupervised, we report the performance both on the test set only, and on the entire dataset, combining the train/test split. Statistics for the dataset are shown in Table 3."}, {"heading": "5 Results", "text": "In the following subsections, we report model performance on each one of the three evaluation tasks."}, {"heading": "5.1 Segmentation", "text": "9We used cosine similarity features in all experiments. But the root forms of German verbs are rarely used, except in imperative sentences. Consequently they barely have trained word vectors, contributing to the low recall value. We suspect better treatment with word vectors can further improve the results.\n10http://www.mathcracker.com/sign-test. php\nFrom Table 4, we observe that our model consistently outperforms the baselines on all four lan-\nguages. Compared to NBJ\u201915, our model has a higher F1 score by 3.7%, 4.4%, 2.9% and 27.7% on English, Turkish, Arabic and German, respectively. While the improved implementation NBJ-Imp benefits from the addition of compounding and sibling features, our model still delivers an absolute increase in F1 score, ranging from 1.8% to 7.7% over NBJImp. Note that our model achieves higher scores even without tuning the threshold K or the number of affixes, whereas the baselines have optimal hyperparameter settings via grid search.\nTo understand the importance of global constraints (the last two terms of equation 1), we analyze our model\u2019s performance with different values of \u03b1 and \u03b2 (see Figure 3). The first constraint, which controls the size of the affix set, plays a more dominant role than the second. By setting \u03b1 = 0.0, the model scores at best 75.7% on English and 63.2% on Turkish, lower than the baseline. While the value of \u03b2 also affects the F1 score, its role is secondary in achieving optimal performance.\nThe results also demonstrate that language properties can greatly affect the feature set choice. For fusional languages such as English, computing of sibling features is unreliable. For example, two descendants of the same parent spot \u2013 spotless and spotty \u2013 may not be necessarily identified as such by a simple sibling computation algorithm, since they undergo different changes. In contrast, Turkish is highly agglutinative, with minimal (if any) transformations, but each word can have up to hundreds of related forms. Consequently, sibling features have different effects on English and Turkish, leading to changes of \u22120.3% and +2.1% in F1 score respectively.\nUnderstanding model behavior We find that much of the gain in model performance comes from the first two rounds of training. As Figure 4 shows, the improvement mainly stems from solving ILP in the first round, followed by training the log-linear model in the second round after removing affixes and pruning candidate sets. This is exactly what we expect from the ILP formulation \u2013 to globally adjust the forest by reducing the number of unique affixes. We find this to be quite effective \u2013 in English, out of 500 prefixes, only 6 remain: de, dis, im, in, re, and un. Similarly, only 72 out of 500 suffixes survive\nafter this reduction.\nRobustness We also investigate how robust our model is to the choice of hyperparameters. Figure 3 illustrates that we can obtain a sizable boost over the baseline by choosing \u03b1 and \u03b2 within a fairly wide region. Note that \u03b1 takes on a much smaller value than \u03b2, to maintain the two constraints (|Affix| and |F | |V | ) at comparable magnitudes.\nNarasimhan et al. (2015) observe that after including more than K = 10000 words, the performance of the unsupervised model drops noticeably. In contrast, our model handles training noise more robustly, resulting in a steady boost or not too big drop in performance with increasing training size (Figure 5). In fact, it scores 83.0% with K = 40000 on English, a 6.0% increase in absolute value over the baseline.\nQualitative analysis Table 5 shows examples of English words that our model segments correctly, while NBJ\u201915 fails on them. We present them in three categories (top to bottom) based on the component of our model that contributes to the successful segmentation. The first category benefits from a refinement of affix set, by removing noisy ones, such as -nce, -ch, and k-. This leads to correct stopping as in the case of knuckle or induction of the right suffix, as in divergence. Further, a smaller affix set also leads to more concentrated weights for the remaining affixes. For example, the feature weight for -ive jumps from 0.06 to 0.25, so that the derivation negative \u2192 negate is favored, as shown in the second category. Finally, the last category lists some compound words that our model successfully segments."}, {"heading": "5.2 Morphological family clustering", "text": "We show the results for morphological family clustering in Table 6. For both languages, our model increases precision by a wide margin, with a modest boost for recall as well. This corroborates our findings in the segmentation task, where our model can effectively remove incorrect affixes while still encouraging words to form tight, cohesive families."}, {"heading": "5.3 Root detection", "text": "Table 7 summarizes the results for the root detection task. Our model shows consistent improvements over the baseline on all three languages. We also include the results on the test set of two supervised systems: Morfette (Chrupala et al., 2008) and Chipmunk (Cotterell et al., 2015). Morfette is a string transducer while Chipmunk is a segmenter. Both systems have access to morphologically annotated corpora.\nOur model is quite competitive against Morfette. In fact, it achieves higher accuracy for English and Turkish. Compared with Chipmunk, our model\nscores 0.65 versus 0.70 on English, bridging the gap significantly. However, the high accuracy for morphologically complex languages such as Turkish and German suggests that unsupervised root detection remains a hard task."}, {"heading": "6 Conclusions", "text": "In this work, we focus on unsupervised modeling of morphological families, collectively defining a forest over the language vocabulary. This formulation enables us to incorporate both local and global properties of morphological assignment. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. Our experiments demonstrate that our model yields consistent gains in three morphological tasks compared with the best published results."}, {"heading": "Acknowledgement", "text": "We thank Tao Lei, Yuan Zhang and the members of the MIT NLP group for helpful discussions and\nfeedback. We are also grateful to anonymous reviewers for their insightful comments."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183\u2013192, Sofia, Bulgaria, August. Association", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "The CELEX lexical data base on CD-ROM", "author": ["R Harald Baayen", "Richard Piepenbrock", "Rijn van H"], "venue": null, "citeRegEx": "Baayen et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Baayen et al\\.", "year": 1993}, {"title": "Global learning of typed entailment rules", "author": ["Jonathan Berant", "Ido Dagan", "Jacob Goldberger."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 610\u2013619. Asso-", "citeRegEx": "Berant et al\\.,? 2011", "shortCiteRegEx": "Berant et al\\.", "year": 2011}, {"title": "Learning morphology with Morfette", "author": ["Grzegorz Chrupala", "Georgiana Dinu", "Josef van Genabith."], "venue": "LREC.", "citeRegEx": "Chrupala et al\\.,? 2008", "shortCiteRegEx": "Chrupala et al\\.", "year": 2008}, {"title": "On shortest arborescence of a directed graph", "author": ["Yoeng-Jin Chu", "Tseng-Hong Liu."], "venue": "Scientia Sinica, 14(10):1396.", "citeRegEx": "Chu and Liu.,? 1965", "shortCiteRegEx": "Chu and Liu.", "year": 1965}, {"title": "Global inference for sentence compression: An integer linear programming approach", "author": ["James Clarke", "Mirella Lapata."], "venue": "Journal of Artificial Intelligence Research, pages 399\u2013429.", "citeRegEx": "Clarke and Lapata.,? 2008", "shortCiteRegEx": "Clarke and Lapata.", "year": 2008}, {"title": "Labeled morphological segmentation with semi-Markov models", "author": ["Ryan Cotterell", "Thomas M\u00fcller", "Alexander Fraser", "Hinrich Sch\u00fctze."], "venue": "CoNLL 2015, page 164.", "citeRegEx": "Cotterell et al\\.,? 2015", "shortCiteRegEx": "Cotterell et al\\.", "year": 2015}, {"title": "Inducing the morphological lexicon of a natural language from unannotated text", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR05), vol-", "citeRegEx": "Creutz and Lagus.,? 2005", "shortCiteRegEx": "Creutz and Lagus.", "year": 2005}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "ACM Transactions on Speech and Language Processing (TSLP), 4(1):3.", "citeRegEx": "Creutz and Lagus.,? 2007", "shortCiteRegEx": "Creutz and Lagus.", "year": 2007}, {"title": "Graphical models over multiple strings", "author": ["Markus Dreyer", "Jason Eisner."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1, pages 101\u2013110. Association for Computational Linguistics.", "citeRegEx": "Dreyer and Eisner.,? 2009", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2009}, {"title": "Optimum branchings", "author": ["Jack Edmonds."], "venue": "Journal of Research of the National Bureau of Standards B, 71(4):233\u2013240.", "citeRegEx": "Edmonds.,? 1967", "shortCiteRegEx": "Edmonds.", "year": 1967}, {"title": "Morpho-syntactic lexicon generation using graph-based semi-supervised learning", "author": ["Manaal Faruqui", "Ryan McDonald", "Radu Soricut."], "venue": "Transactions of the Association for Computational Linguistics, 4:1\u2013", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Priors in Bayesian learning of phonological rules", "author": ["Sharon Goldwater", "Mark Johnson."], "venue": "Proceedings of the 7th Meeting of the ACL Special Interest Group in Computational Phonology: Current Themes in Computational Phonology and Morphology, pages", "citeRegEx": "Goldwater and Johnson.,? 2004", "shortCiteRegEx": "Goldwater and Johnson.", "year": 2004}, {"title": "A Bayesian framework for word segmentation: Exploring the effects of context", "author": ["Sharon Goldwater", "Thomas L Griffiths", "Mark Johnson."], "venue": "Cognition, 112(1):21\u201354.", "citeRegEx": "Goldwater et al\\.,? 2009", "shortCiteRegEx": "Goldwater et al\\.", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Modeling syntactic context improves morphological segmentation", "author": ["Yoong Keok Lee", "Aria Haghighi", "Regina Barzilay."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 1\u20139. Association for Computa-", "citeRegEx": "Lee et al\\.,? 2011", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Arabic treebank: Part 1 v 2.0. Distributed by the Linguistic Data Consortium. LDC Catalog No.: LDC2003T06", "author": ["Mohamed Maamouri", "Ann Bies", "Hubert Jin", "Tim Buckwalter"], "venue": null, "citeRegEx": "Maamouri et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Maamouri et al\\.", "year": 2003}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Unsupervised bilingual morpheme segmentation and alignment with context-rich hidden semi-Markov models", "author": ["Jason Naradowsky", "Kristina Toutanova."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Naradowsky and Toutanova.,? 2011", "shortCiteRegEx": "Naradowsky and Toutanova.", "year": 2011}, {"title": "An unsupervised method for uncovering morphological chains", "author": ["Karthik Narasimhan", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Transactions of the Association for Computational Linguistics, 3:157\u2013167.", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Arabic gigaword fifth edition ldc2011t11", "author": ["Robert Parker", "David Graff", "Ke Chen", "Junbo Kong", "Kazuaki Maeda."], "venue": "Philadelphia: Linguistic Data Consortium.", "citeRegEx": "Parker et al\\.,? 2011", "shortCiteRegEx": "Parker et al\\.", "year": 2011}, {"title": "Dual decomposition inference for graphical models over strings", "author": ["Nanyun Peng", "Ryan Cotterell", "Jason Eisner."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 917\u2013927, Lisbon, Portugal, September. As-", "citeRegEx": "Peng et al\\.,? 2015", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Unsupervised morphological segmentation with log-linear models", "author": ["Hoifung Poon", "Colin Cherry", "Kristina Toutanova."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for", "citeRegEx": "Poon et al\\.,? 2009", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "Relational learning via propositional algorithms: An information extraction case study", "author": ["Dan Roth", "Wen-tau Yih."], "venue": "International Joint Conference on Artificial Intelligence, volume 17, pages 1257\u20131263. LAWRENCE ERLBAUM ASSOCIATES LTD.", "citeRegEx": "Roth and Yih.,? 2001", "shortCiteRegEx": "Roth and Yih.", "year": 2001}, {"title": "Turkish language resources: Morphological parser, morphological disambiguator and web corpus", "author": ["Ha\u015fim Sak", "Tunga G\u00fcng\u00f6r", "Murat Sara\u00e7lar."], "venue": "Advances in natural language processing, pages 417\u2013 427. Springer.", "citeRegEx": "Sak et al\\.,? 2008", "shortCiteRegEx": "Sak et al\\.", "year": 2008}, {"title": "Knowledgefree induction of morphology using latent semantic analysis", "author": ["Patrick Schone", "Daniel Jurafsky."], "venue": "Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning-Volume 7,", "citeRegEx": "Schone and Jurafsky.,? 2000", "shortCiteRegEx": "Schone and Jurafsky.", "year": 2000}, {"title": "Minimallysupervised morphological segmentation using adaptor grammars", "author": ["Kairit Sirts", "Sharon Goldwater."], "venue": "Transactions of the Association for Computational Linguistics, 1:255\u2013266.", "citeRegEx": "Sirts and Goldwater.,? 2013", "shortCiteRegEx": "Sirts and Goldwater.", "year": 2013}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["Noah A Smith", "Jason Eisner."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 354\u2013362. Association for Computational Linguistics.", "citeRegEx": "Smith and Eisner.,? 2005", "shortCiteRegEx": "Smith and Eisner.", "year": 2005}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "ACL, pages 737\u2013745.", "citeRegEx": "Snyder and Barzilay.,? 2008", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2008}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Radu Soricut", "Franz Och."], "venue": "Proc. NAACL.", "citeRegEx": "Soricut and Och.,? 2015", "shortCiteRegEx": "Soricut and Och.", "year": 2015}, {"title": "Unsupervised morphology rivals supervised morphology for Arabic MT", "author": ["David Stallard", "Jacob Devlin", "Michael Kayser", "Yoong Keok Lee", "Regina Barzilay."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational", "citeRegEx": "Stallard et al\\.,? 2012", "shortCiteRegEx": "Stallard et al\\.", "year": 2012}, {"title": "Empirical comparison of evaluation methods for unsupervised learning of morphology", "author": ["Sami Virpioja", "Ville T. Turunen", "Sebastian Spiegler", "Oskar Kohonen", "Mikko Kurimo."], "venue": "Traitement Automatique des Langues, 52(2):45\u201390.", "citeRegEx": "Virpioja et al\\.,? 2011", "shortCiteRegEx": "Virpioja et al\\.", "year": 2011}, {"title": "Morfessor 2.0: Python implementation and extensions for Morfessor baseline", "author": ["Sami Virpioja", "Peter Smit", "Stig-Arne Gr\u00f6nroos", "Mikko Kurimo"], "venue": null, "citeRegEx": "Virpioja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Virpioja et al\\.", "year": 2013}, {"title": "Dsolvemorphological segmentation for German using conditional random fields", "author": ["Kay-Michael W\u00fcrzner", "Bryan Jurish."], "venue": "Systems and Frameworks for Computational Morphology, pages 94\u2013103. Springer.", "citeRegEx": "W\u00fcrzner and Jurish.,? 2015", "shortCiteRegEx": "W\u00fcrzner and Jurish.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "7%, relative to the best published results (Narasimhan et al., 2015).", "startOffset": 43, "endOffset": 68}, {"referenceID": 22, "context": "Unsupervised morphological segmentation Most top performing algorithms for unsupervised segmentation today center around modeling singlestep derivations (Poon et al., 2009; Naradowsky and Toutanova, 2011; Narasimhan et al., 2015).", "startOffset": 153, "endOffset": 229}, {"referenceID": 18, "context": "Unsupervised morphological segmentation Most top performing algorithms for unsupervised segmentation today center around modeling singlestep derivations (Poon et al., 2009; Naradowsky and Toutanova, 2011; Narasimhan et al., 2015).", "startOffset": 153, "endOffset": 229}, {"referenceID": 19, "context": "Unsupervised morphological segmentation Most top performing algorithms for unsupervised segmentation today center around modeling singlestep derivations (Poon et al., 2009; Naradowsky and Toutanova, 2011; Narasimhan et al., 2015).", "startOffset": 153, "endOffset": 229}, {"referenceID": 19, "context": "However, these models generally bypass global constraints (Narasimhan et al., 2015) or require performing inference over very large spaces (Poon et al.", "startOffset": 58, "endOffset": 83}, {"referenceID": 22, "context": ", 2015) or require performing inference over very large spaces (Poon et al., 2009).", "startOffset": 63, "endOffset": 82}, {"referenceID": 8, "context": "In contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013).", "startOffset": 117, "endOffset": 219}, {"referenceID": 28, "context": "In contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013).", "startOffset": 117, "endOffset": 219}, {"referenceID": 13, "context": "In contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013).", "startOffset": 117, "endOffset": 219}, {"referenceID": 26, "context": "In contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013).", "startOffset": 117, "endOffset": 219}, {"referenceID": 9, "context": "Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016).", "startOffset": 153, "endOffset": 242}, {"referenceID": 21, "context": "Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016).", "startOffset": 153, "endOffset": 242}, {"referenceID": 29, "context": "Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016).", "startOffset": 153, "endOffset": 242}, {"referenceID": 11, "context": "Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016).", "startOffset": 153, "endOffset": 242}, {"referenceID": 23, "context": "ILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al.", "startOffset": 184, "endOffset": 204}, {"referenceID": 5, "context": "ILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al.", "startOffset": 227, "endOffset": 252}, {"referenceID": 2, "context": "ILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al., 2011).", "startOffset": 277, "endOffset": 298}, {"referenceID": 19, "context": "For instance, the model of Narasimhan et al. (2015) produces 617 unique affixes when segmenting 10000 English words.", "startOffset": 27, "endOffset": 52}, {"referenceID": 19, "context": "work (Narasimhan et al., 2015), we model this probability using a log-linear model:", "startOffset": 5, "endOffset": 30}, {"referenceID": 27, "context": "Instead, we make use of contrastive estimation (Smith and Eisner, 2005), substituting \u03a3\u2217 with a (limited) set of neighbor strings N(w) that are orthographically close to w.", "startOffset": 47, "endOffset": 71}, {"referenceID": 19, "context": "We obtain N(w) by transposing characters in w, following the method described in Narasimhan et al. (2015). Now for the forest over the set of nodes V , the log-likelihood loss function is defined as:", "startOffset": 81, "endOffset": 106}, {"referenceID": 12, "context": "In addition to suffixation and prefixation, we also consider three types of transformations introduced in Goldwater and Johnson (2004): repetition, deletion, and modification.", "startOffset": 106, "endOffset": 135}, {"referenceID": 19, "context": "Features We use the same set of features shown to be effective in prior work (Narasimhan et al., 2015), including word vector similarity, beginning and ending character bigrams, word frequencies and affixes.", "startOffset": 77, "endOffset": 102}, {"referenceID": 4, "context": "If we had prior knowledge of words belonging to the same family, we can frame the problem as growing a Minimum Spanning Tree (MST), and use Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to solve it.", "startOffset": 166, "endOffset": 200}, {"referenceID": 10, "context": "If we had prior knowledge of words belonging to the same family, we can frame the problem as growing a Minimum Spanning Tree (MST), and use Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to solve it.", "startOffset": 166, "endOffset": 200}, {"referenceID": 17, "context": "In addition, we train word vectors (Mikolov et al., 2013) to obtain cosine similarity features.", "startOffset": 35, "endOffset": 57}, {"referenceID": 18, "context": "Following Narasimhan et al. (2015), we reduce the noise by truncating the training word list to the top K frequent words.", "startOffset": 10, "endOffset": 35}, {"referenceID": 7, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 32, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 26, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 15, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 30, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 22, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 16, "context": "Baselines We compare our approach against the state-of-the-art unsupervised method of Narasimhan et al. (2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al.", "startOffset": 86, "endOffset": 111}, {"referenceID": 24, "context": "Table 1: Data statistics: MC-10 = MorphoChallenge 2010 , MC:05-10 = aggregated from MorphoChallenge 2005-2010, BOUN = BOUN corpus (Sak et al., 2008), Gigaword = Arabic Gigaword corpus (Parker et al.", "startOffset": 130, "endOffset": 148}, {"referenceID": 20, "context": ", 2008), Gigaword = Arabic Gigaword corpus (Parker et al., 2011), ATB = Arabic Treebank (Maamouri et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 16, "context": ", 2011), ATB = Arabic Treebank (Maamouri et al., 2003).", "startOffset": 31, "endOffset": 54}, {"referenceID": 0, "context": "Dsolve is the dataset released by W\u00fcrzner and Jurish (2015), and for training German vectors, we use the pre-processed Wikipedia dump from (Al-Rfou et al., 2013).", "startOffset": 139, "endOffset": 161}, {"referenceID": 15, "context": ", 2011), ATB = Arabic Treebank (Maamouri et al., 2003). Duplicates in Arabic test set are filtered. Dsolve is the dataset released by W\u00fcrzner and Jurish (2015), and for training German vectors, we use the pre-processed Wikipedia dump from (Al-Rfou et al.", "startOffset": 32, "endOffset": 160}, {"referenceID": 31, "context": "Evaluation metric Following prior work (Virpioja et al., 2011), we evaluate all models using the standard boundary precision and recall (BPR).", "startOffset": 39, "endOffset": 62}, {"referenceID": 14, "context": "Training For unsupervised training, we use the gradient descent method ADAM (Kingma and Ba, 2014) and optimize over the whole batch of training words.", "startOffset": 76, "endOffset": 97}, {"referenceID": 1, "context": "Data To obtain gold information about morphological clusters, we use CELEX (Baayen et al., 1993).", "startOffset": 75, "endOffset": 96}, {"referenceID": 25, "context": "Evaluation We use the metrics proposed by Schone and Jurafsky (2000). Specifically, let Xw", "startOffset": 42, "endOffset": 69}, {"referenceID": 6, "context": "Data We report the results on the Chipmunk dataset (Cotterell et al., 2015) which has been used for evaluating supervised models for root detection.", "startOffset": 51, "endOffset": 75}, {"referenceID": 19, "context": "Table 4: Segmentation results for the supervised model and three unsupervised models: the state-ofthe-art system NBJ\u201915 (Narasimhan et al., 2015), our improved implementation of their system NBJImp and our model.", "startOffset": 120, "endOffset": 145}, {"referenceID": 19, "context": "Narasimhan et al. (2015) observe that after including more than K = 10000 words, the performance of the unsupervised model drops noticeably.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "We also include the results on the test set of two supervised systems: Morfette (Chrupala et al., 2008) and Chipmunk (Cotterell et al.", "startOffset": 80, "endOffset": 103}, {"referenceID": 6, "context": ", 2008) and Chipmunk (Cotterell et al., 2015).", "startOffset": 21, "endOffset": 45}, {"referenceID": 6, "context": "Numbers for Morfette and Chipmunk are reported by Cotterell et al. (2015).", "startOffset": 50, "endOffset": 74}], "year": 2017, "abstractText": "This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edgewise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results.1", "creator": "LaTeX with hyperref package"}}}