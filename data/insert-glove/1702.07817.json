{"id": "1702.07817", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "Unsupervised Sequence Classification using Sequential Output Statistics", "abstract": "strugnell We address inaho a class of hoff unsupervised learning problems where newe the purchased same goal of plancha supervised hemert learning is precursors aimed agriculturalists except barkeeper with sharry no kaisha output rados labels provided for saltbush training tisci classifiers. peachcare This autobahns type of realtime unsupervised batted learning is 99.82 highly 46.77 valuable rennard in machine learning harmodio practice 41.36 since obtaining labels dmo in training osinde data gremel is gihanga often neuroradiology costly. 118-109 Instead cloths of n95 pairing machimura input - blackbody output safeer samples, we exploit sequential statistics of output nive labels, in brunei the form earnings of thec N - sixth gram language models, which can be vic obtained duplicating independently keilman of melamid input ol\u00e1h data k-d and eef thus vois with neuquen low or no cost. We introduce a novel dahllof cost aimee function in this 3,867 unsupervised learning 5.54 setting, whose beseeched profiles are analyzed orioli and shown leusden to 3-of-6 be 112.6 highly non - convex with crasson large barriers near ovum the kupelo global optimum. forschungsgruppe A belayer new stochastic threatened primal - dual gradient method is thm developed to lurk optimize this peep very neverland difficult 12/10 type of reconstructionism cost function via sub-17 the reprove use changu of dual donavon variables khmelnytskyi to reduce the catchy barriers. We demonstrate geotagged in garching experimental 631,000 evaluation, blackerby with muhua both aiman synthetic revolution and real - world data set-theoretic sets, that the receiving new kh\u00e1n method satilla for marghiloman unsupervised quandaries learning darwis gives ystwyth drastically kra lower nankov errors brava and diwata higher learning efficiency than the standard mfah stochastic gradient descent, reaching kjeld classification errors skalbmierz about twice 33.19 of those estadio obtained sturdiest by morkel fully duckling supervised learning. We 6000 also show keratitis the protestantism crucial role adeline of tortuga labels ' supplemental sequential statistics parrying exploited for label - 50.6 free b\u00f8e training with the new method, wijnstekers reflected by ergot the ziemann significantly lower gable classification errors when steered higher - order 23-count language models trofim are bardan used schanzer in 37.57 unsupervised k\u00e1n learning 40.48 than ordinariates low - order ones.", "histories": [["v1", "Sat, 25 Feb 2017 01:55:38 GMT  (8541kb,D)", "https://arxiv.org/abs/1702.07817v1", null], ["v2", "Fri, 26 May 2017 18:30:24 GMT  (8590kb,D)", "http://arxiv.org/abs/1702.07817v2", "All authors contributed equally to the paper. 17 pages, 7 figures and 2 tables"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yu liu", "jianshu chen", "li deng"], "accepted": true, "id": "1702.07817"}, "pdf": {"name": "1702.07817.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Sequence Classification using Sequential Output Statistics", "authors": ["Yu Liu", "Jianshu Chen", "Li Deng"], "emails": ["jianshuc@microsoft.com", "deng@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Unsupervised learning is one of the most challenging problems in machine learning. It is often formulated as the modeling of how the world works without requiring a huge amount of human labeling effort, e.g. [8]. To reach this grand goal, it is necessary to first solve a sub-goal of unsupervised learning with high practical value; that is, learning to predict output labels from input data without requiring costly labeled data. Toward this end, we study in this paper the learning of a sequence classifier without labels by using sequential output statistics. The problem is highly valuable since the sequential output statistics, such as language models, could be obtained independently of the input data and thus with no labeling cost.\nThe problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12]. When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10]. In both ways, the above unsupervised methods played an auxiliary role in helping supervised learning when it is applied to prediction tasks.\nRecently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7]. Similar to this work, the authors in [7] proposed an unsupervised cost that also exploits the sequence prior of the output samples to train classifiers. The power of such a strong prior in the form of language models in unsupervised learning was also demonstrated in earlier studies in [21, 3]. However, these earlier methods did not perform well in practical prediction tasks with real-world data without using additional strong generative models. Possible reasons are inappropriately formulated cost functions and inappropriate choices of optimization methods. For example, it was shown in [7] that optimizing\n\u2217All the authors contributed equally to the paper.\nar X\niv :1\n70 2.\n07 81\n7v 2\n[ cs\n.L G\n] 2\n6 M\nay 2\nthe highly non-convex unsupervised cost function could easily get stuck in trivial solutions, although adding a special regularization mitigated the problem somewhat.\nThe solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects. First, we propose a novel cost function for unsupervised learning, and find that it has a desired coverage-seeking property that makes the learning algorithm less inclined to be stuck in trivial solutions than the cost function in [7]. Second, we develop a special empirical formulation of this cost function that avoids the need for a strong generative model as in [30]. Third, although the proposed cost function is more difficult to optimize in its functional form, we develop a stochastic primal-dual gradient (SPDG) algorithm to effectively solve problem. Our analysis of SPDG demonstrates how it is able to reduce the high barriers in the cost function by transforming it into a primal-dual domain. Finally and most importantly, we demonstrate the new cost function and the associated SPDG optimization algorithm work well in two real-world classification tasks. In the rest of the paper, we proceed to demonstrate these points and discuss related works along the way."}, {"heading": "2 Empirical-ODM: An unsupervised learning cost for sequence classifiers", "text": "In this section, we extend the earlier work of [30] and propose an unsupervised learning cost named Empirical Output Distribution Match (Empirical-ODM) for training classifiers without labeled data. We first formulate the unsupervised learning problem with sequential output structures. Then, we introduce the Empirical-ODM cost and discuss its important properties that are closely related to unsupervised learning."}, {"heading": "2.1 Problem formulation", "text": "We consider the problem of learning a sequence classifier that predicts an output sequence (y1, . . . , yT0) from an input sequence (x1, . . . , xT0) without using labeled data, where T0 denotes the length of the sequence. Specifically, the learning algorithm does not have access to a labeled training set DXY , {(xn1 , . . . , xnTn), (y n 1 , . . . , y n Tn\n) : n = 1, . . . ,M}, where Tn denotes the length of the n-th sequence. Instead, what is available is a collection of input sequences, denoted as DX , {(xn1 , . . . , xnTn) : n = 1, . . . ,M}. In addition, we assume that the sequential output statistics (or sequence prior), in the form of an N -gram probability, are available:\npLM(i1, . . . , iN ) , pLM(y n t\u2212N+1 = i1, . . . , y n t = iN )\nwhere i1, . . . , iN \u2208 {1, . . . , C} and the subscript \u201cLM\u201d stands for language model. Our objective is to train the sequence classifier by just using DX and pLM(\u00b7). Note that the sequence prior pLM(\u00b7), in the form of language models, is a type of structure commonly found in natural language data, which can be learned from a large amount of text data freely available without labeling cost. For example, in optical character recognition (OCR) tasks, ynt could be an English character and x n t is the input image containing this character. We can estimate an N -gram character-level language model pLM(\u00b7) from a separate text corpus. Therefore, our learning algorithm will work in a fully unsupervised manner, without any human labeling cost. In our experiment section, we will demonstrate the effectiveness of our method on such a real OCR task. Other potential applications include speech recognition, machine translation, and image/video captioning.\nIn this paper, we focus on the sequence classifier in the form of p\u03b8(ynt |xnt ) that is, it computes the posterior probability p\u03b8(ynt |xnt ) only based on the current input sample xnt in the sequence. Furthermore, we restrict our choice of p\u03b8(ynt |xnt ) to be linear classifiers2 and focus our attention on designing and understanding unsupervised learning costs and methods for label-free prediction. In fact, as we will show in later sections, even with linear models, the unsupervised learning problem is still highly nontrivial and the cost function is also highly non-convex. And we emphasize that developing a successful unsupervised learning approach for linear classifiers, as we do in this paper, provides important insights and is an important first step towards more advanced nonlinear models (e.g., deep neural networks). We expect that, in future work, the insights obtained here could help us generalize our techniques to nonlinear models.\nA recent work that shares the same motivations as our work is [29], which also recognizes the high cost of obtaining labeled data and seeks label-free prediction. Different from our setting, they exploit\n2p\u03b8(y n t = i|xnt ) = e\u03b3w T i x n t / \u2211C j=1 e \u03b3wTj x n t , where the model parameter is \u03b8 , {wi \u2208 Rd, i = 1, . . . , C}.\ndomain knowledge from laws of physics in computer vision applications, whereas our approach exploits sequential statistics in the natural language outputs. Finally, our problem is fundamentally different from the sequence transduction method in [15], although it also exploits language models for sequence prediction. Specifically, the method in [15] is a fully supervised learning in that it requires supervision at the sequence level; that is, for each input sequence, a corresponding output sequence (of possibly different length) is provided as a label. The use of language model in [15] only serves the purpose of regularization in the sequence-level supervised learning. In stark contrast, the unsupervised learning we propose does not require supervision at any level including specifically the sequence level; we do not need the sequence labels but only the prior distribution pLM(\u00b7) of the output sequences."}, {"heading": "2.2 The Empirical-ODM", "text": "We now introduce an unsupervised learning cost that exploits the sequence structure in pLM(\u00b7). It is mainly inspired by the approach to breaking the Caesar cipher, one of the simplest forms of encryption [23]. Caesar cipher is a substitution cipher where each letter in the original message is replaced with a letter corresponding to a certain number of letters up or down in the alphabet. For example, the letter \u201cD\u201d is replaced by the letter \u201cA\u201d, the letter \u201cE\u201d is replaced by the letter \u201cB\u201d, and so on. In this way, the original message that was readable ends up being less understandable. The amount of this shifting is also known to the intended receiver of the message, who can decode the message by shifting back each letter in the encrypted message. However, Caesar cipher could also be broken by an unintended receiver (not knowing the shift) when it analyzes the frequencies of the letters in the encrypted messages and matches them up with the letter distribution of the original text [4, pp.9-11]. More formally, let yt = f(xt) denote a function that maps each encrypted letter xt into an original letter yt. And let pLM(i) , pLM(yt = i) denote the prior letter distribution of the original message, estimated from a regular text corpus. When f(\u00b7) is constructed in a way that all mapped letters {yt : yt = f(xt), t = 1, . . . , T} have the same distribution as the prior pLM(i), it is able to break the Caesar cipher and recover the original letters at the mapping outputs.\nInspired by the above approach, the posterior probability p\u03b8(ynt |xnt ) in our classification problem can be interpreted as a stochastic mapping, which maps each input vector xnt (the \u201cencrypted letter\u201d) into an output vector ynt (the \u201coriginal letter\u201d) with probability p\u03b8(y n t |xnt ). Then in a samplewise manner, each input sequence (xn1 , . . . , x n Tn ) is stochastically mapped into an output sequence (yn1 , . . . , y n Tn\n). We move a step further than the above approach by requiring that the distribution of the N -grams among all the mapped output sequences are close to the prior N -gram distribution pLM(i1, . . . , iN ). With this motivation, we propose to learn the classifier p\u03b8(yt|xt) by minimizing the negative cross entropy between the prior distribution and the expected N -gram frequency of the output sequences:\nmin \u03b8\n{ J (\u03b8) , \u2212 \u2211 i1,...,iN pLM(i1, . . . , iN ) ln p\u03b8(i1, . . . , iN ) } (1)\nwhere p\u03b8(i1, . . . , iN ) denotes the expected N -gram frequency of all the output sequences. In Appendix B of the supplementary material, we derive its expression as\np\u03b8(i1, . . . , iN ) , 1\nT M\u2211 n=1 Tn\u2211 t=1 N\u22121\u220f k=0 p\u03b8(y n t\u2212k = iN\u2212k|xnt\u2212k) (2)\nwhere T , T1 + \u00b7 \u00b7 \u00b7+ TM is the total number of samples in all sequences. Note that minimizing the negative cross entropy in (1) is also equivalent to minimizing the Kullback-Leibler (KL) divergence between the two distributions since they only differ by a constant term, \u2211 pLM ln pLM. Therefore, the cost function (1) seeks to estimate \u03b8 by matching the two output distributions, where the expected N -gram distribution in (2) is an empirical average over all the samples in the training set. For this reason, we name the cost (1) as Empirical Output Distribution Match (Empirical-ODM) cost.\nIn [30], the authors proposed to minimize an output distribution match (ODM) cost, defined as the KL-divergence between the prior output distribution and the marginalized output distribution, D(pLM(y)||p\u03b8(y)), where p\u03b8(y) , \u222b p\u03b8(y|x)p(x)dx. However, evaluating p\u03b8(y) requires integrating over the input space using a generative model p(x). Due to the lack of such a generative model, they were not able to optimize this proposed ODM cost. Instead, alternative approaches such as Dual autoencoders and GANs were proposed as heuristics. Their results were not successful without using\na few labeled data. Our proposed Empirical-ODM cost is different from the ODM cost in [30] in three key aspects. (i) We do not need any labeled data for training. (ii) We exploit sequence structure of output statistics, i.e., in our case y = (y1, . . . , yN ) (N -gram) whereas in [30] y = yt (unigram, i.e., no sequence structure). This is crucial in developing a working unsupervised learning algorithm. The change from unigram to N -gram allows us to explicitly exploit the sequence structures at the output, which makes the technique from non-working to working (see Table 2 in Section 4). It might also explain why the method in [30] failed as it does not exploit the sequence structure. (iii) We replace the marginalized distribution p\u03b8(y) by the expected N -gram frequency in (2). This is critical in that it allows us to directly minimize the divergence between two output distributions without the need for a generative model, which [30] could not do. In fact, we can further show that p\u03b8(i1, . . . , iN ) is an empirical approximation of p\u03b8(y) with y = (y1, . . . , yN ) (see Appendix B.2 of the supplementary material). In this way, our cost (1) can be understood as an N -gram and empirical version of the ODM cost except for an additive constant, i.e., y is replaced by y = (y1, . . . , yN ) and p\u03b8(y) is replaced by its empirical approximation."}, {"heading": "2.3 Coverage-seeking versus mode-seeking", "text": "We now discuss an important property of the proposed Empirical-ODM cost (1) by comparing it with the cost proposed in [7]. We show that the Empirical-ODM cost has a coverage-seeking property, which makes it more suitable for unsupervised learning than the mode-seeking cost in [7].\nIn [7], the authors proposed the expected negative log-likelihood as the unsupervised learning cost function that exploits the output sequential statistics. The intuition was to maximize the aggregated log-likelihood of all the output sequences assumed to be generated by the stochastic mapping p\u03b8(y n t |xnt ). We show in Appendix A of the supplementary material that their cost is equivalent to\n\u2212 \u2211\ni1,...,iN\u22121 \u2211 iN p\u03b8(i1, . . . , iN ) ln pLM(iN |iN\u22121, . . . , i1) (3)\nwhere pLM(iN |iN\u22121, . . . , i1) , p(ynt = iN |ynt\u22121 = iN\u22121, . . . , ynt\u2212N+1 = i1), and the summations are over all possible values of i1, . . . , iN \u2208 {1, . . . , C}. In contrast, we can rewrite our cost (1) as\n\u2212 \u2211\ni1,...,iN\u22121 pLM(i1, . . . , iN\u22121) \u00b7 \u2211 iN pLM(iN |iN\u22121, . . . , i1) ln p\u03b8(i1, . . . , iN ) (4)\nwhere we used the chain rule of conditional probabilities. Note that both costs (3) and (4) are in a cross entropy form. However, a key difference is that the positions of the distributions p\u03b8(\u00b7) and pLM(\u00b7) are swapped. We show that the cost in the form of (3) proposed in [7] is a mode-seeking divergence between two distributions, while by swapping p\u03b8(\u00b7) and pLM(\u00b7), our cost in (4) becomes a coverage-seeking divergence (see [25] for a detailed discussion on divergences with these two different behaviors). To understand this, we consider the following two situations:\n\u2022 If pLM(iN |iN\u22121, . . . , i1)\u2192 0 and p\u03b8(i1, . . . , iN ) > 0 for a certain (i1, . . . , iN ), the cross entropy in (3) goes to +\u221e and the cross entropy in (4) approaches zero.\n\u2022 If pLM(iN |iN\u22121, . . . , i1) > 0 and p\u03b8(i1, . . . , iN )\u2192 0 for a certain (i1, . . . , iN ), the cross entropy in (3) approaches zero and the cross entropy in (4) goes to +\u221e.\nTherefore, the cost function (3) will heavily penalize the classifier if it predicts an output that is believed to be less probable by the prior distribution pLM(\u00b7), and it will not penalize the classifier when it does not predict an output that pLM(\u00b7) believes to be probable. That is, the classifier is encouraged to predict a single output mode with high probability in pLM(\u00b7), a behavior called \u201cmodeseeking\u201d in [25]. This probably explains the phenomena observed in [7]: the training process easily converges to a trivial solution of predicting the same output that has the largest probability in pLM(\u00b7). In contrast, the cost (4) will heavily penalize the classifier if it does not predict the output that pLM(\u00b7) is positive, and will penalize less if it predicts outputs that pLM(\u00b7) is zero. That is, this cost will encourage p\u03b8(y|x) to cover as much of pLM(\u00b7) as possible, a behavior called \u201ccoverage-seeking\u201d in [25]. Therefore, training the classifier using (4) will make it less inclined to learn trivial solutions than that in [7] since it will be heavily penalized. We will verify this fact in our experiment section 4. In summary, our proposed cost (1) is more suitable for unsupervised learning than that in [7]."}, {"heading": "2.4 The difficulties of optimizing J (\u03b8)", "text": "However, there are two main challenges of optimizing the Empirical-ODM cost J (\u03b8) in (1). The first one is that the sample average (over the entire training data set) in the expression of p\u03b8(\u00b7) (see (2)) is inside the logarithmic loss, which is different from traditional machine learning problems where the average is outside loss functions (e.g., \u2211 t ft(\u03b8)). This functional form prevents us from applying stochastic gradient descent (SGD) to minimize (1) as the stochastic gradients would be intrinsically biased (see Appendix C for a detailed discussion and see section 4 for the experiment results). The second challenge is that the cost function J (\u03b8) is highly non-convex even with linear classifiers. To see this, we visualize the profile of the cost function J (\u03b8) (restricted to a two-dimensional sub-space) around the supervised solution in Figure 1.3 We observe that there are local optimal solutions and there are high barriers between the local and global optimal solutions. Therefore, besides the difficulty of having the sample average inside the logarithmic loss, minimizing this cost function directly will be difficult since crossing the high barriers to reach the global optimal solution would be hard if not properly initialized."}, {"heading": "3 The Stochastic Primal-Dual Gradient (SPDG) Algorithm", "text": "To address the first difficulty in Section 2.4, we transform the original cost (1) into an equivalent min-max problem in order to bring the sample average out of the logarithmic loss. Then, we could obtain unbiased stochastic gradients to solve the problem. To this end, we first introduce the concept of convex conjugate functions. For a given convex function f(u), its convex conjugate function f?(\u03bd) is defined as f?(\u03bd) , supu(\u03bd\nTu\u2212 f(u)) [6, pp.90-95], where u and \u03bd are called primal and dual variables, respectively. For a scalar function f(u) = \u2212 lnu, its conjugate function can be calculated as f?(\u03bd) = \u22121 \u2212 ln(\u2212\u03bd) with \u03bd < 0. Furthermore, it holds that f(u) = sup\u03bd(uT \u03bd \u2212 f?(\u03bd)), by which we have\u2212 lnu = max\u03bd(u\u03bd+ 1 + ln(\u2212\u03bd)).4 Substituting it into (1), the original minimization problem becomes the following equivalent min-max problem:\nmin \u03b8 max {\u03bdi1,...,iN<0}\n{ L(\u03b8, V ) , 1\nT M\u2211 n=1 Tn\u2211 t=1 Lnt (\u03b8, V ) + \u2211\ni1,...,iN\npLM(i1, . . . , iN ) ln(\u2212\u03bdi1,...,iN ) } (5)\nwhere V , {\u03bdi1,...,iN } is a collection of all the dual variables \u03bdi1,...,iN , and Lnt (\u03b8, V ) is the t-th component function in the n-th sequence, defined as\nLnt (\u03b8, V ) , \u2211\ni1,...,iN\npLM(i1, . . . , iN )\u03bdi1,...,iN N\u22121\u220f k=0 p\u03b8(y n t\u2212k= iN\u2212k|xnt\u2212k)\n3The approach to visualizing the profile is explained with more detail in Appendix E. More slices and a video of the profiles from many angles can be found in the supplementary material.\n4The supremum is attainable and is thus replaced by maximum.\nAlgorithm 1 Stochastic Primal-Dual Gradient Method 1: Input data: DX = {(xn1 , . . . , xnTn) : n = 1, . . . ,M} and pLM(i1, . . . , iN ). 2: Initialize \u03b8 and V where the elements of V are negative 3: repeat 4: Randomly sample a mini-batch of B subsequences of length N from all the sequences in the\ntraining set DX , i.e., B = {(xnmtm\u2212N+1, . . . , x nm tm )} B m=1.\n5: Compute the stochastic gradients for each subsequence in the mini-batch and average them\n\u2206\u03b8 = 1\nB B\u2211 m=1 \u2202Lnmtm \u2202\u03b8 , \u2206V = 1 B B\u2211 m=1 \u2202Lnmtm \u2202V + \u2202 \u2202V \u2211 i1...iN pLM(i1,. . ., iN ) ln(\u2212\u03bdi1,...,iN)\n6: Update \u03b8 and V according to \u03b8 \u2190 \u03b8 \u2212 \u00b5\u03b8\u2206\u03b8 and V \u2190 V + \u00b5v\u2206V . 7: until convergence or a certain stopping condition is met\nIn the equivalent min-max problem (5), we find the optimal solution (\u03b8?, V ?) by minimizing L with respect to the primal variable \u03b8 and maximizing L with respect to the dual variable V . The obtained optimal solution to (5), (\u03b8?, V ?), is called the saddle point of L [6]. Once it is obtained, we only keep \u03b8?, which is also the optimal solution to (1) and thus the model parameter.\nWe further note that the equivalent min-max problem (5) is now in a form that sums over T = T1 + \u00b7 \u00b7 \u00b7+ TM component functions Lnt (\u03b8, V ). Therefore, the empirical average has been brought out of the logarithmic loss and we are ready to apply stochastic gradient methods. Specifically, we minimize L with respect to the primal variable \u03b8 by stochastic gradient descent and maximize L with respect to the dual variable V by stochastic gradient ascent. Therefore, we name the algorithm stochastic primal-dual gradient (SPDG) method (see its details in Algorithm 1). We implement the SPDG algorithm in TensorFlow, which automatically computes the stochastic gradients.5 Finally, the constraint on dual variables \u03bdi1,...,iN are automatically enforced by the inherent log-barrier, ln(\u2212\u03bdi1,...,iN ), in (5) [6]. Therefore, we do not need a separate method to enforce the constraint. We now show that the above min-max (primal-dual) reformulation also alleviates the second difficulty discussed in Section 2.4. Similar to the case of J (\u03b8), we examine the profile of L(\u03b8, V ) in (5) (restricted to a two-dimensional sub-space) around the optimal (supervised) solution in Figure 2a (see Appendix E for the visualization details). Comparing Figure 2a to Figure 1, we observe that the profile of L(\u03b8, V ) is smoother than that of J (\u03b8) and the barrier is significantly lower. To further compare J (\u03b8) and L(\u03b8, V ), we plot in Figure 2b the values of J (\u03b8) and L(\u03b8, V ) along the same line of \u03b8? + \u03bbp(\u03b81 \u2212 \u03b8?) for different \u03bbp. It shows that the barrier of L(\u03b8, V ) along the primal direction\n5The code will be released soon.\nis lower than that in J (\u03b8). These observations imply that the reformulated min-max problem (5) is better conditioned than the original problem (1), which further justifies the use of SPDG method."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Experimental setup", "text": "We evaluate our unsupervised learning scheme described in earlier secitons using two classification tasks, unsupervised character-level OCR and unsupervised English Spelling Correction (Spell-Corr). In both tasks, there is no label provided during training. Hence, they are both unsupervised.\nFor the OCR task, we obtain our dataset from a public database UWIII English Document Image Database [27], which contains images for each line of text with its corresponding groudtruth. We first use Tesseract [19] to segment the image for each line of text into characters tiles and assign each tile with one character. We verify the segmentation result by training a simple neural network classifier on the segmented results and achieve 0.9% error rate on the test set. Then, we select sentence segments that are longer than 100 and contain only lowercase English characters and common punctuations (space, comma, and period). As a result, we have a vocabulary of size 29 and we obtain 1,175 sentence segments including 153,221 characters for our OCR task. To represent images, we extract VGG19 features with dim = 4096, and project them into 200-dimension vectors using Principal Component Analysis. We train the language models (LM) pLM(\u00b7) to provide the required output sequence statistics from both in-domain and out-of-domain data sources. The out-of-domain data sources are completely different databases, including three different language partitions (CNA, NYT, XIN) in the English Gigaword database [26].\nIn Spell-Corr task, we learn to correct the spelling from a mis-spelled text. From the AFP partition of the Gigaword database, we select 500 sentence segments into our Spell-Corr dataset. We select sentences that are longer than 100 and contain only English characters and common punctuations, resulting in a total of 83,567 characters. The mis-spelled texts are generated by substitution simulations and are treated as our inputs. The objective of this task is to recover the original text.6"}, {"heading": "4.2 Results: Comparing optimization algorithms", "text": "In the first set of experiments, we aim to evaluate the effectiveness of the SPDG method as described in Section 3, which is designed for optimizing the Empirical-ODM cost in Section 2. The analysis provided in Sections 2 and 3 sheds insight to why SPDG is superior to the method in [7] and to the standard stochastic gradient descent (SGD) method. The coverage-seeking behavior of the proposed Empirical-ODM cost helps avoid trivial solutions, and the simultaneous optimization of primal-dual variables reduces the barriers in the highly non-convex profile of the cost function. Furthermore, we do not include the methods from [30] because their approaches could not achieve satisfactory results without a few labeled data, while we only consider fully unsupervised learning setting. In addition, the methods in [30] are not optimizing the ODM cost and do not exploit the output sequential statistics.\nTable 1 provides strong experimental evidence demonstrating the substantially greater effectiveness of the primal-dual method over the SGD and the method in [7] on both tasks. All these results are obtained by training the models until converge. Let us examine the results on the OCR in detail. First, the SPGD on the unsupervised cost function achieves 9.21% error rate, much lower than the error rates of any of mini-batch SGD runs, where the size of the mini-batches ranges from 10 to 10,000. Note that, larger mini-batch sizes produce lower errors here because it becomes closer to full-batch gradient and thus lower bias in SGD. On the other hand, when the mini-batch size is as small as 10, the high error rate of 83.09% is close to a guess by majority rule \u2014 predicting the character (space) that has a largest proportion in the train set, i.e., 25, 499/153, 221 = 83.37%. Furthermore, the method from [7] does not perform well no matter how we tune the hyperparameters for the generative regularization. Finally and perhaps most interestingly, with no labels provided in the training, the classification errors produced by our method are only about twice compared with supervised learning (4.63% shown in Table 1). This clearly demonstrates that the unsupervised learning scheme proposed in this paper is an effective one. For the Spelling Correction data set (see the third column in Table 1), we observe rather consistent results with the OCR data set.\n6We gratefully acknowledge the discussions with Prof. Hermann Ney for private discussions on this task and his work on using likelihood as the objective function for unsupervised training."}, {"heading": "4.3 Results: Comparing orders of language modeling", "text": "In the second set of experiments, we examine to what extent the use of sequential statistics (e.g. 2- and 3-gram LMs) can do better than the uni-gram LM (no sequential information) in unsupervised learning. The unsupervised prediction results are shown in Table 2, using different data sources to estimate N-gram LM parameters. Consistent across all four ways of estimating reliable N-gram LMs, we observe significantly lower error rates when the unsupervised learning exploits 2-gram and 3-gram LM as sequential statistics compared with exploiting the prior with no sequential statistics (i.e. 1-gram). In three of four cases, exploiting a 3-gram LM gives better results than a 2-gram LM. Furthermore, the comparable error rate associated with 3-gram using out-of-domain output character data (10.17% in Table 2) to that using in-domain output character data (9.59% in Table 1) indicates that the effectiveness of the unsupervised learning paradigm presented in this paper is robust to the quality of the LM acting as the sequential prior."}, {"heading": "5 Conclusions and future work", "text": "In this paper, we study the problem of learning a sequence classifier without the need for labeled training data. The practical benefit of such unsupervised learning is tremendous. For example, in large scale speech recognition systems, the currently dominant supervised learning methods typically require a few thousand hours of training data, where each utterance in the acoustic form needs to be labeled by humans. Although there are millions of hours of natural speech data available for training, labeling all of them for supervised learning is less feasible. To make effective use of such huge amounts of acoustic data, the practical unsupervised learning approach discussed in this paper would be called for. Other potential applications such as machine translation, image and video captioning could also benefit from our paradigm. This is mainly because of their common natural language output structure, from which we could exploit the sequential structures for learning the classifier without labels. Furthermore, our proposed Empirical-ODM cost function significantly improves over the one in [7] by emphasizing the coverage-seeking behavior. Although the new cost function has a functional form that is more difficult to optimize, a novel SPDG algorithm is developed to effectively address the problem. An analysis of profiles of the cost functions sheds insight to why SPDG works well and why previous methods failed. Finally, we demonstrate in two datasets that our unsupervised learning method is highly effective, producing only about twice errors as fully supervised learning, which no previous unsupervised learning could produce without additional steps of supervised learning. While the current work is restricted to linear classifiers, we intend to generalize the approach to nonlinear models (e.g., deep neural nets [16]) in our future work. We also plan to extend our current method from exploiting N-gram LM to exploiting the currently state-of-the-art neural-LM."}, {"heading": "A Derivation of the equivalent form of the cost in [7]", "text": "The cost function in [7] can be expressed as:\nE [ \u2212 M\u2211 n=1 ln pLM(y n 1 , . . . , y n Tn)|x n 1 , . . . , x n Tn ] (6)\nWe now show how to derive (3) from the above expression. In N -gram case, the language model can be written as\npLM(y n 1 , . . . , y n Tn) = Tn\u220f t=1 pLM(y n t |ynt\u22121, . . . , ynt\u2212N+1)\nSubstituting the above expression into the cost (6), we obtain\nE [ \u2212 M\u2211 n=1 ln pLM(y n 1 , . . . , y n Tn)|x n 1 , . . . , x n Tn ] = \u2212\nM\u2211 n=1 \u2211 (yn1 ,...,y n Tn ) Tn\u220f t=1 p\u03b8(y n t |xnt ) ln pLM(yn1 , . . . , ynTn)\n= \u2212 M\u2211 n=1 \u2211 (yn1 ,...,y n Tn ) p\u03b8(y n 1 |xn1 ) \u00b7 \u00b7 \u00b7 p\u03b8(ynTn |x n Tn)\u00d7 Tn\u2211 t=1 ln pLM(y n t |ynt\u22121, . . . , ynt\u2212N+1)\n= \u2212 M\u2211 n=1 Tn\u2211 t=1 \u2211 (yn1 ,...,y n Tn ) p\u03b8(y n 1 |xn1 ) \u00b7 \u00b7 \u00b7 p\u03b8(ynTn |x n Tn)\u00d7 ln pLM(y n t |ynt\u22121, . . . , ynt\u2212N+1)\n= \u2212 M\u2211 n=1 Tn\u2211 t=1 \u2211 (ynt ,...,y n t\u2212N+1) p\u03b8(y n t |xnt ) \u00b7 \u00b7 \u00b7 p\u03b8(ynt\u2212N+1|xnt\u2212N+1)\u00d7 ln pLM(ynt |ynt\u22121, . . . , ynt\u2212N+1)\n\u00d7 \u2211\nyn1 ,...,y n t\u2212N\np\u03b8(y n 1 |xn1 ) \u00b7 \u00b7 \u00b7 p\u03b8(ynt\u2212N |xnt\u2212N )\u00d7 \u2211 ynt+1,...,y n Tn p\u03b8(y n t+1|xnt+1) \u00b7 \u00b7 \u00b7 p\u03b8(ynTn |x n Tn)\n= \u2212 M\u2211 n=1 Tn\u2211 t=1 \u2211 (ynt ,...,y n t\u2212N+1) p\u03b8(y n t |xnt ) \u00b7 \u00b7 \u00b7 p\u03b8(ynt\u2212N+1|xnt\u2212N+1)\u00d7 ln pLM(ynt |ynt\u22121, . . . , ynt\u2212N+1) = \u2212 M\u2211 n=1 Tn\u2211 t=1 \u2211 i1,...,iN p\u03b8(y n t = iN |xt) \u00b7 \u00b7 \u00b7 p\u03b8(ynt\u2212N+1 = i1|xnt\u2212N+1)\u00d7 ln pLM(ynt = iN |ynt\u22121 = iN\u22121, . . . , ynt\u2212N+1 = i1)\n= \u2212 M\u2211 n=1 Tn\u2211 t=1 \u2211 i1,...,iN p\u03b8(y n t = iN |xnt ) \u00b7 \u00b7 \u00b7 p\u03b8(ynt\u2212N+1 = i1|xnt\u2212N+1)\u00d7 ln pLM(iN |iN\u22121, . . . , i1)\n= \u2212 \u2211\ni1,...,iN\nln pLM(iN |iN\u22121, . . . , i1)\u00d7 M\u2211 n=1 Tn\u2211 t=1 p\u03b8(y n t = iN |xt) \u00b7 \u00b7 \u00b7 p\u03b8(ynt\u2212N+1 = i1|xnt\u2212N+1)\n= \u2212T \u2211\ni1,...,iN\nln pLM(iN |iN\u22121, . . . , i1)\u00d7 1\nT M\u2211 n=1 Tn\u2211 t=1 p\u03b8(y n t = iN |xnt ) \u00b7 \u00b7 \u00b7 p\u03b8(ynt\u2212N+1 = i1|xnt\u2212N+1)\nB Properties of p\u03b8(i1, . . . , iN)\nB.1 p\u03b8(i1, . . . , iN ) is the expected N -gram frequency of all the output sequences\nIn this section, we formally derive the following relation, which interprets p\u03b8(i1, . . . , iN ) as the expected frequency of (i1, . . . , iN ) in the output sequence:\nE\u220fM n=1 \u220fTn t=1 p\u03b8(y n t |xnt )\n[ n(i1, . . . , iN )\nT\n] = p\u03b8(iN , . . . , i1)\nwhere T , T1 + \u00b7 \u00b7 \u00b7TM . Let (xn1 , . . . , xnTn) be a given n-th input training sequence, and let (yn1 , . . . , y n Tn ) be a sequence generated according to the posterior \u220fTn t=1 p\u03b8(y n t |xnt ) (which is the classifier). Furthermore, let Int (i1, . . . , iN ) denote the indicator function of the event {ynt\u2212N+1 = i1, . . . , y n t = iN}, and let n(i1, . . . , iN ) denote the number of the N -gram (i1, . . . , iN ) appearing in all the output sequences {(yn1 , . . . , ynTn) : n = 1, . . . ,M}. Then, we have the following relation:\nn(i1, . . . , iN ) = M\u2211 n=1 Tn\u2211 t=1 Int (i1, . . . , iN )\nObviously, n(i1, . . . , iN ) is a function of {(yn1 , . . . , ynTn) : n = 1, . . . ,M} and is thus a random variable. Taking the conditional expectation of the above expression with respect to\u220fM n=1 \u220fTn t=1 p\u03b8(y n t |xnt ), we obtain\nE\u220fM n=1 \u220fTn t=1 p\u03b8(y n t |xnt ) [n(i1, . . . , iN )]\n= M\u2211 n=1 Tn\u2211 t=1 E\u220fM n=1 \u220fTn t=1 p\u03b8(y n t |xnt ) [Int (i1, . . . , iN )]\n= M\u2211 n=1 Tn\u2211 t=1 E\u220fTn t=1 p\u03b8(y n t |xnt ) [Int (i1, . . . , iN )]\n(a) = M\u2211 n=1 Tn\u2211 t=1 N\u22121\u220f k=0 p\u03b8(y n t\u2212k = iN\u2212k|xnt\u2212k)\nwhere step (a) uses the fact that the expectation of an indicator function of an event equals the probability of the event. Divide both sides by T , the right hand side of the above expression becomes p\u03b8(i1, . . . , iN ), and we conclude our proof.\nB.2 p\u03b8(iN , . . . , i1) is an empirical approximation of the marginal output N -gram probability\nFirst, define the marginal N -gram probability p\u03b8(i1, . . . , iN ) as\np\u03b8(i1, . . . , iN ) , p\u03b8(y1 = i1, . . . , yN = iN ) (7) For simplicity, we consider the case where the input random variables are discrete, taking finite value from a set X , then p\u03b8(i1, . . . , iN ) can be written as\np\u03b8(i1, . . . , iN ) = \u2211\n(x1,...,xN )\u2208XN\nN\u220f k=1 p\u03b8(yk = ik|xk)p(x1, . . . , xN ) (8)\nTo show that p\u03b8(iN , . . . , i1) is an empirical approximation of p\u03b8(i1, . . . , iN ), it suffices to show that\np\u03b8(i1, . . . , iN ) = \u2211\n(x1,...,xN )\u2208XN\nN\u220f k=1 p\u03b8(yk = ik|xk)p\u0302(x1, . . . , xN ) (9)\nwhere p\u0302(x1, . . . , xN ) is the empirical frequency of the N -tuple (x1, . . . , xN ) in the dataset {(xn1 , . . . , xnTn) : n = 1, . . . ,M}. The result follows in a straightforward manner from the definition of p\u03b8(i1, . . . , iN ):\np\u03b8(i1, . . . , iN ) = 1\nT M\u2211 n=1 Tn\u2211 t=1 N\u22121\u220f k=0 p\u03b8(y n t\u2212k = iN\u2212k|xnt\u2212k)\n= 1\nT \u2211 (x1,...,xN )\u2208XN N\u22121\u220f k=0 p\u03b8(y n t\u2212k = iN\u2212k|xnt\u2212k = xN\u2212k)\u00d7 n(x1, . . . , xN )\n= \u2211\n(x1,...,xN )\u2208XN\nN\u22121\u220f k=0 p\u03b8(y n t\u2212k = iN\u2212k|xnt\u2212k = xN\u2212k)\u00d7 n(x1, . . . , xN ) T (10)\nwhere n(x1, . . . , xN ) denotes the number of N -tuple (x1, . . . , xN ) in the dataset {(xn1 , . . . , xnTn) : n = 1, . . . ,M}. The second equality is simply re-organizing the summation in the first expression according to the value of (x1, . . . , xN ), i.e., accumulating all the terms inside the double-summation with the same value of (x1, . . . , xN ) together. Further note that p\u03b8(ynt\u2212k = iN\u2212k|xnt\u2212k = xN\u2212k) is independent of t and n for any given values of iN\u2212k and xN\u2212k, so that\nN\u22121\u220f k=0 p\u03b8(y n t\u2212k = iN\u2212k|xnt\u2212k = xN\u2212k) = N\u220f k=1 p\u03b8(yk = ik|xk) (11)\nThen, we can conclude the proof by recognizing that p\u0302(x1, . . . , xN ) = n(x1, . . . , xN )/T ."}, {"heading": "C Optimizing Empirical-ODM by SGD is intrinsically biased", "text": "In this section, we show that the stochastic gradient of Empirical-ODM is intrinsically biased. To see this, we can express the (full batch) gradient of J (\u03b8) as\n\u2207\u03b8J (\u03b8) = \u2212 \u2211\ni1,...,iN\npLM(i1, . . . , iN )\n1 T \u2211M n=1 \u2211Tn t \u2207\u03b8 (\u220fN\u22121 k=0 p\u03b8(y n t\u2212k = iN\u2212k|xnt\u2212k) ) 1 T \u2211M n=1 \u2211Tn t \u220fN\u22121 k=0 p\u03b8(y n t\u2212k = iN\u2212k|xnt\u2212k) (12)\nNote that the gradient expression has sample averages in both the numerator and denominator. Therefore, full batch gradient method is less scalable as it needs to go over the entire training set to compute \u2207\u03b8J (\u03b8) at each update. To apply SGD, we may obtain an unbiased estimate of it by sampling the numerator with a single component while keeping the denominator the same:\n\u2212 \u2211\ni1,...,iN\npLM(i1, . . . , iN ) \u2207\u03b8 (\u220fN\u22121 k=0 p\u03b8(y n t\u2212k= iN\u2212k|xnt\u2212k) ) 1 T \u2211M n=1 \u2211Tn t \u220fN\u22121 k=0 p\u03b8(y n t\u2212k= iN\u2212k|xnt\u2212k)\nHowever, this implementation is still not scalable as it needs to average over the entire training set at each update to compute the denominator. On the other hand, if we sample both the numerator and the denominator, i.e.,\n\u2212 \u2211\ni1,...,iN\npLM(i1, . . . , iN ) \u2207\u03b8 (\u220fN\u22121 k=0 p\u03b8(y n t\u2212k= iN\u2212k|xnt\u2212k) ) \u220fN\u22121 k=0 p\u03b8(y n t\u2212k= iN\u2212k|xnt\u2212k)\nthen it will be a biased estimate of the gradient (12). Our experiments in Section 4 showed that this biased SGD does not perform well on the unsupervised learning problem."}, {"heading": "D Experiment Details", "text": "In the experiment, we implement the model with Python 2.7 and Tensorflow 0.12.\nIn training of models both on OCR and Spell-Corr task, we initialize the linear model\u2019s parameters (primal variable) with winit = 1/dim(x) and \u03b3 = 10, where dim(x) is the dimension of input. And we initialize the dual parameters Vinit with uniformly distributed random variables v \u223c U(\u22121, 0). We set the learning rate for primal parameter \u00b5\u03b8 = 10\u22126 and learning rate for dual parameter \u00b5v = 10 \u22124. We use Adam optimization to train our model.\nThe test set of OCR is generated also from UWIII database,but avoiding overlap with training set. The size of test set of OCR is 15000. Furthermore, the size of the test set of Spell-Corr is also 15000 without overlapping with the training set."}, {"heading": "E The details of visualizing the high-dimensional cost functions", "text": "Since J (\u03b8) is a high-dimensional function, it is hard to visualize its full profile. Instead, we use the following procedure to partially visualize J (\u03b8). First, since the supervised learning of linear classifiers is a convex optimization problem, from which we could obtain its global optimal solution \u03b8?.7 Then, we randomly generate two parameter vectors \u03b81 and \u03b82 and plot the two-dimensional function J ( \u03b8? + \u03bb1(\u03b81 \u2212 \u03b8?) + \u03bb2(\u03b82 \u2212 \u03b8?) ) with respect to \u03bb1, \u03bb2 \u2208 R, which is a slice of the cost function on a two-dimensional plane.\nFor the profile of L(\u03b8, V ) in (5), similar to the case of J (\u03b8), in order to visualize L(\u03b8, V ), we first solve the supervised learning problem to get \u03b8?. Then we substitute \u03b8? into (5) and maximize L(\u03b8?, V ) over V to obtain V ? = {\u03bd?i1,...,iN }, where \u03bd ? i1,...,iN =\n\u22121 /\n1 T \u2211M n=1 \u2211Tn t=1 \u220fN\u22121 k=0 p\u03b8?(y n t\u2212k = iN\u2212k|xnt\u2212k). We also randomly generate a (\u03b81, V1) (with the elements of V1 being negative) and plot in Figure 2a the values of L(\u03b8? + \u03bbp(\u03b81 \u2212 \u03b8?), V ? + \u03bbd(V1 \u2212 V ?)) for different \u03bbp, \u03bbd \u2208 R. Clearly, the optimal solution (red dot) is at the saddle point of the profile."}, {"heading": "F Additional visualization of J (\u03b8)", "text": "In Figures 3, 4 and 5, we show three visualization examples of J (\u03b8) for the OCR dataset on three different affine spaces, part of the first example was included in Figure 1. The six sub-figures in each example show the same profile from six different angles, spinning clock-wise from (a)-(f). The red dots indicate the global minimum.\nIn Figure 6, we show the same type of profiles as above except using synthetic data for of a binary classification problem. First, we sequentially generated a sequence of states from 0, 1 by an hidden Markov model. Then we sample the corresponding data points from two separate 2-dimensional Gaussian models. accordingly."}, {"heading": "G Additional visualization of L(\u03b8, V )", "text": "Figure 7 shows the profile of L(\u03b8, V ) for the OCR data set on a two-dimensional affine space viewed from nine different angles. The red dots show the saddle points of the profile, one for each angle.\n7Note that, we solve the supervised learning only for the purpose of understanding our proposed unsupervised learning cost J (\u03b8). In our implementation of the unsupervised learning algorithm, we do not use any of the training label information nor supervised learning algorithms."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Unsupervised transcription of historical documents", "author": ["Taylor Berg-Kirkpatrick", "Greg Durrett", "Dan Klein"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Unsupervised learning of predictors from unpaired input-output samples", "author": ["Jianshu Chen", "Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "A path to unsupervised learning through adversarial networks", "author": ["Soumith Chintala", "Yann LeCun"], "venue": "In https://code.facebook.com/posts/1587249151575490/a-path-to-unsupervisedlearning-through-adversarial-networks/,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep learning for speech and language processing", "author": ["Li Deng"], "venue": "In Tutorial at Interspeech Conf,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow"], "venue": "In Tutorial at NIPS, http://www.cs.toronto.edu/", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Deep Learning, by MIT Press. 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1211.3711,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-Rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Tesseract: An open-source optical character recognition", "author": ["Anthony Kay"], "venue": "engine. Linux Journal,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Unsupervised analysis for decipherment problems", "author": ["Kevin Knight", "Anish Nair", "Nishit Rathod", "Kenji Yamada"], "venue": "In Proceedings of the COLING/ACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc Le", "Marc\u2019Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg Corrado", "Jeff Dean", "Andrew Ng"], "venue": "In International Conference in Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Cryptology: From caesar ciphers to public-key cryptosystems", "author": ["Dennis Luciano", "Gordon Prichett"], "venue": "The College Mathematics Journal,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1987}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Divergence measures and message passing", "author": ["Tom Minka"], "venue": "Technical report, Technical report, Microsoft Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "English gigaword fourth edition ldc2009t13", "author": ["Robert et al Parker"], "venue": "Philadelphia: Linguistic Data Consortium,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, pages 194\u2013281", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1986}, {"title": "Label-free supervision of neural networks with physics and domain knowledge", "author": ["Russell Stewart", "Stefano Ermon"], "venue": "In Proceedings of AAAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "Towards principled unsupervised learning", "author": ["Ilya Sutskever", "Rafal Jozefowicz", "Karol Gregor", "Danilo Rezende", "Tim Lillicrap", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1511.06440,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 24, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 15, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 14, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 3, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 0, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 17, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 11, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 9, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 19, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 6, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 210, "endOffset": 228}, {"referenceID": 15, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 210, "endOffset": 228}, {"referenceID": 1, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 210, "endOffset": 228}, {"referenceID": 21, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 210, "endOffset": 228}, {"referenceID": 7, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 210, "endOffset": 228}, {"referenceID": 8, "context": "Recently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7].", "startOffset": 169, "endOffset": 180}, {"referenceID": 26, "context": "Recently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7].", "startOffset": 169, "endOffset": 180}, {"referenceID": 4, "context": "Recently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7].", "startOffset": 169, "endOffset": 180}, {"referenceID": 4, "context": "Similar to this work, the authors in [7] proposed an unsupervised cost that also exploits the sequence prior of the output samples to train classifiers.", "startOffset": 37, "endOffset": 40}, {"referenceID": 18, "context": "The power of such a strong prior in the form of language models in unsupervised learning was also demonstrated in earlier studies in [21, 3].", "startOffset": 133, "endOffset": 140}, {"referenceID": 2, "context": "The power of such a strong prior in the form of language models in unsupervised learning was also demonstrated in earlier studies in [21, 3].", "startOffset": 133, "endOffset": 140}, {"referenceID": 4, "context": "For example, it was shown in [7] that optimizing \u2217All the authors contributed equally to the paper.", "startOffset": 29, "endOffset": 32}, {"referenceID": 8, "context": "The solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects.", "startOffset": 80, "endOffset": 91}, {"referenceID": 26, "context": "The solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects.", "startOffset": 80, "endOffset": 91}, {"referenceID": 4, "context": "The solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects.", "startOffset": 80, "endOffset": 91}, {"referenceID": 4, "context": "First, we propose a novel cost function for unsupervised learning, and find that it has a desired coverage-seeking property that makes the learning algorithm less inclined to be stuck in trivial solutions than the cost function in [7].", "startOffset": 231, "endOffset": 234}, {"referenceID": 26, "context": "Second, we develop a special empirical formulation of this cost function that avoids the need for a strong generative model as in [30].", "startOffset": 130, "endOffset": 134}, {"referenceID": 26, "context": "2 Empirical-ODM: An unsupervised learning cost for sequence classifiers In this section, we extend the earlier work of [30] and propose an unsupervised learning cost named Empirical Output Distribution Match (Empirical-ODM) for training classifiers without labeled data.", "startOffset": 119, "endOffset": 123}, {"referenceID": 25, "context": "A recent work that shares the same motivations as our work is [29], which also recognizes the high cost of obtaining labeled data and seeks label-free prediction.", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "Finally, our problem is fundamentally different from the sequence transduction method in [15], although it also exploits language models for sequence prediction.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "Specifically, the method in [15] is a fully supervised learning in that it requires supervision at the sequence level; that is, for each input sequence, a corresponding output sequence (of possibly different length) is provided as a label.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "The use of language model in [15] only serves the purpose of regularization in the sequence-level supervised learning.", "startOffset": 29, "endOffset": 33}, {"referenceID": 20, "context": "It is mainly inspired by the approach to breaking the Caesar cipher, one of the simplest forms of encryption [23].", "startOffset": 109, "endOffset": 113}, {"referenceID": 26, "context": "In [30], the authors proposed to minimize an output distribution match (ODM) cost, defined as the KL-divergence between the prior output distribution and the marginalized output distribution, D(pLM(y)||p\u03b8(y)), where p\u03b8(y) , \u222b p\u03b8(y|x)p(x)dx.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "Our proposed Empirical-ODM cost is different from the ODM cost in [30] in three key aspects.", "startOffset": 66, "endOffset": 70}, {"referenceID": 26, "context": ", yN ) (N -gram) whereas in [30] y = yt (unigram, i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "It might also explain why the method in [30] failed as it does not exploit the sequence structure.", "startOffset": 40, "endOffset": 44}, {"referenceID": 26, "context": "This is critical in that it allows us to directly minimize the divergence between two output distributions without the need for a generative model, which [30] could not do.", "startOffset": 154, "endOffset": 158}, {"referenceID": 4, "context": "3 Coverage-seeking versus mode-seeking We now discuss an important property of the proposed Empirical-ODM cost (1) by comparing it with the cost proposed in [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "We show that the Empirical-ODM cost has a coverage-seeking property, which makes it more suitable for unsupervised learning than the mode-seeking cost in [7].", "startOffset": 154, "endOffset": 157}, {"referenceID": 4, "context": "In [7], the authors proposed the expected negative log-likelihood as the unsupervised learning cost function that exploits the output sequential statistics.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "We show that the cost in the form of (3) proposed in [7] is a mode-seeking divergence between two distributions, while by swapping p\u03b8(\u00b7) and pLM(\u00b7), our cost in (4) becomes a coverage-seeking divergence (see [25] for a detailed discussion on divergences with these two different behaviors).", "startOffset": 53, "endOffset": 56}, {"referenceID": 22, "context": "We show that the cost in the form of (3) proposed in [7] is a mode-seeking divergence between two distributions, while by swapping p\u03b8(\u00b7) and pLM(\u00b7), our cost in (4) becomes a coverage-seeking divergence (see [25] for a detailed discussion on divergences with these two different behaviors).", "startOffset": 208, "endOffset": 212}, {"referenceID": 22, "context": "That is, the classifier is encouraged to predict a single output mode with high probability in pLM(\u00b7), a behavior called \u201cmodeseeking\u201d in [25].", "startOffset": 138, "endOffset": 142}, {"referenceID": 4, "context": "This probably explains the phenomena observed in [7]: the training process easily converges to a trivial solution of predicting the same output that has the largest probability in pLM(\u00b7).", "startOffset": 49, "endOffset": 52}, {"referenceID": 22, "context": "That is, this cost will encourage p\u03b8(y|x) to cover as much of pLM(\u00b7) as possible, a behavior called \u201ccoverage-seeking\u201d in [25].", "startOffset": 122, "endOffset": 126}, {"referenceID": 4, "context": "Therefore, training the classifier using (4) will make it less inclined to learn trivial solutions than that in [7] since it will be heavily penalized.", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "In summary, our proposed cost (1) is more suitable for unsupervised learning than that in [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 16, "context": "We first use Tesseract [19] to segment the image for each line of text into characters tiles and assign each tile with one character.", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "The out-of-domain data sources are completely different databases, including three different language partitions (CNA, NYT, XIN) in the English Gigaword database [26].", "startOffset": 162, "endOffset": 166}, {"referenceID": 4, "context": "The analysis provided in Sections 2 and 3 sheds insight to why SPDG is superior to the method in [7] and to the standard stochastic gradient descent (SGD) method.", "startOffset": 97, "endOffset": 100}, {"referenceID": 26, "context": "Furthermore, we do not include the methods from [30] because their approaches could not achieve satisfactory results without a few labeled data, while we only consider fully unsupervised learning setting.", "startOffset": 48, "endOffset": 52}, {"referenceID": 26, "context": "In addition, the methods in [30] are not optimizing the ODM cost and do not exploit the output sequential statistics.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "Table 1 provides strong experimental evidence demonstrating the substantially greater effectiveness of the primal-dual method over the SGD and the method in [7] on both tasks.", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "Furthermore, the method from [7] does not perform well no matter how we tune the hyperparameters for the generative regularization.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Data sets SPDG (Ours) Method from [7] SGD \u300810\u3009 SGD \u3008100\u3009 SGD \u30081k\u3009 SGD \u300810k\u3009 Supervised Learning Majority Guess OCR 9.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "Furthermore, our proposed Empirical-ODM cost function significantly improves over the one in [7] by emphasizing the coverage-seeking behavior.", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": ", deep neural nets [16]) in our future work.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "References [1] Yoshua Bengio.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] David M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] Jianshu Chen, Po-Sen Huang, Xiaodong He, Jianfeng Gao, and Li Deng.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[8] Soumith Chintala and Yann LeCun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] George E Dahl, Dong Yu, Li Deng, and Alex Acero.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[10] Andrew M Dai and Quoc V Le.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] Li Deng.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] Ian Goodfellow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] Alex Graves.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-Rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Geoffrey E Hinton and Ruslan R Salakhutdinov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Anthony Kay.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Diederik P Kingma and Max Welling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] Quoc Le, Marc\u2019Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg Corrado, Jeff Dean, and Andrew Ng.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] Dennis Luciano and Gordon Prichett.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] Tom Minka.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] Robert et al Parker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[28] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[29] Russell Stewart and Stefano Ermon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[30] Ilya Sutskever, Rafal Jozefowicz, Karol Gregor, Danilo Rezende, Tim Lillicrap, and Oriol Vinyals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Supplementary Material for \u201cUnsupervised Sequence Classification using Sequential Output Statistics\u201d A Derivation of the equivalent form of the cost in [7]", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "The cost function in [7] can be expressed as:", "startOffset": 21, "endOffset": 24}], "year": 2017, "abstractText": "We consider learning a sequence classifier without labeled data by using sequential output statistics. The problem is highly valuable since obtaining labels in training data is often costly, while the sequential output statistics (e.g., language models) could be obtained independently of input data and thus with low or no cost. To address the problem, we propose an unsupervised learning cost function and study its properties. We show that, compared to earlier works, it is less inclined to be stuck in trivial solutions and avoids the need for a strong generative model. Although it is harder to optimize in its functional form, a stochastic primal-dual gradient method is developed to effectively solve the problem. Experiment results on real-world datasets demonstrate that the new unsupervised learning method gives drastically lower errors than other baseline methods. Specifically, it reaches test errors about twice of those obtained by fully supervised learning.", "creator": "LaTeX with hyperref package"}}}