{"id": "1702.07908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "CHAOS: A Parallelization Scheme for Training Convolutional Neural Networks on Intel Xeon Phi", "abstract": "Deep 185kg learning is tarigan an usp important component yaque of big - data analytic 520 tools kadu and brizan intelligent applications, www.hollandamerica.com such as, self - driving cars, witcover computer iprs vision, 67.74 speech recognition, or precision svartvadet medicine. larch However, mid-june the bongs training process is computationally skele intensive, ysa\u00ffe and corking often 24.58 requires a large stroppa amount of time vladim\u00edra if rush performed badou sequentially. Modern 37.8 parallel starch computing 5,195 systems provide suning the rasna capability to reduce amphibole the unifier required nasp training time of pomacea deep wedges neural potboilers networks. festhalle In 1994-2002 this paper, collating we fredrikshald present our parallelization scheme for training cove convolutional meyerowitz neural networks (CNN) named 37.37 Controlled Hogwild with Arbitrary 96.84 Order locri of winy Synchronization (CHAOS ). 50-50 Major cicilline features of CHAOS include the 1676 support tharavad for thread dagul and roue vector parallelism, non - instant panchita updates paka of weight stults parameters during enh back - njanji propagation 102.52 without penseroso a significant delay, diabolical and mccleskey implicit in-orbit synchronization shizhong in arbitrary dacascos order. deveaux CHAOS laibach is gural tailored for aluko parallel cabernets computing dismally systems that otc are paribatra accelerated undid with the Intel Xeon lanson Phi. We evaluate our parallelization approach empirically rassinier using habitantes measurement techniques and outselling performance modeling over-harvesting for widowhood various cantes numbers gurrelieder of falanga threads nira and gulden CNN rahimi architectures. muscularly Experimental results nias for the rjb MNIST wimple dataset khiladi of vocal handwritten digits charisma using unattested the uitlanders total number of threads kcbs on moesa the suramericana Xeon architectures Phi show speedups of up subpeak to 103x compared wehbe to fazekas the avrohom execution qh on tipster one 80.95 thread storekeeper of lequire the Xeon Phi, 14x shumsher compared zevin to the sequential execution on 98.06 Intel Xeon E5, formula and vrain 58x submitting compared newstalk to moodysson the olins sequential tualatin execution pindari on tahb Intel Core i5.", "histories": [["v1", "Sat, 25 Feb 2017 15:48:44 GMT  (192kb,D)", "http://arxiv.org/abs/1702.07908v1", "The Journal of Supercomputing, 2017"]], "COMMENTS": "The Journal of Supercomputing, 2017", "reviews": [], "SUBJECTS": "cs.DC cs.CV cs.LG", "authors": ["andre viebke", "suejb memeti", "sabri pllana", "ajith abraham"], "accepted": false, "id": "1702.07908"}, "pdf": {"name": "1702.07908.pdf", "metadata": {"source": "CRF", "title": "CHAOS: A Parallelization Scheme for Training Convolutional Neural Networks on Intel Xeon Phi", "authors": ["Andr\u00e9 Viebke", "Sabri Pllana", "Ajith Abraham"], "emails": ["av22cj@student.lnu.se", "suejb.memeti@lnu.se", "sabri.pllana@lnu.se", "ajith.abraham@ieee.org"], "sections": [{"heading": null, "text": "Keywords parallel programming \u00b7 deep learning \u00b7 convolutional neural networks \u00b7 Intel Xeon Phi\nA. Viebke \u00b7 S. Memeti \u00b7 S. Pllana Linnaeus University Address: Department of Computer Science, 351 95 Va\u0308xjo\u0308, Sweden A. Viebke: E-mail: av22cj@student.lnu.se S. Memeti: E-mail: suejb.memeti@lnu.se S. Pllana: E-mail: sabri.pllana@lnu.se\nA. Abraham Machine Intelligence Research Labs (MIR Labs) Address: 1, 3rd Street NW, P.O. Box 2259 Auburn, Washington 98071, USA E-mail: ajith.abraham@ieee.org\nar X\niv :1\n70 2.\n07 90\n8v 1\n[ cs\n.D C\n] 2\n5 Fe\nb 20\n17"}, {"heading": "1 Introduction", "text": "Traditionally engineers developed applications by specifying computer instructions that determined the application behavior. Nowadays engineers focus on developing and implementing sophisticated deep learning models that can learn to solve complex problems. Moreover, deep learning algorithms [27] can learn from their own experience rather than that of the engineer.\nMany private and public organizations are collecting huge amounts of data that may contain useful information from which valuable knowledge may be derived. With the pervasiveness of the Internet of Things the amount of available data is getting much larger [20]. Deep learning is a useful tool for analyzing and learning from massive amounts of data (also known as Big Data) that may be unlabeled and unstructured [47,44,36]. Deep learning algorithms can be found in many modern applications [54,50,19,56,48,17,21,59], such as, voice recognition, face recognition, autonomous cars, classification of liver diseases and breast cancer, computer vision, or social media.\nA Convolutional Neural Network (CNN) is a variant of a Deep Neural Network (DNN) [14]. Inspired by the visual cortex of animals, CNNs are applied to state-of-the-art applications, including computer vision and speech recognition [15]. However, supervised training of CNNs is computationally demanding and time consuming, and in many cases, several weeks are required to complete a training session. Often applications are tested with different parameters, and each test requires a full session of training.\nMulti-core processors [55] and in particular many-core [5] processing architectures, such as the NVIDIA Graphical Processing Unit (GPU) [37] or the Intel Xeon Phi [8] co-processor, provide processing capabilities that may be used to significantly speed-up the training of CNNs. While existing research [12,53,48,57,41] has addressed extensively the training of CNNs using GPUs, so far not much attention is given to the Intel Xeon Phi co-processor. Beside the performance capabilities, the Xeon Phi deserves our attention because of programmability [38] and portability [23].\nIn this paper, we present our parallelization scheme for training convolutional neural networks, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS). CHAOS is tailored for the Intel Xeon Phi coprocessor and exploits both the thread- and SIMD-level parallelism. The threadlevel parallelism is used to distribute the work across the available threads, whereas SIMD parallelism is used to compute the partial derivatives and weight gradients in convolutional layer. Empirical evaluation of CHAOS is performed on an Intel Xeon Phi 7120 co-processor. For experimentation, we use various number of threads, different CNNs architectures, and the MNIST dataset of handwritten digits [29]. Experimental evaluation results show that using the total number of available threads on the Intel Xeon Phi we can achieve speedups of up to 103\u00d7 compared to the execution on one thread of the Xeon Phi, 14\u00d7 compared to the sequential execution on Intel Xeon E5, and 58\u00d7 compared to the sequential execution on Intel Core i5. The error rates of the parallel execution are comparable to the sequential one. Furthermore, we\nuse performance prediction to study the performance behavior of our parallel solution for training CNNs for numbers of cores that go beyond the generation of the Intel Xeon Phi that was used in this paper. The main contributions of this paper include:\n\u2013 design and implementation of CHAOS parallelization scheme for training CNNs on the Intel Xeon Phi, \u2013 performance modeling of our parallel solution for training CNNs on the Intel Xeon Phi, \u2013 measurement-based empirical evaluation of CHAOS parallelization scheme, \u2013 model-based performance evaluation for future architectures of the Intel\nXeon Phi.\nThe rest of the paper is organized as follows. We discuss the related work in Section 2. Section 3 provides background information on CNNs and the Intel Xeon Phi many-core architecture. Section 4 discusses the design and implementation aspects of our parallelization scheme. The experimental evaluation of our approach is presented in Section 5. We summarize the paper in Section 6."}, {"heading": "2 Related Work", "text": "In comparison to related work that target GPUs, the work related to machine learning for Intel Xeon Phi is sparse. In this section, we describe machine learning approaches that target the Intel Xeon Phi co-processor, and thereafter we discuss CNN solutions for GPUs and contrast them to our CHAOS implementation.\n2.1 Machine Learning targeting Intel Xeon Phi\nIn this section, we discuss existing work for Support Vector Machines (SVMs), Restricted Boltzmann Machines (RBMs), sparse auto encoders and the BrainState-in-a-Box (BSB) model.\nYou et al. [58] present a library for parallel Support Vector Machines, MICSVM, which facilitates the use of SVMs on many- and multi-core architectures including Intel Xeon Phi. Experiments performed on several known datasets showed up to 84x speed up on the Intel Xeon Phi compared to the sequential execution of LIBSVM [6]. In comparison to their work, we target deep learning.\nJin et al. [22] perform the training of sparse auto encoders and restricted Boltzmann machines on the Intel Xeon Phi 5110p. The authors reported a speed up factor of 7 \u2212 10\u00d7 times compared to the Xeon E5620 CPU and more than 300\u00d7 times compared to the un-optimized version executed on one thread on the co-processor. Their work targets unsupervised deep learning of restricted Boltzmann machines and sparse auto encoders, whereas we target supervised deep learning of CNNs.\nThe performance gain on Intel Xeon Phi 7110p for a model called BrainState-in-a-Box (BSB) used for text recognition is studied by Ahmed et al. in [2]. The authors report about two-fold speedup for the co-processor compared to a CPU with 16 cores when parallelizing the algorithm. While both approaches target Intel Xeon Phi, our work addresses training of CNNs on the MNIST dataset.\n2.2 Related Work Targeting CNNs\nIn this section, we will discuss CNNs solutions for GPUs in the context of computer vision (image classification). Work related to MNIST [29] dataset is of most interest, also NORB [30] and CIFAR 10 [25] is considered. Additionally, work done in speech recognition and document processing is briefly addressed. We conclude this section by contrasting the presented related work with our CHAOS parallelization scheme.\nWork presented by Cires\u0327an et al. [12] target a CNN implementation raising the bars for the CIFAR10 (19.51% error rate), NORB (2.53% error rate) and MNIST (0.35% error rate) datasets. The training was performed on GPUs (Nvidia GTX 480 and GTX 580) where the authors managed to decrease the training time severely - up to 60\u00d7 compared to sequential execution on a CPU - and decrease the error rates to an, at the time, state-of-the-art accuracy level.\nLater, Cires\u0327an et al. [11] presented their multi-column deep neural network for classification of traffic sings. The results show that the model performed almost human-like (humans'error rate about 0.20%) on the MNIST dataset, achieving a best error rate of 0.23%. The authors trained the network on a GPU.\nVrtanoski et al. [53] use OpenCL for parallelization of the back-propagation algorithm for pattern recognition. They showed a significant cost reduction, a maximum speedup of 25.8\u00d7 was achieved on an ATI 5870 GPU compared to a Xeon W3530 CPU when training the model on the MNIST dataset.\nThe ImageNet challenge aims to evaluate algorithms for large-scale object detection and image classification based on the ImageNet dataset. Krizhevsky et al. [26] joined the challenge and reduced the error rate of the test set to 15.3% from the second best 26.2% using a CNN with 5 convolutional layers. For the experiments, two GPUs (Nvidia GTX 580) were used only communicating in certain layers. The training lasted for 5 to 6 days.\nIn a later challenge, ILSVRC 2014, a team from Google entered the competition with GoogleNet, a 22-layer deep CNN and won the classification challenge with a 6.67% error rate. The training was carried out on CPUs. The authors state that the network could be trained on GPUs within a week, illuminating the limited amount of memory to be one of the major concerns [48].\nYadan et al. [57] used multiple GPUs to train CNNs on the ImageNet dataset using both data- and model-parallelism, i.e. either the input space is divided into mini-batches where each GPU train its own batch (data paral-\nlelism) or the GPUs train one sample together (model parallelism). There is no direct comparison with the training time on CPU, however, using 4 GPUs (Nvidia Titan) and model- and data-parallelism, the network was trained for 4.8 days.\nSong et al. [46] constructed a CNN to recognize face expressions and developed a smart-phone app in which the user can capture a picture and send it to a server hosting the network. The network, predicts a face expression and sends the result back to the user. With the help of GPUs (Nvidia Titan), the network was trained in a couple of hours on the ImageNet dataset.\nScherer et al. [42] accelerated the large-scale neural networks with parallel GPUs. Experiments with the NORB dataset on an Nvidia GTX 285 GPU showed a maximal speedup of 115\u00d7 compared to a CPU implementation (Core i7 940). After training the network for 360 epochs, an error rate of 8.6% was achieved.\nCires\u0327an et al. [10] combined multiple CNNs to classify German traffic signs and achieved a 99.15% recognition rate (0.85 % error rate). The training was performed using an Intel Core i7 and 4 GPUs (2 x GTX 480 and 2 x GTX 580).\nMore recently Abadi et al. [1] presented TensorFlow, a system for expressing and executing machine learning algorithms including training deep neural network models.\nResearchers have also found CNNs successful for speech tasks. Large vocabulary continuous speech recognition deals with translation of continuous speech for languages with large vocabularies. Sainath et al. [41] investigated the advantages of CNNs performing speech recognition tasks and compared the results with previous DNN approaches. Results indicated on a 12-14% relative improvement of word error rates compared to a DNN trained on GPUs.\nChellapilla et al. [7] investigated GPUs (Nvidia Geforce 7800 Ultra) for document processing on the MNIST dataset and achieved a 4.11\u00d7 speed up compared to the sequential execution a Intel Pentium 4 CPU running at 2.5 GHz clock frequency.\nIn contrast to CHAOS, these studies target training of CNNs using GPUs, whereas our approach addresses training of CNNs on the MNIST dataset using the Intel Xeon Phi co-processor. While there are several review papers (such as, [4,45,49]) and on-line articles (such as, [35]) that compare existing frameworks for parallelization of training CNN architectures, we focus on detailed analysis of our proposed parallelization approach using measurement techniques and performance modeling. We compare the performance improvement achieved with CHAOS parallelization scheme to the sequential version executed on Intel Xeon Phi, Intel Xeon E5 and Intel Core i5 processor."}, {"heading": "3 Background", "text": "In this section, we first provide some background information related to the neural networks focusing on convolutional neural networks, and thereafter we provide some information about the architecture of the Intel Xeon Phi.\n3.1 Neural Networks\nA Convolutional Neural Network is a variant of a Deep Neural Network, which introduces two additional layer types: convolutional layers and pooling layers. The mammal visual processing system is hierarchical (deep) in nature. Higher level features are abstractions of lower level ones. E.g. to understand speech, waveforms are translated through several layers until reaching a linguistic level. A similar analogy can be drawn for images, where edges and corners are lower level abstractions translated into more spatial patterns on higher levels. Moreover, it is also known that the animal cortex consists of both simple and complex cells firing on certain visual inputs in their receptive fields. Simple cells detect edge-like patterns whereas complex cells are locally invariant, spanning larger receptive fields. These are the very fundamental properties of the animal brain inspiring DNNs and CNNs.\nIn this section, we first describe the DNNs and the Forward- and Backpropagation, thereafter we introduce the CNNs."}, {"heading": "3.1.1 Deep Neural Networks", "text": "The architecture of a DNN consists of multiple layers of neurons. Neurons are connected to each other through edges (weights). The network can simply be thought of as a weighted graph; a directed acyclic graph represents a feedforward network. The depth and breadth of the network differs as may the layer types. Regardless of the depth, a network has at least one input and one output layer. A neuron has a set of incoming weights, which have corresponding outgoing edges attached to neurons in the previous layer. Also, a bias term is used at each layer as an intercept term. The goal of the learning process is to adjust the network weights and find a global minimum by reducing the overall error, i.e. the deviation between the predicted and the desired outcome of all the samples. The resulting weight parameters can thereafter be used to make predictions of unseen inputs [3]."}, {"heading": "3.1.2 Forward Propagation", "text": "DNNs can make predictions by forward propagating an input through the network. Forward propagation proceeds by performing calculations at each layer until reaching the output layer, which contains a vector representing the prediction. For example, in image classification problems, the output layer contains the prediction score that indicates the likelihood that an image belongs to a category [18,3].\nThe forward propagation starts from a given input layer, then at each layer the activation for a neuron is activated using the equation yli = \u03c3(x l i) + I l i where yli is the output value of neuron i at layer l, x l i is the input value of the same neuron, and \u03c3 (sigmoid) is the activation function. I li is used for the input layer when there is no previous layer. The goal of the activation function is to return a normalized value (sigmoid return [0,1] and tanh is used in cases where the desired return values are [-1,1]). The input xli can be calculated as xli = \u2211 j(w l jiy l\u22121 j ) where w l ji denotes the weight between neuron i in the current layer l, and j in the previous layer, and yl\u22121j the output of the jth neuron at the previous layer. This process is repeated until reaching the output layer. At the output layer, it is common to apply a soft max function, or similar, to squash the output vector and hence derive the prediction."}, {"heading": "3.1.3 Back-Propagation", "text": "Back-propagation is the process of propagating errors, i.e. the loss calculated as the deviation between the predicted and the desired output, backward in the network, by adjusting the weights at each layer. The error and partial derivatives \u03b4li are calculated at the output layer based on the predicted values from forward propagation and the labeled value (the correct value). At each layer, the relative error of each neuron is calculated and the weight parameters are updated based on how much the neuron participated in the faulty prediction. The equation: \u03b4E\n\u03b4yli =\n\u2211 wlij \u03b4E\n\u03b4xl+1j denotes that the partial derivative of\nneuron i at the current layer l is the sum of the derivatives of connected neurons at the next layer multiplied with the weights, assuming wl denotes the weights between the maps. Additionally, a decay is commonly used to control the impact of the updates, which is omitted in the above calculations. More concretely, the algorithm can be thought of as updating the layer's weights based on \u201dhow much it was responsible for the errors in the output\u201d [18,3]."}, {"heading": "3.1.4 Convolutional Neural Networks", "text": "A Convolutional Neural Network is a multi-layer model constructed to learn various levels of representations where higher level representations are described based on the lower level ones [43]. It is a variant of deep neural network that introduces two new layer types: convolutional and pooling layers.\nThe convolutional layer consists of several feature maps where neurons in each map connect to a grid of neurons in maps in the previous layer through overlapping kernels. The kernels are tiled to cover the whole input space. The approach is inspired by the receptive fields of the mammal visual cortex. All neurons of a map extract the same features from a map in the previous layer as they share the same set of weights.\nPooling layers intervene convolutional layers and have shown to lead to faster convergence. Each neuron in a pooling layer outputs the (maximum/average) value of a partition of neurons in the previous layer, and hence only\nactivates if the underlying grid contains the sought feature. Besides from lowering the computational load, it also enables position invariance and down samples the input by a factor relative to the kernel size [28].\nFigure 1 shows LeNet-5 that is an example of a Convolutional Neural Network. Each layer of convolution and pooling (that is a specific method of sub-sampling used in LeNet) comprise several feature maps. Neurons in the feature map cover different sub-fields of the neurons from the previous layer. All neurons in a map share the same weight parameters, therefore they extract the same features from different parts of the input from the previous layers.\nCNNs are commonly constructed similarly to the LeNet-5, beginning with an input layer, followed by several convolutional/pooling combinations, ending with a fully connected layer and an output layer [28]. Recent networks are much deeper and/or wider, for instance, the GoogleNet [48] consists of 22 layers.\nVarious implementations target the Convolutional Neural Networks, such as: EbLearn at New York University and Caffe at Berkeley. As a basis for our work we selected a project developed by Cires\u0327an [9]. This implementation targets the MNIST dataset of handwritten digits, and has the possibility to dynamically configure the definition of layers, the activation function and the connection types using a configuration file.\n3.2 Parallel Systems accelerated with Intel\u00aeXeon Phi\u2122\nFigure 2 depicts an overview of the Intel Xeon Phi (codenamed Knights Corner) architecture. It is a many-core shared-memory co-processor, which runs a lightweight Linux operating system that offers the possibility to communicate with it over ssh. The Xeon Phi offers two programming models:\n1. offload - parts of the applications running on the host are offloaded to the co-processor 2. native - the code is compiled specifically for running natively on the coprocessor. The code and all the required libraries should be transferred on the device. In this paper, we focus on the native mode.\nThe Intel Xeon Phi (type 7120P used in this paper) comprises 61 x86 cores, each core runs at 1.2 GHz base frequency, and up to 1.3GHz on max turbo frequency [8]. Each core can switch between four hardware threads in a\nround-robin manner, which amounts to a total of 244 threads per co-processor. Theoretically, the co-processor can deliver up to one teraFLOP/s of double precision performance, or two teraFLOP/s of single precision performance. Each core has its own L1 (32KB) and L2 (512KB) cache. The L2 cache is kept fully coherent by a global distributed tag-directory (TD). The cores are connected through a bidirectional ring bus interconnect, which forms a unified shared L2 cache of 30.5MB. In addition to the cores, there are 16 memory channels that in theory offer a maximum memory bandwidth of 352GB/s. The GDDR memory controllers provide direct interface to the GDDR5 memory, and the PCIe Client Logic provides direct interface to the PCIe bus.\nEfficient usage of the available vector processing units of the Intel Xeon Phi is essential to fully utilize the performance of the co-processor [52]. Through the 512-bit wide SIMD registers it can perform 16 (16 wide \u00d7 32 bit) singleprecision or 8 (8 wide \u00d7 64 bit) double-precision operations per cycle.\nThe performance capabilities of the Intel Xeon Phi are discussed and investigated empirically by different researches within several domain applications [16,32,34,51,31,33]."}, {"heading": "4 Our Parallelization Scheme for Training Convolutional Neural Networks on Intel Xeon Phi", "text": "The parallelism can be either divided data-wise, i.e. threads process several inputs concurrently, or model-wise, i.e. several threads share the computational burden of one input. Whether one approach can be advantageous over the other mainly depends on the synchronization overhead of the weight vectors and how well it scales with the number of processing units.\nIn this section, we first discuss the design aspects of our parallelization scheme for training convolutional neural networks. Thereafter, we discuss the\nimplementation aspects that allow full utilization of the Intel Xeon Phi coprocessor.\n4.1 Design Aspects\nOn-line stochastic gradient descent has the advantage of instant updates of weights for each sample. However, the sequential nature of the algorithm yields impediments as the number of multi- and many-core platforms are emerging. We consider different existing parallelization strategies for stochastic gradient descent:\nStrategy A: Hybrid - uses both data- and model parallelism, such that data parallelism is applied in convolutional layers, and the model parallelism is applied in fully connected layers [24].\nStrategy B: Averaged Stochastic Gradient - divides the input into batches and feeds each batch to a node. This strategy proceeds as follows: (1) Initialize the weights of the learner by randomization; (2) Split the training data into n equal chunks and send them to the learners; (3) each learner process the data and calculates the weight gradients for its batch; (4) send the calculated gradients back to the master; (5) the master computes and updates the new weights; and (6) the master sends the new weights to the nodes and a new iteration begins [13]. The convergence speed is slightly worse than for the sequential approach, however the training time is heavily reduced.\nStrategy C: Delayed Stochastic Gradient - suggests updating the weight parameters in a round-robin fashion by the workers. One solution is splitting the samples by the number of threads, and let each thread work on its own distinct chunk of samples, only sharing a common weight vector. Threads are only allowed to update the weight vector in a round-robin fashion, and hence each update will be delayed [60].\nStrategy D: HogWild! - is a stochastic gradient descent without locks. The approach is applicable for sparse optimization problems (threads/core updates do not conflict much) [40].\nIn this paper, we introduce Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS), a parallelization scheme that can exploit both thread- and SIMD-level parallelism available on Intel Xeon Phi. CHAOS is a data-parallel controlled version of HogWild! with delayed updates, which combines parts of strategies A-D. The key aspects of CHAOS are:\n\u2013 Thread parallelism - The overview of our parallelization scheme is depicted in Figure 3. Initially for as many threads as there are available network instances are created, which share weight parameters, whereas to support concurrent processing of images some variables are private to each thread. After the initialization of CNNs and images is done, the process of training starts. The major steps of an epoch include: Training, Validation and Testing. The first step, Training, proceeds with each worker picking an image, forward propagates it through the network, calculates the error, and backpropagates the partial derivatives, adjusting the weight parameters. Since\neach worker picks a new image from the set, other workers do not have to wait for significantly slow workers. After Training, each worker participates in Validation and Testing evaluating the prediction accuracy of the network by predicting images in the validation and test set accordingly. Adoption of data parallelism was inspired by Krizhevsky [24], promoting data parallelism for convolutional layers as they are computationally intensive. \u2013 Controlled HogWild - during the back-propagation the shared weights are updated after each layer's computations (a technique inspired by [60]), whereas the local weight parameters are updated instantly (a technique inspired by [40]), which means that the gradients are calculated locally first then shared with other workers. However, the update to the global gradients can be performed at any time, which means that there is no need to wait for other workers to finish their updates. This technique, which we refer to as non-instant updates of weight parameters without significant delay, allows us to avoid unnecessary cache line invalidation and memory writes. \u2013 Arbitrary Order of Synchronization - There is no need for explicit synchronization, because all workers share weight parameter. However, an implicit synchronization is performed in an arbitrary order because writes are controlled by a first-come-first schedule and reads are performed on demand.\nThe main goal of CHAOS is to minimize the time spent in the convolutional layers, which can be done through data parallelism, adapting the knowledge presented in strategy A. In strategy B, the synchronization is performed because of averaging worker's gradient calculations. Since work is distributed, computations are performed on stale parameters. The strategy can be applied in distributed and non-distributed settings. The division of work over several distributed workers was adapted in CHAOS. In strategy C, the updates are postponed using a round-robin-fashion where each thread gets to update when it is its turn. The difference compared to strategy B is that instances train on the same set of weights and no averaging is performed. The advantage is that all instances train on the same weights. The disadvantage of this approach is the delayed updates of the weight parameters as they are performed on stale data. Training on shared weights and delaying the updates are adopted in CHAOS. Strategy D presents a lock-free approach of updating the weight parameters, updates are performed instantly without any locks. Our updates are not instant, however, after computing the gradients there is nothing prohibiting a worker contributing to the shared weights, the notion of instant inspired CHAOS.\n4.2 Implementation Aspects\nThe main goal is to utilize the many cores of the Intel Xeon Phi co-processor efficiently to lower the training time (execution time) of the selected CNN algorithm, at the same time maintaining low deviation in error rates, especially\non the test set. Moreover, the quality of the implementation is verified using errors and error rates on the validation and test set.\nIn the sequential version, only minor modifications of the original version were performed. Mainly, we added a Reporter class to serialize execution results. The instrumentation should not add any time penalties in practice. However, if these penalties occur in the sequential version they are likely to imply corresponding penalties in the parallel version, therefore it should not impact the results.\nThe main goal of the parallel version is to lower the execution time of the sequential implementation and to scale well with the number of processing\nunits on the co-processor. To facilitate this, it is essential to fully consider the characteristics of the underlying hardware. From results derived in the sequential execution we found the hotspots of the application to be predominantly the convolutional layers. The time spent in both forward- and back-propagation is about 94% of the total time of all layers (up to 99% for the larger network), which is depicted in the Table 1.\nIn our proposed strategy, a set of N network instances are created and assigned to T threads. We assume T == N , i.e. one thread per network instance. T threads are spawned, each responsible for its own instance.\nThe overview of the algorithm is shown in Fig. 3. In Fig. 4 the training, testing and back-propagation phase are shown in details. Training (see Fig. 4a) picks an image, forward propagates it, determines the loss and backpropagates the partial derivatives (deltas) in the network - this process is done simultaneously by all workers, each worker processing one image. Each worker participating in testing (see Fig. 4b), picks an image, forward propagates it and then collects errors and error rates. The results are cumulated for all threads. Perhaps the most interesting part is the back-propagation (see Fig. 4c). The shared weights are used when propagating the deltas, however, before updating the weight gradients, the pointers are set to the local weights. Thereafter the algorithm proceeds by updating the local weights first. When a worker has contributions to the global weights it can update in a controlled manner, avoiding data races. Updates immediately affect other workers in their training process. Hence the update is delayed slightly, to decrease the invalidation of cache lines, yet almost instant and workers do not have to wait for a longer period before contributing with their knowledge.\nTo see why delays are important, consider the following scenario: If training several network instances concurrently, they share the same weight vectors, other variables are thread private. The major consideration lies in the weight updates. Let W jl be the j-th weight on the l-th layer. In accordance with the current implementation, a weight is updated several times since neurons in a map (on the same layer) share the same weights, and the kernel is shifted over the neurons. Further assume that several threads work on the same weight W jl at some point in time. Even if other threads only read the weights, their local data, as saved in the Level 2 cache, will be invalidated and a re-fetch is required to assert their integrity. This happens because cache lines are shared between cores. The approach of slightly delaying the updates and forcing one thread to update in atomicity leads to fewer invalidations. Still a major disadvantages\nis that the shared weights does not infer any data locality (data cannot retain completely in Level 2 cache for a longer period).\nListing 1: An extract from the vectorization report for the partial derivative updates in the convolutional layer.\nremark #15475: --- begin vector loop cost summary --- remark #15476: scalar loop cost: 30 remark #15477: vector loop cost: 7.500 remark #15478: estimated potential speedup: 3.980 remark #15479: lightweight vector operations: 6 remark #15480: medium -overhead vector operations: 1 remark #15481: heavy -overhead vector operations: 1 remark #15488: --- end vector loop cost summary ---\nTo further decrease the time spent in convolutional layers, loops were vectorized to facilitate the vector processing unit of the co-processor. Data was allocated using mm malloc() with 64 byte alignment increasing the accuracy of memory requests. The vectorization was achieved by adding #pragma omp simd instructions and explicitly informing the compiler of the memory alignment using assume aligned(). Some unnecessary overhead is added through the lack of data alignment of the deltas and weights. The computations of partial derivatives and weight gradients in the convolutional layers are performed in a SIMD way, which allows efficient utiliziation of the 512 bit wide vector processing units of the Intel Xeon Phi. An extract from the vectorization report (see Listing 1), for the updates of partial derivatives in the convolutional layer shows an estimated potential speed up of 3.98\u00d7 compared to the scalar loop.\nFurther algorithmic optimizations were performed. For example: (1) The images are loaded into a pre-allocated memory instead of allocating new memory when requesting an image; (2) Hardware pre-fetching was applied to mitigate the shortcomings of the in-order-execution scheme. Pre-fetching loads data to L2 cache to make it available for future computations; (3) Letting workers pick images instead of assigning images to workers, allow for a smaller\noverhead at the end of a work-sharing construct; (4) The number of locks are minimized as far as possible; (5) We made most of the variables thread private to achieve data locality.\nThe training phase was distributed through thread parallelism, dividing the input space over available workers. CHAOS uses the vector processing units to improve performance and tries to retain local variables in local cache as far as possible. The delayed updates decrease the invalidation of cache lines. Since weight parameters are shared among threads, there is a possibility that data can be fetched from another core's cache instead of main memory, reducing the wait times. Also, the memory was aligned to 64 bytes and unnecessary system calls were removed from the parallel work."}, {"heading": "5 Evaluation", "text": "In this section, we first describe the experimentation environment used for evaluation of our CHAOS parallelization scheme. Thereafter, we describe the development of a performance model for CHAOS. Finally we discuss the obtained results with respect to scalability, speedup, and prediction accuracy.\n5.1 Experimental Setup\nIn this study, OpenMP was selected to facilitate the utilization of threadand SIMD-parallelism available in the Intel Xeon Phi co-processor. C++ programming language is used for algorithm implementation. The Intel Compiler 15.0.0 was used for native compilation of the application for the co-processor, whereas the O3 level was used for optimization.\nSystem Configuration - To evaluate our approach we use an Intel Xeon Phi accelerator that comprises 61 cores that run at 1.2 GHz. For evaluation 1, 15, 30, 60, 120, 180, 240, and 244 threads of the co-processor were used. Each thread was responsible for one network instance. For comparison, we use two general purpose CPUs, including the Intel Xeon E5-s695v2 that runs at 2.4 GHz clock frequency, and the Intel Core i5 661 that runs at 3.33GHz clock frequency.\nData Set - To evaluate our approach, the MNIST [29] dataset of handwritten digits is used. In total the MNIST dataset comprises 70000 images, 60000 of which are used for training/validation and the rest for testing.\nCNN Architectures - Three different CNN architectures were used for evaluation, small, medium and large. The small and medium architecture were trained for 70 epochs, and the large one for 15 epochs, using a starting decay (eta) of 0.001 and factor of 0.9. The small and medium network consist of seven layers in total (one input layer, two convolutional layers, two maxpoling layers, one fully connected layer and the output layer). The difference between these two networks is in the number of feature maps per layer and the number of neurons per map. For example, the first convolutional layer of\nthe small network has five feature maps and 3380 neurons, whereas the first convolutional layer of the medium network has 20 feature maps and 13520 neurons. The large network differs from the small and the medium network in the number of layers as well. In total, there are nine layers, one input layer, three convolutional layers, three max-pooling layers, one fully connected layer and the output layer. Detailed information (including the number and the size of feature maps, neurons, the size of the kernels and the weights) about the considered architectures is listed in Table 2.\nTo address the variability in performance measurements we have repeated the execution of each parallel configuration for three times.\n5.2 Performance Model\nA performance model [39] enables us to reason about the behavior of an implementation in future execution contexts. Our performance model for CHAOS implementation can predict the performance for numbers of threads that go beyond the number of hardware threads supported in the Intel Xeon Phi model that we used for evaluation. Additionally, it can predict the performance of different CNN architectures with various number of images and epochs.\nThe goal is to construct a parametrized model with the following parameters ep, i, it and p, where ep stands for the number of epochs, i indicates the\nListing 2: The formula for our performance prediction model.\nT (i, it, ep, p, s) = Tcomp(i, it, ep, p, s) + Tmem(ep, i, p)\n=\n( Prep + 4 \u2217 i + 2 \u2217 it + 10 \u2217 ep\ns (sequential work)\n+ (((FProp + BProp s ) \u2217 i pi \u2217 ep ) (training)\n+ ((FProp s ) \u2217 i pi \u2217 ep ) (validation)\n+ ((FProp s ) \u2217 it pit \u2217 ep )) (testing)\n\u2217CPI ) \u2217OperationFactor + Tmem(ep, i, p)\nnumber of images in the training/validation set, it stands for the number of images in the test set, and p is the number of processing units. Table 3 lists the full set of variables used in our performance model, some of which are hardware dependent and some others are independent of the underlying hardware. Each variable is either measured, calculated, constant, or parameter in the model. Listing 2 shows the formula used for our performance prediction model.\nThe total execution time (T ) is the sum of computations time (Tcomp) and memory operations (Tmem). T depends on several factors including: speed, number of processing units, communication costs (such as network latency), and memory contention. The Tcomp is sum of sequential work, training, validation, and testing. Most interesting is contentions causing wait times, including memory latencies and synchronization overhead. Tmem adds memory and synchronization overheads. The contention is measured through an experimental approach by executing a small script on the co-processor for different thread counts, weights and layers.\nWe define Tmem(ep, i, p) = MemoryContention\u2217ep\u2217i\np whereMemoryContention\nis the measured memory contention when p threads are fighting for the I/O weights concurrently. Table 4 depicts the measured and predicted memory contentions for the Intel Xeon Phi.\nOur performance prediction model is not concerned with any practical measurements except for Tmem. Along with the CPI and OperationFactor it is possible to derive the number of instructions (theoretically) per cycle that each thread can perform.\nWe use Prep to be different for each CNN architecture (109, 1010 and 1011 for small, medium and large architecture respectively). The OperationFactor is adjusted to closely match the measured value for 15 threads, and mitigate\nthe approximations done for instructions in the first place, at the same time account for vectorization.\nWhen one hardware thread is present per core, one instruction per cycle can be assumed. For 4 threads per core, only 0.5 instructions per cycle can be assumed, which means that each thread gets to execute two instructions every fourth cycle (CPI of 2) and hence we use the CPI factor to control the best theoretical amount of instructions a thread can retire. The speed s is defined in Table 3. FProp and BProp are placeholders for the actual number of operations.\n5.3 Results\nIn this section, we analyze the collected data with regards to the execution time and speedup for varying number of threads and CNN architectures. The errors and error rates (incorrect predictions) are used to validate our implementation. Furthermore, we discuss the deviation in number of incorrectly predicted images.\nThe execution time is the total time the algorithm executes, excluding the time required to initialize the network instances and images (for both the sequential and parallel version). The speed up is measured as the relativeness between two execution times, with the sequential execution times of Intel Xeon E5, Intel Core i5, and Xeon Phi as the base. The error rate is the fraction of images the network was unable to predict and the error the cumulated loss from the loss function.\nIn the figures and tables in this section, we use the following notations: Par refers to the parallel version, Seq is the sequential version, and T denotes threads, e.g. Phi Par. 1 T is the parallel version and one thread on the Xeon Phi.\nResult 1: The CHAOS parallelization scheme scales gracefully to large numbers of threads.\nFigure 5 depicts the total execution time of the parallel version of the implementation running on the Xeon Phi and the sequential version running on the Xeon E5 CPU. We vary the number of threads on the Xeon Phi between 1, 15, 30, 60, 120, 180, 240, and 244, and the CNN architectures between small, medium and large. We elide the results of Xeon E5 Seq. and Phi Par. 1T for simplicity and clarity. The large CNN architecture requires 31.1 hours to be completed sequentially on the Xeon E5 CPU, whereas using one thread on the Xeon Phi requires 295.5 hours. By increasing the number of threads to 15, 30, and 60, the execution time decreases to 19.7, 9.9, and 5.0 hours respectively. Using the total number of threads (that is 244) on the Xeon Phi the training may be completed in only 2.9 hours. We may observe a promising scalability\nwhile increasing the number of threads. Similar results may be observed for the small and medium architecture.\nIt should be considered that the selected CNN architectures were trained for different number of epochs, and that larger networks tend to produce better predictions (lower error rates). A fairer comparison would be to compare the execution times until reaching a specific error rate on the test set. In Fig. 6 the total execution times for the different CNN architectures and threads on the Xeon Phi is shown. We have set the stop criteria as the error rate \u2264 1.54%, which is the ending error rate of the test set for the small architecture. The large network executes for a longer period even if it converges in fewer epochs, and that the medium network needs less time to reach an equal (or better) ending error rate than the small and large network. Note that several other factors impact training, including the starting decay, the factor which the decay is decreased, dataset, loss function, preparation of images, initial weight values. Therefore, several combinations of parameters need to be tested before finding a balance. In this study, we focus on the number of epochs as the stop criteria and draw conclusions from this, considering the deviation of the error and error rates.\nResult 2: The total execution time is strongly influenced by the forwardpropagation and back-propagation in the network. The convolutional layers are the most computationally expensive.\nTable 5 depicts the time spent per layer for the large CNN architecture. The results were gathered as the total time spent for all network instances on all layers together. Dividing the total time by the number of network instances and later the number of epochs, yields the number of seconds spent on each layer per network instance and epoch. A lower time spent on each layer per epoch and instance indicates on a speedup. We may observe that the large\narchitecture spends almost all the time in the convolutional layers and almost no time in the other layers. For Phi Par. 240 T about 88% is spent in the backpropagation of convolutional layers and about 10% in forward propagation. We have observed similar results for small and medium CNN architecture, however we elide these results for space.\nWe have observed that the more threads involved in training the more percentage of the total time each thread spends in the back-propagation of the convolutional layer, and less time in the others. Overall, the time spent at each layer is decreased per thread when increasing the number of threads. Therefore, there is an interesting relationship between the layer times and the speed up of the algorithm.\nTable 6 presents the speed up relative to the Phi Par. 1 T for the different architectures on the convolutional layer. The times are collected by each network instance (through instrumentation of the forward- and back-propagate function) and averaged over the number of network instances and epochs. As can be seen, in almost all cases there is an increase in speed up when increasing the network size, more importantly, the speed up does not decrease. Maybe the most interesting phenomena is that the speed up per layer have an almost direct relationship to the speed up of the algorithm, especially if compared to the back-propagation part. This emphasizes the importance of reducing the time spent in the convolutional layers.\nResult 3: Using CHAOS parallel implementation for training of CNNs on Intel Xeon Phi we achieved speedups of up to 103\u00d7, 14\u00d7, and 58\u00d7 compared to the single-thread performance on Intel Xeon Phi, Intel Xeon E5 CPU, and Intel Core i5, respectively.\nFigures 7 and 8 emphasize the facts shown in Fig. 5 in terms of speedup. Figure 7 depicts the speedup compared to the sequential execution on Xeon E5 (Xeon E5 Seq.) for various number of threads and CNN architectures. As can be seen, adding more threads results in speedup increase in all cases. Using 240 threads on the Xeon Phi infer a 13.26\u00d7 speedup for the small CNN architecture. Utilizing the last core of the Xeon Phi, which is used by the OS, shows even higher speedup (14.07\u00d7). We may observe that doubling the number of threads from 15, to 30, and from 30 to 60 almost doubles the speedup (2.03, 4.03, and 7.78). Increasing the number of threads further results with significant speedup, but the double speedup trend breaks.\nFigure 8 shows the speedup compared to the execution running in one thread of the Xeon Phi (Phi Par. 1 T ) while varying the number of threads and the CNN architectures. We may observe that the speedup is close to linear for up to 60 threads for all CNN architectures. Increasing the number of threads further results with significant speedup. Moreover it can be seen that when keeping the number of threads fixed and increasing the architecture size, the speed up increases with a small factor as well, except for 244 threads. It seems like larger architectures are beneficial. However, it could also be the\nSpeedup Compared to Sequential Xeon E5\ncase that Phi Par. 1 T executes relatively slower than Xeon E5 Seq. for larger architectures than for smaller ones.\nFigure 9 shows the speedup compared to the sequential version executed in Intel Core i5 (Core i5 Seq.) while varying the number of threads and the CNN architectures. We may observe that using 15 threads we gain 10\u00d7 speedup. Doubling the number of threads to 30, and then to 60 results with close to double speedup increase (19.8 and 38.3). By using 120 threads (that is two threads per core) the trend of double speedup increase breaks (55.6\u00d7). Increasing the number of threads per core to three and four results with modest speedup increase (62\u00d7 and 65.3\u00d7).\nResult 4: The image classification accuracy of parallel implementation using CHAOS is comparable to the one running sequentially. The deviation error and the number of incorrectly predicted images is not abundant.\nWe validate the implementation by comparing the error and error rates for each epoch and configuration. Figure 10 depicts the ending errors for the three considered CNN architectures for both validation and test set. The black dashed line delineates the base line (that is a ratio of 1). Values below the line are considered better, whereas those above the line are worse than for Xeon E5 Seq. As a base line, we use the Xeon E5, however identical results are derived executing the sequential version on any platform. As can be seen in Fig. 10, the largest difference is encountered by Phi Par. 244 T, about 22 units (0.05%) worse than the base line. On the contrary, Phi Par. 15 T has 9 units'lower error compared to the base line for the large test set. The validation sets are rather stable whereas the test set fluctuates more heavily. Although one should consider the deviation in error respectfully, they are not abundant in this case. Please note that the diagram has a high zoom factor, hence the differences are magnified.\nTable 7 lists the number of incorrectly classified images for each CNN architecture. For each architecture, the total (Tot) number of images and the difference (Diff ) compared to the optimal numbers of Xeon E5 Seq. are shown. Negative values indicate that the ending error rate was better than optimal (less images were incorrectly predicted), whereas positive values indicate that more images than Xeon E5 Seq. were incorrectly predicted. For each column in the table, best and worst values are annotated with underline and bold fonts, respectively. No obvious pattern can be found, however, increasing the number of threads does not lead to worse prediction in general. Phi Par. 180 T stands out as it was 17 images better than Xeon E5 Seq. for small architecture on validation set. Phi Par. 15 T also performs worst on the small architecture on the validation set. The overall worst performance is achieved by Phi par.\n0.92\n0.94\n0.96\n0.98\n1\n1.02\n1.04\n1.06\nPh i P\nar. 24\n4 T\nPh i P\nar. 24\n0 T\nPh i P\nar. 18\n0 T\nPh i P\nar. 12\n0 T\nPh i P\nar. 60\nT\nPh i P\nar. 30\nT\nPh i P\nar. 15\nT\nR a ti\no c\no m\np a re\nd t\no b\na s e l in\ne\nNumber of Threads\nError Relative to Xeon E5 Seq.\nError Validation Small Error Validation Medium\nError Validation Large Error Test Small\nError Test Medium Error Test Large\nXeon E5 Seq. (base line)22\n9\nFig. 10: The relative cumulative error (loss) for the three considered CNN architectures (small, medium, and large) for both validation and test set.\n120 T on the test set for small CNN architecture. Please note that the total number of images in the validation set is 60, 000 and 10, 000 for the test set. Overall, the number of incorrectly predicted images and the deviation from the base line is not abundant.\nResult 5: The predicted execution times obtained from the performance model match well the measured execution times.\nFigures 11, 12, and 13 depict the predicted and measured execution times for small, medium and large CNN architecture. For the small network (see Fig. 11), the predictions are close to the measured values with a slight deviation at the end. The prediction model seems to over-estimate the execution time with a small factor.\nFor the medium architecture (see Fig. 12) the prediction follow the measured values closely, although it underestimates the execution time slightly. At\n0\n500\n1000\n1500\n2000\n2500\nPh i P\nar. 24\n0 T\nPh i P\nar. 18\n0 T\nPh i P\nar. 12\n0 T\nPh i P\nar. 60\nT\nPh i P\nar. 30\nT\nPh i P\nar. 15\nT\nE x e c u ti\no n T\nim e [\ns e c o n d s ]\nNumber of Threads\nPredicted vs Measured Execution Time on Intel Xeon Phi for Small CNN Artchitecture\nMeasured Predicted\n3 7 6\n3 9 6\n4 4 1\n6 4 1\n1 2 3 7\n2 4 5 4\n5 3 4\n5 2 0\n5 2 4\n7 7 8\n1 3 0 5\n2 3 9 7\nNumber of Threads\nFig. 12: The comparison between the predicted execution time and the measured execution time on Intel Xeon Phi for the medium CNN architecture.\n120 threads, the measured and predicted values starts to deviate, which are recovered at 240 threads.\nThe large architecture yields similar results as the medium. As can be seen, the measured values are slightly higher than the predictions, however, the predictions follow the measured values. As can be seen for 120 threads there is a deviation which is recovered for 240 threads. Also, the predictions increase between 120 and 180, and 180 and 240 threads for both predictions, whereas the actual execution time is lowered. This is most probably due to the CPI factor that is added when 3 or more threads are present on the same core.\n0\n10000\n20000\n30000\n40000\n50000\n60000\n70000\nPh i P\nar. 24\n0 T\nPh i P\nar. 18\n0 T\nPh i P\nar. 12\n0 T\nPh i P\nar. 60\nT\nPh i P\nar. 30\nT\nPh i P\nar. 15\nT\nE x e c u ti\no n T\nim e [\ns e c o n d s ]\nNumber of Threads\nPredicted vs Measured Execution Time on Intel Xeon Phi for Large CNN Architecture\nMeasured Predicted\n1 0 8 0 7\n1 1 3 7 9\n1 2 3 5 0 1 7 8 8 9\n3 5 6 0 1\n7 1 0 1 4\n9 4 3 2\n9 4 1 5\n9 4 2 1\n1 7 1 0 1\n3 2 5 2 5\n6 3 3 6 2\nFig. 13: The comparison between the predicted execution time and the measured execution time on Intel Xeon Phi for the large CNN architecture.\nWe use the expression x = |m\u2212 p|\np to calculate the deviation in predictions for\nour prediction model and all considered architectures, where m is the measured and p is the predicted value. The average deviations over all measured thread counts are as follows: 14.57% for the small CNN, 14.76% for medium, and 15.36% for large CNN.\nResult 6: Prediction of execution time for number of threads that go beyond the 240 hardware threads of the model of Intel Xeon Phi used in this paper show that CHAOS scales well up to several thousands of threads.\nWe used the prediction model to predict the execution times for 480, 960, 1920, and 3840 threads for different CNN architectures, using the same parameters. The results in Table 8 show that if 3,840 threads were available, the small network should take about 4.6 minutes to train, the medium 14.5 minutes and the large 36.8 minutes. The predictions for the large CNN architecture are not as well aligned when increasing to larger thread counts as for small and medium.\nAdditionally, we evaluated the execution time for varying image counts, and epochs, for 240 and 480 threads for the small CNN architecture. As can be seen in Table 9 doubling the number of images or epochs, approximately\ndoubles the execution time. However, doubling the number of threads does not reduce the execution time in half."}, {"heading": "6 Summary and Future Work", "text": "Deep learning is important for many modern applications, such as, voice recognition, face recognition, autonomous cars, precision medicine, or computer vision. We have presented CHAOS that is a parallelization scheme to speed up the training process of Convolutional Neural Networks. CHAOS can exploit both thread- and SIMD-parallelism of Intel Xeon Phi co-processor. Moreover, we have described our performance prediction model, which we use to evaluate our parallelization solution and infer the performance on future architectures of the Intel Xeon Phi. Major observations include,\n\u2013 CHAOS parallel implementation scales well with the increase of the number of threads; \u2013 convolutional layers are the most computationally expensive part of the CNN training effort; for instance, for 240 threads, 88% of the time is spent on the back-propagation of convolutional layers; \u2013 using CHAOS for training CNNs on Intel Xeon Phi we achieved up to 103\u00d7, 14\u00d7, and 58\u00d7 speedup compared to the single-thread performance on Intel Xeon Phi, Intel Xeon E5 CPU, and Intel Core i5, respectively; \u2013 image classification accuracy of CHAOS parallel implementation is comparable to the one running sequentially; \u2013 predicted execution times values obtained from our performance model match well the measured execution times; \u2013 results of the performance model indicate that CHAOS scales well beyond the 240 hardware threads of the Intel Xeon Phi that is used in this paper for experimentation.\nFuture work will extend CHAOS to enable the use of all cores of host CPUs and the co-processor(s)."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems (2015)", "author": ["M Abadi"], "venue": "URL http://tensorflow.org/. Software available from tensorflow.org", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Accelerating pattern matching in neuromorphic text recognition system using intel xeon phi coprocessor", "author": ["K. Ahmed", "Q. Qiu", "P. Malani", "M. Tamhankar"], "venue": "Neural Networks (IJCNN), 2014 International Joint Conference on, pp. 4272\u20134279. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Ufldl tutorial on neural networks", "author": ["N. Andrew", "N. Jiquan", "F. Chuan Yu", "M. Yifan", "S. Caroline"], "venue": "Ufldl Tutorial on Neural Networks", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparative study of deep learning software frameworks", "author": ["S. Bahrampour", "N. Ramakrishnan", "L. Schott", "M. Shah"], "venue": "arXiv preprint arXiv:1511.06435", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "PEPPHER: Efficient and Productive Usage of Hybrid Computing Systems", "author": ["S. Benkner", "S. Pllana", "J. Traff", "P. Tsigas", "U. Dolinsky", "C. Augonnet", "B. Bachmayer", "C. Kessler", "D. Moloney", "V. Osipov"], "venue": "Micro, IEEE 31(5), 28\u201341", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Libsvm: a library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST) 2(3), 27", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "High performance convolutional neural networks for document processing", "author": ["K. Chellapilla", "S. Puri", "P. Simard"], "venue": "Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Intel\u00ae Xeon Phi Coprocessor-the Architecture", "author": ["G. Chrysos"], "venue": " Intel Whitepaper", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Simple C/C++ code for training and testing MLPs and CNNs", "author": ["D. Cire\u015fan"], "venue": "http: //people.idsia.ch/~{}ciresan/data/net.zip", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "A committee of neural networks for traffic sign classification", "author": ["D. Cire\u015fan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Neural Networks (IJCNN), The 2011 International Joint Conference on, pp. 1918\u20131921. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-column deep neural network for traffic sign classification", "author": ["D. Cire\u015fan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Neural Networks 32, 333\u2013338", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Highperformance neural networks for visual object classification", "author": ["D.C. Cire\u015fan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1102.0183", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallelization of deep networks", "author": ["M.D.F. De Grazia", "I. Stoianov", "M. Zorzi"], "venue": "ESANN", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning: Methods and applications", "author": ["L. Deng", "D. Yu"], "venue": "Foundations and Trends in Signal Processing 7(3\u20134), 197\u2013387", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Highlevel support for hybrid parallel execution of c++ applications targeting intel xeon phi coprocessors", "author": ["J. Dokulil", "E. Bajrovic", "S. Benkner", "S. Pllana", "M. Sandrieser", "B. Bachmayer"], "venue": "Procedia Computer Science 18(0), 2508 \u2013 2511", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards an understanding of facets and exemplars of big data applications", "author": ["G.C. Fox", "S. Jha", "J. Qiu", "A. Luckow"], "venue": "Proceedings of the 20 Years of Beowulf Workshop on Honor of Thomas Sterling\u2019s 65th Birthday, Beowulf \u201914, pp. 7\u201316. ACM, New York, NY, USA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully connected neural network algorithms", "author": ["A. Gibansky"], "venue": "http://andrew.gibiansky. com/blog/machine-learning/fully-connected-neural-networks/", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning long-range vision for autonomous off-road driving", "author": ["R. Hadsell", "P. Sermanet", "J. Ben", "A. Erkan", "M. Scoffier", "K. Kavukcuoglu", "U. Muller", "Y. LeCun"], "venue": "Journal of Field Robotics 26(2), 120\u2013144", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Editorial", "author": ["C.H. Hsu"], "venue": "Future Generation Computer Systems 36(Complete), 16\u201318", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "An intelligent information forwarder for healthcare big data systems with distributed wearable sensors", "author": ["P. Jiang", "J. Winkley", "C. Zhao", "R. Munnoch", "G. Min", "L.T. Yang"], "venue": "IEEE Systems Journal 10(3), 1147\u20131159", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Training large scale deep neural networks on the intel xeon phi many-core coprocessor", "author": ["L. Jin", "Z. Wang", "R. Gu", "C. Yuan", "Y. Huang"], "venue": "IPDPS Workshops, pp. 1622\u20131630. IEEE Computer Society", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Programmability and performance portability aspects of heterogeneous multi-/manycore systems", "author": ["C.W. Kessler", "U. Dastgeer", "S. Thibault", "R. Namyst", "A. Richards", "U. Dolinsky", "S. Benkner", "J.L. Trff", "S. Pllana"], "venue": "2012 Design, Automation Test in Europe Conference Exhibition (DATE), pp. 1403\u20131408. IEEE, Dresden, Germany", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["A. Krizhevsky"], "venue": "CoRR abs/1404.5997", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pp. 1097\u20131105", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature 521(7553), 436\u2013444", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11), 2278\u20132324", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Mnist handwritten digit database", "author": ["Y. LeCun", "C. Cortes"], "venue": "AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, vol. 2, pp. II\u201397. IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Investigating large-scale feature matching using the intel\u00ae xeon phi coprocessor", "author": ["K.C. Leung", "D. Eyers", "X. Tang", "S. Mills", "Z. Huang"], "venue": "Image and Vision Computing New Zealand (IVCNZ), 2013 28th International Conference of, pp. 148\u2013153. IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimizing the mapreduce framework on intel xeon phi coprocessor", "author": ["M. Lu", "L. Zhang", "H.P. Huynh", "Z. Ong", "Y. Liang", "B. He", "R.S.M. Goh", "R. Huynh"], "venue": "Big Data, 2013 IEEE International Conference on, pp. 125\u2013130. IEEE", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Combinatorial optimization of dna sequence analysis on heterogeneous systems", "author": ["S. Memeti", "S. Pllana"], "venue": "Concurrency and Computation: Practice and Experience", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "A machine learning approach for accelerating dna sequence analysis", "author": ["S. Memeti", "S. Pllana"], "venue": "The International Journal of High Performance Computing Applications", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Learning Frameworks: A Survey of TensorFlow, Torch, Theano, Caffe, Neon, and the IBM Machine Learning Stack", "author": ["J. Murphy"], "venue": " https://www.microway.com/hpc-tech-tips/ deep-learning-frameworks-survey-tensorflow-torch-theano-caffe-neon-ibm-machine-learning-stack/", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning applications and challenges in big data analytics", "author": ["M.M. Najafabadi", "F. Villanustre", "T.M. Khoshgoftaar", "N. Seliya", "R. Wald", "E. Muharemagic"], "venue": "Journal of Big Data 2(1), 1", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards an intelligent environment for programming multi-core computing systems", "author": ["S. Pllana", "S. Benkner", "E. Mehofer", "L. Natvig", "F. Xhafa"], "venue": "Euro-Par 2008 Workshops - Parallel Processing, Lecture Notes in Computer Science, vol. 5415, pp. 141\u2013151. Springer Berlin Heidelberg", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Hybrid performance modeling and prediction of large-scale computing systems", "author": ["S. Pllana", "S. Benkner", "F. Xhafa", "L. Barolli"], "venue": "2008 International Conference on Complex, Intelligent and Software Intensive Systems, pp. 132\u2013138", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "Advances in Neural Information Processing Systems, pp. 693\u2013701", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep convolutional neural networks for large-scale speech tasks", "author": ["T.N. Sainath", "B. Kingsbury", "G. Saon", "H. Soltau", "Mohamed", "A.r.", "G. Dahl", "B. Ramabhadran"], "venue": "Neural Networks 64, 39\u201348", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerating large-scale convolutional neural networks with parallel graphics multiprocessors", "author": ["D. Scherer", "H. Schulz", "S. Behnke"], "venue": "Artificial Neural Networks\u2013ICANN 2010, pp. 82\u201391. Springer", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61, 85\u2013117", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "A brief review on leading big data models", "author": ["S. Sharma", "U.S. Tim", "J. Wong", "S. Gadia", "S. Sharma"], "venue": "Data Science Journal 13, 138\u2013157", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Benchmarking state-of-the-art deep learning software tools", "author": ["S. Shi", "Q. Wang", "P. Xu", "X. Chu"], "venue": "arXiv preprint arXiv:1608.07249", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning for real-time robust facial expression recognition on a smartphone", "author": ["I. Song", "H.J. Kim", "P.B. Jeon"], "venue": "Consumer Electronics (ICCE), 2014 IEEE International Conference on, pp. 564\u2013567. IEEE", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Data scientist", "author": ["G. Strawn"], "venue": "IT Professional 18(3), 55\u201357", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20139", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "A snapshot of image pre-processing for convolutional neural networks: case study of mnist", "author": ["S. Tabik", "D. Peralta", "Herrera", "A.H.P.F."], "venue": "International Journal of Computational Intelligence Systems 10, 555\u2013568", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2017}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1701\u20131708", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparative performance analysis of intel xeon phi, gpu, and cpu", "author": ["G. Teodoro", "T. Kurc", "J. Kong", "L. Cooper", "J. Saltz"], "venue": "arXiv preprint arXiv:1311.0378", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Practical SIMD Vectorization Techniques for Intel Xeon Phi Coprocessors", "author": ["X. Tian", "H. Saito", "S. Preis", "E.N. Garcia", "S. Kozhukhov", "M. Masten", "A.G. Cherkasov", "N. Panchenko"], "venue": "IPDPS Workshops, pp. 1149\u20131158. IEEE", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Pattern recognition with opencl heterogeneous platform", "author": ["J. Vrtanoski", "T.D. Stojanovski"], "venue": "Telecommunications Forum (TELFOR), 2012 20th, pp. 701\u2013704. IEEE", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Siri Will Soon Understand You a Whole Lot Better \u2014 Wired", "author": ["A. Washburn"], "venue": "http: //www.wired.com/2014/06/siri_ai/", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Roofline: An insightful visual performance model for multicore architectures", "author": ["S. Williams", "A. Waterman", "D. Patterson"], "venue": "Commun. ACM 52(4), 65\u201376", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning based classification of focal liver lesions with contrast-enhanced ultrasound", "author": ["K. Wu", "X. Chen", "M. Ding"], "venue": "Optik-International Journal for Light and Electron Optics 125(15), 4057\u20134063", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-gpu training of convnets", "author": ["O. Yadan", "K. Adams", "Y. Taigman", "M. Ranzato"], "venue": "arXiv preprint arXiv:1312.5853 9", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "Mic-svm: Designing a highly efficient support vector machine for advanced modern multi-core and many-core architectures", "author": ["Y. You", "S.L. Song", "H. Fu", "A. Marquez", "M.M. Dehnavi", "K. Barker", "K.W. Cameron", "A.P. Randles", "G. Yang"], "venue": "Parallel and Distributed Processing Symposium, 2014 IEEE 28th International, pp. 809\u2013818. IEEE", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Research of Neural Network Classifier Based on FCM and PSO for Breast Cancer Classification, pp", "author": ["L. Zhang", "L. Wang", "X. Wang", "K. Liu", "A. Abraham"], "venue": "647\u2013654. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2012}, {"title": "Slow learners are fast", "author": ["M. Zinkevich", "A.J. Smola", "J. Langford"], "venue": "Y. Bengio, D. Schuurmans, J.D. Lafferty, C.K.I. Williams, A. Culotta (eds.) NIPS, pp. 2331\u20132339. Curran Associates, Inc.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 25, "context": "Moreover, deep learning algorithms [27] can learn from their own experience rather than that of the engineer.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "With the pervasiveness of the Internet of Things the amount of available data is getting much larger [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 44, "context": "Deep learning is a useful tool for analyzing and learning from massive amounts of data (also known as Big Data) that may be unlabeled and unstructured [47,44,36].", "startOffset": 151, "endOffset": 161}, {"referenceID": 41, "context": "Deep learning is a useful tool for analyzing and learning from massive amounts of data (also known as Big Data) that may be unlabeled and unstructured [47,44,36].", "startOffset": 151, "endOffset": 161}, {"referenceID": 34, "context": "Deep learning is a useful tool for analyzing and learning from massive amounts of data (also known as Big Data) that may be unlabeled and unstructured [47,44,36].", "startOffset": 151, "endOffset": 161}, {"referenceID": 51, "context": "Deep learning algorithms can be found in many modern applications [54,50,19,56,48,17,21,59], such as, voice recognition, face recognition, autonomous cars, classification of liver diseases and breast cancer, computer vision, or social media.", "startOffset": 66, "endOffset": 91}, {"referenceID": 47, "context": "Deep learning algorithms can be found in many modern applications [54,50,19,56,48,17,21,59], such as, voice recognition, face recognition, autonomous cars, classification of liver diseases and breast cancer, computer vision, or social media.", "startOffset": 66, "endOffset": 91}, {"referenceID": 17, "context": "Deep learning algorithms can be found in many modern applications [54,50,19,56,48,17,21,59], such as, voice recognition, face recognition, autonomous cars, classification of liver diseases and breast cancer, computer vision, or social media.", "startOffset": 66, "endOffset": 91}, {"referenceID": 53, "context": "Deep learning algorithms can be found in many modern applications [54,50,19,56,48,17,21,59], such as, voice recognition, face recognition, autonomous cars, classification of liver diseases and breast cancer, computer vision, or social media.", "startOffset": 66, "endOffset": 91}, {"referenceID": 45, "context": "Deep learning algorithms can be found in many modern applications [54,50,19,56,48,17,21,59], such as, voice recognition, face recognition, autonomous cars, classification of liver diseases and breast cancer, computer vision, or social media.", "startOffset": 66, "endOffset": 91}, {"referenceID": 15, "context": "Deep learning algorithms can be found in many modern applications [54,50,19,56,48,17,21,59], such as, voice recognition, face recognition, autonomous cars, classification of liver diseases and breast cancer, computer vision, or social media.", "startOffset": 66, "endOffset": 91}, {"referenceID": 19, "context": "Deep learning algorithms can be found in many modern applications [54,50,19,56,48,17,21,59], such as, voice recognition, face recognition, autonomous cars, classification of liver diseases and breast cancer, computer vision, or social media.", "startOffset": 66, "endOffset": 91}, {"referenceID": 56, "context": "Deep learning algorithms can be found in many modern applications [54,50,19,56,48,17,21,59], such as, voice recognition, face recognition, autonomous cars, classification of liver diseases and breast cancer, computer vision, or social media.", "startOffset": 66, "endOffset": 91}, {"referenceID": 13, "context": "Inspired by the visual cortex of animals, CNNs are applied to state-of-the-art applications, including computer vision and speech recognition [15].", "startOffset": 142, "endOffset": 146}, {"referenceID": 52, "context": "Multi-core processors [55] and in particular many-core [5] processing architectures, such as the NVIDIA Graphical Processing Unit (GPU) [37] or the Intel Xeon Phi [8] co-processor, provide processing capabilities that may be used to significantly speed-up the training of CNNs.", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": "Multi-core processors [55] and in particular many-core [5] processing architectures, such as the NVIDIA Graphical Processing Unit (GPU) [37] or the Intel Xeon Phi [8] co-processor, provide processing capabilities that may be used to significantly speed-up the training of CNNs.", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "Multi-core processors [55] and in particular many-core [5] processing architectures, such as the NVIDIA Graphical Processing Unit (GPU) [37] or the Intel Xeon Phi [8] co-processor, provide processing capabilities that may be used to significantly speed-up the training of CNNs.", "startOffset": 163, "endOffset": 166}, {"referenceID": 11, "context": "While existing research [12,53,48,57,41] has addressed extensively the training of CNNs using GPUs, so far not much attention is given to the Intel Xeon Phi co-processor.", "startOffset": 24, "endOffset": 40}, {"referenceID": 50, "context": "While existing research [12,53,48,57,41] has addressed extensively the training of CNNs using GPUs, so far not much attention is given to the Intel Xeon Phi co-processor.", "startOffset": 24, "endOffset": 40}, {"referenceID": 45, "context": "While existing research [12,53,48,57,41] has addressed extensively the training of CNNs using GPUs, so far not much attention is given to the Intel Xeon Phi co-processor.", "startOffset": 24, "endOffset": 40}, {"referenceID": 54, "context": "While existing research [12,53,48,57,41] has addressed extensively the training of CNNs using GPUs, so far not much attention is given to the Intel Xeon Phi co-processor.", "startOffset": 24, "endOffset": 40}, {"referenceID": 38, "context": "While existing research [12,53,48,57,41] has addressed extensively the training of CNNs using GPUs, so far not much attention is given to the Intel Xeon Phi co-processor.", "startOffset": 24, "endOffset": 40}, {"referenceID": 35, "context": "Beside the performance capabilities, the Xeon Phi deserves our attention because of programmability [38] and portability [23].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Beside the performance capabilities, the Xeon Phi deserves our attention because of programmability [38] and portability [23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "For experimentation, we use various number of threads, different CNNs architectures, and the MNIST dataset of handwritten digits [29].", "startOffset": 129, "endOffset": 133}, {"referenceID": 55, "context": "[58] present a library for parallel Support Vector Machines, MICSVM, which facilitates the use of SVMs on many- and multi-core architectures including Intel Xeon Phi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Experiments performed on several known datasets showed up to 84x speed up on the Intel Xeon Phi compared to the sequential execution of LIBSVM [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 20, "context": "[22] perform the training of sparse auto encoders and restricted Boltzmann machines on the Intel Xeon Phi 5110p.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "in [2].", "startOffset": 3, "endOffset": 6}, {"referenceID": 27, "context": "Work related to MNIST [29] dataset is of most interest, also NORB [30] and CIFAR 10 [25] is considered.", "startOffset": 22, "endOffset": 26}, {"referenceID": 28, "context": "Work related to MNIST [29] dataset is of most interest, also NORB [30] and CIFAR 10 [25] is considered.", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "Work related to MNIST [29] dataset is of most interest, also NORB [30] and CIFAR 10 [25] is considered.", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "[12] target a CNN implementation raising the bars for the CIFAR10 (19.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] presented their multi-column deep neural network for classification of traffic sings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[53] use OpenCL for parallelization of the back-propagation algorithm for pattern recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] joined the challenge and reduced the error rate of the test set to 15.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "The authors state that the network could be trained on GPUs within a week, illuminating the limited amount of memory to be one of the major concerns [48].", "startOffset": 149, "endOffset": 153}, {"referenceID": 54, "context": "[57] used multiple GPUs to train CNNs on the ImageNet dataset using both data- and model-parallelism, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[46] constructed a CNN to recognize face expressions and developed a smart-phone app in which the user can capture a picture and send it to a server hosting the network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[42] accelerated the large-scale neural networks with parallel GPUs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] combined multiple CNNs to classify German traffic signs and achieved a 99.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] presented TensorFlow, a system for expressing and executing machine learning algorithms including training deep neural network models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "[41] investigated the advantages of CNNs performing speech recognition tasks and compared the results with previous DNN approaches.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] investigated GPUs (Nvidia Geforce 7800 Ultra) for document processing on the MNIST dataset and achieved a 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "While there are several review papers (such as, [4,45,49]) and on-line articles (such as, [35]) that compare existing frameworks for parallelization of training CNN architectures, we focus on detailed analysis of our proposed parallelization approach using measurement techniques and performance modeling.", "startOffset": 48, "endOffset": 57}, {"referenceID": 42, "context": "While there are several review papers (such as, [4,45,49]) and on-line articles (such as, [35]) that compare existing frameworks for parallelization of training CNN architectures, we focus on detailed analysis of our proposed parallelization approach using measurement techniques and performance modeling.", "startOffset": 48, "endOffset": 57}, {"referenceID": 46, "context": "While there are several review papers (such as, [4,45,49]) and on-line articles (such as, [35]) that compare existing frameworks for parallelization of training CNN architectures, we focus on detailed analysis of our proposed parallelization approach using measurement techniques and performance modeling.", "startOffset": 48, "endOffset": 57}, {"referenceID": 33, "context": "While there are several review papers (such as, [4,45,49]) and on-line articles (such as, [35]) that compare existing frameworks for parallelization of training CNN architectures, we focus on detailed analysis of our proposed parallelization approach using measurement techniques and performance modeling.", "startOffset": 90, "endOffset": 94}, {"referenceID": 2, "context": "The resulting weight parameters can thereafter be used to make predictions of unseen inputs [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "For example, in image classification problems, the output layer contains the prediction score that indicates the likelihood that an image belongs to a category [18,3].", "startOffset": 160, "endOffset": 166}, {"referenceID": 2, "context": "For example, in image classification problems, the output layer contains the prediction score that indicates the likelihood that an image belongs to a category [18,3].", "startOffset": 160, "endOffset": 166}, {"referenceID": 0, "context": "The goal of the activation function is to return a normalized value (sigmoid return [0,1] and tanh is used in cases where the desired return values are [-1,1]).", "startOffset": 84, "endOffset": 89}, {"referenceID": 0, "context": "The goal of the activation function is to return a normalized value (sigmoid return [0,1] and tanh is used in cases where the desired return values are [-1,1]).", "startOffset": 152, "endOffset": 158}, {"referenceID": 16, "context": "More concretely, the algorithm can be thought of as updating the layer's weights based on \u201dhow much it was responsible for the errors in the output\u201d [18,3].", "startOffset": 149, "endOffset": 155}, {"referenceID": 2, "context": "More concretely, the algorithm can be thought of as updating the layer's weights based on \u201dhow much it was responsible for the errors in the output\u201d [18,3].", "startOffset": 149, "endOffset": 155}, {"referenceID": 40, "context": "A Convolutional Neural Network is a multi-layer model constructed to learn various levels of representations where higher level representations are described based on the lower level ones [43].", "startOffset": 188, "endOffset": 192}, {"referenceID": 26, "context": "Besides from lowering the computational load, it also enables position invariance and down samples the input by a factor relative to the kernel size [28].", "startOffset": 149, "endOffset": 153}, {"referenceID": 26, "context": "CNNs are commonly constructed similarly to the LeNet-5, beginning with an input layer, followed by several convolutional/pooling combinations, ending with a fully connected layer and an output layer [28].", "startOffset": 199, "endOffset": 203}, {"referenceID": 45, "context": "Recent networks are much deeper and/or wider, for instance, the GoogleNet [48] consists of 22 layers.", "startOffset": 74, "endOffset": 78}, {"referenceID": 8, "context": "As a basis for our work we selected a project developed by Cire\u015fan [9].", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "3GHz on max turbo frequency [8].", "startOffset": 28, "endOffset": 31}, {"referenceID": 49, "context": "Efficient usage of the available vector processing units of the Intel Xeon Phi is essential to fully utilize the performance of the co-processor [52].", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "The performance capabilities of the Intel Xeon Phi are discussed and investigated empirically by different researches within several domain applications [16,32,34,51,31,33].", "startOffset": 153, "endOffset": 172}, {"referenceID": 30, "context": "The performance capabilities of the Intel Xeon Phi are discussed and investigated empirically by different researches within several domain applications [16,32,34,51,31,33].", "startOffset": 153, "endOffset": 172}, {"referenceID": 32, "context": "The performance capabilities of the Intel Xeon Phi are discussed and investigated empirically by different researches within several domain applications [16,32,34,51,31,33].", "startOffset": 153, "endOffset": 172}, {"referenceID": 48, "context": "The performance capabilities of the Intel Xeon Phi are discussed and investigated empirically by different researches within several domain applications [16,32,34,51,31,33].", "startOffset": 153, "endOffset": 172}, {"referenceID": 29, "context": "The performance capabilities of the Intel Xeon Phi are discussed and investigated empirically by different researches within several domain applications [16,32,34,51,31,33].", "startOffset": 153, "endOffset": 172}, {"referenceID": 31, "context": "The performance capabilities of the Intel Xeon Phi are discussed and investigated empirically by different researches within several domain applications [16,32,34,51,31,33].", "startOffset": 153, "endOffset": 172}, {"referenceID": 22, "context": "We consider different existing parallelization strategies for stochastic gradient descent: Strategy A: Hybrid - uses both data- and model parallelism, such that data parallelism is applied in convolutional layers, and the model parallelism is applied in fully connected layers [24].", "startOffset": 277, "endOffset": 281}, {"referenceID": 12, "context": "This strategy proceeds as follows: (1) Initialize the weights of the learner by randomization; (2) Split the training data into n equal chunks and send them to the learners; (3) each learner process the data and calculates the weight gradients for its batch; (4) send the calculated gradients back to the master; (5) the master computes and updates the new weights; and (6) the master sends the new weights to the nodes and a new iteration begins [13].", "startOffset": 447, "endOffset": 451}, {"referenceID": 57, "context": "Threads are only allowed to update the weight vector in a round-robin fashion, and hence each update will be delayed [60].", "startOffset": 117, "endOffset": 121}, {"referenceID": 37, "context": "The approach is applicable for sparse optimization problems (threads/core updates do not conflict much) [40].", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "Adoption of data parallelism was inspired by Krizhevsky [24], promoting data parallelism for convolutional layers as they are computationally intensive.", "startOffset": 56, "endOffset": 60}, {"referenceID": 57, "context": "\u2013 Controlled HogWild - during the back-propagation the shared weights are updated after each layer's computations (a technique inspired by [60]), whereas the local weight parameters are updated instantly (a technique inspired by [40]), which means that the gradients are calculated locally first then shared with other workers.", "startOffset": 139, "endOffset": 143}, {"referenceID": 37, "context": "\u2013 Controlled HogWild - during the back-propagation the shared weights are updated after each layer's computations (a technique inspired by [60]), whereas the local weight parameters are updated instantly (a technique inspired by [40]), which means that the gradients are calculated locally first then shared with other workers.", "startOffset": 229, "endOffset": 233}, {"referenceID": 27, "context": "Data Set - To evaluate our approach, the MNIST [29] dataset of handwritten digits is used.", "startOffset": 47, "endOffset": 51}, {"referenceID": 36, "context": "A performance model [39] enables us to reason about the behavior of an implementation in future execution contexts.", "startOffset": 20, "endOffset": 24}], "year": 2017, "abstractText": "Deep learning is an important component of big-data analytic tools and intelligent applications, such as, self-driving cars, computer vision, speech recognition, or precision medicine. However, the training process is computationally intensive, and often requires a large amount of time if performed sequentially. Modern parallel computing systems provide the capability to reduce the required training time of deep neural networks. In this paper, we present our parallelization scheme for training convolutional neural networks (CNN) named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS). Major features of CHAOS include the support for thread and vector parallelism, non-instant updates of weight parameters during backpropagation without a significant delay, and implicit synchronization in arbitrary order. CHAOS is tailored for parallel computing systems that are accelerated with the Intel Xeon Phi. We evaluate our parallelization approach empirically using measurement techniques and performance modeling for various numbers of threads and CNN architectures. Experimental results for the MNIST dataset of handwritten digits using the total number of threads on the Xeon Phi show speedups of up to 103\u00d7 compared to the execution on one thread of the Xeon Phi, 14\u00d7 compared to the sequential execution on Intel Xeon E5, and 58\u00d7 compared to the sequential execution on Intel Core i5.", "creator": "LaTeX with hyperref package"}}}