{"id": "1702.07966", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2017", "title": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs", "abstract": "Deep conscientious learning steblin models 71.22 are owhali often successfully breakfront trained using esperanca gradient descent, despite the muhtadi worst afecta case hardness of carburetors the augustina underlying kats non - unik convex optimization problem. The key question unprimed is 6-4-3 then under bertsch what conditions 7.22 can one barlee prove 36.23 that codacons optimization laub will hronom succeed. reznikoff Here we skwentna provide kafelnikov a mainzer strong result of this davar kind. We consider 4,360 a neural net with longlasting one tatsha hidden vereniki layer and a apoteket convolutional gopalpur structure highmore with no overlap and ineos a horsehide ReLU clotfelter activation function. For chengannur this m1903 architecture 57.48 we 66-2 show that bastions learning is NP - rabie complete in 59.15 the general case, but that districts when sparrer the input saddle-tank distribution gow is km. Gaussian, 1.745 gradient arvo descent visioneer converges to sirhowy the global ayako optimum tali in polynomial coulsdon time. dunsky To rosebery the best of faruj our waterway knowledge, this katu is the escrito first d'arby global thurible optimality guarantee of protectorates gradient duelled descent on a beaupre convolutional neural network with 3,299 ReLU activations.", "histories": [["v1", "Sun, 26 Feb 2017 01:12:20 GMT  (337kb,D)", "http://arxiv.org/abs/1702.07966v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["alon brutzkus", "amir globerson"], "accepted": true, "id": "1702.07966"}, "pdf": {"name": "1702.07966.pdf", "metadata": {"source": "CRF", "title": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs", "authors": ["Alon Brutzkus", "Amir Globerson"], "emails": ["alonbrutzkus@mail.tau.ac.il", "gamir@cs.tau.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks have achieved state-of-the-art performance on many machine learning tasks in areas such as natural language processing (Wu et al., 2016), computer vision (Krizhevsky et al., 2012) and speech recognition (Hinton et al., 2012). Training of such networks is often successfully performed by minimizing a high-dimensional non-convex objective function, using simple first-order methods such as stochastic gradient descent.\nNonetheless, the success of deep learning from an optimization perspective is poorly understood theoretically. Current results are mostly pessimistic, suggesting that even training a 3-node neural network is NP-hard (Blum & Rivest, 1993), and that the objective function of a single neuron can admit exponentially many local minima (Auer et al., 1996; Safran & Shamir, 2016). There have been recent attempts to bridge this gap between theory and practice. Several works focus on the geometric properties of loss functions that neural networks attempt to minimize. For some simplified architectures, such as linear activations, it can be shown that there are no bad local minima (Kawaguchi, 2016). Extension of these results to the non-linear case currently requires very strong independence assumptions (Kawaguchi, 2016).\nSince gradient descent is the main \u201cwork-horse\u201d of deep learning it is of key interest to understand its convergence properties. However, there are no results showing that gradient descent is globally optimal for non-linear models, except for the case of many hidden neurons (Andoni et al., 2014) and non-linear activation functions that are not widely used in practice (Zhang et al., 2017).1 Here we provide the first such result for a neural architecture that has two very common components: namely a ReLU activation function and a convolution layer.\nThe architecture considered in the current paper is shown in Figure 1. We refer to these models as no-overlap networks. A no-overlap network can be viewed as a simple convolution layer with non overlapping filters, followed by a ReLU activation function, and then average pooling. Formally, let w \u2208 Rm denote the filter coefficient, and assume the input x is in Rd. Define k = m/d and assume for simplicity that k is integral. Partition x into k non-overlapping parts and denote x[i] the ith part.\n1See more related work in Section 2.\nar X\niv :1\n70 2.\n07 96\n6v 1\n[ cs\n.L G\n] 2\n6 Fe\nb 20\n17\nFinally, define \u03c3 to be the ReLU activation function, namely \u03c3 (z) = max{0, z}. Then the output of the network in Figure 1 is given by:\nf(x;w) = 1\nk \u2211 i \u03c3 (w \u00b7 x[i]) (1)\nWe note that such architectures have been used in several works (Lin et al., 2013; Milletari et al., 2016), but we view them as important firstly because they capture key properties of general convolutional networks.\nWe address the realizable case, where training data is generated from a function as in Eq. 1 with weight vector w\u2217. Training data is then generated by sampling n training points x1, . . . ,xn from a distribution D, and assigning them labels using y = f(x;w\u2217). The learning problem is then to find a w that minimizes the squared loss. In other words, solve the optimization problem:\nmin w\n1\nn \u2211 i (f(xi;w)\u2212 yi)2 (2)\nIn the limit n\u2192\u221e, this is equivalent to minimizing the population risk: `(w) = Ex\u223cD [ (f(x;w)\u2212 f(x;w\u2217))2 ] (3)\nLike several recent works (Hardt et al., 2016; Hardt & Ma, 2016) we focus on minimizing the population risk, leaving the finite sample case to future work. We believe the population risk captures the key characteristics of the problem, since the large data regime is the one of interest.\nOur key results are as follows:\n\u2022 Worst Case Hardness: Despite the simplicity of No-Overlap Networks, we show that learning them is in fact hard if D is unconstrained. Specifically, in Section 4, we show that learning No-Overlap Networks is NP complete via a reduction from a variant of the set splitting problem.\n\u2022 Distribution Dependent Tractability: When D corresponds to independent Gaussian variables with \u00b5 = 0, \u03c32 = 1, we show in Section 5 that No-Overlap Networks can be learned in polynomial time using gradient descent.\nThe above two results nicely demonstrate the gap between worst-case intractability and tractability under assumptions on the data. We provide an empirical demonstration of this in Section 6 where gradient descent is shown to succeed on the Gaussian case and fail for a different distribution.\nTo further understand the role of overlap in the network, we consider networks that do have overlap between the filters. In Section 7.1 we show that in this case, even under Gaussian distributed inputs, there will be non-optimal local minima. Thus, gradient descent will no longer be optimal in the overlap case. In Section 7.2 we show empirically that these local optima may be overcome in practice by using gradient descent with multiple restarts.\nTaken together, our results are the first to demonstrate distribution dependent optimality of gradient descent for learning a neural architecture with a convolutional like architecture and a ReLU activation function."}, {"heading": "2 Related Work", "text": "Hardness of learning neural networks has been demonstrated for many different settings. For example, (Blum & Rivest, 1993) show that learning a neural network with one hidden layer with a sign activation function is NP-hard in the realizable case. (Livni et al., 2014) extend this to other activation functions and bounded norm optimization. Hardness can also be shown for improper learning under certain cryptographic assumptions (e.g., see Daniely et al., 2014; Klivans, 2008; Livni et al., 2014). Note that these hardness results do not hold for the regression and tied parameter setting that we consider.\nDue to the above hardness results, it is clear that the success of deep-learning can only be explained by making additional assumptions about the data generating distribution. The classic algorithm by (Baum, 1990) shows that intersection of halfspaces (i.e., a specific instance of a one hidden layer network) is PAC learnable under any symmetric distribution. This was later extended in (Klivans et al., 2009) to log-concave distributions.\nThe above works do not consider gradient descent as the optimization method, leaving open the question of which assumptions can lead to global optimality of gradient descent. Such results have been hard to obtain, and we survey some recent ones below. One instance when gradient descent can succeed is when there are enough hidden units such that random initialization of the first layer can lead to zero error even if only the second layer is trained. Such over-specified networks have been considered in (Andoni et al., 2014; Livni et al., 2014) and it was shown that gradient descent can globally learn them in some cases (Andoni et al., 2014). However, the assumption of over-specification is very restrictive and limits generalization. In contrast, we show convergence of gradient descent to a global optimum for any network size and consider convolutional neural networks with shared parameters. Another interesting case is linear dynamical systems, where (Hardt et al., 2016) show that under independence assumptions maximum likelihood is quasi-concave and hence solvable with gradient ascent.\nRecent work by (Mei et al., 2016) shows that regression with a single neuron and certain non-linear activation functions, can be learned with gradient descent for sub-Gaussian inputs. We note that their architecture is significantly simpler than ours, in that it uses a single neuron. In fact, their regression problem can also be solved via methods for generalized linear models such as (Kakade et al., 2011).\n(Shamir, 2016) recently showed that there is a limit to what distribution dependent results can achieve. Namely, it was shown that for large enough one-hidden layer networks, no distributional assumptions can make gradient descent tractable. Importantly, the construction in (Shamir, 2016) does not use parameter tying and thus is not applicable to the architecture we study here.\nSeveral works have focused on understanding the loss surface of neural network objectives, but without direct algorithmic implications. (Kawaguchi, 2016) show that linear neural networks do not suffer from bad local minima. (Hardt & Ma, 2016) consider objectives of linear residual networks and prove that there are no critical points other than the global optimum. (Soudry & Carmon, 2016) show that in the objective of over-parameterized neural networks with dropout-like noise, all differentiable local minima are global. Other works (Safran & Shamir, 2016; Haeffele & Vidal, 2015) give similar results for over-specified networks. All of these results are purely geometric and do not have direct implications on convergence of optimization algorithms. In a different approach, (Janzamin et al., 2015), suggest alternatives to gradient-based methods for learning neural networks. However, these algorithms are not widely used in practice. Finally, (Choromanska et al., 2015) use spin glass models to argue that, under certain generative modelling and architectural constraints, local minima are likely to have low loss values.\nThe theory of non-convex optimization is closely related to the theory of neural networks. Recently, there has been substantial progress in proving convergence guarantees of simple first-order methods in various machine learning problems, that don\u2019t correspond to typical neural nets. These include for example matrix completion (Ge et al., 2016) and tensor decompositions (Ge et al., 2015).\nFinally, recent work by (Zhang et al., 2016) shows that neural nets can perfectly fit random labelings of the data. Understanding this from an optimization perspective is largely an open problem."}, {"heading": "3 Preliminaries", "text": "We use bold-faced letters for vectors and capital letters for matrices. The ith row of a matrix A is denoted by ai.\nIn our analysis in Section 5 and Section 7.1 we assume that the input feature x \u2208 Rd is a vector of IID Gaussian random variables with zero mean and variance one.2 Denote this distribution by G. We consider networks with one hidden layer, and k hidden units. Our main focus will be on No-Overlap Networks, but we begin with a more general one-hidden-layer neural network with a fully-connected layer parameterized by W \u2208 Rk,d followed by average pooling. The network output is then:\nf(x;W ) = 1\nk \u2211 i \u03c3 (wi \u00b7 x) (4)\nwhere \u03c3 () is the pointwise ReLU function. We consider the realizable setting where there exists a true W \u2217 using which the training data is generated. The population risk (see Eq. 3) is then:\n`(W ) = EG [ (f(x;W )\u2212 f(x;W \u2217))2 ] , (5)\nAs we show next, `(W ) can be considerably simplified. First, define:\ng(u,v) = EG [\u03c3 (u \u00b7 x)\u03c3 (v \u00b7 x)] (6)\nSimple algebra then shows that:\n`(W ) = 1\nk2 \u2211 i,j [ g(wi,wj)\u2212 2g(wi,w\u2217j ) + g(w\u2217i ,w\u2217j ) ] (7)\nThe next Lemma from (Cho & Saul, 2009) shows that g(u,v) has a simple form.\nLemma 3.1 ((Cho & Saul, 2009), Section 2). Assume x \u2208 Rd is a vector where the entries are IID Gaussian random variables with mean 0 and variance 1. Given u,v \u2208 Rd denote by \u03b8u,v the angle between u and v. Then:\ng(u,v) = 1\n2\u03c0 \u2016u\u2016 \u2016v\u2016\n( sin \u03b8u,v + ( \u03c0 \u2212 \u03b8u,v ) cos \u03b8u,v ) The gradient of g with respect to u also turns out to have a simple form, as stated in the lemma\nbelow. The proof is deferred to the Appendix A.\nLemma 3.2. Let g be as defined in Eq. 6. Then g is differentiable at all points u 6= 0 and\n\u2202g(u,v)\n\u2202u =\n1 2\u03c0 \u2016v\u2016 u \u2016u\u2016 sin \u03b8u,v + 1 2\u03c0\n( \u03c0 \u2212 \u03b8u,v ) v\nWe conclude by special-casing the results above to No-Overlap Networks. In this case, the entire model is specified by a single filter vector w \u2208 Rm. The rows wi are mostly zeros, except for the indices ((i \u2212 1)m + 1, . . . , im) which take the values of w. Namely, wi = ( 0(i\u22121)m,w,0d\u2212im ) where 0l \u2208 Rl is a zero vector. The same holds for the vectors w\u2217i with a weight vector w\u2217. This simplifies the loss considerably, since for all i: g(wi,wi) = 1 2 \u2016w\u2016 2 , and for all i 6= j: g(wi,wj) = 12\u03c0 \u2016w\u2016 2 and g(wi,w \u2217 j ) = 1 2\u03c0 \u2016w\u2016 \u2016w\n\u2217\u2016. Thus the loss `(w) for No-Overlap Networks yields (up to additive factors in w\u2217):\nl(w) = 1\nk2\n[ \u03b3\u2016w\u20162 \u2212 2kg(w,w\u2217)\u2212 2\u03b2 \u2016w\u2016 \u2016w\u2217\u2016 ] (8)\nwhere \u03b2 = k 2\u2212k 2\u03c0 and \u03b3 = \u03b2 + k 2 .\n2The variance per variable can be arbitrary. We choose one for simplicity.\n4 Learning No-Overlap Networks is NP-Complete\nThe No-Overlap Networks architecture is a simplified convolutional layer with average pooling. However, as we show here, learning it is still a hard problem. This will motivate our exploration of distribution dependent results in Section 5.\nRecall that our focus is on minimizing the squared error in Eq. 3. For this section, we do not make any assumptions on D. Thus D can be a distribution with uniform mass on training points x1, . . . ,xn, recovering the empirical risk in Eq. 2. We know that `(w) in Eq. 3 can be minimized by setting w = w\u2217 and the corresponding squared loss `(w) will be zero. However, we of course do not know w\u2217, and the question is how difficult is it to minimize `(w). In what follows we show that this is hard. Namely, it is an NP-complete problem to find a w that comes 0 close to the minimum of `(w), for some constant 0.\nWe begin by defining the Set-Splitting-by-k-Sets problem, which is a variant of the classic SetSplitting problem (Garey & Johnson, 1990). After establishing the hardness of Set-Splitting-by-k-Sets, we will provide a reduction from it to learning No-Overlap Networks.\nDefinition 1. The Set-Splitting-by-k-Sets decision problem is defined as follows: Given a finite set S of d elements and a collection C of at most (k \u2212 1)d subsets Cj of S, do there exist disjoint sets S1, S2, ..., Sk such that \u22c3 i Si = S and for all j and i, Cj 6\u2286 Si?\nFor k = 2 and without the upper bound on |C| this is known as the Set-Splitting decision problem which is NP-complete (Garey & Johnson, 1990). Next, we show that Set-Splitting-by-k-Sets is NPcomplete. The proof is via a reduction from 3SAT and induction, and is provided in Appendix B.\nProposition 4.1. Set-Splitting-by-k-Sets is NP-complete for all k \u2265 2.\nWe next formulate the No-Overlap Networks optimization problem.\nDefinition 2. The k-Non-Overlap-Opt problem is defined as follows. The input is a distribution DX,Y over input-output pairs x, y where x \u2208 Rd. If the input is realizable by a no-overlap network with k hidden neurons, then the output is a vector w such that:\nEDX,Y [ (f(x;w)\u2212 y))2 ] < 1\n4k5d (9)\nOtherwise an arbitrary weight vector is returned.\nThe above problem returns a w that minimizes the population-risk up to 14k5d accuracy. It is thus easier than minimizing the risk to an arbitrary precision (see Section 5, Theorem 5.2).\nWe prove the following theorem, which uses some ideas from (Blum & Rivest, 1993), but introduces additional constructions needed for the no overlap case.\nTheorem 4.2. For all the k \u2265 2, the k-Non-Overlap-Opt problem is NP-complete.\nProof. We will show a reduction from Set-Splitting-by-k-sets to k-Non-Overlap-Opt. Assume a given instance of the Set-Splitting-by-k-sets problem with a set S and collection of subsets C. Denote S = {1, 2, ..., d} and |C| \u2264 (k \u2212 1)d. Let 0d \u2208 Rd be the all zeros vector. For a vector v \u2208 Rd, define the vector di(v) \u2208 Rkd to be the concatenation of i \u2212 1 vectors 0d, followed by v and k \u2212 i vectors 0d, and let d(v) = (d1(v),d2(v), ...,dk(v)) \u2208 Rk\n2d. We next define a training set for k-Non-Overlap-Opt. For each element i \u2208 S define an input vector xi = d(ei), where ei is the standard basis of Rd. Assign the label yi = 1k to this input. In addition, for each subset Cj \u2208 C define the vector xd+j = d( \u2211 i\u2208Cj ei) and label yd+j = 0. Thus we have |S| + |C| inputs in Rk2d. Let DX,Y be a uniform distribution over the training set points (i.e., each point with probability at least 1kd since |C| \u2264 (k \u2212 1)d).\nWe will now show that the given instance of Set-Splitting-by-k-sets has a solution (i.e., there exist splitting sets) if and only if k-Non-Overlap-Opt returns a weight vector with low risk. First, assume\nthere exist splitting sets S1, ..., Sk. For each 1 \u2264 l \u2264 k define the vector aSl \u2208 Rd such that for all i \u2208 Sl, aSli = 1 and a Sl i = \u2212d otherwise. Define a No-Overlap Network with k2d inputs and weight vector w = (aS1 ,aS2 , ...,aSk) \u2208 Rkd. Then for all 1 \u2264 i \u2264 d we have:\nf(xi;w) =\n\u2211k l=1 \u03c3((a Sl)Tei)\nk =\n1 k = yi (10)\nand for all j:\nf(xd+j ;w) =\n\u2211k l=1 \u03c3((a Sl)T ( \u2211 i\u2208Cj ei))\nk = 0 = yd+j (11)\nwhere the last equality follows since for all l and j, Cj 6\u2286 Sl. Therefore there exists a w for which the error in Eq. 9 is zero and k-Non-Overlap-Opt will return a weight vector with low risk.\nConversely, assume that k-Non-Overlap-Opt returned a w \u2208 Rkd with risk less than 14k5d on DX,Y above. Denote by w = (w1,w2, ...,wk), where wl \u2208 Rd. We will show that this implies that there exist k splitting sets. For all x\u2032, y\u2032 in the training set it holds that:3\n(f(x\u2032;w)\u2212 y\u2032)2\nkd \u2264 EDX,Y [(f(x;w)\u2212 y)2] <\n1\n4k5d\nThis implies that for all i and j,\n|f(d(ei);w)\u2212 1 k | < 1 2k2 , |f(d( \u2211 i\u2208Cj ei);w)| < 1 2k2 (12)\nDefine sets Sl = {i | wTl ei > 12k} for 1 \u2264 l \u2264 k and WLOG assume they are disjoint by arbitrarily assigning points that belong to more than one set, to one of the sets they belong to. We will next show that these Sl are splitting. Namely, it holds that \u22c3 l Sl = S and no subset Cj is a subset of some Sl.\nSince f(d(ei);w) = \u2211k l=1 \u03c3(w T l ei) k > 1 k \u2212 1 2k2 > 1 2k for all i, it follows that for each i \u2208 S there\nexists 1 \u2264 l \u2264 k such that wTl ei > 12k . Therefore, by the definition of Sl we deduce that \u22c3 l Sl = S. To show the second property, assume by contradiction that for some j and m, Cj \u2286 Sm. Then\nwTm( \u2211 i\u2208Cj ei) > |Cj | 2k , which implies that f(d( \u2211 i\u2208Cj ei);w) = \u2211k l=1 \u03c3(w T l ( \u2211 i\u2208Cj ei)) k > |Cj | 2k2 \u2265 1 2k2 , a contradiction. This concludes our proof.\nTo conclude, we have shown that No-Overlap Networks are hard to learn if one does not make any assumptions about the training data. In fact we have shown that finding a w with loss at most\n1 4k5d is hard. In the next section, we show that certain distributional assumptions make the problem tractable.\n5 No-Overlap Networks can be Learned for Gaussian Inputs\nIn this section we assume that the input features x are generated via a Gaussian distribution G, as in Section 3. We will show that in this case, gradient descent will converge with high probability to the global optimum of `(w) (Eq. 8) in polynomial time.\nIn order to analyze convergence of gradient descent on `, we need a characterization of all the critical and non-differentiable points. We show that ` has a non-differentiable point and a degenerate saddle point.4 Therefore, recent methods for showing global convergence of gradient-based optimizers\n3The LHS is true because for a non-negative random variable X, E[X] \u2265 p(x)x for all x, and in our case p(x) \u2265 1 kd\n. 4A saddle point is degenerate if the Hessian at the point has only non-negative eigenvalues and at least one zero\neigenvalue.\non non-convex objectives (Lee et al., 2016; Ge et al., 2015) cannot be used in our case, because they assume all saddles are strict 5 and the objective function is continuously differentiable everywhere.\nThe characterization is given in the following lemma. The proof relies on the fact that `(w) depends only on \u2016w\u2016,\u2016w\u2217\u2016 and \u03b8w,w\u2217 , and therefore w.l.o.g. it can be assumed that w\u2217 lies on one of the axes. Then by a symmetry argument, in order to prove properties of the gradient and the Hessian, it suffices to calculate partial derivatives with respect to at most three variables.\nLemma 5.1. Let `(w) be defined as in Eq. 8. Then the following holds:\n1. `(w) is differentiable if and only if w 6= 0.\n2. For k > 1, `(w) has three critical points:\n(a) A local maximum at w = 0.\n(b) A unique global minimum at w = w\u2217.\n(c) A degenerate saddle point at w = \u2212( k 2\u2212k k2+(\u03c0\u22121)k )w \u2217.\nFor k = 1, w = 0 is not a local maximum and the unique global minimum w\u2217 is the only differentiable critical point.\nWe next consider a simple gradient descent update rule for minimizing `(w) and analyze its convergence. Let \u03bb > 0 denote the step size. Then the update at iteration t is simply:\nwt+1 = wt \u2212 \u03bb\u2207`(wt) (13)\nOur main result, stated formally below, is that the above update is guaranteed to converge to an accurate solution after O( 1 2 ) iterations. We note that the dependence of the convergence rate on is similar to standard results on convergence of gradient descent to stationary points (e.g., see discussion in Allen-Zhu & Hazan, 2016).\nTheorem 5.2. Assume \u2016w\u2217\u2016 = 1.6 For any \u03b4 > 0 and 0 < < \u03b4 sin\u03c0\u03b4k , there exists 0 < \u03bb < 1 7 such that with probability at least 1\u2212\u03b4, gradient descent initialized randomly from the unit sphere with learning rate \u03bb will get to a point w such that `(w) \u2264 O( ) 8 in O( 1 2 ) iterations.\nThe complete proof is provided in Appendix C. Here we provide a high level overview. In particular, we first explain why gradient descent will stay away from the two bad points mentioned in Lemma 5.1.\nFirst we note that the gradient of `(w) at wt is given by:\n\u2207`(wt) = \u2212c1(wt,w\u2217)wt \u2212 c2(wt,w\u2217)w\u2217 , (14)\nwhere c1 and c2 are two functions such that c1 \u2265 \u22121 and c2 \u2265 0. Thus the gradient is a sum of a vector in the direction of wt and a vector in the direction of w \u2217. At iteration t+ 1 we have:\nwt+1 = (1 + \u03bbc1(wt,w \u2217))wt + \u03bbc2(wt,w \u2217)w\u2217 (15)\nIt follows that for \u03bb < 1 the angle between wt and w \u2217 will decrease in each iteration. Therefore, if w0 has an angle with w \u2217 that is not \u03c0, we will never converge to the saddle point in Lemma 5.1.\nNext, assuming \u2016w0\u2016 > 0 and that the angle between w0 and w\u2217 is at most (1\u2212\u03b4)\u03c0 (which occurs with probability 1 \u2212 \u03b4), it can be shown that the norm of wt is always bounded away from zero by\n5A saddle point is strict if the Hessian at the point has at least one negative eigenvalue. 6Assumed for simplicity, otherwise \u2016w\u2217\u2016 is a constant factor. 7\u03bb can be found explicitly. 8O(\u00b7) hides a linear factor in d.\na constant M = \u2126\u0303(1).9 The proof is quite technical and follows from the fact that w = 0 is a local maximum.10\nThe fact that wt stays away from the problematic points allows us to show that `(w) has a Lipschitz continuous gradient on the line between wt and wt+1, with constant L = O\u0303(1).\n9 By standard optimization analysis (Nesterov, 2004) it follows that after T = O( 1 2 ) iterations we will have \u2016\u2207l(wt)\u2016 \u2264 O( ) for some 0 \u2264 t \u2264 T . This in turn can be used to show that wt is O( \u221a )-close to w\u2217. Finally, since `(w) \u2264 d\u2016w \u2212w\u2217\u20162, it follows that wt approximates the global minimum to within O( ) accuracy.\nTheorem 5.2 implies that gradient descent converges to a point w such that `(w) \u2264 1d2 in time O(poly(d)) where d is the input dimension.11 The following corollary thus follows.\nCorollary 5.3. Gradient descent solves the k-Non-Overlap-Opt problem under the Gaussian assumption on D with high probability and in polynomial time."}, {"heading": "6 Empirical Illustration of Tractability Gap", "text": "The results in the previous sections showed that No-Overlap Networks optimization is hard in the general case, but tractable for Gaussian inputs. Here we empirically demonstrate both the easy and hard cases. The training data for the two cases will be generated by using the same w\u2217 but different distributions over x.\nTo generate the \u201chard\u201d case, we begin with a set splitting problem. In particular, we consider a set S with 40 elements and a collection C of 760 subsets of S, each of size 20. We choose Cj such that there exists subsets S1,S2 that split the subsets Cj . We use the reduction in Section 4 to convert this into a No-Overlap Networks optimization problem. This results in a training set of size 800.\nSince we know the w\u2217 that solves the set splitting problem, we can use it to label data from a different distribution. Motivated by Section 5 we use a Gaussian distribution G as defined earlier and generate a training set of the same size (namely 800) and labels given by the no-overlap network with weight w\u2217.\nFor these two learning problems we used AdaGrad (Duchi et al., 2011) to optimize the empirical risk (plain gradient descent also converges, but AdaGrad requires less tuning of step size). For both datasets we used a random normal initializer and for each we chose the best performing learning rate schedule. The training error for each setting as a function of the number of epochs is shown in Figure 2. It is clear that in the non-Gaussian case, AdaGrad gets trapped at a sub-optimal point, whereas the Gaussian case is solved optimally.12 In the Gaussian case AdaGrad converged to w\u2217. Therefore, given the Gaussian dataset we were able to recover the true weight vector w\u2217, whereas given the data constructed via the reduction we were not, even though\nboth datasets were of the same size. We conclude that these empirical findings are in line with our theoretical results.\n9\u2126\u0303 and O\u0303 hide factors of \u2016w\u2217\u2016, \u03b8w0,w\u2217 , k and \u03b4. 10The proof holds even for k = 1 where w = 0 is not a local maximum. 11Note that the complexity of a gradient descent iteration is polynomial in d. 12We note that the value of 0.06 attained by the non-Gaussian case is quite high, since the zero weight vector in this\ncase has loss of order 0.1."}, {"heading": "7 Networks with Overlapping Filters", "text": "Thus far we showed that the non-overlapping case becomes tractable under Gaussian inputs. A natural question is then what happens when overlaps are allowed (namely, the stride is smaller than the filter size). Will gradient descent still find a global optimum? Here we show that this is in fact not the case, and that with probability greater than 14 gradient descent will get stuck in a sub-optimal region. In Section 7.1 we analyze this setting for a two dimensional example and provide bounds on the level of suboptimality. In Section 7.2 we report on an empirical study of optimization for networks with overlapping filters. Our results suggest that by restarting gradient descent a constant number of times, it will converge to the global minimum with high probability. Complete proofs of the results are provided in Appendix D."}, {"heading": "7.1 Suboptimality of Gradient Descent for R2", "text": "We consider an instance where there are k = d \u2212 1 neurons and matrices W,W \u2217 \u2208 Rk\u00d7d correspond to an overlapping filter of size 2 with stride 1, i.e., for all 1 \u2264 i \u2264 k wi = (0i\u22121,w,0d\u2212i\u22121), w\u2217i = (0i\u22121,w\n\u2217,0d\u2212i\u22121) where 0l = (0, 0, ..., 0) \u2208 Rl, w = (w1, w2) is a vector of 2 parameters and w\u2217 = (\u2212w\u2217, w\u2217) \u2208 R2, w\u2217 > 0. Define the following vectors wr = (w1, w2, 0), wl = (0, w1, w2), w\u2217r = (\u2212w\u2217, w\u2217, 0), w\u2217l = (0,\u2212w\u2217, w\u2217) and denote by \u03b8w,v the angle between two vectors w and v.\nOne might wonder why the analysis of the overlapping case should be any different than the nonoverlapping case. However, even for a filter of size two, as above, the loss function and consequently the gradient, are more complex in the overlapping case. Indeed, the loss function in this case is given by:\n`(w) = \u03b1(\u2016w\u20162 + \u2016w\u2217\u20162)\u2212 \u03b2g(w,w\u2217) + (\u03b2 \u2212 2)(g(wr,wl)\u2212 g(wl,w\u2217r) \u2212 g(wr,w\u2217l ) + g(w\u2217r ,w\u2217l ))\u2212 \u03b3 \u2016w\u2016 \u2016w\u2217\u2016\n(16)\nwhere \u03b1 = 1k2 ( k 2 + k2\u22123k+2 2\u03c0 ) , \u03b2 = 2k and \u03b3 = k 2\u22123k+2 \u03c0 .\nCompared to the objective in Eq. 8 which depends only on \u2016w\u2016, \u2016w\u2016 and \u03b8w,w\u2217 , we see that the objective in Eq. 16 has new terms such as g(wr,w \u2217 l ) which has a more complicated dependence on the weight vectors w\u2217 and w. This does not only have implications on the analysis, but also on the geometric properties of the loss function and the dynamics of gradient descent. In particular, in Figure 3 we see that the objective has a large sub-optimal region which is not the case when the filters are non-overlapping.\nAs in the previous section we consider gradient descent updates as in Eq. 13. The following Proposition shows that if w is initialized in the interior of the fourth quadrant of R2, then it will stay there for all remaining iterations.\nThe proof is a straightforward inspection of the components of the gradient, and is provided in the supplementary.\nProposition 7.1. For any \u03bb \u2208 (0, 13 ), if wt is in the interior of the fourth quadrant of R 2 then so is wt+1.\nNote that in our example the global optimum w\u2217 is in the second quadrant (it\u2019s easy to show that it is also unique). Hence, if initialized at the fourth quadrant, gradient descent will remain in a sub-optimal region. The sub-optimality can be clearly seen in Figure 3. In the proposition below we formalize this observation by giving a tight lower bound on the values of `(w) for w in the fourth quadrant. Specifically, we show that the sub-optimality scales with O( 1k2 ). The proof idea is to express all angles between all the vectors that appear in Eq. 16 via a single angle parameter \u03b8 between w in the fourth quadrant and the positive x-axis. Then it is possible to prove the relatively simpler one dimensional inequality that depends on \u03b8.\nProposition 7.2. Let h(k) = k 2\u22123k+2 \u03c0 + \u221a 3(k\u22121) \u03c0 + 2(k\u22121) 3 , then for all w in the fourth quadrant l(w) \u2265 2h(k)+1k2(2h(k)+2)\u2016w \u2217\u20162 and this lower bound is attained by w\u0303 = \u2212 h(k)h(k)+1w \u2217.\nThe above two propositions result in the following characterization of the sub-optimality of gradient descent for w \u2208 R2 and overlapping filters.\nTheorem 7.3. Define h(k) as in Proposition 7.2. Then with probability \u2265 14 , a randomly initialized gradient descent with learning rate \u03bb \u2208 (0, 13 ) will get stuck in a sub-optimal region, where each point in this region has loss at least 2h(k)+1k2(2h(k)+2)\u2016w \u2217\u20162 and this bound is tight."}, {"heading": "7.2 Empirical study of Gradient Descent for m > 2", "text": "In Section 7.1 we showed that already for m = 2, networks with w \u2208 Rm and filter overlaps exhibit more complex behavior than those without overlap. This leaves open the question of what happens in the general case under the Gaussian assumption, for various values of d,m and overlaps. We leave the theoretical analysis of this question to future work, but here report on empirical findings that hint at what the solution should look like.\nWe experimented with a range of d,m and overlap values (see Appendix E for details of the experimental setup). For each value of d, m and overlap we sampled 90 values of w\u2217 from various uniform input distributions with different supports and several pre-defined deterministic values. This resulted in more than 1200 different sampled w\u2217. For each such w\u2217 we ran gradient descent multiple times, each initialized randomly from a different w0. Using the results from these runs, we could estimate the probability of sampling a w0 that would converge to the unique global minimum. Viewed differently, this is the probability mass of the basin of attraction of the global optimum. We note that the uniqueness of the global minimum follows easily from equating the population risk (Eq. 3) to 0 and the full proof is deferred to Appendix F.\nOur results are that across all values of d,m, overlap and w\u2217, the probability mass of the basin of attraction is at least 117 . The practical implication is that multiple restarts of gradient descent (in this case a few dozen) will find the global optimum with high probability. We leave formal analysis of this intriguing fact for future work."}, {"heading": "8 Discussion", "text": "The key theoretical question in deep learning is why it succeeds in finding good models despite the non-convexity of the training loss. It is clear that an answer must characterize specific settings where deep learning provably works. Despite considerable recent effort, such a case has not been shown. Here we provide the first analysis of a non-linear architecture where gradient descent is globally optimal, for a certain input distribution, namely Gaussian. Thus our specific characterization is both in terms of architecture (no-overlap networks, single hidden layer, and average pooling) and input distribution. We show that learning in no-overlap architectures is hard, so that some input distribution restriction is necessary for tractability. Note however, that it is certainly possible that other, non-Gaussian, distributions also result in tractability. Some candidates would be sub-Gaussian and log-concave distributions.\nOur derivation addressed the population risk, which for the Gaussian case can be calculated in closed form. In practice, one minimizes an empirical risk. Our experiments in Section 6 suggest that optimizing the empirical risk in the Gaussian case is tractable. It would be interesting to prove this formally. It is likely that measure concentration results can be used to get similar results to those we had for the population risk (e.g., see Mei et al., 2016; Xu et al., 2016, for use of such tools).\nConvolution layers are among the basic building block of neural networks. Our work is among the first to analyze optimization for these. The architecture we study is similar in structure to convolutional networks, in the sense of using parameter tying and pooling. However, most standard convolutional layers have overlap and use max pooling. In Section 7 we provide initial results for the case of overlap, showing there is hope for proving optimality for gradient descent with random restarts. Analyzing max pooling would be very interesting and is left for future work.\nFinally, we note that distribution dependent tractability has been shown for intersection of halfspaces (Klivans et al., 2009), which is a non-convolutional architecture. However, these results do not use gradient descent. It would be very interesting to use our techniques to try and understand gradient descent for the population risk in these settings."}, {"heading": "A Proof of Lemma 3.2", "text": "First assume that \u03b8u,v 6= 0, \u03c0 . Then by straightforward calculation we have\n\u2202g \u2202ui = 1 2\u03c0 \u2016v\u2016 ui \u2016u\u2016\n(\u221a 1\u2212 ( u \u00b7 v \u2016u\u2016 \u2016v\u2016 )2 + ( \u03c0 \u2212 arccos ( u \u00b7 v \u2016u\u2016 \u2016v\u2016 )) u \u00b7 v \u2016u\u2016 \u2016v\u2016 )\n+ 1\n2\u03c0 \u2016u\u2016 \u2016v\u2016\n(( \u2212 u\u00b7v \u2016u\u2016\u2016v\u2016\u221a\n1\u2212 (\nu\u00b7v \u2016u\u2016\u2016v\u2016\n)2 )( vi \u2016u\u2016 \u2016v\u2016 \u2212 ui \u2016u\u20162 u \u00b7 v \u2016u\u2016 \u2016v\u2016 )\n+ ( u\u00b7v \u2016u\u2016\u2016v\u2016\u221a\n1\u2212 (\nu\u00b7v \u2016u\u2016\u2016v\u2016\n)2 ( vi \u2016u\u2016 \u2016v\u2016 \u2212 ui \u2016u\u20162 u \u00b7 v \u2016u\u2016 \u2016v\u2016 ))\n+ ( \u03c0 \u2212 arccos ( u \u00b7 v \u2016u\u2016 \u2016v\u2016 ))( vi \u2016u\u2016 \u2016v\u2016 \u2212 ui \u2016u\u20162 u \u00b7 v \u2016u\u2016 \u2016v\u2016 ))\n= 1 2\u03c0 \u2016v\u2016 ui \u2016u\u2016\n(\u221a 1\u2212 ( u \u00b7 v \u2016u\u2016 \u2016v\u2016 )2 + ( \u03c0 \u2212 arccos ( u \u00b7 v \u2016u\u2016 \u2016v\u2016 )) u \u00b7 v \u2016u\u2016 \u2016v\u2016 )\n+ 1\n2\u03c0 \u2016u\u2016 \u2016v\u2016\n( \u03c0 \u2212 arccos ( u \u00b7 v \u2016u\u2016 \u2016v\u2016 ))( vi \u2016u\u2016 \u2016v\u2016 \u2212 ui \u2016u\u20162 u \u00b7 v \u2016u\u2016 \u2016v\u2016 ))\n= 1 2\u03c0 \u2016v\u2016 ui \u2016u\u2016\n\u221a 1\u2212 ( u \u00b7 v \u2016u\u2016 \u2016v\u2016 )2 + 1 2\u03c0 ( \u03c0 \u2212 arccos ( u \u00b7 v \u2016u\u2016 \u2016v\u2016 )) vi\n= 1 2\u03c0 \u2016v\u2016 ui \u2016u\u2016 sin \u03b8u,v + 1 2\u03c0\n( \u03c0 \u2212 \u03b8u,v ) vi\n(17)\nHence,\n\u2202g \u2202u = 1 2\u03c0 \u2016v\u2016 u \u2016u\u2016 sin \u03b8u,v + 1 2\u03c0\n( \u03c0 \u2212 \u03b8u,v ) v (18)\nNow we assume that u is parallel to v. We first show that g is differentiable in this case. Without loss of generality we can assume that u and v lie on the u1 axis. This follows since g is a function of \u2016u\u2016, \u2016v\u2016 and \u03b8u,v and therefore g(\u00b7,v) has a directional derivative in direction d at u if and only if g(\u00b7, Rv) has a directional derivative in direction Rd at Ru where R is a rotation matrix. Hence g(\u00b7,v) is differentiable at u if and only if g(\u00b7, Rv) is differentiable at Ru. Furthermore, if v and u are on the u1 axis, then by symmetry the partial derivatives with respect to other axes at u are all equal, hence we only need to consider the partial derivative with respect to the u1 and u2 axes.\nLet v = (1, 0, ..., 0) and u = (u, 0, ..., 0) where u 6= 0. In order to show differentiability, we will prove that g(u,v) has continuous partial derivatives at u (by equality (18) the partial derivatives are clearly continuous at points that are not on the u1 axis. Define u = (u, , 0, ..., 0). Then\n\u2202g\n\u2202u2 (u,v) = lim \u21920\n1 2\u03c0 \u2016u \u2016 \u2016v\u2016 ( sin \u03b8u ,v + ( \u03c0 \u2212 \u03b8u ,v ) cos \u03b8u ,v ) \u2212 g(u,v)\nBy L\u2019hopital\u2019s rule and the calculation of equality (18) we get\n\u2202g\n\u2202u2 (u,v) = lim \u21920\n1\n2\u03c0 \u2016v\u2016 \u2016u \u2016 sin \u03b8 = 0\nFurthermore, by equality (18) we see that limu\u2032\u2192u \u2202g \u2202u2 (u\u2032,v) = 0 since limu\u2032\u2192u sin \u03b8u\u2032,v = 0.\nFor a fixed \u03b8u,v equal to 0 or \u03c0, \u2202g \u2202u1 (u,v) is the same as \u2202g\u2202\u2016u\u2016 (u,v). Hence,\n\u2202g\n\u2202u1 (u,v) =\n1\n2\u03c0 \u2016v\u2016\n( sin \u03b8u,v + ( \u03c0 \u2212 \u03b8u,v ) cos \u03b8u,v ) = { 1 2 if u > 0 0 if u < 0\nand the partial derivative is continuous since\nlim u\u2032\u2192u\n\u2202g\n\u2202u1 (u\u2032,v) = { 1 2 if u > 0 0 if u < 0\nFinally, we see that for the case where u and v are parallel, the values we got for the partial derivatives coincide with equation Eq. 18. This concludes the proof."}, {"heading": "B Proof of Proposition 4.1", "text": "We will prove the claim by induction on k. For the base case we will show that Set-Splitting-by-2-Sets is NP-complete. We will prove this via a reduction from a variant of the 3-SAT problem with the restriction of equal number of variables and clauses, which we denote Equal-3SAT. We will first prove that Equal-3SAT is NP-complete.\nLemma B.1. Equal-3SAT is NP-complete.\nProof. This can be shown via a reduction from 3SAT. Given a formula \u03c6 with n variables and m clauses we can increase n \u2212m by 1 by adding a new clause of the form (x \u2228 y) for new variables x and y. Furthermore, we can decrease n\u2212m by 1 by adding two new identical clauses of the form (z) for a new variable z. In each case the formula with the new clause(s) is satisfiable if and only if \u03c6 is. Therefore given a formula \u03c6 we can construct a new formula \u03c8 with equal number of variables and clauses such that \u03c6 is satisfiable if and only if \u03c8 is.\nWe will now give a reduction from Equal-3SAT to Set-Splitting-by-2-Sets.\nLemma B.2. Set-Splitting-by-2-Sets is NP-complete.\nProof. The following reduction is exactly the reduction from 3SAT to Splitting-Sets and we include it here for completeness. Let \u03c6 be a formula with set of variables V and equal number of variables and clauses. We construct the sets S and C as follows. Define\nS = {x\u0304 | x \u2208 V } \u222a V \u222a {n}\nwhere x\u0304 is the negation of variable x and n is a new variable not in V . For each clause c with set of variables or negations of variables Vc that appear in the clause (for example, if c = (x\u0304 \u2228 y) then Vc = {x\u0304, y}) construct a set Sc = Vc \u222a {n}. Furthermore, for each variable x \u2208 V construct a set Sx = {x, x\u0304}. Let C be the family of subsets Sc and Sx for all clauses c and x \u2208 V . Note that |C|\u2264 |S| which is required by the definition of Set-Splitting-by-2-Sets.\nAssume that \u03c6 is satisfiable and let A be the satisfying assignment. Define S1 = {x|A(x) = true}\u222a{x\u0304|A(x) = false} and S2 = {x|A(x) = false}\u222a{x\u0304|A(x) = true}\u222a{n}. Note that S1\u222aS2 = S. Assume by contradiction that there exists a set T \u2208 C such that T \u2286 S1 or T \u2286 S2. If T \u2286 S1 then T is not a set Sc for some clause c because n /\u2208 S1. However, by the construction of S1 a variable and its negation cannot be in S1. Hence T \u2286 S1 is impossible. If T \u2286 S2 then as in the previous claim T cannot be a set Sx for a variable x. Hence T = Sc for some clause c. However, this implies that A(c) = false, a contradiction.\nConversely, assume there exists splitting sets S1 and S2 and w.l.o.g. n \u2208 S1. We note that it follows that no variable x and its negation x\u0304 are both contained in one of the sets S1 or S2. Define the following assignment A for \u03c6. For all x \u2208 V if x \u2208 S1 let A(x) = false, otherwise let A(x) = true. Note that A is a well defined assignment. Assume by contradiction that there is a clause c in \u03c6 which is not satisfiable. Since S2 splits Sc it follows that there exists a variable x such that it or its negation x\u0304 are in S2 (recall that n \u2208 S1). If x \u2208 S2 then A(x) = true and if x\u0304 \u2208 S2 then A(x\u0304) = true since x \u2208 S1. In both cases c is satisfiable, a contradiction.\nThis proves the base case. We will now prove the induction step by giving a reduction from SetSplitting-by-k-Sets to Set-Splitting-by-(k+1)-Sets. Given S = {1, 2, ..., d} and C = {Cj}j such that |C| \u2264 (k \u2212 1)d, define S\u2032 = {1, 2, ..., d + 1} and C\u2032 = C \u222a{Dj}j where Dj = {j, d + 1} for all 1 \u2264 j \u2264 d. Note that |C\u2032| \u2264 kd < k(d + 1). Assume that there are S1, ..., Sk that split the sets in C. Then if we define Sk+1 = {d+ 1}, it follows that \u22c3k+1 i=1 Si = S and S1, ..., Sk, Sk+1 are disjoint and split the sets in C\u2032. Conversely, assume that S1, ..., Sk, Sk+1 split the sets in C\u2032. Let w.l.o.g. Sk+1 be the set that contains d+ 1. Then for all 1 \u2264 j \u2264 d we have Dj 6\u2286 Sk+1. It follows that for all 1 \u2264 j \u2264 d, j /\u2208 Sk+1, or equivalently, Sk+1 = {d+ 1}. Hence, \u22c3k i=1 Si = S and S1, ..., Sk are disjoint and split the sets in C, as desired."}, {"heading": "C Missing Proofs for Section 5", "text": "C.1 Proof of Lemma 5.1\n1. For w 6= 0, the claim follows from Lemma 3.2. As in the proof of Lemma 3.2 we can assume w.l.o.g. that w = (0, 0, ..., 0) and w\u2217 = (1, 0, ..., 0). Let f(w,w\u2217) = 2kg(w,w\u2217) + (k2 \u2212 k)\u2016w\u2016\u2016w\n\u2217\u2016 \u03c0 . It suffices to show that \u2202f \u2202u2 (w,w\u2217) does not exist. Indeed, let w = (0, , 0, ..., 0) then by L\u2019hopital\u2019s rule\nlim \u21920+\nf(w ,w \u2217)\u2212 f(w,w\u2217)\n= lim \u21920+\nk \u03c0 \u2016w\u2217\u2016 | | sin \u03b8w ,w\u2217 + (k 2 \u2212 k)\u2016w \u2217\u2016 \u03c0 = k \u03c0 + k2 \u2212 k \u03c0\nand\nlim \u21920\u2212\nf(w ,w \u2217)\u2212 f(w,w\u2217)\n= lim \u21920\u2212\nk \u03c0 \u2016w\u2217\u2016 | | sin \u03b8w ,w\u2217 \u2212 (k2 \u2212 k) \u2016w\u2217\u2016 \u03c0 = \u2212k \u03c0 \u2212 k 2 \u2212 k \u03c0\nHence the left and right partial derivatives with respect to variable u2 are not equal, and thus \u2202f \u2202u2 (w,w\u2217) does not exist.\n2. We first show that w = 0 is a local maximum if and only if k > 1. Indeed, by considering the loss function as a function of the variable x = \u2016w\u2016, for any fixed angle \u03b8w,w\u2217 we get a quadratic function of the form `(x) = ax2 \u2212 bx, where a > 0 and b \u2265 0. Since f(\u03b8) = sin \u03b8 + (\u03c0 \u2212 \u03b8) cos \u03b8 is a non-negative function for 0 \u2264 \u03b8 \u2264 \u03c0 and f(\u03b8) = 0 if and only if \u03b8 = \u03c0, it follows that b = 0 if and only if k = 1 and \u03b8w,w\u2217 = \u03c0. Therefore if k > 1, then for all fixed angles \u03b8w,w\u2217 , the minimum of `(x) is attained at x > 0, which implies that w = 0 is a local maximum. If k = 1 and \u03b8w,w\u2217 = \u03c0 the minimum of `(x) is attained at x = 0, and thus w = 0 is not a local maximum in this case.\nWe will now find the other critical points of `. By Lemma 3.2 we get\n\u2207`(w) = 1 k2\n[( k +\nk2 \u2212 k \u03c0\n) w \u2212 k\n\u03c0 \u2016w\u2217\u2016 w \u2016w\u2016 sin \u03b8w,w\u2217 \u2212 k \u03c0\n( \u03c0 \u2212 \u03b8w,w\u2217 ) w\u2217 \u2212 k\n2 \u2212 k \u03c0 \u2016w\u2217\u2016 w \u2016w\u2016\n]\n= 1\nk2\n[( k +\nk2 \u2212 k \u03c0 \u2212 k \u2016w \u2217\u2016 \u03c0 \u2016w\u2016 sin \u03b8w,w\u2217 \u2212 k2 \u2212 k \u03c0 \u2016w\u2217\u2016 \u2016w\u2016\n) w \u2212 k\n\u03c0\n( \u03c0 \u2212 \u03b8w,w\u2217 ) w\u2217 ] (19)\nand assume it vanishes.\nDenote \u03b8 , \u03b8w,w\u2217 . If \u03b8 = 0 then let w = \u03b1w\u2217 for some \u03b1 > 0. It follows that\nk + k2 \u2212 k \u03c0 \u2212 k 2 \u2212 k \u03c0 1 \u03b1 \u2212 k \u03b1 = 0\nor equivalently \u03b1 = 1, and thus w = w\u2217.\nIf \u03b8 = \u03c0 then \u2016w\u2016 = k 2\u2212k k2+(\u03c0\u22121)k \u2016w \u2217\u2016 and thus w = \u2212( k 2\u2212k k2+(\u03c0\u22121)k )w \u2217. By setting \u03b8 = \u03c0 in the loss function, one can see that w = \u2212( k 2\u2212k\nk2+(\u03c0\u22121)k )w \u2217 is a one-dimensional local minimum, whereas\nby fixing \u2016w\u2016 and decreasing \u03b8, the loss function decreases. It follows that w = \u2212( k 2\u2212k k2+(\u03c0\u22121)k )w \u2217 is a saddle point. If \u03b8 6= 0, \u03c0 then w and w\u2217 are linearly independent and thus k\u03c0 ( \u03c0 \u2212 \u03b8 ) = 0 which is a contradiction. It remains to show that u = \u2212\u03b3(k)w\u2217 where \u03b3(k) = k 2\u2212k\nk2+(\u03c0\u22121)k is a degenerate saddle point. We\nwill show that the Hessian at u denoted by \u22072`(u), has only nonnegative eigenvalues and at least one zero eigenvalue. Let \u02dc\u0300(w) , `(w, Rw\u2217), where the second entry denotes the ground truth weight vector and R is a rotation matrix. Denote by fd1,d2 the second directional derivative of a function f in directions d1 and d2. Similarly to the proof of Lemma 3.2, since ` depends only on \u2016w\u2016, \u2016w\u2217\u2016 and \u03b8w,w\u2217 , we notice that\n`d1,d2(w) = \u02dc\u0300 Rd1,Rd2(Rw)\nor equivalently\ndT1\u22072`(w)d2 = (Rd1)T\u22072 \u02dc\u0300(Rw)Rd2 = dT1 RT\u22072 \u02dc\u0300(Rw)Rd2\nfor any w and directions d1 and d2. It follows that\n\u22072`(w) = RT\u22072 \u02dc\u0300(Rw)R\nfor all w. Since R is an orthogonal matrix, we have that \u22072`(w) and \u22072 \u02dc\u0300(Rw) are similar matrices and thus have the same eigenvalues. Therefore, we can w.l.o.g. rotate w\u2217 such that it will be on the w1 axis.\nBy symmetry we have\n\u2202`\n\u2202w1\u2202wi (u) =\n\u2202`\n\u2202w1\u2202wj (u),\n\u2202`\n\u2202wi\u2202w1 (u) =\n\u2202`\n\u2202wj\u2202w1 (u)\nand \u2202`\n\u2202w2i (u) =\n\u2202`\n\u2202w2j (u),\n\u2202`\n\u2202wi\u2202wj (u) =\n\u2202`\n\u2202ws\u2202wt (u)\nfor i 6= j, s 6= t such that i, j, s, t 6= 1. It follows that we only need to consider second partial derivatives with respect to 3 axes w1,w2 and w3. Denote u = (\u2212\u03b3(k), , 0, ..., 0) and w\u2217 = (1, 0, ..., 0) and \u03b2(k) = k\n2\u2212k \u03c0 and note that \u03b3(k) = \u03b2(k) \u03b2(k)+k . Then by equation Eq. 19 we have\n\u2202`\n\u2202w22 (u) = lim \u21920\n\u2207`(u )x \u2212\u2207`(u)x\n= lim \u21920\n1 k2\n[( k + \u03b2(k) ) \u2212 k\u03c0 \u2016w \u2217\u2016 \u2016u \u2016 sin \u03b8u ,w\u2217 \u2212 \u03b2(k) \u2016w \u2217\u2016 \u2016u \u2016 ]\n= 1 k2 ( k + \u03b2(k)\u2212 \u03b2(k) \u03b3(k) ) = 0\n(20)\nFurthermore,\n\u2202`\n\u2202w1\u2202w2 (u) = lim \u21920\n\u2207`(u )y \u2212\u2207`(u)y\n= lim \u21920\n1 k2\n[ \u2212 ( k + \u03b2(k) ) \u03b3(k) + k\u03c0 \u2016w \u2217\u2016 \u03b3(k)\u2016u \u2016 sin \u03b8u ,w\u2217 + \u03b2(k) \u2016w \u2217\u2016 \u03b3(k)\u2016u \u2016 \u2212 k \u03c0 (\u03c0 \u2212 \u03b8u ,w\u2217) ]\n(21)\nwhere \u03b8u ,w\u2217 = arccos( \u2212\u03b3(k)\u221a 2+\u03b32(k) ).\nBy L\u2019Hopital\u2019s rule we have\n\u2202`\n\u2202w1\u2202w2 (u) = lim \u21920 \u2212\u03b3(k) sin \u03b8u ,w \u2217 \u03c0k\u2016u \u20163 + \u03b3(k) cos \u03b8u ,w\u2217\n\u2202\u03b8u ,w\u2217\n\u2202w2 \u03c0k \u2016u \u2016 \u2212 \u03b2(k)\u03b3(k) \u2016u \u20163 +\n\u2202\u03b8u ,w\u2217\n\u2202w2 \u03c0k\n= 1\n\u03c0k lim \u21920\n\u2202\u03b8u ,w\u2217\n\u2202w2\n(\u03b3(k) cos \u03b8u ,w\u2217 \u2016u \u2016 + 1 ) (22)\nSince \u2202\u03b8u ,w\u2217\n\u2202w2 (u ) = \u2212 1 | |\u221a\n2+\u03b32(k)\n\u03b3(k)\n( 2 + \u03b32(k)) 3 2 = \u2212 \u03b3(k) ( 2 + \u03b32(k))| |\nit follows that\n\u2223\u2223\u2223\u2223 \u2202`\u2202w1\u2202w2 (u) \u2223\u2223\u2223\u2223 = 1\u03c0k lim \u21920 \u2223\u2223\u2223\u2223\u2202\u03b8u ,w\u2217\u2202w2 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u03b3(k) cos \u03b8u ,w\u2217\u2016u \u2016 + 1 \u2223\u2223\u2223\u2223 \u2264 1 \u03b3(k)\u03c0k lim \u21920 \u2223\u2223\u2223\u2223\u03b3(k) cos \u03b8u ,w\u2217\u2016u \u2016 + 1 \u2223\u2223\u2223\u2223 = 0 (23)\nand thus \u2202`\u2202w1\u2202w2 (u) = 0. Taking derivatives of the gradient with respect to w1 is easier because the expressions in Eq. 19 that depend on \u03b8w,w\u2217 and w \u2016w\u2016 are constant. Therefore,\n\u2202`\n\u2202w21 (u) =\nk + \u03b2(k)\nk2\nand \u2202`\n\u2202w2\u2202w1 (u) = 0\nFinally let u\u0303 = (0,\u2212\u03b3(k), , 0, ..., 0) then it is easy to see that\n\u2202`\n\u2202w2\u2202w3 (u) = lim \u21920\n\u2207`(u\u0303 )w2 \u2212\u2207`(u)w2\n= 0\n. Therefore, overall we see that \u22072`(u) is a diagonal matrix with zeros and k+\u03b2(k)k2 > 0 on the diagonal, which proves our claim.\nC.2 Proof of Theorem 5.2\nFor the following lemmas let wt+1 = wt \u2212 \u03bb\u2207`(wt), \u03b8t be the angle between wt and w\u2217 (t \u2265 0) and define \u03bb\u0303 = \u03b1(k)\u03bb where \u03b1(k) = 1k + k2\u2212k \u03c0k2 . Note that \u03b1(k) \u2264 1 for all k \u2265 1 The following lemma shows that for \u03bb < 1, the angle between wt and w \u2217 decreases in each iteration.\nLemma C.1. If 0 < \u03b8t < \u03c0 and \u03bb < 1 then \u03b8t+1 < \u03b8t.\nProof. This follows from the fact that adding\n\u2212 \u03bb k2\n( k +\nk2 \u2212 k \u03c0 \u2212 k \u2016w \u2217\u2016 \u03c0 \u2016wt\u2016 sin \u03b8t \u2212 k2 \u2212 k \u03c0 \u2016w\u2217\u2016 \u2016wt\u2016\n) wt\nto wt does not change \u03b8t for \u03bb < 1, since k+ k\n2\u2212k \u03c0\nk2 \u2264 1 for k \u2265 1. In addition, adding \u03bb \u03c0k\n( \u03c0 \u2212 \u03b8 ) w\u2217\ndecreases \u03b8t.\nWe will need the following two lemmas to establish a lower bound on \u2016wt\u2016.\nLemma C.2. If \u03c02 < \u03b8t < \u03c0 then \u2016wt+1\u2016 \u2265 sin \u03b8t sin \u03b8t+1 min{\u2016wt\u2016 , \u2016w \u2217\u2016 sin \u03b8t \u03b1(k)\u03c0 }.\nProof. Let\nut = wt \u2212 \u03bb\nk2\n( k +\nk2 \u2212 k \u03c0 \u2212 k \u2016w \u2217\u2016 \u03c0 \u2016wt\u2016 sin \u03b8t \u2212 k2 \u2212 k \u03c0 \u2016w\u2217\u2016 \u2016wt\u2016\n) wt\nNotice that if \u2016wt\u2016 \u2264 \u2016w \u2217\u2016 sin \u03b8t \u03b1(k)\u03c0 then\n\u2016ut\u2016 = (1\u2212 \u03bb\u0303) \u2016wt\u2016+ \u03bb \u2016w\u2217\u2016 \u03c0k sin \u03b8t + \u03bb(k2 \u2212 k) \u2016w\u2217\u2016 \u03c0k2\n\u2265 (1\u2212 \u03bb\u0303) \u2016wt\u2016+ \u03bbk \u2016w\u2217\u2016 sin \u03b8t \u03c0k2 + \u03bb(k2 \u2212 k) \u2016w\u2217\u2016 sin \u03b8t \u03c0k2 = (1\u2212 \u03bb\u0303) \u2016wt\u2016+ \u03bb\u0303 \u2016w\u2217\u2016 sin \u03b8t\n\u03b1(k)\u03c0 \u2265 \u2016wt\u2016\n(24)\nSimilarly, if \u2016wt\u2016 \u2265 \u2016w \u2217\u2016 sin \u03b8t \u03b1(k)\u03c0 then \u2016ut\u2016 \u2265 \u2016w\u2217\u2016 sin \u03b8t \u03b1(k)\u03c0 . Furthermore, by a simple geometric ob-\nservation we see that \u2016wt+1\u2016 cos(\u03b8t+1 \u2212 \u03c02 ) = \u2016ut\u2016 cos(\u03b8t \u2212 \u03c0 2 ) if \u03b8t+1 > \u03c0 2 and \u2016wt+1\u2016 cos( \u03c0 2 \u2212 \u03b8t+1) = \u2016ut\u2016 cos(\u03b8t \u2212 \u03c02 ) if \u03b8t+1 \u2264 \u03c0 2 . This is equivalent to \u2016wt+1\u2016 = sin \u03b8t sin \u03b8t+1\n\u2016ut\u2016. It follows that \u2016wt+1\u2016 \u2265 sin \u03b8tsin \u03b8t+1 min{\u2016wt\u2016 , \u2016w\u2217\u2016 sin \u03b8t \u03b1(k)\u03c0 } as desired.\nLemma C.3. If 0 < \u03b8t \u2264 \u03c02 and 0 < \u03bb < 1 2 then \u2016wt+1\u2016 \u2265 min{\u2016wt\u2016 , \u2016w\u2217\u2016 8 }\nProof. First assume that k \u2265 2. Let ut be as in Lemma C.2, then\n\u2016ut\u2016 \u2265 (1\u2212 \u03bb\u0303) \u2016wt\u2016+ \u03bb\u0303(k2 \u2212 k) \u2016w\u2217\u2016\n\u03b1(k)\u03c0k2\nIt follows that if \u2016wt\u2016 \u2265 (k 2\u2212k)\u2016w\u2217\u2016 \u03b1(k)\u03c0k2 \u2265 \u2016w\u2217\u2016 2\u03c0 then \u2016ut\u2016 \u2265 \u2016w\u2217\u2016 2\u03c0 . Otherwise if \u2016wt\u2016 \u2264 (k2\u2212k)\u2016w\u2217\u2016 \u03b1(k)\u03c0k2 then \u2016ut\u2016 \u2265 \u2016wt\u2016. Since wt+1 = ut + \u03bb\u03c0k ( \u03c0 \u2212 \u03b8 ) w\u2217 and 0 < \u03b8t \u2264 \u03c02 we have \u2016wt+1\u2016 \u2265 \u2016ut\u2016 \u2265 min{\u2016w \u2217\u2016\n2\u03c0 , \u2016wt\u2016}. Now let k = 1. Note that in this case \u03bb\u0303 = \u03bb. First assume that \u03b8t < \u03c0 3 . If \u2016wt\u2016 \u2265 \u2016w\u2217\u2016 4 then,\nusing the same notation as in Lemma C.2, \u2016ut\u2016 \u2265 (1 \u2212 \u03bb) \u2016wt\u2016 + \u03bb\u2016w \u2217\u2016 sin \u03b8t \u03c0 \u2265 \u2016wt\u2016 2 \u2265 \u2016w\u2217\u2016 8 . Since wt+1 = ut + \u03bb \u03c0 ( \u03c0 \u2212 \u03b8t ) w\u2217 and 0 < \u03b8t \u2264 \u03c02 we have \u2016wt+1\u2016 \u2265 \u2016ut\u2016 \u2265 \u2016w\u2217\u2016 8 . If \u2016wt\u2016 < \u2016w\u2217\u2016 4 then by the facts 0 < \u03b8t \u2264 \u03c02 and cos \u03b8t > 1 2 we get\n\u2016wt+1\u20162 = \u2016ut\u20162 + 2 \u2016ut\u2016 \u2225\u2225\u2225\u2225\u03bb\u03c0(\u03c0 \u2212 \u03b8t)w\u2217 \u2225\u2225\u2225\u2225 cos \u03b8t + \u2225\u2225\u2225\u2225\u03bb\u03c0(\u03c0 \u2212 \u03b8t)w\u2217 \u2225\u2225\u2225\u22252\n\u2265 (1\u2212 \u03bb)2 \u2016wt\u20162 + (1\u2212 \u03bb)\u03bb\n2 \u2016wt\u2016 \u2016w\u2217\u2016+\n\u03bb2\n4 \u2016w\u2217\u20162\n\u2265 (1\u2212 \u03bb)2 \u2016wt\u20162 + 2(1\u2212 \u03bb)\u03bb \u2016wt\u20162 + 4\u03bb2 \u2016wt\u20162 = (1 + 3\u03bb2) \u2016wt\u20162 \u2265 \u2016wt\u20162\n(25)\nFinally, assume \u03b8t \u2265 \u03c03 . As in the proof of Lemma C.2, if \u2016wt\u2016 \u2265 \u2016w\u2217\u2016 sin \u03b8t \u03c0 \u2265 \u221a 3 2 \u2016w\u2217\u2016 \u03c0 then\n\u2016wt+1\u2016 \u2265 \u2016ut\u2016 \u2265 \u221a 3 2 \u2016w\u2217\u2016 \u03c0 . Otherwise, if \u2016wt\u2016 < \u2016w\u2217\u2016 sin \u03b8t \u03c0 then \u2016wt+1\u2016 \u2265 \u2016ut\u2016 \u2265 \u2016wt\u2016. This concludes our proof.\nWe can now show that in each iteration \u2016wt\u2016 is bounded away from 0 by a constant.\nProposition C.4. Assume GD is initialized at w0 such that \u03b80 6= \u03c0 and runs for T iterations with learning rate 0 < \u03bb < 12 . Then for all 0 \u2264 t \u2264 T ,\n\u2016wt\u2016 \u2265 min{\u2016w0\u2016 sin \u03b80, \u2016w\u2217\u2016 sin2 \u03b80 \u03b1(k)\u03c0 , \u2016w\u2217\u2016 8 }\nProof. Let \u03b80 > \u03b81 > ... > \u03b8T (by Lemma C.1). Let i be the last index such that \u03b8i > \u03c0 2 (if such i does not exist let i = \u22121). Since sin \u03b8j > sin \u03b80 for all 0 \u2264 j \u2264 i, by applying Lemma C.2 at most j + 1 times we have\n\u2016wj+1\u2016 \u2265 min{\u2016w0\u2016 sin \u03b80, \u2016w\u2217\u2016 sin2 \u03b80\n\u03b1(k)\u03c0 }\nfor all 0 \u2264 j \u2264 i. Finally, by Lemma C.3 and the fact that \u03b8j \u2264 \u03c02 for all i < j \u2264 T , we get\n\u2016wj\u2016 \u2265 min{\u2016wi+1\u2016 , \u2016w\u2217\u2016\n8 }\nfor all i+ 1 < j \u2264 T , from which the claim follows.\nThe following lemma shows that \u2207` is Lipschitz continuous at points that are bounded away from 0.\nLemma C.5. Assume \u2016w1\u2016 , \u2016w2\u2016 \u2265M , w1,w2 and w\u2217 are on the same two dimensional half-plane defined by w\u2217, then\n\u2016\u2207`(w1)\u2212\u2207`(w2)\u2016 \u2264 L \u2016w1 \u2212w2\u2016\nfor L = 1 + 3\u2016w \u2217\u2016\nM .\nProof. Recall that by equality Eq. 18,\n\u2202g \u2202w (w,w\u2217) = 1 2\u03c0 \u2016w\u2217\u2016 w \u2016w\u2016 sin \u03b8w,w\u2217 + 1 2\u03c0\n( \u03c0 \u2212 \u03b8w,w\u2217 ) w\u2217\nLet \u03b81 and \u03b82 be the angles between w1,w \u2217 and w2,w \u2217, respectively. By the inequality x0 sin xsin x0 \u2265 x for 0 \u2264 x \u2264 x0 < \u03c0 and since |\u03b81\u2212\u03b82|2 \u2264 \u03c0 2 we have\n|\u03b81 \u2212 \u03b82| 2 \u2264 \u03c0 sin |\u03b81\u2212\u03b82|2 2\nFurthermore \u2016w1 \u2212w2\u2016 is minimized (for fixed angles \u03b81 and \u03b82) when \u2016w1\u2016 = \u2016w2\u2016 = M and is equal to 2M sin |\u03b81\u2212\u03b82|2 . Thus, under our assumptions we have,\n|\u03b81 \u2212 \u03b82| 2 \u2264 \u03c0 sin |\u03b81\u2212\u03b82|2 2 \u2264 \u03c0 \u2016w1 \u2212w2\u2016 4M\nThus we get \u2225\u2225\u2225\u2225 12\u03c0(\u03c0 \u2212 \u03b81)w\u2217 \u2212 12\u03c0(\u03c0 \u2212 \u03b82)w\u2217 \u2225\u2225\u2225\u2225 \u2264 \u2016w\u2217\u20164M \u2016w1 \u2212w2\u2016\nFor the first summand, we will first find the parameterization of a two dimensional vector of length sin \u03b8 where \u03b8 is the angle between the vector and the positive x axis. Denote this vector by (a, b), then the following holds\na2 + b2 = sin2 \u03b8\nand b\na = tan \u03b8\nThe solution to these equations is (a, b) = ( sin 2\u03b82 , sin 2 \u03b8). Hence (here we use the fact that w1,w2 are on the same half-plane)\u2225\u2225\u2225\u2225 12\u03c0 \u2016w\u2217\u2016 w1\u2016w1\u2016 sin \u03b81 \u2212 12\u03c0 \u2016w\u2217\u2016 w2\u2016w2\u2016 sin \u03b82 \u2225\u2225\u2225\u2225 = 12\u03c0 \u2016w\u2217\u2016 \u221a( sin 2\u03b81 2 \u2212 sin 2\u03b82 2 )2 + ( sin2 \u03b81 \u2212 sin2 \u03b82 )2\n\u2264 1 2\u03c0 \u2016w\u2217\u2016\n\u221a (\u03b81 \u2212 \u03b82)2 + 4(\u03b81 \u2212 \u03b82)2\n\u2264 \u221a 5\n\u03c0 \u2016w\u2217\u2016 \u03c0 \u2016w1 \u2212w2\u2016 4M\n= \u221a 5 \u2016w\u2217\u2016 4M\n\u2016w1 \u2212w2\u2016 (26)\nwhere the first inequality follows from the fact that | sinx\u2212sin y| \u2264 |x\u2212y| and the second inequality from previous results. In conclusion, we have\u2225\u2225\u2225\u2225 \u2202g\u2202w (w1,w\u2217)\u2212 \u2202g\u2202w (w2,w\u2217)\n\u2225\u2225\u2225\u2225 \u2264 (\u221a5 + 1) \u2016w\u2217\u20164M \u2016w1 \u2212w2\u2016 Similarly, in order to show that the function f(w) = w\u2016w\u2016 is Lipschitz continuous, we parameterize the unit vector by (cos \u03b8, sin \u03b8) where \u03b8 is the angle between the vector and the positive x axis. We now obtain \u2225\u2225\u2225\u2225 w1\u2016w1\u2016 \u2212 w2\u2016w2\u2016\n\u2225\u2225\u2225\u2225 = \u221a(cos \u03b81 \u2212 cos \u03b82)2 + (sin \u03b81 \u2212 sin \u03b82)2 \u2264 \u221a 2(\u03b81 \u2212 \u03b82)2\n\u2264 \u03c0 \u2016w1 \u2212w2\u2016\u221a 2M\n(27)\nNow we can conclude that\n\u2016\u2207`(w1)\u2212\u2207`(w2)\u2016 \u2264 (1 k + k2 \u2212 k \u03c0k2 ) \u2016w1 \u2212w2\u2016+ 2 k \u2225\u2225\u2225\u2225 \u2202g\u2202w (w1,w\u2217)\u2212 \u2202g\u2202w (w2,w\u2217) \u2225\u2225\u2225\u2225\n+ ( (k2 \u2212 k) \u2016w\u2217\u2016\n\u03c0k2\n)\u2225\u2225\u2225\u2225 w1\u2016w1\u2016 \u2212 w2\u2016w2\u2016 \u2225\u2225\u2225\u2225\n\u2264 (1 k + k2 \u2212 k \u03c0k2 + (k2 \u2212 k) \u2016w\u2217\u2016\u221a 2Mk2 + ( \u221a 5 + 1) \u2016w\u2217\u2016 2Mk ) \u2016w1 \u2212w2\u2016 \u2264 1 + \u2016w \u2217\u2016\u221a\n2M +\n( \u221a\n5 + 1) \u2016w\u2217\u2016 2M\n\u2264 1 + 3 \u2016w \u2217\u2016\nM\n(28)\nGiven that ` is Lipschitz continuous we can now follow standard optimization analysis ((Nesterov, 2004)) to show that limt\u2192\u221e \u2016\u2207`(wt)\u2016 = 0.\nProposition C.6. Assume GD is initialized at w0 such that \u03b80 6= \u03c0 and runs with a constant learning rate 0 < \u03bb < min{ 2L , 1 2} where L = O\u0303(1). Then for all T\nT\u2211 t=0 \u2016\u2207`(wt)\u20162 \u2264 1 \u03bb(1\u2212 \u03bb2L) `(w0)\nProof. We will need the following lemma\nLemma C.7. Let f : Rn \u2192 R be a continuously differentiable function on a set D \u2286 Rn and x, y \u2208 D such that for all 0 \u2264 \u03c4 \u2264 1, x + \u03c4(y \u2212 x) \u2208 D and \u2016\u2207f(x+ \u03c4(y \u2212 x))\u2212\u2207f(x)\u2016 \u2264 L \u2016x\u2212 y\u2016. Then we have\n|f(y)\u2212 f(x)\u2212 \u3008\u2207f(x), y \u2212 x\u3009| \u2264 L 2 \u2016x\u2212 y\u20162\nProof. The proof exactly follows the proof of Lemma 1.2.3 in (Nesterov, 2004) and note that the proof only requires Lipschitz continuity of the gradient on the set S = {x+ \u03c4(y \u2212 x) | 0 \u2264 \u03c4 \u2264 1} and that S \u2286 D.\nBy Proposition C.4, for all t, \u2016wt\u2016 \u2265M \u2032 where\nM \u2032 = min{\u2016w0\u2016 sin \u03b80, \u2016w\u2217\u2016 sin2 \u03b80 \u03b1(k)\u03c0 , \u2016w\u2217\u2016 8 }\n. Furthermore, by a simple geometric observation we have\nmin 0\u2264\u03c4\u22641,\u2016w1\u2016,\u2016w2\u2016\u2265M \u2032,arccos (\nw1\u00b7w2 \u2016w1\u2016\u2016w2\u2016\n) =\u03b8 \u2016\u03c4w1 + (1\u2212 \u03c4)w2\u2016 = M \u2032 cos \u03b8 2\n. It follows by Lemma C.5 that for any t and x1,x2 \u2208 St , {wt + \u03c4(wt+1 \u2212wt) | 0 \u2264 \u03c4 \u2264 1},\n\u2016\u2207`(x1)\u2212\u2207`(x2)\u2016 \u2264 L \u2016x1 \u2212 x2\u2016\nwhere L = 1 + 3\u2016w \u2217\u2016\nM and M = M \u2032 cos \u03b802 (Note that cos \u03b8t\u2212\u03b8t+1 2 \u2265 cos \u03b80 2 for all t by Lemma C.1).\nHence by Lemma C.7, for any t we have\n`(wt+1) \u2264 `(wt) + \u3008\u2207`(wt),wt+1 \u2212wt\u3009+ L\n2 \u2016wt+1 \u2212wt\u20162\n= `(wt)\u2212 \u03bb(1\u2212 \u03bb\n2 L) \u2016\u2207`(wt)\u20162\n(29)\nwhich implies that\nT\u2211 t=0 \u2016\u2207`(wt)\u20162 \u2264 1 \u03bb(1\u2212 \u03bb2L) ( `(w0)\u2212 `(wT ) ) \u2264 1 \u03bb(1\u2212 \u03bb2L) `(w0)\nWe are now ready to prove the theorem.\nProof of Theorem 5.2. First, we observe that for a randomly initialized point w0, 0 \u2264 \u03b80 \u2264 \u03c0(1\u2212 \u03b4) with probability 1\u2212\u03b4. Hence by Proposition C.6 we have for L = 1+ 3\u2016w\n\u2217\u2016 M where M = min{sin(\u03c0(1\u2212\n\u03b4)), sin 2(\u03c0(1\u2212\u03b4)) \u03b1(k)\u03c0 , 1 8} cos( \u03c0(1\u2212\u03b4) 2 ) and \u03b1(k) = k + k2\u2212k \u03c0 , and for \u03bb = 1 L (we assume w.l.o.g. that L > 2),\nT\u2211 t=0 \u2016\u2207`(wt)\u20162 \u2264 1 \u03bb(1\u2212 \u03bb2L) `(w0) = 2L`(w0) \u2264 4L k2 (k 2 + k2 \u2212 k 2\u03c0 ) Therefore,\nmin 0\u2264t\u2264T\n{\u2016\u2207`(wt)\u20162} \u2264 4L k2\n( k 2 + k2\u2212k 2\u03c0 ) T\nIt follows that gradient descent reaches a point wt such that \u2016\u2207`(wt)\u2016 < after T iterations where\nT >\n( 4L k2 ( k 2 + k2\u2212k 2\u03c0 ))2 2\nWe will now show that if \u2016\u2207`(wt)\u2016 < then wt is O( \u221a )-close to the global minimum w\u2217. First note that if \u03c02 \u2264 \u03b8t \u2264 \u03c0(1\u2212\u03b4) then a vector of the form v = \u03b1w \u2217+\u03b2w where \u03b1 \u2265 0 is of minimal norm equal to \u03b1 sin(\u03c0\u2212 \u03b8t) \u2016w\u2217\u2016 when it is perpendicular to w. Since the gradient is a vector of this form, we have \u2016\u2207`(wt)\u2016 > \u03c0\u03b4\u2016w \u2217\u2016 sin\u03c0\u03b4 \u03c0k \u2265 \u03b4 sin\u03c0\u03b4 k \u2265 . Hence, from now on we assume that 0 \u2264 \u03b8t < \u03c0 2 .\nSimilarly to the previous argument, we have\n> \u2016\u2207`(wt)\u2016 > \u2016w\u2217\u2016 (\u03c0 \u2212 \u03c02 ) sin \u03b8t\n\u03c0k \u2265 sin \u03b8t 2k\nHence, \u03b8t < arcsin(2k ) = O( ). It follows by the triangle inequality that\nk2 > k2 \u2016\u2207`(wt)\u2016 = \u2225\u2225\u2225\u2225\u2225 ( k + k2 \u2212 k \u03c0 \u2212 k \u2016w \u2217\u2016 \u03c0 \u2016wt\u2016 sin \u03b8t \u2212 k2 \u2212 k \u03c0 \u2016w\u2217\u2016 \u2016wt\u2016 ) wt \u2212 k(\u03c0 \u2212 \u03b8t) \u03c0 w\u2217 \u2225\u2225\u2225\u2225\u2225 \u2265 \u2225\u2225\u2225\u2225\u2225 ( k + k2 \u2212 k \u03c0 \u2212 k 2 \u2212 k \u03c0 \u2016w\u2217\u2016 \u2016wt\u2016 ) wt \u2212 kw\u2217\n\u2225\u2225\u2225\u2225\u2225\u2212 k \u2016w\u2217\u2016\u03c0 sin \u03b8t \u2212 k\u03b8t \u2016w\u2217\u2016\u03c0 \u2265 \u2225\u2225\u2225\u2225\u2225 ( k + k2 \u2212 k \u03c0 \u2212 k 2 \u2212 k \u03c0 \u2016w\u2217\u2016 \u2016wt\u2016 ) wt \u2212 k \u2016w\u2217\u2016 \u2016wt\u2016 wt\n\u2225\u2225\u2225\u2225\u2225 \u2212 \u2225\u2225\u2225\u2225kw\u2217 \u2212 k \u2016w\u2217\u2016\u2016wt\u2016 wt\n\u2225\u2225\u2225\u2225\u2212 k \u2016w\u2217\u2016\u03c0 sin \u03b8t \u2212 k\u03b8t \u2016w\u2217\u2016\u03c0 \u2265 ( k +\nk2 \u2212 k \u03c0\n) | \u2016wt\u2016 \u2212 \u2016w\u2217\u2016 | \u2212 k \u2016w\u2217\u2016 \u03b8t \u2212\nk \u2016w\u2217\u2016 \u03c0 sin \u03b8t \u2212 k\u03b8t \u2016w\u2217\u2016 \u03c0\n(30)\nwhere the last inequality follows since the arc of a circle is larger than its corresponding segment. Therefore we get | \u2016wt\u2016 \u2212 \u2016w\u2217\u2016 | < O( ). By the bounds on \u03b8t and | \u2016wt\u2016 \u2212 \u2016w\u2217\u2016 | and the\ninequality cosx \u2265 1\u2212 x for x \u2265 0, we can give an upper bound on \u2016wt \u2212w\u2217\u2016:\n\u2016wt \u2212w\u2217\u20162 = \u2016wt\u20162 \u2212 2 \u2016wt\u2016 \u2016w\u2217\u2016 cos \u03b8t + \u2016w\u2217\u20162\n= \u2016wt\u2016 (\u2016wt\u2016 \u2212 \u2016w\u2217\u2016 cos \u03b8t) + \u2016w\u2217\u2016 (\u2016w\u2217\u2016 \u2212 \u2016wt\u2016 cos \u03b8t) \u2264 (\u2016w\u2217\u2016+O( ))(O( ) + \u03b8t \u2016w\u2217\u2016) + \u2016w\u2217\u2016 (O( 2) + \u03b8t \u2016w\u2217\u2016) = O( )\n(31)\nFinally, to prove the claim it suffices to show that `(w) \u2264 d\u2016w \u2212w\u2217\u20162. Denote the input vector x = (x1,x2, ...,xk) where xi \u2208 Rm for all 1 \u2264 i \u2264 k. Then we get\n`(w) = Ex [\u2211k i=1 \u03c3(w Txi) k \u2212 \u2211k i=1 \u03c3(w \u2217Txi) k ]2 \u2264 Ex [\u2211k i=1 |\u03c3(wTxi)\u2212 \u03c3(w\u2217\nTxi)| k ]2 \u2264 Ex [\u2211k i=1 |wTxi \u2212w\u2217\nTxi| k ]2 \u2264 Ex [\u2211k i=1 \u2016w \u2212w\u2217\u2016 \u2016xi\u2016\nk ]2 \u2264 \u2016w \u2212w\u2217\u20162Ex\u2016x\u20162 = d\u2016w \u2212w\u2217\u20162\n(32)\nwhere the second inequality follows from Lipschitz continuity of \u03c3, the third inequality from the Cauchy-Schwarz inequality and the last equality since \u2016x\u20162 follows a chi-squared distribution with d degrees of freedom."}, {"heading": "D Missing Proofs for Section 7.1", "text": "D.1 Proof of Proposition 7.1\nDefine wp = (w2, w1), w \u2217 p1 = (0,\u2212w \u2217) and w\u2217p2 = (w \u2217, 0). We first prove the following lemma.\nLemma D.1. Let l be defined as in Eq. 16. Then\n\u2207l(w) = 1 k2\n[( k +\nk2 \u2212 3k + 2 \u03c0\n) w +\n2(k \u2212 1) sin \u03b8wr,wl \u03c0 w\n+ (k \u2212 1)(\u03c0 \u2212 \u03b8wr,wl)\n\u03c0 wp \u2212 (k2 \u2212 3k + 2) \u2016w\u2217\u2016 \u03c0 \u2016w\u2016 w\n\u2212 k \u2016w \u2217\u2016 sin \u03b8w,w\u2217 \u03c0 \u2016w\u2016 w \u2212 k(\u03c0 \u2212 \u03b8w,w \u2217) \u03c0 w\u2217 \u2212 (k \u2212 1) sin \u03b8wl,w\u2217r \u2016w\n\u2217\u2016 \u03c0 \u2016w\u2016 w \u2212 (k \u2212 1)(\u03c0 \u2212 \u03b8wl,w\u2217r ) \u03c0 w\u2217p2\n\u2212 (k \u2212 1) sin \u03b8wr,w\u2217l \u2016w \u2217\u2016 \u03c0 \u2016w\u2016 w \u2212 (k \u2212 1)(\u03c0 \u2212 \u03b8wr,w\u2217l ) \u03c0 w\u2217p1\n]\nProof. The gradient does not follow immediately from Lemma 3.2 because the loss has expressions with of the function g but with different dependencies on the parameters in w. We will only calculate \u2202g(wr,wl)\n\u2202w , the other expressions are calculated in the same manner. Recall that\ng(wr,wl) = 1\n2\u03c0 \u2016w\u20162(sin \u03b8wr,wl + (\u03c0 \u2212 \u03b8wr,wl) cos \u03b8wr,wl)\nIt follows that\n\u2202g(wr,wl)\n\u2202w =\n1 \u03c0 (sin \u03b8wr,wl + (\u03c0 \u2212 \u03b8wr,wl) cos \u03b8wr,wl)w + 1 2\u03c0 \u2016w\u20162(\u03c0 \u2212 \u03b8wr,wl) \u2202 cos \u03b8wr,wl \u2202w\n(33)\nLet w = (w1, w2) then cos \u03b8wr,wl = w1w2 w21+w 2 2 . Then,\n\u2202 cos \u03b8wr,wl \u2202w1 = w2(w\n2 1 + w 2 2)\u2212 2w21w2 (w21 + w 2 2) 2 = w2 \u2016w\u20162 \u2212 2w1 cos \u03b8wr,wl \u2016w\u20162\nand \u2202 cos \u03b8wr,wl\n\u2202w2 = w1(w\n2 1 + w 2 2)\u2212 2w22w1 (w21 + w 2 2) 2 = w1 \u2016w\u20162 \u2212 2w2 cos \u03b8wr,wl \u2016w\u20162\nor equivalently \u2202 cos \u03b8wr,wl \u2202w = wp \u2016w\u20162 \u2212 2w cos \u03b8wr,wl \u2016w\u20162 . It follows that\n\u2202g(wr,wl)\n\u2202w =\nsin \u03b8wr,wlw\n\u03c0 + (\u03c0 \u2212 \u03b8wl,wr ) 2\u03c0 wp\nWe will prove that wt+1 6= 0 and that it is in the interior of the fourth quadrant. Denote w = wt and \u2207l(w) = 1k2 ( B1(w) +B2(w) +B3(w) ) where\nB1(w) = ( k +\nk2 \u2212 3k + 2 \u03c0\n) w +\n2(k \u2212 1) sin \u03b8wr,wl \u03c0 w \u2212 (k 2 \u2212 3k + 2) \u2016w\u2217\u2016 \u03c0 \u2016A\u2016 w\n\u2212 k \u2016w \u2217\u2016 sin \u03b8w,w\u2217 \u03c0 \u2016w\u2016 w \u2212 (k \u2212 1) sin \u03b8wl,w\u2217r \u2016w \u2217\u2016 \u03c0 \u2016w\u2016 w \u2212 (k \u2212 1) sin \u03b8wr,w\u2217l \u2016w \u2217\u2016 \u03c0 \u2016w\u2016 w\nB2(w) = (k \u2212 1)(\u03c0 \u2212 \u03b8wr,wl)\n\u03c0 wp\n(34)\nand\nB3(w) = \u2212 k(\u03c0 \u2212 \u03b8w,w\u2217)\n\u03c0 w\u2217 \u2212 (k \u2212 1)(\u03c0 \u2212 \u03b8wl,w\u2217r ) \u03c0 w\u2217p2 \u2212 (k \u2212 1)(\u03c0 \u2212 \u03b8wr,w\u2217l ) \u03c0 w\u2217p1\n(35)\nLet w = (w,\u2212mw) for w,m \u2265 0. Straightforward calculation shows that cos \u03b8wl,w\u2217r = 1\u221a\n2(1+m2)\nand cos \u03b8wr,w\u2217l = m\u221a 2(m2+1) . Hence \u03c04 \u2264 \u03b8wl,w\u2217r , \u03b8wr,w\u2217l \u2264 \u03c0 2 . Since w is in the fourth quadrant we also have 3\u03c04 \u2264 \u03b8w,w\u2217 \u2264 \u03c0. Therefore, adding \u2212\u03bbB3(w) can only increase \u2016w\u2016. This follows since in the worst case (the least possible increase of \u2016w\u2016)\n\u2212B3(w) = k\n4 w\u2217 + k \u2212 1 2 w\u2217p2 + k \u2212 1 2 w\u2217p1 = ( k \u2212 2 4 w\u2217,\u2212k \u2212 2 4 w\u2217)\nwhich is in the fourth quadrant for k \u2265 2. In addition, since \u2212wp is in the fourth quadrant then adding \u2212\u03bbB2(w) increases \u2016w\u2016.\nIf \u2016w\u2016 < \u2016w \u2217\u2016\n16 then \u2212B1(w) points in the direction of w since in this case \u2212B1(w) = \u03b1w where\n\u03b1 \u2265 (k2 \u2212 3k + 2\n\u03c0 + (k \u2212 1) \u03c0 \u2212 k \u2212 1 8\u03c0 \u2212 k 2 \u2212 3k + 2 16\u03c0 \u2212 k 16\n) \u2016w\u2217\u2016 > 0\nfor k \u2265 2. If \u2212B1(w) points in the direction of \u2212w then by the assumption that \u03bb \u2208 (0, 13 ) we have \u2016\u03bbB1(w)\u2016 < \u2016w\u2016. Thus we can conclude that wt+1 6= 0.\nNow, let w = (w1, w2), \u03b8t be the angle between w = wt and the positive x axis and first assume that w1 > \u2212w2. In this case \u2212B3(w) least increases (or even most decreases) \u03b8t when\n\u2212B3(w) = k\n4 w\u2217 + 3(k \u2212 1) 4 w\u2217p2 + k \u2212 1 2 w\u2217p1 = (2k \u2212 3 4 w\u2217, 2\u2212 k 4 w\u2217 )\nwhich is a vector in the fourth quadrant for k \u2265 2. Otherwise, \u2212B3(w) is a vector in the fourth quadrant as well. Note that we used the facts \u03c04 \u2264 \u03b8wl,w\u2217r , \u03b8wr,w\u2217l \u2264 \u03c0 2 and 3\u03c0 4 \u2264 \u03b8w,w\u2217 \u2264 \u03c0. Since \u2212\u03bbB1(w) does not change \u03b8t and \u2212\u03bbB2(w) increases \u03b8t but never to an angle greater than or equal to \u03c02 , it follows that 0 < \u03b8t+1 < \u03c0 2 .\nIf w1 \u2264 \u2212w2 then by defining all angles with respect to the negative y axis, we get the same argument as before. This shows that wt+1 is in the interior of the fourth quadrant, which concludes our proof.\nD.2 Proof of Proposition 7.2\nWe will need the following auxiliary lemmas. Lemma D.2. Let w be in the fourth quadrant, then g(wl,wr) \u2265 12\u03c0 (\u221a 3 2 \u2212 \u03c0 6 ) \u2016w\u20162.\nProof. First note that the function s(\u03b8) = sin \u03b8+ (\u03c0\u2212 \u03b8) cos \u03b8 is decreasing as a function of \u03b8 \u2208 [0, \u03c0]. Let w = (w,\u2212mw) for w,m \u2265 0. Straightforward calculation shows that cos \u03b8wl,wr = \u2212 mm2+1 . As a function of m \u2208 [0,\u221e), cos \u03b8wl,wr is minimized for m = 1 with value \u2212 12 , i.e., when \u03b8(wl,wr) = 2\u03c0 3\nand this is the largest angle possible. Thus g(wl,wr) \u2265 12\u03c0 s( 2\u03c0 3 ) ) \u2016w\u20162 = 12\u03c0 (\u221a 3 2 \u2212 \u03c0 6 ) \u2016w\u20162.\nLemma D.3. Let f(\u03b8) = 2k ( sin( 3\u03c0\n4 + \u03b8) + (\n\u03c0 4 \u2212 \u03b8) cos(3\u03c0 4 + \u03b8) ) + ( 2k \u2212 2 )(\u221a 1\u2212 cos \u03b8 2\n2 + (\u03c0 \u2212 arccos cos \u03b8\u221a 2 ) cos \u03b8\u221a 2\n) + ( 2k \u2212 2 )(\u221a 1\u2212 sin \u03b8 2\n2 + (\u03c0 \u2212 arccos sin \u03b8\u221a 2 ) sin \u03b8\u221a 2 ) (36)\n, then in the interval \u03b8 \u2208 [0, \u03c04 ], f(\u03b8) is maximized at \u03b8 = \u03c0 4 for all k \u2265 2.\nProof. We will maximize the function f(\u03b8)2(k\u22121) = k k\u22121f1(\u03b8) + f2(\u03b8) + f3(\u03b8) where f1(\u03b8), f2(\u03b8), f3(\u03b8) correspond to the three summands in the expression of f(\u03b8). Since for h(x) = \u221a 1\u2212 x2 + (\u03c0 \u2212 arccos(x))x we have h\u2032(x) = \u03c0 \u2212 arccos(x), it follows that f \u20322(\u03b8) = \u2212(\u03c0 \u2212 arccos cos \u03b8\u221a2 ) sin \u03b8\u221a 2 , f \u20323(\u03b8) = (\u03c0 \u2212 arccos sin \u03b8\u221a2 ) cos \u03b8\u221a 2 and f \u20321(\u03b8) = \u2212(\u03c04 \u2212 \u03b8) sin( 3\u03c0 4 + \u03b8). It therefore suffices to show that\nd1(\u03b8) := (\u03c0 \u2212 arccos sin \u03b8\u221a 2 ) cos \u03b8\u221a 2 \u2212 (\u03c0 \u2212 arccos cos \u03b8\u221a 2 ) sin \u03b8\u221a 2 \u2212 k k \u2212 1 ( \u03c0 4 \u2212 \u03b8) sin(3\u03c0 4 + \u03b8) \u2265 0\nfor \u03b8 \u2208 [0, \u03c04 ]. By applying the inequalities arccos(x) \u2264 \u03c02 \u2212 x for x \u2208 [0, 1] and arccos(x) \u2265 \u03c0 2 \u2212 x \u2212 1 10 for x \u2208 [ 12 , 1\u221a 2 ] we get d1(\u03b8) \u2265 d2(\u03b8) where\nd2(\u03b8) = (\u03c0\n2 + sin \u03b8\u221a 2 )cos \u03b8\u221a 2 \u2212 (\u03c0 2 + cos \u03b8\u221a 2 + 1 10 ) sin \u03b8\u221a 2 \u2212 k k \u2212 1 ( \u03c0 4 \u2212 \u03b8) sin(3\u03c0 4 + \u03b8)\n= \u03c0\n2 \u221a 2 cos \u03b8 \u2212 ( \u03c0 2 \u221a 2 + 1 10 \u221a 2 ) sin \u03b8 \u2212 k k \u2212 1 ( \u03c0 4 \u2212 \u03b8) sin(3\u03c0 4 + \u03b8)\n(37)\nWe notice that d2(0) \u2265 0 and d2( 34 ) \u2265 0 for all k \u2265 2. In addition,\nd\u20322(\u03b8) = \u2212 \u03c0\n2 \u221a 2 sin \u03b8 \u2212 ( \u03c0 2 \u221a 2 + 1 10 \u221a 2 ) cos \u03b8 + k k \u2212 1 sin( 3\u03c0 4 + \u03b8)\u2212 k k \u2212 1 ( \u03c0 4 \u2212 \u03b8) cos(3\u03c0 4 + \u03b8)\nand d\u20322(0) > 0 for all k \u2265 2. It follows that in order to show that d2(\u03b8) \u2265 0 for \u03b8 \u2208 [0, 34 ] and k \u2265 2, it suffices to show that d\u2032\u20322(\u03b8) \u2264 0 for \u03b8 \u2208 [0, 34 ] and k \u2265 2. Indeed,\nd\u2032\u20322(\u03b8) = \u2212 \u03c0\n2 \u221a 2 cos \u03b8 + ( \u03c0 2 \u221a 2 + 1 10 \u221a 2 ) sin \u03b8 + 2k k \u2212 1 cos( 3\u03c0 4 + \u03b8) + k k \u2212 1 ( \u03c0 4 \u2212 \u03b8) sin(3\u03c0 4 + \u03b8)\n\u2264 ( 1\n10 \u221a 2 +\nk k \u2212 1 \u03c0 4\n) max{sin \u03b8, sin(3\u03c0\n4 + \u03b8)}+ 2k k \u2212 1 cos( 3\u03c0 4 + \u03b8) \u2264 0\n(38)\nfor all \u03b8 \u2208 [0, 34 ] and k \u2265 2. Note that the first inequality follows since cos \u03b8 \u2265 sin \u03b8 and the second since cos( 3\u03c04 +\u03b8) \u2265 max{sin \u03b8, sin( 3\u03c0 4 +\u03b8)}, both for \u03b8 \u2208 [0, 3 4 ]. This shows that d1(\u03b8) \u2265 0 for \u03b8 \u2208 [0, 3 4 ].\nNow assume that \u03b8 \u2208 [ 34 , \u03c0 4 ]. Since d1( 3 4 ) \u2265 0 and d1( \u03c0 4 ) \u2265 0, it suffices to prove that d \u2032 1(\u03b8) \u2264 0 for\n\u03b8 \u2208 [ 34 , \u03c0 4 ]. Indeed, for all \u03b8 \u2208 [ 3 4 , \u03c0 4 ]\nd\u20321(\u03b8) = \u2212(\u03c0 \u2212 arccos cos \u03b8\u221a 2 ) cos \u03b8\u221a 2 \u2212 (\u03c0 \u2212 arccos sin \u03b8\u221a 2 ) sin \u03b8\u221a 2\n+ cos2 \u03b8 2 \u221a 1\u2212 sin2 \u03b82 +\nsin2 \u03b8 2 \u221a 1\u2212 cos2 \u03b82 + k k \u2212 1 sin( 3\u03c0 4 + \u03b8)\u2212 k k \u2212 1 ( \u03c0 4 \u2212 \u03b8) cos(3\u03c0 4 + \u03b8)\n\u2264 \u2212(\u03c0 \u2212 arccos cos(\u03c04 )\u221a 2 ) cos(\u03c04 )\u221a 2 \u2212 (\u03c0 \u2212 arccos sin( 34 )\u221a 2 ) sin( 34 )\u221a 2 + cos2( 34 )\n2 \u221a 1\u2212 sin 2(\u03c04 )\n2\n+ sin2(\u03c04 )\n2 \u221a 1\u2212 cos 2( 34 )\n2\n+ 2 sin( 3\u03c0\n4 +\n3 4 )\u2212 2(\u03c0 4 \u2212 3 4 ) cos( 3\u03c0 4 + 3 4 ) < 0\n(39)\nWe conclude that d1(\u03b8) \u2265 0 for all \u03b8 \u2208 [0, \u03c04 ] as desired.\nProof of Proposition 7.2. First assume that w1 \u2265 \u2212w2. Let \u03b8 be the angle between w and the positive x axis. Then cos \u03b8 = w1\u2016w\u2016 and tan \u03b8 = \u2212 w2 w1 . Therefore we get\ncos \u03b8wl,w\u2217r = w1\n\u2016w\u2016 \u221a 2 = cos \u03b8\u221a 2\nand\ncos \u03b8wr,w\u2217l = \u2212w2 \u2016w\u2016 \u221a 2 = cos \u03b8 tan \u03b8\u221a 2 = sin \u03b8\u221a 2\nWe can rewrite `(w) as\n`(w) = 1\nk2\n[ k2 \u2212 3k + 2\n2\u03c0 (\u2016w\u2016 \u2212 \u2016w\u2217\u2016)2 + k 2 \u2016w\u20162 + 2(k \u2212 1)g(wr,wl)\n\u2212 \u2016w\u2016 \u2016w \u2217\u2016\n2\u03c0\n( 2k ( sin( 3\u03c0\n4 + \u03b8) + (\n\u03c0 4 \u2212 \u03b8) cos(3\u03c0 4 + \u03b8) )) + ( 2k \u2212 2 )(\u221a 1\u2212 cos \u03b8 2\n2 + (\u03c0 \u2212 arccos cos \u03b8\u221a 2 ) cos \u03b8\u221a 2\n) + ( 2k \u2212 2 )(\u221a 1\u2212 sin \u03b8 2\n2 + (\u03c0 \u2212 arccos sin \u03b8\u221a 2 ) sin \u03b8\u221a 2 )) + k\n2 \u2016w\u2217\u20162 + 2(k \u2212 1)g(w\u2217r ,w\u2217l ) ] (40)\nHence by Lemma D.2 and Lemma D.3 we can lower bound `(w) as follows\n`(w) \u2265 1 k2\n[ k2 \u2212 3k + 2\n2\u03c0 (\u2016w\u2016 \u2212 \u2016w\u2217\u2016)2 + k 2 \u2016w\u20162 + k \u2212 1 \u03c0 (\u221a3 2 \u2212 \u03c0 6 ) \u2016w\u20162\n\u2212 (k \u2212 1) \u2016w\u2016 \u2016w \u2217\u2016\n\u03c0\n(\u221a 3 + 2\u03c0\n3\n) + k\n2 \u2016w\u2217\u20162 + k \u2212 1 \u03c0 (\u221a3 2 \u2212 \u03c0 6 ) \u2016w\u2217\u20162 ] (41) By setting \u2016w\u2016 = \u03b1 \u2016w\u2217\u2016 we get\n`(w) \u2016w\u2217\u20162 \u2265 1 k2\n[ k2 \u2212 3k + 2\n2\u03c0 (\u03b1\u2212 1)2 + k 2 \u03b12 + k \u2212 1 \u03c0 (\u221a3 2 \u2212 \u03c0 6 ) \u03b12\n\u2212 (k \u2212 1) \u03c0\n(\u221a 3 + 2\u03c0\n3\n) \u03b1+ k\n2 + k \u2212 1 \u03c0 (\u221a3 2 \u2212 \u03c0 6\n)] (42)\nSolving for \u03b1 that minimizes the latter expression we obtain\n\u03b1\u2217 = k2\u22123k+2 \u03c0 + (k\u22121) \u03c0\n(\u221a 3 + 2\u03c03 ) k + k\n2\u22123k+2 \u03c0 + 2(k\u22121) \u03c0 ( \u221a 3 2 \u2212 \u03c0 6\n) = h(k) h(k) + 1\nPlugging \u03b1\u2217 back to the inequality we get\n`(w) \u2265 1 k2 (h(k) + 1 2 (\u03b1\u2217)2 \u2212 h(k)\u03b1\u2217 + h(k) + 1 2 ) \u2016w\u2217\u20162 = 2h(k) + 1 k2(2h(k) + 2) \u2016w\u2217\u20162\nand for w\u0303 = \u2212\u03b1\u2217w\u2217 it holds that `(w\u0303) = 2h(k)+1k2(2h(k)+2)\u2016w \u2217\u20162.\nFinally, assume w1 \u2264 \u2212w2. In this case, let \u03b8 be the angle between w and the negative y axis. Then cos \u03b8 = \u2212w2\u2016w\u2016 and tan \u03b8 = \u2212 w1 w2 . Therefore\ncos \u03b8wl,w\u2217r = w1\n\u2016w\u2016 \u221a 2 = cos \u03b8 tan \u03b8\u221a 2 = sin \u03b8\u221a 2\nand\ncos \u03b8wr,w\u2217l = \u2212w2 \u2016w\u2016 \u221a 2 = cos \u03b8\u221a 2\nNotice that from now on we get the same analysis as in the case where w1 \u2265 \u2212w2, where we switch between expressions with wl,w \u2217 r and expressions with wr,w \u2217 l . This concludes our proof."}, {"heading": "E Experimental Setup for Section 7.2", "text": "In our experiments we estimated the probability of convergence to the global minimum of a randomly initialized gradient descent for many different ground truths w\u2217 of a convolutional neural network with overlapping filters. For each value of number of hidden neurons, filter size, stride length and ground truth distribution we randomly selected 30 different ground truths w\u2217 with respect to the given distribution. We tested with all combinations of values given in Table 1.\nFurthermore, for each combination of values of number of hidden neurons, filter size and stride length we tested with deterministic ground truths: ground truth with all entries equal to 1, all entries equal to -1 and with entries that form an increasing sequence from -1 to 1, -2 to 0 and 0 to 2 or decreasing sequence from 1 to -1, 0 to -2 and 2 to 0.\nFor each ground truth, we ran gradient descent 20 times and for each run we recorded whether it reached a point very close to the unique global minimum or it repeatedly (5000 consecutive iterations) incurred very low gradient values and stayed away from the global minimum. We then calculated the empirical probability p\u0302 = #times reached global minimum20 . To compute the one-sided confidence interval we used the Wilson method ((Brown et al., 2001)) which gives a lower bound\np\u0302+ z2\u03b1 2n + z\u03b1 \u221a p\u0302(1\u2212p\u0302) n + z2\u03b1 4k2\n1 + z2\u03b1 n\n(43)\nwhere z\u03b1 is the Z-score with \u03b1 = 0.05 and in our experiments n = 20. Note that we initialized gradient descent inside a large hypercube such that outside the hypercube the gradient does not vanish (this can be easily proved after writing out the gradient for each setting).\nFor all ground truths we got p\u0302 \u2265 0.15, i.e., for each ground truth we reached the global minimum at least 3 times. Hence the confidence interval lower bound Eq. 43 is greater than 117 in all settings. This suggests that with a few dozen repeated runs of a randomly initialized gradient descent, with high probability it will converge to the global minimum."}, {"heading": "F Uniqueness of Global Minimum in the Population Risk", "text": "Without loss of generality we assume that the filter is of size 2 and the stride is 1. The proof of the general case follows the same lines. Assume that `(w) = 0 and denote w = (w1, w2), w \u2217 = (w\u22171 , w \u2217 2). Recall that `(w) = EG [ (f(x;W )\u2212 f(x;W \u2217))2 ] where f(x;W ) = 1k \u2211 i \u03c3 (wi \u00b7 x) and for all 1 \u2264 i \u2264 k wi = (0i\u22121,w,0d\u2212i\u22121). By equating `(w) to 0 we get that (f(x;W )\u2212 f(x;W \u2217))2 = 0 almost surely. Since (f(x;W ) \u2212 f(x;W \u2217))2 is a continuous function it follows that f(x;W ) \u2212 f(x;W \u2217) = 0 for all x. In particular this is true for x1 = (x, 0, 0, ..., 0), x \u2208 R. Thus \u03c3 (xw1) = \u03c3 (xw\u22171) for all x \u2208 R which implies that w1 = w \u2217 1 . The equality holds also for x2 = (0, x, 0, ..., 0), x \u2208 R which implies that \u03c3 (xw2)+\u03c3 (xw1) = \u03c3 (xw \u2217 2)+\u03c3 (xw \u2217 1) for all x \u2208 R. By the previous result, we get \u03c3 (xw2) = \u03c3 (xw\u22172) for all x \u2208 R and thus w2 = w\u22172 . We proved that w = w\u2217 and therefore w\u2217 is the unique global minimum."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Deep learning models are often successfully trained using gradient descent, despite the worst<lb>case hardness of the underlying non-convex optimization problem. The key question is then under<lb>what conditions can one prove that optimization will succeed. Here we provide a strong result<lb>of this kind. We consider a neural net with one hidden layer and a convolutional structure with<lb>no overlap and a ReLU activation function. For this architecture we show that learning is NP-<lb>complete in the general case, but that when the input distribution is Gaussian, gradient descent<lb>converges to the global optimum in polynomial time. To the best of our knowledge, this is the<lb>first global optimality guarantee of gradient descent on a convolutional neural network with ReLU<lb>activations.", "creator": "LaTeX with hyperref package"}}}