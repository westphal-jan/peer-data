{"id": "1702.08359", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Dynamic Word Embeddings", "abstract": "tezuka We samsonite present d\u00e9 a probabilistic cooperi language model for time - stamped milligrams text data tilberis which turanshah tracks mogens the bamum semantic evolution relentlessly of individual aybak words jazzist over time. bushmaster The bagenal model data represents words iyiola and contexts 63.04 by h\u1edbi latent trajectories in teamster an embedding nemesio space. At whil each moment in baytv time, justly the embedding vectors vuillemin are cdelgado inferred from frauds a pengkalan probabilistic version of word2vec [Mikolov, atholl 2013 ]. These photinia embedding vectors are broughty connected super-heavy in time bregenz through a latent diffusion 'cause process. ronal We tamen describe two scalable janlori variational 108.50 inference facs algorithms - - - skip - gram smoothing friesian and skip - sethupathy gram woosung filtering - - - darcie that allow us 115-year to tuitert train speedboats the wracked model 1999-present jointly over 125.5 all times; edict thus 1178 learning most-popular on non-consecutive all finw\u00eb data birt while simultaneously imia allowing 54.36 word and cow-calf context jerningham vectors to euphrates drift. Experimental results tamisuke on mentors three gabrielli different h-19 corpora muzzio demonstrate 938 that pryzbylewski our dynamic inactive model write-down infers word kulen embedding kachloul trajectories that be\u0142chat\u00f3w are more interpretable centerless and legatum lead to higher japandroids predictive reyno likelihoods than competing methods 112.12 that skillets are kuehnert based on static buku models sedley trained rights-of-way separately on time slices.", "histories": [["v1", "Mon, 27 Feb 2017 16:31:48 GMT  (1421kb,D)", "http://arxiv.org/abs/1702.08359v1", "10 pages + supplement"], ["v2", "Mon, 17 Jul 2017 23:45:06 GMT  (1390kb,D)", "http://arxiv.org/abs/1702.08359v2", "In the proceedings of the International Conference on Machine Learning (ICML 2017); 8 pages + references and supplement"]], "COMMENTS": "10 pages + supplement", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["robert bamler", "stephan mandt"], "accepted": true, "id": "1702.08359"}, "pdf": {"name": "1702.08359.pdf", "metadata": {"source": "CRF", "title": "Dynamic Word Embeddings via Skip-Gram Filtering", "authors": ["Robert Bamler", "Stephan Mandt"], "emails": ["ROBERT.BAMLER@DISNEYRESEARCH.COM", "STEPHAN.MANDT@DISNEYRESEARCH.COM"], "sections": [{"heading": "1. Introduction", "text": "Language evolves over time and words change their meaning due to cultural shifts, technological inventions, or political events. We consider the problem of detecting shifts in the meaning and usage of words over a given time span based on text data. Capturing these semantic shifts requires a dynamic language model.\nWord embeddings are a powerful tool for modeling semantic relations between individual words (Bengio et al., 2003; Mikolov et al., 2013a; Pennington et al., 2014; Mnih & Kavukcuoglu, 2013; Levy & Goldberg, 2014; Vilnis & McCallum, 2014; Rudolph et al., 2016). Word embeddings model the distribution of words based on their surrounding words, and summarize these statistics in terms of lowdimensional vector representations. Geometric distances\nbetween word vectors reflect semantic similarity (Mikolov et al., 2013a) and difference vectors encode semantic and syntactic relations (Mikolov et al., 2013c), which shows that they are sensible representations of language. Pretrained word embeddings are useful for various supervised tasks, including sentiment analysis (Socher et al., 2013b), semantic parsing (Socher et al., 2013a), and computer vision (Fu & Sigal, 2016). As unsupervised models, they have also been used for the exploration of word analogies and linguistics (Mikolov et al., 2013c).\nWord embeddings are currently formulated as static models, which assumes that the meaning of any given word is the same across the entire text corpus. In this paper, we propose a generalization of word embeddings to sequential data, such as corpora of historic texts or streams of text in social media.\nCurrent approaches to learning word embeddings in a dynamic context rely on grouping the data into time bins and training the embeddings separately on these bins (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016). This approach, however, raises three fundamental problems. First, since word embedding models are non-convex, training them twice on the same data will lead to different results. Thus, embedding vectors at successive times can only be approximately related to each other, and only if the embedding dimension is large (Hamilton et al., 2016). Second, dividing a corpus into separate time bins may lead to training sets that are too small to train a word embedding model. Hence, one runs the risk of overfitting to few data whenever the required temporal resolution is fine-grained, as we show in the experimental section. Third, due to the finite corpus size the learned word embedding vectors are subject to random noise. It is difficult to disambiguate this noise from systematic semantic drifts between subsequent times, in particular over short time spans, where we expect only minor semantic drift.\nIn this paper, we circumvent these problems by introducing a dynamic word embedding model. Our contributions are as follows:\nar X\niv :1\n70 2.\n08 35\n9v 1\n[ st\nat .M\nL ]\n2 7\nFe b\n20 17\n\u2022 We derive a probabilistic state space model where word and context embeddings evolve in time according to a diffusion process. It generalizes the skip-gram model (Mikolov et al., 2013b; Barkan, 2016) to a dynamic setup, which allows end-to-end training. This leads to continuous embedding trajectories, smoothes out noise in the word-context statistics, and allows us to share information across all times.\n\u2022 We propose two scalable black-box variational inference algorithms (Ranganath et al., 2014; Rezende et al., 2014) for filtering and smoothing. These algorithms find word embeddings that generalize better to held-out data. Our smoothing algorithm carries out efficient black-box variational inference for Gaussian structured variational distributions with tridiagonal precision matrices, and applies more broadly.\n\u2022 We analyze three massive text corpora that span over long periods of time. Our approach allows us to automatically find the words that change their meaning the most rapidly. It results in smooth word embedding trajectories and therefore allows us to measure and visualize the continuous dynamics of the entire embedding cloud as it deforms over time.\nThe benefits of our approach are best visualized in Figure 1. Here, we show the word embeddings for four consecutive years according to our proposed dynamical model (top) and a static model (bottom). Colored lines indicate the trajectories from the previous year. Both models were trained on the large Google books corpus of historical text (Michel et al., 2011). Our approach (top row) leads to more inter-\npretable trajectories, inferring semantic change over a single year for only few words in the vocabulary. This is not an artifact of the dimensionality reduction, as we show in Figure 3 in the experimental section.\nOur paper is structured as follows. In section 2 we discuss related work, and we introduce our model in section 3. In section 4 we present two efficient variational inference algorithms for our dynamic model. We show experimental results in section 5. Section 6 summarizes our findings."}, {"heading": "2. Related Work", "text": "Probabilistic models that have been extended to latent time series models are ubiquitous (Blei & Lafferty, 2006; Wang et al., 2008; Sahoo et al., 2012; Gultekin & Paisley, 2014; Charlin et al., 2015; Ranganath et al., 2015; Jerfel et al., 2017), but none of them relate to word embeddings. The closest of these models is the dynamic topic model (Blei & Lafferty, 2006; Wang et al., 2008), which learns the evolution of latent topics over time. Topic models are based on bag-of-word representations and thus treat words as symbols without modelling their semantic relations. They therefore serve a different purpose.\nMikolov et al. (2013a;b) proposed the skip-gram model with negative sampling (word2vec) as a scalable word embedding approach that relies on stochastic gradient descent. This approach has been formulated in a Bayesian setup (Barkan, 2016), which we discuss separately in section 3.1. These models, however, do not allow the word embedding vectors to change over time.\nSeveral authors have analyzed different statistics of text data to analyze semantic changes of words over time (Mihalcea & Nastase, 2012; Sagi et al., 2011; Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016). None of them explicitly model a dynamical process; instead, they slice the data into different time bins, fit the model separately on each bin, and further analyze the embedding vectors in post-processing. By construction, these static models can therefore not share statistical strength across time. This limits the applicability of static models to very large corpora.\nMost related to our approach are methods based on word embeddings. Kim et al. (2014) fit word2vec separately on different time bins, where the word vectors obtained for the previous bin are used to initialize the algorithm for the next time bin. The bins have to be sufficiently large and the found trajectories are not as smooth as ours, as we demonstrate in this paper. Hamilton et al. (2016) also trained word2vec separately on several large corpora from different decades. If the embedding dimension is large enough (and hence the optimization problem less non-convex), the authors argue that word embeddings at nearby times ap-\nproximately differ by a global rotation in addition to a small semantic drift, and they approximately compute this rotation. As the latter does not exist in a strict sense, it is difficult to distinguish artifacts of the approximate rotation from a true semantic drift. As discussed in this paper, both variants result in trajectories which are noisier."}, {"heading": "3. Model", "text": "We propose the dynamic skip-gram model, a generalization of the skip-gram model (word2vec) (Mikolov et al., 2013b) to sequential text data. The model finds word embedding vectors that continuously drift over time, allowing to track changes in language and word usage over short and long periods of time. Dynamic skip-gram is a probabilistic model which combines a Bayesian version of the skip-gram model (Barkan, 2016) with a latent time series. It is jointly trained end-to-end and scales to massive data by means of approximate Bayesian inference.\nThe observed data consist of sequences of words from a finite vocabulary of size L. In section 3.1, all sequences (sentences from books, articles, or tweets) are considered time-independent; in section 3.2 they will be associated with different time stamps. The goal is to maximize the probability of every word that occurs in the data given its surrounding words within a so-called context window. As detailed below, the model learns two vectors ui, vi \u2208 Rd for each word i in the vocabulary, where d is the embedding dimension. We refer to ui as the word embedding vector and to vi as the context embedding vector."}, {"heading": "3.1. Bayesian Skip-Gram Model", "text": "The Bayesian skip-gram model (Barkan, 2016) is a probabilistic version of word2vec (Mikolov et al., 2013b) and forms the basis of our approach. The graphical model is shown in Figure 2a). For each pair of words i, j in the vocabulary, the model assigns probabilities that word i appears in the context of word j. This probability is \u03c3(u>i vj) with the sigmoid function \u03c3(x) = 1/(1 + e\u2212x). Let zij \u2208\n{0, 1} be an indicator variable that denotes a draw from that probability distribution, hence p(zij = 1) = \u03c3(u>i vj). The generative model assumes that many word-word pairs (i, j) are uniformly drawn from the vocabulary and tested for being a word-context pair; hence a separate random indicator zij is associated with each drawn pair.\nFocusing on words and their neighbors in a context window, we collect evidence of word-word pairs for which zij = 1. These are called the positive examples. Denote n+ij the number of times that a word-context pair (i, j) is observed in the corpus. This is a sufficient statistic of the model, and its contribution to the likelihood is p(n+ij |ui, vj) = \u03c3(u>i vj)n + ij . However, the generative process also assumes the possibility to reject word-word pairs if zij = 0. Thus, one needs to construct a fictitious second training set of rejected word-word pairs, called negative examples. Let their counts be n\u2212ij . The total likelihood of both positive and negative examples is then\np(n+, n\u2212|U, V ) = L\u220f\ni,j=1\n\u03c3(u>i vj) n+ij\u03c3(\u2212u>i vj)n \u2212 ij . (1)\nAbove we used the antisymmetry \u03c3(\u2212x) = 1 \u2212 \u03c3(x). In our notation, dropping the subscript indices for n+ and n\u2212 denotes the entire L \u00d7 L matrices, U = (u1, \u00b7 \u00b7 \u00b7 , uL) \u2208 Rd\u00d7L is the matrix of all word embedding vectors, and V is defined analogously for the context vectors. To construct negative examples, one typically chooses n\u2212ij \u221d P (i)P (j)3/4 (Mikolov et al., 2013b), where P (i) is the frequency of word i in the training corpus. Thus, n\u2212 is well-defined up to a constant factor which has to be tuned.\nDefining n\u00b1 = (n+, n\u2212) the combination of both positive and negative examples, the resulting log likelihood is\nlog p(n\u00b1|U, V ) = L\u2211\ni,j=1\n(n+ij log \u03c3(u > i vj) + n \u2212 ij log \u03c3(\u2212u>i vj)). (2)\nThis is exactly the objective of the (non-Bayesian) skipgram model, see (Mikolov et al., 2013b). The count matrices n+ and n\u2212 are either pre-computed for the entire corpus, or estimated based on stochastic subsamples from the data in a sequential way, as done by word2vec. Barkan (2016) give an approximate Bayesian treatment of the model with Gaussian priors on the embeddings."}, {"heading": "3.2. Dynamic Skip-Gram Model", "text": "The key extension of our approach is to use a Kalman filter as a prior for the time-evolution of the latent embeddings (Welch & Bishop, 1995). This allows us to share information across all times while still allowing the embeddings to drift.\nNotation. We consider a corpus of T documents which were written at time stamps \u03c41 < . . . < \u03c4T . For each time step t \u2208 {1, . . . , T} the sufficient statistics of word-context pairs are encoded in the L\u00d7L matrices n+t , n\u2212t of positive and negative counts with matrix elements n+ij,t and n \u2212 ij,t, respectively. Denote Ut = (u1,t, \u00b7 \u00b7 \u00b7 , uL,t) \u2208 Rd\u00d7L the matrix of word embeddings at time t, and define Vt correspondingly for the context vectors. Let U, V \u2208 RT\u00d7d\u00d7L denote the tensors of word and context embeddings across all times, respectively.\nModel. The graphical model is shown in Figure 2b). We consider a diffusion process of the embedding vectors over time. The variance \u03c32t of the transition kernel is\n\u03c32t = D(\u03c4t+1 \u2212 \u03c4t), (3) whereD is a global diffusion constant and (\u03c4t+1\u2212\u03c4t) is the time between subsequent observations (Welch & Bishop, 1995). At every time step t, we add an additional Gaussian prior with zero mean and variance \u03c320 which prevents the embedding vectors from growing very large, thus\np(Ut+1|Ut) \u221d N (Ut, \u03c32t )N (0, \u03c320). (4) Computing the normalization, this results in\nUt+1|Ut \u223c N (\nUt 1 + \u03c32t /\u03c3 2 0 , 1 \u03c3\u22122t + \u03c3 \u22122 0 I\n) , (5)\nVt+1|Vt \u223c N (\nVt 1 + \u03c32t /\u03c3 2 0 , 1 \u03c3\u22122t + \u03c3 \u22122 0 I\n) . (6)\nIn practice, \u03c30 \u03c3t, so the damping to the origin is very weak. This is also called Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930). At time index t = 1, we define p(U1|U0) \u2261 N (0, \u03c320I) and do the same for V1. Our joint distribution factorizes as follows:\np(n\u00b1, U, V ) = T\u22121\u220f\nt=0\np(Ut+1|Ut) p(Vt+1|Vt)\n\u00d7 T\u220f\nt=1\nL\u220f\ni,j=1\np(n\u00b1ij,t|uit, vjt) (7)\nThe prior model enforces that the model learns embedding vectors which vary smoothly across time. This allows to associate words unambiguously with each other and to detect semantic changes. The model efficiently shares information across the time domain, which allows to fit the model in setups where the data at every given point in time are small, but the data in total are large."}, {"heading": "4. Inference", "text": "We discuss two scalable approximate inference algorithms for our model, taking into account different versions of inference: filtering, which only uses information from the\npast as required in streaming applications, and smoothing, which learns better embeddings but requires full knowledge of the sequence of documents. In Bayesian inference, we start by formulating a joint distribution (Eq. 7) over observations n\u00b1 and parameters U and V , and we are interested in the posterior distribution over parameters conditioned on observations,\np(U, V |n\u00b1) = p(n \u00b1, U, V )\u222b\np(n\u00b1, U, V ) dUdV (8)\nThe problem is that the normalization is intractable. In variational inference (VI) (Jordan et al., 1999; Blei et al., 2016) one sidesteps this problem and approximates the posterior with a simpler variational distribution q\u03bb(U, V ) by minimizing the Kullback-Leibler divergence to the posterior. This is equivalent to optimizing the evidence lower bound (ELBO) (Blei et al., 2016),\nL(\u03bb) = Eq\u03bb [log p(n\u00b1, U, V )]\u2212Eq\u03bb [log q\u03bb(U, V )]. (9)\nFor a restricted class of models, the ELBO can be computed in closed-form (Hoffman et al., 2013). Our model is non-conjugate and requires instead black-box VI using the reparameterization trick, where one maximizes L(\u03bb) with stochastic gradient ascent and estimates the gradient \u2207\u03bbL by sampling from the variational distribution (Rezende et al., 2014; Kingma & Welling, 2014)."}, {"heading": "4.1. Skip-Gram Filtering", "text": "In many applications such as streaming, the data arrive sequentially. Thus, we can only condition our model on past and not on future observations. We will first describe inference in such a (Kalman) filtering setup (Kalman et al., 1960; Welch & Bishop, 1995).\nIn the filtering scenario, the inference algorithm iteratively updates the variational distribution q as evidence from each time step t becomes available. We thereby use a variational distribution that factorizes across all times, q(U, V ) =\u220fT t=1 q(Ut, Vt) and we update the variational factor at a given time t based on the evidence at time t and the approximate posterior of the previous time step. Furthermore, at every time t we use a fully-factorized distribution:\nq(Ut, Vt) = L\u220f\ni=1\nN (ui,t;\u00b5ui,t,\u03a3ui,t)N (vi,t;\u00b5vi,t.\u03a3vi,t),\nThe variational parameters are the means \u00b5ui,t, \u00b5vi,t \u2208 Rd and the covariance matrices \u03a3ui,t and \u03a3vi,t, which we restrict to be diagonal (mean-field approximation).\nWe now describe how we sequentially compute q(Ut, Vt) and use the result to proceed to the next time step. As other\nMarkovian dynamical systems, our model assumes the following recursion,\np(Ut, Vt|n\u00b11:t) \u221d p(n\u00b1t |Ut, Vt) p(Ut, Vt|n\u00b11:t\u22121). (10) Within our variational approximation, the ELBO (Eq. 9) therefore separates into a sum of T terms, L = \u2211t Lt with\nLt = E[log p(n\u00b1t |Ut, Vt)] + E[log p(Ut, Vt|n\u00b11:t\u22121)] \u2212 E[log q(Ut, Vt)], (11)\nwhere all expectations are taken under q(Ut, Vt). We compute the entropy term in Eq. 11 analytically and estimate the gradient of the log likelihood by sampling from the variational distribution and using the reparameterization trick (Kingma & Welling, 2014; Salimans & Kingma, 2016). However, the second term of Eq. 11, containing the prior at time t, is still intractable. We approximate the prior as\np(Ut, Vt|n\u00b11:t\u22121) \u2261 Ep(Ut\u22121,Vt\u22121|n\u00b11:t\u22121) [ p(Ut, Vt|Ut\u22121, Vt\u22121) ]\n\u2248 Eq(Ut\u22121,Vt\u22121) [ p(Ut, Vt|Ut\u22121, Vt\u22121) ] . (12)\nThe remaining expectation involves only Gaussians and can be carried-out analytically. The resulting approximate prior is a fully factorized distribution p(Ut, Vt|n\u00b11:t\u22121) \u2248\u220fL i=1N (ui,t; \u00b5\u0303ui,t, \u03a3\u0303ui,t)N (vi,t; \u00b5\u0303vi,t, \u03a3\u0303vit) with\n\u00b5\u0303ui,t = \u03a3\u0303ui,t ( \u03a3ui,t\u22121 + \u03c3 2 t I )\u22121 \u00b5ui,t\u22121; \u03a3\u0303ui,t = [( \u03a3ui,t\u22121 + \u03c3 2 t I )\u22121 + (1/\u03c320)I ]\u22121 (13)\nAnalogous update equations hold for \u00b5\u0303vi,t and \u03a3\u0303vi,t. The expected log prior therefore also yields an analytic contribution to Lt."}, {"heading": "4.2. Skip-Gram Smoothing", "text": "In contrast to filtering, where inference is conditioned on past observations until a given time t, (Kalman) smoothing performs inference based on the entire sequence of observations n\u00b11:T . This approach results in smoother trajectories and typically higher likelihoods than with filtering, because evidence is used from both future and past observations.\nBesides the new inference scheme, we also use a different variational distribution. As the model is fitted jointly to all time steps, we are no longer restricted to a variational distribution that factorizes in time. For simplicity we focus here on the variational distribution for the word embeddings U ; the context embeddings V are treated identically. We use a factorized distribution over both embedding space and vocabulary space,\nq(U1:T ) =\nL\u220f\ni=1\nd\u220f\nk=1\nq(uik,1:T ). (14)\nIn the time domain, our variational approximation is structured. To simplify the notation we now drop the indices for words i and embedding dimension k, hence we write q(u1:T ) for q(uik,1:T ) where we focus on a single factor. This factor is a multivariate Gaussian distribution in the time domain with tridiagonal precision matrix \u039b,\nq(u1:T ) = N (\u00b5,\u039b\u22121) (15)\nBoth the means \u00b5 = \u00b51:T and the entries of the tridiagonal precision matrix \u039b \u2208 RT\u00d7T are variational parameters. This gives our variational distribution the interpretation of a posterior of a Kalman filter (Blei & Lafferty, 2006) which captures correlations in time.\nWe fit the variational parameters by training the model jointly on all time steps, using black-box VI and the reparameterization trick. As the computational complexity of an update step scales as \u0398(L2), we first pretrain the model by drawing minibatches of L\u2032 < L random words and L\u2032 random contexts from the vocabulary (Hoffman et al., 2013). We then switch to the full batch to reduce the sampling noise. Since the variational distribution does not factorize in the time domain we always include all time steps {1, . . . , T} in the minibatch. We also derive an efficient algorithm that allows us to estimate the reparametrization gradient using \u0398(T ) time and memory, while a naive implementation of black-box variational inference with our structured variational distribution would require \u0398(T 2) of both resources. The main idea is to parameterize \u039b = B>B in terms of its Cholesky decomposition B, which is bidiagonal (K\u0131l\u0131c\u0327 & Stanica, 2013), and to express gradients of the dense (upper triangular) matrix B\u22121 in terms of gradients of B, which are sparse. We use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) to enforce positive definiteness of B. The algorithm is detailed in our supplementary material. It does not depend on any specific aspects of the dynamic skip-gram model and applies to other latent time-series models."}, {"heading": "5. Experiments", "text": "We evaluate our method on three time-stamped text corpora. We demonstrate that our algorithms find smoother embedding trajectories than methods based on a static model. This allows us to track semantic changes of individual words by following nearest-neighbor relations over time. In our quantitative analysis, we find higher predictive likelihoods on held-out data compared to our baselines.\nAlgorithms and Baselines. We report results from our proposed algorithms from section 4 and compare against baselines from section 2:\n\u2022 SGI denotes the non-Bayesian skip-gram model with independent random initializations of word vectors (Mikolov et al., 2013b). We used our own implementation of the model by dropping the Kalman filtering prior and point-estimating embedding vectors. Word vectors at nearby times are made comparable by approximate orthogonal transformations, which corresponds to Hamilton et al. (2016).\n\u2022 SGP denotes the same approach as above, but with word and context vectors being pre-initialized with the values from the previous year, as in Kim et al. (2014).\n\u2022 DSG-F: dynamic skip-gram filtering (proposed).\n\u2022 DSG-S: dynamic skip-gram smoothing (proposed).\nData and preprocessing. Our three corpora exemplify opposite limits both in the covered time span and in the amount of text per time step.\n1. We used data from the Google books corpus1 (Michel et al., 2011) from the last two centuries (T = 209). This amounts to 5 million digitized books and approximately 1010 observed words. The corpus consists of n-gram tables with n \u2208 {1, . . . , 5}, annotated by year of publication. We considered the years from 1800 to 2008. In 1800, the size of the data is approximately \u223c 7 \u00b7107 words. We used the 5-gram counts, resulting in a context window size of 4.\n2. We used the \u201cState of the Union\u201d (SoU) addresses of U.S. presidents which spans more than two centuries, resulting in T = 230 different time steps and approximately 106 observed words.2 Some presidents gave both a written and an oral address; if these were less than a week apart we concatenated them and used the average date. We converted all words to lower case and constructed the positive sample counts n+ij using a context window size of 4.\n3. We used a Twitter corpus of news tweets for 21 randomly drawn dates from 2010 to 2016. The median number of tweets per day is 722. We converted all tweets to lower case and used a context window size of 4, which we restricted to stay within single tweets.\nHyperparameters. The vocabulary for each corpus was constructed from the 10,000 most frequent words throughout the given time period. In the Google books corpus, the number of words per year grows by a factor of 200 from the\n1http://storage.googleapis.com/books/ ngrams/books/datasetsv2.html\n2http://www.presidency.ucsb.edu/sou.php\nyear 1800 to 2008. To avoid that the vocabulary is dominated by modern words we normalized the word frequencies separately for each year before adding them up.\nFor the Google books corpus, we chose the embedding dimension d = 200, which was also used in Kim et al. (2014). We set d = 100 for SoU and Twitter, as d = 200 resulted in overfitting on these much smaller corpora. The ratio \u03b7 = \u2211 ij n \u2212 ij,t/ \u2211 ij n + ij,t of negative to positive wordcontext pairs was \u03b7 = 1. The precise construction of the matrices n\u00b1t is explained in the supplementary material. We used the global prior variance \u03c320 = 1 for all corpora and all algorithms, including the baselines. The diffusion constant D controls the time scale on which information is shared between time steps. The optimal value for D depends on the application. A single corpus may exhibit semantic shifts of words on different time scales, and the optimal choice for D depends on the time scale in which one is interested. We measured time in years and used D = 10\u22123 for Google books, D = 10\u22124 for SoU, and D = 1 for the Twitter corpus, which spans a much shorter time range. In the supplementary material, we provide details of the optimization procedure.\nQualitative results. We show that our approach results in smooth word embedding trajectories on all corpora. We can automatically detect words that undergo significant semantic changes over time.\nFigure 1 in the introduction visualizes word embedding clouds over four subsequent years of Google books, where we compare DSG-F against SGI. We mapped the normalized embedding vectors to two dimensions using dynamic t-SNE (Rauber et al., 2016) (see supplement for details).\nLines indicate shifts of word vectors relative to the preceding year. In our model only few words change their position in the embedding space rapidly, while embeddings using SGI show strong fluctuations, making the cloud\u2019s motion hard to track.\nFigure 3 visualizes the smoothness of the trajectories directly in the embedding space (without the projection to two dimensions). We consider differences between word vectors in the year 1998 and the subsequent 10 years. In more detail, we compute histograms of the Euclidean distances ||uit \u2212 ui,t+\u03b4|| over the word indexes i, where \u03b4 = 1, . . . , 10 (as discussed previously, SGI uses a global rotation to optimally align embeddings first). In our model, embedding vectors gradually move away from their original position as time progresses, indicating a directed motion. In contrast, both baseline models show no directed\nmotion after the first time step, suggesting that most temporal changes are due to finite-size fluctuations of n\u00b1ij,t.\nOur approach allows us to detect semantic shifts in the usage of specific words. Figures 4 and 5 both show the cosine angle between a given word and its neighboring words (colored lines) as a function of time. Figure 4 shows results on all three corpora and focuses on the comparisons across methods. We see that for DSG-S and DSG-F (proposed) result in trajectories which display less noise than the baselines SGP and SGI. The fact that the baselines predict zero cosine angle (no correlation) between the chosen word pairs on the SoU and Twitter corpora suggests that these corpora are too small to successfully fit these models, in contrast to our approach which shares information in the time domain.\nFigure 5 then focuses on the Google books corpus. Here, we show the ten words that change their meaning most rapidly in terms of cosine distance. We thereby automatically discover words such as \u201dcomputer\u201d, \u201dradio\u201d, and \u201dsoftware\u201d that changed their meaning due to technological advances, but also words as \u201dpeer\u201d and \u201dnotably\u201d whose semantic shift is arguably less obvious.\nQuantitative results. We show that our approach generalizes better to unseen data. We thereby analyze held-out predictive likelihoods on word-context pairs at a given time t, where t is excluded from the training set,\n1 |n\u00b1t | log p(n\u00b1t |U\u0303t, V\u0303t). (16)\nAbove, |n\u00b1t | = \u2211 i,j ( n+ij,t + n \u2212 ij,t ) denotes the total number of word-context pairs at time \u03c4t. Since inference is different in all approaches, the definitions of word and context embedding matrices U\u0303t and V\u0303t in Eq. 16 have to be adjusted:\n\u2022 For SGI and SGP, we did a chronological pass through the time sequence and used the embeddings U\u0303t = Ut\u22121 and V\u0303t = Vt\u22121 from the previous time step to predict the statistics n\u00b1ij,t at time step t.\n\u2022 For DSG-F, we did the same pass to test n\u00b1ij,t. We thereby set U\u0303t and V\u0303t to be the modes Ut\u22121, Vt\u22121 of the approximate posterior at the previous time step.\n\u2022 For DSG-S, we held out 10%, 10% and 20% of the documents from the Google books, SoU, and Twitter corpora for testing, respectively. After training, we estimated the word (context) embeddings U\u0303t (V\u0303t) in Eq. 16 by linear interpolation between the values of Ut\u22121 (Vt\u22121) and Ut+1 (Vt+1) in the mode of the variational distribution, taking into account that the time stamps \u03c4t are in general not equally spaced.\nThe predictive likelihoods as a function of time \u03c4t are shown in Figure 6. Differences between the two implementations of the static model (SGI and SGP) are small. This suggests that pre-initializing the embeddings with the previous result may improve their continuity but seems to have a minor impact on predictive power.\nWe see that both proposed versions of the dynamic model (DSG-F and DSG-S) outperform the baselines SGP and SGI. The improvements are particularly pronounced in the SoU and Twitter corpora (center and right panels in Figure 6), for which sharing information between time steps is crucial because there is little data at each time slice. In the Google Books corpus (left panel), the number of words per year grows by a factor of 230 from 1800 to 2008. This explains why the quantitative improvements by the dynamic model are only noticeable at the beginning of the considered time span, and why the performance of all methods increases over time. Further, smoothing (DSG-S) outperforms filtering (DSG-F), as this algorithm can use information from both past and future observations."}, {"heading": "6. Conclusions", "text": "We presented the dynamic skip-gram model: a Bayesian probabilistic model that combines word2vec with a latent continuous time series. We showed experimentally that both skip-gram filtering (which conditions only on past observations) and skip-gram smoothing (which uses all data) lead to smoothly changing embedding vectors that are better at predicting word-context statistics at held-out time bins. The benefits are most drastic when the data at individual time steps is small; making it hard to fit a static word embedding model. Our approach may be used as a data mining and anomaly detection tool when streaming text on social media, as well as a tool for historians and social scientists interested in the evolution of language."}, {"heading": "1. Dimensionality Reduction in Figure 1", "text": "To create the word-clouds in Figure 1 of the main text we mapped the fitted word embeddings from Rd to the twodimensional plane using dynamic t-SNE (Rauber et al., 2016). Dynamic t-SNE is a non-parametric dimensionality reduction algorithm for sequential data. The algorithm finds a projection to a lower dimension by solving a non-convex optimization problem that aims at preserving nearest-neighbor relations at each individual time step. In addition, projections at neighboring time steps are aligned with each other by a quadratic penalty with prefactor \u03bb \u2265 0\nfor sudden movements.\nThere is a trade-off between finding good local projections for each individual time step (\u03bb \u2192 0), and finding smooth projections (large \u03bb). Since we want to analyze the smoothness of word embedding trajectories, we want to avoid bias towards smooth projections. Unfortunately, setting \u03bb = 0 is not an option since, in this limit, the optimization problem is invariant under independent rotations at each time, rendering trajectories in the two-dimensional projection plane meaningless. To still avoid bias towards smooth projections, we anneal \u03bb exponentially towards zero over the course of the optimization. We start the optimizer with \u03bb = 0.01, and we reduce \u03bb by 5% with each training step. We run 100 optimization steps in total, so that \u03bb \u2248 6\u00d710\u22126 at the end of the training procedure. We used the opensource implementation,1 set the target perplexities to 200, and used default values for all other parameters."}, {"heading": "2. Hyperparemeters and Construction of n\u00b11:T", "text": "Table S1 lists the hyperparameters used in our experiments. For the Google books corpus, we used the same context window size cmax and embedding dimension d as in (Kim et al., 2014). We reduced d for the SoU and Twitter corpora to avoid overfitting to these much smaller data sets.\nIn constrast to word2vec, we construct our positive and negative count matrices n\u00b1ij,t deterministically in a preprocessing step. As detailed below, this is done such that it resembles as closely as possible the stochastic approach in word2vec (Mikolov et al., 2013). In every update step, word2vec stochastically samples a context window size uniformly in an interval [1, \u00b7 \u00b7 \u00b7 , cmax], thus the context size fluctuates and nearby words appear more often in the same context than words that are far apart from each other in the sentence. We follow a deterministic scheme that results in similar statistics. For each pair of words (w1, w2)\n1https://github.com/paulorauber/thesne\nar X\niv :1\n70 2.\n08 35\n9v 1\n[ st\nat .M\nL ]\n2 7\nFe b\n20 17\nAlgorithm 1 Skip-gram filtering; see section 4 of the main text.\nRemark: All updates are analogous for word and context vectors; we drop their indices for simplicity. Input: number of time steps T , time stamps \u03c41:T , positive and negative examples n\u00b11:T , hyperparameters. Init. prior means \u00b5\u0303ik,1 \u2190 0 and variances \u03a3\u0303i,1 = Id\u00d7d. Init. variational means \u00b5ik,1 \u2190 0 and var. \u03a3i,1 = Id\u00d7d. for t = 1 to T do\nif t 6= 1 then Update approximate Gaussian prior with params. \u00b5\u0303ik,t and \u03a3\u0303i,t using \u00b5ik,t\u22121 and \u03a3i,t\u22121, see Eq. 13. end if Compute entropy Eq[log q(\u00b7)] analytically. Compute expected log Gaussian prior with parameters \u00b5\u0303ik,t and \u03a3\u0303k,t analytically. Maximize Lt in Eq. 11, using black-box VI with the reparametrization trick. Obtain \u00b5ik,t and \u03a3i,t as outcome of the optimization.\nend for\nin a given sentence, we increase the counts n+iw1 jw2 by max (0, 1\u2212 k/cmax), where 0 \u2264 k \u2264 cmax is the number of words that appear between w1 and w2, and iw1 and jw2 are the words\u2019 unique indices in the vocabulary.\nWe also used a deterministic variant of word2vec to construct the negative count matrices n\u2212t . In word2vec, \u03b7 negative samples (i, j) are drawn for each positive sample (i, j\u2032) by drawing \u03b7 independent values for j from a distribution P \u2032t (j) defined below. We define n \u2212 ij,t such that it matches the expectation value of the number of times that word2vec would sample the negative word-context pair (i, j). Specifically, we define\nPt(i) =\n\u2211L j=1 n\n+ ij,t\u2211L\ni\u2032,j=1 n + i\u2032j,t\n, (S1)\nP \u2032t (j) =\n( Pt(j) )\u03b3 \u2211L j\u2032=1 ( Pt(j\u2032) )\u03b3 , (S2)\nn\u2212ij,t =\n( L\u2211\ni\u2032,j\u2032=1\nn+i\u2032j\u2032,t ) \u03b7Pt(i)P \u2032 t (j). (S3)\nWe chose \u03b3 = 0.75 as proposed in (Mikolov et al., 2013), and we set \u03b7 = 1. In practice, it is not necessary to explicitly construct the full matrices n\u2212t , and it is more efficient to keep only the distributions Pt(i) and P \u2032t (j) in memory."}, {"heading": "3. Skip-gram Filtering Algorithm", "text": "The skip-gram filtering algorithm is described in section 4 of the main text. We provide a formulation in pseudocode in Algorithm 1.\nAlgorithm 2 Skip-gram smoothing (batch version); see section 4.\nRemark: As in Algorithm 1, we focus on a single word vector u, and we drop indices i and k; we also just consider a single sample (S = 1), and we describe the basic algorithm without minibatch sampling. Input: number of time steps T , time stamps \u03c41:T , positive and negative examples n\u00b11:T , hyperparameters. Find the upper bidiagonal matrixB0 that is the Cholesky decomposition of the prior precision matrix \u03a0, Eq. S11. Initialize the variational parameters \u03bd1:T with the elements of the main diagonal of B0. Initialize the variation parameters \u03c91:T\u22121 with the elements of the secondary diagonal of B0. Initialize the posterior means \u00b51:T \u2190 0. for step = 1 to Ntr do\nDraw T independent Gauss. noises 1:T \u223c N (0, I). Solve Bx1:T = 1:T for x1:T in \u0398(T ) time using the bidiagonal structure of B defined in Eq. S5. Compute \u03931:T from Eqs. S12 and S8. Obtain \u2202L/\u2202\u00b51:T , i.e., the derivative of the stochastic ELBO L with respect to \u00b51:T , see Eq. S10. Solve B>y1:T = \u2202L/\u2202\u00b51:T for y1:T in \u0398(T ) time using the bidiagonal structure of B, see Eq. S16. Obtain \u2202L/\u2202\u03bd1:T from Eq. S14. Obtain \u2202L/\u2202\u03c91:T\u22121 from Eq. S15. Do a stochastic gradient step in \u00b51:T , \u03bd1:T , and \u03c91:T\u22121 using Adam optimizer and mirror ascent, see Eq. S18.\nend for"}, {"heading": "4. Skip-gram Smoothing Algorithm", "text": "In this section, we give details for the skip-gram smoothing algorithm, see section 4 of the main text. A summary is provided in Algorithm 2.\nVariational distribution. For now, we focus on the word embeddings, and we simplify the notation by dropping the indices for the vocabulary and embedding dimensions. The variational distribution for a single embedding dimension of a single word embedding trajectory is\nq(u1:T ) = N (\u00b5u,1:T , (B>u Bu)\u22121). (S4)\nHere, \u00b5u,1:T is the vector of mean values, and Bu is the Cholesky decomposition of the precision matrix. We restrict the latter to be bidiagonal,\nBu =   \u03bdu,1 \u03c9u,1 \u03bdu,2 \u03c9u,2 . . . . . . \u03bdu,T\u22121 \u03c9u,T\u22121\n\u03bdT\n  (S5)\nwith \u03bdu,t > 0 for all t \u2208 {1, . . . , T}. The variational parameters are \u00b5u,1:T , \u03bdu,1:T , and \u03c91:T\u22121. The variational distribution of the context embedding trajectories v1:T has the same structure.\nWith the above form of Bu, the variational distribution is a Gaussian with an arbitrary tridiagonal symmetric precision matrix B>u Bu. We chose this variational distribution because it is the exact posterior of a hidden time-series model with a Kalman filtering prior and Gaussian noise (Blei & Lafferty, 2006). Note that our variational distribution is a generalization of a fully factorized (mean-field) distribution, which is obtained for \u03c9u,t = 0 \u2200t. In the general case, \u03c9u,t 6= 0, the variational distribution can capture correlations between all time steps, with a dense covariance matrix (B>u Bu) \u22121.\nGradient estimation. The skip-gram smoothing algorithm uses stochastic gradient ascent to find the variational parameters that maximize the ELBO, L = Eq [ log p(U1:T , V1:T , n \u00b1 1:T ) ] \u2212 Eq [ log q(U1:T , V1:T ) ] .\n(S6)\nHere, the second term is the entropy, which can be evaluated analytically. We obtain for each component in vocabulary and embedding space,\n\u2212Eq[log q(u1:T )] = \u2212 \u2211\nt\nlog(\u03bdu,t) + const. (S7)\nand analogously for \u2212Eq[log q(v1:T )]. The first term on the right-hand side of Eq. S6 cannot be evaluated analytically. We approximate its gradient by sampling from q using the reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014). A naive calculation would require \u2126(T 2) computing time since the derivatives of L with respect to \u03bdu,t and \u03c9u,t for each t depend on the count matrices n\u00b1t\u2032 of all t\n\u2032. However, as we show next, there is a more efficient way to obtain all gradient estimates in \u0398(T ) time.\nWe focus again on a single dimension of a single word embedding trajectory u1:T , and we drop the indices i and k. We draw S independent samples u[s]1:T with s \u2208 {1, . . . , S} from q(u1:T ) by parameterizing\nu [s] 1:T = \u00b5u,1:T + x [s] u,1:T (S8)\nwith\nx [s] u,1:T = B \u22121 u [s] u,1:T where [s] u,1:T \u223c N (0, I). (S9)\nWe obtain x[s]u,1:T in \u0398(T ) time by solving the bidiagonal linear system Bux [s] u,1:T = [s] u,1:T . Samples v [s] 1:T for the context embedding trajectories are obtained analogously.\nOur implementation uses S = 1, i.e., we draw only a single sample per training step. Averaging over several samples is done implicitly since we calculate the update steps using the Adam optimizer (Kingma & Ba, 2015), which effectively averages over several gradient estimates in its first moment estimate.\nThe derivatives of L with respect to \u00b5u,1:T can be obtained using Eq. S8 and the chain rule. We find\n\u2202L \u2202\u00b5u,1:T \u2248 1 S\nS\u2211\ns=1\n[ \u0393 [s] u,1:T \u2212\u03a0u [s] 1:T ] . (S10)\nHere, \u03a0 \u2208 RT\u00d7T is the precision matrix of the prior u1:T \u223c N (0,\u03a0\u22121). It is tridiagonal and therefore the matrix-multiplication \u03a0u[s]1:T can be carried out efficiently. The non-zero matrix elements of \u03a0 are\n\u03a011 = \u03c3 \u22122 0 + \u03c3 \u22122 1\n\u03a0TT = \u03c3 \u22122 0 + \u03c3 \u22122 T\u22121\n\u03a0tt = \u03c3 \u22122 0 + \u03c3 \u22122 t\u22121 + \u03c3 \u22122 t \u2200t \u2208 {2, . . . , T \u2212 1}\n\u03a01,t+1 = \u03a0t+1,1 = \u2212\u03c3\u22122t . (S11)\nThe term \u0393[s]u,1:T on the right-hand side of Eq. S10 comes from the expectation value of the log-likelihood under q. It is given by\n\u0393 [s] ui,t =\nL\u2211\nj=1\n[( n+ij,t + n \u2212 ij,t ) \u03c3 ( \u2212u[s]>i,t v [s] j,t ) \u2212 n\u2212ij,t ] v [s] j,t\n(S12)\nwhere we temporarily restored the indices i and j for words and contexts, respectively. In deriving Eq. S12, we used the relations \u2202 log \u03c3(x)/\u2202x = \u03c3(\u2212x) and \u03c3(\u2212x) = 1\u2212 \u03c3(x). The derivatives of L with respect to \u03bdu,t and \u03c9u,t are more intricate. Using the parameterization in Eqs. S8\u2013S9, the derivatives are functions of \u2202B\u22121u /\u2202\u03bdt and \u2202B \u22121 u /\u2202\u03c9t, respectively, where B\u22121u is a dense (upper triangular) T \u00d7 T matrix. An efficient way to obtain these derivatives is to use the relation\n\u2202B\u22121u \u2202\u03bdt = \u2212B\u22121u \u2202Bu \u2202\u03bdt B\u22121u (S13)\nand similarly for \u2202B\u22121u /\u2202\u03c9t. Using this relation and Eqs. S8\u2013S9, we obtain the gradient estimates\n\u2202L \u2202\u03bdu,t \u2248 \u2212 1 S\nS\u2211\ns=1\ny [s] u,tx [s] u,t \u2212\n1\n\u03bdu,t , (S14)\n\u2202L \u2202\u03c9u,t \u2248 \u2212 1 S\nS\u2211\ns=1\ny [s] u,tx [s] u,t+1. (S15)\nThe second term on the right-hand side of Eq. S14 is the derivative of the entropy, Eq. S7, and\ny [s] u,1:T = (B > u ) \u22121 [ \u0393 [s] u,1:T \u2212\u03a0u [s] 1:T ] . (S16)\nThe values y[s]u,1:T can again be obtained in \u0398(T ) time by bringing B>u to the left-hand side and solving the corresponding bidiagonal linear system of equations.\nSampling in vocabulary space. In the above paragraph, we described an efficient strategy to obtain gradient estimates in only \u0398(T ) time. However, the gradient estimation scales quadratic in the vocabulary size L because all L2 elements of the positive count matrices n+t contribute to the gradients. In order speed up the optimization, we pretrain the model using a minibatch of size L\u2032 < L in vocabulary space as explained below. The computational complexity of a single training step in this setup scales as (L\u2032)2 rather than L2. After N \u2032tr = 5000 training steps with minibatch size L\u2032, we switch to the full batch size of L and train the model for another Ntr = 1000 steps.\nThe subsampling in vocabulary space works as follows. In each training step, we independently draw a set I of L\u2032 random distinct words and a set J of L\u2032 random distinct contexts from a uniform probability over the vocabulary. We then estimate the gradients of L with respect to only the variational parameters that correspond to words i \u2208 I and contexts j \u2208 J . This is possible because both the prior of our dynamic skip-gram model and the variational distribution factorize in the vocabulary space. The likelihood of the model, however, does not factorize. This affects only the definition of \u0393[s]uik,t in Eq. S12. We replace \u0393 [s] uik,t by an estimate \u0393[s]\u2032uik,t based on only the contexts j \u2208 J in the current minibatch,\n\u0393 [s] ui,t =\nL L\u2032 \u2211\nj\u2208J\n[ ( n+ij,t + n \u2212 ij,t ) \u03c3 ( \u2212u[s]>i,t v [s] j,t )\n\u2212 n\u2212ij,t ] v [s] j,t. (S17)\nHere, the prefactor L/L\u2032 restores the correct ratio between evidence and prior knowledge (Hoffman et al., 2013).\nEnforcing positive definiteness. We update the variational parameters using stochastic gradient ascent with the Adam optimizer (Kingma & Ba, 2015). The parameters \u03bdu,1:T are the eigenvalues of the matrix Bu, which is the Cholesky decomposition of the precision matrix of q. Therefore, \u03bdu,t has to be positive for all t \u2208 {1, . . . , T}. We use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) to enforce \u03bdu,t > 0. Specifically, we update \u03bdt to a new value \u03bd\u2032t defined by\n\u03bd\u2032u,t = 1\n2 \u03bdu,td[\u03bdu,t] +\n\u221a( 1\n2 \u03bdu,td[\u03bdu,t]\n)2 + \u03bd2u,t (S18)\nwhere d[\u03bdu,t] is the step size obtained from the Adam optimizer. Eq. S18 can be derived from the general mirror ascent update rule \u03a6\u2032(\u03bd\u2032u,t) = \u03a6\n\u2032(\u03bdu,t) + d[\u03bdu,t] with the mirror map \u03a6 : x 7\u2192 \u2212c1 log(x)+c2x2/2, where we set the parameters to c1 = \u03bdu,t and c2 = 1/\u03bdu,t for dimensional reasons. The update step in Eq. S18 increases (decreases) \u03bdu,t for positive (negative) d[\u03bdu,t], while always keeping its value positive.\nNatural basis. As a final remark, let us discuss an optional extension to the skip-gram smoothing algorithm that converges in less training steps. This extension only increases the efficiency of the algorithm. It does not change the underlying model or the choice of variational distribution. Observe that the prior of the dynamic skip-gram model connects only neighboring time-steps with each other. Therefore, the gradient of L with respect to \u00b5u,t depends only on the values of \u00b5u,t\u22121 and \u00b5u,t+1. A naive implementation of gradient ascent would thus require T\u22121 update steps until a change of \u00b5u,1 affects updates of \u00b5u,T .\nThis problem can be avoided with a change of basis from \u00b5u,1:T to new parameters \u03c1u,1:T ,\n\u00b5u,1:T = A\u03c1u,1:T (S19)\nwith an appropriately chosen invertible matrix A \u2208 RT\u00d7T . Derivatives of L with respect to \u03c1u,1:T are given by the chain rule, \u2202L/\u2202\u03c1u,1:T = (\u2202L/\u2202\u00b5u,1:T )A. A natural (but inefficient) choice for A is to stack the eigenvectors of the prior precision matrix \u03a0, see Eq. S11, into a matrix. The eigenvectors of \u03a0 are the Fourier modes of the Kalman filtering prior (with a regularization due to \u03c30). Therefore, there is a component \u03c1u,t that corresponds to the zero-mode of \u03a0, and this component learns an average word embedding over all times. Higher modes correspond to changes of the embedding vector over time. A single update to the zero immediately affects all elements of \u00b5u,1:T , and therefore changes the word embeddings at all time steps. Thus, information propagates quickly along the time dimension. The downside of this choice for A is that the transformation in Eq. S19 has complexity \u2126(T 2), which makes update steps slow.\nAs a compromise between efficiency and a natural basis, we propose to set A in Eq. S19 to the Cholesky decomposition of the prior covariance matrix \u03a0\u22121 \u2261 AA>. Thus, A is still a dense (upper triangular) matrix, and, in our experiments, updates to the last component \u03c1u,T affect all components of \u00b5u,1:T in an approximately equal amount. Since \u03a0 is tridiagonal, the inverse of A is bidiagonal, and Eq. S19 can be evaluated in \u0398(T ) time by solving A\u00b5u,1:T = \u03c1u,1:T for \u00b5u,1:T . This is the parameterization we used in our implementation of the skip-gram smoothing algorithm."}], "references": [{"title": "Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization", "author": ["References Beck", "Amir", "Teboulle", "Marc"], "venue": "Operations Research Letters,", "citeRegEx": "Beck et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2003}, {"title": "The Ordered Subsets Mirror Descent Optimization Method with Applications to Tomography", "author": ["Ben-Tal", "Aharon", "Margalit", "Tamar", "Nemirovski", "Arkadi"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Ben.Tal et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 2001}, {"title": "Dynamic Topic Models", "author": ["Blei", "David M", "Lafferty", "John D"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "Stochastic Variational Inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John William"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Temporal Analysis of Language Through Neural Language Models", "author": ["Kim", "Yoon", "Chiu", "Yi-I", "Hanaki", "Kentaro", "Hegde", "Darshan", "Petrov", "Slav"], "venue": "arXiv preprint arXiv:1405.3515,", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-Encoding Variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Visualizing Time-Dependent Data Using Dynamic t-SNE", "author": ["Rauber", "Paulo E", "Falc\u00e3o", "Alexandre X", "Telea", "Alexandru C"], "venue": "In EuroVis 2016 - Short Papers,", "citeRegEx": "Rauber et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rauber et al\\.", "year": 2016}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Current approaches to learning word embeddings in a dynamic context rely on grouping the data into time bins and training the embeddings separately on these bins (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016).", "startOffset": 162, "endOffset": 226}, {"referenceID": 8, "context": "We used dynamic t-SNE (Rauber et al., 2016) for dimensionality reduction.", "startOffset": 22, "endOffset": 43}, {"referenceID": 9, "context": "\u2022 We propose two scalable black-box variational inference algorithms (Ranganath et al., 2014; Rezende et al., 2014) for filtering and smoothing.", "startOffset": 69, "endOffset": 115}, {"referenceID": 4, "context": "Several authors have analyzed different statistics of text data to analyze semantic changes of words over time (Mihalcea & Nastase, 2012; Sagi et al., 2011; Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016).", "startOffset": 111, "endOffset": 220}, {"referenceID": 4, "context": "Kim et al. (2014) fit word2vec separately on different time bins, where the word vectors obtained for the previous bin are used to initialize the algorithm for the next time bin.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Kim et al. (2014) fit word2vec separately on different time bins, where the word vectors obtained for the previous bin are used to initialize the algorithm for the next time bin. The bins have to be sufficiently large and the found trajectories are not as smooth as ours, as we demonstrate in this paper. Hamilton et al. (2016) also trained word2vec separately on several large corpora from different decades.", "startOffset": 0, "endOffset": 328}, {"referenceID": 7, "context": "This is exactly the objective of the (non-Bayesian) skipgram model, see (Mikolov et al., 2013b). The count matrices n and n\u2212 are either pre-computed for the entire corpus, or estimated based on stochastic subsamples from the data in a sequential way, as done by word2vec. Barkan (2016) give an approximate Bayesian treatment of the model with Gaussian priors on the embeddings.", "startOffset": 73, "endOffset": 286}, {"referenceID": 3, "context": "For a restricted class of models, the ELBO can be computed in closed-form (Hoffman et al., 2013).", "startOffset": 74, "endOffset": 96}, {"referenceID": 9, "context": "Our model is non-conjugate and requires instead black-box VI using the reparameterization trick, where one maximizes L(\u03bb) with stochastic gradient ascent and estimates the gradient \u2207\u03bbL by sampling from the variational distribution (Rezende et al., 2014; Kingma & Welling, 2014).", "startOffset": 231, "endOffset": 277}, {"referenceID": 3, "context": "As the computational complexity of an update step scales as \u0398(L), we first pretrain the model by drawing minibatches of L\u2032 < L random words and L\u2032 random contexts from the vocabulary (Hoffman et al., 2013).", "startOffset": 183, "endOffset": 205}, {"referenceID": 1, "context": "We use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) to enforce positive definiteness of B.", "startOffset": 21, "endOffset": 66}, {"referenceID": 7, "context": "\u2022 SGI denotes the non-Bayesian skip-gram model with independent random initializations of word vectors (Mikolov et al., 2013b). We used our own implementation of the model by dropping the Kalman filtering prior and point-estimating embedding vectors. Word vectors at nearby times are made comparable by approximate orthogonal transformations, which corresponds to Hamilton et al. (2016).", "startOffset": 104, "endOffset": 387}, {"referenceID": 4, "context": "\u2022 SGP denotes the same approach as above, but with word and context vectors being pre-initialized with the values from the previous year, as in Kim et al. (2014).", "startOffset": 144, "endOffset": 162}, {"referenceID": 4, "context": "In contrast, in SGP (middle) (Kim et al., 2014) and SGI (bottom) (Hamilton et al.", "startOffset": 29, "endOffset": 47}, {"referenceID": 4, "context": "For the Google books corpus, we chose the embedding dimension d = 200, which was also used in Kim et al. (2014). We set d = 100 for SoU and Twitter, as d = 200 resulted in overfitting on these much smaller corpora.", "startOffset": 94, "endOffset": 112}, {"referenceID": 8, "context": "We mapped the normalized embedding vectors to two dimensions using dynamic t-SNE (Rauber et al., 2016) (see supplement for details).", "startOffset": 81, "endOffset": 102}, {"referenceID": 4, "context": ", 2016) and SGP (Kim et al., 2014) on three different corpora (high values are better).", "startOffset": 16, "endOffset": 34}], "year": 2017, "abstractText": "We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b). These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms\u2014skipgram smoothing and skip-gram filtering\u2014that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.", "creator": "LaTeX with hyperref package"}}}