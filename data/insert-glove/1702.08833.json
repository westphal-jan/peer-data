{"id": "1702.08833", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees", "abstract": "Nearest neighbor (kNN) 111.28 methods have been gaining popularity masterminding in recent years dinsor in indentations light of advances safad in roofless hardware wisla and efficiency nefa of demilitarizing algorithms. virtus There henchman is deol a spring plethora dam of nasraoui methods fine-grained to lepel choose from 158.8 today, jbreyes each with hominick their segregation own tuncay advantages and disadvantages. One muthoka requirement h\u014dsh\u014d shared model-driven between all kNN hualan based methods milgrim is bozzini the need maili for buckhalter a charungvat good representation and distance measure koeppel between samples.", "histories": [["v1", "Tue, 28 Feb 2017 16:01:22 GMT  (660kb,D)", "http://arxiv.org/abs/1702.08833v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel zoran", "balaji lakshminarayanan", "charles blundell"], "accepted": false, "id": "1702.08833"}, "pdf": {"name": "1702.08833.pdf", "metadata": {"source": "CRF", "title": "Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees", "authors": ["Daniel Zoran", "Balaji Lakshminarayanan", "Charles Blundell"], "emails": ["<danielzoran@google.com>."], "sections": [{"heading": null, "text": "We introduce a new method called differentiable boundary tree which allows for learning deep kNN representations. We build on the recently proposed boundary tree algorithm which allows for efficient nearest neighbor classification, regression and retrieval. By modelling traversals in the tree as stochastic events, we are able to form a differentiable cost function which is associated with the tree\u2019s predictions. Using a deep neural network to transform the data and backpropagating through the tree allows us to learn good representations for k-NN methods.\nWe demonstrate that our method is able to learn suitable representations allowing for very efficient trees with a clearly interpretable structure."}, {"heading": "1. Introduction", "text": "There has been a growing interest in k-nearest neighbor (kNN) based methods in recent years (Muja and Lowe, 2009; Weinberger et al., 2005). With the increase in computational power, k-NN based methods have become viable for many different problems and have been used in various different contexts such as classification, regression and retrieval (Friedman et al., 1977; Beygelzimer et al., 2006).\nOne challenge that all nearest neighbor methods share is finding good representations and distance measures between samples. This can make all the difference to the success\n1DeepMind, London, UK. Correspondence to: Daniel Zoran <danielzoran@google.com>.\nof a given k-NN method, and more often than not, the representation is chosen ad-hoc or engineered by hand.\nAnother challenge that k-NN based methods pose is their computational and memory requirements. As much as hardware has advanced, this still remains a big problem. Typically k-NN methods scale quite badly with data size, often with challenging trade-offs \u2013 faster retrieval usually requires more memory (Friedman et al., 1977).\nThe Boundary Tree (or forest) algorithm, recently proposed by (Mathy et al., 2015), has some very appealing properties in tackling the computational and memory requirements of k-NN methods. In a nutshell, a boundary tree is a tree where each node corresponds to a training instance. At query time, the current node is to be the root node and a test data point is compared to the current node and its children: if the current node is the closest data point, the prediction is the class label or regression target at that node. Otherwise, the process recurses along the closest child. Hence, a boundary tree might be thought of as a hierarchical data structure that enables efficient k-NN queries. A boundary forest is an ensemble of randomized boundary trees. Boundary trees allow for fast k-NN classification, regression and retrieval and their memory requirements grow very slowly with the amount of data presented, all within a simple and elegant formulation.\nHowever, like all k-NN based methods, boundary trees still require a good representation and distance measure in order to perform well. Mathy et al. (2015) used L2 distance with raw features between inputs as their distance measure. While the L2 distance might be a sensible metric on pre-processed features (e.g. SIFT and HOG), it is clearly inadequate when dealing with raw (pixel) inputs.\nIn this paper we show how to learn useful representations for k-NN methods in an end-to-end fashion, by deriving a differentiable cost function for boundary trees. Given such a differentiable function we are able to use the power of deep neural networks to learn good representation of the data by back-propagating into the tree and stored samples. We demonstrate the effectiveness of the methods on the MNIST data set and provide further analysis into what boundary trees are capable of given a good learned representation. We\nar X\niv :1\n70 2.\n08 83\n3v 1\n[ cs\n.L G\n] 2\n8 Fe\nb 20\n17\ndiscuss the capabilities and limitations of the method and show show that with a good representation, boundary trees keep either \u201cprototypical\u201d examples of the data in addition to, as their name suggest, boundary cases \u2013 resulting in an interpretable and efficient data structure which still allows good classification results."}, {"heading": "2. Model", "text": ""}, {"heading": "2.1. Boundary Trees", "text": "Since boundary trees are new and relatively unknown, we provide a brief introduction to this elegant method here. We refer the reader to (Mathy et al., 2015) for a complete description of the algorithm. We only deal with the case of classification although regression and retrieval are also possible.\nConsider the 2D example in Figure 1. Samples from the green shaded area are labelled 0 (colored green) and samples from the white area are labelled 1 (colored red). Boundary trees are constructed in an online manner, sample by sample. Each node of the boundary tree corresponds to a sample from training data. The tree is initialized (setting the root xroot) with the first sample and its associated label (label 1, in this case, Figure 1). Given a new query sample y (in blue), we greedily traverse the tree: starting at the root, we find the closest node (according to some distance function) out of the children of the current node and the node itself, and recursively continue that traversal until we either reach a leaf (a node with no children) or stay at the current node (which means that it is closer to the query point than any of its immediate children). We use the label associated with the final node to produce the prediction of the tree (Figure 1); in this case, the prediction would be red. If the prediction is wrong (that is, the label associated with the final node is different than label of the query point) we add a new node containing the query point as a child to the final node (Figure 1). If the prediction is right, we discard the query node.\nThe resulting tree has the following interesting property \u2013 every edge, by definition, crosses a boundary between the classes, and the nodes stored in the tree will tend to reside close to these boundaries (Figure 1). This means that a good representation should try to separate the classes and form simple boundaries between them, resulting in efficient tree structures."}, {"heading": "2.2. Differentiable boundary trees", "text": "Now that we have an understanding of how boundary trees work we ask the question: is there a way to learn a good representation for boundary trees? Such a representation should be one that transforms the samples in a way which transforms the boundaries between different classes to be\nas simple as possible. This would result in simpler tree structures, faster queries and less memory requirements.\nOne way to achieve this is to associate a differentiable cost function with a boundary tree and optimize it directly. In order to achieve this we propose to model transitions in the tree as stochastic events where the respective probabilities are a function of the distance between the query node and the nodes in the tree.\nLet x denote the features of a data point, and let c denote one-hot encoding of the associated class label. Given the current node xi and the query node y we model the transition probability to node xj , where xj \u2208 {child(xi),xi}, that is one of the children of node xi or staying at this node, as the following:\np(xi \u2192 xj |y) = SoftMax i,j\u2208child(i) (\u2212d(xj ,y))\n= exp(\u2212d(xj ,y))\u2211\nj\u2032\u2208{i,j\u2208child(i)} exp(\u2212d(xj\u2032 ,y)) (1)\nWhere d(x,y) is a distance function between x and y. Though there are a wide variety of possible choices for d, we set it to be the L2 distance for the remainder of the paper:\nd(x,y) = \u221a\u2211 k (xk \u2212 yk)2 (2)\nwhere we note that x can be an embedding (as we show below, after obtaining a differentiable cost function we can augment it with a neural network to produce these embeddings) and not necessarily raw data.\nA path conditioned upon a query node y, denoted path(y), travels from the root node to the final node is a series of transitions i\u2192 j in the tree, each conditioned on the query node. Each transition is conditionally independent of the previous transitions. The probability of a path given a query node y is thus the product of probabilities for each transition along the path:\np(path|y) = \u220f\ni\u2192j\u2208path\np(xi \u2192 xj |y) (3)\nFinally, the predicted class c probability distribution given the tree and a query node p(c|y) is the expected prediction of final nodes over all possible paths. In practice, calculating the full expected class distribution may be quite expensive so we approximate this by a greedily chosen path (path\u2217) which is the same path chosen under the boundary tree algorithm:\np(c|y) = Epath|y(p(c|path,y)) \u2248 p(c|path\u2217,y) (4)\nInstead of using just the final node as the prediction we can use all the nodes that participated in the final transition in\nbuilding the output so we get softer class predictions. We remove the last transition from path\u2217 to obtain path\u2020. The final class log probabilities from the tree given the query node is:\nlog p(c|y) = \u2211\nxi\u2192xj\u2208path\u2020|y\nlog p(xi \u2192 xj |y)\n+ log \u2211\nxk\u2208sibling(xfinal\u2217 )\np(parent(xk)\u2192 xk|y)c(xk) (5)\nwhere sibling(xi) are all the nodes sharing a parent with node xi and the node xi itself (because the algorithm may stop at a non-leaf node), c(xi) is an indicator function for the class associated with node xi (a \u201cone hot\u201d encoding vector) and xfinal\u2217 is the final node in the greedy path. We normalize the class probabilities at the output to obtain a proper distribution. See Figure 2 for a visualization of the different elements.\nNow, instead of using the samples in their raw representation, we can transform them using a deep neural net f\u03b8(x) (the transform) such that we can learn a better representation of the data. This yields the following log class probabilities:\nlog p(c|f\u03b8(y)) = \u2211\nxi\u2192xj\u2208path\u2020|f\u03b8(y)\nlog p (f\u03b8(xi)\u2192 f\u03b8(xj)|f\u03b8(y)) +\nlog \u2211\nxk\u2208sibling(xfinal\u2217 )\np(parent(f\u03b8(xk))\u2192 f\u03b8(xk)|f\u03b8(y))c(xk) (6)\nPlugging Equation 6 into a loss function (we use crossentropy loss but other choices are possible) we can perform back-propagation in order to learn the parameters \u03b8 of the transform.\nAll of these calculations assume that the tree structure remains fixed \u2013 see Section 2.4 of how we handle this requirement in practice."}, {"heading": "2.3. Building the neural net", "text": "For each query point, we need to dynamically build a neural network which corresponds to the chosen path in the tree, get its class predictions and loss and then calculate its gradient w.r.t the parameters of the transform f\u03b8. Figure 3 depicts the structure of the network for an arbitrary path.\nOne thing to note is that each query point results in a different network being built, but the parameters of the transform f\u03b8 are shared throughout the process."}, {"heading": "2.4. Optimization", "text": "Since construction of the boundary tree requires discrete steps such as node and edge manipulation in the graph, it does not yield itself to back-propagation easily. In order to tackle this we choose to optimize the model in an iterative manner: we first build a boundary tree with a small number of samples, each transformed using the current transform f\u03b8. Then, using the learned tree and keeping it fixed, we take several gradient steps w.r.t \u03b8 to update the representation. We repeat this process until some convergence criteria is met (in our case, when loss changes less than a specific threshold). We use Adam (Kingma and Ba, 2014) as the gradient descent optimizer , though other stochastic gradient descent approaches are also possible. See Algorithm 1 for a summary.\nThroughout all experiments we use a single tree, not a forest. It is possible to use a forest but we found that this did not make a considerable difference to learning and it is slower (data not shown).It is worth noting that the ordering of the samples presented to the tree can have a significant effect on its performance but over the course of training this effect is subdues as the representation is improved (See below)."}, {"heading": "3. Related work", "text": "There is a large body of literature on k-NN methods and representation learning. Here we will mention some of the works which are most relevant to this one, but many others exist.\nThere are different families of nearest neighbors such as treebased methods, hashing-based methods, etc. Our work falls under the former category. Tree-based methods can be further subdivided into methods that recursively partition input space using splits (e.g. k-d trees (Friedman et al., 1977) and random forests (Breiman, 2001)) and methods that rely on distance comparisons to traverse down the tree. Examples of the latter include algorithms such as cover trees (Beygelzimer et al., 2006), ball trees (Liu et al., 2006), boundary trees (Mathy et al., 2015) and our proposed extension.\nOne of the key components in our method is modelling transition through the tree as stochastic events. Similar stochastic decisions have been explored in the decision literature; for example hierarchical mixture of experts (Jordan and Jacobs, 1994) and more recently, the work on so-called deep neural decision forests (Kontschieder et al., 2015). Though it shares some of the basic ideas, the latter is fundamentally different from our method \u2014 in the tree decision process (features vs. samples), in the tasks solved (directly solving classification vs. representation learning) and in the optimization process.\nAlgorithm 1 Learning representation using differentiable boundary trees 1: Initialize f\u03b8 to random weights 2: while not converged do . Convergence when change is below a threshold 3: Discard current tree T and initialize new tree 4: Train new tree T using samples transformed with current f\u03b8 5: for t = 1,NumSamplesForRepresentations do 6: Get next training sample y 7: Calculate loss using Equation 6 and current tree T 8: Take gradient of loss w.r.t parameters \u03b8 and perform gradient step 9: end for 10: end while\nOn the representation learning side, there are several works which are quite close to this one. In (Koch et al., 2015) a Siamese network is built to solve a \u2018same\u2019 versus \u2019not same\u2019 labelling task. The network is built with two streams, each receiving a sample which undergoes under some transformation (where the weights are shared between the two streams). The task for the net is to say whether the two samples are from the same class. On some level, this can be seen as a special case of our method where the tree consists of exactly two nodes and the loss function is an indicator function for class similarity. Another related representation learning method which is relevant is (Hoffer and Ailon, 2015). In this work a network is trained to predict class similarity using three samples: one sample is the reference sample, one comes from the same class as the reference and the other comes from a different class. The goal of the network is to push samples from the same class closer together and samples from different classes apart. Again, the weights are shared between the different streams of the system. This is can also seen as another special case of our method where the structure of the tree and samples chosen in its construction are pre-set and with a somewhat different loss function.\nTwo particularly related works are (Goldberger et al., 2004) and (Craven and Shavlik, 1996). In (Goldberger et al., 2004) a Mahalanobis distance metric is learned with the objective of improving nearest neighbor classification. Our work can be seen as a deep non-linear version of this work, augmented by a more efficient and structured nearest neighbor method. Finally, in (Craven and Shavlik, 1996) a decision tree is built using trained neural net features in order to obtain an interpretable decision structure. Our work is related to this idea, though we provide an end-to-end solution which learns the neural net together with the tree structure."}, {"heading": "4. Experiments and analysis", "text": ""}, {"heading": "4.1. Half-moon dataset", "text": "To gain an intuition about the workings of the our method we start with the half-moons dataset. This dataset has two\nclasses, each lying on a one dimensional manifold on a 2D plane (Figure 4). With enough samples, this is a task which is easy to solve with a variety of methods \u2013 including nearest neighbors based methods such as the boundary forest. Looking at Figure 4, it is clear that a non-linear transformation of the data may allow for a much simpler solution.\nWe learn a representation using our method \u2014 training on 1000 samples, iteratively building a boundary tree using 20 samples, and taking 10 gradient steps (each step over a different new sample not used in the tree) to update the representation f\u03b8 using the learned tree, repeating until convergence. For the transform f\u03b8 we train a 3 layer, fully connected net (2\u2192 100\u2192 100\u2192 30\u2192 2 units). Figure 4 shows the transformed samples after we train the representation using our method. Indeed, the transformation has learned to separate the two classes into two distinct clusters. When using this representation, training a boundary tree results in a 2 node tree which yields 0% training and 0% test errors."}, {"heading": "4.2. MNIST classification", "text": "To test our method on a more \u201creal world\u201d dataset (albeit a simple one) we use the MNIST handwritten digit dataset.\nAt each iteration we build a tree using 1000 samples transformed with current transform f\u03b8, then take a 1000 gradient steps (each on one new sample) to update the representation. We repeat this process, rebuilding the tree at each iteration until convergence. The representation is a 2 hidden layer, fully connected net (784\u2192 400\u2192 400\u2192 20 units respectively). We use Adam (Kingma and Ba, 2014) as our back-propagation optimizer. No pre-processing or data augmentation is used.\nAt the end of training we obtain an extremely simple tree with which we make the classification decisions. This simple tree is displayed in Figure 5. Samples are shown in their original pixel space (though the transformed representation is used to make the traversal decisions). As can be seen, the tree has a very interpretable structure \u2014 samples are either prototypical (for some of the classes such as \u201c4\u201d a single example is all that is needed) or more esoteric boundary cases (cropped or distorted digits). This is one of the more appealing properties of our method \u2014 we can actually try and understand what is the decision process the tree takes, in contrast to other tree based methods (Breiman, 2001; Kontschieder et al., 2015). Remarkably, this tree still achieves less than 2% error on the test set.\nFull results on the test set are presented in Table 1. As can be seen, using the representation yields better results than\nusing raw-pixels (as in (Mathy et al., 2015)) with a single tree. We also find that training a network with a comparable architecture directly on the classification task yields a representation which is less suited for a k-NN method such as the boundary tree (second row in the table).\nFigure 6 shows the test error and number of nodes in the tree as a function of iterations. As can be clearly seen, as the representation improves, the tree needs to keep less and less nodes \u2014 by the end of learning, the tree has just about 25 nodes, still achieving below 2% error on the test set.\nHow can the tree store such a small number of samples and still achieve this level of performance? In order to understand what the representation is doing, we plotted a t-SNE (Van der Maaten and Hinton, 2008) visualisation for the learned 20 dimensional representation, projected down to 2D. Figure 7 shows the results, together with the result of a pre-trained MNIST network of similar structure (trained directly on classification). As can be seen, the representation we learn clearly separates the classes from one another, much more than the neural net directly trained to classify. This enables the simple tree structure which is learned after transformation."}, {"heading": "4.3. Limitations and scaling", "text": "Due to the discrete nature of the path we take through the tree we need to build a different computation graph for each query node. This makes batching very inefficient and thus prevents us from running experiments on larger scale data such as ImageNet images requiring the use of GPUs. Another limitation is the iterative nature of the algorithm which requires switching between building the tree (a discrete operation) and updating the representation (which is continuous) \u2014 it would be more elegant and more efficient to perform both under the same framework.\nAnother limitation is that early in training the resulting tree may be quite large as the tree frequently errs, many samples are added. This may be alleviated by initially using less samples for building the tree increasing the variance of gradients but reducing computational and memory requirements.\nThough we cannot learn the representation directly on larger datasets we can certainly try to visualize what a boundary tree trained on pre-trained representation would yield. Figure 8 shows an example of this: we used a pre-trained VGGstyle network which was optimized for predicting classes on the CIFAR10 dataset using the cross entropy loss. We then use the outputs from the layer before the softmax, as embeddings for the inputs. As can be seen, the tree learned from 100 samples is quite compact with 22 nodes. The classification error for the tree is 13.06% vs. 9% for the pre-trained net."}, {"heading": "5. Discussion", "text": "We have presented a method to learn useful representations for k-NN methods. Using boundary trees we are able to derive a differentiable cost function which allows for learning of such representations. The resulting representation allows for simple, interpretable tree structures with good performance in query time and accuracy. While the method has some limitations we feel this is an important research direction and are looking forward to further explore this directions using newly available dynamic batching tools such as TensorFlow Fold.1\n1https://github.com/tensorflow/fold"}], "references": [{"title": "Cover trees for nearest neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2006}, {"title": "Extracting tree-structured representations of trained networks", "author": ["M. Craven", "J.W. Shavlik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Craven and Shavlik.,? \\Q1996\\E", "shortCiteRegEx": "Craven and Shavlik.", "year": 1996}, {"title": "An algorithm for finding best matches in logarithmic expected time", "author": ["J.H. Friedman", "J.L. Bentley", "R.A. Finkel"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "Friedman et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1977}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "G.E. Hinton", "S.T. Roweis", "R. Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Deep metric learning using triplet network. In Similarity-Based Pattern Recognition, pages 84\u201392", "author": ["E. Hoffer", "N. Ailon"], "venue": null, "citeRegEx": "Hoffer and Ailon.,? \\Q2015\\E", "shortCiteRegEx": "Hoffer and Ailon.", "year": 2015}, {"title": "Hierarchical mixtures of experts and the EM algorithm", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Neural computation,", "citeRegEx": "Jordan and Jacobs.,? \\Q1994\\E", "shortCiteRegEx": "Jordan and Jacobs.", "year": 1994}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["G. Koch", "R. Zemel", "R. Salakhutdinov"], "venue": "In International Conference for Machine Learning,", "citeRegEx": "Koch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koch et al\\.", "year": 2015}, {"title": "Deep neural decision forests", "author": ["P. Kontschieder", "M. Fiterau", "A. Criminisi", "S. Rota Bulo"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Kontschieder et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kontschieder et al\\.", "year": 2015}, {"title": "New algorithms for efficient high-dimensional nonparametric classification", "author": ["T. Liu", "A.W. Moore", "A. Gray"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "The boundary forest algorithm for online supervised and unsupervised learning", "author": ["C. Mathy", "N. Derbinsky", "J. Bento", "J. Rosenthal", "J. Yedidia"], "venue": "In AAAI,", "citeRegEx": "Mathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathy et al\\.", "year": 2015}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "VISAPP (1),", "citeRegEx": "Muja and Lowe.,? \\Q2009\\E", "shortCiteRegEx": "Muja and Lowe.", "year": 2009}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Weinberger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 11, "context": "There has been a growing interest in k-nearest neighbor (kNN) based methods in recent years (Muja and Lowe, 2009; Weinberger et al., 2005).", "startOffset": 92, "endOffset": 138}, {"referenceID": 13, "context": "There has been a growing interest in k-nearest neighbor (kNN) based methods in recent years (Muja and Lowe, 2009; Weinberger et al., 2005).", "startOffset": 92, "endOffset": 138}, {"referenceID": 2, "context": "With the increase in computational power, k-NN based methods have become viable for many different problems and have been used in various different contexts such as classification, regression and retrieval (Friedman et al., 1977; Beygelzimer et al., 2006).", "startOffset": 206, "endOffset": 255}, {"referenceID": 0, "context": "With the increase in computational power, k-NN based methods have become viable for many different problems and have been used in various different contexts such as classification, regression and retrieval (Friedman et al., 1977; Beygelzimer et al., 2006).", "startOffset": 206, "endOffset": 255}, {"referenceID": 2, "context": "Typically k-NN methods scale quite badly with data size, often with challenging trade-offs \u2013 faster retrieval usually requires more memory (Friedman et al., 1977).", "startOffset": 139, "endOffset": 162}, {"referenceID": 10, "context": "The Boundary Tree (or forest) algorithm, recently proposed by (Mathy et al., 2015), has some very appealing properties in tackling the computational and memory requirements of k-NN methods.", "startOffset": 62, "endOffset": 82}, {"referenceID": 10, "context": "Mathy et al. (2015) used L2 distance with raw features between inputs as their distance measure.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "We refer the reader to (Mathy et al., 2015) for a complete description of the algorithm.", "startOffset": 23, "endOffset": 43}, {"referenceID": 6, "context": "We use Adam (Kingma and Ba, 2014) as the gradient descent optimizer , though other stochastic gradient descent approaches are also possible.", "startOffset": 12, "endOffset": 33}, {"referenceID": 2, "context": "k-d trees (Friedman et al., 1977) and random forests (Breiman, 2001)) and methods that rely on distance comparisons to traverse down the tree.", "startOffset": 10, "endOffset": 33}, {"referenceID": 0, "context": "Examples of the latter include algorithms such as cover trees (Beygelzimer et al., 2006), ball trees (Liu et al.", "startOffset": 62, "endOffset": 88}, {"referenceID": 9, "context": ", 2006), ball trees (Liu et al., 2006), boundary trees (Mathy et al.", "startOffset": 20, "endOffset": 38}, {"referenceID": 10, "context": ", 2006), boundary trees (Mathy et al., 2015) and our proposed extension.", "startOffset": 24, "endOffset": 44}, {"referenceID": 5, "context": "Similar stochastic decisions have been explored in the decision literature; for example hierarchical mixture of experts (Jordan and Jacobs, 1994) and more recently, the work on so-called deep neural decision forests (Kontschieder et al.", "startOffset": 120, "endOffset": 145}, {"referenceID": 8, "context": "Similar stochastic decisions have been explored in the decision literature; for example hierarchical mixture of experts (Jordan and Jacobs, 1994) and more recently, the work on so-called deep neural decision forests (Kontschieder et al., 2015).", "startOffset": 216, "endOffset": 243}, {"referenceID": 7, "context": "In (Koch et al., 2015) a Siamese network is built to solve a \u2018same\u2019 versus \u2019not same\u2019 labelling task.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "Another related representation learning method which is relevant is (Hoffer and Ailon, 2015).", "startOffset": 68, "endOffset": 92}, {"referenceID": 3, "context": "Two particularly related works are (Goldberger et al., 2004) and (Craven and Shavlik, 1996).", "startOffset": 35, "endOffset": 60}, {"referenceID": 1, "context": ", 2004) and (Craven and Shavlik, 1996).", "startOffset": 12, "endOffset": 38}, {"referenceID": 3, "context": "In (Goldberger et al., 2004) a Mahalanobis distance metric is learned with the objective of improving nearest neighbor classification.", "startOffset": 3, "endOffset": 28}, {"referenceID": 1, "context": "Finally, in (Craven and Shavlik, 1996) a decision tree is built using trained neural net features in order to obtain an interpretable decision structure.", "startOffset": 12, "endOffset": 38}, {"referenceID": 6, "context": "We use Adam (Kingma and Ba, 2014) as our back-propagation optimizer.", "startOffset": 12, "endOffset": 33}, {"referenceID": 8, "context": "This is one of the more appealing properties of our method \u2014 we can actually try and understand what is the decision process the tree takes, in contrast to other tree based methods (Breiman, 2001; Kontschieder et al., 2015).", "startOffset": 181, "endOffset": 223}, {"referenceID": 10, "context": "As can be seen, using the representation yields better results than using raw-pixels (as in (Mathy et al., 2015)) with a single tree.", "startOffset": 92, "endOffset": 112}], "year": 2017, "abstractText": "Nearest neighbor (k-NN) methods have been gaining popularity in recent years in light of advances in hardware and efficiency of algorithms. There is a plethora of methods to choose from today, each with their own advantages and disadvantages. One requirement shared between all k-NN based methods is the need for a good representation and distance measure between samples. We introduce a new method called differentiable boundary tree which allows for learning deep kNN representations. We build on the recently proposed boundary tree algorithm which allows for efficient nearest neighbor classification, regression and retrieval. By modelling traversals in the tree as stochastic events, we are able to form a differentiable cost function which is associated with the tree\u2019s predictions. Using a deep neural network to transform the data and backpropagating through the tree allows us to learn good representations for k-NN methods. We demonstrate that our method is able to learn suitable representations allowing for very efficient trees with a clearly interpretable structure.", "creator": "LaTeX with hyperref package"}}}