{"id": "1703.00548", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Evolving Deep Neural Networks", "abstract": "The motorcoaching success singkil of deep learning sportsradio depends on glading finding 17.99 an architecture perishable to fit kerstein the task. robbe As defilippo deep learning andromedae has scaled up rajasuya to azules more hymen challenging tasks, cataractes the smolny architectures have become difficult copaxone to design shermarke by zipaquir\u00e1 hand. hvide This re-export paper novo proposes an prudnik automated communist method, CoDeepNEAT, familiares for sandboarding optimizing deep learning architectures mithu through 12.03 evolution. leguminosae By assizes extending existing neuroevolution frost methods kaski to 20-km topology, components, itajai and rushdoony hyperparameters, kaffir this method loudmouths achieves results breeden comparable agricole to zohar best remounted human courthouse designs in standard l\u2019industrie benchmarks guccio in object 3,307 recognition cerna and trevi language modeling. joukowsky It also awamori supports healthpartners building shyama a real - world application olms of decelerations automated image captioning capelin on a sarmiento magazine website. new-style Given the anticipated id3 increases in available computing power, lithia evolution of derangements deep networks redwood is promising approach sopka to deadliest constructing deep learning news.com.au applications espinosa in fresca the future.", "histories": [["v1", "Wed, 1 Mar 2017 23:40:42 GMT  (2540kb,D)", "http://arxiv.org/abs/1703.00548v1", null], ["v2", "Sat, 4 Mar 2017 23:13:05 GMT  (2540kb,D)", "http://arxiv.org/abs/1703.00548v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["risto miikkulainen", "jason liang", "elliot meyerson", "aditya rawal", "dan fink", "olivier francon", "bala raju", "hormoz shahrzad", "arshak navruzyan", "nigel duffy", "babak hodjat"], "accepted": false, "id": "1703.00548"}, "pdf": {"name": "1703.00548.pdf", "metadata": {"source": "META", "title": "Evolving Deep Neural Networks", "authors": ["Risto Miikkulainen", "Jason Liang", "Elliot Meyerson", "Aditya Rawal", "Dan Fink", "Olivier Francon", "Bala Raju", "Arshak Navruzyan", "Nigel Du\u0082y", "Babak Hodjat"], "emails": [], "sections": [{"heading": null, "text": "KEYWORDS Neural networks, deep learning, LSTMs, bilevel optimization, coevolution, design"}, {"heading": "1 INTRODUCTION", "text": "Large databases (i.e. Big Data) and large amounts of computing power have become readily available since the 2000s. As a result, it has become possible to scale up machine learning systems. Interestingly, not only have these systems been successful in such scaleup, but they have become more powerful. Some ideas that did not quite work before, now do, with million times more compute and data. For instance, deep learning neural networks (DNNs), i.e. convolutional neural networks [30] and recurrent neural networks (in particular long short-term memory, or LSTM [22]), which have existed since the 1990s, have improved state-of-the-art signi cantly in computer vision, speech, language processing, and many other areas [9, 17, 45].\nAs DNNs have been scaled up and improved, they have become much more complex. A new challenge has therefore emerged: How to con gure such systems? Human engineers can optimize a handful of con guration parameters through experimentation, but DNNs have complex topologies and hundreds of hyperparameters. Moreover, such design choices ma er; o en success depends on nding the right architecture for the problem. Much of the recent work in deep learning has indeed focused on proposing di erent hand-designed architectures on new problems [5, 20, 39, 45].\ne complexity challenge is not unique to neural networks. So - ware and many other engineered systems have become too complex for humans to optimize fully. As a result, a new way of thinking about such design has started to emerge. In this approach, humans are responsible for the high-level design, and the details are\nle for computational optimization systems to gure out. For instance, humans write the overall design of a so ware system, and the parameters and low-level code is optimized automatically [23]; humans write imperfect versions of programs, and evolutionary algorithms are then used to repair them [15]; humans de ne the space of possible web designs, and evolution is used to nd e ective ones [35].\nis same approach can be applied to the design of DNN architectures. is problem includes three challenges: how to design the components of the architecture, how to put them together into a full network topology, and how to set the hyperparameters for the components and the global design. ese three aspects need to be optimized separately for each new task.\nis paper develops an approach for automatic design of DNNs. It is based on the existing neuroevolution technique of NEAT [43], which has been successful in evolving topologies and weights of relatively small recurrent networks in the past. In this paper, NEAT is extended to the coevolutionary optimization of components, topologies, and hyperparameters. e tness of the evolved networks is determined based on how well they can be trained, through gradient descent, to perform in the task. e approach is demonstrated in the standard benchmark tasks of object recognition and language modeling, and in a real-world application of captioning images on a magazine website.\ne results show that the approach discovers designs that are comparable to the state of the art, and does it automatically without much development e ort. e approach is computationally extremely demanding\u2014with more computational power, it is likely to be more e ective and possibly surpass human design. Such power is now becoming available in various forms of cloud computing and grid computing, thereby making evolutionary optimization of neural networks a promising approach for the future."}, {"heading": "2 BACKGROUND AND RELATEDWORK", "text": "Neuroevolution techniques have been applied successfully to sequential decision tasks for three decades [11, 31, 36, 50]. In such tasks there is no gradient available, so instead of gradient descent, evolution is used to optimize the weights of the neural network. Neuroevolution is a good approach in particular to POMDP (partially observable Markov decision process) problems because of recurrency: It is possible to evolve recurrent connections to allow disambiguating hidden states.\ne weights can be optimized using various evolutionary techniques. Genetic algorithms are a natural choice because crossover is a good match with neural networks: they recombine parts of existing neural networks to nd be er ones. CMA-ES [24], a technique for continuous optimization, works well on optimizing the\nar X\niv :1\n70 3.\n00 54\n8v 1\n[ cs\n.N E\n] 1\nM ar\n2 01\n7\nweights as well because it can capture interactions between them. Other approaches such as SANE, ESP, and CoSyNE evolve partial neural networks and combine them into fully functional networks [12, 14, 37]. Further, techniques such as Cellular Encoding [18] and NEAT [43] have been developed to evolve the topology of the neural network, which is particularly e ective in determining the required recurrence. Neuroevolution techniques have been shown to work well in many tasks in control, robotics, constructing intelligent agents for games, and arti cial life [31]. However, because of the large number of weights to be optimized, they are generally limited to relatively small networks.\nEvolution has been combined with gradient-descent based learning in several ways, making it possible to utilize much larger networks. ese methods are still usually applied to sequential decision tasks, but gradients from a related task (such as prediction of the next sensory inputs) are used to help search. Much of the work is based on utilizing the Baldwin e ect, where learning only a ects the selection [21]. Computationally, it is possible to utilize Lamarckian evolution as well, i.e. encode the learned weight changes back into the genome [18]. However, care must be taken to maintain diversity so that evolution can continue to innovate when all individuals are learning similar behavior.\nEvolution of DNNs departs from this prior work in that it is applied to supervised domains where gradients are available, and evolution is used only to optimize the design of the neural network. Deep neuroevolution is thus more closely related to bilevel (or multilevel) optimization techniques [40]. e idea is to use an evolutionary optimization process at a high level to optimize the parameters of a low-level evolutionary optimization process.\nConsider for instance the problem of controlling a helicopter through aileron, elevator, rudder, and rotor inputs. is is a challenging benchmark from the 2000s for which various reinforcement learning approaches have been developed [1, 2, 38]. One of the most successful ones is single-level neuroevolution, where the helicopter is controlled by a neural network that is evolved through genetic algorithms [29]. e eight parameters of the neuroevolution method (such mutation and crossover rate, probability, and amount and population and elite size) are optimized by hand. It would be di - cult to include more parameters because the parameters interact nonlinearly. A large part of the parameter space thus remains unexplored in the single-level neuroevolution approach. However, a bilevel approach, where a high-level evolutionary process is employed to optimize these parameters, can search this space more e ectively [32]. With bilevel evolution, the number of parameters optimized could be extended to 15, which resulted in signi cantly be er performance. In this manner, evolution was harnessed in this example task to optimize a system design that was too complex to be optimized by hand.\nVery recently, a studies have started to emerge with the goal of optimizing DNNs. Due to limited computational resources, they have focused on speci c parts of the design. For instance, Loshchilov et al. [33] used CMA-ES to optimize the hyperparameters of existing DNNs obtaining state-of-the-art results on e.g. object recognition. Further, Fernando et al. [10] evolved a CPPN (compositional pa ern-producing network [42]) to output the weights of an auto-encoder neural network. e autoencoder was then trained further through gradient descent, forming gradients for\nthe CPPN training, and its trained weights were then incorporated back into the CPPN genome through Lamarckian adaptation. A related approach was proposed by Zoph and Le [54]: the topology and hyperparameters of a deep network and LSTM network were modi ed through policy iteration.\nBuilding on this foundation, a systematic approach to evolving DNNs is developed in this paper. First, the standard NEAT neuroevolution method is applied to the topology and hyperparameters of convolutional neural networks, and then extended to evolution of components as well, achieving results comparable to state of the art in the CIFAR-10 image classi cation benchmark. Second, a similar method is used to evolve the structure of LSTM networks in language modeling, showing that even small innovations in the components can have a signi cant e ect on performance. ird, the approach is used to build a real-world application on captioning images in the website for an online magazine."}, {"heading": "3 EVOLUTION OF DEEP LEARNING ARCHITECTURES", "text": "NEAT neuroevolution method [43] is rst extended to evolving network topology and hyperparameters of deep neural networks in DeepNEAT, and then further to coevolution of modules and blueprints for combining them in CoDeepNEAT. e approach is tested in the standard CIFAR-10 benchmark of object recognition, and found to be comparable to the state of the art."}, {"heading": "3.1 Extending NEAT to Deep Networks", "text": "DeepNEAT is a most immediate extension of the standard neural network topology-evolution method NEAT to DNN. It follows the same fundamental process as NEAT: First, a population of chromosomes (each represented by a graph) with minimal complexity is created. Over generations, structure (i.e. nodes and edges) is added to the graph incrementally through mutation. During crossover, historical markings are used to determine how genes of two chromosomes can be lined up. e population is divided into species (i.e. subpopulations) based on a similarity metric. Each species grows\nproportionally to its tness and evolution occurs separately in each species.\nDeepNEAT di ers from NEAT in that each node in the chromosome no longer represents a neuron, but a layer in a DNN. Each node contains a table of real and binary valued hyperparameters that are mutated through uniform Gaussian distribution and random bit- ipping, respectively. ese hyperparameters determine the type of layer (such as convolutional, fully connected, or recurrent) and the properties of that layer (such as number of neurons, kernel size, and activation function). e edges in the chromosome are no longer marked with weights; instead they simply indicate how the nodes (layers) are connected together. To construct a DNN from a DeepNEAT chromosome, one simply needs to traverse the chromosome graph, replacing each node with the corresponding layer. e chromosome also contains a set of global hyperparameters applicable to the entire network (such as learning rate, training algorithm, and data preprocessing).\nWhen arbitrary connectivity is allowed between layers, additional complexity is required. If the current layer has multiple parent layers, a merge layer must be applied to the parents in order to ensure that the parent layer\u2019s output is the same size as the current layer\u2019s input. Typically, this adjustment is done through a concatenation or element-wise sum operation. If the parent layers have mismatched output sizes, all of the parent layers must be downsampled to parent layer with the smallest output size. e speci c method for downsampling is domain dependent. For example, in image classi cation, a max-pooling layer is inserted a er speci c parent layers; in image captioning, a fully connected bo leneck layer will serve this function.\nDuring tness evaluation, each chromosome is converted into a DNN. ese DNNs are then trained for a xed number of epochs. A er training, a metric that indicates the network\u2019s performance is returned back to DeepNEAT and assigned as tness to the corresponding chromosome in the population.\nWhile DeepNEAT can be used to evolve DNNs, the resulting structures are o en complex and unprincipled. ey contrast with typical DNN architectures that utilize repetition of basic components. DeepNEAT is therefore extend to evolution of modules and blueprints next."}, {"heading": "3.2 Cooperative Coevolution of Modules and Blueprints", "text": "Many of the most successful DNNs, such as GoogLeNet and ResNet are composed of modules that are repeated multiple times [20, 45]. ese modules o en themselves have complicated structure with branching and merging of various layers. Inspired by this observation, a variant of DeepNEAT, called Coevolution DeepNEAT (CoDeepNEAT), is proposed. e algorithm behind CoDeepNEAT is inspired mainly by Hierarchical SANE [37] but is also in uenced by component-evolution approaches ESP [13] and CoSyNE [14].\nIn CoDeepNEAT, two populations of modules and blueprints are evolved separately, using the same methods as described above for DeepNEAT. e blueprint chromosome is a graph where each node contains a pointer to a particular module species. In turn, each module chromosome is a graph that represents a small DNN. During tness evaluation, the modules and blueprints are combined\ntogether to create a larger assembled network Figure 1. Each node in the blueprint is replaced with a module chosen randomly from the species to which that node points. If multiple blueprint nodes point to the same module species, then the same module is used in all of them. e assembled networks are evaluated the a manner similar to DeepNEAT, but the tnesses of the assembled networks are a ributed back to blueprints and modules as the average tness of all the assembled networks containing that blueprint or module.\nCoDeepNEAT can evolve repetitive modular structure e ciently. Furthermore, because small mutations in the modules and blueprints o en lead to large changes in the assembled network structure, CoDeepNEAT can explore more diverse and deeper architectures than DeepNEAT. An example application to the CIFAR-10 domain is presented next."}, {"heading": "3.3 Evolving DNNs in the CIFAR-10 Benchmark", "text": "In this experiment, CoDeepNEAT was used to evolve the topology of a convolutional neural network (CNN) to maximize its classication performance on the CIFAR-10 dataset, a common image classi cation benchmark. e dataset consists of 50,000 training images and 10,000 testing images. e images consist of 32x32 color pixels and belong to one of 10 classes. For comparison, the neural network layer types were restricted to those used by Snoek et al. [41] in their Bayesian optimization of CNN hyperparameters. Also following Snoek et al., data augmentation consisted of converting the images from RGB to HSV color space, adding random perturbations, distortions, and crops, and converting them back to RGB color space.\nCoDeepNEAT was initialized with populations of 25 blueprints and 45 modules. From these two populations, 100 CNNs were assembled for tness evaluation in every generation. Each node in the module chromosome represents a convolutional layer. Its hyperparameters determine the various properties of the layer and whether additional max-pooling or dropout layers are a ached (Table 1).\nIn addition, a set of global hyperparameters were evolved for the assembled network. During tness evaluation, the 50,000 images were split into a training set of 42,500 samples and a validation set of 7,500 samples. Since training a DNN is computationally very expensive, each network was trained for eight epochs on the training set. e validation set was then used to determine classi cation accuracy, i.e. the tness of the network. A er 72 generations of evolution, the best network in the population was returned.\nA er evolution was complete, the best network was trained on all 50,000 training images for 300 epochs, and the classi cation error measured. is error was 7.3%, comparable to the 6.4% error reported by Snoek et al. [41]. Interestingly, because only limited training could be done during evolution, the best network evolved by CoDeepNEAT trains very fast. While the network of Snoek et al. takes over 30 epochs to reach 20% test error and over 200 epochs to converge, the best network from evolution takes only 12 epochs to reach 20% test error and around 120 epochs to converge. is network utilizes the same modules multiple times, resulting in a deep and repetitive structure typical of many successful DNNs (Figure 2)."}, {"heading": "4 EVOLUTION OF LSTM ARCHITECTURES", "text": "Recurrent neural networks, in particular those utilizing LSTM nodes, is another powerful approach to DNN. Much of the power comes from repetition of LSTM modules and the connectivity between them. In this section, CoDeepNEAT is extended with mutations that allow searching for such connectivity, and the approach is evaluated in the standard benchmark task of language modeling."}, {"heading": "4.1 Extending CoDeepNEAT to LSTMs", "text": "Long Short Term Memory (LSTM) consists of gated memory cells that can integrate information over longer time scales (as compared\nto simply using recurrent connections in a neural network). LSTMs have recently been shown powerful in supervised sequence processing tasks such as speech recognition [16] and machine translation [3].\nRecent research on LSTMs has focused in two directions: Finding variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53]. Both approaches have improved performance over vanilla LSTMs, with best recent results achieved through network design. e CoDeepNEAT method incorporates both approaches: neuroevolution searches for both new LSTM units and their connectivity across multiple layers at the same time.\nCoDeepNEAT was slightly modi ed to make it easier to nd novel connectivities between LSTM layers. Multiple LSTM layers are a ened into a neural network graph that is then modi ed by neuroevolution. ere are two types of mutations: one enables or disables a connection between LSTM layers, and the other adds or removes skip connections between two LSTM nodes. Recently, skip connections have led to performance improvements in deep neural networks, which suggests that they could be useful for LSTM networks as well. us, neuroevolution modi es both the high-level network topology and the low-level LSTM connections.\nIn each generation, a population of these network graphs (i.e. blueprints), consisting of LSTM variants (i.e. modules with possible skip connections), is created. e individual networks are then trained and tested with the supervised data of the task. e experimental setup and the language modeling task are described next."}, {"heading": "4.2 Evolving DNNs in the Language Modeling Benchmark", "text": "One standard benchmark task for LSTM network is language modeling, i.e. predicting the next word in a large text corpus. e benchmark utilizes the Penn Tree Bank (PTB) dataset [34], which consists of 929k training words, 73k validation words, and 82k test words. It has 10k words in its vocabulary.\nA population of 50 LSTM networks was initialized with uniformly random initial connection weights within [-0.05, 0.05]. Each network consisted of two recurrent layers (vanilla LSTM or its variants) with 650 hidden nodes in each layer. e network was unrolled in time upto 35 steps. e hidden states were initialized to zero. e nal hidden states of the current minibatch was used as the initial hidden state of the subsequent minibatch (successive minibatches sequentially traverse the training set). e size of each minibatch was 20. For tness evaluation, each network was trained for 39 epochs. A learning rate decay of 0.8 was applied at the end of every six epochs; the dropout rate was 0.5. e gradients were clipped if their maximum norm (normalized by minibatch size) exceeded 5. Training a single network took about 200 minutes on a GeForce GTX 980 GPU card.\nA er 25 generations of neuroevolution, the best network improved the performance on PTB dataset by 5% (test-perplexity score 78) as compared to the vanilla LSTM [52]. As shown in Figure3, this LSTM variant consists of a feedback skip connection between the memory cells of two LSTM layers. is result is interesting\nbecause it is similar to a recent hand-designed architecture that also outperforms vanilla LSTM [8].\ne initial results thus demonstrate that CoDeepNEAT with just two LSTM-speci c mutations can automatically discover improved LSTM variants. It is likely that expanding the search space with more mutation types and layer and connection types would lead to further improvements."}, {"heading": "5 APPLICATION CASE STUDY: IMAGE CAPTIONING FOR THE BLIND", "text": "In a real-world case study, the vision and language capabilities of CoDeepNEAT were combined to build a real-time online image captioning system. In this application, CoDeepNEAT searches for architectures that learn to integrate image and text representations to produce captions that blind users can access through existing screen readers. is application was implemented for a major online magazine website (site address removed for double-blind reviewing). Evolved networks were trained with the open source MSCOCO image captioning dataset [6], along with a new dataset collected for this website."}, {"heading": "5.1 Evolving DNNs for Image Captioning", "text": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51]. e input to an image captioning system is a raw image, and the output is a text caption intended to describe the contents of the image. In deep learning approaches, a convolutional network is usually used to process the image, and\nrecurrent units, o en LSTMs, to generate coherent sentences with long-range dependencies.\nAs is common in existing approaches, the evolved system uses a pretrained ImageNet model [45] to produce initial image embeddings. e evolved network takes an image embedding as input, along with a one-hot text input. As usual, in training the text input contains the previous word of the ground truth caption; in inference it contains the previous word generated by the model [27, 47].\nIn the initial CoDeepNEAT population the image and text inputs are fed to a shared embedding layer, which is densely connected to a so max output over words. From this simple starting point, CoDeepNEAT evolves architectures that include fully connected layers, LSTM layers, sum layers, concatenation layers, and sets of hyperparameters associated with each layer, along with a set of global hyperparameters (Table 2). In particular, the well-known Show and Tell image captioning architecture [47] is in this search space, providing a baseline with which evolution results can be compared. ese components and the glue that connects them are evolved as described in Section 3.2, with 100 networks trained in each generation. Since there is no single best accepted metric for evaluating captions, the tness function is the mean across three metrics (BLEU, METEOR, and CIDEr; [6]) normalized by their baseline values. Fitness is computed over a holdout set of 5000 images, i.e. 25,000 image-caption pairs.\nTo keep the computational cost reasonable, during evolution the networks are trained for only six epochs, and only with a random 100,000 image subset of the 500,000 MSCOCO image-caption pairs. As a result, there is evolutionary pressure towards networks that converge quickly: e best resulting architectures train to near convergence 6 times faster than the baseline Show and Tell model [47]. A er evolution, the optimized learning rate is scaled by oneh to compensate for the subsampling."}, {"heading": "5.2 Building the Application", "text": "e images in MSCOCO are chosen to depict \u201ccommon objects in context\u201d. e focus is on a relatively small set of objects and their interactions in a relatively small set of se ings. e internet as a whole, and the online magazine website in particular, contain many\nimages that cannot be classi ed as \u201ccommon objects in context\u201d. Other types of images from the magazine include staged portraits of people, infographics, cartoons, abstract designs, and iconic images, i.e. images of one or multiple objects out of context such as on a white or pa erned background. erefore, an additional dataset of 17,000 image-caption pairs was constructed for the case study, targeting iconic images in particular. Four thousand images were rst scraped from the magazine website, and 1000 of them were identi ed as iconic. en, 16,000 images that were visually similar to those 1000 were retrieved automatically from a large image repository. A single ground truth caption for each of these 17K images was generated by human subjects through Spare51. e holdout set for evaluation consisted of 100 of the original 1000 iconic images, along with 3000 other images.\nDuring evolution, networks were trained and evaluated only on the MSCOCO data. e best architecture from evolution was then trained from scratch on both the MSCOCO and Spare5 datasets in an iterative alternating approach: one epoch on MSCOCO, followed by ve epochs on Spare5, until maximum performance was reached on the Spare5 holdout data. Beam search was then used to generate captions from the fully trained models. Performance achieved using the Spare5 data demonstrates the ability of evolved architectures to generalize to domains towards which they were not evolved.\nOnce the model was fully-trained, it was placed on a server where it can be queried with images to caption. A JavaScript snippet was wri en that a developer can embed in his/her site to automatically query the model to caption all images on a page. is snippet runs in an existing Chrome extension for custom scripts and automatically captions images as the user browses the web. ese tools add captions to the \u2018alt\u2019 eld of images, which screen readers can then read to blind internet users (Figure 4)."}, {"heading": "5.3 Image Captioning Results", "text": "Trained in parallel on about 100 GPUs, each generation took around one hour to complete. e most t architecture was discovered on generation 37 (Figure 5). is architecture performs be er than the\n1h ps://mty.ai/computer-vision/\nhand-tuned baseline [47] when trained on the MSCOCO data alone (Table 3).\nHowever, a more important result is the performance of this network on the magazine website. Because no suitable automatic metrics exist for the types of captions collected for the magazine website (and existing metrics are very noisy when there is only one\nreference caption), captions generated by the evolved model on all 3100 holdout images were manually evaluated on a scale from 1 to 4 (Figure 6). Figure 7 shows some examples of good and bad captions for these images.\ne model is not perfect, but the results are promising. ere are many known improvements that can be implemented, including ensembling diverse architectures generated by evolution, ne-tuning of the ImageNet model, using a more recent ImageNet model, and performing beam search or scheduled sampling during training [48]. For this application, it is also important to include methods for automatically evaluation caption quality and ltering captions that would give an incorrect impression to a blind user. However, even without these additions, the results demonstrate that it is now possible to develop practical applications through evolving DNNs."}, {"heading": "6 DISCUSSION AND FUTUREWORK", "text": "e results in this paper show that the evolutionary approach to optimizing deep neural networks is feasible: e results are comparable hand-designed architectures in benchmark tasks, and it is possible to build real-world applications based on the approach. It is important to note that the approach has not yet been pushed to its full potential. It takes a couple of days to train each deep neural network on a state-of-the-art GPU, and over the course of evolution, thousands of them need to be trained. erefore, the results are limited by the available computational power. Interestingly, since it was necessary to train networks only partially during evolution, evolution is biased towards discovering fast learners instead of top performers. is is an interesting result on its own: evolution can be guided with goals other than simply accuracy, including training time, execution time, or memory requirements of the network.\nSigni cantly more computational resources are likely to become available in the near future. Already cloud-based services such as Amazon Web Services o er GPU computation with a reasonable cost, and e orts to harness idle cycles on gaming center GPUs are underway. At Sentient, for example, a distributed AI computing system called DarkCycle is being built that currently utilizes 2M CPUs and 5000 GPUs around the world, resulting in a peak performance of 9 peta ops, on par with the fastest supercomputers in the world. Not many approaches can take advantage of such power, but evolution of deep learning neural networks can. e search space of di erent components and topologies can be extended, and more hyperparameters be optimized. Given the results in this paper, this approach is likely to discover designs that are superior to those that can be developed by hand today; it is also likely to make it possible to apply deep learning to a wider array of tasks and applications in the future."}, {"heading": "7 CONCLUSION", "text": "Evolutionary optimization makes it possible to construct more complex deep learning architectures than can be done by hand. e topology, components, and hyperparameters of the architecture can all be optimized simultaneously to t the requirements of the task, resulting in superior performance. is automated design can make new applications of deep learning possible in vision, speech, language, and other areas. Currently such designs are comparable with best human designs; with anticipated increases in computing power, they should soon surpass them, pu ing the power to good use."}], "references": [{"title": "An Application of Reinforcement Learning to Aerobatic Helicopter Flight", "author": ["Pieter Abbeel", "Adam Coates", "Morgan \u008bigley", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Autonomous Helicopter Control Using Reinforcement Learning Policy Search Methods", "author": ["James Bagnell", "Je\u0082 Schneider"], "venue": "In Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Evolving memory cell structures for sequence learning", "author": ["J. Bayer", "D. Wierstra", "J. Togelius", "J. Schmidhuber"], "venue": "In In Arti\u0080cial Neural Networks ICANN", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "author": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "venue": "CoRR abs/1606.01865", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Microso\u0089 COCO captions: Data collection and evaluation", "author": ["X. Chen", "H. Fang", "T.Y. Lin", "R. Vedantam", "S. Gupta", "P. Dollar", "C.L. Zitnick"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A uni\u0080ed architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Convolution by Evolution: Di\u0082erentiable Pa\u008aern Producing Networks", "author": ["Chrisantha Fernando", "Dylan Banarse", "Frederic Besse", "Max Jaderberg", "David Pfau", "Malcolm Reynolds", "Marc Lactot", "Daan Wierstra"], "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Neuroevolution: From Architectures to Learning", "author": ["Dario Floreano", "Peter D\u00fcrr", "Claudio Ma\u008aiussi"], "venue": "Evolutionary Intelligence", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Incremental Evolution of Complex General Behavior", "author": ["Faustino Gomez", "Risto Miikkulainen"], "venue": "Adaptive Behavior", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Solving Non-Markovian Control Tasks with Neuroevolution", "author": ["Faustino Gomez", "Risto Miikkulainen"], "venue": "In Proceedings of the 16th International Joint Conference on Arti\u0080cial Intelligence. Morgan Kaufmann,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Accelerated Neural Evolution \u008crough Cooperatively Coevolved Synapses", "author": ["Faustino Gomez", "J\u00fcrgen Schmidhuber", "Risto Miikkulainen"], "venue": "Journal of Machine Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "GenProg: Automatic Bug Correction in Real Programs", "author": ["C. Le Goues", "T. Nguyen", "S. Forrest", "W. Weimer"], "venue": "ACM Transactions on So\u0087ware Engineering", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In In Proc", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geo\u0082rey Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Adding Learning to the Cellular Development of Neural Networks: Evolution and the Baldwin E\u0082ect", "author": ["Frederic Gruau", "Darrell Whitley"], "venue": "Evolutionary Computation", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR abs/1512.03385", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR abs/1603.05027", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "How Learning Can Guide Evolution", "author": ["Geo\u0082rey E. Hinton", "Steven J. Nowlan"], "venue": "Complex Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1987}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Programming by Optimization", "author": ["Holger Hoos"], "venue": "Commun. ACM", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Neuroevolution for Reinforcement Learning Using Evolution Strategies", "author": ["Christian Igel"], "venue": "In Proceedings of the 2003 Congress on Evolutionary Computation. IEEE Press, Piscataway,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Grid Long Short-Term Memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "CoRR abs/1507.01526", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In Proc. of CVPR", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "LSTM: A search space odyssey", "author": ["G. Klaus", "R. Srivastava", "J. Koutnk", "R. Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arxiv/1503.04069", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Neuroevolutionary reinforcement learning for generalized control of simulated helicopters", "author": ["Rogier Koppejan", "Shimon Whiteson"], "venue": "Evolutionary Intelligence", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Gradientbased learning applied to document recognition", "author": ["Yann LeCun", "Leon Bo\u008aou", "Yoshua Bengio", "Patrick Ha\u0082ner"], "venue": "Proc. IEEE", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Evolutionary Bilevel Optimization for Complex Control Tasks", "author": ["Jason Zhi Liang", "Risto Miikkulainen"], "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "author": ["Ilya Loshchilov", "Frank Hu\u008aer"], "venue": "CoRR abs/1604.07269", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Building a large annotated corpus of english: \u008ce penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1993}, {"title": "Conversion Rate Optimization through Evolutionary Computation", "author": ["Risto Miikkulainen", "Neil Iscoe"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "Training Feedforward Neural Networks Using Genetic Algorithms", "author": ["David J. Montana", "Lawrence Davis"], "venue": "In Proceedings of the 11th International Joint Conference on Arti\u0080cial Intelligence", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1989}, {"title": "Forming Neural Networks \u008crough E\u0081cient and Adaptive Co-Evolution", "author": ["David E. Moriarty", "Risto Miikkulainen"], "venue": "Evolutionary Computation", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1997}, {"title": "Autonomous Helicopter Flight via Reinforcement Learning", "author": ["Andrew Y. Ng", "H. Jin Kim", "Michael Jordan", "Shankar Sastry"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "Beyond Short Snippets: Deep Networks for Video Classi\u0080cation", "author": ["Joe Yue-Hei Ng", "Ma\u008ahew J. Hausknecht", "Sudheendra Vijayanarasimhan", "Oriol Vinyals", "Rajat Monga", "George Toderici"], "venue": "CoRR abs/1503.08909", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "A bilevel optimization approach to automated parameter tuning", "author": ["Ankur Sinha", "Pekka Malo", "Peng Xu", "Kalyanmoy Deb"], "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Scalable Bayesian Optimization Using Deep Neural Networks", "author": ["J. Snoek", "O. Rippel", "K. Swersky", "R. Kiros", "N. Satish", "N. Sundaram", "M.M.A. Patwary", "M. Prabhat", "R.P. Adams"], "venue": "In Proc. of ICML", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Compositional Pa\u008aern Producing Networks: A Novel Abstraction of Development", "author": ["Kenneth Stanley"], "venue": "Genetic Programming and Evolvable Machines", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Evolving Neural Networks Through Augmenting Topologies", "author": ["Kenneth O. Stanley", "Risto Miikkulainen"], "venue": "Evolutionary Computation", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "Inception-v4, inceptionresnet and the impact of residual connections on learning", "author": ["C. Szegedy", "S. Io\u0082e", "V. Vanhoucke", "A. Alemi"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Io\u0082e", "J. Shlens", "Z. Wojna"], "venue": "In Proc. of CVPR", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Context-aware Captions from Context-agnostic Supervision", "author": ["R. Vedantam", "S. Bengio", "K. Murphy", "D. Parikh", "G. Chechik"], "venue": "arXiv preprint arxiv/1701.02870", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2017}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In Proc. of CVPR", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Trans. on Pa\u0088ern Analysis and Machine Intelligence", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Show, A\u008aend and Tell: Neural Image Caption Generation with Visual A\u008aention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salkhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "In Proc. of ICML", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "Evolving Arti\u0080cial Neural Networks", "author": ["Xin Yao"], "venue": "Proc. IEEE 87,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1999}, {"title": "Image captioning with semantic a\u008aention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "In Proc. of CVPR", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arxiv/1409.2329", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Neural Architecture Search with Reinforcement Learning", "author": ["Barret Zoph", "\u008boc V. Le"], "venue": "CoRR abs/1611.01578", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "convolutional neural networks [30] and recurrent neural networks (in particular long short-term memory, or LSTM [22]), which have existed since the 1990s, have improved state-of-the-art signi\u0080cantly", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "convolutional neural networks [30] and recurrent neural networks (in particular long short-term memory, or LSTM [22]), which have existed since the 1990s, have improved state-of-the-art signi\u0080cantly", "startOffset": 112, "endOffset": 116}, {"referenceID": 7, "context": "in computer vision, speech, language processing, and many other areas [9, 17, 45].", "startOffset": 70, "endOffset": 81}, {"referenceID": 15, "context": "in computer vision, speech, language processing, and many other areas [9, 17, 45].", "startOffset": 70, "endOffset": 81}, {"referenceID": 42, "context": "in computer vision, speech, language processing, and many other areas [9, 17, 45].", "startOffset": 70, "endOffset": 81}, {"referenceID": 4, "context": "hand-designed architectures on new problems [5, 20, 39, 45].", "startOffset": 44, "endOffset": 59}, {"referenceID": 18, "context": "hand-designed architectures on new problems [5, 20, 39, 45].", "startOffset": 44, "endOffset": 59}, {"referenceID": 36, "context": "hand-designed architectures on new problems [5, 20, 39, 45].", "startOffset": 44, "endOffset": 59}, {"referenceID": 42, "context": "hand-designed architectures on new problems [5, 20, 39, 45].", "startOffset": 44, "endOffset": 59}, {"referenceID": 21, "context": "stance, humans write the overall design of a so\u0089ware system, and the parameters and low-level code is optimized automatically [23]; humans write imperfect versions of programs, and evolutionary algorithms are then used to repair them [15]; humans de\u0080ne the space of possible web designs, and evolution is used to \u0080nd e\u0082ective", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "stance, humans write the overall design of a so\u0089ware system, and the parameters and low-level code is optimized automatically [23]; humans write imperfect versions of programs, and evolutionary algorithms are then used to repair them [15]; humans de\u0080ne the space of possible web designs, and evolution is used to \u0080nd e\u0082ective", "startOffset": 234, "endOffset": 238}, {"referenceID": 32, "context": "ones [35].", "startOffset": 5, "endOffset": 9}, {"referenceID": 40, "context": "It is based on the existing neuroevolution technique of NEAT [43], which has been successful in evolving topologies and weights of relatively small recurrent networks in the past.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "Neuroevolution techniques have been applied successfully to sequential decision tasks for three decades [11, 31, 36, 50].", "startOffset": 104, "endOffset": 120}, {"referenceID": 33, "context": "Neuroevolution techniques have been applied successfully to sequential decision tasks for three decades [11, 31, 36, 50].", "startOffset": 104, "endOffset": 120}, {"referenceID": 47, "context": "Neuroevolution techniques have been applied successfully to sequential decision tasks for three decades [11, 31, 36, 50].", "startOffset": 104, "endOffset": 120}, {"referenceID": 22, "context": "CMA-ES [24], a technique for continuous optimization, works well on optimizing the ar X iv :1 70 3.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "Other approaches such as SANE, ESP, and CoSyNE evolve partial neural networks and combine them into fully functional networks [12, 14, 37].", "startOffset": 126, "endOffset": 138}, {"referenceID": 12, "context": "Other approaches such as SANE, ESP, and CoSyNE evolve partial neural networks and combine them into fully functional networks [12, 14, 37].", "startOffset": 126, "endOffset": 138}, {"referenceID": 34, "context": "Other approaches such as SANE, ESP, and CoSyNE evolve partial neural networks and combine them into fully functional networks [12, 14, 37].", "startOffset": 126, "endOffset": 138}, {"referenceID": 16, "context": "Further, techniques such as Cellular Encoding [18] and NEAT [43] have been developed to evolve the topology of the", "startOffset": 46, "endOffset": 50}, {"referenceID": 40, "context": "Further, techniques such as Cellular Encoding [18] and NEAT [43] have been developed to evolve the topology of the", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Much of the work is based on utilizing the Baldwin e\u0082ect, where learning only a\u0082ects the selection [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "encode the learned weight changes back into the genome [18].", "startOffset": 55, "endOffset": 59}, {"referenceID": 37, "context": "Deep neuroevolution is thus more closely related to bilevel (or multilevel) optimization techniques [40].", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "\u008cis is a challenging benchmark from the 2000s for which various reinforcement learning approaches have been developed [1, 2, 38].", "startOffset": 118, "endOffset": 128}, {"referenceID": 1, "context": "\u008cis is a challenging benchmark from the 2000s for which various reinforcement learning approaches have been developed [1, 2, 38].", "startOffset": 118, "endOffset": 128}, {"referenceID": 35, "context": "\u008cis is a challenging benchmark from the 2000s for which various reinforcement learning approaches have been developed [1, 2, 38].", "startOffset": 118, "endOffset": 128}, {"referenceID": 27, "context": "One of the most successful ones is single-level neuroevolution, where the helicopter is controlled by a neural network that is evolved through genetic algorithms [29].", "startOffset": 162, "endOffset": 166}, {"referenceID": 29, "context": "However, a bilevel approach, where a high-level evolutionary process is employed to optimize these parameters, can search this space more e\u0082ectively [32].", "startOffset": 149, "endOffset": 153}, {"referenceID": 30, "context": "[33] used CMA-ES to optimize the hyperparameters of existing DNNs obtaining state-of-the-art results on e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] evolved a CPPN (compositional pa\u008aern-producing network [42]) to output the weights of an auto-encoder neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[10] evolved a CPPN (compositional pa\u008aern-producing network [42]) to output the weights of an auto-encoder neural network.", "startOffset": 60, "endOffset": 64}, {"referenceID": 50, "context": "A related approach was proposed by Zoph and Le [54]: the topology and hyperparameters of a deep network and LSTM network were modi\u0080ed through policy iteration.", "startOffset": 47, "endOffset": 51}, {"referenceID": 40, "context": "NEAT neuroevolution method [43] is \u0080rst extended to evolving", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "are composed of modules that are repeated multiple times [20, 45].", "startOffset": 57, "endOffset": 65}, {"referenceID": 42, "context": "are composed of modules that are repeated multiple times [20, 45].", "startOffset": 57, "endOffset": 65}, {"referenceID": 34, "context": "\u008ce algorithm behind CoDeepNEAT is inspired mainly by Hierarchical SANE [37] but is also in\u0083uenced by component-evolution approaches ESP [13] and CoSyNE [14].", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "\u008ce algorithm behind CoDeepNEAT is inspired mainly by Hierarchical SANE [37] but is also in\u0083uenced by component-evolution approaches ESP [13] and CoSyNE [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "\u008ce algorithm behind CoDeepNEAT is inspired mainly by Hierarchical SANE [37] but is also in\u0083uenced by component-evolution approaches ESP [13] and CoSyNE [14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "Number of Filters [32, 256] Dropout Rate [0, 0.", "startOffset": 18, "endOffset": 27}, {"referenceID": 42, "context": "99] Hue Shi\u0089 [0, 45] Saturation/Value Shi\u0089 [0, 0.", "startOffset": 13, "endOffset": 20}, {"referenceID": 24, "context": "5] Cropped Image Size [26, 32] Spatial Scaling [0, 0.", "startOffset": 22, "endOffset": 30}, {"referenceID": 29, "context": "5] Cropped Image Size [26, 32] Spatial Scaling [0, 0.", "startOffset": 22, "endOffset": 30}, {"referenceID": 38, "context": "[41] in their Bayesian optimization of CNN hyperparameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "LSTMs have recently been shown powerful in supervised sequence processing tasks such as speech recognition [16] and machine translation [3].", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "LSTMs have recently been shown powerful in supervised sequence processing tasks such as speech recognition [16] and machine translation [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53].", "startOffset": 55, "endOffset": 69}, {"referenceID": 6, "context": "variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53].", "startOffset": 55, "endOffset": 69}, {"referenceID": 23, "context": "variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53].", "startOffset": 55, "endOffset": 69}, {"referenceID": 26, "context": "variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53].", "startOffset": 55, "endOffset": 69}, {"referenceID": 24, "context": "variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53].", "startOffset": 136, "endOffset": 147}, {"referenceID": 31, "context": "\u008ce benchmark utilizes the Penn Tree Bank (PTB) dataset [34], which consists of 929k training words, 73k validation words, and 82k test words.", "startOffset": 55, "endOffset": 59}, {"referenceID": 49, "context": "78) as compared to the vanilla LSTM [52].", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": "Evolved networks were trained with the open source MSCOCO image captioning dataset [6], along with a new dataset collected for this website.", "startOffset": 83, "endOffset": 86}, {"referenceID": 25, "context": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51].", "startOffset": 140, "endOffset": 160}, {"referenceID": 43, "context": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51].", "startOffset": 140, "endOffset": 160}, {"referenceID": 44, "context": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51].", "startOffset": 140, "endOffset": 160}, {"referenceID": 46, "context": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51].", "startOffset": 140, "endOffset": 160}, {"referenceID": 48, "context": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51].", "startOffset": 140, "endOffset": 160}, {"referenceID": 42, "context": "As is common in existing approaches, the evolved system uses a pretrained ImageNet model [45] to produce initial image embeddings.", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "As usual, in training the text input contains the previous word of the ground truth caption; in inference it contains the previous word generated by the model [27, 47].", "startOffset": 159, "endOffset": 167}, {"referenceID": 44, "context": "As usual, in training the text input contains the previous word of the ground truth caption; in inference it contains the previous word generated by the model [27, 47].", "startOffset": 159, "endOffset": 167}, {"referenceID": 44, "context": "In particular, the well-known Show and Tell image captioning architecture [47] is in this search space, providing a baseline with which evolution results can be compared.", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "Since there is no single best accepted metric for evaluating captions, the \u0080tness function is the mean across three metrics (BLEU, METEOR, and CIDEr; [6]) normalized by their baseline values.", "startOffset": 150, "endOffset": 153}, {"referenceID": 44, "context": "As a result, there is evolutionary pressure towards networks that converge quickly: \u008ce best resulting architectures train to near convergence 6 times faster than the baseline Show and Tell model [47].", "startOffset": 195, "endOffset": 199}, {"referenceID": 17, "context": "\u0087e motif of skip connections with a summing merge is similar to residual architectures that are currently popular in deep learning [19, 44].", "startOffset": 131, "endOffset": 139}, {"referenceID": 41, "context": "\u0087e motif of skip connections with a summing merge is similar to residual architectures that are currently popular in deep learning [19, 44].", "startOffset": 131, "endOffset": 139}, {"referenceID": 38, "context": "DNGO [41] 26.", "startOffset": 5, "endOffset": 9}, {"referenceID": 44, "context": "Baseline [47] 27.", "startOffset": 9, "endOffset": 13}, {"referenceID": 44, "context": "hand-tuned baseline [47] when trained on the MSCOCO data alone (Table 3).", "startOffset": 20, "endOffset": 24}, {"referenceID": 45, "context": "performing beam search or scheduled sampling during training [48].", "startOffset": 61, "endOffset": 65}], "year": 2017, "abstractText": "\u008ce success of deep learning depends on \u0080nding an architecture to \u0080t the task. As deep learning has scaled up to more challenging tasks, the architectures have become di\u0081cult to design by hand. \u008cis paper proposes an automated method, CoDeepNEAT, for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future.", "creator": "LaTeX with hyperref package"}}}