{"id": "1703.01040", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Learning Robot Activities from First-Person Human Videos Using Convolutional Future Regression", "abstract": "gobena We design timetabling a bont\u00e9 new sea-based approach that 28.41 -1 -0.61954 0.68569 0.023612 0.24307 -0.038125 0.58354 0.19985 -0.39256 0.54723 -0.10944 -0.75553 1.1297 -0.2386 0.067589 -0.14445 -0.46989 -0.5283 0.12718 -0.20152 0.63288 0.044621 -0.65026 -0.15756 0.21606 0.82581 1.2857 -0.37088 0.26956 0.038225 -1.1542 0.22983 -0.224 0.99421 0.072409 -0.20976 -0.47516 0.61431 0.45439 -0.80482 0.93901 0.030153 0.92583 0.37645 -0.44458 0.10815 0.57433 -0.19733 0.86995 -0.6629 allows solok robot learning of revetments new activities disallow from tanusevci unlabeled kivalu human regency example videos. 313,000 Given videos of humans stutters executing the biliverdin same activity from a abriendo human ' s viewpoint (nishimizu i. starless e. , ex-us first - person videos ), 1,867 our objective mithi is tournoi to dampeners make fainted the hoult robot ark. learn polydipsia the 9,620 temporal katheryn structure efqm of ultra-thin the activity as chauffeur its z24 future inner regression deadspin network, and learn condones to 39.45 transfer 19:32 such model bootup for its own motor execution. We cyberangels present a 124.15 new 544 deep bepicolombo learning iste model: We extend unhyphenated the state - of - lynskey the - bramcote art convolutional 872,000 object biazon detection khayal network for bardach the detection sochi of human banned hands limmt in training baronets videos based on image information, infantry and 135.3 newly introduce the concept of tirumalai using sannes a tov fully unani convolutional network ldpr to regress (i. bidart e. , magnetotail predict) wilder the cmf intermediate scene representation devrimci corresponding trembles to the peninsula future stigmatization frame (e. endedness g. , 1 - 2 seconds later ). rcds Combining man these allows giggles direct felbrigg prediction of humanae future 351,000 locations glutamatergic of 2,659 human kanggye hands powderkeg and objects, which enables brownhill the robot wing-back to maisonettes infer khani the unbundled motor control plan huffpost using praenomen our kohona manipulation network. We eyaktek experimentally moradi confirm civis that our surficial approach sannikov makes learning fluviatile of robot fiesta activities llanwern from unlabeled serbian-american human bimetallic interaction videos juicy possible, and demonstrate petia that our robot pinghu is able 22.02 to self-centred execute kadhmiyah the s-200 learned monsanto collaborative pushforward activities wbai in real - double-deckers time directly ludhianvi based veldhoven on its non-identity camera hoftheater input.", "histories": [["v1", "Fri, 3 Mar 2017 05:27:50 GMT  (3229kb,D)", "https://arxiv.org/abs/1703.01040v1", null], ["v2", "Mon, 24 Jul 2017 08:02:11 GMT  (3108kb,D)", "http://arxiv.org/abs/1703.01040v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV cs.LG", "authors": ["jangwon lee", "michael s ryoo"], "accepted": false, "id": "1703.01040"}, "pdf": {"name": "1703.01040.pdf", "metadata": {"source": "CRF", "title": "Learning Robot Activities from First-Person Human Videos Using Convolutional Future Regression", "authors": ["Jangwon Lee", "Michael S. Ryoo"], "emails": ["mryoo}@indiana.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nOne of the important abilities of humans (and animals) is that they are able to learn new activities and their motor controls from others\u2019 behaviors. When a person watches others performing an activity, he/she not only learns to visually predict future consequences of the motion during the activity but also learns how to execute the activity himself/herself.\nRecently, approaches taking advantage of \u201cdeep learning\u201d for robot manipulation have been gaining an increasing amount of attention, directly learning motor control policies given visual inputs (i.e., images and videos) [1]. The use of convolutional neural networks (CNNs) have been particularly successful, since they are able to jointly learn image features optimized for the task based on their training data. Because of such ability, new models incorporating convolutional and recurrent neural networks (i.e., CNNs and RNNs) is likely to become a major trend in robotics, just like what already happened in computer vision and machine learning.\nHowever, although these deep learning oriented approaches showed very promising results on learning video prediction [2] and actual motor control policy [1], they have been limited to relatively simple actions such as object grasping and pushing. This is because a large amount of \u2018robot\u2019 data is necessary for the direct training of these CNNs and RNNs with millions of parameters. A large number of samples of humans (or the robot itself) motor controlling the robot is necessary for generating training data [1], and this is\nSchool of Informatics and Computing, Indiana University, Bloomington, IN 47408, USA. {leejang, mryoo}@indiana.edu\na limiting aspect particularly when we want to teach a robot new (i.e., previously unseen) activities.\nIn this paper, we present a new CNN-based approach that enables robot learning of its activities from \u2018human\u2019 example videos. Human activity videos can be attractive training resources because it does not require any hardware or professional software for teaching robots, even though it might create other difficulties like transferring learned human-based models to the actual robots. Given videos of humans executing the same activity from a human\u2019s viewpoint (i.e., first-person videos), our objective is to make the robot learn the temporal structure of the activity as its future regression network, and learn to transfer such model for its own motor execution. The idea is that a human\u2019s firstperson video and the video a humanoid robot is expected to obtain during its activity execution should be very similar. Providing first-person human videos to the robot is as if we are providing the robot \u2018visual memory\u2019 of itself performing the activities previously. This enables the robot to directly learn what visual observation it is expected to see during the correct execution of the activity and how it will change from its viewpoint.\nThere have been previous works on robot activity learning from human videos [3], [4], extending the previous concept of \u2018robot learning from demonstration\u2019 [5] which was mostly done with direct motor control data. However, these works focused on learning grammar representations of human activities, modeling human activities as a sequence of atomic actions (e.g., grasping). These approaches were limited in the aspect that activities were always represented in terms of pre-defined set of atomic actions, and the users had to teach the robot how to recognize those atomic actions from human activity videos by providing labeled training data (i.e., supervised learning). This prevented the robot learning of activities from scratch, and was also limited in that human had to define new atomic actions when a new activity is added. Furthermore, since it was not trainable in an end-toend fashion, the robot has to somehow figure out how to execute those atomic actions, which was usually done by hand-coding the motion.\nWe introduce a new robot activity learning model using a fully convolutional network for future representation regression. We extend the state-of-the-art convolutional object detection network (SSD [6]) for the representation of human hand-object information in a video frame, and newly introduce the concept of using a fully convolutional network to regress (i.e., predict) how such intermediate scene representation will change in the future frame (e.g., 1-2 seconds\nar X\niv :1\n70 3.\n01 04\n0v 2\n[ cs\n.R O\n] 2\n4 Ju\nl 2 01\n7\nlater). Combining these allows direct and explicit prediction of future hand locations (Figure 1), which then allows the robot to infer the motor control plan. That is, not only feature-level prediction of future representations (similar to [7]) but also semantic-level prediction of explicit future hand locations of humans and robots during the learned activity is being jointly performed in our new network. Such future hand prediction results are used by our manipulation network that learns mapping of the 2-D hand locations in the image coordinate to the actual motor control.\nOur activity learning is an unsupervised approach in the aspect that it does not require activity labels or hand/object labels in the activity videos. It does require hand-annotated training data for the learning of its hand representation network, but there already exists public datasets for this purpose and it does not require any labels for its future regression network. The future regression network is learned without supervision by capturing changes in our hand-based representations in the training videos. In addition, importantly, all our networks were designed to function in real-time for the actual robot operation, and we show such capability with our experiments in this paper."}, {"heading": "II. RELATED WORK", "text": "a) Robot learning from humans: There have been a considerable amount of previous efforts on robot learning\nfrom demonstration (LfD) [5], [8], [9]. Since it enables robots automatically learn a new task from demonstration by non-robotics expert, LfD is very important in robotics. However, there are limitations since most of these approaches focused on making robots learn motor control polices from human data, which usually was in the form of direct control sequences obtained with actual robots or simulation softwares [10]. Moreover, it often requires a knowledge about all primitive actions for teaching high-level tasks [11].\nThere also have been previous works on robot activity learning from visual data [3], [4], [12], extending the previous concept of LfD. These works focused on learning grammar representations of human activities from conventional third-person videos (i.e., videos usually taken with static cameras watching the actors), modeling human activities as a sequence of atomic actions (e.g., grasping). Having a grammar representation composed of atomic actions allows transfer of human activity structure to robots, and the robot replication of human activities was possible usually with hand-coded motion transfer from human atomic actions to robot atomic actions. However, activity learning was generally done in a fully supervised fashion with human annotations in these approaches, and they assumed very reliable estimation of semantic features from videos such human hands and human body skeletons. [13] studied an approach to directly learn object manipulation trajectories\nfrom human videos, but it was limited to one-robot-oneobject scenarios unlike our approach focusing on very general human-robot collaboration scenarios (e.g., humanobject-robot interactions).\nb) Video prediction: Our approach in this paper is to generate proper robot behaviors (particularly for humanrobot collaboration) by predicting \u2018future\u2019 visual representation. The idea is that such representation leads to the estimation of future positions of objects and hands of humans and robots. Visual prediction is one of the core components of our perception system.\nThere have been previous works on the prediction of future frames from the computer vision community [7], [14], [15]. However, there has been very limited attempt on applying such future predictions for robotics systems, since these approaches in general requires more components for interpreting predicted representation to generate robot actions. In the above works, no robot manipulation was actually attempted. There exists a recent robotics work that attempted applying visual prediction for generating robot control actions [16]. This study shows the potential in applying visual prediction for a robotic manipulation task; it enables transferring the visual perception to robot manipulation component for generating motor control commands without any additional components to interpret the recognition results. However, this requires a huge amount of training data using actual physical robots to make the robot learn activities, and thus is limited when the robot needs to learn many new activities.\nc) First-person videos: First-person videos, also called egocentric videos, are the videos taken from the actor\u2019s own viewpoint. Recognition of human/robot activities from such first-person videos has been actively studied particularly in the past 5 years, including recognition of human actions from wearable cameras [17]\u2013[20] and human-robot interactions from robot cameras [21], [22]. However, these focused on building discriminative video classifiers, and the attempt to learn \u2018executable\u2019 representations of human activities or their transfer to robots have been very limited.\nThe main contribution of this paper is in enabling robot activity learning from human interaction videos using our newly proposed convolutional future regression. We believe this is the first work to present a deep learning-based (i.e., entirely CNN-based) method for learning human-robot interactions from human-human videos. We also believe this is the first paper to take advantage of human \u2018first-person videos\u2019 for the robot activity learning."}, {"heading": "III. APPROACH", "text": ""}, {"heading": "A. System Overview", "text": "Given a sequence of current frames, our goal is to (i) predict future hand locations and all interactive objects in front of the robot, then to (ii) generate robot control commands for moving robot\u2019s hands to the predicted hand locations. We employ two components for achieving the goal. The first component is a perception component that consists of two fully convolutional neural networks: (1) an extended version of the Single Shot MultiBox Detector (SSD) [6] to create\na hand-based scene representation and estimate bounding boxes, and (2) a future regression network to model how such intermediate scene representation (should) change in future frames. The second component is a manipulation component that maps 2-D hand locations in the image coordinate to the actual motor control using fully connected layers.\nThe key idea of our approach is that the proposed perception component allows prediction of future (1-2 seconds later) hand locations given current video input from a camera. Such future prediction can be learned based on humans\u2019 firstperson activity videos by using them as training data, with the assumption that the robot camera has a similar viewpoint with the human first-person videos. This allows the robot to directly predict its ideal future hand locations during the activity, inferring how the hand should move if the activity were to be executed successfully. Next, the manipulation component generates actual robot control commands to move the robot\u2019s hands to the predicted future locations."}, {"heading": "B. Perception Component", "text": "Given a video frame X\u0302t at time t, the goal of our perception component is to predict the future hand locations Y\u0302t+\u2206.\na) Hand Representation Network: We first construct a network for the hand-based representation of the image scene by extending the SSD object detection framework. We extended it by inserting a fully convolutional auto-encoder having five convolutional layers followed by five deconvolutional layers for dimensionality reduction. This allows the approach to abstract an image (with hands and objects) into a lower dimensional intermediate representation.\nAll our convolutional/deconvolutional layers use 5\u00d75 kernels and the number of filters for each convolutional layer are: 512, 256, 128, 64, 256. The green convolutional layers in Fig. 1 correspond to them. After such convolutional layers, there are deconvolutional layers (yellow layers in Fig. 1), each having the symmetric number of filters: 256, 64, 128, 256, 512. We do not use any pooling layer, and instead use stride 2 for the last convolutional layer for the dimensionality reduction. We thus increase the number of filters for the last convolutional layer to compensate loss of information.\nLet f denote the hand representation network given an image at time t. Then, this network can be considered as a combination of two sub functions, f = g \u25e6 h:\nY\u0302t = f(X\u0302t) = h(F\u0302t) = h(g(X\u0302t)), (1)\nwhere a function g : X\u0302 \u2192 F\u0302 denotes a feature extractor (from an input video frame to encoder) to get compressed intermediate visual representation (i.e., feature map) F\u0302, and h : F\u0302 \u2192 Y\u0302 indicates a box estimator which uses the compressed representation as an input for locating hand boxes at time t. With the above formulation, the network can predict hand locations Y\u0302t at time t after the training.\nb) Future Regression Network: Although the above hand representation network allows obtaining hand boxes in the \u2018current\u2019 frame, our objective is to get the \u2018future\u2019 hand locations Y\u0302t+\u2206 instead of theirs current locations Y\u0302t."}, {"heading": "Hand representa,on network: t", "text": ""}, {"heading": "Hand representa,on network: t+\u0394", "text": "We formulate this problem as a regression problem. The main idea is that the intermediate representation of the hand representation network F\u0302t abstracts the hand-object information in the scene, and that we are able to take advantage of it to infer the future (intermediate) representation F\u0302t+\u2206. Once such regression becomes possible, we can simply plug-in the predicted future representation F\u0302t+\u2206 to the remaining part of the hand network (i.e., h) to obtain the final future hand prediction results. Therefore, we newly design a network for predicting the intermediate scene representation corresponding to the future frame F\u0302t+\u2206, as a fully convolutional future regression network: Fig. 2.\nGiven a current scene representation F\u0302t from the hand network, our future regression network (r) predicts the future the intermediate scene representation F\u0302t+\u2206:\nF\u0302t+\u2206 = rw(F\u0302t). (2)\nIt has seven convolutional layers having 256 5\u00d75 kernels. In addition, it has a layer with 1024 13\u00d713 kernels followed by the last layer that has 256 1\u00d71 kernel. We trained the weights (w) of the regression network with unlabeled first-person human activity videos using the following loss function:\nw\u2217 = arg min w \u2211 i,t \u2016rw(F\u0302it)\u2212 F\u0302it+\u2206\u201622\n= arg min w \u2211 i,t \u2016rw(g(X\u0302it))\u2212 F\u0302it+\u2206\u201622 (3)\nwhere X\u0302it indicates a video frame at time t from video i, and F\u0302it represents a feature map at time t from video i.\nOur future regression network can use any intermediate scene representation from any intermediate layers of the hand network, but we use the one from auto-encoder due to its lower dimensionality. Finally, the future scene representation F\u0302t+\u2206 is fed into the hand network for estimating hand boxes corresponding to the future frame to get future hand locations Y\u0302t+\u2206.\nY\u0302t+\u2206 = h(F\u0302t+\u2206) (4)\nFig. 2 summarizes data flow of our perception component during testing phase. Given a video frame X\u0302t at time t, (1) we extract the intermediate scene representation F\u0302t using the feature extractor (g), and then (2) feed it into the future regression network (r) to get future scene representation F\u0302t+\u2206. Next, (3) we feed F\u0302t+\u2206 into the box estimator (h), and finally obtain future position of hands Y\u0302t+\u2206 at time t.\nY\u0302t+\u2206 = h(F\u0302t+\u2206) = h(r(F\u0302t)) = h(r(g(X\u0302t))) (5)\nFurthermore, instead of using just a single frame (i.e., the current frame) for the future regression, we extend our network to take advantage of the previous K frames to obtain F\u0302t+\u2206 as illustrated in Fig. 1:\nY\u0302t+\u2206 = h(r([g(X\u0302t), ..., g(X\u0302t\u2212(K\u22121))])). (6)\nThe advantage of our formulation is that it allows us to predict future hand locations while considering the implicit activity and object context, even without explicit detection of objects in the scene. Our auto-encoder-based intermediate representation F\u0302it abstracts the scene configuration by internally representing what objects/hands are currently in the scene and where they are, and our fully convolutional future regressor takes advantage of it for the prediction."}, {"heading": "C. Manipulation Component", "text": "Although our perception component is able to predict future hand locations of humans in first-person human activity videos, it is insufficient for the robot manipulation. Here, we construct another regression network (m) for mapping the predicted 2-D human hand locations in the image coordinate to the actual motor control commands. The main assumption is that a video frame from a robot\u2019s camera will have a similar viewpoint to our training data (first-person human videos), allowing us to take advantage of the learned model for the robot future hand prediction by assuming:\nY\u0302Rt ' Y\u0302t (7)\nwhere, Y\u0302Rt represents robot hand locations. Our manipulation component (m) predicts future robot joint states (Z\u0302t+\u2206) given current robot joint states (Z\u0302t), robot hand locations (Y\u0302Rt), and future hand locations (Y\u0302Rt+\u2206) telling where the robot\u2019s hands should move to. This network can be formulated with the below function:\nZ\u0302t+\u2206 = m\u03b8(Z\u0302t, Y\u0302Rt, Y\u0302Rt+\u2206). (8)\nOur manipulation component consists of seven fully connected layers having the following number of hidden units for each layer: 32, 32, 32, 16, 16, 16, 7. The weights (\u03b8) of this network can be obtained by the same way that used for our perception networks:\n\u03b8\u2217 = arg min \u03b8 \u2211 j,t \u2016m\u03b8(Z\u0302jt , Y\u0302 j Rt, Y\u0302 j Rt+\u2206)\u2212 Z\u0302 j t+\u2206\u2016 2 2 (9)\nwhere Z\u0302jt indicates robot joint states at time t from training episode j, and Y\u0302jRt represents robot hand locations at time t from training episode j. Fig. 3 shows our manipulation component for generating robot control commands.\nThe combination of our perception component and manipulation component provides a real-time robotics system that takes raw video frames as its input and generates motor control commands for its activity execution. Our manipulation component can be replaced with a standard Inverse Kinematics, but our neural network-based model generates more natural arm movements by considering the desired location of the robot\u2019s end-effectors as well as joint configuration sequences (i.e., unlabeled robot logs described in the next section)."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. Datasets", "text": "Our approach consists of three different types of networks (within the two components), and we use three different types of datasets for training each model.\nEgoHands [23]: This is a public dataset containing 48 first-person videos of people interacting in four types of activities (playing cards, playing chess, solving a puzzle, and playing Jenga). It has 4,800 frames with 15,053 ground-truth hand labels. Here, we added 466 frames with 1,267 groundtruth annotations to the original dataset to cover more hand postures. We use this dataset to learn our hand representation network, which is trained to locate hand boxes in a video frame.\nUnlabeled Human-Human Interaction Videos: We collected a total of 47 first-person videos of human-human collaboration scenarios, with each video clip ranging from 4 to 10 seconds. This dataset is a main dataset for teaching a new task to our robot. It contains two types of tasks: (1) a person wearing the camera cleaning up all objects on a table as a partner (i.e., the other subject) approaches the table while holding a heavy box (to make a room for her/him to put the heavy box on the table), and (2) a person wearing the camera pushing a trivet on a table toward to a partner when he/she is approaching the table while holding a hot cooking pan. These videos are unlabeled videos without any activity/hand annotation and we trained our convolutional regression network using this dataset.\nUnlabeled Robot Activity Log Files: We prepared this dataset to train our robot manipulation network. It contains 50 robot log files. Each log has the robot\u2019s hand positions\n(u, v) in an image plane and the robot\u2019s corresponding joint angles at time t. We recorded these log files by making a human operator move the robot arms (i.e., the human grabbed the robot arms and moved them). We obtained such robot joint configuration sequences while moving the robot to cover possible arm motion during general human-robot interaction tasks. Here, we assume that the robot is supposed to operate in a similar environment during the test phase. Note that this was not recorded under the interaction scenario (i.e., just the robot itself was moving), and no annotation regarding the activity or motion was provided. We used a Baxter research robot for recording these files and the Baxter has seven degrees-of-freedom arm: the file contains 9 variables for each arm. In order to estimate the robot\u2019s hand position in the image plane, we projected the 3-D positions of the Baxter\u2019s grippers into the image plane (based on camera calibration) and recorded the projected (u, v) positions with 7 joint angles at 30 Hz."}, {"heading": "B. Baselines", "text": "In order to provide quantitative comparisons, we compared our perception component with four different baselines: (i) Hand-crafted representation uses a hand-crafted state representation based on explicit object and hand detection. It encodes relative distances between all interactive objects in our two scenarios, and uses it to predict the future hand location using neural network-based regression. More specifically, it detects objects using KAZE features [24] and hands using CNN based hand detector in [23], then computes relative distances between all objects and hands for building the state representation which is a 20 dimensional vector. Then, we built a new network which has five fully connected layers trained using the state representations on the same interaction dataset we use. (ii) Hands only uses hand locations for the future regression. It predicts future hand locations solely based on current hand locations without considering any other visual representations. In order to train this baseline model, we extracted hand locations from all frames of the interaction videos using our hand representation network, then made log files to store detected hand locations in each frame and their frame numbers. After this, we trained another neural network model for the future"}, {"heading": "EVALUATION OF FUTURE HAND PREDICTION", "text": "hand location prediction using the log files, which has seven fully connected layers with the same number of hidden units as our robot manipulation network. (iii) SSD with future annotations1 is a baseline that uses the original SSD model [6] trained based on EgoHands dataset. Instead of training the model to infer the current hand locations given the input frame, we fine-tuned this model on EgoHands dataset after changing annotations of the dataset to have \u201cfuture\u201d locations of hands instead of making it to use current hand locations. We also used additionally 466 frames for this finetuning since the original EgoHands dataset was insufficient (too many repetitive hand movements) for this training. (iv) SSD with future annotations2 is a baseline also using the original SSD model, but we trained this model from scratch. This time we changed all annotations of the EgoHands dataset, then trained the model. After that we fine-tuned the model as the same way that used for the \u201cSSD with future annotations1\u201d baseline."}, {"heading": "C. Evaluation of our future hand prediction", "text": "We first evaluated the perception component of our approach in terms of precision, recall, and F-measure, and compared them against the above baselines. In the first evaluation, we made our approach to predict bounding boxes of human hands in the future frame given the current image frame. We measured the \u201cintersection over union\u201d ratio between areas of each predicted box and ground truth (future) hand locations. Only when the ratio was greater than 0.5, the predicted box was accepted as a true positive. In this experiment, we randomly split the set of our Human-Human Interaction Videos into the training and testing sets, so 32 videos were used for training sets and remaining 15 videos were used for testing sets in a total of 47 videos.\nTable I shows quantitative results of our future hand prediction. Here, the plus-minus sign (\u00b1) indicates standard deviation and K represents number of frames we used as an input for our regression network. Our \u2206 was 30 frames (i.e., 1 sec). We are able to clearly observe that our approach significantly outperforms all the baselines, including the state-of-the-art object detector SSD modified for the hand prediction. Our proposed network with K = 10 yielded the best performance in terms of all three metrics, at about 30.9 score in F-measure. The best performance we can get with SSD was only 13.23.\nIn our second evaluation, we measured mean pixel distance between ground truth locations and the predicted posi-"}, {"heading": "MEAN PIXEL DISTANCE BETWEEN GROUND TRUTH AND PREDICTED POSITIONS OF ALL HANDS", "text": ""}, {"heading": "MEAN PIXEL DISTANCE BETWEEN GROUND TRUTH AND PREDICTED POSITION OF RIGHT HAND", "text": "tions of hands. The size of the image plane was 1280*720. We measured this mean pixel distance only when both the ground truths and the predictions are present in the same frame. Table II shows the mean pixel distance errors for all four types of hands (my left, my right, your left, and your right). Once more, we can confirm that our approaches greatly outperform the performance of all the baselines. The overall average distance was a bit high due to changes in human hand shapes and their variations, but they were sufficient in terms of generating robot motion.\nWe also compared accuracies of these methods while only considering my right hand predictions, since position of my right hand is more important for a robot manipulation than locations of other types of hands. This is because, in our test scenarios, the robot\u2019s activities are very focused on its right hand motion. Table III shows mean pixel distance between ground truth and predicted position of \u2018my right hand\u2019. We can see that performances of our approaches are superior to all the baselines. Examples of our visual predictions results are illustrated in Fig. 4."}, {"heading": "D. Real-time robot experiments", "text": "Finally, we conducted a user study to evaluate the success level of robot activities performed based on our proposed approach, with human subjects. A total of 12 participants (5 undergraduate and 7 graduate students) were recruited from the campus, and were asked to perform one of the two activities (clearing the table for a partner and preparing a trivet for a cooking pan) together with our robot. After such interactions, the participants were asked to complete a questionnaire about the robot behaviors for each task. The questionnaire had two statements (one statement for each activity) with scales from 1 (totally do not agree) to 5 (totally agree) to express their impression on the robot behaviors: \u201cI\nthink the robot cleared the table to make a space for me.\u201d for the task 1 and \u201cI think the robot passed a trivet closer to me so that I can put the cooking pan on it.\u201d for the task 2.\nIn addition to our approach (i.e., our perception component + manipulation component), we designed and implemented the following three baselines and compared their quantitative results: (i) Base SSD + Base control uses the baseline SSD with future annotations1 as a perception component and the base manipulation network trained using the same robot activity log files. This base control network direct maps current hand locations in the image plane to current seven joint angles for each robot arm, without the Z\u0302t term in Eq. 8. (ii) Base SSD + Our control uses SSD with future annotations1 as a perception component and our manipulation component (from Section III-C) to generate motor commands. (iii) Our perception + Base control used our perception component to predict future hand locations and the base control network for manipulation. In all these cases, the final control of our robot arm is performed by taking advantage of the Baxter API by providing the estimated future joint angle configuration.\nAs a result, each participant interacted with the robot total of 8 times in a random order. Table IV shows the results. The results indicate that our participants evaluated the robot with our approach performed better on both tasks. We received a higher average score of 3.29 compared to all the baselines (1.72, 1.92, and 2.29) from the participants. Examples of"}, {"heading": "THE SUCCESS LEVEL OF OUR HUMAN-ROBOT COLLABORATION", "text": "our real-time robot experiments with human subjects are illustrated in Fig. 5.\nOur method operates in slow real-time with our unoptimized C++ code. It takes \u223c100 ms per frame using one Nvidia Pascal Titan X GPU, and we were able to conduct real-time human-robot collaboration experiments using it."}, {"heading": "V. CONCLUSION", "text": "In this paper, we proposed a new robot activity learning model using a fully convolutional network for future representation regression. The main idea was to make the robot learn the temporal structure of a human activity as its future regression network, and learn to transfer such model for its own motor execution using our manipulation network. We show that our approach enables the robot to infer the motor control commands based on the prediction of future human hand locations in real-time. The experimental results confirm that our approach not only predicts the future locations of human/robot hands more reliably, but also is able to make\nrobots execute the activities based on predictions. The paper focuses on robot learning of location-based hand movements (i.e., translations and natural rotations), and handling more dynamic hand posture changes remains as one of our future challenges.\nAcknowledgement: This work was supported by the Army Research Laboratory under Cooperative Agreement Number W911NF-10-2-0016."}], "references": [{"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": "Advances In Neural Information Processing Systems (NIPS), 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "A syntactic approach to robot imitation learning using probabilistic activity grammars", "author": ["K. Lee", "Y. Su", "T.-K. Kim", "Y. Demiris"], "venue": "Robotics and Autonomous Systems, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Robot learning manipulation action plans by\u201d watching\u201d unconstrained videos from the world wide web.", "author": ["Y. Yang", "Y. Li", "C. Ferm\u00fcller", "Y. Aloimonos"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and Autonomous Systems, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "SSD: Single shot multibox detector", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C. Fu", "A. Berg"], "venue": "European Conference on Computer Vision (ECCV), 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Anticipating visual representations with unlabeled video", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Robot programming by demonstration", "author": ["A. Billard", "S. Calinon", "R. Dillmann", "S. Schaal"], "venue": "Springer handbook of robotics, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning dexterous manipulation for a soft robotic hand from human demonstrations", "author": ["A. Gupta", "C. Eppner", "S. Levine", "P. Abbeel"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning about objects with human teachers", "author": ["A.L. Thomaz", "M. Cakmak"], "venue": "ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning to select and generalize striking movements in robot table tennis", "author": ["K. M\u00fclling", "J. Kober", "O. Kroemer", "J. Peters"], "venue": "The International Journal of Robotics Research, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning social affordance for human-robot interaction", "author": ["T. Shu", "M.S. Ryoo", "S.-C. Zhu"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI), 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Physically-grounded spatio-temporal object affordances", "author": ["H. Koppula", "A. Saxena"], "venue": "European Conference on Computer Vision (ECCV), 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Patch to the future: Unsupervised visual prediction", "author": ["J. Walker", "A. Gupta", "M. Hebert"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep predictive coding networks for video prediction and unsupervised learning", "author": ["W. Lotter", "G. Kreiman", "D. Cox"], "venue": "arXiv preprint arXiv:1605.08104, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual foresight for planning robot motion", "author": ["C. Finn", "S. Levine"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Fast unsupervised ego-action learning for first-person sports videos", "author": ["K.M. Kitani", "T. Okabe", "Y. Sato", "A. Sugimoto"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding egocentric activities", "author": ["A. Fathi", "A. Farhadi", "J.M. Rehg"], "venue": "International Conference on Computer Vision (ICCV), 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Detecting activities of daily living in first-person camera views", "author": ["H. Pirsiavash", "D. Ramanan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Pooled motion features for first-person videos", "author": ["M.S. Ryoo", "B. Rothrock", "L. Matthies"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Robot-centric activity prediction from first-person videos: What will they do to me?", "author": ["M.S. Ryoo", "T.J. Fuchs", "L. Xia", "J.K. Aggarwal", "L. Matthies"], "venue": "in ACM/IEEE International Conference on Human- Robot Interaction (HRI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Multi-type activity recognition in robot-centric scenarios", "author": ["I. Gori", "J.K. Aggarwal", "L. Matthies", "M.S. Ryoo"], "venue": "IEEE Robotics and Automation Letters (RA-L), 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions", "author": ["S. Bambach", "S. Lee", "D.J. Crandall", "C. Yu"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Kaze features", "author": ["P.F. Alcantarilla", "A. Bartoli", "A.J. Davison"], "venue": "European Conference on Computer Vision (ECCV), 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": ", images and videos) [1].", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "proaches showed very promising results on learning video prediction [2] and actual motor control policy [1], they have been limited to relatively simple actions such as object grasping and pushing.", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "proaches showed very promising results on learning video prediction [2] and actual motor control policy [1], they have been limited to relatively simple actions such as object grasping and pushing.", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "samples of humans (or the robot itself) motor controlling the robot is necessary for generating training data [1], and this is", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "There have been previous works on robot activity learning from human videos [3], [4], extending the previous concept of \u2018robot learning from demonstration\u2019 [5] which was mostly done with direct motor control data.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "There have been previous works on robot activity learning from human videos [3], [4], extending the previous concept of \u2018robot learning from demonstration\u2019 [5] which was mostly done with direct motor control data.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "There have been previous works on robot activity learning from human videos [3], [4], extending the previous concept of \u2018robot learning from demonstration\u2019 [5] which was mostly done with direct motor control data.", "startOffset": 156, "endOffset": 159}, {"referenceID": 5, "context": "detection network (SSD [6]) for the representation of human hand-object information in a video frame, and newly introduce the concept of using a fully convolutional network to regress (i.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "Overview of our perception component: Our perception component consists of two fully convolutional neural networks: The first network is an extended version of the state-of-the-art convolutional object detection network (SSD [6]) for the representation of human hands and estimation of the bounding boxes (top).", "startOffset": 225, "endOffset": 228}, {"referenceID": 6, "context": "That is, not only feature-level prediction of future representations (similar to [7]) but also semantic-level prediction of explicit future hand locations of humans and robots during the learned activity is being jointly performed in our new network.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "a) Robot learning from humans: There have been a considerable amount of previous efforts on robot learning from demonstration (LfD) [5], [8], [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 7, "context": "a) Robot learning from humans: There have been a considerable amount of previous efforts on robot learning from demonstration (LfD) [5], [8], [9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 8, "context": "a) Robot learning from humans: There have been a considerable amount of previous efforts on robot learning from demonstration (LfD) [5], [8], [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 9, "context": "However, there are limitations since most of these approaches focused on making robots learn motor control polices from human data, which usually was in the form of direct control sequences obtained with actual robots or simulation softwares [10].", "startOffset": 242, "endOffset": 246}, {"referenceID": 10, "context": "Moreover, it often requires a knowledge about all primitive actions for teaching high-level tasks [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "There also have been previous works on robot activity learning from visual data [3], [4], [12], extending the previ-", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "There also have been previous works on robot activity learning from visual data [3], [4], [12], extending the previ-", "startOffset": 85, "endOffset": 88}, {"referenceID": 11, "context": "There also have been previous works on robot activity learning from visual data [3], [4], [12], extending the previ-", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "[13] studied an approach to directly learn object manipulation trajectories", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "There have been previous works on the prediction of future frames from the computer vision community [7], [14], [15].", "startOffset": 101, "endOffset": 104}, {"referenceID": 13, "context": "There have been previous works on the prediction of future frames from the computer vision community [7], [14], [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "There have been previous works on the prediction of future frames from the computer vision community [7], [14], [15].", "startOffset": 112, "endOffset": 116}, {"referenceID": 15, "context": "There exists a recent robotics work that attempted applying visual prediction for generating robot control actions [16].", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "Recognition of human/robot activities from such first-person videos has been actively studied particularly in the past 5 years, including recognition of human actions from wearable cameras [17]\u2013[20] and human-robot interactions from robot cameras [21], [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 19, "context": "Recognition of human/robot activities from such first-person videos has been actively studied particularly in the past 5 years, including recognition of human actions from wearable cameras [17]\u2013[20] and human-robot interactions from robot cameras [21], [22].", "startOffset": 194, "endOffset": 198}, {"referenceID": 20, "context": "Recognition of human/robot activities from such first-person videos has been actively studied particularly in the past 5 years, including recognition of human actions from wearable cameras [17]\u2013[20] and human-robot interactions from robot cameras [21], [22].", "startOffset": 247, "endOffset": 251}, {"referenceID": 21, "context": "Recognition of human/robot activities from such first-person videos has been actively studied particularly in the past 5 years, including recognition of human actions from wearable cameras [17]\u2013[20] and human-robot interactions from robot cameras [21], [22].", "startOffset": 253, "endOffset": 257}, {"referenceID": 5, "context": "The first component is a perception component that consists of two fully convolutional neural networks: (1) an extended version of the Single Shot MultiBox Detector (SSD) [6] to create a hand-based scene representation and estimate bounding boxes, and (2) a future regression network to model how such intermediate scene representation (should) change in future frames.", "startOffset": 171, "endOffset": 174}, {"referenceID": 22, "context": "EgoHands [23]: This is a public dataset containing 48 first-person videos of people interacting in four types of activities (playing cards, playing chess, solving a puzzle, and playing Jenga).", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "More specifically, it detects objects using KAZE features [24] and hands using CNN based hand detector in [23], then computes relative distances between all objects and hands for building the state representation which is a 20 dimensional vector.", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "More specifically, it detects objects using KAZE features [24] and hands using CNN based hand detector in [23], then computes relative distances between all objects and hands for building the state representation which is a 20 dimensional vector.", "startOffset": 106, "endOffset": 110}, {"referenceID": 5, "context": "(iii) SSD with future annotations is a baseline that uses the original SSD model [6] trained based on EgoHands dataset.", "startOffset": 81, "endOffset": 84}], "year": 2017, "abstractText": "We design a new approach that allows robot learning of new activities from unlabeled human example videos. Given videos of humans executing the same activity from a human\u2019s viewpoint (i.e., first-person videos), our objective is to make the robot learn the temporal structure of the activity as its future regression network, and learn to transfer such model for its own motor execution. We present a new deep learning model: We extend the state-of-the-art convolutional object detection network for the representation/estimation of human hands in training videos, and newly introduce the concept of using a fully convolutional network to regress (i.e., predict) the intermediate scene representation corresponding to the future frame (e.g., 1-2 seconds later). Combining these allows direct prediction of future locations of human hands and objects, which enables the robot to infer the motor control plan using our manipulation network. We experimentally confirm that our approach makes learning of robot activities from unlabeled human interaction videos possible, and demonstrate that our robot is able to execute the learned collaborative activities in real-time directly based on its camera input.", "creator": "LaTeX with hyperref package"}}}