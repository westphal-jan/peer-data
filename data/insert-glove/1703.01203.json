{"id": "1703.01203", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Stochastic Separation Theorems", "abstract": "amarendra A groaners set $ otcbb S $ korie is linearly 28-9 separable ecclesiological if 103d each $ ying-jeou x \\ in kdam S $ k\u00fcrten can 1,495 be calker separated mihai from the kitovani rest preprocessing of $ diatribes S $ by jehn a shoaib linear seabaugh functional. crocs We drosophila study 4-string random $ self-governing N $ - ci611 element zayn sets \u00f8stfold in $ \\ caronia mathbb {R} ^ bbc4 n $ for aerodrom large $ n $ novation and kurtley demonstrate endiama that motomachi for $ clawfoot N & kaleh lt; a \\ exp (bilonick b {ballarpur n} ) $ unanderra they are 79.39 linearly kuykendall separable muentefering with recognisable probability $ p $, $ enckelman p & tedo gt; jannaeus 1 - \\ carmax vartheta $, anisotropies for a gide given (small) $ \\ intellectualize vartheta & gt; 0 $. Constants $ a, antiharassment b & fastback gt; diek 0 $ deben depend moses on metra the krasnow probability handy distribution masvingo and michon the constant $ \\ vartheta $. pl The lunke results thamby are important carnie for 101.78 machine pomodoro learning martinello in bajevi\u0107 high inazuma dimension, 41.85 especially teamster for leclaire correction 1.5370 of xigui unavoidable marka mistakes skdl of couey legacy 7/16 Artificial shahroudi Intelligence systems.", "histories": [["v1", "Fri, 3 Mar 2017 15:27:38 GMT  (390kb,D)", "https://arxiv.org/abs/1703.01203v1", null], ["v2", "Sat, 1 Apr 2017 16:47:51 GMT  (97kb)", "http://arxiv.org/abs/1703.01203v2", "5 pages, added numerical experiments, extended bibliography, corrected misprints"], ["v3", "Thu, 3 Aug 2017 17:37:06 GMT  (97kb)", "http://arxiv.org/abs/1703.01203v3", "6 pages, accepted for publication in Neural Networks (Letter section)"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["a n gorban", "i y tyukin"], "accepted": false, "id": "1703.01203"}, "pdf": {"name": "1703.01203.pdf", "metadata": {"source": "CRF", "title": "Stochastic Separation Theorems", "authors": ["A.N. Gorbana", "I.Y. Tyukin"], "emails": ["ag153@le.ac.uk", "it37@le.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n01 20\n3v 3\n[ cs\n.L G\n] 3\nA ug\n2 01\n7\nThe problem of non-iterative one-shot and non-destructive correction of unavoidable mistakes arises in all Artificial Intelligence applications in the real world. Its solution requires robust separation of samples with errors from samples where the system works properly. We demonstrate that in (moderately) high dimension this separation could be achieved with probability close to one by linear discriminants. Based on fundamental properties of measure concentration, we show that for M < a exp(bn) random Melement sets in Rn are linearly separable with probability p, p > 1 \u2212 \u03d1, where 1 > \u03d1 > 0 is a given small constant. Exact values of a, b > 0 depend on the probability distribution that determines how the random M-element sets are drawn, and on the constant \u03d1. These stochastic separation theorems provide a new instrument for the development, analysis, and assessment of machine learning methods and algorithms in high dimension. Theoretical statements are illustrated with numerical examples.\nKeywords: Fisher\u2019s discriminant, random set, measure concentration, linear separability, machine learning, extreme point"}, {"heading": "1. Introduction", "text": "Artificial Intelligence (AI) systems make errors. They should be corrected without damage of existing skills. The problem of non-destructive correction arises in many areas of research and development, from AI to mathematical neuroscience, where the reverse engineering of the brain ability to learn on-the-fly remains a great challenge. It is very desirable that the corrector of errors is non-iterative (one-shot) because iterative re-training of a large system requires much time and resource and cannot be done immediately without impeding activity.\nThe non-desrructive correction requires separation of the situations (samples) with errors from the samples corresponding to correct behavior by a simple and robust classifier. Linear discriminants introduced by Fisher (1936) are simple, robust, require just the inverse covariance matrix of data, and may be easily modified for assimilation of new data. Rosenblatt (1962) revived the common interest in linear classifiers. His works sparked intensive scientific debate (Minsky and Papert, 1969) and gave rise to development of numerous crucial concepts such as e.g. Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002). Linear functionals (adaptive summators) are basic building blocks of significantly more sophisticated AI systems such as e.g. multilayer perceptrons, (Rumelhart et al., 1986), Convolutional Neural Networks (Le Cun and Bengio, 1995), (LeCun et al., 2015) and their derivatives. Much is known about linear functionals\n\u2217Corresponding author Email addresses: ag153@le.ac.uk (A.N. Gorban), it37@le.ac.uk\n(I.Y. Tyukin)\nas \u201cstand-alone\u201d learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.\nIn this work, we demonstrate that in high dimensions and even for exponentially large samples, linear classifiers in their classical Fisher\u2019s form are powerful enough to separate errors from correct responses with high probability and to provide efficient solution to the non-destructive corrector problem. We prove that linear functionals, as learning machines, have surprising and, as far as we are concerned, new peculiar extremal properties: in high dimension, with probability p > 1 \u2212 \u03d1 and for M < a exp(bn) with a, b > 0 every point in random i.i.d. drawn M-element sets in Rn is linearly separable from the rest. Moreover, the separating linear functional can be found explicitly, without iterations. This property holds for a broad set of relevant distributions, including products of probability measures with bounded support and equidistribution in a unit ball, providing mathematical foundations for one-trial correction of legacy AI systems (cf. (Gorban et al., 2016a)).\nA problem of data fusion in multiagent systems has clear similarity to the problem of non-destructive correction. According to Forney et al. (2017), data collected by different agents may not be naively combined due to changes in the context, and special procedures for their assimilation without damage of gained skills are needed. The proven stochastic separation effects can be used to approach this problem. They also shed light on the possible origins of remarkable selectivity to stimuli observed in-vivo in the real brain (Quian Quiroga et al., 2005).\nPreprint submitted to Elsevier August 4, 2017"}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Notation", "text": "Throughout the text, Rn is the n-dimensional linear real vector space. Unless stated otherwise, symbols xi = (xi,1, . . . , xi,n) denote elements of Rn, and ( xi, x j ) = \u2211 k xi,kx j,k is the inner product of xi and x j in R n. Symbol Bn stands for the unit ball in Rn centered at the origin: Bn = {x \u2208 Rn| (x, x) \u2264 1}."}, {"heading": "2.2. Linear Separability of Sets", "text": "Definition A set S \u2282 Rn is linearly separable if for each x \u2208 S there exists a linear functional l such that l(x) > l(y) for all y \u2208 S , y , x.\nRecall that x \u2208 Rn is an extreme point of a convex compact K if there exist no points y, z \u2208 K, y , z such that x = (y + z)/2. The basic examples of linearly separable sets are extreme points of a convex compacts: vertices of convex polyhedra or points on the n-dimensional sphere. The sets of extreme points of a compact may be not linearly separable as is demonstrated by simple 2D examples (Simon, 2011).\nProposition 1. Every compact linearly separable set S \u2282 Rn is a set of extreme points of a convex compact K = convS .\nThe proof follows immediately from the previous definitions, the Krein-Milman theorem (Simon, 2011) (its finitedimensional form was known to Minkovsky) and classical theorems about separation of a point from a convex set.\nIf n + 1 points in Rn do not belong to a hyperplane then they are vertices of a simplex and are, obviously, linearly separable. If the lengths of the edges are bounded then the volume of the simplex decreases with n not slower than 1/n!. Therefore, we can expect that for a sufficiently regular distribution of points, a random point does not belong to the simplex, and n+2 independently chosen random points are also linearly separable, with high probability, which tends to 1 as 1 \u2212 c/n! (n \u2192 \u221e, c > 0). This fast convergence allows us to hypothesize that even a large random finite set is linearly separable in high dimension with high probability, if the distribution is regular enough. We prove this statement below for i.i.d. random points from equidistributions in a ball and a cube, and from distributions that are products of measures with bounded support."}, {"heading": "3. Main Results", "text": "Let us start from the equidistribution in the unit ball Bn in R n. The probability p that a random point belongs to a layer Bn \\ rBn (0 < r < 1) between spheres of radius 1 and of radius r is p = 1 \u2212 rn. Let us take a unit vector v. The probability that the projection of a random vector x on v, (x, v), exceeds r can be estimated from above by half of the ratio of volumes of balls of radii \u03c1 = \u221a 1 \u2212 r2 and 1 (see Fig. 1 with \u03b5 = 1 \u2212 r): P((x, v) > r) \u2264 0.5\u03c1n.\nTheorem 1. Let {x1, . . . , xM} be a set of M i.i.d. random points from the equidustribution in the unit ball Bn, 0 < r < 1. Then\nP\n( \u2016xM\u2016 > r and ( xi, xM\n\u2016xM\u2016\n)\n< r for all i , M\n)\n\u2265 1 \u2212 rn \u2212 0.5(M \u2212 1)\u03c1n; (1)\nP\n( \u2016x j\u2016 > r and ( xi, x j\n\u2016x j\u2016\n)\n< r for all i, j, i , j\n)\n\u2265 1 \u2212 Mrn \u2212 0.5M(M \u2212 1)\u03c1n; (2)\nP\n( \u2016x j\u2016 > r and ( xi\n\u2016xi\u2016 ,\nx j\n\u2016x j\u2016\n)\n< r for all i, j, i , j\n)\n\u2265 1 \u2212 Mrn \u2212 M(M \u2212 1)\u03c1n. (3)\nThe proof is based on the independence of random points {x1, . . . , xM}, on the geometric picture presented in Fig. 1, and on an elementary inequality P(A1&A2& . . .&Am) \u2265 1 \u2212 \u2211\ni(1 \u2212 P(Ai)) for any events A1, . . . , Am. In Fig. 1 we should take \u03b5 = 1 \u2212 r and the external radius of the spherical layer A is 1. Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r. The idea of almost orthogonal bases was introduced by Kainen and Ku\u030arkova\u0301 (1993) and used efficiently by Ku\u030arkova\u0301 and Sanguineti (2007) for estimation of the cardinality of \u03b5-nets in compact convex subsets of Hilbert spaces including the sets of functions computable by perceptrons.\nThe following corollary gives simple estimates of exponential growth of the maximal possible M for which inequalities (1) and (2) hold with a given probability value.\nCorollary 1. Let {x1, . . . , xM} be a set of M i.i.d. random points from the equidustribution in the unit ball Bn and 0 < r, \u03d1 < 1. If\nM < 2(\u03d1 \u2212 rn)/\u03c1n, (4)\nthen P((xi, xM) < r\u2016xM\u2016 for all i = 1, . . . ,M \u2212 1) > 1 \u2212 \u03d1. If\nM < (r/\u03c1)n ( \u22121 + \u221a 1 + 2\u03d1\u03c1n/r2n ) , (5)\nthen P((xi, x j) < r\u2016xi\u2016 for all i, j = 1, . . . ,M, i , j) \u2265 1 \u2212 \u03d1. In particular, if inequality (5) holds then the set {x1, . . . , xM} is linearly separable with probability p > 1 \u2212 \u03d1.\nA weaker and simpler estimate (sufficient condition) follows\nimmediately from (5):\n\u03d1/M2 > rn + 0.5\u03c1n. (6)\nRemark 1. According to (6) the pre exponential factor in the estimate for M2 may be chosen as \u03d1, while the exponent depends on r only. For example, for r = 1/ \u221a 2 the simple sufficient condition (6) gives M2 < 2 3 \u03d12n/2. For \u03d1 = 0.01 (or specificity 99%) and n = 100 we get M < 2, 740, 000.\nThus, if we select 2,700,000 i.i.d. points from an equidistribution in a unit ball in R100 then with probability p > 0.99 all these points will be vertices of their convex hull.\nEstimates similar to (3), (5), and (6) are useful for the equidistribution of the normalized data on a unit sphere too. This is because they not only establish the fact of separability but also specify separation margins.\nConsider a product distribution in an n-dimensional unit cube. Let the coordinates of a random point, X1, . . . , Xn (0 \u2264 Xi \u2264 1) be independent random variables with expectations Xi and variances \u03c32\ni > \u03c32 0 > 0. Let x be a vector with coordinates\nXi. For large n, this distribution is concentrated in a relatively small vicinity of a sphere with an arbitrary centre c with coordinates ci and radius R, where\nR2 = E\n       \u2211\ni\n(Xi \u2212 ci)2      \n\n= \u2211\ni\n\u03c32i + \u2016x \u2212 c\u20162. (7)\nConcentration near the spheres with different centres implies concentration in the vicinity of their intersection (an example of the \u2018waist concentration\u2019 (Gromov, 2003)). The vicinity of the spheres, where the distribution is concentrated, can be estimated by the Hoeffding inequality (Hoeffding, 1963). Let Y1, . . . , Yn be independent bounded random variables: 0 \u2264 Yi \u2264 1. The empirical mean of these variables is defined as Y = 1\nn (Y1 + \u00b7 \u00b7 \u00b7 + Yn). Then\nP ( Y \u2212 E [ Y ] \u2265 t ) \u2264 exp ( \u22122nt2 ) ;\nP\n(\u2223\n\u2223 \u2223 \u2223 Y \u2212 E [ Y ]\n\u2223 \u2223 \u2223 \u2223 \u2265 t ) \u2264 2 exp ( \u22122nt2 ) . (8)\nLet us take Yi = (Xi \u2212 ci)2. Consider the centres located in the cube, 0 \u2264 ci \u2264 1. Then 0 \u2264 Yi \u2264 1 and E [ Y ] = 1 n R2. In particular, if ci = Xi then E [ Y ] = 1 n R2 0 (the minimal possible value), where R2 0 = \u2211 i \u03c3 2 i \u2265 n\u03c32 0 . In general, n\u03c32 0 \u2264 R2 \u2264 n.\nWith probability p > 1 \u2212 2 exp ( \u22122nt2 ) a random point x\nbelongs to the spherical layer (\u03b4 = nt/R2 0 , t = \u03b4R2 0 /n):\n1 \u2212 \u03b4 \u2264 \u2016x \u2212 x\u20162/R20 \u2264 1 + \u03b4. (9)\nConsider M i.i.d. points {x1, . . . , xM} from the product distribution. With probability p > 1\u22122M exp ( \u22122nt2 ) they all belong to the spherical layer (9). Therefore, with this probability we return to the situation presented in Fig. 1 with internal radius\u221a 1 \u2212 \u03b4R0 and external radius \u221a 1 + \u03b4R0. The difference from the equidistribution in the ball is that the volume of the ball is concentrated near the external sphere, while the distribution in the layer (9) is concentrated around the sphere \u2016x \u2212 x\u20162 = R2 0 .\nThe radius of ball B is defined by \u03c12 = (1+\u03b4)R2 0 \u2212 (1\u2212\u03b4)R2 0 = 2\u03b4R2 0 . The concentration radius (7) for the spheres concentric with the ball B (Fig. 1) is defined by R2 = R2 0 + (1 \u2212 \u03b4)R2 0 = (2\u2212\u03b4)R2 0 . Therefore, a random point does not belong to the ball Bwith probability p > 1\u2212exp ( \u22122n\u03c42 )\n, where \u03c4 = 1 n (R2\u2212\u03c12) =\n1 n (2 \u2212 3\u03b4)R2 0 . Thus, we get the following statement.\nTheorem 2. Let {x1, . . . , xM} be i.i.d. random points from the product distribution in a unit cube, 0 < \u03b4 < 2/3. Then\nP\n      1 \u2212 \u03b4 \u2264 \u2016x j \u2212 x\u20162\nR2 0 \u2264 1 + \u03b4 and (\nxi \u2212 x R0 , xM \u2212 x \u2016xM \u2212 x\u2016\n) < \u221a 1 \u2212 \u03b4 for all i, j, i , M )\n\u2265 1 \u2212 2M exp ( \u22122\u03b42R40/n ) \u2212 (M \u2212 1) exp ( \u22122R40(2 \u2212 3\u03b4)2/n ) ;\n(10)\nP\n      1 \u2212 \u03b4 \u2264 \u2016x j \u2212 x\u20162\nR2 0 \u2264 1 + \u03b4 and (\nxi \u2212 x R0 , x j \u2212 x \u2016x j \u2212 x\u2016\n) < \u221a 1 \u2212 \u03b4 for all i, j, i , j )\n\u2265 1 \u2212 2M exp ( \u22122\u03b42R40/n ) \u2212 M(M \u2212 1) exp ( \u22122R40(2 \u2212 3\u03b4)2/n )\n(11)\nWhen the value of delta is chosen as \u03b4 = 0.5 and R0 is replaced with its estimate from below, R2 0 \u2265 n\u03c32 0 , inequalities (10) and (11) result in the following estimates:\nP\n      1\n2 \u2264\n\u2016x j \u2212 x\u20162\nR2 0\n\u2264 3 2 and\n(\nxi \u2212 x R0 , xM \u2212 x \u2016xM \u2212 x\u2016\n) < \u221a 1 \u2212 \u03b4 for all i, j, i , M )\n\u2265 1 \u2212 3M exp ( \u22120.5n\u03c340 ) ;\n(12)\nP\n      1\n2 \u2264\n\u2016x j \u2212 x\u20162\nR2 0\n\u2264 3 2 and\n(\nxi \u2212 x R0 , x j \u2212 x \u2016x j \u2212 x\u2016\n) < \u221a 1 \u2212 \u03b4 for all i, j, i , j )\n\u2265 1 \u2212 M(M + 1) exp ( \u22120.5n\u03c340 ) .\n(13)\nCorollary 2. Let {x1, . . . , xM} be i.i.d. random points from the product distribution in a unit cube and 0 < \u03d1 < 1. If\nM < 1\n3 \u03d1 exp\n(\n0.5n\u03c340\n)\n, (14)\nthen with probability p > 1 \u2212 \u03d1\n0.5 \u2264 \u2016x j \u2212 x\u20162\nR2 0\n\u2264 1.5 and ( xi \u2212 x R0 , xM \u2212 x \u2016xM \u2212 x\u2016 ) <\n\u221a 2\n2\nfor all i, j, i , M.\nIf\n(M + 1)2 < 1\n3 \u03d1 exp\n(\n0.5n\u03c340\n)\n, (15)\nthen with probability p > 1 \u2212 \u03d1\n0.5 \u2264 \u2016x j \u2212 x\u20162\nR2 0\n\u2264 1.5 and ( xi \u2212 x R0 , x j \u2212 x \u2016x j \u2212 x\u2016 ) <\n\u221a 2\n2\nfor all i, j, i , j.\nIn particular, if inequality (15) holds then the set {x1, . . . , xM} is linearly separable with probability p > 1 \u2212 \u03d1.\nThe estimates (14), (15) are far from being optimal and can be improved. The main message here is their exponential dependence on n: the upper boundary of M can grow with n exponentially. Numerical experiments show that the equidistribution in cube is not worse, from the practical point of view, than the uniform distribution in a ball. To illustrate this, we empirically assessed linear separability of samples drawn from equidistributions in the unit n-cubes. For selected values of n from the set {1, . . . , 5000} we generated 100 samples S of M = 20000 random points from [0, 1]n. For each sample, a sub-sample S \u2282 S of N = 4000 points was randomly chosen, and for each point xi in this sub-sample linear functionals l(x) = (xi \u2212 x\u0304, x \u2212 x\u0304) \u2212 \u2016xi \u2212 x\u0304\u20162 were constructed. Sings of l(x j), x j \u2208 S , x j , xi were calculated, and the numbers N\u2212 of instances when l(x j) < 0 where recorded. Empirical frequencies N\u2212/N were then derived. Outcomes of this experiment are summarized in Fig. 2. These experiments demonstrate that the probability that a randomly selected point in a sample is linearly separable from the rest could be significantly higher than the simple exponential estimates provided. This, however, is not surprising as the estimates are based on the values of means and variances, and do not take into account other quantitative properties of the sample distribution.\nIn general position, a set of n points in Rn\u22121 is linearly separable. Therefore, if n\u22121 or less points fromM = {x1, . . . , xM\u22121} are not separated from x by the hyperplane L (Fig. 1) then they can be separated by an additional hyperplane orthogonal to L. This means that x can be separated from the wholeM by a conjunction of two linear inequalities, (\u2022, x/\u2016x\u2016) > r& (\u2022, y) > q, for some 0 < r < 0, q > 0, and y, (y, x) = 0. This system can be considered as a cascade of two independent neurons (Gorban et al., 2016a). The probability of such a two-neuron separability is higher than of linear separability. (Compare inequality (16) in the following theorem to (1).)\nTheorem 3. Let S = {x1, . . . , xM} be a set of M i.i.d. random points from the equidustribution in the unit ball Bn, 0 < r < 1. Then\nP\n( \u2016xM\u2016 > r& ( xi, xM\n\u2016xM\u2016\n) < r for at least M\u2212n points xi \u2208 S )\n\u2265 (1 \u2212 rn)(1 \u2212 0.5\u03c1n)M\u22121\n\u00d7 (\n1 \u2212 1 n!\n(\n0.5(M \u2212 n)\u03c1n 1 \u2212 0.5\u03c1n\n)n)\nexp\n[\n0.5(M \u2212 n)\u03c1n 1 \u2212 0.5\u03c1n\n]\n,\n(16)\nwhere \u03c1 = \u221a 1 \u2212 r2.\nFor r = 1/ \u221a 2, n = 100, and M = 2, 74 \u00b7 106, (16) gives:\nP ( \u2016xM\u2016 > r& ( xi, xM\n\u2016xM\u2016\n) < r for at least M\u2212n xi \u2208 S )\n\u2265 1 \u2212 \u03b8 with \u03b8 < 5 \u00b7 10\u221214. The probability stays close to 1 for much larger values ofM, as settingM = 7\u00b71016 results in the estimate: P ( \u2016xM\u2016 > r& ( xi, xM\n\u2016xM\u2016\n) < r for at least M\u2212n xi \u2208 S )\n\u2265 1 \u2212 \u03b8 with \u03b8 < 5 \u00b7 10\u22129."}, {"heading": "4. Conclusion", "text": "Classical measure concentration theorems state that random points are concentrated in a thin layer near a surface (a sphere, an average or median level set of energy or another function, etc.). The stochastic separation theorems describe thin structure of these thin layers: the random points are not only concentrated in a thin layer but are all linearly separable from the rest of the set even for exponentially large random sets. The estimates are produced for two classes of distributions in high dimension: for equidistributions in balls or ellipsoids or for the product distributions with compact support (i.e. for the case when coordinates are bounded independent random variables). Numerous generalisations are possible, for example:\n\u2022 Relax the requirement of independent coordinates in Theorem 2 to that of weakly dependent vector-valued vari-\nables;\n\u2022 Instead of equidistributions, consider distributions with strongly log-concave probability densities;\n\u2022 Use various simple and robust nonlinear classifiers like small neural cascades (compare to Theorem 3), algebraic\nclassifiers and other families. For these generalisations, the VC dimension is expected to play the role similar to dimension n in Theorems 1 and 2.\nStochastic separation Theorems 1\u20133 are important for synthesis and one-shot correction of AI systems. For example, inequalities (1) and (10) evaluate the probability that a randomly selected point xM is linearly separable from all other M \u2212 1 points by the linear functional l(x) = (x, xM \u2212 x). This separation is sufficient to correct a mistake of a legacy AI system without any re-learning and modification of existing skills (Gorban et al., 2016a). Such measure concentration effects reveal the hidden geometric background of the reported success of randomized neural networks models (Scardapane and Wang, 2017).\nStochastic separation theorems can simplify highdimensional data analysis and generate the \u2019blessing of dimensionality\u2019 (Gorban et al., 2016c). For example, according to (6), in a dataset with 100 attributes and M < 2.7 \u00b7 106 samples we should not be surprised to observe the linear separability of each sample from the rest of the database by the inequalities \u3008xi, x j\u3009 < \u221a 1 2 \u3008xi, xi\u3009 (i , j) in the Mahalanobis inner product \u3008x, y\u3009 = (x, S \u22121y), where S is the empirical covariance matrix. The Mahalanobis inner product is used for \u2018whitening\u2019, i.e. for transformation of the data cloud into the spherical form. Of course, these attributes should not be highly correlated and the empiric covariance matrix should be invertible.\nWe analysed separation of random points from random sets. This is the problem of single correction of a legacy AI system. The question of generalisability of this correction is of great practical importance. It leads to a problem of separation of two random sets. A simple series of generalisations can be immediately produced from Theorems 1-3 for separation of an Melement random set S = {x1, . . . , xM} \u2282 Rn from a k-element one {y1, . . . , yk} for k < n. For this purpose, we can consider a linear space E = span{yi\u2212 y1 | i = 2, . . . , k} and study separation of a point from an M-element set in the projection onto the quotient space Rn/E. If y1, . . . , yk are independent then separation would likely be limited to sets of small cardinality k < n. If, in contrast, y1, . . . , yk are pair-wise positively correlated then we can expect that a single functional would separate them from S , with reasonable probability even for some k \u2265 n. This naturally gives rise to generalization of corrections.\nThe reported extreme separation capabilities of linear functionals offer new insights into the Grandmother cell or concept cell phenomena that are broadly reported in neuroscience (Quian Quiroga et al., 2005), (Viskontasa et al., 2009). The essence of the phenomenon is that some neurons in the human brain respond unexpectedly selective to particular persons or objects. Strikingly, not only is the brain able to respond selectively to \u201crare\u201d individual stimuli but also such selectivity\ncan be learnt very rapidly from a limited number of experiences (Ison et al., 2015). The question is: Why small ensembles of neurons may deliver such a sophisticated functionality reliably? Stochastic separation Theorems 1-3 provide a possible answer. If we accept that a) linear functionals followed by nonlinear threshold-modulated response as phenomenological models of cells whose activity was measured, b) the number of inputs converging to these cells is large enough, and c) they are statistically independent, then extreme selectivity of responses of such models follows immediately from Theorem 2."}, {"heading": "Acknowledgement", "text": "Authors are grateful to M. Gromov and S. Utev for seminal questions and comments. The work was partially supported by Innovate UK (KTP009890 and KTP010522)."}], "references": [{"title": "An elementary introduction to modern convex geometry", "author": ["K. Ball"], "venue": "Flavors of geometry", "citeRegEx": "Ball,? \\Q1997\\E", "shortCiteRegEx": "Ball", "year": 1997}, {"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research ,", "citeRegEx": "Bousquet and Elisseeff,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff", "year": 2002}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Machine Learning", "citeRegEx": "Fisher,? \\Q1936\\E", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Counterfactual DataFusion for Online Reinforcement Learners. A talk at the 1st Workshop on Transfer in Reinforcement Learning at AAMAS 2017", "author": ["A. Forney", "J. Pearl", "E. Bareinboim"], "venue": "May 8\u20139,", "citeRegEx": "Forney et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Forney et al\\.", "year": 2017}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Y. Freund", "R. Schapire"], "venue": "Machine Learning", "citeRegEx": "Freund and Schapire,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1999}, {"title": "2016a. Onetrial correction of legacy AI systems and stochastic separation theorems", "author": ["A.N. Gorban", "I. Romanenko", "R. Burton", "I. Tyukin"], "venue": null, "citeRegEx": "Gorban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gorban et al\\.", "year": 2016}, {"title": "2016b. Approximation with random bases: Pro et contra", "author": ["A.N. Gorban", "I. Tyukin", "D. Prokhorov", "K. Sofeikov"], "venue": "Information Sciences", "citeRegEx": "Gorban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gorban et al\\.", "year": 2016}, {"title": "The blessing of dimensionality: Separation theorems in the thermodynamic limit", "author": ["A.N. Gorban", "I.Y. Tyukin", "I. Romanenko"], "venue": "IFACPapersOnLine, 49,", "citeRegEx": "Gorban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gorban et al\\.", "year": 2016}, {"title": "Isoperimetry of waists and concentration of maps. GAFA", "author": ["M. Gromov"], "venue": "Geomteric and Functional Analysis", "citeRegEx": "Gromov,? \\Q2003\\E", "shortCiteRegEx": "Gromov", "year": 2003}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "J. Amer. Statist. Assoc", "citeRegEx": "Hoeffding,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding", "year": 1963}, {"title": "Rapid encoding of new memories by individual neurons in the human", "author": ["M. Ison", "R. Quian Quiroga", "I. Fried"], "venue": "brain. Neuron", "citeRegEx": "Ison et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ison et al\\.", "year": 2015}, {"title": "Quasiorthogonal dimension of euclidian spaces", "author": ["P. Kainen", "V. K\u016frkov\u00e1"], "venue": "Appl. Math. Lett", "citeRegEx": "Kainen and K\u016frkov\u00e1,? \\Q1993\\E", "shortCiteRegEx": "Kainen and K\u016frkov\u00e1", "year": 1993}, {"title": "Estimates of covering numbers of convex sets with slowly decaying orthogonal subsets", "author": ["V. K\u016frkov\u00e1", "M. Sanguineti"], "venue": "Discrete Applied Mathematics", "citeRegEx": "K\u016frkov\u00e1 and Sanguineti,? \\Q2007\\E", "shortCiteRegEx": "K\u016frkov\u00e1 and Sanguineti", "year": 2007}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. Le Cun", "T. Bengio"], "venue": null, "citeRegEx": "Cun and Bengio,? \\Q1995\\E", "shortCiteRegEx": "Cun and Bengio", "year": 1995}, {"title": "On learning sets and functions", "author": ["B. Natarajan"], "venue": "Machine Learning", "citeRegEx": "Natarajan,? \\Q1989\\E", "shortCiteRegEx": "Natarajan", "year": 1989}, {"title": "Invariant visual representation by single neurons in the human brain", "author": ["R. Quian Quiroga", "L. Reddy", "G. Kreiman", "C. Koch", "I. Fried"], "venue": "Nature 435,", "citeRegEx": "Quiroga et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Quiroga et al\\.", "year": 2005}, {"title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms", "author": ["F. Rosenblatt"], "venue": "Spartan Books", "citeRegEx": "Rosenblatt,? \\Q1962\\E", "shortCiteRegEx": "Rosenblatt", "year": 1962}, {"title": "Learning internal representations by error propagation, in: Rumelhart, D.E. McClelland, J., the PDP research group (Eds.), Parallel distributed processing: Explorations", "author": ["D.E. Rumelhart", "G.E. Hinton", "R. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Randomness in neural networks: an overview", "author": ["S. Scardapane", "D. Wang"], "venue": "WIREs Data Mining Knowl. Discov. 7,", "citeRegEx": "Scardapane and Wang,? \\Q2017\\E", "shortCiteRegEx": "Scardapane and Wang", "year": 2017}, {"title": "Convexity, An Analytic Viewpoint, Cambridge Tracts in Mathematics, V. 187", "author": ["B. Simon"], "venue": null, "citeRegEx": "Simon,? \\Q2011\\E", "shortCiteRegEx": "Simon", "year": 2011}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q2000\\E", "shortCiteRegEx": "Vapnik", "year": 2000}, {"title": "Estimation of Dependences Based on Empirical Data", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1982\\E", "shortCiteRegEx": "Vapnik", "year": 1982}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V.N. Vapnik", "A.Y. Chervonenkis"], "venue": "Theory of Probability and Its Applications", "citeRegEx": "Vapnik and Chervonenkis,? \\Q1971\\E", "shortCiteRegEx": "Vapnik and Chervonenkis", "year": 1971}, {"title": "Human medial temporal lobe neurons respond preferentially to personally relevant images", "author": ["I. Viskontasa", "R. Quian Quiroga", "I. Fried"], "venue": "Proc. Nat. Acad. Sci. 120,", "citeRegEx": "Viskontasa et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Viskontasa et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 22, "context": "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).", "startOffset": 27, "endOffset": 58}, {"referenceID": 14, "context": "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).", "startOffset": 73, "endOffset": 90}, {"referenceID": 20, "context": "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).", "startOffset": 143, "endOffset": 157}, {"referenceID": 1, "context": "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).", "startOffset": 159, "endOffset": 189}, {"referenceID": 17, "context": "multilayer perceptrons, (Rumelhart et al., 1986), Convolutional Neural Networks (Le Cun and Bengio, 1995), (LeCun et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 1, "context": "Linear discriminants introduced by Fisher (1936) are simple, robust, require just the inverse covariance matrix of data, and may be easily modified for assimilation of new data.", "startOffset": 35, "endOffset": 49}, {"referenceID": 1, "context": "Linear discriminants introduced by Fisher (1936) are simple, robust, require just the inverse covariance matrix of data, and may be easily modified for assimilation of new data. Rosenblatt (1962) revived the common interest in linear classifiers.", "startOffset": 35, "endOffset": 196}, {"referenceID": 4, "context": "Tyukin) as \u201cstand-alone\u201d learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.", "startOffset": 83, "endOffset": 110}, {"referenceID": 20, "context": "Tyukin) as \u201cstand-alone\u201d learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.", "startOffset": 112, "endOffset": 126}, {"referenceID": 21, "context": "Tyukin) as \u201cstand-alone\u201d learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.", "startOffset": 258, "endOffset": 272}, {"referenceID": 3, "context": "According to Forney et al. (2017), data collected by different agents may not be naively combined due to changes in the context, and special procedures for their assimilation without damage of gained skills are needed.", "startOffset": 13, "endOffset": 34}, {"referenceID": 19, "context": "The sets of extreme points of a compact may be not linearly separable as is demonstrated by simple 2D examples (Simon, 2011).", "startOffset": 111, "endOffset": 124}, {"referenceID": 19, "context": "The proof follows immediately from the previous definitions, the Krein-Milman theorem (Simon, 2011) (its finitedimensional form was known to Minkovsky) and classical theorems about separation of a point from a convex set.", "startOffset": 86, "endOffset": 99}, {"referenceID": 0, "context": "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls.", "startOffset": 0, "endOffset": 12}, {"referenceID": 0, "context": "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r.", "startOffset": 0, "endOffset": 227}, {"referenceID": 0, "context": "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r. The idea of almost orthogonal bases was introduced by Kainen and K\u016frkov\u00e1 (1993) and used efficiently by K\u016frkov\u00e1 and Sanguineti (2007) for estimation of the cardinality of \u03b5-nets in compact convex subsets of Hilbert spaces including the sets of functions computable by perceptrons.", "startOffset": 0, "endOffset": 374}, {"referenceID": 0, "context": "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r. The idea of almost orthogonal bases was introduced by Kainen and K\u016frkov\u00e1 (1993) and used efficiently by K\u016frkov\u00e1 and Sanguineti (2007) for estimation of the cardinality of \u03b5-nets in compact convex subsets of Hilbert spaces including the sets of functions computable by perceptrons.", "startOffset": 0, "endOffset": 428}, {"referenceID": 8, "context": "Concentration near the spheres with different centres implies concentration in the vicinity of their intersection (an example of the \u2018waist concentration\u2019 (Gromov, 2003)).", "startOffset": 155, "endOffset": 169}, {"referenceID": 9, "context": "The vicinity of the spheres, where the distribution is concentrated, can be estimated by the Hoeffding inequality (Hoeffding, 1963).", "startOffset": 114, "endOffset": 131}, {"referenceID": 18, "context": "Such measure concentration effects reveal the hidden geometric background of the reported success of randomized neural networks models (Scardapane and Wang, 2017).", "startOffset": 135, "endOffset": 162}, {"referenceID": 23, "context": ", 2005), (Viskontasa et al., 2009).", "startOffset": 9, "endOffset": 34}, {"referenceID": 10, "context": "Strikingly, not only is the brain able to respond selectively to \u201crare\u201d individual stimuli but also such selectivity can be learnt very rapidly from a limited number of experiences (Ison et al., 2015).", "startOffset": 181, "endOffset": 200}], "year": 2017, "abstractText": "The problem of non-iterative one-shot and non-destructive correction of unavoidable mistakes arises in all Artificial Intelligence applications in the real world. Its solution requires robust separation of samples with errors from samples where the system works properly. We demonstrate that in (moderately) high dimension this separation could be achieved with probability close to one by linear discriminants. Based on fundamental properties of measure concentration, we show that for M < a exp(bn) random Melement sets in R are linearly separable with probability p, p > 1 \u2212 \u03b8, where 1 > \u03b8 > 0 is a given small constant. Exact values of a, b > 0 depend on the probability distribution that determines how the random M-element sets are drawn, and on the constant \u03b8. These stochastic separation theorems provide a new instrument for the development, analysis, and assessment of machine learning methods and algorithms in high dimension. Theoretical statements are illustrated with numerical examples.", "creator": "LaTeX with hyperref package"}}}