{"id": "1703.02618", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Bootstrapped Graph Diffusions: Exposing the Power of Nonlinearity", "abstract": "Graph - based semi - 892,000 supervised tolstoi learning (bittinger SSL) algorithms predict betweem labels headiest for all bustled nodes based on dederich provided labels hide-out of a intermediates small acan set defiant of wafi seed nodes. Classic guillem methods capture renne the graph gorm structure lettere through i-75 some gardaworld underlying diffusion megaliths process martenot that propagates neukoelln through lieberstein the graph 29.08 edges. 5-4 Spectral diffusion, which includes electrocutions personalized infringe page 383.10 rank chopan and rajeswar label 1,918 propagation, dissuade propagates mcgarry through vasbert random walks. Social promulgation diffusion propagates through shortest higazi paths. tapeworm A 239th common ground shilly to afrotropic these 22-3 diffusions 42.42 is their {\\ revive em wylam linearity }, which does celebrity not distinguish acea between contributions xiangjun of self-regulated few \" garc?a strong \" rvu relations murmured and pay-off many \" weak \" relations.", "histories": [["v1", "Tue, 7 Mar 2017 22:10:34 GMT  (1393kb,D)", "https://arxiv.org/abs/1703.02618v1", "11 pages, 5 figures, 6 tables"], ["v2", "Wed, 15 Mar 2017 11:54:41 GMT  (1393kb,D)", "http://arxiv.org/abs/1703.02618v2", "11 pages, 5 figures, 6 tables"]], "COMMENTS": "11 pages, 5 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eliav buchnik", "edith cohen"], "accepted": false, "id": "1703.02618"}, "pdf": {"name": "1703.02618.pdf", "metadata": {"source": "CRF", "title": "Bootstrapped Graph Di\u0080usions: Exposing the Power of Nonlinearity", "authors": ["Eliav Buchnik", "Edith Cohen"], "emails": ["eliavbuh@gmail.com", "edith@cohenwang.com"], "sections": [{"heading": null, "text": "Recently, non-linear methods such as node embeddings and graph convolutional networks (GCN) demonstrated a large gain in quality for SSL tasks. ese methods introduce multiple components and greatly vary on how the graph structure, seed label information, and other features are used.\nWe aim here to study the contribution of non-linearity, as an isolated ingredient, to the performance gain. To do so, we place classic linear graph di usions in a self-training framework. Surprisingly, we observe that SSL using the resulting bootstrapped di usions not only signi cantly improves over the respective non-bootstrapped baselines but also outperform state-of-the-art non-linear SSL methods. Moreover, since the self-training wrapper retains the scalability of the base method, we obtain both higher quality and be er scalability."}, {"heading": "1 INTRODUCTION", "text": "Graph data is prevalent and models entities (nodes) and the strong interactions between them (edges). e source of these graphs can naturally come with the provided interactions (social networks, views, likes, purchases, messages, links) or can be derived from metric data (embedded entities) by retaining only edges that correspond to closest neighbors.\nA fundamental task arises when label information is available only for a small set of seed entities (x j ,y j ) j \u2264 n` and we are interested in learning labels for all other entities xi for i \u2208 (n` ,n` + nu ]. See e.g. the surveys [8, 38]. e learning uses some smoothness assumption that similarity derived from the graph structure implies similarity of labels. O en the graph structure is combined with other features by the learning algorithms.\nClassic methods for semi-supervised learning and many other related fundamental graph tasks (clustering, centrality, in uence) are based on some underlying di usion process that propagates from a node or set of nodes through the edges of the graph. e di usion de nes dense ne a nity relations between nodes using\nthe provided sparse set of strong interactions. With SSL, the a nity relation guides the label learning, for example, by a weighted aggregation of seed labels to obtain so labels.\nMost popular SSL methods can be interpreted through underlying spectral di usions [9], utilizing the graph Laplacian, graph cuts [5, 6], and random walks. ey include label propagation [44], label propagation using the normalized graph Laplacian [25, 42], and many variations. e methods are highly scalable and implemented using Jacobi iterations that roughly translate into repeated averaging over neighboring nodes. e algorithms are applied successfully to massive graphs with billions of edges [33] using highly distributed platforms [26].\nAnother class of graph di usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in uence [13, 17, 19, 23], and similarity [12] of nodes. Social di usion propagate along shortest-paths (distance di usion) or reachability searches (reach di usion). A powerful extension de nes a generative model from a graph by randomizing the presence (with reach di usion) or the length (with distance di usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations. Social di usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11]. e proposed algorithms scale very well: For the simpler nearest-seed variant which matches each label to the closest seed node in each simulation of the randomized model we simply use small number of graph (Dijkstra) searches. e use of distance or reachability sketching based on [10] allows for highly scalable label learning also over the sketched a nity matrix.\nBoth spectral and social di usion based SSL models scale well even with a large number of labels, using heavy hi er sketches with label propagation [33] or naturally with social di usion using sketches. We interpret these methods as linear in the sense that the di usion propagates from seeds through edges without amplifying strong signals or suppressing weak ones.\nRecently proposed non-linear learning methods, based on node embeddings and graph convolutional networks, had impressive success in improving the quality of the learned labels. In particular, DeepWalk [32] applied the hugely successful word embedding framework of [27] to embed the graph nodes in a way that preserves the a nity relation de ned by co-occurrence frequencies of pairs in short random walks: A so max applied to inner products of embeddings approximates the frequency of the pair. A supervised learning algorithm is then trained on the embedding vectors and labels of seed nodes. Node2vec [20] re ned the approach using hyperparameters that tune the depth and breadth of the random\nar X\niv :1\n70 3.\n02 61\n8v 2\n[ cs\n.L G\n] 1\n5 M\nar 2\n01 7\n, , Eliav Buchnik and Edith Cohen\nwalks. Another method, Planetoid, used a multi-layer neural network instead of a single so max layer [40]. With these methods, the lower-dimensional embeddings serve as a \u201clow rank\u201d representation of the a nity matrix and the so max and neural network introduce non-linearities that emphasize larger inner products. Another successful proposal are Graph convolutional networks (GCN) [2, 16, 21, 24], which are neural networks with layers that follow the graph adjacency structure. Each layer applies a non-linear activation function, most o en a sigmoid or ReLU, to the aggregate over neighbors.\nis abundance of recent work introduced many new components and o en at the same time: Low-rank embeddings, non-linear propagations, learning of weights of hidden layers (GCNs), learning of node weights [29]. ese recent methods, however, while demonstrating improved labeling quality, do not scale as well as label propagation and methods based on social graph di usions.\nOur aim here is to isolate the main contributor(s) to the quality gain of the recent non-linear approaches and to seek methods that combine the scalability advantage of the simpler di usion models with state of the art labeling quality.\nWe explore placing these \u201clinear\u201d di usions in a self-training (bootstrapping) framework. Self-training is arguably the earliest approach to SSL, dating back ve decades to Scudder [35], and extensively studied by the NLP community [1, 39, 41]. e self-training framework can be viewed as a wrapper around a base learning algorithm. e base algorithm takes as input a set of labeled examples and makes predictions with associated margins or con dence scores for other examples. e wrapper applies the base algorithm in steps, where at the end of each step, the highest-con dence predictions are converted to become new labeled examples.\nOur bootstrapped di usions retain the high scalability of the base di usions while introducing non-linearity that allows us to work with a richer class of models. In particular, with our base algorithms, \u201cseed\u201d examples have a special role that is not ampli ed by implied high-con dence predictions. Bootstrapping provides such ampli cation by promoting high-margin predictions to \u201cseed\u201d roles.\nWe perform experiments using linear di usion models and their bootstrapped versions. We use classic Label propagation [44], Label propagation using the normalized Laplacian [42], and nearest-seed, which is the simplest distance di usion model [11]. We apply a very basic bootstrapping wrapper that works with a xed fraction of highest-margin predictions in each step. We focus on a multi-class se ing, where each node is a member of one class, even though most of the method can be extended to the multi-label se ing.\nWe apply the di erent methods to benchmark data and seed sets used and made available by previous work [24, 40]. In particular, we use social, citation, and knowledge graph data sets. We compare the quality of the learned labels to state of the art baselines, including DeepWalk [32], node2vec [20], Planetoid [40], and GCNs [24]. We also perform more elaborate experiments on additional data and seed sets and on the well-studied planted partition (stochastic block) model [15] which is o en used to understand the performance of clustering and community detection algorithms.\nOur main focus is the quality of learning from the graph structure alone using di used seed node labels. We observe that bootstrapped di usions consistently improved the quality, by 1% to 12%, over\nthe base di usion, both for spectral and social and across types of graphs. e most surprising outcome was that prediction quality on benchmark data exceeded that of all recent non-linear methods.\ne use of additional available node (or edge) features can signi cantly increase labeling quality but there are multiple ways to integrate them in the learning algorithm. We consider the use of the raw provided node features or smoothing them through a graph di usion. A simple supervised learning algorithm is then trained on the (raw or di used) feature vectors and class labels of seed nodes. We applied this method with and without bootstrapping. We observed that both di usion and bootstrapping signi cantly enhanced performance. Furthermore, our results dominated those reported (with the use of node features) by the state of the art baselines Planetoid [40] and GCNs [24]. In particular, GCNs lend themselves to a direct comparison as they can be viewed as a feature di usion with non-linearities applied a er each set of edge traverasals and node/layer weight tuned through back propagations. It is interesting that we obtained comparable or be er results using bootstrapped linear models and without backprop training.\ne paper is organized as follows. e linear base di usion we use and the bootstrapping wrapper are discussed in Section 2. Our data sets and our experimental methodology are laid out in Section 3. e results on the benchmark data are reported and discussed in Section 4. Additional experimental results using additional data and seed sets are discussed in Section 5. Detailed parameter study of the bootstrapping wrapper is provided in Section 6. Section 7 presents the experiments with feature vectors di usions. We conclude in Section 8."}, {"heading": "2 LINEAR AND BOOTSTRAPPED DIFFUSIONS", "text": "In this section we review SSL methods based on linear (spectral and social) graph di usions, while emphasizing the variants we used in our experiments. In particular, we discuss two textbook label propagations methods [42, 43] and a distance-di usion method [11]. We then discuss the application of self-training to these base methods."}, {"heading": "2.1 Spectral di usion methods", "text": "e label vectors for seed nodes yi are initialized so that the entry that corresponds to the provided label j \u2208 [L] of each seed node i \u2264 n` is set to yi j = 1 and other entries j \u2032 , j are set to yi j\u2032 = \u22121. e graph structure is represented by an a nity matrixW , which can be provided as input or learned. In our experiments, following prior work we compared with, we used the adjacency matrix of the provided undirected graphs with uniform edge weights and no self loops. at isWi j = 1 when the edge (i, j) is present andWi j = 0 otherwise.\ne algorithms we use compute so labels yi for the unlabeled nodes i > n` of dimension that is equal to the number of classes L. e learned label we return for each node i follows the maximum entry arg maxj yi j . We note that o en higher quality learned labels are obtaining by training a learning algorithm on the so label [11] but in our experiments here we used these common simple class predictions.\nLabel propagation (LP) Zhu and Ghahramani [43]. Pseudocode is provided as Algorithm 1 (As presented in [8]). A learned so\nBootstrapped Graph Di usions: Exposing the Power of Nonlinearity , ,\nlabels matrix Y is initialized to the seed label vectors yi for seed nodes i \u2264 n` and to yi = 0 for the unlabeled nodes i > n` . e algorithm performs iterations where in each iteration, each nonseed node obtains a new so label that is the weighted average of the so labels from the previous iteration of its neighbors. We note that this algorithm may not converge and does not necessarily improve with iterations. erefore, in our experiments, we treat the number of iterations performed as a hyperparameter.\nAlgorithm 1: Label propagation (LP) [43] Input: A nity matrixW . Provided labels y1, . . .yn` . // Initialization\n\u2200i, Dii \u2190 \u2211jWi j // diagonal degree matrix Y (0) \u2190 (y1, . . . , yn` , 0, . . . , 0) // Iterate:\nforeach t = 0, . . . , T do Y (t+1) \u2190 D\u22121W Y (t )\nY (t+1)[n` ]\u00b7 \u2190 (y1, . . . , yn` ) // Reset learned labels of\nlabeled nodes\n// Finalize:\nforeach i > n` do // Label points by largest entry labels[i] \u2190 arg maxj yi j return labels\nNormalized Laplacian label propagation Zhou et al [42]. Pseudocode is provided as Algorithm 2. is algorithm is related to Personalized Page Rank (PPR) and uses the normalized graph Laplacian [9]. e so labels Y (t ) converge to Y (\u221e) that satis es\nY (\u221e) = \u03b1(I \u2212 (1 \u2212 \u03b1)A)\u22121Y (0) . (1) e so labels at convergence Y (\u221e) correspond to the stationary distribution when performing random walks from the seed nodes with some probability \u03b1 of returning to the seed set in each step. Ideally, we would perform enough iterations so that the learned so labels Y (t ) are close to Y (\u221e). With this algorithm, the number of iterations is a parameter that trades o quality and computation, that is, we expect performance to improve with iterations. e return probability \u03b1 is a hyperparameter.\nAlgorithm 2: Normalized Laplacian LP [42] Input: A nity matrixW , provided labels y1, . . .yn` , return\nprobability \u03b1 \u2208 (0, 1) // Initialization: Y (0) \u2190 (y1, . . . , yn` , 0, . . . , 0) \u2200i, D ii \u2190 \u2211jWi j // Diagonal degree matrix A\u2190 D\u22121/2W D\u22121/2 // Normalized adjacency matrix // Iterate:\nforeach t = 0, . . . , T do Y (t+1) \u2190 (1 \u2212 \u03b1 )AY (t ) + \u03b1Y (0)\n// Finalize:\nforeach i > n` do // Label points by largest entry labels[i] \u2190 arg maxj yi j return labels"}, {"heading": "2.2 Social di usion methods", "text": "We consider recently proposed SSL methods based on distance di usion [8]. e input is a directed graph G = (V ,E) where nodes [n`] are the seed nodes and a distributionW that generates a set of lengths w > 0 for the edges e \u2208 E. e algorithm iterates the following. It draws a set of edge lengths w(t ) \u223c W from which it computes a set of so labels y(t )i for i > n` . e so label yi is computed from the shortest-path distances d(t )i j from i to each seed node j \u2264 n` and the respective labels label[j] \u2208 [L]. e nal so label we seek for each i > n` is the expectation E[y (t ) i ], and we approximate it by the average over the iterations. e number of iterations here is a parameter that trades o computation and quality.\nNearest-seed [8]. e general formulation allows each so label y(t )i to depend on the set of distances and labels {(di j , label[j])} of all seed nodes j \u2264 n` and requires distance sketching techniques to approximate using near-linear computation. In our experiments we focused only on the Nearest Seed variant (pseudocode provided as Algorithm 3), where in each iteration we only use the label of the seed node that is closest to i:\ny(t )i \u2190 yarg minj\u2264n` d (t ) i j .\ne computation of each iteration (computing the closest seed to each node) is equivalent to one single source shortest path computation such as (a slightly modi ed) Dijkstra\u2019s algorithm. Hence it is near linear.\nSince we use undirected graphs in our experiments, we generate two directed edges for each undirected edge. Guided by [11] e lengths of edges are drawn independently from an exponential distribution with parameter that is equal to the inverse of the degree of the source node with possibly a xed o set:\nw(u,v) \u223c Exp[1/|\u0393(u)|] + \u2206 .\nis achieves the e ect that edges from nodes with larger degrees are in expectation longer and therefore less likely to participate in the shortest paths. e xed o set that is added to edge lengths which allows as to control the balance between the number of hops and the weighted path length.\nWe comment that our experiments did not exploit the full power of the rich class of distance-based model proposed in [11]. Our evaluation was limited to Nearest-seed and exponentially distributed edge lengths with parameter equal to the inverse degree. e only hyperparameter we varied is the o set \u2206."}, {"heading": "2.3 Bootstrapping Wrapper", "text": "e self-training framework has many variants [1, 41]. e bootstrapping wrapper takes as input a base algorithmA and a set S of labeled seed nodes. In each step the algorithm A applied to S and returns a set of learned labels label for all nodes not in S together with prediction margins margin. e wrapper then augments the set S with new nodes. In our experiments we used a very basic formulation, shown as Algorithm 4. We x the number of new seed nodes from each class to be selected at each step to be proportional to the respective frequency \u03c0i of the class i in the data, using a proportion parameter r . More precisely, at each step t , we consider\n, , Eliav Buchnik and Edith Cohen\nAlgorithm 3: Nearest Seed [11] Input: G = (V , E), distributionW of edge lengths, provided classes\nlabel[i] \u2208 [L] for nodes i \u2264 n` // Initialize:\nforeach i > n` do c i \u2190 0 // counter for nearest-seed label\n// Iterate:\nrepeat Draw edge lengths w \u223c W foreach i > n` do // Apply a single-source shortest path algorithm on G = (V , E, w ) from seed nodes [n` ] to compute the closest seed to each node i\nnearestS[i] \u2190 arg minj\u2264n` di j foreach i > n` do // Label of the nearest seed\nci, label[nearestS[i ]] + +\nuntil T times // Finalize:\nforeach i > n` do // Label points by largest entry label[i] \u2190 arg maxj ci j return label\nfor each class i , the set of all non-seed nodes with learned labels label(t )[j] (j < S) in the class i . We then take the r\u03c0i nodes with highest-margin learned labels as new seeds added to S . If there are fewer than r\u03c0i nodes with learned label i , we take all these nodes to be new seeds. We expect quality to decrease with r but also that the number of steps needed to maximize the bootstrapped performance to decrease with r . e algorithm terminates when all nodes become seed nodes but each step t provides a full set of learned labels label(t ) for all nodes. e precision may rst increase with the step t but then might decrease and we therefore use cross validation or a separate validation set to determine which set of learned labels label(t ) to use. Validation can also be used to stop the algorithm when precision starts dropping. e parameters we use here are r , which trades o computation and quality and the hyper/parameters of the base algorithm A.\nAlgorithm 4: Basic Bootstrapping Wrapper Input: Seed set S \u2282 V with labels label : S \u2208 [L], r , frequencies \u03c0i\nfor i \u2208 [L], Base algorithm A that given S and label : S augments label to V \\ S and provides margin : V \\ S .\nt \u2190 0 repeat // Main iteration\nApply A to seeds S and label : S to assign label : V \\ S label(t ) \u2190 label // Remember step t predictions t++ foreach class i \u2208 L do\nC \u2190 {j \u2208 V \\ S such that label[j] == i } Place in S the min{ |C |, r\u03c0i } highest margin nodes in C\nuntil S == V // Finalize: return label(t ) that is best on validation set"}, {"heading": "3 DATASETS AND EXPERIMENTAL SETUP", "text": "To facilitate comparison, we use the benchmark dataset and seed set combinations used in prior work [40]. In our detailed experiments, we use additional data sets, multiple seed sets, and synthetic data sets.\nWe limited our evaluation to data sets that are multi-class but not multi-label, meaning that each node is a member of exactly one class. We note that the base and bootstrapped algorithms we use can naturally be extended to a multi-label se ing but there are di erent mechanisms to do so [11, 20] that may or may not assume that the number of classes of each node is provided. Moreover, some of the algorithms we compare with [24] do not have multi-label variants. We therefore opted to remove this variability by only considering multi-class.\nTable 1 summarizes the statistics of the datasets we used. For each dataset we report the number of nodes, edges and classes. We also list the labeling rate, which is the fraction of the nodes we used as seed labels. e data sets include three citation networks: Cora, Pubmed, and Citeseer from [36], one Knowledge graph (entity classi cation) dataset (NELL) preprocessed by [40] from [7], and the YouTube group membership data set from [28]. We preprocessed the YouTube data by removing groups with less than 500 members and not using for training or testing the users that were members of more than one group. is le us with 14355 labeled nodes to use as seed/validations/test sets.\nOur seed set selection followed [40]. For the citation networks, the seed sets contain 20 nodes randomly selected from each class. For the NELL data, with 210 classes, the seeds selection was proportional to class size with labeling rates 0.1, 0.01 and 0.001. For the YouTube data we used 50 seeds from each of the 14 classes.\ne citation and knowledge graph datasets have associated bagof-words node features which can be used to enhance prediction quality. We report experiments that use these node features in Section 7.\nWe also use synthetic data generated using the planted partition (stochastic block) random graph model [15]. ese random graphs are speci ed by a number L of equal-size classes, the total number of nodes, and two parameters q < p \u2208 [0, 1]. e graph is generated by instantiating each intra-class edge independently with probability p and each inter-class edge independently with probability q. e sets of parameters we used to generate graphs are listed in Table 2. Our seed sets had an equal number of randomly selected nodes from each class.\nWe used our own Python implementation of the three base linear di usions discussed in Section 3: Label propagation (LP) [43] (Algorithm 1) Normalized Laplacian LP [42] (Algorithm 2)\nBootstrapped Graph Di usions: Exposing the Power of Nonlinearity , ,\nand Nearest Seed [11] (Algorithm 3) and also the bootstrapping wrapper (Algorithm 4).\nOur experimental study has multiple aims. e rst is to understand the a ainable precision by the di erent algorithms, base and bootstrapped, and compared it to baseline methods. For this purpose we used a wide range of hyperparameters with a validation set to prevent over ing. e second is to perform a parameter study of the bootstrapped methods. e third focuses on scalability and considers results within a speci ed computation budget.\nWe applied the following hyper/parameters. We run all bootstrapped algorithms with r \u2208 {0.02, 0.03, 0.04, 0.1, 0.15, 0.2, 0.25, 0.3} as the fraction of nodes selected as seeds in each step (see Algorithm 4). For bootstrapped LP and bootstrapped normalized Laplacian LP we used {10, 20, 40, 100} iterations of the base LP algorithm in each step. For the nearest-seed we used {25, 75, 100, 400} iterations in each step. For the normalized Laplacian we used return probabilities {0.0001, 0.01, 0.05, 0.1, 0.2, 0.5} and for the nearestseed we used\u2206 \u2208 {1, 10, 50, 100}. In retrospect, all our o set choices for nearest-seed had similar performance results. e normalized Laplacian LP consistently performed best with return probabilities in the range {0.1, 0.2}. As for the base algorithms performance. For LP we used a validation set to select the iteration to use (as discussed in Section 2) whereas with the normalized Laplacian LP and nearest-seed we took the last iteration."}, {"heading": "4 RESULTS ON BENCHMARK DATASETS", "text": "In this set of experiments we follow the benchmark seed set and test set selection of [24, 40] with the properties as listed in Table 1.\ne provided test sets for the citation networks included exactly 1000 randomly selected nodes. e tests sets for the NELL data were slightly smaller. Our evaluation in this section is intended to study the a ainable quality by the di erent methods and we therefore use a generous range of hyperparameters. An important issue was that a separate validation set was not available with the benchmark data and moreover, because of unknown mappings, we could not produce one from the original raw data without overlapping with the provided seed and test sets. To facilitate a fair comparison that provides learned labels for all test set nodes without over ing (for all methods with hyperparameters), we did the following: We randomly partitioned each provided test set to two equal parts A,B and performed two sets of executions: In one set A was used as a validation set for hyperparameter selection and B for testing and vice versa.\ne results are reported in Table 3 for the three linear di usion methods, their bootstrapped variants, and the following baseline methods:\n\u2022 node2vec [20]: We used the published Python code of [20] (which builds on [27, 32]) to compute an embedding. We then applied logistic regression trained on the embeddings\nand labels of the seed to learn labels for other nodes. e method uses two hyperparameters (p,q) to tradeo the depth and breadth components of the random walks. We used all combinations of the values {0.25, 0.5, 1.0, 2.0, 4.0} forp,q and set other hyperparameters (10 epochs, 10 walks, walk length 80, embedding dimension 128) as in [20] . We use the same validation/test splits to prevent over ing. \u2022 DeepWalk [32]: e parameter se ing p = q = 1.0 with node2vec implements DeepWalk. We list the results we obtained in this implementation. For reference, we also list the results as reported in [40] using a di erent implementation. \u2022 Planetoid-G [40]. Planetoid uses embeddings obtained using a multi-layer neural network, building on the graph relations captured by DeepWalk. Planetoid-G is the (transductive) variant that does not use the node features. We report the results as listed in [40]. \u2022 Transductive support vector machines (TSVM) [22], for which we list the results reported in [40] when available \u2013 due to scalability issues. \u2022 Graph convolutional networks (GCN): Kipf and Welling [24] used the benchmark data and seed sets of [40] but only reported results with the use of node features. We only list their results on the NELL dataset, where node features did not enhance performance according to [40].\nWe can see that the bootstrapped variants consistently improve over the base methods. We also observe that (except in one case), the best result over all methods, including the baselines, is obtained by a bootstrapped di usion method. We can also observe that the base linear methods, the classic spectral and the recent social, perform competitively to state of the art and the bootstrapping elevates performance to above it. We note that the results for LP reported in [40] used a di erent implementation (Junto with a xed number of 100 iterations)."}, {"heading": "5 RANDOM SPLITS EXPERIMENTS", "text": "We performed additional experiments in order to obtain a more robust picture on the bootstrapping performance gain. For each data set we performed multiple repetitions using di erent random splits of the data to seed, test, and validation sets and averaged the results.\nWe used the ve real-word datasets listed in Table 1. For the citation networks (Citeseer, Cora, and Pubmed) we used the raw data that included the full node labeling. We used labeling rates as listed, with balanced seed sets for the citation networks and YouTube. For the NELL graph, produced by [40], we only used the provided 2000 labeled nodes and selected a seed set with 0.001 labeling rate. We used three planted partition parameters as listed in Table 2. A new planted partition graph was generated according to these parameters for each repetition. With all experiments we used a separate validation set of size 500 and used all remaining labeled nodes as a test set. e average precision results are reported in Table 4. e table also lists the number of repetitions we used for each dataset.\nWe also report results for the node2vec and deepwalk methods, when the computation of the embeddings was feasible for us, using\n, , Eliav Buchnik and Edith Cohen\nthe in-memory Python implementation [20]. Results are provided for the citation networks and NELL datasets. We had to exclude the much larger YouTube data set and also the planted partition, since we generated a new graph for each repetition which would require a new embedding (a single embedding su ced for all repetitions on the real-word networks). Note that node2vec was the top performer among the baseline methods on the benchmark data so it provides a good reference point.\nWe can see that the results con rm our observations from the benchmark data experiments: e bootstrapped methods consistently improve over the respective base methods and also consistently achieve the best results. e best performer is most o en the bootstrapped normalized laplacian LP. We suspect it is due to the exibility provided through the return probability hyper parameter which controls the depth versus breadth of the propagation. Such exibility is provided in part by the iterations parameter of the basic label propagation method and also by the hyperparameters of the distance di usion models. For the la er, we did not use incorporate this exibility in our experiments. We observe that the two spectral methods perform similarity whereas the social method seems to supplement the spectral methods and perform well on di erent data sets.\nOur experiments with the planted partition random graphs also demonstrate a clear bootstrapping gain in quality with respect to the baselines. is generative model is used extensively in experiments and analysis of clustering algorithms. e consistent precision gain\nby bootstrapping suggests wider applicability and seeking be er theoretical understanding of the limits of the approach.\nFinally, we note that we tested the bootstrapping wrapper with two other implementations of Label Propagation including: e sklearn Python library and Junto [43]1 and observed similar performance gains over the base methods."}, {"heading": "6 BOOTSTRAPPING PARAMETER STUDY", "text": "In this section we take a closer look and study how the bootstrapping quality gain depends on properties of the data and parameters."}, {"heading": "6.1 Labeling rate", "text": "We study the precision of both the base and bootstrapped algorithms as a function of the labeling rate. Here we used 10\u00d7 random splits of the data sets. For the citation networks and planted partition graphs, we varied the labeling rate while maintaining balanced seed sets that have the same number of seeds from each class. Representative results showing the precision as a function of the labeling rate for selected data sets are visualized in Figure 1. Figure 2 shows the gain in precision due to bootstrapping over the respective base method, as a function of the labeling rate.\ne plots show that, as expected, precision of all methods increases with the labeling rate and that bootstrapping consistently improves performance. We can see that across methods, the gain in precision due to bootstrapping is smaller at the extremes, when the\n1h ps://github.com/parthatalukdar/junto\nBootstrapped Graph Di usions: Exposing the Power of Nonlinearity , ,\nlabeling rate and precision are very low or very high. e largest gains are obtained in the middle range."}, {"heading": "6.2 Seed set augmentations", "text": "We study the gain in precision as we sweep the bootstrapping parameter r , which determines the fraction of nodes that are set as seeds in each bootstrapping step (see Algorithm 4). Figure 3 shows the precision gain as a function of r (in percent) for selected data sets. We can see that as expected the gain decreases with r but that we can obtain signi cant gains also with relatively large values of r ."}, {"heading": "6.3 Number of bootstrapping steps", "text": "We study the precision as a function of the number of bootstrapping steps performed. In our experiments we used the performance on a validation set to choose the step which provides the nal learned labels. Generally, we expect precision to initially improve as the easier-to-predict nodes are added to the seed set and eventually to stabilize or decrease.\nFigure 4 shows the precision for each step on representative data sets. In this set of experiments we xed all other parameters of each algorithm as indicated in the legends of the gure and used a xed value r = 0.03 for the fraction of nodes that become new\nseeds in each step. e results are averaged over 10 random splits of the data. e Figure shows di erent pa erns but all are unimodal\n, , Eliav Buchnik and Edith Cohen\nwhich means they allow us to incorporate a stopping criteria for the bootstrapping algorithm."}, {"heading": "6.4 Scalability", "text": "Optimizing quality is important when labeling is costly. O en, however, on very large graphs, scalability is critical. e computation cost of the methods we considered, social and spectral, bootstrapped or not, depends on the total number of iterations performed.\nWith label propagation, in each iteration for each node we compute an average over its neighbors. e averaging for di erent nodes in the same iteration are independent and can be distributed or parallelized. But the iterations sequentially depend on each other. e computation of each iteration involves a linear number of edge traversals and is highly suitable for Pregel-like [26] distributed graph processing platforms.\nWith distance di usion, each iteration is equivalent to a singlesource shortest-paths computation. e number of edge traversals performed is also linear. e iterations here are independent, and can be performed concurrently, each providing independent samples from a distribution. e shortest-path search performed in each iteration, however, has concurrency that depends on the number of hops in the shortest-paths. is computation can also be performed e ciently on Pregel [26] by essentially performing all iterations (di erent sets of hash-speci ed edge lengths) together. With bootstrapping, the steps must be sequential and in each step we run the base algorithm with multiple iterations.\nIn this set of experiments we study the precision we obtain using a xed total number of iterations, for di erent sets of bootstrapping\nparameters. e plots in Figure 5 show precision as a function of the number of iterations performed in each bootstrapping step. Each graph corresponds to a xed budget of iterations (between 25 and 400 iterations). e number of iterations performed per step varies between 5 and the full budget. When all iterations are performed in one step, that is, when the number of iterations per step is equal to the total budget, we have the precision of the non-bootstrapped base algorithm. When we partition the iterations budget to multiple steps we reduce the e ectiveness of the base algorithm in each step but can leverage the power of bootstrapping.\nRecall that the normalized Laplacian LP and nearest-seed improve with more iterations per step whereas LP may not. We can see that the non-bootstrapped LP algorithm degrades with more iterations (recall that in other experiments we treated the number of iterations with LP it as a hyperparameter). e plot for the Pubmed dataset show that with all budgets, quality picks at 10 iterations per step. With normalized Laplacian LP and nearest-seed (which only improve with iterations), we see that bootstrapped methods with 25 total iterations outperforms the non-bootstrapped algorithms with many more iterations.\nFinally, we consider the parameter r , which determines the fraction of nodes that are instantiated as new seeds in each step. Generally bootstrapping performance improves with smaller values of the parameter r . But with limited iteration budget, and limited number of steps, very low values limit the progress for the bootstrapping. e bo om right plot in Figure 5 shows a sweet spot at r = 0.1.\nBootstrapped Graph Di usions: Exposing the Power of Nonlinearity , ,"}, {"heading": "7 FEATURE DIFFUSION", "text": "In the previous sections we focused on methods that learn labels using only the graph structure: e provided labels of seed nodes are \u201cdi used\u201d along graph edges to obtain so labels for all nodes that are then used for prediction. When we have more information, in the form of node feature vectors f i for all nodes i \u2264 n` +nu , we can use it for learning. e simplest method, which does not use the graph structure, is to train a supervised classi er on the features and labels of seed nodes ( f\u0303 i ,yi ) i \u2264 n` . We can instead use the graph structure to obtain di used feature vectors f\u0303 i . e di used vectors are a smoothed version of the raw vectors that also re ect features of related nodes, where relation is according to the base di usion process. Note that the di used vectors f\u0303 i do not depend on the set of seed nodes or their labels. A supervised classi erC can then be trained on the di used features and labels of seed nodes: ( f\u0303 i ,yi ) i \u2264 n` . When the classi er provides a prediction margin with each classi cation C( f\u0303 i ) for i > n` , it can be bootstrapped (see algorithm 4).\nWe evaluated two di usion methods. Feature Propagation (FP), in Algorithm 5, that uses the di usion rule of the label propagation method of [43] (Algorithm 1), and Normalized Laplacian FP, in Algorithm 6, that uses the di usion rule of [42] (Algorithm 2). We note that our linear feature di usions can be viewed as a toneddown GCN [2, 24], without the backprop training, and non-linear aggregations.\nAlgorithm 5: Feature Propagation (FP) Input: A nity matrixW . Node feature vectors f i for i \u2264 n` + nu // Initialize:\n\u2200i, Dii \u2190 \u2211jWi j // diagonal degree matrix F \u2190 (f 1, . . . , f n`+nu ) // Matrix of feature vectors F\u0303 \u2190 F // Diffused features matrix // Diffuse:\nforeach t = 1, . . . , T do F\u0303 \u2190 D\u22121W F\u0303\n// Finalize:\nreturn (f\u0303 1, . . . , f\u0303 n`+nu ) \u2190 F\u0303 // Diffused feature vectors\nAlgorithm 6: Normalized Laplacian FP Input: A nity matrixW . Node feature vectors f i for i \u2264 n` + nu .\nParameter \u03b1 \u2208 (0, 1) // Initialize:\n\u2200i, Dii \u2190 \u2211jWi j // diagonal degree matrix F \u2190 (f 1, . . . , f n`+nu ) // Matrix of feature vectors F\u0303 \u2190 F // Diffused features matrix A\u2190 D\u22121/2W D\u22121/2 // Normalized adjacency matrix // Diffuse:\nforeach t = 0, . . . , T do F\u0303 \u2190 (1 \u2212 \u03b1 )AF\u0303 + \u03b1 F\nreturn (f\u0303 1, . . . , f\u0303 n`+nu ) \u2190 F\u0303 // Diffused feature vectors"}, {"heading": "7.1 Experiments settings", "text": "We used the three citation networks dataset (Citeseer, Cora, and Pubmed) listed in Table 1. We use the methodology of Section 3 for the selection of training, test, and validations sets. We use the benchmark xed seed sets used in prior work [24, 40], to facilitate comparison, and random splits for robustness. e citation networks contain a bag-of-words representation for each document which, following [24, 40], we treat as a feature vector. e vectors are encoded using 0/1 which indicates the absence/presence of the corresponding term from the dictionary. e dictionaries of Citeseer, Cora and Pubmed contain 3703, 1433 and 500 unique words respectively. For classi cation from (original and di used) feature vectors we used one-versus-all logistic regression with the Python sklearn library implementation 2.\nWe evaluate the classi cation quality when using the raw feature vectors f i and when using di used feature vectors f\u0303 i obtained using Algorithms 5 and Algorithm 6. For each base algorithm we also apply the bootstrapping wrapper. We used a range of hyper/parameters with a validation set to prevent over ing: e bootstrapping wrapper was used with r \u2208 {0.05, 0.1, 0.2} as the fraction of new nodes selected as seeds in each step (see Algorithm 4). For FP and normalized Laplacian FP we used {2, 5, 10, 20, 40, 100, 200} propagation iterations to compute the di used feature vectors. For normalized Laplacian FP we used \u03b1 \u2208 {0.01, 0.05, 0.1, 0.2}. We comment that the best results across data sets were obtained with 10 iterations and with \u03b1 = 0.2."}, {"heading": "7.2 Results on benchmark datasets", "text": "Results on benchmark datasets are reported in Table 5. e table also lists for reference the results obtained without using node features by bootstrapped normalized Laplacian LP (experiments in Section 4). For comparison, we also list the quality reported by GCN [24] and Planetoid [40] (best variant with node features).\nWe observe the following: First, the quality of learning with di used feature vectors is signi cantly be er than with the raw feature vectors, with average improvement of about 12%. Hence in these data sets the use of the graph structure and the particular way it was used were important. Second, the normalized Laplacian FP was more e ective than basic FP. is agrees with our observations with the label propagation experiments. ird, bootstrapping consistently improved performance on two of the data sets (Citeseer and Cora). ere was li le or no improvement on Pubmed, but on that data sets bootstrapped label propagation (that did not used the node features) was the near-best performer. Fourth, the bootstrapped version of normalized Laplacian FP improves over the state of the art results of GCN [24] on Citeseer and Cora. On Pubmed GCN was only slightly be er than our bootstrapped label propagation."}, {"heading": "7.3 Results on random splits", "text": "In this set of experiments, for each data set, we generated multiple random splits of the nodes to seed, test, and validation sets and averaged the results. Our results are reported in Table 6. For reference, we also list the results using label propagation (without the use of node features) that we reported in Section 5 and the results 2h p://scikit-learn.org/stable/about.html\n, , Eliav Buchnik and Edith Cohen\nreported on similar random splits using GCNs [24]. e results add robustness to our observations from the benchmark experiments: e use of di used features signi cantly improves quality, the normalized Laplacian FP consistently achieves the best results on Citeseer and Cora and is very close (within error margins) to the results reported by [24] on Pubmed."}, {"heading": "8 CONCLUSION", "text": "We studied the application of self-training, which is perhaps the most basic form of introducing non-linearity, to SSL methods based on linear graph di usions. We observed that the resulting bootstrapped di usions ubiquitously improved labeling quality over the respective base methods on a variety of real-world and synthetic data sets. Moreover, we obtain state-of-the-art quality, previously achieved by more complex methods, while retaining the high scalability of the base methods.\nOur results are a proof of concept that uses the simplest base algorithms and bootstrapping wrapper. Some natural extensions include ne tuning of the wrapper together with base algorithms that provide more precise con dence scores and the use of a richer set of base algorithms.\nOn a nal note, we recall that spectral and social graph di usions are an important tool in graph mining: ey are the basis of centrality, in uence, and similarity measures of a node or sets of\nnodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37]. Our work suggests that substantial gains in quality might be possible by using bootstrapped di usions as an alternative to classic ones in these wider contexts. We hope to pursue this in future work."}, {"heading": "ACKNOWLEDGMENTS", "text": "e authors would like to thank Aditya Grover, the author of node2vec [20] and Zhilin Yang, the author of Planetoid [40] for prompt and helpful answers to our questions on their work and implementations, and to Fernando Pereira for his advice. is research is partially supported by the Israel Science Foundation (Grant No. 1841/14)."}], "references": [{"title": "Understanding the Yarowsky algorithm", "author": ["S. Abney"], "venue": "Comput. Linguist.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Di\u0082usion-convolutional neural networks", "author": ["J. Atwood", "D. Towsley"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A mathematical model for small group structures", "author": ["A. Bavelas"], "venue": "Human Organization,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1948}, {"title": "\u008ce formation of networks with transfers among players", "author": ["F. Bloch", "M.O. Jackson"], "venue": "Journal of Economic \u008aeory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Learning from labeled and unlabeled data using graph mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Semi-supervised learning using randomized mincuts", "author": ["A. Blum", "J. La\u0082erty", "M.R. Rwebangira", "R. Reddy"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Toward an architecture for never-ending language learning", "author": ["J. Carlson", "A. Be\u008aeridge", "B. Kisiel", "B. Se\u008ales", "E.R. Hruschka", "Jr.", "T.M. Mitchell"], "venue": "In AAAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Semi-supervised learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Spectral Graph \u008aeory", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Size-estimation framework with applications to transitive closure and reachability", "author": ["E. Cohen"], "venue": "J. Comput. System Sci.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Semi-supervised learning on graphs through reach and distance di\u0082usion", "author": ["E. Cohen"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Scalable similarity estimation in social networks: Closeness, node labels, and random edge lengths", "author": ["E. Cohen", "D. Delling", "F. Fuchs", "A. Goldberg", "M. Goldszmidt", "R. Werneck"], "venue": "In COSN. ACM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Distance-based in\u0083uence in networks: Computation and maximization", "author": ["E. Cohen", "D. Delling", "T. Pajor", "R.F. Werneck"], "venue": "Technical Report cs.SI/1410.6976,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Spatially-decaying aggregation over a network: Model and algorithms", "author": ["E. Cohen", "H. Kaplan"], "venue": "J. Comput. System Sci.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R.M. Karp"], "venue": "Random Struct. Algorithms,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Convolutional neural networks on graphs with fast localized spectral \u0080ltering", "author": ["M. De\u0082errard", "X. Bresson", "P. Vandergheynst"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Scalable in\u0083uence estimation in continuous-time di\u0082usion networks", "author": ["N. Du", "L. Song", "M. Gomez-Rodriguez", "H. Zha"], "venue": "In NIPS", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Centrality in social networks: Conceptual clari\u0080cation", "author": ["L.C. Freeman"], "venue": "Social Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1979}, {"title": "Inferring networks of di\u0082usion and in\u0083uence", "author": ["M. Gomez-Rodriguez", "J. Leskovec", "A. Krause"], "venue": "In KDD,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "node2vec: Scalable feature learning for networks", "author": ["A. Grover", "J. Leskovec"], "venue": "In KDD. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Deep convolutional networks on graphstructured data", "author": ["M. Hena", "J. Bruna", "Y. LeCun"], "venue": "CoRR, abs/1506.05163,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Transductive inference for text classi\u0080cation using support vector machines", "author": ["T. Joachims"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Maximizing the spread of in\u0083uence through a social network", "author": ["D. Kempe", "J.M. Kleinberg", "\u00c9. Tardos"], "venue": "In KDD. ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Semi-supervised classi\u0080cation with graph convolutional networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "\u008ce multirank bootstrap algorithm: Self-supervised political blog classi\u0080cation and ranking using semi-supervised link classi\u0080cation", "author": ["F. Lin", "W.W. Cohen"], "venue": "In ICWSM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Pregel: a system for large-scale graph processing", "author": ["G.M.H. Malewicz", "Austern", "A.J.C Bik", "J.C. Dehnert", "I. Horn", "N. Leiser", "G. Czajkowski"], "venue": "In SIGMOD. ACM,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Bha\u008aacharjee. Measurement and Analysis of Online Social Networks", "author": ["A. Mislove", "M. Marcon", "K.P. Gummadi", "P. Druschel"], "venue": "In IMC,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Structural neighborhood based classi\u0080cation of nodes in a network", "author": ["S. Nandanwar", "N.N. Murty"], "venue": "In KDD", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Node centrality in weighted networks: Generalizing degree and shortest paths", "author": ["T. Opsahl", "F. Agneessens", "J. Skvoretz"], "venue": "Social Networks,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "\u008ce pagerank citation ranking: Bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "Technical report, Stanford InfoLab,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "In KDD. ACM,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Large-scale semi-supervised learning using streaming approximation", "author": ["S. Ravi", "Q. Diao"], "venue": "In AISTATS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "\u008ce centrality index of a graph", "author": ["G. Sabidussi"], "venue": "Psychometrika, 31(4):581\u2013603,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1966}, {"title": "Probability of error of some adaptive pa\u008aern-recognition machines", "author": ["H.J. Scudder"], "venue": "IEEE Transactions on Information \u008aeory,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1965}, {"title": "Collective classi\u0080cation in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning", "author": ["D.A. Spielman", "S-H Teng"], "venue": "SIAM J. Comput.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Graph-based semi-supervised learning", "author": ["A. Subramanya", "P.P. Talukdar"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Bootstrapping via graph propagation", "author": ["M. Whitney", "A. Sarkar"], "venue": "In ACL,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Revisiting semi-supervised learning with graph embeddings", "author": ["Z. Yang", "W.W. Cohen", "R. Salakhutdinov"], "venue": "In ICML. JMLR.org,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["D. Yarowsky"], "venue": "In ACL,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1995}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2004}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["X. Zhu", "Z. Ghahramani"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "Semi-supervised learning using Gaussian \u0080elds and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. La\u0082ery"], "venue": "In ICML,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2003}], "referenceMentions": [{"referenceID": 7, "context": "the surveys [8, 38].", "startOffset": 12, "endOffset": 19}, {"referenceID": 37, "context": "the surveys [8, 38].", "startOffset": 12, "endOffset": 19}, {"referenceID": 8, "context": "Most popular SSL methods can be interpreted through underlying spectral di\u0082usions [9], utilizing the graph Laplacian, graph cuts [5, 6], and random walks.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Most popular SSL methods can be interpreted through underlying spectral di\u0082usions [9], utilizing the graph Laplacian, graph cuts [5, 6], and random walks.", "startOffset": 129, "endOffset": 135}, {"referenceID": 5, "context": "Most popular SSL methods can be interpreted through underlying spectral di\u0082usions [9], utilizing the graph Laplacian, graph cuts [5, 6], and random walks.", "startOffset": 129, "endOffset": 135}, {"referenceID": 43, "context": "\u008cey include label propagation [44], label propagation using the normalized graph Laplacian [25, 42], and many variations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "\u008cey include label propagation [44], label propagation using the normalized graph Laplacian [25, 42], and many variations.", "startOffset": 91, "endOffset": 99}, {"referenceID": 41, "context": "\u008cey include label propagation [44], label propagation using the normalized graph Laplacian [25, 42], and many variations.", "startOffset": 91, "endOffset": 99}, {"referenceID": 32, "context": "\u008ce algorithms are applied successfully to massive graphs with billions of edges [33] using highly distributed platforms [26].", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "\u008ce algorithms are applied successfully to massive graphs with billions of edges [33] using highly distributed platforms [26].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 3, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 13, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 17, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 29, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 33, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 12, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 160, "endOffset": 176}, {"referenceID": 16, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 160, "endOffset": 176}, {"referenceID": 18, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 160, "endOffset": 176}, {"referenceID": 22, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 160, "endOffset": 176}, {"referenceID": 11, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 193, "endOffset": 197}, {"referenceID": 11, "context": "A powerful extension de\u0080nes a generative model from a graph by randomizing the presence (with reach di\u0082usion) or the length (with distance di\u0082usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations.", "startOffset": 158, "endOffset": 178}, {"referenceID": 12, "context": "A powerful extension de\u0080nes a generative model from a graph by randomizing the presence (with reach di\u0082usion) or the length (with distance di\u0082usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations.", "startOffset": 158, "endOffset": 178}, {"referenceID": 16, "context": "A powerful extension de\u0080nes a generative model from a graph by randomizing the presence (with reach di\u0082usion) or the length (with distance di\u0082usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations.", "startOffset": 158, "endOffset": 178}, {"referenceID": 18, "context": "A powerful extension de\u0080nes a generative model from a graph by randomizing the presence (with reach di\u0082usion) or the length (with distance di\u0082usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations.", "startOffset": 158, "endOffset": 178}, {"referenceID": 22, "context": "A powerful extension de\u0080nes a generative model from a graph by randomizing the presence (with reach di\u0082usion) or the length (with distance di\u0082usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations.", "startOffset": 158, "endOffset": 178}, {"referenceID": 22, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 117, "endOffset": 133}, {"referenceID": 12, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 117, "endOffset": 133}, {"referenceID": 16, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 117, "endOffset": 133}, {"referenceID": 18, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 117, "endOffset": 133}, {"referenceID": 10, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 162, "endOffset": 166}, {"referenceID": 9, "context": "\u008ce use of distance or reachability sketching based on [10] allows for highly scalable label learning also over the sketched a\u0081nity matrix.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "Both spectral and social di\u0082usion based SSL models scale well even with a large number of labels, using heavy hi\u008aer sketches with label propagation [33] or naturally with social di\u0082usion using sketches.", "startOffset": 148, "endOffset": 152}, {"referenceID": 31, "context": "In particular, DeepWalk [32] applied the hugely successful word embedding framework of [27] to embed the graph nodes in a way that preserves the a\u0081nity relation de\u0080ned by co-occurrence frequencies of pairs in short random walks: A so\u0089max applied to inner products of embeddings approximates the frequency of the pair.", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "In particular, DeepWalk [32] applied the hugely successful word embedding framework of [27] to embed the graph nodes in a way that preserves the a\u0081nity relation de\u0080ned by co-occurrence frequencies of pairs in short random walks: A so\u0089max applied to inner products of embeddings approximates the frequency of the pair.", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "Node2vec [20] re\u0080ned the approach using hyperparameters that tune the depth and breadth of the random", "startOffset": 9, "endOffset": 13}, {"referenceID": 39, "context": "Another method, Planetoid, used a multi-layer neural network instead of a single so\u0089max layer [40].", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": "Another successful proposal are Graph convolutional networks (GCN) [2, 16, 21, 24], which are neural networks with layers that follow the graph adjacency structure.", "startOffset": 67, "endOffset": 82}, {"referenceID": 15, "context": "Another successful proposal are Graph convolutional networks (GCN) [2, 16, 21, 24], which are neural networks with layers that follow the graph adjacency structure.", "startOffset": 67, "endOffset": 82}, {"referenceID": 20, "context": "Another successful proposal are Graph convolutional networks (GCN) [2, 16, 21, 24], which are neural networks with layers that follow the graph adjacency structure.", "startOffset": 67, "endOffset": 82}, {"referenceID": 23, "context": "Another successful proposal are Graph convolutional networks (GCN) [2, 16, 21, 24], which are neural networks with layers that follow the graph adjacency structure.", "startOffset": 67, "endOffset": 82}, {"referenceID": 28, "context": "\u008cis abundance of recent work introduced many new components and o\u0089en at the same time: Low-rank embeddings, non-linear propagations, learning of weights of hidden layers (GCNs), learning of node weights [29].", "startOffset": 203, "endOffset": 207}, {"referenceID": 34, "context": "Self-training is arguably the earliest approach to SSL, dating back \u0080ve decades to Scudder [35], and extensively studied by the NLP community [1, 39, 41].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "Self-training is arguably the earliest approach to SSL, dating back \u0080ve decades to Scudder [35], and extensively studied by the NLP community [1, 39, 41].", "startOffset": 142, "endOffset": 153}, {"referenceID": 38, "context": "Self-training is arguably the earliest approach to SSL, dating back \u0080ve decades to Scudder [35], and extensively studied by the NLP community [1, 39, 41].", "startOffset": 142, "endOffset": 153}, {"referenceID": 40, "context": "Self-training is arguably the earliest approach to SSL, dating back \u0080ve decades to Scudder [35], and extensively studied by the NLP community [1, 39, 41].", "startOffset": 142, "endOffset": 153}, {"referenceID": 43, "context": "We use classic Label propagation [44], Label propagation using the normalized Laplacian [42], and nearest-seed, which is the simplest distance di\u0082usion model [11].", "startOffset": 33, "endOffset": 37}, {"referenceID": 41, "context": "We use classic Label propagation [44], Label propagation using the normalized Laplacian [42], and nearest-seed, which is the simplest distance di\u0082usion model [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "We use classic Label propagation [44], Label propagation using the normalized Laplacian [42], and nearest-seed, which is the simplest distance di\u0082usion model [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 23, "context": "We apply the di\u0082erent methods to benchmark data and seed sets used and made available by previous work [24, 40].", "startOffset": 103, "endOffset": 111}, {"referenceID": 39, "context": "We apply the di\u0082erent methods to benchmark data and seed sets used and made available by previous work [24, 40].", "startOffset": 103, "endOffset": 111}, {"referenceID": 31, "context": "We compare the quality of the learned labels to state of the art baselines, including DeepWalk [32], node2vec [20], Planetoid [40], and GCNs [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 19, "context": "We compare the quality of the learned labels to state of the art baselines, including DeepWalk [32], node2vec [20], Planetoid [40], and GCNs [24].", "startOffset": 110, "endOffset": 114}, {"referenceID": 39, "context": "We compare the quality of the learned labels to state of the art baselines, including DeepWalk [32], node2vec [20], Planetoid [40], and GCNs [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 23, "context": "We compare the quality of the learned labels to state of the art baselines, including DeepWalk [32], node2vec [20], Planetoid [40], and GCNs [24].", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "We also perform more elaborate experiments on additional data and seed sets and on the well-studied planted partition (stochastic block) model [15] which is o\u0089en used to understand the performance of clustering and community detection algorithms.", "startOffset": 143, "endOffset": 147}, {"referenceID": 39, "context": "Furthermore, our results dominated those reported (with the use of node features) by the state of the art baselines Planetoid [40] and GCNs [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 23, "context": "Furthermore, our results dominated those reported (with the use of node features) by the state of the art baselines Planetoid [40] and GCNs [24].", "startOffset": 140, "endOffset": 144}, {"referenceID": 41, "context": "In particular, we discuss two textbook label propagations methods [42, 43] and a distance-di\u0082usion method [11].", "startOffset": 66, "endOffset": 74}, {"referenceID": 42, "context": "In particular, we discuss two textbook label propagations methods [42, 43] and a distance-di\u0082usion method [11].", "startOffset": 66, "endOffset": 74}, {"referenceID": 10, "context": "In particular, we discuss two textbook label propagations methods [42, 43] and a distance-di\u0082usion method [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 10, "context": "We note that o\u0089en higher quality learned labels are obtaining by training a learning algorithm on the so\u0089 label [11] but in our experiments here we used these common simple class predictions.", "startOffset": 112, "endOffset": 116}, {"referenceID": 42, "context": "Label propagation (LP) Zhu and Ghahramani [43].", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "Pseudocode is provided as Algorithm 1 (As presented in [8]).", "startOffset": 55, "endOffset": 58}, {"referenceID": 42, "context": "Algorithm 1: Label propagation (LP) [43]", "startOffset": 36, "endOffset": 40}, {"referenceID": 41, "context": "Normalized Laplacian label propagation Zhou et al [42].", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "\u008cis algorithm is related to Personalized Page Rank (PPR) and uses the normalized graph Laplacian [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 41, "context": "Algorithm 2: Normalized Laplacian LP [42]", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "We consider recently proposed SSL methods based on distance di\u0082usion [8].", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "Nearest-seed [8].", "startOffset": 13, "endOffset": 16}, {"referenceID": 10, "context": "Guided by [11] \u008ce lengths of edges are drawn independently from an exponential distribution with parameter that is equal to the inverse of the degree of the source node with possibly a \u0080xed o\u0082set:", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "We comment that our experiments did not exploit the full power of the rich class of distance-based model proposed in [11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "\u008ce self-training framework has many variants [1, 41].", "startOffset": 45, "endOffset": 52}, {"referenceID": 40, "context": "\u008ce self-training framework has many variants [1, 41].", "startOffset": 45, "endOffset": 52}, {"referenceID": 10, "context": "Algorithm 3: Nearest Seed [11]", "startOffset": 26, "endOffset": 30}, {"referenceID": 39, "context": "To facilitate comparison, we use the benchmark dataset and seed set combinations used in prior work [40].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "We note that the base and bootstrapped algorithms we use can naturally be extended to a multi-label se\u008aing but there are di\u0082erent mechanisms to do so [11, 20] that may or may not assume that the number of classes of each node is provided.", "startOffset": 150, "endOffset": 158}, {"referenceID": 19, "context": "We note that the base and bootstrapped algorithms we use can naturally be extended to a multi-label se\u008aing but there are di\u0082erent mechanisms to do so [11, 20] that may or may not assume that the number of classes of each node is provided.", "startOffset": 150, "endOffset": 158}, {"referenceID": 23, "context": "Moreover, some of the algorithms we compare with [24] do not have multi-label variants.", "startOffset": 49, "endOffset": 53}, {"referenceID": 35, "context": "\u008ce data sets include three citation networks: Cora, Pubmed, and Citeseer from [36], one Knowledge graph (entity classi\u0080cation) dataset (NELL) preprocessed by [40] from [7], and the YouTube group membership data set from [28].", "startOffset": 78, "endOffset": 82}, {"referenceID": 39, "context": "\u008ce data sets include three citation networks: Cora, Pubmed, and Citeseer from [36], one Knowledge graph (entity classi\u0080cation) dataset (NELL) preprocessed by [40] from [7], and the YouTube group membership data set from [28].", "startOffset": 158, "endOffset": 162}, {"referenceID": 6, "context": "\u008ce data sets include three citation networks: Cora, Pubmed, and Citeseer from [36], one Knowledge graph (entity classi\u0080cation) dataset (NELL) preprocessed by [40] from [7], and the YouTube group membership data set from [28].", "startOffset": 168, "endOffset": 171}, {"referenceID": 27, "context": "\u008ce data sets include three citation networks: Cora, Pubmed, and Citeseer from [36], one Knowledge graph (entity classi\u0080cation) dataset (NELL) preprocessed by [40] from [7], and the YouTube group membership data set from [28].", "startOffset": 220, "endOffset": 224}, {"referenceID": 39, "context": "Our seed set selection followed [40].", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "We also use synthetic data generated using the planted partition (stochastic block) random graph model [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "\u008cese random graphs are speci\u0080ed by a number L of equal-size classes, the total number of nodes, and two parameters q < p \u2208 [0, 1].", "startOffset": 123, "endOffset": 129}, {"referenceID": 42, "context": "We used our own Python implementation of the three base linear di\u0082usions discussed in Section 3: Label propagation (LP) [43] (Algorithm 1) Normalized Laplacian LP [42] (Algorithm 2)", "startOffset": 120, "endOffset": 124}, {"referenceID": 41, "context": "We used our own Python implementation of the three base linear di\u0082usions discussed in Section 3: Label propagation (LP) [43] (Algorithm 1) Normalized Laplacian LP [42] (Algorithm 2)", "startOffset": 163, "endOffset": 167}, {"referenceID": 10, "context": "and Nearest Seed [11] (Algorithm 3) and also the bootstrapping wrapper (Algorithm 4).", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": "In this set of experiments we follow the benchmark seed set and test set selection of [24, 40] with the properties as listed in Table 1.", "startOffset": 86, "endOffset": 94}, {"referenceID": 39, "context": "In this set of experiments we follow the benchmark seed set and test set selection of [24, 40] with the properties as listed in Table 1.", "startOffset": 86, "endOffset": 94}, {"referenceID": 19, "context": "\u2022 node2vec [20]: We used the published Python code of [20] (which builds on [27, 32]) to compute an embedding.", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "\u2022 node2vec [20]: We used the published Python code of [20] (which builds on [27, 32]) to compute an embedding.", "startOffset": 54, "endOffset": 58}, {"referenceID": 26, "context": "\u2022 node2vec [20]: We used the published Python code of [20] (which builds on [27, 32]) to compute an embedding.", "startOffset": 76, "endOffset": 84}, {"referenceID": 31, "context": "\u2022 node2vec [20]: We used the published Python code of [20] (which builds on [27, 32]) to compute an embedding.", "startOffset": 76, "endOffset": 84}, {"referenceID": 19, "context": "0} forp,q and set other hyperparameters (10 epochs, 10 walks, walk length 80, embedding dimension 128) as in [20] .", "startOffset": 109, "endOffset": 113}, {"referenceID": 31, "context": "\u2022 DeepWalk [32]: \u008ce parameter se\u008aing p = q = 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 39, "context": "For reference, we also list the results as reported in [40] using a di\u0082erent implementation.", "startOffset": 55, "endOffset": 59}, {"referenceID": 39, "context": "\u2022 Planetoid-G [40].", "startOffset": 14, "endOffset": 18}, {"referenceID": 39, "context": "We report the results as listed in [40].", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "\u2022 Transductive support vector machines (TSVM) [22], for which we list the results reported in [40] when available \u2013 due to scalability issues.", "startOffset": 46, "endOffset": 50}, {"referenceID": 39, "context": "\u2022 Transductive support vector machines (TSVM) [22], for which we list the results reported in [40] when available \u2013 due to scalability issues.", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "\u2022 Graph convolutional networks (GCN): Kipf and Welling [24] used the benchmark data and seed sets of [40] but only reported results with the use of node features.", "startOffset": 55, "endOffset": 59}, {"referenceID": 39, "context": "\u2022 Graph convolutional networks (GCN): Kipf and Welling [24] used the benchmark data and seed sets of [40] but only reported results with the use of node features.", "startOffset": 101, "endOffset": 105}, {"referenceID": 39, "context": "We only list their results on the NELL dataset, where node features did not enhance performance according to [40].", "startOffset": 109, "endOffset": 113}, {"referenceID": 39, "context": "We note that the results for LP reported in [40] used a di\u0082erent implementation (Junto with a \u0080xed number of 100 iterations).", "startOffset": 44, "endOffset": 48}, {"referenceID": 39, "context": "For the NELL graph, produced by [40], we only used the provided 2000 labeled nodes and selected a seed set with 0.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "TSVM [22] *[40] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "TSVM [22] *[40] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 31, "context": "DeepWalk [32] *[40] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 39, "context": "DeepWalk [32] *[40] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "DeepWalk [32] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "node2vec [20] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 39, "context": "Planetoid-G [40] *[40] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 39, "context": "Planetoid-G [40] *[40] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "Graph Conv Nets [24] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "Nearest-seed [11] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 42, "context": "Label Propagation[43] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "Norm Lap LP [42] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "DeepWalk [32] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "node2vec [20] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "Nearest-seed [11] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 42, "context": "Label Propagation[43] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "Norm Lap LP [42] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "the in-memory Python implementation [20].", "startOffset": 36, "endOffset": 40}, {"referenceID": 42, "context": "Finally, we note that we tested the bootstrapping wrapper with two other implementations of Label Propagation including: \u008ce sklearn Python library and Junto [43]1 and observed similar performance gains over the base methods.", "startOffset": 157, "endOffset": 161}, {"referenceID": 25, "context": "\u008ce computation of each iteration involves a linear number of edge traversals and is highly suitable for Pregel-like [26] distributed graph processing platforms.", "startOffset": 116, "endOffset": 120}, {"referenceID": 25, "context": "\u008cis computation can also be performed e\u0081ciently on Pregel [26] by essentially performing all iterations (di\u0082erent sets of hash-speci\u0080ed edge lengths) together.", "startOffset": 58, "endOffset": 62}, {"referenceID": 42, "context": "Feature Propagation (FP), in Algorithm 5, that uses the di\u0082usion rule of the label propagation method of [43] (Algorithm 1), and Normalized Laplacian FP, in Algorithm 6, that uses the di\u0082usion rule of [42] (Algorithm 2).", "startOffset": 105, "endOffset": 109}, {"referenceID": 41, "context": "Feature Propagation (FP), in Algorithm 5, that uses the di\u0082usion rule of the label propagation method of [43] (Algorithm 1), and Normalized Laplacian FP, in Algorithm 6, that uses the di\u0082usion rule of [42] (Algorithm 2).", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "We note that our linear feature di\u0082usions can be viewed as a toneddown GCN [2, 24], without the backprop training, and non-linear aggregations.", "startOffset": 75, "endOffset": 82}, {"referenceID": 23, "context": "We note that our linear feature di\u0082usions can be viewed as a toneddown GCN [2, 24], without the backprop training, and non-linear aggregations.", "startOffset": 75, "endOffset": 82}, {"referenceID": 23, "context": "We use the benchmark \u0080xed seed sets used in prior work [24, 40], to facilitate comparison, and random splits for robustness.", "startOffset": 55, "endOffset": 63}, {"referenceID": 39, "context": "We use the benchmark \u0080xed seed sets used in prior work [24, 40], to facilitate comparison, and random splits for robustness.", "startOffset": 55, "endOffset": 63}, {"referenceID": 23, "context": "\u008ce citation networks contain a bag-of-words representation for each document which, following [24, 40], we treat as a feature vector.", "startOffset": 94, "endOffset": 102}, {"referenceID": 39, "context": "\u008ce citation networks contain a bag-of-words representation for each document which, following [24, 40], we treat as a feature vector.", "startOffset": 94, "endOffset": 102}, {"referenceID": 23, "context": "For comparison, we also list the quality reported by GCN [24] and Planetoid [40] (best variant with node features).", "startOffset": 57, "endOffset": 61}, {"referenceID": 39, "context": "For comparison, we also list the quality reported by GCN [24] and Planetoid [40] (best variant with node features).", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Fourth, the bootstrapped version of normalized Laplacian FP improves over the state of the art results of GCN [24] on Citeseer and Cora.", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "reported on similar random splits using GCNs [24].", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "\u008ce results add robustness to our observations from the benchmark experiments: \u008ce use of di\u0082used features signi\u0080cantly improves quality, the normalized Laplacian FP consistently achieves the best results on Citeseer and Cora and is very close (within error margins) to the results reported by [24] on Pubmed.", "startOffset": 292, "endOffset": 296}, {"referenceID": 41, "context": "Norm Lap LP[42] +Bootstrapped 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "Graph Conv Nets [24] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 39, "context": "Planetoid [40] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 42, "context": "Label Propagation[43] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "Norm Lap LP [42] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "Graph Conv Nets [24] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 8, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 11, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 12, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 13, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 16, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 18, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 22, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 29, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 30, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 36, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 318, "endOffset": 322}, {"referenceID": 19, "context": "\u008ce authors would like to thank Aditya Grover, the author of node2vec [20] and Zhilin Yang, the author of Planetoid [40] for prompt and helpful answers to our questions on their work and implementations, and to Fernando Pereira for his advice.", "startOffset": 69, "endOffset": 73}, {"referenceID": 39, "context": "\u008ce authors would like to thank Aditya Grover, the author of node2vec [20] and Zhilin Yang, the author of Planetoid [40] for prompt and helpful answers to our questions on their work and implementations, and to Fernando Pereira for his advice.", "startOffset": 115, "endOffset": 119}], "year": 2017, "abstractText": "Graph-based semi-supervised learning (SSL) algorithms predict labels for all nodes based on provided labels of a small set of seed nodes. Classic methods capture the graph structure through some underlying di\u0082usion process that propagates through the graph edges. Spectral di\u0082usion, which includes personalized page rank and label propagation, propagates through random walks. Social di\u0082usion propagates through shortest paths. A common ground to these di\u0082usions is their linearity, which does not distinguish between contributions of few \u201cstrong\u201d relations and many \u201cweak\u201d relations. Recently, non-linear methods such as node embeddings and graph convolutional networks (GCN) demonstrated a large gain in quality for SSL tasks. \u008cese methods introduce multiple components and greatly vary on how the graph structure, seed label information, and other features are used. We aim here to study the contribution of non-linearity, as an isolated ingredient, to the performance gain. To do so, we place classic linear graph di\u0082usions in a self-training framework. Surprisingly, we observe that SSL using the resulting bootstrapped di\u0082usions not only signi\u0080cantly improves over the respective non-bootstrapped baselines but also outperform state-of-the-art non-linear SSL methods. Moreover, since the self-training wrapper retains the scalability of the base method, we obtain both higher quality and be\u008aer scalability.", "creator": "LaTeX with hyperref package"}}}