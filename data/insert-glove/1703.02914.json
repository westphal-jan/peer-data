{"id": "1703.02914", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Dropout Inference in Bayesian Neural Networks with Alpha-divergences", "abstract": "nazli To authorized obtain eyestrain uncertainty shary estimates with cosmology real - world Bayesian deep usa learning 71.40 models, practical inference blueish approximations zondervan are needed. gaap Dropout rockfest variational aitaroun inference (VI) disappointedly for swanson example has roussely been xfa used cristofer for machine younis vision moha and medical applications, snk but criminalist VI resynchronization can moland severely ischium underestimates bruyere model uncertainty. Alpha - sify divergences debreceni are sempati alternative mobile-phone divergences shaan to VI ' dueck s hyrcanus KL stingaree objective, holthaus which are able emeriti to avoid municipal VI ' s uncertainty underestimation. But these umayyad are hard to copper-mining use pulasthi in practice: luttazzi existing techniques 39.26 can juehne only use Gaussian hornworm approximating distributions, sirloin and require anti-modernist existing models to burness be changed radically, choctaws thus mattersdorf are garages of hematoma limited v8-powered use 0-2-1 for practitioners. intersperses We propose a formalisation re - parametrisation of the alpha - divergence victoriaville objectives, sekou deriving a cempaka simple inference amadei technique vakilabad which, blm together croakers with abbasov dropout, anti-clerical can be easily implemented s\u00fc\u00df with existing models by madredeus simply hassler changing okerlund the chipp loss of the model. satyr We marmolejo demonstrate deep-level improved sponsoring uncertainty estimates self-releasing and 128k accuracy cena compared gawker to VI in dropout foucras networks. l8 We italeli study our lovably model ' macarthurs s rebstock epistemic tasesa uncertainty far foederati away approves from 72.95 the msaidie data mrr using adversarial images, jadakiss showing that these duleek can subacute be distinguished 3rd-placed from gauteng non - adversarial images cerkno by boisterousness examining our model ' 1458 s fas uncertainty.", "histories": [["v1", "Wed, 8 Mar 2017 17:00:21 GMT  (840kb,D)", "http://arxiv.org/abs/1703.02914v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yingzhen li", "yarin gal"], "accepted": true, "id": "1703.02914"}, "pdf": {"name": "1703.02914.pdf", "metadata": {"source": "META", "title": "Dropout Inference in Bayesian Neural Networks with Alpha-divergences", "authors": ["Yingzhen Li", "Yarin Gal"], "emails": ["<yl494@cam.ac.uk>."], "sections": [{"heading": "1. Introduction", "text": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al., 2016). But unlike deep learning models, Bayesian probabilistic models can capture parameter uncertainty and its induced effects over predictions, capturing the models\u2019 ignorance about the world, and able to convey their increased uncertainty on out-of-data examples. This information can be used, for example, to identify when a vi-\n1University of Cambridge, UK 2The Alan Turing Institute, UK. Correspondence to: Yingzhen Li <yl494@cam.ac.uk>.\nsion model is given an adversarial image (studied below), or to tackle many problems in AI safety (Amodei et al., 2016). With model uncertainty at hand, applications as farreaching as safety in self-driving cars can be explored, using models which can propagate their uncertainty up the decision making pipeline (Gal, 2016). With deterministic deep learning models this invaluable uncertainty information is often lost.\nBayesian deep learning \u2013 an approach to combining Bayesian probability theory together with deep learning \u2013 allows us to use state-of-the-art models and at the same time obtain model uncertainty (Gal, 2016; Gal & Ghahramani, 2016a). Originating in the 90s (Neal, 1995; MacKay, 1992; Denker & LeCun, 1991), Bayesian neural networks (BNNs) in particular have started gaining in popularity again (Graves, 2011; Blundell et al., 2015; HernandezLobato & Adams, 2015). BNNs are standard neural networks (NNs) with prior probability distributions placed over their weights. Given observed data, inference is then performed to find what are the more likely and less likely weights to explain the data. But as easy it is to formulate BNNs, is as difficult to perform inference in them. Many approximations have been proposed over the years (Denker & LeCun, 1991; Neal, 1995; Graves, 2011; Blundell et al., 2015; Hernandez-Lobato & Adams, 2015; Herna\u0301ndez-Lobato et al., 2016), some more practical and some less practical. A practical approximation for inference in Bayesian neural networks should be able to scale well to large data and complex models (such as convolutional neural networks (CNNs) (Rumelhart et al., 1985; LeCun et al., 1989)). Much more important perhaps, it would be impractical to change existing model architectures that have been well studied, and it is often impractical to work with complex and cumbersome techniques which are difficult to explain to non-experts. Many existing approaches to obtain model confidence often do not scale to complex models or large amounts of data, and require us to develop new models for existing tasks for which we already have well performing tools (Gal, 2016).\nOne possible solution for practical inference in BNNs is variational inference (VI) (Jordan et al., 1999), a ubiquitous technique for approximate inference. Dropout variational distributions in particular (a mixture of two Gaussians with small standard deviations, and with one component fixed at\nar X\niv :1\n70 3.\n02 91\n4v 1\n[ cs\n.L G\n] 8\nM ar\n2 01\n7\nzero) can be used to obtain a practical inference technique (Gal & Ghahramani, 2016b). These have been used for machine vision and medical applications (Kendall & Cipolla, 2016; Kendall et al., 2015; Angermueller & Stegle, 2015; Yang et al., 2016). Dropout variational inference can be implemented by adding dropout layers (Hinton et al., 2012; Srivastava et al., 2014) before every weight layer in the NN model. Inference is then carried out by Monte Carlo (MC) integration over the variational distribution, in practice implemented by simulating stochastic forward passes through the model at test time (referred to as MC dropout). Although dropout VI is a practical technique for approximate inference, it also has some major limitations. Dropout VI can severely underestimate model uncertainty (Gal, 2016, Section 3.3.2) \u2013 a property many VI methods share (Turner & Sahani, 2011). This can lead to devastating results in applications that must rely on good uncertainty estimates such as AI safety applications.\nAlternative objectives to VI\u2019s objective are therefore needed. Black-box \u03b1-divergence minimisation (Herna\u0301ndez-Lobato et al., 2016; Li & Turner, 2016; Minka, 2005) is a class of approximate inference methods extending on VI, approximating EP\u2019s energy function (Minka, 2001) as well as the Hellinger distance (Hellinger, 1909). These were proposed as a solution to some of the difficulties encountered with VI. However, the main difficulty with \u03b1-divergences is that the divergences are hard to use in practice. Existing inference techniques only use Gaussian approximating distributions, with the density over the approximation having to be evaluated explicitly many times. The objective offers a limited intuitive interpretation which is difficult to explain to non-experts, and of limited use for engineers (Gal, 2016, Section 2.2.2). Perhaps more important, current \u03b1-divergence inference techniques require existing models and code-bases to be changed radically to perform inference in the Bayesian counterpart to these models. To implement a complex CNN structure with the inference and code of (Herna\u0301ndez-Lobato et al., 2016), for example, one would be required to re-implement many already-implemented software tools.\nIn this paper we propose a re-parametrisation of the induced \u03b1-divergence objectives, and by relying on some mild assumptions (which we justify below), derive a simple approximate inference technique which can easily be implemented with existing models. Further, we rely on the dropout approximate variational distribution and demonstrate how inference can be done in a practical way \u2013 requiring us to only change the loss of the NN, L(\u03b8), and to perform multiple stochastic forward passes at training time. In particular, given l(\u00b7, \u00b7) some standard NN loss such as cross entropy or the Euclidean loss, and {f \u03c9\u0302k(xn)}Kk=1 a set of K stochastic dropout network outputs on input xn with randomly masked weights \u03c9\u0302k, our proposed objective\nis:\nL(\u03b8) = \u2212 1 \u03b1 \u2211 n log-sum-exp [ \u2212\u03b1 \u00b7 l(yn, f \u03c9\u0302k(xn)) ] + L2(\u03b8)\nwith \u03b1 a real number, \u03b8 the set of network weights to be optimised, and an L2 regulariser over \u03b8. By selecting \u03b1 = 1 this objective directly optimises the per-point predictive log-likelihood, while picking \u03b1 \u2192 0 would focus on increasing the training accuracy, recovering VI.\nSpecific choices of \u03b1will result in improved uncertainty estimates (and accuracy) compared to VI in dropout BNNs, without slowing convergence time. We demonstrate this through a myriad of applications, including an assessment of fully connected NNs in regression and classification, and an assessment of Bayesian CNNs. Finally, we study the uncertainty estimates resulting from our approximate inference technique. We show that our models\u2019 uncertainty increases on adversarial images generated from the MNIST dataset, suggesting that these lie outside of the training data distribution. This in practice allows us to tell-apart such adversarial images from non-adversarial images by examining epistemic model uncertainty."}, {"heading": "2. Background", "text": "We review background in Bayesian neural networks and approximate variational inference. In the next section we discuss \u03b1-divergences."}, {"heading": "2.1. Bayesian Neural Networks", "text": "Given training inputs X = {x1, . . . ,xN} and their corresponding outputs Y = {y1, . . . ,yN}, in parametric Bayesian regression we would like to infer a distribution over parameters \u03c9 of a function y = f\u03c9(x) that could have generated the outputs. Following the Bayesian approach, to find parameters that could have generated our data, we put some prior distribution over the space of parameters p0(\u03c9). This distribution captures our prior belief as to which parameters are likely to have generated our outputs before observing any data. We further need to define a probability distribution over the outputs given the inputs p(y|x, \u03c9). For classification tasks we assume a softmax likelihood,\np ( y|x, \u03c9 ) = Softmax (f\u03c9(x))\nor a Gaussian likelihood for regression. Given a dataset X,Y, we then look for the posterior distribution over the space of parameters: p(\u03c9|X,Y). This distribution captures how likely the function parameters are, given our observed data. With it we can predict an output for a new input point x\u2217 by integrating\np(y\u2217|x\u2217,X,Y) = \u222b p(y\u2217|x\u2217, \u03c9)p(\u03c9|X,Y)d\u03c9. (1)\nOne way to define a distribution over a parametric set of functions is to place a prior distribution over a neural network\u2019s weights \u03c9 = {Wi}Li=1, resulting in a Bayesian NN (MacKay, 1992; Neal, 1995). Given weight matrices Wi and bias vectors bi for layer i, we often place standard matrix Gaussian prior distributions over the weight matrices, p0(Wi) = N (Wi;0, I) and often assume a point estimate for the bias vectors for simplicity."}, {"heading": "2.2. Approximate Variational Inference in Bayesian Neural Networks", "text": "In approximate inference, we are interested in finding the distribution of weight matrices (parametrising our functions) that have generated our data. This is the posterior over the weights given our observables X,Y: p(\u03c9|X,Y), which is not tractable in general. Existing approaches to approximate this posterior are through variational inference (as was done in Hinton & Van Camp (1993); Barber & Bishop (1998); Graves (2011); Blundell et al. (2015)). We need to define an approximating variational distribution q\u03b8(\u03c9) (parametrised by variational parameters \u03b8), and then minimise w.r.t. \u03b8 the KL divergence (Kullback & Leibler, 1951; Kullback, 1959) between the approximating distribution and the full posterior: KL ( q\u03b8(\u03c9)||p(\u03c9|X,Y) ) \u221d \u2212 \u222b q\u03b8(\u03c9) log p(Y|X, \u03c9)d\u03c9\n+ KL(q\u03b8(\u03c9)||p0(\u03c9))\n= \u2212 N\u2211 i=1 \u222b q\u03b8(\u03c9) log p(yi|f\u03c9(xi))d\u03c9\n+ KL(q\u03b8(\u03c9)||p0(\u03c9)), (2)\nwhere A \u221d B is slightly abused here to denote equality up to an additive constant (w.r.t. variational parameters \u03b8)."}, {"heading": "2.3. Dropout Approximate Inference", "text": "Given a (deterministic) neural network, stochastic regularisation techniques in the model (such as dropout (Hinton et al., 2012; Srivastava et al., 2014)) can be interpreted as variational Bayesian approximations in a Bayesian NN with the same network structure (Gal & Ghahramani, 2016b). This is because applying a stochastic regularisation technique is equivalent to multiplying the NN weight matrices Mi by some random noise i (with a new noise realisation for each data point). The resulting stochastic weight matrices Wi = iMi can be seen as draws from the approximate posterior over the BNN weights, replacing the deterministic NN\u2019s weight matrices Mi. Our set of variational parameters is then the set of matrices \u03b8 = {Mi}Li=1. For example, dropout can be seen as an approximation to Bayesian NN inference with dropout approximating distributions, where the rows of the matrices Wi distribute ac-\ncording to a mixture of two Gaussians with small variances and the mean of one of the Gaussians fixed at zero. The uncertainty in the weights induces prediction uncertainty by marginalising over the approximate posterior using Monte Carlo integration:\np(y = c|x,X,Y) = \u222b p(y = c|x, \u03c9)p(\u03c9|X,Y)d\u03c9\n\u2248 \u222b p(y = c|x, \u03c9)q\u03b8(\u03c9)d\u03c9\n\u2248 1 K K\u2211 k=1 p(y = c|x, \u03c9\u0302k)\nwith \u03c9\u0302k \u223c q\u03b8(\u03c9), where q\u03b8(\u03c9) is the Dropout distribution (Gal, 2016). Given its popularity, we concentrate on the dropout stochastic regularisation technique throughout the rest of the paper, although any other stochastic regularisation technique could be used instead (such as multiplicative Gaussian noise (Srivastava et al., 2014) or dropConnect (Wan et al., 2013)).\nDropout VI is an example of practical approximate inference, but it also underestimates model uncertainty (Gal, 2016, Section 3.3.2). This is because minimising the KL divergence between q(\u03c9) and p(\u03c9|X,Y) penalises q(\u03c9) for placing probability mass where p(\u03c9|X,Y) has no mass, but does not penalise q(\u03c9) for not placing probability mass at locations where p(\u03c9|X,Y) does have mass. We next discuss \u03b1-divergences as an alternative to the VI objective."}, {"heading": "3. Black-box \u03b1-divergence minimisation", "text": "In this section we provide a brief review of the black box alpha (BB-\u03b1, Herna\u0301ndez-Lobato et al. (2016)) method upon which the main derivation in this paper is based. Consider approximating the following distribution:\np(\u03c9) = 1\nZ p0(\u03c9) \u220f n fn(\u03c9).\nIn Bayesian neural networks context, these factors fn(\u03c9) represent the likelihood terms p(yn|xn, \u03c9), Z = p(Y|X), and the approximation target p(\u03c9) is the exact posterior p(\u03c9|X,Y). Popular methods of approximate inference include variational inference (VI) (Jordan et al., 1999) and expectation propagation (EP) (Minka, 2001), where these two algorithms are special cases of power EP (Minka, 2004) that minimises Amari\u2019s \u03b1-divergence (Amari, 1985) D\u03b1[p||q] in a local way:\nD\u03b1[p||q] = 1\n\u03b1(1\u2212 \u03b1)\n( 1\u2212 \u222b p(\u03c9)\u03b1q(\u03c9)1\u2212\u03b1d\u03c9 ) .\nWe provide details of \u03b1-divergences and local approximation methods in the appendix, and in the rest of the paper we consider three special cases in this rich family:\n1. Exclusive KL divergence:\nD0[p||q] = KL[q||p] = Eq [ log q(\u03c9)\np(\u03c9)\n] ;\n2. Hellinger distance: D0.5[p||q] = 4Hel2[q||p] = 2 \u222b (\u221a p(\u03c9)\u2212 \u221a q(\u03c9) )2 d\u03c9;\n3. Inclusive KL divergence:\nD1[p||q] = KL[p||q] = Ep [ log p(\u03c9)\nq(\u03c9)\n] .\nSince \u03b1 = 0 is used in VI and \u03b1 = 1.0 is used in EP, in later sections we will also refer to these alpha settings as the VI value, Hellinger value, and EP value, respectively.\nPower-EP, though providing a generic variational framework, does not scale with big data. It maintains approximating factors attached to every likelihood term fn(\u03c9), resulting in space complexity O(N) for the posterior approximation which is clearly undesirable. The recently proposed stochastic EP (Li et al., 2015) and BB-\u03b1 (Herna\u0301ndezLobato et al., 2016) inference methods reduce this memory overhead to O(1) by sharing these approximating factors. Moreover, optimisation in BB-\u03b1 is done by descending the so called BB-\u03b1 energy function, where Monte Carlo (MC) methods and automatic differentiation are also deployed to allow fast prototyping.\nBB-\u03b1 has been successfully applied to Bayesian neural networks for regression, classification (Herna\u0301ndez-Lobato et al., 2016) and model-based reinforcement learning (Depeweg et al., 2016). They all found that using \u03b1 6= 0 often returns better approximations than the VI case. The reasons for the worse results of VI are two fold. From the perspective of inference, the zero-forcing behaviour of exclusive KL-divergences enforces the q distribution to be zero in the region where the exact posterior has zero probability mass. Thus VI often fits to a local mode of the exact posterior and is over-confident in prediction. On hyper-parameter learning point of view, as the variational lower-bound is used as a (biased) approximation to the maximum likelihood objective, the learned model could be biased towards oversimplified cases (Turner & Sahani, 2011). These problems could potentially be addressed by using \u03b1-divergences. For example, inclusive KL encourages the coverage of the support set (referred as mass-covering), and when used in local divergence minimisation (Minka, 2005), it can fit an approximation to a mode of p(\u03c9) with better estimates of uncertainty. Moreover the BB-\u03b1 energy provides a better approximation to the marginal likelihood as well, meaning that the learned model will be less biased and thus fitting the data distribution better (Li & Turner, 2016). Hellinger\ndistance seems to provide a good balance between zeroforcing and mass-covering, and empirically it has been found to achieve the best performance.\nGiven the success of \u03b1-divergence methods, it is a natural idea to extend these algorithms to other classes of approximations such as dropout. However this task is non-trivial. First, the original formulation of BB-\u03b1 energy is an ad hoc adaptation of power-EP energy (see appendix), which applies to exponential family q distributions only. Second, the energy function offers a limited intuitive interpretation to non-experts, thus of limited use for practitioners. Third and most importantly, a naive implementation of BB-\u03b1 using dropout would bring in a prohibitive computational burden. To see this, we first review the BB-\u03b1 energy function in the general case (Li & Turner, 2016) given \u03b1 6= 0:\nL\u03b1(q) = \u2212 1\n\u03b1 \u2211 n logEq\n[( fn(\u03c9)p0(\u03c9) 1 N\nq(\u03c9) 1 N\n)\u03b1] . (3)\nOne could verify that this is the same energy function as presented in (Herna\u0301ndez-Lobato et al., 2016) by considering q an exponential family distribution. In practice (3) might be intractable, hence an MC approximation is introduced:\nLMC\u03b1 (q) = \u2212 1\n\u03b1 \u2211 n log 1 K \u2211 k\n[( fn(\u03c9\u0302k)p0(\u03c9\u0302k) 1 N\nq(\u03c9\u0302k) 1 N\n)\u03b1] (4)\nwith \u03c9\u0302k \u223c q(\u03c9). This is a biased approximation as the expectation in (3) is computed before taking the logarithm. But empirically Herna\u0301ndez-Lobato et al. (2016) showed that the bias introduced by the MC approximation is often dominated by the variance of the samples, meaning that the effect of the bias is negligible. When \u03b1 \u2192 0 it returns the variational free energy (the VI objective)\nL0(q) = LVFE(q) = KL[q||p0]\u2212 \u2211 n Eq [log fn(\u03c9)] , (5)\nand the corresponding MC approximation LMCVFE becomes an unbiased estimator of LVFE. Also LMC\u03b1 \u2192 LMCVFE as the number of samples K \u2192 1.\nThe original paper (Herna\u0301ndez-Lobato et al., 2016) proposed a naive implementation which directly evaluates the MC estimation (4) with samples \u03c9\u0302k \u223c q(\u03c9). However as discussed before, dropout implicitly samples different masked weight matrices \u03c9\u0302 \u223c q for different data points. This indicates that the naive approach, when applied to dropout approximation, would gather all these samples for all M datapoints in a mini-batch (i.e. MK sets of neural network weight matrices in total), which brings prohibitive cost if the network is wide and deep. Interestingly, the minimisation of the variational free energy (\u03b1 = 0) with the dropout approximation can be computed very efficiently.\nThe main reason for this success is due to the additive structure of the variational free energy: no evaluation of q density is required if the \u201cregulariser\u201d KL[q||p0] can be computed/approximated efficiently. In the following section we propose an improved version of BB-\u03b1 energy to allow applications with dropout and other flexible approximation structures."}, {"heading": "4. A New Reparameterisation of BB-\u03b1 Energy", "text": "We propose a reparamterisation of the BB-\u03b1 energy to reduce the computational overhead, which uses the so called \u201ccavity distributions\u201d. First we denote q\u0303(\u03c9) as a free-form cavity distribution, and write the approximate posterior q as\nq(\u03c9) = 1\nZq q\u0303(\u03c9)\n( q\u0303(\u03c9)\np0(\u03c9)\n) \u03b1 N\u2212\u03b1\n, (6)\nwhere we assume Zq < +\u221e is the normalising constant to ensure q a valid distribution. When \u03b1/N \u2192 0, the unnormalised density in (6) converges to q\u0303(\u03c9) for every \u03c9, and Zq \u2192 1 by the assumption of Zq < +\u221e (Van Erven & Harremoe\u0308s, 2014). Hence q \u2192 q\u0303 when \u03b1/N \u2192 0, and this happens for example when we choose \u03b1 \u2192 0, or N \u2192 +\u221e as well as when \u03b1 grows sub-linearly to N . Now we rewrite the BB-alpha energy in terms of q\u0303:\nL\u03b1(q) = \u2212 1\n\u03b1 \u2211 n log\n\u222b ( 1\nZq q\u0303(\u03c9)\n( q\u0303(\u03c9)\np0(\u03c9)\n) \u03b1 N\u2212\u03b1 )1\u2212 \u03b1N p0(\u03c9) \u03b1 N fn(\u03c9) \u03b1d\u03c9\n= N \u03b1 (1\u2212 \u03b1 N ) log\n\u222b q\u0303(\u03c9) ( q\u0303(\u03c9)\np0(\u03c9)\n) \u03b1 N\u2212\u03b1\nd\u03c9\n\u2212 1 \u03b1 \u2211 n logEq\u0303 [fn(\u03c9)\u03b1]\n= R\u03b2 [q\u0303||p0]\u2212 1\n\u03b1 \u2211 n logEq\u0303 [fn(\u03c9)\u03b1] , \u03b2 = N N \u2212 \u03b1 ,\nwhere R\u03b2 [q\u0303||p0] represents the Re\u0301nyi divergence (Re\u0301nyi (1961), discussed in the appendix) of order \u03b2. We note again that when \u03b1N \u2192 0 the new energyL\u03b1(q\u0303) converges to LVFE(q\u0303) as well as q \u2192 q\u0303. More importantly, R\u03b2 [q\u0303||p0]\u2192 KL[q\u0303||p0] = KL[q||p0] provided R\u03b2 [q\u0303||p0] < +\u221e (which holds when assuming Zq < +\u221e) and \u03b1N \u2192 0.\nThis means that for a constant \u03b1 that scales sub-linearly with N , in large data settings we can further approximate the BB-\u03b1 energy as\nL\u03b1(q) \u2248 L\u0303\u03b1(q) = KL[q||p0]\u2212 1\n\u03b1 \u2211 n logEq [fn(\u03c9)\u03b1] .\nNote that here we also use the fact that now q \u2248 q\u0303. Critically, the proposed reparameterisation is continuous in \u03b1,\nand by taking \u03b1\u2192 0 the variational free-energy (5) is again recovered.\nGiven a loss function l(\u00b7, \u00b7), e.g. l2 loss in regression or cross entropy in classification, we can define the (unnormalised) likelihood term fn(\u03c9) \u221d p(yn|xn, \u03c9) \u221d exp[\u2212l(yn, f\u03c9(xn))], e.g. see (LeCun et al., 2006)1. Swapping fn(\u03c9) for this last expression, and approximating the expectation over q using Monte Carlo sampling, we obtain our proposed minimisation objective:\nL\u0303MC\u03b1 (q) = KL[q||p0] + const (7)\n\u2212 1 \u03b1 \u2211 n log-sum-exp[\u2212\u03b1l(yn, f \u03c9\u0302k(xn))]\nwith log-sum-exp being the log-sum-exp operator over K samples from the approximate posterior \u03c9\u0302k \u223c q(\u03c9). This objective function also approximates the marginal likelihood. Therefore, compared to the original formulation (3), the improved version (7) is considerably simpler (both to implement and to understand), has a similar form to standard objective functions used in deep learning research, yet remains an approximate Bayesian inference algorithm.\nTo gain some intuitive understanding of this objective, we observe what it reduces to for different \u03b1 and K settings. By selecting \u03b1 = 1 the per-point predictive log-likelihood logEq[p(yn|xn, \u03c9)] is directly optimised. On the other hand, picking the VI value (\u03b1 \u2192 0) would focus on increasing the training accuracy Eq[log p(yn|xn, \u03c9)]. The Hellinger value could be used to achieve a balance between reducing training error and improving predictive likelihood, which has been found to be desirable (Herna\u0301ndezLobato et al., 2016; Depeweg et al., 2016). Lastly, for K = 1 the log-sum-exp disappears, the \u03b1\u2019s cancel out, and the original (stochastic) VI objective is recovered.\nIn summary, our proposal modifies the loss function by multiplying it by \u03b1 and then performing log-sum-exp with a sum over multiple stochastic forward passes sampled from the BNN approximate posterior. The remaining KLdivergence term (between q and the prior p) can often be approximated. It can be viewed as a regulariser added to the objective function, and reduces to L2-norm regulariser for certain popular q choices (Gal, 2016)."}, {"heading": "4.1. Dropout BB-\u03b1", "text": "We now provide a concrete example where the approximate distribution is defined by dropout. With dropout VI, MC samples are used to approximate the expectation w.r.t. q, which in practice is implemented as performing stochastic forward passes through the dropout network \u2013 i.e. given an\n1We note that fn(\u03c9) does not need to be a normalised density of yn unless one would like to optimise the hyper parameters associated with fn.\ninput x, the input is fed through the network and a new dropout mask is sampled and applied at each dropout layer. This gives a stochastic output \u2013 a sample from the dropout network on the input x. A similar approximation is used in our case as well, where to implement the MC sampling in eq. (7) we perform multiple stochastic forward passes through the network.\nRecall the neural network f\u03c9(x) is parameterised by the variable \u03c9. In classification, cross entropy is often used as the loss function\u2211\nn\nl(yn,p \u03c9(xn)) = \u2211 n \u2212yTn logp\u03c9(xn), (8)\np\u03c9(xn) = Softmax(f\u03c9(xn)),\nwhere the label yn is a one-hot binary vector, and the network output Softmax(f\u03c9(xn)) encodes the probability vector of class assignments. Applying the re-formulated BB-\u03b1 energy (7) with a Bayesian equivalent of the network, we arrive at the objective function\nL\u0303MC\u03b1 (q) = \u2211 i pi||Mi||22 \u2212 1 \u03b1 \u2211 n yTn log 1 K \u2211 k (p\u03c9\u0302k(xn)) \u03b1\n= 1\n\u03b1 \u2211 n l\n( yn, 1\nK \u2211 k p\u03c9\u0302k(xn) \u03b1 ) + \u2211 i L2(Mi)\n(9)\nwith {p\u03c9\u0302k(xn)}Kk=1 being K stochastic network outputs on input xn, pi equals to one minus the dropout rate of the ith layer, and the L2 regularization terms coming from an approximation to the KL-divergence (Gal, 2016). I.e. we raise network probability outputs to the power \u03b1 and average them as an input to the standard cross entropy loss. Taking \u03b1 6= 1 can be viewed as training the neural network with an adjusted \u201cpower\u201d loss, regularized by an L2 norm. Implementing this induced loss with Keras (Chollet, 2015) is as simple as a few lines of Python. A code snippet is given in Figure 1, with more details in the appendix.\nIn regression problems, the loss function is defined as l(y, f\u03c9(x)) = \u03c42 ||y\u2212 f\n\u03c9(x)||22 and the likelihood term can be interpreted as y \u223c N (y; f\u03c9(x), \u03c4\u22121I). Plugging this into the energy function returns the following objective\nL\u0303MC\u03b1 (q) = \u2212 1\n\u03b1 \u2211 n log-sum-exp [ \u2212\u03b1\u03c4 2 ||yn \u2212 f \u03c9\u0302k(xn)||22 ] + ND\n2 log \u03c4 + \u2211 i pi||Mi||22, (10)\nwith {f \u03c9\u0302k(xn)}Kk=1 being K stochastic forward passes on input xn. Again, this is reminiscent of the l2 objective in standard deep learning, and can be implemented by simply passing the input through the dropout network multiple times, collecting the stochastic outputs, and feeding the set of outputs through our new BB-alpha loss function."}, {"heading": "5. Experiments", "text": "We test the reparameterised BB-\u03b1 on Bayesian NNs with the dropout approximation. We assess the proposed inference in regression and classification tasks on standard benchmarking datasets, comparing different values of \u03b1. We further assess the training time trade-off between our technique and VI, and study the properties of our model\u2019s uncertainty on out-of-distribution data points. This last experiment leads us to propose a technique that could be used to identify adversarial image attacks."}, {"heading": "5.1. Regression", "text": "The first experiment considers Bayesian neural network regression with approximate posterior induced by dropout. We use benchmark UCI datasets2 that have been tested in related literature. The model is a single-layer neural network with 50 ReLU units for all datasets except for Protein and Year, which use 100 units. We consider \u03b1 \u2208 {0.0, 0.5, 1.0} in order to examine the effect of masscovering/zero-forcing behaviour in dropout. MC approximation with K = 10 samples is also deployed to compute the energy function. Other initialisation settings are largely taken from (Li & Turner, 2016).\nWe summarise the test negative log-likelihood (LL) and RMSE with standard error (across different random splits) for selected datasets in Figure 2 and 3, respectively. The full results are provided in the appendix. Although optimal \u03b1 may vary for different datasets, using non-VI values has significantly improved the test-LL performances, while remaining comparable in test error metric. In particular, \u03b1 = 0.5 produced overall good results for both test LL and RMSE, which is consistent with previous findings. As a comparison we also include test performances of a BNN with a Gaussian approximation (VI-G) (Li & Turner, 2016), a BNN with HMC, and a sparse Gaussian process model with 50 inducing points (Bui et al., 2016). In testLL metric our best dropout model out-performs the Gaus-\n2http://archive.ics.uci.edu/ml/datasets. html\nsian approximation method on almost all datasets, and for some datasets is on par with HMC which is the current gold standard for Bayesian neural works, and with the GP model that is known to be superior in regression."}, {"heading": "5.2. Classification", "text": "We further experiment with a classification task, comparing the accuracy of the various \u03b1 values on the MNIST benchmark (LeCun & Cortes, 1998). We assessed a fully connect NN with 2 hidden layers and 100 units in each layer. We used dropout probability 0.5 and \u03b1 \u2208 {0, 0.5, 1}. Again, we use K = 10 samples at training time for all \u03b1 values, and Ktest = 100 samples at test time. We use weight decay 10\u22126, which is equivalent to prior lengthscale l2 = 0.1 (Gal & Ghahramani, 2016b). We repeat each experiment three times and plot mean and standard error. Test RMSE as well as test log likelihood are given in Figure 4. As can be seen, Hellinger value \u03b1 = 0.5 gives best test RMSE, with test log likelihood matching that of the EP value \u03b1 = 1. The VI value \u03b1 = 0 under-performs according to both metrics.\nWe next assess a convolutional neural network model\n(CNN). For this experiment we use the standard CNN example given in (Chollet, 2015) with 32 convolution filters, 100 hidden units at the top layer, and dropout probability 0.5 before each fully-connected layer. Other settings are as before. Average test accuracy and test log likelihood are given in Figure 5. In this case, VI value \u03b1 = 0 seems to supersede the EP value \u03b1 = 1, and performs similarly to the Hellinger value \u03b1 = 0.5 according to both metrics."}, {"heading": "5.3. Detecting Adversarial Examples", "text": "The third set of experiments considers adversarial attacks on dropout trained Bayesian neural networks. Bayesian neural networks\u2019 uncertainty increases on examples far from the data distribution. We test the hypothesis that certain techniques for generating adversarial examples will give images that lie outside of the image manifold, i.e. far from the data distribution (note though that there exist techniques that will guarantee the images staying near the data manifold, by minimising the perturbation used to construct the adversarial example). By assessing our BNN uncertainty, we should see increased uncertainty for adversarial images if they indeed lie outside of the training data distri-\nbution. The tested model is a fully connected network with 3 hidden layers of 1000 units. The dropout trained models are also compared to a benchmark NN with the same architecture but trained by maximum likelihood. The adversarial examples are generated on MNIST test data that is normalised to be in the range [0, 1]. For the dropout trained networks we perform MC dropout prediction at test time with Ktest = 10 MC samples.\nThe first attack in consideration is the Fast Gradient Sign (FGS) method (Goodfellow et al., 2014). This is an untargeted attack, which attempts to reduces the maximum value of the predicted class label probability\nxadv = x\u2212 \u03b7 \u00b7 sgn(\u2207x max y log p(y|x)).\nWe use the single gradient step FGS implemented in Cleverhans (Papernot et al., 2016) with the stepsize \u03b7 varied between 0.0 and 0.5. The left panel in Figure 6 demonstrates the classification accuracy on adversarial examples, which shows that the dropout networks, especially the one trained with \u03b1 = 1.0, are significantly more robust to adversarial attacks compared to the deterministic NN. More interestingly, the test data examples and adversarial images can be told-apart by investigating the uncertainty representation of the dropout models. In the right panel of Figure 6 we depict the predictive entropy computed on the neural network output probability vector, and show example corresponding adversarial images below the axis for each corresponding stepsize. Clearly the deterministic NN model produces over-confident predictions on adversarial samples, e.g. it predicts the wrong label very confidently even when the input is still visually close to digit \u201c7\u201d (\u03b7 = 0.2). While dropout models, though producing wrong labels, are very uncertain about their predictions. This uncertainty keeps increasing as we move away from the data manifold. Hence the dropout networks are much more immu-\nnised from noise-corrupted inputs, as they can be detected using uncertainty estimates in this example.\nThe second attack we consider is a targeted version of FSG (Carlini & Wagner, 2016), which maximises the predictive probability of a selected class instead. As an example, we fix class 0 as the target and apply the iterative gradientbase attack to all non-zero digits in test data. At step t, the adversarial output is computed as\nxtadv = x t\u22121 adv + \u03b7 \u00b7 sgn(\u2207x log p(ytarget|x t\u22121 adv )),\nwhere the stepsize \u03b7 is fixed at 0.01 in this case. Results are presented in the left panel of Figure 7, and again dropout trained models are more robust to this attack compared with the deterministically trained NN. Similarly these adversarial examples could be detected by the Bayesian neural networks\u2019 uncertainty, by examining the predictive entropy. By visually inspecting the generated adversarial examples in the right panel of Figure 7, it is clear that the NN overconfidently classifies a digit 7 to class 0. On the other hand, the dropout models are still fairly uncertain about their predictions even after 40 gradient steps. More interestingly, running this iterative attack on dropout models produces a smooth interpolation between different digits, and when the model is confident on predicting the target class, the corresponding adversarial images are visually close to digit zero.\nThese initial results suggest that assessing the epistemic uncertainty of classification models can be used as a viable technique to identify adversarial examples. We would note though that we used this experiment to demonstrate our techniques\u2019 uncertainty estimates, and much more research is needed to solve the difficulties faced with adversarial inputs."}, {"heading": "5.4. Run time trade-off", "text": "We finish the experiments section by assessing the running time trade-offs of using an increasing number of samples at training time. Unlike VI, in our inference we rely on a large number of samples to reduce estimator bias. When a small number of samples is used (K = 1) our method collapses to standard VI. In Figure 8 we see both test accuracy as well as test log likelihood for a fully connected NN with four layers of 1024 units trained on the MNIST dataset, with \u03b1 = 1. The two metrics are shown as a function of wallclock run time for different values ofK \u2208 {1, 10, 100}. As can be seen, K = 1 converges to test accuracy of 98.8% faster than the other values of K, which converge to the same accuracy. On the other hand, when assessing test log likelihood, both K = 1 and K = 10 attain value \u2212600 within 1000 seconds, but K = 10 continues improving its test log likelihood and converges to value \u2212500 after 3000 seconds. K = 100 converges to the same value but requires much longer running time, possibly because of noise from other processes."}, {"heading": "6. Conclusions", "text": "We presented a practical extension of the BB-alpha objective which allows us to use the technique with dropout approximating distributions. The technique often supersedes existing approximate inference techniques (even sparse Gaussian processes), and is easy to implement. A code snippet for our induced loss is given in the appendix."}, {"heading": "Acknowledgements", "text": "YL thanks the Schlumberger Foundation FFTF fellowship for supporting her PhD study."}, {"heading": "A. Code Example", "text": "The following is a code snippet showing how our inference can be implemented with a few lines of Keras code (Chollet, 2015). We define a new loss function bbalpha softmax cross entropy with mc logits, that takes MC sampled logits as an input. This is demonstrated for the case of classification. Regression can be implemented in a similar way.\ndef bbalpha_softmax_cross_entropy_with_mc_logits(alpha): def loss(y_true, mc_logits): # mc_logits: output of GenerateMCSamples, of shape M x K x D mc_log_softmax = mc_logits - K.max(mc_logits, axis=2, keepdims=True) mc_log_softmax = mc_log_softmax - logsumexp(mc_log_softmax, 2) mc_ll = K.sum(y_true * mc_log_softmax, -1) # M x K return - 1. / alpha * (logsumexp(alpha * mc_ll, 1) + K.log(1.0 / K_mc))\nreturn loss\nMC samples for this loss can be generated using GenerateMCSamples, with layers being a list of Keras initialised layers:\ndef GenerateMCSamples(inp, layers, K_mc=20): output_list = [] for _ in xrange(K_mc):\noutput_list += [apply_layers(inp, layers)] def pack_out(output_list): output = K.pack(output_list) # K_mc x nb_batch x nb_classes return K.permute_dimensions(output, (1, 0, 2)) # nb_batch x K_mc x nb_classes def pack_shape(s): s = s[0] return (s[0], K_mc, s[1]) out = Lambda(pack_out, output_shape=pack_shape)(output_list) return out\nThe above two functions rely on the following auxiliary functions:\ndef logsumexp(x, axis=None): x_max = K.max(x, axis=axis, keepdims=True) return K.log(K.sum(K.exp(x - x_max), axis=axis, keepdims=True)) + x_max\ndef apply_layers(inp, layers): output = inp for layer in layers:\noutput = layer(output) return output"}, {"heading": "B. Alpha-divergence minimisation", "text": "There are various available definitions of \u03b1-divergences, and in this work we mainly used two of them: Amari\u2019s definition (Amari, 1985) adapted to EP context (Minka, 2005), and Re\u0301nyi divergence (Re\u0301nyi, 1961) which is more used in information theory research.\n\u2022 Amari\u2019s \u03b1-divergence (Amari, 1985):\nD\u03b1[p||q] = 1\n\u03b1(1\u2212 \u03b1)\n( 1\u2212 \u222b p(\u03c9)\u03b1q(\u03c9)1\u2212\u03b1d\u03c9 ) .\n\u2022 Re\u0301nyi\u2019s \u03b1-divergence (Re\u0301nyi, 1961):\nR\u03b1[p||q] = 1\n\u03b1\u2212 1 log\n\u222b p(\u03c9)\u03b1q(\u03c9)1\u2212\u03b1d\u03c9.\nThese two divergence can be converted to each other, e.g. D\u03b1[p||q] = 1\u03b1(1\u2212\u03b1) (1\u2212 exp [(\u03b1\u2212 1)R\u03b1[p||q]]). In power EP (Minka, 2004), this \u03b1-divergence is minimised using projection-based updates. When the approximate posterior q has an exponential family form, minimising D\u03b1[p||q] requires moment matching to the \u201ctilted distribution\u201d p\u0303\u03b1(\u03c9) \u221d p(\u03c9)\u03b1q(\u03c9)1\u2212\u03b1. This projection update might be intractable for non-exponential family q distributions, and instead BB-\u03b1 deploys a gradient-based update to search a local minimum. We will present the original derivation of the BB-\u03b1 energy below and discuss how it relates to power EP."}, {"heading": "C. Original Derivation of BB-\u03b1 Energy", "text": "Here we include the original formulation of the BB-\u03b1 energy for completeness. Consider approximating a distribution of the following form\np(\u03c9) = 1\nZ p0(\u03c9) N\u220f n fn(\u03c9),\nin which the prior distribution p0(\u03c9) has an exponential family form p0(\u03c9) \u221d exp [ \u03bbT0 \u03c6(\u03c9) ] . Here \u03bb0 is called natural parameter or canonical parameter of the exponential family distribution, and \u03c6(\u03c9) is the sufficient statistic. As the factors fn might not be conjugate to the prior, the exact posterior no longer belongs to the same exponential family as the prior, and hence need approximations. EP construct such approximation by first approximating each complicated factor fn with a simpler one f\u0303n(\u03c9) \u221d exp [ \u03bbTn\u03c6(\u03c9) ] , then constructing the approximate distribution as\nq(\u03c9) = 1\nZ(\u03bbq) exp ( N\u2211 n=0 \u03bbn )T \u03c6(\u03c9)  , with \u03bbq = \u03bb0 + \u2211N n=1 \u03bbn and Z(\u03bbq) the normalising constant/partition function. These local parameters are updated using the following procedure (for \u03b1 6= 0):\n1 compute cavity distribution q\\n(\u03c9) \u221d q(\u03c9)/fn(\u03c9), equivalently. \u03bb\\n \u2190 \u03bbq \u2212 \u03bbn;\n2 compute the tilted distribution by inserting the likelihood term p\u0303n(\u03c9) \u221d q\\n(\u03c9)fn(\u03c9);\n3 compute a projection update: \u03bbq \u2190 argmin\u03bbD\u03b1[p\u0303n||q\u03bb] with q\u03bb an exponential family with natural parameter \u03bb; 4 recover the site approximation by \u03bbn \u2190 \u03bbq \u2212 \u03bb\\n and form the final update \u03bbq \u2190 \u2211 n \u03bbn + \u03bb0.\nWhen converged, the solutions of \u03bbn return a fixed point of the so called power EP energy:\nLPEP(\u03bb0, {\u03bbn}) = logZ(\u03bb0) + ( N\n\u03b1 \u2212 1) logZ(\u03bbq)\u2212\n1\n\u03b1 N\u2211 n=1 log \u222b fn(\u03c9) \u03b1 exp [ (\u03bbq \u2212 \u03b1\u03bbn)T\u03c6(\u03c9) ] d\u03c9. (11)\nBut more importantly, before convergence all these local parameters \u03bbn are maintained in memory. This indicates that power EP does not scale with big data: consider Gaussian approximations which has O(d2) parameters with d the dimensionality of \u03c9. Then the space complexity of power EP is O(Nd2), which is clearly prohibitive for big models like neural networks that are typically applied to large datasets. BB-\u03b1 provides a simple solution of this memory overhead by sharing the local parameters, i.e. defining \u03bbn = \u03bb for all n = 1, ..., N . Furthermore, under the mild condition that the exponential family is regular, there exist a one-to-one mapping between \u03bbq and \u03bb (given a fixed \u03bb0). Hence we arrive at a \u201cglobal\u201d optimisation problem in the sense that only one parameter \u03bbq is optimised, where the objective function is the BB-\u03b1 energy\nL\u03b1(\u03bb0, \u03bbq) = logZ(\u03bb0)\u2212 logZ(\u03bbq)\u2212 1\n\u03b1 N\u2211 n=1 logEq [(\nfn(\u03c9)\nexp [\u03bbT\u03c6(\u03c9)]\n)\u03b1] . (12)\nOne could verify that this is equivalent to the BB-\u03b1 energy function presented in the main text by considering exponential family q distributions.\nAlthough empirical evaluations have demonstrated the superior performance of BB-\u03b1, the original formulation is difficult to interpret for practitioners. First the local alpha-divergence minimisation interpretation is inherited from power EP, and\nthe intuition of power EP itself might already pose challenges for practitioners. Second, the derivation of BB-\u03b1 from power EP is ad hoc and lacks theoretical justification. It has been shown that power EP energy can be viewed as the dual objective to a continuous version of Bethe free-energy, in which \u03bbn represents the Lagrange multiplier of the constraints in the primal problem. Hence tying the Lagrange multipliers would effectively changes the primal problem, thus losing a number of nice guarantees. Nevertheless this approximation has been shown to work well in real-world settings, which motivated our work to extend BB-\u03b1 to dropout approximation."}, {"heading": "D. Full Regression Results", "text": ""}], "references": [{"title": "Concrete problems in ai safety", "author": ["Amodei", "Dario", "Olah", "Chris", "Steinhardt", "Jacob", "Christiano", "Paul", "Schulman", "John", "Mane", "Dan"], "venue": "arXiv preprint arXiv:1606.06565,", "citeRegEx": "Amodei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2016}, {"title": "Multi-task deep neural network to predict CpG methylation profiles from low-coverage sequencing data", "author": ["C Angermueller", "O. Stegle"], "venue": "In NIPS MLCB workshop,", "citeRegEx": "Angermueller and Stegle,? \\Q2015\\E", "shortCiteRegEx": "Angermueller and Stegle", "year": 2015}, {"title": "Ensemble learning in Bayesian neural networks", "author": ["Barber", "David", "Bishop", "Christopher M"], "venue": "NATO ASI SERIES F COMPUTER AND SYSTEMS SCIENCES,", "citeRegEx": "Barber et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Barber et al\\.", "year": 1998}, {"title": "Weight uncertainty in neural network", "author": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "Deep gaussian processes for regression using approximate expectation propagation", "author": ["Bui", "Thang D", "Hern\u00e1ndez-Lobato", "Daniel", "Li", "Yingzhen", "Jos\u00e9 Miguel", "Turner", "Richard E"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Bui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bui et al\\.", "year": 2016}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Carlini", "Nicholas", "Wagner", "David"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "Carlini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carlini et al\\.", "year": 2016}, {"title": "Transforming neural-net output levels to probability distributions", "author": ["Denker", "John", "LeCun", "Yann"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denker et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Denker et al\\.", "year": 1991}, {"title": "Learning and policy search in stochastic dynamical systems with bayesian neural networks", "author": ["Depeweg", "Stefan", "Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Doshi-Velez", "Finale", "Udluft", "Steffen"], "venue": "arXiv preprint arXiv:1605.07127,", "citeRegEx": "Depeweg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Depeweg et al\\.", "year": 2016}, {"title": "Uncertainty in Deep Learning", "author": ["Gal", "Yarin"], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "Gal and Yarin.,? \\Q2016\\E", "shortCiteRegEx": "Gal and Yarin.", "year": 2016}, {"title": "Bayesian convolutional neural networks with Bernoulli approximate variational inference", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "ICLR workshop track,", "citeRegEx": "Gal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2016}, {"title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": null, "citeRegEx": "Gal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "Neue begr\u00fcndung der theorie quadratischer formen von unendlichvielen ver\u00e4nderlichen", "author": ["Hellinger", "Ernst"], "venue": "Journal fu\u0308r die reine und angewandte Mathematik,", "citeRegEx": "Hellinger and Ernst.,? \\Q1909\\E", "shortCiteRegEx": "Hellinger and Ernst.", "year": 1909}, {"title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "author": ["Hernandez-Lobato", "Jose Miguel", "Adams", "Ryan"], "venue": "In ICML,", "citeRegEx": "Hernandez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hernandez.Lobato et al\\.", "year": 2015}, {"title": "Black-box alpha divergence minimization", "author": ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Li", "Yingzhen", "Hern\u00e1ndezLobato", "Daniel", "Bui", "Thang", "Turner", "Richard E"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2016}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Hinton", "Geoffrey E", "Van Camp", "Drew"], "venue": "In COLT,", "citeRegEx": "Hinton et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1993}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Modelling uncertainty in deep learning for camera relocalization", "author": ["Kendall", "Alex", "Cipolla", "Roberto"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Kendall et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kendall et al\\.", "year": 2016}, {"title": "Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding", "author": ["Kendall", "Alex", "Badrinarayanan", "Vijay", "Cipolla", "Roberto"], "venue": "arXiv preprint arXiv:1511.02680,", "citeRegEx": "Kendall et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kendall et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Information theory and statistics", "author": ["Kullback", "Solomon"], "venue": null, "citeRegEx": "Kullback and Solomon.,? \\Q1959\\E", "shortCiteRegEx": "Kullback and Solomon.", "year": 1959}, {"title": "On information and sufficiency", "author": ["Kullback", "Solomon", "Leibler", "Richard A"], "venue": "The annals of mathematical statistics,", "citeRegEx": "Kullback et al\\.,? \\Q1951\\E", "shortCiteRegEx": "Kullback et al\\.", "year": 1951}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural Computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia", "M Ranzato", "F. Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "R\u00e9nyi divergence variational inference", "author": ["Li", "Yingzhen", "Turner", "Richard E"], "venue": "In NIPS,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Stochastic expectation propagation", "author": ["Li", "Yingzhen", "Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Turner", "Richard E"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A practical Bayesian framework for backpropagation networks", "author": ["MacKay", "David JC"], "venue": "Neural Computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tom\u00e1\u0161", "Karafi\u00e1t", "Martin", "Burget", "Luk\u00e1\u0161", "\u010cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In Eleventh Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Divergence measures and message passing", "author": ["Minka", "Tom"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "Minka and Tom.,? \\Q2005\\E", "shortCiteRegEx": "Minka and Tom.", "year": 2005}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Minka,? \\Q2001\\E", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Bayesian learning for neural networks", "author": ["Neal", "Radford M"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Neal and M.,? \\Q1995\\E", "shortCiteRegEx": "Neal and M.", "year": 1995}, {"title": "cleverhans v1.0.0: an adversarial machine learning library", "author": ["Papernot", "Nicolas", "Goodfellow", "Ian", "Sheatsley", "Ryan", "Feinman", "Reuben", "McDaniel", "Patrick"], "venue": "arXiv preprint arXiv:1610.00768,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "On measures of entropy and information", "author": ["R\u00e9nyi", "Alfr\u00e9d"], "venue": "Fourth Berkeley symposium on mathematical statistics and probability,", "citeRegEx": "R\u00e9nyi and Alfr\u00e9d.,? \\Q1961\\E", "shortCiteRegEx": "R\u00e9nyi and Alfr\u00e9d.", "year": 1961}, {"title": "Learning internal representations by error propagation", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["Sennrich", "Rico", "Haddow", "Barry", "Birch", "Alexandra"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "LSTM neural networks for language modeling", "author": ["Sundermeyer", "Martin", "Schl\u00fcter", "Ralf", "Ney", "Hermann"], "venue": "In INTERSPEECH,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Two problems with variational expectation maximisation for time-series models. Inference and Estimation in Probabilistic Time-Series", "author": ["RE Turner", "M. Sahani"], "venue": null, "citeRegEx": "Turner and Sahani,? \\Q2011\\E", "shortCiteRegEx": "Turner and Sahani", "year": 2011}, {"title": "R\u00e9nyi divergence and Kullback-Leibler divergence", "author": ["Van Erven", "Tim", "Harremo\u00ebs", "Peter"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Erven et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L Wan", "M Zeiler", "S Zhang", "Y LeCun", "R. Fergus"], "venue": "In ICML-13,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Fast predictive image registration", "author": ["Yang", "Xiao", "Kwitt", "Roland", "Niethammer", "Marc"], "venue": "arXiv preprint arXiv:1607.02504,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al.", "startOffset": 84, "endOffset": 233}, {"referenceID": 42, "context": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al.", "startOffset": 84, "endOffset": 233}, {"referenceID": 41, "context": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al.", "startOffset": 84, "endOffset": 233}, {"referenceID": 40, "context": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al.", "startOffset": 84, "endOffset": 233}, {"referenceID": 31, "context": "Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al.", "startOffset": 84, "endOffset": 233}, {"referenceID": 38, "context": ", 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al., 2016).", "startOffset": 139, "endOffset": 162}, {"referenceID": 0, "context": "sion model is given an adversarial image (studied below), or to tackle many problems in AI safety (Amodei et al., 2016).", "startOffset": 98, "endOffset": 119}, {"referenceID": 3, "context": "Originating in the 90s (Neal, 1995; MacKay, 1992; Denker & LeCun, 1991), Bayesian neural networks (BNNs) in particular have started gaining in popularity again (Graves, 2011; Blundell et al., 2015; HernandezLobato & Adams, 2015).", "startOffset": 160, "endOffset": 228}, {"referenceID": 3, "context": "Many approximations have been proposed over the years (Denker & LeCun, 1991; Neal, 1995; Graves, 2011; Blundell et al., 2015; Hernandez-Lobato & Adams, 2015; Hern\u00e1ndez-Lobato et al., 2016), some more practical and some less practical.", "startOffset": 54, "endOffset": 188}, {"referenceID": 15, "context": "Many approximations have been proposed over the years (Denker & LeCun, 1991; Neal, 1995; Graves, 2011; Blundell et al., 2015; Hernandez-Lobato & Adams, 2015; Hern\u00e1ndez-Lobato et al., 2016), some more practical and some less practical.", "startOffset": 54, "endOffset": 188}, {"referenceID": 37, "context": "A practical approximation for inference in Bayesian neural networks should be able to scale well to large data and complex models (such as convolutional neural networks (CNNs) (Rumelhart et al., 1985; LeCun et al., 1989)).", "startOffset": 176, "endOffset": 220}, {"referenceID": 26, "context": "A practical approximation for inference in Bayesian neural networks should be able to scale well to large data and complex models (such as convolutional neural networks (CNNs) (Rumelhart et al., 1985; LeCun et al., 1989)).", "startOffset": 176, "endOffset": 220}, {"referenceID": 18, "context": "One possible solution for practical inference in BNNs is variational inference (VI) (Jordan et al., 1999), a ubiquitous technique for approximate inference.", "startOffset": 84, "endOffset": 105}, {"referenceID": 21, "context": "These have been used for machine vision and medical applications (Kendall & Cipolla, 2016; Kendall et al., 2015; Angermueller & Stegle, 2015; Yang et al., 2016).", "startOffset": 65, "endOffset": 160}, {"referenceID": 46, "context": "These have been used for machine vision and medical applications (Kendall & Cipolla, 2016; Kendall et al., 2015; Angermueller & Stegle, 2015; Yang et al., 2016).", "startOffset": 65, "endOffset": 160}, {"referenceID": 17, "context": "Dropout variational inference can be implemented by adding dropout layers (Hinton et al., 2012; Srivastava et al., 2014) before every weight layer in the NN model.", "startOffset": 74, "endOffset": 120}, {"referenceID": 15, "context": "Black-box \u03b1-divergence minimisation (Hern\u00e1ndez-Lobato et al., 2016; Li & Turner, 2016; Minka, 2005) is a class of approximate inference methods extending on VI, approximating EP\u2019s energy function (Minka, 2001) as well as the Hellinger distance (Hellinger, 1909).", "startOffset": 36, "endOffset": 99}, {"referenceID": 33, "context": ", 2016; Li & Turner, 2016; Minka, 2005) is a class of approximate inference methods extending on VI, approximating EP\u2019s energy function (Minka, 2001) as well as the Hellinger distance (Hellinger, 1909).", "startOffset": 136, "endOffset": 149}, {"referenceID": 15, "context": "To implement a complex CNN structure with the inference and code of (Hern\u00e1ndez-Lobato et al., 2016), for example, one would be required to re-implement many already-implemented software tools.", "startOffset": 68, "endOffset": 99}, {"referenceID": 3, "context": "Existing approaches to approximate this posterior are through variational inference (as was done in Hinton & Van Camp (1993); Barber & Bishop (1998); Graves (2011); Blundell et al. (2015)).", "startOffset": 165, "endOffset": 188}, {"referenceID": 17, "context": "Given a (deterministic) neural network, stochastic regularisation techniques in the model (such as dropout (Hinton et al., 2012; Srivastava et al., 2014)) can be interpreted as variational Bayesian approximations in a Bayesian NN with the same network structure (Gal & Ghahramani, 2016b).", "startOffset": 107, "endOffset": 153}, {"referenceID": 45, "context": ", 2014) or dropConnect (Wan et al., 2013)).", "startOffset": 23, "endOffset": 41}, {"referenceID": 15, "context": "In this section we provide a brief review of the black box alpha (BB-\u03b1, Hern\u00e1ndez-Lobato et al. (2016)) method upon which the main derivation in this paper is based.", "startOffset": 72, "endOffset": 103}, {"referenceID": 18, "context": "Popular methods of approximate inference include variational inference (VI) (Jordan et al., 1999) and expectation propagation (EP) (Minka, 2001), where these two algorithms are special cases of power EP (Minka, 2004) that minimises Amari\u2019s \u03b1-divergence (Amari, 1985) D\u03b1[p||q] in a local way:", "startOffset": 76, "endOffset": 97}, {"referenceID": 33, "context": ", 1999) and expectation propagation (EP) (Minka, 2001), where these two algorithms are special cases of power EP (Minka, 2004) that minimises Amari\u2019s \u03b1-divergence (Amari, 1985) D\u03b1[p||q] in a local way:", "startOffset": 41, "endOffset": 54}, {"referenceID": 29, "context": "The recently proposed stochastic EP (Li et al., 2015) and BB-\u03b1 (Hern\u00e1ndezLobato et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 15, "context": "BB-\u03b1 has been successfully applied to Bayesian neural networks for regression, classification (Hern\u00e1ndez-Lobato et al., 2016) and model-based reinforcement learning (Depeweg et al.", "startOffset": 94, "endOffset": 125}, {"referenceID": 7, "context": ", 2016) and model-based reinforcement learning (Depeweg et al., 2016).", "startOffset": 47, "endOffset": 69}, {"referenceID": 15, "context": "One could verify that this is the same energy function as presented in (Hern\u00e1ndez-Lobato et al., 2016) by considering q an exponential family distribution.", "startOffset": 71, "endOffset": 102}, {"referenceID": 15, "context": "But empirically Hern\u00e1ndez-Lobato et al. (2016) showed that the bias introduced by the MC approximation is often dominated by the variance of the samples, meaning that the effect of the bias is negligible.", "startOffset": 16, "endOffset": 47}, {"referenceID": 15, "context": "The original paper (Hern\u00e1ndez-Lobato et al., 2016) proposed a naive implementation which directly evaluates the MC estimation (4) with samples \u03c9\u0302k \u223c q(\u03c9).", "startOffset": 19, "endOffset": 50}, {"referenceID": 27, "context": "see (LeCun et al., 2006)1.", "startOffset": 4, "endOffset": 24}, {"referenceID": 7, "context": "The Hellinger value could be used to achieve a balance between reducing training error and improving predictive likelihood, which has been found to be desirable (Hern\u00e1ndezLobato et al., 2016; Depeweg et al., 2016).", "startOffset": 161, "endOffset": 213}, {"referenceID": 4, "context": "As a comparison we also include test performances of a BNN with a Gaussian approximation (VI-G) (Li & Turner, 2016), a BNN with HMC, and a sparse Gaussian process model with 50 inducing points (Bui et al., 2016).", "startOffset": 193, "endOffset": 211}, {"referenceID": 11, "context": "The first attack in consideration is the Fast Gradient Sign (FGS) method (Goodfellow et al., 2014).", "startOffset": 73, "endOffset": 98}, {"referenceID": 35, "context": "We use the single gradient step FGS implemented in Cleverhans (Papernot et al., 2016) with the stepsize \u03b7 varied between 0.", "startOffset": 62, "endOffset": 85}], "year": 2017, "abstractText": "To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI\u2019s KL objective, which are able to avoid VI\u2019s uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model\u2019s epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model\u2019s uncertainty.", "creator": "LaTeX with hyperref package"}}}