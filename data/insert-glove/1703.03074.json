{"id": "1703.03074", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Learning the Probabilistic Structure of Cumulative Phenomena with Suppes-Bayes Causal Networks", "abstract": "christiania One bullins of the darold critical issues novospassky when bumble adopting rets Bayesian reelers networks (castagnetti BNs) bucchi to model dependencies among probabilism random variables curtice is to \" 5.9-magnitude learn \" tidel their structure, given madredeus the huge federa\u00e7\u00e3o search space of kheng possible venson solutions, i. e. , envi all the midan possible direct industriebank acyclic graphs. 2420 This aragvi is muchmoremusic a well - known tvrdik NP - hard kroloff problem, makgadikgadi which aitchison is ferenczy also complicated roys by cuh known cosmopterix pitfalls such sadrist as gaulle the cherkassky issue of printshop I - deidre equivalence golshiri among different braca structures. villian In this work gelang we second-generation restrict bjelanovic the investigations touchier on spiracles BN enevoldson structure ohchr learning to shi\u2019a a g\u00f3recki specific class of borai networks, i. e. , those representing the esos dynamics techno-thriller of phenomena characterized consilience by the monotonic accumulation squeezable of okal events. neyer Such phenomena allow 4-52 to irangate set specific structural constraints igo based on Suppes ' 16:11 theory jamelia of probabilistic causation deputise and, accordingly, susceptibilities to cflp define bandi constrained oyewole BNs, iharos named Suppes - populated Bayes Causal Networks (bases-loaded SBCNs ). We tomodachi here \u215b investigate writeoff the preppy structure bulgargaz learning sudanian of lineage SBCNs via yanked extensive simulations inflammatories with langkloof various state - of - swordfighting the - l\u0129nh art steal search zorros strategies, 90.66 such lollywood as correctly canonical pajhwok local ippolita search jiankun techniques mukono and Genetic sibille Algorithms. Among jelavi\u0107 the main results et we impingement show smikle that Suppes ' constraints pe\u030dh deeply sironi simplify siderca the vivienda learning klauss task, kowroski by reducing the solution mandzukic search cookout space and gianaris providing 0.16 a paju temporal ordering on blinker the variables.", "histories": [["v1", "Wed, 8 Mar 2017 23:50:19 GMT  (640kb,D)", "http://arxiv.org/abs/1703.03074v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["daniele ramazzotti", "marco s nobile", "marco antoniotti", "alex graudenzi"], "accepted": false, "id": "1703.03074"}, "pdf": {"name": "1703.03074.pdf", "metadata": {"source": "CRF", "title": "Learning the Probabilistic Structure of Cumulative Phenomena with Suppes-Bayes Causal Networks", "authors": ["Daniele Ramazzotti", "Marco S. Nobile"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Bayesian Graphical Models, Structure Learning, Probabilistic Causality, Cumulative Phenomena, Cascading Failure\nI. INTRODUCTION\nBAYESIAN networks (BNs) are probabilisticgraphical models representing the relations of conditional dependence among random variables, encoded in directed acyclic graphs (DAGs) [1]. In the last decades, BNs have been effectively applied in several different fields and disciplines, such as\n(but not limited to) diagnostics and predictive analytics [1].\nOne of the most challenging task with BNs is that of learning their structure from data. Two main approaches are commonly used to tackle this problem:\n1) Constraint-based techniques: mainly due to the works by Judea Pearl [2], these approaches aim at discovering the relations of conditional independence from the data, using them as constraints to learn the network; 2) Score-based techniques: in this case the problem of learning the structure of a BN is defined as an optimization problem (specifically, maximization) where the search space of the valid solutions (i.e., all the possible DAGs) is evaluated via a score based on a likelihood function [1].\nRegardless of the approach, the main difficulty in this learning problem is the huge number of valid solutions in the search space, namely, all the possible DAGs, which makes this task a known NP - hard problem, even when constraining each node to have at most two parents [3], [4]. Therefore, all state-of-the-art techniques solve this task by means of meta-heuristics [1], [5], [6].\nThe inference is further complicated by the wellknown issue of I-equivalence: BNs with even very different structures can encode the same set of conditional independence properties [1]. Thus, any algorithm for structural learning can converge to a\nar X\niv :1\n70 3.\n03 07\n4v 1\n[ cs\n.L G\n] 8\nM ar\n2 set of equivalent structures rather than to the correct one, given that the inference itself is performed by learning the statistical relations among the variables emerging from their induced distributions rather than the structure itself [1].\nIn this paper, we investigate the application of BNs for the characterization of a specific class of dynamical phenomena, i.e., those driven by the monotonic accumulation of events. In particular, the process being modeled/observed must imply that there is: (i) a temporal ordering among its events (i.e., the nodes in the BN), and (ii) a monotonic accumulation over time, which probabilistically entails that the occurrence of an earlier event must be positively correlated to the subsequent occurrence of its successors, leading to a significant temporal pattern [7]. An example can be found in the dynamics of cascading failures, that is a failure in a system of interconnected parts where the failure of a part can trigger the failure of successive parts. This phenomenon can happen in different contexts, such as power transmission, computer networking, finance and biological systems.In these scenarios, different configurations may lead to failure, but some of them are more likely than others and, hence, can be modeled probabilistically [8].\nThe two particular conditions mentioned above represent the basis of the notion of probabilistic causation by Patrick Suppes [9], [10], and allow us to define a set of structural constraints to the BNs to be inferred, which, accordingly, have been dubbed as Suppes-Bayes Causal Networks (SBCNs) in previous works [11], [7]. SBCNs have been already applied in a number of different fields, ranging from cancer progression inference [12], [13], [14] to social discrimination discovery [11].\nWe specifically position our work within the aforementioned optimization-based framework for BN structure learning. The goal of this paper is to investigate how structure learning is influenced by different algorithmic choices, when representing cumulative dynamical phenomena. In particular, it is known that given a temporal ordering on the variables (i.e., a partially ordered set among the events, poset in the terminology of Bayesian networks) of a BN, finding the optimal solution that\nis consistent with the ordering can be accomplished in time O(nk), where n is the number of variables and k the bounded in-degree of a node [15], [16]. Thus, the search in the space of orderings can be performed way more efficiently than the search in the space of structures, as the search space is much smaller, the branching factor is lower and acyclicity checks are not necessary [6], [17].\nThe determination of the right ordering in complex dynamical phenomena is generally a difficult task, which often requires considerable domain knowledge. However, the representation of cumulative phenomena via SBCNs allows to soften this hurdle, as Suppes\u2019 constraints dramatically reduce the search space of valid solutions, also providing a temporal ordering on the variables. This represents a serious theoretical advancement in structure learning of BNs for the modeling of cumulative phenomena, which we investigate in this work with a series of synthetic experiments.\nIn particular, in this paper we quantitatively assess the performance of learning the structure of a BN when: \u2022 the temporal ordering among variables is given\n/ not given, i.e., when Suppes\u2019 constraints are imposed / not imposed (in the former case we deal with SBCNs); \u2022 different heuristic search strategies are adopted, i.e., Hill Climbing (HC), Tabu Search (TS), and Genetic Algorithms (GA); \u2022 different regularization terms are used (e.g., Bayesian Information Criterion (BIC) and Akaike information criterion (AIC)).\nThe paper is structured as follows. In Section II we first provide a background on BNs and on the task of structure learning, with details on the classical ideas that are the foundations of our framework. Then, in Section III we describe the key features of the cumulative phenomena we aim at representing, and we formally define SBCNs as a machinery for their modeling. We also introduce an approximate algorithm for the efficient inference of SBCNs. In Section IV we make use of synthetic data to simulate such cumulative phenomena and use them to compare the performance of the various search heuristics and strategies, especially by focus-\n3 ing on the impact of Suppes\u2019 constraints. Finally, in Section V a discussion of the results is provided."}, {"heading": "II. BACKGROUND", "text": "In this Section we provide an introduction to Bayesian networks together with a review of some state-of-the-art methods to tackle to problem of learning their structures from a set of observation D over the variables described in the network."}, {"heading": "A. Bayesian graphical models", "text": "A Bayesian network is a statistical graphical model that succinctly represents a joint distribution over n random variables and encodes it in a direct acyclic graph G = (V,E) over the n nodes V referring to the variables and their relations E (arcs in the DAG). Given the structure of a BN, the full joint distribution of the n variables can be written as the product of the conditional distributions on each variable. In fact, an edge between pair of nodes, e.g., A and B, denotes statistical dependence, i.e., P(A \u2227B) 6= P(A)P(B), regardless of which any other variables we condition on, that is, for any other set of variables C it holds that P(A \u2227B | C) 6= P(A | C)P(B | C) [1].\nIn such a DAG, the set of variables connected toward any node X determines its set of \u201cparent\u201d nodes \u03c0(X). We recall that, being the graph acyclic, a node cannot be both ancestor and descendant of another node, as this would obviously cause a directed cycle. Moreover, the joint distribution over all the variables can be written as\u220f X P(X | \u03c0(X)), where, if a node has no incoming edges (i.e., no parents), in the product we use its marginal probability P(X). Thus, to compute the probability of any combination of values over the variables, only the conditional probabilities of each variable given its parents must be parameterized. However, even in the simplest case of binary variables, the number of parameters in each conditional probability table is locally of exponential size: namely, 2|\u03c0(X)| \u2212 1. Thus, the total number of parameters needed to compute the full joint distribution is of size \u2211 X 2 |\u03c0(X)| \u2212 1, which is anyway considerably less than 2n \u2212 1 for sparse networks.\nA useful property of the graphical structure is that we can define, for each variable, a set of nodes called the Markov blanket such that, conditioned on it, this variable is independent of all the other variables in the network. It can be proven that, for any BN, the Markov blanket consists of a node\u2019s parents, its children and the parents of the children [1].\nWe also point out that the usage of the symmetrical notion of conditional dependence introduces important limitations in the task of learning the structure of a BN. As a matter of fact, we note that the two edges A \u2192 B and B \u2192 A denote equivalent dependence between A and B. Hence, two graphs having a different structure can model an identical set of independence and conditional independence relations (I-equivalence). This yields to the notion of Markov equivalence class as a partially directed acyclic graph, in which the edges that can take either orientation are left undirected. It is also known that two BNs are Markov equivalent when they have the same skeleton and the same v-structures, the former being the set of edges, ignoring their direction (e.g., A \u2192 B and B \u2192 A constitute a unique edge in the skeleton) and the latter being all the edge structures in which a variable has at least two parents, but those do not share an edge (e.g., A\u2192 B \u2190 C) [18].\nBNs have an interesting relation to canonical boolean logical operators \u2227, \u2228 and \u2295 and formulas over variables [19], [7]. In fact these formulas, which are \u201cdeterministic\u201d in principle, in BNs are naturally softened into probabilistic relations to allow some degree of uncertainty or noise. This probabilistic approach to modeling logic allows representation of qualitative relationships among variables in a way that is inherently robust to small perturbations by noise. For instance, the phrase \u201cin order to hear music when listening to an mp3, it is necessary and sufficient that the power is on and the headphones are plugged in\u201d can be represented by a probabilistic conjunctive formulation that relates power, headphones and music, in which the probability that music is audible depends only on whether power and headphones are present. On the other hand, there is a small probability that the\n4 music will still not play (perhaps we forgot to load any songs into the device) even if both power and headphones are on, and there is small probability that we will hear music even without power or headphone (perhaps we are next to a concert and overhear that music) [19], [20]."}, {"heading": "B. Approaches to learn the structure of a BN", "text": "In the the literature, there have been two initial families of methods aimed at learning the structure of a BN from data. The methods belonging to the first family aim to explicitly capture all the conditional independence relations encoded in the edges, and will be referred to as constraint based approaches (II-B1). The second family, that of score based approaches (II-B2), aims at the selection of a model that maximizes the likelihood of the data given the model. Since both approaches lead to intractability (NP -hardness) [3], [4], computing and verifying an exact solution is impractical. For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed. These algorithms are characterized by a polynomial complexity, although they only provide asymptotic guarantees of converging to optimal solutions.\nRecently, a third class of learning algorithms that takes advantage of specialized logical relations (mentioned in the previous section) have been introduced (II-B3). In the rest of this section we describe in detail some of these approaches, leaving to specific readings more detailed discussions [1], [19], [20].\n1) Constraint-based approaches: We briefly present an intuitive explanation of several common algorithms used for structure discovery by explicitly considering conditional independence relations between variables. For more detailed explanations and analyses of complexity, correctness and stability, we refer the reader to the related references [26], [27].\nThe basic idea behind this class of algorithms is to build a graph structure reflecting the independence relations in the observed data, thus matching as closely as possible the empirical distribution. The difficulty in this approach lies in the number\nof conditional pairwise independence tests that an algorithm would have to perform to test all possible relations. This number is indeed exponential, requiring to condition on a power set, when testing for the conditional independence between two variables. Because of this inherent intractability, this class of algorithms requires the introduction of some approximations.\n2) Score-based approaches: This approach to structural learning aims to the maximization of the likelihood of a set of observed data. Since we assume that the data are independent and identically distributed, the likelihood of the data L(\u00b7) is simply the product of the probability of each observation. That is,\nL(D) = \u220f d\u2208D P(d)\nfor a set of observations D. Since we want to infer a model G that best explains the observed data, we define the likelihood of observing the data given a specific model G as:\nLL(G, D) = \u220f d\u2208D P(d | G) .\nHowever, the actual likelihood is never used in practice, as this quantity rapidly becomes very small and impossible to represent in a computer. Instead, the logarithm of the likelihood function is usually adopted for three reasons: (i) the log(\u00b7) function is monotonic; (ii) log-likelihood mitigates the numerical issues caused by normal likelihood; (iii) it is easy to compute, because the logarithm of a product is equal to the sum of the logs (e.g., log(xy) = log x + log y), and the likelihood for a Bayesian network is a product of simple terms [1].\nPractically, however, there is a problem in learning the network structure by maximizing loglikelihood alone. Namely, for any arbitrary set of data, the most likely graph is always the fully connected one (i.e., all edges are present), since adding an edge can only increase the likelihood of the data. To overcome this limitation, log-likelihood is almost always supplemented with a regularization term that penalizes the complexity of the model. There is a plethora of regularization terms, some based on information theory and others on Bayesian\n5 statistics (see [28] and references therein), which all serve to promote sparsity in the learned graph structure, though different regularization terms are better suited for particular applications [1], [20].\n3) Learning logically constrained networks: In Section II-A, we noted that an important class of BNs captures common binary logical operators, such as \u2227, \u2228, and \u2295. Although the learning algorithms mentioned above can be used to infer the structure of such networks, some algorithms employ knowledge of these logical constraints in the learning process.\nA widespread approach for the learning of monotonic progression networks with a directed acyclic graph (DAG) structure and conjunctive events are Conjunctive Bayesian Networks (see CBNs, [29]). This approach was originally adopted to model cancer progression in terms of accumulation of drivers genes [13], [14], in a way closely related to the model we discuss in this work.\nThis model is a standard BN over Bernoulli random variables with the constraint that the probability of a node X taking the value 1 is zero if at least one of its parents has value 0. This defines a conjunctive relationship, in that all the parents of X must be 1 for X to possibly be 1. Thus, this model alone cannot represent noise, which is an essential part of any real data. In response to this, hidden CBNs [30] were developed by augmenting the set of variables: a correspondence to a new variable Y that represents the observed state is assigned to each CBN variable X , which captures the \u201ctrue\u201d state. Thus, each new variable Y takes the value of the corresponding variable X with a high probability, and the opposite value with a low probability, in order to model noise observations. In this model, the variables X are latent, i.e., they are not present in the observed data, and have to be inferred from the observed values for the new variables."}, {"heading": "III. METHOD", "text": "In this Section we present the foundations of our framework and, specifically, we define the main characteristics of the Suppes-Bayes causal networks and of some heuristic strategies for the likelihood fit. Without losing in generality, from now on, we\nconsider a simplified formulation of the problem of learning the structure of BNs where all the variables depicted in the graph are Bernoulli random variables, i.e., their support is (0, 1). All the conclusions derived in these settings can be also directily applied to the general case where the nodes in the BN describe geneal random variables [1].\nMore precisely, we consider as an input for our learning task a dataset D of n Bernoulli variables and m cross-sectional samples. We assume the value 1 to indicate that a given variable has been observed in the sample and 0 that the variable had not been observed."}, {"heading": "A. Suppes-Bayes causal networks", "text": "In [9], Suppes introduced the notion of prima facie causation. A prima facie relation between a cause u and its effect v is verified when the following two conditions are true: \u2022 (i) temporal priority (TP): any cause happens\nbefore its effect; \u2022 (ii) probability raising (PR): the presence of\nthe cause raises the probability of observing its effect.\nDefinition 1 (Probabilistic causation, [9]). For any two events u and v, occurring respectively at times tu and tv , under the mild assumptions that 0 < P(u),P(v) < 1, the event u is called a prima facie cause of v if it occurs before and raises the probability of u, i.e.,{\n(TP ) tu < tv (PR) P(v | u) > P(v | u) (1)\nThe notion of prima facie causality has known limitations in the context of the general theories of causality [10], however, this characterizations seems to appropriate to model the dynamics of phenomena driven by the monotonic accumulation of events where a temporal order among them is implied and, thus, the occurrence of an early event positively correlates to the subsequent occurrence in time of a later one. Let us now recall again systems where cascading failure may occur: some configurations of events, in a specific order, may be more likely to cause a failure than others. This condition leads to\n6 the emergence of an observable temporal pattern among the events captured by Suppes\u2019 definition of causality in terms of statistial relevance, i.e., statistical dependency.\nLet us now consider a graphical representation of the aforementioned dynamics in terms of a BN G = (V,E). Furthermore, let us consider a given node vi \u2208 V and let us name \u03c0(vi) the set of all the nodes in V pointing to (and yet temporally preceding) vi. Then, the joint probability distribution of the n = |V | variables induced by the BN can be written as:\nP(v1 , . . . , vn) = \u220f vi\u2208V P(vi|\u03c0(vi)) (2)\nWhen building our model, we need to constrain the characteristics of the considered relations as depicted in the network (i.e., the arcs in the graph), in order to account for the cumulative process above mentioned, which, in turns, needs to be reflected in its induced probability distribution [7]. To this extent, we can define a class of BNs over Bernoulli random variables named Monotonic Progression Networks (MPNs) [7], [19], [31]. Intuitively, MPNs represent the progression of events monotonically1 accumulating over time, where the conditions for any event to happen is described by a probabilistic version of the canonical boolean operators, i.e., conjunction (\u2227), inclusive disjunction (\u2228), and exclusive disjunction (\u2295).\nMoreover, as discussed in [7], [19], [31], such MPNs can model accumulative phenomena in a probabilistic fashion, i.e., they are also modeling irregularities (noise) in the data as a small probability \u03b5 of not observing later events given their predecessors.\nGiven these premises, in [13] the authors describe an efficient algorithm (named CAPRI, see Algorithm 1) to learn the structure of constrained Bayesian networks which account for Suppes\u2019 criteria and which later on are dubbed Suppes-Bayes Causal Networks (SBCNs) in [11]. SBCNs are well suited to model cumulative phenomena as they may encode irregularities in a similar way to MPNs [7].\n1The events accumulate over time and when later events occur, earlier events are observed as well.\nThe efficient inference schema by Algorithm 1 rely on the observation (see [6]) that a way for circumventing the intrinsic computational complexity of the task of learning the structure of a Bayesian Network is to postulate a pre-determined ordering among the nodes. Intuitively, CAPRI exploits Suppes\u2019 theory to first mine an ordering among the nodes, reducing the complexity of the problem, and then fits the network by means of likelihood maximization. In [7] it is also shown that a SBCN, learned using Algorithm 1, can also embed the notion of accumulation through time as defined in a MPN, and, specifically, conjunctive parent sets; nevertheless SBCNs can easily be generalized to represent all the canonical boolean operetors (Extended Suppes-Bayes Causal Networks), notwithstanding an increase of the algorithmic complexity [7]. We refer the reader to [7] for further details and, following [11], we now formally define a SBCN.\nDefinition 2 (Suppes-Bayes Causal Network). Given an input cross-sectional dataset D of n Bernoulli variables and m samples, the SuppesBayes Causal Network SBCN = (V,E) subsumed by D is a directed acyclic graph such that the following requirements hold: [Suppes\u2019 constraints] for each arc (u \u2192 v) \u2208 E involving the selective advantage relation between nodes u, v \u2208 V , under the mild assumptions that 0 < P(u),P(v) < 1:\nP(u) > P(v) and P(v | u) > P(v | \u00acu) .\n[Simplification] let E\u2032 be the set of arcs satisfying the Suppes\u2019 constraints as before; among all the subsets of E\u2032, the set of arcs E is the one whose corresponding graph maximizes the likelihood of the data and the adopted regularization function R(f):\nE = argmax E\u2286E\u2032,G=(V,E)\nLL(G,D)\u2212R(f) . (3)\nBefore moving on, we once again notice that the efficient implementation of Suppes\u2019 constraints of Algorithm 1 does not, in general, guarantee to converge to the monotonic progression networks as depicted before. In fact, the probabilistic relations of the MPNs are defined on the \u03c0(vi), while Algorithm\n7 1 only considers pair of nodes rather than any pair vi and \u03c0(vi). To overcome this limitation, one could extend Algorithm 1 in order to learn, in addition to the network structure, also the logical relations involving any parent set, increasing the overall computational complexity. Once again, we refer the interested reader to the discussions provided in [13], [7] and, without losing in generality, for the purpose of this work, we consider the efficient implementation of Algorithm 1.\nWe conclude this Section by reporting the pseudocode (see Algorithm 1) of the efficient learning algorithm to infer SBCNs presented in [13], which we adopt for the assessment of the performance in the simulations of the next Section.\nAlgorithm 1: CAPRI Input: a dataset D of n Bernoulli variables. Result: a graphical model G = (V,E)\nrepresenting all the relations of \u201cprobabilistic causation\u201d.\n1 Let G \u2190 a complete graph over the vertices n; 2 forall arcs (u, v) \u2208 G do 3 Compute a score S(\u00b7) for the nodes u and v in terms of Suppes\u2019 criteria; 4 Remove the arc (u, v) if Suppes\u2019 criteria are not met; 5 end\n6 Let E \u2190 the subset of the remaining arcs E\u2032 \u2208 G, that maximize the log-likelihood of the model, computed as: LL(G,D)\u2212R(f); 7 return The resulting graphical model G = (V,E)."}, {"heading": "B. Optimization and Evolutionary Computation", "text": "As reported in line 6 of Algorithm 1, the problem of the inference of SBCNs can be re-stated as an optimization problem, in which the goal is the maximization of a likelihood score. Regardless from the strategy used in the inference process, the huge size of the search space of valid solutions makes this problem very hard to solve. Moreover, as stated above, the problem of learning the structure of a\nBN is NP -hard [4]. Because of that, state-of-theart techniques largely rely on heuristics [1], often based on stochastic population-based global optimization algorithms. For instance, methods based on Genetic Algorithms (GAs) [5], [17] and Ant Colony Optimization [32], [33] have been proposed in the literature.\nGenetic Algorithms: GAs were introduced by Holland in 1975 [34] as a global search methodology inspired by the mechanisms of natural selection. In GA, a population P of candidate solutions (named individuals) iteratively evolves, converging towards the global optimum of a given fitness function f that, in this context, corresponds to the score to be maximized.\nGAs are characterized by a well-known convergence theorem named schema theorem, which proves that the presence of a schema (that is, a template of solutions) in the population, i.e., positively affecting the fitness value, increases exponentially generation after generation. GAs were shown to be effective for BN learning, both in the case of available and not available a priori knowledge about nodes\u2019 ordering [5], [17].\nThe population P is composed of Q randomly created individuals, usually represented as fixedlength strings over a finite alphabet. These strings encode putative solutions of the problem under investigation; in the case of BN learning, individuals represent linearized adjacency matrices of candidate BNs with K nodes, encoded as string of binary values whose length2 is O(K2).\nThe individuals in P undergo an iterative process whereby three genetic operators, i.e., selection, crossover and mutation, are applied in sequence to simulate the evolution process, which results in a new population of possibly improved solutions. During the selection process, individuals from P are chosen, using a fitness-dependent sampling procedure [35]. The crossover operator is then used to recombine the structures of two promising parents taken from P\u2032 into new and improved offsprings.\n2Since BNs are DAGs, the representation can be reduced by not encoding the elements on the diagonal, which are always equal to zero. In such case, the strings representing the individual have length K \u00d7K \u2212K.\n8 The crossover is generally applied with a userdefined probability Pc. Finally, the mutation operator is used to introduce new genetic materials in the population allowing a further exploration of the search space. The mutation operator replaces an arbitrary symbol of the individual, with a probability Pm, with a random symbol taken from the alphabet. In the case of BNs, the mutation consists in flipping a single bit of the individual with a certain probability.\nWhen the genetic operators have been applied, the GA can proceed by replacing the whole population with the new offspring, or by identifying the best Q individuals among parents and offspring, which are then used to create the new population.\nIt is worth noting that in the case of ordered nodes both crossover and mutation are closed operators, because the resulting offsprings always encode valid DAGs. To the aim of ensuring a consistent population of individuals throughout the generations, in the case of unordered nodes the two operators are followed by a correction procedure, in which the candidate BN is analyzed to identify the presence of invalid cycles. For further information about our implementation of GAs for the inference of BNs, including the correction phase, we refer the interested reader to [17]."}, {"heading": "IV. RESULTS", "text": "We now discuss the results of a large number experiments on simulated data with the aim of assessing the performance of the state-of-the-art score-based techniques for the BN structure inference, and comparing the performance of these methods with the learning scheme defined in Algorithm 1. Our main objective is to investigate how the performance is effected by different algorithmic choices at the different steps of the learning process.\nData generation: All simulations are performed with the following generative models. We consider 6 different topological structures.\n1) trees: one predecessor at most for any node, one unique root (i.e., a node with no parents); 2) forests: likewise, more than one possible root;\n3) conjunctive DAGs with single root: 3 predecessors at most for each node, all the confluences are ruled by logical conjunctions, one unique root; 4) conjunctive DAGs with multiple roots: likewise, possible multiple roots; 5) disjunctive DAGs with single root: 3 predecessors at most for each node, all the confluences are ruled by logical disjunctions, one unique root; 6) disjunctive DAGs with multiple roots: likewise, possible multiple roots;\nWe constrain the induced distribution of each generative structure by implying a cumulative model for either conjunctions or disjunctions, i.e., any child node cannot occur if its parent set is not activated as described for the MPN in the Method Section III. For each of these configurations, we generate 100 random structures. Furthermore, we simulate a model of noise in terms of random observations (i.e., false positives and false negatives) included in the generated datasets with different rates, as described in the next Section.\nThese data generation configurations are chosen to reflect: (a) different structural complexities of the models in terms of number of parameters, i.e., arcs, to be learned, (b) different types of induced distributions suitable to model cumulative phenomena as defined by the MPNs (see Section III-A), i.e., conjunction (\u2227) or inclusive disjunction (\u2228)3 and, (c) situations of reduced sample sizes and noisy data.\nWe here provide an example of data generation. Let now n be the number of nodes we want to include in the network and let us set pmin = 0.05 and pmax = 0.95 as the minimum and maximum probabilities of any node. A directed acyclic graph without disconnected components (i.e., an instance of types (3) and (5) topologies) with maximum depth log n and where each node has at most w\u2217 = 3 parents is generated as shown in Algorithm 2.\n3Here we stick with the efficient search scheme of Algorithm 1 and, for this reason, we do not consider exclusive disjunction (\u2295) parent sets\n9 Algorithm 2: Data generation: single source directed acyclic graphs Input: n, the number of nodes of the graph,\npmin = 0.05 and pmax = 0.95 be the minimum and maximum probabilities of any node and w\u2217 = 3 the maximum incoming edges per node.\nResult: a randomly generated single source directed acyclic graph.\n1 Pick an event r \u2208 G as the root of the directed acyclic graph; 2 Assign to each node u 6= r an integer in the interval [2, dlog ne] representing its depth in the graph (1 is reserved for r), ensuring that each level has at least one node; 3 forall nodes u 6= r do 4 Let l be the level assigned to the node; 5 Pick |P(u)| uniformly over (0, w\u2217], and\naccordingly define the parents of u with events selected among those at which level l \u2212 1 was assigned;\n6 end 7 Assign P(r), a random value in the interval\n[pmin, pmax]; 8 forall events u 6= r do 9 Let \u03b1 be a random value in the interval\n[pmin, pmax]; 10 Let \u03c0(u) be the direct predecessor of u; 11 Then assign:\nP(u) = \u03b1P(x \u2208 \u03c0(u)) ;\n12 end 13 return The generated single source directed\nacyclic graph.\nPerformance assessment: In all these configurations, the performance is assessed in terms of:\n\u2022 accuracy = (TP+TN)(TP+TN+FP+FN) ; \u2022 sensitivity = TP(TP+FN) ; \u2022 specificity = TN(FP+TN) ;\nwith TP and FP being the true and false positives (we mark as positive any arc that is present in the network) and TN and FN being the true and false negatives (we mark negative any arc that is\nnot present in the network) with respect to the generative model. All these measures are values in [0, 1] with results close to 1 indicators of good performance.\nImplementation: In all the following experiments, the adopted likelihood functions (i.e., the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.\nAlgorithm settings: We test the performance of classical search strategies, such as Hill Climbing (HC) and Tabu Search (TS), and of more sophisticated algorithms such Genetic Algorithms (GA)4.\nFor HC and TS, we generate data as described above with networks of 10 and 15 nodes (i.e., 0/1 Bernoulli random variables). We generated 10 independent datasets for each combination of the 4 sample levels (i.e., 50, 100, 150 and 200 samples) and the 9 noise levels (i.e., from 0% to 20% with step 2.5%) for a total of 4, 320, 000 independent datasets. The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations. While a detailed description of these regularizators is beyond the scope of this paper, we critically discuss the different performances granted by each strategy for the inference of BNs.\nWith respect to GA we used a restricted data generation settings, using networks of 15 nodes, datasets of 100 samples and 5 noise levels (from 0% to 20% with step 5%) for a total of 3, 000 independent datasets. We tested the GA either (i) with or (ii) without Suppes\u2019 constraints, using BIC\n4Further experiments on multi-objective optimization techniques, such as Non-dominated Sorting Genetic Algorithm (NSGA- II), were performed, but are not shown here because of the worse overall performance, and of the higher computational cost, with respect to canonical GA.\n10\nregularization term, leading to the final total of 6, 000 different configurations. Finally, the GA was launched with a population size of 32 individuals, a mutation rate of 0.01 and 100 generations.\nWe summarize the performance evaluation of the distinct techniques and settings in the next Subsections and in Figures 1, 2, 3 and 4.\nPerformance assessment\nBy looking at Figure 1, one can first appreciate the variation of accuracy with respect to a specific search strategy, i.e., HC with BIC, which is taken as an example of typical behavior. In brief, the overall performance worsens with respect to: (i) a larger number of nodes in the network, (i) more complex generative structures, and (iii) smaller samples sizes / higher noise rates. Although such a trend is intuitively expected, given the larger number of parameters to be learned for more complex models, we here underline the role of statistical complications, such as the presence of spurious correlations [44] and the occurrence of Simpson\u2019s paradox [45].\nFor instance, it is interesting to observe a typical decrease of the accuracy when we compare topologies with the same properties, but different number of roots (i.e., 1 root vs. multiple roots). In the former case, we expect, in fact, a lower number of arcs (i.e., dependencies) to be learned (on average) and, hence, we may attribute the decrease of the performance to the emergence of spurious correlations among independent nodes, such as the children of the different sources of the DAG. This is due to the fact that, when sample sizes are not infinite, it is very unlikely to observe perfect independence and, accordingly, the likelihood scores may lead to overfitting. The trends displayed in Figure 1 are shared by most of the analyzed search strategies.\nRole of the regularization factor: By looking at Figure 2 one can first notice that the accuracy with no regularization is dramatically lower than the other cases, as a consequence of the expected overfitting (in this case we compare the performance of HC on disjunctive DAGs with multiple\nroots, but the trend is maintained in the other cases). Conversely, all regularization terms ensure the inference of sparser models, by penalizing the number of retrieved arcs.\nBDE regularization term seems to be the only exception (see Figure 2), leading to unintuitive behaviors: in fact, while for all the other methods the performance decreases when higher level of noise are applied, for BDE the accuracy seems to improve with higher noise rates. This result might be explained by observing that given a topological structure, structural spurious correlations may arise between a given node and any of its undirected predecessors (i.e., one of the predecessors of its direct parents): with higher error rates, and, accordingly, more random samples in the datasets, all the correlations are reduced, hence leading to a lower impact of the regularization term. Given these considerations, one can hypothesize that the overall trend of BDE is due to a scarce penalization to the likelihood fit, favoring dense networks rather than sparse ones.\nSearch strategies: No significant differences in the performance between the accuracy of HC, TS and GA are observed. However, one can observe a consistent improvement in sensitivity when using GA (see Figures 3 and 4). This suggests different inherent properties of the search schemes: while with HC and TB the regularization terms, rather than the search strategy, account for most of the inference performance, GAs are capable of returning denser networks with better hit rates. This is probably due to GA\u2019s random mutations, which allow jumps into areas of the search space characterized by excellent fitness, which could not be reached by means of greedy approaches like HC.\nSuppes\u2019 structural constraints: Finally, the most important result, which can be observed across all the different experiments, is that the overall performance of all the considered search strategies is dramatically enhanced by the introduction of Suppes\u2019 structural constraints. In particular, as one can see, e.g., in Figure 1, there is a constant improvement in the inference, up to 10%, when\n11\n12\nSuppes\u2019 priors are used. Even though the accuracy of the inference is affected by the noise in the observations, in fact, the results with Suppes\u2019 priors are consistently better than the inference with no constraints, with respect to all the considered inference settings and to all the performance measures. This is an extremely important result as it proves that the introduction of structural constraints based on Suppes\u2019 probabilistic causation indeed simplify the optimization task, by reducing the huge search space, when dealing with BNs describing cumulative phenomena."}, {"heading": "V. CONCLUSION", "text": "In this paper we investigated the structure learning of Bayesian Networks aimed at modeling phenomena driven by the monotonic accumulation of events over time. To this end, we made use of a subclass of constrained Bayesian networks named Suppes-Bayes Causal Networks, which in-\nclude structural constraints grounded in Suppes\u2019 theory of probabilistic causation.\nWhile the problem of learning the structure of a Bayesian Network is known to be untractable, such constraints allow to prune the search space of the possible solutions, leading to a tremendous reduction of the number of valid networks to be considered, hence taming the complexity of the problem in a remarkable way.\nWe here discussed the theoretical implications of the inference process at the different steps, also by comparing various state-of-the-art algorithmic approaches and regularization methods. We finally provided an in-depth study on realistically simulated data of the effect of each inference choice, thus providing some sound guidelines for the design of efficient algorithms for the inference of models of cumulative phenomena.\nAccording to our results, Genetic Algorithms outperform Hill Climbing and Tabu Search as search strategy, with respect to both sensitivity and specificity. Above all, we could prove that Suppes\u2019 constraints consistently improve the inference accuracy, in all the considererd scenarios and with all the inference schemes, hence positioning SBCNs as the new benchmark in the the efficient inference and representation of cumulative phenomena."}], "references": [{"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Learning bayesian networks is npcomplete", "author": ["D.M. Chickering"], "venue": "Learning from data. Springer, 1996, pp. 121\u2013130.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Largesample learning of bayesian networks is np-hard", "author": ["D.M. Chickering", "D. Heckerman", "C. Meek"], "venue": "Journal of Machine Learning Research, vol. 5, no. Oct, pp. 1287\u2013 1330, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Structure learning of bayesian networks by genetic algorithms", "author": ["P. Larranaga", "M. Poza", "Y. Yurramendi", "R.H. Murga", "C.M.H. Kuijpers"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 18, no. 9, pp. 912\u2013926, 1996.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Ordering-based search: A simple and effective algorithm for learning bayesian networks", "author": ["M. Teyssier", "D. Koller"], "venue": "arXiv preprint arXiv:1207.1429, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Catastrophic cascade of failures in interdependent networks", "author": ["S.V. Buldyrev", "R. Parshani", "G. Paul", "H.E. Stanley", "S. Havlin"], "venue": "Nature, vol. 464, no. 7291, pp. 1025\u20131028, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "A probabilistic theory of causality", "author": ["P. Suppes"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1970}, {"title": "Probabilistic causation", "author": ["C. Hitchcock"], "venue": "Stanford encyclopedia of philosophy, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Exposing the probabilistic causal structure of discrimination", "author": ["F. Bonchi", "S. Hajian", "B. Mishra", "D. Ramazzotti"], "venue": "arXiv preprint arXiv:1510.00552, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring tree causal models of cancer progression with probability raising", "author": ["L.O. Loohuis", "G. Caravagna", "A. Graudenzi", "D. Ramazzotti", "G. Mauri", "M. Antoniotti", "B. Mishra"], "venue": "PloS one, vol. 9, no. 10, p. e108358, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Capri: efficient inference of cancer progression models from cross-sectional data", "author": ["D. Ramazzotti", "G. Caravagna", "L.O. Loohuis", "A. Graudenzi", "I. Korsunsky", "G. Mauri", "M. Antoniotti", "B. Mishra"], "venue": "Bioinformatics, vol. 31, no. 18, pp. 3016\u20133026, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithmic methods to infer the evolutionary trajectories in cancer progression", "author": ["G. Caravagna", "A. Graudenzi", "D. Ramazzotti", "R. Sanz- Pamplona", "L. De Sano", "G. Mauri", "V. Moreno", "M. Antoniotti", "B. Mishra"], "venue": "PNAS, in press, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Theory refinement on bayesian networks", "author": ["W. Buntine"], "venue": "Proceedings of the Seventh conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., 1991, pp. 52\u201360.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1991}, {"title": "A bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Machine  learning, vol. 9, no. 4, pp. 309\u2013347, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Parallel implementation of efficient search schemes for the inference of cancer progression models", "author": ["D. Ramazzotti", "M.S. Nobile", "P. Cazzaniga", "G. Mauri", "M. Antoniotti"], "venue": "IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology. IEEE, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Equivalence and synthesis of causal models", "author": ["T.V.J. Judea Pearl"], "venue": "Proceedings of Sixth Conference on Uncertainty in Artificial Intelligence, 1991, pp. 220\u2013227.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1991}, {"title": "Inference of cancer progression models with biological noise", "author": ["I. Korsunsky", "D. Ramazzotti", "G. Caravagna", "B. Mishra"], "venue": "arXiv preprint arXiv:1408.6032, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A model of selective advantage for the efficient inference of cancer clonal evolution", "author": ["D. Ramazzotti"], "venue": "arXiv preprint arXiv:1602.07614, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "The algorithm design manual: Text", "author": ["S.S. Skiena"], "venue": "Springer Science & Business Media,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Tabu search-part i", "author": ["F. Glover"], "venue": "ORSA Journal on computing, vol. 1, no. 3, pp. 190\u2013206, 1989.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1989}, {"title": "Optimization by simulated annealing: Quantitative studies", "author": ["S. Kirkpatrick"], "venue": "Journal of statistical physics, vol. 34, no. 5-6, pp. 975\u2013986, 1984.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1984}, {"title": "Genetic algorithms in search, optimization, and machine learning", "author": ["D.E. Golberg"], "venue": "Addion wesley, vol. 1989, p. 102, 1989.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1989}, {"title": "Holland, adaptation in natural and artificial systems", "author": ["H. John"], "venue": "1992.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1992}, {"title": "Stat-  14 nikov, \u201cAlgorithms for large scale markov blanket discovery.", "author": ["I. Tsamardinos", "C.F. Aliferis", "A.R. Statnikov"], "venue": "in FLAIRS conference,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Scoring functions for learning bayesian networks", "author": ["A.M. Carvalho"], "venue": "Inesc-id Tec. Rep, 2009.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Conjunctive bayesian networks", "author": ["N. Beerenwinkel", "N. Eriksson", "B. Sturmfels"], "venue": "Bernoulli, pp. 893\u2013909, 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Quantifying cancer progression with conjunctive bayesian networks", "author": ["M. Gerstung", "M. Baudis", "H. Moch", "N. Beerenwinkel"], "venue": "Bioinformatics, vol. 25, no. 21, pp. 2809\u20132815, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning oncogenetic networks by reducing to mixed integer linear programming", "author": ["H.S. Farahani", "J. Lagergren"], "venue": "PloS one, vol. 8, no. 6, p. e65773, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Ant colony optimization for learning bayesian networks", "author": ["L.M. De Campos", "J.M. Fernandez-Luna", "J.A. G\u00e1mez", "J.M. Puerta"], "venue": "International Journal of Approximate Reasoning, vol. 31, no. 3, pp. 291\u2013311, 2002.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning bayesian networks by ant colony optimisation: searching in two different spaces", "author": ["L.M. d. Campos", "J.A. G\u00e1mez Mart\u0131\u0301n", "J.M. Puerta Castell\u00f3n"], "venue": "Mathware & soft computing. 2002 Vol. 9 N\u00fam. 2 [-3], 2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence", "author": ["J.H. Holland"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1975}, {"title": "Selective pressure in evolutionary algorithms: A characterization of selection mechanisms", "author": ["T. Back"], "venue": "Evolutionary Computation, 1994. IEEE World Congress on Computational Intelligence., Proceedings of the First IEEE Conference on. IEEE, 1994, pp. 57\u201362.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning bayesian networks with the bnlearn r package", "author": ["M. Scutari"], "venue": "arXiv preprint arXiv:0908.3817, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "inspyred: Bio-inspired algorithms in python", "author": ["A. Garrett"], "venue": "URL https://pypi. python. org/pypi/inspyred, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring network structure, dynamics, and function using networkx", "author": ["D.A. Schult", "P. Swart"], "venue": "Proceedings of the 7th Python in Science Conferences (SciPy 2008), vol. 2008, 2008, pp. 11\u201316.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "A guide to numpy, vol. 1", "author": ["T. Oliphant"], "venue": "Spanish Fork: Trelgol Publishing, 2006.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akaike"], "venue": "Selected Papers of Hirotugu Akaike. Springer, 1998, pp. 199\u2013213.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The annals of statistics, vol. 6, no. 2, pp. 461\u2013464, 1978.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1978}, {"title": "Learning bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Machine learning, vol. 20, no. 3, pp. 197\u2013 243, 1995.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1995}, {"title": "A bayesian method for constructing bayesian belief networks from databases", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Proceedings of the Seventh conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., 1991, pp. 86\u201394.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1991}, {"title": "Mathematical contributions to the theory of evolution.\u2013on a form of spurious correlation which may arise when indices are used in the measurement of organs", "author": ["K. Pearson"], "venue": "Proceedings of the royal society of london, vol. 60, no. 359-367, pp. 489\u2013498, 1896.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1896}], "referenceMentions": [{"referenceID": 0, "context": "BAYESIAN networks (BNs) are probabilistic graphical models representing the relations of conditional dependence among random variables, encoded in directed acyclic graphs (DAGs) [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 0, "context": "the last decades, BNs have been effectively applied in several different fields and disciplines, such as (but not limited to) diagnostics and predictive analytics [1].", "startOffset": 163, "endOffset": 166}, {"referenceID": 0, "context": ", all the possible DAGs) is evaluated via a score based on a likelihood function [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Regardless of the approach, the main difficulty in this learning problem is the huge number of valid solutions in the search space, namely, all the possible DAGs, which makes this task a known NP hard problem, even when constraining each node to have at most two parents [3], [4].", "startOffset": 271, "endOffset": 274}, {"referenceID": 2, "context": "Regardless of the approach, the main difficulty in this learning problem is the huge number of valid solutions in the search space, namely, all the possible DAGs, which makes this task a known NP hard problem, even when constraining each node to have at most two parents [3], [4].", "startOffset": 276, "endOffset": 279}, {"referenceID": 0, "context": "state-of-the-art techniques solve this task by means of meta-heuristics [1], [5], [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "state-of-the-art techniques solve this task by means of meta-heuristics [1], [5], [6].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "state-of-the-art techniques solve this task by means of meta-heuristics [1], [5], [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "different structures can encode the same set of conditional independence properties [1].", "startOffset": 84, "endOffset": 87}, {"referenceID": 0, "context": "emerging from their induced distributions rather than the structure itself [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "In these scenarios, different configurations may lead to failure, but some of them are more likely than others and, hence, can be modeled probabilistically [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 6, "context": "The two particular conditions mentioned above represent the basis of the notion of probabilistic causation by Patrick Suppes [9], [10], and allow us to define a set of structural constraints to the BNs to be inferred, which, accordingly, have been dubbed as Suppes-Bayes Causal Networks (SBCNs) in previous works [11], [7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "The two particular conditions mentioned above represent the basis of the notion of probabilistic causation by Patrick Suppes [9], [10], and allow us to define a set of structural constraints to the BNs to be inferred, which, accordingly, have been dubbed as Suppes-Bayes Causal Networks (SBCNs) in previous works [11], [7].", "startOffset": 130, "endOffset": 134}, {"referenceID": 8, "context": "The two particular conditions mentioned above represent the basis of the notion of probabilistic causation by Patrick Suppes [9], [10], and allow us to define a set of structural constraints to the BNs to be inferred, which, accordingly, have been dubbed as Suppes-Bayes Causal Networks (SBCNs) in previous works [11], [7].", "startOffset": 313, "endOffset": 317}, {"referenceID": 9, "context": "SBCNs have been already applied in a number of different fields, ranging from cancer progression inference [12], [13], [14] to social discrimination discovery [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "SBCNs have been already applied in a number of different fields, ranging from cancer progression inference [12], [13], [14] to social discrimination discovery [11].", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "SBCNs have been already applied in a number of different fields, ranging from cancer progression inference [12], [13], [14] to social discrimination discovery [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "SBCNs have been already applied in a number of different fields, ranging from cancer progression inference [12], [13], [14] to social discrimination discovery [11].", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "events, poset in the terminology of Bayesian networks) of a BN, finding the optimal solution that is consistent with the ordering can be accomplished in time O(n), where n is the number of variables and k the bounded in-degree of a node [15], [16].", "startOffset": 237, "endOffset": 241}, {"referenceID": 13, "context": "events, poset in the terminology of Bayesian networks) of a BN, finding the optimal solution that is consistent with the ordering can be accomplished in time O(n), where n is the number of variables and k the bounded in-degree of a node [15], [16].", "startOffset": 243, "endOffset": 247}, {"referenceID": 4, "context": "Thus, the search in the space of orderings can be performed way more efficiently than the search in the space of structures, as the search space is much smaller, the branching factor is lower and acyclicity checks are not necessary [6], [17].", "startOffset": 232, "endOffset": 235}, {"referenceID": 14, "context": "Thus, the search in the space of orderings can be performed way more efficiently than the search in the space of structures, as the search space is much smaller, the branching factor is lower and acyclicity checks are not necessary [6], [17].", "startOffset": 237, "endOffset": 241}, {"referenceID": 0, "context": ", P(A \u2227B) 6= P(A)P(B), regardless of which any other variables we condition on, that is, for any other set of variables C it holds that P(A \u2227B | C) 6= P(A | C)P(B | C) [1].", "startOffset": 168, "endOffset": 171}, {"referenceID": 0, "context": "It can be proven that, for any BN, the Markov blanket consists of a node\u2019s parents, its children and the parents of the children [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 15, "context": ", A\u2192 B \u2190 C) [18].", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "BNs have an interesting relation to canonical boolean logical operators \u2227, \u2228 and \u2295 and formulas over variables [19], [7].", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "that we will hear music even without power or headphone (perhaps we are next to a concert and overhear that music) [19], [20].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "that we will hear music even without power or headphone (perhaps we are next to a concert and overhear that music) [19], [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 1, "context": "Since both approaches lead to intractability (NP -hardness) [3], [4], computing and verifying an exact solution is impractical.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Since both approaches lead to intractability (NP -hardness) [3], [4], computing and verifying an exact solution is impractical.", "startOffset": 65, "endOffset": 68}, {"referenceID": 18, "context": "For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed.", "startOffset": 72, "endOffset": 76}, {"referenceID": 20, "context": "For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed.", "startOffset": 98, "endOffset": 102}, {"referenceID": 21, "context": "For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed.", "startOffset": 126, "endOffset": 130}, {"referenceID": 22, "context": "For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed.", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "In the rest of this section we describe in detail some of these approaches, leaving to specific readings more detailed discussions [1], [19], [20].", "startOffset": 131, "endOffset": 134}, {"referenceID": 16, "context": "In the rest of this section we describe in detail some of these approaches, leaving to specific readings more detailed discussions [1], [19], [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "In the rest of this section we describe in detail some of these approaches, leaving to specific readings more detailed discussions [1], [19], [20].", "startOffset": 142, "endOffset": 146}, {"referenceID": 23, "context": "For more detailed explanations and analyses of complexity, correctness and stability, we refer the reader to the related references [26], [27].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "Bayesian network is a product of simple terms [1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 24, "context": "statistics (see [28] and references therein), which all serve to promote sparsity in the learned graph structure, though different regularization terms are", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "better suited for particular applications [1], [20].", "startOffset": 42, "endOffset": 45}, {"referenceID": 17, "context": "better suited for particular applications [1], [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "A widespread approach for the learning of monotonic progression networks with a directed acyclic graph (DAG) structure and conjunctive events are Conjunctive Bayesian Networks (see CBNs, [29]).", "startOffset": 187, "endOffset": 191}, {"referenceID": 10, "context": "This approach was originally adopted to model cancer progression in terms of accumulation of drivers genes [13], [14], in a way closely related to the model we discuss in this work.", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "This approach was originally adopted to model cancer progression in terms of accumulation of drivers genes [13], [14], in a way closely related to the model we discuss in this work.", "startOffset": 113, "endOffset": 117}, {"referenceID": 26, "context": "In response to this, hidden CBNs [30] were developed by augmenting the set of variables: a correspondence to a new variable Y that represents the observed state is assigned to each CBN variable X , which captures the \u201ctrue\u201d state.", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "All the conclusions derived in these settings can be also directily applied to the general case where the nodes in the BN describe geneal random variables [1].", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "In [9], Suppes introduced the notion of prima facie causation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Definition 1 (Probabilistic causation, [9]).", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "The notion of prima facie causality has known limitations in the context of the general theories of causality [10], however, this characterizations seems to appropriate to model the dynamics of phenomena driven by the monotonic accumulation of events", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "To this extent, we can define a class of BNs over Bernoulli random variables named Monotonic Progression Networks (MPNs) [7], [19], [31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "To this extent, we can define a class of BNs over Bernoulli random variables named Monotonic Progression Networks (MPNs) [7], [19], [31].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "Moreover, as discussed in [7], [19], [31], such MPNs can model accumulative phenomena in a probabilistic fashion, i.", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "Moreover, as discussed in [7], [19], [31], such MPNs can model accumulative phenomena in a probabilistic fashion, i.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "Given these premises, in [13] the authors describe an efficient algorithm (named CAPRI, see Algorithm 1) to learn the structure of constrained Bayesian networks which account for Suppes\u2019 cri-", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "teria and which later on are dubbed Suppes-Bayes Causal Networks (SBCNs) in [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "on the observation (see [6]) that a way for circumventing the intrinsic computational complexity of the task of learning the structure of a Bayesian Network is to postulate a pre-determined ordering among the nodes.", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "We refer the reader to [7] for further details and, following [11], we now formally define a SBCN.", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "Once again, we refer the interested reader to the discussions provided in [13], [7] and, without losing in generality, for the purpose of this work, we consider the efficient implementation of Algorithm 1.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "We conclude this Section by reporting the pseudocode (see Algorithm 1) of the efficient learning algorithm to infer SBCNs presented in [13], which we adopt for the assessment of the performance in the simulations of the next Section.", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "Moreover, as stated above, the problem of learning the structure of a BN is NP -hard [4].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Because of that, state-of-theart techniques largely rely on heuristics [1], often", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "For instance, methods based on Genetic Algorithms (GAs) [5], [17] and Ant Colony Optimization [32], [33] have been proposed in the literature.", "startOffset": 56, "endOffset": 59}, {"referenceID": 14, "context": "For instance, methods based on Genetic Algorithms (GAs) [5], [17] and Ant Colony Optimization [32], [33] have been proposed in the literature.", "startOffset": 61, "endOffset": 65}, {"referenceID": 28, "context": "For instance, methods based on Genetic Algorithms (GAs) [5], [17] and Ant Colony Optimization [32], [33] have been proposed in the literature.", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "For instance, methods based on Genetic Algorithms (GAs) [5], [17] and Ant Colony Optimization [32], [33] have been proposed in the literature.", "startOffset": 100, "endOffset": 104}, {"referenceID": 30, "context": "Genetic Algorithms: GAs were introduced by Holland in 1975 [34] as a global search methodology inspired by the mechanisms of natural selection.", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "GAs were shown to be effective for BN learning, both in the case of available and not available a priori knowledge about nodes\u2019 ordering [5], [17].", "startOffset": 137, "endOffset": 140}, {"referenceID": 14, "context": "GAs were shown to be effective for BN learning, both in the case of available and not available a priori knowledge about nodes\u2019 ordering [5], [17].", "startOffset": 142, "endOffset": 146}, {"referenceID": 31, "context": "cedure [35].", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "For further information about our implementation of GAs for the inference of BNs, including the correction phase, we refer the interested reader to [17].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "All these measures are values in [0, 1] with results close to 1 indicators of good", "startOffset": 33, "endOffset": 39}, {"referenceID": 32, "context": ", the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.", "startOffset": 69, "endOffset": 73}, {"referenceID": 30, "context": ", the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.", "startOffset": 110, "endOffset": 114}, {"referenceID": 33, "context": ", the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.", "startOffset": 190, "endOffset": 194}, {"referenceID": 34, "context": ", the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.", "startOffset": 205, "endOffset": 209}, {"referenceID": 35, "context": ", the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.", "startOffset": 220, "endOffset": 224}, {"referenceID": 0, "context": "The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations.", "startOffset": 233, "endOffset": 236}, {"referenceID": 36, "context": "The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations.", "startOffset": 247, "endOffset": 251}, {"referenceID": 37, "context": "The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations.", "startOffset": 263, "endOffset": 267}, {"referenceID": 38, "context": "The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations.", "startOffset": 278, "endOffset": 282}, {"referenceID": 39, "context": "The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations.", "startOffset": 294, "endOffset": 298}, {"referenceID": 40, "context": "Although such a trend is intuitively expected, given the larger number of parameters to be learned for more complex models, we here underline the role of statistical complications, such as the presence of spurious correlations [44] and the occurrence of Simpson\u2019s paradox [45].", "startOffset": 227, "endOffset": 231}, {"referenceID": 0, "context": ", the standard log-likelihood [1], AIC [40], BIC [41], BDE [42] and K2 [43].", "startOffset": 30, "endOffset": 33}, {"referenceID": 36, "context": ", the standard log-likelihood [1], AIC [40], BIC [41], BDE [42] and K2 [43].", "startOffset": 39, "endOffset": 43}, {"referenceID": 37, "context": ", the standard log-likelihood [1], AIC [40], BIC [41], BDE [42] and K2 [43].", "startOffset": 49, "endOffset": 53}, {"referenceID": 38, "context": ", the standard log-likelihood [1], AIC [40], BIC [41], BDE [42] and K2 [43].", "startOffset": 59, "endOffset": 63}, {"referenceID": 39, "context": ", the standard log-likelihood [1], AIC [40], BIC [41], BDE [42] and K2 [43].", "startOffset": 71, "endOffset": 75}], "year": 2017, "abstractText": "One of the critical issues when adopting Bayesian networks (BNs) to model dependencies among random variables is to \u201clearn\u201d their structure, given the huge search space of possible solutions, i.e., all the possible direct acyclic graphs. This is a wellknown NP -hard problem, which is also complicated by known pitfalls such as the issue of I-equivalence among different structures. In this work we restrict the investigations on BN structure learning to a specific class of networks, i.e., those representing the dynamics of phenomena characterized by the monotonic accumulation of events. Such phenomena allow to set specific structural constraints based on Suppes\u2019 theory of probabilistic causation and, accordingly, to define constrained BNs, named Suppes-Bayes Causal Networks (SBCNs). We here investigate the structure learning of SBCNs via extensive simulations with various state-of-the-art search strategies, such as canonical local search techniques and Genetic Algorithms. Among the main results we show that Suppes\u2019 constraints deeply simplify the learning task, by reducing the solution search space and providing a temporal ordering on the variables.", "creator": "LaTeX with hyperref package"}}}