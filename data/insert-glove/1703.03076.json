{"id": "1703.03076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Efficient Simulation of Financial Stress Testing Scenarios with Suppes-Bayes Causal Networks", "abstract": "The consumer-level most recent financial buttars upheavals ascent have cast homophobes doubt on the adequacy hankered of some pawlik of the conventional parlak quantitative norwich risk mup management strategies, such as philosophizing VaR (33.72 Value 4.71 at Risk ), polyuria in many common situations. Consequently, 600-year there has coronation been an kharms increasing need .458 for verisimilar exline financial khaine stress testings, namely simulating and analyzing financial portfolios fleetstreet in zetland extreme, negate albeit rare mudan scenarios. Unlike sinistral conventional jastrebarsko risk management boolos which uncooked exploits grody statistical greenview correlations 1990-97 among financial grammes instruments, outdoorsmen here oct we focus li\u00fa our analysis bardon on the file notion artaserse of probabilistic causation, which kazama is proteinaceous embodied superspeed by zoetemelk Suppes - aerocontinente Bayes Causal Networks (suburgatory SBCNs ), daci SBCNs neo-georgian are probabilistic graphical arencibia models cubensis that ystwyth have many attractive metaphilosophy features cotsakos in terms of foregin more accurate causal socialists analysis 131.75 for generating owego financial stress scenarios. In 34.16 this emrich paper, we .175 present trona a novel manetho approach beye for mangling conducting stress mvno testing irapuato of financial portfolios bouillon based 3:22 on SBCNs in mcbryde combination chabala with menahem classical machine learning tratamientos classification sophomoric tools. shahbaz The berislav resulting method toned-down is shown saving to be al-hadith capable ucles of pni correctly ridolfi discovering the umur causal bueno relationships impurities among financial tyrolean factors aup that affect the portfolios and silcox thus, bickle simulating manekshaw stress parsing testing scenarios welfare with a siderurgicas higher novack accuracy 2.705 and tokuma lower norriton computational complexity than cardoen conventional Monte box-to-box Carlo ischl Simulations.", "histories": [["v1", "Wed, 8 Mar 2017 23:54:09 GMT  (842kb,D)", "http://arxiv.org/abs/1703.03076v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CE", "authors": ["gelin gao", "bud mishra", "daniele ramazzotti"], "accepted": false, "id": "1703.03076"}, "pdf": {"name": "1703.03076.pdf", "metadata": {"source": "CRF", "title": "Efficient Simulation of Financial Stress Testing Scenarios with Suppes-Bayes Causal Networks", "authors": ["Gelin Gao", "Bud Mishra", "Daniele Ramazzotti"], "emails": ["permissions@acm.org."], "sections": [{"heading": null, "text": "In this paper, we present a novel approach for conducting stress testing of financial portfolios based on SBCNs in combination with classical machine learning classification tools. The resulting method is shown to be capable of correctly discovering the causal relationships among financial factors that affect the portfolios and thus, simulating stress testing scenarios with a higher accuracy and lower computational complexity than conventional Monte Carlo Simulations.\nKeywords Stress Testing, Graphical Models, Causality, Suppes-Bayes Causal Networks, Classification, Decision Trees"}, {"heading": "1. INTRODUCTION", "text": "Risk management has increasingly become a central part of world finance in the past century. Quantitative risk management generally targets the risk of insolvency: namely, the depletion of capital of a trading agency to the point that the trading agency has to stop its operations. For any trading agency, its account consists of cash, stocks, bonds or other financial instruments, and net equity, where Equity = Cash+Financial Instruments. The task of quantitative risk management is to calculate the amount of equity that has to be reserved so that the net equity will not drop\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. xxxxxxxxx xxxxxxxxx, xxxxxxxxx c\u00a9 2017 ACM. ISBN xxx-xxxx-xx-xxx/xx/xx. . . $xx.xx\nDOI: xx.xxx/xxx x\nto negative [8]. Depending on different financial agencies, hedge funds, banks or clearing houses, and on different financial instruments, stocks, bonds, or derivatives, the method of risk management may vary, but the central idea behind conventional risk management remains: we assess the statistical distribution of the asset (or portfolio), and estimate the worst-case scenarios, generally by methods such as Monte Carlo Simulation [17].\nHowever, such conventional approach got discredited by the recent events leading to major financial catastrophes. For example, in the recent 2008 financial crisis, the reserves calculated the risk by using methods such as VaR (Value at Risk) [11] which proved to be painfully inadequate. Because of that, a different method had to be introduced, namely the one of stress testing. Stress testing refers to the analysis or simulation of the response of financial instruments or institutions, given intensely stressed scenarios that may lead to a financial crisis [5]. For example, narrowly speaking, stress testing may model the response of a portfolio when Dow Jones suddenly drops by 5%. The difference between stress testing and conventional risk management is that stress testing deliberately introduces an adversarial, albeit plausible event, which may be highly improbable but not implausible \u2013 e.g., a black swan event triggering an unforeseen scenario. Thus, stress testing must be capable of observing the response of financial instruments or institutions under extremely rare scenarios. Such scenarios may be deemed to be unlikely to be observed in conventional risk management, where the simpler system may fail to estimate a 99th percentile of the loss distribution, and subsequently leading to a claim that, with 99% confidence level, a specific portfolio will perform well, giving a false sense of security.\nRecently, many different approaches have been developed to implement some form of stress testing. In terms of stress scenario generation, the most direct method is the historical one, in which observed events from the past are used to test contemporary portfolios [12]. The historical approach is objective since it is based on actual events, but it is not necessarily relevant under the present conditions, which necessitate some hypothetical methods. As an alternative, the event-based method has been proposed in order to quantify a specific hypothetical stress scenario subjectively, by domain experts, and then estimate the possible consequence of such event using macroeconomic and financial models [12]. Event-based methods rely intensively on expert judgement on whether a hypothetical event will be severely-damaging, albeit still plausible to occur. Sometime such judgement becomes difficult when the relationship between the underlying\nar X\niv :1\n70 3.\n03 07\n6v 1\n[ cs\n.L G\n] 8\nM ar\n2 01\n7\nrisk factors and the portfolio is unknown. To ensure a scenario is damaging to the portfolio, a portfolio-based method has been also studied in order to link scenarios directly with the portfolio [12]. To this extent, portfolio-based methods rely on Monte Carlo Simulation to identify the movements of risk factors that stress the given portfolio most severely, however brute force Monte Carlo Simulation is computationally inefficient especially when dealing with many risk factors. To solve this problem, Rebonato et al. proposed a sampling approach based on Bayesian networks in [18].\nRoadmap: This paper is organized as follow. Next section describes the theoretical foundations of our approach and, in particular, it shows how combining the expressivity of Suppes-Bayes Causal Networks together with classical classification approaches can effectively capture the dynamics of financial stress testing. Section 3 provides an algorithm for the efficient inference of SBCNs from financial data, discusses its performance in-depth and shows on realistic simulated data how our approach is preferable in comparison to the standard Bayesian methods. Section 4 concludes the paper."}, {"heading": "2. METHOD", "text": "In this work we use Bayesian Graphical Models [9], popularly known as Bayesian networks, as a framework to assess stress testing, as previously done in this context by [18]. Bayesian networks have long been used in biological modeling such as -omics data analysis, cancer progression or genetics [2, 10, 14], but their application to financial data analysis has been rare. Roughly speaking, Bayesian networks attempt to exploit the conditional independence among random variables, whether the variables represent genes or financial instruments. In this paper we adopt a variation of the traditional Bayesian networks as done in [15, 16], where the authors show how constraining the search space of valid solutions by means of a causal theory grounded in Suppes\u2019 notion of probabilistic causation [23] can be exploited in order to devise better learning algorithms. Also, by accounting for Suppes\u2019 notion of probabilistic causation, we ensure not only conditional independence but also prima facie causal relations among variables, leading us to a better definition of the actual factors leading to risk. Moreover, through a maximum likelihood optimization scheme which makes use of a regularization score, we also attempt to only retain edges in the Bayesian network (graphically depicted as a directed acyclic graph, DAG) that correspond to only genuine causation, while eliminating all the spurious causes [15, 16].\nYet, given the inferred network, we can sample from it to generate plausible scenarios, though not necessarily adversarial or rare. In the case of stress testing, it is crucial to also account for rare configurations, for this reason, we adopt auxiliary tools from machine learning to discover random configurations that are both unexpected and undesired.\nIn this Section we expand the concept sketched above and specifically we provide a background of our framework, by describing the adopted Bayesian models and causal theories and we show how classification given an inferred SBCN can effectively guide stress testing simulations."}, {"heading": "2.1 Bayesian networks", "text": "Informally, Bayesian networks are defined as a directed acyclic graph (DAG) G = (V,E), in which each node V\nrepresents a random variable to which is associated a conditional probability table, and each arc E models dependency relationships. The nodes induce an overall joint distribution that can be written as a product of the conditional distributions associated with each variable [9]. In this paper, without any loss of generality, we restrict our attention to Bernoulli random variables with support in (0, 1). Specifically, we will consider as inputs for our analyses a dataset D of m observations over n Bernoulli variables; we refer to the next Sections for a detailed description of the meaning of such variables. More details about Bayesian networks may be found in [9].\nLet us now consider as an example the Bayesian network shown in Figure 1, where A, B, C, and D are 4 random variables represented by four nodes, and the dependencies among the nodes are modelled by directed arcs. Loosely speaking, the link A\u2192 B indicates that the knowledge of A (the parent) influences the probability of B (the child), or A and B are statistically dependent. Furthermore, for node B, node A is called B\u2019s parent and nodes C and D are called B\u2019s children. More precisely, in the conditional probability tables related to the afore mentioned Bayesian network, the rows for node B specifies how the knowledge of A affects the probability of B being observed. For example, let A and B be both binary random variables with support over (0, 1). Table 1 specifies the distribution of B under the condition of A, and we can see clearly the effect of the parent on the child in this example.\nOne of the most significant feature of Bayesian network is the notion of conditional independence. Simply speaking, for any node X in a Bayesian network, given the knowledge of node X\u2019s parents, X is conditionally independent of all nodes that are not its children, or all its predecessors [9]. For example, in the Bayesian network in Figure 1, node C is conditionally independent of node A, when conditioned on node B being fixed. The possibility of exploiting conditional dependencies when computing the induced distribution of the Bayesian network is a powerful property since it simplifies the conditional probability table tremendously. For\nexample, the conditional probability table of node C, will not contain entries P (C|A,B) since P (C|A,B) = P (C|B), or node C is independent of A conditioned on B: A \u22a5 C|B.\nIn the context of stress testing, Rebonato [18] suggests a subjective approach to constructing Bayesian networks. After carefully selecting a set of random variables as the nodes of the network, Rebonato proposes to subjectively connect the variables and assign the relevant conditional probability tables with the help of risk managers or other experts. Then with the inferred Bayesian network, reasoning about stressed events or simulation can be conducted. Please see [18] for details.\nOur framework builds upon many of Rebonato\u2019s intuitions but exploits our recent works on causality to address all the key problems, of which the subjective approach comes short. The subjective approach is handy under the condition of expert knowledge of the causal relationships of some variables. However, such reliance becomes unnatural when experts are confronted with random variables that are clearly beyond their expertise: for example, the relationship of unemployment and stock market performance, or more simply, the relationship of two random stocks. Therefore, instead of completely abandoning the role of data in the construction of Bayesian network, here we adopt learning algorithms that can learn both the structure and the conditional probability table of the Bayesian network from the data, which, in turn, can be further augmented by expert knowledge if deemed necessary."}, {"heading": "2.2 Suppes-Bayes Causal Networks", "text": "Thus, unlike [18], our stress testing approach builds on the foundation of Suppes-Bayes Causal Networks (SBCNs), which are not only more strictly regularized than the general Bayesian networks but also enjoys many other attractive features such as interpretability and refutability. SBCNs exploit the notion of probabilistic causation, originally proposed by Patrick Suppes [23].\nIn [23], Suppes described the notion of prima facie causation. A prima facie causal relation between any event u and its effect v is verified when the following two conditions hold: (i) temporal priority (TP), i.e., any cause happens before its effect and (ii) probability raising (PR), i.e., the presence of the cause raises the probability of observing its effect.\nDefinition 1 (Probabilistic causation, [23]). For any two events u and v, occurring respectively at times tu and tv, under the mild assumptions that 0 < P (u), P (v) < 1, the event u is called a prima facie cause of v if it occurs before and raises the probability of u, i.e.,{\n(TP ) tu < tv (PR) P (v | u) > P (v | u) (1)\nThe notion of prima facie causality was fruitfully exploited for the task of modeling cancer evolution in [10, 14, 4], and the SBCNs were finally described for the first time in [3] and, later on, defined in [15, 16] as follow.\nDefinition 2 (Suppes-Bayes Causal Network). Let us consider an input cross-sectional dataset D of n Bernoulli variables and m samples, the Suppes-Bayes Causal Network SBCN = (V,E) subsumed by D is a directed acyclic graph such that the following requirements hold:\n[Suppes\u2019 constraints] for each arc (u\u2192 v) \u2208 E involving a prima facie relation between nodes u, v \u2208 V , under the mild assumptions that 0 < P (u), P (v) < 1:\nP (u) > P (v) and P (v | u) > P (v | \u00acu) .\n[Simplification] let E\u2032 be the set of arcs satisfying the Suppes\u2019 constraints as before; among all the subsets of E\u2032, the set of arcs E is the one whose corresponding graph maximizes the likelihood of the data and of a certain regularization function R(f):\nE = argmax E\u2286E\u2032,G=(V,E)\n(LL(D|G)\u2212R(f)) .\nIntuitively, the advantage of SBCNs over general Bayesian networks is the following. First, with Temporal Priority, SBCN accommodates the time flow among the nodes. There are obvious cases where some nodes occur before the other and it is generally natural to state that nodes that happen later cannot be causes (or parents) of nodes that happen earlier. Second, when learning general Bayesian networks, arcs A \u2192 B and A \u2190 B may sometimes be equally acceptable, resulting in an undirected arc A\u2212B (this situation is called Markov Equivalence [9]). For SBCNs, such a situation does not arise because of the temporal flow being irreversible [15, 16]. Third, because of the two constraints on the causal links, the SBCN graph is generally more sparse (has fewer edges) than the graph of general Bayesian networks with the final goal of disentangling spurious arcs, e.g., due to spurious correlations [13], from genuine causalities."}, {"heading": "2.3 Machine Learning and Classification", "text": "Even if with SBCNs we typically obtain sparser DAGs than when we use Bayesian networks, the modelled relations both involve positive and negative financial scenarios, but only in the latter financial stress may arise. Thus, the extreme events which are of key relevance for stess testing are still rare in the data and unlikely to be simulated in naively generated stress scenarios by sampling from the SBCN directly. Therefore, in this work we improve this basic model with several key ideas of classic machine learning, namely, feature classification. Recall that, in stress testing, we wish to target the unlikely, but risky scenarios. Specifically, when generating random sample from a SBCN to obtain possible scenarios, each node in the SBCN can take any value in its support according to its conditional probability table, generating different branches of scenarios. To narrow down the search space, we can classify each possible branch as leading to profitable or lossy scenarios, and if, the branch is classified as profitable, then random sampling is guided to very likely avoid that branch, thus focusing on events and causal relations that can be adversarial and risky, though uncommon. In this way, computation can be reduced significantly to discover the extreme events (see the next Sections for details)."}, {"heading": "3. RESULTS AND DISCUSSION", "text": "In this Section we investigate the capabilities of our framework based on SBCNs to perform stress testing assessment. We first describe the adopted generative model and then present our algorithm and investigate its performance paying specific attention to the problem of false discoveries."}, {"heading": "3.1 Simulating the Training Data", "text": "To assess the performance of the algorithm to learn the SBCNs and the quality of inferred Bayesian networks, a set of training data is developed with embedded causal relationships. If the algorithms, when performed on the training data are capable of accurately recover the causal relationships embedded in them, then such accuracy can be expected on real data.\nTo simulate the training data, we adopt a common stock factor model, the Fama French Five Factor Model [7], where the return of the asset is defined as follow:\n(2)r = Rf + \u03b21(Km \u2212Rf ) + \u03b22SMB + \u03b23HML + \u03b24RMW + \u03b23CMA+ \u03b1.\nIn the equation, r is the return of the asset; Rf is the risk free return, usually measured in terms of government treasury returns; Km stands for market factor, measured as value-weighted market portfolio, similar to stock indexes; SMB (Small Minus Big) stands for company size factor, measured by return on a diversified portfolio of small stocks minus the return on a diversified portfolio of big stocks; HML (High Minus Low) stands for company book-to-market (B/M) ratio factor, measured by difference between the returns on diversified portfolios of high and low B/M stocks, where B/M is the ratio between company\u2019s book value to market value; RMW (Robust Minus Weak) stands for company operating profitability factor, measured by the returns on diversified portfolios of stocks with robust and weak profitability and CMA (Conservative Minus Aggressive) stands for company investment factor, difference between the returns on diversified portfolios of low and high investment stocks, called conservative and aggressive [7].\nTo simulate the training data with embedded causal relationship, we linearly regress historical returns r, onto the five factors, and obtain the distribution of each factor coefficient and the empirical residual. We notice that a key characterization of a SBCN is an underlying temporal model of the causal relations depicted in the network, namely the temporal priority between any pair of nodes which are involved in a causal relationship. Therefore, the five factors described in our generative model are lagged with respect to the historical returns to comply with the temporal priority. Thus,\n(3)ri,t = \u2211 i,j \u03b2i,jfj,t\u2212lag + .\nThen, the simple training data is simulated by randomly drawing the factor coefficients \u03b2i,j and residuals from the distribution we obtained from the linear regression, and apply these coefficients and residuals on a set of new factor data. Such historical data consists of daily series of five factors and returns of 10 portfolios also constructed by Fama French, and of 10000 days. We use the first 5000 for regression and the other 5000 for simulation.\nIn reality, many factors will present causal relationships among themselves. For example, some factor do not directly influence the asset, but affect the asset indirectly by affecting other factors. Therefore, the simulated training data can be complicated by embedding spurious relationships also among factors. We linearly regress some factors on the other factors and simulate the training data in the same way. The choice of factors is arbitrary. In this paper, as an example, we regress the other four factors SML, HML, RMW and\nCMA on the market factor Km. Therefore, the causal relationships which are described in the simulated training data can be simplified as shown in Figure 2."}, {"heading": "3.2 Learning the SBCNs", "text": "In this Section we show results on 100 independent random simulations generated on networks of 15 nodes, i.e., 10 stocks and 5 factors with the generative model discussed in the previous Section. Each node represents a Bernoulli random variable taking binary values in (0, 1), where 1 represents the stock or factor going up and 0, the stock or factor going down. Specifically, the input of our learning task is a dataset D of n \u00d7 m binary entries. Starting with such an input, we attempted to experiment with our learning algorithms previously described in [14] and [3]. In particular, as in [3], we lacked explicitly observed time in the data, which are only cross-sectional. To overcome this problem we imputed as a further input to our algorithm a topological ranking r providing information about the temporal priority among the nodes. In interpreting these experiments, we set ranking as a proxy of time precedence among the factors influencing the stocks, i.e., in our model factors can cause stock moves but not the other way around.\nAlgorithm 1 summarizes the learning approach adopted for the inference. Given the above mentioned inputs, Suppes\u2019 constraints are verified (Lines 3-8) to first construct a DAG. Then, the likelihood fit is performed by hill climbing (Lines 9-20), an iterative optimization technique that starts with an arbitrary solution to a problem (in our case an empty graph) and then attempts to find a better solution by incrementally visiting the neighbourhood of the current one. If the new candidate solution is better than the previous one, it is considered in place of it. The procedure is repeated until the stopping criterion is matched. In our implementation, the !StoppingCriterion occurs (Line 11) in two situations: (i) the procedure stops when we have performed a large enough number of iterations or, (ii) it stops when none of the solutions in Gneighbors is better than the current Gfit, where Gneighbors denotes all the solutions that are derivable from Gfit by removing or adding at most one edge.\nFor more information about the algorithm, also refer to [14, 3].\n3.2.1 The Problem of False Discovery We first test the performance of Algorithm 1 on a training\ndata of 10 portfolios, 5 factors and 5000 observations. On\nAlgorithm 1 Learning the SBCN [3]\n1: Inputs: D an input dataset of n Bernoulli variables and m samples, and r a partial order of the variables 2: Output: SBCN(V,E) as in Definition 2 3: [Suppes\u2019 constraints] 4: for all pairs (v, u) among the n Bernoulli variables do 5: if r(v) \u2264 r(u) and P (u | v) > P (u | \u00acv) then 6: add the arc (v, u) to SBCN . 7: end if 8: end for 9: [Likelihood fit by hill climbing] 10: Consider G(V,E)fit = \u2205. 11: while !StoppingCriterion() do 12: Let G(V,E)neighbors be the neighbor solutions of G(V,E)fit. 13: Remove from G(V,E)neighbors any solution whose arcs are not included in SBCN . 14: Consider a random solution Gcurrent in G(V,E)neighbors. 15: if scoreREG(D,Gcurrent) > scoreREG(D,Gfit) then 16: Gfit = Gcurrent. 17: end if 18: end while 19: SBCN = Gfit. 20: return SBCN .\nsuch settings, the algorithm was capable of recovering almost the whole set of embedded causal relationships with only 13 false negatives, roughly, 33% of total arcs; however, the number of false positives were unacceptably larger, reaching around 49% of the total causal arcs obtained, thus requiring more attention to how the model was regularized.\nThe explanation for this trend can be found in how the algorithm implements the regularization via Bayesian Information Criterion (BIC) [21], that is:\nBIC = k \u00b7 ln(N)\u2212 2 ln(L),\nwhere k is the number of arcs in the SBCN (i.e. number of causal relationships), n is the number of observations of the data, and L is the likelihood. The algorithm searches for the Bayesian network that minimizes the BIC.\nFor large number of observations, the maximum likelihood optimization ensures that asymptotically all the embedded relationships are explored and the most likely solution is recovered. However, maximum likelihood is known to be susceptible to overfitting [9], especially when, as in our case, it deals with small sample size in the training data. Furthermore, in the training data, all the portfolios are assumed to depend on the same five factors, although with different coefficients, but very likely some portfolios will have very similar coefficients, resulting in co-movements across the portfolios. This co-movement will often induce correlations that affect the probability raising and thus the spurious prima facie causal relations, making these settings an interesting, and yet a very hard test case. See Figure 3.\n3.2.2 Sample Size and Information Criterion To reduce the spurious causalities, we recall some intrinsic\nproperties of the information criteria. The Bayesian Information Criterion BIC = k \u00b7 ln(N)\u2212 2 ln(L), not only maximizes the likelihood, but also penalizes the complexity of\nthe model by the term k \u00b7 ln(N). For small sample sizes, BIC is generally biased towards simple models because of the penalty. However, for large sample size, BIC is willing to accept complex models. For additional discussion, see the details in [9].\nIn our simulations we adopted a sample size of 5000 which is considerably large relative to the degree of freedom of the score function, therefore using BIC may lead to the inference of a relatively complex model with a number of unnecessary spurious arcs. Counter-intuitively, we could improve the solutions by using smaller sized data and letting the complexity penalty take a bigger effects in BIC score. This also addresses the nonstationarity in the data, an endemic problem for financial data. Following this intuition, we performed further experiments by reducing the original sample size of 5000 samples, which describes around 10 years of data, in turn to 250 and 500, and we observed a significant reduction in the number of false positives, to 38% and 40% of total arcs respectively. However, at the same time, because of smaller sample size, the number of false negatives inevitably increased to more than 50% of total arcs.\nTo reconcile this dilemma, we now next considered an alternative information criterion, the AIC, Akaike Information Criterion [1], defined as in the following:\nAIC = 2k \u2212 ln(L).\nWe notice that for AIC, the coefficient of k is set to 2, leading to definitely smaller factor than ln(N) of BIC when the sample size N is large. For this reason, AIC has the trend of accepting more complex models for given sample sizes than BIC. Applying AIC, the number of false negatives typically decreases, while the number of false positive gets larger.\n3.2.3 Improving Model Selection by Bootstrapping Up to this point, we described the different characteristics\nof two state-of-the-art likelihood scores with respect to the number of resulting false positive and false negative arcs. Specifically, we showed a trade-off where, because of their characteristics, the best results on large sample sizes is obtained using BIC, while for small sample sizes AIC is more effective, but neither of the two regularization schemes have a satisfactory trend. To improve their performance, now we propose to make use of a bootstrap [6] procedure for model selection.\nThe idea of bootstrap is the following: we first learn the structure and parameters of the SBCN as before, but we perform subsequently a resampling procedure where we sample with repetitions data from the dataset in order to generate a set of bootstrapped datasets, e.g., 100 times, and then we\ncalculate the relative confidence level of each arc in the originally inferred SBCN, by performing the inference from each of the bootstrapped dataset and counting how many times a given arc is retrieved. In this way, we obtained a confidence level for any arc in the SBCN.\nWe once again tested such an approach on our simulations and we observed empirically that the confidence level of spurious arcs are typically smaller than the confidence level for true causal relations. Therefore, a simple method of pruning the inferred SBCN to constrain for a given minimum confidence level is here applied. Such a threshold reflect the number of false positives that we are willing to include in the model, with higher thresholds ensuring sparser models. Here, we test our approach by requiring a minimum confidence level of 0.5, i.e., any valid arc must be retrieved at least half of the times.\nWe now conclude our analysis by showing in Tables 2 and 3 the contingency tables resulting from our experiments both for Algorithm 1 (Table 3) and for the standard likelihood fit method to infer Bayesian Networks (Table 2):"}, {"heading": "250 50.4 94.9 31.9 95.3 57.0 89.5 39.3 95.2", "text": ""}, {"heading": "500 51.8 94.7 45.6 96.6 61.6 93.5 51.5 98.7", "text": ""}, {"heading": "1000 55.2 93.5 47.6 94.5 63.7 85.5 50.4 95.2", "text": ""}, {"heading": "2500 60.3 90.3 54.8 99.9 68.4 85.7 62.4 99.8", "text": ""}, {"heading": "3500 62.5 84.2 58.0 94.3 69.5 85.7 63.7 96.8", "text": ""}, {"heading": "5000 66.6 80.9 65.6 85.7 71.8 85.7 70.7 85.7", "text": ""}, {"heading": "250 37.6 67.7 24.7 83.3 42.8 40.1 27.9 51.5", "text": ""}, {"heading": "500 39.6 52.3 36.8 55.9 43.7 38.5 40.9 39.0", "text": ""}, {"heading": "1000 43.1 39.1 40.7 47.8 47.9 37.6 43.9 38.0", "text": ""}, {"heading": "2500 52.2 50.8 45.1 50.8 57.1 28.5 50.2 38.6", "text": ""}, {"heading": "3500 48.7 33.3 48.7 33.3 58.8 28.5 57.1 28.5", "text": ""}, {"heading": "5000 48.7 33.3 47.5 33.3 57.1 14.3 53.8 19.9", "text": "Table 3 presents the results in terms of false positives (FP) and false negatives (FN) by Algorithm 1 with the various methods on the training data with different information criteria, sample sizes, and whether Bootstrapping is applied. The trade-off between false positive rates and false negative rates usually is case-specific. We observe that, in general, the objective of such an approach is to correctly and precisely recover the true distribution underlying the training data. For this reason, unless differently specified for specific uses, there is not an overall preference toward either lower false positive or lower false negative. Therefore, we evaluate our methods by considering the sum of both false positive and false negative rates. This metric is baiased toward a combination of relatively low FP and FN rather that the combination of very low FP and high FN and so on. By\nanalyzing the results shown in Table 3, we can clearly observe a trend where AIC with Bootstrapping on small sample datasets (i.e., 250) and BIC with Bootstrapping on large sample datasets (i.e., 5000) produces the best results, consistently with the discussion of the previous Section. Also, we observe that both for AIC without any bootstrapping on sample sizes of 250 and BIC without any bootstrapping on sample size of 5000, the false positive rates are reduced without a significant increase in the false negative rates.\nWe conclude by pointing out the significant increase in performance (both in termf of FP and FN) when using SBCNs in place of standard BNs (compare Tables 2 and 3).\n3.2.4 Assumption of Sparse Relationships The resulting false positive rate may still seem relatively\nhigh. But, one important assumption is worth mentioning. In the training data, such high false positive rate derives from the fact that portfolios are dependent on the 5 common factors, which very likely will induce co-movements. However, in the real data, such nested dependencies do not always occur, while a feature of sparse relationships appears frequently, and portfolios depend on distinctively small sets of factors. This assumption of sparsity can significantly improve the performance of the algorithm.\nTo implement the assumption of sparsity, we deviate from the original Fama French five factor model. For simplicity, we generate data with sparse relationships using a random linear model with 10 factor variables and 20 stock variables. With 30% probability, each stock variable is dependent linearly on one of the 10 factor variables, so on average, each stock variable will be dependent on 3 factor variables, which will likely be distinct from the dependent factor variables of other stock variables. Then we sample factor variable data from normal distribution and compute the corresponding stock data using the linear model.\nImplementing this sparsity on a new set of purely random training data we obtain with Algorithm 1 much better results, and, e.g., following the BIC with Bootstrapping method mentioned above, we obtain on small sample size (250 samples) 18.1% false positive and 39.2% false negative rates, while on large sample size (2500 samples), we obtain 50.2% false positive and 38.6% false negative rates.\nAlso the results for general BNs improve, but still being less effective than SBCNs.\n3.2.5 Summary, ROC Space We finally summarize in Figure 4 the results of this Sec-\ntion by interpolating and then smoothing out some kinks, and we obtain an ROC Space, whose x axis represent the False Positive Rate and y axis the True Positive Rate. ROC Space depicts the performance of the different methods we discussed on different sample sizes. By examining the plot, one can conclude that AICs generally have high true positive rates but also high false positive rates, as a result of its less stringent complexity penalty. In contrast, BICs generally have smaller false positive rates, but its true positive rates are also lower. Comparing the algorithms with and without bootstrapping, one can notice that the bootstrap procedure shifts the curves to the left. Still, the best performance lies in the data with the assumption of sparse relationships. Based on these results, we can conclude that with Bootstrapping and the assumption of sparse relationships, our algorithm is\ncapable of recovering accurately the causal relationships in the data."}, {"heading": "3.3 Performing Stress Testing", "text": "In this Section we present how to assess stress testing scenarios given the inferred Suppes-Bayes Causal Network and we present the results on the simulated data.\n3.3.1 Risk Management by Simulations After the inference of the SBCN, we perform Monte Carlo\nSimulation in the same way as conventional risk management, by drawing large number of samples to discover the worst 5% scenarios as the value at risk (VaR). Nevertheless, here in stress testing, we are targeting the most extreme events, which have very low but nonzero probability of occurrence. Thus, they still can occur, for example, the 2008 financial crisis or the most recent market reactions to BREXIT. Therefore, when drawing samples from the network, we would like to reject the normal scenarios, and place more importance on the extreme events. To achieve this goal, when conducting random sampling, we classify each possible branch as profitable or risky, and if the branch is classified as profitable, then we will avoid that branch.\nFigure 5 represents a simple binary classification where for this factor only Factor.i with value 0 is considered risky and, hence, this scenario is the only one to be sampled. In this way, we target the extreme risky events and reduce computation. But, unlike conventional risk management, this approach does not allow us to estimate the probability of occurrence of the sampled extreme events, therefore we will not conclude a value at risk with certain confidence level.\nThe simple binary classification with certain features is a standard machine learning problem. Here we explore a simple solution of such a task based on decision trees [20]. A decision tree is a predictive models that maps features of an object to a target value of the object. Here, the features are the factors of interest, and the target value is whether\nthe portfolio is prone to profit or loss. To perform classification, we first draw 1000 sample trajectories from the inferred SBCN. Then we construct a simple portfolio, which is long on all the stocks in the SBCN by the same amount, and calculate the Profit and Loss (P/L) of each observation. Here however, because the underlying SBCN depicts binary variables, exact Profit and Loss (P/L) statistics cannot be obtained. Instead, since the toy portfolio is long on all stocks by the same amount, the ratio of stocks that goes up is an approximate measure of risk. Of course, for continuous Bayesian network, Profit and Loss can be calculated directly. In the next step, we sort this measure, and denote the bottom-most 100 scenarios as risky, and the rest as profitable. The 100 \u2018risky\u2019 scenarios contain at least 7 stocks that fall. Then we consider 1000 samples each of them labeled as \u2018risky\u2019 or \u2018profitable\u2019. In our experiments, we used the R \u2018tree\u2019 package [19].\nUsing the SBCN learned from the simulated training data, we obtain the following decision tree shown in Figure 6.\nIn the decision tree of Figure 6, S denotes factor SMB; M denotes Market Km; H denotes HML; R denotes RMW and C denotes CMA. Here we show only the left part of\nthe entire computed decision tree, the subtree with S = 1 is omitted, since the entire subtree with S = 1 is classified as \u2018Profitable\u2019, which is not of interest for stress testing. In the tree, we identify two paths that are classified as \u2018Risky\u2019. Path S = 0, M = 0, H = 0, R = 0, C = 0 and Path S = 0,M = 0, H = 0, R = 0, C = 1. The paths classified are intuitive, since our example portfolio is long with equal amount invested over all 10 stocks. Since 10 stocks are generally positively dependent on the factors, most factors with 0 values will likely induce a \u2018Risky\u2019 path. For more complicated portfolios and real factors, such intuition cannot be easily found so we have to rely on the result of classification.\n3.3.2 Scenario Generation and Results Given the tree of Figure 6, we then used the bnlearn R\npackage [22] to sample from the SBCN. Given the network, we can simulate random scenarios, however, we wish not to simulate all of them, which will prove to be inefficient, but following the informations provided by the classification tree we choose the configurations which are likely to indicate risk to drive our sampling. For instance, we may pick the first path in the tree, which is S = 0, M = 0, H = 0, R = 0, C = 0, and constrain the distribution induced by the SBCN. In order to avoid sampling the scenarios which are not in accordance with the path, we adjust the conditional probability table of the SBCN. Since we want paths with all five factors taking value 0, we set the conditional probability of these five factors taking value 1 to 0, and the conditional probability of factors taking value 0 to 1. In this way, the undesirable paths will be unlikely to be simulated, while the intrinsic distribution of how factors affect the stocks is still modeled. More sophisticated implementation based on this intuition are possible: e.g., using branch-and-bound, policy valuation, tree-search, etc, but we will leave this to further research.\nComparing the results of the simulations given the original SBCN and the one taking into account the decision tree, we show the distribution of the risk measure, the number of stocks that go up in the Figure 7.\nThe number of stocks going up from 100 samples generated by the original SBCN is roughly evenly distributed. At the same time, the 100 samples generated by the modified SBCN contain no scenarios with more than 5 stocks going up, and 84 out of the 100 samples have at most 1 stock go-\ning up. We can clearly see that the modified SBCN places far more importance on the stressed scenarios, and in turn confirms the result of the classification algorithm by the decision tree. In this way, computational complexity involved in generating stressed scenarios can be improved tremendously. This kind of computational efficiency issues will be more critical when we move from a simple Bernoulli random variable to multi-categorical variables or continuous random variable. Therefore, with the same computing power, the modified SBCN makes it possible to generate more stressed scenarios, and observe how portfolios or other assets respond to stressed factors."}, {"heading": "4. CONCLUSIONS", "text": "In summary, in this paper we develop a novel framework to perform stress testing combining Suppes-Bayes Causal Networks and machine learning classification. We learn SBCNs from data using Algorithm 1 and assess the quality of the learned model by switching information criteria based upon sample sizes and bootstrapping. We then simulate stress scenarios using SBCNs, but reduce computation by classifying each branch of nodes in the network as \u2018profitable\u2019 or \u2018risky\u2019 using classification trees. For simplicity, the paper implements SBCNs with Bernoulli variables and simulates data using Fama French\u2019s Five Factor Model, but the logic of the problem is easily extended to deal with more practical situations. First of all, the SBCNs can accommodate more complicated variables (nodes). In addition to the factor based portfolios considered here, other factor models, or directly other financial and economic factors like foreign exchange rates, can also be included, and the accuracy of the model can ensure that the true causal relationships among the factors are discovered. In practice, variables like stock prices are continuous, thus, one can easily extend to these situations by adopting a hybrid SBCN, where the variables can take either discrete or continuous values, making it possible to represent precisely the values of the variables we are interested in.\nTo use the model, the role of experts is still important. After learning the SBCN from data and applying classification, we can identify a number of stressed scenarios. However, we expect that some of these to be unacceptable for various unforeseen reasons, e.g., as those known to domain experts. These scenarios may be thought of as highly stressed with respect to the corresponding portfolio but they could prove to be less useful in practice. Therefore, experts can select from the identified stressed scenarios only the plausible ones, and discard the ones deemed to be flawed. Even in this case, we can perform simulations following the selected stressed paths in the SBCN and observe the reactions of the portfolios in these stressed scenarios of interest, and thus adjust the portfolios based on the reactions. Another direct usage of our approach is when experts have a particular candidate stress scenario in mind, which can be justified a priori; in this case one can skip the process of classification and directly adjust the SBCN mutatis mutandis. Therefore, simulations of the adjusted SBCN will also offer the reactions of the portfolio to this particular stressed scenario.\nWe believe, based on our empirical analysis, that we have devised an efficient automated stress testing method using machine learning and causality analysis in order to solve a critical regulatory problem, as demonstrated by the algorithm\u2019 ability to recover the causal relationships in the data,\nas well as its efficiency, in terms of computation and data usage."}, {"heading": "5. REFERENCES", "text": "[1] H. Akaike. Information theory and an extension of the\nmaximum likelihood principle. In Selected Papers of Hirotugu Akaike, pages 199\u2013213. Springer, 1998.\n[2] N. Beerenwinkel, N. Eriksson, and B. Sturmfels. Conjunctive bayesian networks. Bernoulli, pages 893\u2013909, 2007.\n[3] F. Bonchi, S. Hajian, B. Mishra, and D. Ramazzotti. Exposing the probabilistic causal structure of discrimination. arXiv preprint arXiv:1510.00552, 2015.\n[4] G. Caravagna, A. Graudenzi, D. Ramazzotti, R. Sanz-Pamplona, L. De Sano, G. Mauri, V. Moreno, M. Antoniotti, and B. Mishra. Algorithmic methods to infer the evolutionary trajectories in cancer progression. PNAS, 2016.\n[5] S. Claessens and M. A. Kose. Financial crises: Explanations, types and implications. IMF Working Paper Series, 2013.\n[6] B. Efron. Nonparametric estimates of standard error: the jackknife, the bootstrap and other methods. Biometrika, 68(3):589\u2013599, 1981.\n[7] E. F. Fama and K. R. French. Multifactor explanations of asset pricing anomalies. The journal of finance, 51(1):55\u201384, 1996.\n[8] A. J. McNeil, R. Frey, and P. Embrechts. Quantitative Risk Management: Concepts, Techniques and Tools. Princeton University Press, 2010.\n[9] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.\n[10] L. O. Loohuis, G. Caravagna, A. Graudenzi, D. Ramazzotti, G. Mauri, M. Antoniotti, and B. Mishra. Inferring tree causal models of cancer progression with probability raising. PloS one, 9(10):e108358, 2014.\n[11] S. Manganell and R. F.Engle. Value at risk models in finance. European Central Bank Working Paper Series, 2001.\n[12] C. on the Global Financial System. Stress testing at major financial institutions: survey results and practice. 2005.\n[13] K. Pearson. Mathematical contributions to the theory of evolution.\u2013on a form of spurious correlation which may arise when indices are used in the measurement of organs. Proceedings of the royal society of london, 60(359-367):489\u2013498, 1896.\n[14] D. Ramazzotti, G. Caravagna, L. O. Loohuis, A. Graudenzi, I. Korsunsky, G. Mauri, M. Antoniotti, and B. Mishra. Capri: efficient inference of cancer progression models from cross-sectional data. Bioinformatics, 31(18):3016\u20133026, 2015.\n[15] D. Ramazzotti, A. Graudenzi, G. Caravagna, and M. Antoniotti. Modeling cumulative biological phenomena with suppes-bayes causal networks. arXiv preprint arXiv:1602.07857, 2016.\n[16] D. Ramazzotti, M. Nobile S, A. Graudenzi, and M. Antoniotti. Learning the probabilistic structure of cumulative phenomena with suppes-bayes causal networks. Submitted, 2016.\n[17] S. Raychaudhuri, S. J. Mason, R. R. Hill, L. MA\u0303u\u030bnch, O. Rose, T. Jefferson, and J. W. Fowler. Introduction to monte carlo simulation. In 2008 Winter Simulation Conference, 2008.\n[18] R. Rebonato. Coherent Stress Testing: a Bayesian approach to the analysis of financial stress. John Wiley & Sons, 2010.\n[19] B. Ripley. Tree: Classification and Regression Trees, 2016. R package version 1.0-37.\n[20] S. R. Safavian and D. Landgrebe. A survey of decision tree classifier methodology. 1990.\n[21] G. Schwarz et al. Estimating the dimension of a model. The annals of statistics, 6(2):461\u2013464, 1978.\n[22] M. Scutari. Learning bayesian networks with the bnlearn r package. arXiv preprint arXiv:0908.3817, 2009.\n[23] P. Suppes. A probabilistic theory of causality. North-Holland Publishing Company Amsterdam, 1970."}], "references": [{"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akaike"], "venue": "In Selected Papers of Hirotugu Akaike,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Conjunctive bayesian networks", "author": ["N. Beerenwinkel", "N. Eriksson", "B. Sturmfels"], "venue": "Bernoulli, pages 893\u2013909,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Exposing the probabilistic causal structure of discrimination", "author": ["F. Bonchi", "S. Hajian", "B. Mishra", "D. Ramazzotti"], "venue": "arXiv preprint arXiv:1510.00552,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Algorithmic methods to infer the evolutionary trajectories in cancer progression", "author": ["G. Caravagna", "A. Graudenzi", "D. Ramazzotti", "R. Sanz-Pamplona", "L. De Sano", "G. Mauri", "V. Moreno", "M. Antoniotti", "B. Mishra"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Financial crises: Explanations, types and implications", "author": ["S. Claessens", "M.A. Kose"], "venue": "IMF Working Paper Series,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Nonparametric estimates of standard error: the jackknife, the bootstrap and other methods", "author": ["B. Efron"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1981}, {"title": "Multifactor explanations of asset pricing anomalies", "author": ["E.F. Fama", "K.R. French"], "venue": "The journal of finance,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Quantitative Risk Management: Concepts, Techniques and Tools", "author": ["A.J. McNeil", "R. Frey", "P. Embrechts"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Inferring tree causal models of cancer progression with probability raising", "author": ["L.O. Loohuis", "G. Caravagna", "A. Graudenzi", "D. Ramazzotti", "G. Mauri", "M. Antoniotti", "B. Mishra"], "venue": "PloS one,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Value at risk models in finance", "author": ["S. Manganell", "R.F.Engle"], "venue": "European Central Bank Working Paper Series,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Capri: efficient inference of cancer progression models from cross-sectional data", "author": ["D. Ramazzotti", "G. Caravagna", "L.O. Loohuis", "A. Graudenzi", "I. Korsunsky", "G. Mauri", "M. Antoniotti", "B. Mishra"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Modeling cumulative biological phenomena with suppes-bayes causal networks", "author": ["D. Ramazzotti", "A. Graudenzi", "G. Caravagna", "M. Antoniotti"], "venue": "arXiv preprint arXiv:1602.07857,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Learning the probabilistic structure of cumulative phenomena with suppes-bayes causal networks", "author": ["D. Ramazzotti", "M. Nobile S", "A. Graudenzi", "M. Antoniotti"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Introduction to monte carlo simulation", "author": ["S. Raychaudhuri", "S.J. Mason", "R.R. Hill", "L. M\u00c3\u0171nch", "O. Rose", "T. Jefferson", "J.W. Fowler"], "venue": "In 2008 Winter Simulation Conference,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Coherent Stress Testing: a Bayesian approach to the analysis of financial stress", "author": ["R. Rebonato"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Tree: Classification and Regression Trees, 2016. R package version 1.0-37", "author": ["B. Ripley"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "A survey of decision tree classifier methodology", "author": ["S.R. Safavian", "D. Landgrebe"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1990}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The annals of statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1978}, {"title": "Learning bayesian networks with the bnlearn r package", "author": ["M. Scutari"], "venue": "arXiv preprint arXiv:0908.3817,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "A probabilistic theory of causality", "author": ["P. Suppes"], "venue": "North-Holland Publishing Company Amsterdam,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1970}], "referenceMentions": [{"referenceID": 7, "context": "xxx/xxx x to negative [8].", "startOffset": 22, "endOffset": 25}, {"referenceID": 14, "context": "Depending on different financial agencies, hedge funds, banks or clearing houses, and on different financial instruments, stocks, bonds, or derivatives, the method of risk management may vary, but the central idea behind conventional risk management remains: we assess the statistical distribution of the asset (or portfolio), and estimate the worst-case scenarios, generally by methods such as Monte Carlo Simulation [17].", "startOffset": 418, "endOffset": 422}, {"referenceID": 10, "context": "For example, in the recent 2008 financial crisis, the reserves calculated the risk by using methods such as VaR (Value at Risk) [11] which proved to be painfully inadequate.", "startOffset": 128, "endOffset": 132}, {"referenceID": 4, "context": "Stress testing refers to the analysis or simulation of the response of financial instruments or institutions, given intensely stressed scenarios that may lead to a financial crisis [5].", "startOffset": 181, "endOffset": 184}, {"referenceID": 15, "context": "proposed a sampling approach based on Bayesian networks in [18].", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "In this work we use Bayesian Graphical Models [9], popularly known as Bayesian networks, as a framework to assess stress testing, as previously done in this context by [18].", "startOffset": 46, "endOffset": 49}, {"referenceID": 15, "context": "In this work we use Bayesian Graphical Models [9], popularly known as Bayesian networks, as a framework to assess stress testing, as previously done in this context by [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 1, "context": "Bayesian networks have long been used in biological modeling such as -omics data analysis, cancer progression or genetics [2, 10, 14], but their application to financial data analysis has been rare.", "startOffset": 122, "endOffset": 133}, {"referenceID": 9, "context": "Bayesian networks have long been used in biological modeling such as -omics data analysis, cancer progression or genetics [2, 10, 14], but their application to financial data analysis has been rare.", "startOffset": 122, "endOffset": 133}, {"referenceID": 11, "context": "Bayesian networks have long been used in biological modeling such as -omics data analysis, cancer progression or genetics [2, 10, 14], but their application to financial data analysis has been rare.", "startOffset": 122, "endOffset": 133}, {"referenceID": 12, "context": "In this paper we adopt a variation of the traditional Bayesian networks as done in [15, 16], where the authors show how constraining the search space of valid solutions by means of a causal theory grounded in Suppes\u2019 notion of probabilistic causation [23] can be exploited in order to devise better learning algorithms.", "startOffset": 83, "endOffset": 91}, {"referenceID": 13, "context": "In this paper we adopt a variation of the traditional Bayesian networks as done in [15, 16], where the authors show how constraining the search space of valid solutions by means of a causal theory grounded in Suppes\u2019 notion of probabilistic causation [23] can be exploited in order to devise better learning algorithms.", "startOffset": 83, "endOffset": 91}, {"referenceID": 20, "context": "In this paper we adopt a variation of the traditional Bayesian networks as done in [15, 16], where the authors show how constraining the search space of valid solutions by means of a causal theory grounded in Suppes\u2019 notion of probabilistic causation [23] can be exploited in order to devise better learning algorithms.", "startOffset": 251, "endOffset": 255}, {"referenceID": 12, "context": "Moreover, through a maximum likelihood optimization scheme which makes use of a regularization score, we also attempt to only retain edges in the Bayesian network (graphically depicted as a directed acyclic graph, DAG) that correspond to only genuine causation, while eliminating all the spurious causes [15, 16].", "startOffset": 304, "endOffset": 312}, {"referenceID": 13, "context": "Moreover, through a maximum likelihood optimization scheme which makes use of a regularization score, we also attempt to only retain edges in the Bayesian network (graphically depicted as a directed acyclic graph, DAG) that correspond to only genuine causation, while eliminating all the spurious causes [15, 16].", "startOffset": 304, "endOffset": 312}, {"referenceID": 8, "context": "The nodes induce an overall joint distribution that can be written as a product of the conditional distributions associated with each variable [9].", "startOffset": 143, "endOffset": 146}, {"referenceID": 8, "context": "More details about Bayesian networks may be found in [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 8, "context": "Simply speaking, for any node X in a Bayesian network, given the knowledge of node X\u2019s parents, X is conditionally independent of all nodes that are not its children, or all its predecessors [9].", "startOffset": 191, "endOffset": 194}, {"referenceID": 15, "context": "In the context of stress testing, Rebonato [18] suggests a subjective approach to constructing Bayesian networks.", "startOffset": 43, "endOffset": 47}, {"referenceID": 15, "context": "Please see [18] for details.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "Thus, unlike [18], our stress testing approach builds on the foundation of Suppes-Bayes Causal Networks (SBCNs), which are not only more strictly regularized than the general Bayesian networks but also enjoys many other attractive features such as interpretability and refutability.", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "SBCNs exploit the notion of probabilistic causation, originally proposed by Patrick Suppes [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 20, "context": "In [23], Suppes described the notion of prima facie causation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Definition 1 (Probabilistic causation, [23]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "The notion of prima facie causality was fruitfully exploited for the task of modeling cancer evolution in [10, 14, 4], and the SBCNs were finally described for the first time in [3] and, later on, defined in [15, 16] as follow.", "startOffset": 106, "endOffset": 117}, {"referenceID": 11, "context": "The notion of prima facie causality was fruitfully exploited for the task of modeling cancer evolution in [10, 14, 4], and the SBCNs were finally described for the first time in [3] and, later on, defined in [15, 16] as follow.", "startOffset": 106, "endOffset": 117}, {"referenceID": 3, "context": "The notion of prima facie causality was fruitfully exploited for the task of modeling cancer evolution in [10, 14, 4], and the SBCNs were finally described for the first time in [3] and, later on, defined in [15, 16] as follow.", "startOffset": 106, "endOffset": 117}, {"referenceID": 2, "context": "The notion of prima facie causality was fruitfully exploited for the task of modeling cancer evolution in [10, 14, 4], and the SBCNs were finally described for the first time in [3] and, later on, defined in [15, 16] as follow.", "startOffset": 178, "endOffset": 181}, {"referenceID": 12, "context": "The notion of prima facie causality was fruitfully exploited for the task of modeling cancer evolution in [10, 14, 4], and the SBCNs were finally described for the first time in [3] and, later on, defined in [15, 16] as follow.", "startOffset": 208, "endOffset": 216}, {"referenceID": 13, "context": "The notion of prima facie causality was fruitfully exploited for the task of modeling cancer evolution in [10, 14, 4], and the SBCNs were finally described for the first time in [3] and, later on, defined in [15, 16] as follow.", "startOffset": 208, "endOffset": 216}, {"referenceID": 8, "context": "Second, when learning general Bayesian networks, arcs A \u2192 B and A \u2190 B may sometimes be equally acceptable, resulting in an undirected arc A\u2212B (this situation is called Markov Equivalence [9]).", "startOffset": 187, "endOffset": 190}, {"referenceID": 12, "context": "For SBCNs, such a situation does not arise because of the temporal flow being irreversible [15, 16].", "startOffset": 91, "endOffset": 99}, {"referenceID": 13, "context": "For SBCNs, such a situation does not arise because of the temporal flow being irreversible [15, 16].", "startOffset": 91, "endOffset": 99}, {"referenceID": 6, "context": "To simulate the training data, we adopt a common stock factor model, the Fama French Five Factor Model [7], where the return of the asset is defined as follow:", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "In the equation, r is the return of the asset; Rf is the risk free return, usually measured in terms of government treasury returns; Km stands for market factor, measured as value-weighted market portfolio, similar to stock indexes; SMB (Small Minus Big) stands for company size factor, measured by return on a diversified portfolio of small stocks minus the return on a diversified portfolio of big stocks; HML (High Minus Low) stands for company book-to-market (B/M) ratio factor, measured by difference between the returns on diversified portfolios of high and low B/M stocks, where B/M is the ratio between company\u2019s book value to market value; RMW (Robust Minus Weak) stands for company operating profitability factor, measured by the returns on diversified portfolios of stocks with robust and weak profitability and CMA (Conservative Minus Aggressive) stands for company investment factor, difference between the returns on diversified portfolios of low and high investment stocks, called conservative and aggressive [7].", "startOffset": 1024, "endOffset": 1027}, {"referenceID": 11, "context": "Starting with such an input, we attempted to experiment with our learning algorithms previously described in [14] and [3].", "startOffset": 109, "endOffset": 113}, {"referenceID": 2, "context": "Starting with such an input, we attempted to experiment with our learning algorithms previously described in [14] and [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "In particular, as in [3], we lacked explicitly observed time in the data, which are only cross-sectional.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "For more information about the algorithm, also refer to [14, 3].", "startOffset": 56, "endOffset": 63}, {"referenceID": 2, "context": "For more information about the algorithm, also refer to [14, 3].", "startOffset": 56, "endOffset": 63}, {"referenceID": 2, "context": "Algorithm 1 Learning the SBCN [3]", "startOffset": 30, "endOffset": 33}, {"referenceID": 18, "context": "The explanation for this trend can be found in how the algorithm implements the regularization via Bayesian Information Criterion (BIC) [21], that is:", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "However, maximum likelihood is known to be susceptible to overfitting [9], especially when, as in our case, it deals with small sample size in the training data.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "For additional discussion, see the details in [9].", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "To reconcile this dilemma, we now next considered an alternative information criterion, the AIC, Akaike Information Criterion [1], defined as in the following:", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "To improve their performance, now we propose to make use of a bootstrap [6] procedure for model selection.", "startOffset": 72, "endOffset": 75}, {"referenceID": 17, "context": "Here we explore a simple solution of such a task based on decision trees [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "In our experiments, we used the R \u2018tree\u2019 package [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "Given the tree of Figure 6, we then used the bnlearn R package [22] to sample from the SBCN.", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "The most recent financial upheavals have cast doubt on the adequacy of some of the conventional quantitative risk management strategies, such as VaR (Value at Risk), in many common situations. Consequently, there has been an increasing need for verisimilar financial stress testings, namely simulating and analyzing financial portfolios in extreme, albeit rare scenarios. Unlike conventional risk management which exploits statistical correlations among financial instruments, here we focus our analysis on the notion of probabilistic causation, which is embodied by Suppes-Bayes Causal Networks (SBCNs), SBCNs are probabilistic graphical models that have many attractive features in terms of more accurate causal analysis for generating financial stress scenarios. In this paper, we present a novel approach for conducting stress testing of financial portfolios based on SBCNs in combination with classical machine learning classification tools. The resulting method is shown to be capable of correctly discovering the causal relationships among financial factors that affect the portfolios and thus, simulating stress testing scenarios with a higher accuracy and lower computational complexity than conventional Monte Carlo Simulations.", "creator": "LaTeX with hyperref package"}}}