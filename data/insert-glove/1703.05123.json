{"id": "1703.05123", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Character-based Neural Embeddings for Tweet Clustering", "abstract": "In tradeoffs this paper bhaktavatsalam we show pyrek how the lovatt performance alor of tweet clustering signal can be improved by leveraging lacemaking character - filmus based ekka neural bread networks. ineradicable The proposed throbbing approach 31-21 overcomes ltx the fehbp limitations simus related to the agronomique vocabulary wesson explosion in asheri the seiter word - based giltinan models villaurrutia and bulges allows for the seamless 10.5 processing of scheff the multilingual content. Our evaluation results indiana-based and mesadieu code are 19.01 available mammographic on - outsmart line at", "histories": [["v1", "Wed, 15 Mar 2017 12:37:22 GMT  (295kb,D)", "https://arxiv.org/abs/1703.05123v1", "Accepted at the SocialNLP 2017 workshop held in conjunction with EACL 2017, April 3, 2017, Valencia, Spain"], ["v2", "Thu, 16 Mar 2017 08:57:29 GMT  (295kb,D)", "http://arxiv.org/abs/1703.05123v2", "Accepted at the SocialNLP 2017 workshop held in conjunction with EACL 2017, April 3, 2017, Valencia, Spain"]], "COMMENTS": "Accepted at the SocialNLP 2017 workshop held in conjunction with EACL 2017, April 3, 2017, Valencia, Spain", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["svitlana vakulenko", "lyndon nixon", "mihai lupu"], "accepted": false, "id": "1703.05123"}, "pdf": {"name": "1703.05123.pdf", "metadata": {"source": "CRF", "title": "Character-based Neural Embeddings for Tweet Clustering", "authors": ["Svitlana Vakulenko", "Lyndon Nixon", "Mihai Lupu"], "emails": ["svitlana.vakulenko@wu.ac.at", "nixon@modultech.eu", "mihai.lupu@tuwien.ac.at"], "sections": [{"heading": null, "text": "In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks. The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content. Our evaluation results and code are available on-line1."}, {"heading": "1 Introduction", "text": "Our use case scenario, as part of the InVID project2, originates from the needs of professional journalists responsible for reporting breaking news in a timely manner. News often appear on social media exclusively or right before they appear in the traditional news media. Social media is also responsible for the rapid propagation of inaccurate or incomplete information (rumors). Therefore, it is important to provide efficient tools to enable journalists rapidly detect breaking news in social media streams (Petrovic et al., 2013).\nThe SNOW 2014 Data Challenge provided the task of extracting newsworthy topics from Twitter. The results of the challenge confirmed that the task is ambitious: The best result was 0.4 F-measure.\nBreaking-news detection involves 3 subtasks: selection, clustering, and ranking of tweets. In this paper, we address the task of tweet clustering as one of the pivotal subtasks required to enable effective breaking news detection from Twitter.\nTraditional approaches to clustering textual documents involve construction of a documentterm matrix, which represents each document as\n1https://github.com/vendi12/tweet2vec_ clustering\n2http://www.invid-project.eu\na bag-of-words. These approaches also require language-specific sentence and word tokenization.\nWord-based approaches fall short when applied to social media data, e.g., Twitter, where a lot of infrequent or misspelled words occur within very short documents. Hence, the document representation matrix becomes increasingly sparse.\nOne way to overcome sparseness in a tweetterm matrix is to consider only the terms that appear frequently across the collection and drop all the infrequent terms. This procedure effectively removes a considerable amount of information content. As a result, all tweets that do not contain any of the frequent terms receive a null-vector representation. These tweets are further ignored by the model and cannot influence clustering outcomes in the subsequent time intervals, where the frequency distribution may change, which hinders the detection of emerging topics.\nArtificial neural networks (ANNs) allow to generate dense vector representation (embeddings), which can be efficiently generated on the word- as well as character levels (dos Santos and Zadrozny, 2014; Zhang et al., 2015; Dhingra et al., 2016). The main advantage of the character-based approaches is their language-independence, since they do not require any language-specific parsing.\nThe major contribution of our work is the evaluation of the character-based neural embeddings on the tweet clustering task. We show how to employ character-based tweet embeddings for the task of tweet clustering and demonstrate in the experimental evaluation that the proposed approach significantly outperforms the current state-of-theart in tweet clustering for breaking news detection.\nThe remaining of this paper is structured as follows: Section 2 provides an overview of the related work; we describe the setup of an extensive evaluation in Section 3; report and discuss the results in Sections 4 and 5, respectively; conclu-\nar X\niv :1\n70 3.\n05 12\n3v 2\n[ cs\n.I R\n] 1\n6 M\nar 2\n01 7\nsion (Section 6) summarizes our findings and directions for future work."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Breaking news detection", "text": "There has been a continuous effort over the recent years to design effective and efficient algorithms capable of detecting newsworthy topics in the Twitter stream (Hayashi et al., 2015; Ifrim et al., 2014; Vosecky et al., 2013; Wurzer et al., 2015). These current state-of-the-art approaches build upon the bag-of-words document model, which results in high-dimensional, sparse representations that do not scale well and are not aware of semantic similarities, such as paraphrases.\nThe problem becomes evident in case of tweets that contain short texts with a long tail of infrequent slang and misspelled words. The performance of the such approaches over Twitter datasets is very low, with F-measure up to 0.2 against the annotated Wikipidea articles as reference topics (Wurzer et al., 2015) and 0.4 against the curated topic pool (Papadopoulos et al., 2014)."}, {"heading": "2.2 Neural embeddings", "text": "Artificial neural networks (ANNs) allow to generate dense vector representations (embeddings). Word2vec (Mikolov et al., 2013) is by far the most popular approach. It accumulates the cooccurrence statistics of words that efficiently summarizes their semantics.\nBrigadir et al. (2014) demonstrated encouraging results using the word2vec Skip-gram model to generate event timelines from tweets. Moran et al. (2016) achieved an improvement over the state-of-the-art first story detection (FSD) results by expanding the tweets with their semantically related terms using word2vec.\nNeural embeddings can be efficiently generated on the character level as well. They repeatedly outperformed the word-level baselines on the tasks of language modeling (Kim et al., 2016), part-ofspeech tagging (dos Santos and Zadrozny, 2014), and text classification (Zhang et al., 2015). The main advantage of the character-based approach is its language-independence, since it does not depend on any language-specific preprocessing.\nDhingra et al. (2016) proposed training a recurrent neural network on the task of hashtag prediction. Vosoughi et al. (2016) demonstrated an improved performance of a character-based neural\nautoencoder on the task of paraphrase and semantic similarity detection in tweets.\nOur work extends the evaluation of the Tweet2Vec model (Dhingra et al., 2016) to the tweet clustering task, versus the traditional document-term matrix representation. To the best of our knowledge, this work is the first attempt to evaluate the performance of character-based neural embeddings on the tweet clustering task."}, {"heading": "3 Experimental Evaluation", "text": ""}, {"heading": "3.1 Dataset", "text": "Description and preprocessing. We use the SNOW 2014 test dataset (Papadopoulos et al., 2014) in our evaluation. It contains the IDs of about 1 million tweets produced within 24 hours.\nWe retrieved 845,626 tweets from the Twitter API, since other tweets had already been deleted from the platform. The preprocessing procedure: remove RT prefixes, urls and user mentions, bring all characters to lower case and separate punctuation with spaces (the later is necessary only for the word-level baseline).\nThe dataset is further separated into 5 subsets corresponding to the 1-hour time intervals (18:00, 22:00, 23:15, 01:00 and 01:30) that are annotated with the list of breaking news topics. In total, we have 48,399 tweets for clustering evaluation; the majority of them (42,758 tweets) are in English.\nThe dataset comes with the list of the breaking news topics. These topics were manually selected by the independent evaluators from the topic pool collected from all challenge participants (external topics). The list of topics contains 70 breaking news headlines extracted from tweets (e.g., \u201cThe new, full Godzilla trailer has roared online\u201d). Each topic is annotated with a few (at most 4) tweet IDs, which is not sufficient for an adequate evaluation of a tweet clustering algorithm.\nDataset extension. We enrich the topic annotations by collecting larger tweet clusters using fuzzy string matching3 for each of the topic labels. Fuzzy string matching uses the Levenstein (edit) distance (Levenshtein, 1966) between the two input strings as the measure of similarity. Levenstein distance corresponds to the minimum number of character edits (insertions, deletions, or substitutions) required to transform one string into the\n3https://github.com/seatgeek/ fuzzywuzzy\nother. We choose only the tweets for which the similarity ratio with the topic string is greater than 0.9 threshold.\nA sample tweet cluster produced with the fuzzy string matching for the topic \u201cJustin Trudeau apologizes for Ukraine joke\u201d:\n\u2022 Justin Trudeau apologizes for Ukraine joke: Justin Trudeau said he\u2019s spoken the head... \u2022 Justin Trudeau apologizes for Ukraine comments http://t.co/7ImWTRONXt \u2022 Justin Trudeau apologizes for Ukraine hockey joke #cdnpoli\nIn total, we matched 2,585 tweets to 132 clusters using this approach. The resulting tweet clusters represent the ground-truth topics within different time intervals. The cluster size varies from 1 to 361 tweets with an average of 20 tweets per cluster (median: 6.5).\nThis simple procedure allows us to automatically generate high-quality partial labeling. We further use this topic assignment as the groundtruth class labels to automatically evaluate different flat clustering partitions."}, {"heading": "3.2 Tweet representation approaches", "text": "TweetTerm. Our baseline is the tweet representation approach that was used in the winnersystem of SNOW 2014 Data Challenge4 (Ifrim et al., 2014). This approach represents a collection of tweets as a tweet-term matrix by keeping the bigrams and trigrams that occur at least in 10 tweets.\nTweet2Vec. This approach includes two stages: (1) training a neural network to predict hashtags using the subset of tweets that contain hashtags (88,148 tweets in our case); (2) encoding: use the trained model to produce tweet embeddings for all the tweets regardless whether they contain hashtags or not. We use Tweet2Vec implementation5 to produce tweet embeddings. Tweet2Vec is a bi-directional recurrent neural network that consumes textual input as a sequence of characters. The network architecture includes two Gated Recurrent Units (GRUs) (Cho et al., 2014): forward and backward GRUs. GRU is an optimized version of a Long Short-Term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997). It includes 2 gates that control the\n4https://github.com/heerme/ twitter-topics\n5https://github.com/bdhingra/tweet2vec\ninformation flow. The gates (reset and update gate) regulate how much the previous output state (ht\u22121) influences the current state (ht).\nThe two GRUs are identical, but the backward GRU receives the same sequence of tweetcharacters in reverse order. Each GRU computes its own vector-representation for every substring (ht) using the current character vector (xt) and the vector-representation it computed a step before (ht\u22121). These two representations of the same tweet are combined in the next layer of the neural network to produce the final tweet embedding (see more details in Dhingra et.al. (2016)).\nThe network is trained in minibatches with an objective function to predict the previously removed hashtags. A hashtag can be considered as the ground-truth cluster label for tweets. Therefore, the network is trained to optimize for the correct tweet classification, which corresponds to a supervised version of the tweet clustering task annotated with the cluster assignment, i.e. hashtags.\nIn order to predict the hashtags the tweet embeddings are passed through the linear layer, which produces the output in the size of the number of hashtags, which we observed in the training dataset. The softmax layer on top normalizes the scores from the linear layer to generate the hashtag probabilities for every input tweet.\nTweet embeddings are produced by passing the tweets through the trained Tweet2Vec model (encoder). In this way we can obtain vector representations for all the tweets including the ones that do not contain any hashtags. The result is a matrix of size n\u00d7 h, where n is the number of tweets and h is the number of hidden states (500)."}, {"heading": "3.3 Clustering", "text": "To cluster tweet vectors (character-based tweet embeddings produced by the neural network for Tweet2Vec evaluation or the document-term matrix for TweetTerm) we employ the hierarchical clustering algorithm implementation from fastcluster library (Mu\u0308llner, 2013).\nHierarchical clustering includes computing pairwise distances between the tweet vectors, followed by their linkage into a single dendrogram. There are several distance metrics (Euclidean, Manhattan, cosine, etc.) and linkage methods to compare distances (single, average, complete, weighted, etc.). We evaluated the performance of different methods using the cophenetic correlation\ncoefficient (CPCC) (Sokal and Rohlf, 1962) and found the best performing combination: Euclidean distance and average linkage method.\nThe hierarchical clustering dendrogram can produce n different flat clusterings for the same dataset: from n single-member clusters with one document per cluster to a single cluster that contains all n documents. The distance threshold defines the granularity (number and size) of the produced clusters."}, {"heading": "3.4 Distance threshold selection", "text": "Grid search helps us to determine the optimal distance threshold for the dendrogram cut-off. We generated a list of values in the range from 0.1 to 1.5 with 0.1 increment step and examine their performance with respect to the ground-truth cluster assignment. We produce flat clusterings for each value of the distance threshold from the grid and compare them with respect to the quality metrics.\nSince we also want to be able to select the optimal distance threshold in absence of the true labels, we examine the scores provided by the mean Silhouette coefficient (Rousseeuw, 1987). Silhouette is an unsupervised intrinsic evaluation metric (cluster validity index) that measures the quality of the produced clusters and can be used for unsupervised intrinsic evaluation (i.e., without the groundtruth labels). It was reported to outperform alternative methods in a comparative study of 30 validity indices (Arbelaitz et al., 2013)."}, {"heading": "3.5 Clustering Evaluation Metrics", "text": "We evaluate the clustering results using the standard metrics for extrinsic clustering evaluation: homogeneity, completeness, V-Measure (Rosenberg and Hirschberg, 2007), Adjusted Rand Index (ARI) (Hubert and Arabie, 1985) and Adjusted Mutual Information (AMI) (Nguyen et al., 2010). All metrics return a score on the range [0; 1] for the pair of sets that contain ground truth and cluster labels as input. The higher the score the more similar the two clusterings are.\nThe Homogeneity score represents the measure for purity of the produced clusters. It penalizes clustering, where members of different classes get clustered together. Thus, the best homogeneity scores are always at the bottom of the dendrogram, i.e., at the level of the leaves, where each document belongs to its own cluster. Completeness, on the contrary, favors larger clusters and reduces the score if the members of the same class are split\ninto different clusters. Therefore, the top of the dendrogram, where all the documents reside in a single cluster always achieves the maximum completeness score.\nV-Measure is designed to balance out the two extremes of homogeneity and completeness. It is the harmonic mean of the two and corresponds to the Normalized Mutual Information (NMI) score.\nAMI score is an extension of NMI adjusted for chance. The more clusters are considered the more chance the labelings correlate. AMI allows us to compare the clustering performance across different time intervals since it normalizes the score by the number of labeled clusters in each interval.\nFinally, ARI is an alternative way to assess the agreement between two clusterings. It counts all pairs clustered together or separated in different clusters. ARI also accounts for the chance of an overlap in a random label assignment."}, {"heading": "3.6 Manual Cluster Evaluation", "text": "Our partial labeling covers a small subset of the data and by design provides the clusters with the high degree of string overlap with the annotated topics. Therefore, we extend the clustering evaluation to the rest of the dataset to evaluate whether the models can uncover less straight-forward semantic similarities in tweets. We select the results for manual evaluation motivated by the cluster label (headline) selection task.\nThe next step in the breaking news detection pipeline after the clustering task is headline selection (cluster labeling task). The most common approach to label a cluster of tweets is to select a single tweet as a representative member for the whole cluster (Papadopoulos et al., 2014). We decided to test this assumption and manually check how many clusters loose their semantics when represented with a single tweet.\nHeadline selection motivates the coherence assessment of the produced clusters since the clusters discarded at this stage will never make it to the final results. To explore coherence of the produced clusters we pick several tweets in each cluster and check whether they are semantically similar.\nThe tweet selected as a headline (cluster label) can be the first published tweet as in First Story Detection (FSD) task, also used in Ifrim et al (2014). Alternative approaches include selection of the most recent tweet published on the topic, or the tweet that is semantically most similar\nto all other tweets in the cluster, i.e., the tweet closest to the centroid of the cluster (medoid-tweets). Therefore, we sample 5 tweets from each cluster: the first published tweet, the most recent tweet and three medoid-tweets.\nWe set up a manual evaluation task as follows:\n1. Take the top 20 largest clusters sorted by the number of tweets that belong to the cluster. 2. For each cluster:\n(a) Take the first and the last published tweet (tweets are previously sorted by the publication date). (b) Take three medoid-tweets, i.e., the tweets that appear closest to the centroid of the cluster. (c) Add the 5 tweets to the set associated with the cluster (removing exact duplicate tweets)\n3. For all clusters, where the set of selected tweets contains at least two unique tweets: 4 human evaluators independently assess the coherence of each cluster.\nAccording to the evaluation setup each model produced 20 top-clusters for each of the 5 intervals, i.e., 20 \u00d7 5 = 100 clusters per model. We manually evaluate only the clusters that contain more than 1 distinct representative tweet (Clusters>1). All other clusters, i.e., the ones for which all 5 selected tweets are identical (Clusters=1), are considered correct by default.\nResults for all 5 intervals were evaluated together in a single pool and the models were anonymized to avoid biases. Each evaluator independently assigned a single score to each cluster:\n\u2022 Correct \u2013 all tweets report the same news; \u2022 Partial \u2013 some tweets are not related; \u2022 Incorrect \u2013 all tweets are not related.\nPartial and Incorrect labels reflect different types of clustering errors. Partial error is less severe indicating that the tweets of the cluster are semantically similar, but they report different news (events) and should be split into several clusters. Incorrect clusters indicate a random collection of tweets with no semantic similarities."}, {"heading": "4 Results", "text": ""}, {"heading": "4.1 Results of Clustering Evaluation", "text": "Table 1 summarizes the results of our evaluation using the ground-truth partial labeling. The scores highlighted with the bold font indicate the best result among the two competing approaches for the same subset of tweets corresponding to the respective time interval.\nTweet2Vec exhibits better clustering performance comparing to the baseline according to the majority of the evaluation metrics in all the intervals. In all cases Tweet2Vec model wins in terms of Homogeneity score and TweetTerm wins in Completeness. This result shows that Tweet2Vec is better at separating tweets that are not similar enough than the baseline model. Tweet2Vec fails only once to perfectly separate the ground-truth clusters (18:00 interval). This result shows that Tweet2Vec is able to replicate the results of the fuzzy string matching algorithm that was used to generate the ground-truth labeling."}, {"heading": "4.2 Results of Distance Threshold Selection", "text": "The rise in V-Measure correlates with the decline of the Silhouette coefficient and the steep drop in the number of produced clusters (see Figure 1). We observed that the optimal distance threshold for Tweet2Vec clustering according to V-Measure is on the interval [0.8; 1] (see Table 1: Distance threshold), which is also consistent with the findings reported in Ifrim et. al (2014)."}, {"heading": "4.3 Results of Manual Cluster Evaluation", "text": "Results of the manual cluster evaluation by four independent evaluators are summarized in Table 2. Bold font indicates the maximum scores achieved across the competing representation approaches. Tables 3 and 4 show sample clusters produced by both models alongside their average score.\nTweetTerm assigns a 0-vector representation to tweets that do not contain any of the frequent terms. Hence, all these tweets end up in a single \u201cgarbage\u201d cluster. Therefore, we discount the number of the expected \u201cgarbage\u201d clusters (1 cluster per interval = 5 clusters) from the score count for TweetTerm (Table 2).\nTweet2Vec model produces the largest number of perfectly homogeneous clusters for which all 5 selected tweets are identical (see Table 2 column Clusters=1). The percentage of correct results among the manually evaluated clusters is higher for the TweetTerm model, but the number of errors (Incorrect) is higher as well. Tweet2Vec produced the highest total % of correct clusters due to the larger proportion of detected clusters that con-\ntain identical tweets (Clusters=1). Tweet2Vec also produced the least number of incorrect clusters: at most 2 incorrect clusters per 100 clusters (Precision: 0.98).\nThe results of Tweet2Vec on the multilingual dataset are lower than on the English-language tweets. However, we do not have alternative results to compare since the baseline approach is not language-independent and requires additional functionality (word-level tokenizers) to handle tweets in other languages, e.g., Arabic or Chinese. We provide this evaluation results to demonstrate that Tweet2Vec overcomes this limitation and is able to cluster tweets in different languages. In particular, we obtained correct clusters of Russian and Arabic tweets.\nWe observed that leaving the urls does not significantly affect clustering performance, i.e., the model tolerates noise. However, replacement of the urls and user mentions with placeholders as in Dhingra et. al. (2016) generates syntactic patterns in text, such as @user @user @user, which causes semantically unrelated tweets appear within the same cluster."}, {"heading": "5 Discussion", "text": "Our experimental evaluation showed that the character-based embeddings produced with a neural network outperform the document-term baseline on the tweet clustering task. The baseline approach (TweetTerm) shows a very good performance in comparison with the simplicity of its implementation, but it naturally falls short in recognizing patterns beyond simple n-gram matching.\nWe attribute this result to the inherent limitation of the document-term model retaining only the frequent terms and disregarding the long tail of infrequent patterns. This limitation appears crucial in the task of emergent news detection, in which the topics need to be detected long before they become popular. Neural embeddings, in contrast, can retain a sufficient level of detail in their representa-\ntions and are able to mirror the fuzzy string matching performance beyond simple n-gram matching.\nIt becomes apparent from the sample clustering results (Tables 3 and 4) that both models perform essentially the same task of unveiling patterns shared between a group of strings. While TweetTerm operates only on the patterns of identical n-grams, Tweet2Vec goes beyond this limitation by providing room for a variation within the n-gram substring similar to fuzzy string matching. This effect allows to capture subtle variations in strings, e.g., misspellings, which word-based approaches are incapable of.\nOur error analysis also revealed the limitation of the neural embeddings to distinguish between semantic and syntactic similarity in strings (see Incorrect samples in Table 3). Tweet2Vec, as a recurrent neural network approach, represents not only the characters but also their order in string that may be a false similarity signal. It is evident that the neural representations in our example would benefit from the stop-word removal or an\nanalogous to TF/IDF weighting scheme to avoid capturing punctuation and other merely syntactic patterns.\nLimitations. Neural networks gain performance when more data is available. We could use only 88,148 tweets from the dataset to train the neural network, which can appear insufficient to unfold the potential of the model to recognize more complex patterns. Also, due to the scarce annotation available we could use only a small subset of the original dataset for our clustering evaluation. Since most of the SNOW tweets are in English, another dataset is needed for comprehensive multilingual clustering evaluation."}, {"heading": "6 Conclusion", "text": "We showed that character-based neural embeddings enable accurate tweet clustering with minimum supervision. They provide fine-grained representations that can help to uncover fuzzy similarities in strings beyond simple n-gram matching. We also demonstrated the limitation of the current\napproach unable to distinguish semantic from syntactic patterns in strings, which provides a clear direction for the future work."}, {"heading": "7 Acknowledgments", "text": "The presented work was supported by the InVID Project (http://www.invid-project.eu/), funded by the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 687786. Mihai Lupu was supported by SelfOptimizer (FFG 852624) in the EUROSTARS programme, funded by EUREKA, the BMWFW and the European Union, and ADMIRE (P25905N23) by FWF. We thank to Bhuwan Dhingra for the support in using Tweet2Vec and Linda Andersson for the review and helpful comments."}], "references": [{"title": "An extensive comparative study of cluster validity indices", "author": ["Ibai Gurrutxaga", "Javier Muguerza", "Jes\u00fas M. P\u00e9rez", "I\u00f1igo Perona"], "venue": "Pattern Recognition,", "citeRegEx": "Arbelaitz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arbelaitz et al\\.", "year": 2013}, {"title": "Adaptive Representations for Tracking Breaking News on Twitter", "author": ["Derek Greene", "Padraig Cunningham"], "venue": "In NewsKDD - Workshop on Data Science for News Publishing at The 20th ACM SIGKDD International", "citeRegEx": "Brigadir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brigadir et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Tweet2vec: Character-based distributed representations for social media", "author": ["Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William W. Cohen"], "venue": "In Proceedings of the 54th Annual Meeting of the Associ-", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Event Detection in Twitter using Aggressive Filtering and Hierarchical Tweet Clustering", "author": ["Bichen Shi", "Igor Brigadir"], "venue": null, "citeRegEx": "Ifrim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ifrim et al\\.", "year": 2014}, {"title": "Characteraware neural language models", "author": ["Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Binary codes capable of correcting deletions, insertions and reversals", "author": ["Vladimir I. Levenshtein"], "venue": "In Soviet physics doklady,", "citeRegEx": "Levenshtein.,? \\Q1966\\E", "shortCiteRegEx": "Levenshtein.", "year": 1966}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Enhancing First Story Detection using Word Embeddings", "author": ["Moran et al.2016] Sean Moran", "Richard McCreadie", "Craig Macdonald", "Iadh Ounis"], "venue": "In Proceedings of the 39th International ACM SIGIR conference on Research and Development", "citeRegEx": "Moran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moran et al\\.", "year": 2016}, {"title": "fastcluster: Fast hierarchical, agglomerative clustering routines for r and python", "author": ["Daniel M\u00fcllner"], "venue": "Journal of Statistical Software,", "citeRegEx": "M\u00fcllner.,? \\Q2013\\E", "shortCiteRegEx": "M\u00fcllner.", "year": 2013}, {"title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance", "author": ["Julien Epps", "James Bailey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "SNOW 2014 Data Challenge: Assessing the Performance of News Topic Detection Methods in Social Media", "author": ["David Corney", "Luca Maria Aiello"], "venue": null, "citeRegEx": "Papadopoulos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2014}, {"title": "Can twitter replace newswire for breaking news", "author": ["Miles Osborne", "Richard McCreadie", "Craig Macdonald", "Iadh Ounis", "Luke Shrimpton"], "venue": null, "citeRegEx": "Petrovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Petrovic et al\\.", "year": 2013}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["Rosenberg", "Hirschberg2007] Andrew Rosenberg", "Julia Hirschberg"], "venue": "EMNLP-CoNLL", "citeRegEx": "Rosenberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rosenberg et al\\.", "year": 2007}, {"title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis", "author": ["Peter J. Rousseeuw"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "Rousseeuw.,? \\Q1987\\E", "shortCiteRegEx": "Rousseeuw.", "year": 1987}, {"title": "The comparison of dendrograms by objective methods", "author": ["Sokal", "Rohlf1962] Robert R. Sokal", "F. James Rohlf"], "venue": "Taxon,", "citeRegEx": "Sokal et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Sokal et al\\.", "year": 1962}, {"title": "Dynamic multi-faceted topic discovery in twitter", "author": ["Vosecky et al.2013] Jan Vosecky", "Di Jiang", "Kenneth Wai-Ting Leung", "Wilfred Ng"], "venue": "In 22nd ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Vosecky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vosecky et al\\.", "year": 2013}, {"title": "Tweet2vec: Learning tweet embeddings using character-level CNN-LSTM encoder-decoder", "author": ["Prashanth Vijayaraghavan", "Deb Roy"], "venue": "In Proceedings of the 39th International ACM SIGIR conference", "citeRegEx": "Vosoughi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vosoughi et al\\.", "year": 2016}, {"title": "Tracking unbounded Topic Streams", "author": ["Victor Lavrenko", "Miles Osborne"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Wurzer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wurzer et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "Therefore, it is important to provide efficient tools to enable journalists rapidly detect breaking news in social media streams (Petrovic et al., 2013).", "startOffset": 129, "endOffset": 152}, {"referenceID": 21, "context": "Artificial neural networks (ANNs) allow to generate dense vector representation (embeddings), which can be efficiently generated on the word- as well as character levels (dos Santos and Zadrozny, 2014; Zhang et al., 2015; Dhingra et al., 2016).", "startOffset": 170, "endOffset": 243}, {"referenceID": 3, "context": "Artificial neural networks (ANNs) allow to generate dense vector representation (embeddings), which can be efficiently generated on the word- as well as character levels (dos Santos and Zadrozny, 2014; Zhang et al., 2015; Dhingra et al., 2016).", "startOffset": 170, "endOffset": 243}, {"referenceID": 6, "context": "There has been a continuous effort over the recent years to design effective and efficient algorithms capable of detecting newsworthy topics in the Twitter stream (Hayashi et al., 2015; Ifrim et al., 2014; Vosecky et al., 2013; Wurzer et al., 2015).", "startOffset": 163, "endOffset": 248}, {"referenceID": 18, "context": "There has been a continuous effort over the recent years to design effective and efficient algorithms capable of detecting newsworthy topics in the Twitter stream (Hayashi et al., 2015; Ifrim et al., 2014; Vosecky et al., 2013; Wurzer et al., 2015).", "startOffset": 163, "endOffset": 248}, {"referenceID": 20, "context": "There has been a continuous effort over the recent years to design effective and efficient algorithms capable of detecting newsworthy topics in the Twitter stream (Hayashi et al., 2015; Ifrim et al., 2014; Vosecky et al., 2013; Wurzer et al., 2015).", "startOffset": 163, "endOffset": 248}, {"referenceID": 20, "context": "ence topics (Wurzer et al., 2015) and 0.", "startOffset": 12, "endOffset": 33}, {"referenceID": 13, "context": "4 against the curated topic pool (Papadopoulos et al., 2014).", "startOffset": 33, "endOffset": 60}, {"referenceID": 9, "context": "Word2vec (Mikolov et al., 2013) is by far the most popular approach.", "startOffset": 9, "endOffset": 31}, {"referenceID": 7, "context": "They repeatedly outperformed the word-level baselines on the tasks of language modeling (Kim et al., 2016), part-ofspeech tagging (dos Santos and Zadrozny, 2014), and text classification (Zhang et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 21, "context": ", 2016), part-ofspeech tagging (dos Santos and Zadrozny, 2014), and text classification (Zhang et al., 2015).", "startOffset": 88, "endOffset": 108}, {"referenceID": 3, "context": "Our work extends the evaluation of the Tweet2Vec model (Dhingra et al., 2016) to the tweet clustering task, versus the traditional document-term matrix representation.", "startOffset": 55, "endOffset": 77}, {"referenceID": 13, "context": "We use the SNOW 2014 test dataset (Papadopoulos et al., 2014) in our evaluation.", "startOffset": 34, "endOffset": 61}, {"referenceID": 8, "context": "Fuzzy string matching uses the Levenstein (edit) distance (Levenshtein, 1966) between the two input strings as the measure of similarity.", "startOffset": 58, "endOffset": 77}, {"referenceID": 6, "context": "Our baseline is the tweet representation approach that was used in the winnersystem of SNOW 2014 Data Challenge4 (Ifrim et al., 2014).", "startOffset": 113, "endOffset": 133}, {"referenceID": 2, "context": "The network architecture includes two Gated Recurrent Units (GRUs) (Cho et al., 2014): forward and backward GRUs.", "startOffset": 67, "endOffset": 85}, {"referenceID": 11, "context": "To cluster tweet vectors (character-based tweet embeddings produced by the neural network for Tweet2Vec evaluation or the document-term matrix for TweetTerm) we employ the hierarchical clustering algorithm implementation from fastcluster library (M\u00fcllner, 2013).", "startOffset": 246, "endOffset": 261}, {"referenceID": 16, "context": "Silhouette coefficient (Rousseeuw, 1987).", "startOffset": 23, "endOffset": 40}, {"referenceID": 0, "context": "native methods in a comparative study of 30 validity indices (Arbelaitz et al., 2013).", "startOffset": 61, "endOffset": 85}, {"referenceID": 12, "context": "We evaluate the clustering results using the standard metrics for extrinsic clustering evaluation: homogeneity, completeness, V-Measure (Rosenberg and Hirschberg, 2007), Adjusted Rand Index (ARI) (Hubert and Arabie, 1985) and Adjusted Mutual Information (AMI) (Nguyen et al., 2010).", "startOffset": 260, "endOffset": 281}, {"referenceID": 13, "context": "The most common approach to label a cluster of tweets is to select a single tweet as a representative member for the whole cluster (Papadopoulos et al., 2014).", "startOffset": 131, "endOffset": 158}], "year": 2017, "abstractText": "In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks. The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content. Our evaluation results and code are available on-line1.", "creator": "LaTeX with hyperref package"}}}