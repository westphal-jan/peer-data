{"id": "1703.05465", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity Model", "abstract": "This galliford paper uexk\u00fcll describes extramural a emissive neural - 1.3-meter network kameel model olang which solvberg performed deronda competitively (top 6) morimura at the SemEval 2017 zorba cross - osburn lingual weichmann Semantic Textual godfried Similarity (STS) mulungushi task. ki\u1ec1u Our system maymi employs scw an attention - based icemark recurrent 24.64 neural anb network model chameleon that diterpene optimizes the sentence al-houthi similarity. In esam this paper, dougy we describe our blonsky participation astro-turf in the 0.303 multilingual relies STS task chanteys which 12-issue measures similarity 79.90 across English, paranagu\u00e1 Spanish, and babra Arabic.", "histories": [["v1", "Thu, 16 Mar 2017 03:15:22 GMT  (93kb,D)", "http://arxiv.org/abs/1703.05465v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenli zhuang", "ernie chang"], "accepted": false, "id": "1703.05465"}, "pdf": {"name": "1703.05465.pdf", "metadata": {"source": "CRF", "title": "Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity Model", "authors": ["WenLi Zhuang", "Ernie Chang"], "emails": ["bibo9901@gmail.com", "cyc025@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "Semantic textual similarity (STS) measures the degree of equivalence between the meanings of two text sequences (Agirre et al., 2016). The similarity of the text pair can be represented as discrete or continuous values ranging from irrelevance (1) to exact semantic equivalence (5). It is widely applicable to many NLP tasks including summarization (Wong et al., 2008; Nenkova et al., 2011), generation and question answering (Vo et al., 2015), paraphrase detection (Fernando and Stevenson, 2008), and machine translation (Corley and Mihalcea, 2005).\nIn this paper, we describe a system that is able to learn context-sensitive features within the sentences. Further, we encode the sequential information with Recurrent Neural Network (RNN) and perform attention mechanism (Bahdanau et al., 2015) on RNN outputs for both sentences. Attention mechanism was performed to increase sensitivity of the system to words of similarity significance. We also optimize directly on the Pearson correlation score as part of our neural-based approach. Moreover, we include a pair feature\n\u2217The author is currently serving in his Alternative Military Service of Education.\nadapter module that could be used to include more features to further improve performance. However, for this competition we include merely the TakeLab features (S\u030caric\u0301 et al., 2012). 1"}, {"heading": "2 Related Works", "text": "Most proposed approaches in the past adopted a hybrid of varying text unit sizes ranging from character-based, token-based, to knowledge-based similarity measure (Gomaa and Fahmy, 2013). The linguistic depths of these measures often vary between lexical, syntactic, and semantic levels.\nMost solutions include an ensemble of modules that employs features coming from different unit sizes and depths. More recent approaches generally include the word embedding-based similarity (Liebeck et al., 2016; Brychc\u0131\u0301n and Svoboda, 2016) as a component of the final ensemble. Top performing team in 2016 (Rychalska et al., 2016) uses an ensemble of multiple modules including recursive autoencoders with WordNet and monolingual aligner (Sultan et al., 2016). UMD-TTICUW (He et al., 2016) proposes the MPCNN model that requires no feature engineering and managed to perform competitively at 6th place. MPCNN is able to extract the hidden information using the Convolutional Neural Network (CNN) and added an attention layer to extract the vital similarity information."}, {"heading": "3 Methods", "text": ""}, {"heading": "3.1 Model", "text": "Given two sentences I1 = {w11, w12, ..., w1n1} and I2 = {w21, w22, ..., w2n2}, where wij denote the jth token of the ith sentence, embedded using a function \u03c6 that maps each token to a D-dimension trainable vector. Two sentences are encoded with\n1Our system and data is available at https://github.com/iamalbert/semval2017task1.\nar X\niv :1\n70 3.\n05 46\n5v 1\n[ cs\n.C L\n] 1\n6 M\nar 2\n01 7\nan attentitve RNN to obtain sentence embeddings u1, u2, respectively.\nSentence Encoder For each sentence, the RNN firstly converts wij into x i j \u2208 R2H , using an bidirectional Gated Recurrent Unit (GRU) (Cho et al., 2014) 2 by sequentially fed wij into the unit, forward and backward. The superscripts of w, x, a, u, n are omitted for clear notation.\nxi = [x F i ;x B i ]\nxFi = GRU(x F i\u22121, wi) xBi = GRU(x F i+1, wi)\n(1)\nThen, we attend each word xj for different salience aj and blend the moemories x1;n into sentence embedding u:\naj \u221d exp(rT tanh(Wxi))\nu = n\u2211 j=1 ajxj (2)\nSurface Features Inspired by the simple system described in (S\u030caric\u0301 et al., 2012), We also extract surface features from the sentence pair as following:\n\u2022Ngram Overlap Similarity: These are features drawn from external knowledge like WordNet (Miller, 1995) and Wikipedia. We use both PathLen similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin et al., 1998) to compute similarity between pairs of words w1i and w 2 j in I1 and I2, re-\nspectively. We employed the suggested preprocessing step (S\u030caric\u0301 et al., 2012), and added both WordNet and corpus-based information\n2 We also explored Longer Short-Term Memory (LSTM), but find it more overfitting than GRU.\nto ngram overlap scores, which was obtained with the harmonic mean of the degree of overlap between the sentences.\n\u2022Semantic Sentence Similarity: We also computed token-based alignment overlap and vector space sentence similarity (S\u030caric\u0301 et al., 2012). Semantic alignment similarity was computed greedily between all pairs of tokens using both the knowledge-based and corpus-based similarity. Scores are further enhanced with the aligned pair information. We obtained the weighted form of latent semantic analysis vectors (Turney and Pantel, 2010) for each word w, before computing the cosine similarity. As such, sentence similarity scores are enhanced with corpus-based information for tokens. The features are concatenated into a vector, denoted as m.\nScoring Let S be a descret random variable over {0, 1, 2, 3, 4, 5} describing the similarity of the given sentence pair {I1, I2}. The representation of the given pair is the concatenation of u1, u2, and m, which is fed into an MLP with one hidden layer to calculate the estimated distribution of S.\np =  P (S = 0) P (S = 1)\n... P (S = 5)\n = softmax(V tanh(U  u1u2 m )) (3)\nTherefore, the score y is the expected value of S:\ny = E[S] = 5\u2211\ni=0\niP (S = i) = vT p (4)\n, where v = [0, 1, 2, 3, 4, 5]T . The entire system is shown in Figure 1."}, {"heading": "3.2 Word Embedding", "text": "We explored initializing word embeddings randomly or with pre-trained word2vec (Mikolov et al., 2013) of dimension 50, 100, 300, respectively. We found that the system works the best with 300-dimension word2vec embeddings."}, {"heading": "3.3 Optimization", "text": "Let pn, yn be the predicted probability density and expected score and y\u0302n be the annotated gold score of the n-th sample. Most of the previous learningbased models are trained to minimize the following objectives on a batch of N samples:\n\u2022 Negative Log-likelihood (NLL) of p and p\u0302 (Aker et al., 2016). The task is viewed as a classification problem for 6 classes.\nLNLL = N\u2211 n=1 \u2212 log pntn\n, where tn is y\u0302n rounded to the nearest integer.\n\u2022 Mean square error (MSE) between yn and y\u0302n (Brychc\u0131\u0301n and Svoboda, 2016).\nLMSE = 1\nN N\u2211 n=1 (yn \u2212 y\u0302n)2\n\u2022 Kullback-Leibler divergence (KLD) of pn and gold distribution p\u0302n estimated by y\u0302n:\nLKLD = N\u2211 n=1\n( 6\u2211\ni=1\np\u0302ni log p\u0302ni pni\n)\nwhere\np\u0302ni =  y\u0302n \u2212 by\u0302nc, if i = by\u0302nc+ 1 by\u0302nc+ 1\u2212 y\u0302n, if i = by\u0302nc 0, otherwise\n(Li and Huang, 2016; Tai et al., 2015). For each n, there exists some k such that p\u0302nk = 1 and \u2200h 6= k, p\u0302nh = 0, KLD is identical to NLL.\nHowever, the evaluation metric of this task is Pearson Correlation Coefficient (PCC), which is invariant to changes in location and scale of yn but none of the above objectives can reflect it. Here we use an example to illustrate that MSE and KLD can even report an inverse tendency. In Table 1,\ngroup A has lower MSE and KLD loss than group B, but its PCC is also lower.\nTo solve this problem, we train the model to maximize PCC directly. Hence, the loss function is given by:\nLPCC = \u2212 \u2211N\nn=1(y n \u2212 y\u0304)(y\u0302n \u2212 \u00af\u0302y)\u221a\u2211N\nn=1(y n \u2212 y\u0304)2 \u221a\u2211N n=1(y\u0302\nn \u2212 \u00af\u0302y)2 (5)\nwhere y\u0304 = 1N \u2211N n=1 y n and \u00af\u0302y = 1N \u2211N n=1 y\u0302\nn. Since N is fixed for every batch, LPCC is differentiable with respect to yn, which means we can apply back propagation to train the network. To the best of our knowledge, we are the first team to adopt this training objective."}, {"heading": "4 Evaluation", "text": ""}, {"heading": "4.1 Data", "text": "We gathered dataset from SICK (Marelli et al., 2014) and past STS across years 2012, 2013, 2014, 2015, and 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016) for both cross-lingual and monolingual subtasks. We shuffling and splitting them according to the ratio 80:20 into training set and validation set, respectively. Table 2 indicates the size of training set and validation set. All nonEnglish sentence appearing in training, validation, and test set are translated into English with Google Cloud Translation API."}, {"heading": "4.2 Experiments", "text": "In the experiment, the size of output of GRU is set to be H = 200. We use ADAM algorithm to optimize the parameters with mini-batches of 125. The learning rate is set to 10\u22124 at the beginning and reduced by half for every 5 epochs. We trained the network for 15 epochs.\nWord embeddings In Table 3, we demonstrate that the system performs better with pretrained word vectors (WI) than randomly initialized (RI).\nLoss function We display performances with systems optimized with KLD, MSE, and PCC. It shows that when using LPCC as the training objective, our system not only performs the best but also converges the fastest. As shown in Table 4 and Figure 2."}, {"heading": "4.3 Final System Results", "text": "We tune the model on validation set, and select the set of hyper-parameters that yields the best performance to obtain the scores of test data. We report the official provisional results in Table 5. There is an obvious performance drop in track4b, which happens to all teams. We hypothesized that the sentences in track4b (en es) are collected from a special domain, due to the fact that the number of out-of-vocabulary words in track 4b is many times more than that in other tracks."}, {"heading": "5 Conclusion and Future Work", "text": "In conclusion, we propose a simple neural-based system with a novel means of optimization. We found that optimizing directly on PCC achieved the best scores, allowing the model to perform competitively on STS-2017. Moreover, we demonstrated that using randomly initialized word embedding does not harm the performance, but allowing it to achieve slightly higher scores against the pre-trained word embedding."}], "references": [{"title": "Semeval2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation", "author": ["Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceed-", "citeRegEx": "Agirre et al\\.,? 2016", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre."], "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Pro-", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "sem 2013 shared task: Semantic textual similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo."], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main", "citeRegEx": "Agirre et al\\.,? 2013", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "Usfd at semeval-2016 task 1: Putting different state-of-the-arts into a box", "author": ["Ahmet Aker", "Frederic Blain", "Andres Duque", "Marina Fomicheva", "Jurica Seva", "Kashif Shah", "Daniel Beck."], "venue": "Proceedings of the 10th International Workshop on Semantic", "citeRegEx": "Aker et al\\.,? 2016", "shortCiteRegEx": "Aker et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Uwb at semeval-2016 task 1: Semantic textual similarity using lexical, syntactic, and semantic information", "author": ["Tom\u00e1\u0161 Brychc\u0131\u0301n", "Luk\u00e1\u0161 Svoboda"], "venue": "In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-", "citeRegEx": "Brychc\u0131\u0301n and Svoboda.,? \\Q2016\\E", "shortCiteRegEx": "Brychc\u0131\u0301n and Svoboda.", "year": 2016}, {"title": "Measuring the semantic similarity of texts", "author": ["Courtney Corley", "Rada Mihalcea."], "venue": "Proceedings of the ACL workshop on empirical modeling of semantic equivalence and entailment. Association for Computational Linguistics, pages 13\u201318.", "citeRegEx": "Corley and Mihalcea.,? 2005", "shortCiteRegEx": "Corley and Mihalcea.", "year": 2005}, {"title": "A semantic similarity approach to paraphrase detection", "author": ["Samuel Fernando", "Mark Stevenson."], "venue": "Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics. Citeseer, pages 45\u201352.", "citeRegEx": "Fernando and Stevenson.,? 2008", "shortCiteRegEx": "Fernando and Stevenson.", "year": 2008}, {"title": "A survey of text similarity approaches", "author": ["Wael H Gomaa", "Aly A Fahmy."], "venue": "International Journal of Computer Applications 68(13).", "citeRegEx": "Gomaa and Fahmy.,? 2013", "shortCiteRegEx": "Gomaa and Fahmy.", "year": 2013}, {"title": "Umd-ttic-uw at semeval-2016 task 1: Attention-based multi-perspective convolutional neural networks for textual similarity measurement", "author": ["Hua He", "John Wieting", "Kevin Gimpel", "Jinfeng Rao", "Jimmy Lin."], "venue": "Proceedings of the 10th Interna-", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Combining local context and wordnet similarity for word sense identification", "author": ["Claudia Leacock", "Martin Chodorow."], "venue": "WordNet: An electronic lexical database 49(2):265\u2013283.", "citeRegEx": "Leacock and Chodorow.,? 1998", "shortCiteRegEx": "Leacock and Chodorow.", "year": 1998}, {"title": "Uta dlnlp at semeval2016 task 1: Semantic textual similarity: A unified framework for semantic processing and evaluation", "author": ["Peng Li", "Heng Huang."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Associa-", "citeRegEx": "Li and Huang.,? 2016", "shortCiteRegEx": "Li and Huang.", "year": 2016}, {"title": "Hhu at semeval-2016 task 1: Multiple approaches to measuring semantic textual similarity", "author": ["Matthias Liebeck", "Philipp Pollack", "Pashutan Modaresi", "Stefan Conrad."], "venue": "Proceedings of the 10th International Workshop on Semantic Evalu-", "citeRegEx": "Liebeck et al\\.,? 2016", "shortCiteRegEx": "Liebeck et al\\.", "year": 2016}, {"title": "An information-theoretic definition of similarity", "author": ["Dekang Lin"], "venue": "ICML. Citeseer, volume 98, pages 296\u2013304.", "citeRegEx": "Lin,? 1998", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri,", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM 38(11):39\u2013", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "2016. Samsung poland nlp team at semeval-2016 task 1: Necessity for diversity; combining recursive autoencoders, wordnet and ensem", "author": ["Barbara Rychalska", "Katarzyna Pakulska", "Krystyna Chodorowska", "Wojciech Walczak", "Piotr Andruszkiewicz"], "venue": null, "citeRegEx": "Rychalska et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rychalska et al\\.", "year": 2016}, {"title": "Takelab: Systems for measuring semantic text similarity", "author": ["Frane \u0160ari\u0107", "Goran Glava\u0161", "Mladen Karan", "Jan \u0160najder", "Bojana Dalbelo Ba\u0161i\u0107."], "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings", "citeRegEx": "\u0160ari\u0107 et al\\.,? 2012", "shortCiteRegEx": "\u0160ari\u0107 et al\\.", "year": 2012}, {"title": "Dls$@$cu at semeval-2016 task 1: Supervised models of sentence similarity", "author": ["Arafat Md Sultan", "Steven Bethard", "Tamara Sumner."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Associa-", "citeRegEx": "Sultan et al\\.,? 2016", "shortCiteRegEx": "Sultan et al\\.", "year": 2016}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of artificial intelligence research 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Fbk-hlt: An application of semantic textual similarity for answer selection in community question answering", "author": ["Ngoc Phuoc An Vo", "Simone Magnolini", "Octavian Popescu."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation, Se-", "citeRegEx": "Vo et al\\.,? 2015", "shortCiteRegEx": "Vo et al\\.", "year": 2015}, {"title": "Takelab: Systems for measuring semantic text similarity", "author": ["Frane \u0160ari\u0107", "Goran Glava\u0161", "Mladen Karan", "Jan \u0160najder", "Bojana Dalbelo Ba\u0161i\u0107."], "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Pro-", "citeRegEx": "\u0160ari\u0107 et al\\.,? 2012", "shortCiteRegEx": "\u0160ari\u0107 et al\\.", "year": 2012}, {"title": "Extractive summarization using supervised and semi-supervised learning", "author": ["Kam-Fai Wong", "Mingli Wu", "Wenjie Li."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1. Association for Computa-", "citeRegEx": "Wong et al\\.,? 2008", "shortCiteRegEx": "Wong et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Semantic textual similarity (STS) measures the degree of equivalence between the meanings of two text sequences (Agirre et al., 2016).", "startOffset": 112, "endOffset": 133}, {"referenceID": 24, "context": "It is widely applicable to many NLP tasks including summarization (Wong et al., 2008; Nenkova et al., 2011), generation and question answering (Vo et al.", "startOffset": 66, "endOffset": 107}, {"referenceID": 22, "context": ", 2011), generation and question answering (Vo et al., 2015), paraphrase detection (Fernando and Stevenson, 2008), and machine translation (Corley and Mihalcea, 2005).", "startOffset": 43, "endOffset": 60}, {"referenceID": 7, "context": ", 2015), paraphrase detection (Fernando and Stevenson, 2008), and machine translation (Corley and Mihalcea, 2005).", "startOffset": 30, "endOffset": 60}, {"referenceID": 6, "context": ", 2015), paraphrase detection (Fernando and Stevenson, 2008), and machine translation (Corley and Mihalcea, 2005).", "startOffset": 86, "endOffset": 113}, {"referenceID": 4, "context": "Further, we encode the sequential information with Recurrent Neural Network (RNN) and perform attention mechanism (Bahdanau et al., 2015) on RNN outputs for both sentences.", "startOffset": 114, "endOffset": 137}, {"referenceID": 18, "context": "However, for this competition we include merely the TakeLab features (\u0160ari\u0107 et al., 2012).", "startOffset": 69, "endOffset": 89}, {"referenceID": 8, "context": "Most proposed approaches in the past adopted a hybrid of varying text unit sizes ranging from character-based, token-based, to knowledge-based similarity measure (Gomaa and Fahmy, 2013).", "startOffset": 162, "endOffset": 185}, {"referenceID": 12, "context": "More recent approaches generally include the word embedding-based similarity (Liebeck et al., 2016; Brychc\u0131\u0301n and Svoboda, 2016) as a component of the final ensemble.", "startOffset": 77, "endOffset": 128}, {"referenceID": 5, "context": "More recent approaches generally include the word embedding-based similarity (Liebeck et al., 2016; Brychc\u0131\u0301n and Svoboda, 2016) as a component of the final ensemble.", "startOffset": 77, "endOffset": 128}, {"referenceID": 17, "context": "Top performing team in 2016 (Rychalska et al., 2016) uses an ensemble of multiple modules including recursive autoencoders with WordNet and monolingual aligner (Sultan et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 19, "context": ", 2016) uses an ensemble of multiple modules including recursive autoencoders with WordNet and monolingual aligner (Sultan et al., 2016).", "startOffset": 115, "endOffset": 136}, {"referenceID": 9, "context": "UMD-TTICUW (He et al., 2016) proposes the MPCNN model that requires no feature engineering and managed to perform competitively at 6th place.", "startOffset": 11, "endOffset": 28}, {"referenceID": 18, "context": "Surface Features Inspired by the simple system described in (\u0160ari\u0107 et al., 2012), We also extract surface features from the sentence pair as following:", "startOffset": 60, "endOffset": 80}, {"referenceID": 16, "context": "\u2022Ngram Overlap Similarity: These are features drawn from external knowledge like WordNet (Miller, 1995) and Wikipedia.", "startOffset": 89, "endOffset": 103}, {"referenceID": 10, "context": "We use both PathLen similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin et al.", "startOffset": 31, "endOffset": 59}, {"referenceID": 18, "context": "We employed the suggested preprocessing step (\u0160ari\u0107 et al., 2012), and added both WordNet and corpus-based information", "startOffset": 45, "endOffset": 65}, {"referenceID": 18, "context": "\u2022Semantic Sentence Similarity: We also computed token-based alignment overlap and vector space sentence similarity (\u0160ari\u0107 et al., 2012).", "startOffset": 115, "endOffset": 135}, {"referenceID": 21, "context": "We obtained the weighted form of latent semantic analysis vectors (Turney and Pantel, 2010) for each word w, before computing the cosine similarity.", "startOffset": 66, "endOffset": 91}, {"referenceID": 15, "context": "We explored initializing word embeddings randomly or with pre-trained word2vec (Mikolov et al., 2013) of dimension 50, 100, 300, respectively.", "startOffset": 79, "endOffset": 101}, {"referenceID": 3, "context": "\u2022 Negative Log-likelihood (NLL) of p and p\u0302 (Aker et al., 2016).", "startOffset": 44, "endOffset": 63}, {"referenceID": 5, "context": "\u2022 Mean square error (MSE) between yn and \u0177n (Brychc\u0131\u0301n and Svoboda, 2016).", "startOffset": 44, "endOffset": 73}, {"referenceID": 11, "context": "(Li and Huang, 2016; Tai et al., 2015).", "startOffset": 0, "endOffset": 38}, {"referenceID": 20, "context": "(Li and Huang, 2016; Tai et al., 2015).", "startOffset": 0, "endOffset": 38}, {"referenceID": 14, "context": "We gathered dataset from SICK (Marelli et al., 2014) and past STS across years 2012, 2013, 2014, 2015, and 2016 (Agirre et al.", "startOffset": 30, "endOffset": 52}], "year": 2017, "abstractText": "This paper describes a neural-network model which performed competitively (top 6) at the SemEval 2017 cross-lingual Semantic Textual Similarity (STS) task. Our system employs an attention-based recurrent neural network model that optimizes the sentence similarity. In this paper, we describe our participation in the multilingual STS task which measures similarity across English, Spanish, and Arabic.", "creator": "LaTeX with hyperref package"}}}