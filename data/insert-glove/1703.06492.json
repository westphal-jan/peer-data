{"id": "1703.06492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "VQABQ: Visual Question Answering by Basic Questions", "abstract": "Taking image and gasparini question tolerability as ratajczak the skvortsov input nagel of our sanderman method, chuasiriporn it oenb can output the text - 400-450 based lilya answer of the microhyla query pallant question about the computable given image, eprom so t22 called Visual 0.59 Question subcategory Answering (masondo VQA ). expressionist There are two main saint-esprit modules prefer in anoca our v-1710 algorithm. haveri Given wiki-based a natural willaert language rockcastle question inuzuka about an january image, 2353 the phytogeography first 232,000 module sheka takes heim the juking question as welders input and then tie-breaker outputs the 48-29 basic airbridge questions santissima of palpable the main question, given question. The second mostowfi module predecessors takes the sibilants main 1,696 question, image physiographic and these basic sanderford questions as input and then bullough outputs the balthazar text - based answer 727s of factory the main chlotrudis question. bereza We brundibar formulate the 1.6875 basic questions generation rudder problem acci\u00f3n as xobni a LASSO optimization genethia problem, and also propose voyageur a idiomatically criterion amambay about sweepback how gait\u00e1n to auskick exploit these kotlin basic questions to help answer boelcke main question. nailatikau Our method is emnes evaluated nayler on tweeters the b\u00e5tsfjord challenging inflamed VQA 24.37 dataset, and dresselhaus yields kynikos the competitive esben performance elland compared unforgiving to state - 1.029 of - the - art.", "histories": [["v1", "Sun, 19 Mar 2017 19:14:55 GMT  (1013kb,D)", "https://arxiv.org/abs/1703.06492v1", "Submitted ICCV 2017"], ["v2", "Mon, 28 Aug 2017 22:40:19 GMT  (1013kb,D)", "http://arxiv.org/abs/1703.06492v2", "Accepted by CVPR 2017 VQA Challenge Workshop. (Tables updated)"]], "COMMENTS": "Submitted ICCV 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["jia-hong huang", "modar alfadly", "bernard ghanem"], "accepted": false, "id": "1703.06492"}, "pdf": {"name": "1703.06492.pdf", "metadata": {"source": "CRF", "title": "VQABQ: Visual Question Answering by Basic Questions", "authors": ["Jia-Hong Huang", "Modar Alfadly", "Bernard Ghanem"], "emails": ["bernard.ghanem}@kaust.edu.sa"], "sections": [{"heading": "1. Introduction", "text": "Visual Question Answering (VQA) is a challenging and young research field, which can help machines achieve one of the ultimate goals in computer vision, holistic scene understanding [34]. VQA is a computer vision task: a system is given an arbitrary text-based question about an image, and then it should output the text-based answer of the given question about the image. The given question may contain many sub-problems in computer vision, e.g.,\n\u2022 Scene classification - Is it a rainy day?\n\u2022 Object recognition - What is on the desk?\n\u2022 Attribute classification - What color is the ground?\n\u2022 Counting - How many people are in the room?\n\u2022 Object detection - Are there any apples in the image?\n\u2022 Activity recognition - What kind of exercise is the man doing?\nBesides, in our real life there are a lot of more complicated questions that can be queried. So, in some sense, VQA can be considered as an important basic research problem in computer vision. From the above sub-problems in computer vision, we can discover that if we want to do holistic scene understanding in one step, it is probably too difficult. So, we try to divide the holistic scene understanding-task into many sub-tasks in computer vision. The task-dividing concept inspires us to do Visual Question Answering by Basic Questions (VQABQ), illustrated by Figure 1. That means, in VQA, we can divide the query question into some basic questions, and then exploit these basic questions to help us answer the main query question. Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6]. Regarding these works, we can consider most of them as visualattention VQA works because most of them do much effort on dealing with the image part but not the text part. However, recently there are some works [14, 12] that try to do more effort on the question part. In [12], authors proposed a Question Representation Update (QRU) mechanism to up-\n1\nar X\niv :1\n70 3.\n06 49\n2v 2\n[ cs\n.C V\n] 2\n8 A\nug 2\n01 7\ndate the original query question to increase the accuracy of the VQA algorithm. Typically, VQA is a strongly imagequestion dependent issue, so we should pay equal attention to both the image and question, not only one of them. In reality, when people have an image and a given question about the image, we usually notice the keywords of the question and then try to focus on some parts of the image related to question to give the answer. So, paying equal attention to both parts is a more reasonable way to do VQA. In [14], the authors proposed a Co-Attention mechanism, jointly utilizing information about visual and question attention, for VQA and achieved the state-of-the-art accuracy.\nThe Co-Attention mechanism inspires us to build part of our VQABQ model, illustrated by Figure 2. In the VQABQ model, there are two main modules, the basic question generation module (Module 1) and co-attention visual question answering module (Module 2). We take the query question, called the main question (MQ), encoded by Skip-Thought Vectors [11], as the input of Module 1. In the Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of VQA [1] dataset as a 4800 by 215623 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem, with MQ, to find the 3 BQ of MQ. These BQ are the output of Module 1. Moreover, we take the MQ, BQ and the given image as the input of Module 2, the VQA module with co-attention mechanism, and then it can output the final answer of MQ. We claim that the BQ can help Module 2 get the correct answer to increase the VQA accuracy. In this work, our main contributions are summarized below:\n\u2022 We propose a method to generate the basic questions of the main question and utilize these basic questions with proper criterion to help answer the main question in VQA.\n\u2022 Also, we propose a new basic question dataset generated by our basic question generation algorithm.\nThe rest of this paper is organized as the following. We first talk about the motivation about this work in Section 2. In Section 3, we review the related work, and then Section 4 shortly introduces the proposed VQABQ dataset. We discuss the detailed methodology in Section 5. Finally, the experimental results are demonstrated in Section 6."}, {"heading": "2. Motivations", "text": "The following two important reasons motivate us to do Visual Question Answering by Basic Questions (VQABQ). First, recently most of VQA works only emphasize more on the image part, the visual features, but put less effort on the question part, the text features. However, image and question features both are important for VQA. If we only\nfocus on one of them, we probably cannot get the good performance of VQA in the near future. Therefore, we should put our effort more on both of them at the same time. In [14], they proposed a novel co-attention mechanism that jointly performs image-guided question attention and question-guided image attention for VQA. [14] also proposed a hierarchical architecture to represent the question, and construct image-question co-attention maps at the word level, phrase level and question level. Then, these co-attended features are combined with word level, phrase level and question level recursively for predicting the final answer of the query question based on the input image. [12] is also a recent work focusing on the text-based question part, text feature. In [12], they presented a reasoning network to update the question representation iteratively after the question interacts with image content each time. Both of [14, 12] yield better performance than previous works by doing more effort on the question part.\nSecondly, in our life , when people try to solve a difficult problem, they usually try to divide this problem into some small basic problems which are usually easier than the original problem. So, why don\u2019t we apply this dividing concept to the input question of VQA ? If we can divide the input main question into some basic questions, then it will help the current VQA algorithm achieve higher probability to get the correct answer of the main question.\nThus, our goal in this paper is trying to generate the basic questions of the input question and then exploit these questions with the given image to help the VQA algorithm get the correct answer of the input question. Note that we can consider the generated basic questions as the extra useful information to VQA algorithm."}, {"heading": "3. Related Work", "text": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue. Our method involves in different areas in machine learning, natural language processing (NLP) and computer vision. The following, we discuss recent works related to our approach for solving VQA problem.\nSequence modeling by Recurrent Neural Networks. Recurrent Neural Networks (RNN) can handle the sequences of flexible length. Long Short Term Memory (LSTM) [7] is a particular variant of RNN and in natural language tasks, such as machine translation [27, 3], LSTM is a successful application. In [25], the authors exploit RNN and Convolutional Neural Network (CNN) to build a question generation algorithm, but the generated question sometimes has invalid grammar. The input in [18] is the concatenation of each word embedding with the same feature vector of image. [6] encodes the input question sentence by LSTM and join the image feature to the final output. [15] groups the neighbouring word and image features by doing convolution. In [21], the question is encoded by Gated Recurrent Unit (GRU) [4] similar to LSTM and the authors also introduce a dynamic parameter layer in CNN whose weights are adaptively predicted by the encoded question feature.\nSentence encoding. In order to analyze the relationship among words, phrases and sentences, several works, such as [23, 11, 20], proposed methods about how to map text into vector space. After we have the vector representation of text, we can exploit the vector analysis skill to analyze the relationship among text. [23, 20] try to map words to vector space, and if the words share common contexts in the corpus, their encoded vectors will close to each other in the vector space. In [11], the authors propose a framework of encoder-decoder models, called skip-thoughts. In this model, the authors exploit an RNN encoder with GRU activations [4] and an RNN decoder with a conditional GRU [4]. Because skipthoughts model emphasizes more on whole sentence encoding, in our work, we encode the whole question sentences into vector space by skip-thoughts model and use these skip-thought vectors to do further analysis of question sentences.\nImage captioning. In some sense, VQA is related to image captioning [32, 10, 28, 5]. [5] uses a language model to combine a set of possible words detected in several regions of the image and generate image description. In [28], the authors use CNN to extract the high-level image features and considered them as the first input of the recurrent network to generate the caption of image. [32] proposes an algorithm\nto generate one word at a time by paying attention to local image regions related to the currently predicted word. In [10], the deep neural network can learn to embed language and visual information into a common multi-modal space. However, the current image captioning algorithms only can generate the rough description of image and there is no so called proper metric to evaluate the quality of image caption , even though BLEU [22] can be used to evaluate the image caption.\nAttention-based VQA. There are several VQA models have ability to focus on specific image regions related to the input question by integrating the image attention mechanism [26, 2, 33, 12]. In [12], in the pooling step, the authors exploit an image attention mechanism to help determine the relevance between original questions and updated ones. Before [14], no work applied language attention mechanism to VQA, but the researchers in NLP they had modeled language attention. In [14], the authors propose a co-attention mechanism that jointly performs language attention and image attention. Because both question and image information are important in VQA, in our work we introduce co-attention mechanism into our VQABQ model."}, {"heading": "4. Basic Question Dataset", "text": "We propose a new dataset, called Basic Question Dataset (BQD), generated by our basic question generation algorithm. BQD is the first basic question dataset. Regarding the BQD, the dataset format is {Image, MQ, 3 (BQ + corresponding similarity score)}. All of our images are from the testing images of MS COCO dataset [13], the MQ, main questions, are from the testing questions of VQA, open-ended, dataset [1], the BQ, basic questions, are from the training and validation questions of VQA, open-ended, dataset [1], and the corresponding similarity score of BQ is generated by our basic question generation method, referring to Section 5. Moreover, we also take the multiplechoice questions in VQA dataset [1] to do the same thing as above. Note that we remove the repeated questions in the VQA dataset, so the total number of questions is slightly less than VQA dataset [1]. In BQD, we have 81434 images, 244302 MQ and 732906 (BQ + corresponding similarity score). At the same time, we also exploit BQD to do VQA and achieve the competitive accuracy compared to state-of-the-art."}, {"heading": "5. Methodology", "text": "In Section 5, we mainly discuss how to encode questions and generate BQ and why we exploit the Co-Attention Mechanism VQA algorithm [14] to answer the query question. The overall architecture of our VQABQ model can be\nreferred to Figure 2. The model has two main parts, Module 1 and Module 2. Regarding Module 1, it takes the encoded MQ as input and uses the matrix of the encoded BQ to output the BQ of query question. Then, the Module 2 is a VQA algorithm with the Co-Attention Mechanism [14], and it takes the output of Module 1, MQ, and the given image as input and then outputs the final answer of MQ. The detailed architecture of Module 1 can be referred to Figure 2."}, {"heading": "5.1. Question encoding", "text": "There are many popular text encoders, such as Word2Vec [20], GloVe [23] and Skip-Thoughts [11]. In these encoders, Skip-Thoughts not only can focus on the word-toword meaning but also the whole sentence semantic meaning. So, we choose Skip-Thoughts to be our question encoding method. In Skip-Thoughts model, it uses an RNN encoder with GRU [4] activations, and then we use this encoder to map an English sentence into a vector. Regarding GRU, it has been shown to perform as well as LSTM [7] on the sequence modeling applications but being conceptually simpler because GRU units only have 2 gates and do not need the use of a cell.\nQuestion encoder. Let w1i , ..., wNi be the words in question si and N is the total number of words in si. Note that wti denotes the t-th word for si and xti denotes its word embedding. The question encoder at each time step generates a hidden state hti. It can be considered as the representation of the sequence w1i , ..., w t i . So, the hidden state h N i can represent the whole question. For convenience, here we drop the index i and iterate the following sequential equations to encode a question:\nrt = \u03c3(Urh t\u22121 + Wrx t) (1)\nzt = \u03c3(Uzh t\u22121 + Wzx t) (2)\nh\u0304t = tanh(U(rt ht\u22121) + Wxt) (3)\nht = zt h\u0304t + (1\u2212 zt) ht\u22121 (4)\n, where Ur, Uz , Wr, Wz , U and W are the matrices of weight parameters. h\u0304t is the state update at time step t, rt is the reset gate, denotes an element-wise product and zt is the update gate. These two update gates take the values between zero and one."}, {"heading": "5.2. Problem Formulation", "text": "Our idea is the BQ generation for MQ and, at the same time, we only want the minimum number of BQ to represent the MQ, so modeling our problem as LASSO optimization problem is an appropriate way:\nmin x\n1 2 \u2016Ax\u2212 b\u201622 + \u03bb \u2016x\u20161 (5)\n, where A is the matrix of encoded BQ, b is the encode MQ and \u03bb is a parameter of the regularization term."}, {"heading": "5.3. Basic Question Generation", "text": "We now describe how to generate the BQ of a query question, illustrated by Figure 2. Note that the following we only describe the open-ended question case because the multiple-choice case is same as open-ended one. According to Section 5.2, we can encode the all questions from the training and validation questions of VQA dataset [1] by Skip-Thought Vectors, and then we have the matrix of these encoded basic questions. Each column of the matrix is the vector representation, 4800 by 1 dimensions, of a basic question and we have 215623 columns. That is, the dimension of BQ matrix, called A, is 4800 by 215623. Also, we encode the query question as a column vector, 4800 by 1 dimensions, by Skip-Thought Vectors, called b. Now, we can solve the LASSO optimization problem, mentioned in Section 5.3, to get the solution, x. Here, we consider the elements, in solution vector x, as the weights of the corresponding BQ in BQ matrix, A. The first element of x corresponds to the first column, i.e. the first BQ, of A. Then, we rank the all weights in x and pick up the top 3 large weights with corresponding BQ to be the BQ of the query question. Intuitively, because BQ are important to MQ, the weights of BQ also can be considered as importance scores and the BQ with larger weight means more important to MQ. Finally, we find the BQ of all 142093 testing questions from VQA dataset and collect them together, with the format {Image, MQ, 3 (BQ + corresponding similarity score)}, as the BQD in Section 4."}, {"heading": "5.4. Basic Question Concatenation", "text": "In this section, we propose a criterion to use these BQ. In BQD, each MQ has three corresponding BQ with scores. We can have the following format, {MQ, (BQ1, score1), (BQ2, score2), (BQ3, score3)}, and these scores are all between 0 and 1 with the following order,\nscore1 \u2265 score2 \u2265 score3 (6)\nand we define 3 thresholds, s1, s2 and s3. Also, we compute the following 3 averages (avg) and 3 standard deviations (std) to score1, score2/score1 and score3/score2, respectively, and then use avg\u00b1std, referring to Table 3, to be the initial guess of proper thresholds. The BQ utilization process can be explained as Table 1. The detailed discussion about BQ concatenation algorithm is described in the Section 6.4."}, {"heading": "5.5. Co-Attention Mechanism", "text": "There are two types of Co-Attention Mechanism [14] , Parallel and Alternating. In our VQABQ model, we only use the VQA algorithm with Alternating Co-Attention Mechanism to be our VQA module, referring to Figure 2, because, in [14], Alternating Co-Attention Mechanism VQA module can get the higher accuracy than the Parallel one. Moreover, we want to compare with the VQA method, Alternating one, with higher accuracy in [14]. In Alternating Co-Attention Mechanism, it sequentially alternates between generating question and image attention. That is, this mechanism consists of three main steps:\n\u2022 First, the input question is summarized into a single vector q.\n\u2022 Second, attend to the given image depended on q.\n\u2022 Third, attend to the question depended on the attended image feature.\nWe can define x\u0302 is an attention operator, which is a function of X and g. This operator takes the question (or image) feature X and attention guider g derived from image (or question) as inputs, and then outputs the attended question (or image) vector. We can explain the above operation as the following steps:\nH = tanh(WxX + (Wgg)1 T) (7)\nax = softmax(wThxH) (8) x\u0302 = \u2211\naxi xi (9)\n, where ax is the attention weight of feature X, 1 is a vector whose elements are all equal to 1, and Wg , Wx and whx are matrices of parameters.\nConcretely, at the first step of Alternating Co-Attention Mechanism, g is 0 and X = Q. Then, at the second step,\nX = V where V is the image features and the guider, g, is intermediate attended question feature, s\u0302, which is from the first step. At the final step, it uses the attended image feature, v\u0302, as the guider to attend the question again. That is, X = Q and g = v\u0302."}, {"heading": "6. Experiment", "text": "In Section 6, we describe the details of our implementation and discuss the experiment results about the proposed method."}, {"heading": "6.1. Datasets", "text": "We conduct our experiments on VQA [1] dataset. VQA dataset is based on the MS COCO dataset [13] and it contains the largest number of questions. There are questions, 248349 for training, 121512 for validation and 244302 for testing. In the VQA dataset, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT). About 98% of answers do not exceed 3 words and 90% of answers have single words. Note that we only test our method on the open-ended case in VQA dataset because it has the most open-ended questions among the all available dataset and we also think openended task is closer to the real situation than multiple-choice one."}, {"heading": "6.2. Setup", "text": "In order to prove our claim that BQ can help accuracy and compare with the state-of-the-art VQA method [14], so, in our Module 2, we use the same setting, dataset and source code mentioned in [14]. Then, the Module 1 in VQABQ model, is our basic question generation module. In other words, in our model ,the only difference compared to [14] is our Module 1, illustrated by Figure 2."}, {"heading": "6.3. Evaluation Metrics", "text": "VQA dataset provides multiple-choice and open-ended task for evaluation. Regarding open-ended task, the answer can be any phrase or word. However, in multiple-choice\ntask, an answer should be chosen from 18 candidate answers. For both cases, answers are evaluated by accuracy which can reflect human consensus. The accuracy is given by the following:\nAccuracy V QA\n= 1\nN N\u2211 i=1 min {\u2211 t\u2208Ti I[ai = t] 3 , 1 } (10)\n, where N is the total number of examples, I[\u00b7] denotes an indicator function, ai is the predicted answer and Ti is an answer set of the ith example. That is, a predicted answer is considered as a correct one if at least 3 annotators agree with it, and the score depends on the total number of agreements when the predicted answer is not correct."}, {"heading": "6.4. Results and Analysis", "text": "Here, we describe our final results and analysis by the following parts:\nDoes Basic Question Help Accuracy ?\nThe answer is yes. Here we only discuss the open-ended case. In our experiment, we use the avg \u00b1 std, referring to Table 3, to be the initial guess of proper thresholds of s1, s2 and s3, in Table 1. We discover that when s1 = 0.43, s2 = 0.82 and s3 = 0.53, we can get the better utilization of BQ. The threshold, s1 = 0.43, can be consider as 43% of testing questions from VQA dataset which cannot find the basic question, from the training and validation sets of VQA dataset, and only 57% of testing questions can find the basic questions. Note that we combine the training and validation sets of VQA dataset to be our basic question dataset. Regarding s2 = 0.82, that means 82% of those 57% testing questions, i.e. 46.74%, only can find 1 basic question, and 18% of those 57% testing questions, i.e. 10.26%, can find at least 2 basic questions. Furthermore, s3 = 0.53 means that 53% of those 10.26% testing question, i.e. around 5.44%, only can find 2 basic questions, and 47% of those 10.26% testing question, i.e. around 4.82%, can find 3 basic questions. The above detail can be referred to Table 2.\nAccordingly to the Table 2, 43% of testing questions from VQA dataset cannot find the proper basic questions from VQA training and validation datasets, and there are some failed examples about this case in Table 6. We also discover that a lot of questions in VQA training and validation datasets are almost the same. This issue reduces the diversity of basic question dataset. Although we only have 57% of testing questions can benefit from the basic questions, our method still can improve the state-of-theart accuracy [14] from 60.32% to 60.34%, referring to Table 4 and 5. Then, we have 142093 testing questions, so that means the number of correctly answering questions of our method is more than state-of-the-art method 28 questions. In other words, if we have well enough basic question dataset, we can increase accuracy more, especially in the counting-type question, referring to Table 4 and 5. Because the Co-Attention Mechanism is good at localizing, the counting-type question is improved more than others. So, based on our experiment, we can conclude that basic question can help accuracy obviously.\nComparison with State-of-the-art. Recently, [14] proposed the Co-Attention Mechanism in VQA and got the state-of-the-art accuracy. However, when we use their code and the same setup mentioned in their paper to re-run the experiment, we cannot get the same accuracy reported in their work. The re-run results are presented in Table 5. So, under the fair conditions, our method is competitive compared to the state-of-the-art."}, {"heading": "7. Conclusion and Future Work", "text": "In this paper, we propose a VQABQ model for visual question answering. The VQABQ model has two main modules, Basic Question Generation Module and Co-\nAttention VQA Module. The former one can generate the basic questions for the query question, and the latter one can take the image , basic and query question as input and then output the text-based answer of the query question. According to the Section 6.4, because the basic question dataset generated from VQA dataset is not well enough, we only have the 57% of all testing questions can benefit from the basic questions. However, we still can increase 28 correctly answering questions compared to the state-of-the-art. We believe that if our basic question dataset is well enough, the increment of accuracy will be much more.\nAccording to the previous state-of-the-art methods in VQA, they all got the highest accuracy in the Yes/No-type question. So, how to effectively only exploit the Yes/Notype basic questions to do VQA will be an interesting work, illustrated by Figure 3. Also, how to generate other specific type of basic questions based on the query question and how to do better combination of visual and textual features in order to decrease the semantic inconsistency? The above future works will be our next research focus."}, {"heading": "Acknowledgements", "text": "This work is supported by competitive research funding from King Abdullah University of Science and Technology (KAUST). Also, we would like to acknowledge Fabian Caba, Humam Alwassel and Adel Bibi. They always can provide us helpful discussion about this work."}], "references": [{"title": "Vqa: Visual question  answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425\u20132433,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["K. Chen", "J. Wang", "L.-C. Chen", "H. Gao", "W. Xu", "R. Nevatia"], "venue": "arXiv preprint arXiv:1511.05960,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "Advances in Neural Information Processing Systems, pages 2296\u20132304,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "A focused dynamic attention model for visual question answering", "author": ["I. Ilievski", "S. Yan", "J. Feng"], "venue": "arXiv preprint arXiv:1604.01485,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Answer-type prediction for visual question answering", "author": ["K. Kafle", "C. Kanan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4976\u2013 4984,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings  Figure 3. Some future work examples. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "NIPS, pages 3294\u20133302,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual question answering with question rep-  resentation update (qru)", "author": ["R. Li", "J. Jia"], "venue": "NIPS, pages 4655\u20134663,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, pages 740\u2013755. Springer,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS, pages 289\u2013297,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["L. Ma", "Z. Lu", "H. Li"], "venue": "arXiv preprint arXiv:1506.00333,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["L. Ma", "Z. Lu", "H. Li"], "venue": "AAAI, page 16,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Advances in Neural Information Processing Systems, pages 1682\u20131690,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1\u20139,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask your neurons: A deep learning approach to visual question answering", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, pages 3111\u20133119,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P. Hongsuck Seo", "B. Han"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 30\u2013 38,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "Advances in Neural Information Processing Systems, pages 2953\u20132961,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "Advances in Neural Information Processing Systems, pages 2953\u20132961,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4613\u20134621,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, pages 3104\u20133112,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask me anything: Free-form visual question answering  based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. Dick", "A. van den Hengel"], "venue": "In CVPR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "arXiv, 1603,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "ECCV, pages 451\u2013466. Springer,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML, volume 14, pages 77\u201381,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 21\u201329,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation", "author": ["J. Yao", "S. Fidler", "R. Urtasun"], "venue": "CVPR 2012, pages 702\u2013709. IEEE,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4995\u20135004,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Our method is evaluated on the challenging VQA dataset [1] and yields state-of-the-art accuracy, 60.", "startOffset": 55, "endOffset": 58}, {"referenceID": 33, "context": "Visual Question Answering (VQA) is a challenging and young research field, which can help machines achieve one of the ultimate goals in computer vision, holistic scene understanding [34].", "startOffset": 182, "endOffset": 186}, {"referenceID": 16, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 0, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 17, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 23, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 15, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 5, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 13, "context": "However, recently there are some works [14, 12] that try to do more effort on the question part.", "startOffset": 39, "endOffset": 47}, {"referenceID": 11, "context": "However, recently there are some works [14, 12] that try to do more effort on the question part.", "startOffset": 39, "endOffset": 47}, {"referenceID": 11, "context": "In [12], authors proposed a Question Representation Update (QRU) mechanism to up-", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [14], the authors proposed a Co-Attention mechanism, jointly utilizing information about visual and question attention, for VQA and achieved the state-of-the-art accuracy.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "We take the query question, called the main question (MQ), encoded by Skip-Thought Vectors [11], as the input of Module 1.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "In the Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of VQA [1] dataset as a 4800 by 215623 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem, with MQ, to find the 3 BQ of MQ.", "startOffset": 124, "endOffset": 127}, {"referenceID": 13, "context": "In [14], they proposed a novel co-attention mechanism that jointly performs image-guided question attention and question-guided image attention for VQA.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "[14] also proposed a hierarchical architecture to represent the question, and construct image-question co-attention maps at the word level, phrase level and question level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] is also a recent work focusing on the text-based question part, text feature.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In [12], they presented a reasoning network to update the question representation iteratively after the question interacts with image content each time.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Both of [14, 12] yield better performance than previous works by doing more effort on the question part.", "startOffset": 8, "endOffset": 16}, {"referenceID": 11, "context": "Both of [14, 12] yield better performance than previous works by doing more effort on the question part.", "startOffset": 8, "endOffset": 16}, {"referenceID": 0, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 25, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 1, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 8, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 14, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 24, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 35, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 28, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 6, "context": "Long Short Term Memory (LSTM) [7] is a particular variant of RNN and in natural language tasks, such as machine translation [27, 3], LSTM is a successful application.", "startOffset": 30, "endOffset": 33}, {"referenceID": 26, "context": "Long Short Term Memory (LSTM) [7] is a particular variant of RNN and in natural language tasks, such as machine translation [27, 3], LSTM is a successful application.", "startOffset": 124, "endOffset": 131}, {"referenceID": 2, "context": "Long Short Term Memory (LSTM) [7] is a particular variant of RNN and in natural language tasks, such as machine translation [27, 3], LSTM is a successful application.", "startOffset": 124, "endOffset": 131}, {"referenceID": 24, "context": "In [25], the authors exploit RNN and Convolutional Neural Network (CNN) to build a question generation algorithm, but the generated question sometimes has invalid grammar.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "The input in [18] is the concatenation of each word embedding with the same feature vector of image.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "[6] encodes the input question sentence by LSTM and join the image feature to the final output.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] groups the neighbouring word and image features by doing convolution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In [21], the question is encoded by Gated Recurrent Unit (GRU) [4] similar to LSTM and the authors also introduce a dynamic parameter layer in CNN whose weights are adaptively predicted by the encoded question feature.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In [21], the question is encoded by Gated Recurrent Unit (GRU) [4] similar to LSTM and the authors also introduce a dynamic parameter layer in CNN whose weights are adaptively predicted by the encoded question feature.", "startOffset": 63, "endOffset": 66}, {"referenceID": 22, "context": "In order to analyze the relationship among words, phrases and sentences, several works, such as [23, 11, 20], proposed methods about how to map text into vector space.", "startOffset": 96, "endOffset": 108}, {"referenceID": 10, "context": "In order to analyze the relationship among words, phrases and sentences, several works, such as [23, 11, 20], proposed methods about how to map text into vector space.", "startOffset": 96, "endOffset": 108}, {"referenceID": 19, "context": "In order to analyze the relationship among words, phrases and sentences, several works, such as [23, 11, 20], proposed methods about how to map text into vector space.", "startOffset": 96, "endOffset": 108}, {"referenceID": 22, "context": "[23, 20] try to map words to vector space, and if the words share common contexts in the corpus, their encoded vectors will close to each other in the vector space.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[23, 20] try to map words to vector space, and if the words share common contexts in the corpus, their encoded vectors will close to each other in the vector space.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "In [11], the authors propose a framework of encoder-decoder models, called skip-thoughts.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In this model, the authors exploit an RNN encoder with GRU activations [4] and an RNN decoder with a conditional GRU [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "In this model, the authors exploit an RNN encoder with GRU activations [4] and an RNN decoder with a conditional GRU [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 31, "context": "In some sense, VQA is related to image captioning [32, 10, 28, 5].", "startOffset": 50, "endOffset": 65}, {"referenceID": 9, "context": "In some sense, VQA is related to image captioning [32, 10, 28, 5].", "startOffset": 50, "endOffset": 65}, {"referenceID": 27, "context": "In some sense, VQA is related to image captioning [32, 10, 28, 5].", "startOffset": 50, "endOffset": 65}, {"referenceID": 4, "context": "In some sense, VQA is related to image captioning [32, 10, 28, 5].", "startOffset": 50, "endOffset": 65}, {"referenceID": 4, "context": "[5] uses a language model to combine a set of possible words detected in several regions of the image and generate image description.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "In [28], the authors use CNN to extract the high-level image features and considered them as the first input of the recurrent network to generate the caption of image.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "[32] proposes an algorithm to generate one word at a time by paying attention to local image regions related to the currently predicted word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "In [10], the deep neural network can learn to embed language and visual information into a common multi-modal space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "However, the current image captioning algorithms only can generate the rough description of image and there is no so called proper metric to evaluate the quality of image caption , even though BLEU [22] can be used to evaluate the image caption.", "startOffset": 198, "endOffset": 202}, {"referenceID": 25, "context": "There are several VQA models have ability to focus on specific image regions related to the input question by integrating the image attention mechanism [26, 2, 33, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 1, "context": "There are several VQA models have ability to focus on specific image regions related to the input question by integrating the image attention mechanism [26, 2, 33, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 32, "context": "There are several VQA models have ability to focus on specific image regions related to the input question by integrating the image attention mechanism [26, 2, 33, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 11, "context": "There are several VQA models have ability to focus on specific image regions related to the input question by integrating the image attention mechanism [26, 2, 33, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 11, "context": "In [12], in the pooling step, the authors exploit an image attention mechanism to help determine the relevance between original questions and updated ones.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Before [14], no work applied language attention mechanism to VQA, but the researchers in NLP they had modeled language attention.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "In [14], the authors propose a co-attention mechanism that jointly performs language attention and image attention.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "All of our images are from the testing images of MS COCO dataset [13], the MQ, main questions, are from the testing questions of VQA, open-ended, dataset [1], the BQ, basic questions, are from the training and validation questions of VQA, open-ended, dataset [1], and the corresponding similarity score of BQ is generated by our basic question generation method, referring to Section 5.", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "All of our images are from the testing images of MS COCO dataset [13], the MQ, main questions, are from the testing questions of VQA, open-ended, dataset [1], the BQ, basic questions, are from the training and validation questions of VQA, open-ended, dataset [1], and the corresponding similarity score of BQ is generated by our basic question generation method, referring to Section 5.", "startOffset": 154, "endOffset": 157}, {"referenceID": 0, "context": "All of our images are from the testing images of MS COCO dataset [13], the MQ, main questions, are from the testing questions of VQA, open-ended, dataset [1], the BQ, basic questions, are from the training and validation questions of VQA, open-ended, dataset [1], and the corresponding similarity score of BQ is generated by our basic question generation method, referring to Section 5.", "startOffset": 259, "endOffset": 262}, {"referenceID": 0, "context": "Moreover, we also take the multiplechoice questions in VQA dataset [1] to do the same thing as above.", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "Note that we remove the repeated questions in the VQA dataset, so the total number of questions is slightly less than VQA dataset [1].", "startOffset": 130, "endOffset": 133}, {"referenceID": 13, "context": "In Section 5, we mainly discuss how to encode questions and generate BQ and why we exploit the Co-Attention Mechanism VQA algorithm [14] to answer the query question.", "startOffset": 132, "endOffset": 136}, {"referenceID": 13, "context": "Then, the Module 2 is a VQA algorithm with the Co-Attention Mechanism [14], and it takes the output of Module 1, MQ, and the given image as input and then outputs the final answer of MQ.", "startOffset": 70, "endOffset": 74}, {"referenceID": 19, "context": "There are many popular text encoders, such as Word2Vec [20], GloVe [23] and Skip-Thoughts [11].", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "There are many popular text encoders, such as Word2Vec [20], GloVe [23] and Skip-Thoughts [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "There are many popular text encoders, such as Word2Vec [20], GloVe [23] and Skip-Thoughts [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "In Skip-Thoughts model, it uses an RNN encoder with GRU [4] activations, and then we use this encoder to map an English sentence into a vector.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "Regarding GRU, it has been shown to perform as well as LSTM [7] on the sequence modeling applications but being conceptually simpler because GRU units only have 2 gates and do not need the use of a cell.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "2, we can encode the all questions from the training and validation questions of VQA dataset [1] by Skip-Thought Vectors, and then we have the matrix of these encoded basic questions.", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": "There are two types of Co-Attention Mechanism [14] , Parallel and Alternating.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "In our VQABQ model, we only use the VQA algorithm with Alternating Co-Attention Mechanism to be our VQA module, referring to Figure 2, because, in [14], Alternating Co-Attention Mechanism VQA module can get the higher accuracy than the Parallel one.", "startOffset": 147, "endOffset": 151}, {"referenceID": 13, "context": "Moreover, we want to compare with the VQA method, Alternating one, with higher accuracy in [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "We only show the open-ended case of VQA dataset [1], and \u201d# Q\u201d denoted number of questions.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "We conduct our experiments on VQA [1] dataset.", "startOffset": 34, "endOffset": 37}, {"referenceID": 12, "context": "VQA dataset is based on the MS COCO dataset [13] and it contains the largest number of questions.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "In order to prove our claim that BQ can help accuracy and compare with the state-of-the-art VQA method [14], so, in our Module 2, we use the same setting, dataset and source code mentioned in [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "In order to prove our claim that BQ can help accuracy and compare with the state-of-the-art VQA method [14], so, in our Module 2, we use the same setting, dataset and source code mentioned in [14].", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "In other words, in our model ,the only difference compared to [14] is our Module 1, illustrated by Figure 2.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "LSTM Q+I [1] 36.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "2 BOWIMG [1] 33.", "startOffset": 9, "endOffset": 12}, {"referenceID": 34, "context": "6 iBOWIMG [35] 35.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "9 DPPnet [21] 37.", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "4 FDA [8] 36.", "startOffset": 6, "endOffset": 9}, {"referenceID": 32, "context": "5 SAN [33] 36.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "9 SMem [31] 37.", "startOffset": 7, "endOffset": 11}, {"referenceID": 29, "context": "2 DMN+ [30] 36.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "4 Refined-Neurons [19] 36.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "4 QRU [12] 37.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "8 CoAtt+VGG [14] 38.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "5 CoAtt+ResNet [14] 38.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "Evaluation results on VQA dataset [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "LSTM Q+I [8] 36.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": "18 CoAtt+VGG [14] 38.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "Re-run evaluation results on VQA dataset [1].", "startOffset": 41, "endOffset": 44}, {"referenceID": 13, "context": "Note that the result of [14] in Table 5 is lower than in Table 4, and CoAtt+VGG is same as our VGGNet.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "Although we only have 57% of testing questions can benefit from the basic questions, our method still can improve the state-of-theart accuracy [14] from 60.", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "Recently, [14] proposed the Co-Attention Mechanism in VQA and got the state-of-the-art accuracy.", "startOffset": 10, "endOffset": 14}], "year": 2017, "abstractText": "Taking an image and question as the input of our method, it can output the text-based answer of the query question about the given image, so called Visual Question Answering (VQA). There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the basic questions of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question. We formulate the basic questions generation problem as a LASSO optimization problem, and also propose a criterion about how to exploit these basic questions to help answer main question. Our method is evaluated on the challenging VQA dataset [1] and yields state-of-the-art accuracy, 60.34% in open-ended task.", "creator": "LaTeX with hyperref package"}}}