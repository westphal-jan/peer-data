{"id": "1703.06939", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Distributed Constraint Problems for Utilitarian Agents with Privacy Concerns, Recast as POMDPs", "abstract": "Privacy novoseltsev has traditionally been a major fis motivation for mandie distributed galleani problem dongfanghong solving. wussler Distributed khojali Constraint Satisfaction Problem (DisCSP) as bruy\u00e8res well as Distributed Constraint Optimization 207.6 Problem (DCOP) are fundamental firths models matar\u00f3 used caravelli to atma solve verducci various families of distributed problems. tarabin Even bardet though noller several oo approaches suv have chaikin been proposed to quantify and preserve idlewild privacy westrick in such pahoehoe problems, none 11,000-acre of them is exempt mainframes from hodgetts limitations. underplaying Here stalwart we bellflower approach the khristenko problem fuzi by esses assuming qilin that computation translocations is harajuku performed among atlantik utilitarian pinewood agents. 3-11 We shackleford introduce dittmar a monograph utilitarian approach where the utility of five-years each state is laggard estimated codify as the difference between minyanim the reward navarrette for incomers reaching junor an agreement materialized on bujold assignments of shared variables mazzaferro and koetter the schmuhl cost of chows privacy loss. bryotropha We caupolic\u00e1n investigate ngwa extensions 38.42 to bunka solvers salifou where 85.81 agents taupe integrate singhs the listin utility puddings function horsedrawn to 195.5 guide their search kristalina and decide which 14:34.56 action to crassula perform, constition defining telicka thereby impac their policy. swigging We 1.3927 show yusupova that these extended mckeague solvers progresses succeed in significantly reducing privacy loss without significant 109.73 degradation of ternes the d'alemberte solution quality.", "histories": [["v1", "Mon, 20 Mar 2017 19:32:40 GMT  (38kb)", "http://arxiv.org/abs/1703.06939v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["julien savaux", "julien vion", "sylvain piechowiak", "ren\\'e mandiau", "toshihiro matsui", "katsutoshi hirayama", "makoto yokoo", "shakre elmane", "marius silaghi"], "accepted": false, "id": "1703.06939"}, "pdf": {"name": "1703.06939.pdf", "metadata": {"source": "CRF", "title": "Distributed Constraint Problems for Utilitarian Agents with Privacy Concerns, Recast as POMDPs", "authors": ["Julien Savaux", "Julien Vion", "Sylvain Piechowiak", "Ren\u00e9 Mandiau"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n06 93\n9v 1\n[ cs\n.A I]\n2 0\nM ar\n2 01\nI. INTRODUCTION\nPrivacy is an important problem in a lot of distributed applications, therefore, the reward for solving the problem should be considered, but also the cost of privacy loss during the process [22]. For example, when users exchange information on social networks [32], they reveal (often unconsciously) personal data (e.g., age, address, date of birth). Related studies were also performed in the domain of Ambient Intelligence which is concerned with the distributed management of confidential information between different components (e.g., camera, computer, PDA), to allow or prevent the sharing of resource information [47].\nIn distributed scheduling problems, confidentiality also happen when information is exchanged between agents/participants (for example, a participant does not want to reveal his/her unavailability for a time slot because the explanation concerns his/her private life, and it should not be necessarily discussed publicly). Indeed, we know that the assignment of time slots can be difficult if participants do not want to reveal their constraints [18], [10]. Such coordinated decisions are in conflict with the need to keep constraints private [16]. We consider that these coordinated decisions may be defined by a set of different constraints distributed among different agents.\nIn Distributed Constraint Satisfaction Problems (DisCSPs) and Distributed Constraint Optimization Problems (DCOPs), agents have to assign values to variables while respecting given constraints. To find such assignments, agents exchange messages until a solution is found or until an agent detects that there is none. Thus, agents have to reveal information during search, causing privacy to be a major concern in DisCSPs [62], [26]. If an agent is concerned about its privacy, then it can associate a cost to the revelation of each information in its local problem. This cost may be embedded into utility driven reasoning.\nA common assumption is that utility-based agents associate each state with a utility value [48]. As such, each communication action\u2019s utility is evaluated as the difference between initial and final utilities. Indeed, if an agent is concerned about its privacy, then it can associate a cost to the revelation of each information in its local problem. Since they are interested in solving the problem, they must also be able to quantify the reward they draw from finding the solution. Here we approach the problem by assuming that privacy is a utility that can be aggregated with the reward for solving this problem. We evaluate how much privacy is lost by the agents during the problem solving process, by the sum of the utility lost for each information that was revealed. For example, the existence and availability of a value from the domain of a variable is the kind of information that the agents want to keep private. The cost of a constraint for a solution is another example of information that agents would like to keep private.\nWhile sometimes possibilistic reasoning is used to guide search, agents were usually assumed to participate in the search process until an agreement is found between all different agents (i.e., a solution). We investigate the case where an agent may modify its search process to optimize utility. Two extensions are introduced, Utilitarian Distributed Constraint Satisfaction Problem (UDisCSP) and Utilitarian Distributed Constraint Optimization Problem (UDCOP), addressing DisCSPs and DCOPs, respectively. These extensions exploit the rewards of agreements and costs representing privacy loss as guidance for the utilitybased agents, where the utility of each state is estimated as the difference between the expected rewards for agreements, and the expected cost representing privacy loss. In this work, Distributed Constrained Problem (DisCP) will refer to both DisCSP and DCOP. Similarly, Utilitarian Distributed Constrained Problems (UDisCPs) will refer to both UDisCSPs and UDCOPs.\n2 The paper is organized as follows. Section II presents existing research concerning solving algorithms and approaches to privacy for DisCPs. Further, Section III describes the concepts and solvers involved in UDisCP to preserve privacy. Section IV discusses theoretical implications. Section V reports our experimental results and evaluates privacy loss on distributed meeting scheduling (DMS) problems. Section VI presents our conclusions and our directions for future research."}, {"heading": "II. BACKGROUND", "text": "We present distributed constrained problems (II-A), and existing approaches for privacy as well as their limits (II-B).\nA. Distributed Constrained Problem\nDistributed Constraint Satisfaction Problems (DisCSPs) and Distributed Constraint Optimization Problems (DCOPs) have been extensively studied as a fundamental way of modelling constrained problems in multi-agent systems, and will be defined in the following, as well as existing solvers.\n1) Definitions: Let us first remind the definitions of the Distributed Constraint Satisfaction Problem (DisCSP) and of the\nDistributed Constraint Optimization Problem (DCOP) [62].\nDefinition 1. A Distributed Constraint Optimization Problem (DCOP) is formally defined as a tuple \u3008A ,X ,D ,C \u3009 where:\n\u2022 A = {A1, A2, . . . , Am} is a finite set of m agents. \u2022 X = {x1, x2, . . . , xn} is a finite set of n variables. Each agent Aself encapsulates variables denoted X (Aself ) (with\nX (Aself ) \u2286 X ). \u2022 D = \u3008D1,D2, . . . ,Dn\u3009 is a set of n domains. Dself is the set of possible values for X (Aself ). \u2022 C = {C1, C2, . . . , Ce} is a finite set of e valued constraints. Each constraint Ci involves some variables X (Ci) \u2286 X\ndefining a cost (positive value) for assignments. We note that Cself = {Ci \u2208 C |X (Ci) \u2229 X (Aself ) 6= \u2205}.\nThe objective is to find an assignment for each variable that minimizes the total cost.\nDefinition 2. A Distributed Constraint Satisfaction Problem (DisCSP) is defined as a DCOP where the constraints are predicates, each one defining for sets of assignments a cost of \u221e (constraint violation). The problem is to find an assignment to variables that does not violate any constraint.\nDefinition 3. Distributed Constrained Problem (DisCP) is any problem modelled with DisCSP or one of its extensions.\nIn the following, we study a particular case of problems, namely mono-variable problems where m = n (i.e., a variable per agent), X (Aself ) containing a single variable, xself . Also, in the following frameworks and algorithms, xself , Dself , and Cself are generalized referring to the projection of the part of different elements from the problem known only by the current agent (called self ). The problem that each agent has to solve in DisCP is a Stochastic Constraint Optimization Problem (Stochastic COP) or a Stochastic CSP [60], which are generalizations of Stochastic SAT, SSAT [34], namely where the local problem has to be solved considering its impact on external variables and constraints whose values are not yet known and that are not under the control of this agent, but that of other agents (commonly, lower priority), values of which the agent may know a probabilistic profile.\n2) Existing Solvers: We now introduce some existing solvers for DisCP with which we exemplify extensions based on our utilitarian approach. We consider two well-known solvers for DisCSP (SyncBT and ABT ), and three solvers for DCOPs (called ADOPT , DSA and DBO ): a) Synchronous Backtracking (SyncBT): is the baseline algorithm for DisCSPs [64], [68]. SyncBT is a simple distribution of the standard backtracking algorithm. Agents consecutively send a satisfying assignment to their variable to the next agent. If an agent is unable to find an instantiation compatible with the current partial assignment it has received, it asks the previous one to change its assignment. The process repeats until a complete solution is built or until the whole search space is explored.\nb) Asynchronous Backtracking (ABT): is a common alternative solver for DisCSPs that allows agents to run concurrently and asynchronously [64]. Each agent finds an assignment to its variable and communicates it to the other connected agents having constraints involving this variable. Then agents wait for incoming messages. The assignments received through ok? form a context called agentView. If an agent\u2019s assignment is inconsistent with its agentView, it is changed and communicated to the other agents. A subset of an agentView preventing an agent from finding an assignment that does not violate any of its constraints is called a nogood. If an agent infers a nogood from its constraints and its agentView, it asks the lowest priority agent involved in the nogood to change its assignment through a nogood message.\nc) Asynchronous Distributed Optimization (ADOPT): guarantees to find the optimal solution and only needs polynomial space [42], [61], [56]. ADOPT organizes agents into a depth first search tree in which constraints are only allowed between a variable and any of its acquaintances (parents and descendants in the tree). When the estimated cost of agents\u2019 assignment is higher than a given threshold, the agent switches its value assignment to the value with smallest estimated cost. When the upper and lower bounds meet at the root agent, a globally optimal solution has been found and the process finishes. Note that other approaches based on a graph re-arrangement have been explored, for example a cluster exploitation like Asynchronous Partial Overlay [39], [38].\n3 d) Distributed Stochastic Algorithm (DSA): makes agents start their process by randomly selecting a value [67]. Agents then enter an iteration where they send their last assigned value to their neighbours and collect any new values from them. The next candidate value is chosen based on the values received from other agents and on maximizing a given utility function. This algorithm is incomplete and does not guarantee optimality, but it is frequently efficient for finding solutions close to optimum.\ne) Distributed Breakout (DBO): is an iterative improvement solver for DCOPs [63], [28]. The evaluation of a given solution is the summation of the weights for all its violated constraints. An assignment is then changed to decrease the solution value. If the evaluation of the solution cannot be decreased by changing a value, the current state may be a local minimum. In this situation, DBO increases the weights of constraint violation pairs in the current state so that the evaluation of this current state becomes higher than the neighbouring states. Thus, the algorithm can escape from the local minimum.\nB. Privacy\nWe present generalities about privacy, in particular about our definition and typology. This typology will be used to compare\nexisting approaches to privacy in DisCP.\n1) Definition and typology: Privacy is the concern of agents to not reveal their personal information. In this work, we define\nprivacy as follows:\nDefinition 4. Privacy is the utility that agents benefit from conserving the secrecy of their personal information.\nContrary to the standard rewards in DisCSPs, privacy costs are proper to each individual agent. Therefore, the computation is now performed by utility-based and self-interested agents, whose decisions aims at maximizing a utility function. The objective is then to define a policy associating an expected utility maximizing action (communication act or computation) to each state, where the state includes the belief about the global state). In existing works, several approaches have been developed to deal with privacy in DCOPs. Some cryptographic approaches offer certain end-to-end security guarantees by integrating the entire solving process in one primitive for DisCSPs or for DCOPs, the highest level of such privacy guarantees being achievable only for problems with a single variable [54]. Other cryptographic approaches are hybrids interlacing cryptographic and artificial intelligence steps [57], [21], [24]. While ensuring privacy in each primitive [29], cryptographic techniques are usually slower, and sometimes require the use of external servers or computationally intensive secure function evaluation techniques that may not always be available or justifiable for their benefits, making them impractical [22], or lacking clear global security guarantees. A couple of such approaches with which we compare in more detail are:\na) Distributed Pseudo-tree Optimization Procedure with Secret Sharing (SS-DPOP): modifies the standard DPOP algorithm [46] to protect leaves in the depth first search tree, where agents sharing constraints are on the same branch [21]. SS-DPOP uses secret sharing [52] to aggregate the results of a single solution, without revealing the individual valuations this solution consists in. The aggregated values are then passed to the bottom agent, who aggregates this information with its own valuations and sends the aggregate up the chain. [33] has also extended DPOP to preserve different types of privacy using secure multi-party computation.\nb) Privacy-Preserving Synchronous Branch and Bound (P-SyncBB): is a cryptographic version of SyncBB [27], [25] for solving DCOPs while enforcing a stronger degree of constraint privacy [24]. P-SyncBB computes the costs of CPAs (current partial assignments) and compares them to the current upper bound, using secure multi-party protocols. Some protocols were proposed in [24] that can solve problems without resorting to costly transfer sub-protocols, and compare the cost of a CPA shared between two agents to the upper bound held by only one of them. Some variations on the standard SyncBB include NCBB and AFB [20], [24].\nWe choose to deal with privacy by embedding it into agents\u2019 decision-making. Other approaches use different metrics and\nframeworks to quantify privacy loss. According to [23], agents privacy may concern the four following aspects:\nDomain privacy: Agents want to keep the domain of their variable private. The common benchmarks and some algorithms (like DPOP , and common cryptographic techniques) assume that all the domains are public, which leads to a complete loss of domain privacy. In the original DisCSP approach a form of privacy of domains is implicit (see ABT ), while being formally required in its PKC extension [6]. Constraint privacy: Agents want to keep the information related to their constraints private [53]. If variables involved in constraints are considered to belong to only one agent, we can distinguish the revelation of information to agents that participate in the constraint (internal constraint privacy) and the one to other agents (external constraint privacy). While common problems with domain privacy can be straightforwardly modelled as problems with constraint privacy, as discussed later, there theoretically exists a kind of domain privacy which cannot be modelled with constraint privacy. Assignment privacy: Agents want to keep the assigned values to their variables private. The revelation of assigned values concerns the assignment of the final solution, as well as the ones proposed during search. Algorithmic privacy: Even though it is commonly assumed that all agents run the same algorithm during the solving, agents may modify the value of some parameters guiding the search process for some personal benefit (e.g., the likelihood of updating its value). This can be achieved by keeping the message structure and contracts of certain existing DisCSP solvers to be used as communication protocols rather than algorithms, as introduced in [55], where protocols obtained in such ways are compared with respect to the flexibility offered for agents to hide their secrets.\n4 2) Existing approaches to privacy in DisCP: We now introduce two examples modelled with DisCP frameworks, and show\nhow existing approaches to privacy deal with these problems and their limits.\na) Distributed Constraint Satisfaction Problem:\nExample 1. Suppose a meeting scheduling problem between a professor (called A1) and two students (A2 and A3). They all consider to agree on a time slot to meet on a given day, having to choose between 8 am, 10 am and 2 pm. Professor A1 is unavailable at 2 pm, A2 is unavailable at 10 am, and A3 is unavailable at 8 am. There can exist various reasons for privacy. For example, A2 does not want to reveal the fact that it is busy at 10 am. The value that A2 associates with not revealing the 10 am unavailability is the salary from a second job ($2,000). The utility of finding an agreement for each student is the stipend for their studies ($5,000). For A1, the utility is a fraction of the value of its project ($4,000). This is an example of privacy for absent values or constraint tuples. Further A3 had recently boasted to A2 that at 8 am it interviews for a job, and it would rather pay $1,000 than to reveal that it is not. This is an example of privacy for feasible values of constraint tuples.\nSimilarly, participants associate a cost to the revelation of each availability and unavailability. Thus, scaling numbers by 1000 for simplicity, corresponding agents associate a cost of 1 to the revelation of their availability at 8 am, a cost of 2 to the one at 10 am, and a cost of 4 to the one at 2 pm. The reward from finding a solution is 4 for A1 and 5 for both A2 and A3. For simplicity, in the next sections, we will refer to the possible values by their identifier: 1, 2, and 3 (corresponding to 8 am, 10 am and 2 pm respectively). As this problem states allowed or forbidden values, it is represented by a DisCSP as follows:\n\u2022 A = {A1, A2, A3} \u2022 X = {x1, x2, x3} \u2022 D = {{1, 2, 3}, {1, 2, 3}, {1, 2, 3}} \u2022 C = {{\u00ac(x1 = x2 = x3)}, {(x1 6= 3)}, {(x2 6= 2)}, {(x3 6= 1)}}\nAs it can be observed, DisCSPs cannot model the information concerning privacy. Now we will show how existing extensions\nmodel them.\nb) Distributed Private Constraint Satisfaction Problem (DisPrivCSP): models the privacy loss for individual revelations [18], [55]. It also lets agents abandon the search process when the incremental privacy loss overcomes the expected gains from finding a solution. Each agent pays a cost if the feasibility of each solution is determined by other agents. The reward for solving the problem is given as a constant. Those concepts were so far used for evaluating qualitatively existing algorithms, but were not integrated as heuristics in the search process. Privacy and the usual optimization criteria of Distributed Constraint Optimization Problems are merged into a unique criterion [15]. The additional parameters are a set of privacy coefficients and a set of rewards.\nThis framework successfully models all the information described in the initial problem and also measures the privacy loss for each agent. However, it was not yet investigated what is the impact of the interruptions when privacy loss exceeds the reward threshold, its relation to utility, or how agents could use this information to modify their behaviour during the search process to preserve more privacy.\nc) Valuation of Possible States (VPS): measures privacy loss by the extent to which the possible states of other agents are reduced [58], [36], [35], [22]. Privacy is interpreted as a valuation on the other agents\u2019 estimates about the possible states that one lives in. During the search process, agents propose their values in an order of decreasing preference. At the end of the search process, the difference between the presupposed order of preferences and the real one observed during search determines the privacy loss: the greater the difference, the more privacy has been lost.\nHowever in our sample problem, agents initially know nothing about others agents but the variable they share a constraint with and cannot suppose an order of preference. Agents have no information about others agents privacy requirements. Thus, agents do not expect to receive any value proposal more than another. In this direction one needs to extend VPS to be able to also model the kind of privacy introduced in this example.\nd) Partially Known Constraints (PKC): uses entropy, as defined in information theory, to quantify privacy loss [6]. In this method, two variables owned by two different agents may share a constraint. However, not all the forbidden couples of values involved in a constraint are known by both agents. Each agent only knows a subset of the constraints. During the search process, assignment privacy is leaked through ok? and nogood messages, like in standard algorithms. This problem is solved by not sending the value that is assigned to a variable in a ok? message, but the set of values compatible with this assignment. For nogood messages, rather than sending the current assignments, an identifier is used to specify the state of the resolution and to check if some assignments are obsolete or not.\nPKC assumes that agents only know their own individual unavailabilities [6]. Only the junction of information known by all agents can rebuild the whole problem. However, while PKC let agents preserve privacy of unary constraints, it does not consider the cost of revelation of assignments.\ne) Distributed Constraint Optimization Problem:\n5 Example 2. Suppose a problem concerning scheduling a meeting between three participants. They all consider to agree on a\nplace to meet on a same slot time, to choose between London, Madrid and Rome. For simplicity, we will refer to these possible values by their identifiers: 1, 2 and 3 (London, Madrid and Rome respectively). A1 lives in Paris, and it will cost it $70, $230 and $270 to attend the meeting in London, Madrid and Rome respectively. A2 lives in Berlin, and it will cost it $120, $400 and $190 to attend the meeting in London, Madrid and Rome respectively. A3 lives in Brussels, and it will cost it $40, $280 and $230 to attend the meeting in London, Madrid and Rome respectively. The objective is to find the meeting location that minimizes the total cost students have to pay in order to attend.\nThe privacy costs for revealing its cost for locations 1, 2 and 3 for A1 are $80, $20, $40. The privacy cost for locations 1, 2 and 3 are $100, $30, $10 for A2 and $80, $30, $10 for A3. There exist various reasons for privacy. For example, students may want to keep their cost for each location private, since it can be used to infer their initial location, and they would pay an additional (privacy) price rather than revealing the said travel cost. For example, A1 associates $80 privacy cost to the revelation of the travel cost of $70 for meeting in London.\nThis example may be defined by a DCOP:\n\u2022 A = {A1, A2, A3} \u2022 X = {x1, x2, x3} \u2022 D = {{1, 2, 3}, {1, 2, 3}, {1, 2, 3}} \u2022 C = {{(x1 = 1), 70}, {(x1 = 2), 230}, {(x1 = 3), 270},\n{(x2 = 1), 120}, {(x2 = 2), 400}, {(x2 = 3), 190}, {(x3 = 1), 40}, {(x3 = 2), 280}, {(x3 = 3), 230}, {\u00ac(x1 = x2 = x3),\u221e}}\nThe notation (x = a) is a predicate p stating that variable x is assigned to value a. Each constraint in C is described with the notation {p, vi,p}, and states that if predicate p holds then a cost vi,p is paid by the Agent Ai enforcing the constraint. One could attempt to model the privacy requirements by aggregating the solution quality, related to obtain the reward for a solution (called SolutionCost) and for keeping the privacy (called PrivacyCost) into a unique cost. However, this is not possible. Indeed, in a DCOP, agents explore the search space to find a better solution, and only pay the corresponding solution cost when the search is over and the solution is accepted. This means that the solution cost decreases with time. However, privacy costs are cumulative and are paid during the search process itself (at each time, a solution is proposed), no matter what solution is accepted at the end of the computation. This means that the total privacy loss increases with time. Aggregating the solution costs and privacy costs or using a multi-criteria DCOP would not consider the privacy cost of the solutions that are proposed but not kept as final. Also, a given solution may imply different privacy losses depending on the algorithm used to reach it.\nNone of the previous techniques consider all aspects of privacy, and while they preserve some privacy, related algorithms require resources or properties from the DisCSP or DCOP that may not be possible due to requirements dictated by the real world problem. We therefore propose to deal with privacy directly from the agents\u2019 decision making perspective, using a utility-based approach."}, {"heading": "III. UTILITARIAN DISTRIBUTED CONSTRAINED PROBLEM", "text": "This section defines our utility-based framework (III-A), and then describes how it models previously presented problems,\nwith different types of privacy requirements (III-B).\nA. Definitions\nWhile some previously described frameworks do model the details of our example regarding privacy, it has until now been an open question as to how they can be dynamically used by algorithms in the solution search process. It can be noticed that the rewards and costs in our problem are similar to the utilities commonly used by planning algorithms [31]. Thus, we propose to define a framework that specifies the elements of the corresponding family of planning problems. To do so, we ground the theory of our interpretation of privacy in the well-principled theory of utility-based agents [51].\nDefinition 5. A utility-based agent is characterized by its ability to associate a value to each state of the problem, representing its contentment to be in this state.\nFor example, the state of Agent Ai can include the subset of Di that it has revealed and the reward associated to the solution under consideration. The problem is to define a policy for each agent such that their utility is maximized. A policy is a function that associates each state with an action that should be performed in it [48]. We evaluate the utility of a state as follows:\nUtility = \u2211\ni\nrewardi \u2212 \u2211\nj\ncostj where rewardi, costj \u2208 R + (1)\n6 Possible actions are communications dictated by the used solver, and each possible solution proposal, acceptation or rejection, is associated with its corresponding reward (for increasing the probability of problem solving) and cost (for revealing information).\nThus, unlike for DisCPs, the solution of a UDisCP does not necessarily include an agreement, as pursuing the search may imply a decrease of utility. As privacy is lost during search, a given set of assignments can have different utilities, depending on the information exchanged before. We define Utilitarian Distributed Constrained Problem (UDisCP):\nDefinition 6. A Utilitarian Distributed Constrained Problem (UDisCP) is formally defined as a tuple \u3008A ,X ,D ,C ,U ,R\u3009, i.e., a DisCP with:\n\u2022 U = \u3008Ud,Ua,Ug,Uc\u3009 is a quadruplet where:\n\u2013 Ud is a matrix of domain privacy costs, where ud(i,j) is the cost for Ai to reveal whether j \u2208 Di. \u2013 Ua is a matrix of assignment privacy costs, where ua(i,j) is the cost for Ai to reveal a local solution including the\nassignment xi = j. \u2013 Ug is a matrix of assignment privacy costs, where ug(i,j) is the cost for Ai to reveal the existence of a global solution\nincluding the assignment xi = j. \u2013 Uc is a vector of constraint privacy costs, where uc(i,j) is the cost for Ai to reveal the weight of constraint Cj .\n\u2022 R = {r1, . . . , rn} is a vector of rewards, where ri is the reward that Agent Ai receives if an agreement is found.\nNote that is the case where a domain can be represented on a computer as a type of a parameter to a predefined software function modelling constraints (e.g., an integer), then Ud can be embedded in Uc, as a unary constraint on the domain. We note also the notations UDisCSP for Utilitarian DisCSP and UDCOP for Utilitarian DCOP.\nB. Description on Problems with Privacy\nIn this section, we present how UDisCP deals with the question of privacy on the two previous examples, and we solve them using our utilitarian extensions of existing solvers. For clarity, unchanged parts of pseudo-codes are shown in gray colour (while our extensions are in black colour). These parts may include variables or procedures that are used in other parts of the solvers but that are not used (and therefore not detailed) in this work.\n1) Privacy of Domains: We propose to revise our previous example and present the extensions for two standard solvers\n(SyncBT and ABT ).\nExample 3. Recall Ud represents the cost for each agent to reveal possible values. R is the reward that each agent gets when a solution is found, motivating them to initiate the solving.\nDisCSP introduced in Example 1 is extended to UDisCSP by specifying the additional parameters Ud and R defined as\nfollows. For example, ud(1,3) refers to the cost for the first agent to reveal the third value in its domain, namely 4.\nUd =\n[ ]\n1 2 4 A1 1 2 4 A2 1 2 4 A3\nR = {4, 5, 5}\nNow we discuss how the standard ABT and SyncBT algorithms are adjusted to UDisCSPs. After each state change, each agent computes the estimated utility of the state reached by each possible action, and selects randomly one of the available actions leading to the state with the maximum expected utility.\nSyncBTU and ABTU are obtained by similar modifications of SyncBT and ABT [64], [62], [68], respectively. In our extended algorithms, agents compute the frequency of rejection of their solution proposal to estimate the expected utilities. This frequency can be re-evaluated at any moment based on data recorded during previous runs on problems of similar tightness (i.e., having the same proportion of forbidden instantiations). Learning from previous experience has been extensively studied [1], [2]. The learning can be off-line or on-line. For off-line learning, agents calculate the number of messages ok? and nogood sent during previous executions, called count. They also count how many messages previously sent lead to the termination of the algorithm, in the variable agreementCount. The frequency with which a solution leads to the termination of the algorithm, is called agreementProb (Equation 2). For on-line learning, one can update the variables count, agreementCount and agreementProb dynamically whenever the corresponding events happen. When previous experiments are not available, the value of agreementProb is set to 1/2 by default (this value is always between 0 and 1).\nagreementProb = agreementCount\ncount (2)\nWhen ok? messages are sent, the agent has the choice of which assignment to propose. When a nogood message is scheduled to be sent, agents also have choices of how to express them. Before each ok? or nogood message, the agents check which available action leads to the highest expected utility. If the highest expected utility is lower than the current one, the agent announces failure. The result is used to decide between proposing assignments, a nogood, or declaring failure.\n7 a) Synchronous Backtracking with Utility (SyncBTU): SyncBTU is obtained by restricting the set of actions to the standard communicative acts of SyncBT , namely ok? and nogood messages. The procedures of a solver like SyncBT define a policy, since they only identify a set of actions (inferences and communications) to be performed in each state. A state of an agent in SyncBT is defined by agentView and a current assignment of the local variable. The local inferences in SyncBTU are obtained from the ones of SyncBT by an extension exploiting the utility information available. The criteria in this research was not to guarantee an optimal policy but to use utility with a minimal change to the original behaviour of SyncBT reinterpreted as a policy. In SyncBTU , the state is extended to also contain a history of revelations of one\u2019s values defining an accumulated privacy loss, and a probability to reach an agreement with each action. Since [64] does not provide pseudo-code for SyncBT , we modify the pseudo-code presented in [68] for SyncBTU : assignCPA (before Line 7), and backtrack (before Line 6). As SyncBTU is close from ABTU , we do not detail the pseudo-code for this algorithm in this report.\nb) Asynchronous Backtracking with Utility (ABTU): Similar modifications are applied to ABT to obtain ABTU : communications of ABTU are composed of ok?, addlink and nogood. The state and local inferences of ABTU are similar to SyncBTU , while also containing the set of nogoods.\nAlgorithm 1: checkAgentView_ABTU\nInput: Dself , agentView, agreementProb, rself 1 when agentV iew and currentV alue are inconsistent do 2 if no value in Dself is consistent with agentView then 3 backtrack;\n4 else 5 select d \u2208 Dself where agentView and d are consistent; 6 currentV alue \u2190 d ; 7 if (estimateCostDisCSP (agreementProb, Dself , 1) > rself ) then 8 interruptSolving;\n9 else\n10 send (ok?,(xself ;d)) to outgoing links\nTo calculate the estimated utility of pursuing an agreement (revealing an alternative assignment), the agent considers all different possible scenarios of the subsets of values that might have to be revealed in the future based on possible rejections received, together with their probability (Algorithm 2). This algorithm assumes as parameters: (i) agreementProb (Equation 2), (ii) an ordered set of possible values D \u2032self for a scenario (by default, the order proposed in Dself ), and (iii) the probability to select a value from D \u2032self , initially 1), called probD. Note that D \u2032 self [j] refers to value at index j of D \u2032 self .\nAlgorithm 2: estimateCostDisCSP\nInput: agreementProb, D \u2032self , probD Output: estimatedCost\n1 valueId = j | (Dself [j] = D \u2032self [1]); 2 if (|D \u2032self | = 1) then 3 return ( \u2211j=valueId\nj=1 ud(self,j)) \u00d7probD; 4 else 5 v \u2190 D \u2032self [1] ; 6 costRound\u2190 estimateCostDisCSP (agreementProb, {v}, agreementProb\u00d7 probD); 7 costT emp \u2190 estimateCostDisCSP (agreementProb, D \u2032self \\ {v}, (1\u2212 agreementProb) \u00d7 probD); 8 estimatedCost \u2190 costRound+ costT emp; 9 return estimatedCost;\nThe algorithm then recursively computes the utility of the next possible states, and whether the revelation of the current value v leads to the termination of the algorithm, values which are stored in variables costRound and costT emp. The algorithm returns the estimated cost (called estimatedCost) of privacy loss for the future possible states currently.\nExample 4. Continuing with Example 1 (whose a solving is illustrated by Figure 1), at the beginning of the solving, Agent A1 has to decide for a first action to perform. We suppose the agreementProb learned from previous solvings is 0.5. To decide whether it should propose an available value or not, it calculates the corresponding estimatedCost by calling Algorithm 2 with parameters: the learned agreementProb = 0.5, the set of possible solutions (D \u20321 = {1, 2, 3}) and probD = 1.\n8 Professor A1 Student A2 Student A3\nM1(OK?(x1 = 1))\nM2(OK?(x2 = 1))\nM3(OK?(x1 = 1))\nM4(BT (x2 = 1))\nM5(BT (x1 = 1))\nM6(OK?(x2 = 3))\nM7(OK?(x1 = 2))\nM8(OK?(x1 = 2))\nM9(BT (x1 = 2))\n\u2211\nFor each possible value, this algorithm recursively sums the cost for the two scenarios corresponding to whether the action leads immediately to termination, or not. Given privacy costs, the availability of three possible subsets of D \u20321 may be revealed in this problem: {1}, {1, 2}, and {1, 2, 3}. Each set of size S consists of S first elements of the list solution based on this initial order.\nThe estimatedCost returned is the sum of the costs for all possible sets, weighted by the probability of their feasibility being revealed if an agreement is pursued. At the function call: costRound = ud(1,1) \u00d7 0.5 = 1\u00d7 0.5 = 0.5. At the next recursion: costRound = (ud(1,1) + ud(1,2))\u00d7 0.25 = (1 + 2)\u00d7 0.25 = 0.75. At the last recursion: costRound = (ud(1,1) + ud(1,2) + ud(1,3))\u00d7 0.25 = (1 + 2 + 4)\u00d7 0.25 = 1.75. The algorithm returns the sum of these three values: estimatedCost= 0.5 + 0.75 + 1.75 = 3. The expected utility of pursuing a solution being positive (reward\u2212estimatedCost= 4\u2212 3 = 1), the first value is proposed.\nNext is an illustrative example of other ABTU operations with the scenario {1, 3, 2}.\nExample 5. With the original ABT, A2 proposes x2 = 1 in Message M2 and x2 = 3 in Message M6. In this case, the privacy loss for A2 is ud(2,1)+ud(2,3) = 1+4 = 5. However, with ABTU, we do not only use the actual utility of the next assignment to be revealed, but we estimate privacy loss using Algorithms 1 and 2. After A2 has already sent x2 = 1 with M2, it considers sending x2 = 3 with M6. This decision making process is depicted in Figure 2. If the next value, 2 pm, is accepted, A2 will reach the final state while having revealed x2 = 1 and x2 = 3, for a total privacy cost of ud(2,1)+ud(2,3) = 1+4 = 5. If it is not, the unavailability of the last value x2 = 2 will have to be revealed to continue the search process, leading to the revelation of all its assignments for a total cost of 7. Since both these scenarios have a probability of 50% to occur, estimatedCost equals (5 + 7)/2 = 6. The utility (reward\u2212estimatedCost) being equal to 5\u2212 6 = \u22121, A2 has no interest in revealing x2 = 3 and interrupts the solving. Its final privacy loss is only ud(2,1) = 2. The utility of the final state reached by A2 being \u22122 with ABTU, and \u22124 with ABT, ABTU preserves more privacy than ABT in this problem.\n2) Privacy of Assignments: We would like illustrate how our approach deals with privacy of assignment in UDCOP by\nextending the previous example and solving it with ADOPTU .\n9 Example 6. DCOP in Example 2 is extended to a UDCOP by specifying the additional parameters Ua and R. Ua represents the cost for each agent to reveal each assigned value. R is a default reward (for example, 1000) that each agent gets for finding a solution to the problem, motivating them to initiate the solving. The description is as follows:\nUa =\n[ ]\n80 20 40 A1 130 30 10 A2 80 30 10 A3\nR = {1000, 1000, 1000}\nNow we discuss how the standard DCOP algorithms are adjusted to UDCOPs. After each state transition, each agent computes the estimated utility of the state reached by each possible action, and selects randomly one of the actions leading to the state with the maximum expected utility. In our algorithms, agents estimate expected utilities using the risk of one of their assignments to not be a part of the final solution.\nTo estimate the cost for a DCOP solution, estimateCostDCOP is introduced (Algorithm 3). Its inputs are the utilities considered (Ui), the domain of possible values (Dself ) and the already revealed informations (revealedInfos). For each solution, the algorithm evaluates estimatedCost including both constraint costs (i.e., SolutionCost and PrivacyCost). Recall that the utility of the reached state is calculated by using Equation 1, and equals the fixed reward (by hypothesis) reduced by estimatedCost. If the initial DCOP is a maximization problem, it is first recast as a minimization one, so that constraint values correctly belong to costs.\nAlgorithm 3: estimateCostDCOP\nInput: utilities, Dself , revealedInfos Output: estimatedCost\n1 SolutionCost \u2190 0; 2 PrivacyCost \u2190 0; 3 foreach value d \u2208 Dself do 4 foreach constraint c \u2208 Cself do 5 if (c contains predicate p such as p = (xi = d)) \u2227 (d \u2208 revealedInfos) then 6 SolutionCost \u2190 SolutionCost + vself,p ; 7 PrivacyCost \u2190 PrivacyCost +utilities(self,d);\n8 estimatedCost \u2190 SolutionCost + PrivacyCost; 9 return estimatedCost;\nBefore proposing a new value, agents estimate the utility that will be reached in the next state. This value is the summation of the costs of revealed agentViews (weighted by their probability to be the final solution) in the said state, and of the corresponding privacy costs. If this utility is lower than the estimation of the current state, the agent proposes the next value, otherwise it keeps its current value.\na) Asynchronous Distributed Optimization with Utility (ADOPTU): is a method obtained from ADOPT by adding Lines 7 to 10 in Algorithm 4 (procedure checkAgentView_ADOPTU). At Line 7, the possible next value is set to the value that has the minimal cost. The cost reached after the next value (Line 8) and the cost of the current state (Line 9) are estimated. At Line 10, if the next cost is lower than the current cost, the maximal improvement and the new value are updated.\nExample 7. Continuing with Example 2, at the beginning of the computation with the ADOPTU solver, the participants select a random value. The resulting set of assignments is x1 = 1, x2 = 3 and x3 = 2 for A1, A2, and A3 respectively. The participants then inform their values to their linked descendants (A2 and A3 for A1, and A3 for A2). A2 will have agentView = {(x1 = 1)} and chooses the value 1, since this value minimizes its cost. A3 sends a VIEW message with cost of 190 to A2. This cost is reported because given the value of x1, it is a lower bound on global solution cost: 190 is a lower bound on local cost at x2, and 0 is a lower bound on local costs at other variables. Now concurrently with the previous execution of A2, A3 receives the assignment x2 = 3. With ADOPTU, A3 realizes that changing its value would increase its cost and decides to keep its value unchanged. However, with ADOPT, A3 decides to change its value to 3, as it reduces the cost. Then, with ADOPT and ADOPTU, A3 receives the assignments x1 = 1 and x2 = 1, and decides to change its value to x3 = 1. At the final step, the previous agentView is the optimal solution. With ADOPTU, the reached costs are: 70 + 80 = 150, 120+ 30 + 100 = 250, 80 + 80 = 160 for A1, A2, and A3 respectively. With standard ADOPT, the final utilities are: 70 + 80 = 150, 120 + 30 + 100 = 250, 80 + 80 + 30 = 190 for A1, A2, and A3 respectively. Therefore, by using ADOPTU instead of ADOPT, A3 avoids a futile revelation of x3 = 2 and reduces its utility by 30.\n3) Privacy of Constraints: Similarly, we detail the model for our example and we present the extensions of two solvers for\nUDCOPs (namely DBOU and DSAU ).\n10\nAlgorithm 4: checkAgentView_ADOPTU\nInput: utilities, Dself , revealedAssignments 1 for each (d \u2208 Dself ) update l[d] and recompute h[d]; 2 for each Aj with higher priority than Aself do; 3 if (h has non-null cost CA for all values of Dself ) then 4 vn \u2190 min resolution(j); 5 if vn 6= lastSent[j] then 6 send nogood(vn,self ) to Aj ;\n7 newV alue \u2190 argmind(cost[h[d]]); 8 nextCost \u2190 estimateCostDCOP (utilities, Dself , revealedAssignments\u222a newV alue); 9 currentCost \u2190 estimateCostDCOP (utilities, Dself , revealedAssignments); 10 if (nextCost < currentCost) then 11 assign newV alue to xself ; 12 if (xself was modified) then 13 send ok? to each neighbour to inform them about the value change;\nExample 8. DCOP described in Example 2 is extended to a UDCOP by specifying the parameters C , Uc and R as follows. C is the updated set of constraints. Uc represents the cost for each agent to reveal each assignment cost. R is a default reward that each agent gets for finding a solution to the problem, motivating them to initiate the solving. C \u2019s subsets known\nto each agent are: C1 = {c1,1 = [(x1 = 1), 70], c1,2 = [(x1 = 2), 230], c1,3 = [(x1 = 3), 270], c1,4 = [\u00ac(x1 = x2 = x3),\u221e]} C2 = {c2,1 = [(x2 = 1), 120], c2,2 = [(x2 = 2), 400], c2,3 = [(x2 = 3), 190], c2,4 = [\u00ac(x1 = x2 = x3),\u221e]} C3 = {c3,1 = [(x3 = 1), 40], c3,2 = [(x3 = 2), 280], c3,3 = [(x3 = 3), 230], c3,4 = [\u00ac(x1 = x2 = x3),\u221e]}\nUc = [ ]80 20 40 A1 100 30 10 A2 80 30 10 A3\nR = {1000, 1000, 1000}\nTo adapt existing algorithms for privacy of constraints, revealed domains and possible revealed domains are changed to the\nrevealed constraints and possible revealed constraints, respectively.\na) Distributed Breakout with Utility (DBOU): is a solver obtained from DBO by adding Lines 5 to 11 in Algorithm 5 (procedure sendImproveDBOU). At each iteration, DBOU does not only uses solution cost to guide search and computes the value giving maximal improvement as in standard DBO (Lines 2 to 4), but also considers constraint privacy costs (Lines 5 to 7). The new chosen value is the one minimizing total cost (Lines 8 to 11). As privacy loss is cumulative, agents update the set of revealed constraints to also consider previously revealed constraints during their estimation of the reached cost for the different considered values.\nb) Distributed Stochastic Algorithm with Utility (DSAU): is obtained from standard DSA by adding Lines 10 to 13 in Algorithm 6 (procedure DSAU). Each agent computes a solution and sends it to its neighbours (Lines 1 to 5). At each iteration, after collecting new values from neighbours (Line 6), each agent compares the new agentView with the previous one (Line 7). If a difference is detected, the agent Aself computes a new solution considering both solution (Lines 8 and 9) and privacy costs (Lines 10 to 13) similarly with ADOPTU and DBOU .\nExample 9. Continuing with Example 2, at the beginning of the computation with the DSAU solver, the participants select a\nrandom value. The resulting agentView of each agent is : {(x1 = 1), (x2 = 1), (x3 = 3)}. The utilities of the reached state are: v1,1 + uc(1,1) = 70 + 80 = 150, v2,1 + uc(2,1) = 120 + 100 = 220, and v3,3 + uc(3,3) = 230 + 10 = 240 for Students A1, A2, and A3 respectively. The participants then inform each other of their value. They consider changing them to a new randomly selected one. The considered agentView is {(x1 = 2), (x2 = 3), (x3 = 1)}. If the participants change their value, the utilities of the reached states would be: (v1,1 + v1,2)/2 + uc(1,1) + uc(1,2) = 250, (v2,1 + v2,3)/2 + uc(2,1) + uc(2,3) = 265,\n11\nAlgorithm 5: sendImproveDBOU\nInput: utilities, Dself , revealedConstraints 1 eval \u2190 evaluation value of xself ; 2 myImprove \u2190 0 ; 3 newV alue \u2190 xself ; 4 nextV alue \u2190 value giving maximal improvement; 5 nextRevealedConstraints \u2190 revealedConstraints \u222a constraints with nextV alue ; 6 nextCost \u2190 estimateCostDCOP (utilities, Dself , nextRevealedConstraints); 7 currentCost \u2190 estimateCostDCOP (utilities, Dself , revealedConstraints); 8 if (nextCost < currentCost) then 9 myImprove \u2190 possible max improvement; 10 newV alue \u2190 value giving maximal improvement; 11 xself \u2190 newV alue ;\n12 if (eval = 0) then 13 consistent \u2190 true 14 else 15 consistent \u2190 false ;\n16 terminationCounter \u2190 0 ; 17 if (myImprove > 0) then 18 canMove \u2190 true ; 19 quasiLocalMin\u2190 false ; 20 else 21 canMove \u2190 false ; 22 quasiLocalMin\u2190 true\n23 send (improve, xself , myImprove, eval, terminationCounter) to neighbours;\nAlgorithm 6: DSAU\nInput: utilities, Dself , revealedConstraints 1 nextV alue \u2190 randomly chosen a value; 2 while (no termination condition is met) do 3 if (nextV alue 6= xself ) then 4 xself \u2190 nextV alue; 5 send xself to neighbours;\n6 nextV iew \u2190 collect xi for each neighbour Ai; 7 if (nextV iew 6= agentView) then 8 agentView\u2190 nextV iew; 9 tempV alue \u2190 randomly chosen value; 10 revealedConstraints \u2190 revealedConstraints \u222a Cself with xself = tempV alue; 11 nextCost \u2190 estimateCostDCOP (utilities, Dself , nextRevealedConstraints); 12 currentCost \u2190 estimateCostDCOP (utilities, Dself , revealedConstraints); 13 if (nextCost < currentCost) then 14 xself \u2190 tempV alue;\n(v3,3 + v3,1)/2 + uc(3,3) + uc(3,1) = 225, for Students A1, A2, and A3 respectively. A1 and A3 do not propose the new value as it would increase their utility. However, A3 chooses to change its value from 2 to 1 which lowers its utility from 240 to 225. In the next step, agentView is {(x1 = 1), (x2 = 1), (x3 = 1)}. Participants then do not change their value any-more, as all other options would not decrease the utility. At the final step, the previous agentView is therefore the optimal solution. With DSAU, the reached utilities are : 70 + 80 = 150, 120 + 100 = 220, 40 + 10 + 80 = 130 for Students A1, A2, and A3 respectively. With standard DSA, the final utilities are: (v1,1 + uc(1,1) + uc(1,2) + uc(1,3)) = 230, (v2,1 + uc(2,1) + uc(2,2) + uc(2,3)) = 260, (v3,1 + uc(3,2) + uc(3,1) + uc(3,3)) = 160, for Students A1, A2, and A3 respectively.\n12\nTherefore, using DSAU instead of DSA reduces the utility by 80, 40, 30.\nIn this work, studied problems include only one type of privac at a time, to illustrate proposed models and algorithms with simple examples. However, problems integrating several types of privacy can also be modelled with UDisCP. Such problems, where agents would have optimize multiple objectives, will be investigated in future works."}, {"heading": "IV. THEORETICAL DISCUSSION", "text": "This section deals with three theoretical studies, i.e., comparisons with DCOPs (IV-A), MO-DCOPs (IV-B) and POMDP\n(IV-C).\nA. Comparison with DCOPs\nThe introduced UDCOP framework can assume that inter-agent constraints are public (without significant loss of generality). This is due to the fact that any problem with private inter-agent constraints, is equivalent with its dual representation where each constraint becomes a variable [3].\nTheorem 1. UDCOP planning and execution is at least as general as DCOPs solving.\nProof. A DCOPs can be modelled as a UDCOPs with all privacy costs equal 0. The obtained UDCOPs would always reach an agreement, if possible. Therefore the goal of a UDCOPs would also match with the goal of the modelled DCOPs. This implies a tougher class of complexity for UDCOPs.\nThe space complexity required by ABTU and SyncBTU in each agent is identical with the one of ABT and SyncBT , since the only additional structures are the privacy costs associated with its values, constituting a constant factor increases for domain storage Similarly, additional structures with constant values are added from DCOP to UDCOP.\nB. Comparison with MO-DCOPs\na) Multi-Objective Distributed Constraint Optimization Problem: A multi-objective optimization problem (MOOP) [37], [11] is defined as the problem of simultaneously maximizing k objective functions that have no common measure, defined over a set of variables, each one taking its value in a given domain. Thus, a solution to MOOP is a set of assignments maximizing the combination of the objective functions. Here, each objective function can be defined over a subset of variables of the problem. However, to simplify our discourse, we assume that each function is defined over the same set of variables. A Multi-Objective DCOP [8], called MO-DCOP, is an extension of the standard mono-objective DCOPs.\nNote that a MO-DCOP is a DCOP where the weight of each constraint tuple is a vector of values [wi], each value wi representing a different metric. Two weights [w1i ] and [w 2 i ] for the same partial solution, inferred from disjoint sets of weighted constraints, are combined into a new vector [w3i ] where each value is obtained by summing the values in the corresponding position two input vectors, namely w3i = w 1 i + w 2 i . The quality of a solution of MO-DCOP is a vector integrating the cost of all weighted constraints. The vectors can be compared using various criteria, such as leximin, maximin, social welfare or Theil index [45], [41].\nTo clarify why Multi-Objective DCOPs (MO-DCOPs) cannot integrate our concept of privacy as one of the criteria they aggregate, we give an example of what would be achieved with MO-DCOPs, as contrasted with the results using DCOPs. We show a comparative trace based on one of the potential techniques in MO-DCOPs, providing a hint on why MO-DCOPs cannot aggregate privacy lost during execution in the same way as UDCOP. In this example, the privacy value of each assignment and its constraint cost are two elements of an ordered pair defining the weight of MO-DCOP. For illustration, in this example pairs of weights are compared lexicographically with the privacy having priority.\nExample 10. We assume to model Example 2 with a MO-DCOP. As also illustrated in the trace (Table I) with lexicographical comparison (UDCOP DSAU vs. MO-DCOP DSA), privacy first. Candidate values are marked with \u2217 if they are better than old values, and will be adopted. For these two approaches, we evaluate both SolutionCost and PrivacyCost. At the beginning of DSA process, the participants select a random value. Resulting agentView is {(x1 = 1), (x2 = 1), (x3 = 3)}. For instance, A1evaluates this state with PrivacyCost of 80 and SolutionCost of 70. Recall that privacy criterion is prevailing; hence a notation [80, 70]. DSAU evaluates this same state as a summation of these costs, with a value of 150. The participants then inform each others of their value. They consider a change of their value to a new randomly selected one. agentView is {(x1 = 2), (x2 = 3), (x3 = 1)} for these two solvers. With UDCOP DSAU (state1), A1and A2 do not propose the new value as it would increase their cost, and A3 chooses to change its variable\u2019s value from 3 to 1. However, with MO-DCOP DSA, A2 changes its value to 3 (as it believed PrivacyCost will drop from 100 to 10), which is not the case with DSAU, which implies privacy loss. The agentView is now {(x1 = 2), (x2 = 3), (x3 = 3)} (the real privacy loss is the summation of all revealed costs, i.e., 100 + 10 = 110, and not only 10). In short, with the MO-DCOP model, A2 reveals more values and loses more privacy (with a difference of 110\u2212 100 = 10 for privacy costs) than with UDCOPs.\n13\nThe assumption that each agent owns a single variable is also not restrictive. Multiple variables in an agent can be aggregated into a single variable by Cartesian product. Nevertheless some algorithms can exploit these underlying structures for efficiency, and this has been the subject of extensive research [17], [40].\nWe now discuss how UDisCP can be interpreted as a planning problem.\nC. Comparison with POMDPs\nThe problem that each agent in UDisCP has to solve have similarities to a Partially Observable Markov Decision Problem (POMDP). Given ways to approximate observation and transition conditional probabilities, these problems could be reduced to POMDPs [43], [30], [12].\nA POMDP agent regularly reasons in terms of belief (probability distribution over the states), and tries to build a policy, namely a recommendation of each action to be executed as function of current belief. For UDisCP, the corresponding POMDP is defined by the tuple \u3008S,A, T,R,\u2126, O, \u03b3\u3009 with components [50]:\n\u2022 S: Set of states of the agent, defined by possible contents of its agentView, of the nogoods stored by the agent, the knowledge the agent gathers about the secret elements of the UDisCP unknown to it, and the information already revealed. \u2022 A: Set of actions available to an agent, consisting in local reasoning and communication actions that are a function of the selected protocols (i.e., communication language). For example, in ABT these communications actions can have as\npayloads assignment announcements (ok? messages) and nogoods in (nogood messages).\n\u2022 T : Set of transition probabilities between states given actions for UDisCP. It is estimated in our approach by training agreementProb. \u2022 R: Set of rewards of POMDP is the same as R for the corresponding UDisCP. \u2022 \u2126: Set of possible observations is given by U , the possible incoming payloads of the communication actions available\nin UDisCP, as well as possible results of local reasoning steps.\n\u2022 O: Set of conditional observation probabilities. In reported experiments, it is assumed that the message payloads truthfully reveal the corresponding elements of the states of the other agents, while the probability of the remaining elements have\nto be inferred by the agent.\n\u2022 \u03b3: Discount factor set to 1, since we have not taken into consideration the impact of time on utilities in this work.\nIn the next section, we will present experiments that use our UDisCP models and algorithms to preserve privacy."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "We describe the experimental protocol for our study (V-A), and then we give the obtained results for DMS context (V-B).\nA. Experimental protocol and DMS context\nWe evaluate our framework and algorithms on randomly generated instances of distributed meeting scheduling problems (DMS) [37], [5], [19]. Existing studies has already addressed the question of privacy in DMS problems by considering the information on whether an agent can attend a meeting to be private [59], [7].\n14\nThe algorithm we use to generate the DMS problems is defined according to the following procedure: (i) Variables are created and associated with the agent controlling them. (ii) Domains (possible values) are initialized for all variables. (iii) The global constraint \u201call equals\u201d is added. (iv) The unary constraints (individual unavailabilities) are added. (v) Privacy costs uniformly distributed between 0 and 9 are generated for each possible variable value, variable assignment and constraint. The experiments are carried out on a computer under Windows 7, using a 1 core 2.16GHz CPU and 4GiB of RAM. Implementation is done in Java (jre 1.8) using the JADE platform (version 4.3.3) to build the multi-agent system [4]. The problems are parametrized as follows: 10, 20, or 40 agents, 10 possible values, and the cost of a revelation is a random number between 0 and 9. These parameters are used to guarantee that the problems are not over-constrained or under-constrained, as the probability to find a solution increases with the increase of domain size, and with the decrease of the number of agents and unary constraint tightness. Density is defined as the proportion of binary constraints in C among Xi \u00d7 Xj (i 6= j). Because of the constraint requiring all couples of variables to be equals, density in DMS is always 1. Tightness is defined as proportion of forbidden tuples among all constraints (binary but also unary ones) of C .\nProof. For DMS, we assume t for constraint tightness, m for the number of agents A and d for the size of each domain set Di. Indeed, the probability that a given value is authorized is:\n1\u2212 t\nThe probability that a given value is authorized by all agents is:\n(1\u2212 t)m\nThe probability that a given value is forbidden by at least one agent is:\n1\u2212 (1\u2212 t)m\nThe problem has a probability s to have at least one solution if and only if:\n1\u2212 s = (1\u2212 (1\u2212 t)m)d\n1\u2212 d \u221a (1\u2212 s) = (1\u2212 t)m\n1\u2212 t = m\n\u221a\n(1\u2212 d \u221a (1\u2212 s))\nFigures 3 corroborates this for a DMS modelled by DisCSP with a probability of 50% to have at least one solution, as depicted according to the previous formula and to experimental results. These figures are described by the number of agents (coordinate x), domain size (coordinate y) and tightness (coordinate z).\nFor example, to have a probability of 50% to have a solution, we see that a DMS with 8 agents (x = 3), and a domain size of 8 (y = 3), needs to be parametrized with a tightness of 27% (z = 27). For a problem with 16 agents (i.e., x = 4) and a same domain size (y = 3) and a probability to have a solution, the tightness becomes 14% (z = 14). Note that in the experimental plot, values may slightly differ due to the discrete nature of domains size. Later, this formula will be used to generate problems with relevant parameters values and that are not under- or over-constrained. We will verify that probability to find a solution to DMS is negatively correlated with the number of agents, constraints tightness, and positively correlated with domains size. Each set of reported experiments is an average estimation of 50 instances for the different algorithms (SyncBT \u2013 SyncBTU , ABT \u2013 ABTU , ADOPT \u2013 ADOPTU , DBO \u2013 DBOU and DSA \u2013 DSAU ).\n15\nB. Experiments on UDisCP\nTables II and III show the average privacy loss per agent during the execution of the solving algorithms for DisCSPs (SyncBT , ABT ), DCOPs (DBO , DSA , and ADOPT ), and of their respective extensions, with several values for the number of agents, the domain size and the constraints tightness. Bold data points show instances with high privacy loss (from 10.0 to 20.0). Privacy loss of 20.0 is the maximal value before interrupting solving. Empty data points show instances with low privacy loss (below 0.1). Moreover, we refer to a data point in these tables as solver(nbAgents, domainSize, tightness). For example, SyncBT(10,20,30) refers to the average privacy loss for instances with 10 agents, a domain size of 20, and a constraint tightness of 30% solved with SyncBT , namely 6.7.\na) : Generally, we see that for both Tables II and III, increasing the number of agents implies a reduction of the number of solutions. Thus, problems being over-constrained, agents interrupt the solving faster, which can explain the reduction of average privacy loss for problems with many agents. Indeed, we see that most instances with 10 agents have a significant average privacy loss, while instances with 40 agents have low privacy loss. For example, in Table II, for solver(10, 10, 50), all solvers imply a privacy loss (3.5, 0.6, 1.6, 1.5) for SyncBT \u2013 SyncBTU and ABT \u2013 ABTU , respectively. However, for solver(40, 10, 50) all the previous solvers have empty data points. Similarly for Table III, results obtained for 40 agents give also a lower privacy losses for the same reasons.\nb) : Moreover, we also see that for these two tables, high privacy loss is correlated with a high tightness and a high domain size. Table II shows that the number of high privacy loss depend on the tightness (more than 30%) and a domain size up to 30. For example, for ABTU (10,10,50), privacy loss is 1.5, while it is 6.0 for ABTU(10,40,50). Table III shows that\n16\nhigh privacy loss (bold data points) never occurs for instances with a domain size of 10, while it does for most problems with a domain size of 40. For example, for ADOPT(10,10,50), privacy loss is 8.8, while ADOPT(10,40,50) gives 20.0. For these two tables, the main reason is that a higher domain size allows more solution proposals and avoids premature interruptions.\nc) : Finally, recall that the difference of values between different families of solvers is explained by the different types of privacy considered (i.e., domain, assignment and constraint), as the number of revelations of assignments differs from the number of revelations of constraints. Table II measures domain privacy loss for agents during the execution of SyncBT , ABT , and their extensions. SyncBT and SyncBTU are better than ABT and ABTU at preserving privacy, likely due to the increase of exchanged messages with asynchronous solvers, as agents can run concurrently. Table III measures assignment privacy with ADOPT and ADOPTU . For ADOPT(10,40,t) with t \u2208 {10, 20, 30, 40, 50}, the assignment privacy loss is {6.2, 7.6, 10.2, 12.4, 16}. However with the same parameters for ADOPTU , it is {4.4, 5.5, 6.2, 7.3, 7.6}. The loss is reduced with our extension as search is driven by objective to minimize the assignment privacy loss, and interrupt solving if necessary. Table III measures constraint privacy loss with DBO \u2013 DBOU and DSA \u2013 DSAU . We see that DBO and DBOU are better than DSA and DSAU , likely due to the fact that with DBO , only one agent changes its value at each iteration, while with DSA all agents do, which implies an increase of the number of exchanged messages. For example, in Table III for solver(10, 20, 50) privacy loss drops from 2.1 to 1.3 and from 10.9 to 3.9 for DBO \u2013 DBOU and DSA \u2013 DSAU respectively. For all these algorithms dealing with different types of privacy, an increase of privacy loss with initial solvers implies a better preservation with extended ones."}, {"heading": "VI. CONCLUSION", "text": "While many approaches have been proposed recently for dealing with privacy in distributed constraint satisfaction and optimization problems, none of them is exempt from limitations. These approaches may require particular properties from the initial problem, or may consider certain aspects of privacy only. In this work, we propose the Utilitarian Distributed Constrained Problem (UDisCP) framework. The framework models the privacy loss for the revelation of an agent\u2019s constraints as a utility function, letting agents integrate privacy requirements directly in their search process. Solving the problem then consists in finding the best compromise between solution quality and privacy loss, instead of focusing only on solution quality. We propose extensions to existing algorithms for DisCSPs (SyncBTU , ABTU ) and DCOPs (DBOU , DSAU , and ADOPTU ) that let agents use information about privacy to modify their behaviour and guide their search process, by proposing values that reduce the amount of privacy loss, and compare them on different types of distributed meeting scheduling problems. The comparison shows that explicit modelling and reasoning with the utility of privacy allows for significant savings in privacy with minimal impact on the quality of the achieved solutions.\nOur approach has several possible extensions. For future works, we first plan to extend our models and algorithms to more general problems modelled with constraints, namely problems with n-ary constraints (not only binary ones) as well as multi-variable problems, where each agent may control several variables. In this case, the solution of an agent is a set of assignments, rather than a single assignment. Further, we want to investigate the notion of ethics [9], implying that agents may have remorse for lying when modifying their behaviour for privacy. These notion of ethics leads to the building of communities of interests between agents [65], [66], where privacy costs will differ according to the recipient of a message, instead of being random. A DisCSP modelling was used for road traffic in [13], [14], and we would like to apply our model of privacy for road traffic simulation where the agents/drivers may choose whether to reveal some private data concerning for example their driving, or their habits. Indeed, we think that our privacy model may also be adapted to other applications such as multi-robot exploration [44] or smart energy [49]."}, {"heading": "ACKNOWLEDGEMENTS", "text": "Authors gratefully acknowledge the reviewers for their constructive remarks, allowing to improve our article. This work was\nrealized while the first author was a visiting researcher at Florida Institute of Technology (Sept. 2015 \u2013 May 2016)."}], "references": [{"title": "Continuous search in constraint programming", "author": ["Alejandro Arbelaez", "Youssef Hamadi", "Mich\u00e8le Sebag"], "venue": "IEEE International Conference on Tools with Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Continuous search in constraint programming", "author": ["Alejandro Arbelaez", "Youssef Hamadi", "Michele Sebag"], "venue": "Autonomous Search,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "On the conversion between non-binary and binary constraint satisfaction problems", "author": ["Fahiem Bacchus", "Peter Van Beek"], "venue": "Proceedings of the Fifteenth National Conference on Artificial Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "JADE - A java agent development framework", "author": ["Fabio Bellifemine", "Federico Bergenti", "Giovanni Caire", "Agostino Poggi"], "venue": "Multi-Agent Programming: Languages, Platforms and Applications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "The complexity of reasoning with global constraints. Constraints", "author": ["Christian Bessiere", "Emmanuel Hebrard", "Brahim Hnich", "Toby Walsh"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Distributed constraint satisfaction with partially known", "author": ["Ismel Brito", "Amnon Meisels", "Pedro Meseguer", "Roie Zivan"], "venue": "constraints. Constraints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Model and algorithm for dynamic multi-objective distributed optimization", "author": ["Maxime Clement", "Tenda Okimoto", "Tony Ribeiro", "Katsumi Inoue"], "venue": "International Conference on Principles and Practice of Multi-Agent Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Ethical judgment of agents\u2019 behaviors in multi-agent systems", "author": ["Nicolas Cointe", "Gr\u00e9gory Bonnet", "Olivier Boissier"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Privacy preservation in a decentralized calendar system", "author": ["Ludivine Cr\u00e9pin", "Yves Demazeau", "Olivier Boissier", "Fran\u00e7ois Jacquenet"], "venue": "7th International Conference on Practical Applications of Agents and Multi-Agent Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Bounded decentralised coordination over multiple objectives", "author": ["Francesco M Delle Fave", "Ruben Stranders", "Alex Rogers", "Nicholas R Jennings"], "venue": "10th International Conference on Autonomous Agents and Multiagent Systems (AAMAS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Topological order planner for pomdps", "author": ["Jilles Steeve Dibangoye", "Guy Shani", "Brahim Chaib-draa", "Abdel-Illah Mouaddib"], "venue": "Proceedings of the 21st International Joint Conference on Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Anticipation based on constraint processing in a multi-agent context", "author": ["Arnaud Doniec", "Ren\u00e9 Mandiau", "Sylvain Piechowiak", "St\u00e9phane Espi\u00e9"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Controlling non-normative behaviors by anticipation for autonomous agents", "author": ["Arnaud Doniec", "Ren\u00e9 Mandiau", "Sylvain Piechowiak", "St\u00e9phane Espi\u00e9"], "venue": "Web Intelligence and Agent Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Distributed private constraint optimization", "author": ["Prashant Doshi", "Toshihiro Matsui", "Marius Silaghi", "Makoto Yokoo", "Markus Zanker"], "venue": "In Proceedings of the 2008 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Privacy guarantees through distributed constraint satisfaction", "author": ["Boi Faltings", "Thomas L\u00e9aut\u00e9", "Adrian Petcu"], "venue": "In Proceedings of the 2008 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Multi-variable agents decomposition for dcops to exploit multi-level parallelism", "author": ["Ferdinando Fioretto", "William Yeoh", "Enrico Pontelli"], "venue": "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Privacy/efficiency tradeoffs in distributed meeting scheduling by constraint-based agents", "author": ["Eugene C Freuder", "Marius Minca", "Richard J Wallace"], "venue": "Workshop on Distributed Constraint Reasoning, August", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Scheduling meetings by agents", "author": ["Amir Gershman", "Alon Grubshtein", "Amnon Meisels", "Lior Rokach", "Roie Zivan"], "venue": "In 10th International Conference . on the Practice and Theory Theory of Automated Timetabling (PATAT", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Asynchronous forward bounding for distributed cops", "author": ["Amir Gershman", "Amnon Meisels", "Roie Zivan"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "SSDPOP: improving the privacy of DCOP with secret sharing", "author": ["Rachel Greenstadt", "Barbara Grosz", "Michael D Smith"], "venue": "Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Analysis of privacy loss in distributed constraint optimization", "author": ["Rachel Greenstadt", "Jonathan P Pearce", "Milind Tambe"], "venue": "In Proceedings, The Twenty- First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference, July 16-20,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "When you say (DCOP) privacy, what do you mean? - categorization of DCOP privacy and insights on internal constraint privacy", "author": ["Tal Grinshpoun"], "venue": "ICAART 2012 - Proceedings of the 4th International Conference on Agents and Artificial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "A privacy-preserving algorithm for distributed constraint optimization", "author": ["Tal Grinshpoun", "Tamir Tassa"], "venue": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "P-syncbb: A privacy preserving branch and bound DCOP algorithm", "author": ["Tal Grinshpoun", "Tamir Tassa"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Backtracking in distributed Constraint Networks", "author": ["Y. Hamadi", "C. Bessiere", "J. Quinqueton"], "venue": "In Proc. of 13th European Conf. on Artificial Intelligence (ECAI),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Distributed partial constraint satisfaction problem", "author": ["Katsutoshi Hirayama", "Makoto Yokoo"], "venue": "Third International Conference,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "The distributed breakout algorithms", "author": ["Katsutoshi Hirayama", "Makoto Yokoo"], "venue": "Artificial Intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Efficient secure multi-party computation", "author": ["Martin Hirt", "Ueli Maurer", "Bartosz Przydatek"], "venue": "Tatsuaki Okamoto, editor, Advances in Cryptology - ASIACRYPT", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Anthony R Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Heuristic search-based replanning", "author": ["Sven Koenig", "David Furcy", "Colin Bauer"], "venue": "Proceedings of the Sixth International Conference on Artificial Intelligence Planning Systems, April", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "Handling privacy as contextual integrity in decentralized virtual communities: The privacias framework", "author": ["Yann Krupa", "Laurent Vercouter"], "venue": "Web Intelligence and Agent Systems: An International Journal,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Protecting privacy through distributed computation in multi-agent decision making", "author": ["Thomas L\u00e9aut\u00e9", "Boi Faltings"], "venue": "Journal Artificial Intelligence Research (JAIR),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Stochastic boolean satisfiability", "author": ["Michael L Littman", "Stephen M Majercik", "Toniann Pitassi"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2001}, {"title": "Privacy loss in distributed constraint reasoning: A quantitative framework for analysis and its applications", "author": ["Rajiv T Maheswaran", "Jonathan P Pearce", "Emma Bowring", "Pradeep Varakantham", "Milind Tambe"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}, {"title": "Valuations of possible states (vps): a quantitative framework for analysis of privacy loss among collaborative personal assistant agents", "author": ["Rajiv T Maheswaran", "Jonathan P Pearce", "Pradeep Varakantham", "Emma Bowring", "Milind Tambe"], "venue": "In 4th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Taking DCOP to the real world: Efficient complete solutions for distributed multi-event scheduling", "author": ["Rajiv T Maheswaran", "Milind Tambe", "Emma Bowring", "Jonathan P Pearce", "Pradeep Varakantham"], "venue": "In 3rd International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}, {"title": "Improving the privacy of the asynchronous partial overlay protocol", "author": ["Roger Mailler"], "venue": "Multiagent and Grid Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Asynchronous partial overlay: A new algorithm for solving distributed constraint satisfaction problems", "author": ["Roger Mailler", "Victor R Lesser"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Multi-variable distributed backtracking with sessions", "author": ["Ren\u00e9 Mandiau", "Julien Vion", "Sylvain Piechowiak", "Pierre Monier"], "venue": "Applied intelligence,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Leximin asymmetric multiple objective DCOP on factor graph", "author": ["Toshihiro Matsui", "Marius Silaghi", "Tenda Okimoto", "Katsutoshi Hirayama", "Makoto Yokoo", "Hiroshi Matsuo"], "venue": "PRIMA 2015: Principles and Practice of Multi-Agent Systems - 18th International Conference,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Adopt: Asynchronous distributed constraint optimization with quality guarantees", "author": ["Pragnesh Jay Modi", "Wei-Min Shen", "Milind Tambe", "Makoto Yokoo"], "venue": "Artificial Intelligence,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2005}, {"title": "State of the art\u2014a survey of partially observable markov decision processes: theory, models, and algorithms", "author": ["George E Monahan"], "venue": "Management Science,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1982}, {"title": "Comparison of DCSP algorithms: A case study for multi-agent exploration", "author": ["Pierre Monier", "Arnaud Doniec", "Sylvain Piechowiak", "Ren\u00e9 Mandiau"], "venue": "Advances on Practical Applications of Agents and Multiagent Systems - 9th International Conference on Practical Applications of Agents and Multiagent Systems,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "SOCIAL DCOP - social choice in distributed constraints optimization", "author": ["Arnon Netzer", "Amnon Meisels"], "venue": "Intelligent Distributed Computing V,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "A scalable method for multiagent constraint optimization", "author": ["Adrian Petcu", "Boi Faltings"], "venue": "Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2005}, {"title": "A multi-agent system for resource privacy: Deployment of ambient applications in smart environments", "author": ["Ferdinand Piette", "Costin Caval", "Amal El Fallah Seghrouchni", "Patrick Taillibert", "C\u00e9dric Dinont"], "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Artificial Intelligence: A modern approach", "author": ["Stuart Russell", "Peter Norvig"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1995}, {"title": "Using message-passing DCOP algorithms to solve energy-efficient smart environment configuration problems", "author": ["Pierre Rust", "Gauthier Picard", "Fano Ramparany"], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Discsps with privacy recast as planning problems for self-interested agents", "author": ["Julien Savaux", "Julien Vion", "Sylvain Piechowiak", "Ren\u00e9 Mandiau", "Toshihiro Matsui", "Katsutoshi Hirayama", "Makoto Yokoo", "Shakre Elmane", "Marius Silaghi"], "venue": "In Web Intelligence (WI),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2016}, {"title": "Privacit\u00e9 dans les discsp pour agents utilitaires", "author": ["Julien Savaux", "Julien Vion", "Sylvain Piechowiak", "Ren\u00e9 Mandiau", "Toshihiro Matsui", "Katsutoshi Hirayama", "Makoto Yokoo", "Shakre Elmane", "Marius Silaghi"], "venue": "JFSMA", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "How to share a secret", "author": ["Adi Shamir"], "venue": "Communications of the ACM,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1979}, {"title": "Distributed asynchronous search with private constraints", "author": ["Marius C Silaghi", "Djamila Sam-Haroud", "Boi Faltings"], "venue": "AGENTS 2000 Proceedings of the fourth international conference on Autonomous agents,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2000}, {"title": "Meeting scheduling guaranteeing n/2-privacy and resistant to statistical analysis (applicable to any DisCSP)", "author": ["Marius-Calin Silaghi"], "venue": "In Proceedings of Web Intelligence,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2004}, {"title": "A comparison of distributed constraint satisfaction techniques with respect to privacy. In Proceedings of the 3rd workshop on distributed constraints reasoning (AAMAS - DCR-02)", "author": ["Marius-Calin Silaghi", "Boi Faltings"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2002}, {"title": "Adopt-ing: unifying asynchronous distributed optimization with asynchronous backtracking", "author": ["Marius-Calin Silaghi", "Makoto Yokoo"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2009}, {"title": "Max-sum goes private", "author": ["Tamir Tassa", "Roie Zivan", "Tal Grinshpoun"], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2015}, {"title": "Constraint-based multi-agent meeting scheduling: Effects of agent heterogeneity on performance and privacy loss. In Proceedings of the 3rd workshop on distributed constraints reasoning (AAMAS - DCR-02)", "author": ["Richard J Wallace", "Eugene C Freuder"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2002}, {"title": "Constraint-based reasoning and privacy/efficiency tradeoffs in multi-agent problem solving", "author": ["Richard J Wallace", "Eugene C Freuder"], "venue": "Artificial Intelligence,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2005}, {"title": "Stochastic constraint programming", "author": ["Toby Walsh"], "venue": "In Proceedings of the 15th European Conference on Artificial Intelligence,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2002}, {"title": "Bnb-adopt: An asynchronous branch-and-bound DCOP algorithm", "author": ["William Yeoh", "Ariel Felner", "Sven Koenig"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2010}, {"title": "The distributed constraint satisfaction problem: Formalization and algorithms", "author": ["Makoto Yokoo", "Edmund H Durfee", "Toru Ishida", "Kazuhiro Kuwabara"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 1998}, {"title": "Distributed breakout algorithm for solving distributed constraint satisfaction problems", "author": ["Makoto Yokoo", "Katsutoshi Hirayama"], "venue": "Proceedings of the Second International Conference on Multi-Agent Systems, December 9-13,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 1996}, {"title": "Distributed constraint satisfaction for formalizing distributed problem solving", "author": ["Makoto Yokoo", "Toru Ishida", "Edmund H Durfee", "Kazuhiro Kuwabara"], "venue": "In Proceedings of the 12th IEEE International Conference on Distributed Computing Systems, June 9-12,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 1992}, {"title": "A multi-agent homophily-based approach for community detection in social networks", "author": ["H\u00e9dia Zardi", "Lotfi Ben Romdhane", "Zahia Guessoum"], "venue": "IEEE International Conference on Tools with Artificial Intelligence,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2014}, {"title": "Efficiently mining community structures in weighted social networks", "author": ["H\u00e9dia Zardi", "Lotfi Ben Romdhane", "Zahia Guessoum"], "venue": "International Journal of Data Mining, Modelling and Management IJDMMM,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2016}, {"title": "Distributed stochastic search for constraint satisfaction and optimization: Parallelism, phase transitions and performance", "author": ["Weixiong Zhang", "Guandong Wang", "Lars Wittenburg"], "venue": "In Proceedings of AAAI Workshop on Probabilistic Approaches in Search,,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2002}, {"title": "Synchronous vs asynchronous search on discsps", "author": ["Roie Zivan", "Amnon Meisels"], "venue": "In Proceedings of the First European Workshop on Multi-Agent Systems (EUMAS),", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2003}], "referenceMentions": [{"referenceID": 20, "context": "Privacy is an important problem in a lot of distributed applications, therefore, the reward for solving the problem should be considered, but also the cost of privacy loss during the process [22].", "startOffset": 191, "endOffset": 195}, {"referenceID": 30, "context": "For example, when users exchange information on social networks [32], they reveal (often unconsciously) personal data (e.", "startOffset": 64, "endOffset": 68}, {"referenceID": 45, "context": ", camera, computer, PDA), to allow or prevent the sharing of resource information [47].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "Indeed, we know that the assignment of time slots can be difficult if participants do not want to reveal their constraints [18], [10].", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "Indeed, we know that the assignment of time slots can be difficult if participants do not want to reveal their constraints [18], [10].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "Such coordinated decisions are in conflict with the need to keep constraints private [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 60, "context": "Thus, agents have to reveal information during search, causing privacy to be a major concern in DisCSPs [62], [26].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "Thus, agents have to reveal information during search, causing privacy to be a major concern in DisCSPs [62], [26].", "startOffset": 110, "endOffset": 114}, {"referenceID": 46, "context": "A common assumption is that utility-based agents associate each state with a utility value [48].", "startOffset": 91, "endOffset": 95}, {"referenceID": 60, "context": "1) Definitions: Let us first remind the definitions of the Distributed Constraint Satisfaction Problem (DisCSP) and of the Distributed Constraint Optimization Problem (DCOP) [62].", "startOffset": 174, "endOffset": 178}, {"referenceID": 58, "context": "The problem that each agent has to solve in DisCP is a Stochastic Constraint Optimization Problem (Stochastic COP) or a Stochastic CSP [60], which are generalizations of Stochastic SAT, SSAT [34], namely where the local problem has to be solved considering its impact on external variables and constraints whose values are not yet known and that are not under the control of this agent, but that of other agents (commonly, lower priority), values of which the agent may know a probabilistic profile.", "startOffset": 135, "endOffset": 139}, {"referenceID": 32, "context": "The problem that each agent has to solve in DisCP is a Stochastic Constraint Optimization Problem (Stochastic COP) or a Stochastic CSP [60], which are generalizations of Stochastic SAT, SSAT [34], namely where the local problem has to be solved considering its impact on external variables and constraints whose values are not yet known and that are not under the control of this agent, but that of other agents (commonly, lower priority), values of which the agent may know a probabilistic profile.", "startOffset": 191, "endOffset": 195}, {"referenceID": 62, "context": "We consider two well-known solvers for DisCSP (SyncBT and ABT ), and three solvers for DCOPs (called ADOPT , DSA and DBO ): a) Synchronous Backtracking (SyncBT): is the baseline algorithm for DisCSPs [64], [68].", "startOffset": 200, "endOffset": 204}, {"referenceID": 66, "context": "We consider two well-known solvers for DisCSP (SyncBT and ABT ), and three solvers for DCOPs (called ADOPT , DSA and DBO ): a) Synchronous Backtracking (SyncBT): is the baseline algorithm for DisCSPs [64], [68].", "startOffset": 206, "endOffset": 210}, {"referenceID": 62, "context": "b) Asynchronous Backtracking (ABT): is a common alternative solver for DisCSPs that allows agents to run concurrently and asynchronously [64].", "startOffset": 137, "endOffset": 141}, {"referenceID": 40, "context": "c) Asynchronous Distributed Optimization (ADOPT): guarantees to find the optimal solution and only needs polynomial space [42], [61], [56].", "startOffset": 122, "endOffset": 126}, {"referenceID": 59, "context": "c) Asynchronous Distributed Optimization (ADOPT): guarantees to find the optimal solution and only needs polynomial space [42], [61], [56].", "startOffset": 128, "endOffset": 132}, {"referenceID": 54, "context": "c) Asynchronous Distributed Optimization (ADOPT): guarantees to find the optimal solution and only needs polynomial space [42], [61], [56].", "startOffset": 134, "endOffset": 138}, {"referenceID": 37, "context": "Note that other approaches based on a graph re-arrangement have been explored, for example a cluster exploitation like Asynchronous Partial Overlay [39], [38].", "startOffset": 148, "endOffset": 152}, {"referenceID": 36, "context": "Note that other approaches based on a graph re-arrangement have been explored, for example a cluster exploitation like Asynchronous Partial Overlay [39], [38].", "startOffset": 154, "endOffset": 158}, {"referenceID": 65, "context": "d) Distributed Stochastic Algorithm (DSA): makes agents start their process by randomly selecting a value [67].", "startOffset": 106, "endOffset": 110}, {"referenceID": 61, "context": "e) Distributed Breakout (DBO): is an iterative improvement solver for DCOPs [63], [28].", "startOffset": 76, "endOffset": 80}, {"referenceID": 26, "context": "e) Distributed Breakout (DBO): is an iterative improvement solver for DCOPs [63], [28].", "startOffset": 82, "endOffset": 86}, {"referenceID": 52, "context": "Some cryptographic approaches offer certain end-to-end security guarantees by integrating the entire solving process in one primitive for DisCSPs or for DCOPs, the highest level of such privacy guarantees being achievable only for problems with a single variable [54].", "startOffset": 263, "endOffset": 267}, {"referenceID": 55, "context": "Other cryptographic approaches are hybrids interlacing cryptographic and artificial intelligence steps [57], [21], [24].", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "Other cryptographic approaches are hybrids interlacing cryptographic and artificial intelligence steps [57], [21], [24].", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "Other cryptographic approaches are hybrids interlacing cryptographic and artificial intelligence steps [57], [21], [24].", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "While ensuring privacy in each primitive [29], cryptographic techniques are usually slower, and sometimes require the use of external servers or computationally intensive secure function evaluation techniques that may not always be available or justifiable for their benefits, making them impractical [22], or lacking clear global security guarantees.", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "While ensuring privacy in each primitive [29], cryptographic techniques are usually slower, and sometimes require the use of external servers or computationally intensive secure function evaluation techniques that may not always be available or justifiable for their benefits, making them impractical [22], or lacking clear global security guarantees.", "startOffset": 301, "endOffset": 305}, {"referenceID": 44, "context": "A couple of such approaches with which we compare in more detail are: a) Distributed Pseudo-tree Optimization Procedure with Secret Sharing (SS-DPOP): modifies the standard DPOP algorithm [46] to protect leaves in the depth first search tree, where agents sharing constraints are on the same branch [21].", "startOffset": 188, "endOffset": 192}, {"referenceID": 19, "context": "A couple of such approaches with which we compare in more detail are: a) Distributed Pseudo-tree Optimization Procedure with Secret Sharing (SS-DPOP): modifies the standard DPOP algorithm [46] to protect leaves in the depth first search tree, where agents sharing constraints are on the same branch [21].", "startOffset": 299, "endOffset": 303}, {"referenceID": 50, "context": "SS-DPOP uses secret sharing [52] to aggregate the results of a single solution, without revealing the individual valuations this solution consists in.", "startOffset": 28, "endOffset": 32}, {"referenceID": 31, "context": "[33] has also extended DPOP to preserve different types of privacy using secure multi-party computation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "b) Privacy-Preserving Synchronous Branch and Bound (P-SyncBB): is a cryptographic version of SyncBB [27], [25] for solving DCOPs while enforcing a stronger degree of constraint privacy [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "b) Privacy-Preserving Synchronous Branch and Bound (P-SyncBB): is a cryptographic version of SyncBB [27], [25] for solving DCOPs while enforcing a stronger degree of constraint privacy [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "b) Privacy-Preserving Synchronous Branch and Bound (P-SyncBB): is a cryptographic version of SyncBB [27], [25] for solving DCOPs while enforcing a stronger degree of constraint privacy [24].", "startOffset": 185, "endOffset": 189}, {"referenceID": 22, "context": "Some protocols were proposed in [24] that can solve problems without resorting to costly transfer sub-protocols, and compare the cost of a CPA shared between two agents to the upper bound held by only one of them.", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "Some variations on the standard SyncBB include NCBB and AFB [20], [24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 22, "context": "Some variations on the standard SyncBB include NCBB and AFB [20], [24].", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "According to [23], agents privacy may concern the four following aspects:", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "In the original DisCSP approach a form of privacy of domains is implicit (see ABT ), while being formally required in its PKC extension [6].", "startOffset": 136, "endOffset": 139}, {"referenceID": 51, "context": "Constraint privacy: Agents want to keep the information related to their constraints private [53].", "startOffset": 93, "endOffset": 97}, {"referenceID": 53, "context": "This can be achieved by keeping the message structure and contracts of certain existing DisCSP solvers to be used as communication protocols rather than algorithms, as introduced in [55], where protocols obtained in such ways are compared with respect to the flexibility offered for agents to hide their secrets.", "startOffset": 182, "endOffset": 186}, {"referenceID": 16, "context": "b) Distributed Private Constraint Satisfaction Problem (DisPrivCSP): models the privacy loss for individual revelations [18], [55].", "startOffset": 120, "endOffset": 124}, {"referenceID": 53, "context": "b) Distributed Private Constraint Satisfaction Problem (DisPrivCSP): models the privacy loss for individual revelations [18], [55].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "Privacy and the usual optimization criteria of Distributed Constraint Optimization Problems are merged into a unique criterion [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 56, "context": "c) Valuation of Possible States (VPS): measures privacy loss by the extent to which the possible states of other agents are reduced [58], [36], [35], [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 34, "context": "c) Valuation of Possible States (VPS): measures privacy loss by the extent to which the possible states of other agents are reduced [58], [36], [35], [22].", "startOffset": 138, "endOffset": 142}, {"referenceID": 33, "context": "c) Valuation of Possible States (VPS): measures privacy loss by the extent to which the possible states of other agents are reduced [58], [36], [35], [22].", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "c) Valuation of Possible States (VPS): measures privacy loss by the extent to which the possible states of other agents are reduced [58], [36], [35], [22].", "startOffset": 150, "endOffset": 154}, {"referenceID": 5, "context": "d) Partially Known Constraints (PKC): uses entropy, as defined in information theory, to quantify privacy loss [6].", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "PKC assumes that agents only know their own individual unavailabilities [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 29, "context": "It can be noticed that the rewards and costs in our problem are similar to the utilities commonly used by planning algorithms [31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 49, "context": "To do so, we ground the theory of our interpretation of privacy in the well-principled theory of utility-based agents [51].", "startOffset": 118, "endOffset": 122}, {"referenceID": 46, "context": "A policy is a function that associates each state with an action that should be performed in it [48].", "startOffset": 96, "endOffset": 100}, {"referenceID": 62, "context": "SyncBTU and ABTU are obtained by similar modifications of SyncBT and ABT [64], [62], [68], respectively.", "startOffset": 73, "endOffset": 77}, {"referenceID": 60, "context": "SyncBTU and ABTU are obtained by similar modifications of SyncBT and ABT [64], [62], [68], respectively.", "startOffset": 79, "endOffset": 83}, {"referenceID": 66, "context": "SyncBTU and ABTU are obtained by similar modifications of SyncBT and ABT [64], [62], [68], respectively.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "Learning from previous experience has been extensively studied [1], [2].", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Learning from previous experience has been extensively studied [1], [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 62, "context": "Since [64] does not provide pseudo-code for SyncBT , we modify the pseudo-code presented in [68] for SyncBTU : assignCPA (before Line 7), and backtrack (before Line 6).", "startOffset": 6, "endOffset": 10}, {"referenceID": 66, "context": "Since [64] does not provide pseudo-code for SyncBT , we modify the pseudo-code presented in [68] for SyncBTU : assignCPA (before Line 7), and backtrack (before Line 6).", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "Algorithm 2: estimateCostDisCSP Input: agreementProb, D \u2032 self , probD Output: estimatedCost 1 valueId = j | (Dself [j] = D \u2032 self [1]); 2 if (|D \u2032 self | = 1) then 3 return ( \u2211j=valueId j=1 ud(self,j)) \u00d7probD; 4 else 5 v \u2190 D \u2032 self [1] ; 6 costRound\u2190 estimateCostDisCSP (agreementProb, {v}, agreementProb\u00d7 probD); 7 costT emp \u2190 estimateCostDisCSP (agreementProb, D \u2032 self \\ {v}, (1\u2212 agreementProb) \u00d7 probD); 8 estimatedCost \u2190 costRound+ costT emp; 9 return estimatedCost;", "startOffset": 131, "endOffset": 134}, {"referenceID": 0, "context": "Algorithm 2: estimateCostDisCSP Input: agreementProb, D \u2032 self , probD Output: estimatedCost 1 valueId = j | (Dself [j] = D \u2032 self [1]); 2 if (|D \u2032 self | = 1) then 3 return ( \u2211j=valueId j=1 ud(self,j)) \u00d7probD; 4 else 5 v \u2190 D \u2032 self [1] ; 6 costRound\u2190 estimateCostDisCSP (agreementProb, {v}, agreementProb\u00d7 probD); 7 costT emp \u2190 estimateCostDisCSP (agreementProb, D \u2032 self \\ {v}, (1\u2212 agreementProb) \u00d7 probD); 8 estimatedCost \u2190 costRound+ costT emp; 9 return estimatedCost;", "startOffset": 233, "endOffset": 236}, {"referenceID": 2, "context": "This is due to the fact that any problem with private inter-agent constraints, is equivalent with its dual representation where each constraint becomes a variable [3].", "startOffset": 163, "endOffset": 166}, {"referenceID": 35, "context": "a) Multi-Objective Distributed Constraint Optimization Problem: A multi-objective optimization problem (MOOP) [37], [11] is defined as the problem of simultaneously maximizing k objective functions that have no common measure, defined over a set of variables, each one taking its value in a given domain.", "startOffset": 110, "endOffset": 114}, {"referenceID": 9, "context": "a) Multi-Objective Distributed Constraint Optimization Problem: A multi-objective optimization problem (MOOP) [37], [11] is defined as the problem of simultaneously maximizing k objective functions that have no common measure, defined over a set of variables, each one taking its value in a given domain.", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "A Multi-Objective DCOP [8], called MO-DCOP, is an extension of the standard mono-objective DCOPs.", "startOffset": 23, "endOffset": 26}, {"referenceID": 43, "context": "The vectors can be compared using various criteria, such as leximin, maximin, social welfare or Theil index [45], [41].", "startOffset": 108, "endOffset": 112}, {"referenceID": 39, "context": "The vectors can be compared using various criteria, such as leximin, maximin, social welfare or Theil index [45], [41].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "state0 1 1 3 1 1 3 SolutionCost 70 120 230 70 120 230 PrivacyCost 80 100 10 80 100 10 evaluation 150 220 240 [80, 70] [100, 120] [10, 230] believed next state", "startOffset": 129, "endOffset": 138}, {"referenceID": 18, "context": "considered state 2 3 1 2 3 1 SolutionCost 150 155 135 230 190 40 PrivacyCost 100 110 90 20 10 80 evaluation 250 265 225* [20, 230]* [10, 190]* [80, 40] achieved next state", "startOffset": 121, "endOffset": 130}, {"referenceID": 8, "context": "considered state 2 3 1 2 3 1 SolutionCost 150 155 135 230 190 40 PrivacyCost 100 110 90 20 10 80 evaluation 250 265 225* [20, 230]* [10, 190]* [80, 40] achieved next state", "startOffset": 132, "endOffset": 141}, {"referenceID": 38, "context": "considered state 2 3 1 2 3 1 SolutionCost 150 155 135 230 190 40 PrivacyCost 100 110 90 20 10 80 evaluation 250 265 225* [20, 230]* [10, 190]* [80, 40] achieved next state", "startOffset": 143, "endOffset": 151}, {"referenceID": 8, "context": "state1 1 1 1 2 3 3 SolutionCost 70 120 40 230 190 230 PrivacyCost 80 100 90 100 110 10 evaluation 150 220 130 [100, 230] [110, 190] [10, 230]", "startOffset": 132, "endOffset": 141}, {"referenceID": 15, "context": "Nevertheless some algorithms can exploit these underlying structures for efficiency, and this has been the subject of extensive research [17], [40].", "startOffset": 137, "endOffset": 141}, {"referenceID": 38, "context": "Nevertheless some algorithms can exploit these underlying structures for efficiency, and this has been the subject of extensive research [17], [40].", "startOffset": 143, "endOffset": 147}, {"referenceID": 41, "context": "Given ways to approximate observation and transition conditional probabilities, these problems could be reduced to POMDPs [43], [30], [12].", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "Given ways to approximate observation and transition conditional probabilities, these problems could be reduced to POMDPs [43], [30], [12].", "startOffset": 128, "endOffset": 132}, {"referenceID": 10, "context": "Given ways to approximate observation and transition conditional probabilities, these problems could be reduced to POMDPs [43], [30], [12].", "startOffset": 134, "endOffset": 138}, {"referenceID": 48, "context": "For UDisCP, the corresponding POMDP is defined by the tuple \u3008S,A, T,R,\u03a9, O, \u03b3\u3009 with components [50]:", "startOffset": 95, "endOffset": 99}, {"referenceID": 35, "context": "We evaluate our framework and algorithms on randomly generated instances of distributed meeting scheduling problems (DMS) [37], [5], [19].", "startOffset": 122, "endOffset": 126}, {"referenceID": 4, "context": "We evaluate our framework and algorithms on randomly generated instances of distributed meeting scheduling problems (DMS) [37], [5], [19].", "startOffset": 128, "endOffset": 131}, {"referenceID": 17, "context": "We evaluate our framework and algorithms on randomly generated instances of distributed meeting scheduling problems (DMS) [37], [5], [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 57, "context": "Existing studies has already addressed the question of privacy in DMS problems by considering the information on whether an agent can attend a meeting to be private [59], [7].", "startOffset": 165, "endOffset": 169}, {"referenceID": 3, "context": "3) to build the multi-agent system [4].", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "Further, we want to investigate the notion of ethics [9], implying that agents may have remorse for lying when modifying their behaviour for privacy.", "startOffset": 53, "endOffset": 56}, {"referenceID": 63, "context": "These notion of ethics leads to the building of communities of interests between agents [65], [66], where privacy costs will differ according to the recipient of a message, instead of being random.", "startOffset": 88, "endOffset": 92}, {"referenceID": 64, "context": "These notion of ethics leads to the building of communities of interests between agents [65], [66], where privacy costs will differ according to the recipient of a message, instead of being random.", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "A DisCSP modelling was used for road traffic in [13], [14], and we would like to apply our model of privacy for road traffic simulation where the agents/drivers may choose whether to reveal some private data concerning for example their driving, or their habits.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "A DisCSP modelling was used for road traffic in [13], [14], and we would like to apply our model of privacy for road traffic simulation where the agents/drivers may choose whether to reveal some private data concerning for example their driving, or their habits.", "startOffset": 54, "endOffset": 58}, {"referenceID": 42, "context": "Indeed, we think that our privacy model may also be adapted to other applications such as multi-robot exploration [44] or smart energy [49].", "startOffset": 114, "endOffset": 118}, {"referenceID": 47, "context": "Indeed, we think that our privacy model may also be adapted to other applications such as multi-robot exploration [44] or smart energy [49].", "startOffset": 135, "endOffset": 139}], "year": 2017, "abstractText": "Privacy has traditionally been a major motivation for distributed problem solving. Distributed Constraint Satisfaction Problem (DisCSP) as well as Distributed Constraint Optimization Problem (DCOP) are fundamental models used to solve various families of distributed problems. Even though several approaches have been proposed to quantify and preserve privacy in such problems, none of them is exempt from limitations. Here we approach the problem by assuming that computation is performed among utilitarian agents. We introduce a utilitarian approach where the utility of each state is estimated as the difference between the reward for reaching an agreement on assignments of shared variables and the cost of privacy loss. We investigate extensions to solvers where agents integrate the utility function to guide their search and decide which action to perform, defining thereby their policy. We show that these extended solvers succeed in significantly reducing privacy loss without significant degradation of the solution quality.", "creator": "LaTeX with hyperref package"}}}