{"id": "1703.07076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules", "abstract": "Simplified telugu-speaking Molecular 54kg Input helali Line variables Entry bierzwnik System (sadikin SMILES) coleraine is a castafiore single popchanka line text representation 1-pete of dandar a crescenzo unique molecule. One molecule uscaa can steffanie however have multiple mariavite SMILES bochtler strings, firefox which is a strang reason alanna that canonical palmitate SMILES have been 2,716 defined, enami which ensures a harbhajan one apta to one anti-imperialist correspondence tsahi between SMILES string slinging and molecule. slither Here the waghmare fact that allanon multiple SMILES represent bedeutung the same betterman molecule is synonymized explored vasanthan as a 25,000-strong technique easygroup for angua data attend augmentation fasciolariidae of o-town a 18:14 molecular hatillo QSAR dataset canada-u.s. modeled kurn by a dedalus long short term ferber memory (norio LSTM) gela cell based keth neural network. mabruk The honeymooner augmented dataset populace was military-backed 130 times bigger dahlmann than straxus the log-in original. The giovanella network trained with the in\u00eas augmented dataset picou shows gallieni better performance luneng on a xanthi test set when compared venomous to a model built 19-inch with only draglines one reregistration canonical kyokutenzan SMILES string per molecule. grandview The correlation bandiagara coefficient cussing R2 rodewald on breastroke the dreamcast test pompidou set middlebrow was shepherds improved 25-yard from cspgs 0. macalla 56 ghassemlou to 0. adventuress 66 200px when aganist using SMILES politicans enumeration, and sisyphean the root phantoms mean prijepolje square error (RMS) horine likewise fell mongoose from droney 0. 62 to 0. temperate 55. gultom The technique also medius works manhattan-bound in the prediction inducements phase. By taking the musaed average per molecule of rakowski the predictions for hostages the enumerated rmk SMILES a further groby improvement to a correlation crassus coefficient toqueville of 0. 68 sterritt and xavin a hitsville RMS kormiltsev of 0. 52 calta was correy found.", "histories": [["v1", "Tue, 21 Mar 2017 07:13:13 GMT  (365kb)", "http://arxiv.org/abs/1703.07076v1", null], ["v2", "Wed, 17 May 2017 11:24:43 GMT  (382kb,D)", "http://arxiv.org/abs/1703.07076v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["esben jannik bjerrum"], "accepted": false, "id": "1703.07076"}, "pdf": {"name": "1703.07076.pdf", "metadata": {"source": "CRF", "title": "SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules", "authors": ["E. J. Bjerrum"], "emails": ["esben@wildcardconsulting.dk"], "sections": [{"heading": null, "text": ""}, {"heading": "Introduction", "text": "Neural networks and deep learning has shown interesting application successes, such as image classification1, and speech recognition2. One of the issues that limits their general applicability in the QSAR domain may be the limited sizes of the labeled datasets available, although successes do appear3. Limited datasets necessitates harsh regularization or shallow and narrow architectures. Within image analysis and classification, data augmentation techniques has been used with excellent results4-7. As an example, a dataset of labeled images can be enlarged by operations such as mirroring, rotation, morphing and zooming. The afterwards trained network gets more robust towards such variations and the neural network can recognize the same object in different versions. Neural networks has also been used on molecular data, where the input may be calculated descriptors3, neural network interpretation of the molecular graph8 or also SMILES representations.9 Simplified Molecular Input Line Entry System (SMILES) is a single line text based molecular notation format.10 A single molecule has multiple possible SMILES strings, which has led to the definition of a canonical SMILES,11 which ensures that a molecule corresponds to a single SMILES string. The possibilities for variation in the SMILES strings of simple molecules are limited. Propane has two possibilities CCC and C(C)C. But as the molecule gets larger in size and more complex in branching, the number of possible SMILES strings grows rapidly. Toluene with seven atoms, has seven possible SMILES strings (Figure 1).\nHere data augmentation of molecular structures with SMILES enumeration for QSAR studies will be investigated using long short term memory (LSTM) cell neural networks inspired by networks used for Twitter tweets sentiment analysis.12"}, {"heading": "Methods", "text": ""}, {"heading": "SMILES enumeration", "text": "SMILES enumeration was done with a Python script utilizing the cheminformatics library RDKit.13 The atom ordering of the molecule is scrambled randomly by converting to molfile format14 and changing the atom order, before converting back to the RDKit mol format. A SMILES is then generated using RDKit with the default option of producing canonical SMILES set to false, where different atom orderings lead to different SMILES. The SMILES strings is then compared and possible added to a growing set of unique SMILES strings. The process is repeated a predefined number of times. The python functions are available on github: https://github.com/Ebjerrum/SMILES-enumeration"}, {"heading": "Molecular dataset", "text": "The dataset was obtained from Sutherland et al 2003.15 It consists of 756 dihydrofolate inhibitors with P. carinii DHFR inhibition data. The dataset was split in test and a training set in a 1:9 ratio. It was expanded with SMILES enumeration and the SMILES strings were padded with spaces to fixed length of 74, which is one characters longer than the longest SMILES in the dataset. It was subsequently vectorized by one-hot encoding the characters into a bit matrix with one bit set for the corresponding character in each row using a generated char to int dictionary. Molecules where the associated affinity was not a number were removed. The associated IC50 data was converted to log IC50 and normalized to unit variance and mean zero with utilities from Scikit-learn.16"}, {"heading": "LSTM neural network", "text": "Two different neural networks were built and trained using Keras version 1.1.217 with Theano v. 0.8.0 18 as back end. One or more LSTM layers were used in batch mode, and the final state fed to a feedforward neural network with a single linear output neuron. The network layout was optimized using Bayesian optimization with Gaussian processes as implemented in the Python package GpyOpt19 version 1.0.3, varying the hyper parameters listed in Table 1. 10 initial trainings was done before using the GP_MCMC and the EI_MCMC acquisition function to sample new hyper parameter sets.20 One network was optimized and trained only using a dataset with canonical SMILES, whereas the other were optimized and trained with the dataset that expanded with SMILES enumeration. In the rest of the publication they will be referred to as the canonical model and enumerated model, respectively.\nTable 1: Hyper parameter Search Space Search Space Type\nNumber of LSTM layers [1,2] Discrete\nNumber of units in LSTM layers [32, 64, 128, 256] Discrete\nDropout for input gates (dropout_W) 0 \u2013 0.2 Continuous\nDropout for recurrent connection (dropout_U) 0 \u2013 0.5 Continuous\nNumber of dense hidden layers [0,1] Discrete\nHidden layer size [4, 8, 16, 32, 64, 128] Discrete\nWeight regularization on dense layer, L1 0 \u2013 0.2 Continuous\nWeight regularization on dense layer, L2 0 \u2013 0.2 Continuous\nLearning rate 0.05-0.0001 Continuous\nAll computations and training were done on a Linux workstation (Ubuntu Mate 16.04) with 4 GB of ram, i5-2405S CPU @ 2.50GHz and an Nvidia Geforce GTX1060 graphics card with 6 GB of ram."}, {"heading": "Results", "text": "Filtering, splitting and SMILES enumeration resulted in a canonical SMILES dataset with 602 train molecules and 71 test molecules, whereas the enumerated dataset had 79143 and 9412 rows for train and test, respectively. This corresponds to an augmentation factor of approximately 130. Each molecule had on average 130 alternative SMILES representations.\nOptimization of the architecture yielded two different best configurations of hyper parameters, depending on the dataset used. The best hyper parameters found for each dataset are shown in Table 2.\nThe train history is shown in Figure 2. The best neural network trained on the canonical dataset had a loss of 0.44 including regularization penalty and a mean square error of 0.22 and 0.41 for train and test set, respectively. The curves for the training using the canonical dataset are very noisy (Figure 2A). The best neural network trained on the enumerated dataset loss of 0.18 including regularization penalty and a mean square error of 0.09 and 0.30 for train and test set, respectively. The training curve is significantly less noisy than for the canonical dataset (Figure 2B).\nBoth neural networks were used to predict the IC50 values from the canonical and enumerated datasets, and the scatter plots are shown in Figure 3. The correlation coefficients and root mean square deviation (RMS) are tabulated in Table 3. The combination with the worst performance was predicting the test set molecules is using enumerated SMILES neural network model trained on the canonical dataset. Which\nhas a correlation coefficient of 0.26 and an RMS of 0.84. The bad correlation is clearly visible from Figure 3 plot C. The best performance predicting the test set, was seen with the combination of the enumerated model and the enumerated SMILES. Here the correlation coefficient is 0.66 and the RMS 0.55. The two other combinations, canonical model-canonical SMILES and enumerated model, canonical SMILES are close in performance (Table 3).\nFigure 4 show a scatter plot of the average prediction for each molecule obtained with the enumerated model. The calculated correlation coefficient is 0.68 for the test set and the RMS is 0.52."}, {"heading": "Discussion", "text": "The results clearly suggest that SMILES enumeration as a data augmentation technique for molecular data has benefits. The model trained on canonical data is not able to predict many of the alternative SMILES of the train and test set as is evident for Figure 3 plot C, where the bad generalization to non canonical SMILES strings are evident. Instead the best performance was observed by taking the average for each molecule of the predictions of the enumerated SMILES using the enumerated model (Figure 4), which shows that the SMILES enumeration can also be of value in the sampling phase.\nThe canonical model needed a lot more epochs to train, but here it must be considered that the dataset contained 130 times fewer examples. Thus each epoch in the training was only 3 mini batches leading to 3 updates of the weights, whereas the enumerated dataset had approximately 360 updates of the weights of the neural network per epoch. The curves in Figure 2 thus represents 3000 and 18000 updates of the weights. The higher overhead of running more epochs however led to approximately the same wall clock time in training.\nThe hyper parameters found during the optimization of the network architecture and amount of regularization was not entirely as expected. The expectation was that the canonical dataset would prefer a smaller and simpler network with a larger regularization. Instead the canonical dataset has a larger amount of LSTM cells (128) with no dropout, but a much larger regularization of the final weights to the input neuron (L1 and L2 maxed out at 0.2). The enumerated model had fewer LSTM cells (64) and thus fewer connections, but nevertheless found dropout on the input to the LSTM cells to be beneficial. To test if the differences were due to the Bayesian optimization getting trapped in a local minimum, the network architecture found for the enumerated dataset was test trained with the dataset with the canonical SMILES only. The first try with a learning rate of 0.005 failed (results not shown), but lowering the learning rate to the one found for the canonical SMILES (0.0001), gave a model with a correlation coefficient of 0.5 and RMS of 0.68 on the train set. The predictive performance was even lower with 0.45 and 0.69, for R2 and RMS respectively. The differences in hyper parameters after optimization of using the two different datasets thus seems justified.\nThe study lacks the division into train, test and validation set, where the hyper parameters are tuned on the test set, but the final performance evaluated on the validation set. The observed prediction performance of the LSTM-QSAR models are thus likely overestimated to some degree. However, this study is focused on the gains of using SMILES enumeration and not on producing the optimal DHFR QSAR model. The performance on both the train and test set are lower for the canonical model. If the differences in performance had been due to to over-fitting, the smaller dataset would probably have had an advantage.\nThe use of SMILES as descriptors for QSAR is not new, as at least one example of usage in toxicological QSAR models exists21 and is implemented in the CORAL software.22 The approach is however very different from the one in this study. CORAL software breaks down the SMILES into single atoms, double atoms and triple atoms (Sk, SSk and SSSk) as well as some extra manually coded extracted features such as BOND, NOSP, HALO and PAIR.21,22 The approach seems close to using a mixture of topological torsions23 with one, two and three atoms and atom-pair24 fingerprints. The LSTM-QSAR used in this approach directly uses the SMILES string and supposedly let the model best extract the features from the SMILES strings that best fit with the task at hand. A 1:1 benchmark of the approaches on multiple datasets could be an interesting future study.\nSMILES were also used recently in an application of a neural network based auto-encoder.9 Here the SMILES are used as input to a neural network with the task of recreating the input sequence. The information is passed through a \u201cbottle-neck\u201d layer in between the encoder and the decoder, which limits the direct transfer of information. The bottle neck layer thus ends up as a more continuous floating point vector representation of the molecule, which can be used to explore the chemical space near an input molecule, interpolate between molecules and link the vector representation to physicochemical properties. The amount of unlabeled molecules for the study already surpassed the needed amount, but could in principle be expanded even more with the SMILES enumeration technique described here. SMILES enumeration could possible allow the autoencoder to be trained with smaller and more focused datasets of biological interest. Additionally, tt would be interesting to see if different\nSMILES of the same molecule ends up with the same vector representation or in entirely different areas in the continuous molecular representations.\nLSTM networks have also been used in QSAR applications demonstrating learning transfer from large datasets to smaller.25 Here the input was however not SMILES strings but rather molecular graph convolution layers26 working directly on the molecular graph representation. The approach thus more directly reads in the topology of the molecular model, rather than indirectly letting the network infer the topology from the SMILES branching and ring closures defined by the brackets and numbering in the SMILES strings."}, {"heading": "Conclusion", "text": "This short investigation has shown promise in using SMILES enumeration as a data augmentation technique for neural network QSAR models based on SMILES data. SMILES enumeration enables the use of more limited sizes of labeled data sets for use in modeling by more complex neural network models. SMILES enumeration gives more robust QSAR models both when predicting single SMILES, but even more when taking the average prediction using enumerated SMILES for the same molecule.\nThe SMILES enumeration code as well as some of the scripts used for generating the LSTM-QSAR models are available on GitHub: https://github.com/Ebjerrum/SMILES-enumeration"}, {"heading": "Conflicts of Interest", "text": "E. J. Bjerrum is the owner of Wildcard Pharmaceutical Consulting. The company is usually contracted by biotechnology/pharmaceutical companies to provide third party services"}], "references": [{"title": "Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis.", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": null, "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine 2012,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "DeepTox: toxicity prediction using deep learning", "author": ["A. Mayr", "G. Klambauer", "T. Unterthiner", "S. Hochreiter"], "venue": "Frontiers in Environmental Science 2016,", "citeRegEx": "Mayr et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mayr et al\\.", "year": 2016}, {"title": "Return of the devil in the details: Delving deep into convolutional nets arXiv preprint arXiv:1405.3531", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": null, "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "Data Augmentation for Deep Neural Network", "author": ["X. Cui", "V. Goel", "B. Kingsbury"], "venue": "Acoustic Modeling IEEE/ACM Trans. Audio, Speech and Lang. Proc. 2015,", "citeRegEx": "Cui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural networks 2015,", "citeRegEx": "Schmidhuber,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber", "year": 2015}, {"title": "Review of advances in neural networks: Neural design technology stack", "author": ["A.-D. Alm\u00e1si", "S. Wo\u017aniak", "V. Cristea", "Y. Leblebici", "T. Engbersen"], "venue": "Neurocomputing 2016,", "citeRegEx": "Alm\u00e1si et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alm\u00e1si et al\\.", "year": 2016}, {"title": "Molecular graph convolutions: moving beyond fingerprints", "author": ["S. Kearnes", "K. McCloskey", "M. Berndl", "V. Pande", "P. Riley"], "venue": "Journal of computer-aided molecular design 2016,", "citeRegEx": "Kearnes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kearnes et al\\.", "year": 2016}, {"title": "Automatic chemical design using a data-driven continuous representation of molecules arXiv preprint arXiv:1610.02415", "author": ["R. G\u00f3mez-Bombarelli", "D. Duvenaud", "J.M. Hern\u00e1ndez-Lobato", "J. Aguilera-Iparraguirre", "T.D. Hirzel", "R.P. Adams", "A. Aspuru-Guzik"], "venue": null, "citeRegEx": "G\u00f3mez.Bombarelli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "G\u00f3mez.Bombarelli et al\\.", "year": 2016}, {"title": "Towards a Universal SMILES representation - A standard method to generate canonical SMILES based on the InChI", "author": ["N.M. O'Boyle"], "venue": "Journal of cheminformatics 2012,", "citeRegEx": "O.Boyle,? \\Q2012\\E", "shortCiteRegEx": "O.Boyle", "year": 2012}, {"title": "Target-dependent sentiment classification with long short term memory CoRR, abs/1512.01100", "author": ["D. Tang", "B. Qin", "X. Feng", "T. Liu"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "RDKit: Open-Source Cheminformatics Software", "author": ["G.A. Landrum"], "venue": "http://www.rdkit.org/, https://github.com/rdkit/rdkit (accessed Dec 22,", "citeRegEx": "Landrum,? \\Q2016\\E", "shortCiteRegEx": "Landrum", "year": 2016}, {"title": "Spline-fitting with a genetic algorithm: a method for developing classification structure-activity relationships", "author": ["J.J. Sutherland", "L.A. O'Brien", "D.F. Weaver"], "venue": "Journal of chemical information and computer sciences 2003,", "citeRegEx": "Sutherland et al\\.,? \\Q1915\\E", "shortCiteRegEx": "Sutherland et al\\.", "year": 1915}, {"title": "A Python framework for fast computation of mathematical expressions arXiv e-prints 2016", "author": ["S.R. Subramanyam", "J. Sygnowski", "J. Tanguay", "G. van Tulder", "J. Turian", "S. Urban", "P. Vincent", "F. Visin", "H. de Vries", "D. Warde-Farley", "D.J. Webb", "M. Willson", "K. Xu", "L. Xue", "L. Yao", "S. Zhang", "Y. Theano Zhang"], "venue": null, "citeRegEx": "Subramanyam et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Subramanyam et al\\.", "year": 2016}, {"title": "GPyOpt: A Bayesian Optimization framework in Python 2016", "author": ["G. T"], "venue": null, "citeRegEx": "T.,? \\Q2016\\E", "shortCiteRegEx": "T.", "year": 2016}, {"title": "MCMC methods for Gaussian process models using fast approximations for the likelihood arXiv preprint arXiv:1305.2235", "author": ["C. Wang", "R.M. Neal"], "venue": null, "citeRegEx": "Wang and Neal,? \\Q2013\\E", "shortCiteRegEx": "Wang and Neal", "year": 2013}, {"title": "Large-scale QSAR study of aromatase inhibitors using SMILES-based descriptors Chemometrics and Intelligent Laboratory Systems", "author": ["A. Worachartcheewan", "P. Mandi", "V. Prachayasittikul", "A.P. Toropova", "A.A. Toropov", "C. Nantasenamat"], "venue": null, "citeRegEx": "Worachartcheewan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Worachartcheewan et al\\.", "year": 2014}, {"title": "CORAL software: prediction of carcinogenicity of drugs by means of the Monte Carlo method", "author": ["A.P. Toropova", "A.A. Toropov"], "venue": "European Journal of Pharmaceutical Sciences 2014,", "citeRegEx": "Toropova and Toropov,? \\Q2014\\E", "shortCiteRegEx": "Toropova and Toropov", "year": 2014}, {"title": "Topological torsion: a new molecular descriptor for SAR applications. Comparison with other descriptors", "author": ["R. Nilakantan", "N. Bauman", "J.S. Dixon", "R. Venkataraghavan"], "venue": "Journal of Chemical Information and Computer Sciences", "citeRegEx": "Nilakantan et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Nilakantan et al\\.", "year": 1987}, {"title": "Atom pairs as molecular features in structureactivity studies: definition and applications", "author": ["R.E. Carhart", "D.H. Smith", "R. Venkataraghavan"], "venue": "Journal of Chemical Information and Computer Sciences 1985,", "citeRegEx": "Carhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Carhart et al\\.", "year": 1985}, {"title": "Low Data Drug Discovery with Oneshot Learning arXiv preprint arXiv:1611.03199", "author": ["H. Altae-Tran", "B. Ramsundar", "A.S. Pappu", "V. Pande"], "venue": null, "citeRegEx": "Altae.Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Altae.Tran et al\\.", "year": 2016}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["D.K. Duvenaud", "D. Maclaurin", "J. Iparraguirre", "R. Bombarell", "T. Hirzel", "A. Aspuru-Guzik", "R.P. Adams"], "venue": null, "citeRegEx": "Duvenaud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Simplified Molecular Input Line Entry System (SMILES) is a single line text representation of a unique molecule. One molecule can however have multiple SMILES strings, which is a reason that canonical SMILES have been defined, which ensures a one to one correspondence between SMILES string and molecule. Here the fact that multiple SMILES represent the same molecule is explored as a technique for data augmentation of a molecular QSAR dataset modeled by a long short term memory (LSTM) cell based neural network. The augmented dataset was 130 times bigger than the original. The network trained with the augmented dataset shows better performance on a test set when compared to a model built with only one canonical SMILES string per molecule. The correlation coefficient R on the test set was improved from 0.56 to 0.66 when using SMILES enumeration, and the root mean square error (RMS) likewise fell from 0.62 to 0.55. The technique also works in the prediction phase. By taking the average per molecule of the predictions for the enumerated SMILES a further improvement to a correlation coefficient of 0.68 and a RMS of 0.52 was found.", "creator": "Writer"}}}