{"id": "1703.07326", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "One-Shot Imitation Learning", "abstract": "hongkui Imitation burgers learning has gribskov been commonly applied labid to kubelik solve different tasks 1941-1943 in materia isolation. stokenchurch This usually rattling requires either careful feature digidesign engineering, jumet or a bachelet significant agia number 86.80 of u.s.-british samples. lymphoblastic This is confronts far from 1458 what we richt desire: vlasova ideally, breathing robots should rami be able to dumps learn non-speaking from very perseus few demonstrations of \u017eu\u017eemberk any prosecutions given shavers task, and instantly montealto generalize trawled to new lakehead situations of the same task, arns without solidum requiring task - specific scatological engineering. In this paper, script we 154.00 propose assistances a 19.52 meta - learning 94.90 framework for cowpens achieving zombo such afs capability, which helichrysum we irresolvable call durgavati one - currey shot ss7 imitation marzano learning.", "histories": [["v1", "Tue, 21 Mar 2017 17:22:29 GMT  (4955kb,D)", "http://arxiv.org/abs/1703.07326v1", null], ["v2", "Wed, 22 Mar 2017 00:24:03 GMT  (4926kb,D)", "http://arxiv.org/abs/1703.07326v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE cs.RO", "authors": ["yan duan", "marcin", "rychowicz", "bradly c stadie", "jonathan ho", "jonas schneider", "ilya sutskever", "pieter abbeel", "wojciech zaremba"], "accepted": true, "id": "1703.07326"}, "pdf": {"name": "1703.07326.pdf", "metadata": {"source": "META", "title": "One-Shot Imitation Learning", "authors": ["Yan Duan", "Marcin Andrychowicz", "Bradly Stadie", "Jonathan Ho", "Jonas Schneider", "Ilya Sutskever", "Pieter Abbeel", "Wojciech Zaremba"], "emails": ["<rocky@openai.com>."], "sections": [{"heading": null, "text": "Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.\n1OpenAI 2University of California, Berkeley. Correspondence to: Yan Duan <rocky@openai.com>. Copyright 2017 by the author(s).\nOne-shot policy. A single policy trained to solve many tasks.\n(left) Task-specific policy. This policy is trained to stack blocks into two towers, each of height 3. (right) A separate task-specific\npolicy. This policy is trained to stack blocks into three towers, each of height 2.\nFigure 1. Traditionally, policies are task-specific. For example, a policy might have been trained (through imitation or reinforcement learning) to stack blocks into towers of height 3, and then another policy would be trained to stack blocks into towers of height 2, etc. In this paper, we are interested in policies that are not specific to one task, but rather can be told (through a single demonstration) what the current new task is, and be successful at this new task. As illustrative examples, we would want to be able to provide a single demonstration of each task, and from that the one-shot policy would know what to do when faced with a new situation of the task, where the blocks are randomly rearranged. Videos of the illustrated tasks are available at http://bit.ly/one-shot-imitation."}, {"heading": "1. Introduction", "text": "We are interested in robotic systems that are able to perform a variety of complex useful tasks, e.g. tidying up a home or preparing a meal. The robot should be able to learn new tasks without long system interaction time. To accomplish this, we must solve two broad problems:\n\u2022 The first problem is that of dexterity: robots should learn how to approach, grasp and pick up complex un-\nar X\niv :1\n70 3.\n07 32\n6v 1\n[ cs\n.A I]\n2 1\nM ar\n2 01\n7\nactuated objects, and how to place or arrange them into a desired configuration.\n\u2022 The second problem is that of communication: how to communicate the intent of the task at hand, so that the robot can replicate it in a broader set of initial conditions.\nDemonstrations are an extremely convenient form of information we can use to teach robots to overcome these two challenges. Using demonstrations, we can unambiguously communicate essentially any manipulation task, and simultaneously provide clues about the specific motor skills required to perform the task. We can compare this with an alternative form of communication, namely natural language. Although language is highly versatile, effective, and efficient, natural language processing systems are not yet at a level where we could easily use language to precisely describe a complex task to a robot. Compared to language, demonstration has two fundamental advantages: first, it does not require the knowledge of language, as it is possible to communicate complex tasks to humans that don\u2019t speak one\u2019s language. And second, there are many tasks that are extremely difficult to explain in words, even if we assume perfect linguistic abilities: for example, explaining how to swim without demonstration and experience seems to be, at the very least, an extremely challenging task.\nHowever, so far imitation learning has not been a silver bullet. Practical applications of imitation learning have either required careful feature engineering, or a significant amount of system interaction time. This is far from what what we desire: ideally, we hope to demonstrate a certain task only once or a few times to the robot, and have it instantly generalize to new situations of the same task, without long system interaction time or domain knowledge about individual tasks.\nIn this paper we explore the one-shot imitation learning setting, where the objective is to maximize the expected performance of the learned policy when faced with a new, previously unseen, task, and having received as input only one demonstration of that task. For the tasks we consider, the policy is expected to achieve good performance without any additional system interaction, once it has received the demonstration.\nWe train a policy on a broad distribution over tasks, where the number of tasks is potentially infinite. For each training task we assume the availability of a set of successful demonstrations. Our learned policy takes as input: (i) the current observation, and (ii) one demonstration that successfully solves a different instance of the same task (this demonstration is fixed for the duration of the episode). The policy outputs the current controls. We note that any pair of demonstrations for the same task provides a super-\nvised training example for the neural net policy, where one demonstration is treated as the the input, while the other as the output.\nTo make this model work, we made essential use of soft attention (Bahdanau et al., 2014) for processing both the (potentially long) sequence of states and action that correspond to the demonstration, and for processing the components of the vector specifying the locations of the various blocks in our environment. The use of soft attention over both types of inputs made strong generalization possible. In particular, on the family of block stacking tasks shown in Fig. 1, our neural network policy was able to perform well on novel block configurations which were not present in any training data.\nThis approach, if scaled up appropriately by training on a very wide variety of tasks and demonstrations, is likely to successfully learn a model for communicating complex tasks to a robot that works well in many practical settings."}, {"heading": "2. Related Work", "text": "Imitation learning considers the problem of acquiring skills from observing demonstrations. Survey articles include (Schaal, 1999; Calinon, 2009; Argall et al., 2009).\nTwo main lines of work within imitation learning are behavioral cloning, which performs supervised learning from observations to actions (e.g., Pomerleau (1989); Ross et al. (2011)); and inverse reinforcement learning (Ng & Russell, 2000), where a reward function (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011; Finn et al., 2016; Ho & Ermon, 2016) is estimated that explains the demonstrations as (near) optimal behavior. While this past work has led to a wide range of impressive robotics results, it considers each skill separately, and having learned to imitate one skill does not accelerate learning to imitate the next skill.\nOne-shot and few-shot learning has been studied for image recognition (Vinyals et al., 2016; Koch, 2015; Santoro et al., 2016; Ravi & Larochelle, 2017), generative modeling (Edwards & Storkey, 2017; Rezende et al., 2016), learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al., 2016; Wang et al., 2016). Fast adaptation has also been achieved through fast-weights (Ba et al., 2016). Like our algorithm, many of the aforementioned approaches are a form of meta-learning (Thrun & Pratt, 1998; Schmidhuber, 1987; Naik & Mammone, 1992), where the algorithm itself is being learned. Metalearning has also been studied to discover neural network weight optimization algorithms (Bengio et al., 1992; Schmidhuber, 1992; Andrychowicz et al., 2016). This prior work on one-shot learning and meta-learning, however, is tailored to respective domains (image recognition, genera-\ntive models, reinforcement learning, optimization) and not directly applicable in the imitation learning setting.\nReinforcement learning (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1995) provides an alternative route to skill acquisition, by learning through trial and error. Reinforcement learning has had many successes, including Backgammon (Tesauro, 1995), helicopter control (Ng et al., 2003), Atari (Mnih et al., 2015), Go (Silver et al., 2016), continuous control in simulation (Schulman et al., 2015; Heess et al., 2015; Lillicrap et al., 2015) and on real robots (Peters & Schaal, 2008; Levine et al., 2016). However, reinforcement learning tends to require a large number of trials and requires specifying a reward function to define the task at hand. The former can be time-consuming and the latter can often be significantly more difficult than providing a demonstration (Ng & Russell, 2000).\nMulti-task and transfer learning considers the problem of learning policies with applicability and re-use beyond a single task. Success stories include domain adaptation in computer vision (Yang et al., 2007; Kulis et al., 2011; Tzeng et al., 2014; Donahue et al., 2014) and control (Tzeng et al., 2015; Rusu et al., 2016; Sadeghi & Levine, 2016; Gupta et al., 2017; Stadie et al., 2017). However, while acquiring a multitude of skills faster than what it would take to acquire each of the skills independently, these approaches do not provide the ability to readily pick up a new skill from a single demonstration.\nOur approach heavily relies on an attention model over the demonstration and an attention model over the current observation. We use the soft attention model proposed in (Bahdanau et al., 2014) for machine translations, and which has also been successful in image captioning (Xu et al., 2015). The interaction networks proposed in (Battaglia et al., 2016; Chang et al., 2017) also leverage locality of physical interaction in learning. Our model is also related to the sequence to sequence model (Sutskever et al., 2014; Cho et al., 2014), as in both cases we consume a very long demonstration sequence and, effectively, emit a long sequence of actions."}, {"heading": "3. One Shot Imitation Learning", "text": ""}, {"heading": "3.1. Problem Formalization", "text": "We denote a distribution of tasks by T, an individual task by t \u223c T, and a distribution of demonstrations for the task t by D(t). A policy is symbolized by \u03c0\u03b8(a|o, d), where a is an action, o is an observation, d is a demonstration, and \u03b8 are the parameters of the policy. A demonstration d \u223c D(t) is a sequence of observations and actions : d = [(o1, a1), (o2, a2), . . . , (oT , aT )]. We assume that the distribution of tasks T is given, and that we can obtain successful demonstrations for each task in t \u2208 T. We assume\nthat there is some scalar-valued evaluation function Rt(d) (e.g. a binary value indicating success) for each task, although this is not required during training. The objective is to maximize the expected performance of the policy, where the expectation is taken over tasks t \u2208 T, and demonstrations d \u2208 D(t)."}, {"heading": "3.2. Example Settings", "text": "To clarify the problem setting, we describe two concrete examples, which we will also later study in the experiments."}, {"heading": "3.2.1. PARTICLE REACHING", "text": "The particle reaching problem is a very simple family of tasks. In each task, we control a point robot to reach a specific landmark, and different tasks are identified by different landmarks. As illustrated in Fig. 2, one task could be to reach the orange square, and another task could be to reach the green triangle. The agent receives its own 2D location, as well as the 2D locations of each of the landmarks. Within each task, the initial position of the agent, as well as the positions of all the landmarks, can vary across different instances of the task.\nWithout a demonstration, the robot does not know which landmark it should reach, and will not be able to accomplish the task. Hence, this setting already gets at the essence of one-shot imitation, namely to communicate the task via a demonstration. After learning, the agent should be able to identify the target landmark from the demonstration, and reach the same landmark in a new instance of the task."}, {"heading": "3.2.2. BLOCK STACKING", "text": "We now consider a more challenging set of tasks, which requires more advanced manipulation skills, and where different tasks share a compositional structure, which allows us to investigate nontrivial generalization to unseen tasks. In the block stacking tasks family, the goal is to control a 7- DOF Fetch robotic arm to stack various numbers of cube-\n0http://fetchrobotics.com/\nshaped blocks into configurations specified by the user. Each configuration consists of a list of blocks arranged into towers of different heights, and can be identified by a string such as ghij or ab cd ef gh, as illustrated in Fig. 3. Each of these configurations correspond to a different task. In a typical task, an observation is a list of (x, y, z) object positions relative to the gripper, and information if gripper is opened or closed. The number of objects may vary across different task instances."}, {"heading": "3.3. Algorithm", "text": "In order to train the neural network policy, we can use any algorithms for policy learning in sequential decision making problems. For example, if rewards are available in the training tasks, we can use reinforcement learning to optimize the policy. The only modification required is to condition the policy on a randomly chosen demonstration at the beginning of each episode. In this paper, we focus on imitation learning algorithms such as behavioral cloning and DAGGER (Ross et al., 2011), which only require demon-\nstrations rather than reward functions to be specified.1 This has the potential to be more scalable, since it is often easier to demonstrate a task than specifying a well-shaped reward function (Ng et al., 1999).\nWe start by collecting a set of demonstrations for each task, where we add noise to the actions in order to have wider coverage in the trajectory space. In each training iteration, we sample a list of tasks (with replacement). For each sampled task, we sample a demonstration as well as a small batch of observation-action pairs. The policy is trained to regress against the desired actions when conditioned on the current observation and the demonstration, by minimizing an `2 or cross-entropy loss based on whether actions are continuous or discrete. Across all experiments, we use Adamax (Kingma & Ba, 2014) to perform the optimization with a learning rate of 0.001."}, {"heading": "4. Architectures", "text": "While, in principle, a generic neural network could learn the mapping from demonstration and current observation to appropriate action, we found it important to use an appropriate architecture. Our architecture for learning block stacking is one of the main contributions of this paper, and we believe it is representative of what architectures for oneshot imitation learning of more complex tasks could look like in the future. Although the particle task is simpler, we also found architectural decisions to be important, and we consider several choices below to be evaluated in Section 5.1."}, {"heading": "4.1. Architecture for Particle Reaching", "text": "We consider three architectures for this problem:\n\u2022 Plain LSTM: The first architecture is a simple LSTM (Hochreiter & Schmidhuber, 1997) with 512 hidden units. It reads the demonstration trajectory, the output of which is then concatenated with the current state, and fed to a multi-layer perceptron (MLP) to produce the action.\n\u2022 LSTM with attention: In this architecture, the LSTM outputs a weighting over the different landmarks from the demonstration sequence. Then, it applies this weighting in the test scene, and produces a weighted combination over landmark positions given the current state. This 2D output is then concatenated with the current agent position, and fed to an MLP to produce the action.\n1To be more exact, DAGGER also requires interactive supervision during training.\n\u2022 Final state with attention: Rather than looking at the entire demonstration trajectory, this architecture only looks at the final state in the demonstration (which is already sufficient to communicate the task), and produce a weighting over landmarks. It then proceeds like the previous architecture.\nNotice that these three architectures are increasingly more specialized to the specific particle reaching setting, which suggests a potential trade-off between expressiveness and generalizability. We will quantify this tradeoff in Section 5.1."}, {"heading": "4.2. Architecture for Block Stacking", "text": "For the block stacking task, it is desirable that the policy architecture has the following properties:\n1. It should be easy to apply to task instances that have varying number of blocks.\n2. It should naturally generalize to different permutations of the same task. For instance, the policy should perform well on task dcba, even if it is only trained on task abcd.\n3. It should accommodate demonstrations of variable lengths.\nOur proposed architecture consists of three modules: the demonstration network, the context network, and the manipulation network. The modules make essential use of a neighborhood attention operation. We will first describe this operation in more detail, followed by describing each of the three modules."}, {"heading": "4.2.1. NEIGHBORHOOD ATTENTION", "text": "Since our neural network needs to handle demonstrations with variable numbers of blocks, it must have modules that can process variable-dimensional inputs. Soft attention is a natural operation which maps variable-dimensional inputs to fixed-dimensional outputs. However, by doing so, it may lose information compared to its input. This is undesirable, since the amount of information contained in a demonstration grows as the number of blocks increases. Therefore, we need an operation that can map variable-dimensional inputs to outputs with comparable dimensions. Intuitively, rather than having a single output as a result of attending to all inputs, we have as many outputs as inputs, and have each output attending to all other inputs in relation to its own corresponding input.\nWe start by describing the soft attention module as specified in (Bahdanau et al., 2014). The input to the attention includes a query q, a list of context vectors {cj}, and a list\nof memory vectors {mj}. The ith attention weight is given by wi \u2190 vT tanh(q + ci) (1) where v is a learned weight vector. The output of attention is a weighted combination of the memory content, where the weights are given by a softmax operation over the attention weights. Formally, we have\noutput\u2190 \u2211\ni\nmi exp(wi)\u2211 j exp(wj)\n(2)\nNote that the output has the same dimension as a memory vector. The attention operation can be generalized to multiple query heads, in which case there will be as many output vectors as there are queries.\nNow we turn to neighborhood attention. We assume there are B blocks in the environment. We denote the robot\u2019s state as srobot (in the block stacking experiment, this only includes information about whether the gripper is open or closed), and the coordinates of each block as (x1, y1, z1), . . . , (xB , yB , zB). The input to neighborhood attention is a list of embeddings hin1 , . . . , h in B of the same dimension, which can be the result of a projection operation over a list of block positions, or the output of a previous neighborhood attention operation. Given this list of embeddings, we use two separate linear layers to compute a query vector and a context embedding for each block:\nqi \u2190 Linear(hini ) ci \u2190 Linear(hini ) (3)\nThe memory content to be extracted consists of the coordinates of each block, concatenated with the input embedding. The ith query result is given by the following soft attention operation:\nresulti \u2190 SoftAttention( query : qi,\ncontext : {cj}Bj=1, memory : {concat((xj , yj , zj), hinj ))}Bj=1 ) (4)\nIntuitively, this operation allows each block to query other blocks in relation to itself (e.g. find the closest block), and extract the queried information. An illustration of the operation is shown in Fig. 4.\nThe gathered results are then combined with each block\u2019s own information, to produce the output embedding per block. Concretely, we have\noutputi \u2190 Linear(concat(hini , resulti, (xi, yi, zi), srobot)) (5)"}, {"heading": "N Attentions i-th attention is applied to i-th block vs others", "text": "In practice, we use multiple query heads per block, so that the size of each resulti will be proportional to the number of query heads."}, {"heading": "4.2.2. DEMONSTRATION NETWORK", "text": "Illustrated in Fig. 5, the demonstration network receives a demonstration trajectory as input, and produces an embedding of the demonstration to be used by the policy. The size of this embedding grows linearly as a function of the length of the demonstration as well as the number of blocks in the environment.\nFor block stacking, the demonstrations can span hundreds to thousands of time steps, and training with such long sequences can be demanding in both time and memory usage. Hence, we randomly discard a subset of time steps during training, an operation we call temporal dropout, analogous to (Srivastava et al., 2014; Krueger et al., 2016). We denote p as the proportion of time steps that are thrown away. In our experiments, we use p = 0.95, which reduces the length of demonstrations by a factor of 20. During test time, we can sample multiple downsampled trajectories, use each of them to compute downstream results, and average these results to produce an ensemble estimate. As shown in Section 5.2.3, this consistently improves the per-\nformance of the policy.\nAfter downsampling the demonstration, we apply a sequence of operations, composed of dilated temporal convolution (Yu & Koltun, 2016) and neighborhood attention."}, {"heading": "4.2.3. CONTEXT NETWORK", "text": "The context network is the crux of our model. Illustrated in Fig. 6, it processes both the current state and the embedding produced by the demonstration network, and outputs a context embedding, whose dimension does not depend on the length of the demonstration, or the number of blocks in the environment. Hence, it is forced to capture only the relevant information, which will be used by the manipulation network.\nThe context network starts by computing a query vector as a function of the current state, which is then used to attend over the different time steps in the demonstration embedding. The attention weights over different blocks within the same time step are summed together, to produce a single weight per time step. The result of this temporal attention is a vector whose size is proportional to the number of blocks in the environment. We then apply neighborhood attention to propagate the information across the embeddings of each block. This process is repeated multiple times, where the state is advanced using an LSTM cell with untied weights.\nThe previous sequence of operations produces an embedding whose size is independent of the length of the demonstration, but still dependent on the number of blocks. We then apply standard soft attention to produce fixeddimensional vectors, where the memory content only consists of positions of each block, which, together with the robot\u2019s state, forms the input passed to the manipulation network.\nIntuitively, although the number of objects in the environment may vary, at each stage of the manipulation operation, the number of relevant objects is small and usually fixed. For the block stacking environment specifically, the\nrobot should only need to pay attention to the position of the block it is trying to pick up (the source block), as well as the position of the block it is trying to place on top of (the target block). Therefore, a properly trained network can learn to match the current state with the corresponding stage in the demonstration, and infer the identities of the source and target blocks expressed as soft attention weights over different blocks, which are then used to extract the corresponding positions to be passed to the manipulation network. Although we do not enforce this interpretation in training, our experiment analysis supports this interpretation of how the learned policy works internally."}, {"heading": "4.2.4. MANIPULATION NETWORK", "text": "The manipulation network is the simplest component. Illustrated in Fig. 7, after extracting the information of the source and target blocks, it computes the action needed to complete the current stage of stacking one block on top of another one, using a simple MLP network.2 This division of labor opens up the possibility of modular training: the manipulation network may be trained to complete this simple procedure, without knowing about demonstrations or more than two blocks present in the environment. We leave this possibility for future work."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Particle Reaching", "text": "To demonstrate the key concepts that underlie the oneshot imitation learning framework, we conduct experiments with the simple 2D particle reaching task described in Section 3.2.1. We consider an increasingly difficult set of task families, where the number of landmarks increases from 2 to 10. For each task family, we collect 10000 trajectories for training, where the positions of landmarks and the starting position of the point robot are randomized. We use a hard-coded expert policy to efficiently generate demonstrations. We add noise to the trajectories by perturbing the computed actions before applying them to the environment, and we use simple behavioral cloning to train the\n2In principle, one can replace this module with an RNN module. But we did not find this necessary for the tasks we consider.\nneural network policy. The trained policy is evaluated on new scenarios and conditioned on new demonstration trajectories unseen during training.\nWe evaluate the performance of the three architectures described in Section 4.1. For the LSTM-based architectures, we apply dropout (Srivastava et al., 2014) to the fully connected layers, by zeroing out activations with probability 0.1 during training."}, {"heading": "5.1.1. RESULTS", "text": "The results are shown in Fig. 8. We observe that as the architecture becomes more specialized, we achieve much better generalization performance. For this simple task, it appears that conditioning on the entire demonstration hurts generalization performance, and conditioning on just the final state performs the best even without explicit regularization. This makes intuitive sense, since the final state already sufficiently characterizes the task at hand.\nHowever, the same conclusion does not appear to hold as the task becomes more complicated, as the next set of experiments shows."}, {"heading": "5.2. Block Stacking", "text": "The particle reaching tasks nicely demonstrates the challenges in generalization in a simplistic scenario. However, the tasks do not share a compositional structure, making the evaluation of generalization to new tasks challenging. The skills and the information content required for each individual task are also simple. Hence, we conduct fur-\nther experiments with the block stacking tasks described in Section 3.2.2. These experiments are designed to answer the following questions:\n\u2022 How does training with behavioral cloning compare with DAGGER, given that sufficient data can be collected offline?\n\u2022 How does conditioning on the entire demonstration compare to conditioning on the final desired configuration, even when the final configuration has enough information to fully specify the task?\n\u2022 How does conditioning on the entire demonstration compare to conditioning on a \u201csnapshot\u201d of the trajectory, which is a small subset of frames that are most informative?\n\u2022 Can our framework successfully generalize to types of tasks that it has never seen during training?\n\u2022 What are the current limitations of the method?\nTo answer these questions, we compare the performance of the following architectures:\n\u2022 DAGGER: We use the architecture described in Section 4.2, and train the policy using DAGGER.\n\u2022 BC: We use the same architecture as previous, but train the policy using behavioral cloning.\n\u2022 Final state: This architecture conditions on the final state rather than on the entire demonstration trajectory. For the block stacking task family, the final state uniquely identifies the task, and there is no need for additional information. However, a full trajectory, one which contains information about intermediate stages of the task\u2019s solution, can make it easier to train the optimal policy, because it could learn to rely on the demonstration directly, without needing to memorize the intermediate steps into its parameters. This is related to the way in which reward shaping can significantly affect performance in reinforcement learning (Ng et al., 1999). A comparison between the two conditioning strategies will tell us whether this hypothesis is valid. We train this policy using DAGGER.\n\u2022 Snapshot: This architecture conditions on a \u201csnapshot\u201d of the trajectory, which includes the last frame of each stage along the demonstration trajectory. This assumes that a segmentation of the demonstration into multiple stages is available at test time, which gives it an unfair advantage compared to the other conditioning strategies. Hence, it may perform better than\nconditioning on the full trajectory, and serves as a reference, to inform us whether the policy conditioned on the entire trajectory can perform as well as if the demonstration is clearly segmented. Again, we train this policy using DAGGER.\nWe evaluate the policy on tasks seen during training, as well as tasks unseen during training. Note that generalization is evaluated at multiple levels: the learned policy not only needs to generalize to new configurations and new demonstrations of tasks seen already, but also needs to generalize to new tasks. We also perform a thorough breakdown analysis of the failure scenarios as the difficulty of the task varies. Videos of our experiments are available at http://bit.ly/one-shot-imitation.\nConcretely, we collect 140 training tasks, and 43 test tasks, each with a different desired layout of the blocks. The number of blocks in each task can vary between 2 and 10. We collect 1000 trajectories per task for training, and maintain a separate set of trajectories and initial configurations to be used for evaluation. Similar to the particle reaching task, we inject noise into the trajectory collection process. The trajectories are collected using a hard-coded policy."}, {"heading": "5.2.1. PERFORMANCE EVALUATION", "text": "Fig. 9 shows the performance of various architectures. Results for training and test tasks are presented separately, where we group tasks by the number of stages required to complete them. This is because tasks that require more stages to complete are typically more challenging. In fact, even our scripted policy frequently fails on the hardest tasks. We measure success rate per task by executing the greedy policy (taking the most confident action at every time step) in 100 different configurations, each conditioned on a different demonstration unseen during training. We report the average success rate over all tasks within the same group. Note that there are no tasks with 8 stages in the training tasks, and no tasks with 1 or 3 stages in the test tasks, and hence the corresponding entries are omitted.\nFrom the figure, we can observe that for the easier tasks with fewer stages, all of the different conditioning strategies perform equally well and almost perfectly. As the difficulty (number of stages) increases, however, conditioning on the entire demonstration starts to outperform conditioning on the final state. One possible explanation is that when conditioned only on the final state, the policy may struggle about which block it should stack first, a piece of information that is readily accessible from demonstration, which not only communicates the task, but also provides valuable information to help accomplish it.\nMore surprisingly, conditioning on the entire demonstration also seems to outperform conditioning on the snapshot,\nwhich we originally expected to perform the best. We suspect that this is due to the regularization effect introduced by temporal dropout, which effectively augments the set of demonstrations seen by the policy during training.\nAnother surprising finding was that training with behavioral cloning has the same level of performance as training with DAGGER, which suggests that the entire training procedure could work without requiring interactive supervision. In our preliminary experiments, we found that injecting noise into the trajectory collection process was important for behavioral cloning to work well, hence in all experiments reported here we use noise injection.3 In practice, such noise can come from natural human-induced noise through tele-operation, or by artificially injecting additional noise before applying it on the physical robot.\nUnless otherwise mentioned, all subsequent experiments\n3Specifically, for each trajectory, we sample a scaling factor uniformly from {0, 0.001, 0.01}, and before computing the next world state, we add standard Gaussian noise to the actions rescaled by this scaling factor.\nare conducted with the architecture conditioned on full demonstrations, and trained using DAGGER."}, {"heading": "5.2.2. EVALUATING PERMUTATION INVARIANCE", "text": "During training and in the previous evaluations, we only select one task per equivalence class, where two tasks are considered equivalent if they are the same up to permuting different blocks. This is based on the assumption that our architecture is invariant to permutations among different blocks. For example, if the policy is only trained on the task abcd, it should perform well on task dcba, given a single demonstration of the task dcba. We now experimentally verify this property by fixing a training task, and evaluating the policy\u2019s performance under all equivalent permutations of it. As Fig. 10 shows, although the policy has only seen the task abcd, it achieves the same level of performance on all other equivalent tasks."}, {"heading": "5.2.3. EFFECT OF ENSEMBLING", "text": "We now evaluate the importance of sampling multiple downsampled demonstrations during evaluation, which was introduced in Section 4.2.2. Fig. 11 shows the performance across all training and test tasks, as the number of ensembles varies from 1 to 20. We observe that more ensembles helps the most for tasks with fewer stages. On the other hand, it consistently improves performance for the harder tasks, although the gap is smaller. We suspect that this is because the policy has learned to attend to frames in the demonstration trajectory where the blocks are already stacked together. In tasks with only 1 stage, for example, it is very easy for these frames to be dropped in a single downsampled demonstration. On the other hand, in tasks with more stages, it becomes more resilient to missing frames. Using more than 10 ensembles appears to provide no significant improvements, and hence we used 10 ensembles in our main evaluation.\n1 2 3 4 5 6 7 8\nNumber of Stages\n0%\n20%\n40%\n60%\n80%\n100%\nA ve ra ge\nS u cc es s R at e\nNumber of Ensembles\n1 2 5 10 20\nFigure 11. Performance of various number of ensembles."}, {"heading": "5.2.4. BREAKDOWN OF FAILURE CASES", "text": "To understand the limitations of the current approach, we perform a breakdown analysis of the failure cases. We consider three failure scenarios: \u201cWrong move\u201d means that the policy has arranged a layout incompatible with the desired layout. This could be because the policy has misinterpreted the demonstration, or due to an accidental bad move that happens to scramble the blocks into the wrong layout. \u201cManipulation failure\u201d means that the policy has made an irrecoverable failure, for example if the block is shaken off the table, which the current hard-coded policy does not know how to handle. \u201cRecoverable failure\u201d means that the policy runs out of time before finishing the task, which may be due to an accidental failure during the operation that would have been recoverable given more time. As shown in Fig. 12, conditioning on only the final state makes more wrong moves compared to other architectures. Apart from that, most of the failure cases are actually due to manipulation failures that are mostly irrecoverable.4 This suggests that better manipulation skills need to be acquired to make the learned one-shot policy more reliable."}, {"heading": "5.2.5. VISUALIZATION", "text": "Finally, we visualize the attention mechanisms underlying the main policy architecture. There are two kinds of attention we are mainly interested in, one where the policy attends to different time steps in the demonstration, and the other where the policy attends to different blocks in the current step, to filter out irrelevant signals. Fig. 13 shows a subset of the attention heatmaps, and the full set of visualizations, together with key frames of the neural network\n4Note that the actual ratio of misinterpreted demonstrations may be different, since the runs that have caused a manipulation failure could later lead to a wrong move, were it successfully executed. On the other hand, by visually inspecting the videos, we observed that most of the trajectories categorized as \u201cWrong Move\u201d are actually due to manipulation failures (except for policy conditioning on the final state, which does seem to occasionally execute an actual wrong move).\nFigure 12. Breakdown of the success and failure scenarios. The area that each color occupies represent the ratio of the corresponding scenario.\npolicy executing the task, can be found in Appendix B.3.\nIn Fig. 13(a), we can observe that the policy almost always focuses on a small subset of the block positions in the current state, which allows the manipulation network to generalize to operations over different blocks.\nIn Fig. 13(b), we can observe a sparse pattern of time steps that have high attention weights. This suggests that the policy has essentially learned to segment the demonstrations, and only attend to important key frames. Note that there are roughly 6 regions of high attention weights, which nicely corresponds to the 6 stages required to complete the task."}, {"heading": "6. Conclusions", "text": "In this work, we presented a simple model that maps a single successful demonstration of a task to an effective policy that solves said task in a new situation. We demonstrated effectiveness of this approach on two domains, particle reaching and block stacking. There are a lot of exciting directions for future work. We plan to extend the framework to demonstrations in the form of image data, which will allow more end-to-end learning without requiring a separate perception module. We are also interested in enabling the policy to condition on multiple demonstrations, in case where one demonstration does not fully resolve ambiguity in the objective.5 Furthermore and most importantly, we hope to scale up our method on a much larger and broader distribution of tasks, and explore its potential towards a general robotics imitation learning system\n5Many tasks involving abstract concepts fall into this category. For instance, in sorting and grouping different objects, whether the objective was to group objects based on color or based on shape.\nthat would be able to achieve an overwhelming variety of tasks."}, {"heading": "7. Acknowledgements", "text": "We would like to thank our colleagues at Berkeley and OpenAI for insightful discussions, and in particular the OpenAI robotics team for infrastructure support. This research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a Huawei Fellowship."}, {"heading": "A. More Details on Particle Reaching", "text": "A.1. Learning Curves\nFig. 14 shows the learning curves for the three architectures designed for the particle reaching tasks, as the number of landmarks is varied, by running the policies over 100 different configurations, and computing success rates over both training and test data. We can clearly observe that both LSTM-based architectures exhibit overfitting as the number of landmarks increases. On the other hand, using attention clearly improves generalization performance, and when conditioning on only the final state, it achieves perfect generalization in all scenarios. It is also interesting to observe that learning undergoes a phase transition. Intuitively, this may be when the network is learning to infer the task from the demonstration. Once this is finished, the learning of control policy is almost trivial.\nA.2. Exact Performance Numbers\nTable 1 and Table 2 show the exact performance numbers for reference."}, {"heading": "B. More Details on Block Stacking", "text": "B.1. Learning Curves\nFig. 15 shows the learning curves for different architectures designed for the block stacking tasks. These learning curves do not reflect final performance: for each evaluation point, we sample tasks and demonstrations from training data, reset the environment to the starting point of some particular stage (so that some blocks are already stacked), and only run the policy for up to one stage. If the training algorithm is DAGGER, these sampled trajectories are annotated and added to the training set. Hence this evaluation does not evaluate generalization. We did not perform full evaluation as training proceeds, because it is very time consuming: each evaluation requires tens of thousands of trajectories across over > 100 tasks. However, these figures are still useful to reflect some relative trend.\nFrom these figures, we can observe that while conditioning on full trajectories gives the best performance which was shown in the main text, it requires much longer training time, simply because conditioning on the entire demonstration requires\nmore computation. In addition, this may also be due to the high variance of the training process due to downsampling demonstrations, as well as the fact that the network needs to learn to properly segment the demonstration. It is also interesting that conditioning on snapshots seems to learn faster than conditioning on just the final state, which again suggests that conditioning on intermediate information is helpful, not only for the final policy, but also to facilitate training. We also observe that learning happens most rapidly for the initial stages, and much slower for the later stages, since manipulation becomes more challenging in the later stages. In addition, there are fewer tasks with more stages, and hence the later stages are not sampled as frequently as the earlier stages during evaluation.\nB.2. Exact Performance Numbers\nExact performance numbers are presented for reference:\n\u2022 Table 3 and Table 4 show the success rates of different architectures on training and test tasks, respectively;\n\u2022 Table 5 shows the success rates across all tasks as the number of ensembles is varied;\n\u2022 Table 6 shows the success rates of tasks that are equivalent to abcd up to permutations;\n\u2022 Table 7, Table 8, Table 9, Table 10, and Table 11 show the breakdown of different success and failure scenarios for all considered architectures.\nB.3. More Visualizations\nFig. 16 and Fig. 17 show the full set of heatmaps of attention weights. Interestingly, in Fig. 16, we observe that rather than attending to two blocks at a time, as we originally expected, the policy has learned to mostly attend to only one block at a time. This makes sense because during each of the grasping and the placing phase of a single stacking operation, the policy needs to only pay attention to the single block that the gripper should aim towards. For context, Fig. 18 and Fig. 19 show key frames of the neural network policy executing the task."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "Learning to learn by gradient descent by gradient descent", "author": ["Andrychowicz", "Marcin", "Denil", "Misha", "Gomez", "Sergio", "Hoffman", "Matthew W", "Pfau", "David", "Schaul", "Tom", "de Freitas", "Nando"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "A survey of robot learning from demonstration", "author": ["Argall", "Brenna D", "Chernova", "Sonia", "Veloso", "Manuela", "Browning", "Brett"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Using fast weights to attend to the recent past", "author": ["Ba", "Jimmy", "Hinton", "Geoffrey E", "Mnih", "Volodymyr", "Leibo", "Joel Z", "Ionescu", "Catalin"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Ba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Interaction networks for learning about objects, relations and physics", "author": ["Battaglia", "Peter", "Pascanu", "Razvan", "Lai", "Matthew", "Rezende", "Danilo Jimenez"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Battaglia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2016}, {"title": "On the optimization of a synaptic learning rule", "author": ["Bengio", "Samy", "Yoshua", "Cloutier", "Jocelyn", "Gecsei", "Jan"], "venue": "In Optimality in Artificial and Biological Neural Networks, pp", "citeRegEx": "Bengio et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1992}, {"title": "Neurodynamic programming: an overview", "author": ["Bertsekas", "Dimitri P", "Tsitsiklis", "John N"], "venue": "In Decision and Control,", "citeRegEx": "Bertsekas et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas et al\\.", "year": 1995}, {"title": "Robot programming by demonstration", "author": ["Calinon", "Sylvain"], "venue": "EPFL Press,", "citeRegEx": "Calinon and Sylvain.,? \\Q2009\\E", "shortCiteRegEx": "Calinon and Sylvain.", "year": 2009}, {"title": "A compositional object-based approach to learning physical dynamics", "author": ["Chang", "Michael B", "Ullman", "Tomer", "Torralba", "Antonio", "Tenenbaum", "Joshua B"], "venue": "In Int. Conf. on Learning Representations (ICLR),", "citeRegEx": "Chang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2017}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"], "venue": "In ICML, pp", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Rl: Fast reinforcement learning via slow reinforcement learning", "author": ["Duan", "Yan", "Schulman", "John", "Chen", "Xi", "Bartlett", "Peter L", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1611.02779,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Towards a neural statistician", "author": ["Edwards", "Harrison", "Storkey", "Amos"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Edwards et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Edwards et al\\.", "year": 2017}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["Finn", "Chelsea", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Learning invariant feature spaces to transfer skills with reinforcement learning", "author": ["Gupta", "Abhishek", "Devin", "Coline", "Liu", "YuXuan", "Abbeel", "Pieter", "Levine", "Sergey"], "venue": "In Int. Conf. on Learning Representations (ICLR),", "citeRegEx": "Gupta et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2017}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Heess", "Nicolas", "Wayne", "Gregory", "Silver", "David", "Lillicrap", "Tim", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "Generative adversarial imitation learning", "author": ["Ho", "Jonathan", "Ermon", "Stefano"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["Koch", "Gregory"], "venue": "ICML Deep Learning Workshop,", "citeRegEx": "Koch and Gregory.,? \\Q2015\\E", "shortCiteRegEx": "Koch and Gregory.", "year": 2015}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["Kulis", "Brian", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kulis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Levine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Wierstra and Daan.,? \\Q2015\\E", "shortCiteRegEx": "Wierstra and Daan.", "year": 2015}, {"title": "Meta-neural networks that learn by learning", "author": ["Naik", "Devang K", "Mammone", "RJ"], "venue": "In International Joint Conference on Neural Netowrks (IJCNN),", "citeRegEx": "Naik et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Naik et al\\.", "year": 1992}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Ng", "Andrew", "Russell", "Stuart"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2000}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Ng", "Andrew Y", "Harada", "Daishi", "Russell", "Stuart"], "venue": "In ICML,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Autonomous helicopter flight via reinforcement learning", "author": ["Ng", "Andrew Y", "Kim", "H Jin", "Jordan", "Michael I", "Sastry", "Shankar", "Ballianda", "Shiv"], "venue": "In NIPS,", "citeRegEx": "Ng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2003}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "Neural networks,", "citeRegEx": "Peters et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2008}, {"title": "Alvinn: An autonomous land vehicle in a neural network", "author": ["Pomerleau", "Dean A"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Pomerleau and A.,? \\Q1989\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1989}, {"title": "Optimization as a model for few-shot learning", "author": ["Ravi", "Sachin", "Larochelle", "Hugo"], "venue": "In Under Review,", "citeRegEx": "Ravi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2017}, {"title": "One-shot generalization in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Danihelka", "Ivo", "Gregor", "Karol", "Wierstra", "Daan"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2016}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "Drew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "cad) rl: Real single-image flight without a single real", "author": ["Sadeghi", "Fereshteh", "Levine", "Sergey"], "venue": null, "citeRegEx": "Sadeghi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sadeghi et al\\.", "year": 2016}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Is imitation learning the route to humanoid robots", "author": ["Schaal", "Stefan"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Schaal and Stefan.,? \\Q1999\\E", "shortCiteRegEx": "Schaal and Stefan.", "year": 1999}, {"title": "Evolutionary principles in selfreferential learning. On learning how to learn: The meta-meta-.", "author": ["Schmidhuber", "Jurgen"], "venue": "hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich,", "citeRegEx": "Schmidhuber and Jurgen.,? \\Q1987\\E", "shortCiteRegEx": "Schmidhuber and Jurgen.", "year": 1987}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In ICML, pp", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Third person imitation learning", "author": ["Stadie", "Bradlie", "Abbeel", "Pieter", "Sutskever", "Ilya"], "venue": "In Int. Conf. on Learning Representations (ICLR),", "citeRegEx": "Stadie et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2017}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Temporal difference learning and tdgammon", "author": ["Tesauro", "Gerald"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "Learning to learn", "author": ["Thrun", "Sebastian", "Pratt", "Lorien"], "venue": "Springer Science & Business Media,", "citeRegEx": "Thrun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1998}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["Tzeng", "Eric", "Hoffman", "Judy", "Zhang", "Ning", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1412.3474,", "citeRegEx": "Tzeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2014}, {"title": "Towards adapting deep visuomotor representations from simulated to real environments", "author": ["Tzeng", "Eric", "Devin", "Coline", "Hoffman", "Judy", "Finn", "Chelsea", "Peng", "Xingchao", "Abbeel", "Pieter", "Levine", "Sergey", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1511.07111,", "citeRegEx": "Tzeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2015}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Learning to reinforcement learn", "author": ["Wang", "Jane X", "Kurth-Nelson", "Zeb", "Tirumala", "Dhruva", "Soyer", "Hubert", "Leibo", "Joel Z", "Munos", "Remi", "Blundell", "Charles", "Kumaran", "Dharshan", "Botvinick", "Matt"], "venue": "arXiv preprint arXiv:1611.05763,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Cross-domain video concept detection using adaptive svms", "author": ["Yang", "Jun", "Yan", "Rong", "Hauptmann", "Alexander G"], "venue": "In Proceedings of the 15th ACM international conference on Multimedia,", "citeRegEx": "Yang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2007}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Yu", "Fisher", "Koltun", "Vladlen"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "To make this model work, we made essential use of soft attention (Bahdanau et al., 2014) for processing both the (potentially long) sequence of states and action that correspond to the demonstration, and for processing the components of the vector specifying the locations of the various blocks in our environment.", "startOffset": 65, "endOffset": 88}, {"referenceID": 2, "context": "Survey articles include (Schaal, 1999; Calinon, 2009; Argall et al., 2009).", "startOffset": 24, "endOffset": 74}, {"referenceID": 53, "context": "(2011)); and inverse reinforcement learning (Ng & Russell, 2000), where a reward function (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011; Finn et al., 2016; Ho & Ermon, 2016) is estimated that explains the demonstrations as (near) optimal behavior.", "startOffset": 90, "endOffset": 189}, {"referenceID": 22, "context": "(2011)); and inverse reinforcement learning (Ng & Russell, 2000), where a reward function (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011; Finn et al., 2016; Ho & Ermon, 2016) is estimated that explains the demonstrations as (near) optimal behavior.", "startOffset": 90, "endOffset": 189}, {"referenceID": 14, "context": "(2011)); and inverse reinforcement learning (Ng & Russell, 2000), where a reward function (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011; Finn et al., 2016; Ho & Ermon, 2016) is estimated that explains the demonstrations as (near) optimal behavior.", "startOffset": 90, "endOffset": 189}, {"referenceID": 30, "context": ", Pomerleau (1989); Ross et al. (2011)); and inverse reinforcement learning (Ng & Russell, 2000), where a reward function (Abbeel & Ng, 2004; Ziebart et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 48, "context": "One-shot and few-shot learning has been studied for image recognition (Vinyals et al., 2016; Koch, 2015; Santoro et al., 2016; Ravi & Larochelle, 2017), generative modeling (Edwards & Storkey, 2017; Rezende et al.", "startOffset": 70, "endOffset": 151}, {"referenceID": 35, "context": "One-shot and few-shot learning has been studied for image recognition (Vinyals et al., 2016; Koch, 2015; Santoro et al., 2016; Ravi & Larochelle, 2017), generative modeling (Edwards & Storkey, 2017; Rezende et al.", "startOffset": 70, "endOffset": 151}, {"referenceID": 32, "context": ", 2016; Ravi & Larochelle, 2017), generative modeling (Edwards & Storkey, 2017; Rezende et al., 2016), learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al.", "startOffset": 54, "endOffset": 101}, {"referenceID": 12, "context": ", 2016), learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al., 2016; Wang et al., 2016).", "startOffset": 79, "endOffset": 117}, {"referenceID": 49, "context": ", 2016), learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al., 2016; Wang et al., 2016).", "startOffset": 79, "endOffset": 117}, {"referenceID": 3, "context": "Fast adaptation has also been achieved through fast-weights (Ba et al., 2016).", "startOffset": 60, "endOffset": 77}, {"referenceID": 6, "context": "Metalearning has also been studied to discover neural network weight optimization algorithms (Bengio et al., 1992; Schmidhuber, 1992; Andrychowicz et al., 2016).", "startOffset": 93, "endOffset": 160}, {"referenceID": 1, "context": "Metalearning has also been studied to discover neural network weight optimization algorithms (Bengio et al., 1992; Schmidhuber, 1992; Andrychowicz et al., 2016).", "startOffset": 93, "endOffset": 160}, {"referenceID": 28, "context": "Reinforcement learning has had many successes, including Backgammon (Tesauro, 1995), helicopter control (Ng et al., 2003), Atari (Mnih et al.", "startOffset": 104, "endOffset": 121}, {"referenceID": 39, "context": ", 2016), continuous control in simulation (Schulman et al., 2015; Heess et al., 2015; Lillicrap et al., 2015) and on real robots (Peters & Schaal, 2008; Levine et al.", "startOffset": 42, "endOffset": 109}, {"referenceID": 16, "context": ", 2016), continuous control in simulation (Schulman et al., 2015; Heess et al., 2015; Lillicrap et al., 2015) and on real robots (Peters & Schaal, 2008; Levine et al.", "startOffset": 42, "endOffset": 109}, {"referenceID": 23, "context": ", 2015) and on real robots (Peters & Schaal, 2008; Levine et al., 2016).", "startOffset": 27, "endOffset": 71}, {"referenceID": 51, "context": "Success stories include domain adaptation in computer vision (Yang et al., 2007; Kulis et al., 2011; Tzeng et al., 2014; Donahue et al., 2014) and control (Tzeng et al.", "startOffset": 61, "endOffset": 142}, {"referenceID": 21, "context": "Success stories include domain adaptation in computer vision (Yang et al., 2007; Kulis et al., 2011; Tzeng et al., 2014; Donahue et al., 2014) and control (Tzeng et al.", "startOffset": 61, "endOffset": 142}, {"referenceID": 46, "context": "Success stories include domain adaptation in computer vision (Yang et al., 2007; Kulis et al., 2011; Tzeng et al., 2014; Donahue et al., 2014) and control (Tzeng et al.", "startOffset": 61, "endOffset": 142}, {"referenceID": 11, "context": "Success stories include domain adaptation in computer vision (Yang et al., 2007; Kulis et al., 2011; Tzeng et al., 2014; Donahue et al., 2014) and control (Tzeng et al.", "startOffset": 61, "endOffset": 142}, {"referenceID": 47, "context": ", 2014) and control (Tzeng et al., 2015; Rusu et al., 2016; Sadeghi & Levine, 2016; Gupta et al., 2017; Stadie et al., 2017).", "startOffset": 20, "endOffset": 124}, {"referenceID": 15, "context": ", 2014) and control (Tzeng et al., 2015; Rusu et al., 2016; Sadeghi & Levine, 2016; Gupta et al., 2017; Stadie et al., 2017).", "startOffset": 20, "endOffset": 124}, {"referenceID": 41, "context": ", 2014) and control (Tzeng et al., 2015; Rusu et al., 2016; Sadeghi & Levine, 2016; Gupta et al., 2017; Stadie et al., 2017).", "startOffset": 20, "endOffset": 124}, {"referenceID": 4, "context": "We use the soft attention model proposed in (Bahdanau et al., 2014) for machine translations, and which has also been successful in image captioning (Xu et al.", "startOffset": 44, "endOffset": 67}, {"referenceID": 50, "context": ", 2014) for machine translations, and which has also been successful in image captioning (Xu et al., 2015).", "startOffset": 89, "endOffset": 106}, {"referenceID": 5, "context": "The interaction networks proposed in (Battaglia et al., 2016; Chang et al., 2017) also leverage locality of physical interaction in learning.", "startOffset": 37, "endOffset": 81}, {"referenceID": 9, "context": "The interaction networks proposed in (Battaglia et al., 2016; Chang et al., 2017) also leverage locality of physical interaction in learning.", "startOffset": 37, "endOffset": 81}, {"referenceID": 42, "context": "Our model is also related to the sequence to sequence model (Sutskever et al., 2014; Cho et al., 2014), as in both cases we consume a very long demonstration sequence and, effectively, emit a long sequence of actions.", "startOffset": 60, "endOffset": 102}, {"referenceID": 10, "context": "Our model is also related to the sequence to sequence model (Sutskever et al., 2014; Cho et al., 2014), as in both cases we consume a very long demonstration sequence and, effectively, emit a long sequence of actions.", "startOffset": 60, "endOffset": 102}, {"referenceID": 33, "context": "In this paper, we focus on imitation learning algorithms such as behavioral cloning and DAGGER (Ross et al., 2011), which only require demonstrations rather than reward functions to be specified.", "startOffset": 95, "endOffset": 114}, {"referenceID": 27, "context": "1 This has the potential to be more scalable, since it is often easier to demonstrate a task than specifying a well-shaped reward function (Ng et al., 1999).", "startOffset": 139, "endOffset": 156}, {"referenceID": 4, "context": "We start by describing the soft attention module as specified in (Bahdanau et al., 2014).", "startOffset": 65, "endOffset": 88}, {"referenceID": 27, "context": "This is related to the way in which reward shaping can significantly affect performance in reinforcement learning (Ng et al., 1999).", "startOffset": 114, "endOffset": 131}], "year": 2017, "abstractText": "Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a metalearning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. OpenAI University of California, Berkeley. Correspondence to: Yan Duan <rocky@openai.com>. Copyright 2017 by the author(s). One-shot policy. A single policy trained to solve many tasks. (left) Task-specific policy. This policy is trained to stack blocks into two towers, each of height 3. (right) A separate task-specific policy. This policy is trained to stack blocks into three towers, each of height 2. Figure 1. Traditionally, policies are task-specific. For example, a policy might have been trained (through imitation or reinforcement learning) to stack blocks into towers of height 3, and then another policy would be trained to stack blocks into towers of height 2, etc. In this paper, we are interested in policies that are not specific to one task, but rather can be told (through a single demonstration) what the current new task is, and be successful at this new task. As illustrative examples, we would want to be able to provide a single demonstration of each task, and from that the one-shot policy would know what to do when faced with a new situation of the task, where the blocks are randomly rearranged. Videos of the illustrated tasks are available at http://bit.ly/one-shot-imitation.", "creator": "LaTeX with hyperref package"}}}