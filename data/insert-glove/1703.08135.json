{"id": "1703.08135", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "An embedded segmental K-means model for unsupervised segmentation and clustering of speech", "abstract": "Unsupervised segmentation redder and kultury clustering kerlikowske of unlabelled steinsaltz speech filled-in are core problems 4-0-25-0 in zero - outsprinted resource speech 3,500-ton processing. Most hz competitive flyable approaches kitingan lie at tv-shows methodological extremes: some follow barolo a gec Bayesian taruskin approach, defining unappealing probabilistic inglesby models with bornova convergence jefferson guarantees, while trosper others marchis opt candice for more speculator efficient claudine heuristic energysolutions techniques. Here serfaty we introduce an ube approximation typhoon to gonzales a 1,400-acre segmental Bayesian chinese-style model that ingleton falls faramarzi in between, with a clear yelps objective tepix function but using hard 14,300 clustering baghdad and segmentation varick rather than garg full handzus Bayesian inference. low-born Like rooms its 19,000-seat Bayesian counterpart, this wennet embedded stocznia segmental 20h00 k - means bphil model (lea ES - doomsayer KMeans) represents all-in arbitrary - length word segments marikana as fixed - sp1 dimensional acoustic 19w word novembro embeddings. On hintz English t\u00e9miscamingue and Xitsonga data, ES - airtricity KMeans outperforms all-seater a shengyou leading heuristic method in klarsfeld word 116.36 segmentation, giving laywers similar reuter scores dark-haired to the immunoassay Bayesian moulted model coastline while archetypical being back-room five fst times reichl faster gazon with stribling fewer hyperparameters. 28/29 However, there k\u00f3pavogur is a wyszk\u00f3w trade - off lianjiang in marim cluster jeret purity, with plushenko the nyc Bayesian 025 model ' winslade s purer clusters yielding about saraco\u011flu 10% better zalambessa unsupervised sinkler word error zimmerli rates.", "histories": [["v1", "Thu, 23 Mar 2017 16:45:22 GMT  (527kb,D)", "http://arxiv.org/abs/1703.08135v1", "5 pages, 3 figures, 2 tables"], ["v2", "Tue, 5 Sep 2017 14:14:11 GMT  (272kb,D)", "http://arxiv.org/abs/1703.08135v2", "8 pages, 3 figures, 3 tables; accepted to ASRU 2017"]], "COMMENTS": "5 pages, 3 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["herman kamper", "karen livescu", "sharon goldwater"], "accepted": false, "id": "1703.08135"}, "pdf": {"name": "1703.08135.pdf", "metadata": {"source": "CRF", "title": "An embedded segmental k-means model for unsupervised segmentation and clustering of speech", "authors": ["Herman Kamper", "Karen Livescu", "Sharon Goldwater"], "emails": ["kamperh@ttic.edu,", "klivescu@ttic.edu,", "sgwater@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "1. Introduction The growing area of zero-resource speech processing aims to develop unsupervised methods that can learn directly from raw speech audio in settings where transcriptions, lexicons and language modelling texts are not available. Such methods are crucial for providing speech technology in languages where transcribed data are hard or impossible to collect, e.g., unwritten or endangered languages [1]. In addition, such methods may shed light on how human infants acquire language [2, 3].\nSeveral zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13]. Early work mainly focused on unsupervised term discovery, where the aim is to automatically find repeated wordor phrase-like patterns in a collection of speech [14\u201316]. While useful, the discovered patterns are typically isolated segments spread out over the data, leaving much speech as background. This has prompted several studies on full-coverage approaches, where the entire speech input is segmented and clustered into word-like units [17\u201320].\nTwo such full-coverage approaches have recently been applied to the data of the Zero Resource Speech Challenge 2015 (ZRSC), giving a useful basis for comparison [21]. The first is the Bayesian embedded segmental Gaussian mixture model (BES-GMM) [22]: a probabilistic model that represents potential word segments as fixed-dimensional acoustic word embeddings, and then builds a whole-word acoustic model in this space while jointly doing segmentation. The second is the recurring syllableunit segmenter (SylSeg) [23], which is a cognitively motivated,\nfast, heuristic method that applies unsupervised syllable segmentation and clustering and then predicts recurring syllable sequences as words. These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].\nHere we introduce an approximation to BES-GMM that falls in between these two extremes. The embedded segmental k-means (ES-KMeans) algorithm uses hard clustering and segmentation, rather than full Bayesian inference. Nevertheless, it has a clear objective function, in contrast to heuristic methods such as SylSeg. Compared to BES-GMM, it has the advantage of fewer hyperparameters and a simpler optimization algorithm since probabilistic sampling is not necessary; ES-KMeans is therefore more efficient, while still having a principled objective.\nHard approximations have been used since the start of probabilistic modelling in supervised speech recognition [24\u201326], and also in more recent work to improve the efficiency of an unsupervised Bayesian model [27]. We are therefore following in a long tradition of using hard approximation. However, all of these studies applied it in frame-by-frame modelling approaches, while our approach operates on embedded representations of whole speech segments. There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.\nWe analyze how this approximation affects speed and accuracy relative to the original BES-GMM and the SylSeg method. On English and Xitsonga data, we show that ES-KMeans outperforms SylSeg in word segmentation and gives similar scores to BES-GMM, while being five times faster. However, the cluster purity of ES-KMeans falls behind that of the other two models. We show that the higher purity for BES-GMM results from a tendency towards smaller clusters which, unlike in ES-KMeans, can also be varied using hyperparameters.\n2. The embedded segmental k-means model Starting from standard k-means, we describe the embedded segmental k-means (ES-KMeans) objective function and algorithm.\n2.1. From k-means to ES-KMeans objective function\nGiven a speech utterance consisting of acoustic frames y1:M = y1,y2, . . . ,yM (e.g., MFCCs), our aim is to break the sequence up into word-like segments, and to cluster these into hypothesized word types.\nIf we knew the segmentation (i.e., where word boundaries occur), the data would consist of several segments of different durations, as shown at the bottom of Figure 1. To cluster these,\nar X\niv :1\n70 3.\n08 13\n5v 1\n[ cs\n.C L\n] 2\n3 M\nar 2\n01 7\nwe need a method to compare variable-length vector sequences. One option would be to use an alignment-based distance measure, such as dynamic time warping. Here we instead follow an acoustic word embedding approach [11, 28]: an embedding function fe is used to map a variable length speech segment to a single embedding vector x \u2208 RD in a fixed-dimensional space, i.e., segment yt1:t2 is mapped to a vector xi = fe(yt1:t2), illustrated as coloured horizontal vectors. The idea is that speech segments that are acoustically similar should lie close together in RD , allowing segments to be efficiently compared directly in the embedding space without requiring alignment.\nEmbedding all segments in the data set would give a set of vectors X = {xi}Ni=1, which could be clustered intoK hypothesized word classes using k-means, as shown at the top of Figure 1. Standard k-means aims to minimize the sum of squared euclidean distances to each cluster mean: minz \u2211K c=1 \u2211 x\u2208Xc ||x\u2212\u00b5c||\n2, where {\u00b5c} K c=1 are the cluster means,Xc are all vectors assigned to cluster c, and element zi in z indicates which cluster xi belongs to. The standard algorithm alternates between reassigning vectors to the closest cluster means, and then updating the means.\nStandard k-means would be appropriate if the segmentation was known, but this is not the case in this zero-resource setting. Rather, the embeddings X can change depending on the current segmentation. For a data set of S utterances, we denote the segmentations asQ = {qi}Si=1, where qi indicates the boundaries for utterance i. X (Q) is used to denote the embeddings under the current segmentation. Our aim now is to jointly optimize the cluster assignments z and the segmentationQ. Under what objective should these be optimized?\nOne option would be to extend the standard k-means objective and optimize min(Q, z) \u2211K c=1 \u2211 x\u2208Xc\u2229X (Q) ||x \u2212 \u00b5c||\n2, where Xc \u2229 X (Q) are embeddings assigned to cluster c under segmentation Q. But this is problematic: imagine the extreme of inserting no boundaries over an utterance, resulting in a single embedding and only a single term in the summation; any other segmentation would result in more terms in the summation, likely giving an overall worse score\u2014even if all embeddings are close to cluster means. Instead of assigning a score per segment, we deal with this by assigning a score per frame. This score is given by the score achieved by the segment to which that frame belongs, implying that segment scores are weighed by duration:\nmin Q , z K\u2211 c=1 \u2211 x\u2208Xc\u2229X (Q) len(x) ||x\u2212 \u00b5c|| 2 (1)\nwhere len(x) is the number of frames in the sequence on which embedding x is calculated.\nThe overall ES-KMeans algorithm initializes word boundaries randomly, and then optimizes (1) by alternating between optimizing segmentationQ while keeping cluster assignments z and means {\u00b5c} K c=1 fixed (top to bottom in Figure 1), and then optimizing the cluster assignments and means while keeping the segmentation fixed (bottom to top in the figure)."}, {"heading": "2.2. Segmentation", "text": "Under a fixed clustering z, the objective (1) becomes\nmin Q \u2211 x\u2208X (Q) len(x) ||x\u2212 \u00b5\u2217x|| 2 = min Q \u2211 x\u2208X (Q) d(x) (2)\nwhere \u00b5\u2217x is the mean of the cluster to which x is currently assigned (according to z), and d(x) , len(x) ||x\u2212 \u00b5\u2217x||2 is the \u201cscore\u201d of embedding x (lower d is better).\nEquation (2) can be optimized separately for each utterance, so we want to find the segmentation q for each utterance that gives the minimum of the sum of the scores of the embeddings under that segmentation. This is exactly the problem addressed by the shortest-path algorithm (Viterbi) which uses dynamic programming to solve this problem efficiently [33, \u00a721.7].\nLet qt be the number of frames in the hypothesized segment (word) that ends at frame t: if qt = j, then yt\u2212j+1:t is a word. 1 We define forward variables \u03b3[t] as the optimal score up to boundary position t: \u03b3[t] , minq:t \u2211 x\u2208X (q:t) d(x), with q:t the sequence of segmentation decisions (the q\u2019s) that have been made up to t. These can be recursively calculated [33, \u00a721.7]:\n\u03b3[t] = t\nmin j=1\n{ d ( fe(yt\u2212j+1:t) ) + \u03b3[t\u2212 j] } (3)\nStarting with \u03b3[0] = 0, we calculate (3) for 1 \u2264 t \u2264 M \u2212 1. We keep track of the optimal choice (argmin) for each \u03b3[t], and the overall optimal segmentation is then given by starting from the final position t = M and moving backwards, repeatedly choosing the optimal boundary."}, {"heading": "2.3. Cluster assignments and mean updates", "text": "For a fixed segmentationQ, the objective (1) becomes\nmin z K\u2211 c=1 \u2211 x\u2208Xc\u2229X (Q) len(x)||x\u2212 \u00b5c|| 2 (4)\nWhen the means {\u00b5c} K c=1 are fixed, the optimal reassignments (5) follow standard k-means, and are guaranteed to improve (1) since the distance between the embedding and its assigned cluster mean never increases:\nzi = argmin c\n{ len(xi) ||xi \u2212 \u00b5c|| 2} = argmin c ||xi \u2212 \u00b5c|| 2\n(5) Finally, we fix the assignments z and update the means:\n\u00b5c = 1\u2211\nx\u2208Xc len(x) \u2211 x\u2208Xc len(x)x \u2248 1 Nc \u2211 x\u2208Xc x (6)\nThe exact equation is the mean of the vectors assigned to cluster c weighed by duration, and is guaranteed to improve (1). We use\n1For a an utterance with frames y1:M , a sequence of q\u2019s ending with qM specifies a unique segmentation; boldface q is this sequence of q\u2019s.\nthe approximation, which is exact if all segments have the same duration, to again match standard k-means, with Nc the number of embeddings currently assigned to cluster c.\nThe complete ES-KMeans algorithm is given below. Since the segmentation, clustering and mean updates each improve (1), the algorithm will converge to a local optimum.\nAlgorithm 1 The embedded segmental k-means algorithm. 1: Initialize segmentationQ randomly. 2: Initialize cluster assignments z randomly. 3: repeat . Optimization iterations 4: for i = randperm(1 to S) do . Select utterance i 5: Calculate \u03b3\u2019s using (3). . Segmentation variables 6: t\u2190Mi 7: while t \u2265 1 do . Perform segmentation 8: qt \u2190 argmintj=1 { d ( fe(yt\u2212j+1:t) ) + \u03b3[t\u2212 j]\n} 9: t\u2190 t\u2212 qt\n10: end while 11: Assign new embeddings X (qi) to clusters using (5). 12: Update means using (6). 13: end for 14: until convergence"}, {"heading": "2.4. The Bayesian embedded segmental GMM", "text": "In previous work [20, 22], we proposed a very similar model, but instead of k-means, we used a Bayesian GMM as wholeword clustering component (top of Figure 1). This Bayesian embedded segmental GMM (BES-GMM) served as inspiration for ES-KMeans; we briefly discuss their relationship here.\nA Bayesian GMM treats its mixture weights \u03c0 and component means {\u00b5c} K c=1 as random variables rather than point estimates, as is done in a regular GMM. We use conjugate priors: a Dirichlet prior over \u03c0 and a spherical-covariance Gaussian prior over \u00b5c. All components share the same fixed spherical covariance matrix \u03c32I. The model is then formally defined as:\n\u03c0 \u223c Dir (a/K1) (7) zi \u223c \u03c0 (8)\n\u00b5c \u223c N (\u00b50, \u03c3 2 0I) (9)\nxi \u223c N (\u00b5zi , \u03c3 2I) (10)\nUnder this model, component assignments and a segmentation can be inferred jointly using a collapsed Gibbs sampler [34]. Full details are given in [20], but the Gibbs sampler looks very similar to Algorithm 1: the Bayesian GMM gives likelihood terms (\u201cscores\u201d) in order to find an optimal segmentation, while the segmentation hypothesizes the boundaries for the word segments which are then clustered using the GMM. However, for BESGMM, component assignments and segmentation are sampled probabilistically, instead of making hard decisions.\nThe link between the two models emerges asymptomatically. It can be shown that standard k-means results from a GMM as the variances approach zero [35, \u00a720.3.5], [36]. In a similar way it can be shown that the Gibbs sampling equations for segmentation and component assignments for BES-GMM (as given in [20]) approach (3) and (5), respectively, in the limit \u03c32 \u2192 0, when all other hyperparameters are fixed.\nWithout giving a full complexity analysis, we note that because ES-KMeans only considers the closest cluster, it is more efficient than BES-GMM, where all components are considered when assigning embeddings to clusters and during segmentation (since embedding \u201cscores\u201d are obtained by marginalizing over all components). ES-KMeans can also be trivially parallelized, since both segmentation and cluster assignment can be performed in parallel for each utterance. This parallelized algorithm is still guaranteed to converge, though possibly to a\ndifferent local optimum than Algorithm 1 since updates are in a different order. Parallelizing BES-GMM is also possible, but the guarantee of converging to the true posterior distribution is lost [27]. We do not consider parallelization in this work, but rather keep the two algorithms as close as possible (using the same update order) to allow for direct comparison."}, {"heading": "2.5. Heuristic recurring syllable-unit word segmentation", "text": "We also compare to the ZRSC submission of Ra\u0308sa\u0308nen et al. [23]. Their system, which we refer to as SylSeg, relies on a novel cognitively motivated unsupervised method that predicts boundaries for syllable-like units, and then clusters these units on a per-speaker basis. Using a bottom-up greedy mapping, recurring syllable cluster sequences are then predicted as words.\nSylSeg has the benefit of being much simpler in terms of computational complexity and implementation than ES-KMeans or BES-GMM. But, in contrast to the heuristic methodology followed in SylSeg, both ES-KMeans and BES-GMM have clear overall objective functions that they optimize, the one using hard clustering, the other using Bayesian inference."}, {"heading": "3. Experiments", "text": ""}, {"heading": "3.1. Experimental setup and evaluation", "text": "Evaluation is performed on the two ZRSC data sets: an English corpus of around 5 hours of speech from 12 speakers, and a Xitsonga corpus of around 2.5 hours from 24 speakers [37]. We also use a separate English set of around 6 hours for development.\nAs in [20, 22, 38], we use several metrics to evaluate against ground truth forced alignments. By mapping every discovered word token to the ground truth token with which it overlaps most and then mapping every cluster to its most common word, average cluster purity and unsupervised word error rate (WER) can be calculated.2 By instead mapping every token to the true phoneme sequence with which it overlaps most, the normalized edit distance (NED) between all segments in the same cluster can be calculated; lower NED is better, with scores from 0 to 1. Word boundary F -score evaluates segmentation performance by comparing proposed and true word boundaries; similarly, word token F -score measures the accuracy of proposed word token intervals. Word type F -score compares the set of unique phoneme mappings (obtained as for NED) to the set in the true lexicon. See [38] for full details.\nOur implementation of ES-KMeans follows as closely as possible that of BES-GMM in [22]. Both use uniform downsampling as embedding function fe: a segment is represented by keeping 10 equally spaced MFCCs and flattening these [39]. Both models use unsupervised syllable pre-segmentation [23] to limit allowed word boundaries. K is set to 20% of the number of first-pass segmented syllables. Word candidates are limited to span at most 6 syllables, and at least 200 ms. For BES-GMM we use simulated annealing, an all-zero vector for \u00b50, \u03c3 2 0 = \u03c3\n2/\u03ba0, \u03ba0 = 0.05, a = 1, \u03c32 = 0.001. See [22] for full details."}, {"heading": "3.2. Results and analysis", "text": "Table 1(a) shows the performance of the three models on the English and Xitsonga corpora. Some of the SylSeg scores are unknown since these were not part of the ZRSC evaluation [23]. Compared to BES-GMM, ES-KMeans achieves worse purity, WER and NED, but similar boundary, token and type F -scores. This comes with a 5\u00d7 improvement in runtime. ES-KMeans\n2We allow more than one cluster to be mapped to the same word.\nTable 1: (a) Performance of models on the two test corpora. Lower NED is better. Runtimes for SylSeg\u2217 are rough estimates, obtained from personal communication with the authors [23]. (b) English development set performance of BES-GMM as the variance is varied.\n(a)\nEnglish (%) Xitsonga (%)\nSylSeg ES-KMeans BES-GMM SylSeg ES-KMeans BES-GMM\nCluster purity - 42.8 56.1 - 40.5 49.8 WER - 73.2 68.3 - 80.3 71.6\nNED 71.1 71.6 55.5 62.8 70.4 58.4 Boundary F 55.2 62.2 62.0 33.4 42.1 43.1 Token F 12.4 18.1 17.9 2.7 3.7 4.0 Type F 12.2 18.9 18.6 3.3 4.9 5.2\nRuntime (s) 100\u2217 193 1052 20\u2217 44 196\n(b)\nBES-GMM on dev. (%)\n\u03c32 Cluster purity WER\n0.00001 56.1 68.9 0.0001 55.7 69.0 0.001 56.9 67.7 0.0015 51.9 69.8 0.00175 41.1 75.8 0.002 35.1 86.8\nES-KMeans BES-GMM\nyeah\num just\num um just its be\njust um\nFigure 2: The 5 biggest clusters for ES-KMeans and BES-GMM. Circle radii are according to cluster size; shading indicates purity. The cluster-to-true-word mapping is also shown.\nachieves worse NED than SylSeg, but much better word boundary, token and type F -scores. SylSeg, however, is twice as fast.\nES-KMeans is therefore competitive in terms of word segmentation scores (boundary, token F -scores) and lexicon quality (type F -score), but falls behind in the purity-based metrics (purity, WER, NED). The difference with BES-GMM is particularly interesting since \u03c32 is set to be quite small, and ES-KMeans results from BES-GMM in the limit \u03c32 \u2192 0 (see \u00a72.4). To understand the discrepancy in purity, we analyzed ES-KMeans and BES-GMM on a single English development speaker.\nFor a qualitative view, Figure 2 shows the 5 biggest clusters for the two models. BES-GMM outputs more smaller clusters with a higher purity (often separating the same word over different clusters) compared to ES-KMeans. By listening to tokens assigned to the same cluster by ES-KMeans, we found that although tokens overlap with different ground truth labels, cluster assignments are qualitatively sensible, capturing similarity in acoustics or prosody. For example, Figure 3 shows spectrograms for tokens assigned to the \u201cbe\u201d cluster in Figure 2. The ground truth word labels with maximal overlap are also shown. For the \u201cseventy\u201d and \u201calready\u201d tokens, the segments only cover part of the true words (shown in bold), and the \u201cthat you\u201d token is actually pronounced in context as [dh uw]. So despite mapping to different true labels, these segments form a reasonable acoustic group. Nevertheless, they are penalized under purity and WER.\nBy spreading out its discovered tokens more evenly over clusters (Figure 2), BES-GMM produces a clustering that is better-matched to the evaluation metrics, although the ESKMeans clustering might be subjectively reasonable. This spreading (or sparsity) of BES-GMM can be controlled through the fixed spherical covariance parameter \u03c32, which impacts both the soft assignments of an embedding to a cluster and the segmentation (\u00a72.4). Table 1(b) shows performance on the development set as \u03c32 is varied. There is a sweet spot: when \u03c32 is too big, most tokens are sucked up by a number of large garbage clusters; when \u03c32 is smaller, more tokens are assigned to separate clusters. In contrast, ES-KMeans has no \u03c32 parameter and considers only the single closest cluster.\nWhen reading the results in Table 1(b) from bottom to top, note that the results of BES-GMM do not seem to converge to ES-KMeans, even though \u03c32 is tending towards 0 (where the two models should be equivalent). This is because, based on the recommendation in [40], we set the variance of the prior on the component means of BES-GMM as \u03c320 = \u03c32/\u03ba0 (see end of \u00a73.1), so the prior variance on the component means is tied to the fixed data variance. Under these conditions, the asymptotic equivalence of BES-GMM and ES-KMeans no longer holds. Murphy [40] explains that this coupling is a sensible way to incorporate prior knowledge of the typical spread of data, and here we indeed show how this helps our Bayesian model; this principled way of including priors is not possible in ES-KMeans or SylSeg. When setting \u03c320 = 1 (rather than tying it), the results of BES-GMM match ES-KMeans when \u03c32 = 0.00001.\n4. Conclusion We introduced the embedded segmental k-means model (ESKMeans), a method that falls in between the fully Bayesian embedded segmental GMM (BES-GMM) and the cognitively motivated heuristic SylSeg method. Its word segmentation performance is on par with BES-GMM and superior to SylSeg, but cluster purity is worse than both other methods. In terms of efficiency, it is 5 times faster than BES-GMM, but half as fast as SylSeg. Despite using hard clustering and segmentation, ESKMeans still has a clear objective function and it is guaranteed to converge (to a local optimum), in contrast to SylSeg. It also has far fewer hyperparameters than BES-GMM, although we show that this is what gives the latter the upper hand. How to balance these trade-offs between speed, performance and having a clear objective will ultimately depend on the downstream task on which the model is applied.\nAcknowledgements: We would like to thank Shreyas Seshadri for helpful feedback on the SylSeg model.\n5. References [1] L. Besacier, E. Barnard, A. Karpov, and T. Schultz, \u201cAutomatic\nspeech recognition for under-resourced languages: A survey,\u201d Speech Commun., vol. 56, pp. 85\u2013100, 2014.\n[2] O. J. Ra\u0308sa\u0308nen, \u201cComputational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions,\u201d Speech Commun., vol. 54, pp. 975\u2013997, 2012.\n[3] E. Dupoux, \u201cCognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner,\u201d arXiv preprint arXiv:1607.08723, 2016.\n[4] B. Varadarajan, S. Khudanpur, and E. Dupoux, \u201cUnsupervised learning of acoustic sub-word units,\u201d in Proc. ACL, 2008.\n[5] C.-y. Lee and J. R. Glass, \u201cA nonparametric Bayesian approach to acoustic model discovery,\u201d in Proc. ACL, 2012.\n[6] L. Ondel, L. Burget, J. Cernocky, and S. Kesiraju, \u201cBayesian phonotactic language model for acoustic unit discovery,\u201d in Proc. ICASSP, 2017.\n[7] L. Badino, C. Canevari, L. Fadiga, and G. Metta, \u201cAn auto-encoder based approach to unsupervised learning of subword units,\u201d in Proc. ICASSP, 2014.\n[8] D. Renshaw, H. Kamper, A. Jansen, and S. J. Goldwater, \u201cA comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge,\u201d in Proc. Interspeech, 2015.\n[9] N. Zeghidour, G. Synnaeve, N. Usunier, and E. Dupoux, \u201cJoint learning of speaker and phonetic similarities with Siamese networks,\u201d in Proc. Interspeech, 2016.\n[10] Y. Zhang, R. Salakhutdinov, H.-A. Chang, and J. R. Glass, \u201cResource configurable spoken query detection using deep Boltzmann machines,\u201d in Proc. ICASSP, 2012.\n[11] K. Levin, A. Jansen, and B. Van Durme, \u201cSegmental acoustic indexing for zero resource keyword search,\u201d in Proc. ICASSP, 2015.\n[12] H. Gish, M.-H. Siu, A. Chan, and B. Belfield, \u201cUnsupervised training of an HMM-based speech recognizer for topic classification,\u201d in Proc. Interspeech, 2009.\n[13] S. Kesiraju, R. Pappagari, L. Ondel, and L. Burget, \u201cTopic identification of spoken documents using unsupervised acoustic unit discovery,\u201d in Proc. ICASSP, 2017.\n[14] A. S. Park and J. R. Glass, \u201cUnsupervised pattern discovery in speech,\u201d IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.\n[15] A. Jansen and B. Van Durme, \u201cEfficient spoken term discovery using randomized algorithms,\u201d in Proc. ASRU, 2011.\n[16] B. Oosterveld, R. Veale, and M. Scheutz, \u201cA parallelized dynamic programming approach to zero resource spoken term discovery,\u201d in Proc. ICASSP, 2017.\n[17] M. Sun and H. Van hamme, \u201cJoint training of non-negative Tucker decomposition and discrete density hidden Markov models,\u201d Comput. Speech Lang., vol. 27, no. 4, pp. 969\u2013988, 2013.\n[18] O. Walter, T. Korthals, R. Haeb-Umbach, and B. Raj, \u201cA hierarchical system for word discovery exploiting DTW-based initialization,\u201d in Proc. ASRU, 2013.\n[19] C.-y. Lee, T. O\u2019Donnell, and J. R. Glass, \u201cUnsupervised lexicon discovery from acoustic input,\u201d Trans. ACL, vol. 3, pp. 389\u2013403, 2015.\n[20] H. Kamper, A. Jansen, and S. J. Goldwater, \u201cUnsupervised word segmentation and lexicon discovery using acoustic word embeddings,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.\n[21] M. Versteegh, X. Anguera, A. Jansen, and E. Dupoux, \u201cThe Zero Resource Speech Challenge 2015: Proposed approaches and results,\u201d in Proc. SLTU, 2016.\n[22] H. Kamper, A. Jansen, and S. Goldwater, \u201cA segmental framework for fully-unsupervised large-vocabulary speech recognition,\u201d arXiv preprint arXiv:1606.06950, 2016.\n[23] O. J. Ra\u0308sa\u0308nen, G. Doyle, and M. C. Frank, \u201cUnsupervised word discovery from speech using automatic segmentation into syllablelike units,\u201d in Proc. Interspeech, 2015.\n[24] J. G. Wilpon and L. R. Rabiner, \u201cA modified K-means clustering algorithm for use in isolated work recognition,\u201d IEEE Trans. Acoust. Speech Signal Process., vol. 33, no. 3, pp. 587\u2013594, 1985.\n[25] L. R. Rabiner, J. G. Wilpon, and B.-H. Juang, \u201cA segmental kmeans training procedure for connected word recognition,\u201d AT&T Tech. J., vol. 65, no. 3, pp. 21\u201331, 1986.\n[26] B.-H. Juang and L. R. Rabiner, \u201cThe segmental K-means algorithm for estimating parameters of hidden Markov models,\u201d IEEE Trans. Acoust. Speech Signal Process., vol. 38, no. 9, pp. 1639\u20131641, 1990.\n[27] S. H. Shum, D. F. Harwath, N. Dehak, and J. R. Glass, \u201cOn the use of acoustic unit discovery for language recognition,\u201d IEEE Trans. Acoust., Speech, Signal Process., vol. 24, no. 9, pp. 1665\u20131676, 2016.\n[28] H. Kamper, W. Wang, and K. Livescu, \u201cDeep convolutional acoustic word embeddings using word-pair side information,\u201d in Proc. ICASSP, 2016.\n[29] Y.-A. Chung, C.-C. Wu, C.-H. Shen, and H.-Y. Lee, \u201cUnsupervised learning of audio segment representations using sequence-tosequence recurrent neural networks,\u201d Proc. Interspeech, 2016.\n[30] S. Settle and K. Livescu, \u201cDiscriminative acoustic word embeddings: Recurrent neural network-based approaches,\u201d in Proc. SLT, 2016.\n[31] W. He, W. Wang, and K. Livescu, \u201cMulti-view recurrent neural acoustic word embeddings,\u201d in Proc. ICLR, 2017.\n[32] K. Audhkhasi, A. Rosenberg, A. Sethy, B. Ramabhadran, and B. Kingsbury, \u201cEnd-to-end ASR-free keyword search from speech,\u201d in Proc. ICASSP, 2017.\n[33] J. Erickson, \u201cAlgorithms, etc.\u201d UIUC Lecture Notes, Jan. 2015. [Online]. Available: http://jeffe.cs.illinois.edu/teaching/algorithms/\n[34] P. Resnik and E. Hardisty, \u201cGibbs sampling for the uninitiated,\u201d University of Maryland, College Park, MD, Tech. Rep., 2010.\n[35] D. Barber, Bayesian Reasoning and Machine Learning. Cambridge, UK: Cambridge University Press, 2013.\n[36] B. Kulis and M. I. Jordan, \u201cRevisiting k-means: New algorithms via Bayesian nonparametrics,\u201d in Proc. ICML, 2012.\n[37] M. Versteegh, R. Thiollie\u0300re, T. Schatz, X. N. Cao, X. Anguera, A. Jansen, and E. Dupoux, \u201cThe Zero Resource Speech Challenge 2015,\u201d in Proc. Interspeech, 2015.\n[38] B. Ludusan, M. Versteegh, A. Jansen, G. Gravier, X.-N. Cao, M. Johnson, and E. Dupoux, \u201cBridging the gap between speech technology and natural language processing: An evaluation toolbox for term discovery systems,\u201d in Proc. LREC, 2014.\n[39] K. Levin, K. Henry, A. Jansen, and K. Livescu, \u201cFixed-dimensional acoustic embeddings of variable-length segments in low-resource settings,\u201d in Proc. ASRU, 2013.\n[40] K. P. Murphy, \u201cConjugate Bayesian analysis of the Gaussian distribution,\u201d 2007. [Online]. Available: http://www.cs.ubc.ca/ \u223cmurphyk/mypapers.html"}], "references": [{"title": "Automatic speech recognition for under-resourced languages: A survey", "author": ["L. Besacier", "E. Barnard", "A. Karpov", "T. Schultz"], "venue": "Speech Commun., vol. 56, pp. 85\u2013100, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions", "author": ["O.J. R\u00e4s\u00e4nen"], "venue": "Speech Commun., vol. 54, pp. 975\u2013997, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner", "author": ["E. Dupoux"], "venue": "arXiv preprint arXiv:1607.08723, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of acoustic sub-word units", "author": ["B. Varadarajan", "S. Khudanpur", "E. Dupoux"], "venue": "Proc. ACL, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "A nonparametric Bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J.R. Glass"], "venue": "Proc. ACL, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian phonotactic language model for acoustic unit discovery", "author": ["L. Ondel", "L. Burget", "J. Cernocky", "S. Kesiraju"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "An auto-encoder based approach to unsupervised learning of subword units", "author": ["L. Badino", "C. Canevari", "L. Fadiga", "G. Metta"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge", "author": ["D. Renshaw", "H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint learning of speaker and phonetic similarities with Siamese networks", "author": ["N. Zeghidour", "G. Synnaeve", "N. Usunier", "E. Dupoux"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Resource configurable spoken query detection using deep Boltzmann machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J.R. Glass"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised training of an HMM-based speech recognizer for topic classification", "author": ["H. Gish", "M.-H. Siu", "A. Chan", "B. Belfield"], "venue": "Proc. Interspeech, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Topic identification of spoken documents using unsupervised acoustic unit discovery", "author": ["S. Kesiraju", "R. Pappagari", "L. Ondel", "L. Burget"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "A parallelized dynamic programming approach to zero resource spoken term discovery", "author": ["B. Oosterveld", "R. Veale", "M. Scheutz"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Joint training of non-negative Tucker decomposition and discrete density hidden Markov models", "author": ["M. Sun", "H. Van hamme"], "venue": "Comput. Speech Lang., vol. 27, no. 4, pp. 969\u2013988, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A hierarchical system for word discovery exploiting DTW-based initialization", "author": ["O. Walter", "T. Korthals", "R. Haeb-Umbach", "B. Raj"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T. O\u2019Donnell", "J.R. Glass"], "venue": "Trans. ACL, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "The Zero Resource Speech Challenge 2015: Proposed approaches and results", "author": ["M. Versteegh", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. SLTU, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "A segmental framework for fully-unsupervised large-vocabulary speech recognition", "author": ["H. Kamper", "A. Jansen", "S. Goldwater"], "venue": "arXiv preprint arXiv:1606.06950, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised word discovery from speech using automatic segmentation into syllablelike units", "author": ["O.J. R\u00e4s\u00e4nen", "G. Doyle", "M.C. Frank"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A modified K-means clustering algorithm for use in isolated work recognition", "author": ["J.G. Wilpon", "L.R. Rabiner"], "venue": "IEEE Trans. Acoust. Speech Signal Process., vol. 33, no. 3, pp. 587\u2013594, 1985.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1985}, {"title": "A segmental kmeans training procedure for connected word recognition", "author": ["L.R. Rabiner", "J.G. Wilpon", "B.-H. Juang"], "venue": "AT&T Tech. J., vol. 65, no. 3, pp. 21\u201331, 1986.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1986}, {"title": "The segmental K-means algorithm for estimating parameters of hidden Markov models", "author": ["B.-H. Juang", "L.R. Rabiner"], "venue": "IEEE Trans. Acoust. Speech Signal Process., vol. 38, no. 9, pp. 1639\u20131641, 1990.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1990}, {"title": "On the use of acoustic unit discovery for language recognition", "author": ["S.H. Shum", "D.F. Harwath", "N. Dehak", "J.R. Glass"], "venue": "IEEE Trans. Acoust., Speech, Signal Process., vol. 24, no. 9, pp. 1665\u20131676, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of audio segment representations using sequence-tosequence recurrent neural networks", "author": ["Y.-A. Chung", "C.-C. Wu", "C.-H. Shen", "H.-Y. Lee"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative acoustic word embeddings: Recurrent neural network-based approaches", "author": ["S. Settle", "K. Livescu"], "venue": "Proc. SLT, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-view recurrent neural acoustic word embeddings", "author": ["W. He", "W. Wang", "K. Livescu"], "venue": "Proc. ICLR, 2017.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "End-to-end ASR-free keyword search from speech", "author": ["K. Audhkhasi", "A. Rosenberg", "A. Sethy", "B. Ramabhadran", "B. Kingsbury"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "Algorithms, etc.", "author": ["J. Erickson"], "venue": "UIUC Lecture Notes,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Gibbs sampling for the uninitiated", "author": ["P. Resnik", "E. Hardisty"], "venue": "University of Maryland, College Park, MD, Tech. Rep., 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian Reasoning and Machine Learning", "author": ["D. Barber"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Revisiting k-means: New algorithms via Bayesian nonparametrics", "author": ["B. Kulis", "M.I. Jordan"], "venue": "Proc. ICML, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "The Zero Resource Speech Challenge 2015", "author": ["M. Versteegh", "R. Thiolli\u00e8re", "T. Schatz", "X.N. Cao", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Bridging the gap between speech technology and natural language processing: An evaluation toolbox for term discovery systems", "author": ["B. Ludusan", "M. Versteegh", "A. Jansen", "G. Gravier", "X.-N. Cao", "M. Johnson", "E. Dupoux"], "venue": "Proc. LREC, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["K. Levin", "K. Henry", "A. Jansen", "K. Livescu"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Conjugate Bayesian analysis of the Gaussian distribution", "author": ["K.P. Murphy"], "venue": "2007. [Online]. Available: http://www.cs.ubc.ca/ \u223cmurphyk/mypapers.html", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": ", unwritten or endangered languages [1].", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "In addition, such methods may shed light on how human infants acquire language [2, 3].", "startOffset": 79, "endOffset": 85}, {"referenceID": 2, "context": "In addition, such methods may shed light on how human infants acquire language [2, 3].", "startOffset": 79, "endOffset": 85}, {"referenceID": 3, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 81, "endOffset": 86}, {"referenceID": 5, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 81, "endOffset": 86}, {"referenceID": 6, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 125, "endOffset": 130}, {"referenceID": 7, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 125, "endOffset": 130}, {"referenceID": 8, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 125, "endOffset": 130}, {"referenceID": 9, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 156, "endOffset": 164}, {"referenceID": 10, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 156, "endOffset": 164}, {"referenceID": 11, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 185, "endOffset": 192}, {"referenceID": 12, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 185, "endOffset": 192}, {"referenceID": 13, "context": "Early work mainly focused on unsupervised term discovery, where the aim is to automatically find repeated wordor phrase-like patterns in a collection of speech [14\u201316].", "startOffset": 160, "endOffset": 167}, {"referenceID": 14, "context": "Early work mainly focused on unsupervised term discovery, where the aim is to automatically find repeated wordor phrase-like patterns in a collection of speech [14\u201316].", "startOffset": 160, "endOffset": 167}, {"referenceID": 15, "context": "Early work mainly focused on unsupervised term discovery, where the aim is to automatically find repeated wordor phrase-like patterns in a collection of speech [14\u201316].", "startOffset": 160, "endOffset": 167}, {"referenceID": 16, "context": "This has prompted several studies on full-coverage approaches, where the entire speech input is segmented and clustered into word-like units [17\u201320].", "startOffset": 141, "endOffset": 148}, {"referenceID": 17, "context": "This has prompted several studies on full-coverage approaches, where the entire speech input is segmented and clustered into word-like units [17\u201320].", "startOffset": 141, "endOffset": 148}, {"referenceID": 18, "context": "This has prompted several studies on full-coverage approaches, where the entire speech input is segmented and clustered into word-like units [17\u201320].", "startOffset": 141, "endOffset": 148}, {"referenceID": 19, "context": "This has prompted several studies on full-coverage approaches, where the entire speech input is segmented and clustered into word-like units [17\u201320].", "startOffset": 141, "endOffset": 148}, {"referenceID": 20, "context": "Two such full-coverage approaches have recently been applied to the data of the Zero Resource Speech Challenge 2015 (ZRSC), giving a useful basis for comparison [21].", "startOffset": 161, "endOffset": 165}, {"referenceID": 21, "context": "The first is the Bayesian embedded segmental Gaussian mixture model (BES-GMM) [22]: a probabilistic model that represents potential word segments as fixed-dimensional acoustic word embeddings, and then builds a whole-word acoustic model in this space while jointly doing segmentation.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "The second is the recurring syllableunit segmenter (SylSeg) [23], which is a cognitively motivated, fast, heuristic method that applies unsupervised syllable segmentation and clustering and then predicts recurring syllable sequences as words.", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].", "startOffset": 198, "endOffset": 209}, {"referenceID": 18, "context": "These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].", "startOffset": 198, "endOffset": 209}, {"referenceID": 21, "context": "These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].", "startOffset": 198, "endOffset": 209}, {"referenceID": 17, "context": "These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].", "startOffset": 267, "endOffset": 275}, {"referenceID": 22, "context": "These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].", "startOffset": 267, "endOffset": 275}, {"referenceID": 23, "context": "Hard approximations have been used since the start of probabilistic modelling in supervised speech recognition [24\u201326], and also in more recent work to improve the efficiency of an unsupervised Bayesian model [27].", "startOffset": 111, "endOffset": 118}, {"referenceID": 24, "context": "Hard approximations have been used since the start of probabilistic modelling in supervised speech recognition [24\u201326], and also in more recent work to improve the efficiency of an unsupervised Bayesian model [27].", "startOffset": 111, "endOffset": 118}, {"referenceID": 25, "context": "Hard approximations have been used since the start of probabilistic modelling in supervised speech recognition [24\u201326], and also in more recent work to improve the efficiency of an unsupervised Bayesian model [27].", "startOffset": 111, "endOffset": 118}, {"referenceID": 26, "context": "Hard approximations have been used since the start of probabilistic modelling in supervised speech recognition [24\u201326], and also in more recent work to improve the efficiency of an unsupervised Bayesian model [27].", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 27, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 28, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 29, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 30, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 31, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 10, "context": "Here we instead follow an acoustic word embedding approach [11, 28]: an embedding function fe is used to map a variable length speech segment to a single embedding vector x \u2208 R in a fixed-dimensional space, i.", "startOffset": 59, "endOffset": 67}, {"referenceID": 27, "context": "Here we instead follow an acoustic word embedding approach [11, 28]: an embedding function fe is used to map a variable length speech segment to a single embedding vector x \u2208 R in a fixed-dimensional space, i.", "startOffset": 59, "endOffset": 67}, {"referenceID": 19, "context": "In previous work [20, 22], we proposed a very similar model, but instead of k-means, we used a Bayesian GMM as wholeword clustering component (top of Figure 1).", "startOffset": 17, "endOffset": 25}, {"referenceID": 21, "context": "In previous work [20, 22], we proposed a very similar model, but instead of k-means, we used a Bayesian GMM as wholeword clustering component (top of Figure 1).", "startOffset": 17, "endOffset": 25}, {"referenceID": 33, "context": "Under this model, component assignments and a segmentation can be inferred jointly using a collapsed Gibbs sampler [34].", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "Full details are given in [20], but the Gibbs sampler looks very similar to Algorithm 1: the Bayesian GMM gives likelihood terms (\u201cscores\u201d) in order to find an optimal segmentation, while the segmentation hypothesizes the boundaries for the word segments which are then clustered using the GMM.", "startOffset": 26, "endOffset": 30}, {"referenceID": 35, "context": "5], [36].", "startOffset": 4, "endOffset": 8}, {"referenceID": 19, "context": "In a similar way it can be shown that the Gibbs sampling equations for segmentation and component assignments for BES-GMM (as given in [20]) approach (3) and (5), respectively, in the limit \u03c3 \u2192 0, when all other hyperparameters are fixed.", "startOffset": 135, "endOffset": 139}, {"referenceID": 26, "context": "Parallelizing BES-GMM is also possible, but the guarantee of converging to the true posterior distribution is lost [27].", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "5 hours from 24 speakers [37].", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "As in [20, 22, 38], we use several metrics to evaluate against ground truth forced alignments.", "startOffset": 6, "endOffset": 18}, {"referenceID": 21, "context": "As in [20, 22, 38], we use several metrics to evaluate against ground truth forced alignments.", "startOffset": 6, "endOffset": 18}, {"referenceID": 37, "context": "As in [20, 22, 38], we use several metrics to evaluate against ground truth forced alignments.", "startOffset": 6, "endOffset": 18}, {"referenceID": 37, "context": "See [38] for full details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "Our implementation of ES-KMeans follows as closely as possible that of BES-GMM in [22].", "startOffset": 82, "endOffset": 86}, {"referenceID": 38, "context": "Both use uniform downsampling as embedding function fe: a segment is represented by keeping 10 equally spaced MFCCs and flattening these [39].", "startOffset": 137, "endOffset": 141}, {"referenceID": 22, "context": "Both models use unsupervised syllable pre-segmentation [23] to limit allowed word boundaries.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "See [22] for full details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "Some of the SylSeg scores are unknown since these were not part of the ZRSC evaluation [23].", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "Runtimes for SylSeg\u2217 are rough estimates, obtained from personal communication with the authors [23].", "startOffset": 96, "endOffset": 100}, {"referenceID": 39, "context": "This is because, based on the recommendation in [40], we set the variance of the prior on the component means of BES-GMM as \u03c3 0 = \u03c3/\u03ba0 (see end of \u00a73.", "startOffset": 48, "endOffset": 52}, {"referenceID": 39, "context": "Murphy [40] explains that this coupling is a sensible way to incorporate prior knowledge of the typical spread of data, and here we indeed show how this helps our Bayesian model; this principled way of including priors is not possible in ES-KMeans or SylSeg.", "startOffset": 7, "endOffset": 11}], "year": 2017, "abstractText": "Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most competitive approaches lie at methodological extremes: some follow a Bayesian approach, defining probabilistic models with convergence guarantees, while others opt for more efficient heuristic techniques. Here we introduce an approximation to a segmental Bayesian model that falls in between, with a clear objective function but using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental k-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. On English and Xitsonga data, ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being five times faster with fewer hyperparameters. However, there is a trade-off in cluster purity, with the Bayesian model\u2019s purer clusters yielding about 10% better unsupervised word error rates.", "creator": "LaTeX with hyperref package"}}}