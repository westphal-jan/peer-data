{"id": "1704.00325", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2017", "title": "Structured Parallel Programming for Monte Carlo Tree Search", "abstract": "In lithological this paper, we 1986-87 present a new 28,800 algorithm cappio for parallel tunics Monte cono Carlo margoth tree anglo-japanese search (MCTS ). It songea is based ethnologist on tremblant the unzue pipeline nh\u01b0 pattern brimming and allows pincay flexible megadeal management of the control rathgar flow echobay of -1.3 the rightful operations cavan in parallel paternity MCTS. goggled The pipeline goole pattern 53-meter provides doorknobs for the first monterrubio structured 21-christoph parallel programming 31.82 approach to MCTS. holidaymakers Moreover, we propose a kalna new follows lock - free belau tree komansky data anti-authoritarian structure wiss for feluda parallel bceao MCTS carracci which removes synchronization aitcheson overhead. The Pipeline Pattern for randles Parallel MCTS algorithm (mubanga called 3PMCTS ), plushchenko scales memories very gondo well graphite to higher thornback numbers hilty of cores saidel when compared to demilitarization the indarti existing methods.", "histories": [["v1", "Sun, 2 Apr 2017 16:22:31 GMT  (1857kb)", "http://arxiv.org/abs/1704.00325v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["s ali mirsoleimani", "aske plaat", "jaap van den herik", "jos vermaseren"], "accepted": false, "id": "1704.00325"}, "pdf": {"name": "1704.00325.pdf", "metadata": {"source": "CRF", "title": "Structured Parallel Programming for Monte Carlo Tree Search", "authors": ["S. Ali Mirsoleimani", "Aske Plaat", "Jaap van den Herik"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n00 32\n5v 1\n[ cs\n.A I]\n2 A\npr 2\n01 7\nI. INTRODUCTION\nIn recent years there has been much interest in the Monte Carlo tree search (MCTS) algorithm, at that time a new, adaptive, randomized optimization algorithm [1], [2]. In fields as diverse as Artificial Intelligence, Operations Research, and High Energy Physics, research has established that MCTS can find valuable approximate answers without domain-dependent heuristics [3]. The strength of the MCTS algorithm is that it provides answers with a random amount of error for any fixed computational budget [4]. The amount of error can typically be reduced by expanding the computational budget for more running time. Much effort has been put into the development of parallel algorithms for MCTS to reduce the running time. The efforts have as their target a broad spectrum of parallel systems; ranging from small shared-memory multicore machines (CPU) to large distributed-memory clusters. The emergence of the Intel Xeon Phi (Phi) co-processor with a large number (over 60) of simple cores has extended this spectrum with shared-memory manycore processors. Indeed, there is a full array of different parallel MCTS algorithms [5]\u2013[10]. However, there is still no structured parallel programming approach, based on computation patterns, for MCTS. In this paper, we propose a new algorithm based on the Pipeline Pattern for Parallel MCTS, called 3PMCTS.\nThe standard MCTS algorithm has four operations inside its main loop (Figure 1). In this loop, the computation associated with each iteration is assumed to be independent. Existing parallel methods use iteration-level (IL) parallelism. They assign each iteration to a separate processing element (thread) for execution on separate processors [5], [8], [9]. All three publications are facing a bottleneck in their implementation since they can not partition the iteration into constituent parts (operations) for parallel execution. Close analysis has learned\nus that the loop can actually be decomposed into separate operations for parallelization. Therefore, in our new design we introduce operation-level (OL) parallelism. The main idea is that the 3PMCTS algorithm assigns each operation to a separate processing element for execution by separate processors. This leads to flexibility in managing the control flow of operations in the MCTS algorithm.\nOur approach of applying structured parallel programming focuses the attention on (1) a design issue and (2) an implementation issue. For the design issue, we describe how the pipeline pattern is used as a building block in the design of 3PMCTS. It consists of a precise arrangement of tasks and data dependencies in MCTS. For the implementation, it is important to measure the performance of 3PMCTS on real machines. We present the implementation of 3PMCT by using Threading Building Blocks (TBB) [11] and we measure its performance on CPU and Phi.1 This paper has three contributions:\n1) A new structured algorithm based on the pipeline pattern\nis introduced for parallel MCTS.\n2) A new lock-free tree data structure for parallel MCTS\nis presented.\n3) A new TBB implementation based on the concept of\ntoken is proposed. The experimental results show that our implementation scales well.\nThe remainder of the paper is organized as follows. In section II the required background information is briefly described. Section III provides necessary definitions and explanations for the design issue of structured parallel programming for 3PMCTS. Section IV gives the explanations for the implementation issue of the 3PMCTS algorithm. Section V provides the proposed lock-free tree data structure, Section VI the experimental setup of the 3PMCTS, and Section VII the experimental results for 3PMCTS. Section VIII discusses related work. Finally, in Section IX we conclude the paper.\nII. BACKGROUND\nBelow we discuss in II-A MCTS, in II-B tree parallelization,\nand in II-C TBB.\n1We also discuss two elements related to the implementation of 3PMCTS, including the concept of token and a new lock-free tree data structure. A lockfree tree data structure plays a critical role in any parallel implementation for MCTS to be scalable."}, {"heading": "A. MCTS", "text": "Figure 1 shows a flowchart of MCTS [1]. The MCTS algorithm iteratively repeats four steps to construct a search tree until a predefined computational budget (i.e., time or iteration constraint) is reached.\n1) SELECT: Starting at the root node, a path of nodes\ninside the search tree is selected until a non-terminal node with unvisited children is reached. Each of the nodes is selected based on a selection policy. Among the proposed selection policies, the Upper Confidence Bounds for Trees (UCT) is one of the most commonly used policies [2], [12]. A child node j is selected to maximize:\nUCT = Xj + Cp\n\u221a\nln(n)\nnj (1)\nwhere Xj = wj nj is the average reward from child j, wj is the reward value of child j, nj is the number of times child j has been visited, n is the number of times the current node has been visited, and Cp \u2265 0 is a constant. The first term in the UCT equation is for exploitation of known parts of the tree and the second term is for exploration of unknown parts [12]. The level of exploration of the UCT algorithm can be adjusted by tuning the Cp constant. 2) EXPAND: One of the children of the selected non-\nterminal node is generated and added to the selected path. 3) PLAYOUT: From the given state of the newly added\nnode, a sequence of randomly simulated actions is performed until a terminal state in the state space is reached. The terminal state is evaluated to produce a\nreward value \u2206. 4) BACKUP: In the selected path, each node\u2019s visit count\nn is incremented by 1 and its reward value w updated according to \u2206 [12].\nAs soon as the computational budget is exhausted, the best child of the root node is returned."}, {"heading": "B. Tree Parallelization", "text": "In tree parallelization, one search tree is shared among several threads that are performing simultaneous searches [5]. The main challenge in this method is the prevention of data corruption. A lock-based method uses fine-grained locks to protect shared data. However, this approach suffers from synchronization overhead due to thread contentions and does not scale well [5]. A lock-free implementation of addresses the problem and scales better [13]. However, the method in [13] does not guarantee the computational consistency of the multithreaded program with the single-threaded program. Figure 2 shows the tree parallelization without locks."}, {"heading": "C. TBB", "text": "TBB is a C++ template library developed by Intel for writing software programs that take advantage of a multicore processor [14]. The TBB implementation of pipelines uses a technique that enables greedy scheduling, but the greed must be constrained to bound memory consumption. The user specifies the constraint as a maximum number of items allowed to flow simultaneously through the pipeline [14].\nIII. DESIGN OF 3PMCTS\nIn this section, we describe our structured parallel programming approach for MCTS. In section III-A we explain how to decompose MCTS into tasks. In section III-B we investigate what types of data dependencies exist among these tasks. Section III-C describes how the pipeline pattern is applied in MCTS. Finally, section III-D provides our design for the 3PMCTS algorithm."}, {"heading": "A. Decomposition into Tasks", "text": "The first step towards designing our 3PMCTS algorithm is to find concurrent tasks in MCTS. There are two levels of task decomposition in MCTS.\n1) Iteration-level (IL) tasks: In MCTS the computa-\ntion associated with each SELECT-EXPAND-PLAYOUTBACKUP-iteration is independent. Therefore, these are candidates to guide a task decomposition by mapping each iteration onto a task. 2) Operation-level (OL) tasks: The task decomposition for\nMCTS occurs inside each iteration. Each of the four MCTS steps can be treated as a separate task."}, {"heading": "B. Data Dependencies", "text": "In 3PMCTS, a search tree is shared among multiple parallel\ntasks. Therefore, there are two levels of data dependency.\n1) Iteration-level (IL) dependencies: Strictly speaking, in\nMCTS, iteration j has a soft dependency to its predecessor iteration j\u2212 1. Obviously, to select an optimal path, it requires updates on the search tree from the previous iteration.2 A parallel MCTS can ignore IL dependencies and simply suffers from the search overhead.3 2) Operation-level (OL) dependencies: Each of the four\noperations in MCTS has a hard dependency to its predecessor.4 For example, the expansion of a path cannot start until the selection operation has been completed."}, {"heading": "C. Pipeline Pattern", "text": "In this section, we focus on the pipeline pattern in MCTS using OL tasks. The pipeline pattern is the most straightforward way to enforce the required ordering among the OL tasks. Below we explain two possible types of pipelines for MCTS.\n1) Linear pipeline: Figure 3a shows a linear MCTS pipeline\nwith the selected paths inside the search tree; from one stage to the next stage buffers are given a path as operations are completed. 2) Non-linear pipeline: Figure 3b shows a non-linear\nMCTS pipeline with two parallel PLAYOUT stages. Both of the PLAYOUT stages take paths produced by the EXPAND stage of the pipeline."}, {"heading": "D. Parallelism of a Pipeline", "text": "The existing parallel methods such as tree parallelization use IL tasks. There are only IL dependencies when performing\n2i.e., a violation of IL dependency does not impact the correctness of the algorithm.\n3Occurs when a parallel implementation in a search algorithm searches more nodes of the search space than the sequential version; for example, since the information to guide the search is not yet available.\n4i.e., a violation of OL dependency yields an incorrect algorithm.\nIL parallelism. The potential concurrency is exploited by assigning each of the IL tasks to a separate processing element and having them work on separate processors. So far IL parallelism is investigated quite extensively [5], [8]\u2013[10], [13].\nIn contrast to the existing methods, our 3PMCTS algorithm uses OL tasks which have both IL and OL dependencies. The OL tasks are assigned to the stages of a pipeline. The pipeline pattern can satisfy the OL dependencies among the OL tasks. The potential concurrency is also exploited by assigning each stage of the pipeline to a separate processing element for execution on separate processors. If the pipeline is linear then the scalability is limited to the number of stages.5 However in MCTS, the operations are not equally computationally intensive, e.g., generally the PLAYOUT operation (random simulations plus evaluation of a terminal state) is more computationally expensive than other operations. Therefore, our 3PMCTS algorithm uses a non-linear pipeline with parallel stages. Introducing parallel stages makes 3PMCTS more scalable. The 3PMCTS algorithm, depicted in Figure 4, has three parallel stages (i.e., EXPAND, Random Simulation, and Evaluation). It will be usable in almost any sufficiently powerful parallel programming model (e.g., TBB [14] or Cilk [15]).\nIV. IMPLEMENTATION OF 3PMCTS\nIn this section, we introduce the implementation of our 3PMCTS algorithm. In section IV-A we present the concept of token (when used as type name, we write Token). Section IV-B describes the implementation of 3PMCTS using TBB."}, {"heading": "A. Token", "text": "A token represents a path inside the search tree during the search. Algorithm 1 presents definition for the type Token. It has four fields. (1) id represents a unique identifier for a token, (2) v represents the current node in the tree, (3) s represents the search state of the current node, and (4) \u2206 represents the reward value of the state. In our implementation for 3PMCTS, each stage (task) performs its operation on a token. We can also specify the number of in-flight tokens.\n5When the operations performed by the various stages are all about equally computationally intensive.\nAlgorithm 1: Type definition for token.\n1 type 2 type id : int; 3 type v : Node*; 4 type s : State*; 5 type \u2206 : int; 6 Token;\nAlgorithm 2: Serial implementation of MCTS, with stages SELECT, EXPAND, PLAYOUT, and BACKUP.\n1 Function UCTSEARCH(s0) 2 v0 = create root node with state s0; 3 t0.s = s0; 4 t0.v = v0; 5 while within search budget do 6 tl = SELECT(t0); 7 tl = EXPAND(tl); 8 tl = PLAYOUT(tl); 9 BACKUP(tl);\n10 end"}, {"heading": "B. TBB Implementation", "text": "The pseudocode of MCTS is shown in Algorithm 2. A data structure of type State describes the search state of the current node in the tree and a data structure of type Node shows the current node being searched inside the tree. The functions of the MCTS algorithm are defined in Algorithm 3. Each function constitutes a stage of the non-linear pipeline in 3PMCTS. There are two approaches for parallel implementation of a non-linear pipeline [16]:\n\u2022 Bind-to-stage: A processing element (e.g., thread) is\nbound to a stage and processes tokens as they arrive. If the stage is parallel, it may have multiple processing elements bound to it. \u2022 Bind-to-item: A processing element is bound to a token\nand carries the token through the pipeline. When the processing element completes the last stage, it goes to the first stage to select another token.\nOur implementation for 3PMCTS algorithm is based on a bind-to-item approach. Figure 4 depicts a five-stage pipeline for 3PMCTS that can be implemented using TBB tbb::parallel pipeline template [14]. The five stages run the functions SELECT, EXPAND, RandomSimulation, Evaluation, and BACKUP, in that order. The first (SELECT) and last stage (BACKUP) are serial in-order; They process one token at a time. The three middle stages (EXPAND, RandomSimulation, and Evaluation) are parallel and do the most time-consuming part of the search. The Evaluation and RandomSimulation functions are extracted out of the PLAYOUT function to achieve more parallelism. The serial version uses a single token. The 3PMCTS algorithm aims to search multiple paths in parallel. Therefore, it needs more than one in-flight token.\nAlgorithm 3: The functions of the MCTS algorithm.\n1 Function SELECT(Token t) : <Token> 2 while t.v \u2192IsFullyExpanded() do\n3 t.v := argmax v \u2032 \u2208childrenofv\nv \u2032\n.UCT(Cp);\n4 t.s \u2192SetMove(t.v \u2192 move); 5 end 6 return t;\n7 Function EXPAND(Token t) : <Token> 8 if !(t.s \u2192IsTerminal()) then\n9 moves := t.s \u2192UntriedMoves(); 10 shuffle moves uniformly at random; 11 t.v \u2192Init(moves); 12 v \u2032 := t.v \u2192AddChild(); 13 if t.v 6= v \u2032 then 14 t.v :=v \u2032 ; 15 t.s \u2192SetMove(v \u2032 \u2192 move); 16 end 17 end 18 return t;\n19 Function RandomSimulation(Token t) 20 moves := t.s \u2192UntriedMoves(); 21 shuffle moves uniformly at random; 22 while !(t.s \u2192IsTerminal()) do 23 choose new move \u2208 moves; 24 t.s \u2192SetMove(move); 25 end 26 return t\n27 Function Evaluation(Token t) 28 t.\u2206 := t.s \u2192 Evalute(); 29 return t\n30 Function BACKUP(Token t) : void 31 while t.v 6= null do 32 t.v \u2192 Update(t.\u2206); 33 t.v := t.v \u2192 parent; 34 end\nFigure 5 shows the key parts of the TBB code with the syntactic details for the 3PMCTS algorithm.\nV. LOCK-FREE SEARCH TREE\nIn this section, we provide our new lock-free tree search. A potential bottleneck in a parallel implementation is the race condition. A race condition occurs when concurrent threads perform operations on the same memory location without proper synchronization, and one of the memory operations is a write [16]. Consider the example search tree in Figure 6a. Three parallel threads attempt to perform MCTS operations on the shared search tree. There are three race condition scenarios.\n1) Shared Expansion (SE): Figure 6b shows two threads\n(1 and 2) concurrently performing EXPAND(v6). In this SE scenario, synchronization is required. Obviously, a\nrace condition exists if two parallel threads intend to initialize the list of children in a node simultaneously. In such an SE race, the list of children for a selected node should be created only once. Enzenberger et al. assign to each thread an own memory array for creating a list of new children [17]. Only after the children are fully created and initialized, they are linked to the parent node. Of course, this causes memory overhead. What usually happens is the following. If several threads expand the same node, only the children created by the last thread will be used in future simulations. It can also happen that some of the children that are lost already received\nsome updates; these updates will also be lost.\n2) Shared Backup(SB): Figure 6c shows two threads (1 and 3) concurrently performing BACKUP(v3). In the SB scenario, synchronization is required because it is a race\ncondition that parallel threads update the value of w and n in a node simultaneously. Enzenberger et. al ignore these race conditions. They accept the possible faulty updates and the inconsistency of parallel computation. 3) Shared Backup and Selection (SBS): Figure 6d shows\nthread 2 performing BACKUP(v3) and thread 3 performing SELECT(v3). In the SBS scenario, synchronization is required because it is a race condition in which a\nthread reads the value of w, and before reading the value of n, another thread updates the value of w and n. In this case, the first thread reads inconsistent values. Enzenberger et al. ignore these race conditions. They accept the possible faulty updates and the inconsistency of parallel computation.\nAlgorithm 4 shows our new lock-free tree data structure of type Node for MCTS. Our lock-free tree data structure uses the new multithreading-aware memory model of the C++11 Standard. In order to avoid the race conditions, the ordering between the memory accesses in the threads has to be enforced [18]. In our lock-free implementation, we use the synchronization properties of atomic operations to enforce an ordering between the accesses. We have used the atomic variants of the built-in types (i.e., atomic int and atomic bool); they are lock-free on most popular platforms. The standard atomic types have different member functions such as load(), store(), exchange(), fetch add(), and fetch sub(). The member function store() is a store operation, whereas the load() is a load operation. The exchange()member function replaces the stored value in the atomic variable with a new value and automatically retrieves the original value. We use two memory models for the memory-ordering option for all operations on atomic types: sequentially consistent ordering (memory order seq cst) and acquire release ordering (memory order acquire and memory order release). The sequentially consistent ordering implies that the behavior of a multithreaded program is consistent with a single threaded\nprogram. In the acquire release ordering model, load() is an acquire operation, store() is a release operation, exchange() or fetch add() or fetch sub() are either acquire, release or both (memory order acq rel). We have solved all the three above cases of race conditions (SE,SB, and SBS) using these two memory models and the atomic operations.\n1) A node has an isparent flag member. This flag indicates\nwhether the list of children is created or not. The isparent flag is initially set to false. To change the state of the node to be a parent, we set its isparent to true. Before an EXPAND adds a child to a node, it must create the list of children for the node and set the isparent to true. After a node successfully becomes a parent, one of the unexpanded children in this list can be added to the node. It prevents the problem in EE that the list of children is created by two threads at the same time. Thus, the key steps in the EXPAND operation are as follows: (A, see Algorithm 4) change v6.isparent to true (i.e., no other thread can enter), (B) create the list of children for v6, (C) set the value of v6.untriedmoves, (D) set the value of v6.isexpandable to true (D1) and load the value of v6.isexpandable (D2), and (E) untriedmoves is used as a count of the number of items in the list of children. 2) In the SB and SBS race conditions, we use atomic\ntypes for variables w and n. The thread accesses to these variables (reads (F1,F2) and writes (G1,G2)) are sequentially consistent. This memory model preserve the order of operations in all threads. Therefore we have no faulty updates and guarantee consistency of computation.\nVI. EXPERIMENTAL SETUP\nThe performance of 3PMCTS is measured by using a High Energy Physics (HEP) expression simplification problem [3]. Our setup follows closely [3]. We discuses in VI-A the case study, in VI-B the hardware, and in VI-C the performance metrics."}, {"heading": "A. Case Study", "text": "Horner\u2019s rule is an algorithm for polynomial computation that reduces the number of multiplications and results in a computationally efficient form. For a polynomial in one variable\np(x) = anx n + an\u22121x n\u22121 + ...+ a0, (2)\nthe rule simply factors out powers of x. Thus, the polynomial can be written in the form\np(x) = ((anx+ an\u22121)x+ ...)x+ a0. (3)\nThis representation reduces the number of multiplications to n and has n additions. Therefore, the total evaluation cost of the polynomial is 2n. Horner\u2019s rule can be generalized for multivariate polynomials. Here, Eq. 3 applies on a polynomial for each variable, treating the other variables as constants. The order of choosing\nAlgorithm 4: Type definition for a lock-free tree data structure.\n1 type 2 type move : int; 3 type w : atomic int; 4 type n : atomic int; 5 type isparent := false : atomic bool; 6 type untriedmoves := \u22121 : atomic int; 7 type isexpandable := false : atomic bool; 8 type isfullyexpanded := false : atomic bool; 9 type parent : Node*;\n10 type children : Node*[]; 11 Function Init(moves) : <void> 12 int nomoves = moves.size(); 13 if !(isparent.exchange(true)); \u22b2 [see A] 14 then 15 initialize list of children using moves; \u22b2 [see B] 16 untriedmoves.store(nomoves); \u22b2 [see C] 17 isexpandable.store(true,memory order release); \u22b2 [see D1] 18 end 19 Function AddChild() : <Node*> 20 int index; 21 if isexpandable.load(memory order acquire); \u22b2 [see D2] 22 then 23 if (index := undriedmoves.fetch sub(1)) = 0; \u22b2 [see E] 24 then 25 isfullyexpanded.store(true); 26 end 27 if index < 0 then 28 return current node; 29 else 30 return children[index]; 31 end 32 else 33 return current node; 34 end 35 Function IsFullyExpanded() : <bool> 36 return isfullyexpanded.load(); 37 Function UCT(Cp) : <float> 38 w \u2032 := w.load(memory order seq cst); \u22b2 [see F1] 39 n \u2032 := n.load(memory order seq cst); \u22b2 [see F2] 40 n\u201d := parent \u2192 n.load(memory order seq cst); 41 return w \u2032\nn \u2032 + Cp\n\u221a\nln(n\u201d)\nn \u2032\n42 Function Update(\u2206) : <void> 43 w.fetch add(\u2206,memory order seq cst); \u22b2 [see G1] 44 n.fetch add(1,memory order seq cst); \u22b2 [see G2] 45 Node;\nvariables may be different, each order of the variables is called a Horner scheme.\nThe number of operations can be reduced even more by performing common subexpression elimination (CSE) after transforming a polynomial with Horner\u2019s rule. CSE creates new symbols for each subexpression that appears twice or\nmore and replaces them inside the polynomial. Then, the subexpression has to be computed only once.\nWe are using the HEP(\u03c3) expression with 15 variables to study the results of 3PMCTS. The MCTS is used to find an order of the variables that gives efficient Horner schemes [3]. The root node has n children, with n the number of\nvariables. The children of other nodes represent the remaining unchosen variables in the order. Starting at the root node, a path of nodes (variables) inside the search tree is selected. The incomplete order is completed with the remaining variables added randomly (RandomSimulation). This complete order is then used for Horners method followed by CSE (Evaluation). The number of operations in this optimized expression is counted (\u2206)."}, {"heading": "B. Hardware", "text": "Our experiments were performed on a dual socket Intel machine with 2 Intel Xeon E5-2596v2 CPUs running at 2.4 GHz. Each CPU has 12 cores, 24 hyperthreads, and 30 MB L3 cache. Each physical core has 256KB L2 cache. The peak TurboBoost frequency is 3.2 GHz. The machine has 192GB physical memory. We compiled the code using the Intel C++ compiler with a -O3 flag."}, {"heading": "C. Performance Metrics", "text": "The primary goal of parallelization is performance. There are two important metrics related to performance and parallelism for MCTS.\n1) Playout Speedup (PS): If we have a fixed number of\nplayouts seen as the search budget, then\nPS = time in sequential\ntime in parallel . (4)\n2) Search Overhead (SO): If we have to find a desired\noptimal point in the search space, then\nSO = required # of playouts in parallel\nrequired # of playouts in sequential \u2212 1. (5)\nIf the parallel MCTS algorithm expands more nodes (i.e., do more playouts) than the equivalent serial algorithm to solve a problem, then there is SO.\nIn this paper, we use playout-speedup to report the performance.\nVII. EXPERIMENTAL RESULTS\nIn this section, the performance of 3PMCTS is measured.\n1) Playout-speedup for CPU: The graph in Figure 7a shows\nthe playout-speedup for both 3PMCTS and tree parallelization, as a function of the number of tokens on CPU. Both 3PMCTS and tree parallelization are doing 1024 playouts. We see a playout-speedup for 3PMCTS close to 22 for 56 tokens. A playout-speedup close to 21 is observed for tree parallelization for 47 tasks. After 48 tasks the playout-speedup for tree parallelization drops (it is run on a machine with 48 hyperthreads) while the performance of 3PMCTS continues to grow until it becomes stable. 2) Playout-speedup for Phi: The graph in Figure 7b shows\nthe playout-speedup for both 3PMCTS and tree parallelization, as a function of the number of tokens on Phi. Both 3PMCTS and tree parallelization are doing 1024 playouts. We see a playout-speedup for 3PMCTS close to 41 for 128 tokens. A playout-speedup close to 36 is observed for tree parallelization for 128 tasks.\nFrom these results, we may provisionally conclude that the 3PMCTS algorithm shows a playout-speedup as good as tree parallelization, for a well-studied problem. It allows finegrained managing of the control flow of operations in MCTS in contrast to tree parallelization.\nVIII. RELATED WORK\nBelow we review related work on MCTS parallelizations. The two major parallelization methods for MCTS are root parallelization and tree parallelization [5]. There exist also less\nfrequently encountered techniques, such as leaf parallelization [5] and approaches based on transposition table driven work scheduling (TDS) [6], [19].\n1) Tree parallelization: For shared memory machines, tree\nparallelization is a suitable method. It is used in FUEGO, a well-known open source Go program [17]. In tree parallelization one MCTS tree is shared among several threads that are performing simultaneous searches [5]. It is shown in [5] that the playout-speedup of tree parallelization with virtual loss does not scale perfectly up to 16 threads. The main challenge is the use of locks to prevent data corruption. 2) Root parallelization: Chaslot et al. [5] report that root\nparallelization shows perfect playout-speedup up to 16 threads. Soejima et al. [20] analyzed the performance of root parallelization in detail. They revealed that a Go program that uses lock-free tree parallelization with 4 to 8 threads outperformed the same program with root parallelization that utilized 64 distributed CPU cores. This result suggests the superiority of tree parallelization over root parallelization in shared memory machines. Fern and Lewis [7] thoroughly investigated an Ensemble UCT approach in which multiple instances of UCT were run independently. Their root statistics were combined to yield the final result, showing that Ensembles can significantly improve performance per unit time in a parallel model. This is also shown in [10].\nIX. CONCLUSION AND FUTURE WORK\nMonte Carlo Tree Search (MCTS) is a randomized algorithm that is successful in a wide range of optimization problems. The main loop in MCTS consists of individual iterations, suggesting that the algorithm is well suited for parallelization. The existing parallelization methods, e.g., tree parallelization, simply fans out the iterations over available cores.\nIn this paper, a structured parallel programming approach is used to develop a new parallel algorithm based on the pipeline pattern for MCTS. The idea is to break-up the iterations themselves, splitting them into individual operations, which are then parallelized in a pipeline. Experiments with an application from High Energy Physics show that our implementation of 3PMCTS scales well. Scalability is only one issue, although it is an important one. The second issue is flexibility of task decomposition in parallelism. These flexibilities allow fine-grained managing of the control flow of operations in MCTS. We consider the flexibility an even more important characteristic of 3PMCTS.\nWe may conclude the following. Our new method is highly suitable for heterogeneous computing because it is possible that some of the MCTS operations might not be suitable for running on a target processor, whereas others are. Our 3PMCTS algorithm gives us full flexibility for offloading a variety of different operations of MCTS to a target processor. For future work, we will study the implementation of 3PMCTS on a heterogeneous machine.\nACKNOWLEDGMENT\nThis work is supported in part by the ERC Advanced Grant\nno. 320651, HEPGAME.\nREFERENCES\n[1] R. Coulom, \u201cEfficient Selectivity and Backup Operators in Monte-Carlo Tree Search,\u201d in Proceedings of the 5th International Conference on Computers and Games, ser. CG\u201906, vol. 4630. Springer-Verlag, may 2006, pp. 72\u201383. [2] L. Kocsis and C. Szepesva\u0301ri, \u201cBandit based Monte-Carlo Planning Levente,\u201d in ECML\u201906 Proceedings of the 17th European conference on Machine Learning, ser. Lecture Notes in Computer Science, J. Fu\u0308rnkranz, T. Scheffer, and M. Spiliopoulou, Eds., vol. 4212. Springer Berlin Heidelberg, sep 2006, pp. 282\u2013293. [3] J. Kuipers, A. Plaat, J. Vermaseren, and J. van den Herik, \u201cImproving Multivariate Horner Schemes with Monte Carlo Tree Search,\u201d Computer Physics Communications, vol. 184, no. 11, pp. 2391\u20132395, nov 2013. [4] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning, ser. Adaptive Computation and Machine Learning Series. MIT Press, 2016. [Online]. Available: https://books.google.nl/books?id=Np9SDQAAQBAJ [5] G. Chaslot, M. Winands, and J. van den Herik, \u201cParallel Monte-Carlo Tree Search,\u201d in the 6th Internatioal Conference on Computers and Games, vol. 5131. Springer Berlin Heidelberg, 2008, pp. 60\u201371. [6] K. Yoshizoe, A. Kishimoto, T. Kaneko, H. Yoshimoto, and Y. Ishikawa, \u201cScalable Distributed Monte-Carlo Tree Search,\u201d in Fourth Annual Symposium on Combinatorial Search, may 2011, pp. 180\u2013187. [7] A. Fern and P. Lewis, \u201cEnsemble Monte-Carlo Planning: An Empirical Study.\u201d in ICAPS, 2011, pp. 58\u201365. [8] L. Schaefers and M. Platzner, \u201cDistributed Monte-Carlo Tree Search : A Novel Technique and its Application to Computer Go,\u201d IEEE Transactions on Computational Intelligence and AI in Games, vol. 6, no. 3, pp. 1\u201315, 2014. [9] S. A. Mirsoleimani, A. Plaat, J. van den Herik, and J. Vermaseren, \u201cParallel Monte Carlo Tree Search from Multi-core to Many-core Processors,\u201d in ISPA 2015 : The 13th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA), Helsinki, 2015, pp. 77\u201383. [10] \u2014\u2014, \u201cScaling Monte Carlo Tree Search on Intel Xeon Phi,\u201d in Parallel and Distributed Systems (ICPADS), 2015 20th IEEE International\nConference on, 2015, pp. 666\u2013673. [11] \u201cIntel threading building blocks TBB.\u201d [Online]. Available:\nhttps://www.threadingbuildingblocks.org [12] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling,\nP. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton, \u201cA Survey of Monte Carlo Tree Search Methods,\u201d Computational Intelligence and AI in Games, IEEE Transactions on, vol. 4, no. 1, pp. 1\u201343, 2012. [13] M. Enzenberger and M. Mu\u0308ller, \u201cA lock-free multithreaded Monte-Carlo tree search algorithm,\u201d Advances in Computer Games, vol. 6048, pp. 14\u201320, 2010. [14] J. Reinders, Intel threading building blocks: outfitting C++ for multicore processor parallelism. \u201d O\u2019Reilly Media, Inc.\u201d, 2007. [15] C. E. Leiserson and A. Plaat, \u201cProgramming Parallel Applications in Cilk,\u201d SINEWS: SIAM News, vol. 31, no. 4, pp. 6\u20137, 1998. [16] M. McCool, J. Reinders, and A. Robison, Structured Parallel Programming: Patterns for Efficient Computation. Elsevier, 2012. [17] M. Enzenberger, M. Muller, B. Arneson, and R. Segal, \u201cFuegoAn OpenSource Framework for Board Games and Go Engine Based on Monte Carlo Tree Search,\u201d IEEE Transactions on Computational Intelligence and AI in Games, vol. 2, no. 4, pp. 259\u2013270, dec 2010. [18] A. Williams, C++ Concurrency in Action: Practical Multithreading, ser. Manning Pubs Co Series. Manning, 2012. [Online]. Available: https://books.google.nl/books?id=EttPPgAACAAJ [19] J. Romein, A. Plaat, H. E. Bal, and J. Schaeffer, \u201cTransposition Table Driven Work Scheduling in Distributed Search,\u201d in In 16th National Conference on Artificial Intelligence (AAAI\u201999), 1999, pp. 725\u2013731. [20] Y. Soejima, A. Kishimoto, and O. Watanabe, \u201cEvaluating Root Parallelization in Go,\u201d IEEE Transactions on Computational Intelligence and AI in Games, vol. 2, no. 4, pp. 278\u2013287, dec 2010."}], "references": [{"title": "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search", "author": ["R. Coulom"], "venue": "Proceedings of the 5th International Conference on Computers and Games, ser. CG\u201906, vol. 4630. Springer-Verlag, may 2006, pp. 72\u201383.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Bandit based Monte-Carlo Planning Levente", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "ECML\u201906 Proceedings of the 17th European conference on Machine Learning, ser. Lecture Notes in Computer Science, J. F\u00fcrnkranz, T. Scheffer, and M. Spiliopoulou, Eds., vol. 4212. Springer Berlin Heidelberg, sep 2006, pp. 282\u2013293.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Improving Multivariate Horner Schemes with Monte Carlo Tree Search", "author": ["J. Kuipers", "A. Plaat", "J. Vermaseren", "J. van den Herik"], "venue": "Computer Physics Communications, vol. 184, no. 11, pp. 2391\u20132395, nov 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep Learning, ser. Adaptive Computation and Machine Learning Series. MIT Press, 2016", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Parallel Monte-Carlo Tree Search", "author": ["G. Chaslot", "M. Winands", "J. van den Herik"], "venue": "the 6th Internatioal Conference on Computers and Games, vol. 5131. Springer Berlin Heidelberg, 2008, pp. 60\u201371.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Scalable Distributed Monte-Carlo Tree Search", "author": ["K. Yoshizoe", "A. Kishimoto", "T. Kaneko", "H. Yoshimoto", "Y. Ishikawa"], "venue": "Fourth Annual Symposium on Combinatorial Search, may 2011, pp. 180\u2013187.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Ensemble Monte-Carlo Planning: An Empirical Study.", "author": ["A. Fern", "P. Lewis"], "venue": "ICAPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Distributed Monte-Carlo Tree Search : A Novel Technique and its Application to Computer Go", "author": ["L. Schaefers", "M. Platzner"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 6, no. 3, pp. 1\u201315, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallel Monte Carlo Tree Search from Multi-core to Many-core Processors", "author": ["S.A. Mirsoleimani", "A. Plaat", "J. van den Herik", "J. Vermaseren"], "venue": "ISPA 2015 : The 13th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA), Helsinki, 2015, pp. 77\u201383.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Scaling Monte Carlo Tree Search on Intel Xeon Phi", "author": ["\u2014\u2014"], "venue": "Parallel and Distributed Systems (ICPADS), 2015 20th IEEE International Conference on, 2015, pp. 666\u2013673.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "A Survey of Monte Carlo Tree Search Methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on, vol. 4, no. 1, pp. 1\u201343, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A lock-free multithreaded Monte-Carlo tree search algorithm", "author": ["M. Enzenberger", "M. M\u00fcller"], "venue": "Advances in Computer Games, vol. 6048, pp. 14\u201320, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Intel threading building blocks: outfitting C++ for multicore processor parallelism", "author": ["J. Reinders"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Programming Parallel Applications in Cilk", "author": ["C.E. Leiserson", "A. Plaat"], "venue": "SINEWS: SIAM News, vol. 31, no. 4, pp. 6\u20137, 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Structured Parallel Programming: Patterns for Efficient Computation", "author": ["M. McCool", "J. Reinders", "A. Robison"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "FuegoAn Open- Source Framework for Board Games and Go Engine Based on Monte Carlo Tree Search", "author": ["M. Enzenberger", "M. Muller", "B. Arneson", "R. Segal"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 2, no. 4, pp. 259\u2013270, dec 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Concurrency in Action: Practical Multithreading", "author": ["C A. Williams"], "venue": "ser. Manning Pubs Co Series. Manning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Transposition Table Driven Work Scheduling in Distributed Search", "author": ["J. Romein", "A. Plaat", "H.E. Bal", "J. Schaeffer"], "venue": "In 16th National Conference on Artificial Intelligence (AAAI\u201999), 1999, pp. 725\u2013731.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Evaluating Root Parallelization in Go", "author": ["Y. Soejima", "A. Kishimoto", "O. Watanabe"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 2, no. 4, pp. 278\u2013287, dec 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "In recent years there has been much interest in the Monte Carlo tree search (MCTS) algorithm, at that time a new, adaptive, randomized optimization algorithm [1], [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "In recent years there has been much interest in the Monte Carlo tree search (MCTS) algorithm, at that time a new, adaptive, randomized optimization algorithm [1], [2].", "startOffset": 163, "endOffset": 166}, {"referenceID": 2, "context": "In fields as diverse as Artificial Intelligence, Operations Research, and High Energy Physics, research has established that MCTS can find valuable approximate answers without domain-dependent heuristics [3].", "startOffset": 204, "endOffset": 207}, {"referenceID": 3, "context": "The strength of the MCTS algorithm is that it provides answers with a random amount of error for any fixed computational budget [4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Indeed, there is a full array of different parallel MCTS algorithms [5]\u2013[10].", "startOffset": 68, "endOffset": 71}, {"referenceID": 9, "context": "Indeed, there is a full array of different parallel MCTS algorithms [5]\u2013[10].", "startOffset": 72, "endOffset": 76}, {"referenceID": 4, "context": "They assign each iteration to a separate processing element (thread) for execution on separate processors [5], [8], [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "They assign each iteration to a separate processing element (thread) for execution on separate processors [5], [8], [9].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "They assign each iteration to a separate processing element (thread) for execution on separate processors [5], [8], [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "Figure 1 shows a flowchart of MCTS [1].", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Among the proposed selection policies, the Upper Confidence Bounds for Trees (UCT) is one of the most commonly used policies [2], [12].", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "Among the proposed selection policies, the Upper Confidence Bounds for Trees (UCT) is one of the most commonly used policies [2], [12].", "startOffset": 130, "endOffset": 134}, {"referenceID": 10, "context": "The first term in the UCT equation is for exploitation of known parts of the tree and the second term is for exploration of unknown parts [12].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "4) BACKUP: In the selected path, each node\u2019s visit count n is incremented by 1 and its reward value w updated according to \u2206 [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 4, "context": "In tree parallelization, one search tree is shared among several threads that are performing simultaneous searches [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "However, this approach suffers from synchronization overhead due to thread contentions and does not scale well [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 11, "context": "A lock-free implementation of addresses the problem and scales better [13].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "However, the method in [13] does not guarantee the computational consistency of the multithreaded program with the single-threaded program.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "TBB is a C++ template library developed by Intel for writing software programs that take advantage of a multicore processor [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "The user specifies the constraint as a maximum number of items allowed to flow simultaneously through the pipeline [14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": "So far IL parallelism is investigated quite extensively [5], [8]\u2013[10], [13].", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "So far IL parallelism is investigated quite extensively [5], [8]\u2013[10], [13].", "startOffset": 61, "endOffset": 64}, {"referenceID": 9, "context": "So far IL parallelism is investigated quite extensively [5], [8]\u2013[10], [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "So far IL parallelism is investigated quite extensively [5], [8]\u2013[10], [13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": ", TBB [14] or Cilk [15]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": ", TBB [14] or Cilk [15]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "There are two approaches for parallel implementation of a non-linear pipeline [16]:", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "Figure 4 depicts a five-stage pipeline for 3PMCTS that can be implemented using TBB tbb::parallel pipeline template [14].", "startOffset": 116, "endOffset": 120}, {"referenceID": 14, "context": "A race condition occurs when concurrent threads perform operations on the same memory location without proper synchronization, and one of the memory operations is a write [16].", "startOffset": 171, "endOffset": 175}, {"referenceID": 15, "context": "assign to each thread an own memory array for creating a list of new children [17].", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "In order to avoid the race conditions, the ordering between the memory accesses in the threads has to be enforced [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "EXPERIMENTAL SETUP The performance of 3PMCTS is measured by using a High Energy Physics (HEP) expression simplification problem [3].", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "Our setup follows closely [3].", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "The MCTS is used to find an order of the variables that gives efficient Horner schemes [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "The two major parallelization methods for MCTS are root parallelization and tree parallelization [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "frequently encountered techniques, such as leaf parallelization [5] and approaches based on transposition table driven work scheduling (TDS) [6], [19].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "frequently encountered techniques, such as leaf parallelization [5] and approaches based on transposition table driven work scheduling (TDS) [6], [19].", "startOffset": 141, "endOffset": 144}, {"referenceID": 17, "context": "frequently encountered techniques, such as leaf parallelization [5] and approaches based on transposition table driven work scheduling (TDS) [6], [19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "It is used in FUEGO, a well-known open source Go program [17].", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "In tree parallelization one MCTS tree is shared among several threads that are performing simultaneous searches [5].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "It is shown in [5] that the playout-speedup of tree parallelization with virtual loss does not scale perfectly up to 16 threads.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "[5] report that root parallelization shows perfect playout-speedup up to 16 threads.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[20] analyzed the performance of root parallelization in detail.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Fern and Lewis [7] thoroughly investigated an Ensemble UCT approach in which multiple instances of UCT were run independently.", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "This is also shown in [10].", "startOffset": 22, "endOffset": 26}], "year": 2017, "abstractText": "In this paper, we present a new algorithm for parallel Monte Carlo tree search (MCTS). It is based on the pipeline pattern and allows flexible management of the control flow of the operations in parallel MCTS. The pipeline pattern provides for the first structured parallel programming approach to MCTS. Moreover, we propose a new lock-free tree data structure for parallel MCTS which removes synchronization overhead. The Pipeline Pattern for Parallel MCTS algorithm (called 3PMCTS), scales very well to higher numbers of cores when compared to the existing methods.", "creator": "LaTeX with hyperref package"}}}