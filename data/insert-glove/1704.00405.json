{"id": "1704.00405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Syntax Aware LSTM Model for Chinese Semantic Role Labeling", "abstract": "Traditional happily approaches 166.00 to sustained Semantic smits Role homepage Labeling (SRL) 6-4 depend heavily bazars on 633,000 manual feature 627 engineering. khodzhent Recurrent neural network (tohru RNN) with long - short - tewa term sculptural memory (jungang LSTM) whiteknights only steam-powered treats space-time sentence songok as kamimura sequence data carriedo and can rubens not salvatti utilize younts higher level pollinator syntactic information. In this barb\u00e8s paper, we tph propose tughlaq Syntax Aware yuzhen LSTM (l'anse SA - LSTM) 5.14 which gives castelletto RNN - mirabell LSTM usta\u0161e ability kadkhodaei to utilize standon higher bongiovanni level syntactic 3-5-2 information gained nrk from dependency moretz relationship glyndon information. ladell SA - bombardieriglobe.com LSTM also assigns different 110.17 trainable weights kups to different fabricated types of casks dependency clad relationship automatically. Experiment pyrolysis results chikelu on an-72 Chinese 100.8 Proposition n70 Bank (CPB) over-reliance show 5-by-7-inch that, fujisawa even shanksville without pre - training ayaka or 11.57 introducing jeezy any wnac other extra sudamericana semantically annotated resources, 146,000 our accounts SA - LSTM tassotti model lascar still beni outperforms galdeano the compartmented state http://www.senate.gov/ of ghul the janaury art monte significantly base on amphisbaena Student ' gaud s kadai t - poonamallee test ($ p & lt; 0. 05 $ ). d.c.-area Trained alberto weights cutworm of asgeir types ye\u015filyurt of cino dependency deltic relationship pvrs form a stable and self - decaf explanatory pattern.", "histories": [["v1", "Mon, 3 Apr 2017 02:10:19 GMT  (986kb,D)", "https://arxiv.org/abs/1704.00405v1", null], ["v2", "Thu, 20 Apr 2017 01:55:26 GMT  (268kb,D)", "http://arxiv.org/abs/1704.00405v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["feng qian", "lei sha", "baobao chang", "lu-chen liu", "ming zhang"], "accepted": false, "id": "1704.00405"}, "pdf": {"name": "1704.00405.pdf", "metadata": {"source": "CRF", "title": "Syntax Aware LSTM Model for Chinese Semantic Role Labeling", "authors": ["Feng Qian", "Lei Sha", "Baobao Chang", "Lu-chen Liu", "Ming Zhang"], "emails": ["chbb}@pku.edu.cn", "cs}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The task of SRL is to recognize arguments of a given predicate in a sentence and assign semantic role labels. Since SRL can give a lot of semantic information, and can help in sentence understanding, a lot of NLP works such as machine translation(Xiong et al., 2012; Aziz et al., 2011) use SRL information. Figure 1 shows an example of SRL task from Chinese Proposition Bank 1.0(CPB 1.0)(Xue and Palmer, 2003).\nTraditional methods on SRL use statistical classifiers such as CRF, MaxEntropy and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun, 2010) to do classification according to manually designed features.\nRecent works based on recurrent neural network (Collobert and Weston, 2008; Zhou and Xu, 2015; Wang et al., 2015) extract features automatically, and outperform traditional methods significantly. However, RNN methods treat language as sequence data, so most of them fail to take tree structured parsing information into account, which\nis considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005). Even though there are some RNN based works trying to utilize parsing information, they still do it in a feature-engineering way.\nWe propose Syntax Aware LSTM (SA-LSTM) to directly model complex dependency parsing information in an architecture engineering way instead of feature engineering way. For example, in Figure 1, the arrowed line stands for dependency relationship, which is rich in syntactic information. Our SA-LSTM architecture is shown in Figure 2. Compares to ordinary LSTM, We add additional connections between dependency related words to capture and model such rich syntactic information in architecture engineering way. Also, to take dependency relationship type into account, we also introduce trainable weights for different types of dependency relationship. The weights can be trained to indicate importance of a dependency type.\nWe experimentally demonstrate that SA-LSTM utilizes parsing information better than traditional feature engineering way. Furthermore, SA-LSTM reaches 79.64%F1 score on CPB 1.0, outperforms the state-of-the-art significantly based on Student\u2019s t-test(p < 0.05)."}, {"heading": "2 Syntax Aware LSTM", "text": "Compares to traditional feature engineering method, RNN-LSTM alleviates the burden of manual feature design and selection. However,\nar X\niv :1\n70 4.\n00 40\n5v 2\n[ cs\n.C L\n] 2\n0 A\npr 2\n01 7\nmost RNN-LSTM based methods failed to utilize dependency parsing relationship. Based on biRNN-LSTM, we propose SA-LSTM which keeps all the merit points of bi-RNN-LSTM, and at the same time can model dependency parsing information directly."}, {"heading": "2.1 Conventional bi-LSTM Model for SRL", "text": "In a sentence, each word wt has a feature representation xt which is generated automatically as (Wang et al., 2015) did. zt is feature embedding for wt, calculated as followed:\nzt = f(W1xt) (1)\nwhere W1 \u2208 Rn1\u00d7n0 . n0 is the length of word feature representation.\nIn a sentence, each wordwt has six internal vectors, C\u0303, gi, gf , go, Ct, and ht, shown in Equation 2:\nC\u0303 = f(Wczt + Ucht\u22121 + bc) gj = \u03c3(Wjzt + Ujht\u22121 + bj) j \u2208 {i, f, o}\nCt = gi C\u0303 + gf Ct\u22121 ht = go f(Ct)\n(2)\nwhere C\u0303 is the candidate value of the current cell state. g are gates used to control the flow of information. Ct is the current cell state. ht is hidden state of wt. Wx and Ux are matrixs used in linear transformation:\nWx, x \u2208 {c, i, f, o} \u2208 Rnh\u00d7n1\nUx, x \u2208 {c, i, f, o} \u2208 Rnh\u00d7nh (3)\nAs convention, f stands for tanh and \u03c3 stands for sigmoid. means the element-wise multiplication.\nIn order to make use of bidirectional information, the forward \u2212\u2192 ht T and backward \u2190\u2212 ht T are concatenated together, as shown in Equation 4:\nat = [ \u2212\u2192 ht T , \u2190\u2212 ht T ] (4)\nFinally, ot is the result vector with each dimension corresponding to the score of each semantic role tag, and are calculated as shown in Equation 5:\not =W3f(W2at) (5)\nwhere W2 \u2208 Rn3\u00d7n2 , n2 is 2 \u00d7 ht, W3 \u2208 Rn4\u00d7n3 and n4 is the number of tags in IOBES tagging schema."}, {"heading": "2.2 Syntax Aware LSTM Model for SRL", "text": "Structure of our SA-LSTM is shown in Figure 3. The most significant change we make to the original RNN-LSTM is shown in the shaded area.\nSt is the syntax information input into current cell, and is calculated as shown in Equation 6:\nSt = f( t\u22121\u2211 i=0 \u03b1\u00d7 hi) (6)\n\u03b1 =  1 If there exists dependency\nrelationship between wi and wt 0 Otherwise (7)\nSt is the weighted sum of all hidden state vectors hi which come from previous wordswi . Note\nthat, \u03b1 \u2208 {0, 1} indicates whether there is a dependency relationship between wi and wt, only dependency related hi can be input into current cell.\nWe add a gate gs to constrain information from St, as shown in Equation 8. To protect the original sentence information from being diluted(Wu et al., 2016) by St, we add St to hidden layer vector ht instead of adding to cell state Ct, as shown in Equation 9:\ngs = \u03c3(Wszt + Usht\u22121 + bs) (8)\nSo ht in our SA-LSTM cell is calculated as:\nht = go f(Ct) + gs St (9)\nSA-LSTM changes structure by adding different connections according to dependency parsing information. In this way, we consider the whole structure of dependency tree into SA-LSTM in an architecture engineering way.\nHowever, by using \u03b1 in Equation 7, we do not take dependency type into account, so we further improve the way \u03b1 is calculated from Equation 7 to Equation 10. Each typem of dependency relationship is assigned a trainable weight \u03b1m. In this way SA-LSTM can model differences between types of dependency relationship.\n\u03b1 =  \u03b1m If there exists typem dependency\nrelationship between wi and wt 0 Otherwise\n(10)"}, {"heading": "2.3 Training Criteria", "text": "We use maximum likelihood criterion to train our model. Stochastic gradient ascent algorithm is used to optimize the parameters. Global normalization is applied.\nGiven a training pair T = (x, y) where T is the current training pair, x denotes current the training sentence, and y is the corresponding correct answer path. yt = k means that the t-th word has the k-th semantic role label. The score of ot is calculated as:\ns(x, y, \u03b8) = Ni\u2211 t=1 otyt (11)\nwhereNi is the word number of the current sentence and \u03b8 stands for all parameters. So the log\nlikelihood of a single sentence is\nlog p(y|x, \u03b8) = log exp(s(x, y, \u03b8))\u2211 y\u2032 exp(s(x, y \u2032, \u03b8))\n= s(x, y, \u03b8)\u2212 log \u2211\ny\u2032 exp(s(x, y\u2032, \u03b8)\n(12)\nwhere y\u2032 ranges from all valid paths of answers."}, {"heading": "3 Experiment", "text": ""}, {"heading": "3.1 Experiment setting", "text": "In order to compare with previous Chinese SRL works, we choose to do experiment on CPB 1.0. We also follow the same data setting as previous Chinese SRL work(Xue, 2008; Sun et al., 2009) did. Pre-trained1 word embeddings are tested on SA-LSTM and shows improvement.\nWe use Stanford Parser(Chen and Manning, 2014) to get dependency parsing information, which now supports Universal Dependency representation in Chinese. Note that the train set of the parser overlaps a part of our test set, so we retrained the parser to avoid overlap.\nDimension of our hyper parameters are tuned according to development set and are shown in Table 1.2"}, {"heading": "3.2 Syntax Aware LSTM Performance", "text": "To prove that SA-LSTM gains more improvement from the new SA-LSTM architecture, than from the extra introduced parsing information, we\n1Trained by word2vec on Chinese Gigaword Corpus 2All experiment code and related files are available on re-\nquest\ndesign a experiment in which dependency relationship is taken into account in traditional feature engineering way.\nGiven a word wt, Ft is the average of all dependency related xi of previous words wi , as shown in Equation 13:\nFt = 1\nT t\u22121\u2211 i=0 \u03b1\u00d7 xi (13)\nwhere T is the number of dependency related words and \u03b1 is a 0-1 variable calculated as in Equation 7.\nThen Ft is concatenated to xt to form a new feature representation. In this way, we model dependency parsing information in a conventional feature engineering way. After that, we feed these new feature representation into ordinary bi-LSTM.\nAs shown in Table 2, SA-LSTM reaches 79.56%F1 score with random initialization and 79.64%F1 score with pre-traind word embedding on CPB1.0 dataset. Both of them are the best F1 score ever published on CPB 1.0 dataset.\nWang et al. (2015) used bi-LSTM without parsing information and got 77.09%F1 score. \u201ccomparison feature engineering method\u201d based on his work reaches 77.75F1 score. This demonstrates the introduction of dependency parsing information has impact on SRL job.\nCompared with the \u201ccomparison feature engineering method\u201d shown in table 2, it is clear that SA-LSTM gain more improvement(77.75% to 79.56%) from the architecture of SA-LSTM than from the introduction of extra dependency parsing information(77.09% to 77.75%). Indeed, it is difficult to introduce the whole tree structure into the model using the simple feature engineering way. By building the dependency relationship directly into the structure of SA-LSTM and changing the way information flows, SA-LSTM is able to consider whole tree structure of dependency parsing information."}, {"heading": "3.3 Visualization of Trained Weights", "text": "According to Equation 10, influence from a single type of dependency relationship will be multiplied with type weight \u03b1m. When \u03b1m is 0, the influence from this type of dependency relationship will be ignored totally. When the weight is bigger, the type of dependency relationship will have more influence on the whole system.\nAs shown in Figure 4, dependency relationship type dobj receives the highest weight after training, as shown by the red bar. According to grammar knowledge, dobj should be an informative relationship for SRL task, and our system give dobj the most influence automatically. This example further demonstrate that the result of SA-LSTM is highly in accordance with grammar knowledge, which further validates SA-LSTM."}, {"heading": "4 Related works", "text": "Semantic role labeling (SRL) was first defined by (Gildea and Jurafsky, 2002). Early works(Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004) on SRL got promising result without large annotated SRL corpus. Xue and Palmer built the Chinese Proposition Bank(Xue and Palmer, 2003) to standardize Chinese SRL research.\nTraditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods. Traditional methods can take parsing information into account in feature engineering way, such as syntactic path feature. However, they suffer from heavy manually feature design workload, and data sparsity problem.\nMore recent SRL works often use neural network based methods. Collobert and Weston (2008) proposed a convolutional neural network method for SRL. Zhou and Xu (2015) proposed bidirectional RNN-LSTM method for English SRL, and Wang et al. (2015) proposed a bi-RNN-\nLSTM method for Chinese SRL on which our method is based. NN based methods extract features automatically and significantly outperforms traditional methods. However, most NN based methods can not utilize parsing information which is considered important for semantic related NLP tasks (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).\nThe work of Roth and Lapata (2016) and Sha et al. (2016) have the same motivation as ours, but in feature engineering way. Roth and Lapata (2016) embed dependency parsing path into feature representations using LSTM. Sha et al. (2016) use dependency parsing information as feature to do argument relationships classification. In contrast, LA-LSTM utilizes parsing information in an architecture engineering way, by absorbing the parsing tree structure into SA-LSTM structure."}, {"heading": "5 Conclusion", "text": "We propose Syntax Aware LSTM model for Chinese semantic role labeling. SA-LSTM is able to model dependency information directly in an architecture engineering way. We experimentally testified that SA-LSTM gains more improvement from the SA-LSTM architecture than from the input of extra dependency parsing information. We push the state-of-the-art F1 to 79.64%, which outperforms the state-of-the-art significantly according to Student t-test(p < 0.05)."}], "references": [{"title": "Shallow semantic trees for smt", "author": ["Wilker Aziz", "Miguel Rios", "Lucia Specia."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics, pages 316\u2013322.", "citeRegEx": "Aziz et al\\.,? 2011", "shortCiteRegEx": "Aziz et al\\.", "year": 2011}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP. pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "An empirical study of chinese chunking", "author": ["Wenliang Chen", "Yujie Zhang", "Hitoshi Isahara."], "venue": "Proceedings of the COLING/ACL on Main conference poster sessions. Association for Computational Linguistics, pages 97\u2013104.", "citeRegEx": "Chen et al\\.,? 2006", "shortCiteRegEx": "Chen et al\\.", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Improving chinese semantic role classification with hierarchical", "author": ["Weiwei Ding", "Baobao Chang"], "venue": null, "citeRegEx": "Ding and Chang.,? \\Q2008\\E", "shortCiteRegEx": "Ding and Chang.", "year": 2008}, {"title": "Word based chinese semantic role labeling with semantic chunking", "author": ["Weiwei Ding", "Baobao Chang."], "venue": "International Journal of Computer Processing Of Languages 22(02n03):133\u2013154.", "citeRegEx": "Ding and Chang.,? 2009", "shortCiteRegEx": "Ding and Chang.", "year": 2009}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky."], "venue": "Computational linguistics 28(3):245\u2013288.", "citeRegEx": "Gildea and Jurafsky.,? 2002", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "Semantic role chunking combining complementary syntactic views", "author": ["Sameer Pradhan", "Kadri Hacioglu", "Wayne Ward", "James H Martin", "Daniel Jurafsky."], "venue": "Proceedings of the Ninth Conference on Computational Natural Language Learning. As-", "citeRegEx": "Pradhan et al\\.,? 2005", "shortCiteRegEx": "Pradhan et al\\.", "year": 2005}, {"title": "The importance of syntactic parsing and inference in semantic role labeling", "author": ["Vasin Punyakanok", "Dan Roth", "Wen-tau Yih."], "venue": "Computational Linguistics 34(2):257\u2013287.", "citeRegEx": "Punyakanok et al\\.,? 2008", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2008}, {"title": "Neural semantic role labeling with dependency path embeddings", "author": ["Michael Roth", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1605.07515 .", "citeRegEx": "Roth and Lapata.,? 2016", "shortCiteRegEx": "Roth and Lapata.", "year": 2016}, {"title": "Capturing argument relationships for chinese semantic role labeling", "author": ["Lei Sha", "Tingsong Jiang", "Sujian Li", "Baobao Chang", "Zhifang Sui."], "venue": "EMNLP. pages 2011\u20132016.", "citeRegEx": "Sha et al\\.,? 2016", "shortCiteRegEx": "Sha et al\\.", "year": 2016}, {"title": "Shallow semantic parsing of chinese", "author": ["Honglin Sun", "Daniel Jurafsky."], "venue": "Proceedings of NAACL 2004. pages 249\u2013256.", "citeRegEx": "Sun and Jurafsky.,? 2004", "shortCiteRegEx": "Sun and Jurafsky.", "year": 2004}, {"title": "Improving chinese semantic role labeling with rich syntactic features", "author": ["Weiwei Sun."], "venue": "Proceedings of the ACL 2010 conference short papers. Association for Computational Linguistics, pages 168\u2013172.", "citeRegEx": "Sun.,? 2010", "shortCiteRegEx": "Sun.", "year": 2010}, {"title": "Chinese semantic role labeling with shallow parsing", "author": ["Weiwei Sun", "Zhifang Sui", "Meng Wang", "Xin Wang."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3. Association for Compu-", "citeRegEx": "Sun et al\\.,? 2009", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Chinese semantic role labeling with bidirectional recurrent neural networks", "author": ["Zhen Wang", "Tingsong Jiang", "Baobao Chang", "Zhifang Sui."], "venue": "EMNLP. pages 1626\u20131631.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "An empirical exploration of skip connections for sequential tagging", "author": ["Huijia Wu", "Jiajun Zhang", "Chengqing Zong."], "venue": "arXiv preprint arXiv:1610.03167 .", "citeRegEx": "Wu et al\\.,? 2016", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Modeling the translation of predicate-argument structure", "author": ["Deyi Xiong", "Min Zhang", "Haizhou Li"], "venue": null, "citeRegEx": "Xiong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2012}, {"title": "Labeling chinese predicates with semantic roles", "author": ["Nianwen Xue."], "venue": "Computational linguistics 34(2):225\u2013255.", "citeRegEx": "Xue.,? 2008", "shortCiteRegEx": "Xue.", "year": 2008}, {"title": "Annotating the propositions in the penn chinese treebank", "author": ["Nianwen Xue", "Martha Palmer."], "venue": "Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17. Association for Computational Linguistics, pages 47\u201354.", "citeRegEx": "Xue and Palmer.,? 2003", "shortCiteRegEx": "Xue and Palmer.", "year": 2003}, {"title": "Automatic semantic role labeling for chinese verbs", "author": ["Nianwen Xue", "Martha Palmer."], "venue": "IJCAI. Citeseer, volume 5, pages 1160\u20131165.", "citeRegEx": "Xue and Palmer.,? 2005", "shortCiteRegEx": "Xue and Palmer.", "year": 2005}, {"title": "Multipredicate semantic role labeling", "author": ["Haitong Yang", "Chengqing Zong"], "venue": "In EMNLP", "citeRegEx": "Yang and Zong,? \\Q2014\\E", "shortCiteRegEx": "Yang and Zong", "year": 2014}, {"title": "End-to-end learning of semantic role labeling using recurrent neural networks", "author": ["Jie Zhou", "Wei Xu."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Zhou and Xu.,? 2015", "shortCiteRegEx": "Zhou and Xu.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Since SRL can give a lot of semantic information, and can help in sentence understanding, a lot of NLP works such as machine translation(Xiong et al., 2012; Aziz et al., 2011) use SRL information.", "startOffset": 136, "endOffset": 175}, {"referenceID": 0, "context": "Since SRL can give a lot of semantic information, and can help in sentence understanding, a lot of NLP works such as machine translation(Xiong et al., 2012; Aziz et al., 2011) use SRL information.", "startOffset": 136, "endOffset": 175}, {"referenceID": 18, "context": "0)(Xue and Palmer, 2003).", "startOffset": 2, "endOffset": 24}, {"referenceID": 11, "context": "Traditional methods on SRL use statistical classifiers such as CRF, MaxEntropy and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun, 2010) to do classification according to manually designed features.", "startOffset": 87, "endOffset": 161}, {"referenceID": 17, "context": "Traditional methods on SRL use statistical classifiers such as CRF, MaxEntropy and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun, 2010) to do classification according to manually designed features.", "startOffset": 87, "endOffset": 161}, {"referenceID": 12, "context": "Traditional methods on SRL use statistical classifiers such as CRF, MaxEntropy and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun, 2010) to do classification according to manually designed features.", "startOffset": 87, "endOffset": 161}, {"referenceID": 3, "context": "Recent works based on recurrent neural network (Collobert and Weston, 2008; Zhou and Xu, 2015; Wang et al., 2015) extract features automatically, and outperform traditional methods significantly.", "startOffset": 47, "endOffset": 113}, {"referenceID": 21, "context": "Recent works based on recurrent neural network (Collobert and Weston, 2008; Zhou and Xu, 2015; Wang et al., 2015) extract features automatically, and outperform traditional methods significantly.", "startOffset": 47, "endOffset": 113}, {"referenceID": 14, "context": "Recent works based on recurrent neural network (Collobert and Weston, 2008; Zhou and Xu, 2015; Wang et al., 2015) extract features automatically, and outperform traditional methods significantly.", "startOffset": 47, "endOffset": 113}, {"referenceID": 17, "context": "However, RNN methods treat language as sequence data, so most of them fail to take tree structured parsing information into account, which is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 176, "endOffset": 234}, {"referenceID": 8, "context": "However, RNN methods treat language as sequence data, so most of them fail to take tree structured parsing information into account, which is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 176, "endOffset": 234}, {"referenceID": 7, "context": "However, RNN methods treat language as sequence data, so most of them fail to take tree structured parsing information into account, which is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 176, "endOffset": 234}, {"referenceID": 14, "context": "1 Conventional bi-LSTM Model for SRL In a sentence, each word wt has a feature representation xt which is generated automatically as (Wang et al., 2015) did.", "startOffset": 133, "endOffset": 152}, {"referenceID": 15, "context": "To protect the original sentence information from being diluted(Wu et al., 2016) by St, we add St to hidden layer vector ht instead of adding to cell state Ct, as shown in Equation 9:", "startOffset": 63, "endOffset": 80}, {"referenceID": 17, "context": "We also follow the same data setting as previous Chinese SRL work(Xue, 2008; Sun et al., 2009) did.", "startOffset": 65, "endOffset": 94}, {"referenceID": 13, "context": "We also follow the same data setting as previous Chinese SRL work(Xue, 2008; Sun et al., 2009) did.", "startOffset": 65, "endOffset": 94}, {"referenceID": 1, "context": "We use Stanford Parser(Chen and Manning, 2014) to get dependency parsing information, which now supports Universal Dependency representation in Chinese.", "startOffset": 22, "endOffset": 46}, {"referenceID": 13, "context": "Method F1% Xue(2008) 71.", "startOffset": 11, "endOffset": 21}, {"referenceID": 11, "context": "90 Sun et al.(2009) 74.", "startOffset": 3, "endOffset": 20}, {"referenceID": 11, "context": "90 Sun et al.(2009) 74.12 Yand and Zong(2014) 75.", "startOffset": 3, "endOffset": 46}, {"referenceID": 11, "context": "90 Sun et al.(2009) 74.12 Yand and Zong(2014) 75.31 Wang et al.(2015)(Random Initialized) 77.", "startOffset": 3, "endOffset": 70}, {"referenceID": 10, "context": "09 Sha et al.(2016) 77.", "startOffset": 3, "endOffset": 20}, {"referenceID": 14, "context": "Wang et al. (2015) used bi-LSTM without parsing information and got 77.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Semantic role labeling (SRL) was first defined by (Gildea and Jurafsky, 2002).", "startOffset": 50, "endOffset": 77}, {"referenceID": 6, "context": "Early works(Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004) on SRL got promising result without large annotated SRL corpus.", "startOffset": 11, "endOffset": 62}, {"referenceID": 11, "context": "Early works(Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004) on SRL got promising result without large annotated SRL corpus.", "startOffset": 11, "endOffset": 62}, {"referenceID": 18, "context": "Xue and Palmer built the Chinese Proposition Bank(Xue and Palmer, 2003) to standardize Chinese SRL research.", "startOffset": 49, "endOffset": 71}, {"referenceID": 19, "context": "Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods.", "startOffset": 26, "endOffset": 137}, {"referenceID": 17, "context": "Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods.", "startOffset": 26, "endOffset": 137}, {"referenceID": 5, "context": "Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods.", "startOffset": 26, "endOffset": 137}, {"referenceID": 13, "context": "Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods.", "startOffset": 26, "endOffset": 137}, {"referenceID": 2, "context": "Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods.", "startOffset": 26, "endOffset": 137}, {"referenceID": 2, "context": ", 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods. Traditional methods can take parsing information into account in feature engineering way, such as syntactic path feature. However, they suffer from heavy manually feature design workload, and data sparsity problem. More recent SRL works often use neural network based methods. Collobert and Weston (2008) proposed a convolutional neural network method for SRL.", "startOffset": 8, "endOffset": 384}, {"referenceID": 2, "context": ", 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods. Traditional methods can take parsing information into account in feature engineering way, such as syntactic path feature. However, they suffer from heavy manually feature design workload, and data sparsity problem. More recent SRL works often use neural network based methods. Collobert and Weston (2008) proposed a convolutional neural network method for SRL. Zhou and Xu (2015) proposed bidirectional RNN-LSTM method for English SRL, and Wang et al.", "startOffset": 8, "endOffset": 459}, {"referenceID": 2, "context": ", 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods. Traditional methods can take parsing information into account in feature engineering way, such as syntactic path feature. However, they suffer from heavy manually feature design workload, and data sparsity problem. More recent SRL works often use neural network based methods. Collobert and Weston (2008) proposed a convolutional neural network method for SRL. Zhou and Xu (2015) proposed bidirectional RNN-LSTM method for English SRL, and Wang et al. (2015) proposed a bi-RNN-", "startOffset": 8, "endOffset": 538}, {"referenceID": 17, "context": "However, most NN based methods can not utilize parsing information which is considered important for semantic related NLP tasks (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 128, "endOffset": 186}, {"referenceID": 8, "context": "However, most NN based methods can not utilize parsing information which is considered important for semantic related NLP tasks (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 128, "endOffset": 186}, {"referenceID": 7, "context": "However, most NN based methods can not utilize parsing information which is considered important for semantic related NLP tasks (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 128, "endOffset": 186}, {"referenceID": 7, "context": ", 2008; Pradhan et al., 2005). The work of Roth and Lapata (2016) and Sha et al.", "startOffset": 8, "endOffset": 66}, {"referenceID": 7, "context": ", 2008; Pradhan et al., 2005). The work of Roth and Lapata (2016) and Sha et al. (2016) have the same motivation as ours, but in feature engineering way.", "startOffset": 8, "endOffset": 88}, {"referenceID": 7, "context": ", 2008; Pradhan et al., 2005). The work of Roth and Lapata (2016) and Sha et al. (2016) have the same motivation as ours, but in feature engineering way. Roth and Lapata (2016) embed dependency parsing path into feature representations using LSTM.", "startOffset": 8, "endOffset": 177}, {"referenceID": 7, "context": ", 2008; Pradhan et al., 2005). The work of Roth and Lapata (2016) and Sha et al. (2016) have the same motivation as ours, but in feature engineering way. Roth and Lapata (2016) embed dependency parsing path into feature representations using LSTM. Sha et al. (2016) use dependency parsing information as feature to do argument relationships classification.", "startOffset": 8, "endOffset": 266}], "year": 2017, "abstractText": "As for semantic role labeling (SRL) task, when it comes to utilizing parsing information, both traditional methods and recent recurrent neural network (RNN) based methods use the feature engineering way. In this paper, we propose Syntax Aware Long Short Time Memory(SALSTM). The structure of SA-LSTM modifies according to dependency parsing information in order to model parsing information directly in an architecture engineering way instead of feature engineering way. We experimentally demonstrate that SA-LSTM gains more improvement from the model architecture. Furthermore, SALSTM outperforms the state-of-the-art on CPB 1.0 significantly according to Student t-test (p < 0.05).", "creator": "LaTeX with hyperref package"}}}