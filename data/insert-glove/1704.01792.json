{"id": "1704.01792", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2017", "title": "Neural Question Generation from Text: A Preliminary Study", "abstract": "Automatic question acu\u00f1a generation aims to generate chulu questions from gary.andres@dutkoworldwide.com a text passage 59-55 where the jolgeh generated mistimed questions telopea can be answered mlambo by certain sub - spans of 19:13 the islamofascism given 8,750 passage. Traditional manhattan methods wuornos mainly use rigid fernon heuristic g/mol rules to transform trimethylamine a sentence into related questions. 49-state In criminalization this work, nicollin we propose sabas to afterthoughts apply tademy the ordovician neural encoder - betterton decoder klingons model to generate fering meaningful and diverse questions from nelsoni natural language pisapia sentences. The querer encoder reads denial-of-service the input va'aiga text siemreap and the long-winded answer 69.77 position, pasdar to endodontics produce an hollendorfer answer - gouverneur aware carbonyls input representation, spangenberg which is fed molapo to 1,274 the decoder mildest to generate an answer hilfiger focused question. velio We conduct naib a guzm\u00e1n preliminary study sagamore on wei\u00dfensee neural question pinguine generation efsa from fungo text farbstein with yamoussoukro the plantarum SQuAD exercises dataset, and cowes the photoemission experiment viewfinders results show robow that our method 569,000 can qoyunlu produce matheos fluent pitcher and mataro diverse vespas questions.", "histories": [["v1", "Thu, 6 Apr 2017 11:44:07 GMT  (244kb,D)", "https://arxiv.org/abs/1704.01792v1", "Submitted to EMNLP 2017"], ["v2", "Sun, 16 Apr 2017 03:27:15 GMT  (338kb,D)", "http://arxiv.org/abs/1704.01792v2", "Submitted to EMNLP 2017"], ["v3", "Tue, 18 Apr 2017 07:54:52 GMT  (337kb,D)", "http://arxiv.org/abs/1704.01792v3", "Submitted to EMNLP 2017"]], "COMMENTS": "Submitted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qingyu zhou", "nan yang", "furu wei", "chuanqi tan", "hangbo bao", "ming zhou"], "accepted": false, "id": "1704.01792"}, "pdf": {"name": "1704.01792.pdf", "metadata": {"source": "CRF", "title": "Neural Question Generation from Text: A Preliminary Study", "authors": ["Qingyu Zhou", "Nan Yang", "Furu Wei", "Chuanqi Tan", "Hangbo Bao", "Ming Zhou"], "emails": ["qyzhgm@gmail.com", "mingzhou}@microsoft.com", "tanchuanqi@nlsde.buaa.edu.cn", "baohangbo@hit.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Automatic question generation from natural language text aims to generate questions taking text as input, which has the potential value of education purpose (Heilman, 2011). As the reverse task of question answering, question generation also has the potential for providing a large scale corpus of question-answer pairs.\nPrevious works for question generation mainly use rigid heuristic rules to transform a sentence into related questions (Heilman, 2011; Chali and Hasan, 2015). However, these methods heavily rely on human-designed transformation and generation rules, which cannot be easily adopted to other domains. Instead of generating questions from texts, Serban et al. (2016) proposed a neu-\n\u2217Contribution during internship at Microsoft Research.\nral network method to generate factoid questions from structured data.\nIn this work we conduct a preliminary study on question generation from text with neural networks, which is denoted as the Neural Question Generation (NQG) framework, to generate natural language questions from text without pre-defined rules. The Neural Question Generation framework extends the sequence-to-sequence models by enriching the encoder with answer and lexical features to generate answer focused questions. Concretely, the encoder reads not only the input sentence, but also the answer position indicator and lexical features. The answer position feature denotes the answer span in the input sentence, which is essential to generate answer relevant questions. The lexical features include part-of-speech (POS) and named entity (NER) tags to help produce better sentence encoding. Lastly, the decoder with attention mechanism (Bahdanau et al., 2015) generates an answer specific question of the sentence.\nLarge-scale manually annotated passage and question pairs play a crucial role in developing question generation systems. We propose to adapt the recently released Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) as the training and development datasets for the question generation task. In SQuAD, the answers are labeled as subsequences in the given sentences by crowed sourcing, and it contains more than 100K questions which makes it feasible to train our neural network models. We conduct the experiments on SQuAD, and the experiment results show the neural network models can produce fluent and diverse questions from text."}, {"heading": "2 Approach", "text": "In this section, we introduce the NQG framework, which consists of a feature-rich encoder and an\nar X\niv :1\n70 4.\n01 79\n2v 3\n[ cs\n.C L\n] 1\n8 A\npr 2\n01 7\nattention-based decoder. Figure 1 provides an overview of our NQG framework."}, {"heading": "2.1 Feature-Rich Encoder", "text": "In the NQG framework, we use Gated Recurrent Unit (GRU) (Cho et al., 2014) to build the encoder. To capture more context information, we use bidirectional GRU (BiGRU) to read the inputs in both forward and backward orders. Inspired by Chen and Manning (2014); Nallapati et al. (2016), the BiGRU encoder not only reads the sentence words, but also handcrafted features, to produce a sequence of word-and-feature vectors. We concatenate the word vector, lexical feature embedding vectors and answer position indicator embedding vector as the input of BiGRU encoder. Concretely, the BiGRU encoder reads the concatenated sentence word vector, lexical features, and answer position feature, x = (x1, x2, . . . , xn), to produce two sequences of hidden vectors, i.e., the forward sequence (~h1,~h2, . . . ,~hn) and the backward sequence ( ~h1, ~h2, . . . , ~hn). Lastly, the output sequence of the encoder is the concatenation of the two sequences, i.e., hi = [~hi; ~hi].\nAnswer Position Feature To generate a question with respect to a specific answer in a sentence, we propose using answer position feature to locate the target answer. In this work, the BIO tagging scheme is used to label the position of a target answer. In this scheme, tag B denotes the start of an answer, tag I continues the answer and tag O marks words that do not form part of an answer. The BIO tags of answer position are embedded to real-valued vectors throu and fed to the featurerich encoder. With the BIO tagging feature, the answer position is encoded to the hidden vectors and used to generate answer focused questions.\nLexical Features Besides the sentence words, we also feed other lexical features to the encoder. To encode more linguistic information, we select word case, POS and NER tags as the lexical features. As an intermediate layer of full parsing, POS tag feature is important in many NLP tasks, such as information extraction and dependency parsing (Manning et al., 1999). Considering that SQuAD is constructed using Wikipedia articles, which contain lots of named entities, we add NER feature to help detecting them."}, {"heading": "2.2 Attention-Based Decoder", "text": "We employ an attention-based GRU decoder to decode the sentence and answer information to generate questions. At decoding time step t, the GRU decoder reads the previous word embedding wt\u22121 and context vector ct\u22121 to compute the new hidden state st. We use a linear layer with the last backward encoder hidden state ~h1 to initialize the decoder GRU hidden state. The context vector ct for current time step t is computed through the concatenate attention mechanism (Luong et al., 2015), which matches the current decoder state st with each encoder hidden state hi to get an importance score. The importance scores are then normalized to get the current context vector by weighted sum:\nst = GRU(wt\u22121, ct\u22121, st\u22121)\ns0 = tanh(Wd ~h1 + b)\net,i = v > a tanh(Wast\u22121 +Uahi) \u03b1t,i = exp(et,i)\u2211n i=1 exp(et,i)\nct = n\u2211\ni=1\n\u03b1t,ihi\n(1)\n(2)\n(3)\n(4)\n(5)\nWe then combine the previous word embedding wt\u22121, the current context vector ct, and the decoder state st to get the readout state rt. The readout state is passed through a maxout hidden layer (Goodfellow et al., 2013) to predict the next word with a softmax layer over the decoder vocabulary:\nrt = Wrwt\u22121 +Urct +Vrst\nmt = [max{rt,2j\u22121, rt,2j}]>j=1,...,d p(yt|y1, . . . , yt\u22121) = softmax(Womt)\n(6)\n(7)\n(8)\nwhere rt is a 2d-dimensional vector."}, {"heading": "2.3 Copy Mechanism", "text": "To deal with the rare and unknown words problem, Gulcehre et al. (2016) propose using pointing mechanism to copy rare words from source sentence. We apply this pointing method in our NQG system. When decoding word t, the copy switch takes current decoder state st and context vector ct as input and generates the probability p of copying a word from source sentence:\np = \u03c3(Wst +Uct + b) (9)\nwhere \u03c3 is sigmoid function. We reuse the attention probability in equation 4 to decide which word to copy."}, {"heading": "3 Experiments and Results", "text": "We use the SQuAD dataset as our training data. SQuAD is composed of more than 100K questions posed by crowd workers on 536 Wikipedia articles. We extract sentence-answer-question triples to build the training, development and test sets1. Since the test set is not publicly available, we randomly halve the development set to construct the new development and test sets. The extracted training, development and test sets contain 86,635, 8,965 and 8,964 triples respectively. We introduce the implementation details in the appendix.\nWe conduct several experiments and ablation tests as follows:\nPCFG-Trans The rule-based system1 modified on the code released by Heilman (2011). We modified the code so that it can generate question based on a given word span. s2s+att We implement a seq2seq with attention as the baseline method. NQG We extend the s2s+att with our feature-rich encoder to build the NQG system. NQG+ Based on NQG, we incorporate copy mechanism to deal with rare words problem. NQG+Pretrain Based on NQG+, we initialize the word embedding matrix with pre-trained GloVe (Pennington et al., 2014) vectors. NQG+STshare Based on NQG+, we make the encoder and decoder share the same embedding matrix. NQG++ Based on NQG+, we use both pre-train word embedding and STshare methods, to further improve the performance.\n1We re-distribute the processed data split and PCFGTrans baseline code at http://res.qyzhou.me\nNQG\u2212Answer Ablation test, the answer position indicator is removed from NQG model. NQG\u2212POS Ablation test, the POS tag feature is removed from NQG model. NQG\u2212NER Ablation test, the NER feature is removed from NQG model. NQG\u2212Case Ablation test, the word case feature is removed from NQG model."}, {"heading": "3.1 Results and Analysis", "text": "We report BLEU-4 score (Papineni et al., 2002) as the evaluation metric of our NQG system.\nTable 1 shows the BLEU-4 scores of different settings. We report the beam search results on both development and test sets. Our NQG framework outperforms the PCFG-Trans and s2s+att baselines by a large margin. This shows that the lexical features and answer position indicator can benefit the question generation. With the help of copy mechanism, NQG+ has a 2.05 BLEU improvement since it solves the rare words problem. The extended version, NQG++, has 1.11 BLEU score gain over NQG+, which shows that initializing with pre-trained word vectors and sharing them between encoder and decoder help learn better word representation.\nHuman Evaluation We evaluate the PCFGTrans baseline and NQG++ with human judges. The rating scheme is, Good (3) - The question is meaningful and matches the sentence and answer very well; Borderline (2) - The question matches the sentence and answer, more or less; Bad (1) - The question either does not make sense or matches the sentence and answer. We provide more detailed rating examples in the supplementary material. Three human raters labeled 200\nquestions sampled from the test set to judge if the generated question matches the given sentence and answer span. The inter-rater aggreement is measured with Fleiss\u2019 kappa (Fleiss, 1971).\nTable 2 reports the human judge results. The kappa scores show a moderate agreement between the human raters. Our NQG++ outperforms the PCFG-Trans baseline by 0.76 score, which shows that the questions generated by NQG++ are more related to the given sentence and answer span.\nAblation Test The answer position indicator, as expected, plays a crucial role in answer focused question generation as shown in the NQG\u2212Answer ablation test. Without it, the performance drops terribly since the decoder has no information about the answer subsequence.\nAblation tests, NQG\u2212Case, NQG\u2212POS and NQG\u2212NER, show that word case, POS and NER tag features contributes to question generation.\nCase Study Table 3 provides three examples generated by NQG++. The words with underline are the target answers. These three examples are with different question types, namely WHEN, WHAT and WHO respectively. It can be observed that the decoder can \u2018copy\u2019 spans from input sentences to generate the questions. Besides the underlined words , other meaningful spans can also be used as answer to generate correct answer focused questions.\nType of Generated Questions Following Wang and Jiang (2016), we classify the questions into different types, i.e., WHAT, HOW, WHO, WHEN, WHICH, WHERE, WHY and OTHER.2 We evaluate the precision and recall of each question types. Figure 2 provides the precision and recall metrics of different question types. The precision\n2We treat questions \u2018what country\u2019, \u2018what place\u2019 and so on as WHERE type questions. Similarly, questions containing \u2018what time\u2019, \u2018what year\u2019 and so forth are counted as WHEN type questions.\nand recall of a question type T are defined as:\nprecision(T) = #(true T-type questions)\n#(generated T-type questions)\nrecall(T) = #(true T-type questions)\n#(all gold T-type questions)\n(10)\n(11)\nFor the majority question types, WHAT, HOW, WHO and WHEN types, our NQG++ model performs well for both precision and recall. For type WHICH, it can be observed that neither precision nor recall are acceptable. Two reasons may cause this: a) some WHICH-type questions can be asked in other manners, e.g., \u2018which team\u2019 can be replaced with \u2018who\u2019; b) WHICH-type questions account for about 7.2% in training data, which may not be sufficient to learn to generate this type of questions. The same reason can also affect the precision and recall of WHY-type questions."}, {"heading": "4 Conclusion and Future Work", "text": "In this paper we conduct a preliminary study of natural language question generation with neu-\nral network models. We propose to apply neural encoder-decoder model to generate answer focused questions based on natural language sentences. The proposed approach uses a featurerich encoder to encode answer position, POS and NER tag information. Experiments show the effectiveness of our NQG method. In future work, we would like to investigate whether the automatically generated questions can help to improve question answering systems."}, {"heading": "B Human Evaluation Examples", "text": "We evaluate the PCFG-Trans baseline and NQG++ with human judges. The rating scheme is provided in Table 4.\nThe human judges are asked to label the generated questions if they match the given sentence and answer span according to the rating scheme and examples. We provide some example questions with different scores in Table 5. For the first\nscore 3 example, the question makes sense and the target answer \u201creason\u201d can be used to answer it given the input sentence. For the second score 2 example, the question is inadequate for answering the sentence since the answer is about prime number. However, given the sentence, a reasonable person will give the targeted answer of the question. For the third score 1 example, the question is totally wrong given the sentence and answer."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of 3rd International Conference for Learning Representations. San Diego.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Towards topicto-question generation", "author": ["Yllias Chali", "Sadid A. Hasan."], "venue": "Comput. Linguist. 41(1):1\u2013", "citeRegEx": "Chali and Hasan.,? 2015", "shortCiteRegEx": "Chali and Hasan.", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of EMNLP 2014. Association for Computational Linguistics, Doha, Qatar, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Measuring nominal scale agreement among many raters", "author": ["Joseph L Fleiss."], "venue": "Psychological bulletin 76(5):378.", "citeRegEx": "Fleiss.,? 1971", "shortCiteRegEx": "Fleiss.", "year": 1971}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Aistats. volume 9, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Maxout networks", "author": ["Ian J Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron C Courville", "Yoshua Bengio."], "venue": "ICML (3) 28:1319\u20131327.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Automatic factual question generation from text", "author": ["Michael Heilman."], "venue": "Ph.D. thesis, Carnegie Mellon University.", "citeRegEx": "Heilman.,? 2011", "shortCiteRegEx": "Heilman.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of EMNLP 2015. Association for Computational Linguistics, Lisbon, Portugal, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Foundations of statistical natural language processing, volume 999", "author": ["Christopher D Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning and Sch\u00fctze,? \\Q1999\\E", "shortCiteRegEx": "Manning and Sch\u00fctze", "year": 1999}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Association for Computational Linguistics (ACL) System Demonstrations.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "\u00c7a glar Gul\u00e7ehre", "Bing Xiang."], "venue": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning.", "citeRegEx": "Nallapati et al\\.,? 2016", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML (3) 28:1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.05250 .", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["Iulian Vlad Serban", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1608.07905 .", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Model Training We initialize model parameters randomly using a Gaussian distribution with Xavier scheme", "author": ["els. A"], "venue": "(Glorot and Bengio,", "citeRegEx": "A.3,? \\Q2010\\E", "shortCiteRegEx": "A.3", "year": 2010}, {"title": "2015) and simple SGD as our the optimizing algorithms. The training is separated into two phases, the first phase is optimizing the loss function with Adam and the second is with simple SGD", "author": ["Adam (Kingma", "Ba"], "venue": "For the Adam optimizer,", "citeRegEx": ".Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": ".Kingma and Ba", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Automatic question generation from natural language text aims to generate questions taking text as input, which has the potential value of education purpose (Heilman, 2011).", "startOffset": 157, "endOffset": 172}, {"referenceID": 8, "context": "Previous works for question generation mainly use rigid heuristic rules to transform a sentence into related questions (Heilman, 2011; Chali and Hasan, 2015).", "startOffset": 119, "endOffset": 157}, {"referenceID": 1, "context": "Previous works for question generation mainly use rigid heuristic rules to transform a sentence into related questions (Heilman, 2011; Chali and Hasan, 2015).", "startOffset": 119, "endOffset": 157}, {"referenceID": 1, "context": "Previous works for question generation mainly use rigid heuristic rules to transform a sentence into related questions (Heilman, 2011; Chali and Hasan, 2015). However, these methods heavily rely on human-designed transformation and generation rules, which cannot be easily adopted to other domains. Instead of generating questions from texts, Serban et al. (2016) proposed a neu-", "startOffset": 135, "endOffset": 364}, {"referenceID": 0, "context": "Lastly, the decoder with attention mechanism (Bahdanau et al., 2015) generates an answer specific question of the sentence.", "startOffset": 45, "endOffset": 68}, {"referenceID": 17, "context": "We propose to adapt the recently released Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) as the training and development datasets for the question generation task.", "startOffset": 86, "endOffset": 110}, {"referenceID": 3, "context": "In the NQG framework, we use Gated Recurrent Unit (GRU) (Cho et al., 2014) to build the encoder.", "startOffset": 56, "endOffset": 74}, {"referenceID": 2, "context": "Inspired by Chen and Manning (2014); Nallapati et al.", "startOffset": 12, "endOffset": 36}, {"referenceID": 2, "context": "Inspired by Chen and Manning (2014); Nallapati et al. (2016), the BiGRU encoder not only reads the sentence words, but also handcrafted features, to produce a sequence of word-and-feature vectors.", "startOffset": 12, "endOffset": 61}, {"referenceID": 10, "context": "The context vector ct for current time step t is computed through the concatenate attention mechanism (Luong et al., 2015), which matches the current decoder state st with each encoder hidden state hi to get an importance score.", "startOffset": 102, "endOffset": 122}, {"referenceID": 6, "context": "The readout state is passed through a maxout hidden layer (Goodfellow et al., 2013) to predict the next word with a softmax layer over the decoder vocabulary:", "startOffset": 58, "endOffset": 83}, {"referenceID": 7, "context": "To deal with the rare and unknown words problem, Gulcehre et al. (2016) propose using pointing mechanism to copy rare words from source sentence.", "startOffset": 49, "endOffset": 72}, {"referenceID": 16, "context": "NQG+Pretrain Based on NQG+, we initialize the word embedding matrix with pre-trained GloVe (Pennington et al., 2014) vectors.", "startOffset": 91, "endOffset": 116}, {"referenceID": 8, "context": "PCFG-Trans The rule-based system1 modified on the code released by Heilman (2011). We modified the code so that it can generate question based on a given word span.", "startOffset": 67, "endOffset": 82}, {"referenceID": 14, "context": "We report BLEU-4 score (Papineni et al., 2002) as the evaluation metric of our NQG system.", "startOffset": 23, "endOffset": 46}, {"referenceID": 4, "context": "The inter-rater aggreement is measured with Fleiss\u2019 kappa (Fleiss, 1971).", "startOffset": 58, "endOffset": 72}, {"referenceID": 20, "context": "Type of Generated Questions Following Wang and Jiang (2016), we classify the questions into different types, i.", "startOffset": 38, "endOffset": 60}], "year": 2017, "abstractText": "Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub-spans of the given passage. Traditional methods mainly use rigid heuristic rules to transform a sentence into related questions. In this work, we propose to apply the neural encoderdecoder model to generate meaningful and diverse questions from natural language sentences. The encoder reads the input text and the answer position, to produce an answer-aware input representation, which is fed to the decoder to generate an answer focused question. We conduct a preliminary study on neural question generation from text with the SQuAD dataset, and the experiment results show that our method can produce fluent and diverse questions.", "creator": "LaTeX with hyperref package"}}}