{"id": "1704.02712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation", "abstract": "346.7 Many casseroles modern 103 computer proti vision and iron-hulled machine learning applications rely panchami on toste solving licavoli difficult allegis optimization problems natalja that bcsc involve non - differentiable objective arabic-speaking functions petronius and fifthly constraints. 39.63 The alternating direction dislocation method articulating of bloodsucker multipliers (xylariaceae ADMM) esox is dayparts a widely someone used stanka approach mid-cap to solve such radomy\u015bl problems. Relaxed practice-oriented ADMM is a generalization branston of ADMM treholt that often 711,000 achieves authoritatively better performance, oommen but dendrobates its efficiency depends strongly on algorithm parameters that aerofoils must be cyberbullying chosen by 018 an expert prosthetic user. We propose tpx an adaptive method hounsell that 58.69 automatically tupamaros tunes the fradkin key lorem algorithm mitrokhin parameters to ferrite achieve kurupt optimal balewadi performance almanza without user oversight. Inspired fearsome by f\u00e9licit\u00e9 recent work uniate on adaptivity, brockes the grown-ups proposed ascalon adaptive t-70 relaxed ADMM (ARADMM) s\u00f8rensen is ehv derived by lidl assuming fabbiano a Barzilai - Borwein parvo style kristof linear gradient. A detailed convergence zakat analysis of 48.12 ARADMM is coulier provided, and numerical scroobius results ndegwa on 71.21 several adapters applications demonstrate pluralization fast 94-89 practical arora convergence.", "histories": [["v1", "Mon, 10 Apr 2017 05:07:38 GMT  (163kb,D)", "http://arxiv.org/abs/1704.02712v1", "CVPR 2017"]], "COMMENTS": "CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["zheng xu", "mario a t figueiredo", "xiaoming yuan", "christoph studer", "tom goldstein"], "accepted": false, "id": "1704.02712"}, "pdf": {"name": "1704.02712.pdf", "metadata": {"source": "CRF", "title": "Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation", "authors": ["Zheng Xu", "M\u00e1rio A. T. Figueiredo", "Xiaoming Yuan", "Christoph Studer", "Tom Goldstein"], "emails": ["xuzh@cs.umd.edu"], "sections": [{"heading": "1. Introduction", "text": "Modern methods in computer vision and machine learning often require solving difficult optimization problems involving non-differentiable objective functions and constraints. Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3]. The alternating direction method of multiplier (ADMM) is one of the most prominent optimization tools to solve such problems, and tackles problems in the following form:\nmin u\u2208Rn,v\u2208Rm h(u) + g(v), subject to Au+Bv = b. (1)\nHere, h : Rn \u2192 R and g : Rm \u2192 R are closed, proper, and convex functions, A \u2208 Rp\u00d7n, B \u2208 Rp\u00d7m, and b \u2208 Rp. ADMM was first introduced in [16] and [12], and has found\n\u2217xuzh@cs.umd.edu\napplications in a variety of optimization problems in machine learning, image processing, computer vision, wireless communications, and many other areas [2, 21].\nRelaxed ADMM is a popular practical variant of ADMM, and proceeds with the following steps:\nuk+1 = arg min u h(u) + \u03c4k 2 \u2225\u2225\u2225\u2225b\u2212Au\u2212Bvk + \u03bbk\u03c4k \u2225\u2225\u2225\u22252 (2)\nu\u0303k+1 = \u03b3kAuk+1 + (1\u2212 \u03b3k)(b\u2212Bvk) (3)\nvk+1 = arg min v g(v) + \u03c4k 2 \u2225\u2225\u2225\u2225b\u2212 u\u0303k+1 \u2212Bv + \u03bbk\u03c4k \u2225\u2225\u2225\u22252 (4)\n\u03bbk+1 = \u03bbk + \u03c4k(b\u2212 u\u0303k+1 \u2212Bvk+1). (5)\nHere, \u03bbk \u2208Rp denotes the dual variables (Lagrange multipliers) on iteration k, and (\u03c4k, \u03b3k) are sequences of penalty and relaxation parameters. Relaxed ADMM coincides with the original non-relaxed version if \u03b3k = 1.\nConvergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant. However, the practical performance of ADMM depends strongly on the choice of these parameters, as well as on the problem being solved. Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the \u201clinearized\u201d ADMM [32, 34].\nAdaptive penalty methods (in which the penalty parameters are tuned automatically as the algorithm proceeds) achieve good performance without user oversight. For nonrelaxed ADMM, the authors of [24] propose methods that modulate the penalty parameter so that the primal and dual residuals (i.e., derivatives of the Lagrangian with respect to primal and dual variables) are of approximately equal size. This \u201cresidual balancing\u201d approach has been generalized to work with preconditioned variants of ADMM [20] and distributed ADMM [44]. In [51], a spectral penalty parameter method is proposed that uses the local curvature of the objective to achieve fast convergence. All of these methods are\n1\nar X\niv :1\n70 4.\n02 71\n2v 1\n[ cs\n.C V\n] 1\n0 A\npr 2\nspecific to (non-relaxed) vanilla ADMM, and do not apply to the more general case involving a relaxation parameter."}, {"heading": "1.1. Overview & contributions", "text": "In this paper, we study adaptive parameter choices for the relaxed ADMM that jointly and automatically tune both the penalty parameter \u03c4k and relaxation parameter \u03b3k. In Section 3, we address theoretical questions about the convergence of ADMM with non-constant penalty and relaxation parameters. In Section 4, we discuss practical methods for choosing these parameters. In Section 6, we apply the proposed ARADMM to several problems in machine learning, computer vision, and image processing. Finally, in Section 7, we compare ARADMM to other ADMM variants and examine the benefits of the proposed approach for real-world regression, classification, and image processing problems."}, {"heading": "2. Related work", "text": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21]. ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.\nThe O(1/k) convergence rate of non-relaxed ADMM is established under mild conditions for convex problems [25, 26]. The O(1/k2) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth. For the general relaxed ADMM formulation, a O(1/k) convergence rate is provided under mild conditions [10]. Linear convergence can be achieved with strong convexity assumptions [5, 38, 15]. All of these results assume constant parameters\u2014it is considerably harder to prove convergence when the algorithm parameters are adaptive.\nFixed optimal parameters are discussed in the literature. For the specific case in which the objective is quadratic, a criterion is proposed in [40, 14]. The authors of [38] suggest a grid search and semidefinite programming based method to determine the optimal relaxation and penalty parameters. These methods, however, make strong assumptions about the objective and require knowledge of condition numbers.\nAdaptive penalty methods are proposed to accelerate the practical convergence of non-relaxed ADMM [24, 51]. For the relaxation parameter, it has been suggested in [6] that over-relaxation (\u03b3 \u2208 (1, 2)) may accelerate convergence and \u03b3 = 1.5 achieves faster convergence in a specific distributed computing application. The proposed ARADMM simultaneously adapts both the penalty and the relaxation parameter, thus being fully automated."}, {"heading": "3. Convergence theory", "text": "We study conditions under which ADMM converges with adaptive penalty and relaxation parameters. Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26]. Our results measure convergence using the primal and dual \u201cresiduals,\u201d which are defined as\nrk = b\u2212Auk \u2212Bvk and dk = \u03c4kATB(vk \u2212 vk\u22121). (6)\nIt has been observed that these residuals approach zero as the algorithm approaches a true solution [2]. Typically, the iterative process is stopped if\n\u2016rk\u2016 \u2264 tol max{\u2016Auk\u2016, \u2016Bvk\u2016, \u2016b\u2016} and \u2016dk\u2016 \u2264 tol\u2016AT\u03bbk\u2016,\n(7)\nwhere tol > 0 is the stopping tolerance [2]. For this reason, it is important to know that the method converges in the sense that the residuals approach zero as k \u2192\u221e.\nIn the sequel, we prove that relaxed ADMM converges in the residual sense, provided that the algorithm parameters satisfy one of the following two assumptions.\nAssumption 1. The relaxation sequence \u03b3k and penalty sequence \u03c4k satisfy\n1 \u2264 \u03b3k < 2, lim k\u2192\u221e 1/\u03c42k <\u221e, \u221e\u2211 k=1 \u03b72k <\u221e,\nwhere \u03b72k = \u03b3k\n(2\u2212 \u03b3k) max\n( \u03c42k/\u03c4 2 k\u22121, 1 ) \u2212 1. (8)\nAssumption 2. The relaxation sequence \u03b3k and penalty sequence \u03c4k satisfy\n1 \u2264 \u03b3k < 2, lim k\u2192\u221e \u03c42k <\u221e, \u221e\u2211 k=1 \u03b82k <\u221e,\nwhere \u03b82k = \u03b3k\n(2\u2212 \u03b3k) max\n( \u03c42k\u22121/\u03c4 2 k , 1 ) \u2212 1. (9)\nIn Section 5, we prove adaptive relaxed ADMM converges if the algorithm parameters satisfy either Assumption 1 or Assumption 2. Before presenting the proof, we show how to choose the relaxation parameters that lead to efficient performance in practice."}, {"heading": "4. ARADMM: Adaptive relaxed ADMM", "text": "Spectral stepsize selection methods for vanilla ADMM were discussed in [51]. Here, we modify the adaptive ADMM framework in two important ways. First, we discuss the selection of penalty parameters in the presence of the relaxation term. Second, we discuss adaptive methods also for automatically selecting the relaxation parameter.\nThe proposed method works by assuming a local linear model for the dual optimization problem, and then selecting\nan optimal stepsize under this assumption. A safeguarding method is adopted to ensure that bad stepsizes are not chosen in case these linearity assumptions fail to hold."}, {"heading": "4.1. Dual interpretation of relaxed ADMM", "text": "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15]. The dual of the general constrained problem (1) is\nmin \u03b6\u2208Rp h\u2217(AT \u03b6)\u2212 \u3008\u03b6, b\u3009\ufe38 \ufe37\ufe37 \ufe38 h\u0302(\u03b6) + g\u2217(BT \u03b6)\ufe38 \ufe37\ufe37 \ufe38 g\u0302(\u03b6) , (10)\nwith f\u2217 denoting the Fenchel conjugate of f , defined as f\u2217(y) = supx\u3008x, y\u3009 \u2212 f(x) [41].\nThe relaxed DRS algorithm solves (10) by generating two sequences, (\u03b6k)k\u2208N and (\u03b6\u0302k)k\u2208N, according to\n0 \u2208 \u03b6\u0302k+1 \u2212 \u03b6k \u03c4k + \u2202h\u0302(\u03b6\u0302k+1) + \u2202g\u0302(\u03b6k), (11) 0 \u2208\u03b6k+1 \u2212 \u03b6k \u03c4k + \u03b3k \u2202h\u0302(\u03b6\u0302k+1)\n\u2212 (1\u2212 \u03b3k)\u2202g\u0302(\u03b6k) + \u2202g\u0302(\u03b6k+1), (12)\nwhere \u03b3k is a relaxation parameter, and \u2202f(x) denotes the subdifferential of f evaluated at x [41]. Referring back to ADMM in (2)\u2013(5), and defining \u03bb\u0302k+1 = \u03bbk + \u03c4k(b \u2212 Auk+1\u2212Bvk), the sequences (\u03bbk)k\u2208N and (\u03bb\u0302k)k\u2208N satisfy the same conditions (11) and (12) as (\u03b6k)k\u2208N and (\u03b6\u0302k)k\u2208N, thus ADMM for the problem (1) is equivalent to DRS on its dual (10). A detailed proof of this is provided in the supplementary material."}, {"heading": "4.2. Spectral adaptive stepsize rule", "text": "Adaptive stepsize rules of the \u201cspectral\u201d type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49]. Spectral stepsize methods work by modeling the gradient of the objective as a linear function, and then selecting the optimal stepsize for this simplified linear model.\nSpectral methods were recently used to determine the penalty parameter for the non-relaxed ADMM in [51]. Inspired by that work, we derive spectral stepsize rules assuming a linear model/approximation for \u2202h\u0302(\u03b6\u0302) and \u2202g\u0302(\u03b6) at iteration k given by\n\u2202h\u0302(\u03b6\u0302) = \u03b1k \u03b6\u0302 + \u03a8k and \u2202g\u0302(\u03b6) = \u03b2k \u03b6 + \u03a6k, (13)\nwhere \u03b1k > 0, \u03b2k > 0 are local curvature estimates of h\u0302 and g\u0302, respectively, and \u03a8k,\u03a6k \u2282 Rp. Once we obtain these curvature estimates, we will exploit the following simple proposition whose proof is given in the supplementary material.\nProposition 1. Suppose the DRS steps (11)\u2013(12) are applied to problem (10), where (omitting iteration k from \u03b1k, \u03b2k,\u03a8k,\u03a6k to lighten the notation in what follows)\n\u2202h\u0302(\u03b6\u0302) = \u03b1 \u03b6\u0302 + \u03a8 and \u2202g\u0302(\u03b6) = \u03b2 \u03b6 + \u03a6. (14)\nThen, the residual of h\u0302(\u03b6k+1) + g\u0302(\u03b6k+1) will be zero if \u03c4 and \u03b3 are chosen to satisfiy \u03b3k = 1 + 1+\u03b1\u03b2\u03c42k (\u03b1+\u03b2)\u03c4k .\nOur adaptive method works by fitting a linear model to the gradient (or subgradient) of our objective, and then using Proposition 1 to select an optimal stepsize pair that obtains zero residual on the model problem. For our convergence theory to hold, we need \u03b3 < 2. For fixed values of \u03b1 and \u03b2, the minimal value of \u03b3k that is still optimal for the linear model occurs if we choose\n\u03c4k = arg min \u03c4\n1 + \u03b1\u03b2\u03c42 (\u03b1+ \u03b2)\u03c4 = 1/\n\u221a \u03b1\u03b2. (15)\nNote this is the same \u201coptimal\u201d penalty parameter proposed for non-relaxed ADMM in [51]. Under this choice of \u03c4k, we then have the \u201coptimal\u201d relaxation parameter\n\u03b3k = 1 + 1 + \u03b1\u03b2\u03c42\n(\u03b1+ \u03b2)\u03c4 = 1 +\n2 \u221a \u03b1\u03b2 \u03b1+ \u03b2 \u2264 2. (16)"}, {"heading": "4.3. Estimation of stepsizes", "text": "We now propose a simple method for fitting a linear model to the dual objective terms so that the formulas in Section 4.2 can be used to obtain stepsizes. Once these linear models are formed, the optimal penalty parameter and relaxation term can be calculated by (15) and (16), thanks to the equivalence of relaxed ADMM and DRS.\nIn what follows, we let \u03b1\u0302k = 1/\u03b1k and \u03b2\u0302k = 1/\u03b2k to simplify notation. The optimal stepsize choice is then written as \u03c4k = (\u03b1\u0302k \u03b2\u0302k)1/2 and \u03b3k = 1 + 2 \u221a \u03b1\u0302k\u03b2\u0302k\n\u03b1\u0302k+\u03b2\u0302k .\nThe estimation of \u03b1\u0302k and \u03b2\u0302k for the dual components h\u0302(\u03bb\u0302k) and g\u0302(\u03bbk) at the k-th iteration of primal ADMM has been described in [51]. It is easy to verify that the model parameters \u03b1\u0302k and \u03b2\u0302k of relaxed ADMM can be estimated based on the results from iteration k and an older iteration k0 < k in a similar way. If we define\n\u2206\u03bb\u0302k := \u03bb\u0302k \u2212 \u03bb\u0302k0 and \u2206h\u0302k := A(uk \u2212 uk0), (17)\nthen the parameter \u03b1\u0302k is obtained from the formula\n\u03b1\u0302k =\n{ \u03b1\u0302MGk if 2 \u03b1\u0302 MG k > \u03b1\u0302 SD k\n\u03b1\u0302SDk \u2212 \u03b1\u0302MGk /2 otherwise, (18)\n\u03b1\u0302SDk = \u3008\u2206\u03bb\u0302k,\u2206\u03bb\u0302k\u3009 \u3008\u2206h\u0302k,\u2206\u03bb\u0302k\u3009 and \u03b1\u0302MGk = \u3008\u2206h\u0302k,\u2206\u03bb\u0302k\u3009 \u3008\u2206h\u0302k,\u2206h\u0302k\u3009 . (19)\nFor a detailed derivation of these formulas, see [51].\nThe spectral stepsize \u03b2\u0302k of g\u0302(\u03bbk) is similarly estimated with \u2206g\u0302k :=B(vk \u2212 vk0), and \u2206\u03bbk := \u03bbk \u2212 \u03bbk0 . It is important to note that \u03b1\u0302k and \u03b2\u0302k are obtained from the iterates of ADMM alone, i.e., our scheme does not require the user to supply the dual problem."}, {"heading": "4.4. Safeguarding", "text": "Spectral stepsize methods for simple gradient descent are paired with a backtracking line search to guarantee convergence in case the linear model assumptions break down and an unstable stepsize is produced. ADMM methods have no analog of backtracking. Rather, we adopt the correlation criterion proposed in [51] to test the validity of the local linear assumption, and only rely on the adaptive model when the assumptions are deemed valid. To this end, we define\n\u03b1cork = \u3008\u2206h\u0302k,\u2206\u03bb\u0302k\u3009 \u2016\u2206h\u0302k\u2016 \u2016\u2206\u03bb\u0302k\u2016 and \u03b2cork = \u3008\u2206g\u0302k,\u2206\u03bbk\u3009 \u2016\u2206g\u0302k\u2016 \u2016\u2206\u03bbk\u2016 . (20)\nWhen the model assumptions (14) hold perfectly, the vectors \u2206h\u0302k and \u2206\u03bb\u0302k should be highly correlated and we get \u03b1cork = 1. When \u03b1cork or \u03b2 cor k is small, the model assumptions are invalid and the spectral stepsize may not be effective. The proposed method uses the following update rules\n\u03c4k+1 =  \u221a \u03b1\u0302k\u03b2\u0302k if \u03b1cork > cor and \u03b2cork > cor \u03b1\u0302k if \u03b1cork > cor and \u03b2cork \u2264 cor \u03b2\u0302k if \u03b1cork \u2264 cor and \u03b2cork > cor\n\u03c4k otherwise,\n(21)\n\u03b3k+1 =  1 + 2 \u221a \u03b1\u0302k\u03b2\u0302k \u03b1\u0302k+\u03b2\u0302k if \u03b1cork > cor and \u03b2cork > cor 1.9 if \u03b1cork > cor and \u03b2cork \u2264 cor 1.1 if \u03b1cork \u2264 cor and \u03b2cork > cor\n1.5 otherwise,\n(22)\nwhere cor is a quality threshold for the curvature estimates, while \u03b1\u0302k and \u03b2\u0302k are the spectral stepsizes estimated in Section 4.3. The update for \u03c4k+1 only uses model parameters that have been accurately estimated. When the model is effective for h but not g, we use a large \u03b3k = 1.9 to make the v update conservative relative to the u update. When the model is effective for g but not h, we use a small \u03b3k = 1.1 to make the v update aggressive relative to the u update."}, {"heading": "4.5. Applying convergence guarantee", "text": "Our convergence theory requires either Assumption 1 or Assumption 2 to be satisfied, which suggests that convergence is guaranteed under \u201cbounded adaptivity\u201d for both penalty and relaxation parameters. These conditions can be guaranteed by explicitly adding constraints to the stepsize choice in ARADMM.\nAlgorithm 1 Adaptive relaxed ADMM (ARADMM) Input: initialize v0, \u03bb0, \u03c40, \u03b30, and k0 =0\n1: while not converge by (7) and k < maxiter do 2: Perform relaxed ADMM, as in (2)\u2013(5) 3: if mod(k, Tf ) = 1 then 4: \u03bb\u0302k+1 = \u03bbk + \u03c4k(b\u2212Auk+1 \u2212Bvk) 5: Compute spectral stepsizes \u03b1\u0302k, \u03b2\u0302k using (18) 6: Estimate correlations \u03b1cork , \u03b2 cor k using (20) 7: Update \u03c4k+1, \u03b3k+1 using (21) and (22) 8: Bound \u03c4k+1, \u03b3k+1 using (23) 9: k0 \u2190 k\n10: else 11: \u03c4k+1 \u2190 \u03c4k and \u03b3k+1 \u2190 \u03b3k 12: end if 13: k \u2190 k + 1 14: end while\nTo guarantee convergence, we simply replace the parameter updates (21) and (22) with\n\u03c4\u0302k+1 = min {\u03c4k+1, (1 + Ccg/k2) \u03c4k} \u03b3\u0302k+1 = min {\u03b3k+1, 1 + Ccg/k2},\n(23)\nwhere Ccg is some (large) constant. It is easily verified that the parameter sequence (\u03c4\u0302k, \u03b3\u0302k) satisfies Assumption 1. In practice, the update schemes (21) and (22) converges reliably without explicitly enforcing these conditions. We use a very large Ccg such that the conditions are not triggered in the first few thousand iterations and provide these constraints for theoretical interests."}, {"heading": "4.6. ARADMM algorithm", "text": "The complete adaptive relaxed ADMM (ARADMM) is shown in Algorithm 1. We suggest only updating the stepsize every Tf = 2 iterations. We suggest a fixed safeguarding threshold cor = 0.2, which is used in all the experiments in Section 6. The overhead of the adaptive scheme is modest, requiring only a few inner product calculations."}, {"heading": "5. Proofs of convergence theorems", "text": "We now prove that relaxed ADMM converges under Assumption 1 or 2. Let\ny = ( u v ) \u2208 Rn+m, z = uv \u03bb  \u2208 Rn+m+p. (24) We use yk = (uk, vk)T and zk = (uk, vk, \u03bbk)T to denote iterates, and y\u2217 = (u\u2217, v\u2217)T and z\u2217 = (u\u2217, v\u2217, \u03bb\u2217)T denote optimal solutions. Set \u2206z+k = (\u2206u + k ,\u2206v + k ,\u2206\u03bb + k ) := zk+1 \u2212 zk, and \u2206z\u2217k = (\u2206u\u2217k,\u2206v\u2217k,\u2206\u03bb\u2217k) := z\u2217 \u2212 zk, and\ndefine\nf(y) = h(u) + g(v), F (z) =  \u2212AT\u03bb\u2212BT\u03bb Au+Bv \u2212 b . (25) Notice that F (z) is monotone, which means \u2200z, z\u2032, (z \u2212 z\u2032)T (F (z)\u2212 F (z\u2032)) \u2265 0.\nProblem formulation (1) can be reformulated as a variational inequality (VI). The optimal solution z\u2217 satisfies\n\u2200z, f(y)\u2212 f(y\u2217) + (z \u2212 z\u2217)TF (z\u2217) \u2265 0. (26)\nLikewise, the ADMM iterates produced by steps (2) and (4) satisfy the variational inequalities\n\u2200u, h(u)\u2212 h(uk+1) + (u\u2212 uk+1)T\n(\u03c4kA T (Auk+1 +Bvk \u2212 b)\u2212AT\u03bbk) \u2265 0, (27)\n\u2200v, g(v)\u2212 g(vk+1) + (v \u2212 vk+1)T\n(\u03c4kB T (u\u0303k+1 +Bvk+1 \u2212 b)\u2212BT\u03bbk) \u2265 0. (28)\nUsing the definitions of y, z, f(y), and F (z) in (24, 25), \u03bb in (5), and u\u0303 in (3), VI (27) and (28) combine to yield\nf(y)\u2212 f(yk+1) + (z \u2212 zk+1)T ( F (zk+1) + \u2126(\u2206z + k , \u03c4k, \u03b3k) ) \u2265 0,\n\u2126(\u2206z+k , \u03c4k, \u03b3k) =  \u03b3k\u22121 \u03b3k AT\u2206\u03bb+k \u2212 \u03c4k \u03b3k ATB\u2206v+k\n0 1\n\u03b3k\u03c4k \u2206\u03bb+k \u2212 \u03b3k\u22121 \u03b3k B\u2206v+k . (29) We then apply VI (26), (28), and (29) in order to prove the following lemmas for our contraction proof, which show that the difference between iterates decreases as the iterates approach the true solution. \u2018The remaining details of the proof are in the supplementary material.\nLemma 1. The iterates zk = (uk, vk, \u03bbk)T generated by ADMM satisfy\n(B\u2206v+k ) T\u2206\u03bb+k \u2265 0. (30)\nLemma 2. Let \u03b3k \u2265 1. The optimal solution z\u2217 and iterates zk generated by ADMM satisfy\n2\u2212 \u03b3k \u03b3k \u2016\u03c4kB\u2206v+k + \u2206\u03bb + k \u2016 2\n\u2264\u03b3k(\u2016\u03c4kB\u2206v\u2217k\u20162 + \u2016\u2206\u03bb\u2217k\u20162) \u2212 (2\u2212 \u03b3k)(\u2016\u03c4kB\u2206v\u2217k+1\u20162 + \u2016\u2206\u03bb\u2217k+1\u20162).\n(31)"}, {"heading": "5.1. Convergence with adaptivity", "text": "We are now ready to state our main convergence results. The proof of Theorem 1 is shown here in full, and leverages Lemma 2 to produce a contraction argument. The proof of Theorem 2 is extremely similar, and is shown in the supplementary material.\nTheorem 1. Suppose Assumption 1 holds. Then, the iterates zk = (uk, vk, \u03bbk) T generated by ADMM satisfy\nlim k\u2192\u221e \u2016rk\u2016 = 0 and lim k\u2192\u221e \u2016dk\u2016 = 0. (32)\nProof. Assumption 1 implies \u03b3k\n2\u2212 \u03b3k \u03c42k \u2264 (1 + \u03b72k)\u03c42k\u22121 and \u03b3k 2\u2212 \u03b3k \u2264 (1 + \u03b72k). (33)\nIf \u03b3k < 2 as in Assumption 1, then Lemma 2 shows\n1\n\u03b3k \u2016\u03c4kB\u2206v+k + \u2206\u03bb + k \u2016 2\n\u2264 \u03b3k 2\u2212 \u03b3k (\u03c42k\u2016B\u2206v\u2217k\u20162 + \u2016\u2206\u03bb\u2217k\u20162)\n\u2212 (\u03c42k\u2016B\u2206v\u2217k+1\u20162 + \u2016\u2206\u03bb\u2217k+1\u20162) (34) \u2264(1 + \u03b72k)(\u03c42k\u22121\u2016B\u2206v\u2217k\u20162 + \u2016\u2206\u03bb\u2217k\u20162) \u2212 (\u03c42k\u2016B\u2206v\u2217k+1\u20162 + \u2016\u2206\u03bb\u2217k+1\u20162), (35)\nwhere (33) is used to get from (34) to (35). Accumulating inequality (35) from k = 0 to N shows\nN\u2211 k=0 N\u220f t=k+1 (1 + \u03b72t ) 1 \u03b3k \u2016\u03c4kB\u2206v+k + \u2206\u03bb + k \u2016 2\n\u2264 N\u220f k=1 (1 + \u03b72t )(\u03c4 2 0 \u2016B\u2206v\u22170\u20162 + \u2016\u2206\u03bb\u22170\u20162). (36)\nAssumption 1 also implies \u220f\u221e t=1(1 + \u03b7\n2 t ) < \u221e, and\u220fN\nt=k+1(1 + \u03b7 2 t ) 1 \u03b3k \u2265 1\u03b3k > 1/2. Then, (36) indicates\u2211\u221e\nk=0 \u2016\u03c4kB\u2206v + k + \u2206\u03bb + k \u20162 <\u221e, and\nlim k\u2192\u221e\n\u2016\u03c4kB\u2206v+k + \u2206\u03bb + k \u2016 2 = 0. (37)\nNow, from Lemma 1, (B\u2206v+k ) T\u2206\u03bb+k \u2265 0, and so\nlim k\u2192\u221e\n\u2016\u2206\u03bb+k \u2016 2 \u2264 lim k\u2192\u221e \u2016\u03c4kB\u2206v+k + \u2206\u03bb + k \u2016 2 = 0, (38)\nlim k\u2192\u221e\n\u2016\u03c4kB\u2206v+k \u2016 2 \u2264 lim k\u2192\u221e \u2016\u03c4kB\u2206v+k + \u2206\u03bb + k \u2016 2 = 0. (39)\nThe residuals rk, dk in (6) satisfy\nrk = 1\n\u03b3k\u03c4k \u2206\u03bb+k\u22121 \u2212 \u03b3k \u2212 1 \u03b3k B\u2206v+k\u22121, (40)\ndk = \u03c4kA TB\u2206v+k\u22121, (41)\nfrom which we get\nlim k\u2192\u221e \u2016rk\u2016 \u2264 lim k\u2192\u221e\n1\n\u03b3k\u03c4k \u2016\u2206\u03bb+k\u22121\u2016\n+ \u03b3k \u2212 1 \u03b3k\u03c42k\u22121 \u2016\u03c4k\u22121B\u2206v+k\u22121\u2016 = 0, and\nlim k\u2192\u221e \u2016dk\u2016 \u2264 lim k\u2192\u221e \u2016A\u2016\u2016\u03c4kB\u2206v+k\u22121\u2016\n\u2264 lim k\u2192\u221e\n\u221a 1 + \u03b72k\u2016A\u2016 \u2016\u03c4k\u22121B\u2206v + k\u22121\u2016 = 0.\nSimilar methods can be used to prove the following about convergence under Assumption 2. The proof of the following theorem is given in the supplementary material.\nTheorem 2. Suppose Assumption 2 holds. Then, the iterates zk = (uk, vk, \u03bbk) T generated by ADMM satisfy\nlim k\u2192\u221e \u2016rk\u2016 = 0 and lim k\u2192\u221e \u2016dk\u2016 = 0. (42)"}, {"heading": "6. Applications", "text": "We focus on the following statistical and image processing problems involving non-differentiable objectives: linear regression with elastic net regularization (EN), low-rank least squares (LRLS), quadratic programming (QP), consensus `1-regularized logistic regression, support vector machine (SVM), total variation image restoration (TVIR), and robust principle component analysis (RPCA). We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28]. We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page. The experimental setups for each problem are briefly described here, and the implementation details are provided in the supplementary material.\nLinear regression with EN regularization Elastic net (EN) is a modification of the `1-norm (or LASSO) regularizer that helps dealing with highly correlated variables [57, 21], and requires solving\nmin x\n1 2 \u2016Dx\u2212 c\u201622 + \u03c11\u2016x\u20161 + \u03c12 2 \u2016x\u201622, (43)\nwhere \u2016 \u00b7 \u20161 denotes the `1-norm, D is the data matrix, c contains measurements, and x is the vector of regression coefficient.\nLow-rank least squares (LRLS) The nuclear norm (the `1-norm of the matrix singular values) is a convex surrogate for matrix rank. ADMM has been applied to solve low rank least squares problems [55, 53]\nmin X\n1 2 \u2016DX \u2212 C\u20162F + \u03c11\u2016X\u2016\u2217 + \u03c12 2 \u2016X\u20162F , (44)\nwhere \u2016 \u00b7 \u2016\u2217 denotes the nuclear norm, \u2016 \u00b7 \u2016F denotes the Frobenius norm, D \u2208 Rn\u00d7m is a data matrix, C \u2208 Rn\u00d7d contains measurements, and X \u2208 Rm\u00d7d contains variables.\nADMM is applied by splitting the regression term and the non-differentiable regularizer composed of nuclear and Frobenius norm. LRLS has been used to formulate exemplar classifiers and discover visual subcategories [53].\n1We use the first batch of CIFAR10 that contains 10000 samples.\nSVM and QP Support vector machine (SVM) is one of the most successful binary classifiers for computer vision. The dual of the SVM is a QP problem,\nmin z\n1 2 zTQz \u2212 eT z\nsubject to cT z = 0 and 0 \u2264 z \u2264 C,\nwhere z is the SVM dual variable, Q is the kernel matrix, c is a vector of labels, e is a vector of ones, and C > 0 [3]. The canonical QP is also considered,\nmin x\n1 2 xTQx+ qTx subject to Dx \u2264 c. (45)\nConsensus `1-regularized logistic regression ADMM has become an important tool for solving distributed optimization problems [2]. A typical problem is the consensus `1-regularized logistic regression\nmin xi,z N\u2211 i=1 ni\u2211 j=1 log(1 + exp(\u2212cjDjxi)) + \u03c1\u2016z\u20161\nsubject to xi \u2212 z = 0, i = 1, . . . , N,\n(46)\nwhere xi \u2208 Rm represents the local variable on the ith distributed node, z is the global variable, ni is the number of samples in the ith block, Dj \u2208 Rm is the jth sample, and cj \u2208 {\u22121,+1} is the corresponding label. Unwrapped SVM The unwrapped formulation of SVM [22], which can be used in distributed computing environments via \u201ctranspose reduction\u201d tricks, applies ADMM to the primal form of SVM to solve\nmin x\n1 2 \u2016x\u201622 + C n\u2211 j=1 max{1\u2212 cjDTj x, 0}, (47)\nwhere Dj \u2208 Rm is the jth sample of training data, and cj \u2208 {\u22121, 1} is the corresponding label. ADMM is applied by splitting the `2-norm regularizer and the non-differentiable hinge loss term. Total variation image denoising (TVID) Total variation image denoising is often performed by solving [42]\nmin x\n1 2 \u2016x\u2212 c\u201622 + \u03c1\u2016\u2207x\u20161 (48)\nwhere c represents given noisy image, and \u2207 is the discrete gradient operator, which computes differences between adjacent image pixels. ADMM is applied by splitting the `2-norm term and the non-differentiable total variation term. RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39]. RPCA recovers a low-rank matrix and a sparse matrix by solving\nmin Z,E \u2016Z\u2016\u2217 + \u03c1\u2016E\u20161 subject to Z + E = C, (49)\nwhere the nuclear norm \u2016 \u00b7 \u2016\u2217 is used to obtain a low rank matrix Z, and \u2016 \u00b7 \u20161 is used to obtain a sparse error E."}, {"heading": "7. Experiments", "text": "The proposed AADMM is implemented as shown in Algorithm 1. We also implemented vanilla ADMM, (nonadaptive) relaxed ADMM, ADMM with residual balancing (RB), and adaptive ADMM (AADMM) for comparison.\nThe relaxation parameter for the non-adaptive relaxed ADMM is fixed at \u03b3k=1.5 as suggested in [6]. The parameters of RB and AADMM are selected as in [24, 2, 51]. The initial penalty \u03c40 =1/10 and initial relaxation \u03b30 =1 are used for all problems except the canonical QP problem, where initial parameters are set to the geometric mean of the maximum and minimum eigenvalues of matrix Q, as proposed for quadratic problems in [40].\nFor each problem, the same randomly generated initial variables v0, \u03bb0 are used for ADMM and its variant methods. As suggested by [24, 51], the adaptivity of RB and AADMM is stopped after 1000 iterations to guarantee convergence."}, {"heading": "7.1. Convergence results", "text": "Table 1 reports the convergence speed of ADMM and its variants for the applications described in Section 6. More experimental results including the table of more test cases, the convergence curves, and visual results of image restoration and robust PCA for face decomposition are provided in the supplementary material. Relaxed ADMM often outperforms vanilla ADMM, but does not compete with adaptive methods like RB, AADMM and ARADMM. The proposed ARADMM performs best in all the test cases."}, {"heading": "7.2. Sensitivity to initialization", "text": "We study the sensitivity of the different ADMM variants to the initial penalty (\u03c40) and initial relaxation parameter (\u03b30). Fig. 1 presents iteration counts for a wide range of values of \u03c40, \u03b30, for elastic net regression with synthetic datasets. In the left and center plots we fix one of \u03c40, \u03b30 and vary the other. The number of iterations needed to convergence is plotted as the algorithm parameters vary. In the right plot, we use a grid search to find the optimal \u03c40 for different\n10 -5 10 -4 10 -3 10 -2 10 -1 10 0 10 1 10 2 10 3 10 4 10 510\n0\n10 1\n10 2 10 3\nInitial penalty parameter\nIte ra\ntio ns\nVanilla ADMM Relaxed ADMM Residual balance Adaptive ADMM ARADMM\n1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 10\n0\n10 1\n10 2 10 3\nInitial relaxation parameter\nIte ra\ntio ns\nVanilla ADMM Relaxed ADMM Residual balance Adaptive ADMM ARADMM\n1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 10\n0\n10 1\n10 2 10 3\nInitial relaxation parameter\nIte ra\ntio ns\nVanilla ADMM Relaxed ADMM Residual balance Adaptive ADMM ARADMM\nvalues of \u03b30. Fig. 1 (left) shows that adaptive methods are relatively stable with respect to the initial penalty \u03c40, while ARADMM outperforms RB and AADMM in all choices of initial \u03c40. Fig. 1 (middle) suggests that the relaxation \u03b30 is generally less important than \u03c40. When a bad value of \u03c4 is chosen, it is unlikely that a good choice of \u03b3 can compensate. The proposed ARADMM that jointly adjusts \u03c4, \u03b3 is generally better than simply adding the relaxation to the existing adaptive methods RB and AADMM.\nFig. 1 (right) shows the sensitivity to \u03b3 when using a grid search to choose the optimal \u03c40. This optimal \u03c40 significantly improves the performance of vanilla ADMM and relaxed ADMM (which use the same \u03c40 for all iterations). Even when using the optimal stepsize for the non-adaptive methods, ARADMM is superior to or competitive with the non-adaptive methods. Note that this experiment is meant to show a best-case scenario for the non-adaptive methods; in practice the user generally has no knowledge of the optimal value of \u03c4. Adaptive methods achieve optimal or nearoptimal performance without an expensive grid search."}, {"heading": "7.3. Sensitivity to safeguarding", "text": "Finally, Fig. 2 presents iteration counts when applying ARADMM with various safeguarding correlation thresholds cor. When cor = 0, the calculated adaptive parameters\nbased on curvature estimations are always accepted, and when cor = 1 the parameters are never changed. The proposed AADMM method is insensitive to cor and performs well for a wide range of cor \u2208 [0.1, 0.4] for various applications, except for unwrapping SVM and RPCA. Though tuning such \u201chyper-parameters\u201d may improve the performance of ARADMM for some applications, the fixed cor = 0.2 performs well in all our experiments (seven applications and over fifty test cases, a full list is in the supplementary material). The proposed ARADMM is fully automated and performs well without parameter tuning."}, {"heading": "8. Conclusion", "text": "We have proposed an adaptive method for jointly tuning the penalty and relaxation parameters of relaxed ADMM without user oversight. We have analyzed adaptive relaxed ADMM schemes, and provided conditions for which convergence is guaranteed. Experiments on a wide range of machine learning, computer vision, and image processing benchmarks have demonstrated that the proposed adaptive method (often significantly) outperforms other ADMM variants without user oversight or parameter tuning. The new adaptive method improves the applicability of relaxed ADMM by facilitating fully automated solvers that exhibit fast convergence and are usable by non-expert users."}, {"heading": "Acknowledgments", "text": "TG and ZX were supported by the US Office of Naval Research under grant N00014-17-1-2078 and by the US National Science Foundation (NSF) under grant CCF-1535902. MF was partially supported by the Fundac\u0327a\u0303o para a Cie\u0302ncia e Tecnologia, grant UID/EEA/5008/2013. XY was supported by the General Research Fund from Hong Kong Research Grants Council under grant HKBU-12313516. CS was supported in part by Xilinx Inc., and by the US NSF under grants ECCS-1408006, CCF-1535897, and CAREER CCF1652065."}], "references": [{"title": "Two-point step size gradient methods", "author": ["J. Barzilai", "J. Borwein"], "venue": "IMA J. Num. Analysis, 8:141\u2013148,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1988}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. and Trends in Mach. Learning, 3:1\u2013122,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Faster convergence rates of relaxed Peaceman-Rachford and ADMM under regularity assumptions", "author": ["D. Davis", "W. Yin"], "venue": "arXiv preprint arXiv:1407.5210,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["J. Eckstein", "D. Bertsekas"], "venue": "Mathematical Programming, 55(1- 3):293\u2013318,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "The Annals of statistics, 32(2):407\u2013499,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 2790\u20132797. IEEE,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.- J. Lin"], "venue": "Journal of machine learning research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Generalized alternating direction method of multipliers: new theoretical insights and applications", "author": ["E.X. Fang", "B. He", "H. Liu", "X. Yuan"], "venue": "Mathematical Programming Computation, 7(2):149\u2013187,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "On the Barzilai-Borwein method", "author": ["R. Fletcher"], "venue": "Optimization and control with applications, pages 235\u2013256. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers & Mathematics with Applications, 2(1):17\u201340,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1976}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 23(6):643\u2013660,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Optimal parameter selection for the alternating direction method of multipliers: quadratic problems", "author": ["E. Ghadimi", "A. Teixeira", "I. Shames", "M. Johansson"], "venue": "IEEE Trans. Autom. Control, 60:644\u2013658,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Linear convergence and metric selection in Douglas-Rachford splitting and ADMM", "author": ["P. Giselsson", "S. Boyd"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Sur l\u2019approximation, par \u00e9l\u00e9ments finis d\u2019ordre un, et la r\u00e9solution, par p\u00e9nalisationdualit\u00e9 d\u2019une classe de probl\u00e9mes de Dirichlet non lin\u00e9aires", "author": ["R. Glowinski", "A. Marroco"], "venue": "ESAIM: Mod\u00e9lisation Math\u00e9matique et Analyse Num\u00e9rique, 9:41\u201376,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1975}, {"title": "Fast alternating linearization methods for minimizing the sum of two convex functions", "author": ["D. Goldfarb", "S. Ma", "K. Scheinberg"], "venue": "Mathematical Programming, 141(1-2):349\u2013382,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust low-rank tensor recovery: Models and algorithms", "author": ["D. Goldfarb", "Z. Qin"], "venue": "SIAM Journal on Matrix Analysis and Applications, 35(1):225\u2013253,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Shapefit and shapekick for robust, scalable structure from motion", "author": ["T. Goldstein", "P. Hand", "C. Lee", "V. Voroninski", "S. Soatto"], "venue": "European Conference on Computer Vision, pages 289\u2013304. Springer,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive primal-dual splitting methods for statistical learning and image processing", "author": ["T. Goldstein", "M. Li", "X. Yuan"], "venue": "Advances in Neural Information Processing Systems, pages 2080\u20132088,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast alternating direction optimization methods", "author": ["T. Goldstein", "B. O\u2019Donoghue", "S. Setzer", "R. Baraniuk"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Unwrapping ADMM: efficient distributed computing via transpose reduction", "author": ["T. Goldstein", "G. Taylor", "K. Barabin", "K. Sayre"], "venue": "AISTATS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale image classification with trace-norm regularization", "author": ["Z. Harchaoui", "M. Douze", "M. Paulin", "M. Dudik", "J. Malick"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3386\u20133393. IEEE,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Alternating direction method with self-adaptive penalty parameters for monotone variational inequalities", "author": ["B. He", "H. Yang", "S. Wang"], "venue": "Jour. Optim. Theory and Appl., 106(2):337\u2013 356,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "On the o(1/n) convergence rate of the Douglas-Rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM Journal on Numerical Analysis, 50(2):700\u2013709,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "On non-ergodic convergence rate of Douglas-Rachford alternating direction method of multipliers", "author": ["B. He", "X. Yuan"], "venue": "Numerische Mathematik, 130:567\u2013577,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerated alternating direction method of multipliers", "author": ["M. Kadkhodaie", "K. Christakopoulou", "M. Sanjabi", "A. Banerjee"], "venue": "ACM SIGKDD, pages 497\u2013506,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient L1 regularized logistic regression", "author": ["S.-I. Lee", "H. Lee", "P. Abbeel", "A. Ng"], "venue": "AAAI, volume 21, page 401,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain generalization and adaptation using low rank exemplar svms", "author": ["W. Li", "Z. Xu", "D. Xu", "D. Dai", "L.V. Gool"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Linearized alternating direction method with adaptive penalty for low-rank representation", "author": ["Z. Lin", "R. Liu", "Z. Su"], "venue": "NIPS, pages 612\u2013620,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale sparse logistic regression", "author": ["J. Liu", "J. Chen", "J. Ye"], "venue": "ACM SIGKDD, pages 547\u2013556,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning", "author": ["R. Liu", "Z. Lin", "Z. Su"], "venue": "ACML, pages 116\u2013132,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Tensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization", "author": ["C. Lu", "J. Feng", "Y. Chen", "W. Liu", "Z. Lin", "S. Yan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Sparse modeling for image and vision processing", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "Foundations and Trends R  \u00a9 in Computer Graphics and Vision, 8(2-3):85\u2013283,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Informative feature selection for object recognition via sparse PCA", "author": ["N. Naikal", "A.Y. Yang", "S.S. Sastry"], "venue": "2011 International Conference on Computer Vision, pages 818\u2013 825. IEEE,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "A general analysis of the convergence of ADMM", "author": ["R. Nishihara", "L. Lessard", "B. Recht", "A. Packard", "M. Jordan"], "venue": "ICML,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust camera location estimation by convex programming", "author": ["O. Ozyesil", "A. Singer"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2674\u20132683,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Alternating direction method of multipliers for strictly convex quadratic programs: Optimal parameter selection", "author": ["A. Raghunathan", "S. Di Cairano"], "venue": "American Control Conf., pages 4324\u20134329,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex Analysis", "author": ["R. Rockafellar"], "venue": "Princeton University Press,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1970}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L.I. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D: Nonlinear Phenomena, 60(1):259\u2013268,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1992}, {"title": "Fast optimization methods for l1 regularization: A comparative study and two new approaches", "author": ["M. Schmidt", "G. Fung", "R. Rosales"], "venue": "ECML, pages 286\u2013297. Springer,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast ADMM algorithm for distributed optimization with adaptive penalty", "author": ["C. Song", "S. Yoon", "V. Pavlovic"], "venue": "arXiv preprint arXiv:1506.08928,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Training neural networks without gradients: A scalable ADMM approach", "author": ["G. Taylor", "R. Burmeister", "Z. Xu", "B. Singh", "A. Patel", "T. Goldstein"], "venue": "ICML,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster alternating direction method of multipliers with a worst-case o (1/n) convergence rate. 2016", "author": ["W. Tian", "X. Yuan"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization", "author": ["J. Wright", "A. Ganesh", "S. Rao", "Y. Peng", "Y. Ma"], "venue": "Advances in neural information processing systems, pages 2080\u20132088,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, 31:210\u2013227,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse reconstruction by separable approximation", "author": ["S. Wright", "R. Nowak", "M. Figueiredo"], "venue": "IEEE Trans. Signal Processing, 57:2479\u20132493,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "An empirical study of ADMM for nonconvex problems", "author": ["Z. Xu", "S. De", "M.A.T. Figueiredo", "C. Studer", "T. Goldstein"], "venue": "NIPS workshop on nonconvex optimization,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive ADMM with spectral penalty parameter selection", "author": ["Z. Xu", "M.A. Figueiredo", "T. Goldstein"], "venue": "AISTATS,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2017}, {"title": "Non-negative factorization of the occurrence tensor from financial contracts", "author": ["Z. Xu", "F. Huang", "L. Raschid", "T. Goldstein"], "venue": "NIPS workshop on tensor methods,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting low-rank structure for discriminative sub-categorization", "author": ["Z. Xu", "X. Li", "K. Yang", "T. Goldstein"], "venue": "BMVC, Swansea, UK, September 7-10, 2015,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1794\u20131801. IEEE,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Linearized augmented lagrangian and alternating direction methods for nuclear norm minimization", "author": ["J. Yang", "X. Yuan"], "venue": "Mathematics of Computation, 82(281):301\u2013329,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient training of very deep neural networks for supervised hashing", "author": ["Z. Zhang", "Y. Chen", "V. Saligrama"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1487\u20131495,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2016}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301\u2013320,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 47, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 48, "endOffset": 63}, {"referenceID": 53, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 48, "endOffset": 63}, {"referenceID": 7, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 48, "endOffset": 63}, {"referenceID": 35, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 48, "endOffset": 63}, {"referenceID": 46, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 81, "endOffset": 97}, {"referenceID": 22, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 81, "endOffset": 97}, {"referenceID": 52, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 81, "endOffset": 97}, {"referenceID": 30, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 81, "endOffset": 97}, {"referenceID": 3, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 134, "endOffset": 140}, {"referenceID": 2, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 134, "endOffset": 140}, {"referenceID": 15, "context": "ADMM was first introduced in [16] and [12], and has found", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "ADMM was first introduced in [16] and [12], and has found", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "edu applications in a variety of optimization problems in machine learning, image processing, computer vision, wireless communications, and many other areas [2, 21].", "startOffset": 157, "endOffset": 164}, {"referenceID": 20, "context": "edu applications in a variety of optimization problems in machine learning, image processing, computer vision, wireless communications, and many other areas [2, 21].", "startOffset": 157, "endOffset": 164}, {"referenceID": 5, "context": "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.", "startOffset": 77, "endOffset": 92}, {"referenceID": 24, "context": "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.", "startOffset": 77, "endOffset": 92}, {"referenceID": 25, "context": "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.", "startOffset": 77, "endOffset": 92}, {"referenceID": 9, "context": "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.", "startOffset": 77, "endOffset": 92}, {"referenceID": 39, "context": "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the \u201clinearized\u201d ADMM [32, 34].", "startOffset": 105, "endOffset": 113}, {"referenceID": 13, "context": "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the \u201clinearized\u201d ADMM [32, 34].", "startOffset": 105, "endOffset": 113}, {"referenceID": 31, "context": "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the \u201clinearized\u201d ADMM [32, 34].", "startOffset": 179, "endOffset": 187}, {"referenceID": 33, "context": "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the \u201clinearized\u201d ADMM [32, 34].", "startOffset": 179, "endOffset": 187}, {"referenceID": 23, "context": "For nonrelaxed ADMM, the authors of [24] propose methods that modulate the penalty parameter so that the primal and dual residuals (i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 19, "context": "This \u201cresidual balancing\u201d approach has been generalized to work with preconditioned variants of ADMM [20] and distributed ADMM [44].", "startOffset": 101, "endOffset": 105}, {"referenceID": 43, "context": "This \u201cresidual balancing\u201d approach has been generalized to work with preconditioned variants of ADMM [20] and distributed ADMM [44].", "startOffset": 127, "endOffset": 131}, {"referenceID": 50, "context": "In [51], a spectral penalty parameter method is proposed that uses the local curvature of the objective to achieve fast convergence.", "startOffset": 3, "endOffset": 7}, {"referenceID": 47, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 53, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 7, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 46, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 22, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 35, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 52, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 30, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 6, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 113, "endOffset": 131}, {"referenceID": 56, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 113, "endOffset": 131}, {"referenceID": 42, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 113, "endOffset": 131}, {"referenceID": 8, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 113, "endOffset": 131}, {"referenceID": 32, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 113, "endOffset": 131}, {"referenceID": 41, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 154, "endOffset": 162}, {"referenceID": 20, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 154, "endOffset": 162}, {"referenceID": 1, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 57, "endOffset": 72}, {"referenceID": 20, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 57, "endOffset": 72}, {"referenceID": 50, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 57, "endOffset": 72}, {"referenceID": 49, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 57, "endOffset": 72}, {"referenceID": 55, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 129, "endOffset": 137}, {"referenceID": 44, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 129, "endOffset": 137}, {"referenceID": 17, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 160, "endOffset": 172}, {"referenceID": 34, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 160, "endOffset": 172}, {"referenceID": 51, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 160, "endOffset": 172}, {"referenceID": 18, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 196, "endOffset": 200}, {"referenceID": 24, "context": "The O(1/k) convergence rate of non-relaxed ADMM is established under mild conditions for convex problems [25, 26].", "startOffset": 105, "endOffset": 113}, {"referenceID": 25, "context": "The O(1/k) convergence rate of non-relaxed ADMM is established under mild conditions for convex problems [25, 26].", "startOffset": 105, "endOffset": 113}, {"referenceID": 16, "context": "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.", "startOffset": 44, "endOffset": 60}, {"referenceID": 20, "context": "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.", "startOffset": 44, "endOffset": 60}, {"referenceID": 26, "context": "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.", "startOffset": 44, "endOffset": 60}, {"referenceID": 45, "context": "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.", "startOffset": 44, "endOffset": 60}, {"referenceID": 9, "context": "For the general relaxed ADMM formulation, a O(1/k) convergence rate is provided under mild conditions [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "Linear convergence can be achieved with strong convexity assumptions [5, 38, 15].", "startOffset": 69, "endOffset": 80}, {"referenceID": 37, "context": "Linear convergence can be achieved with strong convexity assumptions [5, 38, 15].", "startOffset": 69, "endOffset": 80}, {"referenceID": 14, "context": "Linear convergence can be achieved with strong convexity assumptions [5, 38, 15].", "startOffset": 69, "endOffset": 80}, {"referenceID": 39, "context": "For the specific case in which the objective is quadratic, a criterion is proposed in [40, 14].", "startOffset": 86, "endOffset": 94}, {"referenceID": 13, "context": "For the specific case in which the objective is quadratic, a criterion is proposed in [40, 14].", "startOffset": 86, "endOffset": 94}, {"referenceID": 37, "context": "The authors of [38] suggest a grid search and semidefinite programming based method to determine the optimal relaxation and penalty parameters.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "Adaptive penalty methods are proposed to accelerate the practical convergence of non-relaxed ADMM [24, 51].", "startOffset": 98, "endOffset": 106}, {"referenceID": 50, "context": "Adaptive penalty methods are proposed to accelerate the practical convergence of non-relaxed ADMM [24, 51].", "startOffset": 98, "endOffset": 106}, {"referenceID": 5, "context": "For the relaxation parameter, it has been suggested in [6] that over-relaxation (\u03b3 \u2208 (1, 2)) may accelerate convergence and \u03b3 = 1.", "startOffset": 55, "endOffset": 58}, {"referenceID": 23, "context": "Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26].", "startOffset": 77, "endOffset": 89}, {"referenceID": 24, "context": "Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26].", "startOffset": 77, "endOffset": 89}, {"referenceID": 25, "context": "Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26].", "startOffset": 77, "endOffset": 89}, {"referenceID": 1, "context": "It has been observed that these residuals approach zero as the algorithm approaches a true solution [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "where tol > 0 is the stopping tolerance [2].", "startOffset": 40, "endOffset": 43}, {"referenceID": 50, "context": "Spectral stepsize selection methods for vanilla ADMM were discussed in [51].", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15].", "startOffset": 144, "endOffset": 154}, {"referenceID": 4, "context": "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15].", "startOffset": 144, "endOffset": 154}, {"referenceID": 14, "context": "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15].", "startOffset": 144, "endOffset": 154}, {"referenceID": 40, "context": "with f\u2217 denoting the Fenchel conjugate of f , defined as f\u2217(y) = supx\u3008x, y\u3009 \u2212 f(x) [41].", "startOffset": 83, "endOffset": 87}, {"referenceID": 40, "context": "where \u03b3k is a relaxation parameter, and \u2202f(x) denotes the subdifferential of f evaluated at x [41].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "Adaptive stepsize rules of the \u201cspectral\u201d type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49].", "startOffset": 143, "endOffset": 146}, {"referenceID": 10, "context": "Adaptive stepsize rules of the \u201cspectral\u201d type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49].", "startOffset": 235, "endOffset": 243}, {"referenceID": 48, "context": "Adaptive stepsize rules of the \u201cspectral\u201d type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49].", "startOffset": 235, "endOffset": 243}, {"referenceID": 50, "context": "Spectral methods were recently used to determine the penalty parameter for the non-relaxed ADMM in [51].", "startOffset": 99, "endOffset": 103}, {"referenceID": 50, "context": "Note this is the same \u201coptimal\u201d penalty parameter proposed for non-relaxed ADMM in [51].", "startOffset": 83, "endOffset": 87}, {"referenceID": 50, "context": "The estimation of \u03b1\u0302k and \u03b2\u0302k for the dual components \u0125(\u03bb\u0302k) and \u011d(\u03bbk) at the k-th iteration of primal ADMM has been described in [51].", "startOffset": 130, "endOffset": 134}, {"referenceID": 50, "context": "For a detailed derivation of these formulas, see [51].", "startOffset": 49, "endOffset": 53}, {"referenceID": 50, "context": "Rather, we adopt the correlation criterion proposed in [51] to test the validity of the local linear assumption, and only rely on the adaptive model when the assumptions are deemed valid.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28].", "startOffset": 84, "endOffset": 88}, {"referenceID": 28, "context": "We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28].", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28].", "startOffset": 144, "endOffset": 148}, {"referenceID": 6, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 56, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 29, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 42, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 32, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 20, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 56, "context": "Linear regression with EN regularization Elastic net (EN) is a modification of the `1-norm (or LASSO) regularizer that helps dealing with highly correlated variables [57, 21], and requires solving", "startOffset": 166, "endOffset": 174}, {"referenceID": 20, "context": "Linear regression with EN regularization Elastic net (EN) is a modification of the `1-norm (or LASSO) regularizer that helps dealing with highly correlated variables [57, 21], and requires solving", "startOffset": 166, "endOffset": 174}, {"referenceID": 54, "context": "ADMM has been applied to solve low rank least squares problems [55, 53]", "startOffset": 63, "endOffset": 71}, {"referenceID": 52, "context": "ADMM has been applied to solve low rank least squares problems [55, 53]", "startOffset": 63, "endOffset": 71}, {"referenceID": 52, "context": "LRLS has been used to formulate exemplar classifiers and discover visual subcategories [53].", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "where z is the SVM dual variable, Q is the kernel matrix, c is a vector of labels, e is a vector of ones, and C > 0 [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "Consensus `1-regularized logistic regression ADMM has become an important tool for solving distributed optimization problems [2].", "startOffset": 125, "endOffset": 128}, {"referenceID": 21, "context": "Unwrapped SVM The unwrapped formulation of SVM [22], which can be used in distributed computing environments via \u201ctranspose reduction\u201d tricks, applies ADMM to the primal form of SVM to solve", "startOffset": 47, "endOffset": 51}, {"referenceID": 41, "context": "Total variation image denoising (TVID) Total variation image denoising is often performed by solving [42]", "startOffset": 101, "endOffset": 105}, {"referenceID": 46, "context": "RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39].", "startOffset": 102, "endOffset": 114}, {"referenceID": 36, "context": "RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39].", "startOffset": 102, "endOffset": 114}, {"referenceID": 38, "context": "RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39].", "startOffset": 102, "endOffset": 114}, {"referenceID": 5, "context": "5 as suggested in [6].", "startOffset": 18, "endOffset": 21}, {"referenceID": 23, "context": "The parameters of RB and AADMM are selected as in [24, 2, 51].", "startOffset": 50, "endOffset": 61}, {"referenceID": 1, "context": "The parameters of RB and AADMM are selected as in [24, 2, 51].", "startOffset": 50, "endOffset": 61}, {"referenceID": 50, "context": "The parameters of RB and AADMM are selected as in [24, 2, 51].", "startOffset": 50, "endOffset": 61}, {"referenceID": 39, "context": "The initial penalty \u03c40 =1/10 and initial relaxation \u03b30 =1 are used for all problems except the canonical QP problem, where initial parameters are set to the geometric mean of the maximum and minimum eigenvalues of matrix Q, as proposed for quadratic problems in [40].", "startOffset": 262, "endOffset": 266}, {"referenceID": 23, "context": "As suggested by [24, 51], the adaptivity of RB and AADMM is stopped after 1000 iterations to guarantee convergence.", "startOffset": 16, "endOffset": 24}, {"referenceID": 50, "context": "As suggested by [24, 51], the adaptivity of RB and AADMM is stopped after 1000 iterations to guarantee convergence.", "startOffset": 16, "endOffset": 24}], "year": 2017, "abstractText": "Many modern computer vision and machine learning applications rely on solving difficult optimization problems that involve non-differentiable objective functions and constraints. The alternating direction method of multipliers (ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a generalization of ADMM that often achieves better performance, but its efficiency depends strongly on algorithm parameters that must be chosen by an expert user. We propose an adaptive method that automatically tunes the key algorithm parameters to achieve optimal performance without user oversight. Inspired by recent work on adaptivity, the proposed adaptive relaxed ADMM (ARADMM) is derived by assuming a BarzilaiBorwein style linear gradient. A detailed convergence analysis of ARADMM is provided, and numerical results on several applications demonstrate fast practical convergence.", "creator": "LaTeX with hyperref package"}}}