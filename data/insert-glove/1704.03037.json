{"id": "1704.03037", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Learning from Multi-View Structural Data via Structural Factorization Machines", "abstract": "Real - aparecen world lewys relations 454th among postelection entities can often phonetic be selectivity observed lezar and determined by different sidot perspectives / manet views. For example, the decision speculators made bhaskarudu by meals a user on whether to adopt an item relies ferula on d\u00fcnya multiple yakutiya aspects neuhoff such as 371 the 6:15 contextual vittori information of the bishopaccountability.org decision, the item ' s pousse attributes, federalsburg the user ' s profile gulati and saiid the 310-785-0613 reviews given slacks by winschoten other users. companionship Different 10-person views may cordingly exhibit placita multi - pont-aven way interactions mawddach among elanor entities and 120.35 provide waterton complementary baginton information. l\u00fcders In this paper, we timex introduce a milli multi - tensor - based udr approach that mcburnie can preserve allons the underlying structure infection of planigale multi - view cheshunt data caprera in stani\u0161i\u0107 a rollercoasters generic predictive model. Specifically, graziano we pancho propose structural mingas factorization nacelle machines (wishram SFMs) driftin that pm2 learn 40.74 the common terrapass latent spaces qurna-2 shared particularities by 444th multi - 84.14 view mistubishi tensors and automatically yingfan adjust the neurone importance 1966/1967 of 997,000 each slammy view donahoe in the jassy predictive najafabad model. smuggled Furthermore, cfcda the crestar complexity bigness of latke SFMs benami is moutier linear romain in comrades the number of parameters, which yorio make mazula SFMs 150-minute suitable 1985-1995 to large - quarterline scale shpilband problems. biedenharn Extensive experiments on real - nettlebed world datasets demonstrate tyurin that oermann the gaggero proposed broadsided SFMs outperform several battye state - andreou of - the - milles art methods concerti in terms parodied of prediction chmura accuracy riester and computational gelt cost.", "histories": [["v1", "Mon, 10 Apr 2017 19:52:29 GMT  (2674kb)", "http://arxiv.org/abs/1704.03037v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chun-ta lu", "lifang he", "hao ding", "philip s yu"], "accepted": false, "id": "1704.03037"}, "pdf": {"name": "1704.03037.pdf", "metadata": {"source": "META", "title": "Learning from Multi-View Structural Data via Structural Factorization Machines", "authors": ["Chun-Ta Lu", "Lifang He", "Hao Ding", "Philip S. Yu"], "emails": ["clu29@uic.eud", "lifanghescut@gmail.com", "haoding.tourist@gmail.com", "psyu@cs.uic.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n03 03\n7v 1\n[ cs\n.L G\n] 1\n0 A\npr 2\nReal-world relations among entities can o en be observed and determined by di erent perspectives/views. For example, the decision made by a user on whether to adopt an item relies on multiple aspects such as the contextual information of the decision, the item\u2019s a ributes, the user\u2019s pro le and the reviews given by other users. Di erent views may exhibit multi-way interactions among entities and provide complementary information. In this paper, we introduce a multi-tensor-based approach that can preserve the underlying structure of multi-view data in a generic predictive model. Speci cally, we propose structural factorization machines (SFMs) that learn the common latent spaces shared by multi-view tensors and automatically adjust the importance of each view in the predictive model. Furthermore, the complexity of SFMs is linear in the number of parameters, which make SFMs suitable to large-scale problems. Extensive experiments on real-world datasets demonstrate that the proposed SFMs outperform several state-of-the-art methods in terms of prediction accuracy and computational cost.\nCCS CONCEPTS\n\u2022Information systems\u2192Data mining; \u2022Computing methodologies \u2192Machine learning;\nKEYWORDS\nTensor Factorization; Structural Data; Multi-View Learning\nACM Reference format: Chun-Ta Lu, Lifang He, Hao Ding, and Philip S. Yu. 2017. Learning from Multi-View Structural Data via Structural Factorization Machines. In Proceedings ofACMConference, Washington, DC, USA, July 2017 (Conference\u201917), 9 pages. DOI: 10.1145/nnnnnnn.nnnnnnn\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. Conference\u201917, Washington, DC, USA \u00a9 2017 ACM. 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 DOI: 10.1145/nnnnnnn.nnnnnnn"}, {"heading": "1 INTRODUCTION", "text": "With the ability to access massive amounts of heterogeneous data from multiple sources, multi-view data have become prevalent in many real-world applications. For instance, in recommender systems, online review sites (like Amazon and Yelp) have access to contextual information of shopping histories of users, the reviews wri en by the users, the categorizations of the items, as well as the friends of the users. Each view may exhibit pairwise interactions (e.g., the friendships between users) or even higher-order interactions (e.g., a customer write a review for a product) among entities (such as customers, products, and reviews), and can be represented in a multi-way data structure, i.e., tensor. Since di erent views usually provide complementary information [4, 5, 17], how to effectively incorporate information frommultiple structural views is critical to good prediction performance for various machine learning tasks.\nTypically, a predictivemodel is de ned as a function of predictor variables (e.g., the customer id, the product id, and the categories of the product) to some target (e.g., the rating). e most common approach in predictive modeling for multi-view structural data is to describe samples with feature vectors that are a ened and concatenated from structural views, and apply a machine learning method, such as linear regression (LR) and support vectormachines (SVMs), to learn the target function from observed samples. Recent works have shown that linear models fail for tasks with very sparse data [21, 22]. A variety of methods have been proposed to address the data sparsity issue by factorizing the monomials (or feature interactions) with kernel tricks, such as the ANOVA kernels used in FMs [2, 21] and polynominal kernels used in polynominal networks [3, 16]. However, the disadvantages of this approach are that (1) the important structural information of each view will be discarded which may lead to the degraded prediction performance and (2) the feature vectors can grow very large which can make learning and prediction very slow or even infeasible, especially if each view involves relations of high cardinality. For example, including the relation \u201cfriends of a user\u201d in the feature vector (represented by their IDs) can result in a very long feature vector. Further, it will repeatedly appear in many samples that involve the given user.\nMatrix/tensor factorization models have been a topic of interest in the areas of structural data analysis, such as community detection [10], collaborative ltering [13, 24], and neuroimage analysis [9]. Assuming multi-view data have the same underlying lowrank structure (at least in one mode), coupled data analysis such as collective matrix factorization (CMF) [25] and coupled matrix and tensor factorization (CMTF) [1] that jointly factorize multiple matrices (or tensors) has been applied to applications such as clustering and missing data recovery. However, since existing coupled factorizationmodels are unsupervised, the importance of each structural view in modeling the target value cannot be automatically learned. Furthermore, when applying these models to data with rich meta information (e.g., friendships) but extremely sparse target values (e.g., ratings), it is very likely the learning process will be dominated by the meta information without manual tuning some hyperparameters, e.g., the weights of the ing error of each matrix/tensor in the objective function [25], the weights of di erent types of latent factors in the predictive models [14], or the regularization hyperparamters of latent factor alignment [18].\nIn this paper, we propose a general and exible framework for learning the predictive structure from the complex relationships within the multi-view structural data. Each view of an instance in this framework is represented by a tensor that describes the multi-way interactions of subsets of entities, and di erent views have some entities in common. Constructing the tensors for each instance may not be realistic for real-world applications in terms of space and computational complexity, and the model parameters can have exponential growth and tend to be over ing. In order to preserve the structural information of multi-view data without physically constructing the tensors, we introduce structural factorization machines (SFMs) that can learn the consistent representations in the latent feature spaces shared in the multi-view tensors while automatically adjust the contribution of each view in the predictive model. Furthermore, we provide an e cient method to avoid redundant computing on repeating pa erns stemming from the relational structure of the data, such that SFMs can make the same predictions but with largely speed up computation.\ne contributions of this paper are summarized as follows:\n\u2022 We introduce a novel multi-tensor framework for mining\ndata from heterogeneous domains, which can explore the high order correlations underlying multi-view structural data in a generic predictive model. \u2022 We develop structural factorization machines (SFMs) tai-\nlored for learning the common latent spaces shared inmultiview tensors and automatically adjusting the importance of each view in the predictive model. e complexity of SFMs is linear in the number of features, which makes SFMs suitable to large-scale problems.\n\u2022 Extensive experiments on eight real-world datasets are per-\nformed along with comparisons to existing state-of-theart factorization models to demonstrate its advantages.\ne rest of this paper is organized as follows. We introduce the problem de nition and some preliminary works in Section 2. We then propose the framework for learning multi-view structural data, and develop the structural factorizationmachines (SFMs), and\nprovide an e cient computing method in Section 3. e experimental results and parameter analysis are reported in Section 4. Section 5 concludes this paper."}, {"heading": "2 PRELIMINARIES", "text": "In this section, we begin with a brief introduction to some related concepts and notation in tensor algebra, and then proceed to formulate the problem we are concerned with multi-view learning."}, {"heading": "2.1 Tensor Basics and Notation", "text": "Tensor is a mathematical representation of a multi-way array. e order of a tensor is the number of modes (or ways). A zero-order tensor is a scalar, a rst-order tensor is a vector, a second-order tensor is a matrix and a tensor of order three or higher is called a higher-order tensor. To facilitate the distinction between scalars, vectors, matrices and higher-order tensors, the type of a quantity will be re ected by its representation: scalars are denoted by lowercase le ers, vectors by boldfaced lowercase le ers, matrices by boldface uppercase le ers, and tensors are denoted by calligraphic le ers. An element of a vector x, a matrix X, or a tensor X is denoted by xi , xi, j , xi, j,k , etc., depending on the number of modes. All vectors are column vectors unless otherwise speci ed. For an arbitrary matrix X \u2208 RI\u00d7J , its i-th row and j-th column vector are denoted by xi and xj , respectively. Given two matrices X,Y \u2208 RI\u00d7J , X \u2217 Y denotes the element-wise (Hadamard) product between X and Y, de ned as the matrix in RI\u00d7J . For each integer M > 1, [1 : M] denotes all integers from 1 to M . An overview of the basic symbols used in this paper can be found in Table 1.\nDe nition 2.1 (Inner product). e inner product of two samesized tensors X,Y \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IM is de ned as the sum of the products of their entries:\n\u3008X,Y\u3009 = I1\u2211 i1=1 I2\u2211 i2=1 \u00b7 \u00b7 \u00b7 IM\u2211 iM=1 xi1,i2, ...,iMyi1,i2, ...,iM . (1)\nDe nition 2.2 (Outer product). e outer product of two tensors\nX \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN and Y \u2208 RI \u2032 1\u00d7I \u2032 2\u00d7\u00b7\u00b7\u00b7\u00d7I \u2032 M is a (N + M)th-order tensor denoted by X \u25e6 Y, and the elements are de ned by\n(X \u25e6 Y)i1,i2, ...,iN ,i \u20321,i \u2032 2, ...,i \u2032 M = xi1,i2, \u00b7 \u00b7 \u00b7 ,iNyi \u20321,i \u2032 2, \u00b7 \u00b7 \u00b7 ,i \u2032 M (2)\nfor all values of the indices.\nNotice that for rank-one tensors X = x(1) \u25e6 x(2) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 x(M) and\nY = y(1) \u25e6 y(2) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 y(M), it holds that\n\u3008X,Y\u3009 = \u2329 x(1), y(1) \u232a \u2329 x(2), y(2) \u232a \u00b7 \u00b7 \u00b7 \u2329 x(M), y(M) \u232a . (3)\nDe nition 2.3 (CP factorization [12]). Given a tensorX \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IM\nand an integer R, the CP factorization is de ned by factor matrices X(m) \u2208 RIm\u00d7R form \u2208 [1 : M], respectively, such that\nX = R\u2211 r=1 x (1) r \u25e6 x (2) r \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 x (M) r = JX (1),X(2), \u00b7 \u00b7 \u00b7 ,X(M)K , (4)\nwhere x (m) r \u2208 R Im is the r -th column of the factor matrixX(m), and J\u00b7K is used for shorthand notation of the sum of rank-one tensors."}, {"heading": "2.2 Problem Formulation", "text": "Our problem is di erent from conventional multi-view learning approaches where multiple views of data are assumed independent and disjoint, and each view is described by a vector. We formulate the multi-view learning problem using coupled analysis of multiview features in the form of multiple tensors.\nSuppose that the problem includes V views where each view consists of a collection of subsets of entities (such as person, company, location, product) and di erent views have some entities in common. We denote a view as a tuple (x(1), x(2), \u00b7 \u00b7 \u00b7 , x(M)),M \u2265 2, where x(m) \u2208 RIm is a feature vector associated with the entitym. Inspired by [5], we construct tensor representation for each view over its entities by\nX\u0303 = x\u0303(1) \u25e6 x\u0303(2) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 x\u0303(M) \u2208 R(1+I1)\u00d7\u00b7\u00b7\u00b7\u00d7(1+IM ),\nwhere x\u0303(m) = [1;x(m)] \u2208 R1+Im and \u25e6 is the outer product operator. In this manner, the full-order interactions 1 between entities are embeddedwithin the tensor structure, which not only provides a uni ed and compact representation for each view, but also facilitate e cient design methods. Fig. 1 shows an example of two structural views, where the rst view consists of the full-order interactions among the rst three modes (e.g., review text, item ID, and user ID), and the second view consists of the full-order interactions among the last two modes (e.g., user ID and friend IDs).\n1Full-order interactions range from the rst-order interactions (i.e., contributions of single entity features) to the highest-order interactions (i.e., contributions of the outer product of features from all entities).\nA er generating the tensor representation for each view, we de ne the multi-view learning problem as follows. Given a train-\ning set D = {({ X\u0303 (1) n , X\u0303 (2) n , \u00b7 \u00b7 \u00b7 , X\u0303 (V ) n } , yn ) | n \u2208 [1 : N ] } , where X\u0303 (v) n \u2208 R (1+I1)\u00d7\u00b7\u00b7\u00b7\u00d7(1+IMv ) is the tensor representation in the vth view for the n-th instance, yn is the response of the n-th instance, Mv is the number of the constitutive modes in the v-th view, and N is the number of labeled instances. We assume different views have common entities, thus the resulting tensors will share common modes, e.g., the third mode in Fig 1. As we are concerned with predicting unknown values of multiple coupled tensors, our goal is to leverage the relational information from all the views to help predict the unlabeled instances, as well as to use the complementary among di erent views to improve the performance. Speci cally, we are interested in nding a predictive function f : X(1) \u00d7 X(2) \u00b7 \u00b7 \u00b7 \u00d7 X(V ) \u2192 Y that minimizes the expected loss, where X(v),v \u2208 [1 : V ] is the input space in thev-th view and Y is the output space."}, {"heading": "3 METHODOLOGY", "text": "In this section, we rst discuss how to design the predictivemodels for learning from multiple coupled tensors. We then derive structural factorization machines (SFMs) that can learn the common latent spaces shared inmulti-view coupled tensors and automatically adjust the importance of each view in the predictive model."}, {"heading": "3.1 Predictive Models", "text": "Without loss of generality, we take two views as an example to introduce our basic design of the predictive models. Speci cally, we consider coupled analysis of a third-order tensor and a matrix with\none mode in common. Given an input instance ({ X\u0303(1), X\u0303(2) } , y ) , where X\u0303(1) = x\u0303(1) \u25e6 x\u0303(2) \u25e6 x\u0303(3) \u2208 R(1+I )\u00d7(1+J )\u00d7(1+K ) and X\u0303(2) = x\u0303(3) \u25e6 x\u0303(4) \u2208 R(1+K )\u00d7(1+L). An intuitive solution is to build the following multiple linear model:\nf ({ X\u0303(1), X\u0303(2) }) = \u2329 W\u0303(1), X\u0303(1) \u232a + \u2329 W\u0303(2), X\u0303(2) \u232a (5)\nwhere W\u0303(1) \u2208 R(1+I )\u00d7(1+J )\u00d7(1+K ) and W\u0303(2) \u2208 R(1+K )\u00d7(1+L) are the weights for each view to be learned.\nHowever, in this case it does not take into account the relations and di erences between two views. In order to incorporate the relations between two views and also discriminate the importance\nof each view, we introduce an indicator vector ev \u2208 R V for each view v as\nev = [0, \u00b7 \u00b7 \u00b7 , 0\ufe38 \ufe37\ufe37 \ufe38 v-1 , 1, 0, \u00b7 \u00b7 \u00b7 , 0]T,\nand transform the predictive model in Eq. (5) into\nf ({ X\u0303(1), X\u0303(2) }) = \u2329 W\u0302(1), X\u0303(1) \u25e6 e1 \u232a + \u2329 W\u0302(2), X\u0303(2) \u25e6 e2 \u232a , (6)\nwhere W\u0302(1) \u2208 R(1+I )\u00d7(1+J )\u00d7(1+K )\u00d72 and W\u0302(2) \u2208 R(1+K )\u00d7(1+L)\u00d72.\nDirectly learning the weight tensors W\u0302s leads to two drawbacks. First, the weight parameters are learned independently for di erent modes and di erent views. When the feature interactions rarely (or even never) appear during training, it is unlikely to learn the associated parameters appropriately. Second, the number of parameters in Eq. (6) is exponential to the number of features, which can make the model prone to over ing and ine ective on sparse data. Here, we assume that each weight tensor has a low-rank approximation, and W\u0302(1) and W\u0302(2) can be decomposed by CP factorization as\nW\u0302(1) = J\u0398\u0302(1,1), \u0398\u0302(1,2), \u0398\u0302(1,3),\u03a6K\n= J[b(1,1);\u0398(1)], [b(1,2);\u0398(2)], [b(1,3);\u0398(3)],\u03a6K,\nand\nW\u0302(2) = J\u0398\u0302(2,3), \u0398\u0302(2,4),\u03a6K = J[b(2,3);\u0398(3)], [b(2,4);\u0398(4)],\u03a6K,\nwhere \u0398(m) \u2208 RIm\u00d7R is the factor matrix for the features in the m-th mode. It is worth noting that \u0398(3) is shared in the two views. \u03a6 \u2208 R2\u00d7R is the factor matrix for the view indicator, and b(v,m) \u2208 R 1\u00d7R , which is always associated with the constant one in x\u0303(m) = [1; x(m)], represents the bias factors of them-th mode in the v-th view. rough b(v,m), the lower-order interactions (the interactions excluding the features from them-th mode) in the v-th view are explored in the predictive function.\nen we can transform Eq. (6) into\u2329 W\u0302(1), X\u0303(1) \u25e6 e1 \u232a + \u2329 W\u0302(2), X\u0303(2) \u25e6 e2 \u232a\n= R\u2211 r=1 \u2329 \u03b8\u0302 (1,1) r \u25e6 \u03b8\u0302 (1,2) r \u25e6 \u03b8\u0302 (1,3) r \u25e6 \u03d5r , x\u0303 (1) \u25e6 x\u0303(2) \u25e6 x\u0303(3) \u25e6 e1 \u232a\n+ R\u2211 r=1 \u2329 \u03b8\u0302 (2,3) r \u25e6 \u03b8\u0302 (2,4) r \u25e6 \u03d5r , x\u0303 (3) \u25e6 x\u0303(4) \u25e6 e2 \u232a\n=\u03d51\n( 3\u220f\nm=1\n\u2217 ( x\u0303(m) T \u0398\u0302 (1,m) ))T +\u03d52 ( 4\u220f\nm=3\n\u2217 ( x\u0303(m) T \u0398\u0302 (2,m)\n))T\n=\u03d51\n( 3\u220f\nm=1\n\u2217 ( x(m) T \u0398 (m) + b(1,m) ))T +\u03d52 ( 4\u220f\nm=3\n\u2217 ( x(m) T \u0398 (m) + b(2,m)\n))T\n(7)\nwhere \u2217 is the Hadamard (elementwise) product and \u03d5v \u2208 R1\u00d7R is the v-th row of the factor matrix \u03a6.\nFor convenience, we let h(m) = \u0398(m) T x(m), SM (v) denote the\nset of modes in thev-th views, \u03c0 (v) = \u220f m\u2208SM (v) \u2217 ( h(m) + b(v,m) T ) ,\nand \u03c0 (v,\u2212m) = \u220f m\u2032\u2208SM (v),m\u2032,m \u2217 ( h(m \u2032) + b(v,m \u2032)T ) . It is worth noting that h(m) can be regarded as the latent representation of the feature x(m) in m-th mode, and \u03c0 (v) can be regarded as the joint representation of all the modes in the v-th view, which can be easily computed through the Hadamard product. Moreover, the contribution of the latent factors in \u03c0 (v) is automatically adjusted by the weight vector\u03d5v . An graphical illustration of the computational procedure in Eq. (7) is shown in Fig. 2.\ne predictive model for the general cases is given as follows\nf ({X\u0303(v)}) = V\u2211 v=1 \u2329 W\u0302(v), X\u0303(v) \u25e6 ev \u232a\n= V\u2211 v=1 \u03d5v \u220f\nm\u2208SM (v)\n\u2217 ( x(m) T \u0398 (m) + b(v,m) )T\n= V\u2211 v=1 \u03d5v \u220f\nm\u2208SM (v)\n\u2217 ( h(m) + b(v,m) T )\n(8)\nWe name this model as structural factorization machines (SFMs). Clearly, the parameters are jointly factorized, which bene ts parameter estimation under sparsity since dependencies exist when\nthe interactions share the same features. erefore, the model parameters can be e ectively learned without direct observations of such interactions especially in highly sparse data. More importantly, a er factorizing the weight tensor W\u0302s, there is no need to construct the input tensor physically. Furthermore, the model complexity is linear in the number of original features. In partic-\nular, the model complexity is O(R(V + I + \u2211 v Mv )), where Mv is the number of modes in the v-th view."}, {"heading": "3.2 Learning Structural Factorization Machines", "text": "Following the traditional supervised learning framework, we propose to learn the model parameters by minimizing the following regularized empirical risk:\nR = 1\nN N\u2211 n=1 \u2113 ( f ({X (v) n }),yn ) + \u03bb\u2126(\u03a6, {\u0398(m)}, {b(v,m)}) (9)\nwhere \u2113 is a prescribed loss function, \u2126 is the regularizer encoding the prior knowledge of {\u0398(m)} and \u03a6, and \u03bb > 0 is the regularization parameter that controls the trade-o between the empirical loss and the prior knowledge.\ne partial derivative of R w.r.t. \u0398(m) is given by\n\u2202R\n\u2202\u0398(m) =\n\u2202L\n\u2202 f\n\u2202 f\n\u2202\u0398(m) + \u03bb\n\u2202\u2126\u03bb(\u0398 (m))\n\u2202\u0398(m) (10)\nwhere \u2202L \u2202f = 1 N [ \u2202\u21131 \u2202f , \u00b7 \u00b7 \u00b7 , \u2202\u2113N \u2202f ]T \u2208 RN .\nFor convenience, we let SV (m) denote the set of views that con-\ntains them-thmode,X(m) = [x (m) 1 , \u00b7 \u00b7 \u00b7 , x (m) N ],\u03a0(v) = [\u03c0 (v) 1 , \u00b7 \u00b7 \u00b7 ,\u03c0 (v) N ]T and \u03a0(v,\u2212m) = [\u03c0 (v,\u2212m) 1 , \u00b7 \u00b7 \u00b7 ,\u03c0 (v,\u2212m) N ]T. We then have that\n\u2202L\n\u2202 f\n\u2202 f\n\u2202\u0398(m) = X(m) \u00a9\u00ab \u2211 v \u2208SV (m) (( \u2202L \u2202 f \u03d5v ) \u2217 \u03a0(v,\u2212m) )\u00aa\u00ae \u00ac\n(11)\nSimilarly, the partial derivative of R w.r.t. b(v,m) is given by\n\u2202R\n\u2202b(v,m) =\n\u2202L\n\u2202 f\n\u2202 f\n\u2202b(v,m) + \u03bb\n\u2202\u2126\u03bb(b (v,m))\n\u2202b(v,m) = 1T (( \u2202L\n\u2202 f \u03d5v\n) \u2217 \u03a0(v,\u2212m) ) + \u03bb \u2202\u2126\u03bb(b (v,m))\n\u2202b(v,m) (12)\ne partial derivative of R w.r.t. \u03a6 is given by\n\u2202R \u2202\u03a6 = [ ( \u2202L \u2202f )T \u03a0 (1) ; \u00b7 \u00b7 \u00b7 ; ( \u2202L \u2202f )T \u03a0 (V ) ] + \u03bb \u2202\u2126\u03bb(\u03a6) \u2202\u03a6 (13)\nFinally, the gradient of R can be formed by vectorizing the partial derivatives with respect to each factor matrix and concatenating them all, i.e.,\n\u2207R =  vec( \u2202R \u2202\u0398(1) ) . . . vec( \u2202R \u2202\u0398(M ) ) vec( \u2202R \u2202b(1,1) ) . . . vec( \u2202R \u2202b(V ,M ) ) vec( \u2202R \u2202\u03a6 ) \n(14)\nOnce we have the function, R and gradient, \u2207R , values, we can use any gradient-based optimization algorithm to compute the factor matrices. For the results presented in this paper, we use the Adam optimization algorithm [11] for parameter updates."}, {"heading": "3.3 E cient Computing with Relational Structures", "text": "In relational domains, we can o en observe that feature vectors of the same entity repeatedly appear in the plain forma ed feature matrixX, whereX = [X(1); \u00b7 \u00b7 \u00b7 ;X(M)] \u2208 RI\u00d7N andX(m) \u2208 RIm\u00d7N is the feature matrix in the m-th mode. Consider Fig. 3(a) as an example, where the parts highlighted in yellow in the forth mode (which represents the friends of the user) are repeatedly appear in the rst three columns. Clearly, these repeating pa erns stem from the relational structure of the same entity.\nIn the following, we show how the proposed SFM method can make use of relational structure of each mode, such that the learning and prediction can be scaled to predictor variables generated from relational data involving relations of high cardinality. We adopt the idea from [23] to avoid redundant computing on repeating pa erns over a set of feature vectors.\nLet B = {(XB (m) ,\u03c8 B (m)\n)}Mm=1 be the set of relational structures,\nwhere XB (m) \u2208 RIm\u00d7Nm denotes the relational matrix of m-th mode, \u03c8 B (m)\n: {1, \u00b7 \u00b7 \u00b7 ,N } \u2192 {1, \u00b7 \u00b7 \u00b7 ,Nm} denotes the mapping\nfrom columns in the feature matrix X to columns within XB (m) . To shorten notation, the index B is dropped from the mapping\u03c8 B whenever it is clear which block the mapping belongs to. From B, one can reconstructX by concatenating the corresponding columns\nof the relational matrices using themappings. For instance, the feature vector xn of then-th case in the plain featurematrixX is represented as xn = [x (1) \u03c8 (n) ; \u00b7 \u00b7 \u00b7 ; x (M) \u03c8 (n) ]. Fig. 3(b) shows an example how the feature matrix can be represented in relational structures. Let Nz (A) denote the number of non-zeros in a matrix A. e space required for using relational structures to represent the input data\nis |B| = NM + \u2211 m Nz (X B(m) ), which is much smaller than Nz (X) if there are repeating pa erns in the feature matrix X.\nNow we can rewrite the predictive model in Eq. (8) as follows\nf ({X (v) n } = V\u2211 v=1 \u03d5v \u220f\nm\u2208SM (v)\n\u2217 ( hB (m)\n\u03c8 (n) + b(v,m)\nT ) , (15)\nwith the caches HB (m) = [hB (m) 1 , \u00b7 \u00b7 \u00b7 , h B(m) Nm ] for each mode, where hB (m)\nj = \u0398 (m)TxB (m) j , \u2200j \u2208 [1 : Nm].\nis directly shows how N samples can be e ciently predicted:\n(i) compute HB (m) in O(RNz (X B(m) )) for each mode, (ii) compute N predictions with Eq. (15) using caches in O(RN (V + \u2211 v Mv )). With the help of relational structures, SFMs can learn the same parameters and make the same predictions but with a much lower runtime complexity."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 Datasets", "text": "To evaluate the ability and applicability of the proposed SFMs, we include a spectrum of large datasets from di erent domains. e statistics for each dataset is summarized in Table 2, the schema of the structural views in each dataset is presented in Fig. 4, and the details are as follows:\nAmazon2: e rst group of datasets are from Amazon.com recently introduced by [20]. is is among the largest datasets available that includes review texts and metadata of items. Each toplevel category of products on Amazon.com has been constructed as an independent dataset in [20]. In this paper, we take a variety of large categories as listed in Tabel 2.\nEach sample in these datasets has ve modes, i.e., users, items, review texts, categories, and linkage. e user mode and item mode are represented by one-hot encoding. e \u21132-normalized TFIDF vector representation of review text 3 of the item given by the user is used as the text mode. e categorymode and linkage mode consists of all the categories and all the co-purchasing items of the item, which might be from other categories. e last two modes are \u21131-normalized.\n2h p://jmcauley.ucsd.edu/data/amazon/ 3Stemming, lemmatization, removing stop-words and words with frequency less than 100 times, etc., are handled beforehand.\nYelp4: It is a large-scale dataset consisting of venue reviews. Each sample in this dataset contains ve modes, i.e., users, venues, friends, categories and cities. e user mode and venue mode are represented by one-hot encoding. e friend mode consists of the friends\u2019 ids of users. e category mode and city mode consists of all the categories and the city of the venue. e last three modes are \u21131-normalized.\nBookCrossing (BX)5: It is a book review dataset collected from the Book-Crossing community. Each sample in this dataset contains ve modes, i.e., users, books, countries, ages and authors.\ne ages are split in eight bins as in [7]. e country mode and age mode consist of the corresponding meta information of the user. e author modes represents the authors of the book. All the modes are represented by one-hot encoding.\ne values of samples range within [1:5] in Amazon and Yelp\ndatasets, and range within [1:10] in BX dataset."}, {"heading": "4.2 Comparison Methods", "text": "In order to demonstrate the e ectiveness of the proposed SFMs, we compare a series of state-of-the-art methods. Matrix Factorization (MF) is used to validate that meta information is helpful for improving prediction performance. We use the LIBMF implementation [6] for comparison in the experiment. Factorization Machine (FM) [22] is the state-of-the-art method in recommender systems. We compare with its higher-order extension [2] with up to second-order, and third-order feature interactions, and denote them as FM-2 and FM-3. PolynomialNetwork (PolyNet) [16] is a recently proposedmethod that utilizes polynomial kernel on all features. We compare the augmented PolyNet (which adds a constant one to the feature vector [3]) with up to the second-order, and third-order kernel and denote them as PolyNet-2 and PolyNet-3. Multi-View Machine (MVM) [5] is a tensor factorization based method that explores the latent representation embedded in the full-order interactions among all the modes. Structural FactorizationMachine (SFM) is the proposed model that learns the common latent spaces shared in structural data."}, {"heading": "4.3 Experimental Settings", "text": "For each dataset, we randomly split 50%, 10%, and 40% of labeled samples as training set, validation set, and testing set, respectively. Validation sets are used for hyper-parameter tuning for eachmodel. Each of the validation and testing sets does not overlap with any other set so as to ensure the sanity of the experiment. For simplicity and fair comparison, in all the comparison methods, the dimension of latent factors R = 20 and the maximum number of epochs is set as 400, and Forbenius norm regularizers are used to avoid over ing. e regularization hyper-parameter is tuned from {10\u22125, 10\u22124, \u00b7 \u00b7 \u00b7 , 100}.\nAll the methods except MF are implemented in TensorFlow, and the parameters are initialized using scaling variance initializer [8], and Adam optimizer [11]. We tune the scaling factor of initializer \u03c3 from {1, 2, 5, 10, 100} and the learning rate \u03b7 from {0.01, 0.1, 1}\n4h ps://www.yelp.com/dataset-challenge 5h p://www2.informatik.uni-freiburg.de/\u223ccziegler/BX/\nusing the validation sets. In the experiment, we set \u03c3 = 2 (default se ing in TensorFlow) and \u03b7 = 0.01 for these methods except MVM. We found that MVM is more sensitive to the con guration, because MVM will element-wisely multiply the latent factors of all the modes which leads to an extremely small value approaching zero. \u03c3 = 10 and \u03b7 = 0.1 yielded the best performance for MVM.\nTo investigate the performance of comparisonmethods, we adopt\nmean squared error (MSE) on the test data as the evaluation metrics [19, 27]. e smaller value of the metric indicates the be er performance. Each experiment was repeated for 10 times, and the mean and standard deviation of each metric in each data set were reported. All experiments are conducted on a single machine with Intel Xeon 6-Core CPUs of 2.4 GHz and equipped with a Maxwell Titan X GPU. e code has been made available at GitHub 6."}, {"heading": "4.4 Performance Analysis", "text": "e experimental results are shown in Table 3. e best method of each dataset is in bold. For clarity, on the right of the tables we show the percentage improvement of the proposed SFM method over a variety of methods. From these results, we can observe that SFM consistently outperforms all the comparison methods. We also make a few comparisons and summarize our ndings as follows.\n6h ps://github.com/kevin-luc/SFM\nCompared with MF, SFM performs be er with an average improvement of nearly 50%. MF usually performswell in practice [15, 22], while in datasets which are extremely sparse, as is shown in our case, MF is unable to learn an accurate representation of users/items. us MF under-performs other methods which takes the meta information into consideration.\nIn both FM and PolyNet methods, the feature vectors from all the modes are concatenated as a single input feature vector. e major di erence between these twomethods is the choice of kernel applied [2]. e polynomial kernel used in PolyNet considers all monomials (the products of features), i.e., all combinations of features with replacement. e ANOVA kernel used in FM considers only monomials composed of distinct features, i.e., feature combinations without replacement. Compared with the best results obtained from FM methods and from PolyNet methods, SFM leads to an average improvement of 3.3% and 2.4% in MSE, respectively.\ne primary reason behind the results is how the latent factors of each feature are learned. For any factorization based method, the latent factors of a feature are essentially learned from its interactions with other features observed in the data, as can be observed from its update rule. In FM and PolyNet, all the feature interactions are taken into consideration without distinguishing the features from di erent modes. As a result, important feature interactions (e.g., the interactions between the given user and her friend) would be easily buried in irrelevant feature interactions\nfrom the same modes (e.g., the interactions between the friends of the same user). Hence, the learned latent factors are less representative in FM and PolyNet, compared with the proposed SFM. Besides, we can nd that including higher-order interactions in FM and PolyNet (i.e., FM-3 and PolyNet-3) does not always improve the performance. Instead, it may even degrade the performance, as shown in Cloth, Yelp, and BX datasets. is is probably due to over ing, as they need to include more parameters to model the interactions in higher orders while the datasets are extremely sparse such that the parameters cannot be properly learned.\nCompared to the MVMmethod, which models the full-order interactions among all the modes, our proposed SFM leads to an average improvement of 5.87%. is is because not all the modes are relevant, and some irrelevant features interactions may introduce unexpected noise to the learning task. e irrelevant information can even be exaggerated a er combinations, thereby degrading performance. is suggests that preserving the nature of relational structure is important in building predictive models."}, {"heading": "4.5 Computational Cost Analysis", "text": "Next, we investigate the computational cost for comparison methods. e training time (seconds per epoch) required for each datasets are shown in Fig. 5. We can easily nd that the proposed SFM requires much less computational cost on all the datasets, especially for the Yelp dataset (roughly 11% of computational cost required for training FM-3). e e ciency comes from the use of relational structure representation. As shown in Table 2, the number of nonzeros of the feature matrix Nz (X) is much larger than the number of non-zeros of the relational structure representation Nz (B). e amount of repeating pa erns is much higher for the Yelp dataset than for the other dataset, because adding all the friends of a user signi cantly increases results in large repeating blocks in the plain feature matrix. Standard ML algorithms like the compared methods have typically at best a linear complexity inNz (X), while using the relational structure representation for SFM have a linear complexity in Nz (B). is experiment substantiates the e ciency of the proposed SFM for large datasets."}, {"heading": "4.6 Analysis of the Impact of Data Sparsity", "text": "We proceed by further studying the impact of data sparsity on di erent methods. As can be found in the experimental results,\nthe improvement of SFM over the traditional collaborative ltering methods (e.g., MF) is signi cant for datasets that are sparse, mainly because the number of samples is too scarce to model the items and users adequately. We verify this nding by comparing the performance of comparison methods with MF on users with limited training data. Shown in Fig. 6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set. Due to space limit, we only report the results from two Amazon datasets (Sport and Health) while the observations still hold for the rest datasets. It can be seen that the proposed SFM gains the most in group G1, in which the users have extremely few training items. e performance gain starts to decrease with the number of training items available for each user. e results indicate that including meta information can be valuable information especially when limited information available."}, {"heading": "4.7 Sensitivity analysis", "text": "e number of latent factors R is an important hyperparameter for the factorization models. We analyze di erent values of R and report the results in Fig. 7. e results again demo that SFM consistently outperforms other methods with a various values of R. In contrast to ndings in other related models based on latent factors [26]where prediction error can steadily get reducedwith larger R, we observe that the performance of each method is rather stable even with the increasing of R. It is reasonable in a general sense, as the expressiveness of the model is enough to describe the information embedded in data. Although larger R renders the model with greater expressiveness, when the available observations regarding the target values are too sparse but the meta information is rich, only a few number of factors is required to t the data well."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we introduce a generic framework for learning structural data from heterogeneous domains, which can explore the high order correlations underlying multi-view structural data. We develop structural factorization machines (SFMs) that learn the common latent spaces shared in the multi-view tensors while automatically adjust the contribution of each view in the predictive model. With the help of relational structure representation, we further provide an e cient approach to avoid unnecessary computation costs on repeating pa erns of the multi-view data. It was shown that the proposed SFMs outperform state-of-the-art factorization models on eight large-scale datasets in terms of prediction accuracy and computational cost."}, {"heading": "ACKNOWLEDGMENTS", "text": "is work is supported in part by NSF through grants IIS-1526499,\nCNS-1626432, NSFC (61472089, 61503253, 61672357), NSFC-Guangdong Joint Found (U1501254), and the Science Foundation of Guangdong Province (2014A030313556). We gratefully acknowledge the support of NVIDIA Corporationwith the donation of the Titan X GPU used for this research."}], "references": [{"title": "All-at-once optimization for coupled matrix and tensor factorizations", "author": ["E. Acar", "T.G. Kolda", "D.M. Dunlavy"], "venue": "arXiv preprint arXiv:1105.3422", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Higher-order factorization machines", "author": ["M. Blondel", "A. Fujino", "N. Ueda", "M. Ishihata"], "venue": "Advances in Neural Information Processing Systems, pages 3351\u2013 3359", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "A", "author": ["M. Blondel", "M. Ishihata"], "venue": "Fujino, and N.Ueda. Polynomial networks and factorization machines: New insights and e\u0081cient training algorithms. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 850\u2013858", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensor-based multiview feature selection with applications to brain diseases", "author": ["B. Cao", "L. He", "X. Kong", "P.S. Yu", "Z. Hao", "A.B. Ragin"], "venue": "ICDM, pages 40\u201349", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-viewmachines", "author": ["B. Cao", "H. Zhou", "G. Li", "P.S. Yu"], "venue": "ACM International Conference on Web Search and Data Mining, pages 427\u2013436", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Libmf: A library for parallel matrix factorization in shared-memory systems", "author": ["W.-S. Chin", "B.-W. Yuan", "M.-Y. Yang", "Y. Zhuang", "Y.-C. Juan", "C.-J. Lin"], "venue": "\u008ae Journal of Machine Learning Research, 17(1):2971\u20132975", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "\u008ce movielens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS), 5(4):19", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Delving deep into recti\u0080ers: Surpassing human-level performance on imagenet classi\u0080cation", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Dusk: A dual structure-preserving kernel for supervised tensor learning with applications to neuroimages", "author": ["L. He", "X. Kong", "S.Y. Philip", "A.B. Ragin", "Z. Hao", "X. Yang"], "venue": "matrix, 3(1):2", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint community and structural hole spanner detection via harmonic modularity", "author": ["L. He", "C.-T. Lu", "J.Ma", "J. Cao", "L. Shen", "P.S. Yu"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and DataMining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Adam: Amethod for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM review, 51(3):455\u2013500", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative \u0080ltering model", "author": ["Y. Koren"], "venue": "Proceedings of the 14th ACM SIGKDD international conference  on Knowledge discovery and data mining, pages 426\u2013434. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Factor in the neighbors: Scalable and accurate collaborative \u0080ltering", "author": ["Y. Koren"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), 4(1):1", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Ratings meet reviews", "author": ["G. Ling", "M.R. Lyu", "I. King"], "venue": "a combined approach to recommend. In Proceedings of the 8th ACM Conference on Recommender systems, pages 105\u2013112. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "On the computational e\u0081ciency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "Advances in Neural Information Processing Systems, pages 855\u2013863", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilinear factorizationmachines formulti-taskmulti-view learning", "author": ["C.-T. Lu", "L. He", "W. Shao", "B. Cao", "P.S. Yu"], "venue": "Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, pages 701\u2013709. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Item recommendation for emerging online businesses", "author": ["C.-T. Lu", "S. Xie", "W. Shao", "L. He", "P.S. Yu"], "venue": "Proc. 25th Int. Joint Conf. Arti\u0080cial Intell., pages 3797\u20133803", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["J. McAuley", "J. Leskovec"], "venue": "Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring networks of substitutable and complementary products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "SIGKDD, pages 785\u2013794", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Factorization machines", "author": ["S. Rendle"], "venue": "ICDM, pages 995\u20131000", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorization machines with libFM", "author": ["S. Rendle"], "venue": "Intelligent Systems and Technology, 3(3):57", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Scaling factorization machines to relational data", "author": ["S. Rendle"], "venue": "Proceedings of the VLDB Endowment, volume 6, pages 337\u2013348. VLDB Endowment", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Pairwise interaction tensor factorization for personalized tag recommendation", "author": ["S. Rendle", "L. Schmidt-\u008cieme"], "venue": "In Proceedings of the third ACM international conference on Web search and data mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Relational learning via collective matrix factorization", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 650\u2013658. ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Coupled group lasso for web-scale CTR prediction in display advertising", "author": ["L. Yan", "W.-j. Li", "G.-R. Xue", "D. Han"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Joint deep modeling of users and items using reviews for recommendation", "author": ["L. Zheng", "V. Noroozi", "P.S. Yu"], "venue": "Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, pages 425\u2013434. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 3, "context": "Since di\u0082erent views usually provide complementary information [4, 5, 17], how to effectively incorporate information frommultiple structural views is critical to good prediction performance for various machine learning tasks.", "startOffset": 63, "endOffset": 73}, {"referenceID": 4, "context": "Since di\u0082erent views usually provide complementary information [4, 5, 17], how to effectively incorporate information frommultiple structural views is critical to good prediction performance for various machine learning tasks.", "startOffset": 63, "endOffset": 73}, {"referenceID": 16, "context": "Since di\u0082erent views usually provide complementary information [4, 5, 17], how to effectively incorporate information frommultiple structural views is critical to good prediction performance for various machine learning tasks.", "startOffset": 63, "endOffset": 73}, {"referenceID": 20, "context": "Recent works have shown that linear models fail for tasks with very sparse data [21, 22].", "startOffset": 80, "endOffset": 88}, {"referenceID": 21, "context": "Recent works have shown that linear models fail for tasks with very sparse data [21, 22].", "startOffset": 80, "endOffset": 88}, {"referenceID": 1, "context": "A variety of methods have been proposed to address the data sparsity issue by factorizing the monomials (or feature interactions) with kernel tricks, such as the ANOVA kernels used in FMs [2, 21] and polynominal kernels used in polynominal networks [3, 16].", "startOffset": 188, "endOffset": 195}, {"referenceID": 20, "context": "A variety of methods have been proposed to address the data sparsity issue by factorizing the monomials (or feature interactions) with kernel tricks, such as the ANOVA kernels used in FMs [2, 21] and polynominal kernels used in polynominal networks [3, 16].", "startOffset": 188, "endOffset": 195}, {"referenceID": 2, "context": "A variety of methods have been proposed to address the data sparsity issue by factorizing the monomials (or feature interactions) with kernel tricks, such as the ANOVA kernels used in FMs [2, 21] and polynominal kernels used in polynominal networks [3, 16].", "startOffset": 249, "endOffset": 256}, {"referenceID": 15, "context": "A variety of methods have been proposed to address the data sparsity issue by factorizing the monomials (or feature interactions) with kernel tricks, such as the ANOVA kernels used in FMs [2, 21] and polynominal kernels used in polynominal networks [3, 16].", "startOffset": 249, "endOffset": 256}, {"referenceID": 9, "context": "Matrix/tensor factorization models have been a topic of interest in the areas of structural data analysis, such as community detection [10], collaborative \u0080ltering [13, 24], and neuroimage analysis [9].", "startOffset": 135, "endOffset": 139}, {"referenceID": 12, "context": "Matrix/tensor factorization models have been a topic of interest in the areas of structural data analysis, such as community detection [10], collaborative \u0080ltering [13, 24], and neuroimage analysis [9].", "startOffset": 164, "endOffset": 172}, {"referenceID": 23, "context": "Matrix/tensor factorization models have been a topic of interest in the areas of structural data analysis, such as community detection [10], collaborative \u0080ltering [13, 24], and neuroimage analysis [9].", "startOffset": 164, "endOffset": 172}, {"referenceID": 8, "context": "Matrix/tensor factorization models have been a topic of interest in the areas of structural data analysis, such as community detection [10], collaborative \u0080ltering [13, 24], and neuroimage analysis [9].", "startOffset": 198, "endOffset": 201}, {"referenceID": 24, "context": "Assuming multi-view data have the same underlying lowrank structure (at least in one mode), coupled data analysis such as collective matrix factorization (CMF) [25] and coupled matrix and tensor factorization (CMTF) [1] that jointly factorize multiple matrices (or tensors) has been applied to applications such as clustering and missing data recovery.", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "Assuming multi-view data have the same underlying lowrank structure (at least in one mode), coupled data analysis such as collective matrix factorization (CMF) [25] and coupled matrix and tensor factorization (CMTF) [1] that jointly factorize multiple matrices (or tensors) has been applied to applications such as clustering and missing data recovery.", "startOffset": 216, "endOffset": 219}, {"referenceID": 24, "context": ", the weights of the \u0080\u008aing error of each matrix/tensor in the objective function [25], the weights of di\u0082erent types of latent factors in the predictive models [14], or the regularization hyperparamters of latent factor alignment [18].", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": ", the weights of the \u0080\u008aing error of each matrix/tensor in the objective function [25], the weights of di\u0082erent types of latent factors in the predictive models [14], or the regularization hyperparamters of latent factor alignment [18].", "startOffset": 160, "endOffset": 164}, {"referenceID": 17, "context": ", the weights of the \u0080\u008aing error of each matrix/tensor in the objective function [25], the weights of di\u0082erent types of latent factors in the predictive models [14], or the regularization hyperparamters of latent factor alignment [18].", "startOffset": 230, "endOffset": 234}, {"referenceID": 11, "context": "3 (CP factorization [12]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": "Inspired by [5], we construct tensor representation for each view over its entities by X\u0303 = x\u0303 \u25e6 x\u0303 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 x\u0303 \u2208 R1M , where x\u0303(m) = [1;x(m)] \u2208 R1+Im and \u25e6 is the outer product operator.", "startOffset": 12, "endOffset": 15}, {"referenceID": 10, "context": "For the results presented in this paper, we use the Adam optimization algorithm [11] for parameter updates.", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "We adopt the idea from [23] to avoid redundant computing on repeating pa\u008aerns over a set of feature vectors.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "com recently introduced by [20].", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "com has been constructed as an independent dataset in [20].", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "\u008ce ages are split in eight bins as in [7].", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "We use the LIBMF implementation [6] for comparison in the experiment.", "startOffset": 32, "endOffset": 35}, {"referenceID": 21, "context": "Factorization Machine (FM) [22] is the state-of-the-art method in recommender systems.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "We compare with its higher-order extension [2] with up to second-order, and third-order feature interactions, and denote them as FM-2 and FM-3.", "startOffset": 43, "endOffset": 46}, {"referenceID": 15, "context": "PolynomialNetwork (PolyNet) [16] is a recently proposedmethod that utilizes polynomial kernel on all features.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "We compare the augmented PolyNet (which adds a constant one to the feature vector [3]) with up to the second-order, and third-order kernel and denote them as PolyNet-2 and PolyNet-3.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Multi-View Machine (MVM) [5] is a tensor factorization based method that explores the latent representation embedded in the full-order interactions among all the modes.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "All the methods except MF are implemented in TensorFlow, and the parameters are initialized using scaling variance initializer [8], and Adam optimizer [11].", "startOffset": 127, "endOffset": 130}, {"referenceID": 10, "context": "All the methods except MF are implemented in TensorFlow, and the parameters are initialized using scaling variance initializer [8], and Adam optimizer [11].", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "To investigate the performance of comparisonmethods, we adopt mean squared error (MSE) on the test data as the evaluation metrics [19, 27].", "startOffset": 130, "endOffset": 138}, {"referenceID": 26, "context": "To investigate the performance of comparisonmethods, we adopt mean squared error (MSE) on the test data as the evaluation metrics [19, 27].", "startOffset": 130, "endOffset": 138}, {"referenceID": 14, "context": "MF usually performswell in practice [15, 22], while in datasets which are extremely sparse, as is shown in our case, MF is unable to learn an accurate representation of users/items.", "startOffset": 36, "endOffset": 44}, {"referenceID": 21, "context": "MF usually performswell in practice [15, 22], while in datasets which are extremely sparse, as is shown in our case, MF is unable to learn an accurate representation of users/items.", "startOffset": 36, "endOffset": 44}, {"referenceID": 1, "context": "\u008ce major di\u0082erence between these twomethods is the choice of kernel applied [2].", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 133, "endOffset": 139}, {"referenceID": 2, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 133, "endOffset": 139}, {"referenceID": 3, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 141, "endOffset": 147}, {"referenceID": 5, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 141, "endOffset": 147}, {"referenceID": 6, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 153, "endOffset": 160}, {"referenceID": 9, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 153, "endOffset": 160}, {"referenceID": 25, "context": "In contrast to \u0080ndings in other related models based on latent factors [26]where prediction error can steadily get reducedwith larger R, we observe that the performance of each method is rather stable even with the increasing of R.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 40, "endOffset": 46}, {"referenceID": 2, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 40, "endOffset": 46}, {"referenceID": 3, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 48, "endOffset": 54}, {"referenceID": 5, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 48, "endOffset": 54}, {"referenceID": 6, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 60, "endOffset": 67}, {"referenceID": 9, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 60, "endOffset": 67}], "year": 2017, "abstractText": "Real-world relations among entities can o\u0089en be observed and determined by di\u0082erent perspectives/views. For example, the decision made by a user on whether to adopt an item relies on multiple aspects such as the contextual information of the decision, the item\u2019s a\u008aributes, the user\u2019s pro\u0080le and the reviews given by other users. Di\u0082erent views may exhibit multi-way interactions among entities and provide complementary information. In this paper, we introduce a multi-tensor-based approach that can preserve the underlying structure of multi-view data in a generic predictive model. Speci\u0080cally, we propose structural factorization machines (SFMs) that learn the common latent spaces shared by multi-view tensors and automatically adjust the importance of each view in the predictive model. Furthermore, the complexity of SFMs is linear in the number of parameters, which make SFMs suitable to large-scale problems. Extensive experiments on real-world datasets demonstrate that the proposed SFMs outperform several state-of-the-art methods in terms of prediction accuracy and computational cost.", "creator": "LaTeX with hyperref package"}}}