{"id": "1704.03617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning", "abstract": "fuman Although neural networks holdrs are well suited for nobiz sequential vom transfer giedrius learning telcel tasks, the adcock catastrophic fpc forgetting atiak problem hinders suizo proper integration of prior monochromes knowledge. In webgraphics this work, unconfirmed we spank propose germanium a solution cravinho to stoeckel this problem andren by using a ccl multi - task objective nerja based on the idea of silverback distillation and a 13.12 mechanism reopenings that directly penalizes jassi forgetting at the gervasi shared hofkapelle representation kreisau layer during disrupted the knowledge calzaghe integration phase mamora of training. We nohn demonstrate our approach marseilles on bardolph a acetylcysteine Twitter domain tottering sentiment pagal analysis task inclusion with sequential knowledge russet transfer from milliken four related tasks. We show 100,000 that fakel our technique pacificorp outperforms christmasy networks 631,000 fine - limb tuned roussel to ribuffo the 21.90 target task. Additionally, we avici show both minisink through brevin empirical evidence thornleigh and examples that it does not forget useful knowledge smail from the requester source ornately task that is forgotten 19-4 during chloropaschia standard wenxia fine - travel@latimes.com tuning. instrument Surprisingly, mamma.com we waianae find contempor\u00e1neo that defunded first distilling chickasha a human made emissary rule based sentiment monomotapa engine 179.6 into ciralsky a abar recurrent lemercier neural network and dauphine then integrating the 2233 knowledge rattail with maryboy the target lafount task landsbergis data adulteration leads to a 121.21 substantial economicos gain rebuke in generalization skel performance. Our experiments demonstrate sigur the dalecarlia power kugluktuk of turnverein multi - nonindigenous source transfer techniques biannual in budging practical tootsie text kayama analytics phytochemicals problems yod when paired with distillation. In quartararo particular, for the gantries SemEval spini 2016 Task yesh 4 34.05 Subtask kosse A (Nakov coercive et well-meaning al. , idents 2016) dataset fadl we fiskerton surpass fatullah the gfms state lytvyn of tortoise the art 0.12 established 1-million during the competition with a bese comparatively simple boniek model klauser architecture stj that 11:33 is laminates not even \u00e9milie competitive when afreximbank trained on boukhari only the clangorous labeled task specific 19,167 data.", "histories": [["v1", "Wed, 12 Apr 2017 04:38:18 GMT  (40kb)", "http://arxiv.org/abs/1704.03617v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["matthew riemer", "elham khabiri", "richard goodwin"], "accepted": false, "id": "1704.03617"}, "pdf": {"name": "1704.03617.pdf", "metadata": {"source": "CRF", "title": "IMPROVED TEXT ANALYTICS TRANSFER LEARNING", "authors": ["Matthew Riemer", "Elham Khabiri"], "emails": ["rgoodwin}@us.ibm.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n03 61\n7v 1\n[ cs\n.C L\n] 1\n2 A\npr 2\n01 7\nAlthough neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks. We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data."}, {"heading": "1 INTRODUCTION", "text": "Sequential transfer learning methodologies leverage knowledge representations from a source task in order to improve performance for a target task. A significant challenge faced when transferring neural network representations across tasks is that of catastrophic forgetting (or catastrophic interference). This is where a neural network experiences the elimination of important old information when learning new information. The very popular strategy of fine-tuning a neural network involves first training a neural network on a source task and then using the model to simply initialize the weights of a target task network up to the highest allowable common representation layer. However it is highly susceptible to catastrophic forgetting, because in training for the target task it has no explicit incentive to retain what it learned from the source task. While one can argue that forgetting the source task should not matter if only the target task is of interest, our paper adds to the recent empirical evidence across problem domains (Li & Hoiem, 2016),(Rusu et al., 2016) that show additional network stability can lead to empirical benefits over the fine-tuning algorithm. It seems as though for many Deep Learning problems we can benefit from an algorithm that promotes more stability to tackle the well known stability-plasticity dilemma. One popular approach for addressing this problem is rehearsals (Murre, 1992), (Robins, 1995). Rehearsals refers to a neural network training strategy where old examples are relearned as new examples are learned. In the transfer setting it can be seen as related to multi-task learning (Caruana, 1997) where two tasks are trained at the same time, rather than sequentially, while sharing a common input encoder to a shared hidden representation. However, in rehearsals the representation is biased in favor of the source task representation through initialization. This technique is very sensible because while fine-tuning is susceptible to catastrophic forgetting, multi-task learning is not (Caruana, 1997).\nOne of the biggest issues with the standard rehearsals paradigm is that it requires a cached memory of training examples that have been seen in the past. This can be a massive requirement as the number of source tasks and training data sizes scale. One compelling technique for addressing this problem is the concept of pseudorehearsals (Robins, 1995), (Robins, 1996), where relearning is performed on an artificially constructed population of pseudoitems instead of the actual old examples. Unfortunately, current automatic techniques in the text analytics domain have not yet mastered producing linguistically plausible data. As such, the pseudorehearsals paradigm is likely to waste computational time that could be spent on learning realistic patterns that may occur during testing. In our work, we extend the Learning without Forgetting (LwF) paradigm of (Li & Hoiem, 2016) to the text analytics domain using Recurrent Neural Networks. In this approach, the target task data is used both for learning the target task and for rehearsing information learned from the source task by leveraging synthetic examples generated for the target task input by the model that only experienced training on the source task data. As argued by Li & Hoiem (2016), this setup strikes an important balance between classification performance, computational efficiency, and simplicity in deployment.\nRegardless of whether they are applied to real source task examples, real target task examples, or synthetic examples, paradigms in the style of rehearsals all address the shortcomings of neural network forgetting by casting target task integration as a multi-task learning problem. However, this is not quite the purpose of the multi-task learning architecture, which was designed for joint learning of tasks from scratch at the same time. The key disconnect is that in multi-task learning, the transformation from the shared hidden layer to the outputs for each task are all learned and updated with the changing hidden representation. This would imply that, in the framework of rehearsals, it is possible for there to be significant changes during learning of the network\u2019s representation, and thus its abilities on the source task itself. While it would be desirable to claim we were allowing our source task network to become even better based on the target task than it was before, this motivation seems idealistic in practice. One reason this is idealistic is because multi-task learning generally only works well when tasks are sampled at different rates or alternatively given different priority in the neural network loss function (Caruana, 1997). As a result, it is most likely that auxilirary source tasks will receive less priority from the network for optimization than the target task. Additionally, we observe in our experiments, and it has been observed by others in (Rusu et al., 2015), that it is generally not possible to distill multiple complex tasks into a student network at full teacher performance for all tasks. This seems to imply the degradation of the source task performance during training is somewhat inevitable in a multi-task learning paradigm.\nWe address this issue with our proposed forgetting cost technique. We demonstrate that it, in fact, can be valuable to keep the hidden to output transformation of the source tasks fixed during knowledge integration with the target task. This way, we impose a stronger regularization on the hidden representation during target task integration by not allowing it to change aspects that were important to the source task\u2019s performance without direct penalization in the neural network\u2019s loss function. We demonstrate empirically both that freezing the source task specific weights leads to less deterioration in the accuracy on the source task after integration, and that it achieves better generalization performance in our setting. The forgetting cost is practical and easy to implement in training any kind of neural network. In our experiments, we explore application of the forgetting cost in a recurrent neural network to the three way Twitter sentiment analysis task of SemEval 2016 Task 4 Subtask A and find it to achieve consistently superior performance to reasonable baseline transfer learning approaches in four examples of knowledge transfer for this task.\nWe also demonstrate how powerful distillation can be in the domain of text analytics when paired with the idea of the forgetting cost. Significantly, we show that a high quality gazetteer based logical rule engine can be distilled using unlabeled data into a neural network and used to significantly improve performance of the neural network on the target task. This is achieved with a novel extension of the LwF paradigm by Li & Hoiem (2016) to the scenario of a source task with the same output space as the target task. This can be a very promising direction for improving the ability of humans to directly convey knowledge to deep learning algorithms. Indeed, a human defined rule can contain far more information than a single training example, as that rule can be projected on to many unlabeled examples that the neural network can learn from. This is the reason human teachers generally begin teaching human students tasks by going over core rules at the onset of learning. Moreover, we showcase that multiple expert networks trained on the target task with prior knowledge from different source tasks can be effectively combined in an ensemble and then distilled into a single GRU model (Cho et al., 2014), (Chung et al., 2014). Leveraging this combination of distillation\nand knowledge transfer techniques allows us to achieve state of the art accuracy on the SemEval task with a model that performs 11% worse than the best prior techniques when trained only on the labeled data."}, {"heading": "2 RELATED WORK", "text": "Since the work of (Bucilu et al., 2006) and (Hinton et al., 2015) showed that an ensemble of neural network classifier can be distilled into a single model, knowledge distillation from a teacher network to a student network has become a growing topic of neural network research. In (Ba & Caruana, 2014) it was shown that a deep teacher neural network can be learned by a shallow student network. This idea was extended in (Romero et al., 2014), where it was demonstrated that a deep and narrow neural network can learn a representation that surpasses its teacher. The use of distillation as a means of sharing biases from multiple tasks was explored in (Lopez-Paz et al., 2016), where the teacher network is trained with the output of the other tasks as input. It is not obvious how to extend a recurrent neural network to best use this kind of capability over a sequence. The idea of distilling from multiple source task teachers into a student network was highlighted in the reinforcement learning setting in (Rusu et al., 2015). Additionally, the concept of using distillation for knowledge transfer was also explored in (Chen et al., 2015), where function preserving transformations from smaller to bigger neural network architectures were outlined. This technique could also provide value in some instances for our approach where wider or deeper neural networks are needed for the task being transferred to than was needed for the original task. Distillation over target task data was first proposed as a means of elevating catastrophic forgetting in sequential knowledge transfer as applied to image classification in (Li & Hoiem, 2016). We extend this approach for its first application to our knowledge for text analytics problems, with a recurrent neural network architecture, and in the setting where the source task and target task have the same output. The chief distinction of our proposed forgetting cost is that source task specific parameters are held fixed during integration with the target task as opposed to the joint training of all parameters used by Li & Hoiem (2016). Our experiments empirically support the intuition that freezing these parameters leads to greater retention of source task performance after target task integration and better generalization to the target task.\nAn ensemble over multiple diverse models trained for the same sentiment analysis task was also considered in (Mesnil et al., 2014) for the IMDB binary movie reviews sentiment dataset (Maas et al., 2011). We tried this ensemble model in our work and found that it gave very limited improvement. Our ensemble technique learns a more powerful weighted average based on the soft targets of each task and a multi-step greedy binary fusion approach that works better for the Twitter sentiment analysis task in our experiments. Knowledge transfer from multiple tasks was considered to estimate the age of Twitter users based on the content of their tweets in (Riemer et al., 2015). We experimented with the hidden layer sharing approach outlined in that work and found that even when using just a single softmax combining layer, it would overfit on our limited training and validation data. Progressive neural networks (Rusu et al., 2016) is a recently proposed method very similar in motivation to our forgetting cost as it is directly trying to solve the catastrophic forgetting problem. The idea is that learned weight matrices relate the fixed representations learned on the source task to the construction of representations for the target task. In our experiments, the progressive neural networks approach consistently fails to even match the results achieved with fine-tuning. We hypothesize that although using fixed representations to aid learning addresses catastrophic forgetting, it suffers from the curse of dimensionality. As such, when training data is relatively small given the complexity of the task, it is prone to overfitting as it effectively increases the input dimension size through shared fixed representations.\nThe combination of logic rules and neural networks has been explored in a variety of different architectures and settings. These neural-symbolic systems (Garcez et al., 2012) include early examples such as KBANN (Towell et al., 1990) that construct network architectures from given rules to perform reasoning. (Hu et al., 2016) very recently also looked at the problem of distilling logical rules into a neural network text analytics classifier. However, our approach is much more generic as it can be applied to integrate knowledge from any kind of pre-made classifier and treats the rule engine as a black box. In (Hu et al., 2016) they consider the individual rules and leverage an iterative convex optimization algorithm alongside the neural network to regularize the subspace of the network. In our work we demonstrate that, by guarding against catastrophic forgetting, it is possible to efficiently leverage rules for transfer by utilizing a generic sequential knowledge transfer framework. We do\nnot need to make any modification to the architecture of the neural network during testing and do not need iterative convex optimization during training."}, {"heading": "3 FORGETTING COST REGULARIZATION", "text": ""}, {"heading": "3.1 SEQUENTIAL KNOWLEDGE TRANSFER PROBLEM STATEMENT", "text": "In the sequential knowledge transfer problem setting explored in this paper, training is first conducted solely on the source task examples S, includingKS training examples (xSi, ySi) \u2208 S where xSi is the input representation and ySi is the output representation. After training is complete on S, we would like to now use prior knowledge obtained in the model trained on S to improve generalization on a new target task with examples T , which includesKT training examples (xTi, yTi) \u2208 T . Here we assume that the input representations xSi and xTi are semantically aligned in the same representation space. As such, if there is useful knowledge in S that applies in some direct or indirect way to the target task that is not present in T , we would expect a good knowledge integration approach to generalize better to the target task than it is possible to using the training data in T alone. Strong performance for the sequential knowledge transfer problem is a first step towards the greater goal of a mechanism for effective lifelong learning (Thrun, 1996)."}, {"heading": "3.2 FORGETTING COST FOR TUNING A TARGET TASK MODEL", "text": "The most straightforward application of our proposed forgetting cost paradigm is for the case of integrating a neural network that has been trained on source task data S, which has outputs in the same representation space as the outputs for the target task data T . In this case, the forgetting cost amounts to the addition of a regularization term in the objective function during the integration phase when we train using T . This promotes the neural network to be able to recreate the soft labels of the initialized model found after training on S before integration is started with T . More formally:\nLoss = L(y, y\u0302) + \u03b1fL(yinit, y\u0302) (1)\nwhere L is some loss function (we use mean squared error in our experiments) and yinit is the soft label generated for the target task input xTi based on the model after training just on S. The model trained just on S is also used to initialize the weights of the target task model before integration with T as we do in the standard fine-tuning paradigm. \u03b1f is a hyperparameter that can be utilized to control the extent of allowed forgetting. Of course, a very similar way to express this idea would be to mix synthetic training examples T \u2032 with the same input as T and output generated by the model trained just on S with the true target task training examples T . In this case, the mixing rate of the teacher generated training examples is analogous to our forgetting parameter \u03b1f determining the prioritization. These techniques perform quite similarly in our experiments, but we actually find that the formulation in equations 1 and 3 perform slightly better on the test set. For example, this formulation is superior by 0.4% accuracy in tuning a distilled representation of a logical rule engine. We conjecture that learning tasks in the same gradient step when they are related to the same input data results in slightly less noisy gradients."}, {"heading": "3.3 FORGETTING COST FOR KNOWLEDGE TRANSFER FROM A RELATED TASK", "text": "The assumption in section 3.2 that the output of the source task data S should be in the same representation space as the output for the target task data T is quite a big one. It rules out the vast majority of knowledge sources that we can potentially leverage. As such, we propose an extension that does not make this restriction for application in sequential knowledge transfer of tasks that are not directly semantically aligned. We update our model to include another predicted output separate from y\u0302:\ny\u0302init = finit(Wfixedhshared + bfixed) (2)\nwhere y\u0302init is a predicted output attempting to recreate the soft labels of the original model trained just on S. finit is the non-linearity used in the final layer of the source task model. Weight matrix Wfixed and bias bfixed are taken from the final layer of the source task model and are not updated\nduring integration with the target task data T . As a result, the loss function is updated from section 3.2:\nLoss = L(y, y\u0302) + \u03b1fL(yinit, y\u0302init) (3)\nwhere the hidden state is shared between both terms in the objective function. Up to the shared hidden layer, we initialize the model for the target task with the weights learned just using S. Random matrices and bias vectors are now used to initialize the prediction of y\u0302 based on the shared hidden representation. This can be seen as a weak form of restricting the model parameters that can be useful for regularization. The hidden representation is in effect constrained so that it is promoted not to change in key areas that have a large effect on the output vector of the source task model. On the other hand, there is little regularization for parameters that have little effect on the output vector for the source task model."}, {"heading": "4 RECURRENT NEURAL NETWORK MODEL", "text": "In recent years, recurrent neural network models have become a tool of choice for many NLP tasks. In particular, the LSTM variant (Hochreiter & Schmidhuber, 1997) has become popular as it alleviates the vanishing gradients problem (Bengio et al., 1994) known to stop recurrent neural networks from learning long term dependencies over the input sequence. In our experiments we use the simpler GRU network (Cho et al., 2014), (Chung et al., 2014) that generally achieves the same accuracy despite a less complex architecture. Each time step t is associated with an input xt and a hidden state ht. The mechanics of the GRU are defined with the following equations:\nzt = \u03c3(Wxzxt +Whzht\u22121) (4)\nrt = \u03c3(Wxrxt +Whrht\u22121) (5)\nh\u0303t = tanh(Wxhxt + rt \u25e6Whhht\u22121) (6)\nht = zt \u25e6 ht\u22121 + (1 \u2212 zt) \u25e6 h\u0303t (7)\nwhere \u25e6 denotes an element-wise product. Wxz , Wxr, and Wxh represent learned matrices that project from the input size to the hidden size. Whz , Whr, and Whh represent learned matrices that project from the hidden size to the hidden size. In our work we evaluate the GRU in the categorical prediction setting. For each document, the hidden state after the last word hL is used for the prediction y\u0302 of the label y. As such, we treat hL as the shared hidden representation hshared from section 3.3 for our experiments.\ny\u0302 = f(WyhhL + by) (8)\nThe prediction goes through one other non-linear function f after the final hidden state is derived. In our experiments we use the softmax function, but others are useful in different settings. A model that builds on top of GRUs with an external memory storage paradigm (Kumar et al., 2015) currently holds the state of the art on movie review sentiment analysis. However, we focus just on the straightforward single layer GRU model in our experiments so that we can more easily disentangle factors of influence on performance. Our GRU model was fed a sequence of fixed 300 dimensional Glove vectors (Pennington et al., 2014), representing words based on analysis of 840 billion words from a common crawl of the internet, as the input xt for all tasks. It has been shown in a number of papers that tuning the word embeddings during training could increase performance, and it is possible our approach could have performed better had we done so."}, {"heading": "5 SEQUENTIAL KNOWLEDGE TRANSFER EXPERIMENTS", "text": ""}, {"heading": "5.1 EXPERIMENT DETAILS", "text": "Our neural network models were implemented in Theano (Theano Development Team, 2016) and trained with Stochastic Gradient Descent. As we did not use an advanced optimization method and\nnoticed run to run variation in performance, for all of our transfer learning models we trained 10 parallel versions and chose the one with the highest validation accuracy. The SemEval 2016 Task 4 Subtask A training set consists of 10,000 total training examples, but we were only able to receive 8,906 because of tweet removals when we used the downloading script. For the target task data across our experiments, 7,600 examples of the SemEval training set examples were used for training and the rest for validation. The GRU model achieves only 53.6% accuracy on the SemEval testing data when just training with the target task data and random initialization. In order to improve, we consider knowledge transfer from GRUs trained for the following source tasks to the SemEval target task data:\nDistilling Logical Rules: Knowledge distillation can be performed using teacher models that are very different in structure than their neural network based student models. We demonstrate with this task that a compilation of logical linguistic rules can be used as an effective teacher for a GRU by having the GRU attempt to create the output of the rule engine generated over unlabeled in domain data. Specifically, our gazetteer based logical rule engine separates sentences and phrases in the text. It then applies dictionaries of positive and negative sentiment words and phrases to the corresponding text. For each positive or negative phrase found, it checks to see if negation or double negation are applied, and modifies the polarity of the sentiment accordingly. The result for any piece of text is a count of positive and negative sentiment occurrences. For this task, we simply count the total number of positive and negative indicators to give an overall positive, negative or neutral score. We provide addition details on howwe mapped rules to soft targets for the student network to recreate in Appendix A. We utilized a GRU model with 50 hidden units and 50,000 unlabeled examples for our source task model. We distill off the soft labels as in (Hinton et al., 2015), but set our temperature fixed at 1.0. It is possible that our performance could have improved by tuning this parameter. Additional details about the selection of the network and data size are included in Appendix B. The logical rule model itself achieves 57.8% accuracy on the SemEval testing data and the rules distilled into a GRU as explained in section 4 achieves 58.9% accuracy before any integration with the SemEval target task data. We leverage this task for comparison of knowledge transfer techniques when the source task and target task share an output space as discussed in section 3.2.\nBinary Movie Reviews: For knowledge transfer from related tasks as discussed in section 3.3 we first consider the Stanford Sentiment Treebank (Socher et al., 2013), which is a popular sentiment dataset based on the movie review domain. We consider one source task to be the binary (positive, and negative) sentence level sentiment subtask which contains 6,920 training examples, 872 validation examples, and 1,821 testing examples. Our GRU model with 40 hidden units achieves 85.5% accuracy on this task.\nFive Class Movie Reviews: We also consider another source task leveraging the Stanford Sentiment Treebank data from the fine grained (very positive, positive, neutral, negative, and very negative) sentence level sentiment substask which contains 8,544 training examples, 1,101 validation examples, and 2,210 testing examples. We use a GRU model with 200 hidden units to accommodate for the increased task complexity and achieve 45.9% accuracy. This fine grained model can actually be assessed directly on the SemEval task by projecting from five classes to three classes, but it only achieves 44.2% accuracy with no tuning on the target task data. Our performance on these two movie review source tasks is quite similar to what was reported in (Tai et al., 2015) when using a similar setup, but with LSTMs for both subtasks.\nEmoticon Heuristic: Finally, we consider a semi-supervised task based on emoticon prediction motivated by the successful work in (Go et al., 2009), leveraging it in the twitter sentiment domain and its use as a vital component of the SemEval competition winning system (Bethard et al., 2016). We find unlabelled tweets that contain smileys, frowns, or laughing emoticons. We remove emoticons from the tweet before prediction and compile a dataset of 250,000 training examples, 50,000 validation examples, and 100,000 testing examples for each of the three classes. This is multiple orders of magnitude smaller than the 90 million tweets used in (Bethard et al., 2016) to allow for quick experimentation. Our GRU model with 50 hidden units achieves 63.4% accuracy on the emoticon prediction test set."}, {"heading": "5.2 SEQUENTIAL KNOWLEDGE TRANSFER ALGORITHMS", "text": "We consider multiple sequential knowledge transfer algorithms for experimental comparison. Each uses only the source task data for learning the source task and only the target task data for integrating with the target task. This way integration is fast and simple, because it does not incorporate storage and replay of examples from the potentially very large source task as argued in (Li & Hoiem, 2016).\nFine-Tuning: The representation is simply initialized with the representation found after training on the source task and then trained as usual on the target task. This approach was pioneered in (Hinton & Salakhutdinov, 2006), in application to unsupervised source tasks and applied to transfer learning in (Bengio et al., 2012), and (Mesnil et al.). The learning rate is tuned by a grid search based on the validation set performance.\nProgressive Networks: We also compare with our implementation of a progressive neural network (Rusu et al., 2016), where the representation learned for the source task is held fixed and integrated with a target task specific model via lateral connections trained using the target task data. The learning rate is also tuned based on a grid search using the validation set.\nLearning without Forgetting (LwF): In the LwF paradigm, joint training is performed after parameter initialization. This is achieved by treating the target task data and the output generated by the source task model based on the target task input data as two jointly learned tasks as in (Caruana, 1997). As opposed to our proposed forgetting cost, the source task specific parameters are not held fixed while training on the target task data. The learning rate and mixing rate between the tasks are tuned by a grid search based on validation set performance. We first consider a version of the LwF model that leverages a random initialization of the target task specific parameters and initialization of all parameters learned on the source task with the learned values. We also consider another formulation that we call Greedy LwF. This is actually more closely aligned with the original paper (Li & Hoiem, 2016). All source task parameters are first held fixed, and the target task specific parameters are learned alone before joint training with all of the parameters unfrozen as a second step. For the case of source tasks with output in the space of the target task output, there are no source task specific parameters, so the forgetting cost can be viewed as a viable interpretation of the LwF paradigm appropriate in that setting.\nForgetting Cost: Finally, we compare each baseline model with our proposed forgetting cost described in section 3. The learning rate as well as \u03b1f from equations 1 and 3 were tuned by a grid search based on the validation set performance."}, {"heading": "5.3 TARGET TASK RESULTS", "text": "We empirically evaluate the generalization performance of the forgetting cost for sequential knowledge transfer from four different source tasks in Table 1 and Table 2. The source task considered in Table 1 is distilling a logical rule model, leveraging the technique outlined in equation 1. In Table 2 we leverage the forgetting cost for related task knowledge transfer as outlined in equation 3.\nOur experimental results on the SemEval data validate our intuition that the forgetting cost should lead to stronger regularization and better generalization performance. One thing to note about our progressive neural networks implementation is that it effectively has only one hidden layer, because we hold our embeddings fixed during model training and the same embeddings are shared among the models used for all of the tasks. It is possible that having multiple layers of lateral connections is important to achieving good performance. However, this setting was not applicable in our experiments. Our results for sequential knowledge transfer on the SemEval benchmark are quite encouraging as the forgetting cost outperforms baselines significantly in all cases.\nWe additionally have validated the intuition that equation 1 should perform stronger regularization than equation 3 when equation 1 is applicable. In fact, for our distilled logical rule model tuning experiments, we found that equation 1 performs 3% better on the test set. In an attempt to understand more about what caused this performance difference, we monitored testing set performance at each epoch and noticed that equation 3 is actually prone to overfitting away from a good solution on the test set. However, it often finds a pretty good one comparable to equation 1 early in training. When equation 1 could be applied, it seems to be a useful regularization to constrain both the hidden layer and the output layer to align with the model learned on the source task. In equation 3, the\nhidden to output transformation learned for the target task can in contrast learn to deviate from the transformation learned for the source task."}, {"heading": "5.4 SOURCE TASK PERFORMANCE AFTER TARGET TASK INTEGRATION", "text": "In Table 3 we explore the retention of empirical performance on the source task for knowledge transfer algorithms after integration with the target task is complete. Apparently in these cases, allowing relearning of the source task model during integration with the target task data is indeed destructive to source task performance. LwF outperforms Fine-Tuning significantly in knowledge retention for movie reviews, but interestingly does not for the emoticon heuristic. The effect of the greedy target task initialization strategy also appears inconsistent. It seems it is possible that this greedy initialization could improve our proposed forgetting cost paradigm in some cases as well. However, a rigorous analysis of the tradeoffs for this initialization approach is beyond the scope of this paper.\nAs the source task representation is literally stored fixed as part of the target task representation in progressive neural networks, it is not clear how to assess any effective forgetting of the source task during target task integration. As a result, we omit them from our source task forgetting experiments."}, {"heading": "5.5 INSPECTION OF LEARNED REPRESENTATIONS", "text": "Now that we have established the empirical benefits of our proposed forgetting cost, we will demonstrate what it achieves qualitatively through examples. In Table 4 we include a sample of examples that are predicted correctly by transferring the knowledge source with the forgetting cost paradigm and not with fine-tuning based integration. The effect is, perhaps, easiest to understand for the rule based and movie review based transfer scenarios. For the rule based transfer setting you can literally map insights that are not forgotten to their respective logical rule in the model, as is the case in these examples. Moreover, we can see movie domain specific terminology such as \u201dMay the force be with\u201d is seemingly forgotten with standard fine-tuning, but not when the forgetting cost regularization is applied.\nConsidering that we have shown a neural network can distill and improve a representation learned by a logical rule engine, how the final representation differs from the logic of the original engine is of practical interest. We thus compare the agreement of our fine-tuned rule based GRU with the original rule model on the SemEval testing set. We find that the transferred model achieves 78.7% agreement with the rule model when the rule model is right. This clearly indicates that our final model is not deterministic based on the rule engine, and has a probability of adding errors even when the original rule model works well. However, our model actually has 44.7% accuracy on the examples the rule model got wrong. Our approach yields significant gains in comparison to the original rule classifiers, improving from 57.8% to 64.4% test set accuracy before even incorporating in auxiliary knowledge sources."}, {"heading": "6 INTEGRATING TRANSFER LEARNING FROM MULTIPLE TASKS WITH ENSEMBLE DISTILLATION", "text": ""}, {"heading": "6.1 ENSEMBLE METHODOLOGY", "text": "In our experiments we tried to find a balance between an ensemble model that is powerful enough to have an adaptive weighted average decision function and not so powerful that it overfits on our limited training and validation data. Our model is quite similar in architecture to the gating network component of a hierarchical mixture of experts model (Jacobs et al., 1991), (Jordan & Jacobs, 1994). We tried our model over all four representations at once and found that it overfits. Our experiments showed it is more effective to adopt a greedy ensembling strategy where all models are combined with the best performing model on the validation set at each phase until only two models are left. Finally, these two models are combined with the same mechanism. (Riemer et al., 2016) suggests that a many element gating network can be improved with a sparsity constraint, but this did not work as well as the greedy strategy for our model and experiments.\nMore formally, for any two models A and B combined in an ensemble, we train the following mechanism using Stochastic Gradient Descent:\nmA = \u03c3(WAy\u0302A + bA) (9)\nmB = \u03c3(WB y\u0302B + bB) (10)\naA = mA\nmA +mB (11)\naB = mB\nmA +mB (12)\ny\u0302ensemble = aAy\u0302A + aB y\u0302B (13)\nwhere y\u0302ensemble is the prediction vector of the combined ensemble. y\u0302A and y\u0302B are the output vectors of the individual models."}, {"heading": "6.2 ENSEMBLE RESULTS", "text": "Our ensemble model was trained on what was set aside as the validation data during the initial training with early stopping. In the first phase of combining, the model transferred from the logical rule source task was combined with each model. In the second phase, the model based on transfer from the binary movie review sentiment model was combined with each model. In the third phase, the two remaining models were combined. The results of our ensemble in Table 5 suggest that it is possible to further improve the performance of a single sequential transfer model by intelligently combining its predictions with models that have other perspectives. This is because they are modeled using different source tasks for prior knowledge. Impressively, our final distilled model surpasses results from all prior models on the SemEval 2016 benchmark using the same final architecture of a 50 hidden unit GRU model that is clearly not even competitive when trained simply on the task specific labeled data. The prior best model SwissCheese (Bethard et al., 2016) consists of random forests ensemble built utilizing multiple convolutional neural network models and distant supervision. In fact, we achieve superior results despite using over an order of magnitude less total data for training our model.\nWe would also like to underscore that our total improvement of 1.5% as a result of creating an ensemble with our best transferred model from the logical rule source task can be viewed as quite disappointing, despite achieving state of the art results. In fact, in the theoretical limit of having a decision model that switches to the best already learned model at each point, our four transferred representations would achieve 85.1% accuracy together. For the combination of the movie review based models and logical rule based model we can get to 81.4% accuracy. Moreover, we can get 76.5% accuracy with just the logical rule based transfer model and the emoticon prediction based transfer model. Unfortunately, we achieve nowhere near these theoretical results despite representations that are apparently quite diverse. This seems indicative that there are significant gains yet to be uncovered in integrating these representations."}, {"heading": "7 CONCLUSION", "text": "We consider a new methodology called the forgetting cost for preventing the catastrophic forgetting problem of neural network sequential transfer learning. The forgetting cost is practical and easy to implement. We have demonstrated for the challenging task of Twitter sentiment analysis that it can uncover significant gains in generalization performance and that it seems to not forget knowledge traditionally forgotten from the source task during fine-tuning. Our strong empirical results still motivate multiple avenues with high potential for continued exploration in text analytics. Using logical rules to improve neural network models is a promising direction for humans to efficiently contribute to increased model performance. Additionally, the large diversity of representations learned from multiple classifiers with the same target task but different source tasks seems to indicate there is potential to see even much greater gains when integrating multiple sources of knowledge transfer."}, {"heading": "A MAPPING SENTIMENT RULES TO SOFT TARGETS", "text": "The gazetteer based logical rule engine separates sentences and phrases in the text. It then applies dictionaries of positive and negative sentiment words and phrases to the corresponding text. For each positive or negative phrase found, it checks to see if negation or double negation are applied, and modifies the polarity of the sentiment accordingly. The result for any piece of text is a count of positive and negative sentiment occurrences. For this task, we simply count the total number of positive and negative indicators to give an overall positive, negative or neutral score. To be concrete, we have a simple procedure for mapping positive and negative word counts to soft labels that could be used for distillation. If there are no positive or negative words, the output vector is a one hot vector corresponding to a neutral label. If there are an unequal number of positive and negative sentiment words, the neutral label is zero and the raw counts are sent to the softmax function to create a soft label over the positive and negative word occurrences. Finally, if there are an equal amount of positive and negative words, we consider the added total sentiment words plus one in the neutral label as well as the number of positive words and negative words before sending these totals through a softmax function."}, {"heading": "B SIZE SELECTION FOR THE RULE DISTILLATION TASK", "text": "In Table 6 we detail the performance of distilling a logical rule engine into a GRU based recurrent neural network by imposing soft labels over unlabeled tweets. The fact that we keep our word representations fixed with general purpose unsupervised data makes it difficult for the GRU to distill the entire model without a large number of examples. Additionally, as there were a large number of examples in our distillation experiments, we did not experience high run to run variation and only trained a single GRU model for each distillation experiment (as opposed to picking the best validation error of 10 parallel training routines as in our transfer experiments). Our distilled GRU is\nbetter on the testing set than the original classifier, likely because this input representation prevents the model from overfitting to the idiosyncrasies of the rule engine. This actually underscores an important point for the distillation of abstract knowledge. If the target task is known during distillation, it may be beneficial to stop short of totally distilling the original knowledge as it may hurt down stream performance past a certain point. We impose a simple policy where the best hidden unit and training example combination is selected based on performance on the training data of the target task. As a result, we use the model with 50 hidden units based on 50,000 training examples in our experiments integrating with other knowledge. This model is a pretty good one to choose, and achieves high transfer performance relative to models that overfit on the teacher network."}], "references": [{"title": "Unipi at semeval-2016 task 4: Convolutional neural networks for sen-timent classification", "author": ["Giuseppe Attardi", "Daniele Sartiano"], "venue": "Proceedings of SemEval,", "citeRegEx": "Attardi and Sartiano.,? \\Q2016\\E", "shortCiteRegEx": "Attardi and Sartiano.", "year": 2016}, {"title": "Do deep nets really need to be deep? In Advances in neural information processing", "author": ["Jimmy Ba", "Rich Caruana"], "venue": null, "citeRegEx": "Ba and Caruana.,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana.", "year": 2014}, {"title": "Learning long-term dependencieswith gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": "ICML Unsupervised and Transfer Learning,", "citeRegEx": "Bengio,? \\Q2012\\E", "shortCiteRegEx": "Bengio", "year": 2012}, {"title": "The Association for Computer Linguistics. ISBN 978-1-941643-95-2", "author": ["Steven Bethard", "Daniel M. Cer", "Marine Carpuat", "David Jurgens", "Preslav Nakov", "Torsten"], "venue": "Zesch (eds.). Proceedings of the 10th International Workshop on Semantic Evaluation,", "citeRegEx": "Bethard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bethard et al\\.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Net2net: Accelerating learning via knowledge transfer", "author": ["Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens"], "venue": "arXiv preprint arXiv:1511.05641,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Neural-symbolic learning systems: foundations and applications", "author": ["Artur S d\u2019Avila Garcez", "Krysia Broda", "Dov M Gabbay"], "venue": null, "citeRegEx": "Garcez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Garcez et al\\.", "year": 2012}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang"], "venue": null, "citeRegEx": "Go et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507,", "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Harnessing deep neural networks with logic rules", "author": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric Xing"], "venue": "arXiv preprint arXiv:1603.06318,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Adaptive mixtures of local experts", "author": ["Robert A Jacobs", "Michael I Jordan", "Steven J Nowlan", "Geoffrey E Hinton"], "venue": "Neural computation,", "citeRegEx": "Jacobs et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jacobs et al\\.", "year": 1991}, {"title": "Ntnusenteval at semeval-2016 task 4: Combining general classifiers for fast twitter sentiment analysis", "author": ["Brage Ekroll Jahren", "Valerij Fredriksen", "Bj\u00f6rn Gamb\u00e4ck", "Lars Bungum"], "venue": "Proceedings of SemEval,", "citeRegEx": "Jahren et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jahren et al\\.", "year": 2016}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["Michael I Jordan", "Robert A Jacobs"], "venue": "Neural computation,", "citeRegEx": "Jordan and Jacobs.,? \\Q1994\\E", "shortCiteRegEx": "Jordan and Jacobs.", "year": 1994}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Learning without forgetting", "author": ["Zhizhong Li", "Derek Hoiem"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Li and Hoiem.,? \\Q2016\\E", "shortCiteRegEx": "Li and Hoiem.", "year": 2016}, {"title": "Unifying distillation and privileged", "author": ["David Lopez-Paz", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "Vladimir Vapnik"], "venue": "information. stat,", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2016}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["Gr\u00e9goire Mesnil", "Tomas Mikolov", "Marc\u2019Aurelio Ranzato", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.5335,", "citeRegEx": "Mesnil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Learning and categorization in modular neural networks", "author": ["Jacob MJ Murre"], "venue": null, "citeRegEx": "Murre.,? \\Q1992\\E", "shortCiteRegEx": "Murre.", "year": 1992}, {"title": "Cufe at semeval-2016 task 4: A gated recurrent model for sentiment classification", "author": ["Mahmoud Nabil", "Mohamed Aly", "Amir F Atiya"], "venue": "Proceedings of SemEval,", "citeRegEx": "Nabil et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nabil et al\\.", "year": 2016}, {"title": "Semeval2016 task 4: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Veselin Stoyanovand", "Fabrizio Sebastiani"], "venue": "In Proc. of the 10th International Workshop on Semantic Evaluation (SemEval),", "citeRegEx": "Nakov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A deep learning and knowledge transfer based architecture for social media user characteristic determination", "author": ["Matthew Riemer", "Sophia Krasikov", "Harini Srinivasan"], "venue": "SocialNLP 2015 at NAACL, pp", "citeRegEx": "Riemer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Riemer et al\\.", "year": 2015}, {"title": "Correcting forecasts with multifactor neural attention", "author": ["Matthew Riemer", "Aditya Vempaty", "Flavio Calmon", "Fenno Heath", "Richard Hull", "Elham Khabiri"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Riemer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riemer et al\\.", "year": 2016}, {"title": "Catastrophic forgetting, rehearsal and pseudorehearsal", "author": ["Anthony Robins"], "venue": "Connection Science,", "citeRegEx": "Robins.,? \\Q1995\\E", "shortCiteRegEx": "Robins.", "year": 1995}, {"title": "Consolidation in neural networks and in the sleeping brain", "author": ["Anthony Robins"], "venue": "Connection Science,", "citeRegEx": "Robins.,? \\Q1996\\E", "shortCiteRegEx": "Robins.", "year": 1996}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Bengio.", "year": 2014}, {"title": "aspect-based sentiment analysis", "author": ["A Rusu", "Sergio Gomez Colmenarejo", "Caglar Gulcehre", "Guillaume Desjardins", "James Kirk"], "venue": "arXiv preprint arXiv:1609.02748,", "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Improved semantic representations", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q1631\\E", "shortCiteRegEx": "Tai et al\\.", "year": 1631}], "referenceMentions": [{"referenceID": 25, "context": "In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.", "startOffset": 53, "endOffset": 73}, {"referenceID": 32, "context": "While one can argue that forgetting the source task should not matter if only the target task is of interest, our paper adds to the recent empirical evidence across problem domains (Li & Hoiem, 2016),(Rusu et al., 2016) that show additional network stability can lead to empirical benefits over the fine-tuning algorithm.", "startOffset": 200, "endOffset": 219}, {"referenceID": 23, "context": "One popular approach for addressing this problem is rehearsals (Murre, 1992), (Robins, 1995).", "startOffset": 63, "endOffset": 76}, {"referenceID": 29, "context": "One popular approach for addressing this problem is rehearsals (Murre, 1992), (Robins, 1995).", "startOffset": 78, "endOffset": 92}, {"referenceID": 5, "context": "In the transfer setting it can be seen as related to multi-task learning (Caruana, 1997) where two tasks are trained at the same time, rather than sequentially, while sharing a common input encoder to a shared hidden representation.", "startOffset": 73, "endOffset": 88}, {"referenceID": 5, "context": "This technique is very sensible because while fine-tuning is susceptible to catastrophic forgetting, multi-task learning is not (Caruana, 1997).", "startOffset": 128, "endOffset": 143}, {"referenceID": 29, "context": "One compelling technique for addressing this problem is the concept of pseudorehearsals (Robins, 1995), (Robins, 1996), where relearning is performed on an artificially constructed population of pseudoitems instead of the actual old examples.", "startOffset": 88, "endOffset": 102}, {"referenceID": 30, "context": "One compelling technique for addressing this problem is the concept of pseudorehearsals (Robins, 1995), (Robins, 1996), where relearning is performed on an artificially constructed population of pseudoitems instead of the actual old examples.", "startOffset": 104, "endOffset": 118}, {"referenceID": 29, "context": "One compelling technique for addressing this problem is the concept of pseudorehearsals (Robins, 1995), (Robins, 1996), where relearning is performed on an artificially constructed population of pseudoitems instead of the actual old examples. Unfortunately, current automatic techniques in the text analytics domain have not yet mastered producing linguistically plausible data. As such, the pseudorehearsals paradigm is likely to waste computational time that could be spent on learning realistic patterns that may occur during testing. In our work, we extend the Learning without Forgetting (LwF) paradigm of (Li & Hoiem, 2016) to the text analytics domain using Recurrent Neural Networks. In this approach, the target task data is used both for learning the target task and for rehearsing information learned from the source task by leveraging synthetic examples generated for the target task input by the model that only experienced training on the source task data. As argued by Li & Hoiem (2016), this setup strikes an important balance between classification performance, computational efficiency, and simplicity in deployment.", "startOffset": 89, "endOffset": 1002}, {"referenceID": 5, "context": "One reason this is idealistic is because multi-task learning generally only works well when tasks are sampled at different rates or alternatively given different priority in the neural network loss function (Caruana, 1997).", "startOffset": 207, "endOffset": 222}, {"referenceID": 7, "context": "Moreover, we showcase that multiple expert networks trained on the target task with prior knowledge from different source tasks can be effectively combined in an ensemble and then distilled into a single GRU model (Cho et al., 2014), (Chung et al.", "startOffset": 214, "endOffset": 232}, {"referenceID": 8, "context": ", 2014), (Chung et al., 2014).", "startOffset": 9, "endOffset": 29}, {"referenceID": 11, "context": ", 2006) and (Hinton et al., 2015) showed that an ensemble of neural network classifier can be distilled into a single model, knowledge distillation from a teacher network to a student network has become a growing topic of neural network research.", "startOffset": 12, "endOffset": 33}, {"referenceID": 20, "context": "The use of distillation as a means of sharing biases from multiple tasks was explored in (Lopez-Paz et al., 2016), where the teacher network is trained with the output of the other tasks as input.", "startOffset": 89, "endOffset": 113}, {"referenceID": 6, "context": "Additionally, the concept of using distillation for knowledge transfer was also explored in (Chen et al., 2015), where function preserving transformations from smaller to bigger neural network architectures were outlined.", "startOffset": 92, "endOffset": 111}, {"referenceID": 5, "context": "In (Ba & Caruana, 2014) it was shown that a deep teacher neural network can be learned by a shallow student network. This idea was extended in (Romero et al., 2014), where it was demonstrated that a deep and narrow neural network can learn a representation that surpasses its teacher. The use of distillation as a means of sharing biases from multiple tasks was explored in (Lopez-Paz et al., 2016), where the teacher network is trained with the output of the other tasks as input. It is not obvious how to extend a recurrent neural network to best use this kind of capability over a sequence. The idea of distilling from multiple source task teachers into a student network was highlighted in the reinforcement learning setting in (Rusu et al., 2015). Additionally, the concept of using distillation for knowledge transfer was also explored in (Chen et al., 2015), where function preserving transformations from smaller to bigger neural network architectures were outlined. This technique could also provide value in some instances for our approach where wider or deeper neural networks are needed for the task being transferred to than was needed for the original task. Distillation over target task data was first proposed as a means of elevating catastrophic forgetting in sequential knowledge transfer as applied to image classification in (Li & Hoiem, 2016). We extend this approach for its first application to our knowledge for text analytics problems, with a recurrent neural network architecture, and in the setting where the source task and target task have the same output. The chief distinction of our proposed forgetting cost is that source task specific parameters are held fixed during integration with the target task as opposed to the joint training of all parameters used by Li & Hoiem (2016). Our experiments empirically support the intuition that freezing these parameters leads to greater retention of source task performance after target task integration and better generalization to the target task.", "startOffset": 9, "endOffset": 1812}, {"referenceID": 22, "context": "An ensemble over multiple diverse models trained for the same sentiment analysis task was also considered in (Mesnil et al., 2014) for the IMDB binary movie reviews sentiment dataset (Maas et al.", "startOffset": 109, "endOffset": 130}, {"referenceID": 21, "context": ", 2014) for the IMDB binary movie reviews sentiment dataset (Maas et al., 2011).", "startOffset": 60, "endOffset": 79}, {"referenceID": 27, "context": "Knowledge transfer from multiple tasks was considered to estimate the age of Twitter users based on the content of their tweets in (Riemer et al., 2015).", "startOffset": 131, "endOffset": 152}, {"referenceID": 32, "context": "Progressive neural networks (Rusu et al., 2016) is a recently proposed method very similar in motivation to our forgetting cost as it is directly trying to solve the catastrophic forgetting problem.", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "These neural-symbolic systems (Garcez et al., 2012) include early examples such as KBANN (Towell et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 14, "context": "(Hu et al., 2016) very recently also looked at the problem of distilling logical rules into a neural network text analytics classifier.", "startOffset": 0, "endOffset": 17}, {"referenceID": 14, "context": "In (Hu et al., 2016) they consider the individual rules and leverage an iterative convex optimization algorithm alongside the neural network to regularize the subspace of the network.", "startOffset": 3, "endOffset": 20}, {"referenceID": 2, "context": "In particular, the LSTM variant (Hochreiter & Schmidhuber, 1997) has become popular as it alleviates the vanishing gradients problem (Bengio et al., 1994) known to stop recurrent neural networks from learning long term dependencies over the input sequence.", "startOffset": 133, "endOffset": 154}, {"referenceID": 7, "context": "In our experiments we use the simpler GRU network (Cho et al., 2014), (Chung et al.", "startOffset": 50, "endOffset": 68}, {"referenceID": 8, "context": ", 2014), (Chung et al., 2014) that generally achieves the same accuracy despite a less complex architecture.", "startOffset": 9, "endOffset": 29}, {"referenceID": 18, "context": "A model that builds on top of GRUs with an external memory storage paradigm (Kumar et al., 2015) currently holds the state of the art on movie review sentiment analysis.", "startOffset": 76, "endOffset": 96}, {"referenceID": 26, "context": "Our GRU model was fed a sequence of fixed 300 dimensional Glove vectors (Pennington et al., 2014), representing words based on analysis of 840 billion words from a common crawl of the internet, as the input xt for all tasks.", "startOffset": 72, "endOffset": 97}, {"referenceID": 11, "context": "We distill off the soft labels as in (Hinton et al., 2015), but set our temperature fixed at 1.", "startOffset": 37, "endOffset": 58}, {"referenceID": 10, "context": "Emoticon Heuristic: Finally, we consider a semi-supervised task based on emoticon prediction motivated by the successful work in (Go et al., 2009), leveraging it in the twitter sentiment domain and its use as a vital component of the SemEval competition winning system (Bethard et al.", "startOffset": 129, "endOffset": 146}, {"referenceID": 4, "context": ", 2009), leveraging it in the twitter sentiment domain and its use as a vital component of the SemEval competition winning system (Bethard et al., 2016).", "startOffset": 130, "endOffset": 152}, {"referenceID": 4, "context": "This is multiple orders of magnitude smaller than the 90 million tweets used in (Bethard et al., 2016) to allow for quick experimentation.", "startOffset": 80, "endOffset": 102}, {"referenceID": 32, "context": "Progressive Networks: We also compare with our implementation of a progressive neural network (Rusu et al., 2016), where the representation learned for the source task is held fixed and integrated with a target task specific model via lateral connections trained using the target task data.", "startOffset": 94, "endOffset": 113}, {"referenceID": 5, "context": "This is achieved by treating the target task data and the output generated by the source task model based on the target task input data as two jointly learned tasks as in (Caruana, 1997).", "startOffset": 171, "endOffset": 186}, {"referenceID": 15, "context": "Our model is quite similar in architecture to the gating network component of a hierarchical mixture of experts model (Jacobs et al., 1991), (Jordan & Jacobs, 1994).", "startOffset": 118, "endOffset": 139}, {"referenceID": 28, "context": "(Riemer et al., 2016) suggests that a many element gating network can be improved with a sparsity constraint, but this did not work as well as the greedy strategy for our model and experiments.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "6% SwissCheese (Bethard et al., 2016) 64.", "startOffset": 15, "endOffset": 37}, {"referenceID": 16, "context": "6% NTNUSentEval (Jahren et al., 2016) 64.", "startOffset": 16, "endOffset": 37}, {"referenceID": 24, "context": "9% CUFE (Nabil et al., 2016) 63.", "startOffset": 8, "endOffset": 28}, {"referenceID": 4, "context": "The prior best model SwissCheese (Bethard et al., 2016) consists of random forests ensemble built utilizing multiple convolutional neural network models and distant supervision.", "startOffset": 33, "endOffset": 55}], "year": 2017, "abstractText": "Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks. We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.", "creator": "LaTeX with hyperref package"}}}