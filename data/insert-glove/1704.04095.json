{"id": "1704.04095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2017", "title": "Training Neural Networks Based on Imperialist Competitive Algorithm for Predicting Earthquake Intensity", "abstract": "In this expressionist study fritzsche we determined sierad neural usfs network septimania weights and biases time-weighted by hostess Imperialist Competitive Algorithm (ICA) in salwar order to 1.1875 train actor/comedian network for bosiljka predicting tethong earthquake ktrk intensity in jenei Richter. For unwind this reason, 28.06 we abaci used erhart dependent courtailler parameters like stinchfield earthquake mcmains occurrence 537,000 time, epicenter ' louviers s gafr latitude and codice longitude brayden in betuwe degree, feminize focal deadmarsh depth in conjointly kilometer, msaidie and the seismological center distance lipsyte from s.e. epicenter sabarsky and podido earthquake focal center earache in torrado kilometer which has been provided rasulov by tianhe Berkeley 180.4 data base. k'ak The blora studied neural network has roys two hidden layer: 63kg its pinnipeds first wuding layer neki has 16 lahi neurons reekie and 1,237 the veneman second layer fakery has perrysburg 24 sabih neurons. By cirstea using o'shannon ICA nipsey algorithm, generaciones average troisdorf error in-progress for yongbyong testing andresen data is s.o.b. 0. 0007 chrisye with a variance wilhite equal iamx to 0. 318. The tenderizer earthquake prediction error verdal in mitsuya Richter by MSE criteria for amatus ICA hoylake algorithm steamed is 0. breskens 101, but ermengol by using GA, the chandernagore MSE tuono value is ncsoft 0. 115.", "histories": [["v1", "Mon, 13 Feb 2017 22:42:52 GMT  (575kb,D)", "http://arxiv.org/abs/1704.04095v1", "5 pages, 6 figures"]], "COMMENTS": "5 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["mohsen moradi"], "accepted": false, "id": "1704.04095"}, "pdf": {"name": "1704.04095.pdf", "metadata": {"source": "CRF", "title": "Training Neural Networks Based on Imperialist Competitive Algorithm for Predicting Earthquake Intensity", "authors": ["Mohsen Moradi"], "emails": ["m.moradi8111@aut.ac.ir"], "sections": [{"heading": null, "text": "Index Terms\u2014earthquake intensity, multilayer perceptron, imperialist competitive algorithm, artificial neural network.\nI. INTRODUCTION\nTHE Artificial Neural Network (ANN) is constructed ofsome layers and nodes. Any network should have an output and input layers, and some other hidden networks which can be added. Any layer has some nodes which are known as neurons. Relationship between layers are based on connection between neurons. For any of these connections a weight is specified. Moreover, for any neuron, an independent parameter by the name of bias is defined. Weights and biases of the next layers are effected by a transition function, which can be linear or nonlinear and is defined by user. Neural networks have many uses in image processing, communication theory, and finance. Kaastra et al. used neural networks for forecasting time series data [1]. In the study by Shaohui et al., (2003), neural networks are used in steganography for detecting hidden images which this is done by finding statistical features [2]. Steganography is hiding secretive information in other images for deceiving people e.g. concealing substantial features of face images like texture in other pictures [3]. One of the main issues of neural network is determining proper values for weights and biases which in this paper, we use ICA for prognosticating earthquake intensity."}, {"heading": "II. RELATED WORKS", "text": "In [4], for detecting and forecasting natural disasters like earthquake, some data mining techniques have been used. By analyzing data mining approaches like logical models,\nM. Moradi was with the Department of Mathematics, Amirkabir University of Technology - Tehran Polytechnic , Tehran, Iran, ( e-mail: m.moradi8111@aut.ac.ir ).\nneural networks, Bayesian networks, and decision tree, it has been illustrated that all of these methods can be used for prognosticating earthquake, tsunami, landslides, and other microseisms.\nNegarestani used neural networks for measuring Radon density in soil by local parameters [5]. Their analysis illustrates that by neural network approach unnatural phenomenon on earth like earthquake can be distinguished by soil\u2019s Radon density.\nIn another work Hanna used a General Regression Neural Network (GRNN) for estimating soil slide potential [6]. Their data for training network belonged to two disastrous earthquakes in Turkey and Taiwan in 1999. In their research, 620 data set including twelve soil and earthquake parameters have been introduced. For enhancing accuracy, an iterative process has been applied.\nNext, Lin et al. by using a neural network approach and 955 data set related to Alishan\u2019s neighbourhood highways in Taiwan, created a model for research on earthquake [7].\nAnother research done in this area is about assessing structures vulnerability against earthquake [8]. In this study, a Bayesian framework has been introduced which can produce estimates for curves friability derivatives without considering the data values. Moreover, by using Markov Chain Monte Carlo (MCMC) in their methods, they have improved the estimates.\nSlope stability during an earthquake is an important subject in geotechnical engineering. Gordan et al. studied forecasting seismic slope stability by a hybrid Artificial Neural Network and Particle Swarm Optimization [9]. In this study, features which have an effect on safety factor are slope heights, cohesion, and friction angle of earth.\nBaddari and et.al. have used Radial Basis Function (RBF) for studying seismic data to identify and eliminate accidental noises which have been produced [10]. For avoiding local minimums, they used a backward propagation algorithm for training network.\nIII. IMPERIALIST COMPETITIVE ALGORITHM\nAtashpaz et al. have introduced ICA as an optimization goal for modeling processes [11]. The members of initial population in this algorithm are known as countries that are produced randomly. Some powerful countries which are called imperialist colonize weak countries. The population of these colonized countries will be divided to some empires. Each\nar X\niv :1\n70 4.\n04 09\n5v 1\n[ cs\n.N E\n] 1\n3 Fe\nb 20\n17\n2 empire has one imperialist and some colonies. The power of any empire is the sum of the imperialist power and a portion of its colonies powers. When the empires have been stablished, they will compete against each other. The weakest empires would be decomposed, and their colonies would join to other empires as we can see in the history. In nut shell, the strong empires would be more powerful as the result of topple of other empires. Any imperialist would help to their colonies to become stronger. The final step of algorithm is when we have only one empire in all over the world and its colonies power is close to that of imperialist.\nImperialists would attract the colonies toward themselves by helping them in some ways like improving their infrastructures and universities. In any iteration of algorithm, a colony will move toward imperialist by X unit which is a random number by uniform or arbitrary distribution. Moreover, it is possible that in any movement of colonies toward imperialist they become more powerful than their imperialist. In this case, algorithm would choose that colony as a new imperialist, and the old imperialist would become its colony. As a result, other colonies would move toward this new imperialist in the empire."}, {"heading": "IV. GENETIC ALGORITHM", "text": "Genetic Algorithm (GA) is an optimization method which had introduced by John H. Holland in the 1970s [12]. This algorithm is inspired by natural evolution laws in following steps:\na) All the creatures struggle against each other for being alive and those who are stronger have more chance to remain alive and make an adaptation.\nb) Those with high ability for adaptation survive and produce next generation. This next generation will be more advance than the previous one.\nc) Children are similar to their parents but they are not identical with them. Since not only do they inherit their parents\u00e2A\u0306Z\u0301 features, but also, due to genetic mutation, they might become more improved than their parents.\nd) After some generation, children have more adaption to the nature.\nGenetic algorithm does the same thing in a way that for implementing the algorithm, we should define the population size, how they breed, and the rate of adaptation. Defining population is based on chains composed of genes which makes GA implementing complicated. Parameters like population size, mutation rate, and crossover rate are control parameters that should be specified before running the algorithm. Moreover, for ending the process of producing the next generation, the algorithm needs an ending condition which can be the specific number of generations or minimum amount of an optimization value. The genetic algorithm steps are as follows:\n\u2022 Selection In this step, among the available chromosomes, some of them are chosen to be used in producing the next generation. Better chromosomes have more chance for selection.\n\u2022 Crossover In this step, a pair of parental chromosomes generates a new pair. Only a specific kind of parents are chosen for crossover with a probability of Pc .\n\u2022 Mutation After the crossover operation, mutation is done on the chromosomes. In this step, a chromosome\u2019s gene is selected by accident and it will be changed. The probability of mutation action on any chromosome is mutation rate and is shown by Pm .\nAfter mutation, the produced chromosomes, known as the new generation, will be used for the next round of algorithm."}, {"heading": "V. USED NEURAL NETWORK", "text": "For N points in a page like (xi, yi), 1 \u2264 i \u2264 N which are approximately around a straight line, a good approximation can be a linear line. For finding this line, the coefficients (a\u2217, b\u2217) should be so that we obtain y = a\u2217x + b\u2217. An appropriate criterion for finding suitable line is the Minimum Mean Square Error (MMSE) which means the distance of points from that line should be minimized as the equation 1.\nmin a,b N\u2211 i=1 (ei)2 (1)\nOn the other hand, if points be scattered around a cubic curve, we need at least three parameters like (a\u2217, b\u2217, c\u2217) to approximate the curve properly. If we use a linear line, the mean square error would be far higher than what is acceptable. In a situation where the points on the plane do not have a regular pattern and have a lot of maximum and minimum points, high degree curves should be used for approximation. Moreover, if the number of points are not enough or the points on plane are in high dimensional surfaces, the problem would be doubled. Therefore, we should find another solution. To overcome this, we would use Multilayer Perceptron (MLP) neural network. Generally, this neural networks are constructed by some layers which any layer also has some number of neurons.\nAny neuron has an activation function for defining its output. Also, the input of neuron is the sum of a bias and m points (x1, \u00b7 \u00b7 \u00b7 , xm) in which each of them are multiplied by (w0,w1, \u00b7 \u00b7 \u00b7 ,wm), respectively. If these weights and biases are chosen appropriately, a neuron can achieve to a better outcome.\nIn general, a network has its specific number of layers and any layer has its specific number of neurons. Moreover, the number of inputs and outputs of network are based on the research topic, and in result it can be changed. In many situations, if the number of layers is two, an appropriate\n3\nneural network can be designed. In this section, by using NGA-West2 databases provided by Berkeley [13], we design a neural network. The parameters of studied data are as follows:\n1- Occurrence time in year 2- The size of earthquake in Richter 3- Epicenter\u2019s latitude in degree 4- Epicenter\u2019s longitude in degree 5- The focal depth in kilometer 6- Epicentral distance in kilometer 7- Hypocentral distance in kilometer\nThese seven parameters are shown by a1, \u00b7 \u00b7 \u00b7 , a7. We would design a neural network by one target and a set of six inputs. The target value is the severity of magnitude of earthquake a2, and other parameters are network\u2019s input values. For obtaining better results, missing data are eliminated. The data matrix is ordered based on years from the oldest one. All the data are transferred to the (\u22121, 1) interval. Among all the data, ninety percent of it is chosen accidentally for training the neural network and remained ten per cent are used for testing it.\nMean Square Error (MSE) is used for assessment of the error. For a vector y = (y1, \u00b7 \u00b7 \u00b7 , yn) and its approximation y\u0302 = (y\u03021, \u00b7 \u00b7 \u00b7 y\u0302n) , the MSE is as follows:\ne = 1 n n\u2211 i=1 (yi \u2212 y\u0302i)2 (2)\nThe used neural network has one input and one output layer by two hidden layers. The first hidden layer has 16 neurons and the second one has 24 neurons. the program is written in a way that without changing the other parts, the number of neurons for hidden layers would be changeable. The activation function which is used for hidden layers are hyperbolic tangent sigmoid transfer functions (tansig) by following formulas:\ntansig(n) = 2 (1 + e\u22122n) \u2212 1 (3)\nIn general, sigmoid is used for all the functions which have S shape. The activation function for the output layer which has only one neuron is pure linear transfer function.\nThe used neural network is shown in figure 1. The last step is allocating weights and biases to the network. In order to do this, we should train the network in a way that the best weights and biases be chosen to minimize the MSE value. As this neural network has 6 input parameters and the number of neurons in the first hidden layer is 16, on the whole, we need 96 weights in this section. Although, as the number of neurons in the first hidden layer is 16 and in the next hidden layer is 26, we need 384 weights in this section to be aligned. Also, for moving from second layer to output, 24 weights are\nneeded. The number of biases for the first layer is 16, 24 for the second layer, and one for the output. In this neural network, in sum, we need 545 weights and biases. We use ICA algorithm to find the best amount of them. In the next section the training method will be explained.\nAs training any neural network leads to an optimization problem, one way to deal with it is using evolutionary optimization methods e.g. GA, Particle Swarm Optimization (PSO), and ICA. In this section, we will use ICA algorithm for determining neural network\u2019s weights and biases in training network. Also the result will be compared with GA-based network.\nTo solve a problem by ICA algorithm, firstly an accurate definition of countries should be presented, and then cost function should be determined. In training the neural network, these two parameters would be defined as follows:\n\u2022 Country: the set of unknown parameters like biases and weights of neural network. A country plays the role of a neural network; in other words, it plays the role of a classifier.\n\u2022 Cost function: refers to the mean square error of training data.\nThe initial parameters of ICA algorithm are indicated in table I.\nIn used ICA algorithm, the number of iterations is 200 and the number of members of the initial population is 1000. The goal of algorithm is allocating best values for weights and biases in minimizing defined MSE for training data. The best achieved MSE for training data by using ICA algorithm is 0.097. The difference between the target data and output of the training data in network when zoomed in on, has been indicated in the figure 2.\nAverage error for training data is equal to -0.008 and its variance is 0.312. The due histogram has been shown in the figure 3.\nCorrelation between training output data and the training target data is equal to 0.795, MSE=0.097, and RMSE=0.312 that RMSE is the square root of MSE. A portion of error changes and this correlation has shown in figure 4.\nSimilarly, the correlation, error function histogram, MSE, RMSE, and comparison of target values and the neural network output information of the test data has been shown in figure 5. In this figure, for better vision of error figure and difference between test data and neural network output we\n4\nzoomed in on.\nNeural network testing reveals that for input data how much we can predict the magnitude and intensity of an earthquake by Richter. This prediction\u2019s MSE is equal to 0.101 and the correlation between this predicted values by using input parameters is equal to 0.782.\nIn a similar way, we trained the same neural network by genetic algorithm with equivalent structure as we did for ICA algorithm. For GA, the population size is 1000 and algorithm iteration is equal to 200. In any iteration 15 percent of data\nis selected as the elite population, 50 percent for crossover, and remained 35 percent for mutation. Similarly, the goal of algorithm is allocating the best weights and biases parameters to minimize MSE. After training the neural network by GA algorithm, it will be tested. The result for testing neural network is shown in figure 6.\nAs it can be seen in figure 6, the obtained correlation is equal to 0.761 and MSE criteria for test data is 0.115. Therefore, training neural network by using ICA algorithms in figure 5 not only has a higher speed, but also it conducts better in training data by comparison with GA."}, {"heading": "VI. CONCLUSION", "text": "We have used MATLAB R2016b for designing a neural network for predicting intensity of earthquake. Neural network conducts very well in fitting functions and a neural network, also a simple one, can approximate very well any function. For defining the earthquake intensity prediction problem, we determined 6 input parameters as an input matrix, and then specified the earthquake intensity parameter as target variable. For assessing neural network, the MSE measure and regression analysis has been used. Designing artificial neural network in this way has not the ability for choosing parameters like the number of hidden layers. For this reason, after designing the primarily neural network with two hidden layers, the number of neurons of each layer and the kind of training functions has been changed to determine the effects of those parameters by using which ICA algorithms we allocate weights and biases values of the network and it has been compared with a neural network trained by GA. As we obtained, ICA was better than GA for training this network."}, {"heading": "VII. ACKNOWLEDGMENT", "text": "We take immense gratitude to Iran Seismological Center for providing a situation for making a research about application of neural network in seismology."}], "references": [{"title": "Designing a neural network for forecasting financial and economic time series", "author": ["I. Kaastra", "M. Boyd"], "venue": "Neurocomputing, vol.10, no.3, pp.215-236, 1996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Neural network based steganalysis in still images", "author": ["L. Shaohui", "Y. Hongxun", "G. Wen"], "venue": "Multimedia and Expo, 2003. ICME\u201903. Proceedings. 2003 International Conference on, vol.2, pp.II-509, IEEE, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Combining and steganography of 3d face textures", "author": ["M. Moradi"], "venue": "arXiv preprint arXiv:1702.01325, 2017.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2017}, {"title": "A review of application of data mining in earthquake prediction", "author": ["G. Otari", "R. Kulkarni"], "venue": "International Journal of Computer Science and Information Technologies, vol.3, no.2, pp.3570-3574, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Layered neural networks based analysis of radon concentration and environmental parameters in earthquake prediction", "author": ["A. Negarestani", "S. Setayeshi", "M. Ghannadi-Maragheh", "B. Akashe"], "venue": "Journal of environmental radioactivity, vol.62, no.3, pp.225-233, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural network model for liquefaction potential in soil deposits using turkey and taiwan earthquake data", "author": ["A.M. Hanna", "D. Ural", "G. Saygili"], "venue": "Soil Dynamics and Earthquake Engineering, vol.27, no.6, pp.521- 540, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Neural networkbased model for assessing failure potential of highway slopes in the alishan, taiwan area: Pre-and post-earthquake investigation", "author": ["H.-M. Lin", "S.-K. Chang", "J.-H. Wu", "C.H. Juang"], "venue": "Engineering Geology, vol.104, no.3, pp.280-289, 2009.  5 Fig. 5. test data evaluation of trained neural network by ICA algorithm Fig. 6. test data evaluation of trained neural network by genetic algorithm", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Assessing structural vulnerability against earthquakes using multi-dimensional fragility surfaces: a bayesian framework", "author": ["P. Koutsourelakis"], "venue": "Probabilistic Engineering Mechanics, vol.25, no.1, pp.49-60, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Prediction of seismic slope stability through combination of particle swarm optimization and neural network", "author": ["B. Gordan", "D.J. Armaghani", "M. Hajihassani", "M. Monjezi"], "venue": "Engineering with Computers, vol.32, no.1, pp.85-97, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Application of a radial basis function artificial neural network to seismic data inversion", "author": ["K. Baddari", "T. Aifa", "N. Djarfour", "J. Ferahtia"], "venue": "Computers and Geosciences, vol.35, no.12, pp.2338-2344, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Imperialist competitive algorithm:  an algorithm for optimization inspired by imperialistic competition", "author": ["E. Atashpaz-Gargari", "C. Lucas"], "venue": "Evolutionary computation, 2007. CEC 2007. IEEE Congress on, pp.4661- 4667, IEEE, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence.", "author": ["J.H. Holland"], "venue": "MIT press,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1992}], "referenceMentions": [{"referenceID": 0, "context": "used neural networks for forecasting time series data [1].", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": ", (2003), neural networks are used in steganography for detecting hidden images which this is done by finding statistical features [2].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "concealing substantial features of face images like texture in other pictures [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "In [4], for detecting and forecasting natural disasters like earthquake, some data mining techniques have been used.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Negarestani used neural networks for measuring Radon density in soil by local parameters [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "In another work Hanna used a General Regression Neural Network (GRNN) for estimating soil slide potential [6].", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": "by using a neural network approach and 955 data set related to Alishan\u2019s neighbourhood highways in Taiwan, created a model for research on earthquake [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "Another research done in this area is about assessing structures vulnerability against earthquake [8].", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "studied forecasting seismic slope stability by a hybrid Artificial Neural Network and Particle Swarm Optimization [9].", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "have used Radial Basis Function (RBF) for studying seismic data to identify and eliminate accidental noises which have been produced [10].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "have introduced ICA as an optimization goal for modeling processes [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "Holland in the 1970s [12].", "startOffset": 21, "endOffset": 25}], "year": 2017, "abstractText": "In this study we determined neural network weights and biases by Imperialist Competitive Algorithm (ICA) in order to train network for predicting earthquake intensity in Richter. For this reason, we used dependent parameters like earthquake occurrence time, epicenter\u2019s latitude and longitude in degree, focal depth in kilometer, and the seismological center distance from epicenter and earthquake focal center in kilometer which has been provided by Berkeley data base. The studied neural network has two hidden layer: its first layer has 16 neurons and the second layer has 24 neurons. By using ICA algorithm, average error for testing data is 0.0007 with a variance equal to 0.318. The earthquake prediction error in Richter by MSE criteria for ICA algorithm is 0.101, but by using GA, the MSE value is 0.115.", "creator": "LaTeX with hyperref package"}}}