{"id": "1704.04451", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Optimizing Differentiable Relaxations of Coreference Evaluation Metrics", "abstract": "abelda Coreference evaluation damsel metrics are hard kettledrums to optimize iijima directly nepotistic as 100-96 they are non - differentiable functions, dellys not easily decomposable quann into tommorrow elementary mid-level decisions. artic Consequently, most likeness approaches 39.81 optimize coot objectives only indirectly related deduces to the end minus-15 goal, ninnies resulting cingulate in yousry suboptimal ubaida performance. hairy Instead, d'cunha we propose a griet differentiable relaxation silversea that halon lends jasjit itself 28min to gradient - bioelectronics based 1.2823 optimisation, rydwelski thus venerating bypassing the need for reinforcement dreariest learning or 43.38 heuristic iheu modification temerloh of cross - entropy. We theologiae show bardsley that by modifying drue the 482 training buddhaghosa objective of essling a ass competitive neural cedel coreference system, we tafsir obtain clearwater a substantial kumari gain in cond performance. tsuge This 3,990 suggests that marona our dragi\u0161a approach mallock can be bensakhria regarded two-hundred as a viable alternative to using reinforcement shenzhen learning cowle or barcelonnette more engendering computationally expensive 2,736 imitation learning.", "histories": [["v1", "Fri, 14 Apr 2017 15:22:51 GMT  (80kb,D)", "https://arxiv.org/abs/1704.04451v1", "10 pages"], ["v2", "Mon, 17 Apr 2017 08:30:32 GMT  (81kb,D)", "http://arxiv.org/abs/1704.04451v2", "10 pages"], ["v3", "Thu, 22 Jun 2017 07:55:47 GMT  (85kb,D)", "http://arxiv.org/abs/1704.04451v3", "10 pages. CoNLL"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["phong le", "ivan titov"], "accepted": false, "id": "1704.04451"}, "pdf": {"name": "1704.04451.pdf", "metadata": {"source": "CRF", "title": "Optimizing Differentiable Relaxations of Coreference Evaluation Metrics", "authors": ["Phong Le", "Ivan Titov"], "emails": ["p.le@uva.nl", "ititov@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Coreference resolution is the task of identifying all mentions which refer to the same entity in a document. It has been shown beneficial in many natural language processing (NLP) applications, including question answering (Hermann et al., 2015) and information extraction (Kehler, 1997), and often regarded as a prerequisite to any text understanding task.\nCoreference resolution can be regarded as a clustering problem: each cluster corresponds to a single entity and consists of all its mentions in a given text. Consequently, it is natural to evaluate predicted clusters by comparing them with the ones annotated by human experts, and this is exactly what the standard metrics (e.g., MUC, B3, CEAF) do. In contrast, most state-of-theart systems are optimized to make individual co-\nreference decisions, and such losses are only indirectly related to the metrics.\nOne way to deal with this challenge is to optimize directly the non-differentiable metrics using reinforcement learning (RL), for example, relying on the REINFORCE policy gradient algorithm (Williams, 1992). However, this approach has not been very successful, which, as suggested by Clark and Manning (2016a), is possibly due to the discrepancy between sampling decisions at training time and choosing the highest ranking ones at test time. A more successful alternative is using a \u2018roll-out\u2019 stage to associate cost with possible decisions, as in Clark and Manning (2016a), but it is computationally expensive. Imitation learning (Ma et al., 2014b; Clark and Manning, 2015), though also exploiting metrics, requires access to an expert policy, with exact policies not directly computable for the metrics of interest.\nIn this work, we aim at combining the best of both worlds by proposing a simple method that can turn popular coreference evaluation metrics into differentiable functions of model parameters. As we show, this function can be computed recursively using scores of individual local decisions, resulting in a simple and efficient estimation procedure. The key idea is to replace nondifferentiable indicator functions (e.g. the member function I(m \u2208 S)) with the corresponding posterior probabilities (p(m \u2208 S)) computed by the model. Consequently, non-differentiable functions used within the metrics (e.g. the set size function |S| = \u2211 m I(m \u2208 S)) become differ-\nentiable (|S|c = \u2211\nm p(m \u2208 S)). Though we assume that the scores of the underlying statistical model can be used to define a probability model, we show that this is not a serious limitation. Specifically, as a baseline we use a probabilistic version of the neural mention-ranking\nar X\niv :1\n70 4.\n04 45\n1v 3\n[ cs\n.C L\n] 2\n2 Ju\nn 20\n17\nmodel of Wiseman et al. (2015b), which on its own outperforms the original one and achieves similar performance to its global version (Wiseman et al., 2016). Importantly when we use the introduced differentiable relaxations in training, we observe a substantial gain in performance over our probabilistic baseline. Interestingly, the absolute improvement (+0.52) is higher than the one reported in Clark and Manning (2016a) using RL (+0.05) and the one using reward rescaling1 (+0.37). This suggests that our method provides a viable alternative to using RL and reward rescaling.\nThe outline of our paper is as follows: we introduce our neural resolver baseline and the B3 and LEA metrics in Section 2. Our method to turn a mention ranking resolver into an entity-centric resolver is presented in Section 3, and the proposed differentiable relaxations in Section 4. Section 5 shows our experimental results."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Neural mention ranking", "text": "In this section we introduce neural mention ranking, the framework which underpins current stateof-the-art models (Clark and Manning, 2016a). Specifically, we consider a probabilistic version of the method proposed by Wiseman et al. (2015b). In experiments we will use it as our baseline.\nLet (m1,m2, ..,mn) be the list of mentions in a document. For each mentionmi, let ai \u2208 {1, ..., i} be the index of the mention that mi is coreferent with (if ai = i, mi is the first mention of some entity appearing in the document). As standard in coreference resolution literature, we will refer to mai as an antecedent of mi.\n2 Then, in mention ranking the goal is to score antecedents of a mention higher than any other mentions, i.e., if s is the scoring function, we require s(ai = j) > s(ai = k) for all j, k such that mi and mj are coreferent but mi and mk are not.\nLet \u03c6a(mi) \u2208 Rda and \u03c6p(mi,mj) \u2208 Rdp be respectively features of mi and features of pair\n1Reward rescaling is a technique that computes error values for a heuristic loss function based on the reward difference between the best decision according to the current model and the decision leading to the highest metric score.\n2This slightly deviates from the definition of antecedents in linguistics (Crystal, 1997).\n(mi,mj). The scoring function is defined by:\ns(ai = j) = u T [ ha(mi) hp(mi,mj) ] + u0 if j < i\nvTha(mi) + v0 if j = i\nwhere\nha(mi) = tanh(Wa\u03c6a(mi) + ba)\nhp(mi,mj) = tanh(Wp\u03c6p(mi,mj) + bp)\nand u,v,Wa,Wp,ba,bp are real vectors and matrices with proper dimensions, u0, v0 are real scalars.\nUnlike Wiseman et al. (2015b), where the maxmargin loss is used, we define a probabilistic model. The probability3 that mi and mj are coreferent is given by\np(ai = j) = exp{s(ai = j)}\u2211i j\u2032=1 exp{s(ai = j\u2032)}\n(1)\nFollowing Durrett and Klein (2013) we use the following softmax-margin (Gimpel and Smith, 2010) loss function: L(\u0398) = \u2212 n\u2211 i=1 log ( \u2211 j\u2208C(mi) p\u2032(ai = j) ) +\u03bb||\u0398||1,\nwhere \u0398 are model parameters, C(mi) is the set of the indices of correct antecedents of mi, and p\u2032(ai = j) \u221d p(ai = j)e\u2206(j,C(mi)). \u2206 is a cost function used to manipulate the contribution of different error types to the loss function:\n\u2206(j, C(mi)) =  \u03b11 if j 6= i \u2227 i \u2208 C(mi) \u03b12 if j = i \u2227 i /\u2208 C(mi) \u03b13 if j 6= i \u2227 j /\u2208 C(mi) 0 otherwise\nThe error types are \u201cfalse anaphor\u201d, \u201cfalse new\u201d, \u201cwrong link\u201d, and \u201cno mistake\u201d, respectively. In our experiments, we borrow their values from Durrett and Klein (2013): (\u03b11, \u03b12, \u03b13) = (0.1, 3, 1). In the subsequent discussion, we refer to the loss as mention-ranking heuristic cross entropy.\n3For the sake of readability, we do not explicitly mark in our notation that all the probabilities are conditioned on the document (e.g., the mentions) and dependent on model parameters."}, {"heading": "2.2 Evaluation Metrics", "text": "We use five most popular metrics4,\n\u2022 MUC (Vilain et al., 1995),\n\u2022 B3 (Bagga and Baldwin, 1998),\n\u2022 CEAFm, CEAFe (Luo, 2005),\n\u2022 BLANC (Luo et al., 2014),\n\u2022 LEA (Moosavi and Strube, 2016).\nfor evaluation. However, because MUC is the least discriminative metric (Moosavi and Strube, 2016), whereas CEAF is slow to compute, out of the five most popular metrics we incorporate into our loss only B3. In addition, we integrate LEA, as it has been shown to provide a good balance between discriminativity and interpretability.\nLet G = {G1, G2, ..., GN} and S = {S1, S2, ..., SM} be the gold-standard entity set and an entity set given by a resolver. Recall that an entity is a set of mentions. The recall and precision of the B3 metric is computed by:\nRB3 =\n\u2211N v=1 \u2211M u=1\n|Gv\u2229Su|2 |Gv |\u2211N\nv=1 |Gv|\nPB3 =\n\u2211M u=1 \u2211N v=1\n|Gv\u2229Su|2 |Su|\u2211M\nu=1 |Su|\nThe LEA metric is computed as:\nRLEA =\n\u2211N v=1 ( |Gv| \u00d7 \u2211M u=1 link(Gv\u2229Su) link(Gv) )\u2211N v=1 |Gv|\nPLEA =\n\u2211M u=1 ( |Su| \u00d7 \u2211N v=1 link(Gv\u2229Su) link(Su) )\u2211M u=1 |Su|\nwhere link(E) = |E| \u00d7 (|E| \u2212 1)/2 is the number of coreference links in entity E. F\u03b2 , for both metrics, is defined by:\nF\u03b2 = (1 + \u03b2 2) P \u00d7R \u03b22P +R\n\u03b2 = 1 is used in the standard evaluation.\n4All are implemented in Pradhan et al. (2014), https://github.com/conll/ reference-coreference-scorers."}, {"heading": "3 From mention ranking to entity centricity", "text": "Mention-ranking resolvers do not explicitly provide information about entities/clusters which is required by B3 and LEA. We therefore propose a simple solution that can turn a mention-ranking resolver into an entity-centric one.\nFirst note that in a document containing n mentions, there are n potential entities E1, E2, ..., En where Ei has mi as the first mention. Let p(mi \u2208 Eu) be the probability that mention mi corresponds to entity Eu. We now show that it can be computed recursively based on p(ai = j) as follows:\np(mi \u2208 Eu) = \u2211i\u22121 j=u p(ai = j)\u00d7 p(mj \u2208 Eu) if u < i p(ai = i) if u = i 0 if u > i\nIn other words, if u < i, we consider all possible mj with which mi can be coreferent, and which can correspond to entity Eu. If u = i, the link to be considered is the mi\u2019s self-link. And, if u > i, the probability is zero, as it is impossible formi to be assigned to an entity introduced only later. See Figure 1 for extra information.\nWe now turn to two crucial questions about this formula:\n\u2022 Is p(mi \u2208 \u2022) a valid probability distribution?\n\u2022 Is it possible for a mention mu to be mostly anaphoric (i.e. p(mu \u2208 Eu) is low) but for the corresponding cluster Eu to be highly\nprobable (i.e. p(mi \u2208 Eu) is high for some i)?\nThe first question is answered in Proposition 1. The second question is important because, intuitively, when a mention mu is anaphoric, the potential entity Eu does not exist. We will show that the answer is \u201cNo\u201d by proving in Proposition 2 that the probability that mu is anaphoric is always higher than any probability that mi, i > u refers to Eu. Proposition 1. p(mi \u2208 \u2022) is a valid probability distribution, i.e., \u2211n u=1 p(mi \u2208 Eu) = 1, for all i = 1, ..., n.\nProof. We prove this proposition by induction. Basis: it is obvious that \u2211n u=1 p(m1 \u2208 Eu) = p(a1 = 1) = 1. Assume that \u2211n u=1 p(mj \u2208 Eu) = 1 for all j < i. Then,\ni\u22121\u2211 u=1 p(mi \u2208 Eu)\n= i\u22121\u2211 u=1 i\u22121\u2211 j=u p(ai = j)\u00d7 p(mj \u2208 Eu)\nBecause p(mj \u2208 Eu) = 0 for all j < u, this expression is equal to\ni\u22121\u2211 u=1 i\u22121\u2211 j=1 p(ai = j)\u00d7 p(mj \u2208 Eu)\n= i\u22121\u2211 j=1 p(ai = j)\u00d7 i\u22121\u2211 u=1 p(mj \u2208 Eu)\n= i\u22121\u2211 j=1 p(ai = j)\nTherefore,\nn\u2211 u=1 p(mi \u2208 Eu) = i\u22121\u2211 j=1 p(ai = j)+p(ai = i) = 1\n(according to Equation 1).\nProposition 2. p(mi \u2208 Eu) \u2264 p(mu \u2208 Eu) for all i > u.\nProof. We prove this proposition by induction. Basis: for i = u+ 1,\np(mu+1 \u2208 Eu) = p(au+1 = u)\u00d7 p(mu \u2208 Eu) \u2264 p(mu \u2208 Eu)\nAssume that p(mj \u2208 Eu) \u2264 p(mu \u2208 Eu) for all j \u2265 u and j < i. Then\np(mi \u2208 Eu) = i\u22121\u2211 j=u p(ai = j)\u00d7 p(mj \u2208 Eu)\n\u2264 i\u22121\u2211 j=u p(ai = j)\u00d7 p(mu \u2208 Eu)\n\u2264 p(mu \u2208 Eu)\u00d7 i\u2211\nj=1\np(ai = j)\n= p(mu \u2208 Eu)"}, {"heading": "3.1 Entity-centric heuristic cross entropy loss", "text": "Having p(mi \u2208 Eu) computed, we can consider coreference resolution as a multiclass prediction problem. An entity-centric heuristic cross entropy loss is thus given below:\nLec(\u0398) = \u2212 n\u2211 i=1 log p\u2032(mi \u2208 Ee(mi)) + \u03bb||\u0398||1\nwhere Ee(mi) is the correct entity that mi belongs to, p\u2032(mi \u2208 Eu) \u221d p(mi \u2208 Eu)e\u0393(u,e(mi)). Similar to \u2206 in the mention-ranking heuristic loss in Section 2.1, \u0393 is a cost function used to manipulate the contribution of the four different error types (\u201cfalse anaphor\u201d, \u201cfalse new\u201d, \u201cwrong link\u201d, and \u201cno mistake\u201d):\n\u0393(u, e(mi)) = \u03b31 if u 6= i \u2227 e(mi) = i \u03b32 if u = i \u2227 e(mi) 6= i \u03b33 if u 6= e(mi) \u2227 u 6= i \u2227 e(mi) 6= i 0 otherwise"}, {"heading": "4 From non-differentiable metrics to differentiable losses", "text": "There are two functions used in computing B3 and LEA: the set size function |.| and the link function link(.). Because both of them are non-differentiable, the two metrics are nondifferentiable. We thus need to make these two functions differentiable.\nThere are two remarks. Firstly, both functions can be computed using the indicator function\nI(mi \u2208 Su):\n|Su| = n\u2211 i=1 I(mi \u2208 Su)\nlink(Su) = \u2211 j<i I(mi \u2208 Su)\u00d7 I(mj \u2208 Su)\nSecondly, given \u03c0i,u = log p(mi \u2208 Su), the indicator function I(mi \u2208 Su\u2217), u\u2217 = arg maxu p(mi \u2208 Su) is the converging point of the following softmax as T \u2192 0 (see Figure 2):\np(mi \u2208 Su;T ) = exp{\u03c0i,u/T}\u2211 v exp{\u03c0i,v/T}\nwhere T is called temperature (Kirkpatrick et al., 1983).\nTherefore, we propose to represent each Su as a soft-cluster:\nSu = {p(m1 \u2208 Eu;T ), ..., p(mn \u2208 Eu;T )}\nwhere, as defined in Section 3, Eu is the potential entity that has mu as the first mention. Replacing the indicator function I(mi \u2208 Su) by the probability distribution p(mi \u2208 Eu;T ), we then have a differentiable version for the set size function and the link function:\n|Su|d = n\u2211 i=1 p(mi \u2208 Eu;T )\nlinkd(Su) = \u2211 j<i p(mi \u2208 Eu;T )\u00d7 p(mj \u2208 Eu;T )\n|Gv\u2229Su|d and linkd(Gv\u2229Su) are computed similarly with the constraint that only mentions in Gv are taken into account. Plugging these functions into precision and recall of B3 and LEA in Section 2.2, we obtain differentiable F\u0302\u03b2,B3 and F\u0302\u03b2,LEA, which are then used in two loss functions:\nL\u03b2,B3(\u0398;T ) = \u2212F\u0302\u03b2,B3(\u0398;T ) + \u03bb||\u0398||1 L\u03b2,LEA(\u0398;T ) = \u2212F\u0302\u03b2,LEA(\u0398;T ) + \u03bb||\u0398||1\nwhere \u03bb is the hyper-parameter of the L1 regularization terms.\nIt is worth noting that, as T \u2192 0, F\u0302\u03b2,B3 \u2192 F\u03b2,B3 and F\u0302\u03b2,LEA \u2192 F\u03b2,LEA.5 Therefore, when training a model with the proposed losses, we can start at a high temperature (e.g., T = 1) and anneal to a small but non-zero temperature. However, in our experiments we fix T = 1. Annealing is left for future work."}, {"heading": "5 Experiments", "text": "We now demonstrate how to use the proposed differentiable B3 and LEA to train a coreference resolver. The source code and trained models are available at https://github.com/ lephong/diffmetric_coref.\nSetup\nWe run experiments on the English portion of CoNLL 2012 data (Pradhan et al., 2012) which consists of 3,492 documents in various domains and formats. The split provided in the CoNLL 2012 shared task is used. In all our resolvers, we use not the original features of Wiseman et al. (2015b) but their slight modification described in Wiseman et al. (2016) (section 6.1).6\nResolvers\nWe build following baseline and three resolvers:\n\u2022 baseline: the resolver presented in Section 2.1. We use the identical configuration as in Wiseman et al. (2016): Wa \u2208 R200\u00d7da , Wp \u2208 R700\u00d7dp , \u03bb = 10\u22126 (where da, dp are respectively the numbers of mention features and pair-wise features). We also employ their pretraining methodology.\n5We can easily prove this using the algebraic limit theorem.\n6https://github.com/swiseman/nn_coref/\n\u2022 Lec: the resolver using the entity-centric cross entropy loss introduced in Section 3.1. We set (\u03b31, \u03b32, \u03b33) = (\u03b11, \u03b12, \u03b13) = (0.1, 3, 1).\n\u2022 L\u03b2,B3 and L\u03b2,LEA: the resolvers using the losses proposed in Section 4. \u03b2 is tuned on the development set by trying each value in { \u221a 0.8, 1, \u221a 1.2, \u221a 1.4, \u221a 1.6, \u221a 1.8, 1.5, 2}.\nTo train these resolvers we use AdaGrad (Duchi et al., 2011) to minimize their loss functions with the learning rate tuned on the development set and with one-document mini-batches. Note that we use the baseline as the initialization point to train the other three resolvers."}, {"heading": "5.1 Results", "text": "We firstly compare our resolvers against Wiseman et al. (2015b) and Wiseman et al. (2016). Results are shown in the first half of Table 1. Our baseline surpasses Wiseman et al. (2015b). It is likely due to using features from Wiseman et al. (2016). Using the entity-centric heuristic cross entropy loss and the relaxations are clearly beneficial: Lec is slightly better than our baseline and on par with the global model of Wiseman et al. (2016). L\u03b2=1,B3 , L\u03b2=1,LEA outperform the baseline, the global model of Wiseman et al. (2016), and Lec. However, the best values of \u03b2 are \u221a 1.4,\u221a\n1.8 respectively for L\u03b2,B3 , and L\u03b2,LEA. Among these resolvers, L\u03b2= \u221a 1.8,LEA achieves the highest F1 scores across all the metrics except BLANC. When comparing to Clark and Manning (2016a) (the second half of Table 1), we can see that the absolute improvement over the baselines (i.e. \u2018heuristic loss\u2019 for them and the heuristic cross entropy loss for us) is higher than that of reward rescaling but with much shorter training time: +0.37 (7 days7) and +0.52 (15 hours) on the CoNLL metric for Clark and Manning (2016a) and ours, respectively. It is worth noting that our absolute scores are weaker than these of Clark and Manning (2016a), as they build on top of a similar but stronger mention-ranking baseline, which employs deeper neural networks and requires a much larger number of epochs to train (300 epochs, including pretraining). For the purpose of illustrating the proposed losses, we started with a simpler model by Wiseman et al. (2015b) which requires\n7As reported in https://github.com/ clarkkev/deep-coref\na much smaller number of epochs, thus faster, to train (20 epochs, including pretraining)."}, {"heading": "5.2 Analysis", "text": "Table 2 shows the breakdown of errors made by the baseline and our resolvers on the development set. The proposed resolvers make fewer \u201cfalse anaphor\u201d and \u201cwrong link\u201d errors but more \u201cfalse new\u201d errors compared to the baseline. This suggests that loss optimization prevents over-clustering, driving the precision up: when antecedents are difficult to detect, the self-link (i.e., ai = i) is chosen. When \u03b2 increases, they make more \u201cfalse anaphor\u201d and \u201cwrong link\u201d errors but less \u201cfalse new\u201d errors.\nIn Figure 3(a) the baseline, but not L\u03b2=1,B3 nor L\u03b2= \u221a 1.4,B3 , mistakenly links 17[it] with 13[the virus]. Under-clustering, on the other hand, is a problem for our resolvers with \u03b2 = 1: in example (b), L\u03b2=1,B3 missed 165[We]. This behaviour results in a reduced recall but the recall is not damaged severely, as we still obtain a better F1 score. We conjecture that this behaviour is a consequence of using the F1 score in the objective, and, if undesirable, F\u03b2 with \u03b2 > 1 can be used instead. For instance, also in Figure 3, L\u03b2= \u221a 1.4,B3 correctly detects 17[it] as non-anaphoric and links 165[We] with 157[our].\nFigure 4 shows recall, precision, F1 (average of MUC, B3, CEAFe), on the development set when training with L\u03b2,B3 and L\u03b2,LEA. As expected, higher values of \u03b2 yield lower precisions but higher recalls. In contrast, F1 increases until\nreaching the highest point when \u03b2 = \u221a\n1.4 \u2248 1.18 for L\u03b2,B3 (\u03b2 = \u221a 1.8 \u2248 1.34 for L\u03b2,LEA), it then decreases gradually."}, {"heading": "5.3 Discussion", "text": "Because the resolvers are evaluated on F1 score metrics, it should be that L\u03b2,B3 and L\u03b2,LEA perform the best with \u03b2 = 1. Figure 4 and Table 1 however do not confirm that: \u03b2 should be set with values a little bit larger than 1. There are two hypotheses. First, the statistical difference between the training set and the development set leads to the case that the optimal \u03b2 on one set can be suboptimal on the other set. Second, in our experiments we fix T = 1, meaning that the relaxations might not be close to the true evaluation metrics enough. Our future work, to confirm/reject this, is to use annealing, i.e., gradually decreasing T down to (but larger than) 0.\nTable 1 shows that the difference betweenL\u03b2,B3 and L\u03b2,LEA in terms of accuracy is not substan-\ntial (although the latter is slightly better than the former). However, one should expect that L\u03b2,B3 would outperform L\u03b2,LEA on B3 metric while it would be the other way around on LEA metric. It turns out that, B3 and LEA behave quite similarly in non-extreme cases. We can see that in Figure 2, 4, 5, 6, 7 in Moosavi and Strube (2016)."}, {"heading": "6 Related work", "text": "Mention ranking and entity centricity are two main streams in the coreference resolution literature. Mention ranking (Denis and Baldridge, 2007; Durrett and Klein, 2013; Martschat and Strube, 2015; Wiseman et al., 2015a) considers local and independent decisions when choosing a correct antecedent for a mention. This approach is computationally efficient and currently dominant with state-of-the-art performance (Wiseman et al., 2016; Clark and Manning, 2016a). Wiseman et al. (2015b) propose to use simple neural\nnetworks to compute mention ranking scores and to use a heuristic loss to train the model. Wiseman et al. (2016) extend this by employing LSTMs to compute mention-chain representations which are then used to compute ranking scores. They call these representations global features. Clark and Manning (2016a) build a similar resolver as in Wiseman et al. (2015b) but much stronger thanks to deeper neural networks and \u201cbetter mention detection, more effective, hyperparameters, and more epochs of training\u201d. Furthermore, using reward rescaling they achieve the best performance in the literature on the English and Chinese portions of the CoNLL 2012 dataset. Our work is built upon mention ranking by turning a mentionranking model into an entity-centric one. It is worth noting that although we use the model proposed by Wiseman et al. (2015b), any mentionranking models can be employed.\nEntity centricity (Wellner and McCallum, 2003; Poon and Domingos, 2008; Haghighi and Klein, 2010; Ma et al., 2014a; Clark and Manning, 2016b), on the other hand, incorporates entitylevel information to solve the problem. The approach can be top-down as in Haghighi and Klein (2010) where they propose a generative model. It can also be bottom-up by merging smaller clusters into bigger ones as in Clark and Manning (2016b). The method proposed by Ma et al. (2014a) greedily and incrementally adds mentions to previously built clusters using a prune-and-score technique. Importantly, employing imitation learning these two methods can optimize the resolvers directly on evaluation metrics. Our work is similar to Ma et al. (2014a) in the sense that our resolvers incrementally add mentions to previously built clusters.\nHowever, different from both Ma et al. (2014a); Clark and Manning (2016b), our resolvers do not use any discrete decisions (e.g., merge operations). Instead, they seamlessly compute the probability that a mention refers to an entity from mentionranking probabilities, and are optimized on differentiable relaxations of evaluation metrics.\nUsing differentiable relaxations of evaluation metrics as in our work is related to a line of research in reinforcement learning where a nondifferentiable action-value function is replaced by a differentiable critic (Sutton et al., 1999; Silver et al., 2014). The critic is trained so that it is as close to the true action-value function as possible. This technique is applied to machine translation (Gu et al., 2017) where evaluation metrics (e.g., BLUE) are non-differentiable. A disadvantage of using critics is that there is no guarantee that the critic converges to the true evaluation metric given finite training data. In contrast, our differentiable relaxations do not need to train, and the convergence is guaranteed as T \u2192 0."}, {"heading": "7 Conclusions", "text": "We have proposed\n\u2022 a method for turning any mention-ranking resolver into an entity-centric one by using a recursive formula to combine scores of individual local decisions, and\n\u2022 differentiable relaxations for two coreference evaluation metrics, B3 and LEA.\nExperimental results show that our approach outperforms the resolver by Wiseman et al. (2016), and gains a higher improvement over the baseline\nthan that of Clark and Manning (2016a) but with much shorter training time."}, {"heading": "Acknowledgments", "text": "We would like to thank Raquel Ferna\u0301ndez, Wilker Aziz, Nafise Sadat Moosavi, and anonymous reviewers for their suggestions and comments. The project was supported by the European Research Council (ERC StG BroadSem 678254), the Dutch National Science Foundation (NWO VIDI 639.022.518) and an Amazon Web Services (AWS) grant."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Coreference evaluation metrics are hard to optimize directly as they are nondifferentiable functions, not easily decomposable into elementary decisions. Consequently, most approaches optimize objectives only indirectly related to the end goal, resulting in suboptimal performance. Instead, we propose a differentiable relaxation that lends itself to gradient-based optimisation, thus bypassing the need for reinforcement learning or heuristic modification of cross-entropy. We show that by modifying the training objective of a competitive neural coreference system, we obtain a substantial gain in performance. This suggests that our approach can be regarded as a viable alternative to using reinforcement learning or more computationally expensive imitation learning.", "creator": "LaTeX with hyperref package"}}}