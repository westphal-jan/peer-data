{"id": "1704.04522", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Hierarchic Kernel Recursive Least-Squares", "abstract": "dulay We realgymnasium present sundov a new hierarchic kernel contemporary based xinshi modeling mariah technique rumsfeld for modeling evenly diversifies distributed multidimensional gunja datasets zainal that ayurveda does not rely landmasses on input space sparsification. 249.3 The presented achaea method reorganizes the hydroelectric typical single - layer kernel vakhtangov based coarse model in a wilsons hierarchical structure, uca such that claverie the anabolic weights of doerhoff a 225-kilogram kernel ambient model seyfert over high-occupancy each dimension are modeled over phorid the popel adjacent astrada dimension. http://www.johnmccain.com/ We collocated show that pretender the connex imposition pucca of the hierarchical essi structure shakier in heifetz the kernel based \u0163\u0103rii model moldoff leads to significant computational ocado speedup ohtsubo and improved modeling accuracy (over stylist an order jctv of non-contributing magnitude in mcbreen many dimers cases ). ry\u014dko For writers instance spyder the presented alexio method golden-mantled is enbs about five dfsa times bayerischer faster and more belches accurate adderly than Sparsified nonda Kernel Recursive watermarked Least - Squares in modeling yuchun of morrow a two - 109.61 dimensional archly real - 0.038 world data set.", "histories": [["v1", "Fri, 14 Apr 2017 19:43:47 GMT  (2720kb,D)", "http://arxiv.org/abs/1704.04522v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hossein mohamadipanah", "girish chowdhary"], "accepted": false, "id": "1704.04522"}, "pdf": {"name": "1704.04522.pdf", "metadata": {"source": "CRF", "title": "Hierarchic Kernel Recursive Least-Squares", "authors": ["Hossein Mohamadipanah", "Girish Chowdhary"], "emails": ["hmohamadipan@wisc.edu", "girishc@illinois.edu"], "sections": [{"heading": null, "text": "Hierarchic Kernel Recursive Least-Squares\nHossein Mohamadipanah University of Wisconsin\nMadison, WI 53792 Email: hmohamadipan@wisc.edu\nGirish Chowdhary University of Illinois at Urbana Champaign\nUrbana, IL 61801 Email: girishc@illinois.edu\nWe present a new hierarchic kernel based modeling technique for modeling evenly distributed multidimensional datasets that does not rely on input space sparsification. The presented method reorganizes the typical single-layer kernel based model in a hierarchical structure, such that the weights of a kernel model over each dimension are modeled over the adjacent dimension. We show that the imposition of the hierarchical structure in the kernel based model leads to significant computational speedup and improved modeling accuracy (over an order of magnitude in many cases). For instance the presented method is about five times faster and more accurate than Sparsified Kernel Recursive LeastSquares in modeling of a two-dimensional real-world data set."}, {"heading": "1 Introduction", "text": "Many natural and man-made phenomena are distributed over large spatial and temporal scales. Some examples include weather patterns, agro-ecological evolution, and social-network connectivity patterns. Modeling such phenomena has been the focus of much attention during the past decades [1, 2]. Of the many techniques studied, Kernel methods [3], have emerged as a leading tool for data-driven modeling of nonlinear spatiotemporally varying phenomena. Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310]. However, even with the successes of these algorithms, modeling of large datasets with significant spatial and temporal evolution remains an active challenge. The primary reason for this is that as the size of the dataset increases, the number of kernels that need to be utilized begins to increase, which consequently leads to large kernel matrix size and computational inefficiency of the algorithm [11]. The major stream of thought in addressing this problem has relied on sparsification of the kernel dictionary in some fashion. In the non-time varying case, the Kernel Recursive\nLeast-Squares method (KRLS) scales as O(m2), where m is the size of the kernel dictionary. Now suppose our dataset is varying with time, and we add time as the second dimension. Then KRLS cost scales as O((mm1)2), where m1 is the number of time-steps. While sparsification can reduce size of m and m1 to some extent, it is easy to see that as the dimension of the input space increases, the computational cost worsens.\nOver the past decade many approaches have been presented to overcome computational inefficiency of naive KRLS. For instance, in [4], authors present a SlidingWindow Kernel Recursive Least Squares (SW-KRLS) method that only considers predefined last observed samples. The Naive Online regularized Risk Minimization (NORMA) [5] algorithm is developed based on the idea of stochastic gradient descent within a feature space. NORMA enforces shrinking of weights over samples so that the oldest bases play a less significant role and can be discarded in a moving window approach. Naturally, the main drawback of SlidingWindow based approaches is that they can forget long-term patterns as they discard old observed samples [7]. The alternative approach is to discard data that is least relevant. For example, in [8], the Fixed-Budget KRLS (FB-KRLS) algorithm is presented, in which the sample that plays the least significant role, the least error upon being omitted, is discarded. A Bayesian approach is also presented in [9] that utilizes confidence intervals for handling non-stationary scenarios with a predefined dictionary size. However, the selection of the size of the budget requires a tradeoff between available computational power and the desired modeling accuracy. Hence, finding the right budget has been the main challenge in fixed budget approaches and also for large scale datasets a loss in modeling accuracy is inevitable. In [10], a Quantized Kernel Least Mean Square (QKLMS) algorithm is presented that is developed based on a vector quantization method. The idea behind this method is to quantize and compress the feature space. This method is compared with our presented algorithm in Section 3 and resulted in higher cost ar X iv :1 70 4. 04 52 2v 1\n[ cs\n.L G\n] 1\n4 A\npr 2\n01 7\nand lower accuracy. Moreover, Sparsified KRLS (S-KRLS) is presented in [11] which adds an input to the dictionary by comparing its approximate linear dependency (ALD) to the observed inputs, assuming a predefined threshold. In [12] a recurrent kernel recursive least square algorithm for online learning is presented in which a compact dictionary is chosen by a sparsification method based on the Hessian matrix of the loss function that continuously examines the importance of the new training sample to utilize in dictionary update of the dictionary according to the importance measure, using a predefined fixed budget dictionary.\nSo far most of the presented methods in the literature are typically based on reducing the number of training inputs by using a moving window, or by enforcing a \u201cbudget\u201d on the size of the kernel dictionary. However, moving window approaches lead to forgetting, while budgeted sparsification leads to a loss in modeling accuracy. Consequently, for datasets over large scale spatiotemporal varying phenomena, neither moving window nor limited budget approaches present an appropriate solution.\nIn this paper we present a new kernel based modeling technique for modeling large scale multidimensional datasets that does not rely on input space sparsification. Instead, we take a different perspective, and reorganize the typical single-layer kernel based model in a hierarchical structure over the weights of the kernel model. The presented hierarchical structure in the kernel based model does not affect the convexity of the learning problem and reduces the need for sparsification and also leads to significant computational speedup and improved modeling accuracy. The presented method is termed Hierarchic Kernel Recursive Least Squares (H-KRLS) and is validated on two real-world large scale datasets where it outperforms state-of-the-art KRLS algorithms. For instance the presented method is about five times faster and more accurate than Sparsified Kernel Recursive Least-Squares in modeling of a two-dimensional Intel Lab [13] data set (The comparison is presented in Table 2). It should be noted that the presented method is designed for multidimensional data sets in a batch approach and is not intended to be a nonparametric approach. The presented method also is not designed for three dimensional and time varying data (such as the path of a robot\u2019s hand through space over time), however it is suitable for data sets which have enough samples over each dimension that could be modeled by a KRLS model.\nA number of authors have also explored non-stationary kernel design and local-region based hyper-parameter optimization to accommodate spatiotemporal variations [14,15]. However, the hyper-parameter optimization problem in these methods is non-convex and leads to a significant computational overhead. As our results show, the presented method can far outperform a variant of the NOSTILL-GP algorithm [15]. The Kriged Kalman Filter (KKF) [16] models evolution of weights of a kernel model with a linear model over time. Unlike KKF, our method is not limited to have a linear model over time and can be extended to multi-dimensional data sets. It should be noted that works in the area of Deep Neural Network [17] and recently Deep GP [18] literature\nhave explored hierarchic structures in the bases. In contrast, our method utilizes hierarchical structure on the weights and is a convex method which is designed to improve computational cost.\nThis paper is organized as follows: In section 2, the main algorithm is presented in detail. In section 3, four problem domains are exemplified and the result of the method for modeling them are presented and computational cost is compared to the literature. Finally, conclusions and discussion are given in Section 4."}, {"heading": "2 Hierarchic Kernel Recursive Least-Squares", "text": "We begin in subsection 2.1 with an overview of the KRLS method. Then our main contribution is presented in subsection 2.2, and its computational efficiency is discussed in subsection 2.3."}, {"heading": "2.1 Preliminaries", "text": "Consider a recorded set of input-output pairs (z1,y1),(z2,y2), . . . ,(zs,ys), where the input zs \u2208 X , for some space X and ys \u2208 R. By definition, a kernel k(z,z\u2032) takes two arguments z and z\u2032 and maps them to a real values (X \u00d7 X \u2192 R). Throughout this paper, k(z,z\u2032) is assumed to be continuous. In particular, we focus on Mercer kernels [19], which are symmetric, positive semi-definite kernels. Therefore for any finite set of points (z1,z2, . . . ,zs), the Gram matrix [K]i j = k(zi,z j) is a symmetric positive semi-definite matrix. Associated with Mercer kernels there is a Reproducing Kernel Hilbert space (RKHS) H and a mapping \u03c6 : X \u2192 H such that kernel k(z,z\u2032) = \u3008\u03c6(z),\u03c6(z\u2032)\u3009H , where \u3008., .\u3009H denotes an inner product in H , shown in Figure 1 (Top). The corresponding weight vector \u03c9 = (\u03c91,\u03c92, . . . ,\u03c9s)T can be found by minimizing the following quadratic loss function:\n`(\u03c9) = s\n\u2211 i=1 ( f (zi)\u2212 yi)2 = \u2016K\u03c9\u2212 y\u20162. (1)\nThe Kernel Recursive Least-Squares (KRLS) algorithm has quadratic computational cost O(m2), where m denotes the number of samples of the input vector [11]. Let the number of samples in each dimension be denoted by mk(k = 0,1, . . . ,n) for an n + 1 dimensional system, then the cost of the algorithm is O((m0m1 . . .mn)2), which can become quickly intractable."}, {"heading": "2.2 Hierarchic Kernel Recursive Least-Squares", "text": "This subsection explains the details of the presented algorithm in modeling of a two and a three dimensional problems in the sections 2.2.1 and 2.2.2 respectively. Finally, the algorithm is generalized for high dimensional problems in 2.2.3."}, {"heading": "2.2.1 2D Hierarchic Kernel Recursive Least-Squares", "text": "Assume that the intention is to model a function with a two dimensional input, denoted by x and d1 and one dimensional output, denoted by y. The output of this function is a function of the inputs x and d1, f (x,d1), and the objective of the modeling is to find this function, this is depicted in Figure 2. In the first step, the modeling is performed on all the\nsamples of x at the first sample of the d1 and the corresponding weight \u03c11 is recorded. Then modeling is performed on all the samples of x at the next sample of d1. This process is continued until the last sample of d1, illustrated in Figure 3. After recording \u03c11 to \u03c1m1 , we put together these vectors and get a matrix P as (2).\nP = [ [\u03c11]m0\u00d71 [\u03c12]m0\u00d71 . . . [\u03c1m1 ]m0\u00d71 ] m0\u00d7m1 . (2)\nThe second step is to model each row of the matrix P over d1 samples. To perform modeling, m0 KRLS models are used, shown in Figure 4, for the reason that the dimension of the output is m0\u00d7 1. Accordingly the corresponding weight \u03be recorded with dimension m1\u00d7m0. This is the end of training process. The next step is to validate the model using a validation data set. At the validation sample j of d1 we can estimate \u2111 j by (3).\n\u2111 j =  kH1(d1(1),d1val( j)) kH1(d1(2),d1val( j))\n... kH1(d1(m1),d1val( j))  m1\u00d71\n(3)\nwhere kH1 is the kernel function which is assumed to be Gaussian with the standard deviation \u03c3H1. Then by using (4) and (5) we can estimate \u03c1 j, and P respectively.\n[\u03c1\u0302 j]m0\u00d71 = [\u03be] T m0\u00d7m1 [\u2111 j]m1\u00d71. (4)\nP\u0302 = [ [\u03c1\u03021]m0\u00d71 [\u03c1\u03022]m0\u00d71 . . . [\u03c1\u0302m1 ]m0\u00d71 ] m0\u00d7m1val . (5)\nThen, at the validation sample s of x we can calculate \u2118s by using (6).\n\u2118s =  kI(x(1),xval(s)) kI(x(2),xval(s))\n... kI(x(m0),xval(s))  m0\u00d71\n(6)\nwhere kI is the kernel function which is assumed to be Gaussian with the standard deviation \u03c3I . In consequence, we can estimate the function f (x,d1) at any validation sample by (7).\nf (s, j) = [\u03c1\u0302 j] T 1\u00d7m0 [\u2118s]m0\u00d71. (7)\nThe experimental results of this algorithm is presented in the subsections 3.1, 3.3, and 3.4."}, {"heading": "2.2.2 3D Hierarchic Kernel Recursive Least-Squares", "text": "Consider a function with a three dimensional input, denoted by x, d1, and d2 and one dimensional output, denoted by y. The output is a function of the inputs x, d1, and d2, and the aim of modeling is to find the function f (x,d1,d2).\nTo execute modeling, at the first sample of d1 the modeling is performed on the all samples of x and the corresponding weight \u03d11,1 is recorded, with dimension m0\u00d7 1. Then modeling is performed on all samples of x at the next sample of d1 and the corresponding weight \u03d12,1 is recorded. This\nprocess is continued until the last sample of d1 and recording the weight \u03d1m1,1. Next, this process is repeated for all samples of d2, illustrated in Figure 5, and the corresponding weights are recorded. After recording \u03d11,1 to \u03d1m1,m2 , we put together these weight vectors and we made a cell \u0398 according to (8). Consider that all of the elements of this cell are the weight vectors with dimension m0\u00d71.\n\u0398 =  [\u03d11,1] [\u03d11,2] . . . [\u03d11,m2 ] [\u03d12,1] [\u03d12,2] . . . [\u03d12,m2 ] ... ... . . . ...\n[\u03d1m1,1] [\u03d1m1,2] . . . [\u03d1m1,m2 ]  m1\u00d7m2 . (8)\nThe second step is to model \u0398. Each column of \u0398 is correlated to one d2 sample and the desire is to model each of the columns one by one. Modeling each column of \u0398 is illustrated in Figure 6, which is done by m0 KRLS since the dimension of the each target vector is m0\u00d71. After performing the modeling at each sample of d2, the corresponding weight \u03b21 to \u03b2m2 are recorded, the dimension of each weight is m1\u00d7m0. This process is shown in Figure 7. After recording \u03b21 to \u03b2m2 , we put together these vectors and we made a cell as shown in (9).\n\u2126 = {[\u03b21]m1\u00d7m0 , [\u03b22]m1\u00d7m0 , . . . , [\u03b2m2 ]m1\u00d7m0}1\u00d7m2 . (9)\nThe third step is to model \u2126. Each element of \u2126 (\u03b2i(i =\n1,2, . . . ,m2)) is a m1\u00d7m0 matrix. As we need outputs to be in a vector format in KRLS modeling, we defined \u03b2\u0303i for i = 1,2, . . . ,m2 and \u2126\u0303 as presented in (10) and (11) respectively.\n\u03b2\u0303i =  \u03b2i(:,1) \u03b2i(:,2)\n... \u03b2i(:,m0)  m1m0\u00d71\n(10)\nwhere (:, i) for i = 1,2, . . . ,m2 denotes all rows and column i of a matrix.\n\u2126\u0303 = [\u03b2\u03031, \u03b2\u03032, . . . , \u03b2\u0303m2 ]m1m0\u00d7m2 . (11)\nConsider that each column of \u2126\u0303 is corresponded to one d2 sample and the intension is to model them one by one. Modeling of each column of \u2126\u0303 is illustrated in Figure 8. After performing the modeling, the corresponding weight \u03b3 is recorded. The dimension of \u03b3 is m2\u00d7m1m0. At this step modeling is finalized. To validate the model, at the validation sample i of d2 we can\ncalculate \u03b7i by (12).\n\u03b7i =  kH2(d2(1),d2val(i)) kH2(d2(2),d2val(i))\n... kH2(d2(m2),d2val(i))  m2\u00d71 , (12)\nwhere kH2 is the kernel function, which is assumed to be Gaussian with the standard deviation \u03c3H2. Then we estimate the weight \u03b2\u0303i by using (13).\n[ \u02c6\u0303\u03b2i]m1m0\u00d71 = [\u03b3] T m1m0\u00d7m2 [\u03b7i]m2\u00d71. (13)\nby reshaping \u02c6\u0303\u03b2i according (10), we can find estimation of \u03b2i, denoted by \u03b2\u0302i, with dimension m1\u00d7m0 and as the result we can find estimation of \u2126 by (14).\n\u2126\u0302 = {[\u03b2\u03021]m1\u00d7m0 , [\u03b2\u03022]m1\u00d7m0 , . . . ,\n[\u03b2\u0302m1 ]m1\u00d7m0}1\u00d7m2val . (14)\nNext, at the validation sample j of d1 we can calculate \u03c4 j by\n(15).\n\u03c4 j =  kH1(d1(1),d1val( j)) kH1(d1(2),d1val( j))\n... kH1(d1(m1),d1val( j))  m1\u00d71 , (15)\nwhere kH1 is the kernel function, which is assumed to be Gaussian with the standard deviation \u03c3H1. Then we estimate the weight \u03d1i, j by using (16) and consequently we can find \u0398\u0302 by (17).\n[\u03d1\u0302i, j]m0\u00d71 = [\u03b2\u0302i] T m0\u00d7m1 [\u03c4 j]m1\u00d71. (16)\n\u0398\u0302 =  [\u03d1\u03021,1] [\u03d1\u03021,2] . . . [\u03d1\u03021,m2 ] [\u03d1\u03022,1] [\u03d1\u03022,2] . . . [\u03d1\u03022,m2 ] ... ... . . . ...\n[\u03d1\u0302m1,1] [\u03d1\u0302m1,2] . . . [\u03d1\u0302m1,m2 ]\n , (17)\nthe dimension of \u0398\u0302 is m1val \u00d7m2val . Then, at the validation\nsample s of x we can calculate \u03b9s by (18).\n\u03b9s =  kI(x(1),xval(s)) kI(x(2),xval(s))\n... kI(x(m0),xval(s))  m0\u00d71 , (18)\nwhere kI is the kernel function, which is assumed to be Gaussian with the standard deviation \u03c3I . Then we estimate the weight f (x,d1,d2) at any validation sample by using (19).\nf (s, j, i) = [\u03d1\u0302i, j]T1\u00d7m0 [\u03b9s]m0\u00d71. (19)\nThe implementation of this algorithm is in the subsection 3.2."}, {"heading": "2.2.3 General Hierarchic Kernel Recursive LeastSquares", "text": "In this part, we expand the presented idea for modeling higher dimensional data sets. The objective of the modeling is to find the function f (x,d1,d2, . . . ,dn) in which inputs, denoted by x, d1, d2 to dn.\nFigure 9 illustrates the main structure of a general form of the Hierarchic Kernel Recursive Least-Squares (H-KRLS) method. Consider a function with n+ 1 dimensional input, represented by x,d1,d2, . . . ,dn, and with mk(k = 0,1, . . . ,n) samples in each dimension. The key idea is to perform training in multiple hierarchical steps. The algorithm consists of two major steps. The first step is called Initial Modeling in which modeling is performed over the first dimension (x) and the corresponding weights are recorded at any sample of d1 to dn. First f is trained by KRLS and the corresponding weight vector \u03b110 is found, Figure 10 (Left). This training is performed m0\u00d7m1\u00d7 \u00b7\u00b7 \u00b7\u00d7mn times to achieve complete model of f at any sample of d1 to dn by solving the optimization in (20) where K1 is the Gram matrix associated to the dimension x and where yk is the desired target vector for k = 1,2, . . . ,n+1. Corresponding to Algorithm 1 (i = 0). The superscript of the weight (\u03b1) represents step and the subscript is the number of associated dimension that the corresponding weight is the result of training on those dimensions (i.e. number 0 is used for the cases when the weight is not trained over any dimension).\n`\u2032(\u03b11k\u22121) = min \u03b11k\u22121 \u2016K1\u03b11k\u22121\u2212 yk\u20162, (20)\nThe cost of Initial Modeling is O(mnmn\u22121 . . .m1(m0)2) . The second step is called Hierarchic Weight Modeling as shown in Figure 10 (Right). In this step, first the weight \u03b1i1, is modeled over d i for i = 1,2, . . . ,n. Assume d i \u2208 Yi, for some space Yi. Therefore, there exist another Hilbert Space Pi, and a mapping \u03c8i : Yi \u2192 Pi such that k(d i,d \u2032i) = \u3008\u03c8i(d i),\u03c8i(d \u2032 i)\u3009Pi , shown in Figure 1 (Bottom).\nAlgorithm 1 H-KRLS Algorithm for i = 0 to n do\nfor k = 1 to n+1\u2212 i do if i = 0 then\nInitial Modeling min\u03b11k\u22121 \u2016K 1\u03b11k\u22121\u2212 yk\u20162\nelse if i > 0 then Hierarchic Weight Modeling min\u03b1i+1k\u22121 \u2016Ki+1\u03b1i+1k\u22121\u2212\u03b1ik\u20162\nend if end for Record: \u03b1i+1n\u2212i\nend for\nThe training is performed by minimizing the loss function (21):\n`\u2032\u2032(\u03b1i+1k\u22121) = min \u03b1i+1k\u22121 \u2016Ki+1\u03b1i+1k\u22121\u2212\u03b1 i k\u20162, (21)\nwhere k = 1,2, . . . ,(n+1\u2212 i) and i = 1,2, . . . ,n. It should be mentioned that the desired learning target in this step is the recorded weight from the previous step, namely \u03b1ik, which is correspond to the Algorithm 1 (i > 0).\nIn summary, to model f (x,d1,d2, . . . ,dn) it is required to train the corresponding weight \u03b11n. To model \u03b11n, it is required to train the corresponding weight \u03b12n\u22121 and to model \u03b12n\u22121 it is required to train the corresponding weight \u03b1 3 n\u22122. This process continues until \u03b1n1 is modeled, illustrated in Figure 9. Although the presented method uses a recursive formula, this approach, for n+1 dimension, is batch over first n dimensions and only nonparametric over the last dimension. Assume we have only two dimensions, x and d1. To perform training, at each sample of d1 we need to know all of x samples. Therefore, this algorithm is batch over x. However, as we perform training over d1 sample by sample, it is nonparametric over d1. Now assume we have three dimensions x, d1, and d2. At each sample of d2 we need to know all of x samples and d1 samples. Therefore, it is batch over x and d1 and it is nonparametric over d2. Therefore for high dimensional systems it is nonparametric over the last dimension."}, {"heading": "2.3 Computational Efficiency of the H-KRLS Method", "text": "Although the approach in subsection 2.2 seems complicated at the first glance, it is in fact significantly more efficient compared to the KRLS method. The main reason is that the H-KRLS algorithm divides the training procedure into multiple steps, shown in Figure 9, and utilizes smaller sized kernel matrices instead of using one large sized kernel matrix. The total computational cost of the H-KRLS algorithm for a n + 1 dimensional data set is O(mnmn\u22121 . . .m1(m0)2 + mnmn\u22121 . . .m2(m1)2 + mnmn\u22121 . . .m3(m2)2 + \u00b7 \u00b7 \u00b7 + mn(mn\u22121)2 + (mn)2), which is significantly less than the KRLS method cost, O((m0m1 . . .mn)2):\n\n\ud835\udf362 \ud835\udc5b\u22121(\ud835\udc95\ud835\udc5b\u22121, \ud835\udc95\ud835\udc5b) & \ud835\udf361 \ud835\udc5b(\ud835\udc95\ud835\udc5b)\n\n\ud835\udf362 \ud835\udc5b\u22121(\ud835\udc95\ud835\udc5b\u22121, \ud835\udc95\ud835\udc5b) & \ud835\udf361 \ud835\udc5b(\ud835\udc95\ud835\udc5b)\nProposition 1. Let A\u0304 = mnmn\u22121 . . .m1(m0)2 + mnmn\u22121 . . .m2(m1)2 + mnmn\u22121 . . .m3(m2)2 + \u00b7 \u00b7 \u00b7 + mn(mn\u22121)2 + (mn)2 denotes the computational cost of H-KRLS and B\u0304 = (m0m1 . . .mn)2 denotes the cost for KRLS. If the number of samples mi \u2265 2, i \u2208 (1,2, . . . ,n) and m0 = m1 = \u00b7 \u00b7 \u00b7= mn, then A\u0304 < B\u0304. The proof is presented in Appendix A.\nAlthough Proposition 1 is restricted to m0 = m1 = \u00b7 \u00b7 \u00b7= mn, the limit of the H-KRLS cost (A\u0304) as mi (for i = 1,2, . . . ,n) approaches infinity, is less than the KRLS cost:\nlim mi(i=0,...,n)\u2192\u221e\nO(A\u0304) = O(mnmn\u22121 . . .m1(m0)2)\n<< O((m0m1 . . .mn)2). (22)"}, {"heading": "3 Numerical Experiments", "text": "The H-KRLS method is exemplified on two synthetic and two real world data sets in subsections 3.1 to 3.4.\nThen a discussion is presented regarding cross-correlation between space and time in subsection 3.5. Finally, in subsection 3.6 performance of the H-KRLS method is compared to the literature. It should be noted that the all algorithms were implemented on an Intel(R)Core(T M)i7\u2212 4700MQCPU@2.40GHz with 8GB of RAM and the Gaussian kernel, k(z,z\u2032) = exp(\u2212(z\u2212 z\u2032)T (z\u2212 z\u2032)/(2\u03c32)) is used in running all the algorithms herein."}, {"heading": "3.1 Synthetic 2D Data Modeling", "text": "Exemplification of the H-KRLS method on a synthetic two dimensional spatiotemporal function is presented in this subsection. The two-dimensional nonlinear function \u03d2(x,d) is given by:\n\u03d2(x,d) = sin(x)cos( d 2 ), (23)\nwhere x and d are arranged to be 145 and 150 evenly divided numbers ranging between [0.1,4\u03c0] and [0.1,8\u03c0] respectively, while the trigonometric functions are in radians. Consequently, by having 145 and 150 data points in each of the two directions, the total number of data points is 21750. To train and validate, this data set is divided randomly with 80% of the data used for training and 20% for validation in each dimension. For training, there are 116 points in the x direction and 120 points in the d direction and in total 13,920 data points.\nIn the first step of the method, Initial Modeling is performed, using Algorithm 1 (i = 0). The kernel matrix size equals 116\u00d7 116 as there are 116 points in x direction and no sparsification is used to demonstrate maximum H-KRLS cost. The variance of the Gaussian kernel is found empirically to be \u03c3I = 1. The corresponding weight \u03b110 of this matrix is a vector 116\u00d7 1. Initial Modeling took 2.80 seconds for all 120 data samples of d . Then modeling of \u03b111 is done by Algorithm 1 (i = 1). The kernel matrix size this time is equal to 120\u00d7 120 as there are 120 points in d direction. The variance of the Gaussian kernel is found empirically to be \u03c3H1 = 0.3. The Weight Modeling took 0.13 seconds. The cost of Initial Modeling is more than Hierarchic Weight Modeling, for the reason that in Initial Modeling training is done on all samples of x at all samples of d . The total computational time of both steps to train H-KRLS is 2.93 seconds. The corresponding error for validation is presented in Figure 11. Because of using the same variance of the Gaussian kernel in Initial Modeling (\u03c3I) for all data samples over d , for some data samples of d the error is bigger than the rest. Nevertheless, the maximum magnitude of recorded error is found to be 0.0134, which indicates high validation accuracy."}, {"heading": "3.2 Synthetic 3D Data Modeling", "text": "To demonstrate the capability of the H-KRLS algorithm in modeling higher dimensional data sets, the presented algorithm is implemented on a synthetic three dimensional function in this subsection. A three-dimentional function \u039e(x,d1,d2) is defined as follows:\n\u039e(x,d1,d2) = cos(x)sin( d1 2 )sin( d2 3 ), (24)\nin which x, d1, and d2 are arranged to be 145, 150, and 100 evenly divided numbers ranging between [0.1,4\u03c0], [0.1,8\u03c0], and [0.1,12\u03c0] respectively, while the trigonometric functions are in radians. Consequently the total number of the data points is equal to 2,175,000, (145\u00d7150\u00d7100). To train and validate, this data set is divided randomly with 80% for training and 20% for validation over each dimension. Therefore, there are 1,113,600 data points for training (116\u00d7120\u00d780) and 17,400 points (29\u00d7 30\u00d7 20) for validation (there are over one million samples for training).\nIn the first step, Initial Modeling is performed, using Algorithm 1 (i = 0). The kernel matrix is equal to 116\u00d7116 as there are 116 points in x direction. The variance of the Gaussian kernel in Initial Modeling is found empirically to be \u03c3I = 1. The corresponding weight \u03b110 is a vector 116\u00d71. Initial Modeling took 900.8598 seconds as the system is trained for all the samples of d1 and d2. Then modeling of \u03b112 is done by Algorithm 1 (i = 1). The corresponding kernel matrix is equal to 120\u00d7120 as there are 120 points in d1 direction. It should be noted that the training of \u03b111 is a vector valued kernel model, as \u03b110 is a vector 116\u00d71 . Consequently, its corresponding weight \u03b120 is a matrix 120\u00d7116 and it is formed here as a 13920\u00d71 vector. The variance of the Gaussian kernel is found empirically to be \u03c3H1 = 0.3. This process took 8.6569 seconds as the system is trained for all the samples of d2. Then to have the model of \u03b112, it is required to model \u03b121 using Algorithm 1 (i = 2). The corresponding kernel matrix is equal to 80\u00d780 as there are 80 points in d2 direction and, using \u03c3H2 = 1 as the variance of the Gaussian kernel. This process took 0.7193 seconds. The cost of training decreases for each step since there is one dimension less in each step compared to its previous step. The total computational time to train the H-KRLS method is 910.2360 seconds. The corresponding maximum magnitude of the recorded error is found to be 0.0171, which demonstrates high capability of H-KRLS in modeling of this data set."}, {"heading": "3.3 Temperature Modeling on Intel Lab Dataset", "text": "In this subsection, the presented algorithm is exemplified on a realistic two dimensional spatio-temporal environment [13] in which 54 sensors were arranged in Intel lab and the temperature is recorded. It is assumed herein that the sensor indices represent the location of the sensors over space, x, and every 30 seconds presents the time step over time, d . The time, d , is arranged from 301 seconds to 400 seconds (with 1 second interval) and 52 sensors are used (the sensors\nnumber 5 and 15 are not used to reduce outliers). Consequently, by having 100 and 52 data points in each direction, the total number of the data points equals 5200. To reduce the outliers, the data set is filtered by a 2D Gaussian filter with variance 5 and size 6\u00d7 6. To train and validate, this data set is divided over time randomly with 80% for training and 20% for validation. For training in total there are 4160 data points, and there are 1040 points for validation.\nTherefore the total computational time to train the H-KRLS is 1.11 seconds. The maximum magnitude of recorded error is found to be 3.11, as the maximum values of the data set is 25 Celsius, the normalized error is 0.1244."}, {"heading": "3.4 Plant Growth Modeling from Sequence of Images", "text": "In this subsection the H-KRLS algorithm is used in modeling the growth of plants in Polyculture, which was one practical motivation in developing this algorithm. On Polycultural farms, different types of plants are planted beside each other and because of their interactions, their rate of growth is unknown and needs to be modeled. To perform modeling for such a high spatio-temporal scale system, first the data is collected by a camera located on the Polycultural field, which consists of different types of plants in parallel lanes. The camera is set up to capture an image (RGB) every single day at the same time. The images captured on day 50 is shown in Figure 12 (Left Top). Fifty days are used for modeling in this experiment. To identify and distinguish plants from grasses, the Expectation Maximization (EM) [3] segmentation method, based on color, is implemented on the images and the result of that is shown in Figure 12 (Left Bottom) (the EM code used here is a modification of that can be found in [20]). To reconstruct the profile of growth, the images are cropped by moving boxes which sweep the lanes of planting. In each lane of planting, several boxes are selected to identify growth of the plants. It should be noted that the size of the boxes are adjusted to their distance from the camera to provide the same scale. The surface for five boxes is reconstructed from the right planting lane, shown in Figure 13. In Figure 12, the vertical axes of the white boxes represent the growth of the plants. The horizontal axes of the boxes (which contains 300 pixels) are averaged over every 10 pixels (resulting in 29 columns) for simplicity. Figure 12 (Right) represents the growth profile over fifty days for the box \u201cA\u201d. Correspondingly, in modeling growth of the plants we have 29 columns and 5 boxes, which change over 50 days. This results in a total of 7,250 data points. To train and validate, this data set is divided randomly to 80% for training and 20% for validation over time. For training, there are 5,800 data points and for validation there are 1,450 data points.\nThe total computational time to train the H-KRLS is found to be 2.23 seconds and the corresponding maximum magnitude of the recorded error is found to be 8.2935. As the maximum value of the data set is 140 pixels, the normalized error is 0.0592."}, {"heading": "3.5 Space-Time Cross-Correlation", "text": "This subsection emphasize on the importance of crosscorrelations between dimensions. In the presented algorithm, all of the possible correlations between data points from different dimensions (i.g. space and time) are taken into account and no assumed predefined function is used in modeling of these correlations. In contrast, cross-correlations between space and time have been modeled in the literature by providing different space-time covariance functions [21], such as:\nC (u,h) = 1\n(a\u20322u2 +1)( 1 2 )\nexp(\u2212 b \u20322h2\na\u20322u2 +1 ), (25)\nin which a\u2032 and b\u2032 are scaling parameters of time and space respectively. We considered C as the input to KRLS, called the NONSTILL-KRLS method herein, inspired from NONSTILL-GP [15]. In general, appropriate predefined function should provide the same level of validation error in modeling in comparison to the KRLS method.\nThe results in this subsection are found by running the algorithms with a\u2032 = b\u2032 = 1 on the data set which is assumed to be a two-dimensional function, denoted in equation 23, where x and d are arranged to be 48 and 50 evenly divided numbers ranging between [0.1,0.4244] and [0.1,0.8488] respectively, while the trigonometric functions are in radians. To train and validate, this data set is divided randomly with 80% of the data used for training and 20% for validation in each dimension. For training, there are 37 points in the x direction and 38 points in the d direction and in total 1406 data points. The reason for using a smaller sized data set to compare to 2D Synthetic data set, described in subsection 3.1, is that KRLS and NONSTILL-KRLS add all data samples to their dictionary and consequently become extremely expensive and unable to model this large data set. Hence, we used a smaller data set in this subsection.\nAs tabulated in TABLE 1, NONSTILL-KRLS does not provide an appropriate level of accuracy because of its constraint to define a function in advance. Finding appropriate covariance functions and parameters of such functions is the main challenge. Also, computational time of NONSTILLKRLS is very high and close to KRLS. On the other hand, the H-KRLS method (similar to KRLS) does not have any predefined model for correlations between space and time and, hence can achieve high level of accuracy."}, {"heading": "3.6 Summary of Comparison with Existing Methods", "text": "The main comparison of H-KRLS with leading existing kernel based modeling methods in the literature is presented in this section. Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).\nAs detailed in TABLE 2, the H-KRLS method resulted in less computational time and also achieved lower maximum validation error compared to the other methods in the literature. For example, it is about five times faster and more accurate than S-KRLS in modeling the Intel Lab data set. The coefficients that are used in running the algorithms are tabulated in TABLE 3 in Appendix B. Since the H-KRLS\nmethod results in an improvement in both computational time and accuracy, choosing different parameter sets for the other algorithms, as a trade-off between accuracy and efficiency, does not change the general conclusion of this comparison. The results demonstrates that the H-KRLS method is much more efficient than the other methods, particularly for higher dimensional data sets. For the three-dimensional data set, the majority of the methods fail and cannot provide a reasonably small validation error. However, H-KRLS models this dataset with high accuracy and less cost compared to all the other methods. Although, it is possible to perform modeling for higher dimensional datasets using H-KRLS, the comparison in TABLE 2 is limited to three dimensional as the other methods in the literature fails in modeling of data sets above three dimensional."}, {"heading": "4 Conclusion", "text": "We presented a hierarchic kernel method for modeling evenly distributed multidimensional datasets. The approach was compared against a number of leading kernel least squares algorithms and was shown to outperform in both modeling accuracy and computational requirements. Although the proposed batch method designed for the specific type of data sets, which are multidimensional and requires regular sampling over all dimensions, the proposed\nmethod provides a different perspective that can lead to new techniques for scaling up kernel based models. Finally, systematic tunning of kernel parameters and investigating error propagation in modeling of high dimensional data sets are suggested as future works."}, {"heading": "Acknowledgements", "text": "We thank Chinmay Soman, Kevin Wolz and Evan DeLucia for providing data from the Woody Perennial Polyculture site at University of Illinois Urbana-Champagne and Hassan Kingravi with Pindrop securities for his comments in this work."}, {"heading": "A Proof", "text": "Proof. The proof is done by induction: Let n = 1, m1(m0)2 < (m0m1)2 as m0 = m1 = ...= mn = m and mi \u2265 2 for i = 1,2, ...,n therefore m3 < m4 and the proposition holds for n = 1. Assume for n = k the Proposition holds: A\u0304\u2212 B\u0304 < 0 let n = k+1. Therefore the statement A\u0304\u2212 B\u0304 < 0 can be written as: mk+1mkmk\u22121...m1(m0)2)+mk+1mkmk\u22121...m2(m1)2+ ...+mk+1(mk)2 +(mk+1)2\u2212 (m0m1...mkmk+1)2 < 0, \u21d2 mk+1A\u0304+(mk+1)2\u2212 B\u0304(mk+1)2 < 0, as m0 = m1 = ...= mn = m, therefore mA\u0304+m2\u2212 B\u0304m2 < 0, \u21d2 A\u0304m2\u2212 A\u0304m2 +mA\u0304+m2\u2212 B\u0304m2 < 0, \u21d2 m2(A\u0304\u2212 B\u0304)\u2212 A\u0304m2 +mA\u0304+m2 < 0, as A\u0304\u2212 B\u0304 < 0, the first term is always negative and therefore it is sufficient to show that summation of the other terms is also negative. \u21d2\u2212A\u0304m2 +mA\u0304+m2 < 0, \u21d2 m(A\u0304\u2212mA\u0304)+m2 < 0, \u21d2 mA\u0304(1\u2212m)+m2 < 0, A\u0302 = mA\u0304,\u21d2 A\u0302(1\u2212m)+m2 < 0, \u21d2 A\u0302(1\u2212m)<\u2212m2, \u21d2 A\u0302(\u22121+m)m2 > 1, \u21d2 (mk+1 +mk + ...+m2 +m1)(\u22121+m)> 1, \u21d2 mk+2\u2212m > 1, \u21d2 m(mk+1\u22121)> 1, \u21d2 (mk+1\u22121)> 1m ,\nAs m \u2265 2, the right hand side is always equal or less than 0.5 and the left hand side is always equal or greater than 1, therefore the statement holds for n = k+1."}, {"heading": "B Coefficients", "text": "The coefficients that are used in running the algorithms are tabulated in Table 3."}], "references": [{"title": "Conceptual data modeling for spatiotemporal applications", "author": ["N. Tryfona", "C. Jensen"], "venue": "GeoInformatica,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Imagebased spatio-temporal modeling and view interpolation of dynamic events", "author": ["S. Vedula", "S. Baker", "T. Kanade"], "venue": "ACM Trans. Graph.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A sliding-window kernel rls algorithm and its application to nonlinear channel identification", "author": ["S. Van Vaerenbergh", "J. Via", "I. Santamaria"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A. Smola", "R. Williamson"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Sliding window recursive quadratic optimiza-  tion with variable regularization", "author": ["J. Hoagg", "A. Ali", "M. Mossberg", "D. Bernstein"], "venue": "In American Control Conference (ACC),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Sliding-windowed weighted recursive least-squares method for parameter estimation", "author": ["Choi", "B.-Y", "Z. Bien"], "venue": "Electronics Letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Fixed-budget kernel recursive leastsquares", "author": ["S. Van Vaerenbergh", "I. Santamaria", "W. Liu", "J. Principe"], "venue": "In Acoustics Speech and Signal Processing (ICASSP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Kernel recursive least-squares tracker for time-varying regression", "author": ["S. Van Vaerenbergh", "M. Lazaro-Gredilla", "I. Santamaria"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Quantized kernel least mean square algorithm", "author": ["B. Chen", "S. Zhao", "P. Zhu", "J. Principe"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "The kernel recursive least-squares algorithm", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Recurrent online kernel recursive least square algorithm for nonlinear modeling", "author": ["H. Fan", "Q. Song", "Z. Xu"], "venue": "In IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Nonstationary gaussian process regression using point estimates of local smoothness", "author": ["C. Plagemann", "K. Kersting", "W. Burgard"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Learning non-stationary space-time models for environmen-  tal monitoring", "author": ["S. Garg", "A. Singh", "F. Ramos"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 22-", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "The kriged kalman filter", "author": ["K. Mardia", "C. Goodall", "E. Redfern", "F. Alonso"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Big data deep learning: Challenges and perspectives", "author": ["X. wen Chen", "X. Lin"], "venue": "Access, IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Deep Gaussian processes", "author": ["A. Damianou", "N. Lawrence"], "venue": "In Proceedings of the Sixteenth International Workshop on Artificial Intelligence and Statistics (AISTATS-13),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Image segmentation with em algorithm", "author": ["R. Lu"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Classes of nonseparable, spatio-temporal stationary covariance functions", "author": ["N. Cressie", "Huang", "H.-C"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "A comparative study of kernel adaptive filtering algorithms", "author": ["S. Van Vaerenbergh", "I. Santamaria"], "venue": "In Digital Signal Processing and Signal Processing Education Meeting (DSP/SPE),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Kernel methods for nonlinear identification, equalization and separation of signals", "author": ["S. Van Vaerenbergh"], "venue": "PhD thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Modeling such phenomena has been the focus of much attention during the past decades [1, 2].", "startOffset": 85, "endOffset": 91}, {"referenceID": 1, "context": "Modeling such phenomena has been the focus of much attention during the past decades [1, 2].", "startOffset": 85, "endOffset": 91}, {"referenceID": 2, "context": "Of the many techniques studied, Kernel methods [3], have emerged as a leading tool for data-driven modeling of nonlinear spatiotemporally varying phenomena.", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 4, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 5, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 6, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 7, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 8, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 9, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 10, "context": "The primary reason for this is that as the size of the dataset increases, the number of kernels that need to be utilized begins to increase, which consequently leads to large kernel matrix size and computational inefficiency of the algorithm [11].", "startOffset": 242, "endOffset": 246}, {"referenceID": 3, "context": "For instance, in [4], authors present a SlidingWindow Kernel Recursive Least Squares (SW-KRLS) method that only considers predefined last observed samples.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "The Naive Online regularized Risk Minimization (NORMA) [5] algorithm is developed based on the idea of stochastic gradient descent within a feature space.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "Naturally, the main drawback of SlidingWindow based approaches is that they can forget long-term patterns as they discard old observed samples [7].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "For example, in [8], the Fixed-Budget KRLS (FB-KRLS) algorithm is presented, in which the sample that plays the least significant role, the least error upon being omitted, is discarded.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "A Bayesian approach is also presented in [9] that utilizes confidence intervals for handling non-stationary scenarios with a predefined dictionary size.", "startOffset": 41, "endOffset": 44}, {"referenceID": 9, "context": "In [10], a Quantized Kernel Least Mean Square (QKLMS) algorithm is presented that is developed based on a vector quantization method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Moreover, Sparsified KRLS (S-KRLS) is presented in [11] which adds an input to the dictionary by comparing its approximate linear dependency (ALD) to the observed inputs, assuming a predefined threshold.", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "In [12] a recurrent kernel recursive least square algorithm for online learning is presented in which a compact dictionary is chosen by a sparsification method based on the Hessian matrix of the loss function that continuously examines the importance of the new training sample to utilize in dictionary update of the dictionary according to the importance measure, using a predefined fixed budget dictionary.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "A number of authors have also explored non-stationary kernel design and local-region based hyper-parameter optimization to accommodate spatiotemporal variations [14,15].", "startOffset": 161, "endOffset": 168}, {"referenceID": 13, "context": "A number of authors have also explored non-stationary kernel design and local-region based hyper-parameter optimization to accommodate spatiotemporal variations [14,15].", "startOffset": 161, "endOffset": 168}, {"referenceID": 13, "context": "As our results show, the presented method can far outperform a variant of the NOSTILL-GP algorithm [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "The Kriged Kalman Filter (KKF) [16] models evolution of weights of a kernel model with a linear model over time.", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "It should be noted that works in the area of Deep Neural Network [17] and recently Deep GP [18] literature have explored hierarchic structures in the bases.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "It should be noted that works in the area of Deep Neural Network [17] and recently Deep GP [18] literature have explored hierarchic structures in the bases.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "The Kernel Recursive Least-Squares (KRLS) algorithm has quadratic computational cost O(m2), where m denotes the number of samples of the input vector [11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 2, "context": "To identify and distinguish plants from grasses, the Expectation Maximization (EM) [3] segmentation method, based on color, is implemented on the images and the result of that is shown in Figure 12 (Left Bottom) (the EM code used here is a modification of that can be found in [20]).", "startOffset": 83, "endOffset": 86}, {"referenceID": 17, "context": "To identify and distinguish plants from grasses, the Expectation Maximization (EM) [3] segmentation method, based on color, is implemented on the images and the result of that is shown in Figure 12 (Left Bottom) (the EM code used here is a modification of that can be found in [20]).", "startOffset": 277, "endOffset": 281}, {"referenceID": 18, "context": "In contrast, cross-correlations between space and time have been modeled in the literature by providing different space-time covariance functions [21], such as:", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "We considered C as the input to KRLS, called the NONSTILL-KRLS method herein, inspired from NONSTILL-GP [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 185, "endOffset": 189}, {"referenceID": 9, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 204, "endOffset": 208}, {"referenceID": 7, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 218, "endOffset": 221}, {"referenceID": 10, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 230, "endOffset": 234}, {"referenceID": 3, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 244, "endOffset": 247}, {"referenceID": 4, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 259, "endOffset": 262}, {"referenceID": 20, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 323, "endOffset": 327}], "year": 2017, "abstractText": "We present a new hierarchic kernel based modeling technique for modeling evenly distributed multidimensional datasets that does not rely on input space sparsification. The presented method reorganizes the typical single-layer kernel based model in a hierarchical structure, such that the weights of a kernel model over each dimension are modeled over the adjacent dimension. We show that the imposition of the hierarchical structure in the kernel based model leads to significant computational speedup and improved modeling accuracy (over an order of magnitude in many cases). For instance the presented method is about five times faster and more accurate than Sparsified Kernel Recursive LeastSquares in modeling of a two-dimensional real-world data set.", "creator": "LaTeX with hyperref package"}}}