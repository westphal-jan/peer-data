{"id": "1704.04539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Cross-lingual Abstract Meaning Representation Parsing", "abstract": "Abstract Meaning 136 Representation (AMR) schjoenberg annotation efforts have kodr\u0105b mostly focused on English. mordington In clement order bakili to 82.52 train tryst parsers tanagers on other grubben languages, quereshi we propose a non-religious method based on two-year annotation projection, cadent which flanary involves forrer exploiting annotations in a emancipator source language rexburg and a parallel corpus abscissa of semilattice the aar source language thueringen and zengpei a 125.40 target spies language. gaydos Using dirda English mock-heroic as the 108.62 source language, cabiao we sarana show bag101 promising results for Italian, Spanish, German banjica and Chinese demens as 5-65 target languages. elisha Besides evaluating bipac the target obligatorily parsers on eec non - gold datasets, post-show we jctv further propose an evaluation janas method re-made that landslide exploits groundbreakers the polivka English swans gold annotations and does 66,800 not furat require sakami access measurer to r\u00e2ul gold annotations cornbury for nawiliwili the target 4,387 languages. halderman This 10mm is achieved caprica by inverting the shendu projection process: 18-mile a new demokrasi English keis parser leaper is learned metalloproteinases from nicolaescu the stamfordham target chervokas language parser sandwiched and evaluated on 3003 the existing English homebuilt gold standard.", "histories": [["v1", "Fri, 14 Apr 2017 20:41:27 GMT  (26kb)", "http://arxiv.org/abs/1704.04539v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marco damonte", "shay b cohen"], "accepted": false, "id": "1704.04539"}, "pdf": {"name": "1704.04539.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["m.damonte@sms.ed.ac.uk,", "scohen@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n04 53\n9v 1\n[ cs\n.C L\n] 1\n4 A\npr 2\n01 7\nCross-lingual Abstract Meaning Representation Parsing\nMarco Damonte and Shay B. Cohen\nSchool of Informatics\nUniversity of Edinburgh\nm.damonte@sms.ed.ac.uk, scohen@inf.ed.ac.uk\nAbstract\nAbstract Meaning Representation (AMR) annotation efforts have mostly focused on English. In order to train parsers on other languages, we propose a method based on annotation projection, which involves exploiting annotations in a source language and a parallel corpus of the source language and a target language. Using English as the source language, we show promising results for Italian, Spanish, German and Chinese as target languages. Besides evaluating the target parsers on nongold datasets, we further propose an evaluation method that exploits the English gold annotations and does not require access to gold annotations for the target languages. This is achieved by inverting the projection process: a new English parser is learned from the target language parser and evaluated on the existing English gold standard."}, {"heading": "1 Introduction", "text": "Abstract Meaning Representation (AMR) parsing converts natural language sentences into their corresponding AMRs (Banarescu et al., 2013). An AMR is a graph with nodes representing the main events and entities mentioned in the sentence and edges representing the semantic relationships between them. Most datasets for Natural Language Processing (NLP) exist only for the English language and AMR parsing is no exception: the only available AMR datasets large enough to train statistical models are mappings between English sentences and AMR graphs.\nCross-lingual techniques can alleviate this problem by exploiting the English gold annotations and an English parser. One way to do this is by annota-\ntion projection, where the existing annotations are projected to a target language through a parallel corpus (Hwa et al., 2005).\nThe stability of AMR across languages and the extent to which it can be considered as an interlingua have been the subject of preliminary discussions. The original AMR definition states that AMR is not an interlingua (Banarescu et al., 2013). Bojar (2014) categorizes the different kinds of divergences in the annotation between English AMRs and Czech AMRs. Xue et al. (2014) showed that structurally aligning Czech and English AMRs is problematic but reached different conclusions for the language pair EnglishChinese, showing that their AMRs can be aligned, suggesting that, with tailored annotation guidelines, it could be possible to make AMR behave as an interlingua.\nEven though the cross-lingual stability of AMR is still an open question, in this work we assume that AMR is sufficiently stable and enforce this by making the same AMR graph annotated for English valid for the target language as well. Using annotation projection we can then automatically generate large AMR datasets and train parsers for Italian, Spanish, German and Chinese.\nEvaluation is another important issue, as we assume that, for new languages, gold-standard data is not available even for a small test set. We therefore learn a new English parser using the target parser\u2019s output, projected back to English, as training data. We can now evaluate this parser the gold standard, which is available for English. We speculate that the score we obtain can be used as a proxy for evaluating the target parser. However, we also evaluate the target parser on non-gold (\u201csilver\u201d) data, obtained through the same projection process we use for the training data.\nExperiments show promising results for all languages tested, with the Italian, German and Span-\nish parsers performing very similarly between each other.\nContributions This work reports the first results for multilingual AMR parsing. The method proposed allows quick prototyping of multilingual AMR parsers, assuming that an NLP pipeline (POS tagging, NER tagging, dependency parsing) is available for the target language. This prototyping does not require manual annotation. Moreover, using the same cross-lingual method, we propose a novel method to evaluate parsers when gold annotations are missing."}, {"heading": "2 AMR Parsing", "text": "AMR is a meaning representation originally aimed at English sentences. It relies on Propbank (Kingsbury and Palmer, 2002) to define the main predicates in the sentence. AMRs are rooted and directed graphs G = (V,E) with V and E the set of nodes and edges, respectively. Predicates, entities, and concepts mentioned in the sentence are represented as graph nodes and the relationships between them are represented as graph edges. AMR nodes are either Propbank frames or English words, which means that parsers can in some cases use the English tokens in the input sentences, or their stems, to label nodes. A key property of AMRs is that they allow reentrancies, meaning that a node can have multiple incoming edges. This allows AMR to represent coreferences and control verbs. Nodes in the AMR graph are not annotated with the span of words that triggered them, which makes it necessary to use automatic aligners: in the rest of this paper, we call this type of alignments AMR alignments to distinguish them from the word alignments between words in a parallel corpus. Figure 1 shows an example AMR graph, with the dotted lines representing the AMR alignments. In order to evaluate AMR parsers, the Smatch score (Cai and Knight, 2013) is traditionally used. Smatch computes the overlap in terms of recall, precision, and F-score between two graphs by finding the variable alignments between the graphs that maximize the overlap.\nThe AMR parser we use for the experiments in this paper was recently introduced by Damonte et al. (2017). It runs in linear time in the length of the sentence and allows quick adaptation to new languages. The parser works in a similar fashion to the arc-eager dependency\nparser (Nivre, 2004): the sentence is stored in a buffer, which is scanned once left-to-right and the words in the sentences are either ignored or trigger a subgraph in a stack. Items in the stack can be then connected by edges in such a way that, when all words have been scanned, a single AMR graph is created. The Smatch score on the LDC2015E86 dataset is 0.64, with the stateof-the-art parser scoring 0.67 on the same dataset (Wang et al., 2016)."}, {"heading": "3 Cross-lingual Learning for AMR", "text": ""}, {"heading": "3.1 AMR Annotation Projection", "text": "Projecting the AMR annotation to the target language is straightforward: since AMR is not grounded on the input sentences, we can maintain the AMR annotation as it is. We think of English labels for the graph nodes as using an independent language, which incidentally looks very similar to English. AMR parsing for foreign languages presents therefore an additional complexity: the conversion from tokens to node labels includes an implicit lexical translation.\nWe also need to project the AMR alignments, necessary to train AMR parsers, as we do not have an AMR aligner for the target languages: this is the part that depends on the actual word tokens in the sentence and we therefore need to use word alignments, as in other annotation projection work, to project the alignments to the target language. An example of AMR graph shared across English and Italian is shown in Figure 1.\nOur approach depends on an underlying assumption that we make. Let Ae(\u00b7) be the AMR alignment mapping word tokens in the source lan-\nguage sentence to the set of AMR nodes that are triggered by it; Af (\u00b7) be the same function for the target language sentence; ei be a word in the source language; fj be a word in the target language; v be a node in the AMR graph; and finally, W (\u00b7) be an alignment that maps a word in the source language sentence to a subset of words in the target language sentence. The AMR projection assumption is then:\n\u2200i, j, v fj \u2208 W (ei) \u2227 v \u2208 Ae(ei) \u21d2 v \u2208 Af (fj) (1)\nThis means that if a source word is wordaligned to a target word and it is AMR aligned with a node in the graph, then the target word is also aligned to that node. In the example of Figure 1, Questa is word aligned with This and therefore AMR-aligned with the node this, and the same logic applies to the other aligned words. The words is, the and of do not generate any AMR nodes, so we ignore their word alignments.\nWe can use this method to project existing AMR annotations to other languages and then train AMR parsers for them. Unfortunately, there are no parallel corpora with gold AMR annotations available. We therefore use an available English AMR parser to produce these annotations, which we call silver annotations. These are used to train the AMR parser in the target language."}, {"heading": "3.2 Evaluation on gold annotations", "text": "Once a parser has been learned for a target language (target parser), we are interested in evaluating its performance. We can generate a silver test set in the same way we generated the training set and use this for evaluation. However, the silver test set is influenced by the errors made by the English AMR parser as well as the errors made during the projection, which we later analyze. In order to perform the evaluation on a gold test set, we invert the projection process and train a new English parser from the target parser. We can now evaluate the resulting English parser on a gold test set. Its Smatch score can be then used as a proxy to the score of the target parser."}, {"heading": "3.3 Method pipeline", "text": "Our method to train and evaluate a parser in a target language is summarized as follows:\n1. Parse the English side of a parallel corpus\nwith an English AMR parser.\n2. Project the parsed annotation to the target\nside of the parallel corpus (Equation 1).\n3. Train a parser for the target language and\nevaluate it on silver data.\n4. Invert the above procedure to train a new En-\nglish parser from the parser learned in step 3.\n5. Evaluate the new English parser on gold data.\nSince we rely on several automatic tools, there are several sources of noise in the method: 1) the parsers are trained on silver data obtained by an automatic parser for English (e \u2192 f ) or even by an automatic parser trained on data obtained by a parser which in turn was learned with silver data (f \u2192 e); 2) the projection uses noisy word alignments (Pado\u0301 and Lapata, 2009); 3) the AMR alignments on the source side are also noisy; 4) translation divergences exist between the languages, making it sometimes not possible to project the annotation without loss of information."}, {"heading": "4 Experimental setup", "text": "We experiment with several languages: Italian, Spanish, German and Chinese. For Italian, Spanish and German we use Europarl (Koehn, 2005), containing around 1.9M sentences for each language pair, and the TED talks corpus (Cettolo et al., 2012) for Chinese, containing around 120K sentences. For the purpose of training the AMR parsers, we extract from each parallel corpus 20,000 sentences for training, 2,000 for development and 2,000 for testing; we collect two such datasets for each language, in order to have non-overlapping datasets for the two stages of the process (e \u2192 f and f \u2192 e). We use the remainder of the parallel corpora to train the word alignment models. The gold AMR dataset used is LDC2015E86, containing 16,833 training sentences, 1,368 development sentences, and 1,371 testing sentences.\nWord alignments were generated using fast align (Dyer et al., 2013), while AMR alignments were generated with JAMR (Flanigan et al., 2014). As an English AMR parser and a starting point to develop the target parsers we used AMREager (Damonte et al., 2017), which is an open-source AMR parser for English that requires only small modifications for re-use on other languages. The parser requires tokenization, POS tagging, NER tagging and dependency\nparsing, which for English, German and Chinese are provided by CoreNLP (Manning et al., 2014). Although CoreNLP also supports Spanish, dependency parsing is not provided. We use Freeling (Carreras et al., 2004) instead. Italian is not supported in CoreNLP: we use Tint (Aprosio and Moretti, 2016), which is a CoreNLP-compatible NLP pipeline for Italian."}, {"heading": "5 Results", "text": "Besides evaluating on the English gold standard as explained in Section 3.2, we also report silver data evaluation for the target parser. We further define an upper bound for each parser, in which English is used also as the target language: the English side of each parallel corpus is used in both directions, so that we filter out the issues of noisy word alignments and translational divergence. It is necessary to have a different upper bound for each parser because the English sides of each parallel corpus are different. Results are shown in Table 1. The gap between the original English parser (which scores 0.64 on the gold dataset) and the upper bounds shown is approximately 5%, sourced entirely in the effect of re-training on parsed (silver) data. Italian, Spanish and German have comparable performance. For these languages, the gap with the original English parser is around 20%, which is comparable to the 15% gap reported for annotation projection based cross-lingual semantic parsing for French under the combinatory categorial grammar (CCG) framework (Evang and Bos, 2016). The gap with the upper bound is around 15%, a fraction of which is due to noisy alignments, amenable to improvements. For Chinese, the gaps with the English parser and the upper bound are 25% and 18%, respectively. The lower performance of Chinese with respect to the other languages is explained by the different nature and size of the TED talks corpus compared to Europarl. Even within the TED talks corpus, the Chinese-English pair has been shown to be especially challenging (Cettolo et al., 2012). We further analyze the Chinese parser in 6.2."}, {"heading": "6 Analysis", "text": "Figure 2 shows examples of output parses for all languages tested, including the AMR alignments by-product of the parsing process, that we use to discuss the mistakes made by the parsers.\nIn the Italian example, the only evident error\nis that Infine (Lastly) should be ignored. In the Spanish example, the word medida (measure) is wrongly ignored: it should be used to generate a child of the node impact-01. Some of the :ARG roles are also not correct. In the German example, meines (my) should reflect the fact that the speaker is talking about his own country. Finally, in the Chinese examples, product and increase, both central to the meaning of the sentence, are ignored. We note that the rest of the AMR is sound, resulting in the correct AMR graph for a different sentence: Finland exports high technology.\nMost mistakes involve concept identification and in particular relevant words that are erroneously ignored by the parser. This is directly related to the problem of noisy word alignments: the parser learns what words are likely to trigger a node (or a set of nodes) in the AMR by looking at their AMR alignments (which in our approach are induced by the word alignments): if a word is not aligned to any AMR node, it is ignored. Therefore, if an important word is consistently not aligned, the parser will erroneously learn to discard it. This shows that, in order to achieve better parsing results for these languages, more work must be done to allow more accurate alignment projections, which will result in reducing the gap with the upper bound scores."}, {"heading": "6.1 Translational divergence", "text": "Translational divergence is a known issue with cross-lingual methods. In this section, we look at this type of divergence and discuss how it affects parsing, following the classification used in previous work (Dorr et al., 2002; Dorr, 1994), which identify classes of divergences for several languages. Sulem et al. (2015) also follow the same categorization for French.\nTo investigate the effects of these differences, we parsed several sentences demonstrating these phenomena to analyze how the AMR parsers coped with them, reported in Figure 3. We do not show the edge labels, as we intend to focus on the identification of the relations, rather than on their labeling. The quality of the AMRs, which is not optimal, is not the main issue here: we only want to assess how the parsers dealt with the different kind of translational divergences.\nCategorical. It happens when two languages use different POS tags to express the same meaning. For example, the English sentence I am jeal-\nous of you is translated in Spanish as Tengo envidia de ti (I have jealousy of you). The English adjective jealous is translated in the Spanish noun envidia. In Figure 3a we note that the categorical divergence does not create problems, since the parsers correctly recognized that envidia (jealousy/envy) should be used as the predicate, regardless of its POS.\nConflational. It happens when verbs expressed in a language with a single word can be expressed with more words in another language. Two subtypes are distinguished: manner and light verb. Manner is when a manner verb is mapped to a motion verb plus a manner-bearing word. For example, We will answer is translated in the Italian sentence Noi daremo una riposta (We will give an answer), where to answer is translated as daremo una risposta (to give an answer). Figure 3b shows that the Italian parser generates a sensible output for this sentence by creating a single node labeled answer-01 for the expression dare una riposta. In a light verb conflational divergence, a verb is mapped to a light verb plus an additional meaning unit, such as when I fear is translated as Io ho paura (I have fear) in Italian: to fear is mapped to the light verb ho (have) plus the noun paura (fear). Figure 3e shows that also this divergence is dealt properly by the Italian parser: ho paura correctly triggers the root fear-01.\nStructural. This type of divergence happens when verb arguments result in different syntactic configurations, for example, due to an additional PP attachment, such as when translating He entered the house with Lui e\u0300 entrato nella casa (He entered in the house), where the Italian translation has an additional in preposition. Also this parse, in Figure 3c, is structurally correct, apart from miss-\ning the node he.\nHead swapping. It occurs when the direction of the dependency between two words is inverted. For example, I like eating, where like is head of eating, becomes Ich esse gern (I eat likingly) in German, where the dependency is inverted. Unlike the other examples, in this case, the German parser does not cope well with the divergence. Indeed, it is not able to recognize like-01 as the main concept in the sentence, as shown in Figure 3d.\nThematic. Finally, the parse of Figure 3f has to deal with a thematic divergence, which happens when the semantic roles of a predicate are inverted. In the sentence I like grapes, translated to Spanish as Me gustan uvas, I is the subject in English while Me is the object in Spanish. Even though we note an erroneous reentrant edge between grape and I, the Spanish parser correctly recognizes the :ARG0 relationship between like01 and I and the :ARG1 relationship between like01 and grape, dealing with the thematic divergence as desired. In this case, the edge labels are important, as this type of divergence is concerned with the semantic roles assigned."}, {"heading": "6.2 Analysis of the Chinese Parser", "text": "As mentioned, the performance of the Chinese parser is lower than all other parsers. Table 2 shows the differences between the Spanish and the Chinese parsers (silver evaluation), using the submetrics outlined by Damonte et al. (2017) to better investigate where the Chinese parser falls behind. These metrics assess specific problems the AMR parsers need to face such as concept identification, semantic role labeling, and negation detection. We note large gaps for wikification (identification of Wikipedia identifiers for named entities), which\nenvy\nI\n(a) ES: Tengo envidia de ti\n(I am jealous of you)\nanswer-01\nwe\n(b) IT: Noi daremo una risposta\n(We will answer)\nenter-01\nhome\n(c) IT: Lui e\u0300 entrato nella casa\n(He entered the house)\nhowever has a small impact on the overall score, and concept identification. We focus on concept identification as its the key step to allow effective multilingual parsing: once the concepts with the correct labels have been identified, the creation of the correct relations can be seen as language independent.\nThe problem of concept identification and its relationship with noisy alignments was previously discussed. It is present in all languages but more noticeably for Chinese. In the Chinese example of Figure 2, two critical words were ignored in the parsed AMR graph, whereas in the other\nlanguages only less important words were erroneously discarded. Table 3 shows, for each parser, the percentage of words seen in training that the parser learned to consider as non-content bearing. Such words are ignored during parsing, resulting in AMRs that lack important concepts. Compared to the original English parser all languages have a high percentage of these words, but the difference is more marked for Chinese. Table 4 highlights specific Chinese words that are treated as non-content bearing when they should not, unlike their English translations, which instead correctly trigger AMR concepts in the English parser."}, {"heading": "7 Related Work", "text": "AMR parsing for languages other than English had made only a few steps forward. In previous\nwork on AMR for other languages (Li et al., 2016; Xue et al., 2014; Bojar, 2014) nodes of the target graph were labeled with either English words or with words in the target language. We instead use the same AMR annotation used for English to the target language, without translating any word. To the best of our knowledge, the only previous work that attempts to automatically parse AMR graphs for non-English sentences is by Vanderwende et al. (2015). Sentences in several languages (French, German, Spanish and Japanese) are parsed into a logical representation, which is then converted to AMR using a small set of rules. A comparison with this work is difficult, as the authors did not report results for the parsers (due to the lack of an annotated corpora) or released their code.\nBesides AMR, other semantic parsing frameworks for non-English languages have been investigated (Hoffman, 1992; Cinkova\u0301 et al., 2009; Gesmundo et al., 2009; Evang and Bos, 2016). Evang and Bos (2016) is the most closely related to our work as it uses a projection mechanism similar to ours for CCG. A crucial difference is that, in order to project CCG parse trees to the target languages, they only make use of literal translation, which we argue is not as necessary in our case, since AMR is expected to abstract away from the different syntactic realizations.\nPrevious work has also focused on assessing the stability across languages of semantic frameworks such as AMR (Xue et al., 2014; Bojar, 2014), UCCA (Sulem et al., 2015) and Propbank (Van der Plas et al., 2010). This work assumes that AMR is sufficiently stable and enforces it by making the same AMR graph annotated for English valid for the target language as well. Supporting evidence comes from the preliminary work\non Chinese (Xue et al., 2014) and investigation of the cross-lingual stability of Propbank, on which AMR is partially based (Van der Plas et al., 2010).\nCross-lingual techniques can cope with the lack of labeled data on languages when this data is available in at least one language, normally English. The annotation projection method, which we follow in this work, is only one way to address this problem. It was introduced for dependency parsing (Hwa et al., 2005) but it has also been used for role labeling (Pado\u0301 and Lapata, 2009) and semantic parsing (Evang and Bos, 2016). Another common thread of cross-lingual work is of the model transfer type, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen et al., 2011; Cohen and Smith, 2009; McDonald et al., 2011; S\u00f8gaard, 2011)."}, {"heading": "8 Conclusion", "text": "We proposed a method to overcome the lack of non-English AMR datasets and presented the first results for Italian, Spanish, German and Chinese. Automatic and manual evaluations carried out for these languages are promising and raise hope for further development of AMR parsing for languages other than English. We further proposed a novel way to evaluate the target parsers that does not require manual annotations of the target language. This inversion procedure is not limited to AMR parsing and can be used for other problems in NLP. Finally, we identified weaknesses and the sources of noise in the proposed method, which future work could address."}], "references": [{"title": "Italy goes to stanford: a collection of corenlp modules for italian", "author": ["Alessio Palmero Aprosio", "Giovanni Moretti."], "venue": "arXiv preprint arXiv:1609.06204 .", "citeRegEx": "Aprosio and Moretti.,? 2016", "shortCiteRegEx": "Aprosio and Moretti.", "year": 2016}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Linguistic Annotation Workshop .", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Comparing czech and english amrs", "author": ["Zdenka Ure\u0161ov\u00e1 Jan Hajic Ondrej Bojar."], "venue": "InWorkshop on Lexical and Grammatical Resources for Language Processing.", "citeRegEx": "Bojar.,? 2014", "shortCiteRegEx": "Bojar.", "year": 2014}, {"title": "Smatch: an evaluation metric for semantic feature structures", "author": ["Shu Cai", "Kevin Knight."], "venue": "Proceedings of ACL .", "citeRegEx": "Cai and Knight.,? 2013", "shortCiteRegEx": "Cai and Knight.", "year": 2013}, {"title": "Freeling: An open-source suite of language analyzers", "author": ["Xavier Carreras", "Isaac Chao", "Llus Padr", "Muntsa Padr."], "venue": "Proceedings of LREC).", "citeRegEx": "Carreras et al\\.,? 2004", "shortCiteRegEx": "Carreras et al\\.", "year": 2004}, {"title": "Wit: Web inventory of transcribed and translated talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT). Trento, Italy, pages 261\u2013268.", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Tectogrammatical annotation of the wall street", "author": ["Silvie Cinkov\u00e1", "Josef Toman", "Jan Hajic", "Krist\u1ef3na Cerm\u00e1kov\u00e1", "V\u00e1clav Klime\u0161", "Lucie Mladov\u00e1", "Jana \u0160indlerov\u00e1", "Krist\u1ef3na Tom\u0161u", "Zdenek Zabokrtsk\u1ef3."], "venue": "The Prague Bulletin of", "citeRegEx": "Cinkov\u00e1 et al\\.,? 2009", "shortCiteRegEx": "Cinkov\u00e1 et al\\.", "year": 2009}, {"title": "Unsupervised structure prediction with nonparallel multilingual guidance", "author": ["Shay B. Cohen", "Dipanjan Das", "Noah A. Smith."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Cohen et al\\.,? 2011", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction", "author": ["Shay B Cohen", "Noah A Smith."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the", "citeRegEx": "Cohen and Smith.,? 2009", "shortCiteRegEx": "Cohen and Smith.", "year": 2009}, {"title": "An incremental parser for abstract meaning representation", "author": ["Marco Damonte", "Shay B Cohen", "Giorgio Satta."], "venue": "Proceedings of EACL.", "citeRegEx": "Damonte et al\\.,? 2017", "shortCiteRegEx": "Damonte et al\\.", "year": 2017}, {"title": "Machine translation divergences: A formal description and proposed solution", "author": ["Bonnie J Dorr."], "venue": "Computational Linguistics 20(4):597\u2013633.", "citeRegEx": "Dorr.,? 1994", "shortCiteRegEx": "Dorr.", "year": 1994}, {"title": "Improved word-level alignment: Injecting knowledge about mt divergences", "author": ["Bonnie J Dorr", "Lisa Pearl", "Rebecca Hwa", "Nizar Habash."], "venue": "Technical report, DTIC Document.", "citeRegEx": "Dorr et al\\.,? 2002", "shortCiteRegEx": "Dorr et al\\.", "year": 2002}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A Smith."], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Cross-lingual learning of an open-domain semantic parser", "author": ["Kilian Evang", "Johan Bos."], "venue": "Proceedings of COLING.", "citeRegEx": "Evang and Bos.,? 2016", "shortCiteRegEx": "Evang and Bos.", "year": 2016}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Jeffrey Flanigan", "Sam Thomson", "Jaime G Carbonell", "Chris Dyer", "Noah A Smith."], "venue": "Proceedings of ACL .", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "A latent variable model of synchronous syntactic-semantic parsing for multiple languages", "author": ["Andrea Gesmundo", "James Henderson", "Paola Merlo", "Ivan Titov."], "venue": "Proceedings of CoNLL. Association for Computational Linguistics.", "citeRegEx": "Gesmundo et al\\.,? 2009", "shortCiteRegEx": "Gesmundo et al\\.", "year": 2009}, {"title": "A ccg approach to free word order languages", "author": ["Beryl Hoffman."], "venue": "Proceedings of ACL.", "citeRegEx": "Hoffman.,? 1992", "shortCiteRegEx": "Hoffman.", "year": 1992}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak."], "venue": "Natural language engineering 11(03):311\u2013325.", "citeRegEx": "Hwa et al\\.,? 2005", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "From treebank to propbank", "author": ["Paul Kingsbury", "Martha Palmer."], "venue": "Proceedings of LREC .", "citeRegEx": "Kingsbury and Palmer.,? 2002", "shortCiteRegEx": "Kingsbury and Palmer.", "year": 2002}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "MT summit. volume 5, pages 79\u201386.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Annotating the little prince with chinese amrs", "author": ["Bin Li", "Yuan Wen", "Lijun Bu", "Weiguang Qu", "Nianwen Xue."], "venue": "LAW X Workshop .", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "ACL System Demonstrations.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Ryan McDonald", "Slav Petrov", "Keith Hall."], "venue": "Proceedings of EMNLP.", "citeRegEx": "McDonald et al\\.,? 2011", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "ACL Workshop on Incremental Parsing: Bringing Engineering and Cognition Together .", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Crosslingual annotation projection for semantic roles", "author": ["Sebastian Pad\u00f3", "Mirella Lapata."], "venue": "Journal of Artificial Intelligence Research 36(1):307\u2013340.", "citeRegEx": "Pad\u00f3 and Lapata.,? 2009", "shortCiteRegEx": "Pad\u00f3 and Lapata.", "year": 2009}, {"title": "Data point selection for crosslanguage adaptation of dependency parsers", "author": ["Anders S\u00f8gaard."], "venue": "Proceedings of ACL-HLT.", "citeRegEx": "S\u00f8gaard.,? 2011", "shortCiteRegEx": "S\u00f8gaard.", "year": 2011}, {"title": "Conceptual annotations preserve structure across translations: A french-english case study", "author": ["Elior Sulem", "Omri Abend", "Ari Rappoport."], "venue": "Workshop on Semantics-Driven Statistical Machine Translation.", "citeRegEx": "Sulem et al\\.,? 2015", "shortCiteRegEx": "Sulem et al\\.", "year": 2015}, {"title": "Cross-lingual validity of propbank in the manual annotation of french", "author": ["Lonneke Van der Plas", "Tanja Samard\u017ei\u0107", "Paola Merlo."], "venue": "Linguistic Annotation Workshop.", "citeRegEx": "Plas et al\\.,? 2010", "shortCiteRegEx": "Plas et al\\.", "year": 2010}, {"title": "An amr parser for english, french, german, spanish and japanese and a new amr-annotated corpus", "author": ["Lucy Vanderwende", "Arul Menezes", "Chris Quirk."], "venue": "Proceedings of NAACL-HLT. pages 26\u201330.", "citeRegEx": "Vanderwende et al\\.,? 2015", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2015}, {"title": "Camr at semeval-2016 task 8: An extended transition-based amr parser", "author": ["Chuan Wang", "Sameer Pradhan", "Nianwen Xue", "Xiaoman Pan", "Heng Ji."], "venue": "Proceedings of SemEval pages 1173\u20131178.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Not an interlingua, but close: Comparison of english amrs to chinese and czech", "author": ["Nianwen Xue", "Ondrej Bojar", "Jan Hajic", "Martha Palmer", "Zdenka Uresova", "Xiuhong Zhang."], "venue": "Proceedings of LREC.", "citeRegEx": "Xue et al\\.,? 2014", "shortCiteRegEx": "Xue et al\\.", "year": 2014}, {"title": "Crosslanguage parser adaptation between related languages", "author": ["Daniel Zeman", "Philip Resnik."], "venue": "IJCNLP.", "citeRegEx": "Zeman and Resnik.,? 2008", "shortCiteRegEx": "Zeman and Resnik.", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Abstract Meaning Representation (AMR) parsing converts natural language sentences into their corresponding AMRs (Banarescu et al., 2013).", "startOffset": 112, "endOffset": 136}, {"referenceID": 17, "context": "One way to do this is by annotation projection, where the existing annotations are projected to a target language through a parallel corpus (Hwa et al., 2005).", "startOffset": 140, "endOffset": 158}, {"referenceID": 1, "context": "The original AMR definition states that AMR is not an interlingua (Banarescu et al., 2013).", "startOffset": 66, "endOffset": 90}, {"referenceID": 1, "context": "The original AMR definition states that AMR is not an interlingua (Banarescu et al., 2013). Bojar (2014) categorizes the different kinds of divergences in the annotation between English AMRs and Czech AMRs.", "startOffset": 67, "endOffset": 105}, {"referenceID": 1, "context": "The original AMR definition states that AMR is not an interlingua (Banarescu et al., 2013). Bojar (2014) categorizes the different kinds of divergences in the annotation between English AMRs and Czech AMRs. Xue et al. (2014) showed that structurally aligning Czech and English AMRs is problematic but reached different conclusions for the language pair EnglishChinese, showing that their AMRs can be aligned, suggesting that, with tailored annotation guidelines, it could be possible to make AMR behave as an interlingua.", "startOffset": 67, "endOffset": 225}, {"referenceID": 18, "context": "It relies on Propbank (Kingsbury and Palmer, 2002) to define the main predicates in the sentence.", "startOffset": 22, "endOffset": 50}, {"referenceID": 3, "context": "In order to evaluate AMR parsers, the Smatch score (Cai and Knight, 2013) is traditionally used.", "startOffset": 51, "endOffset": 73}, {"referenceID": 23, "context": "parser (Nivre, 2004): the sentence is stored in a buffer, which is scanned once left-to-right and the words in the sentences are either ignored or trigger a subgraph in a stack.", "startOffset": 7, "endOffset": 20}, {"referenceID": 29, "context": "67 on the same dataset (Wang et al., 2016).", "startOffset": 23, "endOffset": 42}, {"referenceID": 24, "context": "Since we rely on several automatic tools, there are several sources of noise in the method: 1) the parsers are trained on silver data obtained by an automatic parser for English (e \u2192 f ) or even by an automatic parser trained on data obtained by a parser which in turn was learned with silver data (f \u2192 e); 2) the projection uses noisy word alignments (Pad\u00f3 and Lapata, 2009); 3) the AMR alignments on the source side are also noisy; 4) translation divergences exist between the languages, making it sometimes not possible to project the annotation without loss of information.", "startOffset": 352, "endOffset": 375}, {"referenceID": 19, "context": "For Italian, Spanish and German we use Europarl (Koehn, 2005), containing around 1.", "startOffset": 48, "endOffset": 61}, {"referenceID": 5, "context": "9M sentences for each language pair, and the TED talks corpus (Cettolo et al., 2012) for Chinese, containing around 120K sentences.", "startOffset": 62, "endOffset": 84}, {"referenceID": 12, "context": "Word alignments were generated using fast align (Dyer et al., 2013), while AMR alignments were generated with JAMR (Flanigan et al.", "startOffset": 48, "endOffset": 67}, {"referenceID": 14, "context": ", 2013), while AMR alignments were generated with JAMR (Flanigan et al., 2014).", "startOffset": 55, "endOffset": 78}, {"referenceID": 9, "context": "As an English AMR parser and a starting point to develop the target parsers we used AMREager (Damonte et al., 2017), which is an open-source AMR parser for English that requires only small modifications for re-use on other languages.", "startOffset": 93, "endOffset": 115}, {"referenceID": 21, "context": "parsing, which for English, German and Chinese are provided by CoreNLP (Manning et al., 2014).", "startOffset": 71, "endOffset": 93}, {"referenceID": 4, "context": "We use Freeling (Carreras et al., 2004) instead.", "startOffset": 16, "endOffset": 39}, {"referenceID": 0, "context": "Italian is not supported in CoreNLP: we use Tint (Aprosio and Moretti, 2016), which is a CoreNLP-compatible NLP pipeline for Italian.", "startOffset": 49, "endOffset": 76}, {"referenceID": 13, "context": "For these languages, the gap with the original English parser is around 20%, which is comparable to the 15% gap reported for annotation projection based cross-lingual semantic parsing for French under the combinatory categorial grammar (CCG) framework (Evang and Bos, 2016).", "startOffset": 252, "endOffset": 273}, {"referenceID": 5, "context": "Even within the TED talks corpus, the Chinese-English pair has been shown to be especially challenging (Cettolo et al., 2012).", "startOffset": 103, "endOffset": 125}, {"referenceID": 11, "context": "In this section, we look at this type of divergence and discuss how it affects parsing, following the classification used in previous work (Dorr et al., 2002; Dorr, 1994), which identify classes of divergences for several languages.", "startOffset": 139, "endOffset": 170}, {"referenceID": 10, "context": "In this section, we look at this type of divergence and discuss how it affects parsing, following the classification used in previous work (Dorr et al., 2002; Dorr, 1994), which identify classes of divergences for several languages.", "startOffset": 139, "endOffset": 170}, {"referenceID": 10, "context": "In this section, we look at this type of divergence and discuss how it affects parsing, following the classification used in previous work (Dorr et al., 2002; Dorr, 1994), which identify classes of divergences for several languages. Sulem et al. (2015) also follow the same categorization for French.", "startOffset": 140, "endOffset": 253}, {"referenceID": 9, "context": "Table 2 shows the differences between the Spanish and the Chinese parsers (silver evaluation), using the submetrics outlined by Damonte et al. (2017) to better investigate where the Chinese parser falls behind.", "startOffset": 128, "endOffset": 150}, {"referenceID": 20, "context": "work on AMR for other languages (Li et al., 2016; Xue et al., 2014; Bojar, 2014) nodes of the target graph were labeled with either English words or with words in the target language.", "startOffset": 32, "endOffset": 80}, {"referenceID": 30, "context": "work on AMR for other languages (Li et al., 2016; Xue et al., 2014; Bojar, 2014) nodes of the target graph were labeled with either English words or with words in the target language.", "startOffset": 32, "endOffset": 80}, {"referenceID": 2, "context": "work on AMR for other languages (Li et al., 2016; Xue et al., 2014; Bojar, 2014) nodes of the target graph were labeled with either English words or with words in the target language.", "startOffset": 32, "endOffset": 80}, {"referenceID": 2, "context": ", 2014; Bojar, 2014) nodes of the target graph were labeled with either English words or with words in the target language. We instead use the same AMR annotation used for English to the target language, without translating any word. To the best of our knowledge, the only previous work that attempts to automatically parse AMR graphs for non-English sentences is by Vanderwende et al. (2015). Sentences in several languages (French, German, Spanish and Japanese) are parsed into a logical representation, which is then converted to AMR using a small set of rules.", "startOffset": 8, "endOffset": 393}, {"referenceID": 16, "context": "Besides AMR, other semantic parsing frameworks for non-English languages have been investigated (Hoffman, 1992; Cinkov\u00e1 et al., 2009; Gesmundo et al., 2009; Evang and Bos, 2016).", "startOffset": 96, "endOffset": 177}, {"referenceID": 6, "context": "Besides AMR, other semantic parsing frameworks for non-English languages have been investigated (Hoffman, 1992; Cinkov\u00e1 et al., 2009; Gesmundo et al., 2009; Evang and Bos, 2016).", "startOffset": 96, "endOffset": 177}, {"referenceID": 15, "context": "Besides AMR, other semantic parsing frameworks for non-English languages have been investigated (Hoffman, 1992; Cinkov\u00e1 et al., 2009; Gesmundo et al., 2009; Evang and Bos, 2016).", "startOffset": 96, "endOffset": 177}, {"referenceID": 13, "context": "Besides AMR, other semantic parsing frameworks for non-English languages have been investigated (Hoffman, 1992; Cinkov\u00e1 et al., 2009; Gesmundo et al., 2009; Evang and Bos, 2016).", "startOffset": 96, "endOffset": 177}, {"referenceID": 6, "context": "Besides AMR, other semantic parsing frameworks for non-English languages have been investigated (Hoffman, 1992; Cinkov\u00e1 et al., 2009; Gesmundo et al., 2009; Evang and Bos, 2016). Evang and Bos (2016) is the most closely related to our work as it uses a projection mechanism similar to ours for CCG.", "startOffset": 112, "endOffset": 200}, {"referenceID": 30, "context": "Previous work has also focused on assessing the stability across languages of semantic frameworks such as AMR (Xue et al., 2014; Bojar, 2014), UCCA (Sulem et al.", "startOffset": 110, "endOffset": 141}, {"referenceID": 2, "context": "Previous work has also focused on assessing the stability across languages of semantic frameworks such as AMR (Xue et al., 2014; Bojar, 2014), UCCA (Sulem et al.", "startOffset": 110, "endOffset": 141}, {"referenceID": 26, "context": ", 2014; Bojar, 2014), UCCA (Sulem et al., 2015) and Propbank (Van der Plas et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 30, "context": "Supporting evidence comes from the preliminary work on Chinese (Xue et al., 2014) and investigation of the cross-lingual stability of Propbank, on which AMR is partially based (Van der Plas et al.", "startOffset": 63, "endOffset": 81}, {"referenceID": 17, "context": "It was introduced for dependency parsing (Hwa et al., 2005) but it has also been used for role labeling (Pad\u00f3 and Lapata, 2009) and semantic parsing (Evang and Bos, 2016).", "startOffset": 41, "endOffset": 59}, {"referenceID": 24, "context": ", 2005) but it has also been used for role labeling (Pad\u00f3 and Lapata, 2009) and semantic parsing (Evang and Bos, 2016).", "startOffset": 52, "endOffset": 75}, {"referenceID": 13, "context": ", 2005) but it has also been used for role labeling (Pad\u00f3 and Lapata, 2009) and semantic parsing (Evang and Bos, 2016).", "startOffset": 97, "endOffset": 118}, {"referenceID": 31, "context": "Another common thread of cross-lingual work is of the model transfer type, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen et al., 2011; Cohen and Smith, 2009; McDonald et al., 2011; S\u00f8gaard, 2011).", "startOffset": 120, "endOffset": 225}, {"referenceID": 7, "context": "Another common thread of cross-lingual work is of the model transfer type, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen et al., 2011; Cohen and Smith, 2009; McDonald et al., 2011; S\u00f8gaard, 2011).", "startOffset": 120, "endOffset": 225}, {"referenceID": 8, "context": "Another common thread of cross-lingual work is of the model transfer type, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen et al., 2011; Cohen and Smith, 2009; McDonald et al., 2011; S\u00f8gaard, 2011).", "startOffset": 120, "endOffset": 225}, {"referenceID": 22, "context": "Another common thread of cross-lingual work is of the model transfer type, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen et al., 2011; Cohen and Smith, 2009; McDonald et al., 2011; S\u00f8gaard, 2011).", "startOffset": 120, "endOffset": 225}, {"referenceID": 25, "context": "Another common thread of cross-lingual work is of the model transfer type, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen et al., 2011; Cohen and Smith, 2009; McDonald et al., 2011; S\u00f8gaard, 2011).", "startOffset": 120, "endOffset": 225}], "year": 2017, "abstractText": "Abstract Meaning Representation (AMR) annotation efforts have mostly focused on English. In order to train parsers on other languages, we propose a method based on annotation projection, which involves exploiting annotations in a source language and a parallel corpus of the source language and a target language. Using English as the source language, we show promising results for Italian, Spanish, German and Chinese as target languages. Besides evaluating the target parsers on nongold datasets, we further propose an evaluation method that exploits the English gold annotations and does not require access to gold annotations for the target languages. This is achieved by inverting the projection process: a new English parser is learned from the target language parser and evaluated on the existing English gold standard.Meaning Representation (AMR) annotation efforts have mostly focused on English. In order to train parsers on other languages, we propose a method based on annotation projection, which involves exploiting annotations in a source language and a parallel corpus of the source language and a target language. Using English as the source language, we show promising results for Italian, Spanish, German and Chinese as target languages. Besides evaluating the target parsers on nongold datasets, we further propose an evaluation method that exploits the English gold annotations and does not require access to gold annotations for the target languages. This is achieved by inverting the projection process: a new English parser is learned from the target language parser and evaluated on the existing English gold standard.", "creator": "LaTeX with hyperref package"}}}