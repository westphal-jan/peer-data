{"id": "1704.04684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "Generic LSH Families for the Angular Distance Based on Johnson-Lindenstrauss Projections and Feature Hashing LSH", "abstract": "In this paper pirdop we 169.7 propose the are/were creation of 33sec generic phule LSH unhappiness families for the 844 angular distance based on recognizing Johnson - filarete Lindenstrauss cornell\u00e0 projections. p40 We show 2,161 that .348 feature cavaleiros hashing is breakeven a tenbury valid jyvaskyla J - L projection pasona and propose same-titled two 1.4075 new LSH families cadete based on feature hashing. heuheu These charizard new cookstoves LSH sabbatai families are benegas tested on both synthetic and precise real 0.23 datasets elzer with very good amen results and keyboarder a considerable ctenophores performance mosleh improvement 20.14 over other iffhs LSH herrn families. zanca While damasceno the aharanot theoretical analysis francais is done deglamorized for the underhanded angular distance, these altana families can sedai also meghdoot be used depature in nefarious practice for hinchey the euclidean distance suchai with excellent results [al-zarqawi 2 ]. tharandt Our tests chintz using tarrifs real nachum datasets show that the pavlos proposed LSH balrog functions work well nuerburgring for right-center the 1.5-percent euclidean distance.", "histories": [["v1", "Sat, 15 Apr 2017 19:32:51 GMT  (876kb,D)", "http://arxiv.org/abs/1704.04684v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.IR", "authors": ["luis argerich", "natalia golmar"], "accepted": false, "id": "1704.04684"}, "pdf": {"name": "1704.04684.pdf", "metadata": {"source": "CRF", "title": "Generic LSH Families for the Angular Distance Based on Johnson-Lindenstrauss Projections and Feature Hashing LSH", "authors": ["Luis Argerich", "Natalia Golmar"], "emails": ["largerich@fi.uba.ar", "ngolmar@gmail.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nLocality Sensitive Hashing (LSH) is a modern solution to the Approximate Nearest Neighbors problem (ANN) for large datasets. The basic idea of LSH is to hash similar items to the same bucket. After hashing a query point we can recover the points in the bucket as candidates and compute the distance to the query only for those points. This is a significant improvement over the brute force approach of comparing the query point against all the other points in the dataset.\nIn this paper we work with LSH for the angular distance over the unit sphere. The angular distance is widely used in areas such as information retrieval and word embeddings.[13][14][15] In the practice the LSH schemes created for the angular distance can also be used for the euclidean distance with very good results. This work will propose two new LSH Families based on Feature Hashing and show, in practice, that they have similar results to other well known LSH families and significant performance improvements for both synthetic and real datasets. We will also generalize the LSH families for the angular distance to the use of any form of a JohnsonLindenstrauss projection and show that classical LSH families for the angular distance can be generalized deriving a minhash function from a Johnson-Lindenstrauss projection."}, {"heading": "II. PRELIMINARIES AND NOTATION", "text": "LSH was introduced by Indyk and Motwani in 1998 [7][6]. They basic idea of LSH is to hash similar points to the same bucket, this allows approximate O(1) query time when\nretrieving near neighbors. Given a query point we hash the point and then go to the bucket pointed by the LSH function and compare the query against the points found in the bucket. A very good LSH function will minimize the number of points to be compared and maximize the number of real near neighbors found in the bucket.\nWe define a minhash h(x) \u2192 [0..m) as the result of applying a hash function to a d dimensional point x and obtaining a bucket number to store the point between 0 and m\u2212 1. For a hash function to be considered a good minhash we require the following conditions:\n1) The hash function needs to be easy to compute. 2) It should be easy to extend the hash function to a\nfamily of hash functions allowing an arbitrary number of minhashes to be created for the same point. 3) The probability of a collision needs to be related to the distance between the points and it has to be a continuous monotonous function. So if d(x, y) \u2264 d(x, z) then P [h(x) = h(y)] \u2265 P [h(x) = h(z)]. The inequalities are not relevant as long as the function is monotonous and continuous.\nWe say an LSH family of hash functions H is p1, p2 sensitive for d1 and d2 distances: H(d1, d2, p1, p2) if for any two given points x, y then when the points are at distance d1 or less then P [h(x) = h(y)] \u2265 p1 and when the points are at a distance p2 or greater then P [h(x) = h(y)] \u2264 p2.\nIt can be shown that conditions 2 and 3 are enough to amplify any family H to arbitrary values of p1 and p2, this can be done using a process known as amplification."}, {"heading": "III. AMPLIFICACION OF LSH FAMILIES", "text": "For any LSH family H(d1, d2, p1, p2) where p1 is the probability of retrieving points that are close to a query point, we would like p1 to be as high as possible. 1 \u2212 p1 is the occurrence of false negatives meaning that some points that are closer than d1 to our query point won\u2019t be retrieved. On the other hand p2 is the probability of retrieving points that are further than desired to our query point; p2 is the probability\nar X\niv :1\n70 4.\n04 68\n4v 1\n[ cs\n.D S]\n1 5\nA pr\nof false positives we would like p2 to be as small as possible. In general terms false negatives affect the precision of the LSH scheme while false positives affect its performance as the number of distances that need to be compared can be high.\nThe rate of false positives can be reduced using more than one minhash over the same hash table, if we use r minhashes then the candidate points are the intersection of the points found in the buckets pointed by hr\u22121i=0 (x). This applies to both p1 and p2 so in general for r hash functions used in conjunction the collision probability p becomes pr.\nThe rate of false negatives can be reduced using more than one hash table, then we can say that a point is candidate to be close to our query if it is a candidate in either of the b hash tables used. This means that the probability of collisions p is 1\u2212 (1\u2212 pr)b\nSo using r hash functions and b hash tables we get the LSH family H(d1, d2, 1\u2212 (1\u2212 pr1)b, 1\u2212 (1\u2212 pr2)b)\nAs long as the probability of a collision is a monotonous function of the distance between the points then, for any LSH family, we can achieve arbitrary values for p1 and p2. The cost of increasing b is related to space as more hash tables are needed while the cost of increasing r is related to performance as more hash functions are needed. The process of amplification shows why condition 2 is important as an arbitrary number of different minhashes might be needed.\nA minhash function is neutral if it\u2019s probability of collision is equal to 1 minus the normalized euclidean distance between the points. Minhash functions that show a curve above this line have a higher probability of collision so they have less false negatives and more false positives. Minhash functions below the neutral line have a lower collision probability and then produce less false positives but more false negatives. Depending on the constraints of space or performance we might prefer one case or the other. This is an important observation because it directly points to a practical rule for choosing minhash functions."}, {"heading": "IV. PREVIOUS WORK", "text": ""}, {"heading": "A. Hyperplanes LSH", "text": "Our first LSH family for the angular distance is based on random hyperplanes minhashes and was proposed by Charikar in 2002 [4]. The construction is very simple: every minhash uses a random hyperplane vi of the same dimensionality as the points to be hashed, the minhash is then defined as the sign of the dot product between the point and the random vector. hi(x) = sign < vi, x >. Since the minhash can only take 2 values each minhash defines a bit, we can create a d\u2032 \u2212 bit minhash using d\u2032 minhashes. Since each hyperplane can be seen as a partition of the unit hypersphere in two halves then we can see that the probability of collision is related to the distance 1 between the points. In concrete the probability of collision is 1\u2212 \u03b1\u03c0 where \u03b1 is the angle between the vectors.\n1from now on we\u2019ll use distance to refer indistinctly to the angular distance or the euclidean distance in the unit hypersphere\nIt was also shown in [4] that a random vector formed by just \u00b11 elements is enough as a random hyperplane. This is related to a Johnson Lindenstrauss projection presentd by Achlioptas[1].\nB. Voronoi LSH\nVoronoi LSH [3] uses T random Gaussian vectors of the same dimensionality as the data points. The minhash is defined as h(x) = argmaxi=0...T\u22121< Gi, x > This means that using T Gaussian vectors each minhash can create T different values. It is easy to show that if T = 2 then Voronoi LSH is the same as Hyperplanes LSH because choosing the closest point from two random points in the sphere is the same as randomly partitioning the sphere bisecting the two random points. In general Voronoi LSH with T = 2K Gaussian vectors is similar to Hyperplanes LSH with k hyperplanes."}, {"heading": "C. Cross Polytope LSH", "text": "The Cross Polytope LSH method was introduced by Teresawa and Tanaka in 2007 [10]. Each minhash uses a random rotation from d to d\u2032 dimensions and then the nearest vertex of the d\u2019-dimensional cross polytope is chosen as the value of the minhash. In d\u2032 dimensions each polytope has 2d\u2032 vertices, for example in <2 the polytope is a square determined by the vertices (0,1);(1,0);(-1,0) and (0,-1).\nThe Cross Polytope method is actually a variant of Voronoi Hashing, if we accept a Gaussian matrix as a pseudo-rotation and we consider the maximum absolute value plus the sign of the element instead of just the maximum value then Voronoi Hashing is the same as the Cross Polytope method. For example using 5 Gaussian vectors we might get the following results for two points x, y: x\u2192 (3, 2,\u22125,\u22121, 2) y \u2192 (1, 4,\u22126, 3, 1). Using Voronoi Hashing the minhash for x is 0 the index of the maximum value while the minhash for y is 1. Using Cross Polytope we observe that both would be closer to the vertex (0, 0,\u22121, 0, 0) and this is the same as taking the index and sign of the maximum absolute value of the vectors as the minhash which is 2 for both vectors.\nTo speed up the computation of the rotation a pseudorotation using Hadamard matrices has been proposed [2]."}, {"heading": "D. Fast Cross Polytope LSH", "text": "Kennedy[8] proposes a faster version of the Cross Polytope method using a Fast Johnson Lindenstrauss transform from the original d dimensions to a reduced space with m dimensions and then the random lifted rotation from m to d\u2032 dimensions. Our experiments show that for small dimensionality vectors this method is actually slower in practice that a direct random rotation and the other methods studied. As the dimensionality of the vectors is larger this method can become more efficient but then a direct dimensionality reduction of the vectors using feature hashing can be applied."}, {"heading": "E. Even Faster Cross Polytope LSH", "text": "Instead of a FJL transform feature hashing can be used making the method from [8] a lot faster. This means FH is\nused to project from d to m dimensions and then a random rotation is used from m to d\u2032 dimensions. Experiments show that the results offer very similar precision but are significant faster but yet not as fast as the other methods studied in this work.In general the two-step approach from d to m and from m to d\u2032 to obtain a minhash is not a speed improvement over a direct minhash from d to d\u2032."}, {"heading": "V. JOHNSON-LINDENSTRAUSS PROJECTIONS FOR LSH", "text": "We have mentioned that any function h(x) where P [h(x) = h(y)] = f(||x \u2212 y||) with f monotonous can be used as a minhash. In particular a family of functions that transforms the points from one dimensional space to another preserving the norms of the vectors can be used and thus any projection derived from the Johnson-Lindenstrauss lemma is usable as a minhash. This can be proven for sparse vectors independently of the number of vectors to be used via the Restricted Isometry Property (RIP) as described in [16], for non-sparse vectors the number of points matters as the Johnson-Lindenstrauss is applied and then for a large number of points the pairwise distances between vectors is preserved with a small error. We notice that if the number of points is not small then we don\u2019t need to use LSH so we can say that the pairwise distances between points are preserved with a high probability.\nVoronoi LSH is indeed a direct application of a JohnsonLindenstrauss projection using a Gaussian Matrix [1]. Hyperplanes LSH is another application of a JL projection using a matrix with \u00b11 random elements [1].\nWe now show that for any random projection that is also a Johnson-Lindenstrauss transform two generic families of LSH functions can be created defining two minhashes.\nTheorem 1: If p is a randomizable Johnson-Lindenstrauss projection from d to d\u2032 dimensions then p can be used to construct at least two different LSH families for the angular distance based in the following minhashes.\n1) argmax0..d\u2032\u22121< pi, x > with pi being the ith column of p 2) sign(< pi, x >) with pi being the ith column of p\nThe theorem is almost self-proven. For method 1 we are comparing the closest point from a set of random points in the unit sphere, since the projection preserves the distances the probability of a collision is a monotonous function of the original distance between the points. For method 2 we are bisecting the sphere in two halves, the probability of two points being in the same halve is again a monotonous function of the original distance between the points because p preserves the distances between the points.\nIt can be seen that when p is a Gaussian projection method 1 is Voronoi Hashing. And when p is a random projection filled with \u00b11 method 2 is hyperplanes LSH.Remembering that Cross-Polytope LSH is a form of Voronoi LSH this means that all the LSH families we have described can be generalized to the creation of a minhash from a Johnson-Lindenstrauss projection.\nIn this work we will show that Feature Hashing is also a JL projection and that will be the theoretical foundation to use it to create a minhash and a LSH family for the angular distance."}, {"heading": "VI. FEATURE HASHING FOR LSH", "text": "Feature Hashing [9] , also known as The Hashing Trick is a very simple method for dimensionality reduction from the original d dimensions to d\u2032. A nice advantage is that when a feature of our data is categorical it can be hashed into several dimensions without needing to know the total number of different values for the feature. To mitigate the effect of collisions Weinberger [11] proposed the use of a second hash function that will return the sign to be used (\u00b11). With the addition of the second function the effect of collisions is mitigated and several features can be hashed into the same target space with minimal interplay[11]."}, {"heading": "A. Feature Hashing is a Johnson Lindenstrauss Transform", "text": "Feature hashing as a random projection is used in [16] to transform sparse vectors preserving pairwise distances, [11] also shows that FH preserves the distances between vectors.\nWe start showing that Feature Hashing can be represented as a matrix. Each feature (column) of our original vectors is mapped by the first hash function from d dimensions to d\u2032 and the second hash function determines the sign. This is the same as multiplying the original (1xd) vector by a dxd\u2032 matrix where each row contains exactly one \u00b11 element. If we use k hash functions instead of just one then each row contains up to k \u00b11 elements and it might contain up to \u00b1k valued elements due to collisions.\nSo for example if we have the data-point V = (0, 1, 0, 3, 0.5, 0, 1) in <7 and we use FH to convert it to d\u2032 = 4 dimensions we can use the following hash function:\nh(0) = 2 s(0)=+1 h(1) = 1 s(1)=+1 h(2) = 3 s(2)=-1 h(3) = 0 s(3)=+1 h(4) = 1 s(4)=-1 h(5) = 2 s(5)=-1 h(6) = 3 s(6)=-1\nThis means the resulting vector is V \u2032 = (3, 0.5, 0,\u22121). This is the same as multiplying our 1x7 vector by a 7x4\nmatrix in the form:\n( 0 1 0 3 0.5 0 1 ) \u2217  0 0 1 0 0 1 0 0 0 0 0 \u22121 1 0 0 0 0 \u22121 0 0 0 0 \u22121 0 0 0 0 \u22121  = ( 3 0.5 0 \u22121 )\nNow we\u2019ll show that these kind of matrices preserve the norms of the vectors from the original space which is a generalization of [9] and [17].\nTheorem 2: Given A a dxd\u2032 matrix where each row has k \u00b11 elements and the rest are zeros. We want to show that this matrix preserves the norms of the vectors from d dimensions projected into d\u2032 dimensions up to a given scale factor.\n||w|| \u2248 \u221a k||v||\nTo prove the theorem we start from the fact that if we have a vector vd in <d and we apply a dense random projection with \u00b11 elements the square of the projection has an expected value equal to the square of the norm this is because of concentration of measures.\nEach column of our dxd\u2032 matrix is equivalent to a random projection of d elements, hence when we compute v \u2217 A we are obtaining d\u2032 different random projections. The difference is that our matrix doesn\u2019t have d \u00b11 elements in each column. The total number of non-zero elements in our matrix is d \u2217 k ignoring collisions. Then the average number of non-zero elements in our matrix is dkd\u2032\nSince we know that with d \u00b11 elements the square of the projection has an expected value equal to the square of the norm, then if we have km \u00b11 elements the square of the projection has an expected value equal to km ||v|| 2\nw2i = k\nm ||v||2\nWe can now compute the expected value of the norm of vm as:\nw2i = k\nm ||v||2\u2211\ni\nw2i = m k\nm ||v||2\u221a\u2211\ni\nw2i =\n\u221a m k\nm ||v||2\n||w|| = \u221a k||v||\nAnd this proves the theorem."}, {"heading": "B. Method 1: Feature Hashing LSH", "text": "Having shown that Feature Hashing is a form of JohnsonLindenstrauss projection and established that any JohnsonLindenstrauss projection can be used as a minhash, we propose two LSH families based in Feature Hashing. The first one is a direct application of Feature Hashing.\nEach minhash uses a random dxd\u2032 matrix M with k \u00b11 elements in each row. The minhash is then computed as:\nh(x) = argmax i=0..d\u2032\u22121 < x,Mi >\nWhere Mi is the ith column of M . Using different random matrices we can create different minhashes to amplify the family."}, {"heading": "C. Method 2: Directional Feature Hashing LSH", "text": "In method 2 we are again using a dxd\u2032 matrix M with k \u00b11 elements, then we consider the sign of each element in d\u2032 and build a d\u2032-bits minhash as the result. This is the same as hyperplane LSH but using only k \u00b11 elements in each random hyperplane. If the method is usable then we have a direct improvement in performance compared to the LSH families we have studied.\nh(x) = sign(< x,Mi >)\nWhere Mi is the ith column of M"}, {"heading": "VII. RESULTS", "text": "We now turn to results we obtained applying these methods to a synthetic and real dataset. For the synthetic dataset we build random vectors with d = 128 dimensions in the unit hypersphere. For the real dataset we used the SIFT 1 million dataset [5] that is widely used for Near Neighbors experiments. We note that for SIFT the vectors are not in the Unit Hypersphere but the methods proposed work very well for the euclidean distance showing that in the practice these LSH families can be used for both angular or euclidean distances.\nThe first graph (Figure 1) shows the probability of collisions by the Euclidean distance for different LSH families in the Unit hypersphere.\nThe graph shows several interesting things. Hyperplane LSH is almost the same as Directional Feature Hashing but the second is faster because its vectors are sparser. Both families have the lowest rate of false positives but also the highest rate of false negatives. Using less bits for each minhash it is easy to decrease the number of false negatives with the cost of increasing the false positives.\nFeature Hashing has the highest rate of false positives and the lowest rate of false negatives. The effect can be mitigated using more hash functions for the same minhash, which means using more than 1 \u00b11 element in each row of the matrix. Voronoi LSH, Cross Polytope and the Fast Cross Polytope are very similar, which is expected as we have shown they are almost exactly the same thing.\nFor the SIFT dataset we run an experiment setting a desired value of p = 0.95 for distances 0.2 or less and p = 0.05 for distances 0.6 or more. This means that vectors that are at distance 0.2 or less will have a probability of collision of 0.95 or more while vectors that are at distances 0.6 or more will have a probability of collision of 0.05 or less.\nThe idea was to see which values of r (number of hash functions) and b (number of hash tables) were needed to amplify each minhash to the target probabilities for each method.\nThe results are very interesting, we can see that all the methods use a similar number of total hash functions. Hyperplane LSH and Directional Feature Hashing need less functions per table but more tables, so they are very good when space is not limited and performance is critical. Feature Hashing needs less hash tables but more hash functions so it is a family to consider when space is critical. In general each LSH family can work better or worst depending on the data and the target probabilities for false negatives and false positives. All these LSH families are usable and need to be considered carefully when choosing a LSH family for an application.\nNext we did some speed tests on the SIFT dataset, we multiplied the speed of a single hash function by the number of hash functions needed in each family to obtain a 0.05 probability for both false positives and false negatives. This is done to even out the advantages and disadvantages of each particular function.\nThe Fast Cross Polytope was discarded because it was very slow compared to the other methods, this is because the FJL transformation takes time and then a further rotation\nis needed. The two step approach was never faster than the direct application of a simpler LSH family. Cross Polytope and Voronoi are very similar because they are the same thing. Hyperplane Hashing and Directional Feature Hashing are much faster and the direct application of Feature Hashing was the fastest method. This means that even needing more hash functions to mitigate false positives Feature Hashing is still the fastest LSH family for the SIFT dataset. This is very logical as each hash function only does a limited number of additions and subtractions, no multiplications are needed and each element in the vector is only added or subtracted once to the resulting vector.\nThe rate of how precision changes was also studied.\nIt can be seen (Figure 3) that as more hash tables are used the precision increases in an almost identical rate in all the LSH families tested, this is logically explained as they are all based on a form of a Johnson-Lindenstrauss transform that preserves the norms of the vectors. The study was made to discard the possibility of an LSH family having a steeper precision increase which would mean that less functions would be needed to achieve a target precision compared to the other methods. As it can be seen that is not the case."}, {"heading": "VIII. ANALYSIS", "text": ""}, {"heading": "A. Feature Hashing LSH", "text": "Feature Hashing is a very flexible LSH family. It can be applied in matrix form or using hash functions, the later is very practical for data with categorical columns or text where the other methods can\u2019t be used without doing a data transformation first. In matrix form we have only k non-zero values in each row making its time complexity O(d\u2217k). Since each member of the original vector is only used once and only an addition or subtraction is performed, we can claim that the method is optimal in terms of speed.\nThe number of hash functions or the number of nonzero values in each row of the associated matrix can be tuned to reduce the number of false-positives independently of amplification. Adding more \u00b11 elements decreases the rate of false positives at the cost of increasing the computation for each minhash.\nWe observe (Figure 4) that for vectors in 128 dimensions we don\u2019t need to use a full 128 dimension rotation to reduce the collision probability and the number of false positives. Even using only a few \u00b11 elements is enough to decrease the rate of false positives. Sensitivity tests are needed to evaluate the optimal number of \u00b11 elements in terms of performance and false-positive rates.\nThe most interesting result is that for a desired LSH family in the form H(d1, d2, p1, p2) Feature Hashing is the fastest method at exactly the same rate of false positives and false negatives."}, {"heading": "B. Directional Feature Hashing LSH", "text": "Directional Feature Hashing is very similar to Hyperplanes LSH but faster because only k\u00b11 elements are present in each projection. When k = d the method is equal to Hyperplanes Hashing. The number of bits can be tuned to make the method work as expected independently of the amplification used.\nThis LSH family shows that hyperplanes LSH can be made faster just sparsifying the random projections used."}, {"heading": "IX. CONCLUSION", "text": "We have generalized LSH families for the angular distance showing the requirements for a function to be considered a minhash. In general terms any randomized function that depends on the distance between the vectors is suitable as a minhash function. The number of different minhashes that can be created is the size of the LSH family and a high number is desired.\nWe showed how any minhash can be extended to work within the parameters of false positives and false negatives we expect using amplification. We proved that any form of a random Johnson-Lindenstrauss projection can be used to create an LSH family for the angular distance because the projections preserve the norms of the vectors.\nWe showed how Feature Hashing is a form of a JohnsonLindenstrauss projection. Then two new LSH families were\nproposed based in Feature Hashing, one with a low rate of false negatives and a higher rate of false positives and the other with a low rate of false positives and a higher rate of false negatives. Depending on the constraints in performance and space one or the other can be used and amplified to achieve the desired results. A very important characteristic of the two methods presented is that they are very fast in performance and very simple to implement.\nIn terms of optimality Spherical hashing [3] is optimal in terms of precision but the minhashes are not practical to compute while Feature Hashing as presented here is optimal in terms of speed and the corresponding minhashes can be used in the practice because they offer good precision after amplification."}], "references": [{"title": "Database-friendly random projections: Johnson- Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Sciences", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Practical and Optimal LSH for Angular Distance", "author": ["A. Andoni", "P. Indyk", "T. Laarhoven", "I. Razenshteyn", "L. Schmidt"], "venue": "Nips pp", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Beyond Locality- Sensitive Hashing", "author": ["A. Andoni", "P. Indyk", "H.L. Nguyen", "I. Razenshteyn"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing - STOC", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Product Quantization for Nearest Neighbor Search Herve", "author": ["M. Douze", "C. Schmid", "H. J\u00e9gou"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Similarity Search in High Dimensions via Hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "Proceedings of the 25th International Conference on Very Large Data Bases 99(1),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality", "author": ["P. Indyk", "R. Motwd"], "venue": "Proceedings of the thirtieth annual ACM symposium on Theory of computing. ACM pp", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Fast Cross-Polytope Locality-Sensitive", "author": ["C. Kennedy", "R. Ward"], "venue": "Hashing pp", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Spherical LSH for Approximate Nearest Neighbor", "author": ["K. Terasawa", "Y. Tanaka"], "venue": "Search on Unit Hypersphere p", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Feature Hashing for Large Scale Multitask Learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Attenberg", "J. Langford", "A. Smola"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning (Icml), (pp. 1113\u20131120)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Optimal data-dependent hashing for approximate near neighbors", "author": ["Alexandr Andoni", "Ilya Razenshteyn"], "venue": "In STOC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Product quantization for nearest neighbor search", "author": ["Herv\u00e9 J\u00e9gou", "Matthijs Douze", "Cordelia Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Largescale speaker identification", "author": ["Ludwig Schmidt", "Matthew Sharifi", "Ignacio Lopez Moreno"], "venue": "In ICASSP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Streaming similarity search over one billion tweets using parallel locality- sensitive hashing", "author": ["Narayanan Sundaram", "Aizana Turmukhametova", "Nadathur Satish", "Todd Mostak", "Piotr Indyk", "Samuel Madden", "Pradeep Dubey"], "venue": "In VLDB,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "While the theoretical analysis is done for the angular distance, these families can also be used in practice for the euclidean distance with excellent results [2].", "startOffset": 159, "endOffset": 162}, {"referenceID": 11, "context": "[13][14][15] In the practice the LSH schemes created for the angular distance can also be used for the euclidean distance with very good results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13][14][15] In the practice the LSH schemes created for the angular distance can also be used for the euclidean distance with very good results.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "[13][14][15] In the practice the LSH schemes created for the angular distance can also be used for the euclidean distance with very good results.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "LSH was introduced by Indyk and Motwani in 1998 [7][6].", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "LSH was introduced by Indyk and Motwani in 1998 [7][6].", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "Our first LSH family for the angular distance is based on random hyperplanes minhashes and was proposed by Charikar in 2002 [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "1from now on we\u2019ll use distance to refer indistinctly to the angular distance or the euclidean distance in the unit hypersphere It was also shown in [4] that a random vector formed by just \u00b11 elements is enough as a random hyperplane.", "startOffset": 149, "endOffset": 152}, {"referenceID": 0, "context": "This is related to a Johnson Lindenstrauss projection presentd by Achlioptas[1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Voronoi LSH [3] uses T random Gaussian vectors of the same dimensionality as the data points.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "The Cross Polytope LSH method was introduced by Teresawa and Tanaka in 2007 [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "To speed up the computation of the rotation a pseudorotation using Hadamard matrices has been proposed [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "Kennedy[8] proposes a faster version of the Cross Polytope method using a Fast Johnson Lindenstrauss transform from the original d dimensions to a reduced space with m dimensions and then the random lifted rotation from m to d\u2032 dimensions.", "startOffset": 7, "endOffset": 10}, {"referenceID": 7, "context": "Instead of a FJL transform feature hashing can be used making the method from [8] a lot faster.", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "Voronoi LSH is indeed a direct application of a JohnsonLindenstrauss projection using a Gaussian Matrix [1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "Hyperplanes LSH is another application of a JL projection using a matrix with \u00b11 random elements [1].", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "To mitigate the effect of collisions Weinberger [11] proposed the use of a second hash function that will return the sign to be used (\u00b11).", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "With the addition of the second function the effect of collisions is mitigated and several features can be hashed into the same target space with minimal interplay[11].", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "Feature hashing as a random projection is used in [16] to transform sparse vectors preserving pairwise distances, [11] also shows that FH preserves the distances between vectors.", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "For the real dataset we used the SIFT 1 million dataset [5] that is widely used for Near Neighbors experiments.", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "In terms of optimality Spherical hashing [3] is optimal in terms of precision but the minhashes are not practical to compute while Feature Hashing as presented here is optimal in terms of speed and the corresponding minhashes can be used in the practice because they offer good precision after amplification.", "startOffset": 41, "endOffset": 44}], "year": 2017, "abstractText": "In this paper we propose the creation of generic LSH families for the angular distance based on Johnson-Lindenstrauss projections. We show that feature hashing is a valid J-L projection and propose two new LSH families based on feature hashing. These new LSH families are tested on both synthetic and real datasets with very good results and a considerable performance improvement over other LSH families. While the theoretical analysis is done for the angular distance, these families can also be used in practice for the euclidean distance with excellent results [2]. Our tests using real datasets show that the proposed LSH functions work well for the euclidean distance.", "creator": "LaTeX with hyperref package"}}}