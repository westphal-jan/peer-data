{"id": "1704.05393", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Mining Worse and Better Opinions. Unsupervised and Agnostic Aggregation of Online Reviews", "abstract": "In earmarked this nine-mile paper, roett we meddings propose biscan a sobriquet novel meimad approach wolfsheim for eurotrain aggregating guberniya online christiane reviews, split-screen according to boletes the opinions \u0d2a they npos express. Our fantasias methodology 259 is fipple unsupervised - arounds due to prisk the fact wvt that eurasia it does not witbank rely jci on pre - labeled \u2019s reviews - ilmari and it is two-dollar agnostic - since chesil it beem does not make 280.7 any assumption demography about tbp the n\u1eb5ng domain s\u00f8rkapp or the boater language of the 10-piece review sinks content. braj We measure the adherence of a pictoris review farmer-labor content 15-kilometre to alongwith the domain terminology extracted from pilolevu a micrografx review set. First, braque we eclesia demonstrate the informativeness of the promocup adherence metric with respect to the score associated regalo with yahalom a review. dysfunctional Then, thanga we agnoletto exploit scott.collins@latimes.com the metric superblock values to 44.90 group parchim reviews, according to peccatte the 9:18 opinions covenant they yarn express. geffen Our yabushita experimental campaign has been carried straighteners out coverture on two hatungimana large datasets monmonier collected leeds-based from freeze-dried Booking and Amazon, bellissima respectively.", "histories": [["v1", "Tue, 18 Apr 2017 15:20:25 GMT  (1143kb,D)", "http://arxiv.org/abs/1704.05393v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.CL cs.IR", "authors": ["michela fazzolari", "marinella petrocchi", "alessandro tommasi", "cesare zavattari"], "accepted": false, "id": "1704.05393"}, "pdf": {"name": "1704.05393.pdf", "metadata": {"source": "CRF", "title": "Mining Worse and Better Opinions Unsupervised and Agnostic Aggregation of Online Reviews", "authors": ["Michela Fazzolari", "Marinella Petrocchi", "Alessandro Tommasi", "Cesare Zavattari"], "emails": ["m.fazzolari@iit.cnr.it,", "m.petrocchi@iit.cnr.it,", "alessandro.tommasi@lucense.it", "cesare.zavattari@lucense.it"], "sections": [{"heading": null, "text": "Keywords: Social Web mining, Online reviews aggregation, Adherence metric, Domain terminology, Contrastive approach"}, {"heading": "1 Introduction", "text": "Online reviews represent an important resource for people to choose among multiple products and services. They also induce a powerful effect on customers\u2019 behaviour and, therefore, they undertake an influential role on the performance of business companies. Since the information available on reviews sites is often overwhelming, both consumers and companies benefit from effective techniques to automatically analysing the good disposition of the reviewers towards the target product. To this aim, opinion mining [11,19] deals with the computational treatment of polarity, sentiment, and subjectivity in texts. However, opinion mining is usually context-sensitive [24], meaning that the accuracy of the sentiment classification can be influenced by the domain of the products to which it is applied [22]. Furthermore, sentiment analysis may rely on annotated textual\n? Extended version of \u201cMining Worse and Better Opinions. Unsupervised and Agnostic Aggregation of Online Reviews\u201d, to appear in Proc. of 17th International Web Engineering Conference (ICWE 2017).\nar X\niv :1\n70 4.\n05 39\n3v 1\n[ cs\n.S I]\n1 8\nA pr\ncorpora, to appropriately train the sentiment classifier, see, e.g., [8]. Also, most of the existing techniques are specialised for the English language: a cross-lingual adaptation is required in order to apply them to a different target language, [10].\nIn this paper, we propose an original approach to aggregate reviews with similar opinions. The approach is unsupervised, since it does not rely on labelled reviews and training phases. Moreover, it is agnostic, needing no previous knowledge on either the reviews domain or language. Grouping reviews is obtained by relying on a novel introduced metric, called adherence, which measures how much a review text inherits from a reference terminology, automatically extracted from an unannotated reviews corpus. Leveraging an extensive experimental campaign over two large reviews datasets, in different languages, from Booking and Amazon we first demonstrate that the value of the adherence metric is informative, since it is correlated with the review score. Then, we exploit adherence to aggregate reviews according to the reviews positiveness. A further analysis on such groups highlights the most characteristic terms therein. This leads to the additional result of learning the best and worst features of a product.\nIn Section 2, we define the adherence metric. Section 3 presents the datasets. Section 4 describes the experiments and their results. In Section 5, we report on related work in the area. Section 6 concludes the paper."}, {"heading": "2 Review Adherence to Typical Terminology", "text": "We aim at proving that positive reviews - in contrast with negative ones - are generally more adherent to the emergent terminology of the whole review collection. This will provide us a form of alternative polarity detection: indeed, we might estimate the relative polarity of a review by measuring how adherent it is to the domain terminology. Because a meaningful comparison against terminology requires a sizeable chunk of text, the proposed approach best applies to a set of reviews. Here, we describe how the domain terminology is extracted and we define a measure of adherence of a piece of text against such terminology."}, {"heading": "2.1 Extracting the Terminology", "text": "Every domain is characterized by key concepts, expressed by a domain terminology: a set of terms that are either specific to the domain (e.g., part of its jargon, such as the term \u201cbluetooth\u201d in the mobile domain) or that feature a specific meaning in the domain, uncommon outside of it (e.g., \u201cmonotonous\u201d in the math domain). Identifying this terminology is important for two main reasons: i) avoiding that irrelevant terms (such as \u201cthe\u201d, \u201cin\u201d, \u201cof\u201d ...) have a weight in the computation of adherence; ii) knowing which key concepts are more relevant in a set of texts provides significant insight over their content. The terminology is extracted in a domain and language agnostic way, with the benefit of not relying on domain and linguistic resources.\nContrastive approaches [2] to terminology extraction only rely on sets of raw texts in the desired language: i) a set belonging to the domain of interest and ii) a few others on generic topics (e.g., a collection of books, newspaper articles, tweets\n\u2013 easily obtainable, nowadays, from the Web). The contrastive approach work by comparing the characteristic frequency of the terms in the domain documents and in generic ones. The rationale is that generic, non-content words like \u201cthe\u201d, as well as non specific words, will be almost equally frequent in all the available sets, whereas words with a relevance to the domain will feature there much more prominently than they do in generic texts.\nThere are many sophisticated ways to deal with multi-words, but any statisticsbased approach needs to consider that, for n-grams3 to be dealt with appropriately, the data needed scales up by orders of magnitude. For our purposes, we stick to the simpler form of single-term (or 1-gram) terminology extraction.\nLet D be a set of documents belonging to the domain of interest D, and let G1 . . .GM be M sets of other documents (the domain of each Gi is not necessarily known, but it is assumed not to be limited to D). All terms occurring in documents of D (TD) as candidate members of TD, the terminology extracted from D. For each term t, we define the term frequency (tf) of a term t in a generic set of documents S as:\ntfS(t) = |{d \u2208 S|t occurs in d}|\n|S| (1)\n(probability that, picking a document d at random from S, it contains t). The tf alone is not adequate to represent the meaningfulness of a term in a set of documents, since the most frequent words are non-content words4. Because of this, inverse document frequency (idf) [23] is often used to compare the frequency of a term in a document with respect to its frequency in the whole collection. In our setting, we can however simplify things, and just compare frequencies of a term inside and outside of the domain. We do this by computing the term specificity (ts) of a term t over domain set D against all Gi\u2019s, which we define as:\ntsDG (t) = tfD(t)\nmin i=1..M\ntfGi(t) (2)\ntsDG (t) is effective at identifying very common words and words that are not specific to the domain (whose ts will be close to 1), as well as words particularly frequent in the domain, with a ts considerably higher than 1. Extremely rare words may cause issues: if D and Gi\u2019s are too small to effectively represent a term, such term will be discarded by default. We chose an empirical threshold \u03b8freq = 0.005, skipping all terms for which tfD(t) < \u03b8freq. This value is justified by the necessity to have enough documents per term, and 0.5% is a reasonable figure given the size of our datasets. We compute ts for all t \u2208 TD. We define:\nTD = {t|tsDG (t) \u2265 \u03b8cutoff} (3)\nTo set the value of \u03b8cutoff, we might i) choose the number of words to keep (e.g., set the threshold so as to pick the highest relevant portion of TD) or ii) use\n3 Constructions of n words: \u201cpresident of the USA\u201d is a 4-gram. 4 The ten most frequent words of the English language, as per Wikipedia (https:// en.wikipedia.org/wiki/Most_common_words_in_English), are \u201cthe\u201d, \u201cbe\u201d, \u201cto\u201d, \u201cof\u201d, \u201cand\u201d, \u201ca\u201d, \u201cin\u201d, \u201cthat\u201d, \u201chave\u201d, and \u201cI\u201d.\nan empirical value (higher than 1), indicating how much more frequent we ask a term to be, being a reliably part of the terminology. For our experiments, we have used this simpler alternative, empirically setting \u03b8cutoff = 16. Higher values include fewer terms in the terminology, improving precision vs. recall, whereas lower values include more terms, negatively affecting precision. This value was the one used in the experiments conducted in [7]."}, {"heading": "2.2 Adherence Definition", "text": "The adherence (adh) of a document d to a terminology T is defined as:\nadhT(d) = |{t|t occurs in d} \u2229 {t \u2208 T}|\n|{t|t occurs in d}| (4)\nIt represents the fraction of terms in document d that belongs to terminology T. This value will typically be much smaller than 1, since a document is likely to contain plenty of non-content words, not part of the domain terminology. The specific value of adherence is however of little interest to us: we show how more adherent reviews tend to be more positive than those with lower values of adherence, only using the value for comparison, and not on an absolute scale."}, {"heading": "3 Datasets", "text": "The first dataset consists of a collection of reviews from the Booking website, during the period between June 2016 and August 2016. The second dataset includes reviews taken from the Amazon website and it is a subset of the dataset available at http://jmcauley.ucsd.edu/data/amazon, previously used in [14,15]. We also used a contrastive dataset to extract the domain terminology.\nBooking Dataset. For the Booking dataset, we had 1,135,493 reviews, related to 1,056 hotels in 6 cities. We only considered hotels with more than 1,000 reviews, in any language. For each review, we focused on:\n\u2013 score: a real value given by the reviewer to the hotel, in the interval [2.5,10]; \u2013 posContent: a text describing the hotel pros; \u2013 negContent: a text describing the hotel cons; \u2013 hotelName: the name of the hotel which the review refers to.\nAs review text, we took the concatenation of posContent and negContent.\nAmazon Dataset. Reviews in the Amazon dataset are already divided according to the individual product categories. We chose two macro-categories, namely Cell Phones & Accessories and Health & Personal Care and we further selected reviews according to 6 product categories. For each review, we focused on:\n\u2013 score: an integer assigned by the reviewer to the product (range [0,5]); \u2013 reviewText: the textual content of the review; \u2013 asin: the Amazon Standard Identification Number, that is a unique code of\n10 letters and/or numbers that identifies a product.\nTable 1 shows statistics extracted from the Booking and the Amazon dataset.\nContrastive Terminology Dataset. In addition to the domain documents, originating from the above datasets, we used various datasets, collected for other purposes and projects, as generic examples of texts in the desired language, in order to extract the terminology as in Section 2.1. Table 2 resumes the data used to construct the contrastive dataset."}, {"heading": "4 Experiments and Results", "text": "Each dataset D is organized in categories Ci. Each category contains items that we represent by the set of their reviews Ij . When performing experiments over D, we extract the terminology of each category Ci (TCi). We then compute adhTCi (r) for each r \u2208 Ij \u2208 Ci (r is the single review).\nFor the Amazon dataset, Ci are the product categories, whereas Ij \u2019s are the products (represented by their sets of reviews). For the Booking dataset, Ci are the hotel categories, whereas Ij \u2019s are the hotels (represented by their sets of reviews). We carried on experiments with and without review balancing. The latter has been considered to avoid bias: reviews with the highest scores are over-represented in the dataset, therefore the computation of the terminology can be biased towards positive terms. Thus, for each Ci and for each score, we randomly selected the same number of reviews."}, {"heading": "4.1 Adherence Informativeness", "text": "A first analysis investigates if there exists a relation between the adherence metric - introduced in Section 2 - and the score assigned to each review.\nAmazon Dataset. For each product category, we extract the reference terminology, by considering all the reviews belonging to that category against the contrastive dataset, for the appropriate language. Then, we compute the adherence value for each review. To show the results in a meaningful way, we grouped reviews in 5 bins, according to their score, and compute the average of the adherence values on each bin. The results are reported in Figure 1.\nThe graph shows a line for each product category. Overall, it highlights that reviews with higher scores have higher adherence, implying a better correspondence with the reference terminology in comparison to reviews with lower scores. This result could be biased by the fact that reviews with higher scores are more represented in the dataset than the others.\nTherefore, we balanced the reviews in bins: we set B as the number of reviews of the less populated bin and we randomly selected the same number of reviews from the other bins. Then, we recomputed the average adherence values and we obtained the results shown in Figure 2, which confirm the trend of the previous graph.\nEven if the Bluetooth Speakers and Oral Irrigators categories feature a slight decreasing trend in the adherence value, when passing from reviews with score 4 to reviews with score 5, the general trend shows that the adherence metric is informative of the review score.\nBooking Dataset. In the second experiment, we group the hotel reviews accordingly to the city they refer to. For each city, we extract the reference terminology and we compute the adherence value for each review. To make the results comparable with the ones obtained for Amazon we re-arrange the Booking scoring system to generate a score evaluation over 5 bins. To this aim, we apply the score distribution suggested by Booking itself, since Booking scores are inflated to the top of the possible range of scores [16]. Therefore, we consider the following bin distribution:\n\u2013 very poor: reviews with a score \u2264 3;\n\u2013 poor: score \u2208 (3, 5]; \u2013 okay: score \u2208 (5, 7]; \u2013 good: score \u2208 (7, 9]; \u2013 excellent: score > 9.\nThe results of the average adherence values on each bin are reported in Figure 3. A line is drawn for each city, by connecting the points in correspondence to the adherence values. The graph suggests that the average adherence is higher for reviews with higher scores. Thus, the higher the score of the hotel reviews, the more adherent the review to the reference terminology.\nTo avoid the bias caused by the over-representation of the positive reviews in the dataset, we compute the average adherence values by using a balanced number of reviews for each bin. The results, reported in Figure 4, confirm the trend of the previous graph, even if the slope is smaller.\nWorking on balanced bins, the typical terminology of the reviews corpus has been recalculated, with respect to what computed for the original, unbalanced dataset. Thus, even considering the less populated bin, the average adherence value for the balanced dataset is not the same as the one in the unbalanced dataset for the same bin (this holds both for Booking and Amazon datasets).\nThe under-sampling applied to the majority classes to balance the reviews number leads to a deterioration of the results (both for Amazon and Booking - Figures 2 and 4."}, {"heading": "4.2 Good Opinions, Higher Adherence", "text": "Interestingly, in the Booking dataset, the text of each review is conveniently divided into positive and negative content. Thus, we perform an additional experiment, by only considering positive and negative chunks of reviews. For each city, we group positive and negative contents of reviews and we compute the\nadherence value for each positive and negative chunk, with respect to the reference terminology. Finally, we average the adherence values according to the score bins. The results are reported in Figure 5, for the unbalanced dataset. In the graph, we report two lines for each city: the solid (dashed) lines are obtained by considering the positive (negative) contents of reviews. The same colour for solid and dashed line corresponds to the same city. We also perform the same calculation by considering a balanced dataset (Figure 6).\nBoth the graphs highlight that there is a clear division between the solid and dashed lines. In particular, the average adherence obtained considering positive contents is, for most of the bins, above the average adherence computed considering negative contents. This separation is more evident when the review score increases (it does not hold for very poor scores). Overall, positive aspects of a hotel are described with a less varied language with respect to its negative aspects. Probably, this phenomenon occurs because unsatisfied reviewers tend to explain what happened in details.\nIn addition to the average value, we also computed the standard deviation within each bin, that resulted to be quite high, as reported in Table 3, Table 4, Table 5 and Table 6. This suggests that, even correlated with the score, the adherence is not a good measure when considering a single review, but its informativeness should be rather exploited by considering an ensemble of reviews, as detailed in Section 4.4."}, {"heading": "4.3 Extension to Different Languages", "text": "The experiments described so far were realised by considering a subset of reviews in English, taken from the original Booking dataset, which features other languages too. To further evaluate the informativness of the adherence metric, we selected two additional review subsets, in Italian and in French. For each subset,\nwe drawn two graphs, the first considering all the reviews content, the second the separation between positive and negative contents. For these experiments, we considered imbalanced bins, due to the limited number of reviews available in each language. The results are reported in Figure 7.\nEssentially, in both cases, it is confirmed that the higher the score, the higher the adherence when considering the overall text (Figure 7-a and Figure 7-c). Similarily, the graphs in Figure 7-b and Figure 7-d show a clear division between positive and negative adherence values, when the score increases."}, {"heading": "4.4 Language and Domain-Agnostic Reviews Aggregation", "text": "In this section, we present an application of the outcome found in previous ones. Given a set of texts, we propose to aggregate texts with positive polarity and texts with negative polarity, without a priori knowing the text language and domain, and without using any technique of Natural Language Processing (NLP), while exploiting only the adherence metric. We apply the following methodology:\n1. For each review r \u2208 Ij \u2208 Ci we compute the adherence adhTCi (r). 2. Reviews r \u2208 Ij are sorted in ascending order w.r.t. their adherence value. 3. Ordered reviews are split in bins with the same cardinality. We defined Kbins\nbins, each holding |{r \u2208 Ij}|/Kbins reviews in ascending order of adherence. 4. For each bin Bi, we compute the average of the adherence value of the reviews\nit contains: Avgadh,i = 1 R \u2211 adhTCi (r), as well as, for the purposes of validation, the average score provided by those reviews, Avgscore,i = 1 R \u2211 score(r). 5. Finally, we aim at proving that, when the average adherence value of each bin increases, the average score value also increases. Thus, we compute the percentage of Ij \u2208 Ci for which we observe:\nAvgscore,Kbins \u2265 Avgscore,1 (5)\nAvgscore,i \u2265 Avgscore,i\u22121 (6)\nwhere Avgscore,Kbins is the average score for the last bin, Avgscore,1 is the average score for the first bin, and i = 1, . . . ,Kbins.\nTable 7 reports the results for the Amazon dataset. For each category Ci, we apply the methodology three times, modifying the minimum number of reviews (minRev) for each item Ij , in order to discard items with few reviews. We set Kbins=3 and we report the number of items (#I) and the total number of reviews (#Rev) considered, plus the percentage of Ij \u2208 Ci for which (5) is true (%).\nThis result shows that, considering 3 bins, the percentage of items for which the average score of the last bin is higher than the average score of the first bin is above 80% for each category (except for Magnifiers in case the minimum number of reviews is 20). Nevertheless, the percentage grows in almost all cases, when the minimum number of reviews increases. It exceeds 90% for every category, when the minimum number of reviews is, at least, 100.\nTherefore, in the majority of cases, it is true that, when the average adherence of reviews belonging to the last bin is higher than the average adherence\nof reviews included in the first bin, the same relation exists between their correspondent average scores. This finding is mostly supported when we consider only items with many reviews (at least 100).\nFor the Booking dataset, we straight consider only hotels with at least 100 reviews. We perform three experiments according to the languages of reviews (English, Italian, and French). For each experiment, Kbins=3 and we report the number of items (#I), the total number of reviews (#Rev) considered and the percentage of Ij \u2208 Ci for which (5) is true (%). The results are in Table 8. The percentage of items for which the average score of the last bin is higher than the average score of the first bin is above 90% in all the cases.\nThus, given a set of reviews on, e.g., hotels, or restaurants, in any language, we can identify a group of reviews that, on average, express better opinions than another group of reviews. Noticeably, this analysis works even if the associated score is not available, i.e., it can be applied to general comments about items.\nWe consider now if also relation (6) is verified for each bin i = 1, . . . ,Kbins, i.e., if the function between the ordered sets of average adherence values Avgadh,i and average score values Avgscore,i is a monotonic function. By plotting the average score vs the average adherence, for some items, we found out a general upward trend. Nevertheless, there were many spikes that prevent the function from being monotonic. Then, we tried to smooth down the curves by applying a moving average with window = 2 and we then computed the percentage of Ij \u2208 Ci for which (6) was verified. For the Amazon dataset, we performed three experiments, modifying the minimum number of reviews required (minRev) for each item, in order to discard items with few reviews. Results are in Table 9.\nSuch results are worse with respect to the ones in Table 7. Nevertheless, in all cases (but Oral Irrigators), the percentage values increase when minRev increase (for Magnifiers, it remains the same with minRev = 50, 100). When minRev = 100, the percentage of Ij \u2208 Ci for which (6) is true is above 72%.\nFor the Booking dataset, due to the high number of available reviews, we also varied the number of bins from 3 to 5. We only considered reviews in English\nand computed the percentage of items for which the equation (6) is true. Table 10 shows a clear degradation of performances when the number of bin increases.\nSo far, the results indicate a relation between the increasing adherence values and the increasing score values. However, we cannot prove a strong correlation between adherence and score, either considering a single review or groups of reviews. Therefore, we followed a different approach, by computing, for each item Ij \u2208 Ci, the difference between the average values of the first and last bin, both for the adherence and the score:\n\u2206adh(j) = Avgadh,Kbins \u2212Avgadh,1 \u2206score(j) = Avgscore,Kbins \u2212Avgscore,1\nIf we average such differences for all the items Ij \u2208 Ci, both for adherence and score, we obtain an average value for each category Ci:\nAvgDadh = 1\nJ J\u2211 j=1 \u2206adh(j) (7)\nAvgDscore = 1\nJ J\u2211 j=1 \u2206score(j) (8)\nwhere J is the total number of items j \u2208 Ij .\nFigure 8 shows the values of (7) and (8) for items belonging to some of the Amazon and Booking categories. In each graph, the x-axis reports the number of bins considered, wheres the y-axis represents the average differences values. We depicted the average differences for the adherence with a solid red line, while the average differences for the score with a dashed blue line. The graphs clearly show that, when the number of bin increases, the first and last bin include reviews which describe the product in a considerably different way, in term of positiveness. Thus, given a category, it is possible to discriminate among groups of reviews, related to that category, in such a way that each group expresses an opinion different from the others, ordered from the most negative to the most positive ones (or vice-versa)."}, {"heading": "4.5 Representative Terms in First and Last Bins", "text": "Here, we extract the most recurrent terms in the positive and negative groups of reviews. Given an item (either a hotel or a product), we consider the terms included in the positive set and in the negative set (last and first bins, with Kbins = 10) that can be also found in the extracted terminology.\nFor each term, we compute the term frequency\u2013inverse document frequency (tf-idf) value (in this case, tf is the term frequency inside the bin, that is the number of reviews that include such term), we sort the terms accordingly and we select the first 20 ones for the positive and negative set. We then remove the terms common to both sets, in order to identify the most discriminating ones. Table 11 shows the terms extracted for two Amazon products.\nAfter having extracted the terms for every item, we calculate their frequency within a whole category. Again, we remove the terms common to both sets (positive and negative), to highlight the most representative positive and negative terms for the category. Table 12 shows the terms extracted for the five Amazon categories with more reviews. Table 13 shows the terms extracted for some Booking categories, for English, Italian and French. However, for Italian and French, few examples are shown, due to the low number of reviews for such datasets."}, {"heading": "5 Related Work", "text": "Terminology extraction. Automatic terminology extraction is one of the pillars of many terminology engineering processes, such as text mining, information retrieval, ontology learning, and semantic web technologies. The aim is to automatically identify relevant concepts (or terms) from a given domain-specific corpus. Within a collection of candidate domain-relevant terms, actual terms are separated from non-terms by using statistical and machine learning methods [20]. Here, we rely on contrastive approaches, where the identification of relevant candidates is performed through inter-domain contrastive analysis [21,6,1].\nOpinion Mining. Opinion mining techniques aim at automatically identifying polarities and sentiments in texts[11], by, e.g., extracting subjective expressions\nrepresenting personal opinions and speculations[25] or detecting so called contextual polarity of a word, i.e., the polarity acquired by the word contextually to the sentence in which it appears, see, e.g., [26,27,17]. Often, opinion mining rely on lexicon-based approaches, involving the extraction of term polarities from sentiment lexicons and the aggregation of such scores to predict the overall sentiment of a piece of text, see, e.g., [5,8,4,3].\nClustering Opinions. Unsupervised learning does not require labelled data for the training process. Among them, clustering algorithms can be profitably used to find the natural clusters in the data, by calculating the distances or similarities from the centres of the clusters. Few efforts have been devoted to the study of polarity detection in online reviews with clustering techniques. In [9], the authors present the clustering-based sentiment analysis approach, by applying three different techniques, namely TF-IDF weighting, voting mechanism and enhancement by hybrid with scoring method. In [13,12], the authors describe and experimental study of some common clustering techniques used for sentiment analysis of online reviews and investigate how any step of the clustering process (pre-processing, term weighting, clustering algorithm) can affect clustering results. The work in [18] studies the relationship between online movie reviews and the box office incomes. The detection of sentiments is carried out by using tf and idf values as features and Fuzzy Clustering as algorithm.\nHere, we proposed an alternative approach to polarity detection, relying on automatic terminology extraction, in a domain and language agnostic fashion, and not relying on linguistic resources."}, {"heading": "6 Final Remarks", "text": "We presented a novel approach for aggregating reviews, based on their polarity. The methodology did not require pre-labeled reviews and the knowledge of the reviews\u2019 domain and language. We introduced the adherence metric and we demonstrated its correlation with the review score. Lastly, we relied on adherence to successfully aggregate reviews, according to the opinions they express.\nAcknowledgments.\nResearch partly supported by MSCA-ITN-2015-ETN grant agreement #675320 (European Network of Excellence in Cybersecurity) and by Fondazione Cassa di Risparmio di Lucca, financing the project Reviewland."}], "references": [{"title": "A contrastive approach to term extraction", "author": ["R Basili"], "venue": "Terminologie et intelligence artificielle. Rencontres. pp. 119\u2013128", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "A contrastive approach to multi-word extraction from domainspecific corpora", "author": ["F Bonin"], "venue": "Language Resources and Evaluation. ELRA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Building a twitter opinion lexicon from automaticallyannotated tweets", "author": ["F Bravo-Marquez"], "venue": "Knowledge-Based Systems 108, 65\u201378", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Sentic Computing: A Common-Sense-Based Framework for Concept-Level Sentiment Analysis", "author": ["E. Cambria", "A. Hussain"], "venue": "Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "SenticNet 3: a common and common-sense knowledge base for cognition-driven sentiment analysis", "author": ["E Cambria"], "venue": "28th AAAI Artificial Intelligence. pp. 1515\u20131521", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Identifying technical vocabulary", "author": ["T.M. Chung", "P. Nation"], "venue": "System 32(2), 251\u2013263", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Semisupervised knowledge extraction for detection of drugs and their effects", "author": ["F. Del Vigna", "M. Petrocchi", "A. Tommasi", "C. Zavattari", "M. Tesconi"], "venue": "Social Informatics I", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "SENTIWORDNET: A publicly available lexical resource for opinion mining", "author": ["A. Esuli", "F. Sebastiani"], "venue": "Language Resources and Evaluation. pp. 417\u2013422", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Application of a clustering method on sentiment analysis", "author": ["G. Li", "F. Liu"], "venue": "J. Inf. Sci. 38(2), 127\u2013139", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "A multilingual semi-supervised approach in deriving Singlish sentic patterns for polarity detection", "author": ["S Ling Lo"], "venue": "Knowledge-Based Systems 105, 236\u2013247", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Sentiment Analysis and Opinion Mining", "author": ["B. Liu"], "venue": "Morgan & Claypool", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring performance of clustering methods on document sentiment analysis", "author": ["B. Ma", "H. Yuan", "Y. Wu"], "venue": "Information Science", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison study of clustering models for online review sentiment analysis", "author": ["B Ma"], "venue": "Web-Age Information Management. pp. 332\u2013337. Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring networks of substitutable and complementary products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "21th KDD. pp. 785\u2013794. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Image-based recommendations on styles and substitutes", "author": ["J McAuley"], "venue": "38th Research and Development in Information Retrieval. pp. 43\u201352. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Ma\u0155\u0131a-Dolores, S.M.M., Gar\u0107\u0131a, J.J.B.: Booking.com: The unexpected scoring system", "author": ["J.P. Mellinas"], "venue": "Tourism Management", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Contextual sentiment analysis for social media genres", "author": ["A. Muhammad", "N. Wiratunga", "R. Lothian"], "venue": "Knowledge-Based Systems 108, 92\u2013101", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "An improved sentiment analysis of online movie reviews based on clustering for box-office prediction", "author": ["P Nagamma"], "venue": "Computing, Communication and Automation. pp. 933\u2013937", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Found. Trends Inf. Retr. 2(1-2), 1\u2013135", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Terminology extraction: An analysis of linguistic and statistical approaches", "author": ["Pazienza", "M.T"], "venue": "Knowledge Mining, pp. 255\u2013279. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Corpus-based terminology extraction applied to information access", "author": ["A. Pe\u00f1as", "F. Verdejo", "J. Gonzalo"], "venue": "Corpus Linguistics. vol. 13, pp. 458\u2013465", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Context-sensitive Twitter sentiment classification using neural network", "author": ["Y. Ren", "Y. Zhang", "M. Zhang", "D. Ji"], "venue": "Artificial Intelligence. pp. 215\u2013221. AAAI", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Inf. Process. Management 24(5), 513\u2013523", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1988}, {"title": "Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews", "author": ["P.D. Turney"], "venue": "Computational Linguistics Meeting. pp. 417\u2013424. ACL", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "OpinionFinder: A system for subjectivity analysis", "author": ["T Wilson"], "venue": "HLT/EMNLP on Interactive Demonstrations. pp. 34\u201335. ACL", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T Wilson"], "venue": "HLT/EMNLP. pp. 347\u2013354. ACL", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis", "author": ["T Wilson"], "venue": "Comput. Linguist. 35(3), 399\u2013433", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "To this aim, opinion mining [11,19] deals with the computational treatment of polarity, sentiment, and subjectivity in texts.", "startOffset": 28, "endOffset": 35}, {"referenceID": 18, "context": "To this aim, opinion mining [11,19] deals with the computational treatment of polarity, sentiment, and subjectivity in texts.", "startOffset": 28, "endOffset": 35}, {"referenceID": 23, "context": "However, opinion mining is usually context-sensitive [24], meaning that the accuracy of the sentiment classification can be influenced by the domain of the products to which it is applied [22].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "However, opinion mining is usually context-sensitive [24], meaning that the accuracy of the sentiment classification can be influenced by the domain of the products to which it is applied [22].", "startOffset": 188, "endOffset": 192}, {"referenceID": 7, "context": ", [8].", "startOffset": 2, "endOffset": 5}, {"referenceID": 9, "context": "Also, most of the existing techniques are specialised for the English language: a cross-lingual adaptation is required in order to apply them to a different target language, [10].", "startOffset": 174, "endOffset": 178}, {"referenceID": 1, "context": "Contrastive approaches [2] to terminology extraction only rely on sets of raw texts in the desired language: i) a set belonging to the domain of interest and ii) a few others on generic topics (e.", "startOffset": 23, "endOffset": 26}, {"referenceID": 22, "context": "Because of this, inverse document frequency (idf) [23] is often used to compare the frequency of a term in a document with respect to its frequency in the whole collection.", "startOffset": 50, "endOffset": 54}, {"referenceID": 6, "context": "This value was the one used in the experiments conducted in [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 13, "context": "edu/data/amazon, previously used in [14,15].", "startOffset": 36, "endOffset": 43}, {"referenceID": 14, "context": "edu/data/amazon, previously used in [14,15].", "startOffset": 36, "endOffset": 43}, {"referenceID": 4, "context": "\u2013 score: an integer assigned by the reviewer to the product (range [0,5]); \u2013 reviewText: the textual content of the review; \u2013 asin: the Amazon Standard Identification Number, that is a unique code of 10 letters and/or numbers that identifies a product.", "startOffset": 67, "endOffset": 72}, {"referenceID": 15, "context": "To this aim, we apply the score distribution suggested by Booking itself, since Booking scores are inflated to the top of the possible range of scores [16].", "startOffset": 151, "endOffset": 155}, {"referenceID": 19, "context": "Within a collection of candidate domain-relevant terms, actual terms are separated from non-terms by using statistical and machine learning methods [20].", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "Here, we rely on contrastive approaches, where the identification of relevant candidates is performed through inter-domain contrastive analysis [21,6,1].", "startOffset": 144, "endOffset": 152}, {"referenceID": 5, "context": "Here, we rely on contrastive approaches, where the identification of relevant candidates is performed through inter-domain contrastive analysis [21,6,1].", "startOffset": 144, "endOffset": 152}, {"referenceID": 0, "context": "Here, we rely on contrastive approaches, where the identification of relevant candidates is performed through inter-domain contrastive analysis [21,6,1].", "startOffset": 144, "endOffset": 152}, {"referenceID": 10, "context": "Opinion mining techniques aim at automatically identifying polarities and sentiments in texts[11], by, e.", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "representing personal opinions and speculations[25] or detecting so called contextual polarity of a word, i.", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": ", [26,27,17].", "startOffset": 2, "endOffset": 12}, {"referenceID": 26, "context": ", [26,27,17].", "startOffset": 2, "endOffset": 12}, {"referenceID": 16, "context": ", [26,27,17].", "startOffset": 2, "endOffset": 12}, {"referenceID": 4, "context": ", [5,8,4,3].", "startOffset": 2, "endOffset": 11}, {"referenceID": 7, "context": ", [5,8,4,3].", "startOffset": 2, "endOffset": 11}, {"referenceID": 3, "context": ", [5,8,4,3].", "startOffset": 2, "endOffset": 11}, {"referenceID": 2, "context": ", [5,8,4,3].", "startOffset": 2, "endOffset": 11}, {"referenceID": 8, "context": "In [9], the authors present the clustering-based sentiment analysis approach, by applying three different techniques, namely TF-IDF weighting, voting mechanism and enhancement by hybrid with scoring method.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [13,12], the authors describe and experimental study of some common clustering techniques used for sentiment analysis of online reviews and investigate how any step of the clustering process (pre-processing, term weighting, clustering algorithm) can affect clustering results.", "startOffset": 3, "endOffset": 10}, {"referenceID": 11, "context": "In [13,12], the authors describe and experimental study of some common clustering techniques used for sentiment analysis of online reviews and investigate how any step of the clustering process (pre-processing, term weighting, clustering algorithm) can affect clustering results.", "startOffset": 3, "endOffset": 10}, {"referenceID": 17, "context": "The work in [18] studies the relationship between online movie reviews and the box office incomes.", "startOffset": 12, "endOffset": 16}], "year": 2017, "abstractText": "In this paper, we propose a novel approach for aggregating online reviews, according to the opinions they express. Our methodology is unsupervised due to the fact that it does not rely on pre-labeled reviews and it is agnostic since it does not make any assumption about the domain or the language of the review content. We measure the adherence of a review content to the domain terminology extracted from a review set. First, we demonstrate the informativeness of the adherence metric with respect to the score associated with a review. Then, we exploit the metric values to group reviews, according to the opinions they express. Our experimental campaign has been carried out on two large datasets collected from Booking and Amazon, respectively.", "creator": "LaTeX with hyperref package"}}}