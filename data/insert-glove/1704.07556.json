{"id": "1704.07556", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Adversarial Multi-Criteria Learning for Chinese Word Segmentation", "abstract": "15:28 Different accretionary linguistic correlations perspectives causes poulos many bandmaster diverse segmentation ascots criteria for vijayakanth Chinese word bommai segmentation (CWS ). aboubakar Most porchester existing prieta methods pranking focus on derivative improve arrue the performance for ferran each single hethum criterion. dyestuff However, fennell it buckton is interesting nijjar to raskulinecz exploit kotte these different criteria fibbed and mining their common dumbfounding underlying pi\u00f1ata knowledge. In this 13.07 paper, umbricht we propose tsien adversarial multi - 258.5 criteria titchmarsh learning oswalt for restrictive CWS by ndmc integrating shared geox knowledge kwinana from mpondwe multiple tonelson heterogeneous rivelino segmentation cricketing criteria. cations Experiments on eight corpora with heterogeneous segmentation criteria tischler show that alienist the performance samira of henrieta each green-winged corpus obtains iskandiriyah a significant improvement, compared to 32-county single - 109-mile criterion learning. trebevic Source codes of nation-states this whims paper oroqen are genske available on Github.", "histories": [["v1", "Tue, 25 Apr 2017 06:42:24 GMT  (2894kb,D)", "http://arxiv.org/abs/1704.07556v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xinchi chen", "zhan shi", "xipeng qiu", "xuanjing huang"], "accepted": true, "id": "1704.07556"}, "pdf": {"name": "1704.07556.pdf", "metadata": {"source": "CRF", "title": "Adversarial Multi-Criteria Learning for Chinese Word Segmentation", "authors": ["Xinchi Chen", "Zhan Shi", "Xipeng Qiu", "Xuanjing Huang"], "emails": ["xinchichen13@fudan.edu.cn", "zshi16@fudan.edu.cn", "xpqiu@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NLP). Currently, the state-ofthe-art methods are based on statistical supervised learning algorithms, and rely on a large-scale annotated corpus whose cost is extremely expensive. Although there have been great achievements in building CWS corpora, they are somewhat incompatible due to different segmentation criteria. As shown in Table 1, given a sentence \u201cYaoMing reaches the final\u201d, the two commonlyused corpora, PKU\u2019s People\u2019s Daily (PKU) (Yu et al.(2001)Yu, Lu, Zhu, Duan, Kang, Sun, Wang, Zhao, and Zhan) and Penn Chinese Treebank (CTB) (Fei(2000)), use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora.\n\u2217Corresponding author. 1https://github.com/FudanNLP\nRecently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.(2013)Qiu, Zhao, and Huang; Li et al.(2015)Li, Chao, Zhang, and Chen; Li et al.(2016)Li, Chao, Zhang, and Yang). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston(2008); Luong et al.(2015)Luong, Le, Sutskever, Vinyals, and Kaiser; Chen et al.(2016)Chen, Zhang, and Liu).\nIn this paper, we propose an adversarial multicriteria learning for CWS by integrating shared knowledge from multiple segmentation criteria. Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana(1997); BenDavid and Schuller(2003)), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteriaspecific features. Inspired by the success of adversarial strategy on domain adaption (Ajakan et al.(2014)Ajakan, Germain, Larochelle, Laviolette, and Marchand; Ganin et al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviar X\niv :1\n70 4.\n07 55\n6v 1\n[ cs\n.C L\n] 2\n5 A\npr 2\n01 7\nolette, Marchand, and Lempitsky; Bousmalis et al.(2016)Bousmalis, Trigeorgis, Silberman, Krishnan, and Erhan), we further utilize adversarial strategy to make sure the shared layer can extract the common underlying and criteria-invariant features, which are suitable for all the criteria. Finally, we exploit the eight segmentation criteria on the five simplified Chinese and three traditional Chinese corpora. Experiments show that our models are effective to improve the performance for CWS. We also observe that traditional Chinese could benefit from incorporating knowledge from simplified Chinese.\nThe contributions of this paper could be summarized as follows.\n\u2022 Multi-criteria learning is first introduced for CWS, in which we propose three sharedprivate models to integrate multiple segmentation criteria. \u2022 An adversarial strategy is used to force the\nshared layer to learn criteria-invariant features, in which an new objective function is also proposed instead of the original crossentropy loss. \u2022 We conduct extensive experiments on eight\nCWS corpora with different segmentation criteria, which is by far the largest number of datasets used simultaneously."}, {"heading": "2 General Neural Model for Chinese Word Segmentation", "text": "Chinese word segmentation task is usually regarded as a character based sequence labeling problem. Specifically, each character in a sentence is labeled as one of L = {B,M,E, S}, indicating the begin, middle, end of a word, or a word with single character. There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc. Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al.(2013)Zheng, Chen, and Xu; Pei et al.(2014)Pei, Ge, and Baobao; Chen et al.(2015a)Chen, Qiu, Zhu, and Huang; Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang).\nSpecifically, given a sequence with n characters X = {x1, . . . , xn}, the aim of CWS task is to figure out the ground truth of labels Y \u2217 =\n{y\u22171, . . . , y\u2217n}:\nY \u2217 = arg max Y \u2208Ln p(Y |X), (1)\nwhere L = {B,M,E, S}. The general architecture of neural CWS could be characterized by three components: (1) a character embedding layer; (2) feature layers consisting of several classical neural networks and (3) a tag inference layer. The role of feature layers is to extract features, which could be either convolution neural network or recurrent neural network. In this paper, we adopt the bi-directional long short-term memory neural networks followed by CRF as the tag inference layer. Figure 2 illustrates the general architecture of CWS."}, {"heading": "2.1 Embedding layer", "text": "In neural models, the first step usually is to map discrete language symbols to distributed embedding vectors. Formally, we lookup embedding vector from embedding matrix for each character xi as exi \u2208 Rde , where de is a hyper-parameter indicating the size of character embedding."}, {"heading": "2.2 Feature layers", "text": "We adopt bi-directional long short-term memory (Bi-LSTM) as feature layers. While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al.(2015)Jozefowicz, Zaremba, and Sutskever), which is similar to the architecture of (Graves(2013)) but without peep-hole connections.\nLSTM LSTM introduces gate mechanism and memory cell to maintain long dependency information and avoid gradient vanishing. Formally, LSTM, with input gate i, output gate o, forget gate f and memory cell c, could be expressed as:\nii oi fi c\u0303i  =  \u03c3 \u03c3 \u03c3 \u03c6 (Wg\u1d40 [ exihi\u22121 ] + bg ) ,\n(2)\nci = ci\u22121 fi + c\u0303i ii, (3) hi = oi \u03c6(ci), (4)\nwhere Wg \u2208 R(de+dh)\u00d74dh and bg \u2208 R4dh are trainable parameters. dh is a hyper-parameter, indicating the hidden state size. Function \u03c3(\u00b7) and \u03c6(\u00b7) are sigmoid and tanh functions respectively.\nBi-LSTM In order to incorporate information from both sides of sequence, we use bi-directional LSTM (Bi-LSTM) with forward and backward directions. The update of each Bi-LSTM unit can be written precisely as follows:\nhi = \u2212\u2192 h i \u2295 \u2190\u2212 h i, (5)\n= Bi-LSTM(exi , \u2212\u2192 h i\u22121, \u2190\u2212 h i+1, \u03b8), (6)\nwhere \u2212\u2192 h i and \u2190\u2212 h i are the hidden states at position i of the forward and backward LSTMs respectively; \u2295 is concatenation operation; \u03b8 denotes all parameters in Bi-LSTM model."}, {"heading": "2.3 Inference Layer", "text": "After extracting features, we employ conditional random fields (CRF) (Lafferty et al.(2001)Lafferty, McCallum, and Pereira) layer to inference tags. In CRF layer, p(Y |X) in Eq (1) could be formalized as:\np(Y |X) = \u03a8(Y |X)\u2211 Y \u2032\u2208Ln \u03a8(Y \u2032|X) . (7)\nHere, \u03a8(Y |X) is the potential function, and we only consider interactions between two successive labels (first order linear chain CRFs):\n\u03a8(Y |X) = n\u220f\ni=2\n\u03c8(X, i, yi\u22121, yi), (8)\n\u03c8(x, i, y\u2032, y) = exp(s(X, i)y + by\u2032y), (9)\nwhere by\u2032y \u2208 R is trainable parameters respective to label pair (y\u2032, y). Score function s(X, i) \u2208 R|L|\nassigns score for each label on tagging the i-th character:\ns(X, i) = W>s hi + bs, (10)\nwhere hi is the hidden state of Bi-LSTM at position i; Ws \u2208 Rdh\u00d7|L| and bs \u2208 R|L| are trainable parameters."}, {"heading": "3 Multi-Criteria Learning for Chinese Word Segmentation", "text": "Although neural models are widely used on CWS, most of them cannot deal with incompatible criteria with heterogonous segmentation criteria simultaneously.\nInspired by the success of multi-task learning (Caruana(1997); Ben-David and Schuller(2003); Liu et al.(2016a)Liu, Qiu, and Huang; Liu et al.(2016b)Liu, Qiu, and Huang), we regard the heterogenous criteria as multiple \u201crelated\u201d tasks, which could improve the performance of each other simultaneously with shared information.\nFormally, assume that there are M corpora with heterogeneous segmentation criteria. We referDm as corpus m with Nm samples:\nDm = {(X(m)i , Y (m) i )} Nm i=1, (11)\nwhere Xmi and Y m i denote the i-th sentence and the corresponding label in corpus m. To exploit the shared information between these different criteria, we propose three sharing models for CWS task as shown in Figure 3. The feature layers of these three models consist of a private (criterion-specific) layer and a shared (criterioninvariant) layer. The difference between three models is the information flow between the task layer and the shared layer. Besides, all of these three models also share the embedding layer."}, {"heading": "3.1 Model-I: Parallel Shared-Private Model", "text": "In the feature layer of Model-I, we regard the private layer and shared layer as two parallel layers. For corpusm, the hidden states of shared layer and private layer are:\nh (s) i =Bi-LSTM(exi ,\n\u2212\u2192 h (s) i\u22121, \u2190\u2212 h (s) i+1, \u03b8s), (12)\nh (m) i =Bi-LSTM(exi ,\n\u2212\u2192 h (m) i\u22121, \u2190\u2212 h (m) i+1, \u03b8m), (13)\nand the score function in the CRF layer is computed as:\ns(m)(X, i) = W(m)s > [ h (s) i\nh (m) i\n] + b(m)s , (14)\nwhere W(m)s \u2208 R2dh\u00d7|L| and b(m)s \u2208 R|L| are criterion-specific parameters for corpus m."}, {"heading": "3.2 Model-II: Stacked Shared-Private Model", "text": "In the feature layer of Model-II, we arrange the shared layer and private layer in stacked manner. The private layer takes output of shared layer as input. For corpus m, the hidden states of shared layer and private layer are:\nh (s) i = Bi-LSTM(exi ,\n\u2212\u2192 h (s) i\u22121, \u2190\u2212 h (s) i+1, \u03b8s), (15)\nh (m) i = Bi-LSTM( [ exi h\n(s) i\n] , \u2212\u2192 h (m) i\u22121, \u2190\u2212 h (m) i+1 , \u03b8m) (16)\nand the score function in the CRF layer is computed as:\ns(m)(X, i) = W(m)s > h (m) i + b (m) s , (17)\nwhere W(m)s \u2208 R2dh\u00d7|L| and b(m)s \u2208 R|L| are criterion-specific parameters for corpus m."}, {"heading": "3.3 Model-III: Skip-Layer Shared-Private Model", "text": "In the feature layer of Model-III, the shared layer and private layer are in stacked manner as ModelII. Additionally, we send the outputs of shared layer to CRF layer directly.\nThe Model III can be regarded as a combination of Model-I and Model-II. For corpus m, the hidden states of shared layer and private layer are the same with Eq (15) and (16), and the score function in CRF layer is computed as the same as Eq (14)."}, {"heading": "3.4 Objective function", "text": "The parameters of the network are trained to maximize the log conditional likelihood of true labels\non all the corpora. The objective function Jseg can be computed as:\nJseg(\u0398m,\u0398s) = M\u2211\nm=1 Nm\u2211 i=1 log p(Y (m) i |X (m) i ; \u0398 m,\u0398s),\n(18)\nwhere \u0398m and \u0398s denote all the parameters in private and shared layers respectively."}, {"heading": "4 Incorporating Adversarial Training for Shared Layer", "text": "Although the shared-private model separates the feature space into shared and private spaces, there is no guarantee that sharable features do not exist in private feature space, or vice versa. Inspired by the work on domain adaptation (Ajakan et al.(2014)Ajakan, Germain, Larochelle, Laviolette, and Marchand; Ganin et al.(2016)Ganin,\nUstinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, and Lempitsky; Bousmalis et al.(2016)Bousmalis, Trigeorgis, Silberman, Krishnan, and Erhan), we hope that the features extracted by shared layer is invariant across the heterogonous segmentation criteria. Therefore, we jointly optimize the shared layer via adversarial training (Goodfellow et al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio).\nTherefore, besides the task loss for CWS, we additionally introduce an adversarial loss to prevent criterion-specific feature from creeping into shared space as shown in Figure 4. We use a criterion discriminator which aims to recognize which criterion the sentence is annotated by using the shared features.\nSpecifically, given a sentence X with length n, we refer to h(s)X as shared features for X in one of the sharing models. Here, we compute h(s)X by simply averaging the hidden states of shared layer h\n(s) X = 1 n \u2211n i h (s) xi . The criterion discriminator computes the probability p(\u00b7|X) over all criteria as:\np(\u00b7|X; \u0398d,\u0398s) = softmax(W>d h (s) X + bd), (19)\nwhere \u0398d indicates the parameters of criterion discriminator Wd \u2208 Rdh\u00d7M and bd \u2208 RM ; \u0398s denotes the parameters of shared layers."}, {"heading": "4.1 Adversarial loss function", "text": "The criterion discriminator maximizes the cross entropy of predicted criterion distribution p(\u00b7|X) and true criterion.\nmax \u0398d J 1adv(\u0398d) = M\u2211 m=1 Nm\u2211 i=1 log p(m|X(m)i ; \u0398 d,\u0398s). (20)\nAn adversarial loss aims to produce shared features, such that a criterion discriminator cannot reliably predict the criterion by using these shared features. Therefore, we maximize the entropy of predicted criterion distribution when training shared parameters.\nmax \u0398s J 2adv(\u0398s) = M\u2211 m=1 Nm\u2211 i=1 H ( p(m|X(m)i ; \u0398 d,\u0398s) ) ,\n(21) where H(p) = \u2212 \u2211\ni pi log pi is an entropy of distribution p.\nUnlike (Ganin et al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, and Lempitsky), we use entropy term instead of negative cross-entropy.\nAlgorithm 1 Adversarial multi-criteria learning for CWS task.\n1: for i = 1; i <= n epoch; i+ + do 2: # Train tag predictor for CWS 3: for m = 1; m <= M ; m+ + do 4: # Randomly pick data from corpus m 5: B = {X,Y }bm1 \u2208 Dm 6: \u0398s += \u03b1\u2207\u0398sJ (\u0398;B) 7: \u0398m += \u03b1\u2207\u0398mJ (\u0398;B) 8: end for 9: # Train criterion discriminator\n10: for m = 1; m <= M ; m+ + do 11: B = {X,Y }bm1 \u2208 Dm 12: \u0398d += \u03b1\u2207\u0398dJ (\u0398;B) 13: end for 14: end for"}, {"heading": "5 Training", "text": "Finally, we combine the task and adversarial objective functions.\nJ (\u0398;D) = Jseg(\u0398m,\u0398s) + J 1adv(\u0398d) + \u03bbJ 2adv(\u0398s), (22) where \u03bb is the weight that controls the interaction of the loss terms and D is the training corpora.\nThe training procedure is to optimize two discriminative classifiers alternately as shown in Algorithm 1. We use Adam (Kingma and Ba(2014)) with minibatchs to maximize the objectives.\nNotably, when using adversarial strategy, we firstly train 2400 epochs (each epoch only trains on eight batches from different corpora), then we only optimize Jseg(\u0398m,\u0398s) with \u0398s fixed until convergence (early stop strategy)."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Datasets", "text": "To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson(2005)) and SIGHAN2008 (Jin and Chen(2008)). Table 1 gives the details of the eight datasets. Among these datasets, AS, CITYU and CKIP are traditional Chinese, while the remains, MSRA, PKU, CTB, NCC and SXU, are simplified Chinese. We use 10% data of shuffled train set as development set for all datasets."}, {"heading": "6.2 Experimental Configurations", "text": "For hyper-parameter configurations, we set both the character embedding size de and the dimen-\nsionality of LSTM hidden states dh to 100. The initial learning rate \u03b1 is set to 0.01. The loss weight coefficient \u03bb is set to 0.05. Since the scale of each dataset varies, we use different training batch sizes for datasets. Specifically, we set batch sizes of AS and MSR datasets as 512 and 256 respectively, and 128 for remains. We employ dropout strategy on embedding layer, keeping 80% inputs (20% dropout rate).\nFor initialization, we randomize all parameters following uniform distribution at (\u22120.05, 0.05). We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (Mikolov et al.(2013)Mikolov, Chen, Corrado, and Dean). Following previous work (Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang; Pei et al.(2014)Pei, Ge, and Baobao), all experiments including baseline results are using pre-trained character embedding with bigram feature."}, {"heading": "6.3 Overall Results", "text": "Table 2 shows the experiment results of the proposed models on test sets of eight CWS datasets, which has three blocks.\n(1) In the first block, we can see that the performance is boosted by using Bi-LSTM, and the performance of Bi-LSTM cannot be improved by merely increasing the depth of networks. In addition, although the F value of LSTM model in (Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang) is 97.4%, they additionally incorporate an external idiom dictionary.\n(2) In the second block, our proposed three models based on multi-criteria learning boost performance. Model-I gains 0.75% improvement on averaging F-measure score compared with BiLSTM result (94.14%). Only the performance on MSRA drops slightly. Compared to the baseline results (Bi-LSTM and stacked Bi-LSTM), the proposed models boost the performance with the help of exploiting information across these heterogeneous segmentation criteria. Although various criteria have different segmentation granularities, there are still some underlying information shared. For instance, MSRA and CTB treat family name and last name as one token \u201c (NingZeTao)\u201d, whereas some other datasets, like PKU, regard them as two tokens, \u201c (Ning)\u201d and \u201c (ZeTao)\u201d. The partial boundaries (before \u201c (Ning)\u201d or after \u201c (Tao)\u201d) can be shared.\n(3) In the third block, we introduce adversarial training. By introducing adversarial training, the performances are further boosted, and Model-I is slightly better than Model-II and Model-III. The adversarial training tries to make shared layer keep criteria-invariant features. For instance, as shown in Table 2, when we use shared information, the performance on MSRA drops (worse than baseline result). The reason may be that the shared parameters bias to other segmentation criteria and introduce noisy features into shared parameters. When we additionally incorporate the adversarial strategy, we observe that the performance on MSRA is improved and outperforms the baseline results. We could also observe the improvements on other datasets. However, the boost from the adversarial strategy is not significant. The main rea-\nson might be that the proposed three sharing models implicitly attempt to keep invariant features by shared parameters and learn discrepancies by the task layer."}, {"heading": "6.4 Speed", "text": "To further explore the convergence speed, we plot the results on development sets through epochs. Figure 5 shows the learning curve of Model-I without incorporating adversarial strategy. As shown in Figure 5, the proposed model makes progress gradually on all datasets. After about 1000 epochs, the performance becomes stable and convergent.\nWe also test the decoding speed, and our mod-\nels process 441.38 sentences per second averagely. As the proposed models and the baseline models (Bi-LSTM and stacked Bi-LSTM) are nearly in the same complexity, all models are nearly the same efficient. However, the time consumption of training process varies from model to model. For the models without adversarial training, it costs about 10 hours for training (the same for stacked Bi-LSTM to train eight datasets), whereas it takes about 16 hours for the models with adversarial training. All the experiments are conducted on the hardware with Intel(R) Xeon(R) CPU E5-2643 v3 @ 3.40GHz and NVIDIA GeForce GTX TITAN X."}, {"heading": "6.5 Error Analysis", "text": "We further investigate the benefits of the proposed models by comparing the error distributions between the single-criterion learning (baseline model Bi-LSTM) and multi-criteria learning (Model-I and Model-I with adversarial training) as shown in Figure 6. According to the results, we could observe that a large proportion of points lie above diagonal lines in Figure 6a and Figure 6b, which implies that performance benefit from integrating knowledge and complementary information from other corpora. As shown in Table 2, on the test set of CITYU, the performance of Model-I and its adversarial version (Model-I+ADV) boost from 92.17% to 95.59% and 95.42% respectively.\nIn addition, we observe that adversarial strategy is effective to prevent criterion specific features\nfrom creeping into shared space. For instance, the segmentation granularity of personal name is often different according to heterogenous criteria. With the help of adversarial strategy, our models could correct a large proportion of mistakes on personal name. Table 7 lists the examples from 2333-th and 89-th sentences in test sets of PKU and MSRA datasets respectively."}, {"heading": "7 Knowledge Transfer", "text": "We also conduct experiments of whether the shared layers can be transferred to the other related tasks or domains. In this section, we investigate the ability of knowledge transfer on two experiments: (1) simplified Chinese to traditional Chinese and (2) formal texts to informal texts."}, {"heading": "7.1 Simplified Chinese to Traditional Chinese", "text": "Traditional Chinese and simplified Chinese are two similar languages with slightly difference on character forms (e.g. multiple traditional characters might map to one simplified character). We investigate that if datasets in traditional Chinese and simplified Chinese could help each other. Table 3 gives the results of Model-I on 3 traditional Chinese datasets under the help of 5 simplified Chinese datasets. Specifically, we firstly train the model on simplified Chinese datasets, then we train traditional Chinese datasets independently with shared parameters fixed.\nAs we can see, the average performance is boosted by 0.41% on F-measure score (from 93.78% to 94.19%), which indicates that shared\nfeatures learned from simplified Chinese segmentation criteria can help to improve performance on traditional Chinese. Like MSRA, as AS dataset is relatively large (train set of 5.4M tokens), the features learned by shared parameters might bias to other datasets and thus hurt performance on such large dataset AS."}, {"heading": "7.2 Formal Texts to Informal Texts", "text": ""}, {"heading": "7.2.1 Dataset", "text": "We use the NLPCC 2016 dataset2 (Qiu et al.(2016)Qiu, Qian, and Shi) to evaluate our model on micro-blog texts. The NLPCC 2016 data are provided by the shared task in the 5th CCF Conference on Natural Language Processing & Chinese Computing (NLPCC 2016): Chinese Word Segmentation and POS Tagging for microblog Text. Unlike the popular used newswire dataset, the NLPCC 2016 dataset is collected from Sina Weibo3, which consists of the informal texts from micro-blog with the various topics, such as finance, sports, entertainment, and so on. The information of the dataset is shown in Table 4."}, {"heading": "7.2.2 Results", "text": "Formal documents (like the eight datasets in Table 1) and micro-blog texts are dissimilar in many aspects. Thus, we further investigate that if the formal texts could help to improve the performance of micro-blog texts. Table 5 gives the results of Model-I on the NLPCC 2016 dataset under the help of the eight datasets in Table 1. Specifically,\n2https://github.com/FudanNLP/ NLPCC-WordSeg-Weibo\n3http://www.weibo.com/\nwe firstly train the model on the eight datasets, then we train on the NLPCC 2016 dataset alone with shared parameters fixed. The baseline model is Bi-LSTM which is trained on the NLPCC 2016 dataset alone.\nAs we can see, the performance is boosted by 0.30% on F-measure score (from 93.94% to 94.24%), and we could also observe that the OOV recall rate is boosted by 3.97%. It shows that the shared features learned from formal texts can help to improve the performance on of micro-blog texts."}, {"heading": "8 Related Works", "text": "There are many works on exploiting heterogeneous annotation data to improve various NLP tasks. Jiang et al.(2009)Jiang, Huang, and Liu) proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations. Sun and Wan(2012)) proposed a structure-based stacking model to reduce the approximation error, which makes use of structured features such as subwords. These models are unidirectional aid and also suffer from error propagation problem.\nQiu et al.(2013)Qiu, Zhao, and Huang) used multi-tasks learning framework to improve the performance of POS tagging on two heterogeneous datasets. Li et al.(2015)Li, Chao, Zhang, and Chen) proposed a coupled sequence labeling model which could directly learn and infer two heterogeneous annotations. Chao et al.(2015)Chao, Li, Chen, and Zhang) also utilize multiple corpora using coupled sequence labeling model. These methods adopt the shallow classifiers, therefore suffering from the problem of defining shared features.\nOur proposed models use deep neural networks, which can easily share information with hidden shared layers. Chen et al.(2016)Chen, Zhang, and Liu) also adopted neural network models for exploiting heterogeneous annotations based on neural multi-view model, which can be regarded as a simplified version of our proposed models by re-\nmoving private hidden layers. Unlike the above models, we design three sharing-private architectures and keep shared layer to extract criterion-invariance features by introducing adversarial training. Moreover, we fully exploit eight corpora with heterogeneous segmentation criteria to model the underlying shared information."}, {"heading": "9 Conclusions & Future Works", "text": "In this paper, we propose adversarial multi-criteria learning for CWS by fully exploiting the underlying shared knowledge across multiple heterogeneous criteria. Experiments show that our proposed three shared-private models are effective to extract the shared information, and achieve significant improvements over the single-criterion methods."}, {"heading": "Acknowledgments", "text": "We appreciate the contribution from Jingjing Gong and Jiacheng Xu. Besides, we would like to thank the anonymous reviewers for their valuable comments. This work is partially funded by National Natural Science Foundation of China (No. 61532011 and 61672162), Shanghai Municipal Science and Technology Commission on (No. 16JC1420401)."}], "references": [{"title": "Domain-adversarial neural networks", "author": ["Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand."], "venue": "arXiv preprint arXiv:1412.4446 .", "citeRegEx": "Ajakan et al\\.,? 2014", "shortCiteRegEx": "Ajakan et al\\.", "year": 2014}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["S. Ben-David", "R. Schuller."], "venue": "Learning Theory and Kernel Machines pages 567\u2013580.", "citeRegEx": "Ben.David and Schuller.,? 2003", "shortCiteRegEx": "Ben.David and Schuller.", "year": 2003}, {"title": "Domain separation networks", "author": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."], "venue": "Advances in Neural Information Processing Systems. pages 343\u2013 351.", "citeRegEx": "Bousmalis et al\\.,? 2016", "shortCiteRegEx": "Bousmalis et al\\.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Machine learning 28(1):41\u201375.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Exploiting heterogeneous annotations for weibo word segmentation and pos tagging", "author": ["Jiayuan Chao", "Zhenghua Li", "Wenliang Chen", "Min Zhang."], "venue": "National CCF Conference on Natural Language Processing and Chinese Computing. Springer, pages", "citeRegEx": "Chao et al\\.,? 2015", "shortCiteRegEx": "Chao et al\\.", "year": 2015}, {"title": "Neural network for heterogeneous annotations", "author": ["Hongshen Chen", "Yue Zhang", "Qun Liu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Gated recursive neural network for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang."], "venue": "Proceedings of Annual Meeting of the Association for Computational Linguistics..", "citeRegEx": "Chen et al\\.,? 2015a", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang."], "venue": "EMNLP. pages 1197\u20131206.", "citeRegEx": "Chen et al\\.,? 2015b", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of ICML.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "The second international chinese word segmentation bakeoff", "author": ["Thomas Emerson."], "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing. volume 133.", "citeRegEx": "Emerson.,? 2005", "shortCiteRegEx": "Emerson.", "year": 2005}, {"title": "The part-of-speech tagging guidelines for the penn chinese treebank (3.0)", "author": ["XIA Fei"], "venue": "URL: http://www. cis. upenn. edu/ \u0303 chinese/segguide", "citeRegEx": "Fei.,? \\Q2000\\E", "shortCiteRegEx": "Fei.", "year": 2000}, {"title": "Domain-adversarial training of neural networks", "author": ["Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand", "Victor Lempitsky."], "venue": "Journal of Machine Learning Research", "citeRegEx": "Ganin et al\\.,? 2016", "shortCiteRegEx": "Ganin et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems. pages 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850 .", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging: a case study", "author": ["W. Jiang", "L. Huang", "Q. Liu."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint", "citeRegEx": "Jiang et al\\.,? 2009", "shortCiteRegEx": "Jiang et al\\.", "year": 2009}, {"title": "The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging", "author": ["G. Jin", "X. Chen."], "venue": "Sixth SIGHAN Workshop on Chinese Language Processing. page 69.", "citeRegEx": "Jin and Chen.,? 2008", "shortCiteRegEx": "Jin and Chen.", "year": 2008}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of The 32nd International Conference on Machine Learning.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Coupled sequence labeling on heterogeneous annotations: Pos tagging as a case study", "author": ["Zhenghua Li", "Jiayuan Chao", "Min Zhang", "Wenliang Chen."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Fast coupled sequence labeling on heterogeneous annotations via context-aware pruning", "author": ["Zhenghua Li", "Jiayuan Chao", "Min Zhang", "Jiwen Yang."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep multi-task learning with shared memory", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Liu et al\\.,? 2016a", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Recurrent neural network for text classification with multi-task learning", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of International Joint Conference on Artificial Intelligence.", "citeRegEx": "Liu et al\\.,? 2016b", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Maxmargin tensor neural network for chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Chang Baobao."], "venue": "Proceedings of ACL.", "citeRegEx": "Pei et al\\.,? 2014", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Overview of the NLPCC-ICCPOL 2016 shared task: Chinese word segmentation for micro-blog texts", "author": ["Xipeng Qiu", "Peng Qian", "Zhan Shi."], "venue": "International Conference on Computer Processing of Oriental Languages. Springer, pages 901\u2013906.", "citeRegEx": "Qiu et al\\.,? 2016", "shortCiteRegEx": "Qiu et al\\.", "year": 2016}, {"title": "Joint chinese word segmentation and pos tagging on heterogeneous annotated corpora with multiple task learning", "author": ["Xipeng Qiu", "Jiayi Zhao", "Xuanjing Huang."], "venue": "EMNLP. pages 658\u2013668.", "citeRegEx": "Qiu et al\\.,? 2013", "shortCiteRegEx": "Qiu et al\\.", "year": 2013}, {"title": "Reducing approximation and estimation errors for chinese lexical processing with heterogeneous annotations", "author": ["Weiwei Sun", "Xiaojun Wan."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-", "citeRegEx": "Sun and Wan.,? 2012", "shortCiteRegEx": "Sun and Wan.", "year": 2012}, {"title": "Processing norms of modern Chinese corpus", "author": ["S. Yu", "J. Lu", "X. Zhu", "H. Duan", "S. Kang", "H. Sun", "H. Wang", "Q. Zhao", "W. Zhan."], "venue": "Technical report, Technical report.", "citeRegEx": "Yu et al\\.,? 2001", "shortCiteRegEx": "Yu et al\\.", "year": 2001}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "EMNLP. pages 647\u2013657.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": "As shown in Table 1, given a sentence \u201cYaoMing reaches the final\u201d, the two commonlyused corpora, PKU\u2019s People\u2019s Daily (PKU) (Yu et al.(2001)Yu, Lu, Zhu, Duan, Kang, Sun, Wang, Zhao, and Zhan) and Penn Chinese Treebank (CTB) (Fei(2000)), use different segmentation criteria.", "startOffset": 125, "endOffset": 141}, {"referenceID": 10, "context": "(2001)Yu, Lu, Zhu, Duan, Kang, Sun, Wang, Zhao, and Zhan) and Penn Chinese Treebank (CTB) (Fei(2000)), use different segmentation criteria.", "startOffset": 91, "endOffset": 101}, {"referenceID": 10, "context": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.", "startOffset": 136, "endOffset": 155}, {"referenceID": 10, "context": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.", "startOffset": 136, "endOffset": 195}, {"referenceID": 10, "context": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.(2013)Qiu, Zhao, and Huang; Li et al.", "startOffset": 136, "endOffset": 213}, {"referenceID": 10, "context": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.(2013)Qiu, Zhao, and Huang; Li et al.(2015)Li, Chao, Zhang, and Chen; Li et al.", "startOffset": 136, "endOffset": 250}, {"referenceID": 10, "context": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al.(2009)Jiang, Huang, and Liu; Sun and Wan(2012); Qiu et al.(2013)Qiu, Zhao, and Huang; Li et al.(2015)Li, Chao, Zhang, and Chen; Li et al.(2016)Li, Chao, Zhang, and Yang).", "startOffset": 136, "endOffset": 292}, {"referenceID": 5, "context": "Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston(2008); Luong et al.", "startOffset": 107, "endOffset": 134}, {"referenceID": 5, "context": "Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston(2008); Luong et al.(2015)Luong, Le, Sutskever, Vinyals, and Kaiser; Chen et al.", "startOffset": 107, "endOffset": 154}, {"referenceID": 5, "context": "(2015)Luong, Le, Sutskever, Vinyals, and Kaiser; Chen et al.(2016)Chen, Zhang, and Liu).", "startOffset": 49, "endOffset": 67}, {"referenceID": 2, "context": "Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana(1997); BenDavid and Schuller(2003)), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteriaspecific features.", "startOffset": 163, "endOffset": 177}, {"referenceID": 2, "context": "Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana(1997); BenDavid and Schuller(2003)), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteriaspecific features.", "startOffset": 163, "endOffset": 206}, {"referenceID": 0, "context": "Inspired by the success of adversarial strategy on domain adaption (Ajakan et al.(2014)Ajakan, Germain, Larochelle, Laviolette, and Marchand; Ganin et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 0, "context": "Inspired by the success of adversarial strategy on domain adaption (Ajakan et al.(2014)Ajakan, Germain, Larochelle, Laviolette, and Marchand; Ganin et al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviar X iv :1 70 4.", "startOffset": 68, "endOffset": 161}, {"referenceID": 2, "context": "olette, Marchand, and Lempitsky; Bousmalis et al.(2016)Bousmalis, Trigeorgis, Silberman, Krishnan, and Erhan), we further utilize adversarial strategy to make sure the shared layer can extract the common underlying and criteria-invariant features, which are suitable for all the criteria.", "startOffset": 33, "endOffset": 56}, {"referenceID": 26, "context": "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al.(2013)Zheng, Chen, and Xu; Pei et al.", "startOffset": 144, "endOffset": 163}, {"referenceID": 22, "context": "(2013)Zheng, Chen, and Xu; Pei et al.(2014)Pei, Ge, and Baobao; Chen et al.", "startOffset": 27, "endOffset": 44}, {"referenceID": 5, "context": "(2014)Pei, Ge, and Baobao; Chen et al.(2015a)Chen, Qiu, Zhu, and Huang; Chen et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 5, "context": "(2014)Pei, Ge, and Baobao; Chen et al.(2015a)Chen, Qiu, Zhu, and Huang; Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang).", "startOffset": 27, "endOffset": 91}, {"referenceID": 15, "context": "While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al.(2015)Jozefowicz, Zaremba, and Sutskever), which is similar to the architecture of (Graves(2013)) but without peep-hole connections.", "startOffset": 83, "endOffset": 107}, {"referenceID": 13, "context": "(2015)Jozefowicz, Zaremba, and Sutskever), which is similar to the architecture of (Graves(2013)) but without peep-hole connections.", "startOffset": 84, "endOffset": 97}, {"referenceID": 18, "context": "After extracting features, we employ conditional random fields (CRF) (Lafferty et al.(2001)Lafferty, McCallum, and Pereira) layer to inference tags.", "startOffset": 70, "endOffset": 92}, {"referenceID": 2, "context": "Inspired by the success of multi-task learning (Caruana(1997); Ben-David and Schuller(2003); Liu et al.", "startOffset": 48, "endOffset": 62}, {"referenceID": 1, "context": "Inspired by the success of multi-task learning (Caruana(1997); Ben-David and Schuller(2003); Liu et al.", "startOffset": 63, "endOffset": 92}, {"referenceID": 1, "context": "Inspired by the success of multi-task learning (Caruana(1997); Ben-David and Schuller(2003); Liu et al.(2016a)Liu, Qiu, and Huang; Liu et al.", "startOffset": 63, "endOffset": 111}, {"referenceID": 1, "context": "Inspired by the success of multi-task learning (Caruana(1997); Ben-David and Schuller(2003); Liu et al.(2016a)Liu, Qiu, and Huang; Liu et al.(2016b)Liu, Qiu, and Huang), we regard the heterogenous criteria as multiple \u201crelated\u201d tasks, which could improve the performance of each other simultaneously with shared information.", "startOffset": 63, "endOffset": 149}, {"referenceID": 0, "context": "Inspired by the work on domain adaptation (Ajakan et al.(2014)Ajakan, Germain, Larochelle, Laviolette, and Marchand; Ganin et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 0, "context": "Inspired by the work on domain adaptation (Ajakan et al.(2014)Ajakan, Germain, Larochelle, Laviolette, and Marchand; Ganin et al.(2016)Ganin,", "startOffset": 43, "endOffset": 136}, {"referenceID": 2, "context": "Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, and Lempitsky; Bousmalis et al.(2016)Bousmalis, Trigeorgis, Silberman, Krishnan, and Erhan), we hope that the features extracted by shared layer is invariant across the heterogonous segmentation criteria.", "startOffset": 76, "endOffset": 99}, {"referenceID": 2, "context": "Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, and Lempitsky; Bousmalis et al.(2016)Bousmalis, Trigeorgis, Silberman, Krishnan, and Erhan), we hope that the features extracted by shared layer is invariant across the heterogonous segmentation criteria. Therefore, we jointly optimize the shared layer via adversarial training (Goodfellow et al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio).", "startOffset": 76, "endOffset": 364}, {"referenceID": 11, "context": "Unlike (Ganin et al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, and Lempitsky), we use entropy term instead of negative cross-entropy.", "startOffset": 8, "endOffset": 27}, {"referenceID": 17, "context": "We use Adam (Kingma and Ba(2014)) with minibatchs to maximize the objectives.", "startOffset": 13, "endOffset": 33}, {"referenceID": 9, "context": "To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson(2005)) and SIGHAN2008 (Jin and Chen(2008)).", "startOffset": 102, "endOffset": 116}, {"referenceID": 9, "context": "To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson(2005)) and SIGHAN2008 (Jin and Chen(2008)).", "startOffset": 102, "endOffset": 152}, {"referenceID": 21, "context": "We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (Mikolov et al.(2013)Mikolov, Chen, Corrado, and Dean).", "startOffset": 211, "endOffset": 232}, {"referenceID": 5, "context": "Following previous work (Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang; Pei et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 5, "context": "Following previous work (Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang; Pei et al.(2014)Pei, Ge, and Baobao), all experiments including baseline results are using pre-trained character embedding with bigram feature.", "startOffset": 25, "endOffset": 92}, {"referenceID": 5, "context": "In addition, although the F value of LSTM model in (Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang) is 97.", "startOffset": 52, "endOffset": 71}, {"referenceID": 26, "context": "1 Dataset We use the NLPCC 2016 dataset2 (Qiu et al.(2016)Qiu, Qian, and Shi) to evaluate our model on micro-blog texts.", "startOffset": 42, "endOffset": 59}, {"referenceID": 10, "context": "Jiang et al.(2009)Jiang, Huang, and Liu) proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "Jiang et al.(2009)Jiang, Huang, and Liu) proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations. Sun and Wan(2012)) proposed a structure-based stacking model to reduce the approximation error, which makes use of structured features such as subwords.", "startOffset": 0, "endOffset": 237}, {"referenceID": 10, "context": "Jiang et al.(2009)Jiang, Huang, and Liu) proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations. Sun and Wan(2012)) proposed a structure-based stacking model to reduce the approximation error, which makes use of structured features such as subwords. These models are unidirectional aid and also suffer from error propagation problem. Qiu et al.(2013)Qiu, Zhao, and Huang) used multi-tasks learning framework to improve the performance of POS tagging on two heterogeneous datasets.", "startOffset": 0, "endOffset": 473}, {"referenceID": 10, "context": "Jiang et al.(2009)Jiang, Huang, and Liu) proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations. Sun and Wan(2012)) proposed a structure-based stacking model to reduce the approximation error, which makes use of structured features such as subwords. These models are unidirectional aid and also suffer from error propagation problem. Qiu et al.(2013)Qiu, Zhao, and Huang) used multi-tasks learning framework to improve the performance of POS tagging on two heterogeneous datasets. Li et al.(2015)Li, Chao, Zhang, and Chen) proposed a coupled sequence labeling model which could directly learn and infer two heterogeneous annotations.", "startOffset": 0, "endOffset": 619}, {"referenceID": 4, "context": "Chao et al.(2015)Chao, Li, Chen, and Zhang) also utilize multiple corpora using coupled sequence labeling model.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Chao et al.(2015)Chao, Li, Chen, and Zhang) also utilize multiple corpora using coupled sequence labeling model. These methods adopt the shallow classifiers, therefore suffering from the problem of defining shared features. Our proposed models use deep neural networks, which can easily share information with hidden shared layers. Chen et al.(2016)Chen, Zhang, and Liu) also adopted neural network models for exploiting heterogeneous annotations based on neural multi-view model, which can be regarded as a simplified version of our proposed models by re-", "startOffset": 0, "endOffset": 350}], "year": 2017, "abstractText": "Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github1.", "creator": "TeX"}}}