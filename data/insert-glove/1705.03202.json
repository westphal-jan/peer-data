{"id": "1705.03202", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Does William Shakespeare REALLY Write Hamlet? Knowledge Representation Learning with Confidence", "abstract": "itochu Knowledge gold-colored graphs (deng KGs) can provide t\u00e2n significant gedrick relational a information cadoxton and propranolol have been widely utilized downslope in 2252 various cess tasks. However, nulla there eziba may exist amounts qv of noises kd4 and conflicts pm in froehlich KGs, especially in those reassignments constructed 11-july automatically maritimus with razin less human supervision. 242-pound To kadare address gratton this problem, we propose a seven-episode novel confidence - powelliphanta aware knowledge retro-styled representation al-muhajiroun learning sicel framework (CKRL ), cicuta which feng detects beitrag possible '85 noises in blackwolf KGs sisti while 2,431 learning knowledge auray representations with confidence simultaneously. big-city Specifically, accetturo we hyperpower introduce adenan the triple confidence to awesomely conventional neckarsulm translation - 28-7 based methods authentification for knowledge trowell representation learning. To schempp make triple reissuing confidence more paukphaw flexible and ofa universal, upshaw we methacrylate only utilize lemak the sciquest internal structural filthiest information crewson in rc-7 KGs, tidey and 62.61 propose three kinds of triple confidences considering bkf both cavities local 2-49 triple 43.37 and global path 0.925 information. cdi We reconstructed evaluate metrobuses our lancets models on 97-1 knowledge barclay graph noise detection, knowledge whitewright graph laso completion and baller triple badaruddin classification. 56a Experimental osuji results warranto demonstrate plate-like that kdot our confidence - programme aware models achieve superville significant pre-dated and consistent yaound\u00e9 improvements on usareur all impugning tasks, which confirms episode the capability of our loafers CKRL model in both grinch noise netas detection shoebat and knowledge representation learning.", "histories": [["v1", "Tue, 9 May 2017 06:46:21 GMT  (214kb,D)", "http://arxiv.org/abs/1705.03202v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ruobing xie", "zhiyuan liu", "maosong sun"], "accepted": false, "id": "1705.03202"}, "pdf": {"name": "1705.03202.pdf", "metadata": {"source": "CRF", "title": "Does William Shakespeare REALLY Write Hamlet? Knowledge Representation Learning with Confidence", "authors": ["Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Recent years have witnessed the great thrive in artificial intelligence that has broad impacts on our daily lives. In fields like information retrieval and question answering, people are not satisfied with merely matching, but expect AI agents to have knowledge for understanding, reasoning and solving. Knowledge graphs (KGs), which provide effective wellstructured relational information between entities, are essential supporters for knowledge-based AI agents. A typical KG usually stores knowledge with triple facts like (head entity, relation, tail entity), also abridged as (h, r, t).\nThere are existing amounts of widely-utilized large-scale knowledge graphs such as Freebase [Bollacker et al., 2008], DBpedia [Auer et al., 2007] and other domain-specific KGs. However, these knowledge graphs are still far from complete\nto describe the infinite real-world facts. Conventional knowledge graph construction methods usually involve huge human supervision or expert annotation, which are extremely timeconsuming and labor-intensive. Hence, automatic mechanism and crowdsourcing take larger parts in knowledge construction, while these methods may suffer from possible noises and conflicts due to limited human supervision. For instance, the state-of-the-art relation extraction model on benchmark achieves only around 60% precision when the recall is 20% [Lin et al., 2016]. Moreover, [Heindorf et al., 2016] focuses on vandalism detection in Wikidata, which also implies the existence and problems of noises in large-scale KGs. Knowledge representation learning (KRL), which represents KGs with distributed representations, provides an effective and flexible way for using knowledge. Therefore, it is crucial to consider noises in knowledge representation learning and knowledge applications.\nWe attempt to detect possible noises and conflicts in existing knowledge graphs, while constructing noise-free knowledge representations simultaneously. Most conventional KRL methods assume that the existing KGs are absolutely correct. To address this problem, we propose a novel confidence-aware knowledge representation learning framework (CKRL) taking possible noises into consideration. More specifically, the CKRL model follows the translationbased framework proposed by [Bordes et al., 2013], and learns knowledge representations with triple confidences. We propose three kinds of triple confidences considering both local triple and global path information. With the helps of global consistency and multi-step path reasoning, we can infer whether Shakespeare writes Hamlet. To make the triple confidence more universal and flexible, we only consider the\nar X\niv :1\n70 5.\n03 20\n2v 1\n[ cs\n.C L\n] 9\nM ay\n2 01\n7\ninternal structural information in KGs to achieve better global consistency, which correspondingly makes noise detection much more challenging due to the limited information.\nIn experiments, we evaluate our models on three tasks including knowledge graph noise detection, knowledge graph completion and triple classification. The experimental results demonstrate that our models achieve the best performances on all tasks, which confirm the capability of CKRL in noise detection and knowledge representation learning. The main contributions of this work are concluded as follows: \u2022 We propose a novel confidence-aware KRL framework\nfor knowledge graph noise detection and knowledge representation learning simultaneously, which only uses internal structural information in KGs. \u2022 We evaluate our models on different noisy knowledge\ngraphs extended from benchmark and achieve promising performances on all tasks."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Knowledge Graph Noise Detection", "text": "It seems to be inevitable that noises do exist and can strongly affect knowledge acquisition [Manago and Kodratoff, 1987], so that noise detection is essential in knowledge construction and knowledge application. Most knowledge graph noise detections happen when constructing knowledge graphs. For instance, YAGO2 extracts knowledge from Wikipedia with human supervision that human judges are presented with selected facts for which they have to assess the correctness [Hoffart et al., 2013]. Wikidata also relies on a crowdsourced human curation software in which contributors can reject or approve a statement [Pellissier Tanon et al., 2016]. DBpedia creates its mappings to Wikipedia infoboxes via a world-wide crowd-sourcing effort [Lehmann et al., 2015]. These noise detections in large-scale KGs are usually involved with huge human efforts, which are extremely laborintensive and time-consuming.\nAs for automatical KG noise detection, a novel task named Wikidata vandalism, which aims to combat with deliberate destructions in knowledge graphs, has attracted wide attention [Heindorf et al., 2015]. However, most existing methods on this task mainly concentrate on feature selection from contents, users, items and revisions [Heindorf et al., 2016], and thus are constrained with the completeness of external information. There are also some efforts working on judging importance on graphs for nodes [Gyo\u0308ngyi et al., 2004] or for edges [De Meo et al., 2012], but few works concentrating on the confidence of each triple. In this paper, we propose triple confidence, which mainly focuses on the internal structural information of KGs in knowledge graph noise detection and knowledge representation learning."}, {"heading": "2.2 Translation-based KRL Methods", "text": "Recent years many efforts concentrate on learning distributed representations for knowledge graphs, among which the translation-based methods are both straightforward and effective with the state-of-the-art performances. TransE [Bordes et al., 2013] projects both entities and relations into a continuous low-dimensional vector space, interpreting relations\nas translating operations between head and tail entities. The translation assumption in TransE implies the equation that h+ r ' t. The energy function is defined as follows:\nE(h, r, t) = ||h+ r\u2212 t||. (1)\nTransE can well balance the effectiveness and efficiency compared to traditional methods, while the over-simplified translation assumption constrains the performance when dealing with complicated relations. Some enhanced translation-based methods attempt to solve this problem with translations on relation-specific hyperplanes [Wang et al., 2014], relationspecific entity projection [Lin et al., 2015b] and type-specific entity projection [Xie et al., 2016]. Moreover, the translation assumption only focuses on the local information in triples, which may fail to make full use of global graph information in KGs. [Lin et al., 2015a] extends TransE by encoding multistep relation path information into knowledge representation learning. However, most conventional KRL methods assume that all triples in KG share the same confidence, which is inappropriate especially for those KGs constructed automatically with less human supervision. To the best of our knowledge, our model is the first embedding method to consider confidence for existing KGs in KRL. In this paper, we follow TransE to learn knowledge representations, and it is not difficult for other enhanced translation-based methods to utilize our confidence-aware KRL framework."}, {"heading": "3 Methodology", "text": "We first give the notations used in this paper. Given a triple fact (h, r, t), we consider the head and tail entities h, t \u2208 E and the relation r \u2208 R, where E and R stand for the sets of entities and relations. T represents the overall training triple facts including possible conflicts and noises.\nTo detect possible noises in knowledge graphs and learn better knowledge representations, we introduce a novel concept triple confidence for each triple fact. Triple confidence describes the likelihood of a triple to be correct, which could be measured with the favor of both internal structural information and external heterogeneous information."}, {"heading": "3.1 Confidence-aware KRL Framework", "text": "We attempt to detect noises and learn better knowledge representations with triple confidence taken into consideration, concentrating more on those triples with high confidences. Following the translation-based framework, we design our confidence-aware KRL energy function as follows:\nE(T ) = \u2211\n(h,r,t)\u2208T\nE(h, r, t) \u00b7 C(h, r, t). (2)\nThe confidence-aware energy function can be divided into two parts: E(h, r, t) = ||h + r \u2212 t|| stands for the dissimilarity between head, relation and tail, which is the same as that of TransE. A lower dissimilarity score indicates its corresponding triple could better fit the translation assumption. While differing from conventional methods, we also introduce the triple confidence C(h, r, t) as the second part of our energy function. A higher triple confidence implies that the relational knowledge in this triple is more credible.\nTriple confidence can be calculated both during and after knowledge graph construction, and from various aspects including internal information such as KG structures and external information such as textual evidence. To make our triple confidence more universal and practical, we only consider the internal structural information after KG construction in our model, and propose three kinds of triple confidences in the following sections. In training, we measure the dissimilarity as well as confidence for each triple simultaneously, and thus could get better knowledge representations."}, {"heading": "3.2 Objective Formalization", "text": "We introduce the detailed training objective of our model in this section. Following TransE [Bordes et al., 2013], we formalize a margin-based score function with negative sampling as objective for training. This pair-wise score function attempts to make the scores of positive triples to be higher than those of negative triples. We have:\nL = \u2211\n(h,r,t)\u2208T \u2211 (h\u2032,r\u2032,t\u2032)\u2208T \u2032 max(0, \u03b3 + E(h, r, t)\n\u2212E(h\u2032, r\u2032, t\u2032)) \u00b7 C(h, r, t), (3)\nwhere E(h, r, t) is the dissimilarity score of positive triple and E(h\u2032, r\u2032, t\u2032) is that of negative triple. \u03b3 > 0 is the hyperparameter of margin, and T \u2032 represents the negative triple set. Here the triple confidence C(h, r, t) instructs our model to pay more attention on those more convincing facts.\nFor pair-wise training, since there are no explicit negative triples in knowledge graphs, we sample negative triples complying with the following rules:\nT \u2032 ={(h\u2032, r, t)|h\u2032 \u2208 E} \u222a {(h, r, t\u2032)|t\u2032 \u2208 E} \u222a {(h, r\u2032, t)|r\u2032 \u2208 R}, (h, r, t) \u2208 T.\n(4)\nIt means that one entity or relation in a positive triple is randomly replaced by another entity or relation in the overall set. Note that differing from TransE, we also add relation replacements for better performances in relation prediction. We also confirm that the generated negative triples after replacement are not in T to make sure they are truly negative."}, {"heading": "3.3 Local Triple Confidence", "text": "We first come up with the local triple confidence (LT) which only focuses on the inside of a triple. Since our CKRL framework follows the translation assumption that h + r ' t, it is straightforward to directly utilize the dissimilarity function to judge triple confidences. Moreover, the promising results of conventional translation-based methods on triple classification also confirm that positive triples should comply with the translation rule well.\nWe assume that the more a triple fits the translation rule, the more convincing this triple should be considered. To measure the local triple confidence during training, we first judge the current conformity of each triple with translation assumption. Inspired by the margin-based training strategy, we directly use the same pair-wise function to calculate the triple quality Q(h, r, t) as follows:\nQ(h, r, t) = \u2212(\u03b3 + E(h, r, t)\u2212 E(h\u2032, r\u2032, t\u2032)). (5)\nA higher score of triple quality usually indicates a better triple under the translation assumption. In training, we first initialize local triple confidence as 1. Due to the evolutionary entity and relation embeddings during training, LT should change according to its dynamic triple quality. Specifically, the local triple confidence for (h, r, t) changes as follows:\nLT (h, r, t) = { \u03b1LT (h, r, t), Q(h, r, t) 6 0. LT (h, r, t) + \u03b2, Q(h, r, t) > 0.\n(6)\nQ(h, r, t) 6 0 implies the current triple doesn\u2019t comply with the translation rule, and thus the corresponding local triple confidence should decrease. On the contrary, Q(h, r, t) > 0 encourages the triple to have a larger local triple confidence. Here, \u03b1 \u2208 (0, 1) and \u03b2 > 0 are hyper-parameters that control the ascend or descend pace of local triple confidence, with the assurance that LT (h, r, t) \u2208 (0, 1]. Note that the local triple confidence will decrease at a geometric rate while increase with a constant addition. It is because that we urge to punish the violations of translation rule, for those triples are more likely to be noises, and thus should have smaller confidences."}, {"heading": "3.4 Global Path Confidence", "text": "The local triple confidence is straightforward and effective, while merely concentrating on the inside of triples will fail to use rich global structural information in knowledge graphs. Relation paths can give rich global information as supporting evidences for triples. For instance, given the relation path (h, born in city, e) and (e, in nation, t), we can infer with high confidence that (h, born in nation, t). Therefore, we propose the global path confidence to take multi-step relation paths into consideration.\nWe assume that a triple will be considered more credible if it has more reliable paths that are semantically close to the corresponding relation. In the following sub-sections, we first introduce how to quantify the relation path reliability given a triple, and then propose two strategies using co-occurrence information and learned knowledge representations to measure the semantic similarity between path and relation."}, {"heading": "Relation Path Reliability", "text": "We assume that a relation path should be considered more important if it carries more information flow from head to tail entity. Specifically, we follow the path-constraint resource allocation (PCRA) [Lin et al., 2015a] to measure the relation path reliability. The key idea of PCRA is inspired by resource allocation [Zhou et al., 2007], which supposes there are certain resources associated with head entity h, and will flow throughout the whole knowledge graph via all relation paths. The resource amount that eventually flows to the tail entity t via a certain path p will be considered as the relation path reliability of p given the entity pair (h, t).\nFormally, given a path p = (r1, \u00b7 \u00b7 \u00b7 , rl) and entity pair (h, t), the resource in h will flow to t through l steps. Since there are probably multiple tails given head and relation, the path is represented asE0\nr1\u2212\u2192 \u00b7 \u00b7 \u00b7 rl\u2212\u2192 El, whereEi represents the entity set at the i-th step, E0 = h and t \u2208 El. For entity e \u2208 Ei, the resource Rp(e) will be calculated as follows:\nRp(e) = \u2211\ne\u2032\u2208Ei\u22121(\u00b7,e)\nRp(e \u2032)\n|Ei(e\u2032, \u00b7)| , (7)\nin whichEi\u22121(\u00b7, e) represents the direct predecessors of e via ri, and Ei(e\u2032, \u00b7) represents the direct successors of e\u2032 via ri. All entities will be initialized with the same resource amount, and finally after l steps from h to t, the resource amountRp(t) is regarded as the relation path reliability R(h, p, t) of p with the given entity pair (h, t)."}, {"heading": "Prior Path Confidence", "text": "We first introduce prior path confidence (PP), which utilize the co-occurrence of relation and path to represent their dissimilarity. We suppose that the more a relation occurs with a path, the more they are likely to represent similar semantic meanings. Formally, given a triple (h, r, t) and its path set S(h,t) containing all paths between h and t, the quality of the i-th relation-path pair (r, pi) is written as follows:\nQPP (r, pi) = + (1\u2212 ) P (r, pi)\nP (pi) , (8)\nwhere P (r, pi) represents the prior probability of r and pi cooccurrence, and P (pi) represents the prior probability of pi in KG. is a hyper-parameter for smoothing. Therefore, the prior path confidence (PP) is designed as follows:\nPP (h, r, t) = \u2211\npi\u2208S(h,t)\nQPP (r, pi) \u00b7R(h, pi, t). (9)\nIt indicates that the prior path confidence of (h, r, t) depends on both relation-path similarities of all paths in S(h,t) and their corresponding relation path reliabilities. Note that since we merely consider the prior probabilities of paths and relations, prior path confidences are fixed during training."}, {"heading": "Adaptive Path Confidence", "text": "The prior path confidence stays static during training, which is inflexible and may be strongly constrained by existing noises and conflicts in KGs. To address this problem, we propose the adaptive path confidence (AP) that could flexibly learn relation-path qualities according to their learned embeddings. Following the translation assumption, given r and pi = {ri1, \u00b7 \u00b7 \u00b7 , rik}, we define a new relation-path quality function of AP as follows:\nQAP (r, pi) = ||r\u2212 pi|| = ||r\u2212 (ri1 + \u00b7 \u00b7 \u00b7+ rik)||. (10)\nSince we assume that the relation embedding should be similar as the path embedding, a lowerQAP (r, pi) implies a more convincing relation-path pair. The adaptive path confidence is then written as follows:\nAP (h, r, t) = \u03c3( \u2211\npi\u2208S(h,t)\nR(h, pi, t) QAP (r, pi) ), (11)\nin which \u03c3(\u00b7) stands for the sigmoid function. Adaptive path confidence can describe triple confidences dynamically with evolutionary relation embeddings during training, making our triple confidences more flexible and precise.\nThe overall triple confidence combines with all three kinds of confidences stated above. We have:\nC(h, r, t) = \u03bb1 \u00b7 LT (h, r, t) + \u03bb2 \u00b7 PP (h, r, t) + \u03bb3 \u00b7AP (h, r, t),\n(12)\nwhere \u03bb1, \u03bb2, \u03bb3 are hyper-parameters."}, {"heading": "3.5 Optimization and Implementation Details", "text": "We utilize mini-batch stochastic gradient descent (SGD) to optimize our model. In training, all entity and relation embeddings could be either initialized randomly or pre-trained with TransE, with the local triple confidence for all triples initialized as 1. For those entity pairs that don\u2019t have paths, we directly set their path-based confidences as 0.\nPath selection is essential in our model that will have significant impacts on the performances. Since the number of all paths grows exponentially with the increase of maximum path length, it is impractical to enumerate all paths in KG. Moreover, the path-based inference will be much weaker when the logical chain goes too far. Considering both effectiveness and efficiency, we limit the maximum length of paths to at most 2-steps. Since relations are directed edges, we also consider those reverse relations when we detect relation paths."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Datasets", "text": "In this paper, we evaluate our confidence-aware models based on FB15K [Bordes et al., 2013], which is a typical benchmark knowledge graph extracted from Freebase [Bollacker et al., 2008]. However, there are no explicit labelled noises or conflicts in FB15K. Therefore, we generate new datasets with different probabilities of noises based on FB15K to simulate the real-world knowledge graphs constructed automatically with less human annotation.\nInspired by the preprocessing in the evaluation task named triple classification, we construct negative triples following the same setting in [Socher et al., 2013]. Specifically, given a positive triple (h, r, t) in KG, we randomly switch one of head or tail entities to form a negative triple (h\u2032, r, t) or (h, r, t\u2032). The entity replacement is constrained that new entities should have appeared in the same position with relation in dataset, which means h\u2032 should have appeared at the head position of r in positive triples (e.g. the head of relation write should be a writer). This constraint focuses on generating harder and more confusing cases, for those negative triples with mistype entities could be easily detected. We can directly utilize entity type information in Freebase or follow the local closed-world assumption [Krompa\u00df et al., 2015] to collect entity constraint information. Following this protocol, we construct 3 noisy KGs based on FB15K with noises to be 10%, 20% and 40% of positive triples, and then discard a small number of negative triples that violate type constraints. These noisy datasets share the same entities, relations, validation and test sets with FB15K, and all generated negative triples are fused into original training set. The statistics of all datasets are listed in Table 1."}, {"heading": "4.2 Experimental Settings", "text": "In experiments, we evaluate our confidence-aware KRL models with three different confidence combination strategies. CKRL (LT) represents the strategy which only considers local triple confidence, CKRL (LT+PP) considers both local triple confidence and prior path confidence, while CKRL (LT+PP+AP) considers all three kinds of confidences. We implement TransE [Bordes et al., 2013] as baseline since our CKRL framework is based on TransE, and it is not difficult for our confidence framework to be utilized in other enhanced translation-based methods.\nWe train our CKRL model using mini-batch SGD with the margin \u03b3 set among {0.5, 1.0, 2.0}. We select the overall learning rate \u03b4 empirically among {0.0005, 0.001, 0.002} which is fixed during training. For local triple confidence, we select the descend controller \u03b1 among {0.5, 0.7, 0.9} and the ascend controller \u03b2 among {0.0001, 0.0005, 0.001}. For prior path confidence, the smoothing is empirically set as 0.01. The optimal configurations of our models are: \u03b3 = 1.0, \u03b4 = 0.001, \u03b1 = 0.9, \u03b2 = 0.0001. We also evaluate various combination weights \u03bbi when we calculate the overall triple confidence based on the three proposed methods. We select a unified weighting strategy for different evaluation tasks and datasets according to their overall performances to show the robustness of our models. Specifically, for CKRL (LT+PP), we select \u03bb1 = 0.9 and \u03bb2 = 0.1, while for CKRL (LT+PP+AP), we select \u03bb1 = 1.5, \u03bb2 = 0.1 and \u03bb2 = 0.4. For fair comparisons, the dimensions of both entity and relation embeddings in all models are equally set to be 50."}, {"heading": "4.3 Knowledge Graph Noise Detection", "text": "To verify the capability of our CKRL models in distinguishing noises and conflicts in knowledge graphs, we propose a novel evaluation task named knowledge graph noise detection. This task aims to detect possible noises in knowledge graphs according to their triple scores."}, {"heading": "Evaluation Protocol", "text": "Inspired by the evaluation metric of triple classification in [Socher et al., 2013], we consider the energy function scores E(h, r, t) = ||h+r\u2212t|| as our triple scores, and then rank all triples in training set according to these scores. Those triples\nwith higher scores are first considered to be noises. We utilize precision/recall curves to show the performances."}, {"heading": "Experimental Results", "text": "Fig. 2 demonstrates the results of knowledge graph noise detection, from which we can observe that: (1) our confidenceaware KRL models achieve the best performances on all three datasets with different noise proportions. It confirms the capability of our CKRL models in modeling triple confidence and detecting noises and conflicts in knowledge graphs. (2) CKRL (LT+PP+AP) has significant and consistent improvements on noise detection compared to other confidence-aware strategies. It indicates that the adaptive path confidence could provide more flexible and credible evidence for noise detection. Moreover, CKRL (LT+PP+AP) achieves impressively 84 \u223c 94% in precision with different noise proportions when the recall is 10%, which implies that our models could truly help in real-world KG noise detection. (3) CKRL (LT+PP) performs better than CKRL (LT), which also indicates that the global path information could be a qualified supplement to local triple confidence.\nFor further comparisons, we also evaluate PTransE which considers multi-step paths in KRL on this task with its energy function, while the results are surprisingly much worse than TransE. We find that PTransE can not distinguish noises from positive triples well, for its path-based energy function scores only work in pair-wise comparisons. We don\u2019t show the results of PTransE due to the limited space."}, {"heading": "4.4 Knowledge Graph Completion", "text": "Knowledge graph completion is a classical evaluation task that concentrates on the quality of knowledge representations [Bordes et al., 2012]. This task aims to complete a triple when one of head, tail or relation is missing, which can be viewed as a simple question answering task."}, {"heading": "Evaluation Protocol", "text": "In this paper, we mainly focus on entity prediction, which is determined by the translation assumption that h+r ' t. Following the same settings in [Bordes et al., 2013], we conduct two measures as our evaluation metrics: (1) Mean Rank of correct entities, and (2) Hits@10 that indicates the proportion of correct answers ranked in top 10. We also follow the\ndifferent evaluation settings of \u201cRaw\u201d and \u201cFilter\u201d utilized in [Bordes et al., 2013]."}, {"heading": "Experimental Results", "text": "In Table 2 we demonstrate the results of entity prediction with different noise proportions. We can observe that: (1) all confidence-aware KRL models consistently and significantly outperform baseline on all noisy datasets with all evaluation metrics. It confirms the quality of learned knowledge representations, for they are not only able to detect noises in knowledge graphs, but also could perform well in knowledge graph completion. (2) Comparing with evaluation results between different datasets, we find that the improvements introduced by our confidence-aware methods become more significant as the noise proportion in KGs goes higher. It indicates that noises are harmful to entity prediction, and on the other hand reaffirms that considering triple confidence in knowledge representation learning is essential. (3) It seems that the global path confidence has few contributions on entity prediction. It may be because of the uncertainty and incompleteness in path information caused by noises and limited path selection. In parameter analysis, we find that a higher weight of path confidence within a reasonable range will improve the performances of entity prediction while harming those of KG noise detection. Taking longer and better paths into consideration will partially solve this problem."}, {"heading": "4.5 Triple Classification", "text": "Triple classification aims to predict whether a triple in test set is correct or not according to the dissimilarity function, which could be viewed as a binary classification task. Triple classification could also be regarded as a simpler knowledge graph noise detection task in test set, for the noises in training set will influence the construction of knowledge representations, while the negative triples generated in test set will not."}, {"heading": "Evaluation Protocol", "text": "Since there are no explicit negative triples in existing knowledge graphs, we construct negative triples in validation and test set following the same protocol in [Socher et al., 2013]. We also assure that the number of generated negative triples should be equal to that of positive triples. The classification is conducted as follows: we first learn different thresholds \u03b4r for each relation, which are optimized by maximizing the classification accuracies on validation set. In classification, if the energy function ||h+r\u2212t|| < \u03b4r, the triple will be classified to be positive, and otherwise to be negative."}, {"heading": "Experimental Results", "text": "Table 3 demonstrates the results of triple classification. We can observe that: (1) The CKRL models outperform baseline on all datasets, and the improvements become more significant with higher noise proportions. It confirms that learning knowledge representations with triple confidence could also help for triple classification. (2) The advantages confidenceaware models have over baseline in this task seem to be smaller than those in KG noise detection. It is because that the CKRL models concentrate more on the internal information of triples in training set, while the improvements in triple classification can only derive from better knowledge representations. Although CKRL models learn better knowledge representations, conventional models without confidence may achieve good results as well."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we propose a novel CKRL model which aims to detect noises in knowledge graphs and learn knowledge representations simultaneously. To make our models more flexible and universal, we only consider the internal structural information in KGs to define the local triple confidence and the global path confidence. Experimental results indicate that our CKRL framework can well capture both local and global structural information to measure triple confidences, which is essential when detecting noises in KGs and learning better knowledge representations.\nWe will explore the following research directions in future: (1) external information such like textual information could provide supplementary messages to judge triple confidence. We will explore to combine external heterogeneous information with internal structural information for better triple confidences. (2) We will investigate appropriate methods to combine our confidence-aware framework with knowledge construction, which could collect structural knowledge while detecting possible noises jointly."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": "The semantic web, pages 722\u2013735.", "citeRegEx": "Auer et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "Proceedings of KDD, pages 1247\u20131250,", "citeRegEx": "Bollacker et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "In Proceedings of AISTATS", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio. Joint learning of words", "meaning representations for open-text semantic parsing"], "venue": "pages 127\u2013135,", "citeRegEx": "Bordes et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of NIPS", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko. Translating embeddings for modeling multirelational data"], "venue": "pages 2787\u20132795,", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Knowledgebased systems", "author": ["Pasquale De Meo", "Emilio Ferrara", "Giacomo Fiumara", "Angela Ricciardello. A novel measure of edge centrality in social networks"], "venue": "30:136\u2013150,", "citeRegEx": "De Meo et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of VLDB", "author": ["Zolt\u00e1n Gy\u00f6ngyi", "Hector GarciaMolina", "Jan Pedersen. Combating web spam with trustrank"], "venue": "pages 576\u2013587,", "citeRegEx": "Gy\u00f6ngyi et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Towards vandalism detection in knowledge bases: Corpus construction and analysis", "author": ["Stefan Heindorf", "Martin Potthast", "Benno Stein", "Gregor Engels"], "venue": "Proceedings of SIGIR, pages 831\u2013834,", "citeRegEx": "Heindorf et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of CIKM", "author": ["Stefan Heindorf", "Martin Potthast", "Benno Stein", "Gregor Engels. Vandalism detection in wikidata"], "venue": "pages 327\u2013336,", "citeRegEx": "Heindorf et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Yago2: A spatially and temporally enhanced knowledge base from wikipedia", "author": ["Johannes Hoffart", "Fabian M Suchanek", "Klaus Berberich", "Gerhard Weikum"], "venue": "Artificial Intelligence, 194:28\u201361,", "citeRegEx": "Hoffart et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of ISWC", "author": ["Denis Krompa\u00df", "Stephan Baier", "Volker Tresp. Type-constrained representation learning in knowledge graphs"], "venue": "pages 640\u2013 655.", "citeRegEx": "Krompa\u00df et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Dbpedia\u2013a large-scale, multilingual knowledge base extracted from wikipedia", "author": ["Lehmann et al", "2015] Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick Van Kleef", "S\u00f6ren Auer"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "In Proceedings of EMNLP", "author": ["Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu. Modeling relation paths for representation learning of knowledge bases"], "venue": "pages 705\u2013714,", "citeRegEx": "Lin et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "Proceedings of AAAI,", "citeRegEx": "Lin et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "volume 1", "author": ["Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun. Neural relation extraction with selective attention over instances. In Proceedings of ACL"], "venue": "pages 2124\u20132133,", "citeRegEx": "Lin et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of IJCAI", "author": ["Michel Manago", "Yves Kodratoff. Noise", "knowledge acquisition"], "venue": "pages 348\u2013354,", "citeRegEx": "Manago and Kodratoff. 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "From freebase to wikidata: The great migration", "author": ["Thomas Pellissier Tanon", "Denny Vrande\u010di\u0107", "Sebastian Schaffert", "Thomas Steiner", "Lydia Pintscher"], "venue": "Proceedings of WWW, pages 1419\u20131428,", "citeRegEx": "Pellissier Tanon et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of NIPS", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of AAAI", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen. Knowledge graph embedding by translating on hyperplanes"], "venue": "pages 1112\u20131119,", "citeRegEx": "Wang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Representation learning of knowledge graphs with hierarchical types", "author": ["Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"], "venue": "Proceedings of IJCAI,", "citeRegEx": "Xie et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Physical Review E", "author": ["Tao Zhou", "Jie Ren", "Mat\u00fa\u0161 Medo", "YiCheng Zhang. Bipartite network projection", "personal recommendation"], "venue": "76(4):046115,", "citeRegEx": "Zhou et al.. 2007", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "There are existing amounts of widely-utilized large-scale knowledge graphs such as Freebase [Bollacker et al., 2008], DBpedia [Auer et al.", "startOffset": 92, "endOffset": 116}, {"referenceID": 0, "context": ", 2008], DBpedia [Auer et al., 2007] and other domain-specific KGs.", "startOffset": 17, "endOffset": 36}, {"referenceID": 13, "context": "For instance, the state-of-the-art relation extraction model on benchmark achieves only around 60% precision when the recall is 20% [Lin et al., 2016].", "startOffset": 132, "endOffset": 150}, {"referenceID": 7, "context": "Moreover, [Heindorf et al., 2016] focuses on vandalism detection in Wikidata, which also implies the existence and problems of noises in large-scale KGs.", "startOffset": 10, "endOffset": 33}, {"referenceID": 3, "context": "More specifically, the CKRL model follows the translationbased framework proposed by [Bordes et al., 2013], and learns knowledge representations with triple confidences.", "startOffset": 85, "endOffset": 106}, {"referenceID": 14, "context": "It seems to be inevitable that noises do exist and can strongly affect knowledge acquisition [Manago and Kodratoff, 1987], so that noise detection is essential in knowledge construction and knowledge application.", "startOffset": 93, "endOffset": 121}, {"referenceID": 8, "context": "For instance, YAGO2 extracts knowledge from Wikipedia with human supervision that human judges are presented with selected facts for which they have to assess the correctness [Hoffart et al., 2013].", "startOffset": 175, "endOffset": 197}, {"referenceID": 15, "context": "Wikidata also relies on a crowdsourced human curation software in which contributors can reject or approve a statement [Pellissier Tanon et al., 2016].", "startOffset": 119, "endOffset": 150}, {"referenceID": 6, "context": "As for automatical KG noise detection, a novel task named Wikidata vandalism, which aims to combat with deliberate destructions in knowledge graphs, has attracted wide attention [Heindorf et al., 2015].", "startOffset": 178, "endOffset": 201}, {"referenceID": 7, "context": "However, most existing methods on this task mainly concentrate on feature selection from contents, users, items and revisions [Heindorf et al., 2016], and thus are constrained with the completeness of external information.", "startOffset": 126, "endOffset": 149}, {"referenceID": 5, "context": "There are also some efforts working on judging importance on graphs for nodes [Gy\u00f6ngyi et al., 2004] or for edges [De Meo et al.", "startOffset": 78, "endOffset": 100}, {"referenceID": 4, "context": ", 2004] or for edges [De Meo et al., 2012], but few works concentrating on the confidence of each triple.", "startOffset": 21, "endOffset": 42}, {"referenceID": 3, "context": "TransE [Bordes et al., 2013] projects both entities and relations into a continuous low-dimensional vector space, interpreting relations as translating operations between head and tail entities.", "startOffset": 7, "endOffset": 28}, {"referenceID": 17, "context": "Some enhanced translation-based methods attempt to solve this problem with translations on relation-specific hyperplanes [Wang et al., 2014], relationspecific entity projection [Lin et al.", "startOffset": 121, "endOffset": 140}, {"referenceID": 12, "context": ", 2014], relationspecific entity projection [Lin et al., 2015b] and type-specific entity projection [Xie et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 18, "context": ", 2015b] and type-specific entity projection [Xie et al., 2016].", "startOffset": 45, "endOffset": 63}, {"referenceID": 11, "context": "[Lin et al., 2015a] extends TransE by encoding multistep relation path information into knowledge representation learning.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Following TransE [Bordes et al., 2013], we formalize a margin-based score function with negative sampling as objective for training.", "startOffset": 17, "endOffset": 38}, {"referenceID": 11, "context": "Specifically, we follow the path-constraint resource allocation (PCRA) [Lin et al., 2015a] to measure the relation path reliability.", "startOffset": 71, "endOffset": 90}, {"referenceID": 19, "context": "The key idea of PCRA is inspired by resource allocation [Zhou et al., 2007], which supposes there are certain resources associated with head entity h, and will flow throughout the whole knowledge graph via all relation paths.", "startOffset": 56, "endOffset": 75}, {"referenceID": 3, "context": "In this paper, we evaluate our confidence-aware models based on FB15K [Bordes et al., 2013], which is a typical benchmark knowledge graph extracted from Freebase [Bollacker et al.", "startOffset": 70, "endOffset": 91}, {"referenceID": 1, "context": ", 2013], which is a typical benchmark knowledge graph extracted from Freebase [Bollacker et al., 2008].", "startOffset": 78, "endOffset": 102}, {"referenceID": 16, "context": "Inspired by the preprocessing in the evaluation task named triple classification, we construct negative triples following the same setting in [Socher et al., 2013].", "startOffset": 142, "endOffset": 163}, {"referenceID": 9, "context": "We can directly utilize entity type information in Freebase or follow the local closed-world assumption [Krompa\u00df et al., 2015] to collect entity constraint information.", "startOffset": 104, "endOffset": 126}, {"referenceID": 3, "context": "We implement TransE [Bordes et al., 2013] as baseline since our CKRL framework is based on TransE, and it is not difficult for our confidence framework to be utilized in other enhanced translation-based methods.", "startOffset": 20, "endOffset": 41}, {"referenceID": 16, "context": "Inspired by the evaluation metric of triple classification in [Socher et al., 2013], we consider the energy function scores E(h, r, t) = ||h+r\u2212t|| as our triple scores, and then rank all triples in training set according to these scores.", "startOffset": 62, "endOffset": 83}, {"referenceID": 2, "context": "Knowledge graph completion is a classical evaluation task that concentrates on the quality of knowledge representations [Bordes et al., 2012].", "startOffset": 120, "endOffset": 141}, {"referenceID": 3, "context": "Following the same settings in [Bordes et al., 2013], we conduct two measures as our evaluation metrics: (1) Mean Rank of correct entities, and (2) Hits@10 that indicates the proportion of correct answers ranked in top 10.", "startOffset": 31, "endOffset": 52}, {"referenceID": 3, "context": "different evaluation settings of \u201cRaw\u201d and \u201cFilter\u201d utilized in [Bordes et al., 2013].", "startOffset": 64, "endOffset": 85}, {"referenceID": 16, "context": "Since there are no explicit negative triples in existing knowledge graphs, we construct negative triples in validation and test set following the same protocol in [Socher et al., 2013].", "startOffset": 163, "endOffset": 184}], "year": 2017, "abstractText": "Knowledge graphs (KGs) can provide significant relational information and have been widely utilized in various tasks. However, there may exist amounts of noises and conflicts in KGs, especially in those constructed automatically with less human supervision. To address this problem, we propose a novel confidence-aware knowledge representation learning framework (CKRL), which detects possible noises in KGs while learning knowledge representations with confidence simultaneously. Specifically, we introduce the triple confidence to conventional translation-based methods for knowledge representation learning. To make triple confidence more flexible and universal, we only utilize the internal structural information in KGs, and propose three kinds of triple confidences considering both local triple and global path information. We evaluate our models on knowledge graph noise detection, knowledge graph completion and triple classification. Experimental results demonstrate that our confidence-aware models achieve significant and consistent improvements on all tasks, which confirms the capability of our CKRL model in both noise detection and knowledge representation learning.", "creator": "LaTeX with hyperref package"}}}