{"id": "1705.03389", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Logical Parsing from Natural Language Based on a Neural Translation Model", "abstract": "Semantic parsing costanera has emerged as slov\u00e1cko a significant pomerol and powerful paradigm for tanque natural wifu language interface and one-quarter question jugoslav answering 2,155 systems. limbert Traditional methods maingot of building a gcc semantic parser demophon rely azanian on high - quality 6.0-percent lexicons, hand - crafted grammars myriam and linguistic features which garriott are popularising limited st.giga by applied domain or pressmen representation. In this paper, we bejo propose a general approach to nogues learn allanon from denotations kitti based d'onofrio on parda Seq2Seq model 1577 augmented episcopalian with esnaider attention merrells mechanism. rauzan We encode input haeusl sequence into vectors beekeepers and use dynamic programming osboa to marshalling infer overpromised candidate brachytherapy logical yongin forms. decrepit We i20 utilize the fact that 2:55 similar belova utterances kandor should school-based have similar logical meadowbank forms to gachsaran help handke reduce assaulted the searching larc space. auglaize Under landel our euroyen learning policy, chowdhary the kaestner Seq2Seq wastebasket model licitra can .594 learn viejos mappings pierre-fran\u00e7ois gradually with modernizers noises. Curriculum 120.69 learning godal is adopted to make tough-minded the juul learning smoother. 7-story We test 521st our method on the scriptx arithmetic loftiest domain which artesian shows our model can successfully goertzel infer neuk the scratchcard correct coxed logical 240 forms khaliq and learn seitaridis the sarona word vytegra meanings, set-ups compositionality predation and operation s-70c orders superstars simultaneously.", "histories": [["v1", "Tue, 9 May 2017 15:35:25 GMT  (250kb)", "http://arxiv.org/abs/1705.03389v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["liang li", "pengyu li", "yifan liu", "tao wan", "zengchang qin"], "accepted": false, "id": "1705.03389"}, "pdf": {"name": "1705.03389.pdf", "metadata": {"source": "CRF", "title": "Logical Parsing from Natural Language Based on a Neural Translation Model", "authors": ["Liang Li", "Pengyu Li", "Yifan Liu", "Tao Wan", "Zengchang Qin"], "emails": ["zcqin}@buaa.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n03 38\n9v 1\n[ cs\n.C L\n] 9\nM ay\n2 01\nIndex Terms\u2014Seq2Seq; Semantic parsing; Weak supervision\nI. INTRODUCTION\nThe problem of learning a semantic parser has been receiving significant attention. Semantic parsers map natural language into a logical form that can be executed on a knowledge base and return an answer (denotation). The early works use logical forms as supervision [1]\u2013[4], given a set of input sentences and their corresponding logical forms, learning a statistical semantic parser by weighting a set of rules mapping lexical items and syntactic patterns to their logical forms. Given an input, these rules are applied recursively to derive the most probable logical form. However, the tremendous labor needed for annotating logical forms has turned the trend to weak supervision \u2212 using denotation of logical forms as the training target. It has been successfully applied in different fields including question-answering [5]\u2013[8] and robot navigation [9]. All these works need hand-crafted grammars that are crucial in semantic parsing but pose an obstacle for generalization. Wang et al. [10] build semantic parsers in 7 different domains and hand engineer a separate grammar for each domain.\nThe rise of Seq2Seq model [11] provides an alternative method to tackle the mapping problem and no more manual grammars are needed. The ability to deal with sequences with changeable length as input and/or output has translated this model into applications including machine translation [11],\n[12], syntactic parsing [13], and question answering [14]. All of these work do not need hand-crafted grammars and are socalled end-to-end learning. But they do not resolve the problem of supervision by denotation, which makes one step further and needs logic reasoning and operation. Our model adopts the encoder-decoder framework and tries to use denotation as the target of supervised learning. We take the advantage of the Seq2Seq model\u2019s ability of tackling input/output with different length and grammar-free form to learn the mappings from natural language to logical forms. Our main focus is to infer logical forms from denotations in a generalizable way. We wish to add minimal extra constraints or manual features, so that it can be applied to other domains. For now, we can not infer correct logical forms all the time but we prove that the Seq2Seq model is capable of learning with noises in training data and the curriculum learning form mitigates this effect.\nA problem of weak supervision is the search of the consistent logical forms when only denotation is available. The number of logical forms grows exponentially as their size increases and inferring from denotations inevitably induces spurious logical forms \u2212 those that do not represent the original sentence semantics but get the correct answer accidently [15]. To control the searching space, previous works relied on restricted sets of rules which limits expressivity and are possible to rule out the correct logical form. Instead, we utilize dynamic programming to construct the candidate logical form set with an extra base case set to filter all the candidates, and use the ongoing training Seq2Seq model to determine the best one. In this way, we can maintain the expressivity and train our model iteratively by feeding the suggested best logical form back to our model. Consequently, a right pick results in a positive learning, which allows our model to put a higher probability on the correct logical form over others and leads to the desired mapping over training.\nWe evaluate our model on arithmetic domain through a toy example. And the model is capable of translating row sentences into mathematical equations in structured tree form and returning the answer directly. We provide a base case set which serves as a pivot for the model to learn.Our model learns the arithmetic calculation in a curriculum way, where simpler sentences with fewer words are inputted at initial state. Our model assumes no prior linguistic knowledge and learns\nthe meaning of all the words, compositionality, and order of operations from just the natural language \u2212 denotation pairs."}, {"heading": "A. Related work", "text": "We adopt the general encoder-decoder framework based on neural networks augmented with attention mechanism [16], which allows the model to learn soft alignment between utterances and logical forms. Our work is related to [17] and [13], both of which use the Seq2Seq model to map natural language to logical forms in tree structure without handengineered features. But our work makes one step further by using denotation of logical form as the learning target and regard logical forms as latent variables.\nHow to reduce the searching space is a chief challenge in weak supervision. A common approach is to constrain the set of possible logical form compositions, which can significantly reduce the searching space but also constrain the expressivity [18]. Lao et al. [19] use random walks to generate logical forms and use denotation to cut down the searching space during learning. Liang et al. [20] adopt a similar method to narrow down the options and allow more complex semantics to be composed. Different from generating logical forms forwardly as mentioned above, an alternative method is to use denotation to infer logical forms using dynamic programming [15], [21]. In this way, it is more likely to recover the full set and find the desired one. Inspired by their work, we also employ this method and store the denotation \u2212 logical form pairs in advance to accelerate the lookup efficiency.\nOur work is similar to [22] in the sense that we both focus on the arithmetic domain and learn from denotations directly. But their work relies on hand-crafted grammars to construct logical forms and hand engineered features to filter out incorrect logical forms. Instead, the Seq2Seq model we use is grammar-free and the features we select to screen logical forms is more general. Our main idea is that similar utterance should have similar logical forms, so that we can filter out incorrect logical forms by using similarity measurement. We build up a small base case set to assist this idea and the similarity function we adopt is simply bag-of-words, which is replaceable when extending to other fields. Furthermore, to sort the candidates, we do not have a particular scoring function to weight extracted features, instead, we use the ongoing training Seq2Seq model to evaluate their loss.\nThere are some other related work, Neural Programmer [23] augmented with a small set of arithmetic and logic operations is able to perform complex reasoning and has shown success in question answering [24]. Neural Turing Machines [25] can infer simple algorithms such as copying and sorting with external memory."}, {"heading": "II. BACKGROUND:SEQUENCE-TO-SEQUENCE MODEL AND ATTENTION MECHANISM", "text": "Before introducing our model, we describe briefly the\nSeq2Seq model and attention mechanism."}, {"heading": "A. Sequence-to-sequence model", "text": "The Seq2Seq model takes a source sequence X = (x1, x2, ..., xT ) as input and outputs a translated sequence Y = (y1, y2, ..., yT \u2032). The model maximizes the generation probability of Y conditioned on X : p(y1, ..., yT \u2032 |x1, x2, ..., xT ). Specifically, the Seq2Seq is in an encoder-decoder structure. In this framework, an encoder reads the input sequence word by word into a vector c through recurrent neural network (RNN).\nht = f(xt, ht\u22121) (1)\nand\nc = q(h1, ..., hT ),\nwhere ht is the hidden state at time t, c is commonly taken directly from the last hidden state of encoder q(h1, ..., hT ) = hT , and f is a non-linear transformation which can be either a long-short term memory unit (LSTM) [26] or a gated recurrent unit (GRU) [27]. In this paper, LSTM is adopted and is parameterized as \n   it ft ot c\u0303t\n\n   =\n\n  \n\u03c3 \u03c3 \u03c3\ntanh\n\n   T\n(\nxt ht\u22121\n)\n(2a)\nct = ft \u25e6 ct\u22121 + it \u25e6 c\u0303t (2b)\nht = ot \u25e6 tanh(ct) (2c)\nwhere \u25e6 is an element-wise multiplication, T is an affine transformation, \u03c3 is the logistic sigmoid that restricts its input to [0,1], it, ft and ot are the input, forget, and output gates of the LSTM, and ct is the memory cell activation vector. The forget and input gates enable the LSTM to regulate the extent to which it forgets its previous memory and the input, while the output gate controls the degree to which the memory affects the hidden state. The encoder employs bidirectionality, encoding the sentences in both the forward and backward directions, an approach adopted in machine translation [12], [16]. In this way, the hidden annotations ht = ( \u2212\u2192 h Tt ; \u2190\u2212 h Tt ) concatenate forward \u2212\u2192 h Tt and backward annotations \u2190\u2212 h Tt together, each determined using Equation 2c.\nThe decoder is trained to estimate generation probability of next word yt given all the previous predicted words and the context vector c. The objective function of Seq2Seq can be written as\np(y1, ..., yT \u2032 |x1, ..., xT ) =\nT \u2032 \u220f\nt=1\np(yt|c, y1, ..., yt\u22121). (3)\nWith an RNN, each conditional probability is modeled as\np(yt|y1, ..., yt\u22121, c) = g(yt\u22121, st, c), (4)\nwhere g is a non-linear function that outputs the probability of yt, yt\u22121 is the predicted word at time t\u2212 1 in the response sequence, and st is the hidden state of the decoder RNN at time t, which can be computed as\nst = f(yt\u22121, st\u22121, c). (5)"}, {"heading": "B. Attention mechanism", "text": "The traditional Seq2Seq model predicts each word from the same context vector c, which deprives the source sequence information and makes the alignment imprecisely. To address this problem, attention mechanism is introduced to allow decoder focusing on the source sequence instead of a compressed vector upon prediction [16]. In Seq2Seq with attention mechanism, each yi in Y corresponds to a context vector ci instead of c. The conditional probability in Equation 4 becomes\np(yi|y1, ..., yi\u22121, x) = g(yi\u22121, si, ci), (6)\nwhere the hidden state si is computed by\nsi = f(yi\u22121, si\u22121, ci). (7)\nThe context vector ci is a weighted average of all hidden states {ht} T t=1 of the encoder, defined as\nci =\nT \u2211\nj=1\n\u03b1ijhj , (8)\nwhere the weight \u03b1ij is given by\n\u03b1ij = exp(eij)\n\u2211T k=1 exp(eik) , (9)\nwhere eij is an alignment model which scores how well the inputs around position j and the output at position i match,\neij = a(si\u22121, hj), (10)\nwhere a is a feed forward neural network, trained jointly with other components of the system."}, {"heading": "III. LEARNING FROM DENOTATIONS", "text": "We use the triple \u3008u, s, d\u3009 to denote the linguistic objects, where u is an utterance, s is a logical form and d is the denotation of s. We use \u230au\u230b to represent the translation of utterance into its logical form, and we use [[s]] for the denotation of logical form s. Each training data is composed by the pair \u3008u, d\u3009 without explicitly telling its correct logical form s.\nWith denotation as target label of learning, the Seq2Seq model is trained to put a high probability on \u230au\u230b\u2019s that are consistent-logical forms that execute to the correct denotation d. When the space of logical forms is large, searching for the correct logical form could be cumbersome. Additionally,\ndifferent from the previous study which incorporates prior knowledge such as word embedding [20], object categories [9], our model in this mathematical expression learning example has no such knowledge and has to learn the meanings of input utterance.\nHere we formally describe the methods to reduce the searching space and how to infer the correct logical forms from denotations."}, {"heading": "A. Dynamic programming on denotations", "text": "Our first step is to generate all logical forms that have the correct denotations. Formally, given a denotation d, we wish to generate a candidate logical form set that satisfy the denotation demand \u2126 = {s|[[s]] = d}. Previous work use beam search to generate candidates but it is hard to recover the full set \u2126 due to pruning. Noticing that one denotation may correspond to multiple logical forms, which leads to the increase of the number of distinct denotations is much slower than the number of logical forms. We use dynamic programming on denotations to recover the full set, following the work of [15], [21]. A necessary condition for dynamic programming to work is denotationally invariant semantic function g, such that the denotation of the resulting logical form g(s1, s2) only depends on the denotations of s1 and s2. In the arithmetic domain, the result of an equation can be computed recursively and independently which certainly satisfies this requirement.\nOur primary purpose is to collapse logical forms with the same denotation together, so that given a denotation d the candidate set \u2126 (Figure 1. (a)) can be returned directly. In order to speed up the lookup efficiency, we store the pair (d,\u2126) in advance."}, {"heading": "B. Filter candidate logical forms", "text": "In order to filter out incorrect logical forms, we utilize the fact that similar utterances should have similar logical forms to reduce the searching space. We build up a base case set B (Table I) that stores several \u3008ub, sb\u3009 pairs with varying utterance length. Specifically, given an input pair \u3008ui, di\u3009, we iterate the base case set to find the one which shares the most similarity with input utterance ui.\nu\u0303b = argmin ub\u2208B e(\u03c6(ub), \u03c6(ui)), (11)\nwhere \u03c6 extracts features from utterance ub and ui, here we use bag-of-words as feature extraction function, and we\nsimply counts the shared features as the feature similarity measurement function e.\nAfter finding the base case utterance u\u0303b which is closest to the input utterance ui, we use the corresponding base case logical form s\u0303b to filter the candidate set \u2126.\n\u0393 = {s|s = argmax sj\u2208\u2126\ne(\u03c6(u\u0303b), \u03c6(ui)) e(\u03c6(s\u0303b), \u03c6(sj)) }. (12)\nWe use a new set \u0393 (Figure 1. (b)) to store these updated candidate logical forms that have similar features with the base case \u3008u\u0303b, s\u0303b\u3009. To further determine the most probable one from set \u0393, we use the Seq2Seq model to examine the loss of each candidate and select the one with least loss to return for training.\ns\u0303i = argmax s\u2208\u0393 pSeq2Seq(s) (13)\nThe selected logical form s\u0303i is paired with its utterance to form a training example \u3008ui, s\u0303i\u3009 which feeds back to the Seq2Seq model for training (Figure 1. (c) - (d))."}, {"heading": "IV. EXPERIMENTAL STUDIES", "text": ""}, {"heading": "A. Dataset", "text": "For the experiments, we randomly generate 8000 utterances with varying length from 3 to 7, split into a training set of 6000 training examples and 2000 test examples. Each utterance consists of integers from one to five and four operators \u2018plus\u2019, \u2018minus\u2019, \u2018times\u2019, \u2018divide\u2019. Every utterance represents a legal arithmetic expression. Even the scope is quite limited, the searching space for an equation with length equal to seven is still numerous: 54 \u2217 43 = 4 \u2217 104, which is caused by the rich compositionality of arithmetic equations. Besides, this nature gives rise to one denotation can correspond to much\nmore logical forms compared with other domains, resulting in increasing noises for inference.\nWe select two ways to represent logical forms, both of them are represented by Arabic numerals and executable operators, but one is inserted with brackets to denote the calculation order, linearized from tree structure, the other assumes knowing calculation order without brackets for denotation. The base case set consists of seven samples, the brackets are omitted directly when considering logical forms with calculation knowledge (Table I).\nThe involvement of brackets largely increases the meaning of our logical form, for the reason that it actually represents tree structure with logic reasoning. Besides, not only has the model to learn soft alignment for brackets which are not introduced in the utterance explicitly, but it has to learn the brackets matching relationship."}, {"heading": "B. Settings", "text": "For the convenience of data preprocessing and vectorization, each input sentence is appended an ending mark \u3008eos\u3009, noting the end of input. Also, we manually set a maximum length for the input and output sentence, where the blank position will be automatically filled by \u2018PAD\u2019 mark.\nWe construct 3 layers of LSTM on both encoder and decoder side with 20 hidden units for each layer. An embedding and a softmax layer is inserted as the first and last layer. Dropout is used for regularizing the model with a constant rate 0.3. Dropout operators are used between different LSTM layers and for the hidden layers before the softmax classifier. This technique can significantly reduce overfitting, especially on datasets of small size. Dimensions of hidden vector and word embedding are set to 20.\nWe use the RMSProp algorithm to update parameters with learning rate 0.001 and smoothing constant 0.9. Parameters are randomly initialized from a uniform distribution U(\u22120.05, 0.05). We run the model for 200 epochs."}, {"heading": "C. Results and analysis", "text": "We report the results with the Seq2Seq model on two variants, i.e., with brackets noting calculation order and without brackets as logical forms. To compare the performance of weak supervision, we also test the performance of traditional training with gold standard logical form. The result is reported on the accuracy of denotation \u2212 the portion of input sentences are translated into the correct denotation. Table II presents the comparison.\nOverall, the accuracy with logical forms as supervision is much higher than with denotations, for the reason that training by gold logical forms does not bring any noises. On the other hand, the spurious logical forms affect our model\u2019s performance unavoidably. For instance, an utterance \u201cFive plus three times four\u201d can be mistakenly translated into [5+(4\u22173)], which has the correct denotation. This is reasonable because we adopt bag-of-words to measure similarity and this method does not take the order into consideration. Besides, not all of the training logical forms returned by our inference process are correct, but at least they have the correct denotation. So it is noticeable that even only approximately 45% logical forms (Figure 2) returned for training are correct, the accuracy on denotation of our model is much higher than this limit.\nTo find the best logical form for training, the final pick decision is made by the ongoing training Seq2Seq model, this training-by-prediction policy makes the model difficult to converge (Figure 2). But we can see this policy is effective to correct its previous prediction, which we call the ability of self-correction. This proves Seq2Seq model can learn word meanings and compositionality with noises.\nIn addition, the performance of training on logical forms with brackets is inferior to the model without brackets, which makes sense that longer sequence add difficulty for learning and brackets introduce more complicated mapping relationships. However, by utilizing soft alignment, the model can learn tree structure successfully with only denotation as supervision."}, {"heading": "V. CONCLUSION", "text": "In this paper, we present an encoder-decoder neural network model for mapping natural language to their meaning representations with only denotation as supervision. We use dynamic programming to infer logical forms from denotations, and utilize similarity measurement to reduce the searching\nspace. Also, curriculum learning strategy is adopted to smooth and accelerate the learning process. Under the policy trainingby-predictions, our model has the ability of self-correction. We apply our model to the arithmetic domain and experimental results show that this model can learn word meanings and compositionality without resources to domain- or representationspecific features. One major problem remained in our work is that the model may confuse the order of predictions, which is caused by the inherent weakness of bag-of-words similarity measurement. This could be enhanced by some sequencebased similarity measurement in future work.\nAlthough the example we test is rather simple, the expansibility to other fields and application scenarios is promising due to the few hand-engineered features and its capability of learning structured form. It would be interesting to learn a question-answering model with only questionanswer pairs, or apply it to robot navigation task. We expect to extend our model to these fields and continue to enrich it."}], "references": [{"title": "A fully statistical approach to natural language interfaces", "author": ["S. Miller", "D. Stallard", "R. Bobrow", "R. Schwartz"], "venue": "Proceedings of the 34th annual meeting on Association for Computational Linguistics, 2002, pp. 55\u201361.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "A statistical semantic parser that integrates syntax and semantics", "author": ["R. Ge", "R.J. Mooney"], "venue": "Conference on Computational Natural Language Learning, 2005, pp. 9\u201316.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "A generative model for parsing natural language to meaning representations", "author": ["W. Lu", "H.T. Ng", "W.S. Lee", "L.S. Zettlemoyer"], "venue": "Conference on Empirical Methods in Natural Language Processing, EMNLP 2008, Proceedings of the Conference, 25-27 October 2008, Honolulu, Hawaii, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL, 2008, pp. 783\u2013792.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to map sentences to logical form", "author": ["L. Zettlemoyer"], "venue": "Eprint Arxiv, pp. 658\u2013666, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["J. Clarke", "D. Goldwasser", "M.W. Chang", "D. Roth"], "venue": "Fourteenth Conference on Computational Natural Language Learning, 2010, pp. 18\u201327.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Meeting of the Association for Computational Linguistics: Human Language Technologies, 2011, pp. 590\u2013599.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Weakly supervised training of semantic parsers", "author": ["J. Krishnamurthy", "T.M. Mitchell"], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 2012, pp. 754\u2013765.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale semantic parsing without question-answer pairs", "author": ["S. Reddy", "M. Lapata", "M. Steedman"], "venue": "Transactions of the Association for Computational Linguistics, vol. 2, pp. 377\u2013392, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L. Zettlemoyer"], "venue": "Transactions of the Association for Computational Linguistics, vol. 1, pp. 49\u201362, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Building a semantic parser overnight", "author": ["Y. Wang", "J. Berant", "P. Liang"], "venue": "Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, 2015, pp. 1332\u20131342.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, vol. 4, pp. 3104\u20133112, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B.V. Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Computer Science, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "Eprint Arxiv, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Ko\u010disk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "Computer Science, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring logical forms from denotations", "author": ["P. Pasupat", "P. Liang"], "venue": "arXiv preprint arXiv:1606.06900, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Computer Science, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Language to logical form with neural attention", "author": ["L. Dong", "M. Lapata"], "venue": "Meeting of the Association for Computational Linguistics, 2016, pp. 33\u201343.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Compositional semantic parsing on semistructured tables", "author": ["P. Pasupat", "P. Liang"], "venue": "arXiv preprint arXiv:1508.00305, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["N. Lao", "T. Mitchell", "W.W. Cohen"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2011, pp. 529\u2013539.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision", "author": ["C. Liang", "J. Berant", "Q. Le", "K.D. Forbus", "N. Lao"], "venue": "2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Programming by demonstration using version space algebra", "author": ["T. Lau", "S.A. Wolfman", "P. Domingos", "D.S. Weld"], "venue": "Machine Learning, vol. 53, no. 1, pp. 111\u2013156, 2003.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Bringing machine learning and compositional semantics together", "author": ["P. Liang", "C. Potts"], "venue": "Annual Review of Linguistics, vol. 1, no. 1, pp. 355\u2013376, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever"], "venue": "Computer Science, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a natural language interface with neural programmer", "author": ["A. Neelakantan", "Q.V. Le", "M. Abadi", "A. Mccallum", "D. Amodei"], "venue": "2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "Computer Science, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory.", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B.V. Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "Computer Science, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The early works use logical forms as supervision [1]\u2013[4], given a set of input sentences and their corresponding logical forms, learning a statistical semantic parser by weighting a set of rules mapping lexical items and syntactic patterns to their logical forms.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "The early works use logical forms as supervision [1]\u2013[4], given a set of input sentences and their corresponding logical forms, learning a statistical semantic parser by weighting a set of rules mapping lexical items and syntactic patterns to their logical forms.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "It has been successfully applied in different fields including question-answering [5]\u2013[8] and robot navigation [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "It has been successfully applied in different fields including question-answering [5]\u2013[8] and robot navigation [9].", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "It has been successfully applied in different fields including question-answering [5]\u2013[8] and robot navigation [9].", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "[10] build semantic parsers in 7 different domains and hand engineer a separate grammar for each domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The rise of Seq2Seq model [11] provides an alternative method to tackle the mapping problem and no more manual grammars are needed.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "The ability to deal with sequences with changeable length as input and/or output has translated this model into applications including machine translation [11], [12], syntactic parsing [13], and question answering [14].", "startOffset": 155, "endOffset": 159}, {"referenceID": 11, "context": "The ability to deal with sequences with changeable length as input and/or output has translated this model into applications including machine translation [11], [12], syntactic parsing [13], and question answering [14].", "startOffset": 161, "endOffset": 165}, {"referenceID": 12, "context": "The ability to deal with sequences with changeable length as input and/or output has translated this model into applications including machine translation [11], [12], syntactic parsing [13], and question answering [14].", "startOffset": 185, "endOffset": 189}, {"referenceID": 13, "context": "The ability to deal with sequences with changeable length as input and/or output has translated this model into applications including machine translation [11], [12], syntactic parsing [13], and question answering [14].", "startOffset": 214, "endOffset": 218}, {"referenceID": 14, "context": "The number of logical forms grows exponentially as their size increases and inferring from denotations inevitably induces spurious logical forms \u2212 those that do not represent the original sentence semantics but get the correct answer accidently [15].", "startOffset": 245, "endOffset": 249}, {"referenceID": 15, "context": "We adopt the general encoder-decoder framework based on neural networks augmented with attention mechanism [16], which allows the model to learn soft alignment between utterances and logical forms.", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "Our work is related to [17] and [13], both of which use the Seq2Seq model to map natural language to logical forms in tree structure without handengineered features.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "Our work is related to [17] and [13], both of which use the Seq2Seq model to map natural language to logical forms in tree structure without handengineered features.", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": "A common approach is to constrain the set of possible logical form compositions, which can significantly reduce the searching space but also constrain the expressivity [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 18, "context": "[19] use random walks to generate logical forms and use denotation to cut down the searching space during learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] adopt a similar method to narrow down the options and allow more complex semantics to be composed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Different from generating logical forms forwardly as mentioned above, an alternative method is to use denotation to infer logical forms using dynamic programming [15], [21].", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "Different from generating logical forms forwardly as mentioned above, an alternative method is to use denotation to infer logical forms using dynamic programming [15], [21].", "startOffset": 168, "endOffset": 172}, {"referenceID": 21, "context": "Our work is similar to [22] in the sense that we both focus on the arithmetic domain and learn from denotations directly.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "There are some other related work, Neural Programmer [23] augmented with a small set of arithmetic and logic operations is able to perform complex reasoning and has shown success in question answering [24].", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "There are some other related work, Neural Programmer [23] augmented with a small set of arithmetic and logic operations is able to perform complex reasoning and has shown success in question answering [24].", "startOffset": 201, "endOffset": 205}, {"referenceID": 24, "context": "Neural Turing Machines [25] can infer simple algorithms such as copying and sorting with external memory.", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": ", hT ) = hT , and f is a non-linear transformation which can be either a long-short term memory unit (LSTM) [26] or a gated recurrent unit (GRU) [27].", "startOffset": 108, "endOffset": 112}, {"referenceID": 26, "context": ", hT ) = hT , and f is a non-linear transformation which can be either a long-short term memory unit (LSTM) [26] or a gated recurrent unit (GRU) [27].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "where \u25e6 is an element-wise multiplication, T is an affine transformation, \u03c3 is the logistic sigmoid that restricts its input to [0,1], it, ft and ot are the input, forget, and output gates of the LSTM, and ct is the memory cell activation vector.", "startOffset": 128, "endOffset": 133}, {"referenceID": 11, "context": "The encoder employs bidirectionality, encoding the sentences in both the forward and backward directions, an approach adopted in machine translation [12], [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "The encoder employs bidirectionality, encoding the sentences in both the forward and backward directions, an approach adopted in machine translation [12], [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "To address this problem, attention mechanism is introduced to allow decoder focusing on the source sequence instead of a compressed vector upon prediction [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 19, "context": "Additionally, different from the previous study which incorporates prior knowledge such as word embedding [20], object categories [9], our model in this mathematical expression learning example has no such knowledge and has to learn the meanings of input utterance.", "startOffset": 106, "endOffset": 110}, {"referenceID": 8, "context": "Additionally, different from the previous study which incorporates prior knowledge such as word embedding [20], object categories [9], our model in this mathematical expression learning example has no such knowledge and has to learn the meanings of input utterance.", "startOffset": 130, "endOffset": 133}, {"referenceID": 14, "context": "We use dynamic programming on denotations to recover the full set, following the work of [15], [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "We use dynamic programming on denotations to recover the full set, following the work of [15], [21].", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "Semantic parsing has emerged as a significant and powerful paradigm for natural language interface and question answering systems. Traditional methods of building a semantic parser rely on high-quality lexicons, hand-crafted grammars and linguistic features which are limited by applied domain or representation. In this paper, we propose a general approach to learn from denotations based on Seq2Seq model augmented with attention mechanism. We encode input sequence into vectors and use dynamic programming to infer candidate logical forms. We utilize the fact that similar utterances should have similar logical forms to help reduce the searching space. Under our learning policy, the Seq2Seq model can learn mappings gradually with noises. Curriculum learning is adopted to make the learning smoother. We test our method on the arithmetic domain which shows our model can successfully infer the correct logical forms and learn the word meanings, compositionality and operation orders simultaneously.", "creator": "LaTeX with hyperref package"}}}