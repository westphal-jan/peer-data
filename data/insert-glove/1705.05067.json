{"id": "1705.05067", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Online Learning Via Regularized Frequent Directions", "abstract": "Online davari Newton 1938-45 step algorithms usually niari achieve good performance with manvantara less superfight training powershift samples memorize than first order methods, seeland but trickster require bogatynia higher space pennington and time 9,600 complexity sampoerna in thur. each iteration. endosperm In blackcurrant this preetz paper, we anodyne develop cse a burra new sketching strategy window called continuous-time regularized frequent direction (RFD) abouts to crystallization improve the elma performance of cavallaro online Newton algorithms. laoretti Unlike the biakabutuka standard frequent thunderstreaks direction (FD) echoic which integrale only maintains buryj a sketching draper matrix, eulimidae the RFD p\u00e5 introduces dehaven a regularization term commented additionally. shabestar The meinig regularization rebounds provides erythronium an euro410 adaptive stepsize baksi for update, looping which passaro makes vsetin the didja algorithm heartbreakers more parathion stable. The rzeznik RFD also reduces the approximation error of crohn FD with almost crewe the accompli same cailloux cost leheny and vizsla makes knock the phony online learning liley more robust gebhardt to hyperparameters. Empirical postrema studies demonstrate ghosn that our approach 12:15 outperforms culpa sate - of - the - atienza art successors second order 2,431 online galleasses learning algorithms.", "histories": [["v1", "Mon, 15 May 2017 04:18:29 GMT  (314kb,D)", "http://arxiv.org/abs/1705.05067v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["luo luo", "cheng chen", "zhihua zhang", "wu-jun li"], "accepted": false, "id": "1705.05067"}, "pdf": {"name": "1705.05067.pdf", "metadata": {"source": "CRF", "title": "Online Learning Via Regularized Frequent Directions", "authors": ["Luo Luo", "Cheng Chen", "Zhihua Zhang", "Wu-Jun Li"], "emails": ["zhzhang@math.pku.edu.cn", "liwujun@nju.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "Online learning is a typical approach for making an algorithm scalable, which constructs a learner incrementally from a sequence of examples. The online Newton step algorithm is the online analogue of the Newton-Raphson method [1, 2, 3], which incorporates the gradient information in earlier iterations. Compared with first order methods, the online Newton step algorithm has logarithmical regret bound without requirement of strongly convex assumption on loss function. Also, it needs less iterations than first order methods for the same prediction error. However, the online Newton step algorithm require quadratical space and time with respect to the number of dimensions, which is expensive for large scale problems.\nIn a recent study, Luo et al. [4] proposed sketched online Newton (SON) algorithms to accelerate second order online learning methods. The authors discussed several sketching strategies [5]\nar X\niv :1\n70 5.\n05 06\n7v 1\nfor approximating the second order information, including random projection [6, 7, 8], frequent direction [9, 10] and Oja\u2019s algorithm [11, 12]. The SON achieves regret bounds nearly as good as the standard online Newton algorithms and performs well in real applications.\nIn this paper, we improve the SON methods with a novel sketching methods that we call regularized frequent directions (RFD). The RFD is a variant of frequent direction (FD) [9, 10], but reduces half of the approximation error bound with almost the same computational cost. Rather than approximating the matrix with a low-rank structure, RFD introduces an additional regularization term. Note that Zhang [13] proposed matrix ridge approximation (MRA) to a positive semi-definite matrix by the similar idea with RFD. However, there are two main differences between RFD and MRA. First, RFD is designed to the case that data samples come sequentially, while MRA relies on the whole dataset. Hence RFD is more suitable to online learning. Second, MRA aims to minimize the approximation error with respect to the Frobenius norm while RFD tries to minimize the spectral norm of the approximation error. In general, the spectral norm error bounds is more meaningful than the Frobenius norm [14].\nIn contrast to existing online Newton algorithms which usually use a fixed regularization term for each update, RFD provides an adaptive way to compute this term. This strategy achieves comparable regret bound with existing sketched online Newton methods, but makes the algorithm less sensitive to the hyperparameter.\nThe remainder of the paper is organized as follows. Firstly, we review the background of second order online learning and its sketched variants. Then we propose our regularized frequent direction (RFD) method with applications in online learning and provide some related theoretical analysis. Finally, we demonstrate empirical comparisons with baselines on serval real-world datasets to show the superiority of our algorithms."}, {"heading": "2. Online Learning by Sketching", "text": "In this section, we first describe the setup of convex online learning and some classical algorithms. Then we introduce the connection between online learning and sketching second order methods."}, {"heading": "2.1 Convex Online Learning", "text": "Online learning is performed in a sequence of consecutive rounds [15]. We consider the problem of online optimization as follows. For a sequence of examples {x(t) \u2208 Rd}, and convex smooth loss functions {ft : Kt \u2192 R} where the ft(w) , `t(w>x(t)) and the Kt \u2282 Rd are convex compact set, the learner must decide a predictor w(t) and suffers loss ft(w(t)) for the t-th round. The regret at round T is then defined as:\nRT (w \u2217) = T\u2211 t=1 ft(w (t))\u2212 T\u2211 t=1 ft(w \u2217),\nwhere w\u2217 = argminw\u2208K \u2211T t=1 ft(w) and K = \u22c2T t=1Kt.\nWe make the following assumptions on the loss functions [4].\nAssumption 1 The loss function `t satisfies |`\u2032t(z)| \u2264 L whenever |z| \u2264 C, where L and C are positive constant scalars.\nAssumption 2 There exists a \u00b5t \u2265 0 such that for all u,w \u2208 K, we have\nft(w) \u2265 ft(u) +\u2207ft(u)>(w \u2212 u) + \u00b5t 2\n( \u2207ft(u)>(w \u2212 u) )2 .\nNote that for bounded diameter of the domain and gradient, holding Assumption 2 only requires the exp-concave property of ft, which is more general than strong convexity [3].\nOne typical online learning algorithm is online gradient descent (OGD) [2, 16]. At round t+ 1, OGD exploits the following update rule:\nu(t+1) = w(t) \u2212 \u03b2tg(t), w(t+1) = argmin\nw\u2208Kt \u2016w \u2212 u(t+1)\u2016,\nwhere g(t) = \u2207ft(w(t)) and \u03b2t is the stepsize. The algorithm has linear computation cost and achieves O(L2H log T ) regret bound for the H-strongly convex loss.\nIn this paper, we are more interested in online Newton step algorithms [2, 4]. The notation H-norm for given positive definite matrix H \u2208 Rd\u00d7d and vector z \u2208 Rd is defined as \u2016z\u2016H =\u221a\nz>Hz. The standard online Newton step keeps the curvature information in the matrix H(t) \u2208 Rd\u00d7d sequentially and iterates as follows:\nu(t+1) = w(t) \u2212 \u03b2t(H(t))\u22121g(t), w(t+1) = argmin\nw\u2208Kt \u2016w \u2212 u(t+1)\u2016H(t) . (1)\nThe matrix H(t) is constructed by the outer product of historical gradients [4, 17]:\nH(t) = \u03b10I + t\u2211 i=1 g(i)(g(i))>, or H(t) = \u03b10I + t\u2211 i=1 (\u00b5t + \u03b7t)g (i)(g(i))>, (2)\nwhere \u03b10 \u2265 0 is a fixed regularization parameter, \u00b5t is the constant in Assumption 2 and \u03b7t is a learning rate which is typical chosen by O(1/t). The second order algorithms enjoy logarithmical regret bound without the strongly convex assumption but require quadratical space and computation cost."}, {"heading": "2.2 Efficient Algorithms by Sketching", "text": "To make the online Newton step scalable, it is natural to use sketching techniques [5]. Since the matrix H(t) in online learning has the form H(t) = \u03b10I + (A(t))>A(t), where A(t) \u2208 Rt\u00d7d is the corresponding term in (2) such as\nA(t) = [g(1), . . . ,g(t)]>, or A(t) = [ \u221a \u00b51 + \u03b71g (1), . . . , \u221a \u00b5t + \u03b7tg (t)]>.\nThe sketching algorithm employs an approximation of A(t) by B(t) \u2208 Rm\u00d7d where m d. Then we can use \u03b10I + (B(t))>B(t) to replace H(t) in update (1). By the Woodbury identity formula, we can reduce the computation of the update from O(d2) to O(m2d). There are several choices of sketching techniques, such as random projection [6, 7, 8], frequent direction [9, 10] and Oja\u2019s algorithm [11, 12]. Luo et al. [4] discussed how to improve online learning by these\ntechniques. In practice, the performance of sketched online Newton methods is sensitive to the choice of hyperparamter \u03b10 and they only achieve good result when \u03b10 is set appropriately.\nWe now give a brief review of frequent directions (FD) [9, 10], because it is closely related to our proposed method. Frequent Directions is a deterministic matrix sketching in the row-updates model. For any input matrix A \u2208 RT\u00d7d which comes from sequentially row by row, it maintains a sketch matrix B \u2208 Rm\u00d7d to approximate A>A by B>B. FD has following properties for any k < m,\nA>A\u2212B>B 0, (3) \u2016A>A\u2212B>B\u20162 \u2264 T\u22121\u2211 i=1 (\u03c3(t)m ) 2 \u2264 1 m\u2212 k \u2016A\u2212Ak\u20162F , (4)\nwhere \u2016 \u00b7 \u20162 denotes the spectral norm of the matrix and Ak is the best rank-k approximation to A in both the Frobenius and spectral norms.\nWe present the detailed implementation of FD in Algorithm 1, where \u03c3(t\u22121)m is the m-th largest singular values of B(t\u22121). The dominated computation of the algorithm is computation of svd(B(t)) which requires O(m2d) by the standard SVD implementation, while one can reduce the total cost from O(Tm2d) into O(Tmd) by doubling the space [5, 9] as in Algorithm 2.\nAlgorithm 1 Frequent Directions 1: Input: A = [a(1), . . . ,a(T )]> \u2208 RT\u00d7d, B(0) = 0m\u00d7d\n2: for t = 1, . . . , T do 3: Insert (a(t))> into the m-th row of B(t\u22121) 4: [U(t\u22121),\u03a3(t\u22121),V(t\u22121)] = svd(B(t\u22121))\n5: B(t) = \u221a( \u03a3(t\u22121) )2 \u2212 (\u03c3(t\u22121)m )2I \u00b7 (V(t\u22121))> 6: end for 7: Output: B = B(T )"}, {"heading": "3. Regularized Frequent Directions", "text": "Sketching usually leads to a low-rank matrix. To make the matrix H(t) invertible and well-conditioned, both the standard online Newton and sketched online Newton methods usually require a fixed regularization term \u03b10I. The hyperparameter \u03b10 typically needs to be tuned manually. On the other hand, the conventional sketching algorithms such as frequent direction only consider the gradient part (A(t))>A(t) in H(t) and do not cover the regularization term. Intuitively, it is better to increase the factor of the identity matrix as iteration goes because there are more and more rows of A(t). These motivate us to propose a new sketching algorithm that is more suitable to online learning."}, {"heading": "3.1 The Algorithm", "text": "The regularized frequent direction (RFD) is a variant of frequent direction. For a given matrix A \u2208 RT\u00d7d whose rows come from sequentially and a fixed scalar \u03b10 \u2208 R, RFD computes the sketch matrix B \u2208 Rm\u00d7d and \u03b1 \u2208 R (m d) to approximate \u03b10I + A>A by \u03b1I + B>B.\nAlgorithm 2 Fast Frequent Directions 1: Input: A = [a(1), . . . ,a(T )]> \u2208 RT\u00d7d, B(0) = 02m\u00d7d\n2: for t = 1, . . . , T do 3: Insert (a(t))> into a zero-valued row of B(t\u22121) 4: if B(t\u22121) has no zero-valued row 5: [U(t\u22121),\u03a3(t\u22121),V(t\u22121)] = svd(B(t\u22121))\n6: B(t) = \u221a( \u03a3(t\u22121) )2 \u2212 (\u03c3(t\u22121)m )2I \u00b7 (V(t\u22121))> 7: else 8: B(t) = B(t\u22121)\n9: end if 10: end for 11: Output: B = B(T )\nAlgorithm 3 Regularized Frequent Directions 1: Input: A = [a(1), . . . ,a(T )]> \u2208 RT\u00d7d, B(0) = 0m\u00d7d, \u03b1(0) = \u03b10 \u2208 R 2: for i = 1, . . . , T do 3: Insert (a(t))> into a zero-valued row of B(t\u22121)\n4: [U(t\u22121),\u03a3(t\u22121),V(t\u22121)] = svd(B(t\u22121))\n5: B(t) = \u221a( \u03a3(t\u22121) )2 \u2212 (\u03c3(t\u22121)m )2I \u00b7 (V(t\u22121))> 6: \u03b1(t) = \u03b1(t\u22121) + (\u03c3 (t\u22121) m )2 /2\n7: end for 8: Output: B = B(T ) and \u03b1 = \u03b1(T ).\nWe demonstrate the detailed implementation of RFD in Algorithm 3. Compared with the standard FD, RFD only maintains an extra variable \u03b1(t) by scalar operations for each iteration, hence the cost of RFD is almost the same as FD. In real applications, \u03b1(t) is typically increasing as iteration from the (m + 1)-th round, which makes \u03b1I + B>B positive definite even the initial \u03b1(0) is zero. Also, we can further accelerate it by doubling the space as shown in Algorithm 4."}, {"heading": "3.2 Theoretical Analysis", "text": "We now explain why the regularization term is updated by the rule of Algorithm 3 and provide the approximation error bound of RFD. Firstly, we give the following theorem about matrix approximation.\nTheorem 1 Given a positive semi-definite matrix M \u2208 Rd\u00d7d and a positive integer k < d, let M = U\u03a3U> be the SVD of M. Let Uk denote the matrix of the first k columns of U and \u03c3k be the top k-th singular value of M. Then the pair (C\u0302, \u03b4\u0302), defined as\nC\u0302 = Uk(\u03a3k \u2212 \u03b4\u0302I)1/2V and \u03b4\u0302 = (\u03c3k+1 + \u03c3d)/2\nAlgorithm 4 Fast Regularized Frequent Directions 1: Input: A = [a(1), . . . ,a(T )]> \u2208 RT\u00d7d, B(0) = 02m\u00d7d, \u03b1(0) = \u03b10 \u2208 R 2: for t = 1, . . . , T do 3: Insert (a(t))> into a zero-valued row of B(t\u22121)\n4: if B(t\u22121) has no zero-valued row 5: [U(t\u22121),\u03a3(t\u22121),V(t\u22121)] = svd(B(t\u22121))\n6: B(t) = \u221a( \u03a3(t\u22121) )2 \u2212 (\u03c3(t\u22121)m )2I \u00b7 (V(t\u22121))> 7: \u03b1(t) = \u03b1(t\u22121) + (\u03c3 (t\u22121) m )2 /2\n8: else 9: B(t) = B(t\u22121)\n10: end if 11: end for 12: Output: B = B(T )\nwhere V is an arbitrary k \u00d7 k orthonormal matrix, is the global minimizer of\nmin C\u2208Rd\u00d7k,\u03b4\u2208R\n\u2016M\u2212CC> \u2212 \u03b4I\u20162.\nTheorem 1 provides the closed form optimal solution for matrix approximation with regularization term. Zhang [13] has established the Frobenius norm based result in optimization view, while our analysis relies on the properties of unitary invariant norms. Moreover, our derivation is more concise. Addiotionally, we prove the solution is global optimal while Zhang\u2019s analysis is local (see Appendix A).\nAt the t-th round, our goal is to approximate the concentration of historical approximation and current data. The following theorem shows that our update is optimal with respect to the spectral norm.\nTheorem 2 Based on the updates in Algorithm 3 and letting B(t) refer to the matrix obtained by Line 5 of Algorithm 3 at the t-th iteration (without inserting a(t) in the next iteration), and B\u0303(t) be the matrix consists of the first m\u2212 1 rows of B(t), we have\n(B\u0303(t), \u03b1(t)) = argmin B\u0303\u2208Rd\u00d7(m\u22121),\n\u03b1\u2208R\n\u2225\u2225\u03b1(t\u22121)I + (B(t\u22121))>B(t\u22121) \u2212 B\u0303>B\u0303\u2212 \u03b1I\u2225\u2225 2 . (5)\nTheorem 2 provides a natural explanation to RFD, making the algorithm reasonable intuitively, while the standard FD sketching is only an extension of approximating item frequencies in streams [18] but lacks optimization view in the matrix case.\nRFD also enjoys a tighter approximation error shown in the following theorem.\nTheorem 3 For any k < m and using the notation of Algorithm 3, we have\u2225\u2225A>A\u2212 (B>B + \u03b1I)\u2225\u2225 2 \u2264 1 2(m\u2212 k) \u2016A\u2212Ak\u20162F , (6)\nwhere Ak is the best rank-k approximation to A in both the Frobenius and spectral norm.\nThe right-hand side of inequality (6) is the half of the one in (4), which means RFD reduces the approximation error significantly with only one extra scalar.\nThe approximation Hessian of sketched online Newton step typically has a regularization term. Hence we are more interested in approximating the matrix that can be expressed as M = \u03b10I + A>A where \u03b10 > 0. Suppose that the standard FD approximates A>A by B>B. Then it estimates M as MFD = \u03b10I + B>B, but RFD uses MRFD = \u03b1I + B>B. Theorem 4 shows that MRFD is well-conditioned no worse than MFD and M. In general, the equality in the theorem usually can not be hold for t > m unless (a(t))> lies in the row space of B(t\u22121) exactly or the first t rows of A have perfect low rank structure, which means that RFD results in a well-conditioned approximation more than others in practice.\nTheorem 4 With the notation of Algorithms 1 and 3, let M = \u03b10I + A>A, MFD = \u03b10I + B>B and MRFD = \u03b1I + B>B, where \u03b10 > 0 is a fixed scalar. Define the condition number of any nonsingular matrix H as \u03ba(H) = \u03c3max(H)/\u03c3min(H), where \u03c3max(H) and \u03c3min(H) are the largest and smallest singular values of H respectively. Then we have \u03ba(MRFD) \u2264 \u03ba(MFD) and \u03ba(MRFD) \u2264 \u03ba(M)."}, {"heading": "4. The Online Newton Step by RFD", "text": "We now present the online Newton step by Regularized frequent directions (RFD-SON). The procedure is shown in Algorithm 5, which is similar to sketched online Newton step (SON) [4] but with the new sketching method RFD.\nAlgorithm 5 RFD for Online Newton Step 1: Input: \u03b1(0) = \u03b10, m < d, \u03b7t = O(1/t) and B(0) = 0m\u00d7d. 2: for t = 1, . . . , T do\n3: Receive example x(t), and loss function ft(w)\n4: g(t) = \u2207ft(w(t)) 5: Insert ( \u221a \u00b5t + \u03b7tg (t))> into the m-th row of B(t\u22121) 6: [U(t\u22121),\u03a3(t\u22121),V(t\u22121)] = svd(B(t\u22121))\n7: B(t) = \u221a( \u03a3(t\u22121) )2 \u2212 (\u03c3(t\u22121)m )2I \u00b7 (V(t\u22121))> 8: \u03b1(t) = \u03b1(t\u22121) + (\u03c3 (t\u22121) m )2 /2\n9: H(t) = \u03b1(t)I + (B(t))>B(t)\n10: u(t+1) = w(t) \u2212 (H(t))\u22121g(t)\n11: w(t+1) = argminw\u2208Kt \u2016w \u2212 u (t+1)\u2016H(t) 12: end for\nTheorem 5 Let \u00b5 = maxTt=1{\u00b5t} and K = \u22c2T t=1Kt. Then under Assumptions 1 and 2 for any w \u2208 K, Algorithm 5 has the following regret\nRT (w) \u2264 \u03b10 2 \u2016w\u20162 + 2(CL)2 T\u2211 t=1 \u03b7t + m 2(\u00b5+ \u03b7T ) ln ( tr ( (B(T ))>B(T ) ) + \u03b1(T ) \u03b10 ) + \u2126RFD (7)\nwhere\n\u2126RFD = d\u2212m 2(\u00b5+ \u03b7T ) ln \u03b1(T ) \u03b10 +\nm\n4(\u00b5+ \u03b7T ) T\u2211 t=1 (\u03c3 (t) m )2 \u03b1(t) + C2 T\u2211 t=1 (\u03c3(t\u22121)m ) 2.\nWe present the regret bound of RFD-SON in Theorem 5. The last term in (7) is the main gap between RFD-SON and the standard online Newton step without sketching, and the terms are logarithmic to T . \u2126RFD is dominated by the last term which can be bounded as (4). If we exploit the standard FD to sketched online Newton step [4] (RFD-SON), the gap will be\n\u2126FD = m\n2(\u00b5+ \u03b7T ) T\u2211 t=1 (\u03c3 (t\u22121) m )2 \u03b10 ,\nwhich looks comparable to our result because it is also dependent on \u2211T\nt=1(\u03c3 (t\u22121) m )2. However, The\nbound of FD-SON is highly related to the hyperparameter \u03b10. If we increase the value of \u03b10, the gap \u2126FD can be reduced, but the term \u03b102 \u2016w\u2016\n2 will increase. For RFD-SON, we can set \u03b10 be sufficient small to reduce \u03b102 \u2016w\u2016 2 and it affects the term \u2126RFD limited. The reason is that the first term of \u2126RFD contains 1\u03b10 in the logarithmic term and the second term contains \u03b1 (t) = \u03b10+ 1 2 \u2211t\u22121 i=1 (\u03c3 (i) m )2\nin the denominator. For large t, \u03b1(t) is mainly dependent on \u2211t\u22121\ni=1 (\u03c3 (i) m )2, rather than \u03b10."}, {"heading": "5. Experiments", "text": "In this section, we evaluate the performance of regularized frequent directions (RFD) and online Newton step by RFD (RFD-SON) on three real-world datasets \u201ca9a\u201d, \u201cgisette\u201d and \u201csido0\u201d whose details are listed in Table 1. The \u201csido0\u201d dataset comes from Causality Workbench1 and the others can be downloaded from LIBSVM repository2. The experiments are conducted in Matlab and run on a server with Intel (R) Core (TM) i7-3770 CPU 3.40GHz\u00d72, 8GB RAM and Windows Server 2012 64-bit system."}, {"heading": "5.1 Matrix Approximation", "text": "We first compare the approximation error of FD and RFD. For a given dataset A \u2208 Rn\u00d7d of n samples with d features, we use FD (Algorithm 2) and RFD (Algorithm 4) to approximate the covariance matrix A>A by B>B and \u03b1I + B>B respectively; that is,\nError-FD = \u2016A>A\u2212B>B\u20162 \u2016A>A\u20162 , and Error-RFD = \u2016A>A\u2212B>B\u2212 \u03b1I\u20162 \u2016A>A\u20162 .\nThe regularization term \u03b10 is not important for this evaluation because it does not change the absolute error in the numerator of Error-FD or Error-RFD. We report the relative spectral norm error by varying the sketch size m. Figure 1 shows the performance of the two Algorithms. The relative error of RFD is always lower than the one of FD and the decrement is nearly the half in most cases. The results match the theoretical analysis (Theorem 3) very well. We do not include the comparison of the running time because they are almost the same."}, {"heading": "5.2 Online Learning", "text": "We now evaluate the performance of RFD-SON. We use the least squares loss (that is, ft(w) = (w>x(t) \u2212 y(t))2), and set Kt = {w : |w>x(t)| \u2264 1}. In the experiments, we use the the doubling space strategy on RFD sketching, which is shown in Algorithm 6.\nBy the Woodbury formula, the parameter w(t) can be updated in O(md) cost as follows\nu(t+1) = w(t) \u2212 1 \u03b1(t)\n( g(t) \u2212 (B(t))>H(t)B(t)g(t) ) ,\nw(t+1) = u(t+1) \u2212 \u03b3(t) ( x(t) \u2212 (B(t))>H(t)B(t)x(t) ) ,\nwhere \u03b3(t) = \u03c4 ( (u(t))>x(t) ) (x(t))>x(t) \u2212 (x(t))>(B(t))>H(t)B(t)x(t) and \u03c4(z) = sgn(z) max{|z| \u2212 1, 0}.\nThe detailed derivation of the projection step can be found in Luo et al. [4]. We use 70% data for training and the rest for test. The algorithms in the experiments include ADAGRAD; standard online Newton step with the full Hessian [17] (FULL-ON); sketched online Newton step with frequent directions (FD-SON), random projections (RP-SON), Oja\u2019s algorithms\nAlgorithm 6 Fast RFD for Online Newton Step 1: Input: \u03b1(0) = \u03b10, m < d, \u03b7t = O(1/t) and B(0) = 0m\u00d7d. 2: for t = 1, . . . , T do\n3: Receive example x(t), and loss function ft(w)\n4: g(t) = \u2207ft(w(t)) 5: Insert ( \u221a \u00b5t + \u03b7tg (t))> into the m-th row of B(t\u22121) 6: if B(t\u22121) has no zero valued rows 7: [U(t\u22121),\u03a3(t\u22121),V(t\u22121)] = svd(B(t\u22121))\n8: B(t) = \u221a( \u03a3(t\u22121) )2 \u2212 (\u03c3(t\u22121)m )2I \u00b7 (V(t\u22121))> 9: \u03b1(t) = \u03b1(t\u22121) + (\u03c3 (t\u22121) m )2 /2\n10: else 11: B(t) = B(t\u22121)\n12: end if 13: H(t) = \u03b1(t)I + (B(t))>B(t)\n14: u(t+1) = w(t) \u2212 (H(t))\u22121g(t)\n15: w(t+1) = argminw\u2208Kt \u2016w \u2212 u (t+1)\u2016H(t) 16: end for\n(Oja-SON) [4], and our proposed sketched online Newton step with regularized frequent directions (RFD-SON). The hyperparameter \u03b10 is tuned from {10\u221210, 10\u22129 . . . 109, 1010} and the sketch size for FD-SON, RP-SON, Oja-SON and RFD-SON is chosen from {5, 10, 20}.\nTable 2 reports the accuracy for all the above algorithms at one epoch with the best \u03b10, and Table 3 includes the corresponding running times. ADAGRAD has apparently less accuracy than the second order methods although it is the fastest. The SON methods are more efficient than FULL-ON as we expected. The costs of FD-SON and RFD-SON are not identical here because the approximation methods will generate different g(t) at each step which makes the matrices to be estimated not same. The accuracy of RFD-SON withm = 20 is (one of) the best on all the datasets.\nWe are also interested in how the hyperparameter \u03b10 affects the performance of the algorithm. Figures 2, 3 and 4 show the results. We display the results of baselines when \u03b10 is near to the best one for all the baseline algorithms. Note that only FULL-ON is robust to the hyperparameter on \u201ca9a\u201d and \u201csido0\u201d. In most cases, the baselines are sensitive to the choice of the hyperparameter. But the proposed RFD-SON is very robust to the hyperparameter. We demonstrate the behaviors of RFD-SON with a large range of \u03b10. The algorithm typically achieves comparable good performance when \u03b10 is not too large (the accuracy of all the second order methods will decrease apparently when \u03b10 reaches 104). Specifically, RFD-SON with \u03b10 = 10\u221210 has almost the same performance as the best choice of the hyperparameter on \u201ca9a\u201d and \u201csido0\u201d. RFD-SON is even more stable than the standard online Newton step algorithm. The reason is that the estimate of the Hessian in our method is typically more well-conditioned."}, {"heading": "6. Conclusions", "text": "In this paper, we have studied the second order online learning algorithms. We have proposed a novel sketching method regularized frequent directions (RFD) with several nice properties. Both the theoretical analysis and experiments show RFD is much better than FD. The online learning algorithm with RFD achieves better accuracy than baselines and more robust to the regularization parameters. The application of RFD is not limited to convex online optimization. We will try to exploit RFD to improve other optimziation algorithms including stochastic or non-convex cases."}, {"heading": "Appendix A: The Proof of Theorem 1", "text": "In this section, we firstly provide serval lemmas from the book \u201cTopics in matrix analysis\u201d [22], then we prove Theorem 1. The proof of Lemma 1 and 2 can be found in the book and we give the proof of Lemma 3 here.\nLemma 1 (Theorem 3.4.5 of [22]) Let A,B \u2208 Rm\u00d7n be given, and suppose A, B and A \u2212 B have decreasingly ordered singular values, \u03c31(A) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3q(A), \u03c31(B) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3q(B), and \u03c31(A \u2212 B) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3q(A \u2212 B), where q = min{m,n}. Define si(A,B) \u2261 |\u03c3i(A) \u2212 \u03c3i(B)|, i = 1, . . . , q and let s[1](A,B) \u2265 \u00b7 \u00b7 \u00b7 \u2265 s[i](A,B) denote a decreasingly ordered rearrangement of the values si(A,B). Then\nk\u2211 i=1 s[i](A,B) \u2264 k\u2211 i=1 \u03c3i(A\u2212B) for k = 1, . . . , q.\nLemma 2 (Corollary 3.5.9 of [22]) Let A,B \u2208 Rm\u00d7n be given, and let q = min{m,n}. The following are equivalent\n1. |||A||| \u2264 |||B||| for every unitarily invariant norm ||| \u00b7 ||| on Rm\u00d7n.\n2. Nk(A) \u2264 Nk(B) for k = 1, . . . , q whereNk(X) \u2261 \u2211k i=1 \u03c3k(X) denotes by Ky Fan k-norm.\nLemma 3 (Page 215 of [22]) Let A,B \u2208 Rm\u00d7n be given, and let q = min{m,n}. Define the diagonal matrix \u03a3(A) = [\u03c3ij ] \u2208 Rm\u00d7n by \u03c3ii = \u03c3i(A), all other \u03c3ij = 0, where \u03c31(A) \u2265, . . . ,\u2265 \u03c3q(A) are the decreasingly ordered singular values of A. We define \u03a3(B) similarly. Then we have |||A\u2212B||| \u2265 |||\u03a3(A)\u2212\u03a3(B)||| for every unitarily invariant norm ||| \u00b7 |||.\nProof Using the notation of Lemma 1 and 2, matrices A\u2212B and \u03a3(A)\u2212\u03a3(B) have the decreasingly ordered singular values \u03c31(A\u2212B) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3q(A\u2212B) and s[1](A,B) \u2265 \u00b7 \u00b7 \u00b7 \u2265 s[q](A,B). Then we have\nNk(A\u2212B) = k\u2211 i=1 \u03c3i(A\u2212B) \u2265 k\u2211 i=1 s[i](A,B) = Nk(\u03a3(A)\u2212\u03a3(B)), (8)\nwhere the inequality is obtain by Lemma 1. The Lemma 2 implies (8) is equivalent to |||A\u2212B||| \u2265 |||\u03a3(A)\u2212\u03a3(B)||| for every unitarily invariant norm ||| \u00b7 |||.\nThen we give the proof of Theorem 1. Proof Using the notation of above lemmas, we can bound the objective function as follow. For any C \u2208 Rd\u00d7k and \u03b4 \u2208 R, we have \u2225\u2225M\u2212CC> \u2212 \u03b4I\u2225\u2225\n2 \u2265 \u2225\u2225\u03a3(M)\u2212\u03a3(CC> + \u03b4I)\u2225\u2225\n2\n= max i\u2208{1,...,d} \u2223\u2223\u03c3i(M)\u2212 \u03c3i(CC>)\u2212 \u03b4\u2223\u2223 \u2265 max i\u2208{k+1,...,d}\n\u2223\u2223\u03c3i(M)\u2212 \u03c3i(CC>)\u2212 \u03b4\u2223\u2223 = max i\u2208{k+1,...,d}\n\u2223\u2223\u03c3i(M)\u2212 \u03b4\u2223\u2223 \u2265 max i\u2208{k+1,...,d}\n\u2223\u2223\u03c3i(M)\u2212 \u03b4\u0302\u2223\u2223. The first inequality is obtained by Lemma 3 since the spectral norm is unitarily invariant, and the second inequality is the property of maximization operator. The last inequality can be checked easily by the property of medium and the equivalence of SVD and eigenvector decomposition for positive semi-definite matrix.\nThe first equality is based on the definition of spectral norm. The second equality holds due to the fact rank(CC>) \u2264 k which leads \u03c3i(CC>) = 0 for any i > k.\nNote that all above equalities occurs for C = C\u0302 and \u03b4 = \u03b4\u0302. Hence we prove the result of this theorem.\nWe also demonstrate the similar result with respect to Frobenius norm in Corollary 1. This analysis includes the global optimality of the problem, while Zhang [13]\u2019s analysis only prove the solution is local optimal. Additionally, our proof is more concise.\nCorollary 1 Using the same notation of Theorem 1, we have the pair (C\u0303, \u03b4\u0303) defined as\nC\u0303 = Uk(\u03a3k \u2212 \u03b4\u0303I)1/2V and \u03b4\u0303 = 1\nd\u2212 k d\u2211 i=j+1 \u03c3i\nis the global minimizer of\nmin C\u2208Rd\u00d7k,\u03b4\u2208R\n\u2016M\u2212CC> \u2212 \u03b4I\u20162F ,\nwhere V is an arbitrary k \u00d7 k orthogonal matrix.\nProof We have the result similarly to Theorem 1.\n\u2016M\u2212CC> \u2212 \u03b4I\u20162F\n\u2265\u2016\u03a3(M)\u2212\u03a3(CC> + \u03b4I)\u20162F = d\u2211 i=1 (\u03c3i(M)\u2212 \u03c3i(CC>)\u2212 \u03b4)2 \u2265 d\u2211\ni=k+1\n(\u03c3i(M)\u2212 \u03c3i(CC>)\u2212 \u03b4)2\n= d\u2211 i=k+1 (\u03c3i(M)\u2212 \u03b4)2 \u2265 d\u2211\ni=k+1\n(\u03c3i(M)\u2212 \u03b4\u0303)2\nThe first four steps are similar to the ones of Theorem 1, but replace the spectral norm and absolute operator with Frobenius norm and square function . The last step comes from the property of the mean value.\nWe can check that all above equalities occurs for C = C\u0303 and \u03b4 = \u03b4\u0303, which completes the proof."}, {"heading": "Appendix B: The Proof of Theorem 2", "text": "Proof The Algorithm 3 implies the singular values of \u03b1(t\u22121)I + (B(t\u22121))>B(t\u22121) are\n(\u03c3 (t) 1 ) 2 + \u03b1(t\u22121) \u2265 \u00b7 \u00b7 \u00b7 \u2265 (\u03c3(t)m )2 + \u03b1(t\u22121) \u2265 \u03b1(t\u22121) = \u00b7 \u00b7 \u00b7 = \u03b1(t\u22121).\nThen we can use the Theorem 1 by taking\nM =\u03b1(t\u22121)I + (B(t\u22121))>B(t\u22121),\nC\u0302 =(V(t\u22121))> \u221a( \u03a3(t\u22121) )2 \u2212 (\u03c3(t\u22121)m )2I = (B(t))>,\n\u03b4\u0302 =[(\u03c3(t\u22121)m ) 2 + \u03b1(t\u22121) + \u03b1(t\u22121)]/2 = \u03b1(t).\nDue to the last column of B(t) is zero and (B\u0303(t))>B\u0303 = (B(t))>B(t), we have (B\u0303(t), \u03b1(t)) is the minimizer of the problem."}, {"heading": "Appendix C: The Proof of Theorem 3", "text": "Proof In this proof, the notation B(t) refer to the matrix obtain by the line 5 of Algorithm 3 at t-th iteration, and B(t) is the same one for (t \u2212 1)-th iteration (without inserting a(t)). In other words, we have V(t\u22121)(\u03a3(t\u22121))2V(t\u22121) = (B(t\u22121))>B(t\u22121) +a(t)(a(t))>. Then we can deserve the lower bound as follow\u2225\u2225A>A\u2212 (B>B + \u03bbI)\u2225\u2225\n2\n= \u2225\u2225\u2225\u2225\u2225 T\u2211 t=1 [ (a(i))>a(t) \u2212 (B(t))>B(t) + (B(t\u22121))>B(t\u22121) \u2212 1 2 (\u03c3(t\u22121)m ) 2I ]\u2225\u2225\u2225\u2225\u2225\n2 \u2264 T\u2211 t=1 \u2225\u2225\u2225(a(t))>a(t) \u2212 (B(t))>B(t) + (B(t\u22121))>B(t\u22121) \u2212 1 2 (\u03c3(t\u22121)m ) 2I \u2225\u2225\u2225 2\n= T\u2211 t=1 \u2225\u2225\u2225V(t\u22121)(\u03a3(t\u22121))2(V(t\u22121))> \u2212V(t\u22121)[(\u03a3(t\u22121))2 \u2212 (\u03c3(t\u22121)m )2I](V(t\u22121))> \u2212 12(\u03c3(t\u22121)m )2I\u2225\u2225\u22252 =\nT\u2211 t=1 \u2225\u2225\u2225(\u03c3(t\u22121)m )2V(t\u22121)(V(t\u22121))> \u2212 12(\u03c3(t\u22121)m )2I\u2225\u2225\u22252 = 1\n2 T\u2211 t=1 (\u03c3(t)m ) 2 \u2264 1 2(m\u2212 k) \u2016A\u2212Ak\u20162F .\nThe first three equalities are direct from the procedure of the algorithm, and the last one is due to V(t\u22121) is column orthonormal. The first inequality comes from the triangle inequality of spectral norm. The last one can be obtain by the properties of (4)."}, {"heading": "Appendix D: The Proof of Theorem 4", "text": "Proof We can compare \u03ba(MRFD) and \u03ba(MFD) by the fact \u03b1 \u2265 \u03b10 as follow\n\u03ba(MRFD) = \u03c3max(B\n>B) + \u03b1\n\u03b1 \u2264 \u03c3max(B >B) + \u03b10 \u03b10 = \u03ba(MFD).\nThe other inequality can be derived as\n\u03ba(MRFD) = \u03c3max(B\n>B) + \u03b1\n\u03b1 \u2264 \u03c3max(A\n>A) + \u03b1\n\u03b1 \u2264 \u03c3max(A >A) + \u03b10 \u03b10 = \u03ba(M),\nwhere the first inequality comes from (3) and the others are easy to obtain."}, {"heading": "Appendix E: The Proof of Theorem 5", "text": "Proof Suppose that V(t)\u22a5 is the orthogonal complement of V (t)\u2019s column space, that is V(t)(V(t))>+ V (t) \u22a5 (V (t) \u22a5 ) > = I, then we have\nH(t) \u2212H(t\u22121)\n=\u03b1(t)I + (B(t))>B(t) \u2212 \u03b1(t\u22121)I\u2212 (B(t\u22121))>B(t\u22121)\n= 1\n2 (\u03c3(t\u22121)m ) 2I\u2212 (\u03c3(t\u22121)m )2V(t\u22121)(V(t\u22121))> + (\u00b5t + \u03b7t)g(t)(g(t))>\n= 1\n2 (\u03c3(t\u22121)m )\n2 [ V\n(t\u22121) \u22a5 (V (t\u22121) \u22a5 )\n> \u2212V(t\u22121)(V(t\u22121))> ] + (\u00b5t + \u03b7t)g (t)(g(t))>. (9)\nBy the proof of Theorem 2 in [4], we have\n2RT (w) \u2264 \u03b10\u2016w\u20162 +RG +RD,\nwhere\nRG = T\u2211 t=1 (g(t))>(H(t))\u22121g(t),\nand\nRD = T\u2211 t=1 (w(t) \u2212w)>[H(t) \u2212H(t\u22121) \u2212 \u00b5tg(t)(g(t))>](w(t) \u2212w).\nWe can bound RG as follow\nT\u2211 t=1 (g(t))>(H(t))\u22121g(t)\n= T\u2211 t=1 \u2329 (H(t))\u22121,g(t)(g(t))> \u232a =\nT\u2211 t=1\n1\n\u00b5t + \u03b7t\n\u2329 (H(t))\u22121,H(t) \u2212H(t\u22121) 1\n2 (\u03c3(t\u22121)m ) 2[V (t\u22121) \u22a5 (V (t\u22121) \u22a5 )\n> \u2212V(t\u22121)(V(t\u22121))> ]\u232a\n\u2264 1 \u00b5+ \u03b7T T\u2211 t=1 \u2329 (H(t))\u22121,H(t) \u2212H(t\u22121) + 1 2 (\u03c3(t\u22121)m ) 2V(t\u22121)(V(t\u22121))> \u232a\n= 1\n\u00b5+ \u03b7T T\u2211 t=1 [\u2329 (H(t))\u22121,H(t) \u2212H(t\u22121) \u232a + 1 2 (\u03c3(t\u22121)m ) 2tr ( V(t\u22121)(H(t))\u22121(V(t\u22121))> )] .\nThe above equalities come from the properties of trace operator and (9) and the inequality is due to \u03b7t is increasing.\nThe term \u2211T\nt=1\u3008(H(t))\u22121,H(t) \u2212H(t\u22121)\u3009 can be bound as\nT\u2211 t=1 \u3008(H(t))\u22121,H(t) \u2212H(t\u22121)\u3009\n\u2264 T\u2211 t=1 ln det(H(t)) det(H(t\u22121)) = ln det(H(T )) det(H(0))\n= ln\n\u220fd i=1 \u03c3i(H (T ))\n\u03b10 = d\u2211 i=1 ln \u03c3i ( (B(T ))>B(T ) ) + \u03b1(T ) \u03b10\n= m\u2211 i=1 ln \u03c3i ( (B(T ))>B(T ) ) + \u03b1(T ) \u03b10 + (d\u2212m) ln \u03b1 (T ) \u03b10\n\u2264m ln \u2211m i=1[\u03c3i ( (B(T ))>B(T ) ) + \u03b1(T )]\nm\u03b10 + (d\u2212m) ln \u03b1\n(T )\n\u03b10 =m ln ( tr ( (B(T ))>B(T ) ) + \u03b1(T )\n\u03b10\n) + (d\u2212m) ln \u03b1 (T )\n\u03b10 .\nThe first inequality is obtain by the concavity of the log determinant function, the second inequality comes from the Jensen\u2019s inequality and the other steps is based on the procedure of the algorithm.\nThe other one 12 \u2211T t=1(\u03c3 (t) m )2tr ( V(t)(H(t))\u22121(V(t))> ) can be bounded as\n1\n2 T\u2211 t=1 (\u03c3(t)m ) 2tr ( V(t)(H(t))\u22121(V(t))> ) \u22641\n2 T\u2211 t=1 (\u03c3 (t) m )2 \u03b1(t) tr ( V(t)(V(t))> ) = m\n2 T\u2211 t=1 (\u03c3 (t) m )2 \u03b1(t) . (10)\nHence, we have\nRG \u2264 1\n\u00b5+ \u03b7T\n[ m ln ( tr ( (B(T ))>B(T ) ) + \u03b1(T )\n\u03b10\n) + (d\u2212m) ln \u03b1 (T )\n\u03b10 + m 2 T\u2211 t=1 (\u03c3 (t) m )2 \u03b1(t)\n] . (11)\nThen we bound the term RD by using the fact (9) and Assumption 1 and 2.\nRD = T\u2211 t=1 (w(t) \u2212w)> [ \u03b7tg (t)(g(t))> + 1 2 (\u03c3(t\u22121)m ) 2I\u2212V(t\u22121)(V(t\u22121))> ] (w(t) \u2212w)\n\u2264 T\u2211 t=1 \u03b7t(w (t) \u2212w)>g(t)(g(t))>(w(t) \u2212w) + 1 2 T\u2211 t=1 (\u03c3(t\u22121)m ) 2(w(t) \u2212w)>(w(t) \u2212w)\n\u22644(CL)2 T\u2211 t=1 \u03b7t + 2C 2 T\u2211 t=1 (\u03c3(t\u22121)m ) 2. (12)\nFinally, we obtain the result by combining (11) and (12)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Online Newton step algorithms usually achieve good performance with less training samples than<lb>first order methods, but require higher space and time complexity in each iteration. In this paper,<lb>we develop a new sketching strategy called regularized frequent direction (RFD) to improve the<lb>performance of online Newton algorithms. Unlike the standard frequent direction (FD) which<lb>only maintains a sketching matrix, the RFD introduces a regularization term additionally. The<lb>regularization provides an adaptive stepsize for update, which makes the algorithm more stable.<lb>The RFD also reduces the approximation error of FD with almost the same cost and makes the<lb>online learning more robust to hyperparameters. Empirical studies demonstrate that our approach<lb>outperforms sate-of-the-art second order online learning algorithms.", "creator": "LaTeX with hyperref package"}}}