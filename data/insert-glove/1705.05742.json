{"id": "1705.05742", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs", "abstract": "Knowledge malin Graphs visioned are bailie important mencap tools to text-to-speech model mimos multi - dual-ignition relational 55-mile data that eighth-century serves as heart-lung information forsikring pool no-good for hochfilzen various 2.725 applications. Traditionally, these graphs letang are maloney considered namik to be stupples static in nature. nnm However, nystagmus recent availability of sagger large entertainingly scale event - egregiously based interaction data has given rise 81.2 to decree dynamically angelicum evolving knowledge graphs \u201c\u201c that caryophyllaceae contain ba\u02bfal temporal chaminade information for 3,853 each edge. Reasoning heatshield over time in such times-1stld graphs is clemensia not cornered yet well oksa understood.", "histories": [["v1", "Tue, 16 May 2017 14:53:02 GMT  (1468kb,D)", "http://arxiv.org/abs/1705.05742v1", null], ["v2", "Wed, 17 May 2017 04:54:07 GMT  (1468kb,D)", "http://arxiv.org/abs/1705.05742v2", null], ["v3", "Wed, 21 Jun 2017 05:21:46 GMT  (1464kb,D)", "http://arxiv.org/abs/1705.05742v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["rakshit trivedi", "hanjun dai", "yichen wang", "le song"], "accepted": true, "id": "1705.05742"}, "pdf": {"name": "1705.05742.pdf", "metadata": {"source": "META", "title": "Know-Evolve: Deep Reasoning in Temporal Knowledge Graphs", "authors": ["Rakshit Trivedi", "Mehrdad Farajtabar", "Yichen Wang", "Hanjun Dai", "Hongyuan Zha", "Le Song"], "emails": ["<rstrivedi@gatech.edu>."], "sections": [{"heading": null, "text": "In this paper, we present a novel deep evolutionary knowledge network architecture to learn entity embeddings that can dynamically and nonlinearly evolve over time. We further propose a multivariate point process framework to model the occurrence of a fact (edge) in continuous time. To facilitate temporal reasoning, the learned embeddings are used to compute relationship score that further parametrizes intensity function of the point process. We demonstrate improved performance over various existing relational learning models on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to any prior reasoning approaches in multi-relational setting."}, {"heading": "1. Introduction", "text": "Reasoning is a key concept in artificial intelligence. A host of applications such as search engines, question-answering systems, conversational dialogue systems, and social networks require reasoning over underlying structured knowledge. Effective representation and learning over such knowledge is a very important task. In particular, Knowledge Graphs have gained much attention as an effective model for studying complex multi-relational settings. Many such settings generate or leverage on event based data that exhibits complex temporal dynamics in addition to its multirelational nature. For instance, GDELT (Leetaru & Schrodt, 2013) and ICEWS (Boschee et al., 2017) are two popular\n1College of Computing, Georgia Institute of Technology. Correspondence to: Rakshit Trivedi <rstrivedi@gatech.edu>.\nFigure 1. Sample temporal knowledge subgraph between persons, organizations and countries.\nevent based data repository that contains knowledge about entity interactions across the globe.\nKnowledge graphs can be effective to model systems where the edges representing relationships have temporal information associated with it. Facts occur and recur evolve over time in these temporal knowledge graphs. Figure (1) shows a subgraph snapshot of such temporal knowledge graph.\nHowever, traditional knowledge graphs are considered to be static snapshot of multi-relational data and consequently there is a lack of approach that can characterize and reason over temporally evolving systems. Static knowledge graphs suffer from incompleteness resulting in their limited reasoning ability. Most work on static graphs have therefore focussed on perfecting entity-relationship representations and performing inference about missing facts based on available knowledge. But these methods lack ability to use rich temporal dynamics present in underlying data.\nEffectively capturing temporal dependencies across facts in addition to the relational (structural) dependencies can help improve the understanding on behavior of entities and how they contributed to generation of facts over time. For example, one can effectively answer questions like:\n\u2022 Object prediction. (Who) will Donald Trump mention next?\n\u2022 Subject prediction. (Which country) will provide material support to US next month?\nar X\niv :1\n70 5.\n05 74\n2v 1\n[ cs\n.A I]\n1 6\nM ay\n2 01\n7\n\u2022 Time prediction. (When) will Bob visit Burger King?\nHere, the focus can now shift on reasoning about temporal link prediction of future relationships.\n\u201dPeople (entities) change over time and so do relationships.\u201d When two entities forge a relationship, the newly formed edge drives their preferences and behavior. This change is effected by combination of their own historical factors (evolutionary effect) and their compatibility with the historical factors of the other entity (co-evolutionary effect).\nFor instance, if a country generally engaged in verbal cooperation relationships in past, it may now start engaging in material cooperation due to improvement in economy. Further, if two countries have tense relationships, they are more likely to engage in conflicts. On the other hand, two countries forging an alliance are most likely to take confrontational stands against enemies of each other. Finally, time plays a vital role in this process. A country that was once peaceful may not have same characteristics 10 years in future due to various facts (events) that may occur during that period. Being able to capture this temporal and evolutionary effects can help us reason better about future relationship of an entity. We term this combined phenomenon of evolving entities and their dynamically changing relationships over time as \u201cknowledge evolution\u201d.\nIn this paper, we address the challenge of modelling knowledge evolution and reasoning over complex non-linear interactions between entities in a multi-relational setting. The key idea of our work is to model the occurrence of a fact in knowledge graph as multidimensional point process by parametrizing the conditional intensity function with relationship score for that fact. The relationship score further depends on the dynamically evolving entity embeddings based on their past events. More specifically, our work makes the following contributions:\n\u2022 We propose a novel deep learning architecture that evolves over time based on new facts as they occur or recur. The dynamically evolving network will ingest the incoming new facts, learn from them and update the embeddings of involved entities based on their recent relationships and temporal behavior. \u2022 Besides predicting the occurrence of a fact, our architecture has ability to predict time when the fact may potentially occur which is not possible by any prior relational learning approaches to the best of our knowledge. \u2022 Our model supports Open World Assumption as missing links are not considered to be false and may potentially occur in future time. Moreover, it supports prediction over unseen entities due to its novel dynamic embedding process. \u2022 The large-scale experiments on two real world datasets show that our framework has consistently and signifi-\ncantly better performance for link prediction than stateof-arts that do not account for temporal and evolving non-linear dynamics. \u2022 Our work aims to introduce the use of powerful mathematical tool of point process process models for temporal reasoning over dynamically evolving knowledge graphs. It has potential to open a new research direction in reasoning over time for various multi-relational settings with underlying spatio-temporal dynamics."}, {"heading": "2. Preliminaries", "text": "We provide necessary background of temporal point processes and temporal knowledge graphs."}, {"heading": "2.1. Temporal Point Process", "text": "A temporal point process (Cox & Lewis, 2006) is a random process whose realization consists of a list of events localized in time, {ti} with ti \u2208 R+. Equivalently, a given temporal point process can be represented as a counting process, N(t), which records the number of events before time t.\nAn important way to characterize temporal point processes is via the conditional intensity function \u03bb(t), a stochastic model for the time of the next event given all the previous events. Formally, \u03bb(t)dt is the conditional probability of observing an event in a small window [t, t+ dt) given the history T (t) := {tk|tk < t} up to t, i.e.,\n\u03bb(t)dt := P {event in [t, t+ dt)|T (t)} = E[dN(t)|T (t)]\n(1)\nwhere one typically assumes that only one event can happen in a small window of size dt, i.e., dN(t) \u2208 {0, 1}.\nFrom the survival analysis theory (Aalen et al., 2008), given the history T = {t1, . . . , tn}, for any t > tn, we characterize the conditional probability that no event happens during [tn, t) as S(t|T ) = exp ( \u2212 \u222b t tn \u03bb(\u03c4) d\u03c4 ) . Moreover, the conditional density that an event occurs at time t is defined as : f(t) = \u03bb(t)S(t) (2)\nThe functional form of the intensity \u03bb(t) is often designed to capture the phenomena of interests. Some Common forms include: Poisson Process, Hawkes processes (Hawkes, 1971; Wang et al., 2016), Self-Correcting Process (Isham & Westcott, 1979), Power Law and Rayleigh Process.\nRayleigh Process is a non-monotonic process and is welladapted to modeling fads, where event likelihood drops rapidly after rising to a peak. Its intensity function is \u03bb(t) = \u03b1 \u00b7 (t), where \u03b1 > 0 is the weight parameter, and the log survival function is logS(t|\u03b1) = \u2212\u03b1 \u00b7 (t)2/2."}, {"heading": "2.2. Temporal Knowledge Graph representation", "text": "We define a Temporal Knowledge Graph (TKG) as a multirelational directed graph with timestamped edges between\nany pair of nodes and can be seen as a dynamic counterpart of static knowledge graphs described above. In TKG, each edge between two nodes represent an event in the real world between two entities and edge type (relationship) represent the corresponding event type. Further an edge may or may not be available multiple times (recurrence). We do not allow duplicate edges and self-loops in graph. Hence, all recurrent edges will have different time points and every edge will have distinct subject and object entities.\nGiven ne entities and nr relationships, we extend traditional triplet representation for knowledge graphs to introduce time dimension and represent each fact in TKG as the quadruplet (es, r, eo, t), where es, eo \u2208 {1, . . . , ne}, es 6= eo, r \u2208 {1, . . . , nr}, t \u2208 R+. It represents the creation of relationship edge r between subject entity es, and object entity eo at time t. The complete TKG can therefore be represented as an ne \u00d7 ne \u00d7 nr \u00d7 T - dimensional tensor where T is the total number of available time points. Consider a TKG comprising of N edges and denote the globally ordered set of corresponding N observed events as D = {(es, r, eo, t)n}Nn=1, where 0 \u2264 t1 \u2264 t2 . . . \u2264 T ."}, {"heading": "3. Evolutionary Knowledge Network", "text": "We present our unified knowledge evolution framework (Know-Evolve) for reasoning over temporal knowledge graphs. The reasoning power of Know-Evolve stems from three major components of the Evolutionary Knowledge Network:\n1. A powerful mathematical tool of temporal point process that models occurrence of a fact.\n2. A bilinear relationship score that captures multirelational interactions between entities and parametrizes the intensity function of above point process.\n3. A novel deep recurrent network that learns dynamically and mutually evolving latent representations of entities based on their interactions with other entities in multirelational space over time."}, {"heading": "3.1. Temporal Process", "text": "Large scale temporal knowledge graphs exhibit highly heterogeneous temporal patterns of events between different entities. Further, as these entities coevolve non-linearly, they add to the intricate temporal dependencies. Discrete epoch based methods have been widely used to model temporal behavior in various settings that have interaction data. But these methods fail to capture the underlying intricate temporal dependencies. We therefore model time as a random variable and use temporal point process to model occurrence of fact.\nMore concretely, given a set of observed events O corresponding to a TKG, we construct a relationship-modulated multidimensional point process to model occurrence of these events. We characterize this point process with the following conditional intensity function:\n\u03bbe s,eo r (t|t\u0304) = f(ge s,eo r (t\u0304)) \u2217 (t\u2212 t\u0304) (3)\nwhere t > t\u0304, t is the time of the current event and t\u0304 = max(te\ns\u2212, teo\u2212) is the most recent time point when either subject or object entity was involved in an event before time t. Thus, \u03bbe s,eo\nr (t|t\u0304) represents intensity of event involving triplet (es, r, ej) at time t given previous time point t\u0304 when either es or eo was involved in an event. This modulates the intensity of current event based on most recent activity on either entities\u2019 timeline and allows to capture scenarios like non-periodic events and previously unseen events. In our case, f(\u00b7) = exp(\u00b7) ensures that intensity is positive and well defined."}, {"heading": "3.2. Relational Score Function", "text": "The first term in (3) modulates the intensity function by the relational compatibility score between the involved entities in that specific relationship. Specifically, for an event (es, r, eo, t) \u2208 D occurring at time t, the score term ges,eor is computed using a bilinear formulation as follows:\nge s,eo r (t) = v es(t\u2212)T \u00b7Rr \u00b7 ve o (t\u2212) (4)\nwhere ve s , ve s \u2208 Rd represent latent feature embeddings of entities appearing in subject and object position respectively. Rr \u2208 Rd\u00d7d represents relationship weight matrix which attempts to capture interaction between two entities in the specific relationship space r. This matrix is unique for each relation in dataset and is learned during training. t is the time of current event and t\u2212 represent time point just before time t. ve s\n(t\u2212) and veo(t\u2212), therefore represent most recently updated vector embeddings of subject and object entities respectively before time t. As these entity embeddings dynamically evolve and update over time, the relationship score is able to capture cumulative knowledge learned about the subjects and objects over the history of events that have affected their embeddings. We discuss these embedding processes in much detail in Section 3.3."}, {"heading": "3.3. Dynamically Evolving Entity Representations", "text": "We represent latent feature embedding of an entity e at time t with a low-dimensional vector ve(t). We add superscript s and o as shown in Eq. (4) to indicate if the embedding corresponds to entity in subject or object position respectively. We also use relationship-specific low-dimensional representation for each relation type.\nThe latent representations of entities change over time as entities forge relationships with each other. We design novel deep recurrent neural network based update functions to\ncapture mutually evolving and nonlinear dynamics of entities in their vector space representations. We again consider an event m = (es, r, eo, t)m \u2208 D occurring at time t. Also, consider that event m is entity es\u2019s p-th event while it is entity eo\u2019s q-th event. As entities participate in events in a heterogeneous pattern, it is less likely that p = q although not impossible. Having observed this event, we update the embeddings of two involved entities as follows:\nSubject Embedding:\nve s (tp) = \u03c3(W es t (tp \u2212 tp\u22121) + Whh \u00b7 he s\n(tp\u2212)) he s (tp\u2212) = \u03c3(Wh \u00b7 [ve s (tp\u22121)\u2295 ve o (tp\u2212)\u2295 re s p\u22121])\n(5)\nObject Embedding:\nve o (tq) = \u03c3(W eo t (tq \u2212 tq\u22121) + Whh \u00b7 he o\n(tq\u2212)) he o (tq\u2212) = \u03c3(Wh \u00b7 [ve o (tq\u22121)\u2295 ve s (tq\u2212)\u2295 re o q\u22121])\n(6)\nwhere, ve s , ve o \u2208 Rd. tp = tq = tm is the time of observed event. For subject embedding update in Eq. (5), tp\u22121 is the time point of the previous event in which entity es was involved. tp\u2212 is the timepoint just before time tp. Hence,\nve s\n(tp\u22121) represents latest embedding for entity es that was updated after (p \u2212 1)-th event for that entity. veo(tp\u2212) represents latest embedding for entity eo that was updated any time just before tp = tm. This accounts for the fact that entity eo may have been involved in some other event during the interval between current (p) and previous (p\u2212 1) event of entity es. re s\np\u22121 \u2208 Rc represent relationship embedding that corresponds to relationship type of the (p\u2212 1)-th event of entity es. Note that the relationship vectors are static and do not evolve over time. he s\n(tp\u2212) \u2208 Rd is the hidden layer. The semantics of notations apply similarly to object embedding update in Eq. (6)\nWe s t ,W eo t \u2208 Rd\u00d71, Whh \u2208 Rd\u00d7l and Wh \u2208 Rl\u00d7(2d+m) are weight parameters in network learned during training. We s\nt ,W eo\nt captures variation in temporal drift for subject and object respectively. Whh is shared parameter that captures recurrent participation effect for each entity. Wh is a shared projection matrix applied to consider the compatibility of entities in their previous relationships. \u2295 represent simple concatenation operator. \u03c3(\u00b7) denotes nonlinear activation function (tanh in our case). Our formulations use simple RNN units but it can be replaced with more expressive units like LSTM or GRU in straightforward manner.\nIn our experiments, we choose d = l and d 6= c but they can be chosen differently. Below we explain the rationales of our deep recurrent architecture that captures nonlinear evolutionary dynamics of entities over time.\nReasoning Based on Structural Dependency: The hidden layer (he s\n) reasons for an event by capturing the compatibility of most recent subject embedding with most recent object embedding in the previous relationship of subject entity. This accounts for the behavior that within a short period of time, entities tend to form relationships with other entities that have similar recent actions and goals. This layer thereby uses historical information of the two nodes involved in current event and the edges they both created before this event. This holds symmetrically for hidden layer (he o ).\nReasoning based on Temporal Dependency: The recurrent layer uses hidden layer information to model the intertwined evolution and co-evolution of embeddings over time. Specifically this layer has two main components:\n\u2022 Drift over time: The first term captures the temporal difference between consecutive events on respective dimension of each entity. This captures the external influences that entities may have experienced between events and allows to smoothly drift their features over time. This term will not contribute anything in case when the multiple events happen for an entity at same time point (e.g. within a day in our dataset). While tp \u2212 tp\u22121 may exhibit high variation, the corresponding weight parameter will capture these variations and along with the second recurrent term, it will prevent ve s (tp) to collapse.\n\u2022 Relation-specific Mutual Evolution: The latent features of both subject and object entities influence each other. In multi-relational setting, this is further affected by the relationship they form. Recurrent update to entity embedding with the information from the hidden layer allows to capture the intricate non-linear and evolutionary dynamics of an entity with respect to itself (self-evolution) and the other entity (co-evolution) in a specific relationship space."}, {"heading": "3.4. Understanding Unified View of Know-Evolve", "text": "Figure (2) and Figure (3) shows the architecture of knowledge evolution framework and one complete step of our model. We provide summary of all notations in Appendix.\nThe updates to the entity representations in Eq. (5) and (6) are driven by the events involving those entities which makes the embeddings piecewise constant i.e. an entity embedding remains unchanged in the duration between two events involving that entity and updates only when an event happens on its dimension. This is justifiable as an entity\u2019s features may update only when it forges a relationship with\nother entity within the graph. Note that the first term in Eq. (5) and (6) already accounts for any external influences.\nHaving observed an event at time t, Know-Evolve considers it as an incoming fact that brings new knowledge about the entities involved in that event. It computes the intensity of that event in Eq. (3) which is based on relational compatibility score in Eq. (4) between most recent latent embeddings of involved entities. As these embeddings are piecewise constant, we use time interval term (t\u2212 t\u0304) in Eq. (3) to make the overall intensity piecewise linear which is standard mathematical choice for efficient computation in point process framework. This formulation naturally leads to Rayleigh distribution which models time interval between current event and most recent event on either entities\u2019 dimension. Rayleigh distribution has an added benefit of having a simple analytic form of likelihood which can be further used to find entity for which the likelihood reaches maximum value and thereby make precise entity predictions."}, {"heading": "4. Efficient Training Procedure", "text": "The complete parameter space for the above model is: \u2126 = {Vee=1:ne , {Rk}r=1:nr ,We,We s t ,W eo t ,W h,Whh,Wr}. Although our model gains expressive power from recurrent latent features and temporal features, Table 1 demonstrates that the memory footprint of our model is comparable to simple relational models with less expressive power. The well-defined intensity function in (3) allows us to use maximum likelihood estimation over all the events (facts) as our objective to learn the parameters of our model. Concretely, given a collection of facts recorded in a temporal window [0, T ), we learn the model by minimizing the joint negative log likelihood of conditional intensity function (Daley & Vere-Jones, 2007) written as:\nL = \u2212 N\u2211 p=1 log ( \u03bbe s,eo r (tp|t\u0304p) )\n\ufe38 \ufe37\ufe37 \ufe38 happened events\n+ r\u2211 r=1 n\u2211 es=1 n\u2211 eo=1 \u222b T 0 \u03bbe s,eo\nr (\u03c4 |\u03c4\u0304) d\u03c4\ufe38 \ufe37\ufe37 \ufe38 survival term\n(7)\nThe first term maximizes the probability of specific type of event between two entities; the second term penalizes nonpresence of all possible types of events between all possible entity pairs in a given observation window. This formulation deviates from the conventional method of directly using relationship score (e.g. using max-margin objective functions) to estimate parameters.\nWe use Back Propagation Through Time (BPTT) algorithm to train our model. With our framework and intensity based objective function, we face two major technical challenges: Non-decomposable event sequence. Previous techniques\n(Du et al., 2016; Hidasi et al., 2016) that use BPTT algorithm decompose data into independent sequences and train on mini-batches of those sequences. As we model dyadic relationships between dynamically evolving entities, there exists intricate relational and temporal dependencies between data points. This fact limits our ability to do efficiently train by decomposing events into independent sequences.\nTo address this challenge, we first sort the complete event set on global timeline and create mini-batches in sliding window fashion. During each step of training, we build computational graph using consecutive events in the sliding window of a fixed size. We then move sliding window further and train till the end of timeline in similar fashion. Algorithm 2 illustrates SGD-based Global BPTT training for our model.\nIntractable Survival Term. To compute the second survival term in (7), since our intensity function is modulated by relation-specific parameter, for each relationship we need to compute survival probability over all pairs of entities. Next, given a relation rk and entity pair (es, eo), we denote P(es,eo) as total number of events of type r involving either es or eo in window [T0, T ). As our intensity function is piecewise-linear, we can decompose the integration term\u2212 \u222b T T0 \u03bbe s,eo\nr (\u03c4 |\u03c4\u0304)d\u03c4 into multiple time intervals where intensity is constant:\u222b T\nT0\n\u03bbe s,eo\nr (\u03c4 |\u03c4\u0304)d\u03c4\n= P(es,eo)\u22121\u2211 p=1 \u222b tp+1 tp \u03bbe s,eo r (\u03c4 |\u03c4\u0304)d\u03c4\n= P(es,eo)\u22121\u2211 p=1 (t2p+1 \u2212 t2p) \u00b7 exp(ve s (tp) T \u00b7Rr \u00b7 ve o (tp))\n(8)\nThe integral calculations in (8) for all possible triplets requiresO(n2r) computations (n is number of entities and r is the number of relations). This is computationally intractable and also unnecessary. Knowledge tensors are inherently sparse with very few highly active nodes and it is plausible to approximate the survival loss in a stochastic setting to make it computationally tractable. We take inspiration from\nAlgorithm 1 Survival Loss Computation in mini-batch Input: Minibatch E , size s, Batch Entity List bl loss = 0.0 for p = 0 to s\u2212 1 do\nsubj feat = Ep \u2192 ve s\n(t\u2212) obj feat = Ep \u2192 ve o\n(t\u2212) rel weight = Ep \u2192 Rr(t) t end = Ep \u2192 t subj surv = 0, obj surv = 0, total surv = 0 for i = 0 to bl.size do\nobj other = bl[i] if obj other == Ep \u2192 es then\ncontinue end if t\u0304 = max(te\ns\u2212, teo\u2212) subj surv += (t end2 \u2212 t\u03042) \u00b7 exp(subj featT \u00b7 rel weight \u00b7 obj other feat)\nend for for j = 0 to bl.size do\nsubj other = bl[i] if subj other == Ep \u2192 eo then\ncontinue end if t\u0304 = max(te\ni\u2212, tej\u2212) obj surv += (t end2 \u2212 t\u03042) \u00b7 exp(subj other featT \u00b7 rel weight \u00b7 obj feat)\nend for loss += subj surv + obj surv\nend for\ntechniques like noise contrastive (Gutmann & Hyva\u0308rinen, 2012) estimation and adopt a simple random sampling strategy to compute overall survival loss: Given a mini-batch of events, for each relation in the mini-batch, we compute dyadic survival term across all entities in that batch. Algorithm 1 presents the survival loss computation procedure. While this procedure may randomly avoid penalizing some dimensions in a specific relationship, it still includes all dimensions that had events on them. The computational complexity for this procedure will be O(2n\u2032r\u2032m) where m is size of mini-batch and n\u2032 and r\u2032 represent number of entities and relations in the mini-batch."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Temporal Knowledge Graph Data", "text": "We use two datasets: Global Database of Events, Language, and Tone (GDELT) (Leetaru & Schrodt, 2013) and Integrated Crisis Early Warning System (ICEWS) (Boschee et al., 2017) which has recently gained attention in learning community (Schein et al., 2016) as useful temporal KGs. GDELT data is collected from April 1, 2015 to Mar 31, 2016 (temporal granularity of 15 mins). ICEWS dataset is\ncollected from Jan 1, 2014 to Dec 31, 2014 (temporal granularity of 24 hrs). Both datasets contain records of events that include actors, action associated with the event and timestamp of event. We use different hierarchy of actions in two datasets - (top level 20 relations for GDELT while last level 260 relations for ICEWS) - to test on variety of knowledge tensor configurations. Note that this does not filter any record from the dataset. We process both datasets to remove any duplicate quadruples, any mono-actor events (i.e., we use only dyadic events), and self-loops.\nWe report our main results on full versions of each dataset. We create smaller version of both datasets to use for exploration purposes. Table 2 provide statistics about the data and Table 3 demonstrates the sparsity of knowledge tensor."}, {"heading": "5.2. Competitors", "text": "We compare the performance of our method with following relational learning methods: RESCAL, Neural Tensor Network (NTN), Multiway Neural Network, TransE and TransR. To the best of our knowledge, there are no existing relational learning approaches that can predict time for a new fact. Hence we devised two baseline methods for evaluating time prediction performance:\nMulti-dimensional Hawkes process (MHP): We model dyadic entity interactions as multi-dimensional Hawkes process similar to (Du et al., 2015). Here, an entity pair constitutes a dimension and for each pair we collect sequence of events on its dimension and train and test on that sequence. Relationship is not modelled in this setup.\nRecurrent Temporal Point Process (RTPP): We implement a simplified version of RMTPP (Du et al., 2016) where we do not predict the marker. For training, we concatenate static entity and relationship embeddings and augment the resulting vector with temporal feature. This augmented unit is used as input to global RNN which produces output vector ht. During test time, for a given triplet, we use this vector ht to compute conditional intensity of the event given history which is further used to predict the time for given triplet."}, {"heading": "5.3. Evaluation metric", "text": "We report experimental results on two tasks: Link prediction and Time prediction.\nLink prediction: Given a test quadruplet (es, r, eo, t), we replace eo with all the entities in the dataset and compute the conditional density de s,eo\nr = \u03bb es,eo r (t)S es,eo\nr (t) for the resulting quadruplets including the ground truth. We then sort\nall the quadruplets in the descending order of this density to rank the correct entity for object position. This procedure can be symmetrically computed for both subject and object. Next, we also conduct testing after applying the filtering techniques described in (Bordes et al., 2013) - we only rank against the entities that do not generate a true triplet (seen in train) when it replaces ground truth object. We report Mean Average Rank, Standard Deviation for MAR across test samples and HITS@10 (correct entity in top 10 predictions) for both Raw and Filtered Versions.\nTime prediction: Give a test triplet (es, r, eo), we predict the expected value of next time the fact (es, r, eo) can occur. This expectation is defined by: Ees,eor (t) =\u221a\n\u03c0\n2 exp(ge s,eo r (t)) , where ge\ns,eo\nr (t) is computed using equa-\ntion (4). We report Mean Absolute Error (MAE) between the predicted time and true time in hours.\nSliding Window Evaluation. As our work concentrates on temporally evolving knowledge graphs, it is more interesting to see the performance of methods across time span of test set as compared to single rank value. This evaluation method can help to realize the effect of modelling temporal and evolutionary knowledge. For sliding window testing, we partition our test set in 12 different slides and test in each window. For both datasets, each such slide included 2 weeks of time. Appendix C provides complete implementation details of our method and competitors."}, {"heading": "5.4. Quantitative Analysis", "text": "Link Prediction Results. Figure (4, 5, 6) demonstrate link prediction performance comparison on both datasets. Our method significantly outperforms all the competitors in both metrics (MAR and HITS@10). Neural Tensor Network (NTN) shows second best performance as compared to other baselines. This demonstrates expressive power of NTN but it fails to capture the evolving dynamics of intricate dependencies over time. This is further substantiated by its decreasing performance as we move test window further in time. Our method performs consistently and significantly better without any deterioration over time and uses comparable memory resources making it both effective and scalable\nThe second row represents deviation error for the mean average rank across samples in a given test window. Our method achieves significantly low deviation error compared to competitors making it most stable. Finally, our high performance on HITS@10 metric demonstrates extensive discriminative ability of our method. For instance, GDELT has only 20 relations but 32M events where many entities interact with each other as both subject and object in multiple relationships. In this complex setting, other methods only depend on static entity and relationship embedding to perform prediction. Instead, our method uses the powerful evolutionary network to consider evolution of entities to\nmake the best possible decision.\nUnlike competitors, the entity embeddings in our model get updated after every event in the test, but the model parameters remain unchanged after training. To balance out the advantage that this may give to our method, we explore the use of sliding window training paradigm for baselines: We train on first six months of dataset and evaluate on the first test window. Next we throw away as many days (2 weeks) from start of train set as found in test set and incorporate the test data into training. We retrain the model using previously learned parameters as warm start. This can effectively aid the baselines to adapt to the evolving knowledge over time. Figure 7 shows that the sliding window training contributes to stable performance of baselines across the time window (i.e.the temporal deterioration is no longer observed significantly for baselines). But the overall performance of our method still surpasses all the competitors.\nTime Prediction Results. Figure 8 shows time prediction results for our method and demonstrates that Know-Evolve significantly outperforms other point process based methods for predicting time. MHP uses a specific parametric form of the intensity function which limits its expressiveness to capture real world temporal patterns. Further, each entity pair interaction is modelled as an independent dimension which fails to capture the intricate influence of different entities on each other. Finally, it does not take into account relationship feature which also contributes to time of an event. On the other hand, RTPP uses relationship features as part of input, but it sees all the events globally and cannot model the intricate temporal and evolutionary dependencies on the past events effectively. We observe that our method is able to capture the non-linear relational and temporal dependence\nbetween dynamically evolving entities effectively.\nIn addition to the superior quantitative performance, we demonstrate the effectiveness of our method by providing extensive exploratory analysis on two real-world case studies in Appendix D."}, {"heading": "6. Related Work", "text": "In this section, we discuss relevant works in relational learning and recent work that considers temporal features in relational data."}, {"heading": "6.1. Relational Learning", "text": "Among various relational learning techniques, neural embedding models that focuses on learning low-dimensional representations of entities and relations have shown stateof-the-art performance. Specifically, these methods embed entities and relations into low-dimensional continuous vector space and learn a score for the fact based on different operations on these new representations. To learn these representations, either logistic loss or pairwise ranking loss objective is minimized using stochastic gradient descent algorithms (Nickel et al., 2016a). Such models can be mainly categorized into two variants:\nCompositional Models. Factorization models like RESCAL (Nickel et al., 2011) and Neural Tensor Network (NTN) (Socher et al., 2013) uses tensor product to capture interactions between entities and relations. RESCAL uses a relation specific weight matrix to explain triplets via pairwise interactions of latent features. NTN is more expressive model as it combines a standard linear neural network layer with a bilinear tensor layer but it suffers from scalability issues. (Dong et al., 2014) employs a concatenation-projection method to project entities and relations to lower dimensional space. Other sophisticated models include Holographic Embeddings (HoLE) (Nickel et al., 2016b) that employs circular correlation on entity embeddings and Neural Association Models (NAM) (Liu et al., 2016),a deep network used for probabilistic reasoning.\nTranslation Based Models. (Bordes et al., 2011) defines a score function as an L1 distance between two entity vectors projected into relationship space. To alleviate memory complexity associated with that model, (Bordes et al., 2013) proposed TransE model that computes the score as a distance between relation-specific translations of entity embeddings. (Wang et al., 2014) improved TransE by allowing entities to have relation specific distributed representations on a relation specific hyperplane where distance between them is computed. TransR (Lin et al., 2015) extends this model to use separate semantic spaces for entities and relations and does translation in the relationship space.\n(Nickel et al., 2016a) contains comprehensive reviews of relational learning techniques and to (Yang et al., 2015;\nToutanova & Chen, 2015) contains empirical comparison between various methods. While all these models have shown varying performance on different tasks and knowledge graphs, their main focus is missing link prediction and relation classification. Also, these methods consider knowledge graphs as static models and lack the ability to capture dynamic evolution of entity embedding. Further, most existing embedding methods incurs a trade-off between expressive power and scalability. We show in Table 4 and our experiments that our evolutionary network is both highly scalable and expressive at same time."}, {"heading": "6.2. Temporal features in relational models", "text": "Recently, (Esteban et al., 2016) proposed multiway neural network based model for task of event prediction in event based relational graph. In this paper, the authors draw a synergistic relation between a static knowledge graph and an event set based on that graph wherein the knowledge graph provide information about entities participating in events and new events in turn contribute to enhancement of knowledge graph. They do not capture the evolving dynamics of entities. Further, they model time as discrete points and adopts time series based approach which limits the capacity to model complex temporal dynamics. (Jiang et al., 2016) proposes a method for time-aware link prediction but they model dependence of relationship on time. The do not capture nonlinearly evolving entity embedding dynamics."}, {"heading": "7. Conclusion", "text": "We propose a deep novel knowledge evolutionary network that can efficiently learn dynamically evolving entity embeddings and capture intricate temporal dependencies in multi-relational setting. Evolutionary dynamics of both subject and object entities are captured by deep recurrent neural network that learns from historical evolution of entity embeddings in a specific relationship space. We demonstrate superior performance and high scalability of our method through extensive experiments on large real world event graphs. Therefore, it validates the importance of modelling temporal dynamics in dynamically evolving relational processes. Our model establishes previously unexplored connection between relational processes and temporal point processes and it has the potential to open a new direction of research on reasoning over time."}, {"heading": "A. Algorithm for Global BPTT Computation", "text": "Algorithm 2 Global-BPTT Input: Global Event Sequence O, Steps s, Stopping Condition max iter cur index = 0, t begin = 0 for iter = 0 to max iter do\nif cur index > 0 then t begin = O[cur index\u2212 1]\u2192 t end if e mini batch = O[cur index : cur index+ s] Build Training Network specific to e mini batch Feed Forward inputs over network of s time steps Compute Total Loss L over s steps: L = \u2212 \u2211s p=1 log ( \u03bbe s,eo r (tp|t\u0304p) )\n+ Survival loss computed using Algorithm 1 Backpropagate error through s time steps and update all weights if cur index+ s > O.size then cur index = 0 else cur index = cur index+ s\nend if end for"}, {"heading": "B. Data Statistics and Sparsity of Knowledge Tensor", "text": "Table 3. Sparsity of Knowledge Tensor.\nDataset Name # Possible # Available % Proportion Entries Entries\nGDELT-full 3.93B 4.52M 0.12 GDELT-500 5M 0.76M 15.21 ICEWS-full 39.98B 0.31M 7e-3 ICEWS-500 64M 0.15M 0.24\nC. Implementation Details Know-Evolve. Both Algorithm 1 and Algorithm 2 demonstrate that the computational graph for each mini-batch will be significantly different due to high variations in the interactions happening in each window. To facilitate efficient training over dynamic computational graph setting, we leverage on graph embedding framework proposed in (Dai et al., 2016) that allows to learn over graph structure where the objective function may potentially have different computational graph for each batch. We use Adam Optimizer with gradient clipping for making parameter updates. Using grid search method across hyper-parameters, we set mini-batch size = 200, weight scale = 0.1 and learning rate = 0.0005 for all datasets. We used zero initialization for our entity embeddings which is reasonable choice for dynamically evolving entities.\nCompetitors. We implemented all the reported baselines in Tensorflow and created a unified package to evaluate all methods uniformly. For each method, we use grid search on hyper-parameters and embedding size and chose the ones providing best performance in respective methods. All the baseline methods are trained using contrastive max-margin objective function described in (Socher et al., 2013). We use Adagrad optimization provided in Tensorflow for optimizing this objective function. We randomly initialize entity embeddings as typically done for these models."}, {"heading": "D. Exploratory Analysis", "text": "D.1. Temporal Reasoning\nWe have shown that our model can achieve high accuracy when predicting a future event triplet or the time of event. Here, we present two case studies to demonstrate the ability of evolutionary knowledge network to perform superior transitive reasoning across multiple relationships in the knowledge graphs.\nCASE STUDY I: ENEMY\u2019S FRIENDS IS AN ENEMY\nWe considered a test quadruplet (Istanbul, Fight, South Korean, 10/03/2015). Our model gives rank-1 to the object entity South Korean while all the other baselines do not predict well (rank > 250). The above test event is a new relationship and was never seen in training. This case is interesting as there are atleast 5 different entities involved in intertwined relationships which makes a very challenging test case to predict. The story associated with the event involved an attack on South Korean people in Istanbul during a protest. The interesting part is that the protester considered those South Korean citizens as Chinese and they were protesting again China due to problems with Uighur community. This fact is represented by the edge in our test set. Figure 9 shows some sample edges between these countries. Our prediction result show that we are able to capture this hidden nonlinear and intricate evolution and mutual influence between entities which helps to reason over multiple paths and get us the correct result.\nOur model successfully captures this intertwined chain of reasoning and provide better prediction as compared to baselines. A further possible work in the above examples is to investigate longer path of reasoning.\nCASE STUDY II: COMMON ENEMY FORGES FRIENDSHIP\nWe concentrate on the prediction of a quadruplet (Cairo,Assault,Croatia,July 5,2015) available in test set. This event relates to the news report of an assault on a Croation prisoner in Cairo on July 6 2015. Our model gives rank-1 to the object entity Croatia while the baselines did not predict them well (rank > 250).\nWe first consider one-hop transitive reasoning between Cairo and Croatia. Please note that the edges should be considered directed as our task focuses on predicting object given subject and relationship pair. In the current train span, there are 142 nodes for which Cairo was involved in a relationship with that node as a subject (total of 1369 events) and Croatia was involved in a relationship with those nodes as an object (total of 1037 events). As a subject, Cairo was involved in an assault relationship only 59 times while as an object, Croatia was involved in assault only 5 times. As mentioned earlier, there was no direct edge present between Cairo and Croatia with relationship type assault. Figure 10 shows 2-hop graph for these entities.\nWhile the conventional reasoning methods consider the static interactions of entities in a specific relationship space, they fail to account for the temporal effect on certain relationships and dynamic evolution of entity embeddings. We believe that our method is able to capture this multi-faceted knowledge that helps to reason better than the competitors for the above case.\nTemporal Effect. It is observed in the dataset that many entities were involved more in negative relationships in the last month of training data as compared to earlier months of the year. Further, a lot of assault activities on foreign prisoners were being reported in Cairo starting from May 2015. Our model successfully captures this increased intensity of such events in recent past. The interesting observation is that overall, Cairo has been involved in much higher number of positive relationships as compared to negative ones and that would lead conventional baselines to use that path to reason for new entity \u2013 instead our model tries to capture effect of most recent events.\nDynamic Knowledge Evolution. It can be seen from the dataset that Cairo got associated with more and more negative events towards the mid of year 2015 as compared to start of the year where it was mostly involved in positive and cooperation relationships. While this was not very prominent in case of Croatia, it still showed some change in the type of relationships over time. There were multiple where Cairo was involved in a negative relationship with a node which in turn had positive relationship with Croatia. This signifies that the features of the two entities were jointly and non-linearly evolving with the features of the third entity in different relationship spaces.\nD.2. Recurrent Facts vs. New facts\nOne fundamental distinction in our multi-relational setting is the existence of recurrence relations which is not the case for traditional knowledge graphs. To that end, we compare our method with the best performing competitor - NTN on two different testing setups: 1.) Only Recurrent Facts in test set 2.) Only New facts in test set. We perform this experiment on GDELT-500 data. Figure 11 demonstrates that our method performs consistently and significantly better in both cases."}, {"heading": "E. Full Memory Complexity Table", "text": ""}], "references": [{"title": "Survival and event history analysis: a process point of view", "author": ["Aalen", "Odd", "Borgan", "Ornulf", "Gjessing", "Hakon"], "venue": null, "citeRegEx": "Aalen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Aalen et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes", "Antoine", "Weston", "Jason", "Collobert", "Ronan", "Bengio", "Yoshua"], "venue": "In Conference on Artificial Intelligence, number EPFL-CONF-192344,", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Multivariate point processes. Selected Statistical Papers of Sir David Cox: Volume 1, Design of Investigations", "author": ["D.R. Cox", "P.A.W. Lewis"], "venue": "Statistical Methods and Applications,", "citeRegEx": "Cox and Lewis,? \\Q2006\\E", "shortCiteRegEx": "Cox and Lewis", "year": 2006}, {"title": "Discriminative embeddings of latent variable models for structured data", "author": ["Dai", "Hanjun", "Bo", "Song", "Le"], "venue": "In ICML,", "citeRegEx": "Dai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2016}, {"title": "An introduction to the theory of point processes: volume II: general theory and structure, volume", "author": ["D.J. Daley", "D. Vere-Jones"], "venue": null, "citeRegEx": "Daley and Vere.Jones,? \\Q2007\\E", "shortCiteRegEx": "Daley and Vere.Jones", "year": 2007}, {"title": "Time sensitive recommendation from recurrent user activities", "author": ["Du", "Nan", "Wang", "Yichen", "He", "Niao", "Song", "Le"], "venue": "In NIPS,", "citeRegEx": "Du et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "Recurrent marked temporal point processes: Embedding event history to vector", "author": ["Du", "Nan", "Dai", "Hanjun", "Trivedi", "Rakshit", "Upadhyay", "Utkarsh", "Gomez-Rodriguez", "Manuel", "Song", "Le"], "venue": null, "citeRegEx": "Du et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Du et al\\.", "year": 2016}, {"title": "Predicting the co-evolution of event and knowledge graphs", "author": ["Esteban", "Cristobal", "Tresp", "Volker", "Yang", "Yinchong", "Baier", "Stephan", "Krompa", "Denis"], "venue": "In 2016 19th International Conference on Information Fusion (FUSION),", "citeRegEx": "Esteban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Esteban et al\\.", "year": 2016}, {"title": "Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Michael U", "Hyv\u00e4rinen", "Aapo"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Spectra of some self-exciting and mutually exciting point processes", "author": ["Hawkes", "Alan G"], "venue": null, "citeRegEx": "Hawkes and G.,? \\Q1971\\E", "shortCiteRegEx": "Hawkes and G.", "year": 1971}, {"title": "Session-based recommendations with recurrent neural networks", "author": ["Hidasi", "Balazs", "Karatzoglou", "Alexandros", "Baltrunas", "Linas", "Tikk", "Domonkos"], "venue": "In ICLR,", "citeRegEx": "Hidasi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hidasi et al\\.", "year": 2016}, {"title": "A self-correcting pint process", "author": ["V. Isham", "M. Westcott"], "venue": "Advances in Applied Probability,", "citeRegEx": "Isham and Westcott,? \\Q1979\\E", "shortCiteRegEx": "Isham and Westcott", "year": 1979}, {"title": "Encoding temporal information for time-aware link", "author": ["Jiang", "Tingsong", "Liu", "Tianyu", "Ge", "Tao", "Lei", "Sha", "Li", "Suijan", "Chang", "Baobao", "Sui", "Zhifang"], "venue": null, "citeRegEx": "Jiang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "Gdelt: Global data on events, location, and tone", "author": ["Leetaru", "Kalev", "Schrodt", "Philip A"], "venue": "ISA Annual Convention,", "citeRegEx": "Leetaru et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Leetaru et al\\.", "year": 2013}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin", "Yankai", "Liu", "Zhiyuan", "Sun", "Maosong", "Zhu", "Xuan"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Probabilistic reasoning via deep learning: Neural association models", "author": ["Liu", "Quan", "Jiang", "Hui", "Evdokimov", "Andrew", "Ling", "ZhenHua", "Zhu", "Xiaodan", "Wei", "Si", "Hu", "Yu"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "A three-way model for collective learning on multirelational data", "author": ["Nickel", "Maximilian", "Tresp", "Volker", "Kriegel", "HansPeter"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "A review of relational machine learning for knowledge graphs", "author": ["Nickel", "Maximilian", "Murphy", "Kevin", "Tresp", "Volker", "Gabrilovich", "Evgeniy"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Nickel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "Holographic embeddings of knowledge graphs. 2016b", "author": ["Nickel", "Maximilian", "Rosasco", "Lorenzo", "Poggio", "Tomaso"], "venue": null, "citeRegEx": "Nickel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "Bayesian poisson tucker decomposition for learning the structure of international relations", "author": ["Schein", "Aaron", "Zhou", "Mingyuan", "Blei", "David", "Wallach", "Hanna"], "venue": null, "citeRegEx": "Schein et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schein et al\\.", "year": 2016}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Observed versus latent features for knowledge base and text inference", "author": ["Toutanova", "Kristina", "Chen", "Danqi"], "venue": null, "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Isotonic hawkes processes", "author": ["Wang", "Yichen", "Xie", "Bo", "Du", "Nan", "Song", "Le"], "venue": "In ICML,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang", "Zhen", "Zhang", "Jianwen", "Feng", "Jianlin", "Chen", "Zheng"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang", "Bishan", "Yih", "Wen-tau", "He", "Xiaodong", "Gao", "Jianfeng", "Deng", "Li"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "From the survival analysis theory (Aalen et al., 2008), given the history T = {t1, .", "startOffset": 34, "endOffset": 54}, {"referenceID": 23, "context": "Some Common forms include: Poisson Process, Hawkes processes (Hawkes, 1971; Wang et al., 2016), Self-Correcting Process (Isham & Westcott, 1979), Power Law and Rayleigh Process.", "startOffset": 61, "endOffset": 94}, {"referenceID": 7, "context": "(Du et al., 2016; Hidasi et al., 2016) that use BPTT algorithm decompose data into independent sequences and train on mini-batches of those sequences.", "startOffset": 0, "endOffset": 38}, {"referenceID": 11, "context": "(Du et al., 2016; Hidasi et al., 2016) that use BPTT algorithm decompose data into independent sequences and train on mini-batches of those sequences.", "startOffset": 0, "endOffset": 38}, {"referenceID": 20, "context": ", 2017) which has recently gained attention in learning community (Schein et al., 2016) as useful temporal KGs.", "startOffset": 66, "endOffset": 87}, {"referenceID": 6, "context": "Hence we devised two baseline methods for evaluating time prediction performance: Multi-dimensional Hawkes process (MHP): We model dyadic entity interactions as multi-dimensional Hawkes process similar to (Du et al., 2015).", "startOffset": 205, "endOffset": 222}, {"referenceID": 7, "context": "Recurrent Temporal Point Process (RTPP): We implement a simplified version of RMTPP (Du et al., 2016) where we do not predict the marker.", "startOffset": 84, "endOffset": 101}, {"referenceID": 2, "context": "Next, we also conduct testing after applying the filtering techniques described in (Bordes et al., 2013) - we only rank against the entities that do not generate a true triplet (seen in train) when it replaces ground truth object.", "startOffset": 83, "endOffset": 104}, {"referenceID": 17, "context": "Factorization models like RESCAL (Nickel et al., 2011) and Neural Tensor Network (NTN) (Socher et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 21, "context": ", 2011) and Neural Tensor Network (NTN) (Socher et al., 2013) uses tensor product to capture interactions between entities and relations.", "startOffset": 40, "endOffset": 61}, {"referenceID": 16, "context": ", 2016b) that employs circular correlation on entity embeddings and Neural Association Models (NAM) (Liu et al., 2016),a deep network used for probabilistic reasoning.", "startOffset": 100, "endOffset": 118}, {"referenceID": 1, "context": "(Bordes et al., 2011) defines a score function as an L1 distance between two entity vectors projected into relationship space.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "To alleviate memory complexity associated with that model, (Bordes et al., 2013) proposed TransE model that computes the score as a distance between relation-specific translations of entity embeddings.", "startOffset": 59, "endOffset": 80}, {"referenceID": 24, "context": "(Wang et al., 2014) improved TransE by allowing entities to have relation specific distributed representations on a relation specific hyperplane where distance between them is computed.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "TransR (Lin et al., 2015) extends this model to use separate semantic spaces for entities and relations and does translation in the relationship space.", "startOffset": 7, "endOffset": 25}, {"referenceID": 25, "context": ", 2016a) contains comprehensive reviews of relational learning techniques and to (Yang et al., 2015; Toutanova & Chen, 2015) contains empirical comparison between various methods.", "startOffset": 81, "endOffset": 124}, {"referenceID": 8, "context": "Recently, (Esteban et al., 2016) proposed multiway neural network based model for task of event prediction in event based relational graph.", "startOffset": 10, "endOffset": 32}, {"referenceID": 13, "context": "(Jiang et al., 2016) proposes a method for time-aware link prediction but they model dependence of relationship on time.", "startOffset": 0, "endOffset": 20}], "year": 2017, "abstractText": "Knowledge Graphs are important tools to model multi-relational data that serves as information pool for various applications. Traditionally, these graphs are considered to be static in nature. However, recent availability of large scale event-based interaction data has given rise to dynamically evolving knowledge graphs that contain temporal information for each edge. Reasoning over time in such graphs is not yet well understood. In this paper, we present a novel deep evolutionary knowledge network architecture to learn entity embeddings that can dynamically and nonlinearly evolve over time. We further propose a multivariate point process framework to model the occurrence of a fact (edge) in continuous time. To facilitate temporal reasoning, the learned embeddings are used to compute relationship score that further parametrizes intensity function of the point process. We demonstrate improved performance over various existing relational learning models on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to any prior reasoning approaches in multi-relational setting.", "creator": "LaTeX with hyperref package"}}}