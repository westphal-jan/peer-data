{"id": "1705.07041", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Posterior sampling for reinforcement learning: worst-case regret bounds", "abstract": "barrena We longest-lasting present krasinski an algorithm based 217.8 on posterior ecco sampling (1011 aka jumpstart Thompson refocused sampling) 28.87 that metalurgs achieves jesu near - ihi optimal 0-for-7 worst - prepstar case trato regret bounds headlee when 2-all the feh underlying grocer Markov mahm Decision Process (MDP) is chequy communicating declassifies with a t55 finite, moissanite though queuing unknown, ceq diameter. Our hogle main karamanids result is a high birla probability regret co-artistic upper karaeskaki bound of $ \\ 2,789 tilde {co-pastor O} (D \\ re-modelled sqrt {rhondda SAT} ) $ hengshui for orpiment any manitoban communicating MDP confortable with $ S $ 215,000 states, $ dealu A $ rock/pop actions producto and pejic diameter $ secretarial D $, when $ dungeness T \\ ge S ^ 5A $. dentons Here, parinirvana regret mollica compares code-switching the anette total reward achieved by bel the algorithm to the deke total expected liquid-filled reward aktiengesellschaft of cengkareng an optimal aggresively infinite - dhammika horizon invertebrate undiscounted senlin average gyr reward policy, in mudan time 46.05 horizon $ pitchfork T $. mcmaster This wildcards result compa\u00f1ia improves treponema over homerton the fryxell best previously ibomed known foard upper wiremu bound maggots of $ \\ courtauld tilde {rap/hip-hop O} (musical/comedy DS \\ tebessa sqrt {AT} ) $ camelback achieved pre-assembled by jagath any algorithm in armloads this 60-80 setting, brightens and almanza matches 24,000-member the azhwars dependence inlet on $ jakrapob S $ 121.4 in preemies the established 71.05 lower bound of $ \\ bl\u0161any Omega (\\ 9,260 sqrt {DSAT} ) $ for rhinoceros this problem. 2/2 Our techniques involve 88.85 proving lr3 some mahal novel results l'administration about 99.52 the privados anti - concentration scriptwriter of reinjure Dirichlet shigeaki distribution, which sarlat may be awn of olimpia independent \u017eivkovi\u0107 interest.", "histories": [["v1", "Fri, 19 May 2017 15:10:21 GMT  (48kb)", "http://arxiv.org/abs/1705.07041v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shipra agrawal", "randy jia"], "accepted": true, "id": "1705.07041"}, "pdf": {"name": "1705.07041.pdf", "metadata": {"source": "CRF", "title": "Posterior sampling for reinforcement learning: worst-case regret bounds", "authors": ["Shipra Agrawal"], "emails": ["sa3305@columbia.edu", "rqj2000@columbia.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n07 04\n1v 1\n[ cs\n.L G\n] 1\n9 M\nay 2\neter. Our main result is a high probability regret upper bound of O\u0303(D \u221a SAT ) for any communicating MDP with S states, A actions and diameter D, when T \u2265 S5A. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon T . This result improves over the best previously known upper bound of O\u0303(DS \u221a AT ) achieved by any algorithm in this setting, and matches the dependence on S in the established lower bound of \u2126( \u221a DSAT ) for this problem. Our techniques involve proving some novel results about the anticoncentration of Dirichlet distribution, which may be of independent interest."}, {"heading": "1 Introduction", "text": "Reinforcement Learning (RL) refers to the problem of learning and planning in sequential decision making systems when the underlying system dynamics are unknown, and may need to be learned by trying out different options and observing their outcomes. A typical model for the sequential decision making problem is a Markov Decision Process (MDP), which proceeds in discrete time steps. At each time step, the system is in some state s, and the decision maker may take any available action a to obtain a (possibly stochastic) reward. The system then transitions to the next state according to a fixed state transition distribution. The reward and the next state depend on the current state s and the action a, but are independent of all the previous states and actions. In the reinforcement learning problem, the underlying state transition distributions and/or reward distributions are unknown, and need to be learned using the observed rewards and state transitions, while aiming to maximize the cumulative reward. This requires the algorithm to manage the tradeoff between exploration vs. exploitation, i.e., exploring different actions in different states in order to learn the model more accurately vs. taking actions that currently seem to be reward maximizing.\nExploration-exploitation tradeoff has been studied extensively in the context of stochastic multiarmed bandit (MAB) problems, which are essentially MDPs with a single state. The performance of MAB algorithms is typically measured through regret, which compares the total reward obtained by the algorithm to the total expected reward of an optimal action. Optimal regret bounds have been established for many variations of MAB (see Bubeck et al. [2012] for a survey), with a large majority of results obtained using the Upper Confidence Bound (UCB) algorithm, or more generally, the optimism in the face of uncertainty principle. Under this principle, the learning algorithm maintains tight over-estimates (or optimistic estimates) of the expected rewards for individual actions, and at any given step, picks the action with the highest optimistic estimate. More recently, posterior sampling, aka Thompson Sampling [Thompson, 1933], has emerged as another popular algorithm design principle in MAB, owing its popularity to a simple and extendible algorithmic structure, an attractive empirical performance [Chapelle and Li, 2011, Kaufmann et al., 2012], as well as prov-\nably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value.\nWe consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s\u2032 using an appropriate policy, for each pair s, s\u2032. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of O\u0303(DS \u221a AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of \u2126( \u221a DSAT ) on the regret of any algorithm for this problem.\nOur main contribution is a posterior sampling based algorithm with a high probability worst-case regret upper bound of O\u0303(D \u221a SAT + DS7/4A3/4T 1/4), which is O\u0303(D \u221a SAT ) when T \u2265 S5A. This improves the previously best known upper bound for this problem by a factor of \u221a S, and matches the dependence on S in the lower bound, for large enough T .\nOur algorithm uses an \u2018optimistic version\u2019 of the posterior sampling heuristic, while utilizing several ideas from the algorithm design structure in Jaksch et al. [2010], such as an epoch based execution and the extended MDP construction. The algorithm proceeds in epochs, where in the beginning of\nevery epoch, it generates \u03c8 = O\u0303(S) sample transition probability vectors from a posterior distribution for every state and action, and solves an extended MDP with \u03c8A actions and S states formed using these samples. The optimal policy computed for this extended MDP is used throughout the epoch. Posterior Sampling for Reinforcement Learning (PSRL) approach has been used previously in Osband et al. [2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1\nWe should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of O\u0303( \u221a HSAT ) when T \u2265 H3S3A. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper.\nAmong other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al. [2009], Dann and Brunskill [2015]. There, the aim is to bound the performance of the policy\n1Worst-case regret is a strictly stronger notion of regret in case the reward distribution function is known and only the transition probability distribution is unknown, as we will assume here for the most part. In case of unknown reward distribution, extending our worst-case regret bounds would require an assumption of bounded rewards, where as the Bayesian regret bounds in the above-mentioned literature allow more general (known) priors on the reward distributions with possibly unbounded support. Bayesian regret bounds in those more general settings are incomparable to the worst-case regret bounds presented here.\nlearned at the end of the learning horizon, and not the performance during learning as quantified by regret. Strehl and Littman [2005], Strehl and Littman [2008] provide an optimistic algorithm for bounding regret in a discounted reward setting, but the definition of regret is slightly different in that it measures the difference between the rewards of an optimal policy and the rewards of the learning algorithm along the trajectory taken by the learning algorithm."}, {"heading": "2 Preliminaries and Problem Definition", "text": ""}, {"heading": "2.1 Markov Decision Process (MDP)", "text": "We consider a Markov Decision Process M defined by tuple {S,A, P, r, s1}, where S is a finite state-space of size S, A is a finite action-space of size A, P : S \u00d7A \u2192 \u2206S is the transition model, r : S \u00d7 A \u2192 [0, 1] is the reward function, and s1 is the starting state. When an action a \u2208 A is taken in a state s \u2208 S, a reward rs,a is generated and the system transitions to the next state s\u2032 \u2208 S with probability Ps,a(s \u2032), where \u2211 s\u2032\u2208S Ps,a(s \u2032) = 1.\nWe consider \u2018communicating\u2019 MDPs with finite \u2018diameter\u2019. Below we define communicating MDPs, and recall some useful known results for such MDPs.\nDefinition 1 (Policy). A deterministic policy \u03c0 : S \u2192 A is a mapping from state space to action space.\nDefinition 2 (Diameter D(M)). Diameter D(M) of an MDP M is defined as the minimum time required to go from one state to another in the MDP using some deterministic policy:\nD(M) = max s6=s\u2032,s,s\u2032\u2208S min \u03c0:S\u2192A T \u03c0s\u2192s\u2032 ,\nwhere T \u03c0s\u2192s\u2032 is the expected number of steps it takes to reach state s \u2032 when starting from state s and using policy \u03c0.\nDefinition 3 (Communicating MDP). An MDP M is communicating if and only if it has a finite diameter. That is, for any two states s 6= s\u2032, there exists a policy \u03c0 such that the expected number of steps to reach s\u2032 from s, T \u03c0s\u2192s\u2032 , is at mostD, for some finiteD \u2265 0. Definition 4 (Gain of a policy). The gain of a policy \u03c0, from starting state s1 = s, is defined as the infinite horizon undiscounted average reward, given by\n\u03bb\u03c0(s) = E[ lim T\u2192\u221e\n1\nT\nT \u2211\ni=1\nrst,\u03c0(st)|s1 = s].\nwhere st is the state reached at time t. Lemma 2.1 (Optimal gain for communicating MDPs). For a communicating MDP M with diameter D:\n(a) (Puterman [2014] Theorem 8.1.2, Theorem 8.3.2) The optimal (maximum) gain \u03bb\u2217 is state independent and is achieved by a deterministic stationary policy \u03c0\u2217, i.e., there exists a deterministic policy \u03c0\u2217 such that\n\u03bb\u2217 := max s\u2032\u2208S max \u03c0\n\u03bb\u03c0(s\u2032) = \u03bb\u03c0 \u2217 (s), \u2200s \u2208 S.\nHere, \u03c0\u2217 is referred to as an optimal policy for MDP M. (b) (Tewari and Bartlett [2008], Theorem 4) The optimal gain \u03bb\u2217 satisfies the following equa-\ntions, \u03bb\u2217 = min\nh\u2208RS max s,a rs,a + P T s,ah\u2212 hs = maxa rs,a + P T s,ah \u2217 \u2212 h\u2217s, \u2200s (1)\nwhere h\u2217, referred to as the bias vector of MDP M, satisfies: max\ns h\u2217s \u2212min s h\u2217s \u2264 D.\nGiven the above definitions and results, we can now define the reinforcement learning problem studied in this paper."}, {"heading": "2.2 The reinforcement learning problem", "text": "The reinforcement learning problem proceeds in rounds t = 1, . . . , T . The learning agent starts from a state s1 at round t = 1. In the beginning of every round t, the agent takes an action at \u2208 A and observes the reward rst,at as well as the next state st+1 \u223c Pst,at , where r and P are the reward function and the transition model, respectively, for a communicating MDP M with diameterD. The learning agent knows the state-space S, the action space A, as well as the rewards rs,a, \u2200s \u2208 S, a \u2208 A, for the underlying MDP, but not the transition model P or the diameterD. (The assumption of known and deterministic rewards has been made here only for simplicity of exposition, since the unknown transition model is the main source of difficulty in this problem. Our algorithm and results can be extended to bounded stochastic rewards with unknown distributions using standard Thompson Sampling for MAB, e.g., using the techniques in Agrawal and Goyal [2013b].)\nThe agent can use the past observations to learn the underlyingMDPmodel and decide future actions. The goal is to maximize the total reward \u2211T\nt=1 rst,at , or equivalently, minimize the total regret over a time horizon T , defined as R(T,M) := T\u03bb\u2217 \u2212\u2211Tt=1 rst,at (2) where \u03bb\u2217 is the optimal gain of MDP M. We present an algorithm for the learning agent with a near-optimal upper bound on the regret R(T,M) for any communicating MDP M with diameter D, thus bounding the worst-case regret over this class of MDPs."}, {"heading": "3 Algorithm Description", "text": "Our algorithm combines the ideas of Posterior sampling (aka Thompson Sampling) with the extended MDP construction used in Jaksch et al. [2010]. Below we describe the main components of our algorithm.\nSome notations: N ts,a denotes the total number of times the algorithm visited state s and played action a until before time t, and N ts,a(i) denotes the number of time steps among these N t s,a steps where the next state was i, i.e., a transition from state s to i was observed. We index the states from 1 to S, so that\n\u2211S i=1 N t s,a(i) = N t s,a for any t. We use the symbol 1 to denote the vector of all 1s,\nand 1i to denote the vector with 1 at the i th coordinate and 0 elsewhere.\nDoubling epochs: Our algorithm uses the epoch based execution framework of Jaksch et al. [2010]. An epoch is a group of consecutive rounds. The rounds t = 1, . . . , T are broken into consecutive epochs as follows: the kth epoch begins at the round \u03c4k immediately after the end of (k\u2212 1)th epoch and ends at the first round \u03c4 such that for some state-action pair s, a, N \u03c4s,a \u2265 2N \u03c4ks,a. The algorithm computes a new policy \u03c0\u0303k at the beginning of every epoch k, and uses that policy through all the rounds in that epoch. It is easy to observe that irrespective of how the policy \u03c0\u0303k is computed, the number of epochs in T rounds is bounded by SA log(T ).\nPosterior Sampling: We use posterior sampling to compute the policy \u03c0\u0303k in the beginning of every epoch. Dirichlet distribution is a convenient choice maintaining posteriors for the transition probability vectors Ps,a for every s \u2208 S, a \u2208 A, as they satisfy the following useful property: given a prior Dirichlet(\u03b11, . . . , \u03b1S) on Ps,a, after observing a transition from state s to i (with underlying probability Ps,a(i)), the posterior distribution is given by Dirichlet(\u03b11, . . . , \u03b1i+1, . . . , \u03b1S). By this property, for any s \u2208 S, a \u2208 A, on starting from prior Dirichlet(1) for Ps,a, the posterior at time t is Dirichlet({N ts,a(i) + 1}i=1,...,S). Our algorithm uses a modified, optimistic version of this approach. At the beginning of every epoch k, for every s \u2208 S, a \u2208 A such thatNs,a \u2265 \u03b7, it generates multiple samples forPs,a from a \u2018boosted\u2019 posterior. Specifically, it generates \u03c8 = O(S log(SA/\u03c1)) independent sample probability vectors Q1,ks,a, . . . , Q \u03c8,k s,a as Qj,ks,a \u223c Dirichlet(M\u03c4ks,a), whereMts,a denotes the vector [M t s,a(i)]i=1,...,S , with\nM ts,a(i) := 1 \u03ba (N t s,a(i) + \u03c9), for i = 1, . . . , S. (3)\nHere, \u03ba = O(log(T/\u03c1)), \u03c9 = O(log(T/\u03c1)), \u03b7 = \u221a\nTS A + 12\u03c9S 2, and \u03c1 \u2208 (0, 1) is a parameter of the algorithm. In the regret analysis, we derive sufficiently large constants that can be used in the definition of \u03c8, \u03ba, \u03c9 to guarantee the bounds. However, no attempt has been made to optimize those constants, and it is likely that much smaller constants suffice.\nFor every remaining s, a, i.e., those with small Ns,a (Ns,a < \u03b7) the algorithm use a simple optimistic sampling described in Algorithm 1. This special sampling for s, a with small Ns,a has been introduced to handle a technical difficulty in analyzing the anti-concentration of Dirichlet posteriors when the parameters are very small. We suspect that with an improved analysis, this may not be required.\nExtended MDP: The policy \u03c0\u0303k to be used in epoch k is computed as the optimal policy of an extended MDP M\u0303k defined by the sampled transition probability vectors, using the construction of Jaksch et al. [2010]. Given sampled vectors Qj,ks,a, j = 1, . . . , \u03c8, for every state-action pair s, a, we define extended MDP M\u0303k by extending the original action space as follows: for every s, a, create \u03c8 actions for every action a \u2208 A, denoting by aj the action corresponding to action a and sample j; then, in MDP M\u0303k, on taking action aj in state s, reward is rs,a but transitions to the next state follows the transition probability vectorQj,ks,a.\nNote that the algorithm uses the optimal policy \u03c0\u0303k of extended MDP M\u0303k to take actions in the action space A which is technically different from the action space of MDP M\u0303k, where the policy \u03c0\u0303k is defined. We slightly abuse the notation to say that the algorithm takes action at = \u03c0\u0303(st) to mean that the algorithm takes action at = a \u2208 A when \u03c0\u0303k(st) = aj for some j. Our algorithm is summarized as Algorithm 1."}, {"heading": "4 Regret Bounds", "text": "We prove the following bound on the regret of Algorithm 1 for the reinforcement learning problem. Theorem 1. For any communicating MDP M with S states, A actions, and diameter D, with probability 1\u2212 \u03c1. the regret of Algorithm 1 in time T \u2265 CDA log2(T/\u03c1) is bounded as:\nR(T,M) \u2264 O\u0303 ( D \u221a SAT +DS7/4A3/4T 1/4 +DS5/2A )\nwhere C is an absolute constant. For T \u2265 S5A, this implies a regret bound of\nR(T,M) \u2264 O\u0303 ( D \u221a SAT ) .\nHere O\u0303 hides logarithmic factors in S,A, T, \u03c1 and absolute constants.\nThe rest of this section is devoted to proving the above theorem. Here, we provide a sketch of the proof and discuss some of the key lemmas, all missing details are provided in the supplementary material."}, {"heading": "4.1 Proof of Theorem 1", "text": "As defined in Section 2, regret R(T,M) is given by R(T,M) = T\u03bb\u2217 \u2212 \u2211Tt=1 rst,at , where \u03bb\u2217 is the optimal gain of MDP M, at is the action taken and st is the state reached by the algorithm at time t. Algorithm 1 proceeds in epochs k = 1, 2, . . . ,K , where K \u2264 SA log(T ). To bound its regret in time T , we first analyze the regret in each epoch k, namely,\nRk := (\u03c4k+1 \u2212 \u03c4k)\u03bb\u2217 \u2212 \u2211\u03c4k+1\u22121\nt=\u03c4k rst,at ,\nand boundRk by roughly D \u2211\ns,a\nN \u03c4k+1 s,a \u2212N \u03c4ks,a \u221a\nN \u03c4ks,a\nwhere, by definition, for every s, a, (N \u03c4k+1 s,a \u2212 N \u03c4ks,a) is the number of times this state-action pair is visited in epoch k. The proof of this bound has two main components:\nAlgorithm 1 A posterior sampling based algorithm for the reinforcement learning problem\nInputs: State space S, Action space A, starting state s1, reward function r, time horizon T , parameters \u03c1 \u2208 (0, 1], \u03c8 = O(S log(SA/\u03c1)), \u03c9 = O(log(T/\u03c1)), \u03ba = O(log(T/\u03c1)), \u03b7 = \u221a\nTS A + 12\u03c9S 2.\nInitialize: \u03c41 := 1,M\u03c41s,a = \u03c91.\nfor all epochs k = 1, 2, . . . , do\nSample transition probability vectors: For each s, a, generate \u03c8 independent sample probability vectors Qj,ks,a, j = 1, . . . , \u03c8, as follows: \u2022 (Posterior sampling): For s, a such that N \u03c4ks,a \u2265 \u03b7, use samples from the Dirichlet\ndistribution: Qj,ks,a \u223c Dirichlet(M\u03c4ks,a),\n\u2022 (Simple optimistic sampling): For remaining s, a, with N \u03c4ks,a < \u03b7, use the following simple optimistic sampling: let\nP\u2212s,a = P\u0302s,a \u2212\u2206,\nwhere P\u0302s,a(i) = N\n\u03c4k s,a(i) N \u03c4k s,a , and\u2206i = min\n{\u221a\n3P\u0302s,a(i) log(4S)\nN \u03c4k s,a\n+ 3 log(4S) N \u03c4k s,a , P\u0302s,a(i)\n}\n, and\nlet z be a random vector picked uniformly at random from {11, . . . ,1S}; set\nQj,ks,a = P \u2212 s,a + (1\u2212 \u2211S i=1 P \u2212 s,a(i))z.\nCompute policy \u03c0\u0303k: as the optimal gain policy for extended MDP M\u0303k constructed using sample set {Qj,ks,a, j = 1, . . . , \u03c8, s \u2208 S, a \u2208 A}. Execute policy \u03c0\u0303k: for all time steps t = \u03c4k, \u03c4k + 1, . . . , until break epoch do Play action at = \u03c0\u0303k(st). Observe the transition to the next state st+1. Set N t+1s,a (i),M t+1 s,a (i) for all a \u2208 A, s, i \u2208 S as defined (refer to Equation (3)).\nIf N t+1st,at \u2265 2N \u03c4kst,at , then set \u03c4k+1 = t+ 1 and break epoch. end for\nend for\n(a) Optimism: The policy \u03c0\u0303k used by the algorithm in epoch k is computed as an optimal gain policy of the extendedMDP M\u0303k. The first part of the proof is to show that with high probability, the extended MDP M\u0303k is (i) a communicating MDP with diameter at most 2D, and (ii) optimistic, i.e., has optimal gain at least (close to) \u03bb\u2217. Part (i) is stated as Lemma 4.1, with a proof\nprovided in the supplementary material. Now, let \u03bb\u0303k be the optimal gain of the extended MDP M\u0303k. In Lemma 4.2, which forms one of the main novel technical components of our proof, we show that with probability 1\u2212 \u03c1,\n\u03bb\u0303k \u2265 \u03bb\u2217 \u2212 O\u0303(D \u221a SA T ).\nWe first show that above holds if for every s, a, there exists a sample transition probability vector whose projection on a fixed unknown vector (h\u2217) is optimistic. Then, in Lemma 4.3 we prove this optimism by deriving a fundamental new result on the anti-concentration of any fixed projection of a Dirichlet random vector (Proposition A.1 in the supplementary material). Substituting this upper bound on \u03bb\u2217, we have the following bound onRkwith probability 1\u2212 \u03c1:\nRk \u2264 \u2211\u03c4k+1\u22121\nt=\u03c4k\n( \u03bb\u0303k \u2212 rst,at + O\u0303(D \u221a SA T ) ) . (4)\n(b) Deviation bounds: Optimism guarantees that with high probability, the optimal gain \u03bb\u0303k for\nMDP M\u0303k is at least \u03bb\u2217. And, by definition of \u03c0\u0303k, \u03bb\u0303k is the gain of the chosen policy \u03c0\u0303k\nfor MDP M\u0303k. However, the algorithm executes this policy on the true MDP M. The only difference between the two is the transition model: on taking an action aj := \u03c0\u0303k(s) in state s in MDP M\u0303k, the next state follows the sampled distribution P\u0303s,a := Q j,k s,a, (5) where as on taking the corresponding action a in MDPM, the next state follows the distribution Ps,a. The next step is to bound the difference between \u03bb\u0303k and the average reward obtained by the algorithm by bounding the deviation (P\u0303s,a \u2212 Ps,a). This line of argument bears similarities to the analysis of UCRL2 in Jaksch et al. [2010], but with tighter deviation bounds that we are able to guarantee due to the use of posterior sampling instead of deterministic optimistic bias\nused in UCRL2. Now, since at = \u03c0\u0303k(st), using the relation between the gain \u03bb\u0303k , the bias vector h\u0303, and reward vector of optimal policy \u03c0\u0303k for communicating MDP M\u0303k (refer to Lemma 2.1) \u2211\u03c4k+1\u22121\nt=\u03c4k\n( \u03bb\u0303\u2212 rst,at ) = \u2211\u03c4k+1\u22121 t=\u03c4k (P\u0303st,at \u2212 1st)T h\u0303\n= \u2211\u03c4k+1\u22121 t=\u03c4k (P\u0303st,at \u2212 Pst,at + Pst,at \u2212 1st)T h\u0303 (6)\nwhere with high probability, h\u0303 \u2208 RS , the bias vector of MDP M\u0303k satisfies maxs h\u0303s \u2212mins h\u0303s \u2264 D(M\u0303k) \u2264 2D (refer to Lemma 4.1).\nNext, we bound the deviation (P\u0303s,a \u2212 Ps,a)T h\u0303 for all s, a, to bound the first term in above. Note that h\u0303 is random and can be arbitrarily correlated with P\u0303 , therefore, we need to bound maxh\u2208[0,2D]S(P\u0303s,a \u2212 Ps,a)Th. (For the above term, w.l.o.g. we can assume h\u0303 \u2208 [0, 2D]S). For s, a such thatN \u03c4ks,a > \u03b7, P\u0303s,a = Q j,k s,a is a sample from the Dirichlet posterior. In Lemma 4.4, we show that with high probability,\nmax h\u2208[0,2D]S\n(P\u0303 ks,a \u2212 Ps,a)Th \u2264 O\u0303( D \u221a\nN \u03c4ks,a +\nDS N \u03c4ks,a ). (7)\nThis bound is an improvement by a \u221a S factor over the corresponding deviation bound obtainable for the optimistic estimates of Ps,a in UCRL2. The derivation of this bound utilizes and extends the stochastic optimism technique from Osband et al. [2014]. For s, a with N \u03c4ks,a \u2264 \u03b7, P\u0303s,a = Qj,ks,a is a sample from the simple optimistic sampling, where we can only show the following weaker bound, but since this is used only while N \u03c4ks,a is small, the total contribution of this deviation will be small:\nmax h\u2208[0,2D]S\n(P\u0303 ks,a \u2212 Ps,a)Th \u2264 O\u0303 ( D\n\u221a\nS\nN \u03c4ks,a +\nDS N \u03c4ks,a\n)\n. (8)\nFinally, to bound the second term in (6), we observe that E[1Tst+1 h\u0303|\u03c0\u0303k, h\u0303, st] = PTst,at h\u0303 and use Azuma-Hoeffding inequality to obtain with probability (1 \u2212 \u03c1SA):\n\u2211\u03c4k+1\u22121 t=\u03c4k (Pst,at \u2212 1st)T h\u0303 \u2264 O( \u221a (\u03c4k+1 \u2212 \u03c4k) log(SA/\u03c1)). (9) Combining the above observations (equations (4), (6), (7), (8), (9)), we obtain the following bound onRk within logarithmic factors: D(\u03c4k+1\u2212\u03c4k) \u221a SA\nT +D\n\u2211\ns,a\nN \u03c4k+1 s,a \u2212N \u03c4ks,a \u221a\nN \u03c4ks,a\n( 1(N \u03c4k+1s,a > \u03b7) + \u221a S1(N \u03c4k+1s,a \u2264 \u03b7) ) +D \u221a \u03c4k+1 \u2212 \u03c4k.\n(10)\nWe can finish the proof by observing that (by definition of an epoch) the number of visits of any state-action pair can at most double in an epoch, N \u03c4k+1s,a \u2212N \u03c4ks,a \u2264 N \u03c4ks,a, and therefore, substituting this observation in (10), we can bound (within logarithmic factors) the total regretR(T ) = \u2211Kk=1 Rk as: K \u2211\nk=1\n(\nD(\u03c4k+1 \u2212 \u03c4k) \u221a SA T +D\n\u2211\ns,a:N \u03c4k s,a>\u03b7\n\u221a\nN \u03c4ks,a +D \u2211\ns,a:N \u03c4k s,a<\u03b7\n\u221a SN \u03c4ks,a +D \u221a \u03c4k+1 \u2212 \u03c4k\n)\n\u2264 D \u221a SAT +D log(K)( \u2211\ns,a\n\u221a N \u03c4Ks,a ) +D log(K)(SA \u221a S\u03b7) +D \u221a KT\nwhere we used N \u03c4k+1 s,a \u2264 2N \u03c4ks,a and \u2211 k(\u03c4k+1 \u2212 \u03c4k) = T . Now, we use that K \u2264 SA log(T ), and SA \u221a S\u03b7 = O(S7/4A3/4T 1/4 + S5/2A log(T/\u03c1)) (using \u03b7 = \u221a\nTS A + 12\u03c9S 2). Also, since \u2211\ns,a N \u03c4K s,a \u2264 T , by simple worst scenario analysis,\n\u2211\ns,a\n\u221a N \u03c4Ks,a \u2264 \u221a SAT , and we obtain,\nR(T,M) \u2264 O\u0303(D \u221a SAT +DS7/4A3/4T 1/4 +DS5/2A)."}, {"heading": "4.2 Main lemmas", "text": "Following lemma form the main technical components of our proof. All the missing proofs are provided in the supplementary material.\nLemma 4.1. Assume T \u2265 CDA log2(T/\u03c1) for a large enough constant C. Then, with probability 1\u2212 \u03c1, for every epoch k, the diameter of MDP M\u0303k is bounded by 2D. Lemma 4.2. With probability 1 \u2212 \u03c1, for every epoch k, the optimal gain \u03bb\u0303k of the extended MDP M\u0303k satisfies:\n\u03bb\u0303k \u2265 \u03bb\u2217 \u2212O ( D log2(T/\u03c1) \u221a SA T ) ,\nwhere \u03bb\u2217 the optimal gain of MDP M andD is the diameter.\nProof. Let h\u2217 be the bias vector for an optimal policy \u03c0\u2217 of MDP M (refer to Lemma 2.1 in the preliminaries section). Since h\u2217 is a fixed (though unknown) vector with |hi \u2212 hj | \u2264 D, we can apply Lemma 4.3 to obtain that with probability 1\u2212 \u03c1, for all s, a, there exists a sample vectorQj,ks,a for some j \u2208 {1, . . . , \u03c8} such that\n(Qj,ks,a) Th\u2217 \u2265 PTs,ah\u2217 \u2212 \u03b4\nwhere \u03b4 = O ( D log2(T/\u03c1) \u221a\nSA T\n)\n. Now, consider the policy \u03c0 forMDP M\u0303k which for any s, takes action aj , with a = \u03c0\u2217(s) and j being a sample satisfying above inequality. LetQ\u03c0 be the transition matrix for this policy, whose rows are formed by the vectors Qj,ks,\u03c0\u2217(s), and P\u03c0\u2217 be the transition matrix whose rows are formed by the vectors Ps,\u03c0\u2217(s). Above implies Q\u03c0h \u2217 \u2265 P\u03c0\u2217h\u2217 \u2212 \u03b41. We use this inequality along with the known relations between the gain and the bias of optimal policy in communicating MDPs to obtain that the gain \u03bb\u0303(\u03c0) of policy in \u03c0 for MDP M\u0303k satisfies \u03bb\u0303(\u03c0) \u2265 \u03bb\u2217 \u2212 \u03b4 (details provided in the supplementary material), which proves the lemma statement since by optimality \u03bb\u0303k \u2265 \u03bb\u0303(\u03c0).\nLemma 4.3. (Optimistic Sampling) Fix any vector h \u2208 RS such that |hi \u2212 hi\u2032 | \u2264 D for any i, i\u2032, and any epoch k. Then, for every s, a, with probability 1\u2212 \u03c1SA there exists at least one j such that\n(Qj,ks,a) Th \u2265 PTs,ah\u2212O\n( D log2(T/\u03c1) \u221a\nSA T\n)\n.\nLemma 4.4. (Deviation bound) With probability 1\u2212 \u03c1, for all epochs k, sample j, all s, a\nmax h\u2208[0,2D]S\n(Qj,ks,a \u2212 Ps,a)Th \u2264\n\n    \n    \nO\n(\nD\n\u221a\nlog(SAT/\u03c1)\nN \u03c4ks,a +D\nS log(SAT/\u03c1)\nN \u03c4ks,a\n)\n, N \u03c4ks,a > \u03b7\nO\n(\nD\n\u221a\nS log(SAT/\u03c1)\nN \u03c4ks,a +D\nS log(S)\nN \u03c4ks,a\n)\n, N \u03c4ks,a \u2264 \u03b7"}, {"heading": "5 Conclusions", "text": "We presented an algorithm inspired by posterior sampling that achieves near-optimal worst-case regret bounds for the reinforcement learning problem with communicating MDPs in a non-episodic, undiscounted average reward setting. Our algorithm may be viewed as a more efficient randomized version of the UCRL2 algorithm of Jaksch et al. [2010], with randomization via posterior sampling forming the key to the \u221a S factor improvement in the regret bound provided by our algorithm. Our analysis demonstrates that posterior sampling provides the right amount of uncertainty in the samples, so that an optimistic policy can be obtained without excess over-estimation."}, {"heading": "A Anti-concentration of Dirichlet distribution", "text": "We prove the following general result on anti-concentration of Dirichlet distributions, which will be used to prove optimism.\nPropositionA.1. Consider a random vector p\u0303 generated from Dirichlet distribution with parameters (mp\u03041, . . . ,mp\u0304S), wheremp\u0304i \u2265 6. Then, for any fixed h \u2208 [0, D]S , with probability \u2126(1/S)\u2212 S\u03c1,\n(p\u0303\u2212 p\u0304)Th \u2265 1 8\n\u221a \u221a \u221a \u221a \u2211\ni<S\n\u03b3\u0304ic\u03042i m \u2212 2SD log(2/\u03c1) m\nwhere\n\u03b3\u0304i := p\u0304i(p\u0304i+1 + . . .+ p\u0304S)\n(p\u0304i + . . .+ p\u0304S) , c\u0304i = (hi \u2212 H\u0304i+1), H\u0304i+1 =\n1 \u2211S\nj=i+1 p\u0304j\nS \u2211\nj=i+1\nhj p\u0304j .\nWe use an equivalent representation of a Dirichlet vector in terms of independent Beta random variables.\nFact 1. Fix an ordering of indices 1, . . . , S, and define y\u0303i := p\u0303i\np\u0303i+\u00b7\u00b7\u00b7+p\u0303S , y\u0304i := p\u0304i p\u0304i+\u00b7\u00b7\u00b7+p\u0304S . Then, for\nany h \u2208 RS ,\n(p\u0303\u2212 p\u0304)Th = \u2211\ni\n(y\u0303i \u2212 y\u0304i)(hi \u2212 H\u0303i+1)(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S) = \u2211\ni\n(y\u0303i \u2212 y\u0304i)(hi \u2212 H\u0304i+1)(p\u0303i + \u00b7 \u00b7 \u00b7+ p\u0303S)\nwhere H\u0303i+1 = 1\u2211\nS j=i+1 p\u0303j\n\u2211S j=i+1 hj p\u0303j , H\u0304i+1 = 1\u2211 S j=i+1 p\u0304j \u2211S j=i+1 hj p\u0304j .\nFact 2. For i = 1, . . . , S, y\u0303i := p\u0303i\np\u0303i+\u00b7\u00b7\u00b7+p\u0303S are independent Beta random variables distributed as\nBeta(mp\u0304i,m(p\u0304i+1 + \u00b7 \u00b7 \u00b7+ p\u0304S)), with mean\nE[y\u0303i] = mp\u0304i\nm(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S) = y\u0304i,\nand variance\n\u03c3\u03042i := E[(y\u0303i \u2212 y\u0304i)2] = p\u0304i(p\u0304i+1 + \u00b7 \u00b7 \u00b7+ p\u0304S)\n(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)2(m(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S) + 1) .\nLemma A.2 (Corollary of Lemma E.2). Let y\u0303i, y\u0304i, \u03c3\u0304i be defined as in Fact 2. Ifmp\u0304i,m(p\u0304i+1+ \u00b7 \u00b7 \u00b7+ p\u0304S) \u2265 6, then, for any positive constant C \u2264 12 ,\nP (y\u0303i \u2265 y\u0304i + C\u03c3\u0304i + C\nm(p\u0304i + ...+ p\u0304S) ) \u2265 0.15 =: \u03b7.\nProof. Apply Lemma E.2 with a = mp\u0304i, b = m(p\u0304i+1 + \u00b7 \u00b7 \u00b7+ p\u0304S). Lemma A.3. (Application of Berry-Esseen theorem) LetG \u2286 {1, . . . , S} be a set of indices, zi \u2208 R be fixed. Let\nXG := \u2211\ni\u2208G\n(y\u0303i \u2212 y\u0304i)zi.\nLet F be the cumulative distribution function of\nXG \u03c3G , where, \u03c32G = \u2211\ni\u2208G\nz2i \u03c3\u0304 2 i ,\n\u03c3\u0304i being the standard deviation of y\u0303i (refer to Fact 2). Let \u03a6 be the cumulative distribution function of standard normal distribution. Then, for all \u01eb > 0:\nsup x\n|F (x)\u2212 \u03a6(x)| \u2264 \u01eb\nas long as \u221a\n|G| \u2265 RC \u01eb , where R := max i,j\u2208G zi\u03c3\u0304i zj \u03c3\u0304j\nfor some C \u2264 3 + 6mp\u0304i .\nProof. Yi = (y\u0303i \u2212 y\u0304i)zi. Then, Yi, i \u2208 G are independent variables, with E[Yi] = 0, \u03c32i := E[Y 2 i ] = E[(y\u0303i \u2212 y\u0304i)2(zi)2]\n= z2i \u03c3\u0304 2 i\n\u03c1i := E[|Yi|3] \u2264 E[|Yi|4]3/4\n= E[|y\u0303 \u2212 y\u0304|4]3/4z3i \u2264 \u03baE[|y\u0303 \u2212 y\u0304|2]3/2z3i = \u03ba\u03c3\u03043i z 3 i\nwhere the first inequality is by using Jensen\u2019s inequality and \u03ba is the Kurtosis of Beta distribution. Next, we use that y\u0303 is Beta distributed, and Kurtosis of Beta(\u03bd\u00b5, \u03bd(1 \u2212 \u00b5)) Distribution is\n\u03ba = 3 + 6\n(3 + \u03bd)\n( (1\u2212 2\u00b5)2(1 + \u03bd) \u00b5(1\u2212 \u00b5)(2 + \u03bd) \u2212 1 ) \u2264 3 + 6 (3 + \u03bd)\u00b5 .\nHere, \u03b1 = m(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)y\u0304i, \u03b2 = m(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)(1 \u2212 y\u0304i), so that\n\u03ba \u2264 3 + 6 3 +m(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S) 1 y\u0304i \u2264 3 + 6 mp\u0304i .\nNow, we use Berry-Esseen theorem (Fact 6), with\n\u03c81 = 1 \u221a\n\u2211\ni\u2208G \u03c3 2 i\nmax i\u2208G \u03c1i \u03c32i\n\u2264 \u03ba\u221a |G| maxi\u2208G zi\u03c3\u0304i mini\u2208G zi\u03c3\u0304i\nto obtain the lemma statement.\nLemma A.4. Assumingmp\u0304i \u2265 6, \u2200i, for any fixed zi, i = 1, . . . , S,\nPr\n\n\n\u2211\ni\n(y\u0303i \u2212 y\u0304i)zi \u2265 1\n4\n\u221a\n\u2211\ni\n\u03c3\u03042i z 2 i\n\n \u2265 \u2126(1/S).\nProof. Define constant \u03b4 := (1\u2212\u03a6)( 1 2 ) 2 and k(\u03b4) := C2 \u03b44 , where C \u2264 4. Consider the the group of indices with the k(\u03b4) largest values of |zi\u03c3\u0304i|, call it groupG(1), and then divide the remaining into smallest possible collection G of groups such that |zi\u03c3\u0304i|/|zj\u03c3\u0304j | \u2264 1\u03b4 for all i, j in any given groupG. Define an ordering \u227a on groups by ordering them by maximum value\nof |zi\u03c3\u0304i| in the group. That is G \u227b G\u2032 if maxi\u2208G z2i \u03c3\u03042i \u2265 maxj\u2208G\u2032 z2j \u03c3\u03042j Note that by construction, for G \u227b G\u2032, we havemaxi\u2208G z2i \u03c3\u03042i \u2265 1\u03b42 maxj\u2208G\u2032 z2j \u03c3\u03042j .\nRecall from Lemma A.3, for every group G \u2208 G of size \u221a |G| > C\u03b4\u01eb , we have that its cdf is within \u01eb of normal distribution cdf, giving that Pr(XG \u2265 12\u03c3G) \u2265 2\u03b4 \u2212 \u01eb. Using this result for \u01eb = \u03b4, we get that for every group of size at least k(\u03b4), we have\nPr(XG \u2265 1\n2 \u03c3G) \u2265 \u03b4. (11)\nWe will look at three types of the groups we created above:\n\u2022 Top big groups: those among the top log1/\u03b4(S) groups that have cardinality at least k(\u03b4)\n\u2022 Top small groups: those among the top log1/\u03b4(S) groups that have cardinality smaller than k(\u03b4)\n\u2022 Bottom groups: those not among the top log1/\u03b4(S) groups\nHere, top groups refers to the those ranked higher according to the ordering\u227b. For the first group type above, apply (11) to obtain,\nfor all big groups among top log1/\u03b4(S), XG \u2265 12\u03c3G\nwith probability at least \u03b4log1/\u03b4(S) = 1\nS . (12)\nNext, we analyze the remaining indices (among top small groups and bottom groups). Consider the groupG(1) we set aside. Using Lemma A.2 k(\u03b4) times, we have:\nPr\n\n\n\u2211\ni\u2208G(1)\n(y\u0303i \u2212 y\u0304i)zi \u2265 0.5 \u221a \u2211\ni\u2208G(1)\nz2i \u03c3\u0304 2 i\n\n \u2265 \u03b7k(\u03b4)\nwhere \u03b7 \u2265 0.15. Now, if it is the case where the top group is of small size, we apply the above anticoncentration of beta for each element in the group, so that for all indices i in this group, (y\u0303i \u2212 y\u0304i)zi \u2265 0.5zi\u03c3\u0304i, with probability \u03b7k(\u03b4). To conclude, so far, we have with probability at least 1S \u03b7 2k(\u03b4)\n\u2211\ni\u2208G(1),i\u2208top big groups\n(y\u0303i \u2212 y\u0304i)zi \u2265 0.5 \u221a\n\u2211\ni\u2208G(1),i\u2208top big groups\nz2i \u03c3\u0304 2 i .\nFor every other small group G, the group\u2019s total variance is at most k(\u03b4)maxi\u2208G z 2 i \u03c3\u0304 2 i \u2264 k(\u03b4)\u03b42jz2(1)\u03c3\u0304 2 (1), where j is the rank of the group in ordering \u227b and (1) is the index of the smallest variance in G(1). So, the sum of the standard deviation for top log1/\u03b4(S) small groups is at most\nk(\u03b4) \u2211\nG:top small groups\nmax i\u2208G\nz2i \u03c3\u0304 2 i \u2264 k(\u03b4)\nlog1/\u03b4(S) \u2211\nj=1\n\u03b42jz(1)\u03c3\u0304(1) \u2264 k(\u03b4)\u03b42\n1\u2212 \u03b42 z 2 (1)\u03c3\u0304 2 (1)\nas it is a geometric series with \u03b4 multiplier. For the remaining bottom group, each element\u2019s variance is at most 1S2 z 2 (1)\u03c3\u0304 2 (1), therefore\n\u2211\ni:top small groups, bottom groups\nz2i \u03c3\u0304 2 i \u2264 (\nk(\u03b4)\u03b42\n1\u2212 \u03b42 + 1 S )z2(1)\u03c3\u0304 2 (1) \u2264 k(\u03b4) 25 z2(1)\u03c3\u0304 2 (1) \u2264 1 25 \u2211\ni\u2208G(1)\nz2i \u03c3\u0304 2 i .\nBy Cantelli\u2019s Inequality (Fact 5), with probability at least 12 ,\n\u2211\ni:top small groups, bottom groups\n(y\u0303i \u2212 y\u0304i)zi \u2265 \u2212 \u221a\n\u2211\ni\u2208top small groups, bottom groups\nz2i \u03c3\u0304 2 i \u2265 \u2212\n1\n5\n\u221a\n\u2211\ni\u2208G(1)\nz2i \u03c3\u0304 2 i .\nHence combining our results above,\n\u2211\ni\n(y\u0303i \u2212 y\u0304i)zi \u2265 1\n2\n\u221a\n\u2211\ni\u2208G(1),top big groups\nz2i \u03c3\u0304 2 i \u2212\n1\n5\n\u221a\n\u2211\ni\u2208G(1)\nz2i \u03c3\u0304 2 i\n\u2265 3 10\n\u221a\n\u2211\ni\u2208G(1),top big groups\nz2i \u03c3\u0304 2 i +\n1\n25\n\u221a\n\u2211\ni\u2208G(1)\nz2i \u03c3\u0304 2 i \u2212\n1\n25\n\u221a\n\u2211\ni\u2208G(1)\nz2i \u03c3\u0304 2 i\n\u2265 13 50\n\u221a\n\u2211\ni\u2208G(1),top big groups\nz2i \u03c3\u0304 2 i +\n1\n25\n\u221a\n\u2211\ni\u2208G(1)\nz2i \u03c3\u0304 2 i\n\u2265 1 4\n\u221a\n\u2211\ni\nz2i \u03c3\u0304 2 i\nwith probability \u03b72k(\u03b4) 12S = \u2126(1/S).\nProof. (Proof of Proposition A.1) Use Fact 1 to express (p\u0303\u2212 p\u0304)Th as: (p\u0303\u2212 p\u0304)Th = \u2211\ni\n(y\u0303i \u2212 y\u0304i)(hi \u2212 H\u0303i+1)(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\nUsing Lemma E.3 and Corollary E.7,\n|H\u0303i \u2212 H\u0304i| \u2264 D \u221a\n2 log(2/\u03c1)\nm(p\u0304i + . . .+ p\u0304S)\nwith probability 1\u2212 \u03c1 for any i. and similarly using Lemma E.3 and Corollary E.7,\n|y\u0303i \u2212 y\u0304i| \u2264 \u221a 2 log (2/\u03c1)\nm(p\u0304i + ...+ p\u0304S) .\nTherefore, with probability 1\u2212 S\u03c1, (p\u0303\u2212 p\u0304)Th\u2212 \u2211\ni\n(y\u0303i \u2212 y\u0304i)(hi \u2212 H\u0304i+1)(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\n= \u2211\ni\n(y\u0303i \u2212 y\u0304i)(H\u0303i+1 \u2212 H\u0304i+1)(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\n\u2265 \u2212 \u2211\ni\n\u221a\n2 log(2/\u03c1)\nm(p\u0304i + ...+ p\u0304S) D\n\u221a\n2 log(2/\u03c1)\nm(p\u0304i + ...+ p\u0304S) (p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\n\u2265 \u22122SD log(2/\u03c1) m . (13)\nThen, applying Lemma A.4 (given mp\u0304i \u2265 6) for zi = (hi \u2212 H\u0304i+1)(p\u0304i + \u00b7 \u00b7 \u00b7 + p\u0304S), i = 1, . . . , S, with probability \u2126(1/S),\n(p\u0303\u2212 p\u0304)Th \u2265 1 4\n\u221a\n\u2211\ni\nz2i \u03c3\u0304 2 i \u2212\n2SD log(2/\u03c1)\nm .\nNow, we observe\n\u2211\ni\nz2i \u03c3\u0304 2 i = (hi \u2212 H\u0304i+1)2(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)2\u03c3\u03042i =\nc\u03042i p\u0304i(p\u0304i + . . . , p\u0304S)\nm(p\u0304i + . . .+ p\u0304S) + 1 ,\nto obtain\n(p\u0303\u2212 p\u0304)Th \u2265 1 8\n\u221a\n\u2211\ni\n\u03b3\u0304ic\u03042i m \u2212 2SD log(2/\u03c1) m\nwhere\n\u03b3\u0304i = p\u0304i(p\u0304i+1 + . . .+ p\u0304S)\n(p\u0304i + . . .+ p\u0304S) ."}, {"heading": "B Optimism", "text": "In this section, we prove the following lemmas.\nLemma 4.2. With probability 1 \u2212 \u03c1, for every epoch k, the optimal gain \u03bb\u0303k of the extended MDP M\u0303k satisfies:\n\u03bb\u0303k \u2265 \u03bb\u2217 \u2212O ( D log2(T/\u03c1) \u221a SA T ) ,\nwhere \u03bb\u2217 the optimal gain of MDP M andD is the diameter.\nProof. Let h\u2217 be the bias vector for an optimal policy \u03c0\u2217 of MDP M (refer to Lemma 2.1 in the preliminaries section). Since h\u2217 is a fixed (though unknown) vector with |hi \u2212 hj | \u2264 D, we can apply Lemma 4.3 to obtain that with probability 1\u2212 \u03c1, for all s, a, there exists a sample vectorQj,ks,a for some j \u2208 {1, . . . , \u03c8} such that\n(Qj,ks,a) Th\u2217 \u2265 PTs,ah\u2217 \u2212 \u03b4\nwhere \u03b4 = O ( D log2(T/\u03c1) \u221a\nSA T\n)\n. Now, consider the policy \u03c0 for MDP M\u0303k which for any s, takes action aj , where a = \u03c0\u2217(s), and j is a sample satisfying above inequality. Note that \u03c0 is essentially \u03c0\u2217 but with a different transition probability model. Let Q\u03c0 be the transition matrix for this policy, whose rows are formed by the vectors Qj,ks,\u03c0\u2217(s), and P\u03c0\u2217 be the transition matrix whose rows are formed by the vectors Ps,\u03c0\u2217(s). Above implies\nQ\u03c0h \u2217 \u2265 P\u03c0\u2217h\u2217 \u2212 \u03b41.\nLet Q\u2217\u03c0 denote the limiting matrix for Markov chain with transition matrix Q\u03c0. Observe that Q\u03c0 is aperiodic, recurrent and irreducible : it is aperiodic and irreducible because each entry of Q\u03c0 being a sample from Dirichlet distribution is non-zero, and it is positive recurrent because in a finite irreducible Markov chain, all states are positive and recurrent. This implies that Q\u2217\u03c0 is of the form 1q\u2217T where q\u2217 is the stationary distribution of Q\u03c0, and 1 is the vector of all 1s (refer to (A.6) in Puterman [2014]). Also, Q\u2217\u03c0Q\u03c0 = Q\u03c0, and Q \u2217 \u03c01 = 1.\nTherefore, the gain of policy \u03c0\n\u03bb\u0303(\u03c0)1 = (rT\u03c0 q \u2217)1 = Q\u2217\u03c0r\u03c0\nwhere r\u03c0 is the S dimensional vector [rs,\u03c0(s)]s=1,...,S . Now,\n\u03bb\u0303(\u03c0)1\u2212 \u03bb\u22171 = Q\u2217\u03c0r\u03c0 \u2212 \u03bb\u22171 = Q\u2217\u03c0r\u03c0 \u2212 \u03bb\u2217(Q\u2217\u03c01) . . . (using Q\u2217\u03c01 = 1) = Q\u2217\u03c0(r\u03c0 \u2212 \u03bb\u22171) = Q\u2217\u03c0(I \u2212 P\u03c0\u2217)h\u2217 . . . (using (1)) = Q\u2217\u03c0(Q\u03c0 \u2212 P\u03c0\u2217)h\u2217 . . . (using Q\u2217\u03c0Q\u03c0 = Q\u2217\u03c0) \u2265 \u2212\u03b41 . . . (using (Q\u03c0 \u2212 P\u03c0\u2217)h\u2217 \u2265 \u2212\u03b41, Q\u2217\u03c01 = 1).\nThen, by optimality,\n\u03bb\u0303k \u2265 \u03bb\u0303(\u03c0) \u2265 \u03bb\u2217 \u2212 \u03b4.\nLemma 4.3. (Optimistic Sampling) Fix any vector h \u2208 RS such that |hi \u2212 hi\u2032 | \u2264 D for any i, i\u2032, and any epoch k. Then, for every s, a, with probability 1\u2212 \u03c1SA there exists at least one j such that\n(Qj,ks,a) Th \u2265 PTs,ah\u2212O\n( D log2(T/\u03c1) \u221a\nSA T\n)\n.\nProof. For s, a with N \u03c4ks,a \u2265 \u03b7, Qj,ks,a were generated using posterior sampling from Dirichlet distribution Dirichlet(M \u03c4ks,a(i), i = 1, . . . , S). We use Proposition B.3 for optimism of a Dirichlet posterior sample. Let\u2019s verify the conditions applying for this proposition. We have N \u03c4ks,a \u2265 \u03b7 = \u221a\nTS A + 12\u03c9S 2 \u2265 12\u03c9S2. and \u03c9 = 720 log(n/\u03c1).\nTherefore, applying Proposition B.3, with probability \u2126(1/S), the jth sample Qj,ks,a satisfies the following kind of optimism:\n(Qj,ks,a) Th \u2265 PTs,ah\u2212O(\nDS log2(n/\u03c1)\nN \u03c4ks,a ).\nSubstituting N \u03c4ks,a \u2265 \u03b7 = \u221a TS A + 12\u03c9S 2 we get that every j satisfies the stated condition with probability \u2126(1/S).\nFor s, a with N \u03c4ks,a \u2264 \u03b7, we used simple optimistic sampling. In Lemma B.1 we show for such s, a the condition (Qj,ks,a)\nTh \u2265 PTs,ah is satisfied by any j with probability 1/2S. Therefore, given that the number of samples is \u03c8 = CS log(SA/\u03c1) for some large enough constant C, for every s, a, with probability 1 \u2212 \u03c1SA , there exists at least one sample Qj,ks,a satisfying the required condition.\nNotations We fix some notations for the rest of the section. Fix an epoch k, state and action pair s, a, sample j. In below, we denote n = N \u03c4ks,a, ni = N \u03c4k s,a(i), pi = Ps,a(i), p\u0302i := ni n , p\u0304i = ni+\u03c9 n+\u03c9S , p\u0303i = Q j,k s,a(i), for i \u2208 S.\nB.1 Optimism for n \u2264 \u03b7 (Simple Optimistic Sampling)\nWhen n < \u03b7, simple optimistic sampling is used, so that any sample vector p\u0303 was generated as follows: we let p\u2212 = [p\u0302 \u2212 ( \u221a\n3p\u0302i log(4S) n + 3 log(4S) n )1] +, and let z be a random vector picked\nuniformly at random from {11, . . . ,1S}, and set\np\u0303 = p\u2212 + (1\u2212\u2211j p\u2212j )z.\nWe prove the following lemmas for this sample vector.\nLemma B.1. For any fixed h \u2208 [0, D]S , we have\np\u0303Th \u2265 pTh,\nwith probability at least \u2126(1/S).\nProof. Define \u03b4i := p\u0302i \u2212 pi (and hence \u2211 i \u03b4i = 0). By multiplicative Chernoff bounds (Fact 4), with probability 1 \u2212 12S , |\u03b4i| \u2264 \u221a 3p\u0302i log(4S) n + 3 log(4S) n . Also define \u2206i := p\u0302i \u2212 p\u2212i =\nmin\n{\n\u221a\n3p\u0302i log(4S) n + 3 log(4S) n , p\u0302i\n}\n. Note that\u2206i \u2265 \u03b4i and \u2211 i \u2206i = \u2211 i(p\u0302i \u2212 p\u2212i ) = 1\u2212 \u2211 i p \u2212 i .\nWith probability 1/S, z = 1i is picked such that hi = D, and (by union bound over all i) with probability 1\u2212 S 12S = 12 , |\u03b4i| \u2264 \u221a 3 log(4S) n + 3 log(4S) n for every i. So with probability 1/2S:\n\u2211 i p\u0303ihi = \u2211 i p\u2212i hi +D(1\u2212 \u2211 j p\u2212j ) = \u2211 i p\u2212i hi +D \u2211 j \u2206j\n= \u2211\ni\n(p\u0302i \u2212\u2206i)hi +D\u2206i = \u2211\ni\np\u0302ihi + (D \u2212 hi)\u2206i\n\u2265 \u2211\ni\np\u0302ihi + (D \u2212 hi)\u03b4i = \u2211\ni\n(p\u0302i \u2212 \u03b4i)hi +D\u03b4i\n= \u2211\ni\npihi +D \u2211\ni\n\u03b4i = \u2211\ni\npihi.\nUsing the same technique as above, we can also prove the following \u201cpessimism\u201d for these samples, which will be used later, in bounding the diameter in Section D.\nLemma B.2 (Pessimism). When n < \u03b7, we have for any fixed h \u2208 [0, D]S\np\u0303Th \u2264 pTh, with probability at least \u2126(1/S).\nProof. Define \u03b4i,\u2206i as before. With probability 1/S, z = 1i is picked such that hi = 0, and again with probability 1\u2212 S 12S = 12 , |\u03b4i| \u2264 \u221a 3 log(4S) n + 3 log(4S) n for every i. So with probability 1/2S:\n\u2211\ni\np\u0303ihi = \u2211\ni\np\u2212i hi\n= \u2211\ni\n(p\u0302i \u2212\u2206i)hi\n\u2264 \u2211\ni\n(p\u0302i \u2212 \u03b4i)hi\n= \u2211\ni\npihi.\nB.2 Optimism for n > \u03b7 (Dirichlet posterior sampling)\nWhen n > \u03b7, Dirichlet posterior sampling is used so that p\u0303 is a random vector distributed as Dirichlet(mp\u03041, . . . ,mp\u0304S), where m = n+\u03c9S \u03ba , p\u0304 = ni+\u03c9 n+\u03c9S . We prove an optimism property for this sample vector. Following notations will be useful.\n\u03b3i := pi(pi+1 + . . .+ pS)\n(pi + . . .+ pS) , ci := (hi \u2212Hi+1), Hi+1 =\n1 \u2211S\nj=i+1 pj\nS \u2211\nj=i+1\nhjpj\n\u03b3\u0304i := p\u0304i(p\u0304i+1 + . . .+ p\u0304S)\n(p\u0304i + . . .+ p\u0304S) , c\u0304i := (hi \u2212 H\u0304i+1), H\u0304i+1 =\n1 \u2211S\nj=i+1 p\u0304j\nS \u2211\nj=i+1\nhj p\u0304j\nwhere the states are indexed from 1 to S such that p\u03041 \u2264 \u00b7 \u00b7 \u00b7 \u2264 p\u0304S . Proposition B.3. Assuming \u03c9 = 720 log(n/\u03c1) \u2265 613 log(2/\u03c1), n > 12\u03c9S2, \u03ba = 120 log(n/\u03c1) = \u03c9 6 , then with probability \u2126(1/S)\u2212 8S\u03c1,\np\u0303Th \u2265 pTh\u2212O(DS log 2(n/\u03c1)\nn ).\nProof. The proof of this proposition involves showing that with probaility \u2126(1/S) \u2212 8S\u03c1, the random quantity p\u0303Th exceeds its mean p\u0304Th enough to overcome the possible deviation of empirical estimate p\u0304Th from the true value pTh. This involves a Dirichlet anti-concentration bound (Proposition A.1 and Lemma B.4) to lower bound p\u0303Th, and a concentration bound on empirical estimates p\u0302 (Lemma C.3) to lower bound p\u0304Th which by definition is close to p\u0302Th.\nIn Lemma B.4, we show that with probability \u2126(1/S)\u2212 7S\u03c1,\n(p\u0303\u2212 p\u0304)Th \u2265 0.188 \u221a \u221a \u221a \u221a \u2211\ni<S\n\u03b3ic2i m \u2212O(DS\u03c9 log(n/\u03c1) n ).\nNote thatm = n+\u03c9S\u03ba and so n \u03ba < m < 25n 24\u03ba since n > 12\u03c9S 2. Then we have that\n(p\u0303\u2212 p\u0304)Th \u2265 0.184 \u221a \u03ba \u2211\ni\n\u03b3ic2i n \u2212O(DS\u03c9 log(n/\u03c1) n ).\nWe can also calculate\n|(p\u0304\u2212 p\u0302)Th| = | S \u2211\ni=1\nhi( np\u0302i + \u03c9 n+ \u03c9S \u2212 np\u0302i n )| = | \u2211\ni\nhi( \u03c9(1 \u2212 Sp\u0302i) n+ \u03c9S )| \u2264 \u03c9DS n+ \u03c9S \u2264 \u03c9DS n .\nFinally, from Lemma C.3 bounding the deviation of empirical estimates, we have that with probability 1\u2212 \u03c1,\n|(p\u0302\u2212 p)Th| \u2264 2 \u221a \u221a \u221a \u221alog(n/\u03c1) \u2211\ni<S\n\u03b3ic2i n + 2D log(n/\u03c1) n .\nHence putting everything together we have that with probability \u2126(1/S)\u2212 8S\u03c1, (p\u0303\u2212 p)Th = (p\u0303\u2212 p\u0304)Th+ (p\u0304\u2212 p\u0302)Th+ (p\u0302\u2212 p)Th\n\u2265 (p\u0303\u2212 p\u0304)Th\u2212 |(p\u0304\u2212 p\u0302)Th| \u2212 |(p\u0302\u2212 p)Th|\n\u2265 0.184 \u221a \u03ba \u2211\ni\n\u03b3ic2i n\n\u2212 2 \u221a \u221a \u221a \u221alog(n/\u03c1) \u2211\ni<S\n\u03b3ic2i n \u2212O(DS\u03c9 log(n/\u03c1) n )\n\u2265 \u2212O(DS log 2(n/\u03c1)\nn )\nwhere the last inequality follows with \u03c9 = 720 log(n/\u03c1) and \u03ba = 120 log(n/\u03c1).\nLemma B.4. Assume that h \u2208 [0, D]S , and \u03c9 \u2265 613 log(2/\u03c1), n > 12\u03c9S2, \u03ba = \u03c96 , and an ordering of i such that p\u03041 \u2264 \u00b7 \u00b7 \u00b7 \u2264 p\u0304S . Then, with probability \u2126(1/S)\u2212 7S\u03c1,\n(p\u0303\u2212 p\u0304)Th \u2265 0.188 \u221a \u2211\ni\n\u03b3ic2i m \u2212O(DS\u03c9 log(n/\u03c1) n ).\nProof. The proof is obtained by a modification to the proof of Proposition A.1, which proves a similar bound but in terms of \u03b3\u0304i\u2019s and c\u0304i\u2019s.\nIn the proof of that proposition, we obtain (refer to Equation (13)), with probability 1\u2212S\u03c1 (assuming mp\u0304i \u2265 6),\n(p\u0303\u2212 p\u0304)Th \u2265 \u2211\ni\n(y\u0303i \u2212 y\u0304i)(hi \u2212 H\u0304i+1)(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\u2212 2DS log(2/\u03c1)\nm\n\u2265 \u2211\ni\n(y\u0303i \u2212 y\u0304i)(hi \u2212 H\u0304i+1)(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\u2212O( DS\u03c9 log(n/\u03c1)\nn )\nwhere y\u0303i := p\u0303i\np\u0303i+\u00b7\u00b7\u00b7+p\u0303S , y\u0304i := p\u0304i p\u0304i+\u00b7\u00b7\u00b7+p\u0304S , H\u0303i+1 = 1\u2211\nS j=i+1 p\u0303j\n\u2211S j=i+1 hj p\u0303j , H\u0304i+1 =\n1\u2211 S j=i+1 p\u0304j \u2211S j=i+1 hj p\u0304j . Now, breaking up the term in the summation and using Lemma B.7 to bound |Hi+1 \u2212 H\u0304i+1|(p\u0304i + \u00b7 \u00b7 \u00b7 + p\u0304S) (since we have by assumption that \u03c9 \u2265 613 log(2/\u03c1) and n > 12\u03c9S2) and Lemma E.4 and Corollary E.7 to bound |y\u0303i \u2212 y\u0304i|, we get that for every i, with probability 1\u2212 4S\u03c1,\n(p\u0303\u2212 p\u0304)Th\u2212 \u2211\ni\n(y\u0303i \u2212 y\u0304i)(hi \u2212Hi+1)(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S) +O( DS\u03c9 log(n/\u03c1)\nm )\n\u2265 \u2211\ni\n(y\u0303i \u2212 y\u0304i)(H\u0304i+1 \u2212Hi+1)(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\n\u2265 \u2212 \u2211\ni\n\u221a\n2 log(2/\u03c1)\nm(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\n(\n3D\n\u221a\nlog(n/\u03c1) (p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\nn + 4\n(\u03c9S + log(n/\u03c1))D\nn\n)\n(\u2217) \u2265 \u22126DS \u221a\nlog(2/\u03c1) log(n/\u03c1)\u221a mn\n\u2212 4(\u03c9S + log(n/\u03c1))D \u221a 2 log(2/\u03c1)\nn \u221a m\n\u2211\ni\n1 \u221a\n(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S) .\nRecall that m = n+\u03c9S\u03ba , so that for n > S\u03c9, n \u2265 m\u03ba2 = m\u03c912 \u2265 m log(2/\u03c1), and the first term of (\u2217) is at least:\n\u22126DS \u221a log(2/\u03c1) log(n/\u03c1) \u221a\nm2 log(2/\u03c1) = \u22126DS\n\u221a\nlog(n/\u03c1) m = \u2212O(DS\u03c9 log(n/\u03c1) n ).\nThen using Lemma B.5 andm = (n+ S\u03c9)/\u03ba > 6n/\u03c9 > 72S2, the second term in (\u2217) is at least:\n\u22128S(\u03c9S + log(n/\u03c1))D \u221a 2 log(2/\u03c1)\nn \u221a 72S2\n= \u2212O(DS\u03c9 log(n/\u03c1) n ).\nThen, applying Lemma A.4 (given mp\u0304i \u2265 6) for zi = (hi \u2212 Hi+1)(p\u0304i + \u00b7 \u00b7 \u00b7 + p\u0304S), i = 1, . . . , S, with probability \u2126(1/S),\n\u2211\ni\n(y\u0303i \u2212 y\u0304i)zi \u2265 1\n4\n\u221a\n\u2211\ni\n\u03c3\u03042i z 2 i .\nWe substitute this in the above, with the observation\n\u2211\ni\nz2i \u03c3\u0304 2 i =\n\u2211\ni\n(hi \u2212Hi+1)2(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)2\u03c3\u03042i = \u2211\ni\nc2i p\u0304i(p\u0304i + . . . , p\u0304S)\nm(p\u0304i + . . .+ p\u0304S) + 1 \u2265\n\u2211\ni\n6\n7\n\u03b3\u0304ic 2 i\nm .\nSo far we have that with probability \u2126(1/S)\u2212 4S\u03c1,\n(p\u0303\u2212 p\u0304)Th \u2265 \u221a 6\n4 \u221a 7\n\u221a\n\u2211\ni\n\u03b3\u0304ic2i m \u2212O(DS\u03c9 log(n/\u03c1) n ). (14)\nFinally, we use Lemma B.6 with k = 14 (this requires \u03c9 \u2265 613 log(2/\u03c1)) to lower bound \u03b3\u0304i by 1 1.51\u03b3i \u2212O(\u03c9Sn ) to get with probability \u2126(1/S)\u2212 7S\u03c1,\n(p\u0303\u2212 p\u0304)Th \u2265 0.188 \u221a \u2211\ni\n\u03b3ic2i m \u2212O(DS\u03c9 log(n/\u03c1) n ).\nLemma B.5. Let x \u2208 Rn such that 0 \u2264 x1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 xn \u2264 1 and \u2211 i xi = 1. Then\nn \u2211\ni=1\n1\u221a xi + \u00b7 \u00b7 \u00b7xn \u2264 2n.\nProof. Define f(y) := 1\u221a xy+\u00b7\u00b7\u00b7+xn for all y = 1, \u00b7 \u00b7 \u00b7 , n. We prove that x\u2217 := ( 1n , 1n , \u00b7 \u00b7 \u00b7 , 1n ) achieves the maximum value. Consider any solution x\u2032. Assume there exists some index pair i, j with i < j and some \u01eb > 0 such that x\u2032i 6= x\u2032j and increasing x\u2032i by \u01eb and decreasing x\u2032j by \u01eb preserves the ordering of the indices. This strictly increases the objective, because f(k) strictly increases for all i < k \u2264 j and remains unchanged otherwise, and hence x\u2032 is not an optimal solution. The only case where no such index pair (i, j) exists is when every xi is equal- this is precisely the solution x\u2217. Since \u2211\ni f(i) is a continuous functions over a compact set, it has a maximum, which therefore must be attained at x\u2217.\nThis means n \u2211\ni=1\n1\u221a xi + \u00b7 \u00b7 \u00b7xn\n\u2264 n \u2211\ni=1\n1 \u221a\nx\u2217i + \u00b7 \u00b7 \u00b7+ x\u2217n =\nn \u2211\ni=1\n\u221a\nn i \u2264 \u221an \u222b n\ni=0\n1\u221a i di = 2n.\nLemma B.6. Let A = 3 log( 2\u03c1 ) and \u03c9 \u2265 2524k2A. Also let n > 12\u03c9S2. Then for any group G of indices, with probability 1\u2212 \u03c1,\n(1\u2212 1 k ) \u2211\ni\u2208G\np\u0304i \u2212 2\u03c9S\nn \u2264\n\u2211\ni\u2208G\npi \u2264 (1 + 1 k ) \u2211\ni\u2208G\np\u0304i + 2\u03c9S\nn .\nIf in the definition of \u03b3\u0304i, we use an ordering of i such that p\u0304S \u2265 1S (e.g., if max p\u0304i is the last in the ordering), then for all i, with probability 1\u2212 3\u03c1,\n\u03b3i \u2264 (1 + 1k ) 2\n1\u2212 1k \u2212 16 \u03b3\u0304i +\n2(1 + 1k + 1 6 )\n1\u2212 1k \u2212 16 \u03c9S n .\nProof. By multiplicative Chernoff-Hoeffding bounds (Fact 4), with probability 1\u2212 \u03c1,\n| \u2211\ni\npi \u2212 \u2211\ni\np\u0302i| \u2264 \u221a A \u2211\ni p\u0302i n + A n\nwhere A = 3 log( 2\u03c1 ) so that using | \u2211 i p\u0304i \u2212 \u2211 i p\u0302i| \u2264 \u03c9Sn ,\n| \u2211\ni\npi \u2212 \u2211\ni\np\u0304i| \u2264 \u221a \u2211 i p\u0304iA\nn +\n\u221a A\u03c9S\nn +\nA n + \u03c9S n \u2264\n\u221a\n\u2211\ni p\u0304iA\nn +\n2\u03c9S\nn .\nNow, for n > 12\u03c9S2, np\u0304i = n np\u0302i+\u03c9 n+\u03c9S \u2265 n\u03c9n+\u03c9S \u2265 24\u03c925 \u2265 k2A.\n| \u2211\ni\npi \u2212 \u2211\ni\np\u0304i| \u2264 \u2211\ni\np\u0304i\n\u221a\nA n \u2211 i p\u0304i + 2\u03c9S n \u2264 \u2211\ni\np\u0304i\n\u221a\nA\nk2A +\n2\u03c9S n \u2264 1 k \u2211\ni\np\u0304i + 2\u03c9S\nn\nso that \u2211\ni\npi \u2264 (1 + 1 k ) \u2211\ni\np\u0304i + 2\u03c9S n , \u2211\ni\npi \u2265 (1\u2212 1 k ) \u2211\ni\np\u0304i \u2212 2\u03c9S\nn .\nFor the second statement of the lemma, using what we just proved, we have that with probability 1\u2212 3\u03c1,\n\u03b3i = pi(pi+1 + \u00b7 \u00b7 \u00b7+ pS) pi + \u00b7 \u00b7 \u00b7+ pS \u2264 (1 + 1 k )\n2p\u0304i(p\u0304i+1 + \u00b7 \u00b7 \u00b7+ p\u0304S) + 2(1+ 1 k )\u03c9S(p\u0304i+\u00b7\u00b7\u00b7+p\u0304S) n + 4\u03c92S2 n2\n(1\u2212 1k )(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\u2212 2\u03c9Sn .\nNow, if indices i are ordered such that p\u0304S \u2265 1S , then p\u0304i+ \u00b7 \u00b7 \u00b7+ p\u0304S \u2265 1S for all i. Also, if n > 12\u03c9S2, we have the following bound on the denominator in above: (1 \u2212 1k )(p\u0304i + \u00b7 \u00b7 \u00b7 + p\u0304S) \u2212 2\u03c9Sn \u2265 (1 \u2212 1k \u2212 16 )(p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S), so that from above\n\u03b3i \u2264 (1 + 1k ) 2\n1\u2212 1k \u2212 16 \u03b3\u0304i +\n2(1 + 1k + 1 6 )\n1\u2212 1k \u2212 16 \u03c9S n .\nLemma B.7. For any fixed h \u2208 RS , and i, let H\u0302i = 1\u2211S j=i p\u0302j \u2211S j=i hj p\u0302j ,Hi = 1\u2211 S j=i pj \u2211S j=i hjpj , H\u0304i = 1\u2211\nS j=i p\u0304j\n\u2211S j=i hj p\u0304j . Then if n \u2265 96, with probability 1\u2212 \u03c1,\n|(H\u0304i \u2212Hi)(p\u0304i + . . .+ p\u0304S)| \u2264 2D \u221a log(n/\u03c1) (pi + \u00b7 \u00b7 \u00b7+ pS)\nn + 3\n(\u03c9S + log(n/\u03c1))D\nn .\nMoreover, if we also assume that \u03c9 \u2265 30 log(2/\u03c1) and n > 12\u03c9S2, then with probability 1\u2212 2\u03c1,\n|(H\u0304i \u2212Hi)(p\u0304i + . . .+ p\u0304S)| \u2264 3D \u221a log(n/\u03c1) (p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\nn + 4\n(\u03c9S + log(n/\u03c1))D\nn .\nProof. For every t, k \u2265 i, define\nZt,k =\n(\nhk1(st = k)\u2212 hk pk\npi + \u00b7 \u00b7 \u00b7+ pS \u00b7 1(st \u2208 {i, . . . , S})\n)\n1(st\u22121 = s, at\u22121 = a),\nZt = \u2211\nk\u2265i\nZt,k.\nThen, \u2211\u03c4\nt=1 Zt n = \u2211\nk\u2265i\nhkp\u0302k \u2212 \u2211\nk\u2265i\nhk pk\npi + \u00b7 \u00b7 \u00b7+ pS \u00b7 (p\u0302i + . . .+ p\u0302S) = (H\u0302i \u2212Hi)(p\u0302i + . . .+ p\u0302S)\nwhere we used Fact 1 for the last equality. Now, E[Zt|st\u22121, at\u22121] = \u2211 k\u2265i E[Zt,k|st\u22121, at\u22121] = 0. Also, we observe that for any t, Zt,k and Zt,j for any k 6= j are negatively correlated given the current state and action: E[Zt,kZt,j|st\u22121, at\u22121] = hkhjE[1(st = k)1(st = j)\u2212 1(st = j) pk\npi + \u00b7 \u00b7 \u00b7+ pS \u00b7 1(st \u2208 {i, . . . , S})\n\u22121(st = k) pj\npi + \u00b7 \u00b7 \u00b7+ pS \u00b7 1(st \u2208 {i, . . . , S})\n+ pjpk\n(pi + \u00b7 \u00b7 \u00b7+ pS)2 \u00b7 1(st \u2208 {i, . . . , S})]\n= hkhjE[\u2212 2pjpk\npi + \u00b7 \u00b7 \u00b7+ pS + pkpj (pi + \u00b7 \u00b7 \u00b7+ pS)2 \u00b7 1(st \u2208 {i, . . . , S})]\n= hkhjE[\u2212 pjpi\npi + \u00b7 \u00b7 \u00b7+ pS ]\n\u2264 0. And,\nE[ \u03c4 \u2211\nt=1\nZ2t,k|st\u22121 = s, at\u22121 = a] = h2k t \u2211\n\u03c4=1\n1(st\u22121 = s, at\u22121 = a)\n(\npk \u2212 p2k\n(pi + \u00b7 \u00b7 \u00b7+ pS)2 (pi + \u00b7 \u00b7 \u00b7+ pS)\n)\n= h2k\n\u03c4 \u2211\nt=1\n1(st\u22121 = s, at\u22121 = a) pk(\n\u2211\nj\u2265i,j 6=k pj)\npi + \u00b7 \u00b7 \u00b7+ pS\n= nh2k pk(\n\u2211\nj\u2265i,j 6=k pj)\npi + \u00b7 \u00b7 \u00b7+ pS \u2264 nD2pk.\nTherefore, \u03c4 \u2211\nt=1\nE[Z2t |st\u22121, at\u22121] \u2264 \u03c4 \u2211\nt=1\n\u2211\nk\u2265i\nE[Z2t,k|st\u22121, at\u22121] \u2264 nD2(pi + \u00b7 \u00b7 \u00b7+ pS).\nThen, applying Bernstein\u2019s inequality (refer to Corollary E.1) to bound |\u2211\u03c4t=1 Zt|, we get the following bound on 1n \u2211\u03c4 t=1 Zt = (H\u0302i \u2212Hi)(p\u0302i + . . .+ p\u0302S) with probability 1\u2212 \u03c1:\n|(H\u0302i \u2212Hi)(p\u0302i + . . .+ p\u0302S)| = | 1\nn\n\u03c4 \u2211\nt=1\nZt| \u2264 2D \u221a log(n/\u03c1) (pi + \u00b7 \u00b7 \u00b7+ pS)\nn + 3D\nlog(n/\u03c1)\nn .\nAlso,\n|H\u0302i \u2212 H\u0304i| = | \u2211\nk\np\u0302k p\u0302i + \u00b7 \u00b7 \u00b7+ p\u0302S hk \u2212 p\u0304k p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S hk| \u2264\n\u03c9SD\nn(p\u0302i + \u00b7 \u00b7 \u00b7+ p\u0302S) ,\nCombining,\n|(H\u0304i \u2212Hi)(p\u0302i + . . .+ p\u0302S)| \u2264 2D \u221a log(n/\u03c1) (pi + \u00b7 \u00b7 \u00b7+ pS)\nn + 3D\nlog(n/\u03c1)\nn +\n\u03c9SD\nn .\nReplacing p\u0302i by p\u0304i,\n|(H\u0304i \u2212Hi)(p\u0304i + . . .+ p\u0304S)| \u2264 2D \u221a log(n/\u03c1) (pi + \u00b7 \u00b7 \u00b7+ pS)\nn + 3\n(\u03c9S + log(n/\u03c1))D\nn\nwith probability 1\u2212 \u03c1. Now, if we also have that \u03c9 \u2265 30 log(2/\u03c1) and n > 12\u03c9S2, using lemma B.6 with k = 3 to replace pi by p\u0304i, with probability 1\u2212 2\u03c1,\n|(H\u0304i \u2212Hi)(p\u0304i + . . .+ p\u0304S)| \u2264 3D \u221a log(n/\u03c1) (p\u0304i + \u00b7 \u00b7 \u00b7+ p\u0304S)\nn + 4\n(\u03c9S + log(n/\u03c1))D\nn ."}, {"heading": "C Deviation bounds", "text": "Lemma 4.4. (Deviation bound) With probability 1\u2212 \u03c1, for all epochs k, sample j, all s, a\nmax h\u2208[0,2D]S\n(Qj,ks,a \u2212 Ps,a)Th \u2264\n\n   \n    \nO\n(\nD\n\u221a\nlog(SAT/\u03c1)\nN \u03c4ks,a +D\nS log(SAT/\u03c1)\nN \u03c4ks,a\n)\n, N \u03c4ks,a > \u03b7\nO\n(\nD\n\u221a\nS log(SAT/\u03c1)\nN \u03c4ks,a +D\nS log(S)\nN \u03c4ks,a\n)\n, N \u03c4ks,a \u2264 \u03b7\nProof. For n > \u03b7, express the above as\nmax h\u2208[0,2D]S (Qj,ks,a \u2212 Ps,a)Th \u2264 max h\u2208[0,2D]S (Qj,ks,a \u2212 P\u0304s,a)Th+ (P\u0304s,a \u2212 P\u0302s,a)Th+ (P\u0302s,a \u2212 Ps,a)Th\nwhere P\u0304s,a = M\n\u03c4k s,a(i)\nM \u03c4k s,a\n= N\n\u03c4k s,a(i)+\u03c9 N \u03c4k s,a+\u03c9S is the mean of Dirichlet(M\u03c4ks,a) distribution used to sampleQ j,k,\nand P\u0302s,a = N\n\u03c4k s,a(i) N \u03c4k s,a . Now,\nmax h\u2208[0,2D]S\n(P\u0304s,a \u2212 P\u0302s,a)Th \u2264 2\u03c9SD\nN \u03c4ks,a .\nAnd, to bound the first and the last terms in above, we use Lemma C.1 and Lemma C.2 with union bound for all S,A, \u03c8, k, to get the lemma statement for n > \u03b7.\nFor n < \u03b7, we use Lemma C.4 with a union bound for S,A, \u03c8, k, we get the lemma statement.\nC.1 Dirichlet concentration\nA similar result as the lemma below for concentration of Dirichlet random vectors was proven in Osband and Van Roy [2016]. We include (an expanded version of) the proof for completeness.\nLemma C.1 (Osband and Van Roy [2016]). Let p\u0303 \u223c Dirichlet(mp\u0304). Let\nZ := max v\u2208[0,D]S\n(p\u0303\u2212 p\u0304)T v.\nThen, Z \u2264 D \u221a\n2 log(2/\u03c1) m , with probability 1\u2212 \u03c1.\nProof. Define disjoint events Ev, v \u2208 [0, D]S in the sample space of Z as Ev = {Z : Z = max\nw\u2208[0,D]S (p\u0303\u2212 p\u0304)Tw = (p\u0303\u2212 p\u0304)T v}.\nLet f(v) be the probability of event Ev. (Here, ties are broken in arbitrary but fixed manner to assign each Z to one of the Ev so that Ev are disjoint and f(v) integrate to 1). Now, define a random variable Y distributed as follows: Y = Yv \u2212 E[Yv] with probability f(v), where Yvs are Beta variables distributed as Yv \u223c Beta(m 1D p\u0304T v,m(1 \u2212 1D p\u0304T v)). We show that Y is stochastically optimistic compared to Z .\nWe couple Y and Z as follows: when Z \u2208 Ev, which is with probability f(v), we set Y is Yv. By definition, under this event, Z = (p\u0303 \u2212 p\u0304)T v. By Dirichlet-Beta optimism (Lemma E.5), for any v, DYv is stochastically optimistic compared to p\u0303\nT v. Now, since they have the same mean, from equivalence condition for stochastic optimism (Condition 3 in Lemma 3 of Osband et al. [2014])\nE[DYv \u2212 p\u0303T v|p\u0303T v] = 0 for all values of v, p\u0303T v. Since we coupled Y and Z so that Y is Yv \u2212 E[Yv] when Z \u2208 Ev, we can derive that for any v, and z \u2208 Ev,\nE[DY \u2212 Z|Z = z : z \u2208 Ev] = E[DYv \u2212DE[Yv]\u2212 Z |Z = z : z \u2208 Ev] = E[DYv \u2212DE[Yv]\u2212 (p\u0303\u2212 p\u0304)T v | (p\u0303\u2212 p\u0304)T v] = E[DYv \u2212 p\u0303T v | p\u0303T v] = 0.\nThis is true for all z, since every z \u2208 Ev for some v, thus proving DY so Z.\nLet X be distributed as Gaussian with mean 0 and variance 1m . By Gaussian-Beta stochastic optimismX so Yv \u2212 E[Yv], which implies for any convex increasing u(\u00b7),\nE[u(Y )] =\n\u222b\nv\nE[u(Yv \u2212 E[Yv ])f(v) \u2264 \u222b\nv\nE[u(X)]f(v) = E[u(X)]\nso thatX so Y , and X so Y so 1\nD Z.\nTherefore, we can use Corollary E.7 to bound Z byD \u221a\n2 log(2/\u03c1) m with probability 1\u2212 \u03c1.\nC.2 Concentration of average of independent multinoulli trials\nBelow we study concentration properties of vector p\u0302 defined as the average of n independent multinoulli trials with parameter p \u2208 \u2206S , i.e., p\u0302 = \u2211nj=1 xj , where xjs are iid random vectors, with xij = 1 with probability pi.\nLemma C.2. Let p\u0302 be the average of n independent multinoulli trials with parameter p. Let\nZ := max v\u2208[0,D]S\n(p\u0302\u2212 p)T v.\nThen, Z \u2264 D \u221a\n2 log(1/\u03c1) n , with probability 1\u2212 \u03c1.\nProof. Define disjoint events Ev, v \u2208 [0, D]S in the sample space of Z as Ev = {Z : Z = max\nw\u2208[0,D]S (p\u0302\u2212 p)Tw = (p\u0302\u2212 p)T v}.\nLet f(v) be the probability of event Ev. (Here, ties are broken in arbitrary but fixed manner to assign each Z to one of the Ev so that Ev are disjoint and f(v) integrate to 1). Now, define a random variable Y distributed as follows: Y = Yv \u2212 E[Yv] with probability f(v), where Yvs are independent Binomial variables distributed as Yv \u223c 1nBinomial(n, 1DpT v). We show that Y is stochastically optimistic compared to Z .\nWe couple Y and Z as follows: when Z \u2208 Ev, which is with probability f(v), we set Y is Yv. By definition, under this event, Z = (p\u0302\u2212 p)T v. By Multinomial-Binomial optimism (Lemma E.8 and Corollary E.9), for any v, DYv is stochastically optimistic compared to p\u0302\nT v. Now, since they have the same mean, from equivalence condition for stochastic optimism (Condition 3 in Lemma 3 of Osband et al. [2014]) E[DYv \u2212 p\u0302T v|p\u0302T v] = 0 for all values of v, p\u0303T v. Since we coupled Y and Z so that Y is Yv \u2212 E[Yv] when Z \u2208 Ev, we can derive that for any v, and z \u2208 Ev,\nE[DY \u2212 Z|Z = z : z \u2208 Ev] = E[DYv \u2212DE[Yv]\u2212 Z |Z = z : z \u2208 Ev] = E[DYv \u2212DE[Yv]\u2212 (p\u0302\u2212 p)T v | (p\u0302\u2212 p)T v] = E[DYv \u2212 p\u0302T v | p\u0302T v] = 0.\nThis is true for all z, since every z \u2208 Ev for some v, thus proving DY so Z.\nNext, we boundZ using the stochastic optimism. First, let us express the distribution of Y in a more convenient way. Let \u00b5v = 1 Dp T v, \u00b5 = \u222b v f(v)\u00b5v . Define\nX = n \u2211\nj=1\nXj\nwhereXjs are iid random variables, distributed as follows: Xj takes value 1\u2212 \u00b5v with probability f(v)\u00b5v and \u2212\u00b5v w.p. f(v)(1 \u2212 \u00b5v), for v \u2208 [0, D]d. Therefore, E[Xj ] = \u222b\nv(1 \u2212 \u00b5v)f(v)\u00b5v \u2212 \u00b5vf(v)(1 \u2212 \u00b5v) = 0, andXj \u2208 [\u22121, 1]. We show that X and Y have the same distribution. Since each Yv is Binomial(n, \u00b5v), we can write it as Yv = \u2211n j=1 Y j v where Y j v are independent Bernoulli(\u00b5v) random variables. Define a random variable v\u0303 which is v with probability f(v). Then, since Y is Yv \u2212 \u00b5v w.p. f(v),\nY \u223c \u222b\nv\n(Yv\u2212\u00b5v)1(v\u0303 = v) = 1\nn\n\u2211\nj\n\u222b\nv\n(1\u2212\u00b5v)1(v\u0303 = v, Y jv = 1)\u2212\u00b5v1(v\u0303 = v, Y jv = 0) \u223c 1\nn\n\u2211\nj\nXj.\nTherefore,\nX \u223c Y so 1\nD Z\nwhere X = 1n \u2211n j=1 X j , is the sum of n mean 0, bounded [\u22121, 1], iid random variables. By Hoeffding\u2019s lemma, for any s \u2208 R\nE[esX j ] \u2264 e s 2 2 , so that, E[esnX ] \u2264 ens 2 2 .\nUsing stochastic optimism E[u(Z/d)] \u2264 E[u(Y )] = E[u(X)] for all convex increasing u(\u00b7), therefore for s > 0,\nP (n Z D > nt) \u2264 E[e\nsn ZD ]\nesnt \u2264 E[e\nsnX ]\nesnt \u2264 ens\n2\n2 \u2212snt.\nChoosing s = t = \u221a\n2 log(1/\u03c1) n ,\nP ( Z\nD >\n\u221a\nlog(1/\u03c1)\nn ) \u2264 e\u2212t\n2\n2 < \u03c1.\nLemma C.3. Let p\u0302 \u2208 \u2206S be the average n independent multinoulli trials with parameter p \u2208 \u2206S . Then, for any fixed h \u2208 [0, D]S and n \u2265 96, with probability 1\u2212 \u03c1,\n|(p\u0302\u2212 p)Th| \u2264 2 \u221a \u221a \u221a \u221alog(n/\u03c1) \u2211\ni<S\n\u03b3ic2i n + 3D log(2/\u03c1) n ,\nwhere \u03b3i = pi(pi+1+\u00b7\u00b7\u00b7+pS) (pi+\u00b7\u00b7\u00b7+pS) , ci = hi \u2212Hi+1, Hi+1 = 1\u2211S\nj=i+1 pj\n\u2211S j=i+1 hjpj .\nProof. For every t, i, define\nZt,i =\n(\nci1(st = i)\u2212 ci pi\npi + \u00b7 \u00b7 \u00b7+ pS \u00b7 1(st \u2208 {i, . . . , S})\n)\n1(st\u22121 = s, at\u22121 = a),\nZt = \u2211\ni\nZt,i.\nThen,\n\u2211\u03c4 t=1 Zt n = \u2211\ni\ncip\u0302i\u2212 \u2211\ni\ncipi pi + \u00b7 \u00b7 \u00b7+ pS\n\u00b7(p\u0302i+. . .+p\u0302S) = S\u22121 \u2211\ni=1\n(y\u0302i\u2212yi)(p\u0302i+. . .+p\u0302S)ci = (p\u0302\u2212p)Th\nwhere we used Fact 1 for the last equality. Now, E[Zt|st\u22121, at\u22121] = \u2211 iE[Zt,i|st\u22121, at\u22121] = 0. Also, we observe that for any t, Zt,i and Zt,j for any i 6= j are independent given the current state and action: (assume j > i w.l.o.g.)\nE[Zt,iZt,j|st\u22121, at\u22121] = cicjE[1(st = i)1(st = j)\u2212 1(st = j) pi\npi + \u00b7 \u00b7 \u00b7+ pS \u00b7 1(st \u2208 {i, . . . , S})\n\u22121(st = i) pj\npj + \u00b7 \u00b7 \u00b7+ pS \u00b7 1(st \u2208 {j, . . . , S})\n+ pjpi\n(pj + \u00b7 \u00b7 \u00b7+ pS)(pi + \u00b7 \u00b7 \u00b7+ pS) \u00b7 1(st \u2208 {j, . . . , S})]\n= cicjE[\u22121(st = j) pi\npi + \u00b7 \u00b7 \u00b7+ pS +\npjpi (pj + \u00b7 \u00b7 \u00b7+ pS)(pi + \u00b7 \u00b7 \u00b7+ pS) \u00b7 1(st \u2208 {j, . . . , S})]\n= cicjE[\u2212 pjpi\npi + \u00b7 \u00b7 \u00b7+ pS + pjpi (pi + \u00b7 \u00b7 \u00b7+ pS) ]\n= 0.\nTherefore, \u03c4\n\u2211\nt=1\nE[Z2t |st\u22121, at\u22121] = \u03c4 \u2211\nt=1\n\u2211\ni\nc2iE[Z 2 t,i|st\u22121, at\u22121] =\n\u2211\ni\nc2in\u03b3i,\nwhere the last equality is obtained using the following derivation:\nE[\n\u03c4 \u2211\nt=1\nZ2t,i|st\u22121 = s, at\u22121 = a] = \u03c4 \u2211\nt=1\n1(st\u22121 = s, at\u22121 = a)\n(\npi \u2212 p2i\n(pi + \u00b7 \u00b7 \u00b7+ pS)2 (pi + \u00b7 \u00b7 \u00b7+ pS)\n)\n= \u03c4 \u2211\nt=1\n1(st\u22121 = s, at\u22121 = a) pi(pi+1 + \u00b7 \u00b7 \u00b7+ pS)\npi + \u00b7 \u00b7 \u00b7+ pS\n= n pi(pi+1 + \u00b7 \u00b7 \u00b7+ pS)\npi + \u00b7 \u00b7 \u00b7+ pS = n\u03b3i.\nThen, applying Bernstein\u2019s inequality (refer to Corollary E.1) to bound |\u2211\u03c4t=1 Zt|, we get the desired bound on (p\u2212 p\u0302)Th = 1n \u2211\u03c4 t=1 Zt.\nC.3 Concentration of simple optimistic samples\nLemma C.4. Let p\u0303 = p\u2212 + (1 \u2212 \u2211Si=1 p\u2212i )z where z be a random vector picked uniformly at random from {11, . . . ,1S}, and p\u2212 = p\u0302\u2212\u2206,\u2206i = min { \u221a 3p\u0302i log(4S) n + 3 log(4S) n , p\u0302i } , then with probability at least 1\u2212 \u03c1, for anyD, we have\nmax h\u2208[0,D]S\n(p\u0303Th\u2212 pTh) \u2264 O(D \u221a S log(nS/\u03c1)\nn +\nDS log(2S)\nn ).\nProof. By definition of p\u0303 and using Lemma C.2, with probability 1\u2212 \u03c1,\nmax h\u2208[0,D]S\n(p\u0303Th\u2212 pTh) \u2264 (p\u0302Th\u2212 pTh) +D \u2211\ni\n\u221a\n3p\u0302i log(4S)\nn + \u2211\ni\n3D log(4S)\nn\n\u2264 2D \u221a 2 log(1/\u03c1)\nn +D\n\u221a\nS 3 log(4S)\nn +\nDS log(4S)\nn\n= O(D\n\u221a\nS log(4S/\u03c1)\nn +\nDS log(4S)\nn )."}, {"heading": "D Diameter of the extended MDP M\u0303k", "text": "Lemma 4.1. Assume T \u2265 CDA log2(T/\u03c1) for a large enough constant C. Then, with probability 1\u2212 \u03c1, for every epoch k, the diameter of MDP M\u0303k is bounded by 2D.\nProof. Using Lemma D.2, along with Lemma D.1 for h = Es, we obtain that the diameter of M\u0303k is bounded by D/(1 \u2212 \u03b4) for \u03b4 = O(D \u221a\nlog(1/\u03c1) \u03b7 +D log(T/\u03c1) \u03b7 )), where \u03b7 =\n\u221a\nTS A . Therefore, if\nT \u2265 CDA log2(T/\u03c1), then \u03b7 \u2265 CDS log(T/\u03c1) \u2265 CD2 log(1/\u03c1), making \u03b4 \u2264 1/2 for some large enough constant C.\nLemma D.1. For every k, and any fixed h \u2208 [0, D]S , with probability 1 \u2212 \u03c1, there exists a sample vector Qj,ks,a such that\nQj,ks,a \u00b7 h \u2264 Ps,a \u00b7 h+O(D \u221a log(1/\u03c1)\n\u03b7 +DS\nlog(T/\u03c1)\n\u03b7 )).\nProof. First consider s, a with N \u03c4ks,a \u2265 \u03b7. For such s, a posterior sampling is used, and by Lemmas C.1 and C.2,\nQj,ks,a \u00b7 h \u2264 Ps,a \u00b7 h+O(D \u221a log(1/\u03c1)\nN \u03c4ks,a +D\n\u03c9S\nN \u03c4ks,a ) \u2264 Ps,a \u00b7 h+O(D\n\u221a\nlog(1/\u03c1)\n\u03b7 +DS\nlog(T/\u03c1)\n\u03b7 ).\nFor s, a with N \u03c4ks,a \u2264 \u03b7, we use a simple optimistic sampling. In Lemma B.2, we prove that under such samplingQj,ks,a \u00b7h \u2264 Ps,a \u00b7h with probability 1/2S for every sample j. Then, since the number of samples is \u0398(S log(1/\u03c1)), we get that it holds for some j with probability 1\u2212 \u03c1. Lemma D.2. Let Es \u2208 RS+ be the vector of the minimum expected times to reach s from s\u2032 \u2208 S in true MDP M, i.e., Ess\u2032 = min\u03c0 T \u03c0s\u2032\u2192s. Note that Ess = 0. For any episode k, if for every s, a there exists some j such that Qj,ks,a \u00b7Es \u2264 Ps,a \u00b7 Es + \u03b4, (15) for some \u03b4 \u2208 [0, 1), then the diameter of extended MDP M\u0303k is at most D1\u2212\u03b4 , whereD is the diameter of MDP M.\nProof. Fix a k. For brevity, we omit the superscript k in below.\nFix any two states s1 6= s2. We prove the lemma statement by constructing a policy \u03c0\u0303 for M\u0303 such that the expected time to reach s2 from s1 is at most D 1\u2212\u03b4 . Let \u03c0 be the policy for MDPM for which the expected time to reach s2 from s1 is at most D (since M has diameterD, such a policy exists). Let E be the |S| \u2212 1 dimensional vector of expected times to reach s2 from every state, except s2 itself, using \u03c0 (E is the sub-vector formed by removing sth2 coordinate of vector E\ns2 where Es was defined in the lemma statement. Note that Es2s2 = 0). By first step analysis, E is a solution of:\nE = 1+ P \u2020\u03c0E,\nwhere P \u2020\u03c0 is defined as the (S \u2212 1)\u00d7 (S \u2212 1) transition matrix for policy \u03c0, with the (s, s\u2032)th entry being the transition probability Ps,\u03c0(s)(s\n\u2032) for all s, s\u2032 6= s2. Also, by choice of \u03c0, E satisfies Es1 \u2264 D.\nNow, we define \u03c0\u0303 using \u03c0 as follows: For any state s 6= s2, let a = \u03c0(s) and jth sample satisfies the property (15) for s, a, Es2 , then we define \u03c0\u0303(s) := aj . Let Q\u03c0\u0303 be the transition matrix (dimension S \u00d7 S) for this policy. Q\u03c0\u0303 defines a Markov chain. Next, we modify this Markov chain to construct an absorbing Markov chain with a single absorbing state s2. Let Q \u2020 \u03c0\u0303 be the submatrix (S \u2212 1)\u00d7 (S \u2212 1) submatrix ofQ\u03c0\u0303 obtained by removing the row and column corresponding to the state s2. Then Q \u2032 is defined as (an appropriate reordering of) the following matrix:\nQ\u2032\u03c0\u0303 =\n[\nQ\u2020\u03c0\u0303 q 0 1\n]\nwhere q is an (S \u2212 1)-length vector such that the rows of Q\u2032\u03c0\u0303 sum to 1. Since the probabilities in Q\u03c0\u0303 were drawn from Dirichlet distribution, they are all strictly greater than 0 and less than 1. Therefore each row-sum of Q\u2020\u03c0\u0303 is strictly less than 1, so that the vector q has no zero entries and the Markov chain is indeed an absorbing chain with single absorbing state s2. Then we notice that (I \u2212 Q\u2020\u03c0\u0303)\u22121 is precisely the fundamental matrix of this absorbing Markov chain and hence exists and is non-negative (see Grinstead and Snell [2012], Theorem 11.4). Let E\u0303 be defined as the S \u2212 1 dimensional vector of expected time to reach s2 from s\n\u2032 6= s2 in MDP M\u0303k using \u03c0\u0303. Then, it is same as the expected time to reach the absorbing state s2 from s \u2032 6= s2 in the Markov chain Q\u2032\u03c0\u0303, given by\nE\u0303 = (I \u2212 Q\u0304\u2020\u03c0\u0303)\u221211.\nThen using (15) (since Es2s2 = 0, the inequality holds for P \u2020, Q\u2020),\nE = 1+ P \u2020\u03c0E \u2265 1+Q\u2020\u03c0\u0303E \u2212 \u03b41 \u21d2 (I \u2212Q\u2020\u03c0\u0303)E \u2265 (1 \u2212 \u03b4)1. (16)\nMultiplying the non-negative matrix (I \u2212Q\u2020\u03c0\u0303)\u22121 on both sides of this inequality, it follows that\nE \u2265 (1\u2212 \u03b4)(I \u2212Q\u2020\u03c0\u0303)\u221211 = (1\u2212 \u03b4)E\u0303\nso that E\u0303s1 \u2264 1(1\u2212\u03b4)Es1 \u2264 D1\u2212\u03b4 , proving that the expected time to reach s2 from s1 using policy \u03c0\u0303 in MDP M\u0303k is at most D1\u2212\u03b4 ."}, {"heading": "E Useful deviation inequalities", "text": "Fact 3 (Bernstein\u2019s Inequality, from Seldin et al. [2012] Lem 11/Cor 12). Let Z1, Z2, ..., Zn be a bounded martingale difference sequence so that |Zi| \u2264 K and E[Zi|Fi\u22121] = 0. Define Mn = \u2211n\ni=1 Zi and Vn = \u2211n i=1 E[(Zi) 2|Fi\u22121]. For any c > 1 and \u03b4 \u2208 (0, 1), with probability greater\nthan 1\u2212 \u03b4, if \u221a\nln 2\u03bd\u03b4 (e\u2212 2)Vn \u2264 1 K\nthen\n|Mn| \u2264 (1 + c) \u221a (e\u2212 2)Vn ln 2\u03bd\n\u03b4 ,\notherwise,\n|Mn| \u2264 2K ln 2\u03bd\n\u03b4 ,\nwhere\n\u03bd = \u2308 ln (\n\u221a\n(e\u22122)n\nln 2\u03b4 )\nln c \u2309+ 1.\nCorollary E.1 (to Bernstein\u2019s Inequality above). Let Zi for i = 1, \u00b7 \u00b7 \u00b7 , n, Mn, and Vn as above. For n \u2265 96 and \u03b4 \u2208 (0, 1), with probability greater than 1\u2212 \u03b4,\n|Mn| \u2264 2 \u221a Vn ln n\n\u03b4 + 3K ln\nn \u03b4 .\nProof. Applying Bernstein\u2019s Inequality above with c = 1 + 4n , with probability greater than 1\u2212 \u03b4,\n|Mn| \u2264 (1 + c) \u221a (e\u2212 2)Vn ln 2\u03bd\n\u03b4 + 2K ln\n2\u03bd\n\u03b4\n\u2264 (1 + c)\n\u221a\n(e\u2212 2)Vn ln n\n4 3\n\u03b4 + 2K ln\nn 4 3\n\u03b4\n\u2264 (1 + c) \u221a\n(e\u2212 2)4 3 Vn ln n \u03b4 + 3K ln n \u03b4\n\u2264 2 \u221a Vn ln n\n\u03b4 + 3K ln\nn\n\u03b4\nwhere\n\u03bd = \u2308 ln (\n\u221a\n(e\u22122)n\nln 2\u03b4 )\nln c \u2309+ 1 = \u2308n 2 ln (\n\u221a\n(e\u2212 2)n ln 2\u03b4 )\u2309+ 1 \u2264 n 2 ln (\n\u221a\n(e\u2212 2)n ln 2 ) + 2 \u2264 1 2 n 4 3 .\nFact 4 (Multiplicative Chernoff Bound, Kleinberg et al. [2008] Lemma 4.9). Consider n i.i.d. random variablesX1, \u00b7 \u00b7 \u00b7 , Xn on [0, 1]. Let \u00b5 be their mean and let X be their average. Then for any \u03b1 > 0 the following holds:\nP (|X \u2212 \u00b5| < r(\u03b1,X) < 3r(\u03b1, \u00b5)) > 1\u2212 e\u2126(\u03b1), where r(\u03b1, x) = \u221a\n\u03b1x n + \u03b1 n .\nMore explicitly, we have that with probability 1\u2212 \u03c1,\n|X \u2212 \u00b5| < \u221a 3 log(2/\u03c1)X\nn +\n3 log(2/\u03c1)\nn .\nFact 5 (Cantelli\u2019s Inequality). Let X be a real-valued random variable with expectation \u00b5 and variance \u03c32. Then P (X \u2212 \u00b5 \u2265 \u03bb) \u2264 \u03c32\u03c32+\u03bb2 for \u03bb > 0 and P (X \u2212 \u00b5 \u2265 \u03bb) \u2265 1\u2212 \u03c3 2 \u03c32+\u03bb2 for \u03bb < 0.\nFact 6 (Berry-Esseen Theorem). Let X1, X2, ..., Xn be independent random variables with E[Xi] = 0, E[X 2 i ] = \u03c3 2 i > 0, and E[|Xi|3] = \u03c1i < \u221e. Let\nSn = X1 +X2 + ...+Xn \u221a\n\u03c321 + ...+ \u03c3 2 n\nand denote Fn the cumulative distribution function of Sn and\u03a6 the cumulative distribution function of the standard normal distribution. Then for all n, there exists an absolute constant C1 such that\nsupx\u2208R|Fn(x)\u2212 \u03a6(x)| \u2264 C1\u03c81\nwhere \u03c81 = ( n \u2211\ni=1\n\u03c32i ) \u22121/2 max1\u2264i\u2264n \u03c1i \u03c32i . The best upper bound on C1 known is C1 \u2264 0.56 (see\nShevtsova [2010]).\nFact 7 (Abramowitz and Stegun [1964] 26.5.21). Consider the regularized incomplete Beta function Iz(a, b) (cdf) for the Beta random variable with parameters (a, b). For any z such that (a + b \u2212 1)(1\u2212 z) \u2265 0.8, Iz(a, b) = \u03a6(y) + \u01eb, with |\u01eb| < 0.005 if a+ b > 6. Here \u03a6 is the standard normal CDF with\ny = 3[w1(1\u2212 19b )\u2212 w2(1 \u2212 19a )]\n[ w2 1 b + w2 2 a ] 1/2\n,\nwhere w1 = (bz) 1/3 and w2 = [a(1\u2212 z)]1/3.\nThe following lemma uses the above fact to lower bound the probability of a Beta random variable to exceed its mean by a quantity close to its standard deviation.\nLemma E.2 (Anti-concentration for Beta Random Variables). Let Fa,b denote the cdf of a Beta random variable with parameter (a, b), with a \u2265 6, b \u2265 6. Let z = aa+b +C \u221a ab (a+b)2(a+b+1) + C a+b , with C \u2264 0.5. Then, 1\u2212 F(a,b)(z) \u2265 1\u2212 \u03a6(1)\u2212 0.005 \u2265 0.15.\nProof. Let x = C \u221a\nab (a+b+1) + C. Then, z = a+x a+b ,w1 = (b(a + x)/(a + b)) 1/3 and w2 =\n[a(b \u2212 x)/(a + b))]1/3. Also, z \u2264 2C \u221a\nab a+b . Also, (a+ b \u2212 1)(1 \u2212 z) \u2265 (a + b \u2212 1)(1 \u2212 aa+b \u2212\nC \u221a\nab (a+b)2(a+b+1) \u2212 Ca+b ) = (a+ b\u2212 1)( ba+b \u2212 Ca+b\n\u221a\nab a+b+1 \u2212 Ca+b ) \u2265 a+b\u22121a+b (b\u2212C\n\u221a\nab a+b+1 \u2212\nC a+b ) \u2265 1112 (b\u2212C \u221a b\u2212 C12 ) \u2265 0.8. Hence we can apply Fact 7 relating Beta with Normal. We bound the numerator and denominator in the expression of y, to show that the relation Iz(a, b) \u2264 \u03a6(y) + \u01eb holds for some y \u2264 1.\nnumerator(y) = 3[w1(1\u2212 1\n9b )\u2212 w2(1\u2212\n1\n9a )]\n= 3( ab\na+ b )\n1 3 [(1 +\nx a ) 1 3 (1\u2212 1 9b )\u2212 (1 \u2212 x b ) 1 3 (1\u2212 1 9a )]\n\u2264 3( ab a+ b ) 1 3 [(1 + x 3a )(1\u2212 1 9b )\u2212 (1\u2212 x 3b \u2212 2x\n2\n9b2 )(1\u2212 1 9a )]\n= 3( ab\na+ b )\n1 3 [( b\u2212 a 9ab ) + ( x(a+ b) 3ab )\u2212 ( 2x 27ab )] + 3( ab a+ b ) 1 3 [ 2x2 9b2 (1\u2212 1 9a )]\n\u2264 3( ab a+ b ) 1 3 [( b\u2212 a 9ab ) + ( x(a+ b) 3ab )] + 3( ab a+ b ) 1 3 [ 2x2 9b2 (1 \u2212 1 9a )]\n= ( ab\na+ b )\n1 3 (\na+ b\nab )[( b\u2212 a 3(a+ b) ) + x+ 2x2 3b2 (1\u2212 1 9a )]\n\u2264 ( ab a+ b ) 1 3 ( a+ b ab )[( b\u2212 a 3(a+ b) ) + 2x2 3b2 (1\u2212 1 9a ) + C + C( ab a+ b ) 1 2 ]\n\u2264 ( b\u2212 a 3 \u221a ab(a+ b) +\n4C2 \u221a ab b2 \u221a a+ b + C \u221a a+ b\u221a ab + C)( ab a+ b ) 5 6 ( a+ b ab )\n\u2264 ( 1 3 \u221a 6 + 1 6 \u221a 6 + 1 2 \u221a 3 + 1 2 )( ab a+ b ) 5 6 ( a+ b ab ).\nIn above, we used that C \u2264 12 and a, b \u2265 6. Similarly,\ndenominator(y) = [ w21 b + w22 a ]1/2\n= ( ab a+ b )[ (1 + xa ) 2 3 b + (1\u2212 xb ) 2 3 a ] 1 2\n\u2265 ( ab a+ b\n) 1 3 [ (1 + 2x3a \u2212 x\n2\n9a2 )\nb + (1\u2212 2x3b ) a \u2212 x 2 9a2 ] 1 2\n= ( ab\na+ b )\n1 3 [ a(1 + 2x3a \u2212 x\n2 9a2 ) + b(1\u2212 2x3b \u2212 x 2 9b2 )\nab ] 1 2\n= ( ab\na+ b )\n1 3 (\na+ b ab (1\u2212 x\n2\n9ab ))\n1 2\n\u2265 ( ab a+ b ) 1 3 ( a+ b ab (1\u2212 4C\n2\n9(a+ b) ))\n1 2\n\u2265 ( ab a+ b ) 1 3 ( a+ b ab ( 107 108 )) 1 2 .\nHence we have that y \u2264 1 3 \u221a 6 + 1 6 \u221a 6 + 1 2 \u221a 3 + 1\n2\u221a 107 108 \u2264 1, so that Iz(a, b) \u2264 \u03c6(1) + \u01eb for \u01eb \u2264 0.005. The lemma statement follows by observing that 1 \u2212 F(a,b)(z) = 1 \u2212 Iz(a, b) \u2265 1 \u2212 \u03c6(1) \u2212 \u01eb \u2265 1\u2212 0.845\u2212 0.005 \u2265 0.15. Definition 5. For any X and Y real-valued random variables,X is stochastically optimistic for Y if for any u : R \u2192 R convex and increasing E[u(X)] \u2265 E[u(Y )]. Lemma E.3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PTV for V \u2208 [0, 1]S fixed and P \u223c Dirichlet(\u03b1) with \u03b1 \u2208 RS+ and \u2211S\ni=1 \u03b1i \u2265 2. Let X \u223c N(\u00b5, \u03c32) with \u00b5 =\n\u2211S i=1 \u03b1iVi\u2211 S i=1 \u03b1i , \u03c32 = ( \u2211S i=1 \u03b1i) \u22121, thenX is stochastically optimistic for Y .\nLemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6). Let Y\u0303 \u223c Beta(\u03b1, \u03b2) for any \u03b1, \u03b2 > 0 and X \u223c N( \u03b1\u03b1+\u03b2 , 1\u03b1+\u03b2 ). Then X is stochastically optimistic for Y\u0303 whenever \u03b1+ \u03b2 \u2265 2. Lemma E.5 (Dirichlet vs Beta optimism, Osband et al. [2014] Lemma 5). Let y = pT v for some random variable p \u223c Dirichlet(\u03b1) and constants v \u2208 Rd and \u03b1 \u2208 N d. Without loss of generality, assume v1 \u2264 v2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 vd. Let \u03b1\u0303 = \u2211d i=1 \u03b1i(vi\u2212v1)/(vd\u2212v1) and \u03b2\u0303 = \u2211d i=1 \u03b1i(vd\u2212vi)/(vd\u2212 v1). Then, there exists a random variable p\u0303 \u223c Beta(\u03b1\u0303, \u03b2\u0303) such that, for y\u0303 = p\u0303vd + (1 \u2212 p\u0303)v1, E[y\u0303|y] = E[y]. Lemma E.6. If E[X ] = E[Y ] and X is stochastically optimistic for Y , then \u2212X is stochastically optimistic for \u2212Y .\nProof. By Lemma 3.3 in Osband et al. [2014], X stochastically optimistic for Y is equivalent to having X =D Y + A + W with A \u2265 0 and E[W |Y + A] = 0 for all values y + a. Taking expectation of both sides, we get that E[X ] = E[Y ] + E[A] + E[W ] and since E[X ] = E[Y ] = 0 and E[W ] = E[E[W |Y + A]] = 0 we get that E[A] = 0. Since A \u2265 0, A = 0. Also note that E[W |Y = y] = 0 for all y. Now we can show that \u2212X is stochastically optimistic for \u2212Y as follows: From above, \u2212X =D \u2212(Y + A +W ) = \u2212Y + (\u2212W ). Then for all y\u2032, E[\u2212W | \u2212 Y = y\u2032] = \u2212E[W |Y = \u2212y\u2032] = 0 by definition ofW . Therefore,\u2212X is stochastically optimistic for\u2212Y . Corollary E.7. Let Y be any distribution with mean \u00b5 such that X \u223c N(\u00b5, \u03c32) is stochastically optimistic for Y . Then with probability 1\u2212 \u03c1,\n|Y \u2212 \u00b5| \u2264 \u221a 2\u03c32 log(2/\u03c1).\nProof. For any s > 0, and t, and applying Markov\u2019s inequality,\nP (Y \u2212 \u00b5 > t) = P (Y > \u00b5+ t) = P (esY > es(\u00b5+t)) \u2264 E[e sY ]\nes(\u00b5+t) .\nBy Definition 5, taking u(a) = esa, which is a convex and increasing function, E[esY ] \u2264 E[esX ], and hence\nP (Y \u2212 \u00b5 > t) \u2264 E[e sX ]\nes(\u00b5+t) =\ne\u00b5s+ 1 2 \u03c32s2\nes(\u00b5+t) = e\n1 2 \u03c32s2\u2212st.\nSince the above holds for all s > 0, using s = t\u03c32 , P (Y \u2212 \u00b5 > t) \u2264 e \u2212 t\n2\n2\u03c32 .\nSimilarly, for the lower tail bound, we have for any s > 0,\nP (Y \u2212 \u00b5 < \u2212t) = P (\u2212Y > \u2212\u00b5+ t) = P (es(\u2212Y ) > es(\u2212\u00b5+t)) \u2264 E[e s(\u2212Y )]\nes(\u2212\u00b5+t) .\nBy Lemma E.6, \u2212X is stochastically optimistic for\u2212Y , so E[es(\u2212Y )] \u2264 E[es(\u2212X)], and hence\nP (Y \u2212 \u00b5 < \u2212t) \u2264 E[e s(\u2212X)]\nes(\u2212\u00b5+t) =\ne\u2212\u00b5s+ 1 2 \u03c32s2\nes(\u2212\u00b5+t) = e\n1 2 \u03c32s2\u2212st.\nAgain letting s = t\u03c32 , P (Y \u2212 \u00b5 < \u2212t) \u2264 e \u2212 t\n2\n2\u03c32 .\nThen, for t = \u221a 2\u03c32 log(2/\u03c1), we have that\nP (|Y \u2212 \u00b5| \u2264 \u221a 2\u03c32 log(2/\u03c1)) \u2265 1\u2212 \u03c1.\nLemma E.8 (Binomial, Multinomial). Let Y\u0302 = p\u0302T v where p\u0302 \u2208 \u2206S be distributed as multinomial average with parameter n, p and fixed v \u2208 Rd, where 0 \u2264 vi \u2264 D. Then, there exists a random variable distributed as q\u0302 \u223c 1nBinomial(n, pTh D ) such that, E[q\u0302|Y\u0302 ] = 1D Y\u0302 .\nProof. LetXji , j = 1, . . . , n denote the outcomes of the trials used to define p\u0302i, that is,\np\u0302i :=\nn \u2211\nj=1\nXji /n\nwhereXji , j = 1, . . . , n are distributed as X j i \u223c Multivariate(p, 1).\nFor every i, define n i.i.d. variables Y ji , j = 1, . . . , n, where Y j i \u223c Bernoulli(vi/D), and is independent ofXji . Define q\u0302 as:\nq\u0302 = 1\nn\n\u2211\ni\nn \u2211\nj=1\nXji Y j i /n\nLet X = {Xi,j , i = 1, . . . , S, j = 1, . . . , n}. Then,\nE[q\u0302|p\u0302T v, n] = E[E[q\u0302|X , p\u0302T v, n]|p\u0302T v, n] = E[E[q\u0302|X , n]|p\u0302T v, n]\n= 1\nn E[E[\n\u2211\ni,j\nXji Y j i |X , n]|p\u0302T v, n]\n= 1 n E[ \u2211\ni,j\nXji E[Y j i ]|p\u0302T v, n]\n= 1 n E[ \u2211\ni,j\nXji vi D |p\u0302T v, n]\n= p\u0302T v/D.\nAlso, nq\u0302 is a binomial random variableBinomial(n, 1Dp T v) since it is formed by sum of outcomes of n trials \u2211n\nj=1 Z j , where each trail Zj =\n\u2211 i X j i Y j i is an independent Bernoulli trial: takes value\n1 with probability \u2211\ni pivi/D.\nCorollary E.9. For X = Dq\u0302, Y = p\u0302T v (with q\u0302 and p\u0302T v as defined in the previous lemma), X is stochastically optimistic for Y .\nProof. We have E[X \u2212 Y |Y ] = E[Dq\u0302 \u2212 p\u0302T v|p\u0302T v] = 0. Then stochastic optimism follows from applying the optimism equivalence condition from Lemma 3 (Condition 3) of Osband et al. [2014]."}], "references": [{"title": "Bayesian optimal control of smoothly parameterized systems: The lazy posterior sampling algorithm", "author": ["Yasin Abbasi-Yadkori", "Csaba Szepesvari"], "venue": "arXiv preprint arXiv:1406.3926,", "citeRegEx": "Abbasi.Yadkori and Szepesvari.,? \\Q2014\\E", "shortCiteRegEx": "Abbasi.Yadkori and Szepesvari.", "year": 2014}, {"title": "Handbook of mathematical functions: with formulas, graphs, and mathematical tables, volume 55", "author": ["Milton Abramowitz", "Irene A Stegun"], "venue": "Courier Corporation,", "citeRegEx": "Abramowitz and Stegun.,? \\Q1964\\E", "shortCiteRegEx": "Abramowitz and Stegun.", "year": 1964}, {"title": "Analysis of Thompson Sampling for the Multi-armed Bandit Problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Agrawal and Goyal.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2013}, {"title": "Further Optimal Regret Bounds for Thompson Sampling", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In AISTATS,", "citeRegEx": "Agrawal and Goyal.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2013}, {"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["John Asmuth", "Lihong Li", "Michael L Littman", "Ali Nouri", "David Wingate"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Asmuth et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2009}, {"title": "Minimax regret bounds for reinforcement learning", "author": ["Mohammad Gheshlaghi Azar", "Ian Osband", "R\u00e9mi Munos"], "venue": "arXiv preprint arXiv:1703.05449,", "citeRegEx": "Azar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2017}, {"title": "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "author": ["Peter L Bartlett", "Ambuj Tewari"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Bartlett and Tewari.,? \\Q2009\\E", "shortCiteRegEx": "Bartlett and Tewari.", "year": 2009}, {"title": "R-max-a general polynomial time algorithm for nearoptimal reinforcement learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz.,? \\Q2002\\E", "shortCiteRegEx": "Brafman and Tennenholtz.", "year": 2002}, {"title": "Prior-free and prior-dependent regret bounds for Thompson sampling", "author": ["S\u00e9bastien Bubeck", "Che-Yu Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bubeck and Liu.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck and Liu.", "year": 2013}, {"title": "Optimal adaptive policies for Markov decision processes", "author": ["Apostolos N Burnetas", "Michael N Katehakis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Burnetas and Katehakis.,? \\Q1997\\E", "shortCiteRegEx": "Burnetas and Katehakis.", "year": 1997}, {"title": "An empirical evaluation of Thompson sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Chapelle and Li.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Li.", "year": 2011}, {"title": "Sample complexity of episodic fixed-horizon reinforcement learning", "author": ["Christoph Dann", "Emma Brunskill"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dann and Brunskill.,? \\Q2015\\E", "shortCiteRegEx": "Dann and Brunskill.", "year": 2015}, {"title": "Introduction to probability", "author": ["Charles Miller Grinstead", "James Laurie Snell"], "venue": "American Mathematical Soc.,", "citeRegEx": "Grinstead and Snell.,? \\Q2012\\E", "shortCiteRegEx": "Grinstead and Snell.", "year": 2012}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "On the sample complexity of reinforcement learning", "author": ["ShamMachandranath Kakade"], "venue": "PhD thesis,", "citeRegEx": "Kakade,? \\Q2003\\E", "shortCiteRegEx": "Kakade", "year": 2003}, {"title": "Thompson Sampling: An Optimal Finite Time Analysis", "author": ["Emilie Kaufmann", "Nathaniel Korda", "R\u00e9mi Munos"], "venue": "In International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Finite-sample convergence rates for Q-learning and indirect algorithms", "author": ["Michael J Kearns", "Satinder P Singh"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kearns and Singh.,? \\Q1999\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 1999}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Why is posterior sampling better than optimism for reinforcement learning", "author": ["Ian Osband", "Benjamin Van Roy"], "venue": "arXiv preprint arXiv:1607.00215,", "citeRegEx": "Osband and Roy.,? \\Q2016\\E", "shortCiteRegEx": "Osband and Roy.", "year": 2016}, {"title": "More) efficient reinforcement learning via posterior sampling", "author": ["Ian Osband", "Dan Russo", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "Generalization and exploration via randomized value functions", "author": ["Ian Osband", "Benjamin Van Roy", "Zheng Wen"], "venue": "arXiv preprint arXiv:1402.0635,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["Martin L Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q2014\\E", "shortCiteRegEx": "Puterman.", "year": 2014}, {"title": "Learning to Optimize Via Posterior Sampling", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo and Roy.,? \\Q2014\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2014}, {"title": "An Information-Theoretic Analysis of Thompson Sampling", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "Journal of Machine Learning Research (to appear),", "citeRegEx": "Russo and Roy.,? \\Q2015\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2015}, {"title": "PAC-Bayesian inequalities for martingales", "author": ["Yevgeny Seldin", "Fran\u00e7ois Laviolette", "Nicolo Cesa-Bianchi", "John Shawe-Taylor", "Peter Auer"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Seldin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2012}, {"title": "An improvement of convergence rate estimates in the Lyapunov theorem", "author": ["I.G. Shevtsova"], "venue": null, "citeRegEx": "Shevtsova.,? \\Q2010\\E", "shortCiteRegEx": "Shevtsova.", "year": 2010}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Alexander L Strehl", "Michael L Littman"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Strehl and Littman.,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2005}, {"title": "An analysis of model-based interval estimation for Markov decision processes", "author": ["Alexander L Strehl", "Michael L Littman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Strehl and Littman.,? \\Q2008\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2008}, {"title": "Optimistic linear programming gives logarithmic regret for irreducible MDPs", "author": ["Ambuj Tewari", "Peter L Bartlett"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tewari and Bartlett.,? \\Q2008\\E", "shortCiteRegEx": "Tewari and Bartlett.", "year": 2008}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "In Section D, we prove Lemma 4.1 bounding the diameter of extended MDP with high probability. And, in Section E we list some known results (or easy corollaries of known results) that are utilized in our proofs. A Anti-concentration of Dirichlet distribution We prove the following general result on anti-concentration", "author": ["Osband"], "venue": null, "citeRegEx": "Osband,? \\Q2014\\E", "shortCiteRegEx": "Osband", "year": 2014}, {"title": "Gaussian vs Dirichlet optimism, from Osband et al", "author": ["Lemma E"], "venue": "Let Y = PV for V \u2208 [0,", "citeRegEx": "E.3,? \\Q2014\\E", "shortCiteRegEx": "E.3", "year": 2014}], "referenceMentions": [{"referenceID": 30, "context": "More recently, posterior sampling, aka Thompson Sampling [Thompson, 1933], has emerged as another popular algorithm design principle in MAB, owing its popularity to a simple and extendible algorithmic structure, an attractive empirical performance [Chapelle and Li, 2011, Kaufmann et al.", "startOffset": 57, "endOffset": 73}, {"referenceID": 14, "context": "A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010].", "startOffset": 118, "endOffset": 139}, {"referenceID": 1, "context": "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of \u00d5(DS \u221a AT ) for this problem.", "startOffset": 93, "endOffset": 1278}, {"referenceID": 1, "context": "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of \u00d5(DS \u221a AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D.", "startOffset": 93, "endOffset": 1460}, {"referenceID": 1, "context": "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of \u00d5(DS \u221a AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of \u03a9( \u221a DSAT ) on the regret of any algorithm for this problem.", "startOffset": 93, "endOffset": 1531}, {"referenceID": 1, "context": "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of \u00d5(DS \u221a AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of \u03a9( \u221a DSAT ) on the regret of any algorithm for this problem. Our main contribution is a posterior sampling based algorithm with a high probability worst-case regret upper bound of \u00d5(D \u221a SAT + DSAT ), which is \u00d5(D \u221a SAT ) when T \u2265 SA. This improves the previously best known upper bound for this problem by a factor of \u221a S, and matches the dependence on S in the lower bound, for large enough T . Our algorithm uses an \u2018optimistic version\u2019 of the posterior sampling heuristic, while utilizing several ideas from the algorithm design structure in Jaksch et al. [2010], such as an epoch based execution and the extended MDP construction.", "startOffset": 93, "endOffset": 2142}, {"referenceID": 1, "context": "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of \u00d5(DS \u221a AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of \u03a9( \u221a DSAT ) on the regret of any algorithm for this problem. Our main contribution is a posterior sampling based algorithm with a high probability worst-case regret upper bound of \u00d5(D \u221a SAT + DSAT ), which is \u00d5(D \u221a SAT ) when T \u2265 SA. This improves the previously best known upper bound for this problem by a factor of \u221a S, and matches the dependence on S in the lower bound, for large enough T . Our algorithm uses an \u2018optimistic version\u2019 of the posterior sampling heuristic, while utilizing several ideas from the algorithm design structure in Jaksch et al. [2010], such as an epoch based execution and the extended MDP construction. The algorithm proceeds in epochs, where in the beginning of every epoch, it generates \u03c8 = \u00d5(S) sample transition probability vectors from a posterior distribution for every state and action, and solves an extended MDP with \u03c8A actions and S states formed using these samples. The optimal policy computed for this extended MDP is used throughout the epoch. Posterior Sampling for Reinforcement Learning (PSRL) approach has been used previously in Osband et al. [2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework.", "startOffset": 93, "endOffset": 2677}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework.", "startOffset": 8, "endOffset": 45}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework.", "startOffset": 8, "endOffset": 72}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA.", "startOffset": 8, "endOffset": 458}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H .", "startOffset": 8, "endOffset": 636}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper.", "startOffset": 8, "endOffset": 1289}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants.", "startOffset": 8, "endOffset": 1463}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants.", "startOffset": 8, "endOffset": 1494}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al.", "startOffset": 8, "endOffset": 1680}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al.", "startOffset": 8, "endOffset": 1712}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al.", "startOffset": 8, "endOffset": 1734}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al. [2009], Dann and Brunskill [2015].", "startOffset": 8, "endOffset": 1756}, {"referenceID": 0, "context": "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of \u00d5( \u221a HSAT ) when T \u2265 HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm\u2019s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al. [2009], Dann and Brunskill [2015]. There, the aim is to bound the performance of the policy Worst-case regret is a strictly stronger notion of regret in case the reward distribution function is known and only the transition probability distribution is unknown, as we will assume here for the most part.", "startOffset": 8, "endOffset": 1783}, {"referenceID": 27, "context": "Strehl and Littman [2005], Strehl and Littman [2008] provide an optimistic algorithm for bounding regret in a discounted reward setting, but the definition of regret is slightly different in that it measures the difference between the rewards of an optimal policy and the rewards of the learning algorithm along the trajectory taken by the learning algorithm.", "startOffset": 0, "endOffset": 26}, {"referenceID": 27, "context": "Strehl and Littman [2005], Strehl and Littman [2008] provide an optimistic algorithm for bounding regret in a discounted reward setting, but the definition of regret is slightly different in that it measures the difference between the rewards of an optimal policy and the rewards of the learning algorithm along the trajectory taken by the learning algorithm.", "startOffset": 0, "endOffset": 53}, {"referenceID": 22, "context": "For a communicating MDP M with diameter D: (a) (Puterman [2014] Theorem 8.", "startOffset": 48, "endOffset": 64}, {"referenceID": 22, "context": "For a communicating MDP M with diameter D: (a) (Puterman [2014] Theorem 8.1.2, Theorem 8.3.2) The optimal (maximum) gain \u03bb is state independent and is achieved by a deterministic stationary policy \u03c0, i.e., there exists a deterministic policy \u03c0 such that \u03bb := max s\u2032\u2208S max \u03c0 \u03bb(s) = \u03bb \u2217 (s), \u2200s \u2208 S. Here, \u03c0 is referred to as an optimal policy for MDP M. (b) (Tewari and Bartlett [2008], Theorem 4) The optimal gain \u03bb satisfies the following equations, \u03bb = min h\u2208RS max s,a rs,a + P T s,ah\u2212 hs = max a rs,a + P T s,ah \u2217 \u2212 h\u2217s, \u2200s (1) where h, referred to as the bias vector of MDP M, satisfies: max s hs \u2212min s hs \u2264 D.", "startOffset": 48, "endOffset": 385}, {"referenceID": 2, "context": ", using the techniques in Agrawal and Goyal [2013b].) The agent can use the past observations to learn the underlyingMDPmodel and decide future actions.", "startOffset": 26, "endOffset": 52}, {"referenceID": 2, "context": ", using the techniques in Agrawal and Goyal [2013b].) The agent can use the past observations to learn the underlyingMDPmodel and decide future actions. The goal is to maximize the total reward \u2211T t=1 rst,at , or equivalently, minimize the total regret over a time horizon T , defined as R(T,M) := T\u03bb \u2212\u2211Tt=1 rst,at (2) where \u03bb is the optimal gain of MDP M. We present an algorithm for the learning agent with a near-optimal upper bound on the regret R(T,M) for any communicating MDP M with diameter D, thus bounding the worst-case regret over this class of MDPs. 3 Algorithm Description Our algorithm combines the ideas of Posterior sampling (aka Thompson Sampling) with the extended MDP construction used in Jaksch et al. [2010]. Below we describe the main components of our algorithm.", "startOffset": 26, "endOffset": 730}, {"referenceID": 2, "context": ", using the techniques in Agrawal and Goyal [2013b].) The agent can use the past observations to learn the underlyingMDPmodel and decide future actions. The goal is to maximize the total reward \u2211T t=1 rst,at , or equivalently, minimize the total regret over a time horizon T , defined as R(T,M) := T\u03bb \u2212\u2211Tt=1 rst,at (2) where \u03bb is the optimal gain of MDP M. We present an algorithm for the learning agent with a near-optimal upper bound on the regret R(T,M) for any communicating MDP M with diameter D, thus bounding the worst-case regret over this class of MDPs. 3 Algorithm Description Our algorithm combines the ideas of Posterior sampling (aka Thompson Sampling) with the extended MDP construction used in Jaksch et al. [2010]. Below we describe the main components of our algorithm. Some notations: N t s,a denotes the total number of times the algorithm visited state s and played action a until before time t, and N t s,a(i) denotes the number of time steps among these N t s,a steps where the next state was i, i.e., a transition from state s to i was observed. We index the states from 1 to S, so that \u2211S i=1 N t s,a(i) = N t s,a for any t. We use the symbol 1 to denote the vector of all 1s, and 1i to denote the vector with 1 at the i th coordinate and 0 elsewhere. Doubling epochs: Our algorithm uses the epoch based execution framework of Jaksch et al. [2010]. An epoch is a group of consecutive rounds.", "startOffset": 26, "endOffset": 1372}, {"referenceID": 14, "context": "Extended MDP: The policy \u03c0\u0303k to be used in epoch k is computed as the optimal policy of an extended MDP M\u0303k defined by the sampled transition probability vectors, using the construction of Jaksch et al. [2010]. Given sampled vectors Q s,a, j = 1, .", "startOffset": 189, "endOffset": 210}, {"referenceID": 14, "context": "This line of argument bears similarities to the analysis of UCRL2 in Jaksch et al. [2010], but with tighter deviation bounds that we are able to guarantee due to the use of posterior sampling instead of deterministic optimistic bias used in UCRL2.", "startOffset": 69, "endOffset": 90}, {"referenceID": 20, "context": "The derivation of this bound utilizes and extends the stochastic optimism technique from Osband et al. [2014]. For s, a with N \u03c4k s,a \u2264 \u03b7, P\u0303s,a = Q s,a is a sample from the simple optimistic sampling, where we can only show the following weaker bound, but since this is used only while N \u03c4k s,a is small, the total contribution of this deviation will be small: max h\u2208[0,2D]S (P\u0303 k s,a \u2212 Ps,a)h \u2264 \u00d5 (", "startOffset": 89, "endOffset": 110}, {"referenceID": 14, "context": "Our algorithm may be viewed as a more efficient randomized version of the UCRL2 algorithm of Jaksch et al. [2010], with randomization via posterior sampling forming the key to the \u221a S factor improvement in the regret bound provided by our algorithm.", "startOffset": 93, "endOffset": 114}, {"referenceID": 20, "context": "Here, we utilize the stochastic optimism technique from Osband et al. [2014]. In Section D, we prove Lemma 4.", "startOffset": 56, "endOffset": 77}, {"referenceID": 22, "context": "6) in Puterman [2014]).", "startOffset": 6, "endOffset": 22}, {"referenceID": 31, "context": "1 Dirichlet concentration A similar result as the lemma below for concentration of Dirichlet random vectors was proven in Osband and Van Roy [2016]. We include (an expanded version of) the proof for completeness.", "startOffset": 122, "endOffset": 148}, {"referenceID": 31, "context": "1 Dirichlet concentration A similar result as the lemma below for concentration of Dirichlet random vectors was proven in Osband and Van Roy [2016]. We include (an expanded version of) the proof for completeness. Lemma C.1 (Osband and Van Roy [2016]).", "startOffset": 122, "endOffset": 250}, {"referenceID": 20, "context": "Now, since they have the same mean, from equivalence condition for stochastic optimism (Condition 3 in Lemma 3 of Osband et al. [2014]) E[DYv \u2212 p\u0303 v|p\u0303 v] = 0 for all values of v, p\u0303 v.", "startOffset": 114, "endOffset": 135}, {"referenceID": 20, "context": "Now, since they have the same mean, from equivalence condition for stochastic optimism (Condition 3 in Lemma 3 of Osband et al. [2014]) E[DYv \u2212 p\u0302 v|p\u0302 v] = 0 for all values of v, p\u0303 v.", "startOffset": 114, "endOffset": 135}, {"referenceID": 13, "context": "Then we notice that (I \u2212 Q\u2020\u03c0\u0303) is precisely the fundamental matrix of this absorbing Markov chain and hence exists and is non-negative (see Grinstead and Snell [2012], Theorem 11.", "startOffset": 140, "endOffset": 167}, {"referenceID": 25, "context": "E Useful deviation inequalities Fact 3 (Bernstein\u2019s Inequality, from Seldin et al. [2012] Lem 11/Cor 12).", "startOffset": 69, "endOffset": 90}, {"referenceID": 18, "context": "Fact 4 (Multiplicative Chernoff Bound, Kleinberg et al. [2008] Lemma 4.", "startOffset": 39, "endOffset": 63}, {"referenceID": 25, "context": "56 (see Shevtsova [2010]).", "startOffset": 8, "endOffset": 25}, {"referenceID": 1, "context": "Fact 7 (Abramowitz and Stegun [1964] 26.", "startOffset": 8, "endOffset": 37}, {"referenceID": 20, "context": "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1).", "startOffset": 40, "endOffset": 61}, {"referenceID": 20, "context": "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PV for V \u2208 [0, 1] fixed and P \u223c Dirichlet(\u03b1) with \u03b1 \u2208 R + and \u2211S i=1 \u03b1i \u2265 2. Let X \u223c N(\u03bc, \u03c3) with \u03bc = \u2211S i=1 \u03b1iVi \u2211 S i=1 \u03b1i , \u03c3 = ( \u2211S i=1 \u03b1i) , thenX is stochastically optimistic for Y . Lemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6).", "startOffset": 40, "endOffset": 327}, {"referenceID": 20, "context": "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PV for V \u2208 [0, 1] fixed and P \u223c Dirichlet(\u03b1) with \u03b1 \u2208 R + and \u2211S i=1 \u03b1i \u2265 2. Let X \u223c N(\u03bc, \u03c3) with \u03bc = \u2211S i=1 \u03b1iVi \u2211 S i=1 \u03b1i , \u03c3 = ( \u2211S i=1 \u03b1i) , thenX is stochastically optimistic for Y . Lemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6). Let \u1ef8 \u223c Beta(\u03b1, \u03b2) for any \u03b1, \u03b2 > 0 and X \u223c N( \u03b1 \u03b1+\u03b2 , 1 \u03b1+\u03b2 ). Then X is stochastically optimistic for \u1ef8 whenever \u03b1+ \u03b2 \u2265 2. Lemma E.5 (Dirichlet vs Beta optimism, Osband et al. [2014] Lemma 5).", "startOffset": 40, "endOffset": 522}, {"referenceID": 20, "context": "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PV for V \u2208 [0, 1] fixed and P \u223c Dirichlet(\u03b1) with \u03b1 \u2208 R + and \u2211S i=1 \u03b1i \u2265 2. Let X \u223c N(\u03bc, \u03c3) with \u03bc = \u2211S i=1 \u03b1iVi \u2211 S i=1 \u03b1i , \u03c3 = ( \u2211S i=1 \u03b1i) , thenX is stochastically optimistic for Y . Lemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6). Let \u1ef8 \u223c Beta(\u03b1, \u03b2) for any \u03b1, \u03b2 > 0 and X \u223c N( \u03b1 \u03b1+\u03b2 , 1 \u03b1+\u03b2 ). Then X is stochastically optimistic for \u1ef8 whenever \u03b1+ \u03b2 \u2265 2. Lemma E.5 (Dirichlet vs Beta optimism, Osband et al. [2014] Lemma 5). Let y = p v for some random variable p \u223c Dirichlet(\u03b1) and constants v \u2208 Rd and \u03b1 \u2208 N . Without loss of generality, assume v1 \u2264 v2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 vd. Let \u03b1\u0303 = \u2211d i=1 \u03b1i(vi\u2212v1)/(vd\u2212v1) and \u03b2\u0303 = \u2211d i=1 \u03b1i(vd\u2212vi)/(vd\u2212 v1). Then, there exists a random variable p\u0303 \u223c Beta(\u03b1\u0303, \u03b2\u0303) such that, for \u1ef9 = p\u0303vd + (1 \u2212 p\u0303)v1, E[\u1ef9|y] = E[y]. Lemma E.6. If E[X ] = E[Y ] and X is stochastically optimistic for Y , then \u2212X is stochastically optimistic for \u2212Y . Proof. By Lemma 3.3 in Osband et al. [2014], X stochastically optimistic for Y is equivalent to having X =D Y + A + W with A \u2265 0 and E[W |Y + A] = 0 for all values y + a.", "startOffset": 40, "endOffset": 1015}, {"referenceID": 20, "context": "Then stochastic optimism follows from applying the optimism equivalence condition from Lemma 3 (Condition 3) of Osband et al. [2014].", "startOffset": 112, "endOffset": 133}], "year": 2017, "abstractText": "We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of \u00d5(D \u221a SAT ) for any communicating MDP with S states, A actions and diameter D, when T \u2265 SA. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon T . This result improves over the best previously known upper bound of \u00d5(DS \u221a AT ) achieved by any algorithm in this setting, and matches the dependence on S in the established lower bound of \u03a9( \u221a DSAT ) for this problem. Our techniques involve proving some novel results about the anticoncentration of Dirichlet distribution, which may be of independent interest.", "creator": "LaTeX with hyperref package"}}}