{"id": "1705.07157", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Clustering under Local Stability: Bridging the Gap between Worst-Case and Beyond Worst-Case Analysis", "abstract": "Recently, classifying there has kalak been substantial samit interest in blowpipe clustering 111-106 research that bonano takes engulfing a gilstrap beyond worst - osasuna case approach to the analysis of imbecility algorithms. The typical idea is 55.64 to design amiya a marelli clustering 77s algorithm tappahannock that aline outputs subutai a qiyue near - optimal solution, provided the data glycogen satisfy thuban a omnipotence natural 88-75 stability limehouse notion. For example, Bilu and Linial (kady 2010) and Awasthi 40.8 et al. (sihk 2012) presented 4,685 algorithms paradigmatic that frostbitten output deng near - mpanda optimal nurtures solutions, unmoving assuming the rock-wallaby optimal solution laine is conneely preserved tomaso under small perturbations to run-dmc the pordes input 1,489 distances. A c.o.d. drawback brundibar to extensive this hendershot approach is tinnevelly that jagdalpur the algorithms 1331 are often netbeans explicitly built sorosky according to chenaran the hospitalist stability moriori assumption creutzfeldt-jakob and zviadauri give 5.71 no guarantees turhapuro in yem the worst heegner case; indeed, neretljak several taichiro recent algorithms output arbitrarily gaga bad solutions semington even when just wral-tv a small sinti section alisa of the data actioner does first-stage not 83-79 satisfy the 1-to-5 given chestnut-bellied stability a-porter notion.", "histories": [["v1", "Fri, 19 May 2017 19:30:33 GMT  (172kb,D)", "http://arxiv.org/abs/1705.07157v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["maria-florina balcan", "colin white"], "accepted": false, "id": "1705.07157"}, "pdf": {"name": "1705.07157.pdf", "metadata": {"source": "CRF", "title": "Clustering under Local Stability: Bridging the Gap between Worst-Case and Beyond Worst-Case Analysis", "authors": ["Maria-Florina Balcan", "Colin White"], "emails": ["ninamf@cs.cmu.edu,", "crwhite@cs.cmu.edu."], "sections": [{"heading": null, "text": "In this work, we address this concern in two ways. First, we provide algorithms that inherit the worst-case guarantees of clustering approximation algorithms, while simultaneously guaranteeing nearoptimal solutions when the data is stable. Our algorithms are natural modifications to existing state-ofthe-art approximation algorithms. Second, we initiate the study of local stability, which is a property of a single optimal cluster rather than an entire optimal solution. We show our algorithms output all optimal clusters which satisfy stability locally. Specifically, we achieve strong positive results in our local framework under recent stability notions including metric perturbation resilience (Angelidakis et al. 2017) and robust perturbation resilience (Balcan and Liang 2012) for the k-median, k-means, and symmetric/asymmetric k-center objectives.\n\u2217Authors\u2019 addresses: ninamf@cs.cmu.edu, crwhite@cs.cmu.edu. This work was supported in part by grants nsfccf 1535967, NSF CCF-1422910, NSF IIS-1618714, a Sloan Fellowship, a Microsoft Research Fellowship, and a National Defense Science and Engineering Graduate (NDSEG) fellowship.\nar X\niv :1\n70 5.\n07 15\n7v 1\n[ cs\n.D S]\n1 9\nM ay\n2 01"}, {"heading": "1 Introduction", "text": "Clustering is a fundamental problem in combinatorial optimization with numerous real-life applications in areas from bioinformatics to computer vision to text analysis and so on. The underlying goal is to group a given set of points to maximize similarity inside a group and minimize similarity among groups. A common approach to clustering is to set up an objective function and then approximately find the optimal solution according to the objective. Given a set of points S and a distance metric d, common clustering objectives include finding k centers to minimize the sum of the distance, or squared distance, from each point to its closest center (k-median and k-means, respectively), or to minimize the maximum distance from a point to its closest center (k-center). These popular objective functions are provably NP-hard to optimize [23, 28, 32], so research has focused on finding approximation algorithms. This has attracted significant attention in the theoretical computer science community [5, 15, 16, 17, 18, 23, 33].\nTraditionally, the theory of clustering (and more generally, the theory of algorithms) has focused on the analysis of worst-case instances. While this approach has led to many elegant algorithms and lower bounds, it is often overly pessimistic of an algorithm\u2019s performance on \u201ctypical\u201d instances or real world instances. A rapidly developing line of work in the algorithms community, the so-called beyond worstcase analysis of algorithms (BWCA), considers designing algorithms for instances that satisfy some natural structural properties. BWCA has given rise to many positive results [26, 31, 37], especially for clustering problems [6, 7, 10, 30]. For example, the popular notion of \u03b1-perturbation resilience, introduced by Bilu and Linial [14], informally states that the optimal solution does not change when the input distances are allowed to increase by up to a factor of \u03b1. This definition seeks to capture a phenomenon in practice: the optimal solution is often significantly better than all other solutions, thereby the optimal solution does not change even when the input is slightly perturbed.\nHowever, there are two potential downsides to this approach. The first downside is that many of these algorithms aggressively exploit the given structural assumptions, which can lead to contrived algorithms with no guarantees in the worst case. Therefore, a user can only use algorithms from this line of work if she is certain her data satisfies the assumption (even though none of these assumptions are computationally efficient to verify). The second downside is that while the algorithms return the optimal solution when the input is stable, there are no partial guarantees when most, but not all, of the data is stable. For example, the algorithms of Balcan and Liang [12] and Angelidakis et al. [3] return the optimal clustering when the instance is resilient to perturbations, however, both algorithms use a dynamic programming subroutine that is susceptible to errors which can propagate when a small fraction of the data does not satisfy perturbation resilience (see Appendix A for more details). From these setbacks, two natural questions arise. (1) Can we find natural algorithms that achieve the worst-case approximation ratio, while outputting the optimal solution if the data is stable, 1 and (2) Can we construct robust algorithms that still output good results even when only part of the data satisfies stability? The current work seeks to answer both of these questions."}, {"heading": "1.1 Our results and techniques", "text": "In this work, we answer both questions affirmatively for a variety of clustering objectives under perturbation resilience. We present algorithms that simultaneously achieve state-of-the-art approximation ratios in the worst case, while outputting the optimal solution when the data is stable. All of our algorithms are natural modifications to existing approximation algorithms. To answer question (2), we define the notion of local perturbation resilience in Section 2. This is the first definition that applies to an individual cluster, rather than the dataset as a whole. Informally, an optimal cluster satisfies \u03b1-local perturbation resilience if it remains in the optimal solution under any \u03b1 perturbation to the input. We show that every optimal cluster\n1 A trivial solution is to run an approximation algorithm and a BWCA algorithm in parallel, and output the better of the two solutions. We seek a more satisfactory and \u201cnatural\u201d answer to this question.\nsatisfying local stability will be returned by our algorithms. Therefore, given an instance with a mix of stable and non-stable data, our algorithms will return the optimal clusters over the stable data, and a worst-case approximation guarantee over the rest of the data. Specifically, we prove the following results.\nApproximation algorithms under local perturbation resilience In Section 3, we introduce a condition that is sufficient for an \u03b1-approximation algorithm to return the optimal k-median or k-means clusters satisfying \u03b1-local perturbation resilience. Intuitively, our condition requires that the approximation guarantee must be true locally as well as globally. We show the popular local search algorithm satisfies the property for a sufficiently large search parameter. For k-center, we show that any \u03b1-approximation algorithm for k-center will always return the clusters satisfying \u03b1-local metric perturbation resilience, which is a slightly weaker notion of perturbation resilience.\nAsymmetric k-center The asymmetric k-center problem admits an O(log\u2217 n) approximation algorithm due to Vishnwanathan [38], which is tight [19]. In Section 4, we show a simple modification to this algorithm ensures that it returns all optimal clusters satisfying a condition slightly stronger than 2-local perturbation resilience (all neighboring clusters must satisfy 2-local perturbation resilience). The first phase of Vishwanathan\u2019s approximation algorithm involves iteratively removing the neighborhood of special points called center-capturing vertices (CCVs). We show the centers of locally perturbation resilient clusters are CCVs and satisfy a separation condition, by constructing 2-perturbations in which neighboring clusters cannot be too close to the locally perturbation resilient centers without causing a contradiction. This allows us to modify the approximation algorithm by first removing the neighborhood around CCVs satisfying the separation condition. With a careful reasoning, we maintain the original approximation guarantee while only removing points from a single local perturbation resilient cluster at a time.\nRobust perturbation resilience In Section 5, we consider (\u03b1, )-local perturbation resilience, which states that at most n points can swap into or out of the cluster under any \u03b1-perturbation. For k-center, we show that any 2-approximation algorithm will return the optimal (3, )-locally perturbation resilient clusters, assuming a mild lower bound on optimal cluster sizes. To prove this, we show that if points from two different locally perturbation resilient clusters are close to each other, then k\u2212 1 centers achieve the optimal value under a carefully constructed 3-perturbation. The rest of the analysis involves building up conditional claims dictating the possible centers for each locally perturbation resilient cluster under the 3-perturbation. We utilize the idea of a cluster-capturing center [11] along with machinery specific to handling local perturbation resilience to show that a locally perturbation resilient cluster must split into two clusters under the 3-perturbation, causing a contradiction. Finally, we show that the mild lower bound on the cluster sizes is necessary. Specifically, we show hardness of approximation for k-median, k-means, and k-center, even when it is guaranteed the clustering satisfies (\u03b1, )-perturbation resilience for any \u03b1 \u2265 1 and > 0. In fact, the result holds even for a stronger notion called (\u03b1, )-approximation stability. The hardness is based on a reduction from the general clustering instances, so the APX-hardness constants match the worst-case APXhardness results of 2, 1.73, and 1.0013 for k-center [23], k-median [28], and k-means [32], respectively. This generalizes prior hardness results in BWCA [10, 11]."}, {"heading": "1.2 Related work", "text": "Clustering The first constant-factor approximation algorithm for k-median was given by Charikar et al. [16], and the current best approximation ratio is 2.675 by Byrka et al. [15]. Jain et al. proved k-median is NP-hard to approximate to a factor better than 1.73 [28]. For k-center, Gonzalez showed a tight 2- approximation algorithm [23]. For k-means, the best approximation ratio was recently lowered to 6.357 by\nAhmadian et al. [2]. k-means was shown to be APX-hard by Awasthi et al. [8], and the constant was recently improved to 1.0013 [32]. Perturbation resilience Perturbation resilience was introduced by Bilu and Linial, who showed algorithms that outputted the optimal solution for max cut under \u2126( \u221a n)-perturbation resilience [14]. This\nresult was improved by Markarychev et al. [34], who showed the standard SDP relaxation is integral for \u2126( \u221a\nlog n log logn)-perturbation resilient instances. They also show an optimal algorithm for minimum multiway cut under 4-perturbation resilience. The study of clustering under perturbation resilience was initiated by Awasthi et al. [7], who provided an optimal algorithm for center-based clustering objectives (which includes k-median, k-means, and k-center clustering, as well as other objectives) under 3-perturbation resilience. This result was improved by Balcan and Liang [12], who showed an algorithm for center-based clustering under (1 + \u221a 2)-perturbation resilience. They also gave a near-optimal algorithm for k-median (2 + \u221a\n3, )-perturbation resilience, a robust version of perturbation resilience, when the optimal clusters are not too small. Balcan et al. [11] constructed algorithms for k-center and asymmetric k-center under 2- perturbation resilience and (3, )-perturbation resilience, and they showed no polynomial-time algorithm can solve k-center under (2 \u2212 )-approximation stability (a notion that is stronger than perturbation resilience) unless NP = RP . Recently, Angelidakis et al. [3], gave algorithms for center-based clustering under 2- perturbation resilience and minimum multiway cut with k terminals under (2\u22122/k)-perturbation resilience. They also define the more general notion of metric perturbation resilience. In Appendix A, we discuss prior work in the context of local perturbation resilience. Perturbation resilience has also been applied to other problems, such as the traveling salesman problem, and finding Nash equilibria [10, 35].\nApproximation stability Approximation stability is a related definition that is stronger than perturbation resilience. It was introduced by Balcan et al. [10], who showed algorithms that outputted nearly optimal solutions under (\u03b1, )-approximation stability for k-median and k-means when \u03b1 > 1. Balcan et al. [13] studied a relaxed notion of approximation stability in which a specified \u03bd fraction of the data satisfies approximation stability. In this setting, there may not be a unique approximation stable solution. The authors provided an algorithm which outputted a small list of clusterings, such that all approximation stable clusterings are close to one clustering in the list. We remark the property itself is similar in spirit to local stability, although the solution/results are much different. Voevodski et al. [39] gave an algorithm for empirically clustering protein sequences using the min-sum objective under approximation stability, which compares favorably to popular clustering algorithms used in practice. Gupta et al. [25] showed algorithm for finding near-optimal solutions for k-median under approximation stability in the context of finding triangle-dense graphs.\nOther stability notions Ostrovsky et al. show how to efficiently cluster instances in which the k-means clustering cost is much lower than the (k \u2212 1)-means cost [36]. Kumar and Kannan give an efficient clustering algorithm for instances in which the projection of any point onto the line between its cluster center to any other cluster center is a large additive factor closer to its own center than the other center [30]. This result was later improved along multiple axes by Awasthi and Sheffet [9]. There are many other works that show positive results for different natural notions of stability in various settings [4, 6, 25, 26, 30, 31, 37]."}, {"heading": "2 Preliminaries", "text": "A clustering instance consists of a set S of n points, as well as a distance function d : S \u00d7 S \u2192 R\u22650. For a point u \u2208 S and a set A \u2286 S, we define d(u,A) = minv\u2208A d(u, v). The k-median, k-means, and k-center objectives are to find a set of points X = {x1, . . . , xk} \u2286 S called centers to minimize \u2211 v\u2208S d(v,X),\u2211\nv\u2208S d(v,X) 2, and maxv\u2208S d(v,X), respectively. We denote by VorX(x) the Voronoi tile of x induced\nby X on the set of points S, and we denote VorX(X \u2032) = \u22c3 x\u2208X\u2032 VorX(x) for a subset X\n\u2032 \u2286 X . We refer to the Voronoi partition induced by X as a clustering. Throughout the paper, we denote the clustering with minimum cost by OPT = {C1, . . . , Ck}, and we denote the optimal centers by c1, . . . , ck, where ci is the center of Ci for all 1 \u2264 i \u2264 k.\nAll of the distance functions we study are metrics, except for Section 4, in which we study an asymmetric distance function. An asymmetric distance function satisfies all the properties of a metric space except for symmetry. In particular, an asymmetric distance function must satisfy the directed triangle inequality: for all u, v, w \u2208 S, d(u,w) \u2264 d(u, v) + d(v, w).\nWe formally define perturbation resilience, a notion introduced by Bilu and Linial [14]. d\u2032 is called an \u03b1-perturbation of the distance function d, if for all u, v \u2208 S, d(u, v) \u2264 d\u2032(u, v) \u2264 \u03b1d(u, v). (We only consider perturbations in which the distances increase because WLOG we can scale the distances to simulate decreasing distances.)\nDefinition 1. A clustering instance (S, d) satisfies \u03b1-perturbation resilience (\u03b1-PR) if for any \u03b1-perturbation d\u2032 of d, the optimal clustering under d\u2032 is unique and equal to OPT .\nNow we define local perturbation resilience, a property of an optimal cluster rather than a dataset.\nDefinition 2. Given a clustering instance (S, d) with optimal clustering C = {C1, . . . , Ck}, an optimal cluster Ci satisfies \u03b1-local perturbation resilience (\u03b1-LPR) if for any \u03b1-perturbation d\u2032 of d, the optimal clustering C\u2032 under d\u2032 contains Ci.\nWe will sometimes refer to a center ci of an \u03b1-LPR clusterCi as an \u03b1-LPR center. Clearly, if a clustering instance is perturbation resilient, then every optimal cluster satisfies local perturbation resilience. Now we will show the converse is also true.\nFact 3. A clustering instance (S, d) satisfies \u03b1-PR if and only if each optimal cluster satisfies \u03b1-LPR.\nProof. Given a clustering instance (S, d), the forward direction follows by definition: assume (S, d) satisfies \u03b1-PR, and given an optimal cluster Ci, then for each \u03b1-perturbation d\u2032, the optimal clustering stays the same under d\u2032, therefore Ci is contained in the optimal clustering under d\u2032. Now we prove the reverse direction. Given a clustering instance with optimal clustering C, and given an \u03b1-perturbation d\u2032, let the optimal clusetring under d\u2032 be C\u2032. For each Ci \u2208 C, by assumption, Ci satisfies \u03b1-LPR, so Ci \u2208 C\u2032. Therefore C = C\u2032.\nIn Section 4, we define a stronger version of Definition 2 specifically for k-center. Next, we define a more robust version of \u03b1-PR and \u03b1-LPR that allows a small change in the optimal clusters when the distances are perturbed. We say that two clusters A and B are -close if they differ by only n points, i.e., |A \\B|+ |B \\A| \u2264 n. We say that two clusterings C and C\u2032 are -close if min\u03c3 \u2211k i=1 |Ci \\ C \u2032\u03c3(i)| \u2264 n.\nDefinition 4. [12] A clustering instance (S, d) satisfies (\u03b1, )-perturbation resilience ((\u03b1, )-PR) if for any \u03b1- perturbation d\u2032 of d, all optimal clusterings under d\u2032 must be -close to OPT .\nDefinition 5. Given a clustering instance (S, d) with optimal clustering C = {C1, . . . , Ck}, an optimal cluster Ci satisfies (\u03b1, )-local perturbation resilience ((\u03b1, )-LPR) if for any \u03b1-perturbation d\u2032 of d, the optimal clustering C\u2032 under d\u2032 contains a cluster C \u2032i which is -close to Ci.\nWe prove a statement similar to Fact 3, for (\u03b1, )-PR, but the error adds up among the clusters. See Appendix B for the proof.\nLemma 6. A clustering instance (S, d) satisfies (\u03b1, )-PR if and only if each optimal cluster Ci satisfies (\u03b1, i)-LPR and \u2211 i i \u2264 2 n.\nIn all definitions thus far, we do not assume that the \u03b1-perturbations satisfy the triangle inequality. Angelidakis et al. [3] recently studied the weaker definition in which the \u03b1-perturbations must satisfy the triangle inequality, called metric perturbation resilience. All of our definitions can be generalized accordingly, and some of our results hold under this weaker assumption. To this end, we will sometimes take the metric completion d\u2032 of a non-metric distance function d\u2032\u2032, by setting the distances in d\u2032 as the length of the shortest path on the graph whose edges are the lengths in d\u2032\u2032."}, {"heading": "3 Approximation algorithms under local perturbation resilience", "text": "In this section, we show that local search for k-median will always return the (3 + )-LPR clusters, and for k-means it will return the (9 + )-LPR clusters. We also show that any 2-approximation for k-center will return the 2-LPR clusters.\nk-median We start by giving a condition on an approximate k-median solution, which is sufficient to show the solution contains all \u03b1-LPR clusters.\nLemma 7. Given a k-median instance (S, d) and a set of k centers X , if for all sets Y of size k,\u2211 v\u2208VorX(X\\Y )\u222aVorY (Y \\X) d(v,X) \u2264 \u2211 v\u2208VorY (Y \\X) min(d(v,X), \u03b1d(v, Y )) + \u03b1 \u2211 v\u2208VorX(X\\Y ) d(v, Y ),\nthen all \u03b1-LPR clusters Ci are contained in the clustering defined by X .\nProof. Given such a set of centers X , we construct an \u03b1-perturbation d\u2032 as follows. Increase all distances by a factor of \u03b1 except for the distances between each point v and its closest center in X . Now our goal is to show that X is the optimal set of centers under d\u2032.\nGiven any other set Y of k centers, we consider four types of points: VorX(X \u2229 Y ) \u2229 VorY (X \u2229 Y ), VorX(X \u2229 Y ) \\ VorY (X \u2229 Y ), VorY (X \u2229 Y ) \\ VorX(X \u2229 Y ), and VorX(X \\ Y ) \u2229 VorY (Y \\X), which we denote by A1, A2, A3, and A4, respectively (see Figure 1a). The distance from a point v \u2208 S to its center in Y might stay the same under d\u2032, or increase, depending on its type. For each point v \u2208 A1, d\u2032(v, Y ) = d(v, Y ) = d(v,X) because these points have centers in X \u2229 Y . For each point v \u2208 A3 \u222a A4, d\u2032(v, Y ) = \u03b1d(v, Y ) because their centers are in Y \\ X . The points in A2 were originally closest to a center in Y \\ X , but might switch to their center in X , since it is in X \u2229 Y . Therefore, for each v \u2208 A2, d\u2032(v, Y ) = min(d(v,X), \u03b1d(v, Y )). Altogether,\u2211\nv\u2208S d\u2032(v, Y ) \u2265 \u2211 v\u2208A1 d(v,X) + \u2211 v\u2208A2 min(d(v,X), \u03b1d(v, Y )) + \u03b1 \u2211 v\u2208A3\u222aA4 d(v, Y ).\nThe cost of the clustering induced by X under d\u2032, using our assumption, is equal to\u2211 v\u2208S d\u2032(v,X) \u2264 \u2211 v\u2208A1 d(v,X) + \u2211 v\u2208A2 min(d(v,X), \u03b1d(v, Y )) + \u03b1 \u2211 v\u2208A3\u222aA4 d(v, Y ).\nTherefore, X is the optimal set of centers under the perturbation d\u2032. Given an \u03b1-LPR cluster Ci, by definition, there exists x \u2208 X such that VorX(x) = Ci under d\u2032, therefore by construction, VorX(x) = Ci under d as well. This proves the theorem.\nEssentially, Lemma 7 claims that any \u03b1-approximation algorithm will return the \u03b1-LPR clusters as long as the approximation ratio is \u201cuniform\u201d across all clusters. For example, a 3-approximation algorithm that returns half of the clusters paying 1.1 times their cost in OPT , and half of the clusters paying 5 times their\ncost in OPT , would fail the property. Luckily, the local search algorithm is well-suited for this property, due to the local optimum guarantee.\nThe local search algorithm starts with any set of k centers, and iteratively replaces t centers with t different centers if it leads to a better clustering (see Algorithm 1). The number of iterations is O(n ). The classic Local Search heuristic is widely used in practice, and many works have studied local search theoretically for k-median and k-means [5, 24, 29]. For more information on local search, see a general introduction by Aarts and Lenstra [1].\nAlgorithm 1 Local Search algorithm for k-median clustering Input: k-median instance (S, d), parameter\n1: Pick an arbitrary set of centers C of size k. 2: While \u2203C \u2032 of size k such that |C \\ C \u2032|+ |C \u2032 \\ C| \u2264 2 and cost(C\n\u2032) \u2264 (1\u2212 n)cost(C) \u2022 Replace C with C \u2032.\nOutput: Centers C\nThe next theorem utilizes Lemma 7 and a result by Cohen-Addad and Schwiegelshohn [21], who showed that local search returns the optimal clustering under a stronger version of (3 + 2 )-PR. For the formal proof of Theorem 8, see Appendix C.\nTheorem 8. Given a k-median instance (S, d), running local search with search size 1 returns a clustering that contains every (3 + 2 )-LPR cluster, and it gives a (3 + 2 )-approximation overall.\nThe value of local perturbation resilience, 3 + 2 , is tight due to a counterexample by Cohen-Addad et al. [20]. For k-means, we can use local search to return each (9 + )-LPR cluster by using Lemma 7 (which works for squared distances as well) and adding the result from Kanungo et al. [29].\nk-center Because the k-center objective takes the maximum rather than the average of all cluster costs, the equivalent of the condition in Lemma 7 is essentially satisfied by any \u03b1-approximation algorithm. We will now prove an even stronger result. Any \u03b1-approximation for k-center returns the \u03b1-LPR clusters, even for metric perturbation resilience. First we state a lemma which allows us to reason about a specific class\nof \u03b1-perturbations which will be useful in this section as well as throughout the paper, for symmetric and asymmetric k-center. For the full proofs, see Appendix C.\nLemma 9. Given \u03b1 \u2265 1 and an asymmetric k-center clustering instance (S, d) with optimal radius r\u2217, let d\u2032\u2032 denote an \u03b1-perturbation such that for all u, v, either d\u2032\u2032(u, v) = min(\u03b1r\u2217, \u03b1d(u, v)) or d\u2032\u2032(u, v) = \u03b1d(u, v). Let d\u2032 denote the metric completion of d\u2032\u2032. Then d\u2032 is an \u03b1-metric perturbation of d, and the optimal cost under d\u2032 is \u03b1r\u2217.\nProof sketch. First, d\u2032 is a valid \u03b1-metric perturbation of d because for all u, v, d(u, v) \u2264 d\u2032(u, v) \u2264 d\u2032\u2032(u, v) \u2264 \u03b1d(u, v). To show the optimal cost under d\u2032 is \u03b1r\u2217, it suffices to prove that for all u, v, if d(u, v) \u2265 r\u2217, then d\u2032(u, v) \u2265 \u03b1r\u2217. This is true of d\u2032\u2032 by construction, so we show it still holds after taking the metric completion of d\u2032\u2032, which can shrink some distances. Given u, v such that d(u, v) \u2265 r\u2217, there exists a path u = u0\u2013u1\u2013\u00b7 \u00b7 \u00b7 \u2013us\u22121\u2013us = v such that d\u2032\u2032(u, v) = \u2211s\u22121 i=0 d\n\u2032(ui, ui+1) and for all 0 \u2264 i \u2264 s \u2212 1, d\u2032(ui, ui+1) = d\n\u2032\u2032(ui, ui+1). If one of the segments has length \u2265 r\u2217 in d, then it has length \u2265 \u03b1r\u2217 in d\u2032\u2032 and we are done. If not, all distances increase by exactly a factor of \u03b1, so we sum up all distances to show d\u2032(u, v) \u2265 \u03b1r\u2217.\nTheorem 10. Given an asymmetric k-center clustering instance (S, d) and an \u03b1-approximate clustering C, each \u03b1-LPR cluster is contained in C, even under the weaker metric perturbation resilience condition.\nProof sketch. Similar to the proof of Lemma 7, we construct an \u03b1-perturbation d\u2032 and argue that C becomes the optimal clustering under d\u2032. Let r\u2217 denote the optimal k-center radius of (S, d). First we define an \u03b1-perturbation d\u2032\u2032 by increasing the distance from each point v \u2208 S to its center c in C to min{\u03b1r\u2217, \u03b1d(v, C(v))}, and increase all other distances by a factor of \u03b1. Then by Lemma 9, the metric completion d\u2032 of d\u2032\u2032 has optimal cost \u03b1r\u2217, and so C is the optimal clustering. Now we finish off the proof in a manner identical to Lemma 7.\nWe remark that Theorem 10 generalizes the result from Balcan et al. [11]. Although Theorem 10 applies more generally to asymmetric k-center, it is most useful for symmetric k-center, for which there exist several 2-approximation algorithms [22, 23, 27]. Asymmetric k-center is NP-hard to approximate to within a factor of o(log\u2217n) [19], so Theorem 10 only guarantees returning the O(log\u2217 n)-LPR clusters. In the next section, we show how to substantially improve this result.\n4 Asymmetric k-center\nIn this section, we show that a slight modification to the O(log\u2217 n) approximation algorithm of Vishwanathan [38] leads to an algorithm that maintains its performance in the worst case, while returning each cluster Ci with the following property: Ci is 2-LPR, and all nearby clusters are 2-LPR as well. This result also holds for metric perturbation resilience. We start by formally giving the stronger version of Definition 2. Throughout this section, we denote the optimal k-center radius by r\u2217.\nDefinition 11. An optimal cluster Ci satisfies \u03b1-strong local perturbation resilience (\u03b1-SLPR) if for each j such that there exists u \u2208 Ci, v \u2208 Cj and d(u, v) \u2264 r\u2217, then Cj is \u03b1-LPR.\nTheorem 12. Given an asymmetric k-center clustering instance (S, d) of size n, Algorithm 3 returns each 2-SLPR cluster exactly. For each 2-LPR cluster Ci, Algorithm 3 outputs a cluster that is a superset of Ci and does not contain any other 2-LPR cluster. These statements hold for metric perturbation resilience as well. Finally, the overall clustering returned by Algorithm 3 is an O(log\u2217 n)-approximation.\nApproximation algorithm for asymmetric k-center We start with a recap of theO(log\u2217 n)-approximation algorithm of Vishwanathan [38]. This was the first nontrivial algorithm for asymmetric k-center, and the approximation ratio was later proven to be tight [19]. To explain the algorithm, it is convenient to think of asymmetric k-center as a set covering problem. Given an asymmetric k-center instance (S, d), define the directed graph D(S,d) = (S,A), where A = {(u, v) | d(u, v) \u2264 r\u2217}. For a point v \u2208 S, we define \u0393+(v) and \u0393\u2212(v) as the set of vertices with an arc to and from v, respectively. The asymmetric k-center problem is equivalent to finding a subset C \u2286 S of size k such that \u222ac\u2208C\u0393+(c) = S. We also define \u0393\u2212x (v) and \u0393+x (v) as the set of vertices which have a path of length \u2264 x to and from v in D(S,d), respectively, and we define \u0393+x (A) = \u22c3 v\u2208A \u0393 + x (v) for a set A \u2286 S, and similarly for \u0393\u2212x (A). It is standard to assume the value of r\u2217 is known; since it is one of O(n2) distances, the algorithm can search for the correct value in polynomial time. Vishwanathan\u2019s algorithm crucially utilizes the following concept.\nDefinition 13. Given an asymmetric k-center clustering instance (S, d), a point v \u2208 S is a center-capturing vertex (CCV) if \u0393\u2212(v) \u2286 \u0393+(v). In other words, for all u \u2208 S, d(u, v) \u2264 r\u2217 implies d(v, u) \u2264 r\u2217.\nAs the name suggests, each CCV v \u2208 Ci, \u201ccaptures\u201d its center, i.e. ci \u2208 \u0393+(v) (see Figure 1b). Therefore, v\u2019s entire cluster is contained inside \u0393+2 (v), which is a nice property that the approximation algorithm exploits. At a high level, the approximation algorithm has two phases. In the first phase, the algorithm iteratively picks a CCV v arbitrarily and removes all points in \u0393+2 (v). This continues until there are no more CCVs. For every CCV picked, the algorithm is guaranteed to remove an entire optimal cluster. In the second phase, the algorithm runs log\u2217 n rounds of a greedy set-cover subroutine on the remaining points. See Algorithm 2. To prove the second phase terminates in O(log\u2217 n) rounds, the analysis crucially assumes there are no CCVs among the remaining points. We refer the reader to [38] for these details.\nAlgorithm 2 O(log\u2217 n) APPROXIMATION ALGORITHM FOR ASYMMETRIC k-CENTER [38] Input: Asymmetric k-center instance (S, d), optimal radius r\u2217 (or try all possible candidates)\nSet C = \u2205. Phase I: Pull out arbitrary CCVs While there exists an unmarked CCV\n\u2022 Pick an unmarked CCV c, add c to C, and mark all vertices in \u0393+2 (c) Phase II: Recursive set cover Set A0 = S \\ \u0393+5 (C), i = 0. While |Ai| > k: \u2022 Set A\u2032i+1 = \u2205. While there exists an unmarked point in Ai:\n\u2013 Pick v \u2208 S which maximizes \u0393+5 (v) \u2229Ai \u2013 Mark all points \u0393+5 (v) \u2229Ai and add v to A\u2032i+1.\n\u2022 Set Ai+1 = A\u2032i+1 \u2229A and i = i+ 1 Output: Centers C \u222aAi+1\nRobust algorithm for asymmetric k-center We show a small modification to Vishwanathan\u2019s approximation algorithm leads to simultaneous guarantees in the worst case and under local perturbation resilience. We show that each 2-LPR center is itself a CCV, and displays other structure which allows us to distinguish it from non-center CCVs. This suggests a simple modification to Algorithm 2: instead of picking CCVs arbitrarily, we first pick CCVs which display the added structure, and then when none are left, we go back to picking regular CCVs. However, we need to ensure that a CCV chosen by the algorithm marks points from at most one 2-LPR cluster, or else we will not be able to output a separate cluster for each 2-LPR\ncluster. Thus, the difficulty in our argument is carefully specifying which CCVs the algorithm picks, and which nearby points get marked by the CCVs, so that we do not mark other LPR clusters and simultaneously maintain the guarantee of the original approximation algorithm, namely that in every round, we mark an entire optimal cluster. To accomplish this tradeoff, we start by defining two properties. The first property will determine which CCVs are picked by the algorithm. The second property is used in the proof of correctness, but is not used explicitly by the algorithm. We give the full details of the proofs in Appendix D.\nDefinition 14. (1) A point c satisfies CCV-proximity if it is a CCV, and each point in \u0393\u2212(c) is closer to c than any CCV outside of \u0393+(c). That is, for all points v \u2208 \u0393\u2212(c) and CCVs c\u2032 /\u2208 \u0393+(c), d(c, v) < d(c\u2032, v). 2 (2) An optimal center ci satisfies center-separation if any point within distance r\u2217 of ci belongs to its cluster Ci. That is, for all v /\u2208 Ci, ci /\u2208 \u0393+(v).\nLemma 15. Given an asymmetric k-center clustering instance (S, d) and a 2-LPR cluster Ci, ci satisfies CCV-proximity and center-separation. Furthermore, given a CCV c \u2208 Ci, a CCV c\u2032 /\u2208 Ci, and a point v \u2208 Ci, we have d(c, v) < d(c\u2032, v).\nProof sketch. Given an instance (S, d) and a 2-LPR cluster Ci, we show that ci has the desired properties. Center Separation: Assume there exists a point v \u2208 Cj for j 6= i such that d(v, ci) \u2264 r\u2217. The idea is to construct a 2-perturbation in which v becomes the center for Ci, since the distance from v to each point in Ci is \u2264 2r\u2217 by the triangle inequality. Define d\u2032\u2032 by increasing all distances by a factor of 2, except for the distances between v and each point u in Ci, which we increase to min(2r\u2217, 2d(v, u)). By Lemma 9, the metric completion d\u2032 of d\u2032\u2032 is a 2-metric perturbation with optimal cost 2r\u2217, so we can replace ci with v in the set of optimal centers under d\u2032. However, now ci switches to a different cluster, contradicting 2-LPR.\nFinal property: Given CCVs c \u2208 Ci, c\u2032 \u2208 Cj , and a point v \u2208 Ci, assume d(c\u2032, v) \u2264 d(c, v). Again, we will use a perturbation to construct a contradiction. Since c and c\u2032 are CCVs and thus distance 2r\u2217 to their clusters, we can construct a 2-metric perturbation with optimal cost 2r\u2217 in which c and c\u2032 become centers for their respective clusters. Then v switches clusters, so we have a contradiction.\nCCV-proximity: By center-separation and the definition of r\u2217, we have that \u0393\u2212(ci) \u2286 Ci \u2286 \u0393+(ci), so ci is a CCV. Now given a point v \u2208 \u0393\u2212(ci) and a CCV c /\u2208 \u0393+(ci), from center-separation and definition of r\u2217, v \u2208 Ci and c \u2208 Cj for j 6= i. Then from the property in the previous paragraph, d(ci, v) < d(c, v).\nNow we can modify the algorithm so that it first chooses CCVs satisfying CCV-proximity. The other crucial change is instead of each chosen CCV c marking all points in \u0393+2 (c), it instead marks all points v such that v \u2208 \u0393+(c\u2032) for some c\u2032 \u2208 \u0393\u2212(c). See Algorithm 3. Note this new way of marking preserves the guarantee that each CCV c Ci marks its own cluster, because ci \u2208 \u0393\u2212(c). It also allows us to prove that each CCV c satisfying CCV-proximity can never mark an LPR center ci from a different cluster. Intuitively, if c marks ci, then there exists a point v \u2208 \u0393\u2212(ci) \u2229 \u0393\u2212(c), but there can never exist a point v distance \u2264 r\u2217 to two points satisfying CCV-proximity, since both would need to be closer to v by definition. Finally, the last property in Lemma 15 allows us to prove that when the algorithm computes the Voronoi tiles after Phase 1, all points will be correctly assigned. Now we are ready to prove Theorem 12.\nProof sketch of Theorem 12. First we explain why Algorithm 3 retains the approximation guarantee of Algorithm 2. Given any CCV c \u2208 Ci chosen in Phase I, c marks its entire cluster by definition, and we start Phase II with no remaining CCVs. This condition is sufficient for Phase II to return an O(log\u2217 n) approximation (Theorem 3.1 from [38]).\nNext we claim that for each 2-LPR cluster Ci, there exists a cluster outputted by Algorithm 3 that is a superset of Ci and does not contain any other 2-LPR cluster. To prove this claim, we first show there exists a point from Ci satisfying CCV-proximity that cannot be marked by any point from a different cluster in\n2 This property loosely resembles \u03b1-center proximity [7], a property defined over an entire clustering instance, which states for all i, for all v \u2208 Ci, j 6= i, we have \u03b1d(ci, v) < d(cj , v).\nAlgorithm 3 Robust algorithm for asymmetric k-center Input: Asymmetric k-center instance (S, d), distance r\u2217 (or try all possible candidates)\nSet C = \u2205. Redefine d using the shortest path length in D(S,d), breaking ties by distance to first common vertex in the shortest path. Phase I: Pull out special CCVs\n\u2022 While there exists an unmarked CCV: \u2013 Pick an unmarked point c which satisfies CCV-proximity. If no such c exists, then pick an\narbitrary unmarked CCV instead. Add c to C. \u2013 For all points c\u2032 \u2208 \u0393\u2212(c), mark all points in \u0393+(c\u2032).\n\u2022 For each c \u2208 C, let Vc denote c\u2019s Voronoi tile of the marked points induced by C. Phase II: Recursive set cover Run Phase II as in Algorithm 2, outputting Ai+1. Compute the Voronoi tile for each center in C \u222aAi+1, but a point in Vc must remain in c\u2019s Voronoi tile. 3\nPhase I. From Lemma 15, ci satisfies CCV-proximity and center-separation. If a point c /\u2208 Ci marks ci, then \u2203v \u2208 \u0393\u2212(c) \u2229 \u0393\u2212(ci). By center-separation, c /\u2208 \u0393\u2212(ci). Then from the definition of CCV-proximity, both c and ci must be closer to v than the other, causing a contradiction. At this point, we know a point c \u2208 Ci will always be chosen by the algorithm in Phase I. To finish the claim, we show that each point v from Ci is closer to c than to any other point c\u2032 /\u2208 Ci chosen in Phase I. Since c and c\u2032 are both CCVs, this follows directly from Lemma 15. However, it is possible that a center c\u2032 \u2208 Ai+1 is closer to v than c is to v, causing c\u2032 to \u201csteal\u201d v; this is unavoidable. Therefore, we forbid the algorithm from decreasing the size of the Voronoi tiles of C after Phase I.\nFinally, we claim that Algorithm 3 returns each 2-SLPR cluster exactly. Given a 2-SLPR cluster Ci, by our previous argument, the algorithm chooses a CCV c \u2208 Ci such that Ci \u2286 Vc. It is left to show that Vc \u2286 Ci. The intuition is that since Ci is 2-SLPR, its neighboring clusters were also marked in Phase I, and these clusters \u201cshield\u201d Vc from picking up superfluous points in Phase II. Specifically, there are two cases. If there exists v \u2208 Vc \u2286 Ci that was marked in Phase I, then we can prove that v comes from a 2-LPR cluster, so v \u2208 Vc contradicts our previous argument. If there exists v \u2208 Vc \u2286 Ci from Phase II, then the shortest path in D(S,d) from c to v is length at least 5 (see Figure 2a). The first point u\u2032 \u2208 Cj , j 6= i on the shortest path must come from a 2-LPR cluster, and we prove that v is closer to Cj\u2019s cluster using CCV-proximity."}, {"heading": "5 Robust local perturbation resilience", "text": "In this section, we show that any 2-approximation algorithm for k-center returns the optimal (3, )-SLPR clusters, provided the clusters are size > 2 n. Our main structural result is the following theorem.\nTheorem 16. Given a k-center clustering instance (S, d) with optimal radius r\u2217 such that all optimal clusters are size > 2 n and there are at least three (3, )-LPR clusters, then for each pair of (3, )-LPR clusters Ci and Cj , for all u \u2208 Ci and v \u2208 Cj , we have d(u, v) > r\u2217.\nLater in this section, we will show that both added conditions (the lower bound on the size of the clusters, and that there are at least three (3, )-LPR clusters) are necessary. Since the distance from each point to its closest center is\u2264 r\u2217, a corollary of Theorem 16 is that any 2-approximate solution must contain the optimal (3, )-SLPR clusters, as long as the 2-approximation satisfies two sensible conditions: (1) for every edge\nd(u, v) \u2264 2r\u2217 in the 2-approximation, \u2203w s.t. d(u,w) and d(w, v) are\u2264 r\u2217, and (2) there cannot be multiple clusters outputted in the 2-approximation that can be combined into one cluster with the same radius. Both of these properties are easily satisfied using quick pre- or post-processing steps. 4 We may also combine this result with Theorem 10 to obtain a more powerful result for k-center.\nTheorem 17. Given a k-center clustering instance (S, d) such that all optimal clusters are size > 2 n and there are at least three (3, )-LPR clusters, then any 2-approximate solution satisfying conditions (1) and (2) must contain all optimal 2-LPR clusters and (3, )-SLPR clusters.\nProof. Given such a clustering instance, then Theorem 16 ensures that there is no edge of length r\u2217 between points from two different (3, )-LPR clusters. Given a (3, )-SLPR cluster Ci, it follows that there is no point v /\u2208 Ci such that d(v, Ci) \u2264 r\u2217. Therefore, given a 2-approximate solution C satisfying condition (1), any u \u2208 Ci and v /\u2208 Ci cannot be in the same cluster. Furthermore, by condition (2), Ci must not be split into two clusters. Therefore, Ci \u2208 C. The second part of the statement follows directly from Theorem 10.\nProof idea for Theorem 16 The proof consists of two parts. The first part is to show that if two points from different LPR clusters are close together, then all points in the clustering instance must be near each other, in some sense (Lemma 22). The second part of the proof consists of showing that the points from three LPR clusters must be reasonably far from one another; therefore, we achieve the final result by contradiction.\nHere is the intution for part 1. Assume that there are points u \u2208 Ci and v \u2208 Cj from different LPR clusters, but d(u, v) \u2264 r\u2217. Then by the triangle inequality, the distance from u to Ci \u222a Cj is less than 3r\u2217. We show that under a suitable 3-perturbation, we can replace ci and cj with u in the set of optimal centers. So, there is a 3-perturbation in which the optimal solution uses just k \u2212 1 centers. However, as pointed out in [11], we are still a long way off from showing a contradiction. Since the definiton of local perturbation resilience reasons about sets of k centers, we must add a \u201cdummy center\u201d. But adding any point as a dummy center might not immediately result in a contradiction, if the voronoi partition \u201caccidentally\u201d outputs the LPR clusters. To handle this problem, we use the notion of a cluster-capturing center [11],\n4 For condition (1), before running the algorithm, remove all edges of distance > r\u2217, and then take the metric completion of the resulting graph. For condition (2), given the radius r\u0302 of the outputted solution, for each v \u2208 S, check if the ball of radius r\u0302 around v captures multiple clusters. If so, combine them.\nintuitively, a center which is within r\u2217 of most of the points of a different optimal cluster (see Figure 2b). This allows us to construct perturbations and control which points become centers for which clusters. We show all of the points in the instance are close together, in some sense.\nThe second part of the argument diverges from all previous work in perturbation resilience, since finding a contradiction under local perturbation resilience poses a novel challenge. From the previous part of the proof, we are able to find two noncenters p and q, which are collectively close to all other points in the dataset. Then we construct a 3-perturbation such that any size k subset of {ci}ki=1 \u222a {p, q} is an optimal set of centers. Our goal is to show that at least one of these subsets must break up a LPR cluster, causing a contradiction. There are many cases to consider, so we build up conditional structural claims dictating the possible centers for each LPR cluster under the 3-perturbation. For instance, if a center cj is the best center for a LPR cluster Ci under some set of optimal centers, then p or q must be the best center for Cj , otherwise we would arrive at a contradiction by definition of LPR (Lemma 24). We build up enough structural results to examine every possibility of center-cluster combinations, showing they all lead to contradictions, thus negating our original assumption.\nFormal analysis of Theorem 16 Now we give the proof details for Theorem 16. The first part of the proof resembles the argument for (3, )-perturbation resilience by Balcan et al. [11]. We start with the following fact.\nFact 18. Given a k-center clustering instance (S, d) such that all optimal clusters have size > 2 n, let d\u2032 denote an \u03b1-perturbation with optimal centers C \u2032 = {c\u20321, . . . , c\u2032k}. Let C\u2032 denote the set of (\u03b1, )-LPR clusters. Then there exists a one-to-one function f : C\u2032 \u2192 C \u2032 such that for all Ci \u2208 C\u2032, f(Ci) is the center for more than half of the points in Ci under d\u2032. 5\nIn words, for any set of optimal centers under an \u03b1-perturbation, each LPR cluster can be paired to a unique center. This follows simply because all optimal clusters are size > 2 n, yet under a perturbation, < n points can switch out of each LPR cluster. Next, we give the following definition, which will be a key point in the first part of the proof.\nDefinition 19. [11] A center ci is a first-order cluster-capturing center (CCC) for Cj if for all x 6= j, for all but n points v \u2208 Cj , d(ci, v) < d(cx, v) and d(ci, v) \u2264 r\u2217. ci is a second-order cluster-capturing center (CCC2) for Cj if there exists an ` such that for all x 6= j, `, for all but n points v \u2208 Cj , d(ci, v) < d(cx, v) and d(ci, v) \u2264 r\u2217. Then we say that ci is a CCC2 for cj discounting c`. See Figure 2b.\nIntuitively, a center cx is a CCC for Cy if cx is a valid center for Cy when cy is removed from the set of optimal centers. This is particularly useful when Cy is (\u03b1, )-LPR, since we can combine it with Fact 18 to show that cx is the unique center for the majority of points in Cy. Another key idea in our analysis is the following concept.\nDefinition 20. A set C \u2286 S (\u03b2, \u03b3)-hits S if for all s \u2208 S, there exist \u03b2 points in C at distance \u2264 \u03b3r\u2217 to s.\nWe present the following lemma to demonstrate the usefulness of Definition 20, although this lemma will not be used until the second half of the proof of Theorem 16.\nLemma 21. Given a k-center clustering instance (S, d), given z \u2265 0, and given a set C \u2286 S of size k + z which (z + 1, \u03b1)-hits S, there exists an \u03b1-perturbation d\u2032 such that all size k subsets of C are optimal sets of centers under d\u2032.\n5A non local version of this fact appeared in [11].\nProof. Consider the following perturbation d\u2032\u2032.\nd\u2032\u2032(s, t) =\n{ min(\u03b1r\u2217, \u03b1d(s, t)) if s \u2208 C and d(s, t) \u2264 \u03b1r\u2217\n\u03b1d(s, t) otherwise.\nThis is an \u03b1-perturbation by construction. Define d\u2032 as the metric completion of d\u2032\u2032. Then by Lemma 9, d\u2032 is an \u03b1-metric perturbation with optimal cost \u03b1r\u2217. Given any size k subset C \u2032 \u2286 C, then for all v \u2208 S, there is still at least one c \u2208 C \u2032 such that d(c, v) \u2264 \u03b1r\u2217, therefore by construction, d\u2032(c, v) \u2264 \u03b1r\u2217. It follows that C \u2032 is a set of optimal centers under d\u2032.\nNow we are ready to prove the first half of Theorem 16, stated in the following lemma. For the full details, see Appendix E.\nLemma 22. Given a k-center clustering instance (S, d) such that all optimal clusters are size > 2 n and there exist two points at distance r\u2217 from different (3, )-LPR clusters, then there exists a partition Sx \u222a Sy of the non-centers S \\ {c`}k`=1 such that for all pairs p \u2208 Sx, q \u2208 Sy, {c`}k`=1 \u222a {p, q} (3, 3)-hits S.\nProof sketch. This proof is split into two main cases. The first case is the following: there exists a CCC2 for a (3, )-LPR cluster, discounting a (3, )-LPR cluster. In fact, in this case, we do not need the assumption that two points from different LPR clusters are close. If there exists a CCC to a (3, )-LPR cluster, denote the CCC by cx and the cluster by Cy. Otherwise, let cx denote a CCC2 to a (3, )-LPR cluster Cy, discounting a (3, )-LPR center cz . Then cx is at distance \u2264 r\u2217 to all but n points in Cy. Therefore, d(cx, cy) \u2264 2r\u2217 and so cx is at distance \u2264 3r\u2217 to all points in Cy. Now we can create a 3-perturbation d\u2032 by increasing all distances by a factor of 3 except for the distances between cx and each point v \u2208 Cy, which we increase to min(3r\u2217, 3d(cx, v)). Then by Lemma 9, d\u2032 is a 3-perturbation with optimal cost 3r\u2217. Therefore, given any non-center v \u2208 S, the set of centers {c`}k`=1 \\ {cy} \u222a {v} achieves the optimal score, and from Fact 18, one of the centers in {c`}k`=1 \\ {cy} \u222a {v} must be the center for the majority of points in Cy under d\u2032. If this center is c`, ` 6= x, y, then by definition, c` is a CCC for the (3, )-LPR cluster, Cy, which creates a contradiction because ` 6= x. Therefore, either v or cx must be the center for the majority of points in Cy under d\u2032.\nIf cx is the center for the majority of points in Cy, then because Cy is (3, )-LPR, the corresponding cluster must contain fewer than n points from Cx. Furthermore, since for all ` 6= x and u \u2208 Cx, d(u, cx) < d(u, c`), it follows that v must be the center for the majority of points in Cx. Therefore, every non-center v \u2208 S is at distance \u2264 r\u2217 to the majority of points in either Cx or Cy.\nNow partition all the non-centers into two sets Sx and Sy, such that Sx = {p | for the majority of points q \u2208 Cx, d(p, q) \u2264 r\u2217} and Sy = {p | p /\u2208 Sx and for the majority of points q \u2208 Cy, d(p, q) \u2264 r\u2217}. Given p, q \u2208 Sx, then d(p, q) \u2264 2r\u2217 since both points are close to more than half the points in Cx. Similarly, any two points p, q \u2208 Sy are \u2264 2r\u2217 apart.\nNow we claim that {c`}k`=1 \u222a {p, q} (3, 3)-hits S for any pair p \u2208 Sx, q \u2208 Sy. This is because a point v \u2208 Ci from Sx is 3r\u2217 to p, ci, and cx and a point v \u2208 Cx is 3r\u2217 to cx, cy, and p. The latter follows because d(cx, cy) \u2264 2r\u2217. Similar statements are true for Sy and Cy.\nNow we turn to the other case. Assume there does not exist a CCC2 to a LPR cluster, discounting a LPR center. In this case, we need to use the assumption that there exist (3, )-LPR clusters Cx and Cy, and p \u2208 Cx, q \u2208 Cy such that d(p, q) \u2264 r\u2217. Then by the triangle inequality, p is distance \u2264 3r\u2217 to all points in Cx and Cy. Again we construct a 3-perturbation d\u2032 by increasing all distances by a factor of 3 except for the distances between p and v \u2208 Cx \u222a Cy, which we increase to min(3r\u2217, 3d(s, t)). By Lemma 9, d\u2032 has optimal cost 3r\u2217. Then given any non-center s \u2208 S, the set of centers {c`}k`=1 \\ {cx, cy} \u222a {p, s} achieves the optimal score.\nFrom Fact 18, one of the centers in {c`}k`=1 \\ {cx, cy} \u222a {p, s} must be the center for the majority of points in Cx under d\u2032. If this center is c` for ` 6= x, y, then c` is a CCC2 for Cx discounting cy, which\ncontradicts our assumption. Similar logic applies to the center for the majority of points in Cy. Therefore, p and s must be the centers for Cx and Cy. Since s was an arbitrary non-center, all non-centers are distance \u2264 r\u2217 to all but n points in either Cx or Cy.\nSimilar to Case 1, we now partition all the non-centers into two sets Sx and Sy, such that Sx = {u | for the majority of points v \u2208 Cx, d(u, v) \u2264 r\u2217} and Sy = {u | u /\u2208 Sx and for the majority of points v \u2208 Cy, d(u, v) \u2264 r\u2217}. As before, each pair of points in Sx are distance \u2264 2r\u2217 apart, and similarly for Sy.\nAgain we must show that {c`}k`=1\u222a{p, q} (3, 3)-hits S for each pair p \u2208 Sx, q \u2208 Sy. It is no longer true that d(cx, cy) \u2264 2r\u2217, however, we can prove that for both Sx and Sy, there exist points from two distinct clusters each. From the previous paragraph, given a non-center s \u2208 Ci for i 6= x, y, we know that p and s are centers for Cx and Cy. With an identical argument, given t \u2208 Cj for j 6= x, y, i, we can show that q and t are centers for Cx and Cy. It follows that Sx and Sy both contain points from at least two distinct clusters. Now given a non-center s \u2208 Ci, WLOG s \u2208 Sx, then there exists j 6= i and t \u2208 Cj \u2229 Sx. Then ci, cj , and p are 3r\u2217 to s and ci, cx, and p are 3r\u2217 to ci. In the case where i = x, then ci, cj , and p are 3r\u2217 to ci. This concludes the proof.\nNow we move to the second half of the proof of Theorem 16. Recall that our goal is to show a contradiction assuming two points from different LPR clusters are close. From Lemma 21 and Lemma 22, we know there is a set of k + 2 points, and any size k subset is optimal under a suitable perturbation. And by Lemma 18, each size k subset must have a mapping from LPR clusters to centers. Now we state a fact which states these mappings are derived from a ranking of all possible center points by the LPR clusters. In other words, each LPR cluster Cx can rank all the points in S, so that for any set of optimal centers for an \u03b1-perturbation, the top-ranked center is the one whose cluster is -close to Cx. We defer the proof to Appendix E.\nFact 23. Given a k-center clustering instance (S, d) such that all optimal clusters have size > 2 n, and an \u03b1-perturbation d\u2032 of d, let C\u2032 denote the set of (\u03b1, )-LPR clusters. For each Cx \u2208 C\u2032, there exists a bijection Rx,d\u2032 : S \u2192 [n] such that for all sets of k centers C that achieve the optimal cost under d\u2032, then c = argminc\u2032\u2208CRx,d\u2032(c \u2032) if and only if VorC(c) is -close to Cx.\nFor an LPR cluster Cx and a subset S\u2032 \u2286 S of size n\u2032, we also define Rx,d\u2032,S\u2032 : S\u2032 \u2192 [n\u2032] as the ranking specific to S\u2032. Now, using Fact 23 with the previous Lemmas, we can try to give a contradiction by showing that there is no set of rankings for the LPR clusters that is consistent with all the optimal sets of centers guaranteed by Lemmas 21 and 22. The following lemma gives relationships among the possible rankings. These will be our main tools for contradicting LPR and thus finishing the proof of Theorem 16. Again, the proof is provided in Appendix E.\nLemma 24. Given a k-center clustering instance (S, d) such that all optimal clusters are size > 2 n, and given non-centers p, q \u2208 S such that C = {c`}k`=1 \u222a {p, q} (3, 3)-hits S, let the set C\u2032 denote the set of (3, )-LPR clusters. Define the 3-perturbation d\u2032 as in Lemma 21. The following are true.\n1. Given Cx \u2208 C\u2032 and Ci such that i 6= x, Rx,d\u2032(cx) < Rx,d\u2032(ci).\n2. There do not exist s \u2208 C and Cx, Cy \u2208 C\u2032 such that x 6= y, and Rx,d\u2032,C(s) +Ry,d\u2032,C(s) \u2264 4.\n3. Given Ci and Cx, Cy \u2208 C\u2032 such that x 6= y 6= i, if Rx,d\u2032,C(ci) \u2264 3, then Ry,d\u2032,C(p) \u2265 3 and Ry,d\u2032,C(q) \u2265 3.\nWe are almost ready to bring everything together to give a contradiction. Recall that Lemma 22 allows us to choose a pair (p, q) such that {c`}k`=1\u222a{p, q} (3, 3)-hits S. For an arbitrary choice of p and q, we may not end up with a contradiction. It turns out, we will need to make sure one of the points comes from an LPR cluster, and is very high in the ranking list of its own cluster. This motivates the following fact, which is the final piece to the puzzle.\nFact 25. Given a k-center clustering instance (S, d) such that all optimal clusters are size > 2 n, given an (\u03b1, )-LPR cluster Cx, and given i 6= x, then there are fewer than n points s \u2208 Cx such that d(ci, s) \u2264 min(r\u2217, \u03b1d(cx, s)).\nProof. Assume the fact is false. Then let B \u2286 Cx denote a set of size n such that for all s \u2208 B, d(ci, s) \u2264 min(r\u2217, \u03b1d(cx, s)). Construct the following perturbation d\u2032. For all s \u2208 B, set d\u2032(cx, s) = \u03b1d(cx, s). For all other pairs s, t, set d\u2032(s, t) = d(s, t). This is clearly an \u03b1-perturbation by construction. Then the original set of optimal centers still achieves cost r\u2217 under d\u2032 because for all s \u2208 B, d\u2032(ci, s) \u2264 r\u2217. Clearly, the optimal cost under d\u2032 cannot be < r\u2217. It follows that the original set of optimal centers C is still optimal under d\u2032. However, all points in B are no longer in VorC(cx) under d\u2032, contradicting the fact that Cx is (\u03b1, )-LPR.\nNow we are ready to prove Theorem 16.\nProof of Theorem 16. Assume towards contradiction that there are two points at distance \u2264 r\u2217 from different (3, )-LPR clusters. Then by Lemma 22, there exists a partition S1, S2 of non-centers of S such that for all pairs p \u2208 S1, q \u2208 S2, {c`}k`=1 \u222a {p, q} (3, 3)-hit S. Given three (3, )-LPR clusters Cx, Cy, and Cz , let c\u2032x, c \u2032 y, and c \u2032 z denote the optimal centers ranked highest by Cx, Cy, and Cz disregarding cx, cy, and cz , respectively. Define p = argmins\u2208Cxd(ci, s), and WLOG let p \u2208 S1. Then pick an arbitrary point q from S2, and define C = {c`}k`=1 \u222a {p, q}. Define d\u2032 as in Lemma 21. We claim that Rx,d\u2032,C(p) < Rx,d\u2032,C(c\u2032x): from Fact 25, there are fewer than n points s \u2208 Cx such that d(c\u2032x, s) \u2264 min(r\u2217, 3d(cx, s)). Among each remaining point s \u2208 Cx, we will show d\u2032(p, s) \u2264 d\u2032(c\u2032x, s). Recall that d(p, s) \u2264 d(p, cx) + d(cx, s) \u2264 2r\u2217, so d\u2032(p, s) = min(3r\u2217, 3d(p, s)). There are two cases to consider.\nCase 1: d(c\u2032x, s) > r \u2217. Then by construction, d\u2032(c\u2032x, s) \u2265 3r\u2217, and so d\u2032(p, s) \u2264 d\u2032(c\u2032x, s). Case 2: 3d(cx, s) < d(c\u2032x, s). Then d \u2032(p, s) \u2264 3d(p, s) \u2264 3(d(p, cx) + d(cx, s)) \u2264 6d(cx, s) \u2264 2d(c\u2032x, s) \u2264 min(3r\u2217, 3d(c\u2032x, s)) = d\u2032(c\u2032x, s), and this proves our claim. Because Rx,d\u2032,C(p) < Rx,d\u2032,C(c\u2032x), it follows that either Rx,d\u2032,C(p) \u2264 2 or Rx,d\u2032,C(q) \u2264 2, since the top two can only be cx, p, or q. The rest of the argument is broken up into cases. Case 1: Rx,d\u2032,C(c\u2032x) \u2264 3. From Lemma 24, then Ry,d\u2032,C(p) \u2265 3 and Ry,d\u2032,C(q) \u2265 3. It follows by process of elimination that Ry,d\u2032,C(cy) = 1 and Ry,d\u2032,C(cy\u2032) = 2. Again by Lemma 24, Rx,d\u2032,C(p) \u2265 3 and Rx,d\u2032,C(q) \u2265 3, causing a contradiction.\nCase 2: Rx,d\u2032,C(cx\u2032) > 3 and Ry,d\u2032,C(cy\u2032) \u2264 3. Then Rx,d\u2032,C(p) \u2264 3 and Rx,d\u2032,C(q) \u2264 3. From Lemma 24, Rx,d\u2032,C(p) \u2265 3 and Rx,d\u2032,C(q) \u2265 3, therefore we have a contradiction. Note, the case where Rx,d\u2032,C(cx\u2032) > 3 and Rz,d\u2032,C(cz\u2032) \u2264 3 is identical to this case.\nCase 3: The final case is when Rx,d\u2032,C(cx\u2032) > 3, Ry,d\u2032,C(cy\u2032) > 3, and Rz,d\u2032,C(cz\u2032) > 3. So for each i \u2208 {x, y, z}, the top three for Ci in C is a permutation of {ci, p, q}. Then each i \u2208 {x, y, z} must rank p or q in the top two, so by the Pigeonhole Principle, either p or q is ranked top two by two different LPR clusters, contradicting Lemma 24. This completes the proof.\nWe note that Case 3 in Theorem 16 is the reason why we need to assume there are at least three (3, )- LPR clusters. If there are only two, Cx and Cy, it is possible that there exist u \u2208 Cx, v \u2208 Cy such that d(u, v) \u2264 r\u2217. In this case, for p, q, d\u2032, and C as defined in the proof of Theorem 16, if Cx ranks cx, p, q as its top three and Cy ranks cy, q, p as its top three, then there is no contradiction.\nAPX-Hardness under approximation stability Now we show the lower bound on the cluster sizes in Theorem 17 is necessary, by showing hardness of approximation even when it is guaranteed the clustering satisfies (\u03b1, )-perturbation resilience for \u03b1 \u2265 1 and > 0. In fact, this hardness holds even under the strictly stronger notion of approximation stability [10]. We say that two clusterings C and C\u2032 are -close if only an -fraction of the input points are clustered differently, i.e., min\u03c3 \u2211k i=1 |Ci \\ C \u2032\u03c3(i)| \u2264 n.\nDefinition 26. A clustering instance (S, d) satisfies (\u03b1, )-approximation stability ((\u03b1, )-AS) if any clustering C\u2032 (not necessarily a Voronoi partition) such that cost(C\u2032) \u2264 \u03b1cost(OPT ) is -close to OPT .\nThe hardness is based on a reduction from the general clustering instances, so the APX-hardness constants match the non-stable APX-hardness results.\nTheorem 27. Given \u03b1 \u2265 1, > 0, it is NP-hard to approximate k-center to 2, k-median to 1.73, or k-means to 1.0013, even when it is guaranteed the instance satisfies (\u03b1, )-approximation stability.\nThis theorem generalizes hardness results from [10] and [11]. Also, because of Fact 3, a corollary is that unless P = NP , there is no efficient algorithm which outputs each (\u03b1, )-LPR cluster for k-center (showing the condition on the cluster sizes in Theorem 17 is necessary).\nProof. Given \u03b1 \u2265 1, > 0, assume there exists a \u03b2-approximation algorithm A for k-median under (\u03b1, )- approximation stability. We will show a reduction to k-median without approximation stability. Given a k-median clustering instance (S, d) of size n, we will create a new instance (S\u2032, d\u2032) for k\u2032 = k + n/ with size n\u2032 = n/ as follows. First, set S\u2032 = S and d\u2032 = d, and then add n/ new points to S\u2032, such that their distance to every other point is 2\u03b1nmaxu,v\u2208S d(u, v). LetOPT denote the optimal solution of (S, d). Then the optimal solution to (S\u2032, d\u2032) is to use OPT for the vertices in S, and make each of the n/ added points a center. Note that the cost of OPT and the optimal clustering for (S\u2032, d\u2032) are identical, since the added points are distance 0 to their center. Given a clustering C on (S, d), let C\u2032 denote the clustering of (S\u2032, d\u2032) that clusters S as in C, and then adds n/ extra centers on each of the added points. Then the cost of C and C\u2032 are the same, so it follows that C is a \u03b2-approximation to (S, d) if and only if C\u2032 is a \u03b2-approximation to (S\u2032, d\u2032). Next, we claim that (S\u2032, d\u2032) satisfies (\u03b1, )-approximation stability. Given a clustering C\u2032 which is an \u03b1-approximation to (S\u2032, d\u2032), then there must be a center located at all n/ of the added points, otherwise the cost of C\u2032 would be > \u03b1OPT . Therefore, C\u2032 agrees with the optimal solution on all points except for S, therefore, C\u2032 must be -close to the optimal solution. Now that we have established a reduction, the theorem follows from hardness of 1.73-approximation for k-median [28]. The proofs for k-center and k-means are identical, using hardness from [23] and [32], respectively."}, {"heading": "6 Conclusion", "text": "In this work, we initiate the study of clustering under local stability. We define local perturbation resilience, a property of a single optimal cluster rather than the instance as a whole. We give algorithms that simultaneously achieve guarantees in the worst case, as well as guarantees when the data is stable.\nSpecifically, we show that local search outputs the optimal (3 + )-LPR clusters for k-median and the (9 + )-LPR clusters for k-means. For k-center, we show that any 2-approximation outputs the optimal 2-LPR clusters, as well as the optimal (3, )-LPR clusters when assuming the optimal clusters are not too small. We provide a natural modification to the asymmetric k-center approximation algorithm of Vishwanathan [38] to prove it outputs all 2-SLPR clusters. Finally, we show APX-hardness of clustering under (\u03b1, )-approximation stability for any \u03b1 \u2265 1, > 0. It would be interesting to find other approximation algorithms satisfying the condition in Lemma 7, and in general to further study local stability for other objectives and conditions."}, {"heading": "A Prior algorithms in the context of local stability", "text": "In this section, we discuss previous algorithms in the context of our new local stability framework. All of these results are useful under the standard definition of perturbation resilience for which they were designed. However, we will see that the prior algorithms (except for the one designed specifically for k-center) use the global structure of the data, which causes the algorithms to behave poorly when a fraction of the dataset is not perturbation resilient.\nWe start with the recent algorithm of Angelidakis et al. [3] to optimally cluster 2-perturbation resilient instances for any center-based objective (which includes k-median, k-means, and k-center). The algorithm is intuitively simple to describe. The first step is to create a minimum spanning tree T on the dataset. The second step is to perform dynamic programming on T to find the k-clustering with the lowest cost. The key fact is that 2-perturbation resilience implies that each cluster Ci is a connected subtree in T . This fact is partially preserved for each 2-LPR cluster Ci. For example, it can be made to hold if nearby clusters are also 2-LPR. However, the dynamic programming step heavily relies on the entire dataset satisfying stability. For instance, consider a dataset in which there are k clusters in a line, and all clusters are 2-LPR except for the cluster in the center. It is possible for the non-LPR cluster causes an offset in the dynamic program, which forces the minimum cost pruning to split each 2-LPR cluster in two, and merge each half with half of its\nneighboring cluster. Therefore, none of the LPR clusters are returned by the algorithm. We conclude that the dynamic programming step is not robust with respect to perturbation resilience.\nNext, we consider the algorithm of Balcan and Liang [12] to optimally cluster (1 + \u221a\n2)-perturbation resilient instances for center-based objectives. This was the leading algorithm for perturbation resilient instances until the recent result by Angelidakis et al. This algorithm also utilizes a dynamic programming step, although on a different type of tree. The algorithm starts with a linkage procedure, which the authors call closure linkage. It starts with n singleton sets, and merges the two sets with the minimum closure distance, which is the minimum radius that covers the sets and satisfies a margin condition that exploits the perturbation resilient structure. The merge procedure is iterated until all sets merge into a single set containing all n points. Thus, the algorithm creates a tree T where the leaves are the n singleton sets, each internal node is a set of points, and the root is the set of all n datapoints. If the dataset satisfies (1 + \u221a 2)-\nperturbation resilience, then each optimal cluster appears as a node in the tree. Then dynamic programming on T to find the minimum k-pruning outputs the optimal clusters. Just as in the previous algorithm, the initial guarantee is partially satisfied for LPR clusters. For example, if a cluster Ci and all nearby clusters satisfy (1+ \u221a 2)-LPR, then Ci will appear as a node in T . However, the dynamic programming step is again not robust to just a few non-LPR clusters. For example, consider a dataset where k/2 clusters are LPR, k/2 clusters are non-LPR. It is possible that the minimum closure distance for each non-LPR cluster contains all k/2 non-LPR clusters. Then the non-LPR clusters are grouped together in a single merge step, forcing the dynamic program to split every LPR cluster in half so that k clusters are outputted. In this example, the LPR clusters can be very far away from the non-LPR clusters, but the algorithm still fails.\nFinally, we consider the algorithm of Balcan et al. [11] which returns the optimal asymmetric k-center solution under 2-perturbation resilience. This algorithm starts by finding a subset of the datapoints which \u201cbehave symmetrically.\u201d We note that their symmetric set is equivalent to the set of all CCVs. Next, the algorithm uses a margin condition to separate out the optimal clusters in the symmetric set, and greedily attaches the non-symmetric points at the end. The first part of the algorithm will correctly output all LPR clusters within the symmetric set. However, when the non-symmetric points are reattached, many points from non-LPR clusters can attach to LPR clusters, forcing the final outputted clusters to have a huge radius. There is no easy fix to this algorithm, since the LPR clusters might \u201csteal\u201d the centers of non-LPR clusters, so the optimal radius of the remaining points may increase significantly.\nIn conclusion, existing algorithms for k-median and k-means heavily rely on the global structure provided by perturbation resilience. If just a single cluster does not satisfy local perturbation resilience, then the global structure is broken and the algorithm may not output any of the optimal clusters. Therefore, algorithms which exploit the local structure of perturbation resilience, such as in Section 3, are needed to have guarantees that are robust with respect to the level of perturbation resilience of the dataset. For k-center, the nature of the objective ensures that all algorithms have local guarantees, in some sense. However, it is still nontrivial to exploit the structure of the clusters satisfying local perturbation resilience, while ensuring the guarantees for the rest of the dataset are not arbitrarily bad (which is what we accomplish in Section 4)."}, {"heading": "B Proofs from Section 2", "text": "In this section, we give a proof of Lemma 6.\nLemma 6 (restated). A clustering instance (S, d) satisfies (\u03b1, )-PR if and only if each optimal cluster Ci satisfies (\u03b1, i)-LPR and \u2211 i i \u2264 2 n.\nProof. Given a clustering instance (S, d) satisfying (\u03b1, )-PR, given an \u03b1-perturbation d\u2032 and optimal clustering C \u20321, . . . , C \u2032 i under d \u2032, then there exists \u03c3 such that \u2211k\ni=1 |Ci \\C \u2032\u03c3(i)| \u2264 n. WLOG, we let \u03c3 equal the identity permutation. Now we claim that \u2211k i=1 |Ci \\ C \u2032i| = \u2211k i=1 |C \u2032i \\ Ci|. Intuitively, this is true because\nC1, . . . , Ck and C \u20321, . . . , C \u2032 k are both partitions of the same point set k. Formally,\nk\u2211 i=1 |Ci \\ C \u2032i| = k\u2211 i=1 |(Ci \u222a C \u2032i) \\ C \u2032i| because C \u2032i \u2286 C \u2032i\n= k\u2211 i=1 ( |Ci \u222a C \u2032i| \u2212 |C \u2032i| ) because C \u2032i \u2286 (Ci \u222a C \u2032i)\n= k\u2211 i=1 |Ci \u222a C \u2032i| \u2212 k\u2211 i=1 |C \u2032i|\n= k\u2211 i=1 |Ci \u222a C \u2032i| \u2212 k\u2211 i=1 |Ci| because k\u2211 i=1 |C \u2032i| = k\u2211 i=1 |Ci|\n= k\u2211 i=1 ( |Ci \u222a C \u2032i| \u2212 |Ci| ) =\nk\u2211 i=1 |(Ci \u222a C \u2032i) \\ Ci| because Ci \u2286 (Ci \u222a C \u2032i)\n= k\u2211 i=1 |C \u2032i \\ Ci| because Ci \u2286 Ci\nNow for each i, set i = |Ci \\C \u2032i|+ |C \u2032i \\Ci|. Clearly, this ensures Ci satisfies (\u03b1, i)-LPR. Finally, we have \u2211\ni i = \u2211 i ( |Ci \\ C \u2032i|+ |C \u2032i \\ Ci| ) = 2\n\u2211 i |Ci \\ C \u2032i|\n\u2264 2 n.\nNow we prove the reverse direction. Given a clustering instance (S, d) such that each cluster Ci satisfies (\u03b1, i)-LPR and \u2211 i i \u2264 2 n, given an \u03b1-perturbation d\u2032 and optimal clustering C \u20321, . . . , C \u2032i under d\u2032, then for each i, |Ci \\ C \u2032i|+ |C \u2032i \\ Ci| \u2264 i. Therefore,\u2211 i |Ci \\ C \u2032i| = 1 2 \u2211 i ( |Ci \\ C \u2032i|+ |C \u2032i \\ Ci|\n) \u2264 n.\nThis concludes the proof."}, {"heading": "C Proofs from Section 3", "text": "In this section, we give the details from Section 3.\nTheorem 8 (restated). Given a k-median instance (S, d), running local search with search size 1 returns a clustering that contains every (3 + 2 )-LPR cluster, and it gives a (3 + 2 )-approximation overall.\nProof. Given a k-median instance (S, d), letX denote a set of centers obtained by running local search with search size 1 . Let Y denote a different set of k centers. As in the proof of Lemma 7, let A1, A2, A3, and A4 denote VorX(X \u2229 Y ) \u2229VorY (X \u2229 Y ), VorX(X \u2229 Y ) \\VorY (X \u2229 Y ), VorY (X \u2229 Y ) \\VorX(X \u2229 Y ), and VorX(X \\ Y ) \u2229VorY (Y \\X), respectively (see Figure 1a). From Cohen-Addad and Schwiegelshohn [21], from Lemmas B.3 and B.4, we have the following.\u2211\nv\u2208A2\u222aA4\nd(v,X) \u2264 \u2211\nv\u2208A2\u222aA4\nd(v, Y ) + 2(1 + ) \u2211\nv\u2208A3\u222aA4\nd(v, Y )\nGiven a point v \u2208 A2, then its center in X is from X \u2229Y , and its center from Y is in Y \\X . We deduce that d(v, Y ) \u2264 d(v,X), otherwise v\u2019s center from Y would be the same as in X . Similarly, for a point v \u2208 A3, we can conclude that d(v,X) \u2264 d(v, Y ), by definition of A3.\nTherefore,\u2211 v\u2208A2\u222aA3\u222aA4 d(v,X) \u2264 \u2211 v\u2208A3 d(v,X) + \u2211 v\u2208A2\u222aA4 d(v,X)\n\u2264 \u2211 v\u2208A3 d(v, Y ) +  \u2211 v\u2208A2\u222aA4 d(v, Y ) + 2(1 + ) \u2211 v\u2208A3\u222aA4 d(v, Y )  \u2264 \u2211 v\u2208A2 d(v, Y ) + (3 + 2 ) \u2211 v\u2208A3\u222aA4 d(v, Y )\n\u2264 \u2211 v\u2208A2 min(d(v,X), (3 + 2 )d(v, Y )) + (3 + 2 ) \u2211 v\u2208A3\u222aA4 d(v, Y )\nNow the proof follows from Lemma 7.\nLemma 9 (restated). Given \u03b1 \u2265 1 and an asymmetric k-center clustering instance (S, d) with optimal radius r\u2217, let d\u2032\u2032 denote an \u03b1-perturbation such that for all u, v, either d\u2032\u2032(u, v) = min(\u03b1r\u2217, \u03b1d(u, v)) or d\u2032\u2032(u, v) = \u03b1d(u, v). Let d\u2032 denote the metric completion of d\u2032\u2032. Then d\u2032 is an \u03b1-metric perturbation of d, and the optimal cost under d\u2032 is \u03b1r\u2217.\nProof. By construction, d\u2032(u, v) \u2264 d\u2032\u2032(u, v) \u2264 \u03b1d(u, v). Since d satisfies the triangle inequality, we have that d(u, v) \u2264 d\u2032(u, v), so d\u2032 is a valid \u03b1-metric perturbation of d.\nNow given u, v such that d(u, v) \u2265 r\u2217, we will prove that d\u2032(u, v) \u2265 \u03b1r\u2217. By construction, d\u2032\u2032(u, v) \u2265 \u03b1r\u2217. Then since d\u2032 is the metric completion of d\u2032\u2032, there exists a path u = u0\u2013u1\u2013\u00b7 \u00b7 \u00b7 \u2013us\u22121\u2013us = v such that d\u2032(u, v) = \u2211s\u22121 i=0 d\n\u2032(ui, ui+1) and for all 0 \u2264 i \u2264 s \u2212 1, d\u2032(ui, ui+1) = d\u2032\u2032(ui, ui+1). If there exists an i such that d\u2032\u2032(ui, ui+1) \u2265 \u03b1r\u2217, then d\u2032(u, v) \u2265 \u03b1r\u2217 and we are done. Now assume for all 0 \u2264 i \u2264 s\u2212 1, d\u2032\u2032(ui, ui+1) < \u03b1r\u2217. Then by construction, d\u2032(ui, ui+1) = d\u2032\u2032(ui, ui+1) = \u03b1d(ui, ui+1), and so d\u2032(u, v) = \u2211s\u22121 i=0 d \u2032(ui, ui+1) = \u03b1 \u2211s\u22121\ni=0 d(ui, ui+1) \u2265 \u03b1d(u, v) \u2265 \u03b1r\u2217. We have proven that for all u, v, if d(u, v) \u2265 r\u2217, then d\u2032(u, v) \u2265 \u03b1r\u2217. Assume there exists a set of centers C \u2032 = {c\u20321, . . . , c\u2032k} whose k-center cost under d\u2032 is < \u03b1r\u2217. Then for all i and s \u2208 VorC\u2032,d\u2032(c\u2032i), d\u2032(c\u2032i, s) < r \u2217, implying d(c\u2032i, s) < \u03b1r \u2217 by construction. It follows that the k-center cost of C \u2032 under d is r\u2217, which is a contradiction. Therefore, the optimal cost under d\u2032 must be \u03b1r\u2217.\nTheorem 10 (restated). Given an asymmetric k-center clustering instance (S, d) and an \u03b1-approximate clustering C, each \u03b1-LPR cluster is contained in C, even under the weaker metric perturbation resilience condition.\nProof. Given an \u03b1-approximate solution C to a clustering instance (S, d), and given an \u03b1-LPR cluster Ci, we will create an \u03b1-perturbation as follows. For all v \u2208 S, set d\u2032\u2032(v, C(v)) = min{\u03b1r\u2217, \u03b1d(v, C(v))}. For all other points u \u2208 S, set d\u2032\u2032(v, u) = \u03b1d(v, u). Then by Lemma 9, the metric completion d\u2032 of d\u2032\u2032 is an \u03b1-perturbation of d with optimal cost \u03b1r\u2217. By construction, the cost of C is \u2264 \u03b1r\u2217 under d\u2032, therefore, C is an optimal clustering. Denote the set of centers of C by C. By definition of \u03b1-LPR, there exists vi \u2208 C such that VorC(vi) = Ci in d\u2032. Now, given v \u2208 Ci, argminu\u2208Cd\u2032(u, v) = vi, so by construction, argminu\u2208Cd(u, v) = vi. Therefore, VorC(vi) = Ci, so Ci \u2208 C."}, {"heading": "D Proofs from Section 4", "text": "In this section, we give the details of the proofs from Section 4.\nLemma 15 (restated). Given an asymmetric k-center clustering instance (S, d) and a 2-LPR cluster Ci, ci satisfies CCV-proximity and center-separation. Furthermore, given a CCV c \u2208 Ci, a CCV c\u2032 /\u2208 Ci, and a point v \u2208 Ci, we have d(c, v) < d(c\u2032, v).\nProof. Given an instance (S, d) and a 2-metric perturbation resilient cluster Ci, we show that ci has the desired properties.\nCenter Separation: Assume there exists a point v \u2208 Cj for j 6= i such that d(v, ci) \u2264 r\u2217. The idea is to construct a 2-perturbation in which v becomes the center for Ci.\nd\u2032\u2032(s, t) = { min(2r\u2217, 2d(s, t)) if s = v, t \u2208 Ci 2d(s, t) otherwise.\nd\u2032\u2032 is a valid 2-perturbation of d because for each point u \u2208 Ci, d(v, u) \u2264 d(v, ci) + d(ci, u) \u2264 2r\u2217. Define d\u2032 as the metric completion of d\u2032\u2032. Then by Lemma 9, d\u2032 is a 2-metric perturbation with optimal cost 2r\u2217. The set of centers {ci\u2032}ki\u2032=1 \\ {ci} \u222a {v} achieves the optimal cost, since v is distance 2r\u2217 from Ci, and all other clusters have the same center as in OPT (achieving radius 2r\u2217). But in this new optimal clustering, ci\u2019s center is a point in {ci\u2032}ki\u2032=1 \\ {ci} \u222a {v}, none of which are from Ci. We conclude that Ci is no longer an optimal cluster, contradicting 2-LPR.\nFinal property Next we prove the final part of the lemma. Given a CCV c from a 2-LPR cluster Ci, a CCV c\u2032 \u2208 Cj such that j 6= i, and a point v \u2208 Ci, and assume d(c\u2032, v) \u2264 d(c, v). We will construct a perturbation in which c and c\u2032 become centers of their respective clusters, and then v switches clusters. Define the following perturbation d\u2032\u2032.\nd\u2032\u2032(s, t) = { min(2r\u2217, 2d(s, t)) if s = c, t \u2208 Ci or s = c\u2032, t \u2208 Cj \u222a {v} 2d(s, t) otherwise.\nd\u2032\u2032 is a valid 2-perturbation of d because for each point u \u2208 Ci, d(c, u) \u2264 d(c, ci) + d(ci, u) \u2264 2r\u2217, for each point u \u2208 Cj , d(c\u2032, u) \u2264 d(c\u2032, cj) + d(cj , u) \u2264 2r\u2217, and d(c\u2032, v) \u2264 d(c, v) \u2264 d(c, ci) + d(ci, v) \u2264 2r\u2217. Define d\u2032 as the metric completion of d\u2032\u2032. Then by Lemma 9, d\u2032 is a 2-metric perturbation with optimal cost 2r\u2217. The set of centers {ci\u2032}ki\u2032=1 \\ {ci, cj}\u222a {c, c\u2032} achieves the optimal cost, since c and c\u2032 are distance 2r\u2217 from Ci and Cj , and all other clusters have the same center as in OPT (achieving radius 2r\u2217). Then since d\u2032(c\u2032, v) < d(c, v), v will not be in the same optimal cluster as ci, causing a contradiction.\nCCV-proximity: By center-separation, we have that \u0393\u2212(ci) \u2286 Ci, and by definition of r\u2217, we have that Ci \u2286 \u0393+(ci). Therefore, \u0393\u2212(ci) \u2286 \u0393+(ci), so ci is a CCV Now given a point v \u2208 \u0393\u2212(ci) and a CCV c /\u2208 \u0393+(ci), from center-separation and definition of r\u2217, v \u2208 Ci and c \u2208 Cj for j 6= i. Then from the property in the previous paragraph, d(ci, v) < d(c, v).\nTheorem 12 (restated). Given an asymmetric k-center clustering instance (S, d) of size n, Algorithm 3 returns each 2-SLPR cluster exactly. For each 2-LPR cluster Ci, Algorithm 3 outputs a cluster that is a superset of Ci and does not contain any other 2-LPR cluster. These statements hold for metric perturbation resilience as well. Finally, the overall clustering returned by Algorithm 3 is an O(log\u2217 n)-approximation.\nProof of Theorem 12. First we explain why Algorithm 3 retains the approximation guarantee of Algorithm 2. Given any CCV c \u2208 Ci chosen in Phase I, since c is a CCV, then ci \u2208 \u0393+(c), and by definition of r\u2217, Ci \u2286 \u0393+(ci). Therefore, each chosen CCV always marks its cluster, and we start Phase II with no remaining CCVs. This condition is sufficient for Phase II to return an O(log\u2217 n) approximation (Theorem 3.1 from [38]).\nNext we claim that for each 2-LPR cluster Ci, there exists a cluster outputted by Algorithm 3 that is a superset of Ci and does not contain any other 2-LPR cluster. To prove this claim, we first show there exists a point from Ci satisfying CCV-proximity that cannot be marked by any point from a different cluster in Phase I. From Lemma 15, ci satisfies CCV-proximity and center-separation. If a point c /\u2208 Ci marks ci, then \u2203v \u2208 \u0393\u2212(c)\u2229\u0393\u2212(ci). By center-separation and by definition of CCV, c /\u2208 \u0393\u2212(ci) and ci /\u2208 \u0393(c). Then from the definition of CCV-proximity for ci and c, we have d(c, v) < d(ci, v) and d(ci, v) < d(c, v), so we have reached a contradiction. At this point, we know a point c \u2208 Ci will always be chosen by the algorithm in Phase I. To finish the claim, we show that each point v from Ci is closer to c than to any other point c\u2032 /\u2208 Ci chosen in Phase I. Since c and c\u2032 are both CCVs, this follows directly from the last property in Lemma 15. However, it is possible that a center c\u2032 \u2208 Ai+1 is closer to v than c is to v, causing c\u2032 to \u201csteal\u201d v; this is unavoidable. Therefore, we forbid the algorithm from decreasing the size of the Voronoi tiles of C after Phase I.\nFinally, we claim that Algorithm 3 returns each 2-SLPR cluster exactly. Given a 2-SLPR cluster Ci, by our previous argument, there exists a CCV c \u2208 Ci from Phase I satisfying CCV-proximity such that Ci \u2286 Vc. First we assume towards contradiction that there exists a point v \u2208 \u0393\u2212(c) \\Ci. Let v \u2208 Cj . Since c is a CCV, then v \u2208 \u0393+(c), so Cj must be 2-LPR by definition of 2-SLPR. By Lemma 15, cj is a CCV and d(cj , v) < d(c, v). But this violates CCV-proximity of c, so we have reached a contradiction. Therefore, \u0393\u2212(c) \u2286 Ci. Now assume there exists v \u2208 Vc \\ Ci at the end of the algorithm.\nCase 1: v was marked by c in Phase I. Let v \u2208 Cj . Then there exists a point u \u2208 \u0393\u2212(c) such that v \u2208 \u0393+(u). Then u \u2208 Ci and v \u2208 \u0393+(u), so Cj must be 2-LPR. Since v is from a different 2-LPR cluster, it cannot be contained in Vc, so we have a contradiction.\nCase 2: v was not marked by c in Phase I. Denote the shortest path in D(S,d) from c to v by c = v0\u2013v1\u2013 \u00b7 \u00b7 \u00b7 \u2013vL\u22121\u2013vL = v. Let v` \u2208 Cj denote the first vertex on the shortest path that is not in Ci (such a vertex must exist because v /\u2208 Ci). See Figure 2a. Then v`\u22121 \u2208 Ci and d(v`\u22121, v`) \u2264 r\u2217, so Cj is 2-LPR. Let c\u2032 denote the CCV chosen in Phase I such that Cj \u2286 Vc\u2032 . If v` is not on the shortest path c\u2032\u2013v, then that shortest path must be shorter than c\u2013v, and we are done. If v`\u22121 is on the shortest path c\u2032\u2013v, then since v` \u2208 Cj , d(c\u2032, v`) \u2264 2r\u2217, so the shortest path must start with c\u2032\u2013v`\u22121\u2013v`. If d(c, v`\u22121) > r\u2217, then we are done because c\u2032 is then closer to v. Therefore, the distance from both c and c\u2032 to v` is in (r\u2217, 2r\u2217]. We can set up a 2-perturbation as in the proof of the final property of Lemma 15, so that c and c\u2032 become the centers of their respective clusters, and v` switches clusters.\nd\u2032\u2032(s, t) = { min(2r\u2217, 2d(s, t)) if s = c, t \u2208 Ci or s = c\u2032, t \u2208 Cj \u222a {v`} 2d(s, t) otherwise.\nd\u2032\u2032 is a valid 2-perturbation of d because for each point u \u2208 Ci, d(c, u) \u2264 d(c, ci) + d(ci, u) \u2264 2r\u2217, for each point u \u2208 Cj , d(c\u2032, u) \u2264 d(c\u2032, cj) + d(cj , u) \u2264 2r\u2217, and d(c\u2032, v`) \u2264 d(c, v`) \u2264 d(c, ci) + d(ci, v`) \u2264 2r\u2217. Define d\u2032 as the metric completion of d\u2032\u2032. Then by Lemma 9, d\u2032 is a 2-metric perturbation with optimal cost 2r\u2217. The set of centers {ci\u2032}ki\u2032=1 \\ {ci, cj}\u222a {c, c\u2032} achieves the optimal cost, since c and c\u2032 are distance 2r\u2217\nfrom Ci and Cj , and all other clusters have the same center as in OPT (achieving radius 2r\u2217). Then since d(c, v`) and d(c\u2032, v`) are both in (r\u2217, 2r\u2217], by construction d\u2032(c, v`) = d\u2032(c\u2032, v`). This contradicts 2-LPR, since v` can switch clusters to Ci.\nWe conclude that v` is the first common vertex on the shortest paths c\u2013v and c\u2032\u2013v. Since c and c\u2032 are both CCVs, d(c\u2032, v`) < d(c, v`). Therefore, v cannot be in Vc, so we have reached a contradiction. This completes the proof."}, {"heading": "E Proofs from Section 5", "text": "In this section, we give the details of the proofs from Section 16.\nLemma 22 (restated). Given a k-center clustering instance (S, d) such that all optimal clusters are size > 2 n and there exist two points at distance r\u2217 from different (3, )-LPR clusters, then there exists a partition Sx \u222a Sy of the non-centers S \\ {c`}k`=1 such that for all pairs p \u2208 Sx, q \u2208 Sy, {c`}k`=1 \u222a {p, q} (3, 3)-hits S.\nProof. This proof is split into two main cases. The first case is the following: there exists a CCC2 for a (3, )-LPR cluster, discounting a (3, )-LPR cluster. In fact, in this case, we do not need the assumption that two points from different LPR clusters are close. If there exists a CCC to a (3, )-LPR cluster, denote the CCC by cx and the cluster by Cy. Otherwise, let cx denote a CCC2 to a (3, )-LPR cluster Cy, discounting a (3, )-LPR center cz . Then cx is at distance \u2264 r\u2217 to all but n points in Cy. Therefore, d(cx, cy) \u2264 2r\u2217 and so cx is at distance \u2264 3r\u2217 to all points in Cy. Consider the following perturbation d\u2032\u2032.\nd\u2032\u2032(s, t) = { min(3r\u2217, 3d(s, t)) if s = cx, t \u2208 Cy 3d(s, t) otherwise.\nThis is a 3-perturbation because for all v \u2208 Cy, d(cx, v) \u2264 3r\u2217. Define d\u2032 as the metric completion of d\u2032\u2032. Then by Lemma 9, d\u2032 is a 3-metric perturbation with optimal cost 3r\u2217. Given any non-center v \u2208 S, the set of centers {c`}k`=1 \\ {cy} \u222a {v} achieves the optimal score, since cx is at distance 3r\u2217 from Cy, and all other clusters have the same center as in OPT (achieving radius 3r\u2217). Therefore, from Fact 18, one of the centers in {c`}k`=1 \\ {cy} \u222a {v} must be the center for the majority of points in Cy under d\u2032. If this center is c`, ` 6= x, y, then for the majority of points u \u2208 Cy, d(c`, u) \u2264 r\u2217 and d(c`, u) < d(cz, u) for all z 6= `, y. Then by definition, c` is a CCC for the (3, )-LPR cluster, Cy. But then by construction, ` must equal x, so we have a contradiction. Note that if some c` has for the majority of u \u2208 Cy, d(c`, u) \u2264 d(cz, u) (non-strict inequality) for all z 6= `, y, then there is another equally good partition in which c` is not the center for the majority of points in Cy, so we still obtain a contradiction. Therefore, either v or cx must be the center for the majority of points in Cy under d\u2032.\nIf cx is the center for the majority of points in Cy, then because Cy is (3, )-LPR, the corresponding cluster must contain fewer than n points from Cx. Furthermore, since for all ` 6= x and u \u2208 Cx, d(u, cx) < d(u, c`), it follows that v must be the center for the majority of points in Cx. Therefore, every non-center v \u2208 S is at distance \u2264 r\u2217 to the majority of points in either Cx or Cy.\nNow partition all the non-centers into two sets Sx and Sy, such that Sx = {p | for the majority of points q \u2208 Cx, d(p, q) \u2264 r\u2217} and Sy = {p | p /\u2208 Sx and for the majority of points q \u2208 Cy, d(p, q) \u2264 r\u2217}. Given p, q \u2208 Sx, there exists an s \u2208 Cx such that d(p, q) \u2264 d(p, s) + d(s, q) \u2264 2r\u2217 (since both points are close to more than half of points in Cx). Similarly, any two points p, q \u2208 Sy are \u2264 2r\u2217 apart.\nFor now, assume that Sx and Sy are both nonempty. Given a pair p \u2208 Sx, q \u2208 Sy, we claim that {c`}k`=1 \u222a {p, q} (3, 3)-hits S. Given a point s \u2208 Ci such that i 6= x, y, WLOG s \u2208 Sx. Then ci, p, and cx are all distance 3r\u2217 to s. Furthermore, ci, cx and p are all distance 3r\u2217 to ci. Given a point s \u2208 Cx, then\ncx, cy, and p are distance 3r\u2217 to s because d(cx, cy) \u2264 2r\u2217. Finally, cx, cy and p are distance 3r\u2217 to cx, and similar arguments hold for s \u2208 Cy and cy. Therefore, {c`}k`=1 \u222a {p, q} (3, 3)-hits S.\nIf Sx = \u2205 or Sy = \u2205, then we can prove a slightly stronger statement: for each pair of non-centers {p, q}, {c`}k`=1 \u222a {p, q} (3, 3)-hits S. The proof is the same as the previous paragraph. Thus, the lemma is true when assuming there exists a CCC2 for a (3, )-LPR cluster, discounting a (3, )-LPR cluster.\nNow we turn to the other case. Assume there does not exist a CCC2 to a LPR cluster, discounting a LPR center. In this case, we need to use the assumption that there exist (3, )-LPR clusters Cx and Cy, and p \u2208 Cx, q \u2208 Cy such that d(p, q) \u2264 r\u2217. Then by the triangle inequality, p is distance \u2264 3r\u2217 to all points in Cx and Cy. Consider the following d\u2032\u2032.\nd\u2032\u2032(s, t) = { min(3r\u2217, 3d(s, t)) if s = p, t \u2208 Cx \u222a Cy 3d(s, t) otherwise.\nThis is a 3-perturbation because d(p, Cx \u222a Cy) \u2264 3r\u2217. Define d\u2032 as the metric completion of d\u2032\u2032. Then by Lemma 9, d\u2032 is a 3-metric perturbation with optimal cost 3r\u2217. Given any non-center s \u2208 S, the set of centers {c`}k`=1 \\ {cx, cy} \u222a {p, s} achieves the optimal score, since p is distance 3r\u2217 from Cx \u222aCy, and all other clusters have the same center as in OPT (achieving radius 3r\u2217).\nFrom Fact 18, one of the centers in {c`}k`=1 \\ {cx, cy} \u222a {p, s} must be the center for the majority of points in Cx under d\u2032. If this center is c` for ` 6= x, y, then for the majority of points t \u2208 Cx, d(c`, t) \u2264 r\u2217 and d(c`, t) < d(cz, t) for all z 6= `, x, y. So by definition, c` is a CCC2 for Cx discounting cy, which contradicts our assumption. Similar logic applies to the center for the majority of points in Cy. Therefore, p and s must be the centers for Cx and Cy. Since s was an arbitrary non-center, all non-centers are distance \u2264 r\u2217 to all but n points in either Cx or Cy.\nSimilar to Case 1, we now partition all the non-centers into two sets Sx and Sy, such that Sx = {u | for the majority of points v \u2208 Cx, d(u, v) \u2264 r\u2217} and Sy = {u | u /\u2208 Sx and for the majority of points v \u2208 Cy, d(u, v) \u2264 r\u2217}. As before, each pair of points in Sx are distance \u2264 2r\u2217 apart, and similarly for Sy. It is no longer true that d(cx, cy) \u2264 2r\u2217, however, we can prove that for both Sx and Sy, there exist points from two distinct clusters each. From the previous paragraph, given a non-center s \u2208 Ci for i 6= x, y, we know that p and s are centers for Cx and Cy. With an identical argument, given t \u2208 Cj for j 6= x, y, i, we can show that q and t are centers for Cx and Cy. It follows that Sx and Sy both contain points from at least two distinct clusters.\nNow we finish the proof by showing that for each pair p \u2208 Sx, q \u2208 Sy, {c`}k`=1 \u222a {p, q} (3, 3)-hits S. Given a non-center s \u2208 Ci, WLOG s \u2208 Sx, then there exists j 6= i and t \u2208 Cj \u2229 Sx. Then ci, cj , and p are 3r\u2217 to s and ci, cx, and p are 3r\u2217 to ci. In the case where i = x, then ci, cj , and p are 3r\u2217 to ci. This concludes the proof.\nFact 23 (restated). Given a k-center clustering instance (S, d) such that all optimal clusters have size > 2 n, and an \u03b1-perturbation d\u2032 of d, let C\u2032 denote the set of (\u03b1, )-LPR clusters. For each Cx \u2208 C\u2032, there exists a bijection Rx,d\u2032 : S \u2192 [n] such that for all sets of k centers C that achieve the optimal cost under d\u2032, then c = argminc\u2032\u2208CRx,d\u2032(c \u2032) if and only if VorC(c) is -close to Cx.\nProof. Assume the lemma is false. Then there exists an (\u03b1, )-LPR cluster Ci, two distinct points u, v \u2208 S, and two sets of k centers C and C \u2032 both containing u and v, and both sets achieve the optimal score under an \u03b1-perturbation d\u2032, but u is the center for Ci in C while v is the center for Ci in C \u2032. Then VorC(u) is -close to Ci; similarly, VorC\u2032(v) is -close to Ci. This implies u is closer to all but n points in Ci than v, and v is closer to all but n points in Ci than u. Since |Ci| > 2 n, this causes a contradiction.\nLemma 24 (restated). Given a k-center clustering instance (S, d) such that all optimal clusters are size > 2 n, and given non-centers p, q \u2208 S such that C = {c`}k`=1 \u222a {p, q} (3, 3)-hits S, let the set C\u2032 denote the set of (3, )-LPR clusters. Define the 3-perturbation d\u2032 as in Lemma 21. The following are true.\n1. Given Cx \u2208 C\u2032 and Ci such that i 6= x, Rx,d\u2032(cx) < Rx,d\u2032(ci).\n2. There do not exist s \u2208 C and Cx, Cy \u2208 C\u2032 such that x 6= y, and Rx,d\u2032,C(s) +Ry,d\u2032,C(s) \u2264 4.\n3. Given Ci and Cx, Cy \u2208 C\u2032 such that x 6= y 6= i, if Rx,d\u2032,C(ci) \u2264 3, then Ry,d\u2032,C(p) \u2265 3 and Ry,d\u2032,C(q) \u2265 3.\nProof. 1. By definition of the optimal clusters, for each s \u2208 Cx, d(cx, s) < d(ci, s), and therefore by construction, d\u2032(cx, s) < d\u2032(ci, s). It follows that Rx,d\u2032(cx) < Rx,d\u2032(ci).\n2. Assume there exists s \u2208 C and Cx, Cy \u2208 C\u2032 such that Rx,d\u2032,C(s) +Ry,d\u2032,C(s) \u2264 4. Case 1: Rx,d\u2032,C(s) = 1 andRy,d\u2032,C(s) \u2264 3. Define u and v such thatRy,d\u2032,C(u) = 1 andRy,d\u2032,C(v) = 2. (If u or v is equal to s, then redefine it to an arbitrary center in C \\ {s, u, v}.) Consider the set of centers C \u2032 = C \\ {u, v} which is optimal under d\u2032 by Lemma 21. By Fact 23, s is the center for the majority of points in both Cx and Cy, causing a contradiction.\nCase 2: Rx,d\u2032,C(s) = 2 andRy,d\u2032,C(s) = 2. Define u and v such thatRx,d\u2032,C(u) = 1 andRy,d\u2032,C(v) = 1. (Again, if u or v is equal to s, then redefine it to an arbitrary center in C \\ {s, u, v}.) Consider the set of centers C \u2032 = C \\ {u, v} which is optimal under d\u2032 by Lemma 21. However, by Fact 23, s is the center for the majority of points in both Cx and Cy, causing a contradiction.\n3. Assume Rx,d\u2032,C(ci) \u2264 3. Case 1: Rx,d\u2032,C(ci) = 2. Then by Lemma 24 part 1, Rx,d\u2032,C(cx) = 1. Consider the set of centers C \u2032 = C \\ {cx, p}, which is optimal under d\u2032. By Fact 23, VorC\u2032(ci) must be -close to Cx. In particular, VorC\u2032(ci) cannot contain more than n points from Ci. But by definition, for all j 6= i and s \u2208 Ci, d(ci, s) < d(cj , s). It follows that VorC\u2032(q) must contain all but n points fromCi. Therefore, for all but n points s \u2208 Ci, for all j, d\u2032(q, s) < d\u2032(cj , s). If Ry,d\u2032,C(q) \u2264 2, then Cy ranks cy or p number one. Then for the set of centers C \u2032 = C \\{cy, p}, VorC\u2032(q) contains more than n points from Cy and Ci, contradicting the fact that Cy is (3, )-LPR. Therefore, Ry,d\u2032,C(q) \u2265 3. The argument to show Ry,d\u2032,C(p) \u2265 3 is identical. Case 2: Rx,d\u2032,C(ci) = 3. If there exists j 6= i, x such that Rx,d\u2032,C(ci) = 2, then WLOG we are back in case 1. By Lemma 24 part 1, Rx,d\u2032,C(cx) \u2264 2. Then either p or q are ranked top two, WLOG Rx,d\u2032,C(p) \u2264 2. Consider the set C \u2032 = C \\ {cx, p}. Then as in the previous case, VorC\u2032(ci) must be -close to Cx, implying for all but n points s \u2208 Ci, for all j, d\u2032(q, s) < d\u2032(cj , s). If Ry,d\u2032,C(q) \u2264 2, again, Cy ranks cy or p as number one. Let C \u2032 = C \\ {cy, p}, and then VorC\u2032(q) contains more than n points from Cy and Ci, causing a contradiction. Furthermore, if Ry,d\u2032,C(p) \u2264 2, then we arrive at a contradiction by Lemma 24 part 2."}], "references": [{"title": "Local search in combinatorial optimization", "author": ["E Aarts", "JK Lenstra"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Better guarantees for k-means and euclidean k-median by primal-dual algorithms", "author": ["Sara Ahmadian", "Ashkan Norouzi-Fard", "Ola Svensson", "Justin Ward"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Algorithms for stable and perturbationresilient problems", "author": ["Haris Angelidakis", "Konstantin Makarychev", "Yury Makarychev"], "venue": "In Proceedings of the Annual Symposium on Theory of Computing (STOC),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Learning topic models - going beyond SVD", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["Vijay Arya", "Naveen Garg", "Rohit Khandekar", "Adam Meyerson", "Kamesh Munagala", "Vinayaka Pandit"], "venue": "SIAM Journal on Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Stability yields a ptas for k-median and k-means clustering", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Information Processing Letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The hardness of approximation of euclidean k-means", "author": ["Pranjal Awasthi", "Moses Charikar", "Ravishankar Krishnaswamy", "Ali Kemal Sinop"], "venue": "Proceedings of the Annual Symposium on Computational Geometry (SOCG),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Improved spectral-norm bounds for clustering", "author": ["Pranjal Awasthi", "Or Sheffet"], "venue": "In Proceedings of the International Workshop on Approximation, Randomization, and Combinatorial Optimization Algorithms and Techniques (APPROX-RANDOM),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Clustering under approximation stability", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "k-center clustering under perturbation resilience", "author": ["Maria-Florina Balcan", "Nika Haghtalab", "Colin White"], "venue": "In Proceedings of the Annual International Colloquium on Automata, Languages, and Programming (ICALP),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Clustering under perturbation resilience", "author": ["Maria Florina Balcan", "Yingyu Liang"], "venue": "SIAM Journal on Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Agnostic clustering", "author": ["Maria Florina Balcan", "Heiko R\u00f6glin", "Shang-Hua Teng"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Are stable instances easy? Combinatorics", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "Probability and Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "An improved approximation for k-median, and positive correlation in budgeted optimization", "author": ["Jaros\u0142aw Byrka", "Thomas Pensyl", "Bartosz Rybicki", "Aravind Srinivasan", "Khoa Trinh"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "A constant-factor approximation algorithm for the k-median problem", "author": ["Moses Charikar", "Sudipto Guha", "\u00c9va Tardos", "David B Shmoys"], "venue": "In Proceedings of the Annual Symposium on Theory of Computing (STOC),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Algorithms for facility location problems with outliers", "author": ["Moses Charikar", "Samir Khuller", "David M Mount", "Giri Narasimhan"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "A constant factor approximation algorithm for k-median clustering with outliers", "author": ["Ke Chen"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Asymmetric k-center is log* n-hard to approximate", "author": ["Julia Chuzhoy", "Sudipto Guha", "Eran Halperin", "Sanjeev Khanna", "Guy Kortsarz", "Robert Krauthgamer", "Joseph Seffi Naor"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Local search yields approximation schemes for k-means and k-median in euclidean and minor-free metrics", "author": ["Vincent Cohen-Addad", "Philip N Klein", "Claire Mathieu"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "On the local structure of stable clustering", "author": ["Vincent Cohen-Addad", "Chris Schwiegelshohn"], "venue": "instances. CoRR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "A simple heuristic for the p-centre problem", "author": ["Martin E Dyer", "Alan M Frieze"], "venue": "Operations Research Letters,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1985}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["Teofilo F Gonzalez"], "venue": "Theoretical Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1985}, {"title": "Simpler analyses of local search algorithms for facility location", "author": ["Anupam Gupta", "Kanat Tangwongsan"], "venue": "CoRR, abs/0809.2554,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Decompositions of triangle-dense graphs", "author": ["Rishi Gupta", "Tim Roughgarden", "C Seshadhri"], "venue": "In Proceedings of the Annual Conference on Innovations in Theoretical Computer Science (ITCS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Beyond worst-case analysis in private singular vector computation", "author": ["Moritz Hardt", "Aaron Roth"], "venue": "In Proceedings of the Annual Symposium on Theory of Computing (STOC),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "A best possible heuristic for the k-center problem", "author": ["Dorit S Hochbaum", "David B Shmoys"], "venue": "Mathematics of operations research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1985}, {"title": "A new greedy approach for facility location problems", "author": ["Kamal Jain", "Mohammad Mahdian", "Amin Saberi"], "venue": "In Proceedings of the Annual Symposium on Theory of Computing (STOC),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["Tapas Kanungo", "David M Mount", "Nathan S Netanyahu", "Christine D Piatko", "Ruth Silverman", "Angela Y Wu"], "venue": "In Proceedings of the Annual Symposium on Computational Geometry (SOCG),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["Amit Kumar", "Ravindran Kannan"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "A simple linear time (1+ \u03b5)-approximation algorithm for geometric k-means clustering in any dimensions", "author": ["Amit Kumar", "Yogish Sabharwal", "Sandeep Sen"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Improved and simplified inapproximability for k-means", "author": ["Euiwoong Lee", "Melanie Schmidt", "John Wright"], "venue": "Information Processing Letters,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2017}, {"title": "A bi-criteria approximation algorithm for k means", "author": ["Konstantin Makarychev", "Yury Makarychev", "Maxim Sviridenko", "Justin Ward"], "venue": "In Proceedings of the International Workshop on Approximation, Randomization, and Combinatorial Optimization Algorithms and Techniques (APPROX-RANDOM),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Bilu-linial stable instances of max cut and minimum multiway cut", "author": ["Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "On the complexity of the metric tsp under stability considerations", "author": ["Mat\u00fa\u0161 Mihal\u00e1k", "Marcel Sch\u00f6ngens", "Rastislav \u0160r\u00e1mek", "Peter Widmayer"], "venue": "In SOFSEM: Theory and Practice of Computer Science,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J Schulman", "Chaitanya Swamy"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Beyond worst-case analysis", "author": ["Tim Roughgarden"], "venue": "http://theory.stanford.edu/ tim/f14/f14.html,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "An o(log*n) approximation algorithm for the asymmetric p-center problem", "author": ["Sundar Vishwanathan"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1996}], "referenceMentions": [{"referenceID": 22, "context": "These popular objective functions are provably NP-hard to optimize [23, 28, 32], so research has focused on finding approximation algorithms.", "startOffset": 67, "endOffset": 79}, {"referenceID": 27, "context": "These popular objective functions are provably NP-hard to optimize [23, 28, 32], so research has focused on finding approximation algorithms.", "startOffset": 67, "endOffset": 79}, {"referenceID": 31, "context": "These popular objective functions are provably NP-hard to optimize [23, 28, 32], so research has focused on finding approximation algorithms.", "startOffset": 67, "endOffset": 79}, {"referenceID": 4, "context": "This has attracted significant attention in the theoretical computer science community [5, 15, 16, 17, 18, 23, 33].", "startOffset": 87, "endOffset": 114}, {"referenceID": 14, "context": "This has attracted significant attention in the theoretical computer science community [5, 15, 16, 17, 18, 23, 33].", "startOffset": 87, "endOffset": 114}, {"referenceID": 15, "context": "This has attracted significant attention in the theoretical computer science community [5, 15, 16, 17, 18, 23, 33].", "startOffset": 87, "endOffset": 114}, {"referenceID": 16, "context": "This has attracted significant attention in the theoretical computer science community [5, 15, 16, 17, 18, 23, 33].", "startOffset": 87, "endOffset": 114}, {"referenceID": 17, "context": "This has attracted significant attention in the theoretical computer science community [5, 15, 16, 17, 18, 23, 33].", "startOffset": 87, "endOffset": 114}, {"referenceID": 22, "context": "This has attracted significant attention in the theoretical computer science community [5, 15, 16, 17, 18, 23, 33].", "startOffset": 87, "endOffset": 114}, {"referenceID": 32, "context": "This has attracted significant attention in the theoretical computer science community [5, 15, 16, 17, 18, 23, 33].", "startOffset": 87, "endOffset": 114}, {"referenceID": 25, "context": "BWCA has given rise to many positive results [26, 31, 37], especially for clustering problems [6, 7, 10, 30].", "startOffset": 45, "endOffset": 57}, {"referenceID": 30, "context": "BWCA has given rise to many positive results [26, 31, 37], especially for clustering problems [6, 7, 10, 30].", "startOffset": 45, "endOffset": 57}, {"referenceID": 36, "context": "BWCA has given rise to many positive results [26, 31, 37], especially for clustering problems [6, 7, 10, 30].", "startOffset": 45, "endOffset": 57}, {"referenceID": 5, "context": "BWCA has given rise to many positive results [26, 31, 37], especially for clustering problems [6, 7, 10, 30].", "startOffset": 94, "endOffset": 108}, {"referenceID": 6, "context": "BWCA has given rise to many positive results [26, 31, 37], especially for clustering problems [6, 7, 10, 30].", "startOffset": 94, "endOffset": 108}, {"referenceID": 9, "context": "BWCA has given rise to many positive results [26, 31, 37], especially for clustering problems [6, 7, 10, 30].", "startOffset": 94, "endOffset": 108}, {"referenceID": 29, "context": "BWCA has given rise to many positive results [26, 31, 37], especially for clustering problems [6, 7, 10, 30].", "startOffset": 94, "endOffset": 108}, {"referenceID": 13, "context": "For example, the popular notion of \u03b1-perturbation resilience, introduced by Bilu and Linial [14], informally states that the optimal solution does not change when the input distances are allowed to increase by up to a factor of \u03b1.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "For example, the algorithms of Balcan and Liang [12] and Angelidakis et al.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "[3] return the optimal clustering when the instance is resilient to perturbations, however, both algorithms use a dynamic programming subroutine that is susceptible to errors which can propagate when a small fraction of the data does not satisfy perturbation resilience (see Appendix A for more details).", "startOffset": 0, "endOffset": 3}, {"referenceID": 37, "context": "Asymmetric k-center The asymmetric k-center problem admits an O(log\u2217 n) approximation algorithm due to Vishnwanathan [38], which is tight [19].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "Asymmetric k-center The asymmetric k-center problem admits an O(log\u2217 n) approximation algorithm due to Vishnwanathan [38], which is tight [19].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "We utilize the idea of a cluster-capturing center [11] along with machinery specific to handling local perturbation resilience to show that a locally perturbation resilient cluster must split into two clusters under the 3-perturbation, causing a contradiction.", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "0013 for k-center [23], k-median [28], and k-means [32], respectively.", "startOffset": 18, "endOffset": 22}, {"referenceID": 27, "context": "0013 for k-center [23], k-median [28], and k-means [32], respectively.", "startOffset": 33, "endOffset": 37}, {"referenceID": 31, "context": "0013 for k-center [23], k-median [28], and k-means [32], respectively.", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "This generalizes prior hardness results in BWCA [10, 11].", "startOffset": 48, "endOffset": 56}, {"referenceID": 10, "context": "This generalizes prior hardness results in BWCA [10, 11].", "startOffset": 48, "endOffset": 56}, {"referenceID": 15, "context": "[16], and the current best approximation ratio is 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "73 [28].", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "For k-center, Gonzalez showed a tight 2approximation algorithm [23].", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], and the constant was recently improved to 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "0013 [32].", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "Perturbation resilience Perturbation resilience was introduced by Bilu and Linial, who showed algorithms that outputted the optimal solution for max cut under \u03a9( \u221a n)-perturbation resilience [14].", "startOffset": 191, "endOffset": 195}, {"referenceID": 33, "context": "[34], who showed the standard SDP relaxation is integral for \u03a9( \u221a log n log logn)-perturbation resilient instances.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7], who provided an optimal algorithm for center-based clustering objectives (which includes k-median, k-means, and k-center clustering, as well as other objectives) under 3-perturbation resilience.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "This result was improved by Balcan and Liang [12], who showed an algorithm for center-based clustering under (1 + \u221a 2)-perturbation resilience.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "[11] constructed algorithms for k-center and asymmetric k-center under 2perturbation resilience and (3, )-perturbation resilience, and they showed no polynomial-time algorithm can solve k-center under (2 \u2212 )-approximation stability (a notion that is stronger than perturbation resilience) unless NP = RP .", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3], gave algorithms for center-based clustering under 2perturbation resilience and minimum multiway cut with k terminals under (2\u22122/k)-perturbation resilience.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Perturbation resilience has also been applied to other problems, such as the traveling salesman problem, and finding Nash equilibria [10, 35].", "startOffset": 133, "endOffset": 141}, {"referenceID": 34, "context": "Perturbation resilience has also been applied to other problems, such as the traveling salesman problem, and finding Nash equilibria [10, 35].", "startOffset": 133, "endOffset": 141}, {"referenceID": 9, "context": "[10], who showed algorithms that outputted nearly optimal solutions under (\u03b1, )-approximation stability for k-median and k-means when \u03b1 > 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] studied a relaxed notion of approximation stability in which a specified \u03bd fraction of the data satisfies approximation stability.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] showed algorithm for finding near-optimal solutions for k-median under approximation stability in the context of finding triangle-dense graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "show how to efficiently cluster instances in which the k-means clustering cost is much lower than the (k \u2212 1)-means cost [36].", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "Kumar and Kannan give an efficient clustering algorithm for instances in which the projection of any point onto the line between its cluster center to any other cluster center is a large additive factor closer to its own center than the other center [30].", "startOffset": 250, "endOffset": 254}, {"referenceID": 8, "context": "This result was later improved along multiple axes by Awasthi and Sheffet [9].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "There are many other works that show positive results for different natural notions of stability in various settings [4, 6, 25, 26, 30, 31, 37].", "startOffset": 117, "endOffset": 143}, {"referenceID": 5, "context": "There are many other works that show positive results for different natural notions of stability in various settings [4, 6, 25, 26, 30, 31, 37].", "startOffset": 117, "endOffset": 143}, {"referenceID": 24, "context": "There are many other works that show positive results for different natural notions of stability in various settings [4, 6, 25, 26, 30, 31, 37].", "startOffset": 117, "endOffset": 143}, {"referenceID": 25, "context": "There are many other works that show positive results for different natural notions of stability in various settings [4, 6, 25, 26, 30, 31, 37].", "startOffset": 117, "endOffset": 143}, {"referenceID": 29, "context": "There are many other works that show positive results for different natural notions of stability in various settings [4, 6, 25, 26, 30, 31, 37].", "startOffset": 117, "endOffset": 143}, {"referenceID": 30, "context": "There are many other works that show positive results for different natural notions of stability in various settings [4, 6, 25, 26, 30, 31, 37].", "startOffset": 117, "endOffset": 143}, {"referenceID": 36, "context": "There are many other works that show positive results for different natural notions of stability in various settings [4, 6, 25, 26, 30, 31, 37].", "startOffset": 117, "endOffset": 143}, {"referenceID": 13, "context": "We formally define perturbation resilience, a notion introduced by Bilu and Linial [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "[12] A clustering instance (S, d) satisfies (\u03b1, )-perturbation resilience ((\u03b1, )-PR) if for any \u03b1- perturbation d\u2032 of d, all optimal clusterings under d\u2032 must be -close to OPT .", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] recently studied the weaker definition in which the \u03b1-perturbations must satisfy the triangle inequality, called metric perturbation resilience.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "The classic Local Search heuristic is widely used in practice, and many works have studied local search theoretically for k-median and k-means [5, 24, 29].", "startOffset": 143, "endOffset": 154}, {"referenceID": 23, "context": "The classic Local Search heuristic is widely used in practice, and many works have studied local search theoretically for k-median and k-means [5, 24, 29].", "startOffset": 143, "endOffset": 154}, {"referenceID": 28, "context": "The classic Local Search heuristic is widely used in practice, and many works have studied local search theoretically for k-median and k-means [5, 24, 29].", "startOffset": 143, "endOffset": 154}, {"referenceID": 0, "context": "For more information on local search, see a general introduction by Aarts and Lenstra [1].", "startOffset": 86, "endOffset": 89}, {"referenceID": 20, "context": "The next theorem utilizes Lemma 7 and a result by Cohen-Addad and Schwiegelshohn [21], who showed that local search returns the optimal clustering under a stronger version of (3 + 2 )-PR.", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Although Theorem 10 applies more generally to asymmetric k-center, it is most useful for symmetric k-center, for which there exist several 2-approximation algorithms [22, 23, 27].", "startOffset": 166, "endOffset": 178}, {"referenceID": 22, "context": "Although Theorem 10 applies more generally to asymmetric k-center, it is most useful for symmetric k-center, for which there exist several 2-approximation algorithms [22, 23, 27].", "startOffset": 166, "endOffset": 178}, {"referenceID": 26, "context": "Although Theorem 10 applies more generally to asymmetric k-center, it is most useful for symmetric k-center, for which there exist several 2-approximation algorithms [22, 23, 27].", "startOffset": 166, "endOffset": 178}, {"referenceID": 18, "context": "Asymmetric k-center is NP-hard to approximate to within a factor of o(log\u2217n) [19], so Theorem 10 only guarantees returning the O(log\u2217 n)-LPR clusters.", "startOffset": 77, "endOffset": 81}, {"referenceID": 37, "context": "4 Asymmetric k-center In this section, we show that a slight modification to the O(log\u2217 n) approximation algorithm of Vishwanathan [38] leads to an algorithm that maintains its performance in the worst case, while returning each cluster Ci with the following property: Ci is 2-LPR, and all nearby clusters are 2-LPR as well.", "startOffset": 131, "endOffset": 135}, {"referenceID": 37, "context": "Approximation algorithm for asymmetric k-center We start with a recap of theO(log\u2217 n)-approximation algorithm of Vishwanathan [38].", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "This was the first nontrivial algorithm for asymmetric k-center, and the approximation ratio was later proven to be tight [19].", "startOffset": 122, "endOffset": 126}, {"referenceID": 37, "context": "We refer the reader to [38] for these details.", "startOffset": 23, "endOffset": 27}, {"referenceID": 37, "context": "Algorithm 2 O(log\u2217 n) APPROXIMATION ALGORITHM FOR ASYMMETRIC k-CENTER [38] Input: Asymmetric k-center instance (S, d), optimal radius r\u2217 (or try all possible candidates) Set C = \u2205.", "startOffset": 70, "endOffset": 74}, {"referenceID": 37, "context": "1 from [38]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 6, "context": "To prove this claim, we first show there exists a point from Ci satisfying CCV-proximity that cannot be marked by any point from a different cluster in 2 This property loosely resembles \u03b1-center proximity [7], a property defined over an entire clustering instance, which states for all i, for all v \u2208 Ci, j 6= i, we have \u03b1d(ci, v) < d(cj , v).", "startOffset": 205, "endOffset": 208}, {"referenceID": 10, "context": "However, as pointed out in [11], we are still a long way off from showing a contradiction.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "To handle this problem, we use the notion of a cluster-capturing center [11],", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] A center ci is a first-order cluster-capturing center (CCC) for Cj if for all x 6= j, for all but n points v \u2208 Cj , d(ci, v) < d(cx, v) and d(ci, v) \u2264 r\u2217.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "A non local version of this fact appeared in [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "In fact, this hardness holds even under the strictly stronger notion of approximation stability [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 9, "context": "This theorem generalizes hardness results from [10] and [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "This theorem generalizes hardness results from [10] and [11].", "startOffset": 56, "endOffset": 60}, {"referenceID": 27, "context": "73-approximation for k-median [28].", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "The proofs for k-center and k-means are identical, using hardness from [23] and [32], respectively.", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "The proofs for k-center and k-means are identical, using hardness from [23] and [32], respectively.", "startOffset": 80, "endOffset": 84}, {"referenceID": 37, "context": "We provide a natural modification to the asymmetric k-center approximation algorithm of Vishwanathan [38] to prove it outputs all 2-SLPR clusters.", "startOffset": 101, "endOffset": 105}], "year": 2017, "abstractText": "Recently, there has been substantial interest in clustering research that takes a beyond worst-case approach to the analysis of algorithms. The typical idea is to design a clustering algorithm that outputs a near-optimal solution, provided the data satisfy a natural stability notion. For example, Bilu and Linial (2010) and Awasthi et al. (2012) presented algorithms that output near-optimal solutions, assuming the optimal solution is preserved under small perturbations to the input distances. A drawback to this approach is that the algorithms are often explicitly built according to the stability assumption and give no guarantees in the worst case; indeed, several recent algorithms output arbitrarily bad solutions even when just a small section of the data does not satisfy the given stability notion. In this work, we address this concern in two ways. First, we provide algorithms that inherit the worst-case guarantees of clustering approximation algorithms, while simultaneously guaranteeing nearoptimal solutions when the data is stable. Our algorithms are natural modifications to existing state-ofthe-art approximation algorithms. Second, we initiate the study of local stability, which is a property of a single optimal cluster rather than an entire optimal solution. We show our algorithms output all optimal clusters which satisfy stability locally. Specifically, we achieve strong positive results in our local framework under recent stability notions including metric perturbation resilience (Angelidakis et al. 2017) and robust perturbation resilience (Balcan and Liang 2012) for the k-median, k-means, and symmetric/asymmetric k-center objectives. \u2217Authors\u2019 addresses: ninamf@cs.cmu.edu, crwhite@cs.cmu.edu. This work was supported in part by grants nsfccf 1535967, NSF CCF-1422910, NSF IIS-1618714, a Sloan Fellowship, a Microsoft Research Fellowship, and a National Defense Science and Engineering Graduate (NDSEG) fellowship. ar X iv :1 70 5. 07 15 7v 1 [ cs .D S] 1 9 M ay 2 01 7", "creator": "LaTeX with hyperref package"}}}