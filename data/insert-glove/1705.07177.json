{"id": "1705.07177", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Model-Based Planning in Discrete Action Spaces", "abstract": "castellina Planning birgitte actions nexen using learned flatiron and episodic differentiable 2,116 forward carmer models gamov of the 2,249 world is fetzner a keynoter general bezirksligas approach which shihong has nh\u01a1n a number rukungiri of desirable 12p-4p properties, convicting including improved curio sample complexity janardhan over model - free 2,151 RL nassariidae methods, keating reuse of learned models across different tasks, and the ability to perform dreamlike efficient gradient - 958 based optimization l'institut in iorg continuous intramolecular action spaces. awwww However, obtusifolia this xl-7 approach to-be does zeolite not apply straightforwardly esquina when resealable the fgs action apparao space fws is sukari discrete, time-wasting which lidya may manacled have limited 8-of-9 its milicja adoption. vandenhurk In this escapists work, we introduce 35.90 two discrete planning collates tasks inspired by sonority existing question - hamra answering pindaris datasets and show that mcnutt it is in foys fact 2-15 possible \u00e5rets to kintanar effectively dorrell perform planning via ataxic backprop wwbt in taiwan discrete action spaces corporatewatch using totalitarianism two attraction simple yet principled modifications. pointlessness Our experiments gerolmo show sossamon that myhre this approach can pejanovic significantly outperform model - timander free RL opequon based methods and supervised imitation stateira learners.", "histories": [["v1", "Fri, 19 May 2017 20:38:49 GMT  (126kb,D)", "http://arxiv.org/abs/1705.07177v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mikael henaff", "william f whitney", "yann lecun"], "accepted": false, "id": "1705.07177"}, "pdf": {"name": "1705.07177.pdf", "metadata": {"source": "CRF", "title": "Model-Based Planning in Discrete Action Spaces", "authors": ["Mikael Henaff", "William F. Whitney"], "emails": ["mbh305@nyu.edu", "ww1114@nyu.edu", "yann@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Planning actions in order to accomplish a specific goal is a challenge of fundamental interest in artificial intelligence. One of the main paradigms for addressing planning problems is model-free reinforcement learning (RL), a general approach where an agent samples actions according to an internal policy and then adjusts the policy as a function of the reward it receives for different actions. This method has the advantage of making minimal assumptions about the task at hand and can learn complex policies using only the raw state representation and a scalar reward signal. In recent years model-free RL using deep learning has proven successful for a number of applications including Atari games [26], robotic manipulation [11], navigation and reasoning tasks [28], and machine translation [16]. However, it also suffers from several limitations, including a difficult temporal credit assignment problem, high sample complexity [31], and non-stationarity of the input distribution.\nAn alternative approach is imitation learning [29] where the agent is trained to predict actions of an expert given a state; this can be cast as a supervised learning problem. However, small inaccuracies can accumulate over time, leading to situations where the agent finds itself in situations not encountered during training [4]. Furthermore, since the learner does not directly optimize a reward function and instead relies on the expert\u2019s policy to be close to optimal, in the absence of additional queries to the environment its performance will be bounded by that of the expert [2].\nModel-based planning assumes the existence of a forward model of the environment which can predict how the world will evolve in response to different actions. Actions can then be planned by using this forward model to select a sequence of actions which will take the agent from its current state to a desired goal state. It is particularly appealing to consider differentiable forward models, as they provide gradients which define a direction of improvement for a sequence of actions. These gradients with respect to plans make it possible to do \"planning by backprop\", directly optimizing\nar X\niv :1\n70 5.\n07 17\n7v 1\n[ cs\n.A I]\n1 9\nM ay\na sequence of actions by backpropagating gradients from a goal state through a learned model to update a plan.\nPlanning with a learned forward model has several desirable properties: training a forward model has lower sample complexity than model-free RL due to the rich information content of its highdimensional error signal; it decouples the learned model from the rewards of the specific task it was trained on, allowing the same model to be used for multiple planning tasks; and it can be applied in settings without a known structure. Evidence from neuroscience also suggests that humans use forward models for certain decision-making tasks [10]. Applying gradient-based optimization is straightforward when the action space is continuous, since the backpropagation rule allows easy computation of the gradients with respect to the input actions. However, many planning tasks require an agent to select actions from a discrete set, and performing gradient-based optimization in this discrete action space is not straightforward, since it may yield a solution which is not a valid action.\nIn this work, we show that by using a simple paramaterization of actions on the simplex combined with input noise during planning, we are able to effectively perform gradient-based planning in discrete action spaces. We provide theoretical and empirical evidence that adding random noise to the inputs naturally leads to solutions close to one-hot action vectors. We introduce two novel discrete planning tasks inspired by existing question-answering benchmarks, and compare our model-based planning method, which we call the Forward Planner, to a model-free RL baseline as well as a supervised imitation learner. Planning by backprop consistently outperforms the alternative methods (at the cost of additional planning time). Although the discrete planning tasks introduced here are simple to describe, none of the methods are able to solve them completely, suggesting they may be used as a testbed for further model development. We hope these results encourage renewed interest in model-based planning in the machine learning community."}, {"heading": "2 Planning with Forward Models", "text": "Planning with a forward model consists of two stages. In the first, the agent constructs a forward model of the world by observing how the environment responds to actions. More precisely, the agent is presented with triplets (s0, a, s\u2032) from the environment E each consisting of an initial state s0, an action a, and a final state s\u2032. Here s0, a, s\u2032 can each represent either single instances or sequences of actions or states. The agent learns a forward model f(s0, a, \u03b8) with trainable parameters \u03b8 to predict a state s which minimizes a loss function L(s, s\u2032) measuring the discrepancy between the predicted state and the observed state:\narg min \u03b8\nE(s0,a,s\u2032)\u223cE [ L(f(s0, a, \u03b8), s\u2032) ] After training, given some initial state s0 the agent can use the forward model to plan actions with the goal of achieving a desired final state s\u2032. This is done by solving the following optimization problem:\narg min a\nL(f(s0, a, \u03b8), s\u2032)\nIf f is a neural network model and the input action space is continuous, this optimization procedure can be done by straightforwardly applying the backpropagation rule which computes the gradients with respect to both the weights and the inputs. Starting from a randomly initialized action vector, the loss function can be minimized by performing gradient descent in the input action space. When the input space is discrete, input tokens are typically encoded as one-hot vectors, and there is no guarantee that the result of the above optimization procedure will constitute a valid input action sequence where each inferred input element is placed at a one-hot vector.\nWe propose to address this problem with two simple steps. Denote the action space by A = {e1, ..., ed}, where each action is represented by a one-hot vector. The first step is to restrict the input space for each possible action to the simplex \u2206d, which includes the one-hot vectors, rather than all of Rd. Note that the points on the simplex can be written as \u2206n = {z : zi \u2265 0, \u2211 i zi = 1} = {z :\nAlgorithm 1 Forward Planner Require: Trained forward model f , initial state s0, desired final state s\u2032, learning rate \u03b7.\n1: Initialize xt \u2208 R|A| from N (0, 0.1) for t = 1, ..., T . 2: for i = 1 : k do 3: xt \u2190 xt + for t = 1, ..., T . . Add noise to inputs 4: at \u2190 \u03c3(xt) for t = 1, ..., T . . Compute action vectors 5: s\u2190 f(s0, a1, ..., aT , \u03b8) . Predict final state for this action sequence 6: Compute L(s, s\u2032) and \u2207s . Forward and backprop through L 7: Compute \u2207at for t = 1, ..., T . Backprop through f using \u2207s 8: Compute \u2207xt for t = 1, ..., T . Backprop through \u03c3 using \u2207at 9: xt \u2190 ADAM(xt,\u2207xt, \u03b7) for t = 1, ..., T . . Update using ADAM 10: end for 11: at \u2190 \u03c3(xt) for t = 1, ..., T . 12: at \u2190 arg minei\u2208{e1,...,ed}||at \u2212 ei|| for t = 1, ..., T . . Quantize actions to one-hot vectors 13: return a1, ..., aT\nz = \u03c3(x), x \u2208 Rd}, where \u03c3 represents the softmax function. A relaxed optimization problem can thus be reformulated as:\narg min x=(x1,...,xT )\nL(f(s0, \u03c3(x), \u03b8), s\u2032) (1)\nwhere \u03c3(x) corresponds to softmaxes applied to each xt separately. A sequence of tokens can be chosen by minimizing the above loss function, quantizing each \u03c3(xt) to the closest one-hot vector, and mapping back to the corresponding token.\nThe second step consists of adding random noise to the inputs x during optimization, which we show has the effect of biasing solutions towards the one-hot vectors. The work of [3] shows that adding independent noise to the weights of a neural network during training induces an additional term on the loss function. Applying the same derivation, we can show that adding noise to the inputs x during minimization corresponds to adding the following penalty:\nP = \u2211 i \u22022f(s0, \u03c3(x), \u03b8) \u22022xi \u2202L \u2202s\u2032 i + \u2211 i \u2223\u2223\u2223\u2223 \u2202f\u2202xi \u2223\u2223\u2223\u22232 i (2)\nHere i denotes the variance of the noise along each component. The second-order part of the first term represents the trace of the Hessian of the loss with respect to the inputs, which is equal to the sum of its eigenvalues. This can be viewed as a rough approximation to the curvature at the current point. The first-order part \u2202L\u2202s\u2032 changes the sign of the term from negative to positive as the current action vector moves from regions of high loss to regions of low loss. This term thus penalizes regions of the input space which have low loss but high curvature, e.g. very sharp local minima. The second term penalizes input configurations which lead the function to have high sensitivity to input variations. This biases the inputs towards regions where small perturbations result in near-zero changes to the output, such as saturated regions of the softmax. In particular, this term is minimized when at = \u03c3(xt) is a unit vector. All together this regularizer encourages the optimization to find broad, flat regions of the space instead of narrow optima.\nAn alternative to adding noise to the inputs xt would be to add noise to the gradients \u2207xt. In the case of a fixed learning rate, this is essentially equivalent since adding noise to the gradient the same as adding noise to the updated xt scaled by the learning rate. In the case of adaptive learning rates such as that used in ADAM [20], this corresponds to scaling the noise by the running average of the gradient magnitude, which may be desirable.\nFinally, an explicit way of encouraging solutions towards the one-hot vectors would be to directly penalize the entropy of the actions, i.e. add an extra term H(\u03c3(xt)) to the loss function, where H represents the entropy operator. In our experiments, we investigate these three approaches."}, {"heading": "3 Tasks and Architectures", "text": "In this section we introduce two discrete planning tasks inspired by question-answering datasets and the various methods we apply to solve them. Textual question-answering tasks are a common benchmark in machine learning [34, 15, 14]. In these tasks, the model is given triplets consisting of a story (or context), a question, and an answer, and it is trained to predict the correct answer given the question and context. Several of the bAbI tasks introduced in [34] consist of stories describing the actions of one or more agents acting in the world, and the question requires the model to produce an answer about some aspect of the world which depends on the agent\u2019s actions. The question together with its answer can jointly be seen as representing some aspect of the final state of the story, which we denote by s\u2032, and the story can be seen as consisting of a sequence of actions a.\nTasks of this nature can be straightforwardly adapted to become planning tasks. Instead of requiring the model to produce s\u2032 given a, we require it to produce a given s\u2032, together with an initial state of the story. In other words, we give the model a first part of the story describing the initial state of the world, fix a set of questions and their answers, and require the model to produce a sequence of actions a such that the answers to the questions must be the ones specified."}, {"heading": "3.1 Navigation Task", "text": "The first task we introduce is a navigation task, and is inspired by the World Model task in [13]. In this scenario, an agent moves around on a 2-D grid, and the model must learn to plan sequences of actions to take the agent from one point to another. The agent can perform one of 10 actions: face in a given direction (north, south, east or west), move ahead between one and five steps, or do nothing.\nWe generated sequences as follows. The agent was first placed at a random location on a 15\u00d7 15 grid, and then a number of actions was chosen uniformly between 1 and a maximum length Tmax. For each action, the agent either chooses to change direction or to move ahead with equal probability. If it changes direction, it randomly faces north, south, east or west, again with equal probability. If it moves, it makes a number of steps uniformly chosen between 1 and 5. If this would ever take the agent off the grid, the agent simply stops at the edge of the grid. There are thus no undefined sequences of actions, but some actions may have no effect (for example, taking steps if the agent is facing the edge of the grid).\nWe define the path distance to be the distance between the start and end points of the agent, measured in `1 norm. Note that this will typically be different than the number of steps taken, since the agent may change direction or double back. We constructed the training set so that the random walks all had path distance at most 5, by resampling sequences until they had path distance at most 5. This allowed us to test models on sequences with path distance greater than 5 and see whether they could generalize to plan paths which are longer than those seen during training. We generated two datasets with maximum number of steps Tmax \u2208 {5, 10}, each with two million samples."}, {"heading": "3.2 Transport Task", "text": "The second task we introduce requires an agent to travel to different locations to pick up objects. There are 4 objects and 8 locations. In the initial state, each object is placed at a distinct location. At every subsequent timestep, the agent can either travel to a location or attempt to pick up the object at its current location. If the agent attemps to pick up an object when there is none, this action has no effect. At the end of the sequence, the model must provide the current location of each of the objects. The model must thus learn to keep a list of objects being carried in memory, track the agent\u2019s location and update the list as a function of the agent\u2019s actions. This task is related to bAbI tasks 2 and 8 (\u201cLists and Sets\u201d, \u201cTwo supporting Facts\u201d) and requires up to four supporting facts: the original location of an object, whether the agent has moved to that location, whether the agent has picked up the object while at that location, and the current location of the agent.\nWe generated sequences as follows. First, distinct locations for each of the objects are given, and then a number of actions is chosen between 1 and a maximum length Tmax. For each action, the agent follows the following policy: if the location has an object which is not being carried by the agent, the agent picks it up with probability 0.5; if not, the agent still attempts to pick up an object with probability 0.25 (this action has no effect). If the agent does not attempt to pick up an object, it moves to a random location. We generated three versions of this dataset with different maximum lengths Tmax = 10, 15, 20, each with 2 million samples. The shorter the maximum sequence length, the more the dataset is biased towards action sequences which do not move any objects (see the Appendix for distributions of objects moved for each sequence length)."}, {"heading": "3.3 Architectures", "text": "We now describe the different methods we tested to address these planning tasks; more details are included in the Appendix. As a forward model, we used an RNN with 100 hidden units and parametric ReLU non-linearities combined with a spherical normalization layer to prevent divergence of the activations. The forward model was trained to minimize the cross-entropy between its output at the final timestep and the correct final state. We then used this forward model to perform planning by backprop using the method described in Section 2, which we call the Forward Planner.\nOur first baseline was a Q-learning agent in the discrete space, trained via a method similar to [26] including target networks and replay memory. To encode stories into state vectors, we used the same recurrent architecture as the forward model where we returned the hidden state at the final step of each sequence. We used the same distribution of initial and final states to train the forward model and the Q-learner, i.e. for some triplet (s0, a, s\u2032) in the training set, we used s0, s\u2032 as initial and goal states for the Q-learner. The Q-learner was free to interact with the environment according to its action policy. At the end of each action sequence, the agent received a reward of 1 from the environment if its current state matched the desired goal state, and a reward of 0 otherwise. Note that this setting, with no intermediate rewards and no intermediate states other than the actions taken, is quite difficult for model-free RL methods; they typically rely on full state observations at each timestep to factorize the problem.\nAs a second baseline, we trained a purely supervised Imitation Learner to predict each action conditioned on previous actions and the desired final state, using the Contextual RNN architecture introduced in [25]. The model is first given the initial state s0 and then is required to predict the sequence of actions a1, ..., aT conditioned on the desired final state s\u2032 (which is in this case the \"context\" per Mikolov). At each timestep, the hidden state is updated using the current action and the previous hidden state, and is then combined with the desired final state to predict the next action. At test time, at each timestep we sample an action from the output distribution over actions and use it as the input action at the next timestep, similarly to how recurrent language models are used to generate text."}, {"heading": "4 Related Work", "text": "The idea of planning actions by backpropagating along a policy trajectory has existed since the 1960\u2019s [19, 7]. These methods were applied to settings where the state transition dynamics of the environment were known analytically and backward derivatives could be computed exactly, such as\nplanning flight paths. Later works [30, 27, 17] explored the idea of backpropagating through learned, approximate forward models of the environment.\nSeveral recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18]. These works focused mostly on challenges associated with training an accurate forward model, by designing model architectures with shared parameters across different object instances or adversarial loss functions. Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].\nThere has been recent work in continuous relaxations of discrete random variables [23, 8], that also uses a softmax to form a continuous approximation to a discrete set. However, their work is focused on parameterizing categorical distributions over discrete variables in a manner which can be learned by gradient descent, whereas we are interested in the deterministic case."}, {"heading": "5 Experiments", "text": "In this section, we describe the results of applying the Forward Planner (FP), Q-learner and Imitation Learner on the two planning tasks. For the Navigation Task, the performance is measured in accuracy, i.e. the proportion of times the model generates an action sequence which takes it to the specified location. For the Transport task, the distribution is skewed towards sequences where few objects have moved. We therefore calculated the sensitivity (the proportion of objects which have been moved which are correctly classified) and the specificity (the proportion of objects that have not been moved which are correctly classified). Since we found that all models had high specificity (>0.95), we report sensitivity as our measure of performance on the Transport Task unless otherwise noted. Inference with the forward planner was done for 100 optimization updates using a learning rate of 1, unless otherwise noted."}, {"heading": "5.1 Effect of Regularizers", "text": "Our first experiment compared the three different methods to bias actions toward the one-hot vectors: input noise, gradient noise and entropy regularization. Results for several different hyperparameter values are shown in Table 2. For the Navigation tasks, all three methods yielded an improvement over the unregularized case (\u03b3 = 0). The input noise yielded the biggest improvement, with the other two being roughly equivalent. For the Transport task, the entropy regularization did not help at all. Input noise and gradient noise had similar best values, however the gradient noise was more\nrobust to the hyperparameter \u03b3. We then plotted the evolution of the network loss and entropy during minimization for each of the methods, using the values of \u03b3 which gave the best performance (Figure 1). When no regularization is used, the entropy decreases initially but then stays constant; all the other regularization methods cause the entropy to decrease further throughout the optimization procedure. Out of these, the entropy regularization causes it to decreases the most quickly and smoothly, which is expected since it is being explicitly minimized. Both the input noise and gradient noise cause the entropy to decrease, which confirms our theoretical prediction. Interestingly, the network loss quickly decreases for all of the methods and both tasks independently of the entropy, which indicates that there are many low loss minima on the simplex at varying distances from the one-hot vectors. Even though the entropy-regularized method produces solutions where actions are close to the one-hot vectors, the fact that it has relatively poor performance on the Transport task suggests the existence of configurations of actions close to the one-hot vectors which yield low loss yet still produce incorrect final states, reminiscent of adversarial examples."}, {"heading": "5.2 Comparison to Baselines", "text": "We now compare the Forward Planner to the other two models on both tasks. We chose to use input noise with \u03b3 = 1 based on the previous results, and evaluated the subsequent methods on data different from that used in the previous section. We used 100 optimization updates, which means that planning actions with the Forward Planner is more time consuming than with the other models (we later study the tradeoff between planning time and performance).\nResults comparing the various models on the Navigation Task are shown in Table 3. We report performance on all path distances mixed together according to the distribution used to generate the training set (denoted 1-5), as well as a breakdown of performance by path distance. Note that no sequences with path distance greater than 5 were seen during training; we wanted to see if the models could learn to compose what they had learned about planning smaller paths in order to plan longer ones. The Q-learner performs poorly for all path distances except for 0, indicating that it has learned a policy where it simply stays at the same location (note that paths of distance 0 are the most frequent in the dataset, see Appendix). The Imitation Learner performs well for shorter sequences (Tmax = 5), especially when planning shorter paths. However its performance drops for path distances beyond 5,\nindicating it does not generalize outside its training distribution. For longer sequences (Tmax = 10), it generalizes somewhat better but the overall performance is lower, possibly because early actions are less correlated with the final states, which makes learning difficult, or due to the compounding of errors when generating actions. In contrast, the Forward Planner performs better for longer sequences and is able to generalize fairly well to sequences longer than those seen during training, indicating it has been able to make use of the information contained in the longer sequences to learn general dynamics of the environment it can use for planning.\nTable 4 shows the performance of the different models on the Transport Task. Here the Q-learner is not able to learn to move objects as desired. The Imitation Learner is able to learn a sensible policy for the longer sequences, but not the shortest one. This is probably because the shorter sequence dataset has many sequences where objects to not move, and the model does not have enough signal to learn to move them. The Forward Planner is able to learn some information from the short sequence dataset despite the sparsity of its signal, and outperforms the other methods on the longer sequence datasets.\nThe time required to plan a sequence of actions may be an important factor. Although the Forward Planner outperforms the other methods, it also takes 100 times longer to plan sequences, since it performs 100 optimization steps each requiring a forward and backward pass through the recurrent network. We therefore conducted an additional experiment measuring how performance changes with the number of optimization steps. In Figure 2 we plot the accuracy of the Forward Planner for different numbers of inference steps. We see that using 50 updates for the Forward Planner matches the accuracy of the other two methods on the Navigation Task, and accuracy steadily increases with the number of updates. For the Transport Task, the Forward Planner is able to match the performance of the Imitation Learner with very few updates."}, {"heading": "6 Conclusion", "text": "In this work, we introduced a simple yet effective method to perform planning in discrete action spaces using differentiable forward models of the environment. We addressed the issue of backpropagating into undefined regions by using a softmax paramaterization of the action vectors combined with added input noise, which we have shown theoretically and empirically to produce low-entropy solutions, i.e. solutions close to valid actions. Interestingly, this method performs better than explicitly imposing an entropy penalty term. This may be due to the additional term in the penalty induced by the added noise which penalizes the curvature of the loss surface, or because added noise may help escape local minima. Investigating this more thoroughly would be an interesting direction for future work. We also introduced two discrete planning tasks building on previous frameworks for question-answering. Although simple to describe, none of the methods we evaluated solved these tasks completely, hence they may serve for further development of planning algorithms."}, {"heading": "6.1 Experimental Details", "text": "All methods were trained using ADAM [20] using minibatches of size 32 for a maximum of two million updates. The forward models had an initial learning rate of 0.01, which we found to work well, and both Q-learners and Imitation Learners had their initial learning rate chosen by grid search over the set {0.01, 0.001, 0.0001}. For both the forward models and the Imitation Learner, we divided the learning rate by 2 during training whenever the training loss did not decrease for more than 3 epochs, where one epoch was set to be 2000 updates. Due to the noisiness of the reward signal, we did not use this annealing strategy for the Q-learner and instead linearly decreased the learning rate by a factor of 10 over the course of a million updates.\nThe target network of the Q-learner was updated after every 1000 batches of experience. Actions were chosen at training time using an -greedy policy, with starting at 0 = 0.5 and linearly interpolated to 0/10 over the course of a million updates.\nThe architecture of the forward model is given by:\nht = \u03c6(Uat + V ht\u22121) yt = \u03c3(Cht) (3)\nyt is a distribution over final states, and the model is trained to minimize the cross-entropy between yt and the desired final state.\nThe architecture of the Imitation Learner is given by the following equations:\nht = \u03c6(Uat + V ht\u22121 +Ws \u2032) zt = \u03c6(Aht +Bs \u2032)\nyt = \u03c3(Czt)\n(4)\nHere \u03c3 represents a softmax function, yt is a distribution over possible actions, and the model is trained to minimize the cross-entropy between yt and at+1 and thus to predict the next action conditioned on the current action sequence and goal state."}, {"heading": "6.2 Dataset Statistics", "text": ""}], "references": [{"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["Pieter Abbeel", "Adam Coates", "Morgan Quigley", "Andrew Y Ng"], "venue": "Advances in neural information processing systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y. Ng"], "venue": "In Proceedings of the Twenty-first International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "The effects of adding noise during backpropagation training on a generalization performance", "author": ["Guozhong An"], "venue": "Neural Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Efficient Reductions for Imitation Learning", "author": ["J Andrew Bagnell", "St\u00e9phane Ross"], "venue": "In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS) 2010,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Interaction networks for learning about objects, relations and physics", "author": ["Peter W. Battaglia", "Razvan Pascanu", "Matthew Lai", "Danilo Jimenez Rezende", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "A compositional object-based approach to learning physical dynamics", "author": ["Michael B. Chang", "Tomer Ullman", "Antonio Torralba", "Joshua B. Tenenbaum"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "The numerical solution of variational problems", "author": ["Stuart Dreyfus"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1962}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Shixiang Gu Eric Jang", "Ben Poole"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Learning visual predictive models of physics for playing billiards", "author": ["Katerina Fragkiadaki", "Pulkit Agrawal", "Sergey Levine", "Jitendra Malik"], "venue": "CoRR, abs/1511.07404,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "States versus rewards: Dissociable neural prediction error signals underlying model-based and model-free reinforcement learning", "author": ["Jan Glascher", "Nathaniel Daw", "Peter Dayan", "John P. O\u2019Doherty"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Deep reinforcement learning for robotic manipulation", "author": ["Shixiang Gu", "Ethan Holly", "Timothy P. Lillicrap", "Sergey Levine"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Metacontrol for adaptive imagination-based optimization", "author": ["Jessica Hamrick", "Andrew Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter Battaglia"], "venue": "ICLR 2017,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Tracking the world state with recurrent entity", "author": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "venue": "networks. ICLR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Don\u2019t until the final verb wait: Reinforcement learning for simultaneous machine translation", "author": ["Alvin C. Grissom Ii", "He He", "John Morgan", "Hal Daume III"], "venue": "Proceedings of EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Forward models: Supervised learning with a distal teacher", "author": ["Michael I. Jordan", "David E. Rumelhart"], "venue": "Cognitive Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "Video pixel networks", "author": ["Nal Kalchbrenner", "Aaron van den Oord", "Karen Simonyan", "Ivo Danihelka", "Oriol Vinyals", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1610.00527,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "An on-line algorithm for dynamic reinforcement learning and planning in reactive environments", "author": ["Henry Kelley"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1960}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Optimal control with learned local models: Application to dexterous manipulation", "author": ["Vikash Kumar", "Emanuel Todorov", "Sergey Levine"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Learning physical intuition of block towers by example", "author": ["Adam Lerer", "Sam Gross", "Rob Fergus"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Micha\u00ebl Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "CoRR, abs/1511.05440,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "IEEE Spoken Language Technology Workshop (SLT), Miami, FL,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Neural networks for control. chapter The Truck Backerupper: An Example of Self-learning in Neural Networks, pages 287\u2013299", "author": ["Derrick Nguyen", "Bernard Widrow"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1990}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder P. Singh", "Honglak Lee"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["Dean A. Pomerleau"], "venue": "Neural Computation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1991}, {"title": "An on-line algorithm for dynamic reinforcement learning and planning in reactive environments", "author": ["Jurgen Schmidhuber"], "venue": "In Proceedings of the International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1990}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I. Jordan", "Philipp Moritz"], "venue": "ICML, volume 37 of JMLR Workshop and Conference Proceedings,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "A generalized iterative lqg method for locally-optimal feedback control of constrained nonlinear stochastic systems", "author": ["Emanuel Todorov", "Weiwei Li"], "venue": "In American Control Conference,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "Towards ai-complete question answering: A set of prerequisite toy", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "tasks. CoRR,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "In recent years model-free RL using deep learning has proven successful for a number of applications including Atari games [26], robotic manipulation [11], navigation and reasoning tasks [28], and machine translation [16].", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "In recent years model-free RL using deep learning has proven successful for a number of applications including Atari games [26], robotic manipulation [11], navigation and reasoning tasks [28], and machine translation [16].", "startOffset": 150, "endOffset": 154}, {"referenceID": 27, "context": "In recent years model-free RL using deep learning has proven successful for a number of applications including Atari games [26], robotic manipulation [11], navigation and reasoning tasks [28], and machine translation [16].", "startOffset": 187, "endOffset": 191}, {"referenceID": 15, "context": "In recent years model-free RL using deep learning has proven successful for a number of applications including Atari games [26], robotic manipulation [11], navigation and reasoning tasks [28], and machine translation [16].", "startOffset": 217, "endOffset": 221}, {"referenceID": 30, "context": "However, it also suffers from several limitations, including a difficult temporal credit assignment problem, high sample complexity [31], and non-stationarity of the input distribution.", "startOffset": 132, "endOffset": 136}, {"referenceID": 28, "context": "An alternative approach is imitation learning [29] where the agent is trained to predict actions of an expert given a state; this can be cast as a supervised learning problem.", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "However, small inaccuracies can accumulate over time, leading to situations where the agent finds itself in situations not encountered during training [4].", "startOffset": 151, "endOffset": 154}, {"referenceID": 1, "context": "Furthermore, since the learner does not directly optimize a reward function and instead relies on the expert\u2019s policy to be close to optimal, in the absence of additional queries to the environment its performance will be bounded by that of the expert [2].", "startOffset": 252, "endOffset": 255}, {"referenceID": 9, "context": "Evidence from neuroscience also suggests that humans use forward models for certain decision-making tasks [10].", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "The work of [3] shows that adding independent noise to the weights of a neural network during training induces an additional term on the loss function.", "startOffset": 12, "endOffset": 15}, {"referenceID": 19, "context": "In the case of adaptive learning rates such as that used in ADAM [20], this corresponds to scaling the noise by the running average of the gradient magnitude, which may be desirable.", "startOffset": 65, "endOffset": 69}, {"referenceID": 33, "context": "Textual question-answering tasks are a common benchmark in machine learning [34, 15, 14].", "startOffset": 76, "endOffset": 88}, {"referenceID": 14, "context": "Textual question-answering tasks are a common benchmark in machine learning [34, 15, 14].", "startOffset": 76, "endOffset": 88}, {"referenceID": 13, "context": "Textual question-answering tasks are a common benchmark in machine learning [34, 15, 14].", "startOffset": 76, "endOffset": 88}, {"referenceID": 33, "context": "Several of the bAbI tasks introduced in [34] consist of stories describing the actions of one or more agents acting in the world, and the question requires the model to produce an answer about some aspect of the world which depends on the agent\u2019s actions.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "The first task we introduce is a navigation task, and is inspired by the World Model task in [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 25, "context": "Our first baseline was a Q-learning agent in the discrete space, trained via a method similar to [26] including target networks and replay memory.", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "As a second baseline, we trained a purely supervised Imitation Learner to predict each action conditioned on previous actions and the desired final state, using the Contextual RNN architecture introduced in [25].", "startOffset": 207, "endOffset": 211}, {"referenceID": 18, "context": "The idea of planning actions by backpropagating along a policy trajectory has existed since the 1960\u2019s [19, 7].", "startOffset": 103, "endOffset": 110}, {"referenceID": 6, "context": "The idea of planning actions by backpropagating along a policy trajectory has existed since the 1960\u2019s [19, 7].", "startOffset": 103, "endOffset": 110}, {"referenceID": 29, "context": "Later works [30, 27, 17] explored the idea of backpropagating through learned, approximate forward models of the environment.", "startOffset": 12, "endOffset": 24}, {"referenceID": 26, "context": "Later works [30, 27, 17] explored the idea of backpropagating through learned, approximate forward models of the environment.", "startOffset": 12, "endOffset": 24}, {"referenceID": 16, "context": "Later works [30, 27, 17] explored the idea of backpropagating through learned, approximate forward models of the environment.", "startOffset": 12, "endOffset": 24}, {"referenceID": 8, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 114, "endOffset": 127}, {"referenceID": 4, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 114, "endOffset": 127}, {"referenceID": 5, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 114, "endOffset": 127}, {"referenceID": 21, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 114, "endOffset": 127}, {"referenceID": 23, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 138, "endOffset": 146}, {"referenceID": 17, "context": "Several recent works have explored predictive modeling in more general settings, such as complex physical systems [9, 5, 6, 22] and video [24, 18].", "startOffset": 138, "endOffset": 146}, {"referenceID": 8, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 96, "endOffset": 99}, {"referenceID": 11, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 32, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 137, "endOffset": 152}, {"referenceID": 0, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 137, "endOffset": 152}, {"referenceID": 31, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 137, "endOffset": 152}, {"referenceID": 20, "context": "Other works have used forward models for planning in continous action spaces, such as billiards [9], vehicle navigation [12] or robotics [33, 1, 32, 21].", "startOffset": 137, "endOffset": 152}, {"referenceID": 22, "context": "There has been recent work in continuous relaxations of discrete random variables [23, 8], that also uses a softmax to form a continuous approximation to a discrete set.", "startOffset": 82, "endOffset": 89}, {"referenceID": 7, "context": "There has been recent work in continuous relaxations of discrete random variables [23, 8], that also uses a softmax to form a continuous approximation to a discrete set.", "startOffset": 82, "endOffset": 89}], "year": 2017, "abstractText": "Planning actions using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over model-free RL methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete, which may have limited its adoption. In this work, we introduce two discrete planning tasks inspired by existing question-answering datasets and show that it is in fact possible to effectively perform planning via backprop in discrete action spaces using two simple yet principled modifications. Our experiments show that this approach can significantly outperform model-free RL based methods and supervised imitation learners.", "creator": "LaTeX with hyperref package"}}}